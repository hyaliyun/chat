import{_ as p,o as a,c as n,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as w,e as v,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function A(i,e,l,m,s,r){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const S=p(k,[["render",A],["__scopeId","data-v-e0ed5816"]]),I=JSON.parse('[{"question":"**Question: Custom Exception Handling in Python** You are required to implement a custom exception handling system in Python. Your task is to define a set of custom exceptions and implement a function that processes a list of operations. Each operation consists of a dictionary with two keys: \\"operation\\" and \\"value\\". The function should perform error checking and handle any exceptions that may occur during the operations. # Definitions: 1. **Custom Exceptions**: - `InvalidOperationError`: Raised when an invalid operation is encountered. - `DivisionByZeroError`: Raised when a division by zero is attempted. - `OperationValueError`: Raised when the value provided for an operation is invalid (e.g., non-numeric value for arithmetic operations). 2. **Function**: ```python def process_operations(operations: List[Dict[str, Union[str, Any]]]) -> List[Union[int, float, str]]: # Function implementation ``` # Function Requirements: - The function `process_operations` takes a list of dictionaries as input. Each dictionary has two keys: - `operation`: A string that can be one of the following: \\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\". - `value`: An integer or float representing the value for the operation. - Implement error checking and use the custom exceptions to handle errors. You should catch these exceptions and append an appropriate error message to the result list. # Input/Output: - **Input**: `operations` is a list of dictionaries. Each dictionary has the following format: ```python [ {\\"operation\\": \\"add\\", \\"value\\": 10}, {\\"operation\\": \\"subtract\\", \\"value\\": 5}, {\\"operation\\": \\"multiply\\", \\"value\\": 3}, {\\"operation\\": \\"divide\\", \\"value\\": 0}, {\\"operation\\": \\"divide\\", \\"value\\": 2} ] ``` - **Output**: The function returns a list of results for each operation. If an error occurs, the corresponding result should be an error message. Example output for the provided input: ```python [10, 5, 15, \\"Division by zero error\\", 7.5] ``` # Constraints: - The operations list will always contain dictionaries with \\"operation\\" and \\"value\\" keys. - You can assume that \\"value\\" will either be an integer or a float, and \\"operation\\" will always be a string. - You do not need to handle unexpected dictionary formats or types. # Example: ```python class InvalidOperationError(Exception): pass class DivisionByZeroError(Exception): pass class OperationValueError(Exception): pass def process_operations(operations): result = [] accumulator = 0 for op in operations: try: operation = op[\'operation\'] value = op[\'value\'] if not isinstance(value, (int, float)): raise OperationValueError(\\"Operation value must be numeric\\") if operation == \'add\': accumulator += value elif operation == \'subtract\': accumulator -= value elif operation == \'multiply\': accumulator *= value elif operation == \'divide\': if value == 0: raise DivisionByZeroError(\\"Cannot divide by zero\\") accumulator /= value else: raise InvalidOperationError(f\\"Invalid operation: {operation}\\") result.append(accumulator) except (InvalidOperationError, DivisionByZeroError, OperationValueError) as e: result.append(str(e)) return result # Test case operations = [ {\\"operation\\": \\"add\\", \\"value\\": 10}, {\\"operation\\": \\"subtract\\", \\"value\\": 5}, {\\"operation\\": \\"multiply\\", \\"value\\": 3}, {\\"operation\\": \\"divide\\", \\"value\\": 0}, {\\"operation\\": \\"divide\\", \\"value\\": 2} ] print(process_operations(operations)) # Output: [10, 5, 15, \\"Cannot divide by zero\\", 7.5] ```","solution":"class InvalidOperationError(Exception): Raised when an invalid operation is encountered. pass class DivisionByZeroError(Exception): Raised when a division by zero is attempted. pass class OperationValueError(Exception): Raised when the value provided for an operation is invalid (e.g., non-numeric value for arithmetic operations). pass def process_operations(operations): Processes a list of operations and returns the result of each operation. Parameters: operations (List[Dict[str, Union[str, Any]]]): A list of dictionaries representing operations. Returns: List[Union[int, float, str]]: A list of results for each operation. If an error occurs, the result will be an error message. result = [] accumulator = 0 for op in operations: try: operation = op[\'operation\'] value = op[\'value\'] if not isinstance(value, (int, float)): raise OperationValueError(\\"Operation value must be numeric\\") if operation == \'add\': accumulator += value elif operation == \'subtract\': accumulator -= value elif operation == \'multiply\': accumulator *= value elif operation == \'divide\': if value == 0: raise DivisionByZeroError(\\"Cannot divide by zero\\") accumulator /= value else: raise InvalidOperationError(f\\"Invalid operation: {operation}\\") result.append(accumulator) except (InvalidOperationError, DivisionByZeroError, OperationValueError) as e: result.append(str(e)) return result"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of seaborn\'s `pointplot` function and its various customizations. # Task You are given a dataset of penguin measurements and flight passenger numbers. Your task is to generate a pointplot that meets the following specifications: 1. **Data Preparation**: - Load the penguins dataset using `sns.load_dataset(\\"penguins\\")`. - Load the flights dataset using `sns.load_dataset(\\"flights\\")`. 2. **Penguin Data Plot**: - Create a pointplot of `body_mass_g` across different islands. - Differentiate the data by `sex` using different colors. - Use different markers (`\\"o\\"` for male and `\\"s\\"` for female) and linestyles (`\\"-\\"` for male and `\\"--\\"` for female). - Show error bars representing the standard deviation (`\\"sd\\"`). 3. **Flights Data Plot**: - Transform the flights data into a wide format using `pivot` function. - Create a pointplot of the number of passengers in June across different years. - Customize the x-tick labels to show only the last two digits of the year, e.g., `1950` should be shown as `\'50`. # Expected Input and Output Formats - There are no input parameters for the function. All tasks should be performed within the specified function and parameters loaded using seaborn datasets. - Save the plots as `penguins_plot.png` and `flights_plot.png`. # Constraints - Ensure the plots are clear and well-labeled. - Follow seaborn best practices and settings to maintain visual consistency. # Performance Requirements - The function should execute within a reasonable time frame (a few seconds). # Write a function `create_plots` to implement the above specifications. ```python import seaborn as sns def create_plots(): # Set theme for seaborn sns.set_theme(style=\\"whitegrid\\") # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Create penguins plot penguins_plot = sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\") penguins_plot.figure.savefig(\\"penguins_plot.png\\") penguins_plot.figure.clf() # Clear the figure for reuse # Prepare flights data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create flights plot flights_plot = sns.pointplot(flights_wide[\\"Jun\\"], formatter=lambda x: f\\"\'{x % 1900}\\") flights_plot.figure.savefig(\\"flights_plot.png\\") ``` # Example When you run `create_plots()`, it should generate the following plots and save them as `.png` files: - A pointplot of body mass of penguins differentiated by island and sex with specified styles. - A pointplot of the number of flight passengers in June across different years with custom x-tick labels.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Set theme for seaborn sns.set_theme(style=\\"whitegrid\\") # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Create penguins plot penguins_plot = sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\") penguins_plot.set_title(\'Penguins: Body Mass by Island and Sex\') penguins_plot.figure.savefig(\\"penguins_plot.png\\") plt.clf() # Clear the current figure # Prepare flights data flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create flights plot flights_plot = sns.pointplot(x=flights_wide.index, y=flights_wide[\\"Jun\\"]) flights_plot.set_xticklabels([f\\"\'{str(year)[-2:]}\\" for year in flights_wide.index]) flights_plot.set_title(\'Flight Passengers in June over the Years\') flights_plot.set(xlabel=\'Year\', ylabel=\'Number of Passengers\') flights_plot.figure.savefig(\\"flights_plot.png\\") plt.clf() # Clear the current figure"},{"question":"Objective Implement a Python script that utilizes the `time` module to perform and measure various time-related tasks efficiently. Description You are required to implement a program that performs the following tasks: 1. **Get Current Time:** - Print the current local time in the format \\"YYYY-MM-DD HH:MM:SS\\". 2. **Elapsed Time Measurement:** - Implement a function `measure_function_runtime(func)` which takes another function `func` as an argument, measures and returns the time taken to execute that function (in seconds). 3. **Convert Time Formats:** - Implement a function `convert_to_gmt(struct_time_obj)` that takes a `struct_time` object representing a local time and returns a string of the equivalent GMT time in the format \\"YYYY-MM-DD HH:MM:SS\\". 4. **Performance Analysis:** - Implement a function `compare_performance()` which performs the following: - Measures the time taken to sleep for 2 seconds using `time.sleep(2)`. - Measures the time taken to execute an empty loop running 1,000,000 iterations. - Prints out the time taken for each task and determines which function was more efficient. Input and Output - The functions `measure_function_runtime(func)` and `convert_to_gmt(struct_time_obj)` should be properly tested within the `compare_performance()` function. Ensure the results are displayed clearly. - Handle any exceptions or errors gracefully and ensure that the output is well formatted. **Example:** ```python import time def measure_function_runtime(func): start_time = time.time() func() end_time = time.time() return end_time - start_time def convert_to_gmt(struct_time_obj): gmt_time = time.gmtime(time.mktime(struct_time_obj)) return time.strftime(\'%Y-%m-%d %H:%M:%S\', gmt_time) def compare_performance(): # Measuring sleep time def sleep_func(): time.sleep(2) sleep_time = measure_function_runtime(sleep_func) print(f\\"Time taken to sleep: {sleep_time} seconds\\") # Measuring loop time def loop_func(): for _ in range(1000000): pass loop_time = measure_function_runtime(loop_func) print(f\\"Time taken to run a loop: {loop_time} seconds\\") # More efficient task if sleep_time < loop_time: print(\\"Sleeping was more efficient.\\") else: print(\\"Running a loop was more efficient.\\") # Current local time local_time = time.localtime() print(\\"Current Local Time:\\", time.strftime(\'%Y-%m-%d %H:%M:%S\', local_time)) # Test time conversion print(\\"GMT Time:\\", convert_to_gmt(local_time)) # Performance comparison compare_performance() ``` Constraints - Ensure the solution is efficient in terms of both time and space complexity. - Use only the functions and methods provided by the `time` module. - The code should handle edge cases, such as the current time crossing from one second/minute/hour/day into the next. This task will test students\' ability to interact with the `time` module, format times correctly, and measure execution time of functions accurately.","solution":"import time def measure_function_runtime(func): Measures the runtime of a given function. start_time = time.time() func() end_time = time.time() return end_time - start_time def convert_to_gmt(struct_time_obj): Converts a local struct_time object to a GMT time string. gmt_time = time.gmtime(time.mktime(struct_time_obj)) return time.strftime(\'%Y-%m-%d %H:%M:%S\', gmt_time) def compare_performance(): Compares performance of sleeping for 2 seconds vs running a loop with 1,000,000 iterations. # Measuring sleep time def sleep_func(): time.sleep(2) sleep_time = measure_function_runtime(sleep_func) print(f\\"Time taken to sleep: {sleep_time} seconds\\") # Measuring loop time def loop_func(): for _ in range(1000000): pass loop_time = measure_function_runtime(loop_func) print(f\\"Time taken to run a loop: {loop_time} seconds\\") # More efficient task if sleep_time < loop_time: print(\\"Sleeping was more efficient.\\") else: print(\\"Running a loop was more efficient.\\") # Get and print current local time local_time = time.localtime() print(\\"Current Local Time:\\", time.strftime(\'%Y-%m-%d %H:%M:%S\', local_time)) # Test time conversion print(\\"GMT Time:\\", convert_to_gmt(local_time)) # Performance comparison compare_performance()"},{"question":"Objective Design and implement a Python function that demonstrates proficiency in handling lists using various methods and comprehensions. Problem Write a Python function named `matrix_sorter` that: 1. Accepts a list of lists (a 2D list) `matrix` as input, where each sublist represents a row of a matrix. 2. Sorts each row of the matrix in ascending order. 3. Sorts the entire matrix by the sum of the elements in each row in ascending order. 4. Flattens the sorted 2D list into a single list of tuples, where each tuple contains the original row index and the sorted elements of that row. **Function Signature:** ```python def matrix_sorter(matrix: list) -> list: pass ``` **Input:** - `matrix` (list of lists of integers): A 2D list where each sublist contains integers and represents a row of the matrix. The length of rows may vary. **Output:** - A list of tuples, where each tuple consists of an integer (the original row index) and list (sorted elements of the row). **Constraints:** - Each sublist will contain at least one integer. - The matrix will contain at least one row. - The matrix will contain at most 100 rows, and each row will contain at most 100 elements. Requirements 1. **Sort individual rows:** Each row should be sorted in ascending order. 2. **Sort by row sum:** The entire matrix should be sorted based on the sum of the elements of each row. 3. **Flatten the matrix:** The output should be a single list of tuples, where each tuple contains the original row index and the sorted row elements. Example Given the following `matrix`: ```python matrix = [ [3, 2, 5], [1, 4, 4], [6, 0, 0] ] ``` **Step-by-step**: 1. Sort individual rows: ```python sorted_rows = [ [2, 3, 5], [1, 4, 4], [0, 0, 6] ] ``` 2. Sort the matrix by the sum of the elements in each row: ```python sorted_matrix = [ [0, 0, 6], # sum is 6 [1, 4, 4], # sum is 9 [2, 3, 5] # sum is 10 ] ``` 3. Flatten the sorted matrix into a list of tuples: ```python result = [ (2, [0, 0, 6]), (1, [1, 4, 4]), (0, [2, 3, 5]) ] ``` Therefore, the expected output is: ```python [(2, [0, 0, 6]), (1, [1, 4, 4]), (0, [2, 3, 5])] ``` Usage You can test your function with different inputs to ensure its correctness and efficiency. ```python result = matrix_sorter([[3, 2, 5], [1, 4, 4], [6, 0, 0]]) print(result) # Output: [(2, [0, 0, 6]), (1, [1, 4, 4]), (0, [2, 3, 5])] ``` Good luck and happy coding!","solution":"def matrix_sorter(matrix: list) -> list: This function sorts each row of the matrix in ascending order, then sorts the entire matrix by the sum of the elements in each row, and finally returns a list of tuples where each tuple contains the original row index and the sorted elements of that row. # Create a list of tuples where each tuple contains the original row index and sorted row indexed_sorted_rows = [(i, sorted(row)) for i, row in enumerate(matrix)] # Sort the list of tuples by the sum of the elements in each row indexed_sorted_rows.sort(key=lambda x: sum(x[1])) return indexed_sorted_rows"},{"question":"**Problem Statement:** You are provided with a dataset to classify whether a patient has a specific medical condition. Your goal is to build a classifier using scikit-learn that optimally tunes its decision threshold to maximize the F1-score for the positive class (`1`). This is crucial because your classifier will be used in a real-world medical scenario where identifying positive cases accurately is more important than minimizing false positives. **Dataset:** You will use the `make_classification` function from scikit-learn to generate a synthetic dataset with the following specifications: - `n_samples=1000`: 1000 samples. - `n_features=20`: 20 features. - `n_classes=2`: Binary classification with two classes. - `weights=[0.1, 0.9]`: The classes are imbalanced with the positive class being 10% of the total. **Tasks:** 1. Generate the dataset using the above specifications. 2. Split the dataset into training and testing sets with `test_size` of 0.3. 3. Train a logistic regression model using the training data. 4. Use the `TunedThresholdClassifierCV` to tune the decision threshold of the trained logistic regression model to maximize the F1-score for the positive class (`1`). 5. Evaluate the tuned model on the testing set and provide the F1-score, precision, and recall for the positive class. **Input:** None (The dataset is generated within the function). **Output:** - A dictionary containing: - `f1_score`: The F1-score of the tuned model on the testing set. - `precision`: The precision of the tuned model on the testing set. - `recall`: The recall of the tuned model on the testing set. **Constraints:** - You must use scikit-learn\'s `LogisticRegression`, `TunedThresholdClassifierCV`, `train_test_split`, and necessary metrics (`f1_score`, `precision_score`, and `recall_score`). - You are not allowed to change the default parameters of the `TunedThresholdClassifierCV` for cross-validation. **Example Code for Generating Dataset:** ```python from sklearn.datasets import make_classification X, y = make_classification( n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=42 ) ``` **Function Signature:** ```python def tune_decision_threshold(): # Task 1: Generate the dataset X, y = make_classification( n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=42 ) # Task 2: Split the dataset into training and testing sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Task 3: Train a logistic regression model from sklearn.linear_model import LogisticRegression base_model = LogisticRegression() base_model.fit(X_train, y_train) # Task 4: Tune the decision threshold to maximize F1-score for positive class from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, f1_score scorer = make_scorer(f1_score, pos_label=1) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Task 5: Evaluate the tuned model on the test set from sklearn.metrics import precision_score, recall_score y_pred_tuned = tuned_model.predict(X_test) results = { \'f1_score\': f1_score(y_test, y_pred_tuned, pos_label=1), \'precision\': precision_score(y_test, y_pred_tuned, pos_label=1), \'recall\': recall_score(y_test, y_pred_tuned, pos_label=1) } return results ``` **Your implementation should follow the structure of the function and fulfill all the given tasks. Make sure to import necessary packages within the function.**","solution":"def tune_decision_threshold(): # Task 1: Generate the dataset from sklearn.datasets import make_classification X, y = make_classification( n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=42 ) # Task 2: Split the dataset into training and testing sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Task 3: Train a logistic regression model from sklearn.linear_model import LogisticRegression base_model = LogisticRegression() base_model.fit(X_train, y_train) # Task 4: Tune the decision threshold to maximize F1-score for positive class from sklearn.metrics import make_scorer, f1_score from sklearn.base import BaseEstimator, ClassifierMixin import numpy as np class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, classifier, scoring=None): self.classifier = classifier self.scoring = scoring def fit(self, X, y): self.classifier.fit(X, y) thresholds = np.linspace(0, 1, 101) best_threshold = 0 best_score = 0 for threshold in thresholds: y_scores = self.classifier.predict_proba(X)[:, 1] y_pred = (y_scores >= threshold).astype(int) score = self.scoring._score_func(y, y_pred) if score > best_score: best_score = score best_threshold = threshold self.best_threshold = best_threshold return self def predict(self, X): y_scores = self.classifier.predict_proba(X)[:, 1] return (y_scores >= self.best_threshold).astype(int) scorer = make_scorer(f1_score, pos_label=1) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Task 5: Evaluate the tuned model on the testing set from sklearn.metrics import precision_score, recall_score y_pred_tuned = tuned_model.predict(X_test) results = { \'f1_score\': f1_score(y_test, y_pred_tuned, pos_label=1), \'precision\': precision_score(y_test, y_pred_tuned, pos_label=1), \'recall\': recall_score(y_test, y_pred_tuned, pos_label=1) } return results"},{"question":"Objective You are tasked with building a machine learning pipeline that performs feature selection followed by classification to predict a target variable. To assess your understanding of scikit-learn\'s feature selection capabilities, you must implement a solution that combines various feature selection techniques with a classifier in a pipeline. Problem Statement Use the `load_wine` dataset from scikit-learn. This dataset contains various chemical properties of wines and a target label indicating the class of the wine. 1. **Load the Dataset**: Load the wine dataset using `load_wine` function from `sklearn.datasets`. 2. **Preprocess the Data**: - Remove features with low variance using `VarianceThreshold` with a threshold of 0.1. - Perform univariate feature selection using `SelectKBest` with `f_classif` scoring function to select 75% of the features. 3. **Build a Classifier**: - Use a `RandomForestClassifier` to classify the wine types based on the selected features. 4. **Create a Pipeline**: - Combine the preprocessing steps and the classifier into a pipeline using `Pipeline` from `sklearn.pipeline`. 5. **Evaluate the Model**: - Perform cross-validation using `cross_val_score` to evaluate the model\'s accuracy with 5-fold cross-validation. 6. **Output**: - Print the cross-validation scores and the mean accuracy. Constraints - You must use `VarianceThreshold` and `SelectKBest` for feature selection. - Use `RandomForestClassifier` from `sklearn.ensemble` for classification. - Use 5-fold cross-validation. Expected Functions ```python import numpy as np from sklearn.datasets import load_wine from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def wine_classification_pipeline(): # Load the dataset data = load_wine() X, y = data.data, data.target # Define the feature selection and classifier variance_selector = VarianceThreshold(threshold=0.1) kbest_selector = SelectKBest(score_func=f_classif, k=\'all\') classifier = RandomForestClassifier(random_state=42) # Create the pipeline pipeline = Pipeline([ (\'variance_threshold\', variance_selector), (\'select_k_best\', kbest_selector), (\'classifier\', classifier) ]) # Perform cross-validation cv_scores = cross_val_score(pipeline, X, y, cv=5) # Output results print(\\"Cross-validation scores:\\", cv_scores) print(\\"Mean accuracy:\\", np.mean(cv_scores)) # Execute the function wine_classification_pipeline() ``` Additional Information - You can find the required functions and classes in the following modules: - `sklearn.datasets` - `sklearn.feature_selection` - `sklearn.ensemble` - `sklearn.pipeline` - `sklearn.model_selection` Please follow the constraints and expected functions to implement your solution correctly.","solution":"import numpy as np from sklearn.datasets import load_wine from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def wine_classification_pipeline(): # Load the dataset data = load_wine() X, y = data.data, data.target # Define the feature selection and classifier variance_selector = VarianceThreshold(threshold=0.1) kbest_selector = SelectKBest(score_func=f_classif, k=int(0.75 * X.shape[1])) classifier = RandomForestClassifier(random_state=42) # Create the pipeline pipeline = Pipeline([ (\'variance_threshold\', variance_selector), (\'select_k_best\', kbest_selector), (\'classifier\', classifier) ]) # Perform cross-validation cv_scores = cross_val_score(pipeline, X, y, cv=5) # Output results print(\\"Cross-validation scores:\\", cv_scores) print(\\"Mean accuracy:\\", np.mean(cv_scores)) # Return results for testing return cv_scores, np.mean(cv_scores) # Execute the function to see output wine_classification_pipeline()"},{"question":"<|Analysis Begin|> The provided documentation offers an extensive overview of the abstract base classes (ABCs) in the `collections.abc` module. These classes are intended to provide interfaces that can be used to test whether a class provides particular functionalities, such as being hashable, iterable, or a mapping. Each ABC defines certain abstract methods that must be implemented by the inheriting or registering class. Some ABCs also provide mixin methods that offer a default implementation of some methods based on the abstract methods. Key points to focus on: 1. ABCs like \\"Container\\", \\"Hashable\\", \\"Sized\\", etc. 2. How to inherit directly from these ABCs or register existing classes as virtual subclasses. 3. The importance of implementing required abstract methods and the optional mixin methods. 4. Examples of creating custom classes using these ABCs and mixin methods. 5. Performance considerations when implementing methods that may have linear or constant access speeds. Given this information, the question should challenge students to design and implement a custom class that inherits from one or more of these ABCs. This will test their understanding of abstract methods, inheritance, mixin methods, and how to correctly implement and use ABCs. <|Analysis End|> <|Question Begin|> # Coding Question: Implementing a Custom Collection Class Objective Design a custom class that represents a unique collection of integer numbers. This collection should inherit from appropriate abstract base classes from the `collections.abc` module. The class should demonstrate your understanding of abstract methods and mixin methods provided by these ABCs. Requirements 1. **Class Declaration:** - Create a class named `UniqueIntCollection` that should inherit from `collections.abc.MutableSet`. 2. **Abstract Methods:** - Implement all necessary abstract methods required by the `MutableSet` ABC: - `__contains__(self, value)` - `__iter__(self)` - `__len__(self)` - `add(self, value)` - `discard(self, value)` 3. **Mixin Methods:** - Utilize mixin methods provided by `MutableSet` to ensure the class supports the following operations: - Union (`|`) - Intersection (`&`) - Difference (`-`) - Symmetric Difference (`^`) - Subset (`<`) - Superset (`>`) 4. **Additional Methods:** - Implement a method `__str__(self)` that returns a string representation of the collection elements in ascending order. 5. **Performance:** - Ensure that the `__contains__`, `add`, and `discard` methods have an average time complexity of O(1) by using an appropriate internal data structure. Input and Output Formats 1. **Initialization:** - The class should be initialized with an optional iterable of integers. Any duplicates in the initial iterable should be ignored. ```python collection = UniqueIntCollection([1, 2, 2, 3]) ``` 2. **Containment Check:** - Checking if a value is in the collection should be done using the `in` operator. ```python assert 1 in collection # should return True assert 4 in collection # should return False ``` 3. **Adding and Removing Elements:** - Elements should be added or removed from the collection using the `add` and `discard` methods. ```python collection.add(4) collection.discard(2) ``` 4. **String Representation:** - The string representation should display elements in ascending order. ```python print(collection) # e.g., \\"{1, 3, 4}\\" ``` 5. **Set Operations:** - The class should support union, intersection, difference, symmetric difference, subset, and superset operations. ```python collection2 = UniqueIntCollection([3, 4, 5]) union = collection | collection2 intersection = collection & collection2 ``` Constraints - All elements in the collection are guaranteed to be integers. - The initial iterable provided for initialization may contain duplicate values. ```python # Sample Implementation Skeleton from collections.abc import MutableSet class UniqueIntCollection(MutableSet): def __init__(self, iterable=None): pass def __contains__(self, value): pass def __iter__(self): pass def __len__(self): pass def add(self, value): pass def discard(self, value): pass def __str__(self): pass ``` Implement the `UniqueIntCollection` class according to the requirements above.","solution":"from collections.abc import MutableSet class UniqueIntCollection(MutableSet): def __init__(self, iterable=None): self._data = set() if iterable: for value in iterable: self.add(value) def __contains__(self, value): return value in self._data def __iter__(self): return iter(self._data) def __len__(self): return len(self._data) def add(self, value): self._data.add(value) def discard(self, value): self._data.discard(value) def __str__(self): return \\"{\\" + \\", \\".join(map(str, sorted(self._data))) + \\"}\\" # Example usage: # collection = UniqueIntCollection([1, 2, 3]) # print(collection) # Should print: {1, 2, 3}"},{"question":"# Advanced PyTorch XPU Assessment Objective: To assess your understanding and proficiency with the PyTorch `torch.xpu` module, specifically focusing on device management, stream handling, and memory management. Problem Statement: You are tasked with writing a PyTorch-based function that: 1. Initializes and sets up the XPU device. 2. Creates two different streams for parallel execution. 3. Allocates a tensor in the XPU memory and performs operations within different streams. 4. Synchronizes the streams and then performs an additional operation on the result. 5. Manages memory efficiently, clearing any cached memory at the end. You will need to implement the function `xpu_tensor_operations` with the following specifications: Function Signature: ```python def xpu_tensor_operations(size: int) -> torch.Tensor: pass ``` Input: - `size` (int): The size of the one-dimensional tensor to be created on the XPU device. Output: - Returns a `torch.Tensor` that results from the execution of the operations on the XPU device. Details: 1. **Device Initialization**: - Ensure that the XPU is available and initialized. - Set the active device to the XPU. Raise an appropriate error if no XPU is available. 2. **Stream Creation and Execution**: - Create two different streams. - In Stream 1: - Allocate a tensor of the given size with values initialized to 1.0. - Perform an element-wise multiplication of the tensor by 2. - In Stream 2: - Allocate a tensor of the same size with values initialized to 2.0. - Perform an element-wise addition of the tensor with 3. 3. **Synchronization**: - Synchronize both streams to ensure all operations are completed. 4. **Post-Synchronization Operation**: - After synchronization, sum the results of the operations from the two streams. - For example, if the first stream results in tensor `A` and the second in tensor `B`, the final result should be `A + B`. 5. **Memory Management**: - Ensure that all allocated memory is properly managed and clear any cached memory at the end of the function using `empty_cache`. Constraints: - You must use XPU-specific operations and handle potential errors gracefully. - Optimizations for performance are encouraged. Example: ```python >>> result_tensor = xpu_tensor_operations(5) >>> print(result_tensor) tensor([7., 7., 7., 7., 7.], device=\'xpu:0\') ``` The value computation steps are: - Stream 1 results (multiplying tensor by 2): tensor([2, 2, 2, 2, 2]) - Stream 2 results (adding 3 to tensor): tensor([5, 5, 5, 5, 5]) - Final result (sum of both results): tensor([7, 7, 7, 7, 7]) **Note**: Ensure your solution handles unforeseen issues and manages memory efficiently on the XPU device.","solution":"import torch def xpu_tensor_operations(size: int) -> torch.Tensor: if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available.\\") device = torch.device(\\"xpu\\") stream1 = torch.xpu.Stream(device) stream2 = torch.xpu.Stream(device) with torch.xpu.stream(stream1): tensor1 = torch.ones(size, device=device) tensor1.mul_(2) with torch.xpu.stream(stream2): tensor2 = torch.full((size,), 2.0, device=device) tensor2.add_(3) stream1.synchronize() stream2.synchronize() result = tensor1 + tensor2 torch.xpu.empty_cache() return result"},{"question":"# Advanced Python Coding Assessment **Objective:** You are required to demonstrate your comprehension and application of the `bz2` module for bzip2 compression and decompression by implementing a function that compresses a directory of text files and then decompresses them back to verify the content. **Problem Statement:** Create a Python script that: 1. Compresses all `.txt` files in a given directory using the highest compression level (9). 2. Stores these compressed files with the same name but with a `.bz2` extension in the same directory. 3. Decompresses these `.bz2` files to a new directory, ensuring the original content is preserved. 4. Verifies the integrity of the decompressed files by comparing their content with the original files. **Function Signature:** ```python def compress_and_verify_directory(input_dir: str, output_dir: str) -> bool: Compresses all `.txt` files in the input_dir, decompresses them into the output_dir, and verifies if the decompressed files match the original files. Args: input_dir (str): The path to the input directory containing text files to be compressed. output_dir (str): The path to the output directory where decompressed files will be stored. Returns: bool: True if all decompressed files match the original files, False otherwise. pass ``` **Input:** - `input_dir`: A string representing the path to the directory containing `.txt` files. - `output_dir`: A string representing the path to the directory where decompressed files will be stored. **Output:** - The function should return `True` if all decompressed files match the original files, `False` otherwise. **Constraints:** - The directory paths provided will always be valid. - Both input and output directories will be on the local file system. **Example Usage:** ```python input_dir = \\"path/to/input\\" output_dir = \\"path/to/output\\" result = compress_and_verify_directory(input_dir, output_dir) print(result) # Should print True if all decompressed files match the original files, False otherwise ``` **Notes:** 1. Ensure that the function handles file reading, writing, and exceptional cases, such as empty directories or non-text files in the input directory. 2. You can assume that the input directory only contains text files for the sake of simplicity. # Performance Requirements: 1. Ensure the solution handles large files and numerous files efficiently. 2. The function should not leak resources (i.e., make sure all files are properly closed after operation). Good luck!","solution":"import os import bz2 def compress_and_verify_directory(input_dir: str, output_dir: str) -> bool: Compresses all `.txt` files in the input_dir, decompresses them into the output_dir, and verifies if the decompressed files match the original files. Args: input_dir (str): The path to the input directory containing text files to be compressed. output_dir (str): The path to the output directory where decompressed files will be stored. Returns: bool: True if all decompressed files match the original files, False otherwise. if not os.path.exists(output_dir): os.makedirs(output_dir) txt_files = [f for f in os.listdir(input_dir) if f.endswith(\'.txt\')] for file_name in txt_files: with open(os.path.join(input_dir, file_name), \'rb\') as f_in: with bz2.open(os.path.join(input_dir, file_name + \'.bz2\'), \'wb\', compresslevel=9) as f_out: f_out.write(f_in.read()) match = True for file_name in txt_files: with bz2.open(os.path.join(input_dir, file_name + \'.bz2\'), \'rb\') as f_in: decompressed_data = f_in.read() with open(os.path.join(output_dir, file_name), \'wb\') as f_out: f_out.write(decompressed_data) with open(os.path.join(input_dir, file_name), \'rb\') as original, open(os.path.join(output_dir, file_name), \'rb\') as decompressed: if original.read() != decompressed.read(): match = False break return match"},{"question":"**Question: Implement an Audit Event Logger in Python** Python 3.8 introduced the ability to audit and log events through `sys.audit` calls. This allows developers to monitor certain sensitive actions more closely. For this task, you need to implement an auditing hook that logs specific events into a file. **Task:** 1. Write a function `log_audit_event(event, args)` that: - Takes two arguments: * `event`: a string representing the event name. * `args`: a tuple containing the event arguments. - Writes the event name and its arguments to a file named `audit_log.txt`. Each event should be on a new line in the format: `EVENT_NAME: ARG1, ARG2, ...`. 2. Write a function `setup_audit_hook()` that: - Sets up an audit hook using `sys.addaudithook(log_audit_event)`. 3. Simulate several actions that trigger audit events and ensure that they are logged correctly to `audit_log.txt`. **Constraints:** - The audit hook should be installed only once. - Handle potential file writing exceptions gracefully. **Input and Output:** * The main function will set up the audit hook and perform several actions (e.g., creating an array, using input, accessing id). * The output should be checked in the file `audit_log.txt`. **Example Execution:** ```python import sys # Step 1: Define the logging function def log_audit_event(event, args): try: with open(\\"audit_log.txt\\", \\"a\\") as log_file: log_file.write(f\\"{event}: {\', \'.join(map(str, args))}n\\") except Exception as e: print(f\\"Error logging event: {e}\\") # Step 2: Setup the audit hook def setup_audit_hook(): sys.addaudithook(log_audit_event) # Step 3: Trigger some audit events if __name__ == \\"__main__\\": setup_audit_hook() import array arr = array.array(\'i\', [1, 2, 3]) user_input = input(\\"Please enter something: \\") obj_id = id(arr) ``` **File `audit_log.txt` should contain entries such as:** ``` array.__new__: i, [1, 2, 3] builtins.input: Please enter something: builtins.input/result: user_input builtins.id: <id_value> ``` **Note:** - Make sure to simulate a variety of events to verify the implementation correctly logs different types of audit events. - Verify the content of `audit_log.txt` to ensure it contains the expected entries.","solution":"import sys def log_audit_event(event, args): Log the audit event to a file. Parameters: - event (str): The name of the event. - args (tuple): The arguments associated with the event. try: with open(\\"audit_log.txt\\", \\"a\\") as log_file: log_file.write(f\\"{event}: {\', \'.join(map(str, args))}n\\") except Exception as e: print(f\\"Error logging event: {e}\\") def setup_audit_hook(): Setup the audit hook to log audit events. sys.addaudithook(log_audit_event) # Simulate several actions that trigger audit events if __name__ == \\"__main__\\": setup_audit_hook() import array arr = array.array(\'i\', [1, 2, 3]) user_input = input(\\"Please enter something: \\") obj_id = id(arr) print(f\\"Array ID: {obj_id}\\")"},{"question":"**Coding Assessment Question:** **Objective:** Demonstrate your understanding of the `seaborn.husl_palette` function by creating a function that generates and visualizes different color palettes based on user inputs. **Problem Statement:** Write a function `visualize_palettes` that generates and visualizes different color palettes based on user inputs. The function should take three parameters: 1. `n_colors` (int): The number of colors in the palette. 2. `lightness` (float): The lightness level of the colors, where 0.0 is black and 1.0 is white. 3. `saturation` (float): The saturation level of the colors, where 0.0 is grayscale and 1.0 is fully saturated. The function should: 1. Generate a color palette using the `seaborn.husl_palette` function with the specified `n_colors`, `lightness`, and `saturation`. 2. Return the color palette. 3. Plot the generated color palette using `seaborn` to visualize the colors. **Additional Requirements:** - Ensure appropriate handling of edge cases such as invalid input values (e.g., negative numbers, numbers greater than allowed ranges). - Use appropriate plotting techniques to clearly illustrate the generated palette. **Constraints:** - `n_colors` should be an integer between 1 and 20. - `lightness` and `saturation` should be floats between 0.0 and 1.0. **Example:** ```python def visualize_palettes(n_colors, lightness, saturation): import seaborn as sns import matplotlib.pyplot as plt # Validate inputs if not (1 <= n_colors <= 20): raise ValueError(\\"n_colors must be between 1 and 20\\") if not (0.0 <= lightness <= 1.0): raise ValueError(\\"lightness must be between 0.0 and 1.0\\") if not (0.0 <= saturation <= 1.0): raise ValueError(\\"saturation must be between 0.0 and 1.0\\") # Generate color palette palette = sns.husl_palette(n_colors=n_colors, l=lightness, s=saturation) # Plot the palette sns.palplot(palette) plt.show() return palette # Example usage: visualize_palettes(10, 0.5, 0.7) ``` When completed, your function should generate and visualize the specified color palette.","solution":"def visualize_palettes(n_colors, lightness, saturation): import seaborn as sns import matplotlib.pyplot as plt # Validate inputs if not isinstance(n_colors, int) or not (1 <= n_colors <= 20): raise ValueError(\\"n_colors must be an integer between 1 and 20\\") if not (isinstance(lightness, float) or isinstance(lightness, int)) or not (0.0 <= lightness <= 1.0): raise ValueError(\\"lightness must be a float between 0.0 and 1.0\\") if not (isinstance(saturation, float) or isinstance(saturation, int)) or not (0.0 <= saturation <= 1.0): raise ValueError(\\"saturation must be a float between 0.0 and 1.0\\") # Generate color palette palette = sns.husl_palette(n_colors=n_colors, l=lightness, s=saturation) # Plot the palette sns.palplot(palette) plt.show() return palette"},{"question":"You are given a task to simulate handling opaque data using Python\'s PyCapsule API (a concept borrowed from the C extension modules). Although you won\'t be using actual C code, you\'ll be simulating the handling of opaque data structures in Python. Your task is to write Python functions that mimic some of the actions performed by the PyCapsule API. Task Implement a Python class `PyCapsuleSimulator` with the following methods: 1. `__init__(self, pointer, name=None, destructor=None)`: Initializes the capsule with a pointer (any Python object), an optional name (string), and an optional destructor (a callable that takes one argument). 2. `get_pointer(self)`: Returns the encapsulated pointer. 3. `get_name(self)`: Returns the name stored in the capsule. Returns `None` if there is no name. 4. `set_pointer(self, pointer)`: Sets a new pointer for the capsule. Raises a `ValueError` if the pointer is `None`. 5. `set_name(self, name)`: Sets a new name for the capsule. 6. `call_destructor(self)`: Calls the destructor if it is not `None`, passing the capsule itself as an argument. 7. `is_valid(self, name)`: Checks if the capsule is valid. A capsule is considered valid if its pointer is not `None` and if the given name (string) matches the capsule\'s name using exact string comparison (case-sensitive). Returns `True` if valid, otherwise `False`. ```python class PyCapsuleSimulator: def __init__(self, pointer, name=None, destructor=None): # Implementation here def get_pointer(self): # Implementation here def get_name(self): # Implementation here def set_pointer(self, pointer): # Implementation here def set_name(self, name): # Implementation here def call_destructor(self): # Implementation here def is_valid(self, name): # Implementation here ``` Example Usage ```python def my_destructor(capsule): print(\\"Destructor called for capsule with name:\\", capsule.get_name()) capsule = PyCapsuleSimulator(pointer=42, name=\\"my_module.my_attribute\\", destructor=my_destructor) print(capsule.get_pointer()) # Output: 42 print(capsule.get_name()) # Output: my_module.my_attribute capsule.set_pointer(99) print(capsule.get_pointer()) # Output: 99 capsule.call_destructor() # Output: Destructor called for capsule with name: my_module.my_attribute print(capsule.is_valid(\\"my_module.my_attribute\\")) # Output: True print(capsule.is_valid(\\"wrong_name\\")) # Output: False ``` Constraints - The `pointer` can be any non-None Python object. - For `set_pointer`, raising `ValueError` if `None` is provided. - The `name` can be `None` or a non-empty string. - The `destructor` should be callable if provided, and will be called with the capsule as an argument. - You should not use any external libraries beyond Python\'s standard library.","solution":"class PyCapsuleSimulator: def __init__(self, pointer, name=None, destructor=None): if pointer is None: raise ValueError(\\"Pointer cannot be None\\") self.pointer = pointer self.name = name self.destructor = destructor def get_pointer(self): return self.pointer def get_name(self): return self.name def set_pointer(self, pointer): if pointer is None: raise ValueError(\\"Pointer cannot be None\\") self.pointer = pointer def set_name(self, name): self.name = name def call_destructor(self): if self.destructor: self.destructor(self) def is_valid(self, name): return self.pointer is not None and self.name == name"},{"question":"# Clustering and Evaluation using Scikit-learn **Objective:** Implement a clustering algorithm using scikit-learn, apply it to a dataset, and evaluate its performance using applicable clustering metrics. **Problem Statement:** You are provided with a dataset of points in a 2D space. Your task is to: 1. Implement K-Means clustering using scikit-learn. 2. Evaluate the performance of the clustering using the Silhouette Coefficient and the Davies-Bouldin Index. **Specifications:** 1. **Input:** - `data`: A list of n points where each point is represented as a tuple (x, y). Example: `data = [(1, 2), (2, 3), (3, 4), (8, 9), (9, 10)]` - `n_clusters`: An integer specifying the number of clusters to form. 2. **Output:** - A dictionary with the following keys: - `labels`: A list of integers where each integer indicates the cluster label assigned to each point. - `silhouette_score`: A float representing the Silhouette Coefficient of the clustering. - `davies_bouldin_score`: A float representing the Davies-Bouldin Index of the clustering. 3. **Constraints:** - Ensure `n_clusters` is at least 2 and at most the number of points. - Validate the input to handle any edge cases or incorrect data formats. 4. **Performance Requirements:** - Make use of scikit-learn\'s built-in implementations for algorithms and metrics to ensure efficiency. # Implementation Guidelines: 1. **Setup:** - Import necessary modules from scikit-learn. 2. **Data Preprocessing:** Convert the given list of points to a format compatible with scikit-learn (e.g., NumPy array). 3. **Clustering:** - Use `KMeans` from scikit-learn to perform clustering on the dataset. - Store the resulting cluster labels. 4. **Evaluation:** - Use `metrics.silhouette_score` to compute the Silhouette Coefficient. - Use `metrics.davies_bouldin_score` to compute the Davies-Bouldin Index. # Function Signature: ```python from typing import List, Tuple, Dict def cluster_and_evaluate(data: List[Tuple[float, float]], n_clusters: int) -> Dict: pass ``` # Example: ```python data = [(1, 2), (2, 3), (3, 4), (8, 9), (9, 10)] n_clusters = 2 result = cluster_and_evaluate(data, n_clusters) print(result) # Expected Output: # { # \\"labels\\": [0, 0, 0, 1, 1], # or similar depending on random initialization # \\"silhouette_score\\": float_value, # \\"davies_bouldin_score\\": float_value # } ``` Ensure that your implementation is robust, handles edge cases, and validates inputs appropriately.","solution":"from typing import List, Tuple, Dict from sklearn.cluster import KMeans from sklearn import metrics import numpy as np def cluster_and_evaluate(data: List[Tuple[float, float]], n_clusters: int) -> Dict: if not data or n_clusters < 2 or n_clusters > len(data): raise ValueError(\\"Number of clusters must be between 2 and the number of data points.\\") # Convert data to numpy array X = np.array(data) # Apply KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(X) labels = kmeans.labels_ # Evaluate the clustering silhouette_score = metrics.silhouette_score(X, labels) davies_bouldin_score = metrics.davies_bouldin_score(X, labels) return { \\"labels\\": labels.tolist(), \\"silhouette_score\\": silhouette_score, \\"davies_bouldin_score\\": davies_bouldin_score }"},{"question":"**Objective:** Design a function that programmatically creates and sends a MIME email using the \\"email\\" package in Python. The email should contain both plain text and HTML content, along with multiple file attachments of different MIME types. **Function Signature:** ```python def send_complex_email( subject: str, sender: str, recipients: list, plain_text: str, html_content: str, attachment_paths: list ) -> None: ``` **Inputs:** - `subject` (str): The subject of the email. - `sender` (str): The sender’s email address. - `recipients` (list): A list of recipient email addresses. - `plain_text` (str): The plain text content of the email. - `html_content` (str): The HTML content of the email. - `attachment_paths` (list): A list of file paths for the attachments. **Function Requirements:** 1. Create an email message with both plain text and HTML content. 2. Attach multiple files to the email. The MIME types of the attachments should be correctly identified and set. 3. Send the email using an SMTP server (you can assume the SMTP server is running on localhost). **Constraints:** 1. Use the examples provided in the `email` package documentation for guidance. 2. Ensure the attachments have the correct MIME types. 3. Handle any potential exceptions that may occur during the email creation or sending process (e.g., file not found, SMTP connection issues). **Example:** ```python subject = \\"Project Update\\" sender = \\"project.manager@example.com\\" recipients = [\\"employee1@example.com\\", \\"employee2@example.com\\"] plain_text = \\"Please find the project update attached.\\" html_content = \\"<html><body><h1>Project Update</h1><p>Please find the project update attached.</p></body></html>\\" attachment_paths = [\\"update.pdf\\", \\"report.xlsx\\"] send_complex_email(subject, sender, recipients, plain_text, html_content, attachment_paths) ``` **Output:** The function should not return anything. Instead, it should send the email with the specified content and attachments to the recipients. **Important Notes:** - Ensure all email headers and contents, including text and HTML parts, are set correctly. - Ensure that all attachments are included with the correct MIME types. - Proper error handling should be implemented to deal with common issues such as missing files or SMTP server errors.","solution":"import smtplib from email.message import EmailMessage from email.utils import make_msgid from mimetypes import guess_type import os def send_complex_email( subject: str, sender: str, recipients: list, plain_text: str, html_content: str, attachment_paths: list ) -> None: msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) # Set the message ID to ensure HTML and plain text maintain their relation. msg.set_content(plain_text) msg.add_alternative(html_content, subtype=\'html\') # Add attachments for path in attachment_paths: try: # Guess the MIME type and encoding mime_type, _ = guess_type(path) if mime_type is None: mime_type = \'application/octet-stream\' main_type, sub_type = mime_type.split(\'/\') with open(path, \'rb\') as file: file_data = file.read() file_name = os.path.basename(path) msg.add_attachment(file_data, maintype=main_type, subtype=sub_type, filename=file_name) except FileNotFoundError as e: print(f\\"Error: {e}\\") return except Exception as e: print(f\\"Unexpected Error: {e}\\") return try: with smtplib.SMTP(\'localhost\') as smtp: smtp.send_message(msg) print(\\"Email sent successfully!\\") except Exception as e: print(f\\"Failed to send email: {e}\\") return"},{"question":"# Command-Line Tool Development Using `optparse` **Objective** Your task is to implement a Python script using the `optparse` module to handle command-line options and arguments. This script will involve creating a command-line interface (CLI) for a fictional application that processes a list of numbers. The program should offer functionalities to read numbers from a file, specify the type of operation to be performed on them (e.g., sum, average), and an optional verbosity flag. **Requirements** 1. Implement the command-line parsing using the `optparse` module. 2. Handle the following command-line options: - `-f`, `--file`: Specifies the filename to read the numbers from. - `-o`, `--operation`: Specifies the operation to perform on the numbers. Allowed values are `sum` and `average`. - `-v`, `--verbose`: Optional flag to enable verbose mode, which prints additional details during execution. 3. The script should read numbers from the specified file (one number per line). 4. Based on the chosen operation, perform the calculation and print the result. 5. If the verbose flag is enabled, print additional information during processing (e.g., the list of numbers read, intermediate steps, etc.). 6. Handle errors gracefully (e.g., file not found, invalid operation). **Specification and Constraints** - **Input Format**: The script is run from the command line with the specified options. - **Output Format**: - The result of the calculation is printed to the standard output. - If verbose mode is enabled, print additional debug information. - **Constraints**: - The filename must be provided. - The operation must be either `sum` or `average`. **Example Usage** ```sh # To sum the numbers in the file \'numbers.txt\' python yourscript.py --file=numbers.txt --operation=sum # To calculate the average in verbose mode python yourscript.py -f numbers.txt -o average -v ``` **Implementation** You need to implement the following function inside the script as a starting point: ```python from optparse import OptionParser def parse_arguments(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"read numbers from FILE\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--operation\\", dest=\\"operation\\", type=\\"choice\\", choices=[\\"sum\\", \\"average\\"], help=\\"operation to perform on the numbers: sum or average\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"enable verbose mode\\") (options, args) = parser.parse_args() if not options.filename: parser.error(\\"Filename not provided\\") if not options.operation: parser.error(\\"Operation not provided\\") return options def main(): options = parse_arguments() try: with open(options.filename, \'r\') as file: numbers = [float(line.strip()) for line in file.readlines()] except FileNotFoundError: print(\\"Error: file not found\\") return except ValueError: print(\\"Error: file contains non-numeric data\\") return if options.verbose: print(f\\"Numbers read from file: {numbers}\\") if options.operation == \\"sum\\": result = sum(numbers) elif options.operation == \\"average\\": result = sum(numbers) / len(numbers) if numbers else 0 print(f\\"Result: {result}\\") if __name__ == \\"__main__\\": main() ``` Explain your implementation approach and the command-line parsing logic, then write the full script based on the provided scaffold. Your implementation will be evaluated based on correctness, adherence to specifications, and the proper use of the `optparse` features.","solution":"from optparse import OptionParser def parse_arguments(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"read numbers from FILE\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--operation\\", dest=\\"operation\\", type=\\"choice\\", choices=[\\"sum\\", \\"average\\"], help=\\"operation to perform on the numbers: sum or average\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"enable verbose mode\\") (options, args) = parser.parse_args() if not options.filename: parser.error(\\"Filename not provided\\") if not options.operation: parser.error(\\"Operation not provided\\") return options def main(): options = parse_arguments() try: with open(options.filename, \'r\') as file: numbers = [float(line.strip()) for line in file.readlines()] except FileNotFoundError: print(\\"Error: file not found\\") return except ValueError: print(\\"Error: file contains non-numeric data\\") return if options.verbose: print(f\\"Numbers read from file: {numbers}\\") if options.operation == \\"sum\\": result = sum(numbers) elif options.operation == \\"average\\": result = sum(numbers) / len(numbers) if numbers else 0 print(f\\"Result: {result}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implementing Incremental Principal Component Analysis with Custom Batch Processing You are provided with a large dataset `data.csv` that does not fit into memory. Your task is to implement Incremental Principal Component Analysis (IncrementalPCA) to reduce the dimensionality of this data. The dataset contains numerical features only. Requirements: 1. Implement a function `perform_incremental_pca` that takes the following inputs: - `data_path`: A string, the path to the CSV file containing the dataset. - `n_components`: An integer, the number of principal components to compute. - `batch_size`: An integer, the size of the data chunks to be processed incrementally. 2. The function should perform the following operations: - Load the data incrementally from the CSV file. - Normalize the data (center the features but do not scale them). - Apply IncrementalPCA to the dataset with the specified number of components. - Return the transformed dataset with reduced dimensions. 3. The function should be efficient with respect to memory usage, handling data in chunks as specified by `batch_size`. Constraints: - You can assume that the CSV file has variable number of rows and more than 10 columns. - You should not load the entire dataset into memory at once. - Use the `IncrementalPCA` class from the `scikit-learn.decomposition` module. - Handle any appropriate exceptions that might occur during file I/O operations. Function Signature: ```python def perform_incremental_pca(data_path: str, n_components: int, batch_size: int) -> np.ndarray: pass ``` Example: ```python # Suppose data.csv path is \'data/data.csv\' transformed_data = perform_incremental_pca(\'data/data.csv\', n_components=5, batch_size=1000) print(transformed_data.shape) # Expected Output: (number_of_samples, 5) depending on the total count of samples in the data. ``` Good luck!","solution":"import numpy as np import pandas as pd from sklearn.decomposition import IncrementalPCA from sklearn.preprocessing import StandardScaler def perform_incremental_pca(data_path: str, n_components: int, batch_size: int) -> np.ndarray: Perform Incremental PCA on a large dataset that doesn\'t fit in memory. Parameters: data_path (str): The path to the CSV file containing the dataset. n_components (int): The number of principal components to compute. batch_size (int): The size of data chunks to be processed incrementally. Returns: np.ndarray: The dataset transformed to the reduced dimensions. # Initialize IncrementalPCA and StandardScaler ipca = IncrementalPCA(n_components=n_components) scaler = StandardScaler(with_mean=True, with_std=False) # Read the data in chunks and apply standard scaling for centering chunk_iterator = pd.read_csv(data_path, chunksize=batch_size) for chunk in chunk_iterator: # Fit the scaler on the current chunk to compute the mean for centering scaler.partial_fit(chunk) chunk_iterator = pd.read_csv(data_path, chunksize=batch_size) transformed_data = [] for chunk in chunk_iterator: # Center the chunk by removing the mean centered_chunk = scaler.transform(chunk) # Fit the IPCA on the centered chunk ipca.partial_fit(centered_chunk) chunk_iterator = pd.read_csv(data_path, chunksize=batch_size) for chunk in chunk_iterator: # Center the chunk by removing the mean centered_chunk = scaler.transform(chunk) # Transform the chunk using the fitted IPCA transformed_chunk = ipca.transform(centered_chunk) transformed_data.append(transformed_chunk) # Concatenate all transformed chunks transformed_data = np.vstack(transformed_data) return transformed_data"},{"question":"You are required to implement a function that finds the k-th smallest element in a given list of integers using the bisection algorithms provided by the `bisect` module. If the list has duplicate values, they should be counted separately (i.e., consider their multiple occurrences). # Function Signature ```python def find_kth_smallest_element(arr: List[int], k: int) -> int: pass ``` # Input - `arr`: A list of integers (1 ≤ len(arr) ≤ 10^5, -10^9 ≤ arr[i] ≤ 10^9). The list is not necessarily sorted. - `k`: An integer (1 ≤ k ≤ len(arr)). `k` is always valid (i.e., it is guaranteed that `k` is within the list bounds). # Output - The function should return the k-th smallest element in the sorted version of the list. # Constraints - You are **not allowed** to use Python\'s built-in `sorted()` or `sort()` functions. - The solution should efficiently handle up to 100,000 elements in the input list. # Examples ```python assert find_kth_smallest_element([7, 10, 4, 3, 20, 15], 3) == 7 assert find_kth_smallest_element([7, 10, 4, 3, 20, 15], 4) == 10 assert find_kth_smallest_element([1, 2, 3, 4, 5, 6], 6) == 6 ``` # Explanation In the first example, the list `[7, 10, 4, 3, 20, 15]`, when sorted, becomes `[3, 4, 7, 10, 15, 20]`. The third smallest element is `7`. In the second example, the list `[7, 10, 4, 3, 20, 15]`, when sorted, becomes `[3, 4, 7, 10, 15, 20]`. The fourth smallest element is `10`. In the third example, the list `[1, 2, 3, 4, 5, 6]` is already sorted, and the sixth smallest element is `6`. # Note To implement this function, you may utilize the `bisect` module\'s functionalities to maintain the list in a sorted order as you process elements. This task ensures you demonstrate a strong understanding of using bisection algorithms for maintaining and searching sorted lists efficiently.","solution":"import bisect def find_kth_smallest_element(arr, k): Find the k-th smallest element in the list using the bisect module # Create an empty list to store sorted elements sorted_list = [] # Insert each element of arr into sorted_list maintaining order for number in arr: bisect.insort(sorted_list, number) # Return the k-th smallest element (k is 1-indexed, list is 0-indexed) return sorted_list[k - 1]"},{"question":"# Mocking and Testing a Complex Interaction **Objective**: Use the `unittest.mock` library to test complex interactions between different classes and methods. You are provided with the following two classes, `OrderService` and `ShippingService`. Your task is to write unit tests for the `OrderHandler` class using the `unittest.mock` library. ```python class OrderService: def place_order(self, order_id): # Simulate placing an order pass def cancel_order(self, order_id): # Simulate canceling an order pass class ShippingService: def ship_order(self, order_id): # Simulate shipping an order pass class OrderHandler: def __init__(self, order_service: OrderService, shipping_service: ShippingService): self.order_service = order_service self.shipping_service = shipping_service def handle_order(self, order_id): self.order_service.place_order(order_id) self.shipping_service.ship_order(order_id) def handle_cancel_order(self, order_id): self.order_service.cancel_order(order_id) ``` Your unit tests must: 1. Verify that the `place_order` and `ship_order` methods are correctly called in the `handle_order` method. 2. Check that the `cancel_order` method is called in the `handle_cancel_order` method. 3. Use the `patch` decorator to replace the `place_order`, `cancel_order`, and `ship_order` methods with mocks. 4. Set return values for the `place_order` and `ship_order` methods and verify them. 5. Ensure that any exceptions raised within these methods are correctly handled and logged (if applicable). **Constraints**: - Do not modify the implementations of `OrderService`, `ShippingService`, or `OrderHandler` classes. - Use appropriate mock assertions to verify the method calls and arguments. **Expected Input and Output**: - There are no direct input and output respectively, but the assertions within the test cases will reflect the correct functionality. **Performance Requirements**: - The tests should run efficiently and be able to handle any typical unit test scenarios. **Example**: Here\'s a snippet of how the test class should look: ```python import unittest from unittest.mock import patch, MagicMock from your_module import OrderService, ShippingService, OrderHandler class TestOrderHandler(unittest.TestCase): @patch(\'your_module.OrderService.place_order\') @patch(\'your_module.ShippingService.ship_order\') def test_handle_order(self, mock_place_order, mock_ship_order): # Set up mocks and return values... # Assertions... @patch(\'your_module.OrderService.cancel_order\') def test_handle_cancel_order(self, mock_cancel_order): # Set up mocks... # Assertions... # Additional tests to ensure exception handling... ``` **Task**: Complete the implementation of the `TestOrderHandler` class with the appropriate mock setups and assertions as outlined in the example above.","solution":"from unittest.mock import MagicMock class OrderService: def place_order(self, order_id): # Simulate placing an order pass def cancel_order(self, order_id): # Simulate canceling an order pass class ShippingService: def ship_order(self, order_id): # Simulate shipping an order pass class OrderHandler: def __init__(self, order_service: OrderService, shipping_service: ShippingService): self.order_service = order_service self.shipping_service = shipping_service def handle_order(self, order_id): self.order_service.place_order(order_id) self.shipping_service.ship_order(order_id) def handle_cancel_order(self, order_id): self.order_service.cancel_order(order_id)"},{"question":"Objective: Demonstrate comprehension and effective use of the \\"marshal\\" module for serialization and deserialization of basic Python object types. Problem Statement: Implement a function `serialize_objects` that takes a list of supported Python objects, serializes them using the `marshal` module, writes them to a binary file, and then reads them back from the file. Return the deserialized objects to verify correctness. Function Signature: ```python def serialize_objects(objects: list) -> list: pass ``` Input: - `objects` (list): A list of supported Python objects. The list can contain booleans, integers, floating point numbers, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, `None`, `Ellipsis`, and `StopIteration`. Output: - A list of deserialized objects read from the binary file, expected to be identical to the input list. Constraints: - The function must handle unsupported types by raising a `ValueError`. - The file operations should be handled appropriately ensuring the file is closed after operations. - Assume all objects in the list are of supported types. - The file path can be hardcoded as \\"temp_marshal_data.bin\\". Example: ```python input_objects = [True, 123, 456.78, None, \\"Hello, World!\\", [1, 2, 3], {\\"key\\": \\"value\\"}] output_objects = serialize_objects(input_objects) # output_objects should be [True, 123, 456.78, None, \\"Hello, World!\\", [1, 2, 3], {\\"key\\": \\"value\\"}] ``` Notes: - Ensure the serialized and deserialized contents are the same. - Use appropriate exception handling for reading and writing operations. Performance Requirements: - The function should run efficiently with a focus on correct handling of file operations and the integrity of the serialized/deserialized data. - The test cases will include a variety of supported types and nested data structures to ensure the robustness of the function. Implementation Notes: - Utilize `marshal.dump` to serialize the objects to the file. - Use `marshal.load` to deserialize the objects from the file. - Handle any potential errors that might arise from file I/O operations.","solution":"import marshal def serialize_objects(objects: list) -> list: Serializes the given list of objects using marshal, writes them to a binary file, then reads back the objects from the file and returns them. Args: - objects (list): A list of supported Python objects. Returns: - list: The list of deserialized objects. file_path = \\"temp_marshal_data.bin\\" # Serialize and write objects to file with open(file_path, \'wb\') as file: marshal.dump(objects, file) # Deserialize and read objects from file with open(file_path, \'rb\') as file: deserialized_objects = marshal.load(file) return deserialized_objects"},{"question":"# Question: Tensor Views and Contiguity in PyTorch In this exercise, you will demonstrate your understanding of tensor views and contiguity in PyTorch by implementing a series of operations that optimize memory usage and evaluate tensor properties. Task: 1. Write a function `create_view` that takes a 2D tensor `t` and two integers `new_rows` and `new_cols`, and returns a view of `t` with the new shape `(new_rows, new_cols)`. 2. Write a function `is_contiguous` that checks if the given tensor is contiguous. 3. Write a function `make_contiguous` that takes a non-contiguous tensor and returns a contiguous version of it. 4. Combine the above functions in a function `process_tensor` that: - Receives a 2D tensor of shape `(m, n)`. - Creates a view of this tensor in shape `(new_rows, new_cols)` using `create_view`. - Checks if this new view is contiguous using `is_contiguous`. - If not contiguous, makes it contiguous using `make_contiguous`, and returns the contiguous tensor and a boolean indicating the original contiguity. - If already contiguous, returns the view and a boolean indicating the original contiguity. Input: - A tensor `t` with shape `(m, n)` where `m*n = new_rows*new_cols`. - Integers `new_rows` and `new_cols` which define the new shape of the tensor view. Output: - A tensor with shape `(new_rows, new_cols)`. - A boolean indicating whether the original view was contiguous. Constraints: - The input tensor will always be a 2D tensor. - The product of `new_rows` and `new_cols` will always equal the product of `m` and `n`. Example: ```python import torch # Sample tensor t = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) new_rows = 2 new_cols = 8 # Expected output tensor_view, was_contiguous = process_tensor(t, new_rows, new_cols) print(tensor_view) # Output: tensor([[ 1, 2, 3, 4, 5, 6, 7, 8], # [ 9, 10, 11, 12, 13, 14, 15, 16]]) print(was_contiguous) # Output: True ``` Implement the functions `create_view`, `is_contiguous`, `make_contiguous`, and `process_tensor` to complete this task.","solution":"import torch def create_view(t, new_rows, new_cols): Create a view of tensor t with shape (new_rows, new_cols). return t.view(new_rows, new_cols) def is_contiguous(t): Check if the tensor t is contiguous. return t.is_contiguous() def make_contiguous(t): Return a contiguous version of tensor t. return t.contiguous() def process_tensor(t, new_rows, new_cols): Process the tensor t to create a view of shape (new_rows, new_cols), check its contiguity and make it contiguous if necessary. tensor_view = create_view(t, new_rows, new_cols) was_contiguous = is_contiguous(tensor_view) if not was_contiguous: tensor_view = make_contiguous(tensor_view) return tensor_view, was_contiguous"},{"question":"Objective Your task is to implement a Python function that uses the `sysconfig` module to gather and print detailed configuration information about the current Python environment. This function should demonstrate your understanding of various functions in the `sysconfig` module. Problem Statement Write a Python function `print_python_config_info()` that performs the following tasks: 1. Prints the Python version in the format \\"MAJOR.MINOR\\". 2. Prints the platform identifier string. 3. Prints the default installation scheme for the current platform. 4. Prints the list of all supported installation schemes. 5. Prints the list of all path names currently supported. 6. Prints detailed paths for all installation paths using the default scheme. 7. Prints the value of the configuration variable \'LIBDIR\', if it exists. Function Signature ```python def print_python_config_info(): pass ``` Expected Output The function should print the following details: 1. Python version: X.Y 2. Platform: platform_name 3. Default Installation Scheme: scheme_name 4. Supported Installation Schemes: [scheme1, scheme2, ...] 5. Supported Path Names: [path1, path2, ...] 6. Installation Paths: - path_name1: path1 - path_name2: path2 - ... 7. LIBDIR value: path (if exists), otherwise \\"LIBDIR not found\\". Constraints - Use the functions provided in the `sysconfig` module only. - Ensure the function handles scenarios where certain configuration variables or paths might not be present. Example Assuming an environment with Python 3.10 on a generic Linux platform, a possible output could be: ``` Python version: 3.10 Platform: linux-x86_64 Default Installation Scheme: posix_prefix Supported Installation Schemes: [\'posix_prefix\', \'posix_home\', \'posix_user\', \'nt\', \'nt_user\', \'osx_framework_user\'] Supported Path Names: [\'stdlib\', \'platstdlib\', \'purelib\', \'platlib\', \'include\', \'platinclude\', \'scripts\', \'data\'] Installation Paths: stdlib: /usr/local/lib/python3.10 platstdlib: /usr/local/lib/python3.10 purelib: /usr/local/lib/python3.10/site-packages platlib: /usr/local/lib/python3.10/site-packages include: /usr/local/include/python3.10 platinclude: /usr/local/include/python3.10 scripts: /usr/local/bin data: /usr/local LIBDIR value: /usr/local/lib ``` Note - Ensure your function is robust and handles any edge cases appropriately.","solution":"import sysconfig def print_python_config_info(): # Get the Python version version_info = sysconfig.get_python_version() print(f\\"Python version: {version_info}\\") # Get the platform identifier string platform_info = sysconfig.get_platform() print(f\\"Platform: {platform_info}\\") # Get the default installation scheme for the current platform default_scheme = sysconfig.get_default_scheme() print(f\\"Default Installation Scheme: {default_scheme}\\") # Get the list of all supported installation schemes schemes = sysconfig.get_scheme_names() print(f\\"Supported Installation Schemes: {schemes}\\") # Get the list of all path names currently supported path_names = sysconfig.get_path_names() print(f\\"Supported Path Names: {path_names}\\") # Get the detailed paths for all installation paths using the default scheme print(\\"Installation Paths:\\") for path_name in path_names: path = sysconfig.get_path(path_name) print(f\\" {path_name}: {path}\\") # Get the value of the configuration variable \'LIBDIR\', if it exists libdir = sysconfig.get_config_var(\'LIBDIR\') if libdir: print(f\\"LIBDIR value: {libdir}\\") else: print(\\"LIBDIR not found.\\")"},{"question":"# Implementing a Complex Data Structure with Proper Memory Management in Python Introduction You are required to implement a complex data structure in Python that mimics the behavior of a hypothetical C extension module managing its memory. This exercise is designed to test your understanding of Python\'s memory management provided by the `python310` API, including safe allocation, reallocation, and deallocation. Task Implement a class `ComplexStructure` that manages an internal buffer using the raw memory interface functions (`PyMem_RawMalloc`, `PyMem_RawRealloc`, and `PyMem_RawFree`). The structure should support the following operations: 1. **Initialization:** Allocate an initial buffer of a given size. 2. **Resize Buffer:** Adjust the size of the buffer, preserving its current content up to the minimum of the old and new sizes. 3. **Free Buffer:** Safely free the allocated buffer. 4. **Add String:** Add a null-terminated string to the internal buffer and manage the necessary memory allocation. Requirements 1. **Initialize Method:** ```python def __init__(self, initial_size: int) -> None: ``` - `initial_size` (int): The initial size of the buffer to allocate. - Allocates an initial memory block of `initial_size` bytes using `PyMem_RawMalloc`. 2. **Resize Method:** ```python def resize(self, new_size: int) -> None: ``` - `new_size` (int): The new size to which the buffer should be resized. - Resizes the buffer using `PyMem_RawRealloc`. If resizing fails, the buffer remains valid at its previous size. 3. **Add String Method:** ```python def add_string(self, data: str) -> None: ``` - `data` (str): The string to add to the buffer. - Ensures enough space in the buffer for the string and its null terminator. Resizes the buffer if necessary. Adds the null-terminated string to the buffer. 4. **Free Method:** ```python def free(self) -> None: ``` - Frees the allocated buffer using `PyMem_RawFree`. 5. **Properties and Constraints:** - The buffer should be dynamically managed correctly without memory leaks. - Appropriate error handling should be implemented for all memory operations. - Methods should ensure that memory is safely allocated, reallocated, and deallocated using the raw memory allocation functions provided in the documentation. Input and Output Formats - **Input:** No direct input; methods are called on an instance of `ComplexStructure`. - **Output:** No direct output; implement the class correctly as described. Example Usage ```python cs = ComplexStructure(initial_size=10) cs.add_string(\\"Hello\\") cs.resize(20) cs.add_string(\\"World\\") cs.free() ``` Constraints - You must use the raw memory functions (`PyMem_RawMalloc`, `PyMem_RawRealloc`, `PyMem_RawFree`) for all memory management tasks. - Ensure that your implementation handles all potential memory management errors robustly. Additional Notes - You may assume a maximum length of 1,000 characters for any string added to the buffer. - The warnings and constraints provided in the documentation must be strictly followed to avoid undefined behavior.","solution":"import ctypes class ComplexStructure: def __init__(self, initial_size: int) -> None: self.size = initial_size self.buffer = ctypes.create_string_buffer(self.size) def resize(self, new_size: int) -> None: new_buffer = ctypes.create_string_buffer(new_size) ctypes.memmove(new_buffer, self.buffer, min(self.size, new_size)) self.buffer = new_buffer self.size = new_size def add_string(self, data: str) -> None: if len(data) + 1 > self.size: self.resize(len(data) + 1) ctypes.memset(self.buffer, 0, self.size) ctypes.memmove(self.buffer, data.encode(\'utf-8\'), len(data)) self.buffer[len(data)] = b\'0\' def free(self) -> None: del self.buffer self.buffer = None self.size = 0"},{"question":"**Topic**: Advanced Windowing Operations in Pandas **Objective**: This question aims to assess your understanding and ability to implement windowing operations in pandas, including rolling, expanding, and exponentially weighted windows, as well as custom window indexers. **Problem Statement**: You are given a pandas `DataFrame` representing stock prices of two companies over a 20-day period. Your task is to perform several windowing operations to analyze the data. **Input**: 1. A pandas DataFrame `df` with columns: - `Date`: datetime64 series with daily frequency. - `Stock_A`: float series representing the stock prices of Company A. - `Stock_B`: float series representing the stock prices of Company B. ```python import pandas as pd import numpy as np # Sample Data data = { \\"Date\\": pd.date_range(start=\\"2023-01-01\\", periods=20, freq=\\"D\\"), \\"Stock_A\\": np.random.uniform(100, 200, size=20), \\"Stock_B\\": np.random.uniform(50, 150, size=20), } df = pd.DataFrame(data) ``` **Your Tasks**: 1. Compute a **7-day rolling mean** for each stock and add these as new columns `Stock_A_rolling_mean` and `Stock_B_rolling_mean`. 2. Calculate the **expanding sum** for each stock and add these as new columns `Stock_A_expanding_sum` and `Stock_B_expanding_sum`. 3. Calculate the **exponentially weighted moving average (EWMA)** with a span equivalent to 5 days for each stock and add these as new columns `Stock_A_EWMA` and `Stock_B_EWMA`. 4. Define and apply a **custom rolling window** indexer that uses a 3-day sliding window only for Mondays (or the first day of any week) and adds the sum within this window as new columns `Stock_A_custom_sum` and `Stock_B_custom_sum`. **Output**: A modified DataFrame with the original columns along with the new columns described above. **Constraints**: 1. Ensure that the columns have the correct data types. 2. The custom window should work even if the \'Date\' column contains gaps (e.g., missing weekend days). **Function Signature**: ```python import pandas as pd def analyze_stock_prices(df: pd.DataFrame) -> pd.DataFrame: pass # Example usage df = pd.DataFrame({ \\"Date\\": pd.date_range(start=\\"2023-01-01\\", periods=20, freq=\\"D\\"), \\"Stock_A\\": np.random.uniform(100, 200, size=20), \\"Stock_B\\": np.random.uniform(50, 150, size=20), }) result_df = analyze_stock_prices(df) print(result_df) ``` **Additional Notes**: 1. You may assume that necessary libraries (`pandas`, `numpy`) are imported. 2. Be mindful of handling NaN values that arise from windowing operations. 3. Ensure that the \'Date\' column is set as the index of the DataFrame before performing any windowing operations. **Grading Criteria**: - Correct implementation of required windowing operations. - Proper application of the custom rolling window indexer. - Accurate and effective handling of edge cases and NaN values.","solution":"import pandas as pd import numpy as np def analyze_stock_prices(df: pd.DataFrame) -> pd.DataFrame: # Ensure \'Date\' column is set as the index df.set_index(\'Date\', inplace=True) # 1. Compute a 7-day rolling mean for each stock df[\'Stock_A_rolling_mean\'] = df[\'Stock_A\'].rolling(window=7).mean() df[\'Stock_B_rolling_mean\'] = df[\'Stock_B\'].rolling(window=7).mean() # 2. Calculate the expanding sum for each stock df[\'Stock_A_expanding_sum\'] = df[\'Stock_A\'].expanding(min_periods=1).sum() df[\'Stock_B_expanding_sum\'] = df[\'Stock_B\'].expanding(min_periods=1).sum() # 3. Calculate the exponentially weighted moving average (EWMA) with a span of 5 days for each stock df[\'Stock_A_EWMA\'] = df[\'Stock_A\'].ewm(span=5, adjust=False).mean() df[\'Stock_B_EWMA\'] = df[\'Stock_B\'].ewm(span=5, adjust=False).mean() # 4. Define and apply a custom rolling window indexer for a 3-day sliding window on Mondays def custom_rolling_monday_sum(series): custom_sum = [] for i in range(len(series)): if series.index[i].weekday() == 0: # Monday window_sum = series.iloc[max(0, i-2):i+1].sum() custom_sum.append(window_sum) else: custom_sum.append(np.nan) return pd.Series(custom_sum, index=series.index) df[\'Stock_A_custom_sum\'] = custom_rolling_monday_sum(df[\'Stock_A\']) df[\'Stock_B_custom_sum\'] = custom_rolling_monday_sum(df[\'Stock_B\']) return df"},{"question":"# Question: Custom Autograd Function in PyTorch Objective: Implement a custom autograd `Function` in PyTorch that defines both the forward and backward passes for a mathematical operation not directly available in PyTorch. Problem Statement: You are required to implement a custom autograd `Function` called `ExpSquareFunction` that applies the following operation: [ y = e^{x^2} ] on input tensor ( x ). Additionally, you must define the gradient (backward pass) for this operation. Requirements: 1. **Forward Pass**: Implement the forward pass to compute ( y = e^{x^2} ). 2. **Backward Pass**: Compute the gradient of ( y ) with respect to ( x ). Steps: 1. **Subclass `torch.autograd.Function`**: - Implement `forward` and `backward` methods. - Use `ctx` (context) to save any tensors needed for the backward pass. 2. **Define the Forward Method**: - The `forward` method should take `ctx` and an input tensor ( x ). - Compute the output tensor ( y = e^{x^2} ) and return it. 3. **Define the Backward Method**: - The `backward` method should take `ctx` and the gradient of the output tensor ( grad_output ). - Retrieve saved tensors from the forward pass if needed. - Compute the gradient of the input tensor ( grad_input ) and return it. Example Usage: After defining `ExpSquareFunction`, use it within a PyTorch operation as follows: ```python import torch class ExpSquareFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): # Implement forward computation y = torch.exp(x ** 2) ctx.save_for_backward(x) return y @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor from context x, = ctx.saved_tensors # Implement backward computation grad_input = grad_output * 2 * x * torch.exp(x ** 2) return grad_input # Alias or wrap for easier usage exp_square = ExpSquareFunction.apply # Test the function x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = exp_square(x) y.sum().backward() print(\\"Input:\\", x) print(\\"Output:\\", y) print(\\"Gradients:\\", x.grad) ``` Constraints: - The input tensor can be any shape but will always be a FloatTensor. - You should not modify the input tensor ( x ) in-place. - Ensure your implementation can handle higher-order gradients using `torch.autograd.gradcheck`. Notes: - Include functionality to perform gradient checking for your custom function. # Submission: Submit the implementation of the custom autograd `Function` and a small test script that validates its correctness by comparing computed gradients with PyTorch\'s automatic differentiation.","solution":"import torch class ExpSquareFunction(torch.autograd.Function): @staticmethod def forward(ctx, x): # Implement forward computation y = torch.exp(x ** 2) ctx.save_for_backward(x) return y @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor from context x, = ctx.saved_tensors # Implement backward computation grad_input = grad_output * 2 * x * torch.exp(x ** 2) return grad_input # Alias or wrap for easier usage exp_square = ExpSquareFunction.apply # Example usage: x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = exp_square(x) y.sum().backward() print(\\"Input:\\", x) print(\\"Output:\\", y) print(\\"Gradients:\\", x.grad)"},{"question":"Objective: The task is to assess the student\'s understanding of creating and manipulating color palettes using seaborn. Problem Statement: You are provided with a dataset containing two numerical features (`feature1` and `feature2`). Your task is to create a joint distribution plot of these features, and customize the color palette using seaborn\'s `dark_palette` function. Instructions: 1. Load the given dataset into a pandas DataFrame. 2. Create a joint distribution plot using `seaborn.jointplot()`. 3. Customize the plot using a dark color palette, which transitions from dark gray to a specified color. 4. Use three different methods to specify the color in the palette (name, hex code, HUSL system). 5. Increase the number of colors in the palette to 10. 6. Return the final plot. Requirements: - The dataset is a CSV file named `data.csv` with columns `feature1` and `feature2`. - The color palette should be applied to both the scatter plots and the marginal histograms. - The plot should be displayed using matplotlib. Example input and output: **Input:** data.csv ``` feature1,feature2 5.1,3.5 4.9,3.0 ... ``` **Output:** A seaborn joint plot with a customized dark color palette. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'data.csv\') # Create a dark palette specified by color name dark_palette_name = sns.dark_palette(\\"seagreen\\", 10, reverse=False) # Create the joint plot sns.jointplot(x=\'feature1\', y=\'feature2\', data=df, kind=\'scatter\', palette=dark_palette_name) plt.show() # Create a dark palette specified by hex code dark_palette_hex = sns.dark_palette(\\"#79C\\", 10, reverse=False) # Create the joint plot sns.jointplot(x=\'feature1\', y=\'feature2\', data=df, kind=\'scatter\', palette=dark_palette_hex) plt.show() # Create a dark palette specified by HUSL system dark_palette_husl = sns.dark_palette((20, 60, 50), 10, input=\\"husl\\", reverse=False) # Create the joint plot sns.jointplot(x=\'feature1\', y=\'feature2\', data=df, kind=\'scatter\', palette=dark_palette_husl) plt.show() ``` Note: - Ensure the joint plots are displayed with the three different palettes. - Adjust reverse parameter in dark_palette function appropriately if color transitions need to be displayed differently. Constraints: - You must use the `seaborn` library to create the plots. - `matplotlib` should be used to display the plots. Evaluation Criteria: - Correct loading of the dataset. - Correct implementation of three different color palette specifications. - Proper use of seaborn\'s `jointplot` and `dark_palette` functions. - Clarity and visual appeal of the final plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_joint_plot_with_custom_palette(csv_filepath): # Load the dataset df = pd.read_csv(csv_filepath) # Create a dark palette specified by color name dark_palette_name = sns.dark_palette(\\"seagreen\\", 10, reverse=False) # Create the joint plot joint_plot_name = sns.jointplot(x=\'feature1\', y=\'feature2\', data=df, kind=\'scatter\', palette=dark_palette_name) plt.show() # Create a dark palette specified by hex code dark_palette_hex = sns.dark_palette(\\"#79C\\", 10, reverse=False) # Create the joint plot joint_plot_hex = sns.jointplot(x=\'feature1\', y=\'feature2\', data=df, kind=\'scatter\', palette=dark_palette_hex) plt.show() # Create a dark palette specified by HUSL system dark_palette_husl = sns.dark_palette((20, 60, 50), 10, input=\\"husl\\", reverse=False) # Create the joint plot joint_plot_husl = sns.jointplot(x=\'feature1\', y=\'feature2\', data=df, kind=\'scatter\', palette=dark_palette_husl) plt.show() # Return the matplotlib figure objects return joint_plot_name.fig, joint_plot_hex.fig, joint_plot_husl.fig"},{"question":"Objective To assess the student\'s capability to utilize different types of simple statements in Python correctly, including basic expressions, assignments (regular, augmented, and annotated), control-flow statements (`break`, `continue`, `return`, `yield`, `raise`), and import statements. Problem Statement You need to implement a class called `SimpleStatementsEvaluator`. This class will contain methods that demonstrate the usage of various simple statements in Python as described in the provided documentation. 1. **Assignment Operations** 2. **Control Flow Statements** 3. **Import Statements** Your task is to implement the following methods within the `SimpleStatementsEvaluator` class: 1. **Assignment Operations**: - `split_and_assign(input_list: list) -> tuple`: This method takes a list of at least 4 integers as input, splits it into four variables using tuple unpacking, and then returns these variables. - `augmented_assign(input_dict: dict, key: str, value: int)`: - This method takes a dictionary, a key, and an integer value. - It adds the integer value to the existing value of the key using an augmented assignment. - If the key does not exist, create the key and set its value to the given integer. 2. **Control Flow Statements**: - `sum_within_range(input_list: list, lower: int, upper: int) -> int`: This method takes a list of integers and two integers (lower and upper bounds) as parameters. It should return the sum of all integers in the list within this range (inclusive). Use `continue` to skip out-of-bound values and `break` to stop further iteration once an out-of-bound value is encountered. - `five_integers_yield() -> generator`: This method should continuously yield integers starting from 1. Use the `yield` statement to return the next integer sequentially every time the generator is called. Stop the generator after yielding five integers. 3. **Import Statements**: - `import_and_execute(module_name: str, func_name: str, *args, **kwargs) -> any`: This method dynamically imports a module specified by `module_name`, and executes the function specified by `func_name` with given arguments (`args`) and keyword arguments (`kwargs`). Return the function\'s result. Constraints - You must use `assert` statements to verify that the methods are functioning correctly. - If an illegal assignment occurs, handle it gracefully and raise an appropriate exception. Example Usage ```python evaluator = SimpleStatementsEvaluator() # Assignment Operations result = evaluator.split_and_assign([1, 2, 3, 4]) print(result) # Output: (1, 2, 3, 4) input_dict = {\'a\': 1} evaluator.augmented_assign(input_dict, \'a\', 5) evaluator.augmented_assign(input_dict, \'b\', 3) assert input_dict == {\'a\': 6, \'b\': 3} # Control Flow Statements sum_result = evaluator.sum_within_range([1, 2, 3, 4, 5, 6], 2, 5) assert sum_result == 14 gen = evaluator.five_integers_yield() assert list(gen) == [1, 2, 3, 4, 5] # Import Statements result = evaluator.import_and_execute(\'math\', \'sqrt\', 16) assert result == 4.0 ```","solution":"class SimpleStatementsEvaluator: def split_and_assign(self, input_list: list) -> tuple: Splits a list of at least 4 integers into four variables using tuple unpacking. assert len(input_list) >= 4 a, b, c, d = input_list[:4] return a, b, c, d def augmented_assign(self, input_dict: dict, key: str, value: int): Adds the integer value to the existing value of the key using an augmented assignment. If the key does not exist, create the key and set its value to the given integer. if key in input_dict: input_dict[key] += value else: input_dict[key] = value def sum_within_range(self, input_list: list, lower: int, upper: int) -> int: Returns the sum of all integers in the list within the specified range (inclusive). Uses `continue` to skip out-of-bound values and `break` to stop further iteration once an out-of-bound value is encountered. total = 0 for number in input_list: if number < lower or number > upper: continue total += number return total def five_integers_yield(self): Continuously yields integers starting from 1. Stops after yielding five integers. count = 1 while count <= 5: yield count count += 1 def import_and_execute(self, module_name: str, func_name: str, *args, **kwargs) -> any: Dynamically imports a module and executes the specified function with given arguments and keyword arguments. module = __import__(module_name) func = getattr(module, func_name) return func(*args, **kwargs)"},{"question":"**Objective:** Write code demonstrating a comprehensive understanding of the Seaborn `residplot` function by creating and analyzing residual plots from a given dataset. **Problem Statement:** You are provided with a dataset about car specifications called `car_specs.csv`. Use Seaborn\'s `residplot` functionality to analyze residuals and identify potential violations in linear regression assumptions. Follow the steps below to complete this assessment: 1. Load the dataset `car_specs.csv` which contains columns: - `horsepower`: The power output of the car. - `weight`: The weight of the car. - `displacement`: The engine displacement. - `mpg`: Miles per gallon (fuel efficiency). 2. Perform the following tasks and create the corresponding residual plots: - Task 1: Create a residual plot to evaluate the fit of a simple linear regression model where `weight` is the predictor variable, and `displacement` is the response variable. - Task 2: Create a residual plot for a linear regression model with `horsepower` as the predictor and `mpg` as the response variable. - Task 3: Create a residual plot for the model in Task 2, but fit a second-order polynomial model (order=2) to check if the residuals stabilize. - Task 4: Add a LOWESS curve to the residual plot from Task 2 to emphasize any structure in the residuals. **Input:** - A CSV file named `car_specs.csv` containing the car specifications data. **Output:** - Four residual plots corresponding to the tasks defined above. **Constraints:** - You must use Seaborn\'s `residplot` function for all plots. - The plots should clearly show any patterns or structures in the residuals. **Requirements:** - Load the dataset correctly and ensure it is in the appropriate format for plotting. - Use clear titles and labels for each plot to indicate what it represents. - Ensure your code is well-organized and commented. **Additional Information:** - You may assume the dataset file is provided and correctly formatted. ```python # Example skeleton code to get you started import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset car_specs = pd.read_csv(\'car_specs.csv\') # Set the Seaborn theme sns.set_theme() # Task 1: Residual plot for weight vs displacement plt.figure(figsize=(10,6)) sns.residplot(data=car_specs, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot: Weight vs Displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Displacement\') plt.show() # Task 2: Residual plot for horsepower vs mpg plt.figure(figsize=(10,6)) sns.residplot(data=car_specs, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\'Residual Plot: Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show() # Task 3: Residual plot using second-order polynomial model for horsepower vs mpg plt.figure(figsize=(10,6)) sns.residplot(data=car_specs, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residual Plot: Horsepower vs MPG (2nd Order Polynomial)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show() # Task 4: Residual plot with LOWESS curve for horsepower vs mpg plt.figure(figsize=(10,6)) sns.residplot(data=car_specs, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\'r\')) plt.title(\'Residual Plot: Horsepower vs MPG with LOWESS Curve\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def analyze_residual_plots(file_path): Analyze residual plots from the dataset provided. Parameters: file_path (str): The file path to the car_specs.csv dataset. Returns: None # Load the dataset car_specs = pd.read_csv(file_path) # Set the Seaborn theme sns.set_theme() # Task 1: Residual plot for weight vs displacement plt.figure(figsize=(10,6)) sns.residplot(data=car_specs, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot: Weight vs Displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Displacement\') plt.show() # Task 2: Residual plot for horsepower vs mpg plt.figure(figsize=(10,6)) sns.residplot(data=car_specs, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\'Residual Plot: Horsepower vs MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show() # Task 3: Residual plot using second-order polynomial model for horsepower vs mpg plt.figure(figsize=(10,6)) sns.residplot(data=car_specs, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residual Plot: Horsepower vs MPG (2nd Order Polynomial)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show() # Task 4: Residual plot with LOWESS curve for horsepower vs mpg plt.figure(figsize=(10,6)) sns.residplot(data=car_specs, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\'r\')) plt.title(\'Residual Plot: Horsepower vs MPG with LOWESS Curve\') plt.xlabel(\'Horsepower\') plt.ylabel(\'MPG\') plt.show()"},{"question":"Objective You are required to implement a custom class that inherits from `reprlib.Repr` to control the representation of different types of objects with specific size limits. This will test your understanding of class inheritance, method overriding, and dynamic dispatching. Task 1. Create a class `CustomRepr` that inherits from `reprlib.Repr`. 2. Override the following methods to customize the representations: - dictionaries (`repr_dict`) - lists (`repr_list`) - strings (`repr_str`) 3. For dictionaries, display a maximum of 3 key-value pairs. 4. For lists, display a maximum of 4 elements. 5. For strings, display a maximum of 15 characters, and if truncated, the remaining part should be replaced by ellipses (`...`). 6. Implement a method to handle any other types of objects using the default `Repr.repr1()` behavior. Constraints - The `repr_dict` method should use a maximum limit of 3 key-value pairs. - The `repr_list` method should use a maximum limit of 4 elements. - The `repr_str` method should limit the string representation to 15 characters. Input - A Python object (dictionary, list, string, or any other type) that you need to represent. Output - A string representation of the input object with the specified size limits. Example ```python import reprlib # Define the custom class class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 3 self.maxlist = 4 self.maxstring = 15 def repr_dict(self, obj, level): return super().repr_dict(obj, level) def repr_list(self, obj, level): return super().repr_list(obj, level) def repr_str(self, obj, level): return super().repr_str(obj, level) def repr1(self, obj, level): return super().repr1(obj, level) # Test the implementation crepr = CustomRepr() # Examples for testing print(crepr.repr({\\"key1\\": 1, \\"key2\\": 2, \\"key3\\": 3, \\"key4\\": 4})) # Expecting a dictionary representation with max 3 key-value pairs print(crepr.repr([\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"])) # Expecting a list representation with max 4 elements print(crepr.repr(\\"This is a very long string\\")) # Expecting a string representation with max 15 characters ``` Notes - Use the dynamic dispatching feature of `reprlib.Repr.repr1()` for implementing type-specific methods. - Ensure the custom class handles recursive representations correctly to avoid infinite loops.","solution":"import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 3 self.maxlist = 4 self.maxstring = 15 def repr_dict(self, obj, level): n = len(obj) if n > self.maxdict: keys = list(obj.keys())[:self.maxdict] items = \', \'.join(f\'{repr(k)}: {self.repr1(obj[k], level - 1)}\' for k in keys) return \'{\' + items + \', ...}\' return repr(obj) def repr_list(self, obj, level): n = len(obj) if n > self.maxlist: items = \', \'.join(self.repr1(x, level - 1) for x in obj[:self.maxlist]) return \'[\' + items + \', ...]\' return repr(obj) def repr_str(self, obj, level): if len(obj) > self.maxstring: return repr(obj[:self.maxstring] + \'...\') return repr(obj) def repr1(self, obj, level): return super().repr1(obj, level) # Test the implementation crepr = CustomRepr() # Examples for testing print(crepr.repr({\\"key1\\": 1, \\"key2\\": 2, \\"key3\\": 3, \\"key4\\": 4})) # Expecting a dictionary representation with max 3 key-value pairs print(crepr.repr([\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"])) # Expecting a list representation with max 4 elements print(crepr.repr(\\"This is a very long string\\")) # Expecting a string representation with max 15 characters"},{"question":"Objective Implement the Spectral Co-Clustering algorithm using scikit-learn to identify biclusters in a noisy data matrix. Evaluate the identified biclusters using the Jaccard index against the true biclusters. Problem Statement Given a noisy data matrix, your task is to: 1. Apply the Spectral Co-Clustering algorithm to identify biclusters. 2. Evaluate the overlap between the identified biclusters and the true biclusters using the Jaccard index. Input 1. `data_matrix`: A 2D NumPy array with shape `(m, n)` representing the noisy data matrix. 2. `true_row_labels`: A 1D NumPy array with shape `(m,)` representing the true row labels of the biclusters. 3. `true_column_labels`: A 1D NumPy array with shape `(n,)` representing the true column labels of the biclusters. 4. `n_clusters`: An integer representing the number of biclusters to find. Output 1. A dictionary with keys: - `row_labels`: Identified row labels of the biclusters. - `column_labels`: Identified column labels of the biclusters. - `jaccard_index` : The Jaccard index score evaluating the quality of the biclusters. Constraints - The data matrix will have at most 1000 rows and 1000 columns. - The number of biclusters `n_clusters` will be a positive integer less than or equal to 20. Function Signature ```python def bicluster_evaluation(data_matrix: np.ndarray, true_row_labels: np.ndarray, true_column_labels: np.ndarray, n_clusters: int) -> dict: pass ``` Example ```python import numpy as np from sklearn.cluster import SpectralCoclustering from sklearn.metrics import jaccard_score def bicluster_evaluation(data_matrix: np.ndarray, true_row_labels: np.ndarray, true_column_labels: np.ndarray, n_clusters: int) -> dict: # Apply Spectral Co-Clustering model = SpectralCoclustering(n_clusters=n_clusters, random_state=0) model.fit(data_matrix) # Identified labels row_labels = model.row_labels_ column_labels = model.column_labels_ # Evaluating the quality of biclusters using Jaccard index jaccard_index_rows = jaccard_score(true_row_labels, row_labels, average=\'macro\') jaccard_index_columns = jaccard_score(true_column_labels, column_labels, average=\'macro\') jaccard_index = (jaccard_index_rows + jaccard_index_columns) / 2 return { \\"row_labels\\": row_labels, \\"column_labels\\": column_labels, \\"jaccard_index\\": jaccard_index } # Example usage data_matrix = np.random.rand(100, 100) true_row_labels = np.random.randint(0, 5, 100) true_column_labels = np.random.randint(0, 5, 100) result = bicluster_evaluation(data_matrix, true_row_labels, true_column_labels, n_clusters=5) print(result) ```","solution":"import numpy as np from sklearn.cluster import SpectralCoclustering from sklearn.metrics import jaccard_score def bicluster_evaluation(data_matrix: np.ndarray, true_row_labels: np.ndarray, true_column_labels: np.ndarray, n_clusters: int) -> dict: Apply the Spectral Co-Clustering algorithm to identify biclusters and evaluate the overlap with the true biclusters using the Jaccard index. Args: data_matrix (np.ndarray): A 2D NumPy array with shape (m, n) representing the noisy data matrix. true_row_labels (np.ndarray): A 1D NumPy array with shape (m,) representing the true row labels of the biclusters. true_column_labels (np.ndarray): A 1D NumPy array with shape (n,) representing the true column labels of the biclusters. n_clusters (int): The number of biclusters to find. Returns: dict: A dictionary with keys \\"row_labels\\", \\"column_labels\\", and \\"jaccard_index\\". # Apply Spectral Co-Clustering model = SpectralCoclustering(n_clusters=n_clusters, random_state=0) model.fit(data_matrix) # Identified labels row_labels = model.row_labels_ column_labels = model.column_labels_ # Evaluating the quality of biclusters using Jaccard index jaccard_index_rows = jaccard_score(true_row_labels, row_labels, average=\'macro\') jaccard_index_columns = jaccard_score(true_column_labels, column_labels, average=\'macro\') jaccard_index = (jaccard_index_rows + jaccard_index_columns) / 2 return { \\"row_labels\\": row_labels, \\"column_labels\\": column_labels, \\"jaccard_index\\": jaccard_index }"},{"question":"**Objective**: Demonstrate your comprehension of the seaborn library by implementing a function that visualizes complex relationships in a dataset using both scatter plots and line plots. --- **Question**: You are given a dataset containing information about daily weather measurements. Your task is to implement a function, `visualize_weather_data`, that generates visualizations to analyze relationships between different weather parameters. **Function Signature**: ```python def visualize_weather_data(weather_data: pd.DataFrame) -> None: pass ``` **Parameters**: - `weather_data`: A DataFrame containing columns: - `\'date\'`: Date of the measurement (as `datetime64[ns]`). - `\'temperature\'`: Daily average temperature in Celsius (numeric). - `\'humidity\'`: Daily average humidity as a percentage (numeric). - `\'wind_speed\'`: Daily average wind speed in km/h (numeric). - `\'precipitation\'`: Daily precipitation in mm (numeric). --- **Requirements**: 1. **Scatter Plot**: - Plot a scatter plot visualizing the relationship between `temperature` (x-axis) and `humidity` (y-axis). - Use `wind_speed` to color the points using a hue semantic. - Use `precipitation` to change the size of the points. 2. **Line Plot**: - Plot a line plot showing the trend of `temperature` over `date`. - Include a shaded area representing the 95% confidence interval around the mean temperature. - Color the lines based on the `wind_speed` using a hue semantic. 3. **Faceting**: - Generate a grid of scatter plots showing the relationship between `temperature` and `humidity` for different levels of `wind_speed`. - Arrange these plots using the `precipitation` variable to create columns. - The plots should have different colors for the points representing different wind speeds. **Constraints**: - Ensure that the visualizations are clear and provide meaningful insights into the relationships. - Handle the case where the input DataFrame might contain missing values. **Example**: Assume `weather_data` is a DataFrame structured as follows: ```plaintext date temperature humidity wind_speed precipitation 0 2023-01-01 23.4 50.3 14.8 0.5 1 2023-01-02 22.1 60.1 10.2 1.1 ... ``` The function `visualize_weather_data(weather_data)` should generate the specified visualizations using seaborn. **Note**: Do not return any value. The function should only produce the plots as output. --- **Hints**: - Utilize `sns.relplot` for scatter plots and line plots. - Use `sns.FacetGrid` for faceting the scatter plots. - Pay attention to handling missing data by using methods like `dropna()`. **Good Luck!**","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_weather_data(weather_data: pd.DataFrame) -> None: This function takes a weather dataset as input and generates visualizations to analyze relationships between different weather parameters. # Handle missing values by dropping them weather_data_clean = weather_data.dropna() # Scatter Plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( x=\'temperature\', y=\'humidity\', hue=\'wind_speed\', size=\'precipitation\', sizes=(20, 200), data=weather_data_clean, palette=\\"viridis\\" ) plt.title(\'Scatter Plot of Temperature vs Humidity\') plt.xlabel(\'Temperature (Celsius)\') plt.ylabel(\'Humidity (%)\') plt.legend(title=\'Wind Speed (km/h)\') scatter_plot.figure.show() # Line Plot plt.figure(figsize=(10, 6)) line_plot = sns.lineplot( x=\'date\', y=\'temperature\', hue=\'wind_speed\', ci=95, data=weather_data_clean, palette=\\"viridis\\" ) plt.title(\'Line Plot of Temperature over Date\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature (Celsius)\') plt.legend(title=\'Wind Speed (km/h)\') plt.xticks(rotation=45) line_plot.figure.show() # Faceting facet = sns.FacetGrid( weather_data_clean, col=\'precipitation\', hue=\'wind_speed\', palette=\\"viridis\\", col_wrap=3 ) facet.map(sns.scatterplot, \'temperature\', \'humidity\') facet.add_legend() plt.subplots_adjust(top=0.9) facet.fig.suptitle(\'Faceted Scatter Plots of Temperature vs Humidity for Different Precipitation Levels\') plt.show() # Example usage: # Assuming `weather_data` is a DataFrame structured as specified # weather_data = pd.read_csv(\'weather_data.csv\', parse_dates=[\'date\']) # visualize_weather_data(weather_data)"},{"question":"Problem Statement: You are tasked with writing a function to classify a list of strings into Python keywords, soft keywords, and regular identifiers. This function will help in analyzing the content of Python code and identifying which elements are reserved for special use within the language. Function Signature ```python def classify_identifiers(identifiers: list[str]) -> dict[str, list[str]]: pass ``` Input: - A list of strings `identifiers` where `0 <= len(identifiers) <= 1000` Output: - A dictionary with three keys: `\\"keywords\\"`, `\\"soft_keywords\\"`, and `\\"identifiers\\"`, each mapping to a list of strings: - `\\"keywords\\"`: Contains all the strings that are documented as keywords in Python. - `\\"soft_keywords\\"`: Contains all the strings that are documented as soft keywords. - `\\"identifiers\\"`: Contains all remaining strings that are neither keywords nor soft keywords. Example: ```python identifiers = [\\"if\\", \\"else\\", \\"match\\", \\"case\\", \\"foobar\\"] result = classify_identifiers(identifiers) # Expected output { \\"keywords\\": [\\"if\\", \\"else\\"], \\"soft_keywords\\": [\\"match\\", \\"case\\"], \\"identifiers\\": [\\"foobar\\"] } ``` Constraints: - Assume that the identifiers list will only contain lowercase alphabetic strings and will not include any non-alphabetic characters. Requirements: - Implement the function using the `keyword` module to determine keywords and soft keywords. - Ensure the function is optimized for performance given the constraints. Notes: - Utilize `keyword.iskeyword()` and `keyword.issoftkeyword()` for checking keywords and soft keywords, respectively. - Lists in the returned dictionary should maintain the order of their occurrence in the input list.","solution":"import keyword def classify_identifiers(identifiers: list[str]) -> dict[str, list[str]]: keywords = [] soft_keywords = [] regular_identifiers = [] for identifier in identifiers: if keyword.iskeyword(identifier): keywords.append(identifier) elif keyword.issoftkeyword(identifier): soft_keywords.append(identifier) else: regular_identifiers.append(identifier) return { \\"keywords\\": keywords, \\"soft_keywords\\": soft_keywords, \\"identifiers\\": regular_identifiers }"},{"question":"**Question:** You are given a scenario where you need to transfer a complex data structure using XDR (External Data Representation). In this task, you will need to use the `xdrlib` module to pack and unpack this data structure. # Data Structure The data structure consists of: 1. **user_id** (an integer) 2. **name** (a string) 3. **balance** (a float) 4. **transactions** (a list of float values representing transaction amounts) # Task 1. Write a function `pack_user_data(user_id, name, balance, transactions)` that takes the components of the data structure, packs it using `xdrlib.Packer`, and returns the packed data as bytes. 2. Write a function `unpack_user_data(packed_data)` that takes the packed data (produced by `pack_user_data`), unpacks it using `xdrlib.Unpacker`, and returns the data structure as a Python dictionary. Function Signatures ```python def pack_user_data(user_id: int, name: str, balance: float, transactions: list) -> bytes: # Your implementation here pass def unpack_user_data(packed_data: bytes) -> dict: # Your implementation here pass ``` # Constraints - `user_id` will be a non-negative integer. - `name` will be a non-empty string with a maximum length of 100 characters. - `balance` will be a floating-point number. - `transactions` will be a list containing between 0 and 1000 floating-point numbers. # Example Usage ```python user_id = 123 name = \\"John Doe\\" balance = 1024.75 transactions = [50.0, -25.5, 100.0] # Pack the data packed_data = pack_user_data(user_id, name, balance, transactions) # Unpack the data unpacked_data = unpack_user_data(packed_data) print(unpacked_data) # Output: # { # \'user_id\': 123, # \'name\': \'John Doe\', # \'balance\': 1024.75, # \'transactions\': [50.0, -25.5, 100.0] # } ``` Use appropriate methods for packing and unpacking integer, string, float, and list types from `xdrlib`. Good luck, and happy coding!","solution":"import xdrlib def pack_user_data(user_id: int, name: str, balance: float, transactions: list) -> bytes: Packs user data into XDR format. packer = xdrlib.Packer() # Packing the integer user_id packer.pack_int(user_id) # Packing the string name packer.pack_string(name.encode(\'utf-8\')) # Packing the float balance packer.pack_double(balance) # Packing the list of transactions packer.pack_int(len(transactions)) for transaction in transactions: packer.pack_double(transaction) return packer.get_buffer() def unpack_user_data(packed_data: bytes) -> dict: Unpacks user data from XDR format. unpacker = xdrlib.Unpacker(packed_data) # Unpacking the integer user_id user_id = unpacker.unpack_int() # Unpacking the string name name = unpacker.unpack_string().decode(\'utf-8\') # Unpacking the float balance balance = unpacker.unpack_double() # Unpacking the list of transactions transactions_length = unpacker.unpack_int() transactions = [] for _ in range(transactions_length): transactions.append(unpacker.unpack_double()) return { \'user_id\': user_id, \'name\': name, \'balance\': balance, \'transactions\': transactions, }"},{"question":"**Objective:** Implement a function `capture_exception_details` which captures and logs detailed traceback information whenever an exception occurs. **Instructions:** 1. Write a function `capture_exception_details` that takes a function `func` and its arguments `*args`, `**kwargs`. 2. When the function `func` is called with the provided arguments, if it raises an exception, the `capture_exception_details` function should: - Capture the exception and its traceback. - Format the exception and traceback details. - Return a string that includes the formatted traceback and exception message. 3. Make use of the `traceback` module\'s functionalities such as `format_exception` or `TracebackException`. **Function Signature:** ```python def capture_exception_details(func, *args, **kwargs) -> str: ``` **Example Usage:** ```python def faulty_function(x, y): return x / y try: capture_exception_details(faulty_function, 10, 0) except ZeroDivisionError: pass ``` **Output Specification:** - The function should return a precise and formatted single string containing the traceback and error message. **Constraints:** - You MUST use Python\'s `traceback` module. - You SHOULD handle any function and any arguments. - Assume that the function `func` can raise any kind of exception. **Example Output:** For the provided example, the output could be: ``` \'Traceback (most recent call last): File \\"your_script.py\\", line 23, in capture_exception_details func(*args, **kwargs) File \\"your_script.py\\", line 18, in faulty_function return x / y ZeroDivisionError: division by zero ``` **Requirement:** - Provide a solution that captures and formats exception details accurately. - Handle edge cases where no exception might be raised, and in such cases, return an empty string or a specific message indicating no error occurred.","solution":"import traceback def capture_exception_details(func, *args, **kwargs) -> str: Captures and returns traceback details of any exception raised by the provided function when called with the given arguments. try: func(*args, **kwargs) except Exception as e: return \'\'.join(traceback.format_exception(None, e, e.__traceback__)) return \\"No exception occurred\\""},{"question":"Wildcard File Search Objective: Design a function that searches for files in a given directory and its subdirectories that match a specific Unix shell-style wildcard pattern. Function Signature: ```python def search_files(directory: str, pattern: str) -> list: Searches for files in the specified directory and its subdirectories that match the given Unix shell-style wildcard pattern. Args: directory (str): The path to the directory to search in. pattern (str): The Unix shell-style wildcard pattern to match filenames. Returns: list: A list of paths to files that match the given pattern. ``` Input: - `directory`: A string representing the path to the directory in which the search should begin. - `pattern`: A string representing the Unix shell-style wildcard pattern to use for matching filenames. Output: - A list of strings, where each string is the path to a file that matches the provided pattern. The list should be sorted lexically. Constraints: - The function should be robust and handle large directories efficiently. - The function should recursively search all subdirectories. - The function should return the full path of each matching file. Example: ```python # Example Directory Structure: # . # ├── subdir1 # │ ├── file1.txt # │ └── file2.csv # ├── subdir2 # │ └── file3.txt # └── file4.md search_files(\'.\', \'*.txt\') # Output: [\'./subdir1/file1.txt\', \'./subdir2/file3.txt\'] search_files(\'.\', \'*.md\') # Output: [\'./file4.md\'] ``` Notes: - Use the `fnmatch` module to match filenames against the pattern. - You may find the `os` and `fnmatch` modules helpful for implementing the solution. - Ensure that the solution is efficient in terms of both time and space complexity.","solution":"import os import fnmatch def search_files(directory: str, pattern: str) -> list: Searches for files in the specified directory and its subdirectories that match the given Unix shell-style wildcard pattern. Args: directory (str): The path to the directory to search in. pattern (str): The Unix shell-style wildcard pattern to match filenames. Returns: list: A list of paths to files that match the given pattern. matches = [] for root, dirnames, filenames in os.walk(directory): for filename in fnmatch.filter(filenames, pattern): full_path = os.path.join(root, filename) matches.append(full_path) return sorted(matches)"},{"question":"Objective Design a function that processes a list of dictionaries, which represent customer orders, and categorizes the orders based on given conditions. Your task is to implement a function that demonstrates your understanding of control flow, function parameters, and pattern matching in Python. Problem Description You need to implement a function called `process_orders` that takes a list of customer orders and categorizes them into three categories: \\"small\\", \\"medium\\", and \\"large\\" based on the total value of the order. Each order is represented as a dictionary with at least two keys: `\\"customer\\"` and `\\"items\\"`, where `\\"items\\"` is a list of dictionaries with `\\"item\\"` and `\\"price\\"`. The function should return a dictionary with three keys: `\\"small\\"`, `\\"medium\\"`, and `\\"large\\"`, each containing a list of customer names corresponding to each category. Function Signature ```python def process_orders(orders: list) -> dict: pass ``` Input - `orders`: A list of dictionaries, where each dictionary has the following structure: ```python { \\"customer\\": str, # Name of the customer \\"items\\": [ { \\"item\\": str, # Name of the item \\"price\\": float # Price of the item } # ... more items ] } ``` Output - A dictionary with three keys: `\\"small\\"`, `\\"medium\\"`, and `\\"large\\"`. Each key maps to a list of customer names whose orders fall into the respective categories. Categories - \\"small\\": Total order value is less than 50. - \\"medium\\": Total order value is between 50 and 200 (inclusive). - \\"large\\": Total order value is greater than 200. Example ```python orders = [ { \\"customer\\": \\"Alice\\", \\"items\\": [ {\\"item\\": \\"apple\\", \\"price\\": 5.0}, {\\"item\\": \\"banana\\", \\"price\\": 3.0} ] }, { \\"customer\\": \\"Bob\\", \\"items\\": [ {\\"item\\": \\"laptop\\", \\"price\\": 1500.0}, {\\"item\\": \\"mouse\\", \\"price\\": 25.0} ] }, { \\"customer\\": \\"Charlie\\", \\"items\\": [ {\\"item\\": \\"book\\", \\"price\\": 20.0}, {\\"item\\": \\"pen\\", \\"price\\": 2.0}, {\\"item\\": \\"notebook\\", \\"price\\": 1.5} ] } ] result = process_orders(orders) print(result) # Expected Output: # { # \\"small\\": [\\"Charlie\\"], # \\"medium\\": [\\"Alice\\"], # \\"large\\": [\\"Bob\\"] # } ``` Constraints - You must use `if-elif-else` statements to categorize orders. - You must iterate over the list of orders using a `for` loop. - Use the `sum` function to calculate the total value of the items in an order. - Each order\'s `\\"items\\"` list will contain at least one item. - You must utilize the `match` statement to handle categorization.","solution":"def process_orders(orders: list) -> dict: Processes a list of customer orders and categorizes them into \'small\', \'medium\', and \'large\' based on total order value. Args: orders (list): List of dictionaries representing customer orders. Returns: dict: A dictionary with keys \'small\', \'medium\', and \'large\' containing lists of customer names. categorized_orders = { \\"small\\": [], \\"medium\\": [], \\"large\\": [] } for order in orders: customer = order[\'customer\'] total_value = sum(item[\'price\'] for item in order[\'items\']) if total_value < 50: categorized_orders[\\"small\\"].append(customer) elif 50 <= total_value <= 200: categorized_orders[\\"medium\\"].append(customer) elif total_value > 200: categorized_orders[\\"large\\"].append(customer) return categorized_orders"},{"question":"**Filename Pattern Matching using `fnmatch` Module** # Objective Create a Python script that processes a list of filenames and filters them based on specified patterns using the `fnmatch` module. The program should also provide an option for case-sensitive or case-insensitive matching. # Input 1. A list of strings representing filenames. For simplicity, you can assume the filenames do not contain directory paths. 2. A list of strings representing patterns to match against the filenames. 3. A boolean flag indicating whether the matching should be case-sensitive (`True`) or case-insensitive (`False`). # Output - A list of lists. Each sublist corresponds to the filenames matching each pattern provided. # Function Signature ```python def match_filenames(filenames: list[str], patterns: list[str], case_sensitive: bool) -> list[list[str]]: pass ``` # Constraints - The filenames list and patterns list will each have a length in the range `[1, 1000]`. - Each filename and pattern will have a length in the range `[1, 255]`. - Patterns may include any valid wildcards defined in the `fnmatch` module. # Example ```python # Example 1 filenames = [\\"readme.txt\\", \\"README.md\\", \\"tutorial.pdf\\", \\"example.TXT\\"] patterns = [\\"*.txt\\", \\"*.md\\"] case_sensitive = False print(match_filenames(filenames, patterns, case_sensitive)) # Output: [[\\"readme.txt\\", \\"example.TXT\\"], [\\"README.md\\"]] # Example 2 filenames = [\\"readme.txt\\", \\"README.md\\", \\"tutorial.pdf\\", \\"example.TXT\\"] patterns = [\\"*.txt\\", \\"*.md\\"] case_sensitive = True print(match_filenames(filenames, patterns, case_sensitive)) # Output: [[\\"readme.txt\\"], [\\"README.md\\"]] ``` # Description 1. Implement the `match_filenames` function to filter filenames based on the given patterns. 2. If `case_sensitive` is `True`, use `fnmatch.fnmatchcase` for matching. If `case_sensitive` is `False`, use `fnmatch.fnmatch`. 3. Return a list of lists, where each sublist contains filenames matching the corresponding pattern in the patterns list. **Note:** You need to import the `fnmatch` module in your implementation. # Evaluation Criteria - Correctness: The implementation correctly handles different patterns and flag settings. - Efficiency: The implementation performs well within the given constraints. - Clarity: The code is well-structured and easy to understand.","solution":"import fnmatch def match_filenames(filenames: list[str], patterns: list[str], case_sensitive: bool) -> list[list[str]]: Filters a list of filenames based on specified patterns with an option for case-sensitive or case-insensitive matching. Parameters: - filenames: List of filenames (strings). - patterns: List of patterns (strings) to filter filenames. - case_sensitive: Boolean flag for case-sensitive matching. Returns: - A list of lists. Each sublist contains filenames matching each pattern. result = [] for pattern in patterns: if case_sensitive: matched_files = [filename for filename in filenames if fnmatch.fnmatchcase(filename, pattern)] else: pattern_lower = pattern.lower() matched_files = [filename for filename in filenames if fnmatch.fnmatch(filename.lower(), pattern_lower)] result.append(matched_files) return result"},{"question":"# **Custom Class Behavior and Special Methods Implementation** **Objective:** You are tasked with implementing a custom class in Python that models a simple bank account system. The class should exhibit proper use of special methods, demonstrate mutability and immutability, and manage attributes appropriately using concepts from Python’s data model and type system. **Instructions:** 1. **Define a class named `BankAccount`.** The class should have the following attributes: - `account_number` (immutable, type `int`) - `account_holder` (immutable, type `str`) - `balance` (mutable, type `float`) - `transaction_history` (mutable, type `list` of tuples, each containing a string and a float) 2. **Implement the following methods:** - `__init__(self, account_number: int, account_holder: str, initial_deposit: float)`: Initializes the account with the given account number, account holder name, and an initial deposit. - `__repr__(self)`: Returns a string representation of the account that includes the account number, holder name, and balance. - `__str__(self)`: Returns a user-friendly string describing the account. - `__enter__(self)`: To be used with a context manager, should print a message indicating that the account is being accessed. - `__exit__(self, exc_type, exc_value, traceback)`: To be used with a context manager, should print a message indicating that the account is no longer being accessed. - `deposit(self, amount: float)`: Adds the amount to the account balance and records the transaction. - `withdraw(self, amount: float)`: Subtracts the amount from the balance if sufficient funds are available, otherwise raises an exception. Records the transaction if successful. - `__getitem__(self, index: int)`: Returns the transaction at the given index from the transaction history. 3. **Constraints:** - `account_number` should be unique and immutable once set. - `account_holder` should be immutable once set. - Proper error handling should be included for withdrawals that exceed the current balance. - Ensure that all relevant special methods are used appropriately to manage the state and behavior of the class. **Example Usage:** ```python with BankAccount(1234, \'John Doe\', 1000.0) as account: print(account) # Prints a user-friendly string of the account details. account.deposit(500.0) account.withdraw(200.0) print(account[0]) # Prints the first transaction in the history. print(repr(account)) # Prints the official string representation. ``` **Requirements:** - Ensure your class properly manages the immutability of certain attributes. - Demonstrate correct usage of context managers. - Implement error handling for invalid operations. - Utilize the special methods to provide both functional and user-friendly interfaces for your class. **Submit your implementation of the `BankAccount` class.**","solution":"class BankAccount: def __init__(self, account_number: int, account_holder: str, initial_deposit: float): self._account_number = account_number self._account_holder = account_holder self._balance = initial_deposit self._transaction_history = [(\'Initial deposit\', initial_deposit)] @property def account_number(self): return self._account_number @property def account_holder(self): return self._account_holder @property def balance(self): return self._balance @property def transaction_history(self): return self._transaction_history def __repr__(self): return f\\"BankAccount(account_number={self.account_number}, account_holder=\'{self.account_holder}\', balance={self.balance})\\" def __str__(self): return f\\"BankAccount of {self.account_holder}, Number: {self.account_number}, Balance: {self.balance:.2f}\\" def __enter__(self): print(f\\"Accessing account {self.account_number}\\") return self def __exit__(self, exc_type, exc_value, traceback): print(f\\"No longer accessing account {self.account_number}\\") def deposit(self, amount: float): if amount <= 0: raise ValueError(\\"Deposit amount must be positive.\\") self._balance += amount self._transaction_history.append((\'Deposit\', amount)) def withdraw(self, amount: float): if amount <= 0: raise ValueError(\\"Withdrawal amount must be positive.\\") if amount > self.balance: raise ValueError(\\"Insufficient funds.\\") self._balance -= amount self._transaction_history.append((\'Withdrawal\', amount)) def __getitem__(self, index: int): return self._transaction_history[index]"},{"question":"# XML Content Parsing and Handling with xml.sax.xmlreader Objective: The goal of this task is to assess your ability to create and use an XML parser using the `xml.sax.xmlreader` module in Python. You will be required to parse an XML document incrementally and handle its content using custom handlers for different XML components. Problem Statement: You need to implement a custom XML parser that reads an XML document incrementally and prints specific elements, attributes, or text content as specified. The parser should handle the content via an `IncrementalParser`. Task: 1. **Create a custom content handler** by inheriting from `xml.sax.handler.ContentHandler`. 2. **Create an incremental XML parser** using the `xml.sax.xmlreader.IncrementalParser` class. 3. **Override the necessary methods** in your content handler to print specific elements or attributes when encountered during parsing. 4. **Write a function `parse_xml_incrementally`** that: - Accepts the path to an XML file. - Uses the custom content handler with the incremental parser to print the desired XML content as specified. Example: Given the following sample XML file `example.xml`: ```xml <root> <element id=\\"1\\">This is element 1</element> <element id=\\"2\\">This is element 2</element> </root> ``` The expected output should print each element and its `id` attribute: ``` Element: element, ID: 1, Text: This is element 1 Element: element, ID: 2, Text: This is element 2 ``` Input: - File path to an XML document. Output: - Prints elements and associated attributes/text content as specified in the task. Constraints: - Assume the XML file is well-formed. - Handle the file incrementally to ensure memory efficiency. Implementation Notes: - Define a content handler that inherits from `xml.sax.handler.ContentHandler`. - In the content handler, override methods such as `startElement`, `endElement`, and `characters` to handle the XML content. - Use `xml.sax.xmlreader.IncrementalParser` to parse the XML content in chunks. Example Code Structure: ```python import xml.sax from xml.sax.handler import ContentHandler from xml.sax.xmlreader import IncrementalParser class CustomContentHandler(ContentHandler): def startElement(self, name, attrs): pass # Implement element handling logic def endElement(self, name): pass # Implement element handling logic def characters(self, content): pass # Implement text handling logic def parse_xml_incrementally(file_path): pass # Implement the function to parse XML incrementally # Sample call to the function parse_xml_incrementally(\'example.xml\') ```","solution":"import xml.sax from xml.sax.handler import ContentHandler from xml.sax.xmlreader import IncrementalParser class CustomContentHandler(ContentHandler): def __init__(self): self.current_data = \'\' self.current_element = \'\' self.current_id = \'\' def startElement(self, name, attrs): self.current_data = \'\' if name == \'element\': self.current_element = name self.current_id = attrs.get(\'id\', \'\') def endElement(self, name): if name == self.current_element: print(f\'Element: {self.current_element}, ID: {self.current_id}, Text: {self.current_data.strip()}\') self.current_element = \'\' self.current_id = \'\' def characters(self, content): if self.current_element: self.current_data += content def parse_xml_incrementally(file_path): parser = xml.sax.make_parser() handler = CustomContentHandler() parser.setContentHandler(handler) with open(file_path, \'r\') as file: for line in file: parser.feed(line) parser.close()"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of linear regression, ridge regression, and polynomial regression using scikit-learn. # Task: Write a Python script that accomplishes the following: 1. **Data Generation**: - Generate a synthetic dataset suitable for a regression task. - The dataset should have 100 samples and 3 features. Create a target variable that is a polynomial combination of the features, with some added noise. 2. **Ordinary Least Squares (OLS) Regression**: - Fit a linear regression model to the dataset using `LinearRegression`. - Extract and print the model coefficients and intercept. - Predict and print the first 5 predicted values on the training data. 3. **Ridge Regression** - Fit a ridge regression model to the dataset using `Ridge`. Set the regularization parameter alpha to 1. - Extract and print the model coefficients and intercept. - Predict and print the first 5 predicted values on the training data. 4. **Polynomial Features and Regression** - Transform the features to include polynomial features up to degree 2. - Fit a linear regression model to this transformed dataset. - Print the model coefficients and intercept after transformation. - Predict and print the first 5 predicted values on the transformed training data. # Expected Input and Output **Input:** - No external input required. The script should generate the dataset internally. **Output:** - Model coefficients, intercept, and first 5 predictions for each of the regression tasks. Specifically: - OLS: coefficients, intercept, first 5 predictions - Ridge: coefficients, intercept, first 5 predictions - Polynomial Regression: coefficients, intercept, first 5 predictions # Constraints - Use scikit-learn for implementing the regressions and generating polynomial features. - Handle any data preprocessing required internally within the script. # Performance Requirements - The script should execute within a reasonable time frame (<10 seconds). ```python Coding Solution Template: import numpy as np from sklearn.linear_model import LinearRegression, Ridge from sklearn.preprocessing import PolynomialFeatures # 1. Data Generation np.random.seed(42) X = np.random.randn(100, 3) y = 3 * X[:, 0]**2 + 2 * X[:, 1] + X[:, 2] + np.random.randn(100) * 0.5 # 2. Ordinary Least Squares (OLS) Regression lin_reg = LinearRegression() lin_reg.fit(X, y) print(\\"OLS Coefficients:\\", lin_reg.coef_) print(\\"OLS Intercept:\\", lin_reg.intercept_) ols_predictions = lin_reg.predict(X) print(\\"OLS Predictions (first 5):\\", ols_predictions[:5]) # 3. Ridge Regression ridge_reg = Ridge(alpha=1) ridge_reg.fit(X, y) print(\\"Ridge Coefficients:\\", ridge_reg.coef_) print(\\"Ridge Intercept:\\", ridge_reg.intercept_) ridge_predictions = ridge_reg.predict(X) print(\\"Ridge Predictions (first 5):\\", ridge_predictions[:5]) # 4. Polynomial Features and Regression poly = PolynomialFeatures(degree=2) X_poly = poly.fit_transform(X) lin_reg_poly = LinearRegression() lin_reg_poly.fit(X_poly, y) print(\\"Polynomial Regression Coefficients:\\", lin_reg_poly.coef_) print(\\"Polynomial Regression Intercept:\\", lin_reg_poly.intercept_) poly_predictions = lin_reg_poly.predict(X_poly) print(\\"Polynomial Regression Predictions (first 5):\\", poly_predictions[:5]) ```","solution":"import numpy as np from sklearn.linear_model import LinearRegression, Ridge from sklearn.preprocessing import PolynomialFeatures def generate_data(): np.random.seed(42) X = np.random.randn(100, 3) y = 3 * X[:, 0]**2 + 2 * X[:, 1] + X[:, 2] + np.random.randn(100) * 0.5 return X, y def perform_ols_regression(X, y): lin_reg = LinearRegression() lin_reg.fit(X, y) coefficients = lin_reg.coef_ intercept = lin_reg.intercept_ predictions = lin_reg.predict(X)[:5] # Get the first 5 predictions return coefficients, intercept, predictions def perform_ridge_regression(X, y): ridge_reg = Ridge(alpha=1) ridge_reg.fit(X, y) coefficients = ridge_reg.coef_ intercept = ridge_reg.intercept_ predictions = ridge_reg.predict(X)[:5] # Get the first 5 predictions return coefficients, intercept, predictions def perform_polynomial_regression(X, y): poly = PolynomialFeatures(degree=2) X_poly = poly.fit_transform(X) lin_reg_poly = LinearRegression() lin_reg_poly.fit(X_poly, y) coefficients = lin_reg_poly.coef_ intercept = lin_reg_poly.intercept_ predictions = lin_reg_poly.predict(X_poly)[:5] # Get the first 5 predictions return coefficients, intercept, predictions # Generate data X, y = generate_data() # OLS Regression ols_coef, ols_intercept, ols_predictions = perform_ols_regression(X, y) print(\\"OLS Coefficients:\\", ols_coef) print(\\"OLS Intercept:\\", ols_intercept) print(\\"OLS Predictions (first 5):\\", ols_predictions) # Ridge Regression ridge_coef, ridge_intercept, ridge_predictions = perform_ridge_regression(X, y) print(\\"Ridge Coefficients:\\", ridge_coef) print(\\"Ridge Intercept:\\", ridge_intercept) print(\\"Ridge Predictions (first 5):\\", ridge_predictions) # Polynomial Regression poly_coef, poly_intercept, poly_predictions = perform_polynomial_regression(X, y) print(\\"Polynomial Regression Coefficients:\\", poly_coef) print(\\"Polynomial Regression Intercept:\\", poly_intercept) print(\\"Polynomial Regression Predictions (first 5):\\", poly_predictions)"},{"question":"# Customizing a Python Class to Behave Like a Mutable Sequence **Objective:** To assess your understanding of Python\'s data model and customizing class behavior with special methods. # Task: Implement a custom Python class `CustomList` that behaves like a mutable sequence. You need to implement the following special methods to achieve this: 1. **`__init__(self, iterable=[])`**: Initialize the `CustomList` with data from an iterable. 2. **`__len__(self)`**: Return the number of elements in the list. 3. **`__getitem__(self, index)`**: Get the element at the given index. 4. **`__setitem__(self, index, value)`**: Set the element at the given index. 5. **`__delitem__(self, index)`**: Delete the element at the given index. 6. **`__iter__(self)`**: Return an iterator over the elements. 7. **`__contains__(self, item)`**: Return `True` if the item is in the list, `False` otherwise. 8. **`__repr__(self)`**: Return a string representation of the `CustomList`. Example Usage: ```python cl = CustomList([1, 2, 3]) print(len(cl)) # 3 print(cl[1]) # 2 cl[1] = 5 print(cl) # CustomList([1, 5, 3]) del cl[0] print(cl) # CustomList([5, 3]) print(5 in cl) # True for item in cl: print(item) # 5, 3 ``` # Constraints: - You must not use Python\'s built-in list type directly for storage; you can use other structures like dictionaries or other custom objects. - Ensure that your implementation handles edge cases, such as invalid indices, gracefully. # Submission: Submit your implementation of the `CustomList` class with the required special methods.","solution":"class CustomList: def __init__(self, iterable=[]): self._data = list(iterable) def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __iter__(self): return iter(self._data) def __contains__(self, item): return item in self._data def __repr__(self): return f\'CustomList({self._data})\'"},{"question":"You are tasked with implementing an event logging system using PyTorch\'s distributed elastic events package. Your goal is to create a function that constructs and records a custom event. Here\'s what you need to do: 1. Implement a function `log_custom_event` which takes the following parameters: - `event_source (str)`: The source of the event. - `event_name (str)`: The name of the event. - `event_metadata (dict)`: Additional metadata about the event. 2. Utilize the API methods and classes mentioned in the documentation snippet to achieve this. The function should: - Construct an event using the `Event`, `EventSource`, and `EventMetadataValue` classes. - Record the event using the `record` method. - Ensure the logging is handled appropriately using `get_logging_handler`. Your final function should resemble the outline below: ```python import torch.distributed.elastic.events as events import torch.distributed.elastic.events.api as api def log_custom_event(event_source: str, event_name: str, event_metadata: dict): # Create EventSource source = api.EventSource(event_source) # Create EventMetadata metadata = {key: api.EventMetadataValue(value) for key, value in event_metadata.items()} # Create Event event = api.Event(event_name, source, metadata) # Initialize logging handler handler = events.get_logging_handler() # Record Event handler(event) # Additionally use the record method if necessary events.record(event) # Example usage log_custom_event(\'test_source\', \'test_event\', {\'key1\': \'value1\', \'key2\': \'value2\'}) ``` **Constraints:** - Ensure the function appropriately converts metadata dictionary values into `EventMetadataValue` types. - Handle any potential exceptions during event creation and logging gracefully. **Input:** ```python log_custom_event(\\"example_source\\", \\"example_name\\", {\\"user_id\\": \\"12345\\", \\"status\\": \\"initialized\\"}) ``` **Output:** The function should correctly log the event with the provided source, name, and metadata. There will be no explicit output from the function, but it should internally log the event as specified. Implement this function inside a script or a Jupyter notebook as part of your solution.","solution":"import torch.distributed.elastic.events as events import torch.distributed.elastic.events.api as api def log_custom_event(event_source: str, event_name: str, event_metadata: dict): try: # Create EventSource source = api.EventSource(event_source) # Create EventMetadata metadata = {key: api.EventMetadataValue(value) for key, value in event_metadata.items()} # Create Event event = api.Event(event_name, source, metadata) # Initialize logging handler handler = events.get_logging_handler() # Record Event handler(event) # Use the record method to ensure it\'s handled by the event system events.record(event) except Exception as e: print(f\\"Failed to log event: {e}\\") # Example usage log_custom_event(\'test_source\', \'test_event\', {\'key1\': \'value1\', \'key2\': \'value2\'})"},{"question":"# Python Coding Assessment: Custom Python Auto-Completer Consider you are developing a custom Python interactive shell. One essential feature is the ability to auto-complete Python identifiers and keywords to help users by suggesting possible completions while they type. You are required to implement a simplified version of a Python auto-completer modeled after the `rlcompleter` module. Task: Implement a class `CustomCompleter` with a method `complete` that provides auto-completing capabilities given a text input. The class and method should function as follows: - **Class**: `CustomCompleter` - **Method**: `complete` - **Input**: - `text` (str): The input text that needs completion. - `state` (int): An integer specifying the completion state (0 for the initial request, 1 for the first match, etc.). - **Output**: Returns the `state`th completion option for the provided `text`. Requirements: 1. If the `text` does not contain a period (`.`), the method should suggest completions from: - Names defined in the current global scope (simulated as a dictionary for this task). - Python built-in functions and types. - Python keywords. 2. If the `text` contains a period (`.`), the method should: - Safely evaluate the expression up to the last dot without causing side-effects. - Provide completion options from the attributes of the resulting object. - Handle exceptions gracefully and return `None` if an error occurs during evaluation. 3. Provide basic exception handling to ensure that the method does not crash due to invalid input. Example Usage: ```python import keyword import builtins class CustomCompleter: def __init__(self): # Simulate a global scope (e.g., names defined in `__main__`) self.global_scope = {\\"hello\\": \\"world\\", \\"sample_function\\": lambda x: x} def complete(self, text, state): # Your implementation here pass # Example of how the `complete` method should be used: completer = CustomCompleter() # Simulating keyword completion print(completer.complete(\\"im\\", 0)) # Expected: \'import\' print(completer.complete(\\"im\\", 1)) # Expected: None (or the next match if there was any) # Simulating global scope completion print(completer.complete(\\"he\\", 0)) # Expected: \'hello\' print(completer.complete(\\"he\\", 1)) # Expected: None (or the next match if there was any) # Simulating attribute completion of a built-in type print(completer.complete(\\"str.u\\", 0)) # Expected: \'upper\' print(completer.complete(\\"str.u\\", 1)) # Expected: None (or the next match if there was any) ``` Constraints: 1. Consider standard Python keywords and built-ins. 2. Assume the expression evaluation does not involve function calls to prevent side-effects. 3. Performance should allow instantaneous suggestions in an interactive shell. Good luck!","solution":"import keyword import builtins class CustomCompleter: def __init__(self): self.global_scope = {\\"hello\\": \\"world\\", \\"sample_function\\": lambda x: x} def complete(self, text, state): if \'.\' not in text: return self.complete_without_dot(text, state) else: return self.complete_with_dot(text, state) def complete_without_dot(self, text, state): options = [] options.extend(self.global_scope.keys()) options.extend(dir(builtins)) options.extend(keyword.kwlist) options = [item for item in options if item.startswith(text)] options = sorted(set(options)) # Removing duplicates and sorting if state < len(options): return options[state] return None def complete_with_dot(self, text, state): try: obj, _, attr = text.rpartition(\'.\') obj_eval = eval(obj, {}, self.global_scope) options = [item for item in dir(obj_eval) if item.startswith(attr)] options = sorted(set(options)) # Removing duplicates and sorting if state < len(options): return f\\"{obj}.{options[state]}\\" return None except: return None"},{"question":"You are given access to sound files in a directory named `sounds_dir`. Your task is to write a Python function that scans all the sound files in this directory, analyzes them using the `sndhdr` module, and outputs the results in a structured format. # Requirements 1. Implement a function `analyze_sound_files(directory: str) -> dict` that: - Takes the path to the directory as an input. - Uses the `sndhdr.what()` function to determine the sound file attributes. - Returns a dictionary where the keys are filenames and the values are dictionaries with the sound file properties (`filetype`, `framerate`, `nchannels`, `nframes`, `sampwidth`). If a file could not be processed, its value should be `None`. # Input & Output Format - **Input:** A string representing the path to the directory containing sound files. - **Output:** A dictionary with filenames as keys and dictionaries of file attributes (or `None`) as values. # Constraints - The directory may contain a mix of sound file types. - Handle any exceptions gracefully and ensure that your function does not crash even if a file cannot be processed. # Example ```python import os def analyze_sound_files(directory: str) -> dict: import os import sndhdr results = {} for filename in os.listdir(directory): filepath = os.path.join(directory, filename) try: hdr_info = sndhdr.what(filepath) if hdr_info: results[filename] = { \'filetype\': hdr_info.filetype, \'framerate\': hdr_info.framerate, \'nchannels\': hdr_info.nchannels, \'nframes\': hdr_info.nframes, \'sampwidth\': hdr_info.sampwidth } else: results[filename] = None except Exception as e: results[filename] = None return results # Example usage directory_path = \'sounds_dir\' print(analyze_sound_files(directory_path)) # Potential output # { # \\"sound1.wav\\": { # \\"filetype\\": \\"wav\\", # \\"framerate\\": 44100, # \\"nchannels\\": 2, # \\"nframes\\": 1048576, # \\"sampwidth\\": 16 # }, # \\"sound2.aiff\\": None, # ... # } ``` **Notes:** - The function should iterate through the given directory and analyze each file. - Use the `os` module for file operations. - The dictionary returned should contain all the files from the directory, regardless of their ability to be processed as sound files.","solution":"import os import sndhdr def analyze_sound_files(directory: str) -> dict: Analyzes all sound files in the given directory using the sndhdr module. Parameters: - directory (str): The path to the directory containing sound files. Returns: - dict: A dictionary where the keys are filenames and the values are dictionaries of sound file properties (\'filetype\', \'framerate\', \'nchannels\', \'nframes\', \'sampwidth\') or None if the file could not be processed. results = {} for filename in os.listdir(directory): filepath = os.path.join(directory, filename) try: hdr_info = sndhdr.what(filepath) if hdr_info: results[filename] = { \'filetype\': hdr_info.filetype, \'framerate\': hdr_info.framerate, \'nchannels\': hdr_info.nchannels, \'nframes\': hdr_info.nframes, \'sampwidth\': hdr_info.sampwidth } else: results[filename] = None except Exception: results[filename] = None return results"},{"question":"You are tasked with creating a utility function to parse the Unix group database and derive specific information about group memberships. The function needs to leverage the `grp` module to return detailed information about group memberships in a defined format. # Function Signature ```python def get_group_membership_summary(group_ids: list[int], group_names: list[str]) -> dict: pass ``` # Inputs - `group_ids`: A list of integer Group IDs. - `group_names`: A list of string Group Names. # Outputs - A dictionary with group IDs as keys and a dictionary as values. Each nested dictionary should have: - `\'group_name\'`: The name of the group. - `\'group_members\'`: A list containing the usernames of all members in the group. # Constraints 1. If a group ID or group name is invalid or does not exist, it should be skipped. 2. Handle `TypeError` and `KeyError` exceptions appropriately. 3. Your solution should combine entries from both `group_ids` and `group_names` without duplications. # Example ```python import grp def get_group_membership_summary(group_ids, group_names): # Your code here pass # Example usage: group_ids = [0, 1000, 1001] group_names = [\\"staff\\", \\"nobody\\", \\"admin\\"] # Expected Output: # { # 0: {\'group_name\': \'root\', \'group_members\': [\'root\']}, # 1000: {\'group_name\': \'group1\', \'group_members\': [\'user1\', \'user2\']}, # 1001: {\'group_name\': \'group2\', \'group_members\': [\'user3\']}, # \'staff\': {\'group_name\': \'staff\', \'group_members\': [\'admin\', \'user4\']}, # \'nobody\': {\'group_name\': \'nobody\', \'group_members\': []}, # \'admin\': {\'group_name\': \'admin\', \'group_members\': [\'root\', \'admin\']} # } ``` # Notes - Be sure to test your code on a Unix system. - Pay attention to grouping and output formatting requirements. - Verify your solution by comparing it against actual system group information. Good luck, and think critically about efficient data processing and combining results from multiple queries.","solution":"import grp def get_group_membership_summary(group_ids: list[int], group_names: list[str]) -> dict: summary = {} # Process group_ids for gid in group_ids: try: group_info = grp.getgrgid(gid) summary[gid] = { \'group_name\': group_info.gr_name, \'group_members\': list(group_info.gr_mem) } except (KeyError, TypeError): continue # Process group_names for gname in group_names: try: group_info = grp.getgrnam(gname) if group_info.gr_gid not in summary: summary[gname] = { \'group_name\': gname, \'group_members\': list(group_info.gr_mem) } except (KeyError, TypeError): continue return summary"},{"question":"<|Analysis Begin|> The `pathlib` module in Python provides an object-oriented approach to handling and manipulating file system paths. This module is especially useful as it abstracts many filesystem operations, allowing developers to work with paths in a platform-independent manner. There are two main categories of classes within `pathlib`: 1. **Pure Paths** - These classes handle path manipulations without actual file system calls. They are useful for string-based path operations. 2. **Concrete Paths** - These classes inherit from Pure Paths and add methods that interact with the file system to perform operations like file querying, file creation, and directory traversal. The key classes and their functionalities include: - `PurePath`, `PurePosixPath`, and `PureWindowsPath` for string manipulations. - `Path`, `PosixPath`, and `WindowsPath` for interacting with the filesystem. - Methods for creating, querying, and manipulating paths (`exists()`, `is_file()`, `mkdir()`, `iterdir()`, etc.). - Special path manipulation methods (`glob()`, `rglob()`, `resolve()`, etc.). The documentation provides an extensive overview of methods that can be used to navigate, modify, and retrieve information about file system paths. For a challenging assessment, a question can be designed to test a student\'s ability to navigate through directories, filter specific files, and perform some read/write operations using the `pathlib` module. Specifically, the task would be to perform directory traversal, file filtering, and file content manipulation. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Create a Python function that traverses a specified directory and its subdirectories, filters out all text files (.txt), and performs the following operations: 1. Lists all the text files. 2. Reads the content of each text file and counts the number of lines containing a specific keyword. 3. Writes a summary file (`summary.txt`) in the root of the specified directory that contains the name of each text file and its corresponding count of lines containing the keyword. Function Signature ```python def summarize_text_files(root_directory: str, keyword: str) -> None: Traverse the specified directory and its subdirectories to find text files. Count the number of lines containing the keyword in each text file. Write a summary file named \'summary.txt\' in the root directory. Args: root_directory (str): The root directory to start the traversal. keyword (str): The keyword to search for in text files. Returns: None ``` Input - **root_directory**: A string representing the path to the root directory where the traversal starts. This directory and its subdirectories contain the text files. - **keyword**: A string keyword to search within the text files. Output - A file named `summary.txt` in the root directory containing: - Each line should have the format `filename: count` where `filename` is the path of the text file relative to the root directory, and `count` is the number of lines containing the keyword. Constraints 1. The function should handle both absolute and relative paths. 2. Only files with the `.txt` extension should be processed. 3. Performance: The solution should efficiently handle large directory structures and numerous files. 4. The function should handle possible exceptions (e.g., file not found, permission issues) gracefully. Example Usage Suppose the directory structure is as follows: ``` root_directory/ |- folder1/ |- file1.txt |- file2.txt |- folder2/ |- file3.txt |- summary.txt (to be created by your function) ``` Content of the text files: - `file1.txt`: \\"hello worldnkeyword presentnanother linen\\" - `file2.txt`: \\"nothing herenjust some textn\\" - `file3.txt`: \\"keyword is herenkeyword againnkeyword!n\\" Keyword to search: \\"keyword\\" The `summary.txt` should contain: ``` folder1/file1.txt: 1 folder1/folder2/file3.txt: 3 ``` Implementation Notes - Use the `pathlib` module for handling paths and file operations. - Ensure to handle edge cases such as no text files, no lines containing the keyword, or directories with restricted permissions. Hints - You may find `Path.glob(\'**/*.txt\')` useful for recursive searching of text files. - Use the `read_text()` method to read the content of the text files and `write_text()` to write to the `summary.txt` file. - Consider the `relative_to()` method to get the relative path from the root directory. Good luck!","solution":"from pathlib import Path def summarize_text_files(root_directory: str, keyword: str) -> None: Traverse the specified directory and its subdirectories to find text files. Count the number of lines containing the keyword in each text file. Write a summary file named \'summary.txt\' in the root directory. Args: root_directory (str): The root directory to start the traversal. keyword (str): The keyword to search for in text files. Returns: None root_path = Path(root_directory) summary_lines = [] for text_file in root_path.rglob(\'*.txt\'): try: content = text_file.read_text() count = sum(1 for line in content.splitlines() if keyword in line) if count > 0: relative_path = text_file.relative_to(root_path) summary_lines.append(f\\"{relative_path}: {count}\\") except Exception as e: print(f\\"Could not process file {text_file}: {e}\\") summary_file = root_path / \'summary.txt\' summary_file.write_text(\'n\'.join(summary_lines))"},{"question":"Context In CUDA programming with PyTorch, tensors can be processed on different streams to achieve concurrency. However, improper synchronization between streams can lead to data races, causing undefined behavior when accessing tensors. To assist in identifying and debugging such issues, PyTorch offers a CUDA Stream Sanitizer. This tool helps detect data races and provides insights into their origins. Task Write a function `detect_and_fix_stream_data_race` to demonstrate the following: 1. Create a tensor on the default CUDA stream. 2. Access and modify this tensor on a new CUDA stream, intentionally introducing a data race. 3. Use PyTorch\'s CUDA Stream Sanitizer to detect the data race. 4. Synchronize the streams to fix the data race and ensure no errors are reported. Function Signature ```python def detect_and_fix_stream_data_race(): # Your implementation here ``` Requirements 1. Create a tensor on the default CUDA stream, and print its initial state. 2. Modify the tensor on a new CUDA stream without synchronization and run the script with the CUDA Stream Sanitizer enabled to detect the data race. 3. Print the sanitizer\'s output to demonstrate the detection of the data race. 4. Fix the data race by synchronizing the streams appropriately. 5. Modify the tensor again on the new stream with proper synchronization and ensure no errors are detected. Expected Output The function should demonstrate the following: 1. Creation and initial state of the tensor. 2. Outputs generated by the CUDA Stream Sanitizer detecting the data race. 3. Verification that the data race is fixed after synchronization. Input and Output This function does not require user input. The function should output diagnostic information to the console. Example Run ```python def detect_and_fix_stream_data_race(): import torch # Step 1: Create tensor on default CUDA stream a = torch.rand(4, 2, device=\\"cuda\\") print(\\"Initial Tensor (default stream):\\", a) # Step 2: Modify the tensor on a new CUDA stream without synchronization new_stream = torch.cuda.Stream() with torch.cuda.stream(new_stream): torch.mul(a, 5, out=a) # Step 3: Run script with CUDA Stream Sanitizer enabled to detect data race # Hint: Set the environment variable `TORCH_CUDA_SANITIZER=1` when running the script in commandline # Simulate the detected data race (for illustration; actual detection happens externally) print(\\"Sanitizer Output: Detected a possible data race...\\") # Step 4: Fix the data race with proper stream synchronization with torch.cuda.stream(new_stream): torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) torch.mul(a, 5, out=a) print(\\"Final Tensor (synchronized):\\", a) # Example run (ensure `TORCH_CUDA_SANITIZER` is set when running this script) detect_and_fix_stream_data_race() ``` Note: To observe actual CUDA Stream Sanitizer output, this script should be run in an environment where the `TORCH_CUDA_SANITIZER=1` environment variable is set.","solution":"import torch def detect_and_fix_stream_data_race(): This function demonstrates creating a tensor on default CUDA stream, introducing and detecting a data race, then fixing the data race by synchronizing streams. # Step 1: Create tensor on the default CUDA stream a = torch.rand(4, 2, device=\\"cuda\\") print(\\"Initial Tensor (default stream):\\", a) # Step 2: Modify the tensor on a new CUDA stream without synchronization new_stream = torch.cuda.Stream() with torch.cuda.stream(new_stream): torch.mul(a, 5, out=a) # Step 3: This step requires you to run the script with CUDA Stream Sanitizer enabled # e.g., setting the environment variable `TORCH_CUDA_SANITIZER` to `1`. # Since we cannot simulate this directly here, we will describe the expected output instead. print(\\"Sanitizer Output: Detected a possible data race...\\") # Step 4: Fix the data race by synchronizing streams with torch.cuda.stream(new_stream): torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) torch.mul(a, 5, out=a) # Print the final tensor after synchronization print(\\"Final Tensor (synchronized):\\", a) # The main function to execute the data race detection and fixing detect_and_fix_stream_data_race()"},{"question":"# Password Management System Your task is to create a simple password management system using the `crypt` module. This system will allow users to set a password and then authenticate themselves by verifying their password. You should support multiple hashing methods for password storage and ensure that the password verification is secure. Functions to Implement 1. **`set_password(password: str, method: Optional[str] = None, rounds: Optional[int] = None) -> str`**: - **Input**: - `password` (str): The user\'s password. - `method` (str, optional): The hashing method to use. It can be one of the following: \'SHA512\', \'SHA256\', \'BLOWFISH\', \'MD5\', \'CRYPT\'. If no method is provided, the strongest available method should be used. - `rounds` (int, optional): The number of rounds for the hashing algorithm (applicable only for SHA256, SHA512, and Blowfish). If no rounds are provided, default values should be used. - **Output**: - A string representing the hashed password. 2. **`authenticate(stored_password: str, password_attempt: str) -> bool`**: - **Input**: - `stored_password` (str): The hashed password stored in the system. - `password_attempt` (str): The user\'s password attempt to authenticate. - **Output**: - A boolean value indicating whether the password attempt is correct. Constraints - You should ensure proper usage of available hashing methods and handle the cases where a method is not supported on a platform. - Utilize constant-time comparison to avoid timing attacks during password verification. - If an unsupported method is requested, raise a ValueError with a meaningful message. Example Usage ``` python # Set a password using SHA-512 hashed_password = set_password(\'my_super_secure_password\', method=\'SHA512\') # Authenticate using the correct password assert authenticate(hashed_password, \'my_super_secure_password\') == True # Authenticate using an incorrect password assert authenticate(hashed_password, \'wrong_password\') == False # Set a password using the strongest available method hashed_password = set_password(\'another_secure_password\') # Authenticate using the correct password assert authenticate(hashed_password, \'another_secure_password\') == True ``` **Note**: You are allowed to use Python\'s built-in `crypt` module and the `compare_digest` function from the `hmac` module for secure comparison.","solution":"import crypt import hmac from typing import Optional SUPPORTED_METHODS = { \'SHA512\': crypt.METHOD_SHA512, \'SHA256\': crypt.METHOD_SHA256, \'BLOWFISH\': crypt.METHOD_BLOWFISH, \'MD5\': crypt.METHOD_MD5, \'CRYPT\': crypt.METHOD_CRYPT } def set_password(password: str, method: Optional[str] = None, rounds: Optional[int] = None) -> str: Returns a hashed password using the specified method and rounds. if method and method not in SUPPORTED_METHODS: raise ValueError(f\\"Unsupported hashing method: {method}\\") method = SUPPORTED_METHODS.get(method, crypt.METHOD_SHA512) if rounds: return crypt.crypt(password, crypt.mksalt(method, rounds=rounds)) else: return crypt.crypt(password, crypt.mksalt(method)) def authenticate(stored_password: str, password_attempt: str) -> bool: Verifies a password attempt against the stored hashed password. hashed_attempt = crypt.crypt(password_attempt, stored_password) return hmac.compare_digest(hashed_attempt, stored_password)"},{"question":"# Mocking and Patching in Python with `unittest.mock` Objective: Demonstrate your ability to use the `unittest.mock` library to mock dependencies in a system and perform assertions on how they are used. Description: You are part of a team developing a notification system. One of the modules you are responsible for sends out email notifications. This module is dependent on `SMTPClient` to actually send the emails. The class is designed as follows: ```python class SMTPClient: def __init__(self, server: str, port: int): self.server = server self.port = port def send_email(self, from_addr: str, to_addrs: list, msg: str) -> bool: # Imagine this function connects to the SMTP server and sends the email pass class EmailNotifier: def __init__(self, smtp_client: SMTPClient): self.smtp_client = smtp_client def notify(self, from_addr: str, to_addrs: list, msg: str) -> bool: return self.smtp_client.send_email(from_addr, to_addrs, msg) ``` Task: 1. Write unit tests to validate the behavior of the `EmailNotifier` class, making sure to: - Mock the `SMTPClient.send_email` method. - Use `patch` to substitute the `SMTPClient` during tests. - Assert that `send_email` was called with the correct parameters. - Simulate different side effects (e.g., `send_email` returning `True` or raising exceptions). 2. For bonus points, demonstrate how to use `autospec` to ensure the mock adheres to the interface of `SMTPClient`. Requirements: 1. Implement the `test_notify_success` method to validate that `notify` returns `True` when `send_email` returns `True`. 2. Implement the `test_notify_failure` method to validate that `notify` raises an exception if `send_email` raises an exception. 3. Ensure that all assertions are made using the `unittest` framework. Input: - The `EmailNotifier` and `SMTPClient` class definitions. Output: - Python unittest code that asserts the functionality described above. ```python import unittest from unittest.mock import Mock, patch, create_autospec from email_notifier import EmailNotifier, SMTPClient class TestEmailNotifier(unittest.TestCase): @patch(\'email_notifier.SMTPClient\') def test_notify_success(self, MockSMTPClient): # Arrange mock_smtp_client = MockSMTPClient.return_value mock_smtp_client.send_email.return_value = True notifier = EmailNotifier(mock_smtp_client) # Act result = notifier.notify(\'from@example.com\', [\'to@example.com\'], \'Test message\') # Assert self.assertTrue(result) mock_smtp_client.send_email.assert_called_once_with(\'from@example.com\', [\'to@example.com\'], \'Test message\') @patch(\'email_notifier.SMTPClient\') def test_notify_failure(self, MockSMTPClient): # Arrange mock_smtp_client = MockSMTPClient.return_value mock_smtp_client.send_email.side_effect = Exception(\\"Failed to send email\\") notifier = EmailNotifier(mock_smtp_client) # Act and Assert with self.assertRaises(Exception): notifier.notify(\'from@example.com\', [\'to@example.com\'], \'Test message\') mock_smtp_client.send_email.assert_called_once_with(\'from@example.com\', [\'to@example.com\'], \'Test message\') def test_autospec_usage(self): # Arrange mock_smtp_client = create_autospec(SMTPClient, instance=True) mock_smtp_client.send_email.return_value = True notifier = EmailNotifier(mock_smtp_client) # Act result = notifier.notify(\'from@example.com\', [\'to@example.com\'], \'Test message\') # Assert self.assertTrue(result) mock_smtp_client.send_email.assert_called_once_with(\'from@example.com\', [\'to@example.com\'], \'Test message\') if __name__ == \'__main__\': unittest.main() ``` Submit your answer as a `.py` file containing the test cases.","solution":"def add(a, b): Returns the sum of a and b. return a + b"},{"question":"Design a function `custom_policy_email_handler` that reads an email message from a binary file with a specific policy, manipulates the policy, and then writes the modified email message to a new file. Your function should demonstrate the usage of at least two different policy attributes (e.g., `max_line_length` and `linesep`). # Function Signature ```python def custom_policy_email_handler(input_file: str, output_file: str, max_line_length: int, linesep: str) -> None: ``` # Input - `input_file` (str): The path to the binary input file containing the email message. - `output_file` (str): The path to the output file where the modified message will be written. - `max_line_length` (int): The maximum allowable line length for the email headers. - `linesep` (str): The string to be used to terminate lines in the serialized output. # Output - None: The function should modify and write the email message to the specified output file. # Constraints - Ensure `max_line_length` is greater than 0. - `linesep` should be a valid line separator (either \'n\' or \'rn\'). # Example ```python custom_policy_email_handler(\'input_email.eml\', \'output_email.eml\', 100, \'rn\') ``` # Notes 1. You need to use the `email` module\'s functionality to read, modify, and write the email message. 2. Be aware of handling defaults and cloning policies when modifying attributes. 3. Test the email operations by creating simple test email files. Your implementation should demonstrate understanding and application of advanced concepts in the `email.policy` package, such as policy creation, cloning, and the impact of policy attributes on email handling.","solution":"import email from email import policy from email.parser import BytesParser from email.generator import BytesGenerator def custom_policy_email_handler(input_file: str, output_file: str, max_line_length: int, linesep: str) -> None: Reads an email message from a binary file with a specific policy, manipulates the policy, and then writes the modified email message to a new file. Args: - input_file (str): Path to the binary input file containing the email message. - output_file (str): Path to the output file where the modified message will be written. - max_line_length (int): Maximum allowable line length for the email headers. - linesep (str): The string to be used to terminate lines in the serialized output. # Ensure max_line_length is greater than 0 if max_line_length <= 0: raise ValueError(\\"max_line_length must be greater than 0\\") # Ensure linesep is either \'n\' or \'rn\' if linesep not in (\'n\', \'rn\'): raise ValueError(\\"linesep must be either \'n\' or \'rn\'\\") # Read the email from the input file with open(input_file, \'rb\') as f: msg = BytesParser(policy=policy.default).parse(f) # Create a new policy with the modified attributes custom_policy = policy.default.clone(max_line_length=max_line_length, linesep=linesep) # Write the modified email to the output file using the custom policy with open(output_file, \'wb\') as f: BytesGenerator(f, policy=custom_policy).flatten(msg)"},{"question":"Objective: To understand the student\'s ability to implement and work with concepts similar to cell objects in Python, ensuring comprehension of reference management, scope sharing, and dereferencing. Problem Statement: You are required to simulate the behavior of \\"Cell\\" objects described in the documentation within Python. Design a class `Cell` and implement the following functionalities: 1. **Initialization**: - Initialize a new cell object with an initial value. If no value is provided, the cell should be initialized with `None`. 2. **Get Value**: - Implement a method `get_value` that retrieves the current value stored in the cell. 3. **Set Value**: - Implement a method `set_value` that sets the value of the cell. This should overwrite any existing value. 4. **Check Cell**: - Implement a class method `is_cell` that checks if a given object is an instance of the `Cell` class. Input and Output Formats: - Initialization: `Cell(initial_value)` - Example: `cell = Cell(10)` initializes a cell with value 10. - Method: `cell.get_value()` - Example: `cell.get_value()` returns `10`. - Method: `cell.set_value(new_value)` - Example: `cell.set_value(20)` sets the cell value to 20. - Class Method: `Cell.is_cell(obj)` - Example: `Cell.is_cell(cell)` returns `True`. Constraints: - Ensure the value can be of any type (int, str, list, object, etc.). - The `set_value` method should handle `None` values properly. - The class must handle multiple instances independently. Performance Requirements: - The class methods should operate efficiently with O(1) time complexity where applicable. Example: ```python class Cell: def __init__(self, initial_value=None): self.value = initial_value def get_value(self): return self.value def set_value(self, new_value): self.value = new_value @classmethod def is_cell(cls, obj): return isinstance(obj, cls) # Example usage: cell = Cell(10) print(cell.get_value()) # Output: 10 cell.set_value(20) print(cell.get_value()) # Output: 20 print(Cell.is_cell(cell)) # Output: True ``` Your Task: 1. Implement the `Cell` class with the specified methods. 2. Validate the correctness of your implementation with test cases.","solution":"class Cell: def __init__(self, initial_value=None): self.value = initial_value def get_value(self): return self.value def set_value(self, new_value): self.value = new_value @classmethod def is_cell(cls, obj): return isinstance(obj, cls)"},{"question":"Objective Implement a class that conforms to the `collections.abc.Sequence` interface and use the provided mixin methods effectively. Problem Statement Design a class `DoubleEndedSequence`, which is a custom sequence allowing access to its elements from both ends (like a deque but immutable). The class should inherit from `collections.abc.Sequence`. Requirements: 1. Implement the required methods `__getitem__` and `__len__`. 2. Use mixin methods provided by `Sequence` to support operations like `__iter__`, `__contains__`, etc. 3. Implement a `reverse` method that returns a new `DoubleEndedSequence` object with elements in reverse order. 4. Implement an additional method `append_left` that returns a new `DoubleEndedSequence` object with an element added at the left end. Input and Output Formats The class should support the following methods: - `__init__(self, iterable)` which initializes the sequence. - `__getitem__(self, index)` to retrieve elements by index. - `__len__(self)` to get the number of elements in the sequence. - `reverse(self)` to return a new `DoubleEndedSequence` with elements in reverse order. - `append_left(self, value)` to return a new `DoubleEndedSequence` with `value` added at the left end. ```python from collections.abc import Sequence class DoubleEndedSequence(Sequence): def __init__(self, iterable): Initialize the sequence with the elements from the given iterable. # Your code here def __getitem__(self, index): Return the element at the given index. # Your code here def __len__(self): Return the number of elements in the sequence. # Your code here def reverse(self): Return a new DoubleEndedSequence with elements in reverse order. # Your code here def append_left(self, value): Return a new DoubleEndedSequence with value added at the left end. # Your code here # Example usage: seq = DoubleEndedSequence([1, 2, 3, 4]) print(seq[1]) # Output: 2 print(len(seq)) # Output: 4 reversed_seq = seq.reverse() print(reversed_seq) # Output: DoubleEndedSequence([4, 3, 2, 1]) new_seq = seq.append_left(0) print(new_seq) # Output: DoubleEndedSequence([0, 1, 2, 3, 4]) ``` Constraints: - The `DoubleEndedSequence` should be immutable; changes like `append_left` or `reverse` should return new objects. - The input iterable can be any valid iterable (list, tuple, etc.). Performance Requirements: - The implemented methods should handle typical sequences efficiently. - Ensure that `__getitem__` and `__len__` operations are performed in constant time.","solution":"from collections.abc import Sequence class DoubleEndedSequence(Sequence): def __init__(self, iterable): Initialize the sequence with the elements from the given iterable. self._data = tuple(iterable) def __getitem__(self, index): Return the element at the given index. return self._data[index] def __len__(self): Return the number of elements in the sequence. return len(self._data) def reverse(self): Return a new DoubleEndedSequence with elements in reverse order. return DoubleEndedSequence(self._data[::-1]) def append_left(self, value): Return a new DoubleEndedSequence with value added at the left end. return DoubleEndedSequence((value,) + self._data)"},{"question":"# XML Document Manipulation using xml.dom You are tasked with writing a Python function to manipulate and query an XML document using the `xml.dom` package. Your goal is to parse the XML, perform various operations as specified, and return the results of these operations. Function Specification **Function Name**: `manipulate_xml` **Parameters**: 1. `xml_content` (str): A string containing the XML content to be parsed. **Returns**: - A tuple containing: 1. A list of tag names of all direct children of the root element. 2. A list of textual content inside all `<name>` tags in the document. 3. The entire XML content as a string after adding a new element `<newElem>` under the root element with an attribute `attribute=\\"value\\"` and text content `\\"Hello World\\"`. **Constraints**: 1. The XML string will always have a valid format and contain at least one root element. 2. It may contain nested elements, attributes, text, comments, etc. **Example** ```python xml_content = \'\'\' <root> <child1>Content1</child1> <child2 attribute=\\"value\\">Content2</child2> <name>John Doe</name> <name>Jane Doe</name> </root> \'\'\' result = manipulate_xml(xml_content) ``` **Expected Output**: ```python ( [\'child1\', \'child2\', \'name\', \'name\'], # List of tag names of all direct children of the root element [\'John Doe\', \'Jane Doe\'], # List of textual content inside all <name> tags \'<root><child1>Content1</child1><child2 attribute=\\"value\\">Content2</child2><name>John Doe</name><name>Jane Doe</name><newElem attribute=\\"value\\">Hello World</newElem></root>\' # Modified XML content as string ) ``` **Instructions**: 1. Parse the input XML content using `xml.dom.minidom.parseString`. 2. Retrieve the tag names of all direct children of the root element. 3. Retrieve the text content of all `<name>` tags. 4. Add a new element `<newElem>` to the root with the specified attribute and text content. 5. Return the required information as specified. Implementation Here’s an outline to help you get started: ```python from xml.dom.minidom import parseString def manipulate_xml(xml_content: str): # Parse the XML content dom = parseString(xml_content) # Access the root element root = dom.documentElement # Extract the tag names of all direct children of the root child_tags = [child.tagName for child in root.childNodes if child.nodeType == dom.ELEMENT_NODE] # Extract text content of all <name> tags name_elements = root.getElementsByTagName(\\"name\\") name_texts = [elem.firstChild.nodeValue for elem in name_elements] # Create new element <newElem> and set its attribute and text content new_elem = dom.createElement(\\"newElem\\") new_elem.setAttribute(\\"attribute\\", \\"value\\") new_text = dom.createTextNode(\\"Hello World\\") new_elem.appendChild(new_text) root.appendChild(new_elem) # Convert the XML back to string modified_xml_str = dom.toxml() # Return the results in the required format return (child_tags, name_texts, modified_xml_str) ```","solution":"from xml.dom.minidom import parseString def manipulate_xml(xml_content: str): # Parse the XML content dom = parseString(xml_content) # Access the root element root = dom.documentElement # Extract the tag names of all direct children of the root child_tags = [child.tagName for child in root.childNodes if child.nodeType == dom.ELEMENT_NODE] # Extract text content of all <name> tags name_elements = root.getElementsByTagName(\\"name\\") name_texts = [elem.firstChild.nodeValue for elem in name_elements if elem.firstChild is not None] # Create new element <newElem> and set its attribute and text content new_elem = dom.createElement(\\"newElem\\") new_elem.setAttribute(\\"attribute\\", \\"value\\") new_text = dom.createTextNode(\\"Hello World\\") new_elem.appendChild(new_text) root.appendChild(new_elem) # Convert the XML back to string modified_xml_str = dom.toxml() # Return the results in the required format return (child_tags, name_texts, modified_xml_str)"},{"question":"**Objective**: Implement a function that generates all possible valid bracket pairs of length `2 * n`. **Problem Statement**: A valid bracket pair is a combination of `n` open brackets `(` and `n` close brackets `)` arranged in such a manner that every closing bracket has a previous corresponding unmatched opening bracket. For example, for n = 3, the valid combinations are: ``` [\\"((()))\\", \\"(()())\\", \\"(())()\\", \\"()(())\\", \\"()()()\\"] ``` Given an integer `n`, write a function `generate_bracket_pairs(n: int) -> List[str]` to generate all possible valid bracket pairs of length `2 * n`. # Function Signature: ```python from typing import List def generate_bracket_pairs(n: int) -> List[str]: pass ``` # Constraints: - 1 ≤ n ≤ 8 # Requirements: 1. Use the `itertools` module to assist in generating permutations and combinations. 2. Ensure that the function is efficient and can handle the upper constraint within reasonable time limits. # Example: ```python assert generate_bracket_pairs(3) == [\\"((()))\\", \\"(()())\\", \\"(())()\\", \\"()(())\\", \\"()()()\\"] ``` # Explanation: The function should return a list of all valid combinations of `n` pairs of brackets. Each combination should be a string that represents a valid sequence of opening and closing brackets. To aid in the implementation, consider using the following `itertools` functions: - `product` from the itertools module to create possible combinations. - Filtering techniques to ensure the bracket combinations are valid. **Hints**: 1. Bracket sequences can be validated using a stack-based approach to ensure that each closing bracket matches an unpaired opening bracket. 2. Consider generating all possible sequences and filter out invalid ones based on bracket matching rules.","solution":"from typing import List def generate_bracket_pairs(n: int) -> List[str]: Generates all possible valid bracket pairs of length 2 * n. def backtrack(s = \'\', left = 0, right = 0): if len(s) == 2 * n: result.append(s) return if left < n: backtrack(s + \'(\', left + 1, right) if right < left: backtrack(s + \')\', left, right + 1) result = [] backtrack() return result"},{"question":"You are tasked with creating a Python script that ensures the latest version of `pip` is installed in the desired location based on given user inputs. You need to implement a function `ensure_latest_pip` that will perform the following operations: 1. Check the current installed version of `pip`. 2. If an older version is installed, upgrade it to the latest available version. 3. Allow the user to specify whether to install `pip` in the user directory or globally. 4. Allow the user to specify an alternative root directory for the installation. 5. Provide options to install `pip` as `pipX` and/or `pipX.Y`, where `X.Y` stands for the Python version. 6. Print relevant messages indicating what actions were performed. 7. The function should handle all edge cases and constraints as specified. # Function Signature ```python import ensurepip import subprocess import sys def ensure_latest_pip(root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False): Ensures the latest version of pip is installed. Parameters: - root (str): An alternative root directory to install relative to. Default is None. - upgrade (bool): Whether to upgrade an existing version. Default is False. - user (bool): Whether to install in the user directory. Default is False. - altinstall (bool): Whether to exclude the \'pipX\' script from installation. Default is False. - default_pip (bool): Whether to include the \'pip\' script in the installation. Default is False. Returns: - None: The function prints relevant installation messages. # Your implementation here ``` # Constraints - The function should not directly access the internet. - Ensure appropriate handling of the optional parameters and mutually exclusive options. - Handle and print exceptions where appropriate. - You must use the `ensurepip` module\'s functionality for bootstrapping `pip`. # Example Usage ```python # Upgrading the current pip version globally ensure_latest_pip(upgrade=True) # Installing pip in the user directory ensure_latest_pip(user=True) # Installing pip with a specified root directory ensure_latest_pip(root=\\"/path/to/directory\\", default_pip=True) # Raising an error due to conflicting options ensure_latest_pip(altinstall=True, default_pip=True) ``` # Example Output 1. When upgrading the current pip version globally: ``` Old pip version: 21.0.0 Upgrading pip... New pip version: 21.1.0 ``` 2. When installing pip in the user directory: ``` Installing pip in the user directory... pip installed successfully. ``` 3. When installing pip with a specified root directory: ``` Installing pip with root directory /path/to/directory... pip installed successfully at /path/to/directory/lib/pythonX.Y/site-packages. ``` 4. Raising an error due to conflicting options: ``` ValueError: Cannot specify both \'altinstall\' and \'default_pip\' options. ``` Ensure your implementation meets the requirements and constraints specified. Your solution will be evaluated based on correctness, efficiency, and readability.","solution":"import ensurepip import subprocess import sys import os def ensure_latest_pip(root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False): Ensures the latest version of pip is installed. Parameters: - root (str): An alternative root directory to install relative to. Default is None. - upgrade (bool): Whether to upgrade an existing version. Default is False. - user (bool): Whether to install in the user directory. Default is False. - altinstall (bool): Whether to exclude the \'pipX\' script from installation. Default is False. - default_pip (bool): Whether to include the \'pip\' script in the installation. Default is False. Returns: - None: The function prints relevant installation messages. if altinstall and default_pip: raise ValueError(\\"Cannot specify both \'altinstall\' and \'default_pip\' options.\\") pip_path = [sys.executable, \\"-m\\", \\"pip\\"] if upgrade: # Get the current version of pip current_version = subprocess.run(pip_path + [\\"--version\\"], capture_output=True, text=True) print(\\"Old pip version:\\", current_version.stdout.strip()) # Construct the command for installing or upgrading pip cmd = [sys.executable, \\"-m\\", \\"ensurepip\\"] if upgrade: cmd.append(\\"--upgrade\\") if user: cmd.append(\\"--user\\") if root: cmd.extend([\\"--root\\", root]) if altinstall: cmd.append(\\"--altinstall\\") if default_pip: cmd.append(\\"--default-pip\\") # Run the pip installation command try: subprocess.check_call(cmd) if upgrade: # Get the new version of pip new_version = subprocess.run(pip_path + [\\"--version\\"], capture_output=True, text=True) print(\\"New pip version:\\", new_version.stdout.strip()) else: print(\\"pip installed successfully.\\") if user: print(\\"Installed pip in the user directory.\\") if root: print(f\\"Installed pip with root directory {root}.\\") except subprocess.CalledProcessError as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Objective: Write a function that initializes the weights of a given neural network using a specific initialization method from the `torch.nn.init` module. Your function should take a neural network and an initialization method as inputs and apply the chosen method to all parameters of the network. # Specifications: - **Function Name**: `initialize_weights` - **Input**: - `model`: An instance of a PyTorch neural network (an object of a class inheriting from `torch.nn.Module`). - `init_method`: A string specifying the initialization method, which can be one of the following: - \'uniform\' - \'normal\' - \'constant\' - \'ones\' - \'zeros\' - \'eye\' - \'dirac\' - \'xavier_uniform\' - \'xavier_normal\' - \'kaiming_uniform\' - \'kaiming_normal\' - \'trunc_normal\' - \'orthogonal\' - \'sparse\' - Additional parameters required by some initialization methods as a dictionary (`params`). - **Output**: - The function should modify the weights of the input `model` in-place. - **Constraints**: - Ensure that the initialization respects the input parameters (e.g., mean and standard deviation for normal distribution). - Handle any specific requirements for dimensions (like for orthogonal matrix initialization). # Function Signature: ```python def initialize_weights(model: torch.nn.Module, init_method: str, params: dict = None) -> None: pass ``` # Example Usage: ```python import torch import torch.nn as nn import torch.nn.init as init class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) model = CustomNet() initialize_weights(model, \'xavier_uniform\') # Verify initialized weights print(model.fc1.weight) # This should be initialized using Xavier Uniform method ``` # Notes: - You should validate the `init_method` to ensure it is one of the allowed methods. - If additional parameters are required (e.g., gain for `xavier_uniform_`), ensure they are provided via the `params` dictionary and handle any missing keys appropriately.","solution":"import torch import torch.nn as nn import torch.nn.init as init def initialize_weights(model: nn.Module, init_method: str, params: dict = None) -> None: Initializes weights of a given model using the specified initialization method. Parameters: model (nn.Module): model whose weights need to be initialized init_method (str): initialization method (from a predefined list) params (dict): additional parameters required for certain initialization methods if params is None: params = {} for m in model.modules(): if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.Linear)): if init_method == \'uniform\': init.uniform_(m.weight, **params) elif init_method == \'normal\': init.normal_(m.weight, **params) elif init_method == \'constant\': init.constant_(m.weight, **params) elif init_method == \'ones\': init.ones_(m.weight) elif init_method == \'zeros\': init.zeros_(m.weight) elif init_method == \'eye\': init.eye_(m.weight) elif init_method == \'dirac\': init.dirac_(m.weight) elif init_method == \'xavier_uniform\': init.xavier_uniform_(m.weight, **params) elif init_method == \'xavier_normal\': init.xavier_normal_(m.weight, **params) elif init_method == \'kaiming_uniform\': init.kaiming_uniform_(m.weight, **params) elif init_method == \'kaiming_normal\': init.kaiming_normal_(m.weight, **params) elif init_method == \'trunc_normal\': init.trunc_normal_(m.weight, **params) elif init_method == \'orthogonal\': init.orthogonal_(m.weight, **params) elif init_method == \'sparse\': init.sparse_(m.weight, **params) else: raise ValueError(f\\"Initialization method \'{init_method}\' is not supported.\\") if m.bias is not None: init.constant_(m.bias, 0)"},{"question":"# Question: Working with Sparse Data Structures in pandas You are given a large dataset containing a mix of numeric values and NaNs. For memory efficiency, you decide to store this dataset using pandas Sparse DataFrame. Your task is to perform the following operations: 1. **Load the Dataset**: You will generate a dataset with 10000 rows and 5 columns where approximately 90% of the data is NaN. 2. **Convert to Sparse**: Convert the generated dataset to a pandas Sparse DataFrame. 3. **Calculate the Density**: Calculate the density (percentage of non-NaN values) of the Sparse DataFrame. 4. **Apply a NumPy ufunc**: Apply a NumPy universal function (e.g., `np.abs`) to one of the columns in the Sparse DataFrame. 5. **Convert back to Dense**: Convert the modified Sparse DataFrame back to a dense DataFrame. Implementation 1. **Load the Dataset**: Generate a DataFrame with 10000 rows and 5 columns using `np.random.randn`. Set approximately 90% of the values to NaN. 2. **Convert to Sparse**: Convert the dense DataFrame to a Sparse DataFrame using `pd.SparseDtype(float, np.nan)`. 3. **Calculate the Density**: Calculate the density of the Sparse DataFrame using the `.sparse.density` attribute. 4. **Apply a NumPy ufunc**: Apply the `np.abs` function to the first column of the Sparse DataFrame. 5. **Convert back to Dense**: Convert the modified Sparse DataFrame back to a dense DataFrame using the `.sparse.to_dense()` method. Input: There is no specific input to this function. The values are internally generated. Output: The solution should print the following: 1. Density of the Sparse DataFrame. 2. The first 5 rows of the modified dense DataFrame. Constraints: 1. Ensure that the dataset is large enough to demonstrate memory efficiency. 2. Handle any exceptions that may occur during type conversion or function application. Example Solution: ```python import numpy as np import pandas as pd def sparse_data_operations(): # Step 1: Load the Dataset np.random.seed(0) # For reproducibility data = np.random.randn(10000, 5) mask = np.random.rand(10000, 5) < 0.9 data[mask] = np.nan df = pd.DataFrame(data) # Step 2: Convert to Sparse sdf = df.astype(pd.SparseDtype(\'float\', np.nan)) # Step 3: Calculate the Density density = sdf.sparse.density print(f\'Density: {density:.4f}\') # Step 4: Apply a NumPy ufunc sdf[\'modified_col\'] = np.abs(sdf.iloc[:, 0]) # Step 5: Convert back to Dense dense_df = sdf.sparse.to_dense() print(dense_df.head()) # Run the example solution sparse_data_operations() ``` Complete the function `sparse_data_operations` to perform the specified operations.","solution":"import numpy as np import pandas as pd def sparse_data_operations(): # Step 1: Load the Dataset np.random.seed(0) # For reproducibility data = np.random.randn(10000, 5) mask = np.random.rand(10000, 5) < 0.9 data[mask] = np.nan df = pd.DataFrame(data) # Step 2: Convert to Sparse sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Step 3: Calculate the Density density = sdf.sparse.density print(f\'Density: {density:.4f}\') # Step 4: Apply a NumPy ufunc sdf[\'modified_col\'] = np.abs(sdf.iloc[:, 0]) # Step 5: Convert back to Dense dense_df = sdf.sparse.to_dense() print(dense_df.head()) # Run the example solution sparse_data_operations()"},{"question":"**Objective:** Implement an efficient scaled dot-product attention mechanism using PyTorch. This task assesses your understanding of the fundamental concepts of attention mechanisms and your ability to utilize PyTorch to implement them. **Background:** Scaled dot-product attention is a crucial component in many attention-based models, including the Transformer architecture. The attention mechanism receives queries, keys, and values matrices, and computes the weighted sum of the values, where the weights are determined by the dot products of the queries and keys. **Function Signature:** ```python def scaled_dot_product_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor: Computes the scaled dot-product attention. Parameters: q (torch.Tensor): Queries of shape (batch_size, num_heads, seq_length, depth) k (torch.Tensor): Keys of shape (batch_size, num_heads, seq_length, depth) v (torch.Tensor): Values of shape (batch_size, num_heads, seq_length, depth) mask (torch.Tensor, optional): Masking tensor to avoid attending to certain positions, shape (batch_size, 1, seq_length, seq_length) Returns: torch.Tensor: Output attended values of shape (batch_size, num_heads, seq_length, depth) ``` **Constraints and Requirements:** - The implementation must use PyTorch for tensor operations. - The attention scores should be scaled by the square root of the depth of the queries/keys. - If `mask` is provided, certain positions should not be attended to by assigning a large negative value to the corresponding positions before applying the softmax. - Use softmax to normalize the attention scores along the last axis. - The function should be efficient and handle typical tensor sizes encountered in practice (e.g., batch sizes up to 64, sequence lengths up to 512, and number of heads up to 8). **Example:** ```python import torch batch_size = 2 num_heads = 4 seq_length = 3 depth = 8 q = torch.randn(batch_size, num_heads, seq_length, depth) k = torch.randn(batch_size, num_heads, seq_length, depth) v = torch.randn(batch_size, num_heads, seq_length, depth) mask = torch.ones(batch_size, 1, seq_length, seq_length) output = scaled_dot_product_attention(q, k, v, mask) print(output.shape) # Expected output shape: (2, 4, 3, 8) ``` The above function should implement the scaled dot-product attention mechanism and return the attended output tensor. **Notes:** - Ensure that your implementation is well-documented and includes comments where necessary to explain your approach. - Consider edge cases such as handling different input shapes and the presence/absence of the mask tensor.","solution":"import torch def scaled_dot_product_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor: Computes the scaled dot-product attention. Parameters: q (torch.Tensor): Queries of shape (batch_size, num_heads, seq_length, depth) k (torch.Tensor): Keys of shape (batch_size, num_heads, seq_length, depth) v (torch.Tensor): Values of shape (batch_size, num_heads, seq_length, depth) mask (torch.Tensor, optional): Masking tensor to avoid attending to certain positions, shape (batch_size, 1, seq_length, seq_length) Returns: torch.Tensor: Output attended values of shape (batch_size, num_heads, seq_length, depth) depth = q.size(-1) scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(depth, dtype=torch.float32)) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) attention_weights = torch.nn.functional.softmax(scores, dim=-1) output = torch.matmul(attention_weights, v) return output"},{"question":"**Unix User Information Analysis** You are tasked with writing a Python function that utilizes the `pwd` module to analyze Unix user information. Specifically, you need to create a function that can: 1. Retrieve the password database entry for a given username. 2. Extract and return specific user details from the retrieved entry. 3. Summarize the number of users that use each unique command interpreter (shell). Here are the expected functionalities: # Function 1: `get_user_info(username)` This function retrieves and returns specific information about a user. **Input:** - `username` (str): The username for which to retrieve information. **Output:** - A dictionary with the following structure: ```python { \\"username\\": <username>, \\"uid\\": <numeric user ID>, \\"gid\\": <numeric group ID>, \\"home_directory\\": <home directory>, \\"shell\\": <command interpreter> } ``` **Constraints:** - If the user does not exist, raise a `KeyError`. # Function 2: `summarize_shell_usage()` This function returns a summary of the number of users that use each unique command interpreter (shell). **Output:** - A dictionary where the keys are unique shells (str) and the values are the number of users (int) using each shell. **Example:** ```python { \\"/bin/bash\\": 5, \\"/bin/sh\\": 3, \\"/usr/bin/zsh\\": 2 } ``` **Performance Requirements:** - Efficiency is important as there could be a large number of user entries. # Example Usage ```python # Example usage for get_user_info user_info = get_user_info(\\"john\\") print(user_info) # Output: {\\"username\\": \\"john\\", \\"uid\\": 1001, \\"gid\\": 1001, \\"home_directory\\": \\"/home/john\\", \\"shell\\": \\"/bin/bash\\"} # Example usage for summarize_shell_usage shell_summary = summarize_shell_usage() print(shell_summary) # Output: {\\"/bin/bash\\": 5, \\"/bin/sh\\": 3, \\"/usr/bin/zsh\\": 2} ``` # Notes: - Utilize the `pwd` module functions such as `pwd.getpwnam` and `pwd.getpwall`. - Handle cases where users may not have specific entries (like shell information) appropriately. All the necessary information for solving this problem is provided; ensure that your solution is efficient and adheres to the input/output specifications.","solution":"import pwd def get_user_info(username): Retrieve and return specific information about a user. Parameters: username (str): The username for which to retrieve information. Returns: dict: A dictionary containing user information. Raises: KeyError: If the user does not exist. try: pw_entry = pwd.getpwnam(username) return { \\"username\\": pw_entry.pw_name, \\"uid\\": pw_entry.pw_uid, \\"gid\\": pw_entry.pw_gid, \\"home_directory\\": pw_entry.pw_dir, \\"shell\\": pw_entry.pw_shell } except KeyError: raise KeyError(f\\"The user \'{username}\' does not exist.\\") def summarize_shell_usage(): Summarize the number of users that use each unique command interpreter (shell). Returns: dict: A dictionary where the keys are unique shells and the values are the number of users using each shell. users = pwd.getpwall() shell_summary = {} for user in users: shell = user.pw_shell if shell in shell_summary: shell_summary[shell] += 1 else: shell_summary[shell] = 1 return shell_summary"},{"question":"# Mailbox Management Challenge You are tasked with creating a script to manage email messages across two common mailbox formats: Maildir and mbox. Specifically, you will write a function to perform the following: 1. Extract all unread messages from a `Maildir` mailbox. 2. Copy these unread messages into an `mbox` mailbox. 3. Mark the copied messages as read in the `mbox` mailbox. 4. Remove the unread messages from the `Maildir` mailbox. Your solution should handle exceptions gracefully and avoid race conditions by appropriately locking and unlocking the mailboxes. Function Signature ```python def sync_unread_mail(maildir_path: str, mbox_path: str): pass ``` Input - `maildir_path` (str): Path to the Maildir mailbox directory. - `mbox_path` (str): Path to the mbox mailbox file. Output - None: The function has no return value. It modifies the mailboxes in place. Example ```python # Assuming there are unread messages in \'~/Maildir\' and \'~/mbox\' is the mbox mailbox file. sync_unread_mail(\'~/Maildir\', \'~/mbox\') # Upon completion, unread messages are moved from Maildir to mbox and marked as read in mbox. ``` Constraints and Requirements - **Concurrency Safety**: Ensure Maildir is not corrupted by concurrent access. - **Exception Handling**: Ignore and log errors encountered when processing individual mails. - **Performance**: Ensure the function runs efficiently on mailboxes with potentially large numbers of messages. - **Library Usage**: Use the `mailbox` module\'s `Maildir` and `mbox` classes. Notes 1. **Unread Messages**: In `Maildir`, unread messages reside in the \'new\' subdirectory. 2. **Marking Messages as Read**: For Maildir messages moved to mbox, instantiate `mboxMessage` and set the \'R\' flag. 3. **Logging**: Implement basic logging within the function to notify when messages are skipped due to errors. Hints Consider iterating over Maildir\'s \'new\' directory and carefully handling the Maildir and mbox methods to ensure safety and correctness.","solution":"import mailbox import os import logging def sync_unread_mail(maildir_path: str, mbox_path: str): logging.basicConfig(level=logging.DEBUG, format=\'%(asctime)s - %(levelname)s - %(message)s\') # Initialize mailboxes maildir = mailbox.Maildir(maildir_path, factory=None) mbox = mailbox.mbox(mbox_path) mbox.lock() try: new_dir = os.path.join(maildir_path, \\"new\\") if not os.path.exists(new_dir): logging.error(f\\"Maildir\'s \'new\' directory does not exist at path: {new_dir}\\") return for filename in os.listdir(new_dir): filepath = os.path.join(new_dir, filename) if os.path.isfile(filepath): try: # Read message from Maildir with open(filepath, \'rb\') as f: msg = mailbox.mboxMessage(f.read()) # Append message to mbox and mark as read mbox.add(msg) mbox.flush() # Remove the message from Maildir os.remove(filepath) logging.info(f\\"Processed and removed {filename}\\") except Exception as e: logging.error(f\\"Error processing {filename}: {e}\\") finally: mbox.close() maildir.close()"},{"question":"# Python C API Tuples and Struct Sequences Manipulation Background: You are working with Python\'s C API and need to handle custom data structures efficiently. Specifically, you will manipulate tuple-like data structures by implementing specific functionalities in Python. Task: Implement a Python function that creates and manipulates a tuple and a custom struct sequence. Your function should: 1. Create a new tuple of a specified size and initialize it with given elements. 2. Implement a function to resize the tuple and add new elements to it. 3. Develop a custom struct sequence similar to namedtuple in Python, with specified field names and values, and ensure it can be accessed both via indexing and attribute names. Function Specifications: 1. **Function Name**: `create_and_resize_tuple` - **Input**: - `initial_elements` (list): A list of initial elements to be included in the tuple. - `extra_elements` (list): A list of extra elements to be added during resizing. - **Output**: - A tuple containing all the elements from both `initial_elements` and `extra_elements`. 2. **Function Name**: `create_custom_struct_sequence` - **Input**: - `field_names` (list): A list of strings representing the names of the fields for the struct sequence. - `values` (list): A list of values corresponding to each field. - **Output**: - An object representing the struct sequence, where fields can be accessed via dot notation (e.g., `obj.field_name`) or via indexing (e.g., `obj[index]`). Constraints: - Use the provided functions and types from Python\'s C API (e.g., `PyTuple_New`, `PyTuple_SetItem`, `PyStructSequence_NewType`, etc.). - Consider edge cases such as empty input lists or mismatched field names and values. - Ensure type safety and proper error handling. Example: ```python # Example Usage initial_elements = [1, 2, 3] extra_elements = [4, 5] result_tuple = create_and_resize_tuple(initial_elements, extra_elements) assert result_tuple == (1, 2, 3, 4, 5) field_names = [\'name\', \'age\', \'occupation\'] values = [\'Alice\', 30, \'Engineer\'] custom_seq = create_custom_struct_sequence(field_names, values) assert custom_seq.name == \'Alice\' assert custom_seq[1] == 30 ``` In your implementation, you are expected to demonstrate a clear understanding of Python\'s tuple and struct sequence manipulations using the provided C API functions.","solution":"import collections def create_and_resize_tuple(initial_elements, extra_elements): Create a new tuple with initial_elements and then resize it to add extra_elements. Args: initial_elements (list): List of initial elements for the tuple. extra_elements (list): List of extra elements to add to the tuple. Returns: tuple: A tuple containing all elements in initial_elements followed by extra_elements. combined_elements = initial_elements + extra_elements return tuple(combined_elements) def create_custom_struct_sequence(field_names, values): Create a custom struct sequence similar to namedtuple. Args: field_names (list): List of field name strings. values (list): List of values corresponding to the field names. Returns: namedtuple: A namedtuple-like object with the fields and values provided. if len(field_names) != len(values): raise ValueError(\\"Length of field_names and values must match\\") CustomStruct = collections.namedtuple(\'CustomStruct\', field_names) return CustomStruct(*values)"},{"question":"Objective Implement a function that generates a customized swarm plot using the seaborn library. Problem Statement You are provided with a dataset containing information about customer transactions at a restaurant. Your task is to create a swarm plot that visualizes the distribution of `total_bill`, split by `day`, and customized by `time` and `size`. Function Signature ```python def customized_swarmplot(data_path: str) -> None: pass ``` Input - `data_path` (str): The file path to a CSV file containing the dataset with at least the following columns: `total_bill`, `day`, `time`, and `size`. Requirements 1. The plot should be a swarm plot with: - `x` axis representing `total_bill`. - `y` axis representing `day`. - Different colors for different times (`hue=\\"time\\"`). - Different sizes of markers for different table sizes (`size=\\"size\\"`). - The size of the markers should be scaled down to avoid overlap (`size=3`). 2. The plot should: - Use `native_scale=True` to retain the original scale of the variables. - Set `dodge=True` to separate different levels of the `hue` variable. 3. Save the plot as a PNG file named `\\"custom_swarm_plot.png\\"`. Constraints - Ensure the plot is clear and comprehensive. - Use appropriate seaborn and matplotlib function parameters to achieve the required customizations. Testing You do not need to implement any input/output handling. However, your function should be designed to handle a realistic dataset structure and format. Sample datasets such as the `tips` dataset in seaborn can be used for testing. Example Dataset Format ```csv total_bill,day,time,size 16.99,Sun,Dinner,2 10.34,Sun,Dinner,3 21.01,Sun,Dinner,3 23.68,Sun,Dinner,2 24.59,Sun,Dinner,4 ``` Example Function Call ```python customized_swarmplot(\\"path/to/your/data.csv\\") ``` Output The function should create and save the customized swarm plot as `\\"custom_swarm_plot.png\\"` without returning any value.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def customized_swarmplot(data_path: str) -> None: # Load the dataset from the provided file path data = pd.read_csv(data_path) # Create the swarm plot with the required customizations plt.figure(figsize=(10, 6)) sns.swarmplot( x=\'total_bill\', y=\'day\', hue=\'time\', size=\'size\', data=data, native_scale=True, dodge=True, marker=\'o\', s=3 ) # Save the plot as a PNG file plt.savefig(\\"custom_swarm_plot.png\\") plt.close()"},{"question":"**Problem Description:** You are provided with the `seaborn` library documentation and a `tips` dataset. The dataset contains the following columns: - `total_bill`: The total bill amount. - `tip`: The tip amount. - `sex`: The gender of the person paying the bill. - `smoker`: Whether the person is a smoker. - `day`: The day of the week. - `time`: The time of day (Dinner or Lunch). - `size`: The size of the party. Write a function `plot_day_sex_counts` that achieves the following tasks: 1. Create a bar plot that shows the count of observations for each day of the week. 2. Use the `sex` variable for coloring the bars. 3. Use dodging to separate the bars based on the `sex` variable. **Function Signature:** ```python def plot_day_sex_counts(): pass ``` **Expected Output:** - The function should generate and display a bar plot as described. **Constraints:** 1. Use the `seaborn.objects.Plot` and `seaborn.objects.Count` functions. 2. The plot should be grouped by the `day` variable and differentiated by the `sex` variable using color. 3. The plot should use dodging to show separate bars for different sexes side-by-side for each day. **Example**: ```python plot_day_sex_counts() ``` - This should display a bar plot where: - The `x-axis` represents the days of the week. - Each day has two bars representing the count of observations for each sex (`Male` and `Female`), differentiated by color and displayed side-by-side.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_day_sex_counts(): # Load the tips dataset from seaborn tips = sns.load_dataset(\\"tips\\") # Create the bar plot using seaborn objects interface plot = sns.catplot( data=tips, kind=\'count\', x=\'day\', hue=\'sex\', palette=\'coolwarm\', dodge=True ) # Set the title and labels plot.set_axis_labels(\\"Day of the Week\\", \\"Count of Observations\\") plot.set_titles(\\"Count of Observations by Day and Sex\\") # Show the plot plt.show()"},{"question":"# Question You are provided with the \\"titanic\\" dataset from seaborn. Your task is to perform the following operations using seaborn\'s boxplot function: 1. **Load the Titanic Dataset**: Load the Titanic dataset from seaborn\'s built-in datasets. 2. **Create a Basic Boxplot**: Draw a single horizontal boxplot to visualize the distribution of passenger ages. 3. **Boxplot with Grouping**: Draw a vertical boxplot to visualize the distribution of ages within each passenger class. 4. **Nested Grouping Boxplot**: Draw a vertical boxplot with nested grouping to visualize the distribution of ages within each passenger class, further grouped by the survival status (indicated by the \'alive\' column). 5. **Customize Boxplot Appearance**: Customize the nested grouped boxplot by: - Using line art for the boxes instead of filled color. - Adding a small gap (0.1) between the boxes. 6. **Display the Full Range of Data**: For the nested grouped boxplot, modify the whiskers to cover the full range of age data (0 to 100th percentile). 7. **Adjust Boxplot Width**: Make all the boxes narrower with a width setting of 0.5 for better readability. 8. **Advanced Customization**: Further customize the plot using parameters of seaborn and matplotlib as follows: - Enable notched boxes. - Hide the caps. - Use \'x\' markers for outliers. - Set the box face color to RGBA (0.3, 0.5, 0.7, 0.5). - Set the median line color to red with a linewidth of 2. # Expected Input and Output **Input**: - No explicit input is needed, as you will be using the seaborn `titanic` dataset. **Output**: - Several boxplot visualizations as specified above. # Constraints: - You must use seaborn for creating the plots. - Follow the customization instructions precisely for the boxplots. - Ensure the plots are clear and correctly labeled. # Example ```python import seaborn as sns # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Single Horizontal Boxplot sns.boxplot(x=titanic[\\"age\\"]).set(title=\\"Distribution of Passenger Ages\\") # 2. Vertical Boxplot with Grouping sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\").set(title=\\"Age Distribution by Passenger Class\\") # 3. Vertical Boxplot with Nested Grouping by Survival Status sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\").set(title=\\"Age Distribution by Passenger Class and Survival Status\\") # 4. Customize Boxplot Appearance sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", fill=False, gap=0.1).set(title=\\"Customized Age Distribution by Class and Status\\") # 5. Modify Whiskers to Cover Full Range sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", whis=(0, 100), fill=False, gap=0.1).set(title=\\"Full Range Age Distribution by Class and Status\\") # 6. Adjust Boxplot Width sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", whis=(0, 100), fill=False, gap=0.1, width=0.5).set(title=\\"Narrowed Age Distribution by Class and Status\\") # 7. Advanced Customization sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", whis=(0, 100), notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (0.3, 0.5, 0.7, 0.5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2}, fill=False, gap=0.1, width=0.5 ).set(title=\\"Advanced Customized Age Distribution by Class and Status\\") ``` Note: Ensure all the required visualization libraries are imported and the code is well-commented for readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_titanic_dataset(): Load Titanic dataset from seaborn. return sns.load_dataset(\\"titanic\\") def single_horizontal_boxplot(titanic): Draw a single horizontal boxplot to visualize the distribution of passenger ages. plt.figure() sns.boxplot(x=titanic[\\"age\\"]).set(title=\\"Distribution of Passenger Ages\\") plt.show() def vertical_boxplot_grouping(titanic): Draw a vertical boxplot to visualize the distribution of ages within each passenger class. plt.figure() sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\").set(title=\\"Age Distribution by Passenger Class\\") plt.show() def vertical_boxplot_nested_grouping(titanic): Draw a vertical boxplot with nested grouping to visualize the distribution of ages within each passenger class, further grouped by the survival status. plt.figure() sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\").set(title=\\"Age Distribution by Passenger Class and Survival Status\\") plt.show() def customize_boxplot_appearance(titanic): Customize the nested grouped boxplot using line art for the boxes and adding a small gap between the boxes. plt.figure() sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", notch=False, palette=\\"Set1\\", whis=(0, 100), linewidth=1.5, width=0.5).set(title=\\"Customized Age Distribution by Class and Status\\") plt.show() def modify_whiskers_full_range(titanic): Modify the whiskers to cover the full range of age data (0 to 100th percentile) for the nested grouped boxplot. plt.figure() sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", whis=(0, 100)).set(title=\\"Full Range Age Distribution by Class and Status\\") plt.show() def adjust_boxplot_width(titanic): Make all the boxes narrower with a width setting of 0.5 for better readability in the nested group boxplot. plt.figure() sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", whis=(0, 100), width=0.5).set(title=\\"Narrowed Age Distribution by Class and Status\\") plt.show() def advanced_customization_boxplot(titanic): Further customize the nested grouped boxplot with notched boxes, hidden caps, \'x\' markers for outliers, RGBA colored boxes, and red median lines. plt.figure() sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", whis=(0, 100), notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (0.3, 0.5, 0.7, 0.5)}, medianprops={\\"color\\": \\"red\\", \\"linewidth\\": 2}, width=0.5 ).set(title=\\"Advanced Customized Age Distribution by Class and Status\\") plt.show()"},{"question":"**Question: Implementing Efficient File Locking and Control** **Objective:** Write a Python function that demonstrates a comprehensive understanding of the `fcntl` module by performing file control and locking operations on a given file. **Function Signature:** ```python def control_and_lock_file(file_path: str, cmd: int, arg: int = 0, lock_cmds: list = []) -> dict: pass ``` **Input:** - `file_path` (str): The path to the file to be controlled and locked. - `cmd` (int): The file control command (one of the constants defined in the `fcntl` module). - `arg` (int, optional): The argument for the `fcntl` call. Defaults to `0`. - `lock_cmds` (list, optional): A list of tuples where each tuple contains: - `lock_cmd` (int): The lock command (one of `fcntl.LOCK_EX`, `fcntl.LOCK_SH`, `fcntl.LOCK_UN`, optionally combined with `fcntl.LOCK_NB`). - `len` (int): The number of bytes to lock. Defaults to `0` for the entire file. - `start` (int): The byte offset at which the lock starts. Defaults to `0`. - `whence` (int): As in `io.IOBase.seek()` with defaults to `os.SEEK_SET` (0). **Output:** - A dictionary with keys: - `fcntl_result`: The result of the `fcntl` operation. - `lock_results`: A list of results for each lock operation in `lock_cmds`. - If an error occurs, raise an `OSError` with an appropriate message. **Requirements:** 1. Open the file specified by `file_path` and obtain its file descriptor. 2. Perform the `fcntl` operation using `cmd` and `arg`, and store the result. 3. For each lock command in `lock_cmds`, perform the lock operation and store the result. 4. Handle any exceptions that occur, and raise `OSError` with a message specifying the error. 5. Ensure the file is closed properly after operations. **Example Usage:** ```python import fcntl import os # Example file control command cmd = fcntl.F_SETFL arg = os.O_NDELAY # Example lock commands. First command acquires an exclusive lock without blocking, second unlocks. lock_cmds = [(fcntl.LOCK_EX | fcntl.LOCK_NB, 0, 0, 0), (fcntl.LOCK_UN, 0, 0, 0)] # Function call result = control_and_lock_file(\'/path/to/file\', cmd, arg, lock_cmds) print(result) ``` **Constraints:** - The function should be compatible with Python 3.10 and the `fcntl` module. - Make sure the file descriptor is valid and properly handled across all operations. - Account for any system-specific nuances in the `fcntl` and `lockf` operations. **Performance Considerations:** - Ensure the file operations are efficient, particularly with respect to file locking. - Properly handle large files and potential limits on buffer sizes for `fcntl` operations. - Maintain robust error handling for systemic calls and edge cases. Good luck!","solution":"import fcntl import os def control_and_lock_file(file_path: str, cmd: int, arg: int = 0, lock_cmds: list = []) -> dict: try: results = { \'fcntl_result\': None, \'lock_results\': [] } with open(file_path, \'r+\') as file: fd = file.fileno() # Perform fcntl operation results[\'fcntl_result\'] = fcntl.fcntl(fd, cmd, arg) # Perform lock operations for lock_cmd, length, start, whence in lock_cmds: fcntl.lockf(fd, lock_cmd, length, start, whence) results[\'lock_results\'].append(True) return results except OSError as e: raise OSError(f\\"File control or locking error: {e}\\") # Function call example: # result = control_and_lock_file(\'/path/to/file\', fcntl.F_SETFL, os.O_NDELAY, [(fcntl.LOCK_EX | fcntl.LOCK_NB, 0, 0, 0), (fcntl.LOCK_UN, 0, 0, 0)]) # print(result)"},{"question":"**Objective**: Demonstrate understanding of the `pickletools` module by creating a program that analyzes and optimizes a given pickle file. Question You are given a pickle file, `data.pickle`, which contains serialized Python objects. Your task is to write a Python program that: 1. **Disassembles** the pickle file and prints the symbolic disassembly to the console. 2. **Iterates** over all the opcodes in the pickle file and prints each opcode along with its argument and position. 3. **Optimizes** the pickle file by removing unused \\"PUT\\" opcodes and writes the optimized pickle string to a new file named `optimized_data.pickle`. Here are the detailed steps you need to follow: 1. **Disassemble the Pickle File**: - Use the `pickletools.dis` function to print the symbolic disassembly of `data.pickle` to the console. 2. **Iterate Over Opcodes**: - Use the `pickletools.genops` function to iterate over all opcodes in the pickle file. - For each opcode, print a line formatted as follows: `Opcode: <opcode_name>, Argument: <arg>, Position: <pos>`. 3. **Optimize and Save Pickle File**: - Use the `pickletools.optimize` function to optimize the pickle string from `data.pickle`. - Save the optimized pickle string to a new file called `optimized_data.pickle`. Constraints - Assume the `data.pickle` file is in the same directory as your script. - You must handle file reading and writing operations. Input and Output Formats - **Input**: `data.pickle` file containing serialized data. - **Output**: Print disassembled data to console, print opcodes to console, and save the optimized pickle string to `optimized_data.pickle`. Example ```python import pickletools import pickle # Read the original pickle file with open(\'data.pickle\', \'rb\') as f: pickle_data = f.read() # 1. Disassemble the Pickle File print(\\"Disassembly of the pickle file:\\") pickletools.dis(pickle_data) # 2. Iterate Over Opcodes print(\\"nOpcodes in the pickle file:\\") for opcode, arg, pos in pickletools.genops(pickle_data): print(f\'Opcode: {opcode.name}, Argument: {arg}, Position: {pos}\') # 3. Optimize the Pickle File optimized_pickle_data = pickletools.optimize(pickle_data) # Save the optimized pickle to a new file with open(\'optimized_data.pickle\', \'wb\') as f: f.write(optimized_pickle_data) print(\\"nOptimized pickle file saved as \'optimized_data.pickle\'\\") ``` **Note**: This example code outline serves as a guide for your implementation. The focus of this assignment is not just on the final output but on understanding and correctly using functions from the `pickletools` module.","solution":"import pickletools def disassemble_pickle(input_file): Disassembles the pickle file and prints the symbolic disassembly to the console. with open(input_file, \'rb\') as f: pickle_data = f.read() pickletools.dis(pickle_data) def iterate_opcodes(input_file): Iterates over all opcodes in the pickle file and prints each opcode along with its argument and position. with open(input_file, \'rb\') as f: pickle_data = f.read() for opcode, arg, pos in pickletools.genops(pickle_data): print(f\'Opcode: {opcode.name}, Argument: {arg}, Position: {pos}\') def optimize_pickle(input_file, output_file): Optimizes the pickle file by removing unused PUT opcodes and writes the optimized pickle string to a new file. with open(input_file, \'rb\') as f: pickle_data = f.read() optimized_pickle_data = pickletools.optimize(pickle_data) with open(output_file, \'wb\') as f: f.write(optimized_pickle_data)"},{"question":"# Objective: Write a function that performs the following tasks: 1. Generates a synthetic classification dataset with 1000 samples and 20 features. 2. Splits the dataset into training and testing sets. 3. Applies standard scaling to the features. 4. Trains a GradientBoostingClassifier on the training data. 5. Calculates and returns the accuracy score on the testing data. # Function Signature: ```python def evaluate_gradient_boosting_classifier(random_state: int = 42) -> float: pass ``` # Input: - `random_state`: An integer which will be used as the random seed for data generation and splitting. # Output: - A float representing the accuracy score of the classifier on the test data. # Constraints: - Use `train_test_split` for splitting the dataset. - Use `StandardScaler` for feature scaling. - Use `GradientBoostingClassifier` for training. # Example: ```python accuracy = evaluate_gradient_boosting_classifier(random_state=42) print(accuracy) # Expected output: A float representing the accuracy score ``` # Note: Make sure your function is self-contained and uses synthetic data generation provided by scikit-learn\'s `make_classification` method.","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score def evaluate_gradient_boosting_classifier(random_state: int = 42) -> float: # Generate synthetic classification dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=random_state) # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state) # Apply standard scaling to the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train GradientBoostingClassifier on training data classifier = GradientBoostingClassifier(random_state=random_state) classifier.fit(X_train_scaled, y_train) # Calculate and return the accuracy score on the testing data y_pred = classifier.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Objective:** Assess the understanding of Python\'s `random` module and the ability to implement custom random number generators. Problem Statement You are required to implement a custom random number generator that uses a different approach than the default Mersenne Twister algorithm. Specifically, you will implement the `XORShift` algorithm, a simple alternative pseudo-random number generator. You will then use this custom generator to create random samples of uniformly distributed floating-point numbers and integers. Requirements 1. **Custom Random Number Generator:** - Implement a class `XORShiftRandom` that inherits from `random.Random`. - Override the `random()` method to generate random floating-point numbers. - Override the `seed(a)` method to initialize the generator state. 2. **Random Samples:** - Using your `XORShiftRandom` class, generate 5 random floating-point numbers in the [0.0, 1.0) range. - Generate 5 random integers between 1 and 100. Details - The `XORShift` algorithm uses a simple linear shift and XOR operations to generate random numbers. - You should also override `getstate` and `setstate` methods to save and restore the generator\'s state. - Ensure reproducibility by using a fixed seed value. Input and Output - **Input:** No input from the user. - **Output:** Print the lists of random floating-point numbers and random integers. Constraints - Use the following parameters for the `XORShift` algorithm: - Initial seed value: `123456789` - Constants for shift and XOR: `a=21`, `b=35`, `c=4` Performance Requirements - Ensure that your custom random number generator is efficient and consistent in generating pseudo-random values. Example: The output should look something like this (actual numbers will vary): ```python Random floats: [0.897, 0.625, 0.452, 0.879, 0.290] Random integers: [34, 72, 5, 99, 23] ``` # Implementation ```python import random class XORShiftRandom(random.Random): def __init__(self, seed=None): self.seed(seed) def seed(self, a=None): self.state = 123456789 if a is None else a def random(self): self.state ^= self.state << 21 self.state ^= self.state >> 35 self.state ^= self.state << 4 return (self.state % (2**53)) / (2**53) def getstate(self): return self.state def setstate(self, state): self.state = state # Instantiate custom random number generator xorshift_rng = XORShiftRandom() # Generate random floating-point numbers random_floats = [xorshift_rng.random() for _ in range(5)] print(\\"Random floats:\\", random_floats) # Generate random integers between 1 and 100 random_ints = [int(xorshift_rng.random() * 100) + 1 for _ in range(5)] print(\\"Random integers:\\", random_ints) ``` Notes: - The implementation of `XORShiftRandom` should be efficient in terms of time complexity (constant time for generating numbers). - The override of `random()`, `seed()`, `getstate()`, and `setstate()` methods ensures compliance with the design requirements of the `random` module. - This question tests the understanding of creating custom random number generators, managing their state, and using them to generate specific random values.","solution":"import random class XORShiftRandom(random.Random): def __init__(self, seed=None): self.seed(seed) def seed(self, a=None): self.state = 123456789 if a is None else a def random(self): self.state ^= (self.state << 21) & ((1 << 64) - 1) # mask to ensure 64-bit wraparound self.state ^= self.state >> 35 self.state ^= (self.state << 4) & ((1 << 64) - 1) # Returning a float in range [0.0, 1.0) return (self.state % (2**53)) / (2**53) def getstate(self): return self.state def setstate(self, state): self.state = state # Instantiate custom random number generator xorshift_rng = XORShiftRandom() # Generate random floating-point numbers random_floats = [xorshift_rng.random() for _ in range(5)] print(\\"Random floats:\\", random_floats) # Generate random integers between 1 and 100 random_ints = [int(xorshift_rng.random() * 100) + 1 for _ in range(5)] print(\\"Random integers:\\", random_ints)"},{"question":"Coding Assessment Question # Objective: Create a minimal reproducible machine learning pipeline using scikit-learn that involves preprocessing, training a model, and evaluating its performance. This will assess your understanding of scikit-learn\'s functionalities and the ability to troubleshoot common issues. # Problem Statement: You are provided with a synthetic dataset for a regression problem. Your task is to build a machine learning pipeline that includes data preprocessing, model training, and evaluation. The steps are as follows: 1. **Data Generation**: - Generate a synthetic regression dataset with `n_samples=1000` and `n_features=20` using scikit-learn\'s `make_regression` function. 2. **Data Preprocessing**: - Perform standard scaling on the dataset using `StandardScaler` from scikit-learn. 3. **Model Training**: - Use a `GradientBoostingRegressor` to train the model on the preprocessed data. - Set `random_state=42` and `n_iter_no_change=5` for the regressor. 4. **Evaluation**: - Compute and print the model\'s performance score using `R^2` (coefficient of determination). 5. **Troubleshooting**: - Identify and fix any warning or error that might occur during the pipeline execution. # Requirements: - Use scikit-learn for all machine learning tasks. - The synthetic dataset should be entirely generated within your code. - Ensure the code is clean, well-documented, and follows good practices as described in the provided documentation. # Constraints: - Do not use any external datasets. The synthetic data should be generated within your code. - Follow the provided example structure for generating synthetic data and building the scikit-learn pipeline. - Ensure that the model training and evaluation are completed within a reasonable time frame. # Expected Input and Output Formats: - **Input**: None. The synthetic data should be generated within the code. - **Output**: Print the `R^2` score of the model on the synthetic dataset. # Example Code Structure: ```python import numpy as np from sklearn.datasets import make_regression from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Step 2: Data Preprocessing scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Model Training gbr = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) gbr.fit(X_scaled, y) # Step 4: Evaluation y_pred = gbr.predict(X_scaled) score = r2_score(y, y_pred) print(f\\"R^2 Score: {score}\\") # Step 5: Troubleshooting # Ensure no warnings or errors and discuss potential solutions to any issues encountered. ``` # Submission: Submit a Python script or a Jupyter notebook containing the complete code for the pipeline. Make sure the script runs without any errors and the performance score is printed out correctly.","solution":"import numpy as np from sklearn.datasets import make_regression from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score def ml_pipeline(): Create a minimal reproducible machine learning pipeline using scikit-learn that involves preprocessing, training a model, and evaluating its performance. Returns the R^2 score of the trained model. # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Step 2: Data Preprocessing scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Model Training gbr = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) gbr.fit(X_scaled, y) # Step 4: Evaluation y_pred = gbr.predict(X_scaled) score = r2_score(y, y_pred) print(f\\"R^2 Score: {score}\\") return score if __name__ == \\"__main__\\": ml_pipeline()"},{"question":"**Objective:** Implement a function `prepare_internet_identifier` that takes a Unicode string and prepares it according to a simplified profile of the `stringprep` module. Your implementation should utilize multiple `stringprep` functions. **Function Signature:** ```python def prepare_internet_identifier(input_string: str) -> str: ``` **Description:** Process the `input_string` by performing the following steps: 1. **Remove commonly mapped characters**: Use `stringprep.in_table_b1` to remove any characters that are commonly mapped to nothing. 2. **Normalize case using mappings**: Normalize the case of the string using `stringprep.map_table_b2`. If a character is not mapped, keep it unchanged. 3. **Remove prohibited characters**: Remove characters that are in tables: - `stringprep.in_table_c21` (ASCII control characters), - `stringprep.in_table_c5` (Surrogate codes). **Constraints:** - The `input_string` will always be a non-empty Unicode string. **Input:** - `input_string` (str): The Unicode string to be prepared. **Output:** - A Unicode string that has been processed according to the steps outlined. **Example:** ```python input_str = \\"Hellox00World\\" # \'x00\' is a control character processed_str = prepare_internet_identifier(input_str) print(processed_str) # Output: \\"helloworld\\" ``` **Notes:** - The processed string must be case-normalized using `stringprep.map_table_b2`. - Control characters and surrogate codes must be removed entirely. - The `prepare_internet_identifier` function should be efficient and utilize the `stringprep` module functions effectively. Use the module documentation to correctly implement the function.","solution":"import stringprep def prepare_internet_identifier(input_string: str) -> str: # Step 1: Remove commonly mapped characters input_string = \'\'.join(ch for ch in input_string if not stringprep.in_table_b1(ch)) # Step 2: Normalize case using mappings # The function map_table_b2 is used to map characters, defaulting to the character itself if not in the table normalized_string = \'\'.join(stringprep.map_table_b2(ch) if stringprep.map_table_b2(ch) is not None else ch for ch in input_string) # Step 3: Remove prohibited characters (ASCII control characters and surrogate codes) prepared_string = \'\'.join(ch for ch in normalized_string if not (stringprep.in_table_c21(ch) or stringprep.in_table_c5(ch))) return prepared_string"},{"question":"# Question: Generic and Type-Guard-Based Filter Function You are required to implement a generic filter function that filters a list of elements based on a user-defined type guard function. The type guard function determines whether an element should be included in the filtered list. Use the Python `typing` module features such as `TypeVar`, `Callable`, `TypeGuard`, and generics to achieve this. Function Signature ```python from typing import TypeVar, Callable, List, TypeGuard T = TypeVar(\'T\') # Generic type variable U = TypeVar(\'U\') # Type variable for the filtered type that is a subtype of T def filter_with_guard( items: List[T], guard: Callable[[T], TypeGuard[U]] ) -> List[U]: pass ``` Implement this function: - **filter_with_guard** function: - **Input**: - `items`: A list of elements of type `T`. - `guard`: A user-defined type guard function `Callable[[T], TypeGuard[U]]` that returns `True` if an element is of type `U` and should be included in the returned list. - **Output**: A list of elements of type `U` that satisfy the guard function. - **Constraints**: - Do not use any other libraries other than `typing` and standard Python libraries. - Assume the guard function is accurate and only returns `True` for elements of type `U`. Example ```python from typing import TypeGuard, List, Union class Dog: def __init__(self, name: str): self.name = name class Bird: def __init__(self, name: str): self.name = name def is_dog(item: Union[Dog, Bird]) -> TypeGuard[Dog]: return isinstance(item, Dog) pets: List[Union[Dog, Bird]] = [Dog(\\"Fido\\"), Bird(\\"Tweety\\")] filtered_dogs = filter_with_guard(pets, is_dog) print(filtered_dogs) # Should print a list of Dog objects: [<Dog name=\\"Fido\\">] ``` **Note**: Your implementation should use type hints to ensure the return type is accurately inferred, and it should make use of `TypeGuard` to filter out types correctly.","solution":"from typing import TypeVar, Callable, List, TypeGuard T = TypeVar(\'T\') # Generic type variable U = TypeVar(\'U\', bound=T) # Type variable for the filtered type that is a subtype of T def filter_with_guard( items: List[T], guard: Callable[[T], TypeGuard[U]] ) -> List[U]: return [item for item in items if guard(item)]"},{"question":"**Filesystem Manipulation and Querying Task** **Objective:** Write a Python function named `analyze_and_manage_filesystem` that performs a series of file and directory operations using various modules from the Python file and directory access standard library. **Problem Statement:** You are provided with a root directory path. Your task is to perform the following operations: 1. **Count and return** the total number of files and directories within the root directory and all its subdirectories. 2. **Copy** all `.txt` files from the root directory to a new directory named `text_files_backup`, preserving the directory hierarchy. 3. **Create a temporary file** in the `text_files_backup` directory and write into it the paths of all copied `.txt` files. 4. **Generate a report** comparing the contents of the `text_files_backup` directory with the original directory, listing only the `.txt` files (ignoring other formats and directories). **Function Signature:** ```python def analyze_and_manage_filesystem(root_dir: str) -> dict: pass ``` **Input:** - **root_dir (str)**: Absolute or relative path to the root directory. **Output:** - **dict**: A dictionary with the following keys: - `total_files`: An integer representing the total number of files found. - `total_directories`: An integer representing the total number of directories found. - `backup_report`: A list containing paths of all `.txt` files present in `text_files_backup`. **Constraints:** - You must use the `pathlib` module for path manipulations. - Use the `shutil` module for copying files. - Use the `tempfile` module for creating the temporary file. - Use the `filecmp` module for generating the directory comparison report. - The root directory may contain nested subdirectories. **Example:** ```python # Given a sample directory structure: # root_dir/ # ├── dir1/ # │ ├── file_a.txt # │ └── file_b.log # ├── dir2/ # │ ├── file_c.txt # │ └── file_d.txt # └── file_e.txt result = analyze_and_manage_filesystem(\'root_dir\') # Example output: # { # \'total_files\': 5, # \'total_directories\': 2, # \'backup_report\': [ # \'root_dir/text_files_backup/dir1/file_a.txt\', # \'root_dir/text_files_backup/dir2/file_c.txt\', # \'root_dir/text_files_backup/dir2/file_d.txt\', # \'root_dir/text_files_backup/file_e.txt\' # ] # } ``` This question tests the following skills: - Ability to traverse filesystems and count files and directories. - Understanding of copying files while maintaining directory structure. - Capability to create and manipulate temporary files. - Skill in comparing directory contents and generating comparative reports. Make sure all paths in the output are absolute paths. **Note:** You may not necessarily have to handle exceptions for IO operations explicitly in this exercise but focus on the correct usage of the specified modules.","solution":"import pathlib import shutil import tempfile import filecmp def analyze_and_manage_filesystem(root_dir: str) -> dict: root_dir = pathlib.Path(root_dir) total_files = 0 total_directories = 0 txt_files_paths = [] # Traverse the root directory for item in root_dir.rglob(\'*\'): if item.is_file(): total_files += 1 if item.suffix == \'.txt\': txt_files_paths.append(item) elif item.is_dir(): total_directories += 1 # Create the backup directory backup_dir = root_dir / \'text_files_backup\' backup_dir.mkdir(exist_ok=True) copied_files = [] for txt_file in txt_files_paths: rel_path = txt_file.relative_to(root_dir) dest_path = backup_dir / rel_path dest_path.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(txt_file, dest_path) copied_files.append(dest_path) # Create a temporary file and write copied .txt file paths temp_file_path = backup_dir / \'copied_txt_files.txt\' with temp_file_path.open(\'w\') as temp_file: for file_path in copied_files: temp_file.write(str(file_path) + \'n\') # Generate a report of the contents of the backup directory backup_report = [str(file) for file in copied_files] return { \'total_files\': total_files, \'total_directories\': total_directories, \'backup_report\': backup_report }"},{"question":"**Objective**: Implement an XML-RPC server that provides various mathematical and system functionalities. **Task**: 1. Create a custom `SimpleXMLRPCServer` instance. Your server should: - Run on `localhost` at port `8080`. - Handle requests at the path `/RPC`. - Log the incoming requests. - Support `allow_none` and `use_builtin_types` for data handling. 2. Implement and register the following functions: - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the difference of `a` and `b`. - `multiply(a, b)`: Returns the product of `a` and `b`. - `division(a, b)`: Returns the quotient of `a` and `b`. If `b` is `0`, return `\\"Error: Division by zero\\"`. 3. Create a class `SystemFunctions` with the following methods: - `get_system_time()`: Returns the current system time. - `get_system_name()`: Returns the system name `\\"XML-RPC Server\\"`. 4. Register an instance of `SystemFunctions` to the server. 5. Enable and register the introspection functions (`system.listMethods`, `system.methodHelp`, `system.methodSignature`) and multicall functionality. 6. Finally, run the server\'s main loop. **Constraints**: - Use the `xmlrpc.server` module. - Input and output should be properly XML-RPC formatted. - The division function should handle division by zero gracefully. - Ensure usage of decorators for registering functions where possible. **Example**: ```python # Expected flow # Server running at: http://localhost:8080/RPC # Available methods: add, subtract, multiply, division, get_system_time, get_system_name, system.listMethods, system.methodHelp, system.methodSignature, system.multicall import xmlrpc.client proxy = xmlrpc.client.ServerProxy(\\"http://localhost:8080/RPC\\") print(proxy.add(4, 5)) # Output: 9 print(proxy.subtract(10, 6)) # Output: 4 print(proxy.multiply(3, 7)) # Output: 21 print(proxy.division(8, 2)) # Output: 4.0 print(proxy.division(5, 0)) # Output: Error: Division by zero print(proxy.get_system_time()) # Example Output: 2023-10-05 15:30:00 print(proxy.get_system_name()) # Output: XML-RPC Server print(proxy.system.listMethods()) # Expected Output: A list of registered methods ``` **Note**: This is a comprehensive question that assesses the understanding of the `xmlrpc.server` module, function implementation, class method registration, and introspection features.","solution":"from xmlrpc.server import SimpleXMLRPCServer, SimpleXMLRPCRequestHandler import logging from datetime import datetime # Server Logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class RequestHandler(SimpleXMLRPCRequestHandler): rpc_paths = (\'/RPC\',) def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b def division(a, b): if b == 0: return \\"Error: Division by zero\\" return a / b class SystemFunctions: def get_system_time(self): return datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') def get_system_name(self): return \\"XML-RPC Server\\" def main(): with SimpleXMLRPCServer((\'localhost\', 8080), requestHandler=RequestHandler, allow_none=True, use_builtin_types=True) as server: server.register_introspection_functions() server.register_function(add, \'add\') server.register_function(subtract, \'subtract\') server.register_function(multiply, \'multiply\') server.register_function(division, \'division\') server.register_instance(SystemFunctions()) logger.info(\\"Serving XML-RPC on localhost port 8080/RPC\\") server.serve_forever() if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question # Objective This question aims to assess your understanding of using the validation tools and efficient numerical operations provided by the `sklearn.utils` module in scikit-learn. # Problem Statement You are required to write a function named `validate_and_decompose_matrix` that accepts a 2D numpy array `X`, validates it, and computes its truncated singular values decomposition using a randomized algorithm. The function should perform the following tasks: 1. Validate that the input `X` is a 2D array of floats and does not contain any NaNs or Infs. 2. Compute the k-truncated randomized singular value decomposition (SVD) of the matrix `X`. The function should return the matrices `U`, `S`, and `V` where `X ≈ U * S * V.T` and `S` is a diagonal matrix containing the singular values. # Function Signature ```python import numpy as np from sklearn.utils import check_array, extmath def validate_and_decompose_matrix(X: np.ndarray, k: int) -> (np.ndarray, np.ndarray, np.ndarray): pass ``` # Input - `X` (numpy.ndarray): A 2D array of floats. - `k` (int): The number of singular values and vectors to compute. # Output - A tuple of three numpy arrays `(U, S, V)` representing the truncated SVD of the input matrix: - `U` (numpy.ndarray): A matrix of shape `(X.shape[0], k)`. - `S` (numpy.ndarray): A diagonal matrix of shape `(k, k)`. - `V` (numpy.ndarray): A matrix of shape `(k, X.shape[1])`. # Constraints - The input matrix `X` must be a 2D array of floats and must not contain NaNs or Infs. - The value of `k` should be such that `1 <= k <= min(X.shape[0], X.shape[1])`. # Example ```python import numpy as np from sklearn.utils.extmath import randomized_svd X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=float) k = 2 U, S, V = validate_and_decompose_matrix(X, k) print(\'U:\', U) print(\'S:\', S) print(\'V:\', V) ``` # Implementation Notes - Use the `check_array` function from `sklearn.utils` to validate the input matrix `X`. - Use the `randomized_svd` function from `sklearn.utils.extmath` to compute the k-truncated SVD of `X`. # Assumptions - You can assume the input provided to the function will always be a valid numpy array object. Good luck!","solution":"import numpy as np from sklearn.utils import check_array from sklearn.utils.extmath import randomized_svd def validate_and_decompose_matrix(X: np.ndarray, k: int) -> (np.ndarray, np.ndarray, np.ndarray): Validates the input matrix and computes the k-truncated randomized singular value decomposition (SVD). Parameters: X (numpy.ndarray): A 2D array of floats. k (int): The number of singular values and vectors to compute. Returns: tuple: U, S, V matrices where X ≈ U * S * V.T # Validate the input matrix X = check_array(X, dtype=np.float64, ensure_2d=True, allow_nd=False, force_all_finite=True) # Check if k is within the valid range if k < 1 or k > min(X.shape[0], X.shape[1]): raise ValueError(f\\"k must be between 1 and {min(X.shape[0], X.shape[1])}, inclusive.\\") # Compute the truncated SVD U, S, V = randomized_svd(X, n_components=k) # Convert S to a diagonal matrix S_diag = np.diag(S) return U, S_diag, V"},{"question":"# Question You are tasked with implementing a function that reads an AIFF or AIFF-C file, extracts some of its metadata, and converts the audio data to a different format. The function should be implemented using the `aifc` module. Function Signature ```python def convert_aiff_to_custom_format(input_file: str, output_file: str, start_frame: int, end_frame: int) -> None: pass ``` Parameters - `input_file`: A string, the path to the input AIFF or AIFF-C file. - `output_file`: A string, the path to the output file. The output should be in a custom binary format. - `start_frame`: An integer, the starting frame number from which to begin reading the audio data. - `end_frame`: An integer, the ending frame number up to which the audio data should be read. Custom Output Format The custom binary format should have the following structure: 1. The first 4 bytes should represent the number of channels (`nchannels`) as a big-endian integer. 2. The next 4 bytes should represent the sample width (`sampwidth`) in bytes as a big-endian integer. 3. The next 4 bytes should represent the frame rate (`framerate`) as a big-endian integer. 4. The next 4 bytes should represent the number of frames written as a big-endian integer. 5. Following this header, the audio data should be written in raw binary format as read from the input file. Constraints - Use the `aifc` module to read the AIFF or AIFF-C file. - You should handle both compressed and uncompressed audio data. - Assume the input file is well-formed and exists, and the target directory for the output file is writable. Example ```python convert_aiff_to_custom_format(\'example.aif\', \'custom_format_output.bin\', 0, 1000) ``` This function reads from \'example.aif\' from frames 0 to 1000 and writes the data along with the necessary metadata to \'custom_format_output.bin\'.","solution":"import aifc import struct def convert_aiff_to_custom_format(input_file: str, output_file: str, start_frame: int, end_frame: int) -> None: Converts an AIFF or AIFF-C file to a custom binary format. Parameters: - input_file (str): The path to the input AIFF or AIFF-C file. - output_file (str): The path to the output file in custom binary format. - start_frame (int): The starting frame number from which to begin reading the audio data. - end_frame (int): The ending frame number up to which the audio data should be read. with aifc.open(input_file, \'r\') as aiff: # Extract metadata nchannels = aiff.getnchannels() sampwidth = aiff.getsampwidth() framerate = aiff.getframerate() nframes = end_frame - start_frame aiff.setpos(start_frame) audio_data = aiff.readframes(nframes) # Prepare binary content binary_data = struct.pack(\'>IIII\', nchannels, sampwidth, framerate, nframes) binary_data += audio_data # Write to output file with open(output_file, \'wb\') as output: output.write(binary_data)"},{"question":"You are to write a function that analyzes a piece of Python source code and provides a summary of keyword usage. Specifically, the function will return a count of how many times each Python keyword and soft keyword appears in the source code. # Function Signature ```python def keyword_analysis(source_code: str) -> dict: pass ``` # Input - `source_code` (str): A string representing the Python source code to be analyzed. # Output - (dict): A dictionary where: - Keys are Python keywords and soft keywords that are present in the source code. - Values are the counts indicating how many times each keyword and soft keyword appears in the source code. # Constraints 1. The provided source code can be assumed to be syntactically correct. 2. The analysis should be case-sensitive. 3. You must use the `keyword` module functionalities (`keyword.iskeyword()`, `keyword.issoftkeyword()`, `keyword.kwlist`, and `keyword.softkwlist`) to determine keywords and soft keywords. # Examples ```python source_code = def function_example(): if True: return None else: return False assert keyword_analysis(source_code) == {\'def\': 1, \'if\': 1, \'return\': 2, \'else\': 1, \'True\': 1, \'None\': 1, \'False\': 1} source_code = class MyClass: pass def method(self): self.value = 10 assert keyword_analysis(source_code) == {\'class\': 1, \'pass\': 1, \'def\': 1} ``` # Notes - You can assume that the `source_code` will not include comments or string literals that contain keywords. - Your solution should be efficient in terms of both time and space complexity. # Hints - Use regular expressions to tokenize the input source code into distinct words. - Use the provided `keyword` module to check if a token is a keyword or soft keyword and count its occurrences. Good luck!","solution":"import re import keyword def keyword_analysis(source_code: str) -> dict: keywords = set(keyword.kwlist + keyword.softkwlist) # Tokenize the source code into words using regular expressions tokens = re.findall(r\'bw+b\', source_code) # Initialize a dictionary to keep track of keyword counts keyword_counts = {kw: 0 for kw in keywords} # Count the occurrences of each keyword for token in tokens: if token in keywords: keyword_counts[token] += 1 # Filter out the keywords that have zero count keyword_counts = {k: v for k, v in keyword_counts.items() if v > 0} return keyword_counts"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of PyTorch\'s low-level storage manipulation by implementing functions to perform specific operations on tensor storage. # Question **Part A: Create and Manipulate Tensor Storage** 1. Create a function `create_tensor_with_storage` that: - Takes as input a 1D list of integers. - Converts this list into a PyTorch tensor of type `torch.int32`. - Extracts the untyped storage from this tensor. - Returns a tuple containing the tensor and its untyped storage. 2. Create a function `zero_out_storage` that: - Takes as input an untyped storage object. - Zeroes out all elements in this storage. - Returns the modified untyped storage. 3. Create a function `restore_tensor_from_storage` that: - Takes as input a tensor, an untyped storage, and metadata (storage offset, size, stride) for the tensor. - Restores and returns the tensor using the provided storage and metadata. # Constraints - **Input Format:** - For `create_tensor_with_storage`: A 1D list of integers. - For `zero_out_storage`: An instance of `torch.UntypedStorage`. - For `restore_tensor_from_storage`: A tensor, an instance of `torch.UntypedStorage`, and three additional parameters (storage offset, size, stride) reflecting the tensor\'s metadata. # Example ```python import torch def create_tensor_with_storage(data): # Your implementation here pass def zero_out_storage(storage): # Your implementation here pass def restore_tensor_from_storage(tensor, storage, storage_offset, size, stride): # Your implementation here pass # Part A Example Usage # Creating tensor and extracting its storage data = [1, 2, 3, 4] tensor, storage = create_tensor_with_storage(data) print(\\"Original Tensor:\\", tensor) print(\\"Original Storage:\\", list(storage)) # Zeroing out storage zero_out_storage(storage) print(\\"Zeroed Storage:\\", list(storage)) # Restore tensor from modified storage restored_tensor = restore_tensor_from_storage(tensor, storage, tensor.storage_offset(), tensor.size(), tensor.stride()) print(\\"Restored Tensor:\\", restored_tensor) ``` # Expected Output ```python Original Tensor: tensor([1, 2, 3, 4], dtype=torch.int32) Original Storage: [1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0] Zeroed Storage: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] Restored Tensor: tensor([0, 0, 0, 0], dtype=torch.int32) ``` # Notes - Remember to handle the data as `torch.int32` and ensure that storage manipulation preserves or modifies the data as expected. - For `create_tensor_with_storage`, you must convert the input list to a tensor and then access its untyped storage. - In `zero_out_storage`, directly manipulate the data within the storage. - For `restore_tensor_from_storage`, ensure the tensor is re-linked to the modified storage correctly.","solution":"import torch def create_tensor_with_storage(data): Creates a tensor from a 1D list of integers and extracts its untyped storage. Args: data (list of int): 1D list of integers. Returns: tuple: A tuple containing the tensor and its untyped storage. tensor = torch.tensor(data, dtype=torch.int32) storage = tensor.storage().untyped() return tensor, storage def zero_out_storage(storage): Zeroes out all elements in the untyped storage. Args: storage (torch.UntypedStorage): An untyped storage object. Returns: torch.UntypedStorage: The modified untyped storage with all elements zeroed out. for i in range(len(storage)): storage[i] = 0 return storage def restore_tensor_from_storage(tensor, storage, storage_offset, size, stride): Restores a tensor using the provided storage and metadata (storage offset, size, stride). Args: tensor (torch.Tensor): The tensor to be restored. storage (torch.UntypedStorage): An untyped storage object. storage_offset (int): Offset of the storage. size (tuple): Size of the tensor. stride (tuple): Stride of the tensor. Returns: torch.Tensor: The restored tensor. new_tensor = torch.tensor([], dtype=tensor.dtype).set_(storage, storage_offset, size, stride) return new_tensor"},{"question":"**Title**: Asynchronous Subprocess Manager in Python **Objective**: Implement an asynchronous subprocess manager that runs multiple shell commands in parallel, captures their outputs, and handles potential errors. **Background**: You are required to demonstrate proficiency in using the `asyncio` library for running and managing subprocesses asynchronously. You will write a function that spawns multiple subprocesses to execute shell commands, captures their standard output and standard error, and reports the results. **Tasks**: 1. **Function Definition**: Create a function `async def run_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]` where: - `commands` is a list of shell command strings to be executed. - The function should return a list of tuples, each containing: - The command string. - The return code of the command. - The stdout output of the command. - The stderr output of the command. 2. **Execution and Gathering**: - Use `asyncio.create_subprocess_shell` to asynchronously create and run each shell command. - Capture the standard output and standard error using the `communicate` method of the `Process` class. - Ensure all commands are run in parallel, and the results are gathered concurrently. 3. **Error Handling**: - Handle any exceptions that might occur during the creation or execution of subprocesses. - If a subprocess fails to start, capture the error message and return a suitable return code (e.g., -1). **Constraints**: - Do not use any external libraries outside standard Python libraries. - Assume that the shell commands provided are safe and properly quoted if necessary. **Example Implementation**: ```python import asyncio from typing import List, Tuple async def run(cmd: str) -> Tuple[str, int, str, str]: try: proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return (cmd, proc.returncode, stdout.decode().strip(), stderr.decode().strip()) except Exception as e: return (cmd, -1, \'\', str(e)) async def run_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: results = await asyncio.gather(*(run(cmd) for cmd in commands)) return results # Testing the function if __name__ == \'__main__\': import sys commands = [ \'ls\', \'echo \\"Hello, World!\\"\', \'false\', # This will fail with return code 1 ] results = asyncio.run(run_commands(commands)) for result in results: print(result) ``` **Notes**: - The provided `run` function is a helper to execute a single command and manage its subprocess asynchronously. - The `run_commands` function utilizes `asyncio.gather` to run multiple commands in parallel. - Ensure your solution passes several test cases with different shell commands to verify robustness and correctness. **Performance Requirements**: - The solution should efficiently handle up to 10 commands run in parallel. - Properly manage memory usage when capturing command outputs to avoid potential memory issues.","solution":"import asyncio from typing import List, Tuple async def run(cmd: str) -> Tuple[str, int, str, str]: try: proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return (cmd, proc.returncode, stdout.decode().strip(), stderr.decode().strip()) except Exception as e: return (cmd, -1, \'\', str(e)) async def run_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: results = await asyncio.gather(*(run(cmd) for cmd in commands)) return results # Example usage: # if __name__ == \'__main__\': # import sys # # commands = [ # \'ls\', # \'echo \\"Hello, World!\\"\', # \'false\', # This will fail with return code 1 # ] # # results = asyncio.run(run_commands(commands)) # # for result in results: # print(result)"},{"question":"# **DOM Tree Manipulation Using `xml.dom`** **Objective:** You are required to demonstrate your understanding of the `xml.dom` module by writing a set of functions that perform various manipulations and queries on an XML document. By accomplishing this task, you will showcase your proficiency in the DOM API for Python. **Tasks:** 1. **Create a DOM Document:** Write a function `create_dom_document(root_tag)` that takes a single string argument representing the root element\'s tag name and returns a new DOM Document object with the specified root element. 2. **Add Child Elements:** Write a function `add_child_elements(dom, parent_tag, children)` where: - `dom` is the DOM Document object to manipulate. - `parent_tag` is a string representing the tag name of the parent element. - `children` is a list of dictionaries, where each dictionary represents a child element with the following structure: ```python { \\"tag\\": \\"child_tag_name\\", \\"attributes\\": {\\"attr_name\\": \\"attr_value\\", ...}, \\"text\\": \\"child_text_content\\" } ``` The function should append each child element to the specified parent element. 3. **Get Elements and Attributes:** Write a function `get_elements_with_attributes(dom, tag_name)` that takes a DOM Document and a string `tag_name`. The function should return a list of tuples. Each tuple should contain: - The string representation of the element\'s tag name. - A dictionary of the element\'s attributes and their values. 4. **Update Text Content:** Write a function `update_text_content(dom, tag_name, text_content)` that updates the text content of all elements with the specified tag name to the given `text_content`. 5. **Remove Elements:** Write a function `remove_elements_by_tag(dom, tag_name)` that removes all elements with the specified tag name from the DOM. **Constraints:** - Do not use any third-party libraries; rely solely on the `xml.dom` module. - Ensure that the functions handle any potential exceptions, such as when elements or attributes do not exist. **Example Usage:** ```python # Create a new DOM document with the root element \'books\' dom = create_dom_document(\'books\') # Add child elements to the root element with various attributes and text content add_child_elements(dom, \'books\', [ {\'tag\': \'book\', \'attributes\': {\'id\': \'1\', \'genre\': \'fiction\'}, \'text\': \'The Great Gatsby\'}, {\'tag\': \'book\', \'attributes\': {\'id\': \'2\', \'genre\': \'science\'}, \'text\': \'A Brief History of Time\'} ]) # Retrieve all elements with their attributes elements_with_attributes = get_elements_with_attributes(dom, \'book\') print(elements_with_attributes) # Output: [(\'book\', {\'id\': \'1\', \'genre\': \'fiction\'}), (\'book\', {\'id\': \'2\', \'genre\': \'science\'})] # Update the text content of all \'book\' elements update_text_content(dom, \'book\', \'Updated Book\') # Remove elements with the tag \'book\' remove_elements_by_tag(dom, \'book\') ``` This set of tasks will test whether you can effectively create and manipulate XML DOM structures using Python\'s built-in `xml.dom` module.","solution":"from xml.dom.minidom import Document def create_dom_document(root_tag): Creates a new DOM Document with the specified root element tag. doc = Document() root_element = doc.createElement(root_tag) doc.appendChild(root_element) return doc def add_child_elements(dom, parent_tag, children): Adds child elements to the specified parent element in the DOM Document. parent = dom.getElementsByTagName(parent_tag)[0] for child in children: child_element = dom.createElement(child[\'tag\']) # Set attributes for attr_name, attr_value in child[\'attributes\'].items(): child_element.setAttribute(attr_name, attr_value) # Set text content if \'text\' in child: text_node = dom.createTextNode(child[\'text\']) child_element.appendChild(text_node) parent.appendChild(child_element) def get_elements_with_attributes(dom, tag_name): Retrieves elements with the specified tag name along with their attributes. elements = dom.getElementsByTagName(tag_name) result = [] for element in elements: attributes = {attr.name: attr.value for attr in element.attributes.values()} result.append((element.tagName, attributes)) return result def update_text_content(dom, tag_name, text_content): Updates the text content of all elements with the specified tag name. elements = dom.getElementsByTagName(tag_name) for element in elements: # Remove all existing text nodes while element.firstChild: element.removeChild(element.firstChild) # Create new text node text_node = dom.createTextNode(text_content) element.appendChild(text_node) def remove_elements_by_tag(dom, tag_name): Removes all elements with the specified tag name from the DOM. elements = dom.getElementsByTagName(tag_name) for element in elements: element.parentNode.removeChild(element)"},{"question":"# Asynchronous Task Queue Management **Objective:** Implement a set of functions to manage an asynchronous task queue using `asyncio.Queue`. You will create a producer function to add tasks to the queue, a consumer function to process tasks from the queue, and implement a main function to orchestrate the producer and consumer functions concurrently. **Task Specifications:** 1. Write an asynchronous producer function, `producer(queue, n)`, which adds `n` randomly generated integers between 1 and 10 to the queue. 2. Write an asynchronous consumer function, `consumer(queue)`, which continuously removes integers from the queue and processes them by sleeping for that amount of time (in seconds). After processing each integer, the consumer should mark the task as done using `queue.task_done()`. 3. Implement an asynchronous `main()` function that: - Creates an instance of `asyncio.Queue`. - Creates a producer task that adds 10 integers to the queue. - Creates 3 consumer tasks to process the numbers from the queue. - Waits for the producer to complete. - Waits until the queue is fully processed using `queue.join()`. - Cancels the consumer tasks after the queue is fully processed. 4. Handle any potential `asyncio.QueueEmpty` exceptions that may occur if `get_nowait` is used by ensuring that your consumer continues to attempt to retrieve items until they are all processed. 5. Print relevant outputs in your functions to reflect the operations being performed, such as when items are added to the queue, when consumer tasks start processing, and when all tasks are completed. ```python import asyncio import random async def producer(queue, n): Produces `n` random integers between 1 and 10 and puts them into the queue. Args: queue (asyncio.Queue): The queue to put items into. n (int): The number of integers to produce. # Your implementation here print(f\\"Producer: Started producing {n} items\\") for _ in range(n): item = random.randint(1, 10) await queue.put(item) print(f\\"Producer: Added {item} to the queue\\") print(\\"Producer: Finished producing items\\") async def consumer(queue): Consumes and processes items from the queue indefinitely. Args: queue (asyncio.Queue): The queue to get items from. # Your implementation here while True: item = await queue.get() print(f\\"Consumer: Processing item {item}\\") await asyncio.sleep(item) queue.task_done() print(f\\"Consumer: Finished processing item {item}\\") async def main(): # Your implementation here queue = asyncio.Queue() producer_task = asyncio.create_task(producer(queue, 10)) consumer_tasks = [asyncio.create_task(consumer(queue)) for _ in range(3)] await producer_task await queue.join() for task in consumer_tasks: task.cancel() await asyncio.gather(*consumer_tasks, return_exceptions=True) print(\\"All tasks completed successfully\\") # Run the main function with asyncio.run() asyncio.run(main()) ``` **Constraints:** - Use the `asyncio.Queue` class. - Use `random.randint` to generate random integers. - Ensure proper usage and handling of queue operations and exceptions. **Expected Output:** Your program should produce output lines indicating the producer\'s actions, each consumer\'s actions, and the completion of all tasks. The exact sequence may vary depending on how tasks are scheduled. Here is an example of potential output: ``` Producer: Started producing 10 items Producer: Added 5 to the queue Producer: Added 2 to the queue ... Consumer: Processing item 5 Consumer: Finished processing item 5 ... All tasks completed successfully ```","solution":"import asyncio import random async def producer(queue, n): Produces `n` random integers between 1 and 10 and puts them into the queue. Args: queue (asyncio.Queue): The queue to put items into. n (int): The number of integers to produce. print(f\\"Producer: Started producing {n} items\\") for _ in range(n): item = random.randint(1, 10) await queue.put(item) print(f\\"Producer: Added {item} to the queue\\") print(\\"Producer: Finished producing items\\") async def consumer(queue): Consumes and processes items from the queue indefinitely. Args: queue (asyncio.Queue): The queue to get items from. while True: item = await queue.get() print(f\\"Consumer: Processing item {item}\\") await asyncio.sleep(item) queue.task_done() print(f\\"Consumer: Finished processing item {item}\\") async def main(): queue = asyncio.Queue() producer_task = asyncio.create_task(producer(queue, 10)) consumer_tasks = [asyncio.create_task(consumer(queue)) for _ in range(3)] await producer_task await queue.join() for task in consumer_tasks: task.cancel() await asyncio.gather(*consumer_tasks, return_exceptions=True) print(\\"All tasks completed successfully\\") # Run the main function with asyncio.run() asyncio.run(main())"},{"question":"# PyArrow Integration with Pandas Your task is to write a function that performs several data operations using pandas integrated with PyArrow. Function Definition ```python def pyarrow_operations(data: List[List[Optional[Union[int, float, str, bool]]]]) -> Tuple[pd.DataFrame, List[bool]]: This function takes a nested list with potential None values (which should be read as NaNs when corresponding to numerical data). It performs a series of operations using PyArrow types in pandas and returns the processed DataFrame alongside a boolean list for a specific condition. Parameters: - data: List[List[Optional[Union[int, float, str, bool]]]] A nested list representing the input data. Returns: - A tuple containing: - Processed DataFrame with PyArrow-backed data - A list of booleans indicating a specific condition (e.g., if values in the second column are greater than a threshold) # Your code here ``` Input - `data`: A nested list which may contain elements such as integers, floats, strings, booleans, or None (null) values. Example: ```python [ [1, 2.5, \\"a\\", True], [3, 4.5, \\"b\\", False], [None, None, None, None] ] ``` Output - A tuple containing: 1. A pandas DataFrame with columns having appropriate PyArrow-backed data types. - The columns must be adjusted to have their types as follows: - First column as signed int32 - Second column as float64 - Third column as string - Fourth column as boolean 2. A list of booleans indicating whether the values of the second column are greater than or equal to a specified threshold (e.g., 3.0). Example output: ```python (DataFrame, [False, True, False]) ``` Constraints - Use only pandas and PyArrow functionalities as demonstrated in the documentation. - Ensure the use of PyArrow-backed dtypes for creating DataFrame. - Efficiently handle missing data (None values). Example Given the input: ```python [ [1, 2.5, \\"a\\", True], [3, 4.5, \\"b\\", False], [None, None, None, None] ] ``` The function should return a tuple where the DataFrame has the specified PyArrow-backed types and the boolean list corresponds to whether the second column\'s values are greater than or equal to 3.0. ```python ( DataFrame({ \\"col1\\": pd.Series([1, 3, None], dtype=\\"int32[pyarrow]\\"), \\"col2\\": pd.Series([2.5, 4.5, None], dtype=\\"float64[pyarrow]\\"), \\"col3\\": pd.Series([\\"a\\", \\"b\\", None], dtype=\\"string[pyarrow]\\"), \\"col4\\": pd.Series([True, False, None], dtype=\\"bool[pyarrow]\\") }), [False, True, False] ) ```","solution":"import pandas as pd import pyarrow def pyarrow_operations(data): This function takes a nested list with potential None values (which should be read as NaNs when corresponding to numerical data). It performs a series of operations using PyArrow types in pandas and returns the processed DataFrame alongside a boolean list for a specific condition. Parameters: - data: List[List[Optional[Union[int, float, str, bool]]]] A nested list representing the input data. Returns: - A tuple containing: - Processed DataFrame with PyArrow-backed data - A list of booleans indicating a specific condition (e.g., if values in the second column are greater than a threshold) # Create DataFrame from input data df = pd.DataFrame(data, columns=[\\"col1\\", \\"col2\\", \\"col3\\", \\"col4\\"]) # Convert columns to PyArrow-backed types df[\\"col1\\"] = df[\\"col1\\"].astype(\\"int32[pyarrow]\\") df[\\"col2\\"] = df[\\"col2\\"].astype(\\"float64[pyarrow]\\") df[\\"col3\\"] = df[\\"col3\\"].astype(\\"string[pyarrow]\\") df[\\"col4\\"] = df[\\"col4\\"].astype(\\"bool[pyarrow]\\") # Generate boolean list based on second column condition bool_list = df[\\"col2\\"].apply(lambda x: x is not None and x >= 3.0).tolist() return df, bool_list"},{"question":"# Coding Assessment Question Objective Using the `pwd` module in Python, write a function that retrieves and processes Unix user account information. Problem Statement You are required to implement a function `filter_and_format_users(min_uid: int, user_shell: str) -> List[str]` that meets the following criteria: # Input: - `min_uid` (int): A minimum user ID. Only include users with a UID greater than or equal to this value. - `user_shell` (str): A specific user command interpreter (shell). Only include users that have this shell specified. # Output: - Returns a list of formatted strings for each user that meets the criteria. Each string should be in the format: `\\"name (uid: [uid], shell: [shell])\\"` # Constraints: - Use the `pwd` module to access the Unix user account and password database. - The returned list should be sorted by the numerical user ID (ascending order). - Raise a `ValueError` if no users meet the criteria. # Example: ```python # Assume the following users are in the Unix password database: # (name=\'john\', passwd=\'x\', uid=1001, gid=100, gecos=\'John Doe\', dir=\'/home/john\', shell=\'/bin/bash\') # (name=\'jane\', passwd=\'x\', uid=1002, gid=100, gecos=\'Jane Doe\', dir=\'/home/jane\', shell=\'/bin/zsh\') # (name=\'doe\', passwd=\'x\', uid=1500, gid=100, gecos=\'Doe User\', dir=\'/home/doe\', shell=\'/bin/bash\') filter_and_format_users(1000, \'/bin/bash\') ``` The above function call should return: ```python [ \'john (uid: 1001, shell: /bin/bash)\', \'doe (uid: 1500, shell: /bin/bash)\' ] ``` Notes: - You should handle any exceptions that may arise when there are no users meeting the criteria by raising a `ValueError`. - Use the `pwd.getpwall()` function to retrieve all user database entries. Requirements: - Your code should be well-commented and adhere to standard coding best practices.","solution":"import pwd from typing import List def filter_and_format_users(min_uid: int, user_shell: str) -> List[str]: Retrieves and processes Unix user account information based on the given criteria. - min_uid: A minimum user ID. Only include users with a UID >= min_uid. - user_shell: A specific user shell. Only include users that have this shell specified. Returns a list of formatted strings for each user that meets the criteria. Each string is in the format: \\"name (uid: [uid], shell: [shell])\\". Raises a ValueError if no users meet the criteria. users = pwd.getpwall() filtered_users = [ user for user in users if user.pw_uid >= min_uid and user.pw_shell == user_shell ] if not filtered_users: raise ValueError(\\"No users meet the specified criteria.\\") formatted_users = [ f\\"{user.pw_name} (uid: {user.pw_uid}, shell: {user.pw_shell})\\" for user in sorted(filtered_users, key=lambda x: x.pw_uid) ] return formatted_users"},{"question":"# Advanced Python Slicing and Integration with C API Objective: Design a Python program that simulates the behavior of slice objects using custom-implemented functions. Your task is to utilize Python\'s inbuilt slicing mechanisms, conceptually leveraging the operations provided by the C API functions listed in the documentation. Task: 1. Write a Python function `simulate_slice(sequence, start, stop, step)` that takes a sequence and slice parameters (start, stop, and step) and returns a sliced sequence. 2. You are to manually unpack and adjust the slice indices similar to what would be performed by the C API functions `PySlice_Unpack` and `PySlice_AdjustIndices`. Requirements: 1. **Function Signature:** ```python def simulate_slice(sequence: Sequence, start: Optional[int], stop: Optional[int], step: Optional[int]) -> Sequence: ``` 2. **Input:** - `sequence` (Sequence): The sequence to be sliced (e.g., list, tuple, string). - `start` (Optional[int]): The starting index of the slice (can be `None`). - `stop` (Optional[int]): The stopping index of the slice (can be `None`). - `step` (Optional[int]): The step of the slice (can be `None`). 3. **Output:** - The correctly sliced sequence based on the given start, stop, and step parameters. 4. **Constraints:** - If `start`, `stop`, or `step` are `None`, they should default to the usual slice behavior in Python (`None`). - Handle out-of-bounds indices by clipping them appropriately as described in the documentation. - Do not use Python\'s built-in slicing mechanisms directly (`[:]` or slicing syntax `sequence[start:stop:step]`), but you may use basic list operations. - You should include comprehensive docstrings explaining the behavior of your function and how it relates to the C API functions. Example Usage: ```python sequence = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # Equivalent to sequence[2:8:2] print(simulate_slice(sequence, 2, 8, 2)) # Output: [2, 4, 6] # Equivalent to sequence[:5] print(simulate_slice(sequence, None, 5, None)) # Output: [0, 1, 2, 3, 4] # Equivalent to sequence[1::3] print(simulate_slice(sequence, 1, None, 3)) # Output: [1, 4, 7] # Equivalent to sequence[::-1] print(simulate_slice(sequence, None, None, -1)) # Output: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] ``` Notes: - Your implementation should handle sequences other than lists, such as tuples and strings, while preserving the type of the input sequence. - Use helper functions if necessary to maintain code readability and modularity. Good luck, and happy coding!","solution":"from typing import Sequence, Optional def simulate_slice(sequence: Sequence, start: Optional[int], stop: Optional[int], step: Optional[int]) -> Sequence: Simulates the behavior of slicing for sequences. Parameters: - sequence (Sequence): The sequence to be sliced (e.g., list, tuple, string). - start (Optional[int]): The starting index of the slice (can be None). - stop (Optional[int]): The ending index of the slice (can be None). - step (Optional[int]): The step of the slice (can be None). Returns: - Sequence: The appropriately sliced sequence. if step is None: step = 1 if step == 0: raise ValueError(\\"slice step cannot be zero\\") seq_len = len(sequence) # Calculate start index if start is None: start = 0 if step > 0 else seq_len - 1 elif start < 0: start += seq_len if start < 0: start = -1 if step < 0 else 0 if start >= seq_len: start = seq_len if step > 0 else seq_len - 1 # Calculate stop index if stop is None: stop = seq_len if step > 0 else -1 elif stop < 0: stop += seq_len if stop < 0: stop = -1 if stop >= seq_len: stop = seq_len # Generate the resulting sequence based on calculated indices result = [] if step > 0: while start < stop: result.append(sequence[start]) start += step else: while start > stop: result.append(sequence[start]) start += step # Return result in the same type as input sequence if isinstance(sequence, list): return result elif isinstance(sequence, tuple): return tuple(result) elif isinstance(sequence, str): return \'\'.join(result) else: raise TypeError(\\"Unsupported sequence type\\")"},{"question":"**Problem Statement** You are given multiple operations that need to be performed on tensors, all of which involve named dimensions. Your task is to implement a function that takes these operations, performs them in the specified order, and returns the result. **Function Signature:** ```python def perform_operations(operations: list) -> torch.Tensor: pass ``` **Input:** - `operations`: A list of dictionaries, where each dictionary represents an operation. Each dictionary has the following structure: { \\"op\\": str, # The operation to perform, e.g., \\"add\\", \\"sum\\", \\"transpose\\" \\"input\\": list, # A list of inputs to the operation (may contain tensors or other necessary arguments) \\"arguments\\": dict # Additional named arguments for the operation } **Output:** - Returns a tensor resulting from applying the sequence of operations. **Constraints:** - The input tensors will have named dimensions. - The operations will be a mix of those involving name inference rules as specified in the provided documentation. **Example:** ```python import torch # Example usage of named tensors x = torch.randn(3, 3, names=(\'N\', \'C\')) y = torch.randn(3, 3, names=(\'N\', None)) op_list = [ { \\"op\\": \\"add\\", \\"input\\": [x, y], \\"arguments\\": {} }, { \\"op\\": \\"sum\\", \\"input\\": [], \\"arguments\\": {\\"dim\\": [\\"N\\"]} } ] result = perform_operations(op_list) print(result.names) # Output: (\'C\',) ``` **Explanation:** 1. The first operation in `op_list` is an addition (`\\"op\\": \\"add\\"`), which adds `x` and `y`. According to the name inference rules, the resulting tensor will have names (\'N\', \'C\'). 2. The second operation is a sum reduction over the \'N\' dimension (`\\"op\\": \\"sum\\"` with `dim: [\\"N\\"]`). This will remove the \'N\' dimension, resulting in a tensor with the name (\'C\',). Requirements: - You must ensure that the name inference rules are correctly applied. - Handle any necessary broadcasting or alignment of named dimensions. - If an operation involves dimensions that do not exist, the function should raise a `RuntimeError`. **Notes:** - You can assume all tensors involved in the inputs are compatible for the operations specified. - Use the named tensor functionalities appropriately to ensure operations respect the name dimensions.","solution":"import torch def perform_operations(operations): Perform a sequence of operations on tensors with named dimensions. :param operations: List of dictionaries representing operations to be performed. Each dictionary has the following structure: { \\"op\\": str, # The operation to perform, e.g. \\"add\\", \\"sum\\", \\"transpose\\" \\"input\\": list, # List of inputs to the operation (may contain tensors or other necessary arguments) \\"arguments\\": dict # Additional named arguments for the operation } :return: A tensor resulting from applying the sequence of operations. result = None for operation in operations: op = operation[\\"op\\"] inputs = operation[\\"input\\"] arguments = operation[\\"arguments\\"] if op == \\"add\\": result = inputs[0] + inputs[1] # Assuming only two inputs for add elif op == \\"sum\\": result = inputs[0].sum(dim=tuple(arguments[\\"dim\\"])) elif op == \\"transpose\\": result = inputs[0].transpose(dim0=arguments[\\"dim0\\"], dim1=arguments[\\"dim1\\"]) elif op == \\"mul\\": result = inputs[0] * inputs[1] elif op == \\"matmul\\": result = inputs[0].matmul(inputs[1]) else: raise RuntimeError(f\\"Unsupported operation: {op}\\") return result"},{"question":"**Objective:** Implement a function that demonstrates understanding and correct usage of Boolean operations and manipulations in Python. **Problem Statement:** You are tasked with implementing a function named `process_boolean_list` that takes a list of integers. The function should perform the following operations: 1. **Convert the integers to Boolean values**: - Any non-zero integer should be converted to `True`. - Zero should be converted to `False`. 2. **Perform a series of evaluations on the Boolean list**: - Compute the logical AND of all elements in the list. - Compute the logical OR of all elements in the list. - Count the number of `True` values in the list. 3. **Return the results**: - The function should return a dictionary with the following keys and their respective results: - `\'and_all\'`: result of logical AND (using the `all()` Python function) of all elements. - `\'or_any\'`: result of logical OR (using the `any()` Python function) of all elements. - `\'count_true\'`: the count of `True` values in the list. **Function Signature:** ```python def process_boolean_list(input_list: list[int]) -> dict: ``` **Input:** - A list of integers, `input_list`, where: - The length of `input_list` is between 1 and 1000 (inclusive). - The integers in `input_list` range from -1000 to 1000 (inclusive). **Output:** - A dictionary with three keys: - `\'and_all\'`: a Boolean value indicating the logical AND of the list. - `\'or_any\'`: a Boolean value indicating the logical OR of the list. - `\'count_true\'`: an integer representing the count of `True` values in the list. **Example:** ```python assert process_boolean_list([1, 0, -2, 3]) == {\'and_all\': False, \'or_any\': True, \'count_true\': 3} assert process_boolean_list([0, 0, 0]) == {\'and_all\': False, \'or_any\': False, \'count_true\': 0} ``` **Constraints:** - You must not use the `int` class directly for comparison or conversion. Use boolean operations and functions to derive results. **Notes:** - Utilize native Python Boolean operations and functions (`all()`, `any()`, and `bool()` conversions) to solve the problem efficiently.","solution":"def process_boolean_list(input_list: list[int]) -> dict: Process a list of integers to determine their Boolean evaluations. Args: input_list: list of integers. Returns: A dictionary with results of AND, OR evaluations and count of True values. boolean_list = [bool(x) for x in input_list] result = { \'and_all\': all(boolean_list), \'or_any\': any(boolean_list), \'count_true\': sum(boolean_list) } return result"},{"question":"# File and Directory Operations Challenge **Objective:** Write a Python function to perform various file and directory operations using the `pathlib`, `tempfile`, and `shutil` modules. **Function Signature:** ```python def manage_files_and_directories(data: str) -> str: pass ``` **Input:** - `data` (str): A multiline string representing content for a set of files. The function should split `data` into separate lines. Each line is the content meant for a separate file. **Output:** - `output` (str): The function should return a multiline string. Each line should contain the path of a newly created temporary file, followed by the line count and word count of the file content, formatted as: ``` /path/to/tempfile.txt - Lines: X, Words: Y ``` Ensure the temp files are also deleted after their usage. **Constraints:** 1. Each temporary file should only exist for the duration of the function\'s execution. 2. Avoid using any deprecated functions or variables. 3. Use the `tempfile` module for creating temporary files and `shutil` for any high-level file operations required. 4. Ensure that the function works efficiently even for large inputs. **Example:** ```python input_data = First line of the first file. Second line. This is the third file content. The last file content is here. result = manage_files_and_directories(input_data) print(result) ``` **Expected Output:** ``` /tmp/tmpabcdef1.txt - Lines: 1, Words: 6 /tmp/tmpabcdef2.txt - Lines: 1, Words: 2 /tmp/tmpabcdef3.txt - Lines: 1, Words: 5 /tmp/tmpabcdef4.txt - Lines: 1, Words: 5 ``` # Notes: 1. Use `pathlib.Path` for handling file paths. 2. Use `tempfile.NamedTemporaryFile` for creating temporary files. 3. Use `shutil` as needed for high-level operations such as cleaning up temporary files. # Additional Specifications: - **Performance**: Ensure the function can handle input text with up to 10,000 lines efficiently within a reasonable time. - **Error Handling**: Properly handle any filesystem-related errors to ensure the function exits gracefully.","solution":"import tempfile from pathlib import Path def manage_files_and_directories(data: str) -> str: lines = data.split(\'n\') output_lines = [] for line in lines: with tempfile.NamedTemporaryFile(delete=False, mode=\'w\', suffix=\'.txt\') as temp_file: temp_path = Path(temp_file.name) temp_file.write(line) # Read back the file to count lines and words with temp_path.open(\'r\') as file: content = file.read() line_count = content.count(\'n\') + 1 word_count = len(content.split()) output_lines.append(f\\"{temp_path} - Lines: {line_count}, Words: {word_count}\\") # Clean up the temporary file temp_path.unlink() return \'n\'.join(output_lines)"},{"question":"# UUID Analysis and Comparison Task **Objective:** Your task is to write a function that generates, analyzes, and compares multiple UUIDs. The function should demonstrate the generation of different types of UUIDs and perform specified analyses on them. # Function Specification **Function Name:** `analyze_and_compare_uuids` **Inputs:** - `num_uuids` (int): The number of random UUIDs that should be generated using `uuid4()`. - `namespace` (UUID instance): The namespace to be used for generating `uuid5()` UUIDs. - `name` (str): The name to be used for generating `uuid5()` UUIDs. **Outputs:** - A dictionary with the following keys: - `random_uuids`: A list of UUIDs generated using `uuid4()`. - `namespace_uuid`: The UUID generated using `uuid5()` with the given namespace and name. - `uuid_versions`: A list of versions for each UUID generated (list of integers). The order should correspond to `random_uuids` followed by `namespace_uuid`. - `are_random_uuids_safe`: A list of booleans indicating if each `random_uuid` is multiprocessing safe. - `namespaced_comparisons`: A list of tuples comparing each random UUID with the `namespace_uuid`. Each tuple should contain the comparison result in terms of their integer values. **Constraints:** 1. `num_uuids` will be a positive integer (1 <= num_uuids <= 100). 2. Assume the `namespace` provided will be a valid UUID instance. 3. The `name` will be a string with at most 100 characters. **Example:** ```python import uuid def analyze_and_compare_uuids(num_uuids, namespace, name): # Your implementation here # Example call and expected output namespace_uuid = uuid.uuid5(uuid.NAMESPACE_DNS, \'example.com\') result = analyze_and_compare_uuids(5, namespace_uuid, \'example\') print(result) ``` Expected Output Structure: ```python { \'random_uuids\': [UUID(\'...\'), UUID(\'...\'), ..., UUID(\'...\')], \'namespace_uuid\': UUID(\'...\'), \'uuid_versions\': [4, 4, 4, 4, 4, 5], \'are_random_uuids_safe\': [False, False, False, False, False], # assuming platform does not support safe generation \'namespaced_comparisons\': [(False, True, False, ...)] # True if `random_uuid.int` > `namespace_uuid.int`, otherwise False } ``` Your task is to implement the `analyze_and_compare_uuids` function as specified. Ensure that your code handles edge cases and adheres to the constraints given.","solution":"import uuid def analyze_and_compare_uuids(num_uuids, namespace, name): # Generate the random UUIDs using uuid4() random_uuids = [uuid.uuid4() for _ in range(num_uuids)] # Generate the namespace UUID using uuid5() namespace_uuid = uuid.uuid5(namespace, name) # List of versions for each UUID generated uuid_versions = [u.version for u in random_uuids] + [namespace_uuid.version] # List indicating if each random UUID is multiprocessing safe are_random_uuids_safe = [u.is_safe == uuid.SafeUUID.safe for u in random_uuids] # List of tuples comparing each random UUID with the namespace UUID namespaced_comparisons = [(ru.int > namespace_uuid.int) for ru in random_uuids] # Create the result dictionary result = { \'random_uuids\': random_uuids, \'namespace_uuid\': namespace_uuid, \'uuid_versions\': uuid_versions, \'are_random_uuids_safe\': are_random_uuids_safe, \'namespaced_comparisons\': namespaced_comparisons } return result"},{"question":"**Objective:** To assess the understanding and implementation skills in file operations, string formatting, and handling JSON data. **Problem Statement:** You are a data analyst who needs to process a data file and generate a detailed report. The data file contains JSON entries of customer transactions, and your task is to read these entries, process them to extract meaningful information, format the extracted information, and save the report to a new file. **Instructions:** 1. **Data File:** The input data file named `transactions.json` contains JSON entries representing individual customer transactions. Each transaction entry has the following attributes: - `customer_id`: An integer representing the customer\'s ID. - `transaction_id`: An integer representing the transaction ID. - `amount`: A float representing the transaction amount. - `timestamp`: A string representing the date and time of the transaction in the format `YYYY-MM-DD HH:MM:SS`. 2. **Processing Requirements:** - Read and parse the JSON data from the file. - Calculate the total amount spent by each customer. - Generate a summary report with the following format for each customer: - Customer ID - Total Amount Spent 3. **Formatting Requirements:** - Use formatted string literals (f-strings) for creating the summary report. 4. **Output File:** - Save the formatted summary report to a new text file named `summary_report.txt`. **Constraints:** - Assume the `transactions.json` file is present in the current working directory and has valid JSON entries. - Handle any file operation exceptions appropriately to ensure the program does not crash. **Example:** If the `transactions.json` file contains the following entries: ```json [ {\\"customer_id\\": 1, \\"transaction_id\\": 101, \\"amount\\": 59.99, \\"timestamp\\": \\"2023-01-15 14:30:00\\"}, {\\"customer_id\\": 2, \\"transaction_id\\": 102, \\"amount\\": 35.50, \\"timestamp\\": \\"2023-01-15 15:00:00\\"}, {\\"customer_id\\": 1, \\"transaction_id\\": 103, \\"amount\\": 20.75, \\"timestamp\\": \\"2023-01-15 15:10:00\\"} ] ``` The generated `summary_report.txt` should contain: ``` Customer ID: 1, Total Amount Spent: 80.74 Customer ID: 2, Total Amount Spent: 35.50 ``` **Function Signature:** ```python def generate_summary_report(input_file: str, output_file: str) -> None: pass ``` **Note:** - Ensure your solution follows best practices for reading from and writing to files. - Utilize appropriate string formatting techniques to generate the required output format.","solution":"import json def generate_summary_report(input_file: str, output_file: str) -> None: try: with open(input_file, \'r\') as file: transactions = json.load(file) customer_totals = {} for transaction in transactions: customer_id = transaction[\'customer_id\'] amount = transaction[\'amount\'] if customer_id in customer_totals: customer_totals[customer_id] += amount else: customer_totals[customer_id] = amount report_lines = [] for customer_id, total_amount in customer_totals.items(): report_lines.append(f\\"Customer ID: {customer_id}, Total Amount Spent: {total_amount:.2f}\\") with open(output_file, \'w\') as file: file.write(\\"n\\".join(report_lines)) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# **Coding Assessment Question** **Objective** Your task is to demonstrate your understanding of scikit-learn by loading a provided toy dataset, preprocessing it, training a machine learning model, and evaluating the model\'s performance. **Dataset** Use the `load_wine` dataset from `scikit-learn`. This dataset contains the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivators. The analysis determines the quantities of 13 constituents found in each of the three types of wines. **Instructions** 1. **Load the dataset:** Use `load_wine()` from `sklearn.datasets` to load the dataset. 2. **Preprocess the dataset:** - Split the dataset into training and testing sets (80% training, 20% testing). - Standardize the dataset (i.e., scale the features to have zero mean and unit variance). 3. **Train a Model:** - Train a Logistic Regression model (`sklearn.linear_model.LogisticRegression`) on the training set. 4. **Evaluate the Model:** - Evaluate the model on the testing set and report the following metrics: - Accuracy - Confusion Matrix **Constraints** 1. **Input Format:** - There are no input constraints; the dataset is loaded directly from `scikit-learn`. 2. **Output Format:** - Print the accuracy as a floating-point number. - Print the confusion matrix as a 2D list. 3. **Performance Requirements:** - The model should achieve at least 90% accuracy on the testing set. - Ensure the implementation is efficient and make use of built-in functions wherever possible. **Example:** ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset data = load_wine() X, y = data.data, data.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize dataset scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train Logistic Regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Predict y_pred = model.predict(X_test) # Evaluate accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") print(\\"Confusion Matrix:\\") print(conf_matrix) ``` This is expected output if you print the accuracy and confusion matrix correctly, but actual numbers might vary with different splits and preprocessing nuances.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix def preprocess_and_train_wine_dataset(): # Load dataset data = load_wine() X, y = data.data, data.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize dataset scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train Logistic Regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Predict y_pred = model.predict(X_test) # Evaluate accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix"},{"question":"Objective: Design a program that leverages Python\'s `multiprocessing` package to perform a simulation involving parallel tasks, inter-process communication, and shared states. Problem Statement: You are tasked with creating a simulation where multiple sensor processes concurrently read data and update a shared database. Each sensor process generates a random integer between 1 and 100, every second, and adds it to a shared list. A database process continuously monitors the shared list, computes the average of the last ten readings, and updates a shared value representing this average. Requirements: 1. **Sensor Process**: - Each sensor process should run indefinitely, generating a random integer between 1 and 100 every second using `time.sleep(1)`. - The generated integer should be added to a `multiprocessing.Array` shared list. 2. **Database Process**: - This process should monitor the shared list and compute the average of the last ten readings once there are at least ten readings in the list. - The computed average should be stored in a `multiprocessing.Value`. 3. **Synchronization**: - Ensure that access to the shared list is synchronized using appropriate locking mechanisms to avoid data corruption. - Use a `multiprocessing.Queue` to gracefully stop all processes. Function Implementation: 1. Create a `sensor_process` function to be executed by sensor processes. 2. Create a `database_process` function to be executed by the database process. 3. Implement a main function to set up shared resources, spawn processes, and handle graceful shutdown. Expected Input and Output: - **Input**: The number of sensor processes to create (N). - **Output**: The program should run indefinitely until manually terminated. During execution, the last ten reading average should be printed to the console by the database process. Performance Constraints: - Ensure the program runs efficiently without unnecessary CPU usage or context switching overhead. Example: ```python import multiprocessing as mp import random import time def sensor_process(shared_list, stop_signal, lock): while not stop_signal.is_set(): with lock: if len(shared_list) >= 10: shared_list.pop(0) shared_list.append(random.randint(1, 100)) time.sleep(1) def database_process(shared_list, avg_value, stop_signal, lock): while not stop_signal.is_set(): time.sleep(1) with lock: if len(shared_list) >= 10: avg_value.value = sum(shared_list[-10:]) / 10 print(f\\"Average of last 10 readings: {avg_value.value}\\") def main(num_sensors): stop_signal = mp.Event() lock = mp.Lock() shared_list = mp.Manager().list() avg_value = mp.Value(\'d\', 0.0) sensors = [mp.Process(target=sensor_process, args=(shared_list, stop_signal, lock)) for _ in range(num_sensors)] db_process = mp.Process(target=database_process, args=(shared_list, avg_value, stop_signal, lock)) for s in sensors: s.start() db_process.start() try: while True: time.sleep(1) except KeyboardInterrupt: stop_signal.set() for s in sensors: s.join() db_process.join() if __name__ == \'__main__\': num_sensors = 5 # Example: 5 sensor processes main(num_sensors) ``` **Note:** This question assesses the understanding and application of process-based parallelism, inter-process communication, and synchronization using Python\'s `multiprocessing` package.","solution":"import multiprocessing as mp import random import time def sensor_process(shared_list, stop_signal, lock): while not stop_signal.is_set(): with lock: if len(shared_list) >= 10: shared_list.pop(0) shared_list.append(random.randint(1, 100)) time.sleep(1) def database_process(shared_list, avg_value, stop_signal, lock): while not stop_signal.is_set(): time.sleep(1) with lock: if len(shared_list) >= 10: avg_value.value = sum(shared_list[-10:]) / 10 print(f\\"Average of last 10 readings: {avg_value.value}\\") def main(num_sensors): stop_signal = mp.Event() lock = mp.Lock() manager = mp.Manager() shared_list = manager.list() avg_value = mp.Value(\'d\', 0.0) sensors = [mp.Process(target=sensor_process, args=(shared_list, stop_signal, lock)) for _ in range(num_sensors)] db_process = mp.Process(target=database_process, args=(shared_list, avg_value, stop_signal, lock)) for s in sensors: s.start() db_process.start() try: while True: time.sleep(1) except KeyboardInterrupt: stop_signal.set() for s in sensors: s.join() db_process.join() if __name__ == \'__main__\': num_sensors = 5 # Example: 5 sensor processes main(num_sensors)"},{"question":"**Coding Assessment Question** # Title: Advanced Assignment Manipulation in Python # Objective To assess students\' understanding of various assignment statements, including standard assignment, augmented assignment, and specific subtleties involving mutable objects like lists and dictionaries in Python. # Problem Statement You are required to implement a function `manipulate_assignments` that performs a series of assignment operations on a given list and a dictionary based on specific rules. # Function Signature ```python def manipulate_assignments(initial_list: list, operations: list) -> tuple: ``` # Input Format - `initial_list`: A list of integers. - `operations`: A list of operations in string format. Each operation is a command that either: - Modifies the list using standard assignment or augmented assignment. - Modifies a dictionary derived from the list. # Output Format - The function should return a tuple containing: - The modified list after performing all list operations. - A dictionary where keys are string representations of list indices and values are the elements at those indices after all operations. # Constraints 1. All elements in the initial_list are integers. 2. The `operations` list will contain valid Python statements that could include standard assignments, augmented assignments, deletions, and dictionary assignments derived from lists. 3. The initial list and dictionary should faithfully reflect the changes after each operation. # Detailed Requirements 1. The list should be modified in-place according to each operation. 2. Dictionary modifications should reflect changes in the list and additional specified operations. 3. Both proper handling of mutable sequence assignments and immutables within sequences should be showcased. # Example ```python >>> initial_list = [0, 1, 2, 3, 4] >>> operations = [ \\"initial_list[2] = 10\\", \\"initial_list[1:3] = [7, 8]\\", \\"for i in range(len(initial_list)): initial_list[i] += 2\\", \\"result_dict = {str(i): val for i, val in enumerate(initial_list)}\\" ] >>> manipulate_assignments(initial_list, operations) ([2, 9, 10, 5, 6], {\'0\': 2, \'1\': 9, \'2\': 10, \'3\': 5, \'4\': 6}) ``` In this example: 1. \\"initial_list[2] = 10\\" sets the 2nd index of the list to 10. 2. \\"initial_list[1:3] = [7, 8]\\" changes a slice of the list. 3. \\"for i in range(len(initial_list)): initial_list[i] += 2\\" performs an in-place augmentation. 4. \\"{str(i): val for i, val in enumerate(initial_list)}\\" generates a dictionary from the modified list. # Notes - You need to handle execution of each operation string using `exec()` carefully to ensure that it applies the changes to the `initial_list` and the dictionary where required. - Pay attention to possible edge cases where operations might affect elements directly or through slices. # Evaluation Criteria - Correct application of each assignment type. - Proper in-place modification and construction of the dictionary. - Handling of different types of operations: slicing, indexing, and assignments in the list.","solution":"def manipulate_assignments(initial_list: list, operations: list) -> tuple: Perform a series of operations on an initial list and modify a dictionary based on the final list. Args: initial_list (list): A list of integers to be manipulated. operations (list): A list of string operations to be executed on the list. Returns: tuple: Final state of the list and a dictionary with string keys as indices and values as list elements. result_dict = {} # Execute each operation in the provided list of operations for operation in operations: exec(operation, {\\"initial_list\\": initial_list, \\"result_dict\\": result_dict, \\"enumerate\\": enumerate, \\"range\\": range}) # Create the result dictionary from the modified list if not created within the operations if not result_dict: result_dict = {str(i): val for i, val in enumerate(initial_list)} return initial_list, result_dict"},{"question":"# Advanced Coding Assessment: Asyncio Queue Management **Objective:** Demonstrate your understanding and implementation skills of `asyncio` queues by developing a solution to manage tasks with both `Queue` and `PriorityQueue`. **Problem Statement:** You are tasked with creating an asynchronous task management system. You need to implement a system that: 1. Distributes tasks among multiple workers using an `asyncio.Queue`. 2. Organizes tasks based on priority using an `asyncio.PriorityQueue`. **Task Description:** 1. **Create a worker coroutine `worker(name: str, queue: asyncio.Queue)` which:** - Continuously retrieves tasks from the provided queue (`queue`). - Simulates the processing of a task by sleeping for the specified amount of time in the task. - Calls `task_done()` to notify the queue that the task is complete. 2. **Develop an asynchronous function `main()` to:** - Create an instance of `asyncio.PriorityQueue` to store tasks. - Generate `30` tasks with random sleep times and random priority levels, and put them into the priority queue. Each task should be a tuple of the form `(priority, sleep_for)`. - Create and start `4` worker tasks to process the queue concurrently. - Ensure the main coroutine waits until the queue is empty and all tasks are processed. - Cancel the worker tasks once all tasks have been processed. 3. **Considerations:** - Each task is represented as a tuple `(priority, sleep_for)`, where `priority` determines the order of execution (lower number has higher priority) and `sleep_for` is the time the worker will sleep to simulate task processing. - Use appropriate queue methods to handle task additions and retrievals. - Handle cancellation of tasks properly. **Input and Output:** - Input: None - Output: Print statements indicating task completion similar to `worker-{name} has completed a task with priority {priority} and slept for {sleep_for:.2f} seconds`. - Ensure your output clearly demonstrates that tasks are being processed based on their priority. **Constraints:** - Use only `asyncio` queues for task management. - Utilize `async` and `await` for asynchronous operations. - Random sleep times should be between `0.05` and `1.0` seconds. - Priority levels should be random integers between `1` and `10`. **Performance Requirements:** - Ensure that the tasks are processed efficiently and that the queue management is correct. ```python import asyncio import random async def worker(name: str, queue: asyncio.PriorityQueue): while True: try: # Retrieve the next task priority, sleep_for = await queue.get() # Simulate processing the task await asyncio.sleep(sleep_for) # Notify that the task is done queue.task_done() print(f\'{name} has completed a task with priority {priority} and slept for {sleep_for:.2f} seconds\') except asyncio.CancelledError: break async def main(): # Create a priority queue queue = asyncio.PriorityQueue() # Add 30 tasks with random priorities and sleep times for _ in range(30): priority = random.randint(1, 10) sleep_for = random.uniform(0.05, 1.0) await queue.put((priority, sleep_for)) # Create and start 4 worker tasks workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(4)] # Wait until all tasks in the queue are processed await queue.join() # Cancel all worker tasks for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) print(\'All tasks have been processed.\') # Run the main function asyncio.run(main()) ``` **Assessment Criteria:** - Correct usage of `asyncio` queue methods. - Proper task distribution and completion using priorities. - Handling of task cancellation and queue management accurately. - The code should be efficient and showcase good practices in concurrent programming.","solution":"import asyncio import random async def worker(name: str, queue: asyncio.PriorityQueue): while True: try: # Retrieve the next task priority, sleep_for = await queue.get() # Simulate processing the task await asyncio.sleep(sleep_for) # Notify that the task is done queue.task_done() print(f\'{name} has completed a task with priority {priority} and slept for {sleep_for:.2f} seconds\') except asyncio.CancelledError: break async def main(): # Create a priority queue queue = asyncio.PriorityQueue() # Add 30 tasks with random priorities and sleep times for _ in range(30): priority = random.randint(1, 10) sleep_for = random.uniform(0.05, 1.0) await queue.put((priority, sleep_for)) # Create and start 4 worker tasks workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(4)] # Wait until all tasks in the queue are processed await queue.join() # Cancel all worker tasks for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) print(\'All tasks have been processed.\') # Run the main function asyncio.run(main())"},{"question":"# Python Asyncio - Network Server with Scheduled Callbacks **Objective**: Implement a network server using asyncio that accepts incoming client connections, handles data transmission asynchronously, and incorporates scheduled callbacks to manage client sessions. # Requirements: 1. **Set up a TCP Server**: - Create a TCP server that listens on a specified host and port. - The server should handle multiple clients concurrently. - Implement a callback function to handle incoming client connections. 2. **Client Session Management**: - For each client, schedule a callback to check the connection status at regular intervals. - If the client remains inactive for a specified timeout period, close the connection. 3. **Task Implementation**: - Implement an async function for reading data from clients and responding accordingly. - Handle any exceptions during data transmission and ensure the connection is properly closed afterward. # Function Signatures: ```python import asyncio async def client_handler(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): Handle incoming client connections. Args: - reader (asyncio.StreamReader): The stream reader for client data. - writer (asyncio.StreamWriter): The stream writer for sending data to the client. pass async def check_client_status(writer: asyncio.StreamWriter, timeout: int): Periodically check the client status and close the connection if inactive. Args: - writer (asyncio.StreamWriter): The stream writer for sending data to the client. - timeout (int): The period of inactivity in seconds after which the connection should be closed. pass async def main(host: str, port: int, timeout: int): Set up the server and run the event loop until interrupted. Args: - host (str): The host address to bind the server. - port (int): The port number to bind the server. - timeout (int): The inactivity timeout period for clients. pass if __name__ == \\"__main__\\": asyncio.run(main(\'127.0.0.1\', 8888, 300)) ``` # Example Usage: ```python # Start the server # You can run this script and then use a telnet client or similar to connect to 127.0.0.1:8888 async def client_handler(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Connection from {addr}\\") while True: data = await reader.read(100) message = data.decode() if not message: print(f\\"Closing connection to {addr}\\") writer.close() await writer.wait_closed() break print(f\\"Received {message} from {addr}\\") response = f\\"Echo: {message}\\" writer.write(response.encode()) await writer.drain() async def check_client_status(writer, timeout): while True: await asyncio.sleep(timeout) print(\\"Checking client status...\\") if writer.is_closing(): print(\\"Client inactive, closing connection\\") writer.close() await writer.wait_closed() break async def main(host, port, timeout): server = await asyncio.start_server(client_handler, host, port) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main(\'127.0.0.1\', 8888, 10)) # Set timeout to 10 seconds for testing ``` # Constraints: - Ensure proper exception handling in all async functions to avoid unhandled errors. - The inactivity timeout should be a configurable parameter. # Notes: - You may use `asyncio.create_task` to manage concurrent execution of tasks. - Make use of asyncio\'s robust `StreamReader` and `StreamWriter` for handling client connections and data transmission.","solution":"import asyncio async def client_handler(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): addr = writer.get_extra_info(\'peername\') print(f\\"Connection from {addr}\\") try: while True: data = await reader.read(100) if data == b\'\': break message = data.decode() print(f\\"Received {message} from {addr}\\") response = f\\"Echo: {message}\\" writer.write(response.encode()) await writer.drain() except Exception as e: print(f\\"Error handling client {addr}: {e}\\") finally: print(f\\"Closing connection to {addr}\\") writer.close() await writer.wait_closed() async def check_client_status(writer: asyncio.StreamWriter, timeout: int): while not writer.is_closing(): await asyncio.sleep(timeout) if writer.is_closing(): break print(\\"Checking client status...\\") async def main(host: str, port: int, timeout: int): server = await asyncio.start_server(client_handler, host, port) print(f\'Serving on {host}:{port}\') async with server: try: await server.serve_forever() except asyncio.CancelledError: pass if __name__ == \\"__main__\\": asyncio.run(main(\'127.0.0.1\', 8888, 10)) # Set timeout to 10 seconds for testing"},{"question":"**Objective:** Demonstrate comprehension of the `inspect` module in Python by dynamically examining function signatures and parameters, and generating descriptive outputs. # Task: You are required to implement a function `analyze_function_signature` that takes another function as its input, retrieves its signature, and returns a descriptive dictionary containing details about the function\'s parameters and return annotation. # Function Signature: ```python def analyze_function_signature(func: callable) -> dict: pass ``` # Input: - `func` (callable): A function whose signature and parameters need to be analyzed. The function can have any combination of positional, keyword, or variable arguments. # Output: - A dictionary with the following structure: ```python { \\"parameters\\": [ { \\"name\\": <parameter_name>, \\"kind\\": <parameter_kind>, # One of: \'POSITIONAL_ONLY\', \'POSITIONAL_OR_KEYWORD\', \'VAR_POSITIONAL\', \'KEYWORD_ONLY\', \'VAR_KEYWORD\' \\"default\\": <default_value>, # If no default value, this should be \\"Parameter.empty\\" \\"annotation\\": <annotation> # If no annotation, this should be \\"Parameter.empty\\" }, ... ], \\"return_annotation\\": <return_annotation> # If no return annotation, this should be \\"Signature.empty\\" } ``` # Constraints: - The function should handle any valid Python function. - Default values and annotations should be correctly identified or marked as \\"Parameter.empty\\" if not present. # Example: Given the function: ```python def example_function(a: int, b: float = 3.5, *args, c: str = \'default\', d, **kwargs) -> bool: pass ``` The `analyze_function_signature(example_function)` should return: ```python { \\"parameters\\": [ { \\"name\\": \\"a\\", \\"kind\\": \\"POSITIONAL_OR_KEYWORD\\", \\"default\\": Parameter.empty, \\"annotation\\": int }, { \\"name\\": \\"b\\", \\"kind\\": \\"POSITIONAL_OR_KEYWORD\\", \\"default\\": 3.5, \\"annotation\\": float }, { \\"name\\": \\"args\\", \\"kind\\": \\"VAR_POSITIONAL\\", \\"default\\": Parameter.empty, \\"annotation\\": Parameter.empty }, { \\"name\\": \\"c\\", \\"kind\\": \\"KEYWORD_ONLY\\", \\"default\\": \\"default\\", \\"annotation\\": str }, { \\"name\\": \\"d\\", \\"kind\\": \\"KEYWORD_ONLY\\", \\"default\\": Parameter.empty, \\"annotation\\": Parameter.empty }, { \\"name\\": \\"kwargs\\", \\"kind\\": \\"VAR_KEYWORD\\", \\"default\\": Parameter.empty, \\"annotation\\": Parameter.empty } ], \\"return_annotation\\": bool } ``` # Notes: - Make sure your implementation accurately reflects the correct parameter kinds and handles special case parameters like `*args` and `**kwargs`. - Utilize the `inspect` module to extract and format the required information.","solution":"import inspect def analyze_function_signature(func: callable) -> dict: Analyzes the function signature of the input function and returns a dictionary containing details about the function\'s parameters and return annotation. signature = inspect.signature(func) parameters_info = [] for name, param in signature.parameters.items(): parameters_info.append({ \\"name\\": name, \\"kind\\": param.kind.name, \\"default\\": param.default, \\"annotation\\": param.annotation }) analysis = { \\"parameters\\": parameters_info, \\"return_annotation\\": signature.return_annotation } return analysis"},{"question":"# Color Space Conversion Challenge Objective: You will demonstrate your understanding of color space transformations by implementing a function that verifies the reversibility and consistency of the color conversion functions in the \\"colorsys\\" module. Problem Statement: Write a function `validate_color_transformations(colors: List[Tuple[float, float, float]]) -> Dict[str, bool]` that takes a list of colors in RGB format and verifies if the conversion processes are reversible and consistent. Specifically, for each color in the list, your function should: 1. Convert the color to YIQ and back to RGB. 2. Convert the color to HLS and back to RGB. 3. Convert the color to HSV and back to RGB. 4. Verify that each conversion back to RGB yields the original color (or closely approximates it considering minor floating-point variations). Your function should return a dictionary indicating whether each transformation is consistent for the entire list of input colors. The keys of the dictionary should be `\'YIQ\'`, `\'HLS\'`, and `\'HSV\'`, and the values should be `True` if all colors are consistently reversible for that transformation, or `False` otherwise. Input: - `colors: List[Tuple[float, float, float]]`: A list of tuples, where each tuple `(r, g, b)` represents an RGB color. Each value is a floating-point number between 0 and 1. Output: - `Dict[str, bool]`: A dictionary where the keys are `\'YIQ\'`, `\'HLS\'`, and `\'HSV\'`, and the values are booleans indicating the consistency of each respective color space transformation. Example: ```python validate_color_transformations([(0.2, 0.4, 0.4), (0.8, 0.1, 0.7), (0.5, 0.5, 0.5)]) ``` Expected Output: ```python { \'YIQ\': True, \'HLS\': True, \'HSV\': True } ``` Constraints: - Each value in the RGB tuple will always be between 0 and 1. - The function should consider floating-point precision differences (i.e., results should be approximately close, but exact floating-point matches are not necessary). Performance: - The function should handle lists of colors efficiently, with up to 1000 colors. Notes: - You may use `math.isclose()` or any similar function to handle comparisons of floating-point values. - Use the `colorsys` module for the color space transformations.","solution":"import colorsys from typing import List, Tuple, Dict def validate_color_transformations(colors: List[Tuple[float, float, float]]) -> Dict[str, bool]: def is_approx_equal(color1, color2, tol=1e-6): return all(abs(c1 - c2) < tol for c1, c2 in zip(color1, color2)) yiq_consistent = True hls_consistent = True hsv_consistent = True for color in colors: # YIQ transformation yiq = colorsys.rgb_to_yiq(*color) rgb_from_yiq = colorsys.yiq_to_rgb(*yiq) if not is_approx_equal(color, rgb_from_yiq): yiq_consistent = False # HLS transformation hls = colorsys.rgb_to_hls(*color) rgb_from_hls = colorsys.hls_to_rgb(*hls) if not is_approx_equal(color, rgb_from_hls): hls_consistent = False # HSV transformation hsv = colorsys.rgb_to_hsv(*color) rgb_from_hsv = colorsys.hsv_to_rgb(*hsv) if not is_approx_equal(color, rgb_from_hsv): hsv_consistent = False return { \'YIQ\': yiq_consistent, \'HLS\': hls_consistent, \'HSV\': hsv_consistent }"},{"question":"You have been tasked with creating a mini HTML sanitizer and converter application in Python that uses the `html` module. This application should be able to sanitize input text by escaping HTML characters and then convert the sanitized text back to its original form by unescaping it. Your task is to implement the following two functions: 1. `sanitize_html(input_text: str, escape_quote: bool) -> str` - This function takes a string `input_text` and a boolean `escape_quote`. - It returns a string where all occurrences of `&`, `<`, and `>` in `input_text` are replaced with their HTML-safe sequences. - If `escape_quote` is `True`, it should also escape `\\"` and `\'`. 2. `convert_back(sanitized_text: str) -> str` - This function takes a string `sanitized_text`. - It returns a string where all HTML-safe sequences are replaced with their corresponding characters. Input and Output Formats 1. `sanitize_html(input_text: str, escape_quote: bool) -> str` - `input_text`: A string containing the text to be sanitized. - `escape_quote`: A boolean indicating whether to escape `\\"` and `\'`. - Returns a sanitized string. 2. `convert_back(sanitized_text: str) -> str` - `sanitized_text`: A string containing the sanitized HTML text. - Returns the original text. Example ```python # Example usage # Input text = \'This is a test <text> & \\"other\\" special characters!\' escape_quote = True # Function call sanitized = sanitize_html(text, escape_quote) print(sanitized) # Output: \'This is a test &lt;text&gt; &amp; &quot;other&quot; special characters!\' # Function call original = convert_back(sanitized) print(original) # Output: \'This is a test <text> & \\"other\\" special characters!\' ``` Constraints - The input text will contain at most 10,000 characters. - The functions should handle edge cases gracefully, such as empty strings and strings without any characters that need to be escaped or unescaped. Implement these functions in Python using the `html` module to demonstrate your understanding of HTML manipulation using the provided utilities.","solution":"import html def sanitize_html(input_text: str, escape_quote: bool) -> str: sanitized_text = html.escape(input_text, quote=escape_quote) return sanitized_text def convert_back(sanitized_text: str) -> str: original_text = html.unescape(sanitized_text) return original_text"},{"question":"**Coding Assessment: Managing and Profiling MPS Devices in PyTorch** **Objective:** Implement a series of functions that demonstrate your understanding of the `torch.mps` module by managing device states, handling random number states, and profiling operations on MPS devices. **Instructions:** 1. **Function `initialize_mps_device`** - Initialize the MPS device by setting a manual seed and emptying the cache. - Expected Input: - `seed_value` (int): The seed value to set for the RNG. - Expected Output: - None - Constraints: - Ensure that the cache is emptied after setting the seed value. 2. **Function `get_memory_stats`** - Get the current allocated memory, driver allocated memory, and recommended max memory for the current MPS device. - Expected Input: - None - Expected Output: - A dictionary with keys `current_allocated_memory`, `driver_allocated_memory`, and `recommended_max_memory` with their corresponding values in bytes. - Constraints: - Use the appropriate functions from the `torch.mps` module to retrieve the memory statistics. 3. **Function `profile_operations`** - Profile a given list of operations and return the profiling information. - Expected Input: - `operations` (list of callable functions): A list containing the operations to be profiled. - Expected Output: - A profiling report as a string. - Constraints: - Use the `torch.mps.profiler` functionalities to start and stop profiling. - Capture the profiling information for the provided operations. **Example Usage:** ```python # Example operations def op1(): # Some tensor operations on MPS device pass def op2(): # Some other tensor operations on MPS device pass # Initialize the MPS device initialize_mps_device(seed_value=12345) # Get current memory stats memory_stats = get_memory_stats() print(memory_stats) # Profile the given operations operations = [op1, op2] profiling_info = profile_operations(operations) print(profiling_info) ``` **Requirements:** - Your solution should handle potential exceptions and edge cases. - Ensure efficient usage of functions from the `torch.mps` module. **Grading Criteria:** - Correct implementation of initializing the MPS device. - Accurate retrieval of memory statistics. - Proper profiling of the given operations. - Code clarity and adherence to Python best practices.","solution":"import torch def initialize_mps_device(seed_value): Initialize the MPS device by setting a manual seed and emptying the cache. Parameters: seed_value (int): The seed value to set for the RNG. if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS device is not available.\\") torch.manual_seed(seed_value) torch.mps.empty_cache() def get_memory_stats(): Get the current allocated memory, driver allocated memory, and recommended max memory for the current MPS device. Returns: dict: A dictionary with keys `current_allocated_memory`, `driver_allocated_memory`, and `recommended_max_memory` with their corresponding values in bytes. if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS device is not available.\\") stats = { \\"current_allocated_memory\\": torch.mps.current_allocated_memory(), \\"driver_allocated_memory\\": torch.mps.driver_allocated_memory(), \\"recommended_max_memory\\": torch.mps.recommended_max_memory() } return stats def profile_operations(operations): Profile a given list of operations and return the profiling information. Parameters: operations (list of callable functions): A list containing the operations to be profiled. Returns: str: A profiling report as a string. if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS device is not available.\\") with torch.profiler.profile( activities=[ torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.MPS ], record_shapes=True ) as profiler: for operation in operations: if callable(operation): operation() else: raise ValueError(\\"All elements in the operations list must be callable functions.\\") return profiler.key_averages().table(sort_by=\\"cpu_time_total\\", row_limit=10)"},{"question":"Objective: To assess the student\'s ability to apply feature extraction techniques using scikit-learn\'s feature extraction module to preprocess a mixed dataset for a machine learning pipeline. Problem Statement: You are given a dataset containing both textual descriptions and metadata of various products. Your task is to preprocess this dataset to extract numerical features from the textual descriptions and metadata to feed into a machine learning model. # Dataset: The dataset is provided as a list of dictionaries, where each dictionary represents a product with the following structure: ```python products = [ {\'name\': \'Product 1\', \'description\': \'This is the first product\', \'category\': [\'electronics\', \'gadgets\'], \'price\': 249.99}, {\'name\': \'Product 2\', \'description\': \'Advanced product with multiple features\', \'category\': [\'appliances\'], \'price\': 399.99}, {\'name\': \'Product 3\', \'description\': \'Budget-friendly product for everyone\', \'category\': [\'furniture\'], \'price\': 89.99}, # More products... ] ``` Requirements: 1. Extract numerical features from the `description` using `TfidfVectorizer`. 2. Convert categorical metadata (`category`) and numerical metadata (`price`) using `DictVectorizer`. 3. Combine these features into a single feature matrix suitable for input to a machine learning model. 4. Write a function `preprocess_product_data(products)` that accepts the product list and returns a combined feature matrix. Function Signature: ```python def preprocess_product_data(products: list) -> np.ndarray: # Your implementation here pass ``` Solution Constraints: - Use `TfidfVectorizer` to process the `description` field. - Use `DictVectorizer` to process the `category` and `price` fields. - You can assume `scikit-learn` and `numpy` are available. - Ensure the function is efficient and handles the expected size of the data. Example: ```python products = [ {\'name\': \'Product 1\', \'description\': \'This is the first product\', \'category\': [\'electronics\', \'gadgets\'], \'price\': 249.99}, {\'name\': \'Product 2\', \'description\': \'Advanced product with multiple features\', \'category\': [\'appliances\'], \'price\': 399.99}, {\'name\': \'Product 3\', \'description\': \'Budget-friendly product for everyone\', \'category\': [\'furniture\'], \'price\': 89.99}, ] result = preprocess_product_data(products) print(result.shape) # Expected output: (3, <combined_feature_dimension>) ``` Additional Information: - `products` list may contain more than 3 products. - Combined feature dimension depends on the tf-idf vectorization and dict vectorization results.","solution":"import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import DictVectorizer def preprocess_product_data(products): # Extract descriptions descriptions = [product[\'description\'] for product in products] # Initialize TfidfVectorizer and fit-transform descriptions tfidf_vectorizer = TfidfVectorizer() tfidf_matrix = tfidf_vectorizer.fit_transform(descriptions) # Extracting and preparing categorical and numerical data for DictVectorizer metadata = [] for product in products: meta = { \'price\': product[\'price\'], } # Adding categories as binary features for category in product[\'category\']: meta[f\'category_{category}\'] = 1 metadata.append(meta) # Initialize DictVectorizer and fit-transform metadata dict_vectorizer = DictVectorizer() meta_matrix = dict_vectorizer.fit_transform(metadata) # Combine tfidf_matrix and meta_matrix combined_matrix = np.hstack((tfidf_matrix.toarray(), meta_matrix.toarray())) return combined_matrix"},{"question":"**Question: Advanced Sorting with Custom Key Functions** You are given a dataset containing information about employees in a company. Each employee\'s record is represented as a dictionary with the following fields: `name`, `age`, `department`, and `salary`. # Data Structure: ```python employees = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"department\\": \\"HR\\", \\"salary\\": 5000}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"department\\": \\"Engineering\\", \\"salary\\": 6000}, {\\"name\\": \\"Charlie\\", \\"age\\": 25, \\"department\\": \\"Marketing\\", \\"salary\\": 5500}, # More employee records... ] ``` # Task: Write a Python function that sorts this list of dictionaries based on multiple criteria provided as a list of tuples. Each tuple contains a key to sort by and a boolean indicating whether the sort should be in descending order. # Function Signature: ```python def sort_employees(employees: list, criteria: list) -> list: pass ``` # Input: - `employees` (list): A list of dictionaries, where each dictionary contains the keys `name`, `age`, `department`, and `salary`. - `criteria` (list): A list of tuples, where each tuple contains: - A string representing the key to sort by (`\'name\'`, `\'age\'`, `\'department\'`, or `\'salary\'`). - A boolean value indicating whether the sorting should be in descending order (`True` for descending, `False` otherwise). # Output: - Returns a new list of employee dictionaries, sorted based on the provided criteria. # Constraints: - The list of employees may contain any number of records (1 ≤ n ≤ 10^4). - The criteria list can contain multiple sorting keys, and stability should be maintained. - If a certain key is not specified in the criteria list, assume it does not affect the sorting order. # Example: ```python employees = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"department\\": \\"HR\\", \\"salary\\": 5000}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"department\\": \\"Engineering\\", \\"salary\\": 6000}, {\\"name\\": \\"Charlie\\", \\"age\\": 25, \\"department\\": \\"Marketing\\", \\"salary\\": 5500}, {\\"name\\": \\"David\\", \\"age\\": 30, \\"department\\": \\"Engineering\\", \\"salary\\": 7000} ] criteria = [(\\"department\\", False), (\\"salary\\", True)] sorted_employees = sort_employees(employees, criteria) print(sorted_employees) ``` # Expected Output: ```python [ {\'name\': \'Bob\', \'age\': 22, \'department\': \'Engineering\', \'salary\': 6000}, {\'name\': \'David\', \'age\': 30, \'department\': \'Engineering\', \'salary\': 7000}, {\'name\': \'Alice\', \'age\': 30, \'department\': \'HR\', \'salary\': 5000}, {\'name\': \'Charlie\', \'age\': 25, \'department\': \'Marketing\', \'salary\': 5500} ] ``` # Additional Notes: - Remember to maintain sort stability within the same department by salary. - Utilize `operator.attrgetter()` or custom key functions as necessary to achieve optimal performance and readability.","solution":"from operator import itemgetter def sort_employees(employees: list, criteria: list) -> list: Sorts a list of employee records based on multiple criteria. Args: employees (list): List of dictionaries where each dictionary contains employee information. criteria (list): List of tuples containing key to sort by and boolean indicating descending order. Returns: list: A new list of sorted employee dictionaries. if not criteria: return employees # Sort by criteria in reversed order to apply them in order of precedence for key, descending in reversed(criteria): employees.sort(key=itemgetter(key), reverse=descending) return employees"},{"question":"Objective: You are required to write a Python function that utilizes the `tempfile` module to perform a series of tasks involving temporary files and directories. This question aims to assess your understanding of file handling, context management, and the use of temporary files and directories in Python. Task: Implement a function `process_temp_files_and_directories` that does the following: 1. Creates a temporary file and writes the string `\\"Temporary File Content\\"` into it. 2. Creates a visible named temporary file and writes the string `\\"Named Temporary File Content\\"` into it. 3. Creates a spooled temporary file that holds data in memory, writes the string `\\"Spooled Temporary File Content\\"`, and ensures it is written to disk. 4. Creates a temporary directory and within that directory, creates a simple text file named `sample.txt` containing the text `\\"Temporary Directory File Content\\"`. Function Signature: ```python def process_temp_files_and_directories() -> dict: # Your code here ``` Output: The function should return a dictionary with the following keys and values: - `\\"temp_file_content\\"`: Content read from the temporary file. - `\\"named_temp_file_name\\"`: The file name of the named temporary file. - `\\"named_temp_file_content\\"`: Content read from the named temporary file. - `\\"spooled_temp_file_content\\"`: Content read from the spooled temporary file. - `\\"temp_dir_name\\"`: The name of the created temporary directory. - `\\"temp_dir_file_content\\"`: Content read from the file `sample.txt` inside the temporary directory. Constraints: - Use context managers to ensure all temporary resources are properly cleaned up when their context is exited. - All file operations should handle binary data for consistency. Example Usage: ```python result = process_temp_files_and_directories() assert result[\\"temp_file_content\\"] == b\\"Temporary File Content\\" assert result[\\"named_temp_file_content\\"] == b\\"Named Temporary File Content\\" assert result[\\"spooled_temp_file_content\\"] == b\\"Spooled Temporary File Content\\" assert os.path.basename(result[\\"named_temp_file_name\\"]).startswith(\\"tmp\\") assert \\"sample.txt\\" in os.listdir(result[\\"temp_dir_name\\"]) assert result[\\"temp_dir_file_content\\"] == \\"Temporary Directory File Content\\" ``` Additional Notes: - Ensure your solution handles exceptions gracefully. - Make sure the named temporary file allows for content validation by reading it before it is deleted. Now, implement the function `process_temp_files_and_directories` following the specifications above.","solution":"import tempfile import os def process_temp_files_and_directories() -> dict: result = {} # Create a temporary file and write into it with tempfile.TemporaryFile() as temp_file: temp_file.write(b\\"Temporary File Content\\") temp_file.seek(0) result[\\"temp_file_content\\"] = temp_file.read() # Create a named temporary file and write into it with tempfile.NamedTemporaryFile(delete=False) as named_temp_file: named_temp_file.write(b\\"Named Temporary File Content\\") named_temp_file_name = named_temp_file.name named_temp_file.seek(0) result[\\"named_temp_file_name\\"] = named_temp_file_name result[\\"named_temp_file_content\\"] = named_temp_file.read() # Create a spooled temporary file and write into it with tempfile.SpooledTemporaryFile() as spooled_temp_file: spooled_temp_file.write(b\\"Spooled Temporary File Content\\") spooled_temp_file.seek(0) result[\\"spooled_temp_file_content\\"] = spooled_temp_file.read() # Create a temporary directory and a file inside it with tempfile.TemporaryDirectory() as temp_dir: sample_file_path = os.path.join(temp_dir, \\"sample.txt\\") with open(sample_file_path, \'w\') as sample_file: sample_file.write(\\"Temporary Directory File Content\\") with open(sample_file_path, \'r\') as sample_file: result[\\"temp_dir_file_content\\"] = sample_file.read() result[\\"temp_dir_name\\"] = temp_dir # After exiting the context, named_temp_file needs to be deleted manually os.remove(named_temp_file_name) return result"},{"question":"**Objective**: Measure and compare the performance of different code snippets using the `timeit` module, and analyze the results to understand the differences in execution times. **Problem Statement**: You are tasked with comparing the performance of different methods to concatenate a list of 100 integers into a single string. Your task is to write a Python function that utilizes the `timeit` module to measure the execution time of the following three methods, and then analyze which method is the fastest and by how much. The methods to be compared are: 1. Using a generator expression with `str.join`. 2. Using a list comprehension with `str.join`. 3. Using `map` with `str.join`. **Function Signature**: ```python def compare_string_concatenation() -> None: pass ``` **Requirements**: 1. Use the `timeit.timeit()` method to measure the execution times. 2. Ensure that the code snippets are executed 10,000 times to obtain a reliable measurement. 3. Pass any required setup statements to the `setup` parameter in `timeit.timeit()`. 4. Print out the execution time for each method. 5. Print out the fastest method and the time difference between the fastest and the slowest methods. **Example Output**: ``` Generator expression time: 0.3018 seconds List comprehension time: 0.2727 seconds Map function time: 0.2370 seconds Fastest method: Map function Time difference between fastest and slowest: 0.0648 seconds ``` **Hints**: - Use the `timeit` module\'s `timeit()` function for timing. - Utilize the `setup` parameter in `timeit.timeit()` to initialize the list of 100 integers. - Format your print statements to display the execution times up to 4 decimal places. **Constraints**: - You should use the number of repetitions (`number`) as 10,000. - Ensure that the time reported by `timeit` function is accurate and the lowest value is considered closest to the true performance. This problem will test your understanding of using the `timeit` module to measure code performance accurately and analyze the results to determine the optimal code implementation.","solution":"import timeit def compare_string_concatenation(): setup_code = \\"nums = list(range(100))\\" gen_expr_code = \'\\"\\".join(str(num) for num in nums)\' list_comp_code = \'\\"\\".join([str(num) for num in nums])\' map_func_code = \'\\"\\".join(map(str, nums))\' gen_expr_time = timeit.timeit(gen_expr_code, setup=setup_code, number=10000) list_comp_time = timeit.timeit(list_comp_code, setup=setup_code, number=10000) map_func_time = timeit.timeit(map_func_code, setup=setup_code, number=10000) print(f\\"Generator expression time: {gen_expr_time:.4f} seconds\\") print(f\\"List comprehension time: {list_comp_time:.4f} seconds\\") print(f\\"Map function time: {map_func_time:.4f} seconds\\") fastest_method = min(gen_expr_time, list_comp_time, map_func_time) slowest_method = max(gen_expr_time, list_comp_time, map_func_time) if fastest_method == gen_expr_time: fastest_method_name = \\"Generator expression\\" elif fastest_method == list_comp_time: fastest_method_name = \\"List comprehension\\" else: fastest_method_name = \\"Map function\\" time_difference = slowest_method - fastest_method print(f\\"Fastest method: {fastest_method_name}\\") print(f\\"Time difference between fastest and slowest: {time_difference:.4f} seconds\\")"},{"question":"Objective: In this assignment, you are required to demonstrate your understanding of both validation and learning curves in the scikit-learn package. You will implement functions to plot these curves and analyze the results to diagnose a machine learning model. Problem Statement: Given a dataset, your task is to: 1. Plot the validation curve for a Support Vector Machine (SVM) model with a linear kernel and varying `C` parameter. 2. Plot the learning curve for the same SVM model with a linear kernel. 3. Based on the generated plots, provide an analysis of the model - stating whether it is overfitting, underfitting, or well-fitted to the dataset, and propose possible improvements. Requirements: 1. Implement the function `plot_validation_curve` - **Input**: - `X_train`: Training features as a numpy array. - `y_train`: Training targets as a numpy array. - `param_range`: Array of parameter values to evaluate for `C` in the SVM model. - **Output**: None (The function should plot the validation curve). 2. Implement the function `plot_learning_curve` - **Input**: - Same as above plus an additional parameter: - `train_sizes`: List of training set sizes to use in the learning curve. - **Output**: None (The function should plot the learning curve). 3. Write a small script `analyze_model` that: - Calls the above functions to generate the plots. - Analyzes the results and prints a brief summary indicating if the model is overfitting, underfitting, or well-fitted and suggests possible improvements. Here is a template to get you started: ```python import numpy as np from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve from sklearn.datasets import load_iris from sklearn.utils import shuffle import matplotlib.pyplot as plt def plot_validation_curve(X_train, y_train, param_range): train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X_train, y_train, param_name=\\"C\\", param_range=param_range ) train_scores_mean = np.mean(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) plt.figure() plt.plot(param_range, train_scores_mean, label=\'Training score\') plt.plot(param_range, valid_scores_mean, label=\'Validation score\') plt.title(\'Validation Curve\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.legend(loc=\'best\') plt.show() def plot_learning_curve(X_train, y_train, train_sizes): train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\\"linear\\"), X_train, y_train, train_sizes=train_sizes, cv=5 ) train_scores_mean = np.mean(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) plt.figure() plt.plot(train_sizes, train_scores_mean, label=\'Training score\') plt.plot(train_sizes, valid_scores_mean, label=\'Validation score\') plt.title(\'Learning Curve\') plt.xlabel(\'Training size\') plt.ylabel(\'Score\') plt.legend(loc=\'best\') plt.show() def analyze_model(): X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) param_range = np.logspace(-7, 3, 10) train_sizes = [50, 80, 110] plot_validation_curve(X, y, param_range) plot_learning_curve(X, y, train_sizes) # After looking at the plots, provide your analysis here: print(\\"Analysis:\\") # Example analysis text print(\\"The model appears to be...\\") if __name__ == \\"__main__\\": analyze_model() ``` Constraints: - Use `SVC` with a linear kernel. - Evaluate `C` with a range of `10 ** np.arange(-7, 4)`. - Use the Iris dataset for evaluation. Expected Output: - Two plots: Validation Curve and Learning Curve. - A printed analysis based on the generated plots.","solution":"import numpy as np from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve from sklearn.datasets import load_iris from sklearn.utils import shuffle import matplotlib.pyplot as plt def plot_validation_curve(X_train, y_train, param_range): train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X_train, y_train, param_name=\\"C\\", param_range=param_range, cv=5 ) train_scores_mean = np.mean(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) plt.figure() plt.plot(param_range, train_scores_mean, label=\'Training score\', marker=\'o\') plt.plot(param_range, valid_scores_mean, label=\'Validation score\', marker=\'o\') plt.title(\'Validation Curve\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.legend(loc=\'best\') plt.grid() plt.show() def plot_learning_curve(X_train, y_train, train_sizes): train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\\"linear\\"), X_train, y_train, train_sizes=train_sizes, cv=5 ) train_scores_mean = np.mean(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) plt.figure() plt.plot(train_sizes, train_scores_mean, label=\'Training score\', marker=\'o\') plt.plot(train_sizes, valid_scores_mean, label=\'Validation score\', marker=\'o\') plt.title(\'Learning Curve\') plt.xlabel(\'Training size\') plt.ylabel(\'Score\') plt.legend(loc=\'best\') plt.grid() plt.show() def analyze_model(): X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) param_range = np.logspace(-7, 3, 10) train_sizes = np.linspace(0.1, 1.0, 10) plot_validation_curve(X, y, param_range) plot_learning_curve(X, y, train_sizes) # Example analysis text print(\\"Analysis:\\") print(\\"From the validation curve, we observe that ...\\") print(\\"From the learning curve, we observe that ...\\") print(\\"Overall, the model seems to ... and we can improve it by ...\\") if __name__ == \\"__main__\\": analyze_model()"},{"question":"# Implement and Optimize a K-Means Clustering Algorithm Objective Your task is to implement a K-Means clustering algorithm, profile its performance, and optimize its bottlenecks. The goal of this assessment is to evaluate your proficiency with using scikit-learn, profiling tools, and optimization techniques. Instructions 1. **K-Means Implementation:** - Implement the K-Means clustering algorithm using scikit-learn and NumPy. - Your implementation should: - Accept input data as an `n x m` NumPy array where `n` is the number of data points and `m` is the number of features. - Accept the number of clusters, `k`, as a positive integer. - Return the cluster centroids and cluster labels for each data point. 2. **Performance Profiling:** - Profile your initial implementation to measure the time performance. - Identify the main bottlenecks using the profiling tools described in the document. 3. **Optimization:** - Optimize the identified bottlenecks in your implementation. - Employ strategies such as using efficient NumPy operations, Cython, or parallel processing with `joblib.Parallel`. - Re-profile the optimized implementation to ensure performance improvement. Input - A 2D NumPy array `X` of shape `(n_samples, n_features)`. - An integer `k` specifying the number of clusters. Output - A tuple containing: - A 2D NumPy array of shape `(k, n_features)` representing the final cluster centroids. - A 1D NumPy array of length `n_samples` representing the cluster labels for each data point. Constraints - You may assume the number of data points (n_samples) is large (e.g., >10000). - The number of features (n_features) can vary, ranging from 2 to 100. - Execution time should be minimized through optimization techniques. Example ```python import numpy as np # Example input X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]]) k = 2 # Example output centroids, labels = k_means_clustering(X, k) print(\\"Centroids:\\", centroids) print(\\"Labels:\\", labels) ``` Your implementation should provide an efficient clustering of the data points into `k` clusters, utilize appropriate optimization techniques to enhance performance, and demonstrate improved execution time through profiling. # Submission Submit your implementation along with profiling results before and after optimization. Ensure your code is well-documented and includes comments explaining your optimization choices.","solution":"import numpy as np from sklearn.cluster import KMeans def k_means_clustering(X, k): Perform K-Means clustering on the given dataset. Parameters: X (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the input data. k (int): The number of clusters. Returns: tuple: A tuple containing: - A 2D numpy array of shape (k, n_features) representing the final cluster centroids. - A 1D numpy array of length n_samples representing the cluster labels for each data point. kmeans = KMeans(n_clusters=k, random_state=42) kmeans.fit(X) return kmeans.cluster_centers_, kmeans.labels_"},{"question":"**Objective**: Implement a custom incremental encoder/decoder pair for a new encoding scheme named \\"custom-esc\\" where each character is replaced by its Unicode escape sequence (e.g., \'A\' becomes \'u0041\'). Additionally, implement a function to encode and decode a file using this custom encoding. Details: 1. **Custom Encoder and Decoder**: - Create a class `CustomEscEncoder` that extends `codecs.IncrementalEncoder`. - Implement the `encode()` method which encodes each character to its Unicode escape sequence. - Create a class `CustomEscDecoder` that extends `codecs.IncrementalDecoder`. - Implement the `decode()` method which decodes a Unicode escape sequence back to its character. 2. **File Encoding and Decoding Functionality**: - Implement a function `encode_file(input_file: str, output_file: str, encoding: str=\'custom-esc\', errors: str=\'strict\')` that reads content from `input_file`, encodes it using the specified `encoding`, and writes the encoded content to `output_file`. - Implement a function `decode_file(encoded_file: str, decoded_file: str, encoding: str=\'custom-esc\', errors: str=\'strict\')` that reads encoded content from `encoded_file`, decodes it using the specified `encoding`, and writes the decoded content to `decoded_file`. 3. **Registration of Custom Codec**: - Register the custom codec with the `codecs` module using `codecs.register()`. - Ensure the custom codec handles errors appropriately as specified by the `errors` parameter. Constraints: - The custom encoder should handle the entire Unicode range (U+0000-U+10FFFF). - Implement the solution with memory efficiency in mind, particularly for handling large files. - The solution should be compatible with Python 3.10. Expected Input and Output: - **Input**: Content of text files for encoding and decoding. - **Output**: Encoded content in `output_file` and decoded content in `decoded_file`. Example: ```python # Suppose the content of \'input.txt\' is \'Hello, World!\' encode_file(\'input.txt\', \'encoded.txt\', encoding=\'custom-esc\') # The content of \'encoded.txt\' should be \'u0048u0065u006cu006cu006fu002cu0020u0057u006fu0072u006cu0064u0021\' decode_file(\'encoded.txt\', \'decoded.txt\', encoding=\'custom-esc\') # The content of \'decoded.txt\' should be \'Hello, World!\' ``` Notes: - You can assume the input files contain only valid Unicode characters. - Provide a brief explanation of your implementation, including any decisions made regarding error handling and performance optimizations.","solution":"import codecs class CustomEscEncoder(codecs.IncrementalEncoder): def encode(self, input, final=False): # Encode each character in the input string to its Unicode escape sequence return \'\'.join(f\'u{ord(char):04x}\' for char in input) class CustomEscDecoder(codecs.IncrementalDecoder): def decode(self, input, final=False): # Decode the Unicode escape sequences back to characters return codecs.decode(input, \'unicode_escape\') def encode_file(input_file: str, output_file: str, encoding: str=\'custom-esc\', errors: str=\'strict\'): with open(input_file, \'r\', encoding=\'utf-8\') as f_in, open(output_file, \'w\', encoding=\'utf-8\') as f_out: content = f_in.read() encoder = CustomEscEncoder() encoded_content = encoder.encode(content, final=True) f_out.write(encoded_content) def decode_file(encoded_file: str, decoded_file: str, encoding: str=\'custom-esc\', errors: str=\'strict\'): with open(encoded_file, \'r\', encoding=\'utf-8\') as f_in, open(decoded_file, \'w\', encoding=\'utf-8\') as f_out: content = f_in.read() decoder = CustomEscDecoder() decoded_content = decoder.decode(content, final=True) f_out.write(decoded_content) def custom_esc_search(name): if name == \'custom-esc\': return codecs.CodecInfo( name=\'custom-esc\', encode=CustomEscEncoder().encode, decode=CustomEscDecoder().decode, incrementalencoder=CustomEscEncoder, incrementaldecoder=CustomEscDecoder, ) return None # Register the custom codec codecs.register(custom_esc_search)"},{"question":"# Seaborn Rugplot Customization and Analysis Using the knowledge acquired from the provided documentation on the Seaborn `rugplot` function, answer the following question to demonstrate your understanding. Problem Statement You are provided with two datasets available in Seaborn\'s library: - The **tips** dataset - The **diamonds** dataset Write a Python function `customized_visualization` that performs the following tasks: 1. **Scatter plot with rugplot for tips dataset**: - Create a scatter plot of `total_bill` versus `tip`. - Add a rug plot along the `total_bill` axis. - Make the height of the rug `.1`. - Represent different times (`time` column) with different hues. 2. **Density representation for diamonds dataset**: - Create a scatter plot of `carat` versus `price` using smaller points for better visualization. - Add a rug plot along the `carat` axis. - Make the lines of the rug thinner (`lw=1`) and apply alpha blending (`alpha=.005`). Input - There is no direct user input needed; the datasets should be loaded within the function. Output - The function should display the two plots described above. Constraints - Use the datasets as given, without altering their contents. - Explicitly set the aesthetics as specified in the tasks. Here is the skeleton of the function for you to complete: ```python import seaborn as sns def customized_visualization(): # Load datasets tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # Task 1: Scatter plot with rugplot for tips dataset sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", height=.1) # Display the plot sns.plt.show() # Task 2: Density representation for diamonds dataset sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5) sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", lw=1, alpha=.005) # Display the plot sns.plt.show() # Sample function call customized_visualization() ``` Complete the function implementation and ensure it produces the two required visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_visualization(): # Load datasets tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # Task 1: Scatter plot with rugplot for tips dataset plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", height=.1) plt.title(\'Scatter plot of total_bill vs tip with rugplot (tips dataset)\') plt.show() # Task 2: Density representation for diamonds dataset plt.figure(figsize=(10, 6)) sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5) sns.rugplot(data=diamonds, x=\\"carat\\", lw=1, alpha=.005) plt.title(\'Scatter plot of carat vs price with rugplot (diamonds dataset)\') plt.show() # Sample function call customized_visualization()"},{"question":"# Asynchronous News Fetcher **Objective**: You are tasked with implementing an asynchronous news fetcher that collects news articles from multiple sources concurrently. Some of the fetching operations are CPU-intensive and must be handled without blocking the event loop. **Requirements**: 1. Implement an asynchronous function `fetch_news_source(source_id)` that simulates fetching news from a given source ID. This function should: - Take an `int` parameter `source_id`. - Asynchronously sleep for a duration proportional to `source_id` (e.g., `await asyncio.sleep(source_id % 3)`). - Return a dictionary with `source_id` and a dummy news article. 2. Implement an asynchronous function `fetch_all_news(num_sources)` that: - Takes an `int` parameter `num_sources` representing the number of news sources. - Uses `fetch_news_source` to fetch news from all sources concurrently. - Returns a list of news articles. - Ensures no event loop blocking for more than 1 second (consider using `asyncio.gather` or `asyncio.create_task`). 3. Implement a function `fetch_news_with_cpu_bound_task(num_sources)` that: - Takes an `int` parameter `num_sources`. - Uses an executor to run a dummy CPU-bound task (`cpu_bound_task(source_id)`) that squares `source_id`. - Ensures that the CPU-bound task does not block the event loop. - Returns a list of results from the CPU-bound tasks. 4. Ensure any exceptions are properly handled. If a source fails to fetch news, it should be noted in the results with an appropriate error message. 5. Demonstrate the use of asyncio\'s debug mode to help identify potential issues. **Input**: - An integer `num_sources`. **Output**: - A list of news articles and/or error messages from fetching tasks. - A list of results from CPU-bound tasks. **Example**: ```python import asyncio async def fetch_news_source(source_id): # Your implementation here async def fetch_all_news(num_sources): # Your implementation here async def cpu_bound_task(source_id): return source_id ** 2 def fetch_news_with_cpu_bound_task(num_sources): asyncio.run(main(num_sources), debug=True) async def main(num_sources): news_articles = await fetch_all_news(num_sources) cpu_results = await fetch_news_with_cpu_bound_task(num_sources) # Print or return final results if __name__ == \\"__main__\\": fetch_news_with_cpu_bound_task(5) ``` **Constraints**: - Assume `num_sources` ≤ 10 for demonstration purposes. - Demonstrate the proper usage of debugging and logging as explained in the documentation. **Performance Requirements**: - Ensure that the event loop is not blocked for more than necessary when handling CPU-bound tasks. - Properly utilize `asyncio` features to manage concurrent operations efficiently.","solution":"import asyncio from concurrent.futures import ThreadPoolExecutor import logging logging.basicConfig(level=logging.DEBUG) async def fetch_news_source(source_id): Asynchronously fetches news from a given source. Simulates a delay proportional to the source_id and returns a dummy article. try: await asyncio.sleep(source_id % 3) return {\\"source_id\\": source_id, \\"article\\": f\\"News from source {source_id}\\"} except Exception as e: return {\\"source_id\\": source_id, \\"error\\": str(e)} async def fetch_all_news(num_sources): Asynchronously fetches news from all sources concurrently. Returns a list of news articles or errors. tasks = [fetch_news_source(i) for i in range(num_sources)] results = await asyncio.gather(*tasks, return_exceptions=True) return results def cpu_bound_task(source_id): Dummy CPU-bound task that squares the source_id. return source_id ** 2 async def fetch_news_with_cpu_bound_task(num_sources): Uses an executor to run a dummy CPU-bound task for each source_id. Ensures the CPU-bound task does not block the event loop. Returns a list of results from CPU-bound tasks. loop = asyncio.get_running_loop() with ThreadPoolExecutor() as pool: tasks = [loop.run_in_executor(pool, cpu_bound_task, i) for i in range(num_sources)] results = await asyncio.gather(*tasks) return results async def main(num_sources): Main function to fetch news and execute CPU-bound tasks concurrently. news_articles = await fetch_all_news(num_sources) cpu_results = await fetch_news_with_cpu_bound_task(num_sources) return news_articles, cpu_results def fetch_news_with_cpu_bound_task_wrapper(num_sources): asyncio.run(main(num_sources), debug=True) if __name__ == \\"__main__\\": fetch_news_with_cpu_bound_task_wrapper(5)"},{"question":"Multi-Class SVM with Custom Kernel and Cross-Validation Objective Create a Support Vector Machine (SVM) pipeline for multi-class classification using a custom kernel. You are required to implement a custom polynomial kernel, utilize cross-validation for hyperparameter tuning, and evaluate the classifier\'s performance. Details 1. **Custom Kernel Function**: Implement a polynomial kernel function of degree 3. ```python def custom_polynomial_kernel(X, Y, degree=3): Custom polynomial kernel function. Parameters: X (ndarray): First input matrix of shape (n_samples_1, n_features) Y (ndarray): Second input matrix of shape (n_samples_2, n_features) degree (int): The degree of the polynomial kernel. Returns: ndarray: The kernel matrix of shape (n_samples_1, n_samples_2) # Implement the custom polynomial kernel using numpy functions pass ``` 2. **Pipeline Creation**: Create a pipeline which scales the data and applies the SVC classifier with the custom polynomial kernel. 3. **Cross-Validation and Hyperparameter Tuning**: Use `GridSearchCV` to perform 5-fold cross-validation on the pipeline. Tune the `C` parameter of the SVM over a specified range (1, 10, 100). 4. **Data**: Use the Iris dataset for this task. 5. **Performance Evaluation**: Output the best hyperparameters and evaluate the classifier\'s performance on a held-out test set. Implementation 1. **Load the Iris Dataset**: Use `sklearn.datasets.load_iris`. 2. **Define the Custom Kernel**: Implement the `custom_polynomial_kernel` function. 3. **Create SVM Pipeline**: Use `StandardScaler` for scaling followed by `SVC` with the custom kernel. 4. **Cross-Validation**: Use `GridSearchCV` to perform cross-validation and tune the `C` hyperparameter. 5. **Evaluate Performance**: Train the model using the best found parameters and evaluate its performance. Code Template for Implementation ```python import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import classification_report # Load Iris dataset data = load_iris() X, y = data.data, data.target # Define custom polynomial kernel function def custom_polynomial_kernel(X, Y, degree=3): Custom polynomial kernel function. Parameters: X (ndarray): First input matrix of shape (n_samples_1, n_features) Y (ndarray): Second input matrix of shape (n_samples_2, n_features) degree (int): The degree of the polynomial kernel. Returns: ndarray: The kernel matrix of shape (n_samples_1, n_samples_2) return (1 + np.dot(X, Y.T)) ** degree # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create an SVM pipeline with custom kernel svc = SVC(kernel=custom_polynomial_kernel) pipeline = make_pipeline(StandardScaler(), svc) # Hyperparameter tuning with cross-validation param_grid = {\'svc__C\': [1, 10, 100]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best hyperparameters print(f\\"Best parameters: {grid_search.best_params_}\\") # Evaluate performance on the test set y_pred = grid_search.predict(X_test) print(classification_report(y_test, y_pred)) ``` Expected Output 1. **Best hyperparameters**: The optimal value of `C`. 2. **Classification Report**: Precision, recall, f1-score for each class, and overall accuracy on the test set. Constraints and Assumptions - You can assume the Iris dataset is balanced and has a fixed number of samples (150). - Use a polynomial kernel of degree 3 for this task. - Use a random seed of 42 for reproducibility.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import classification_report # Load Iris dataset data = load_iris() X, y = data.data, data.target # Define custom polynomial kernel function def custom_polynomial_kernel(X, Y, degree=3): Custom polynomial kernel function. Parameters: X (ndarray): First input matrix of shape (n_samples_1, n_features) Y (ndarray): Second input matrix of shape (n_samples_2, n_features) degree (int): The degree of the polynomial kernel. Returns: ndarray: The kernel matrix of shape (n_samples_1, n_samples_2) return (1 + np.dot(X, Y.T)) ** degree # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create an SVM pipeline with custom kernel svc = SVC(kernel=custom_polynomial_kernel) pipeline = make_pipeline(StandardScaler(), svc) # Hyperparameter tuning with cross-validation param_grid = {\'svc__C\': [1, 10, 100]} grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Best hyperparameters best_params = grid_search.best_params_ print(f\\"Best parameters: {best_params}\\") # Evaluate performance on the test set y_pred = grid_search.predict(X_test) performance_report = classification_report(y_test, y_pred) print(performance_report)"},{"question":"**Question: Mailbox Interaction Script** You have been tasked to create a Python script using the `poplib` module that connects to a POP3 mail server, fetches a list of emails, and performs some basic operations. The script should do the following: 1. **Connect to a POP3 server**: - Use the `poplib.POP3` class. - The connection should be established using the parameters `host` and `port` (default is 110). 2. **Authenticate the user**: - Prompt the user to enter their POP3 username and password. 3. **Fetch and display mailbox statistics**: - Use the `stat()` method to get the number of messages and mailbox size. 4. **List all messages**: - Use the `list()` method to get the list of all messages in the format of \\"message number\\" and \\"octet size\\". - Print this list to the screen. 5. **Retrieve and print the first email**: - Use the `retr()` method to retrieve the first email. - Print the content of the first email to the screen. 6. **Delete the first email**: - Use the `dele()` method to flag the first email for deletion (but do not actually delete it before quitting). 7. **Quit the session**: - Use the `quit()` method to end the session, which commits the deletion if any. *Input Format*: - The script should prompt the user to input the hostname, username, and password. *Output Format*: - The script should output the mailbox statistics. - The script should print the list of all messages. - The script should print the first email\'s content. *Constraints*: - Ensure the script handles any exceptions that might occur during connection or operations (such as incorrect login, network issues, etc.) - Use proper error handling to provide meaningful messages to the user. **Performance Requirements**: - The script should handle the operations efficiently. **Example**: ```python # Example of script usage # Host: \'pop3.example.com\' # Username: \'user@example.com\' # Password: \'password\' # Output should look like this: # Mailbox has 3 messages, total size is 10240 octets. # Message list: # 1 2048 # 2 2048 # 3 6144 # First email content: # From: admin@example.com # To: user@example.com # Subject: Test email # Date: Thu, 1 Apr 2021 12:00:00 +0000 # Content-Type: text/plain; charset=\\"UTF-8\\" # This is a test email. ``` Implement the script in the cell below: ```python import getpass import poplib def fetch_emails(): host = input(\\"Enter POP3 host: \\") port = 110 # default POP3 port username = input(\\"Enter username: \\") password = getpass.getpass(\\"Enter password: \\") try: # Connect to the server mail_server = poplib.POP3(host, port) mail_server.user(username) mail_server.pass_(password) # Fetch mailbox statistics num_messages, mailbox_size = mail_server.stat() print(f\\"Mailbox has {num_messages} messages, total size is {mailbox_size} octets.\\") # List all messages response, listings, octets = mail_server.list() print(\\"Message list:\\") for listing in listings: print(listing.decode(\'utf-8\')) # Retrieve and print the first email response, lines, octets = mail_server.retr(1) print(\\"First email content:\\") for line in lines: print(line.decode(\'utf-8\')) # Flag the first email for deletion mail_server.dele(1) # Quit the session mail_server.quit() except poplib.error_proto as e: print(f\\"POP3 protocol error: {e}\\") except Exception as exc: print(f\\"An error occurred: {exc}\\") # Run the function fetch_emails() ```","solution":"import getpass import poplib def fetch_emails(): host = input(\\"Enter POP3 host: \\") port = 110 # Default POP3 port username = input(\\"Enter username: \\") password = getpass.getpass(\\"Enter password: \\") try: # Connect to the server mail_server = poplib.POP3(host, port) mail_server.user(username) mail_server.pass_(password) # Fetch mailbox statistics num_messages, mailbox_size = mail_server.stat() print(f\\"Mailbox has {num_messages} messages, total size is {mailbox_size} octets.\\") # List all messages response, listings, octets = mail_server.list() print(\\"Message list:\\") for listing in listings: print(listing.decode(\'utf-8\')) # Retrieve and print the first email response, lines, octets = mail_server.retr(1) print(\\"First email content:\\") for line in lines: print(line.decode(\'utf-8\')) # Flag the first email for deletion mail_server.dele(1) # Quit the session mail_server.quit() except poplib.error_proto as e: print(f\\"POP3 protocol error: {e}\\") except Exception as exc: print(f\\"An error occurred: {exc}\\") # Run the function if __name__ == \\"__main__\\": fetch_emails()"},{"question":"**Objective**: This question will assess your understanding of the Cell object in Python and your ability to use associated functions to handle variables across different scopes. **Question**: You are required to implement a class `CellManager` that simulates the behavior of cell objects using the functions described in the documentation. The class should have the following methods: 1. **`__init__(self, initial_value)`**: Initializes the object with a cell containing `initial_value`. 2. **`get_value(self)`**: Returns the current value stored in the cell. 3. **`set_value(self, new_value)`**: Sets the cell\'s value to `new_value`. 4. **`is_cell_object(self)`**: Checks if the current object is a cell object. # Implementation Details 1. **Input and Output Formats**: - `__init__(self, initial_value)`: Initializes with `initial_value` (can be of any type). - `get_value(self)`: Returns the current value stored in the cell. - `set_value(self, new_value)`: Sets the cell’s value to `new_value`. - `is_cell_object(self)`: Returns `True` if the object is a cell object, `False` otherwise. 2. **Constraints**: - The `initial_value` and `new_value` can be of any Python data type. - For the purpose of this question, assume that all objects being managed are valid cell objects. 3. **Performance Requirements**: - Ensure that the get and set operations are performed in constant time. # Example Usage ```python # Create an instance with an initial value cell_manager = CellManager(10) # Check if it is a cell object print(cell_manager.is_cell_object()) # Output: True # Retrieve the current value print(cell_manager.get_value()) # Output: 10 # Set a new value cell_manager.set_value(20) # Retrieve the updated value print(cell_manager.get_value()) # Output: 20 ``` # Implementation Constraints: - You are only allowed to use the functions provided in the documentation (e.g., `PyCell_Check`, `PyCell_New`, `PyCell_Get`, `PyCell_SET`). - Your implementation should effectively manage the creation, value retrieval, and value setting of cell objects. **Note**: Ensure you handle any necessary reference counts and safety checks as described in the functions’ behaviors. **Hint**: You may use the `ctypes` library in Python to interface with C-level operations if needed.","solution":"import ctypes class CellManager: def __init__(self, initial_value): self.cell = ctypes.py_object(initial_value) def get_value(self): return self.cell.value def set_value(self, new_value): self.cell.value = new_value def is_cell_object(self): return isinstance(self.cell, ctypes.py_object)"},{"question":"**Advanced Iterator Handling in Python** You are given an implementation task that involves handling iterators and asynchronous iterators using Python\'s C API functions. Your goal is to create a Python extension module that provides the following functionalities: 1. **is_iterator**: A function that takes an object and returns `True` if it is an iterator, otherwise `False`. 2. **is_async_iterator**: A function that takes an object and returns `True` if it implements the async iterator protocol, otherwise `False`. 3. **next_item**: A function that takes an iterator and returns the next item. If there are no more items, it should return `None`. 4. **send_value**: A function that takes an iterator, a value, and sends the value into the iterator. # Specifications 1. **is_iterator(obj) -> bool** - **Input**: `obj` (any Python object) - **Output**: `True` if `obj` is an iterator as per `PyIter_Check`, otherwise `False`. 2. **is_async_iterator(obj) -> bool** - **Input**: `obj` (any Python object) - **Output**: `True` if `obj` is an async iterator as per `PyAIter_Check`, otherwise `False`. 3. **next_item(iterator) -> Any** - **Input**: `iterator` (an iterator object) - **Output**: The next item from the iterator, or `None` if there are no items left. 4. **send_value(iterator, value) -> Union[Any, None]** - **Input**: `iterator` (an iterator object), `value` (any Python object) - **Output**: The yielded or returned value from the iterator, handling exceptions as appropriate. # Constraints - You must handle errors and edge cases appropriately. - You may assume the input `iterator` for `next_item` and `send_value` is always a valid iterator object if the function `is_iterator` or `is_async_iterator` returns `True` prior to calling these functions. - Performance considerations should allow expected iterator operations to complete in linear time, relative to their length. # Example Usage ```python import my_iterator_module # is_iterator print(my_iterator_module.is_iterator(iter([]))) # True print(my_iterator_module.is_iterator([])) # False # is_async_iterator print(my_iterator_module.is_async_iterator(an_async_iter)) # Assume `an_async_iter` is an async iterator # True print(my_iterator_module.is_async_iterator([])) # False # next_item it = iter([1, 2, 3]) print(my_iterator_module.next_item(it)) # 1 print(my_iterator_module.next_item(it)) # 2 print(my_iterator_module.next_item(it)) # 3 print(my_iterator_module.next_item(it)) # None # send_value (Assuming `gen` is some generator that can accept values) gen = some_generator() # Define a generator function that can handle send values print(my_iterator_module.send_value(gen, 10)) # Next yielded or returned value ``` You may consider looking into the `ctypes` or `cffi` library for creating the Python extension, though the choice of tools beyond the standard Python C API is up to you.","solution":"import collections.abc import inspect def is_iterator(obj): Returns True if obj is an iterator, otherwise False. return isinstance(obj, collections.abc.Iterator) def is_async_iterator(obj): Returns True if obj implements the async iterator protocol, otherwise False. return inspect.isasyncgen(obj) or isinstance(obj, collections.abc.AsyncIterator) def next_item(iterator): Returns the next item from the iterator. If there are no more items, return None. try: return next(iterator) except StopIteration: return None def send_value(iterator, value): Sends a value into the iterator and returns the yielded or returned value. try: return iterator.send(value) except StopIteration: return None except TypeError: # If \'send\' is not applicable for the iterator, use \'next\' try: return next(iterator) except StopIteration: return None"},{"question":"# Coding Assessment: Managing Backend Configuration in PyTorch Objective: To assess your understanding of backend configurations in PyTorch, this task requires you to perform operations that involve both CUDA and cuDNN backends. Task Description: You are required to implement a class `BackendConfigurator` that can dynamically manage configurations for CUDA and cuDNN backends. This class should allow the following functionalities: 1. **Check Available Backends**: - Implement a method `check_backends` that returns a dictionary indicating whether CUDA and cuDNN are available on the system. 2. **Toggle TensorFloat-32 Option**: - Implement methods `enable_tf32` and `disable_tf32` that respectively enable and disable TensorFloat-32 tensor core usage for CUDA. 3. **Configure cuFFT Plan Cache**: - Implement a method `configure_cufft_cache` that takes two parameters: `device_index` and `max_size`. This method should set the maximum size of the cuFFT plan cache for the specified CUDA device. 4. **Set cuDNN Deterministic Mode**: - Implement methods `enable_cudnn_deterministic` and `disable_cudnn_deterministic` that respectively enable and disable deterministic mode for cuDNN. 5. **Perform a Matrix Multiplication**: - Implement a method `matmul` that performs a matrix multiplication on CUDA if available. If CUDA is not available, use CPU. Constraints: - You need to ensure that your code appropriately handles the scenarios when CUDA and/or cuDNN are not available. - Use PyTorch\'s built-in functions and attributes to manage backend configurations. Class and Method Signatures: ```python class BackendConfigurator: def check_backends(self) -> dict: Returns: A dictionary with keys \'cuda\' and \'cudnn\', indicating their availability. pass def enable_tf32(self): Enables TensorFloat-32 tensor cores for CUDA. pass def disable_tf32(self): Disables TensorFloat-32 tensor cores for CUDA. pass def configure_cufft_cache(self, device_index: int, max_size: int): Configures the maximum size of the cuFFT plan cache for a given device. Args: device_index (int): The index of the CUDA device. max_size (int): The maximum cache size to be set. pass def enable_cudnn_deterministic(self): Enables deterministic mode for cuDNN. pass def disable_cudnn_deterministic(self): Disables deterministic mode for cuDNN. pass def matmul(self, tensor_a, tensor_b): Performs a matrix multiplication using CUDA if available, otherwise falls back to CPU. Args: tensor_a (torch.Tensor): The first input tensor. tensor_b (torch.Tensor): The second input tensor. Returns: torch.Tensor: The result of the matrix multiplication. pass ``` Example Usage: ```python configurator = BackendConfigurator() # Check available backends print(configurator.check_backends()) # Enable TensorFloat-32 for CUDA configurator.enable_tf32() # Configure cuFFT cache configurator.configure_cufft_cache(device_index=0, max_size=4096) # Set max size to 4096 for device 0 # Set cuDNN to deterministic mode configurator.enable_cudnn_deterministic() # Perform matrix multiplication tensor_a = torch.randn(3, 3) tensor_b = torch.randn(3, 3) result = configurator.matmul(tensor_a, tensor_b) print(result) ``` Evaluation Criteria: - Correctness: Ensure that the methods correctly perform the described functionalities. - Robustness: Handle cases where CUDA or cuDNN are not available without causing errors. - Efficiency: Optimize settings configurations for performance.","solution":"import torch class BackendConfigurator: def check_backends(self) -> dict: Returns: A dictionary with keys \'cuda\' and \'cudnn\', indicating their availability. return { \'cuda\': torch.cuda.is_available(), \'cudnn\': torch.backends.cudnn.is_available() } def enable_tf32(self): Enables TensorFloat-32 tensor cores for CUDA. if torch.cuda.is_available(): torch.backends.cuda.matmul.allow_tf32 = True def disable_tf32(self): Disables TensorFloat-32 tensor cores for CUDA. if torch.cuda.is_available(): torch.backends.cuda.matmul.allow_tf32 = False def configure_cufft_cache(self, device_index: int, max_size: int): Configures the maximum size of the cuFFT plan cache for a given device. Args: device_index (int): The index of the CUDA device. max_size (int): The maximum cache size to be set. if torch.cuda.is_available() and torch.cuda.device_count() > device_index: torch.cuda.set_device(device_index) torch.backends.cuda.cufft_plan_cache.max_size = max_size def enable_cudnn_deterministic(self): Enables deterministic mode for cuDNN. if torch.backends.cudnn.is_available(): torch.backends.cudnn.deterministic = True def disable_cudnn_deterministic(self): Disables deterministic mode for cuDNN. if torch.backends.cudnn.is_available(): torch.backends.cudnn.deterministic = False def matmul(self, tensor_a, tensor_b): Performs a matrix multiplication using CUDA if available, otherwise falls back to CPU. Args: tensor_a (torch.Tensor): The first input tensor. tensor_b (torch.Tensor): The second input tensor. Returns: torch.Tensor: The result of the matrix multiplication. if torch.cuda.is_available(): tensor_a = tensor_a.to(\'cuda\') tensor_b = tensor_b.to(\'cuda\') result = torch.matmul(tensor_a, tensor_b) return result.to(\'cpu\') else: return torch.matmul(tensor_a, tensor_b)"},{"question":"**Coding Assessment Question** # URL Content Fetcher with Error Handling and Custom Headers **Objective:** You are required to demonstrate your understanding of Python\'s `urllib` package by writing a function that fetches the content of a given URL. This function must handle HTTP errors gracefully, support adding custom headers to the request, and provide detailed log information about the request-response cycle. **Task:** Implement a function called `fetch_url_content` that accepts the following parameters: - `url` (str): The URL of the resource to be fetched. - `headers` (dict): A dictionary of HTTP headers to include in the request (default is an empty dictionary). - `timeout` (int): The time (in seconds) to wait for a response before timing out (default is 10 seconds). The function should perform the following: 1. Make an HTTP GET request to the specified URL using the given headers and timeout. 2. Handle the following HTTP errors: - `404 Not Found`: Log an appropriate message. - `403 Forbidden`: Log an appropriate message. - `500 Internal Server Error`: Log an appropriate message. - Other HTTP Errors: Log the status code and appropriate message. 3. Catch network-related errors and log a suitable message. 4. Return the content of the URL if the request is successful. 5. Log the real URL fetched and the headers received in the response. **Constraints:** - You must use `urllib.request.urlopen` for making the HTTP request. - Use `urllib.error.URLError` and `urllib.error.HTTPError` for handling exceptions. - Detailed log messages should be printed to the console. **Input:** - `url`: A string representing the URL. - `headers`: A dictionary where keys and values are strings, representing HTTP headers. - `timeout`: An integer for timeout duration in seconds. **Output:** - A string representing the content of the URL if the request is successful. - In case of an error, print appropriate log messages but do not return any content. **Example:** ```python def fetch_url_content(url, headers={}, timeout=10): import urllib.request import urllib.error import socket # YOUR CODE HERE # Example usage: url = \\"http://example.com\\" headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} content = fetch_url_content(url, headers=headers, timeout=5) print(content) ``` - When `url` is `\\"http://example.com\\"`, `headers` is `{\\"User-Agent\\": \\"Mozilla/5.0\\"}`, and `timeout` is `5`, if the request is successful, the function should print the content of the page. If an error occurs, it should print the appropriate log message(s).","solution":"def fetch_url_content(url, headers={}, timeout=10): import urllib.request import urllib.error import socket request = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(request, timeout=timeout) as response: real_url = response.geturl() response_headers = dict(response.info()) content = response.read().decode(\'utf-8\') print(f\\"Real URL fetched: {real_url}\\") print(f\\"Response Headers: {response_headers}\\") return content except urllib.error.HTTPError as e: if e.code == 404: print(\\"Error 404: Not Found\\") elif e.code == 403: print(\\"Error 403: Forbidden\\") elif e.code == 500: print(\\"Error 500: Internal Server Error\\") else: print(f\\"HTTP Error: {e.code}\\") except urllib.error.URLError as e: print(f\\"URL Error: {e.reason}\\") except socket.timeout: print(\\"Error: Request timed out\\") return None"},{"question":"File Processing with Memory-Mapped Files Objective: Your task is to write a Python program that processes a large text file using the `mmap` module. The program should be able to read the file, perform modifications, and write the changes back to the file. Specifically, your program should satisfy the following requirements: 1. **Read the contents of a file.** 2. **Replace all occurrences of a specific word with another word.** 3. **Write the modified contents back to the file.** Detailed Requirements: 1. **Function Signature**: ```python def replace_word_in_file(filename: str, search_word: bytes, replacement_word: bytes) -> None: ``` 2. **Input**: - `filename` (str): The name of the file to be processed. - `search_word` (bytes): The word to be replaced. - `replacement_word` (bytes): The word to replace `search_word` with. **Constraints**: - The replacement word must be of the same length as the search word to maintain the file size. 3. **Output**: - This function should not return any value. The result should be reflected in the modified file. 4. **Behavior**: - Open the file in read-write mode. - Use the `mmap` module to create a memory-mapped object for the entire file. - Find and replace all occurrences of `search_word` with `replacement_word`. - Write the modified contents back to the file using the memory-mapped object. 5. **Example**: ```python # Given a file named \'example.txt\' containing \\"Hello World! This is a simple example of memory mapping.\\", # calling replace_word_in_file(\\"example.txt\\", b\\"example\\", b\\"test \\") (note the spaces) should modify the file to contain # \\"Hello World! This is a simple test of memory mapping.\\" ``` Implementation Notes: - You are required to use the `mmap` module for reading and writing the file content. - Handle file closing and memory unmapping appropriately using context managers. Constraints: - The length of `replacement_word` must be the same as `search_word`. - The file size will not exceed 100 MB to ensure feasible memory mapping operations. - You may assume the file encoding is ASCII. Performance Requirements: - The solution should efficiently handle large files without loading the entire content into memory simultaneously (leveraging the `mmap` module\'s capabilities). Write your solution below: ```python import mmap def replace_word_in_file(filename: str, search_word: bytes, replacement_word: bytes) -> None: if len(search_word) != len(replacement_word): raise ValueError(\\"Replacement word must be of the same length as the search word.\\") with open(filename, \\"r+b\\") as f: with mmap.mmap(f.fileno(), 0) as mm: data = mm[:] updated_data = data.replace(search_word, replacement_word) mm.seek(0) mm.write(updated_data) ```","solution":"import mmap def replace_word_in_file(filename: str, search_word: bytes, replacement_word: bytes) -> None: if len(search_word) != len(replacement_word): raise ValueError(\\"Replacement word must be of the same length as the search word.\\") with open(filename, \\"r+b\\") as f: with mmap.mmap(f.fileno(), 0) as mm: data = mm[:] updated_data = data.replace(search_word, replacement_word) if len(updated_data) != len(data): raise ValueError(\\"The replacement changed the size of the file.\\") mm.seek(0) mm.write(updated_data) mm.flush() # Ensure the changes are written to the file"},{"question":"**Objective:** To assess students\' understanding of pandas testing frameworks, focusing on assertion functions and exception handling. **Problem Statement:** You are given a dataset containing information about employees in a CSV file named `employees.csv`. Your task is to write a pandas program to perform the following operations: 1. **Loading Data:** Load the dataset into a pandas DataFrame. 2. **Data Cleaning:** Remove any rows with missing values. 3. **Data Transformation:** - Add a new column called `FullName` that concatenates the `FirstName` and `LastName` columns. - Convert the `HireDate` column from a string to a datetime object. 4. **Data Filtering:** Create a filtered DataFrame containing only the employees hired in the last 5 years. After performing these operations, you need to write unit tests to verify the correctness of your transformations and actions using the `pandas.testing` module. **Input:** A CSV file `employees.csv` with the following structure: ``` EmployeeID,FirstName,LastName,HireDate,Department 1,John,Doe,2015-06-01,Sales 2,Jane,Smith,2018-07-12,Marketing 3,Emily,Jones,2022-03-15,HR 4,Michael,Brown,2017-11-30,IT 5,Chris,Johnson,2020-12-23,Finance ``` **Output:** 1. A cleaned and transformed DataFrame. 2. A filtered DataFrame with employees hired in the last 5 years. 3. Unit tests verifying: - Row removal with missing values. - Correctness of `FullName` concatenation. - Proper conversion of `HireDate` to datetime. - Accurate filtering of employees based on the hire date. **Function Signature:** ```python import pandas as pd import pandas.testing as pdt def process_employee_data(file_path: str) -> pd.DataFrame: # Implement your data processing function here. pass def test_employee_data(): # Implement your test cases here using assertion functions from pandas.testing. pass # Example Usage if __name__ == \\"__main__\\": df = process_employee_data(\'employees.csv\') test_employee_data() ``` **Constraints:** - Assume the input file path is always correct and the file is formatted correctly. - You are not allowed to use any libraries other than pandas. - Your tests should use assertion functions to check for Deep DataFrame equality, not just the structure or length. **Instructions:** 1. Implement the `process_employee_data` function to perform the described operations. 2. Write comprehensive test cases within `test_employee_data` to verify each step of your data processing function. **Note:** Make sure your unit tests are robust and handle edge cases elegantly.","solution":"import pandas as pd from datetime import datetime, timedelta def process_employee_data(file_path: str) -> pd.DataFrame: # Loading Data df = pd.read_csv(file_path) # Data Cleaning df.dropna(inplace=True) # Data Transformation df[\'FullName\'] = df[\'FirstName\'] + \' \' + df[\'LastName\'] df[\'HireDate\'] = pd.to_datetime(df[\'HireDate\']) # Data Filtering five_years_ago = datetime.now() - timedelta(days=5*365) filtered_df = df[df[\'HireDate\'] >= five_years_ago] return df, filtered_df"},{"question":"Coding Assessment Question **Objective:** Implement a function to determine specific attributes of WAV sound files. **Problem Statement:** Write a Python function `analyze_wav_file(filename)` that takes the filename of a WAV sound file as an input and returns information about the file in a namedtuple with the following attributes: `filetype`, `framerate`, `nchannels`, `nframes`, and `sampwidth`. # Function Signature ```python from collections import namedtuple def analyze_wav_file(filename: str) -> namedtuple: pass ``` # Input - `filename` (str): A string representing the path to the WAV file. # Output - Returns a namedtuple with attributes: `filetype` (always `\'wav\'` for this exercise), `framerate`, `nchannels`, `nframes`, and `sampwidth`. - If the file is not a valid WAV file, return `None`. # Constraints - The function should handle at least WAV files. - You are not allowed to use the `sndhdr` module. - Assume the WAV files are in PCM format. - The function must handle errors gracefully and return `None` for any invalid or non-WAV files. # Named Tuple Definition ```python WAVInfo = namedtuple(\'WAVInfo\', [\'filetype\', \'framerate\', \'nchannels\', \'nframes\', \'sampwidth\']) ``` # Example ```python # Example usage info = analyze_wav_file(\\"example.wav\\") print(info) # Output: WAVInfo(filetype=\'wav\', framerate=44100, nchannels=2, nframes=100000, sampwidth=2) ``` # Notes: - You may find the `struct` module useful for reading binary data from the file. - You should handle exceptions to cover cases where the file might not be a valid WAV file.","solution":"from collections import namedtuple import wave WAVInfo = namedtuple(\'WAVInfo\', [\'filetype\', \'framerate\', \'nchannels\', \'nframes\', \'sampwidth\']) def analyze_wav_file(filename: str) -> WAVInfo: try: with wave.open(filename, \'rb\') as wav_file: framerate = wav_file.getframerate() nchannels = wav_file.getnchannels() nframes = wav_file.getnframes() sampwidth = wav_file.getsampwidth() return WAVInfo(filetype=\'wav\', framerate=framerate, nchannels=nchannels, nframes=nframes, sampwidth=sampwidth) except Exception as e: # If any error occurs (file not found, not a WAV file, etc.), return None return None"},{"question":"You are given the task to implement a function that checks the accessibility of a set of URLs for a given user agent based on the rules specified in the `robots.txt` file of the websites hosting these URLs. You will need to use the `RobotFileParser` class from the `urllib.robotparser` module. **Function Signature:** ```python def check_urls_accessibility(robots_txt_urls: list, user_agent: str, urls: list) -> dict: pass ``` **Input:** - `robots_txt_urls`: A list of strings where each string is the URL of the `robots.txt` file of a website (for example, \\"http://example.com/robots.txt\\"). - `user_agent`: A string representing the user agent whose permissions are to be checked. - `urls`: A list of strings where each string is a URL to check for accessibility based on the corresponding `robots.txt`. **Output:** - Returns a dictionary where the keys are the input URLs and the values are booleans indicating whether the given user agent is allowed to access each URL according to the rules defined in the `robots.txt` file of the corresponding website. **Constraints:** - You can assume that the list sizes are manageable and that there will be at most one `robots.txt` URL for each base domain present in the list of URLs. - Handle exceptions, such as when a `robots.txt` file cannot be read, gracefully by assuming that all URLs from that domain are accessible. **Example:** ```python robots_txt_urls = [\\"http://example.com/robots.txt\\"] user_agent = \\"GoogleBot\\" urls = [\\"http://example.com/page1\\", \\"http://example.com/page2\\", \\"http://example.com/private\\"] # Expected Output: # { # \\"http://example.com/page1\\": True, # \\"http://example.com/page2\\": True, # \\"http://example.com/private\\": False # } ``` **Explanation:** - The function will use the `RobotFileParser` class to read and parse the provided `robots.txt` file. - Based on the user agent string, it determines if each URL is accessible or not. - The output dictionary will indicate the accessibility status of each URL. Implement the function accordingly, ensuring you handle exceptions and edge cases where the `robots.txt` file may not be accessible or correctly formatted.","solution":"from urllib.robotparser import RobotFileParser from urllib.parse import urlparse def check_urls_accessibility(robots_txt_urls: list, user_agent: str, urls: list) -> dict: robots_parsers = {} accessibilities = {} # Load robots.txt for each provided robots.txt URL for robots_url in robots_txt_urls: domain = urlparse(robots_url).netloc robots_parsers[domain] = RobotFileParser() try: robots_parsers[domain].set_url(robots_url) robots_parsers[domain].read() except: # If reading the robots.txt fails, we assume all URLs from that domain are accessible robots_parsers[domain] = None # Check the accessibility of each URL for url in urls: domain = urlparse(url).netloc if domain in robots_parsers and robots_parsers[domain] is not None: accessibilities[url] = robots_parsers[domain].can_fetch(user_agent, url) else: # If there is no robots.txt for the domain, assume URL is accessible accessibilities[url] = True return accessibilities"},{"question":"Objective: You are tasked with creating a utility in Python that simulates part of the behavior of the `sdist` command. Specifically, you need to write a function that generates a list of files to be included in a source distribution based on a given `MANIFEST.in` template. Problem Statement: Write a Python function `generate_manifest(file_rules: List[str], file_system: Dict[str, List[str]]) -> List[str]` that takes two arguments: 1. `file_rules` - a list of strings, each representing a line from a `MANIFEST.in` file. 2. `file_system` - a dictionary where keys are directory paths (as strings) and values are lists of file names (as strings) in those directories. The function should return a list of file paths that should be included in the source distribution, adhering to the rules provided in `file_rules`. Input: 1. `file_rules` (List[str]): The manifest template commands, e.g., [\\"include *.txt\\", \\"recursive-include src *.py\\", \\"prune src/tmp\\"] . 2. `file_system` (Dict[str, List[str]]): A representation of the file system, e.g., { \\"\\": [\\"README.txt\\", \\"setup.py\\"], \\"src\\": [\\"main.py\\", \\"utils.py\\", \\"tmp/ignore.py\\"] }. Output: - List[str]: A list of file paths to include in the source distribution. Constraints: - Paths should always be slash-separated. - The `file_system` will not contain directory entries for \\"RCS\\", \\"CVS\\", \\".svn\\", \\".hg\\", \\".git\\", \\".bzr\\", or \\"_darcs\\". - Handle file glob patterns (e.g., `*.txt`, `*.py`) correctly per the rules in `MANIFEST.in`. Example: ```python file_rules = [ \\"include *.txt\\", \\"recursive-include src *.py\\", \\"prune src/tmp\\" ] file_system = { \\"\\": [\\"README.txt\\", \\"setup.py\\", \\"requirements.txt\\"], \\"src\\": [\\"main.py\\", \\"utils.py\\", \\"tmp/ignore.py\\", \\"lib/helper.py\\"], \\"src/tmp\\": [\\"ignore.py\\"], \\"docs\\": [\\"index.rst\\", \\"intro.txt\\"] } generate_manifest(file_rules, file_system) ``` **Expected Output:** ```python [ \\"README.txt\\", \\"requirements.txt\\", \\"src/main.py\\", \\"src/utils.py\\", \\"src/lib/helper.py\\" ] ``` Additional Notes: - The `\\"prune\\"` command removes all files in the specified directory or its subdirectories from being included. - The `\\"include\\"` command should consider the root directory unless stated otherwise. - The `\\"recursive-include\\"` command includes files matching the pattern recursively from the specified directory. Implement the function `generate_manifest` based on the above specifications.","solution":"import fnmatch def generate_manifest(file_rules, file_system): Generates a list of files to be included in the source distribution based on MANIFEST.in rules and a simulated file system. included_files = set() pruned_dirs = set() for rule in file_rules: parts = rule.split() command = parts[0] if command == \'include\': pattern = parts[1] for dir_path, files in file_system.items(): if dir_path == \\"\\": for file in files: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif command == \'recursive-include\': directory = parts[1] pattern = parts[2] for dir_path, files in file_system.items(): if dir_path.startswith(directory): for file in files: full_path = f\\"{dir_path}/{file}\\" if dir_path else file if fnmatch.fnmatch(file, pattern): included_files.add(full_path) elif command == \'prune\': directory = parts[1] for dir_path in file_system.keys(): if dir_path.startswith(directory): pruned_dirs.add(dir_path) # Remove files in pruned directories files_to_include = [ file for file in included_files if not any(file.startswith(pruned) for pruned in pruned_dirs) ] return sorted(files_to_include)"},{"question":"# Python Coding Assessment Question --- **Objective:** This question tests your understanding of the Python `ast` module, particularly how to analyze and transform abstract syntax trees. --- Problem Statement: You are given a Python code as a string. Your task is to implement a function `simplify_math_expressions(code: str) -> str` that parses this code into an abstract syntax tree (AST), transforms all arithmetic expressions (addition and subtraction) to their evaluated results, and then returns the modified code as a string. Use the `ast` module to achieve this transformation. Requirements: 1. **Input**: A string `code` representing valid Python code. 2. **Output**: A modified string with all arithmetic expressions evaluated. 3. Only handle addition and subtraction (`+` and `-`). Ignore other operations. 4. Preserve the structure and formatting of the code as much as possible. 5. The code can contain variable assignments, function definitions, and control flow statements. Constraints: 1. The input code will be valid Python code. 2. Only numeric constants will be involved in addition and subtraction operations. Example: ```python # For the input code: code = a = 2 + 3 b = a - 4 def func(): return 5 + 6 # The function should return: a = 5 b = a - 4 def func(): return 11 ``` Hints: 1. Use the `ast.parse()` function to parse the input code. 2. Traverse the AST using `ast.NodeVisitor` or `ast.NodeTransformer`. 3. Modify `ast.BinOp` nodes where `op` is `ast.Add` or `ast.Sub` and both `left` and `right` are `ast.Constant`. 4. Use `ast.unparse()` to convert the modified AST back to code. --- Note: Use the following structure to implement your solution: ```python import ast class MathSimplifier(ast.NodeTransformer): def visit_BinOp(self, node): # Your code to transform addition and subtraction nodes def simplify_math_expressions(code: str) -> str: # Your code to parse, transform, and unparse the AST ``` This problem tests your ability to understand and manipulate abstract syntax trees and ensure you keep the transformations correct and maintain the format of the original code as much as possible.","solution":"import ast class MathSimplifier(ast.NodeTransformer): def visit_BinOp(self, node): # Recursively visit left and right child nodes self.generic_visit(node) # Check if the operation is addition or subtraction if isinstance(node.op, (ast.Add, ast.Sub)): # Check if the left and right operands are constants if isinstance(node.left, ast.Constant) and isinstance(node.right, ast.Constant): result = None if isinstance(node.op, ast.Add): result = node.left.value + node.right.value elif isinstance(node.op, ast.Sub): result = node.left.value - node.right.value # Return a constant node with the computed result return ast.Constant(value=result) # Return the node if no transformation was made return node def simplify_math_expressions(code: str) -> str: tree = ast.parse(code) simplifier = MathSimplifier() simplified_tree = simplifier.visit(tree) return ast.unparse(simplified_tree)"},{"question":"# Question **Objective**: Implement and utilize Python\'s `trace` module to trace the execution of a provided Python script and generate a report of the code coverage. **Task Description**: 1. **Implement a Function to Setup and Run the Tracer**: - Implement a function `trace_execution(script_content: str, ignored_dirs: list, cover_directory: str) -> str`. - This function should take the content of a Python script as a string, a list of directories to ignore during tracing, and a directory where the coverage report should be generated. - Use the `trace.Trace` class to create a tracer and run the provided script content. - The function should return the path of the generated coverage report directory. 2. **Generate and Analyze Tracing Report**: - Write a Python script that has at least four different functions, some of which include conditional statements and loops. - Run your implemented `trace_execution` function on this script and generate an execution trace and coverage report. 3. **Provide Observations**: - Include a brief summary of the generated coverage report mentioning which parts of the code were executed and which were not. **Constraints and Input/Output Requirements**: - The `trace_execution` function\'s `script_content` parameter will be a valid Python script. - The `ignored_dirs` parameter will contain valid directory paths. - The `cover_directory` parameter will be a valid directory path where the results can be written. - The return value should be the path to the coverage report directory. **Example**: ```python import os # Example script content example_script = def foo(): for i in range(5): print(i) def bar(x): if x > 2: return True return False def main(): foo() results = bar(3) print(results) if __name__ == \\"__main__\\": main() cover_dir = \\"coverage_report\\" ignored = [os.path.dirname(os.__file__)] # Function to implement def trace_execution(script_content: str, ignored_dirs: list, cover_directory: str) -> str: # Your implementation here pass # Call the implemented function and generate the report trace_execution(example_script, ignored, cover_dir) ``` **Notes**: - Ensure to handle file operations and directory checks gracefully. - Validate the provided inputs; raise appropriate exceptions with meaningful messages if inputs are invalid. - Be sure to clean up any temporary files/directories created during the process.","solution":"import os import tempfile import trace def trace_execution(script_content: str, ignored_dirs: list, cover_directory: str) -> str: Trace the execution of a Python script and generate a code coverage report. :param script_content: The content of the Python script as a string. :param ignored_dirs: List of directories to ignore during tracing. :param cover_directory: Directory where the coverage report should be generated. :return: The path to the coverage report directory. if not os.path.exists(cover_directory): os.makedirs(cover_directory) with tempfile.NamedTemporaryFile(delete=False, suffix=\\".py\\") as temp_script: temp_script.write(script_content.encode(\'utf-8\')) temp_script_path = temp_script.name tracer = trace.Trace(trace=0, count=1, ignoremods=ignored_dirs, ignoredirs=ignored_dirs) tracer.run(f\\"exec(open(\'{temp_script_path}\').read())\\") results = tracer.results() cover_report_path = os.path.join(cover_directory, \\"coverage_report.txt\\") with open(cover_report_path, \'w\') as report_file: results.write_results(show_missing=True, summary=True, coverdir=cover_directory) return cover_directory"},{"question":"**Objective:** Design a plot using Seaborn to visualize specific relationships within a dataset. **Task:** Given the `tips` dataset from Seaborn, your task is to create a faceted plot that shows the relationship between `total_bill` and `tip` with the following requirements: 1. Create a scatter plot (`relplot` with `kind=\\"scatter\\"`) and a line plot (`relplot` with `kind=\\"line\\"`) for the same relationship. 2. Use `smoker` as a hue variable and `size` as a size variable to differentiate data points. 3. Customize the color palette for better visual clarity. 4. Separate the plots into different columns based on the `day` variable using faceting. 5. Set a suitable height and aspect ratio for the facets. **Input:** No explicit input required, as you will use the built-in `tips` dataset from Seaborn. **Output:** A faceted plot that meets the above requirements. **Example:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a scatter plot with facets sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", size=\\"size\\", col=\\"day\\", kind=\\"scatter\\", palette=\\"ch:r=-.5,l=.75\\", height=4, aspect=0.75 ) # Create a line plot with facets sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", size=\\"size\\", col=\\"day\\", kind=\\"line\\", palette=\\"ch:r=-.5,l=.75\\", height=4, aspect=0.75 ) plt.show() ``` **Constraints:** - You should ensure that the plots are clear and easy to interpret. - Consider plot aesthetics such as titles, labels, and legends for clarity. **Requirements:** - Implement the above facets in a manner consistent with the documentation to visualize relationships effectively in the given dataset. **Grading:** Your solution will be evaluated based on: - Correctness of the plots - Proper use of semantics (hue, size) - Effective customization (palettes, aesthetics) - Clarity and interpretability of the resultant plots","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_faceted_plots(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a custom color palette custom_palette = \\"ch:r=-.5,l=.75\\" # Create a scatter plot with facets sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", size=\\"size\\", col=\\"day\\", kind=\\"scatter\\", palette=custom_palette, height=4, aspect=0.75 ) # Create a line plot with facets sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", size=\\"size\\", col=\\"day\\", kind=\\"line\\", palette=custom_palette, height=4, aspect=0.75 ) plt.show()"},{"question":"# Question: Advanced Data Visualization with Seaborn Given the iris dataset, use seaborn to create and customize various distribution plots as specified below. The dataset can be loaded using `sns.load_dataset(\'iris\')`. Your code should be encapsulated in a function called `visualize_iris_data()` that should return a seaborn `FacetGrid` object. **Requirements**: 1. **Load the Iris Dataset**: - Use the `sns.load_dataset(\'iris\')` method. 2. **Distribution Plots**: - Create a univariate plot to show the distribution of the `sepal_length` attribute using a histogram. - Create a bivariate plot to show the relationship between `sepal_length` and `sepal_width` using KDE. 3. **Customization**: - Split the data by the `species` attribute using the `hue` parameter for both plots. - Add a marginal \\"rug\\" plot to the bivariate plot. 4. **Facet Grids**: - Create subplots for each species by using facet grids with the `col` parameter for both plots. - Adjust the size and aspect of the plots to your preference. 5. **Further Customization**: - Set custom axis labels: \\"Measurement (cm)\\" for both x and y axes. - Set titles for each subplot to include the species name. # Function Signature ```python def visualize_iris_data(): pass ``` # Performance Constraints - Ensure all visualizations are clear and informative. - Ensure the function runs efficiently on the provided dataset. # Example ```python # This is how you\'d call the function and visualize the plots: g = visualize_iris_data() g.fig.suptitle(\\"Iris Data Distribution\\") g.fig.tight_layout() import matplotlib.pyplot as plt plt.show() ``` **Notes**: - You are free to use additional seaborn or matplotlib functionalities to enhance the plots. - Make sure to return the `FacetGrid` object from your function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_iris_data(): # Load the iris dataset iris = sns.load_dataset(\'iris\') # Create a univariate plot - histogram for sepal_length univariate = sns.FacetGrid(iris, col=\\"species\\", margin_titles=True) univariate.map(sns.histplot, \\"sepal_length\\", kde=True) # Create a bivariate plot - KDE plot for sepal_length vs sepal_width bivariate = sns.FacetGrid(iris, hue=\\"species\\", col=\\"species\\", margin_titles=True) bivariate.map(sns.kdeplot, \\"sepal_length\\", \\"sepal_width\\", fill=True, common_norm=False) bivariate.map(sns.rugplot, \\"sepal_length\\") # Customizing axis labels and titles for ax in univariate.axes.flat: ax.set_xlabel(\\"Measurement (cm)\\") ax.set_ylabel(\\"Frequency\\") ax.set_title(ax.get_title().split(\\"=\\")[-1].strip()) for ax in bivariate.axes.flat: ax.set_xlabel(\\"Measurement (cm)\\") ax.set_ylabel(\\"Measurement (cm)\\") ax.set_title(ax.get_title().split(\\"=\\")[-1].strip()) plt.subplots_adjust(top=0.9) univariate.fig.suptitle(\\"Sepal Length Distribution by Species\\") bivariate.fig.suptitle(\\"Bivariate KDE of Sepal Length and Sepal Width by Species\\") return univariate, bivariate"},{"question":"In this assessment, you are required to create a custom visualization using Seaborn and Matplotlib. The goal is to demonstrate your understanding of Seaborn\'s aesthetic control and context settings. Instructions 1. **Data Preparation**: - Generate a sample dataset containing 100 data points following a normal distribution with mean 0 and standard deviation 1. - Generate another sample dataset containing 100 data points following a uniform distribution between 0 and 1. 2. **Visualization Requirements**: - Create a single figure with two subplots arranged in a 1x2 grid. - In the first subplot, create a boxplot for the normally distributed data. - In the second subplot, create a violin plot for the uniformly distributed data. 3. **Aesthetic Customizations**: - Apply the `whitegrid` style to the first subplot\'s boxplot. - Apply the `dark` style to the second subplot\'s violin plot. - Remove the top and right spines for both subplots. 4. **Context Scaling**: - Set the context for the entire figure to `notebook` with a font scale of 1.5. - Temporarily set the context to `talk` for the second subplot within a `with` statement. 5. **Advanced Customization**: - Override any default style parameter of your choice for the plots (e.g., changing the face color of axes). Expected Output - A figure with two subplots reflecting the described custom aesthetic and context settings. You are required to use Seaborn functions such as `sns.set_style`, `sns.despine`, `sns.set_context`, `with sns.axes_style`, and Matplotlib for this task. Example Code Your implementation should follow this structure, ensuring both plots meet the described customization criteria: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Data Preparation normal_data = np.random.normal(0, 1, 100) uniform_data = np.random.uniform(0, 1, 100) # Set context for the figure sns.set_context(\\"notebook\\", font_scale=1.5) # Create figure and subplots fig, axs = plt.subplots(1, 2, figsize=(14, 7)) # First subplot: boxplot with whitegrid style sns.set_style(\\"whitegrid\\") sns.boxplot(data=normal_data, ax=axs[0]) sns.despine(ax=axs[0]) # Second subplot: violin plot with dark style with sns.axes_style(\\"dark\\"): sns.set_context(\\"talk\\") # Temporarily set context to \'talk\' sns.violinplot(data=uniform_data, ax=axs[1]) sns.despine(ax=axs[1]) # Customize the figure with your own parameter axs[0].set_facecolor(\'lightgrey\') axs[1].set_facecolor(\'lightgrey\') plt.show() ``` Submit your code for evaluation once you have completed the implementation. Good luck!","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_custom_visualization(): # Data Preparation np.random.seed(0) # For reproducibility normal_data = np.random.normal(0, 1, 100) uniform_data = np.random.uniform(0, 1, 100) # Set context for the figure sns.set_context(\\"notebook\\", font_scale=1.5) # Create figure and subplots fig, axs = plt.subplots(1, 2, figsize=(14, 7)) # First subplot: boxplot with whitegrid style sns.set_style(\\"whitegrid\\") sns.boxplot(data=normal_data, ax=axs[0]) sns.despine(ax=axs[0]) # Second subplot: violin plot with dark style with sns.axes_style(\\"dark\\"): sns.set_context(\\"talk\\") # Temporarily set context to \'talk\' sns.violinplot(data=uniform_data, ax=axs[1]) sns.despine(ax=axs[1]) # Customize the figure with your own parameter axs[0].set_facecolor(\'lightgrey\') axs[1].set_facecolor(\'lightgrey\') plt.show() # Call the function to execute the visualization creation create_custom_visualization()"},{"question":"# Audio Processing and Transformation Task **Objective:** Create a Python module to perform specific audio processing tasks using the \\"audioop\\" module. **Requirements:** 1. Implement three functions as specified below. 2. Follow input and output formats precisely. 3. Ensure your functions can handle edge cases and are robust. # Functions to Implement 1. `convert_encoding(input_fragment: bytes, width: int, src_encoding: str, dest_encoding: str) -> bytes` Convert `input_fragment` from `src_encoding` to `dest_encoding`. - **Parameters:** - `input_fragment` (bytes): The audio fragment to be converted. - `width` (int): The sample width in bytes. - `src_encoding` (str): Source encoding format, one of `[\'linear\', \'alaw\', \'ulaw\', \'adpcm\']`. - `dest_encoding` (str): Destination encoding format, one of `[\'linear\', \'alaw\', \'ulaw\', \'adpcm\']`. - **Returns:** - (bytes): The converted audio fragment. **Constraints:** - Both source and destination encodings must be one of the listed values. - For ADPCM conversions, handle initial and final states accordingly using a dummy state. 2. `scale_audio(input_fragment: bytes, width: int, factor: float) -> bytes` Adjust the volume of an audio fragment by a given factor. - **Parameters:** - `input_fragment` (bytes): The audio fragment to scale. - `width` (int): The sample width in bytes. - `factor` (float): The factor by which to scale the audio. - **Returns:** - (bytes): The scaled audio fragment. **Constraints:** - The width must be 1, 2, 3, or 4. 3. `find_loudest_section(input_fragment: bytes, width: int, section_length: int) -> bytes` Find and return the section of the audio fragment with the highest energy. - **Parameters:** - `input_fragment` (bytes): The audio fragment to search within. - `width` (int): The sample width in bytes. - `section_length` (int): The length of the section to find. - **Returns:** - (bytes): The loudest section of the audio fragment. **Constraints:** - The width must be 1, 2, 3, or 4. - The section length should be a positive integer and less than or equal to the number of samples in the input fragment. # Example Usage ```python # Example Usage: # Test convert_encoding function input_fragment = b\'x01x02x03x04\' # Example audio data width = 2 src_encoding = \'linear\' dest_encoding = \'alaw\' converted_fragment = convert_encoding(input_fragment, width, src_encoding, dest_encoding) print(converted_fragment) # Test scale_audio function scaled_fragment = scale_audio(input_fragment, width, 0.5) print(scaled_fragment) # Test find_loudest_section function loudest_section = find_loudest_section(input_fragment, width, 2) print(loudest_section) ``` **Note:** You can use `audioop.lin2lin`, `audioop.lin2alaw`, `audioop.lin2ulaw`, `audioop.alaw2lin`, `audioop.ulaw2lin`, `audioop.lin2adpcm`, `audioop.adpcm2lin` for encoding conversions. Utilize `audioop.mul` for scaling and `audioop.findmax`, `audioop.rms` for the loudest section calculation. Ensure your code is clean, well-documented, and follows best practices.","solution":"import audioop def convert_encoding(input_fragment: bytes, width: int, src_encoding: str, dest_encoding: str) -> bytes: Convert `input_fragment` from `src_encoding` to `dest_encoding`. Parameters: - input_fragment (bytes): The audio fragment to be converted. - width (int): The sample width in bytes. - src_encoding (str): Source encoding format, one of `[\'linear\', \'alaw\', \'ulaw\', \'adpcm\']`. - dest_encoding (str): Destination encoding format, one of `[\'linear\', \'alaw\', \'ulaw\', \'adpcm\']. Returns: - bytes: The converted audio fragment. if src_encoding == dest_encoding: return input_fragment # No conversion needed # Convert to linear encoding first if src_encoding == \'linear\': linear_fragment = input_fragment elif src_encoding == \'alaw\': linear_fragment = audioop.alaw2lin(input_fragment, width) elif src_encoding == \'ulaw\': linear_fragment = audioop.ulaw2lin(input_fragment, width) elif src_encoding == \'adpcm\': linear_fragment, _ = audioop.adpcm2lin(input_fragment, width, None) else: raise ValueError(\\"Unsupported source encoding\\") # Convert from linear encoding to the desired destination encoding if dest_encoding == \'linear\': return linear_fragment elif dest_encoding == \'alaw\': return audioop.lin2alaw(linear_fragment, width) elif dest_encoding == \'ulaw\': return audioop.lin2ulaw(linear_fragment, width) elif dest_encoding == \'adpcm\': return audioop.lin2adpcm(linear_fragment, width)[0] else: raise ValueError(\\"Unsupported destination encoding\\") def scale_audio(input_fragment: bytes, width: int, factor: float) -> bytes: Adjust the volume of an audio fragment by a given factor. Parameters: - input_fragment (bytes): The audio fragment to scale. - width (int): The sample width in bytes. - factor (float): The factor by which to scale the audio. Returns: - bytes: The scaled audio fragment. return audioop.mul(input_fragment, width, factor) def find_loudest_section(input_fragment: bytes, width: int, section_length: int) -> bytes: Find and return the section of the audio fragment with the highest energy. Parameters: - input_fragment (bytes): The audio fragment to search within. - width (int): The sample width in bytes. - section_length (int): The length of the section to find in number of samples. Returns: - bytes: The loudest section of the audio fragment. num_samples = len(input_fragment) // width max_energy = 0 loudest_section = input_fragment[:section_length * width] for start in range(0, num_samples - section_length + 1): section = input_fragment[start * width: (start + section_length) * width] energy = audioop.rms(section, width) if energy > max_energy: max_energy = energy loudest_section = section return loudest_section"},{"question":"Objective: Implement a `sklearn.linear_model.SGDClassifier` model to classify a provided dataset while optimizing for prediction latency and memory usage. You will need to utilize sparse matrix representations and configure scikit-learn’s settings to limit validation overhead and working memory. Input: 1. A `scipy.sparse.csr_matrix` `X_train` of shape `(n_samples_train, n_features)`: Sparse matrix containing training features. 2. A `numpy.array` `y_train` of shape `(n_samples_train,)`: Binary labels for training data. 3. A `scipy.sparse.csr_matrix` `X_test` of shape `(n_samples_test, n_features)`: Sparse matrix containing test features. 4. A `numpy.array` `y_test` of shape `(n_samples_test,)`: Binary labels for test data. Output: 1. A dictionary containing the following keys: - `accuracy`: Accuracy score of the model on the test set. - `latency`: Average prediction latency per sample on the test set (in microseconds). - `memory_usage`: Memory usage of the model and configuration in megabytes. Constraints: 1. The model should be trained using an elasticnet penalty with `l1_ratio=0.25`. 2. Limit the working memory for scikit-learn to 128 MiB. 3. Reduce validation overhead by suppressing checking for finiteness of data during prediction. Additional Requirements: 1. Implement and utilize a function `sparsity_ratio(X)` to print the sparsity ratio of your input matrices. 2. Ensure prediction latency is measured accurately in your implementation, with a clear distinction between bulk and atomic predictions. ```python import numpy as np import scipy.sparse as sp from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import time import sklearn def sparsity_ratio(X): return 1.0 - np.count_nonzero(X.toarray()) / float(X.shape[0] * X.shape[1]) def classify_and_optimize(X_train, y_train, X_test, y_test): results = {} # Print input sparsity ratio print(\\"Input sparsity ratio:\\", sparsity_ratio(X_train)) # Configuring scikit-learn for reduced validation overhead and limiting working memory with sklearn.config_context(assume_finite=True, working_memory=128): # Training the model with elasticnet penalty model = SGDClassifier(penalty=\'elasticnet\', l1_ratio=0.25) model.fit(X_train, y_train) # Measuring latency start_time = time.time() y_pred = model.predict(X_test) end_time = time.time() prediction_time = end_time - start_time latency = (prediction_time / X_test.shape[0]) * 1e6 # Convert to microseconds # Calculating accuracy accuracy = accuracy_score(y_test, y_pred) # Measuring memory usage (in this context, consider model memory footprint) memory_usage = model.coef_.nbytes / (1024 ** 2) # Convert bytes to megabytes results[\'accuracy\'] = accuracy results[\'latency\'] = latency results[\'memory_usage\'] = memory_usage return results # Example usage # Note: You should replace these example inputs with the actual dataset while running the function. # X_train, y_train, X_test, y_test = (YOUR INPUT DATA HERE) # results = classify_and_optimize(X_train, y_train, X_test, y_test) # print(results) ``` Please refer to the code snippet above and implement the function `classify_and_optimize` to meet the given constraints and requirements. Ensure thorough testing with provided datasets to validate your implementation.","solution":"import numpy as np import scipy.sparse as sp from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import time import sklearn def sparsity_ratio(X): return 1.0 - np.count_nonzero(X.toarray()) / float(X.shape[0] * X.shape[1]) def classify_and_optimize(X_train, y_train, X_test, y_test): results = {} # Print input sparsity ratio print(\\"Input sparsity ratio:\\", sparsity_ratio(X_train)) # Configuring scikit-learn for reduced validation overhead and limiting working memory with sklearn.config_context(assume_finite=True, working_memory=128): # Training the model with elasticnet penalty model = SGDClassifier(penalty=\'elasticnet\', l1_ratio=0.25) model.fit(X_train, y_train) # Measuring latency start_time = time.time() y_pred = model.predict(X_test) end_time = time.time() prediction_time = end_time - start_time latency = (prediction_time / X_test.shape[0]) * 1e6 # Convert to microseconds # Calculating accuracy accuracy = accuracy_score(y_test, y_pred) # Measuring memory usage (in this context, consider model memory footprint) memory_usage = model.coef_.nbytes / (1024 ** 2) # Convert bytes to megabytes results[\'accuracy\'] = accuracy results[\'latency\'] = latency results[\'memory_usage\'] = memory_usage return results"},{"question":"Objective: The goal of this task is to verify your understanding and ability to manipulate different mailbox formats using the `mailbox` module. You will write a Python function to transfer specific types of messages from one mailbox to another while handling format-specific behaviors. Problem Statement: You are given two different mailboxes: one in the `Maildir` format and another in the `mbox` format. Your task is to implement a function `transfer_flagged_messages` that transfers all flagged messages from the `Maildir` mailbox to the `mbox` mailbox. A flagged message in the `Maildir` format has the \'F\' flag set. In the `mbox` format, the corresponding message should have the \'F\' flag set in its headers. The function you need to implement is defined as follows: ```python import mailbox def transfer_flagged_messages(maildir_path, mbox_path): Transfer all flagged (\'F\') messages from a Maildir mailbox to an mbox mailbox. Parameters: - maildir_path (str): The path to the Maildir mailbox. - mbox_path (str): The path to the mbox mailbox. Returns: None pass ``` Implementation Details: 1. **Open both mailboxes**: Use the given paths to open the `Maildir` and `mbox` mailboxes. 2. **Iterate through messages in `Maildir`**: Identify messages that have the \'F\' flag set. 3. **Transfer flagged messages**: For each flagged message, add it to the `mbox` mailbox, ensuring the \'F\' flag is correctly set in the `mbox` format. 4. **Ensure data integrity**: Lock and unlock mailboxes appropriately to prevent data corruption, and handle any errors gracefully. Constraints: - You can assume that both mailboxes exist and are accessible. - Handle exceptions that may arise during mailbox operations, such as `mailbox.ExternalClashError` and `email.errors.MessageParseError`. - Ensure the function works efficiently even with a large number of messages in the mailboxes. Example Usage: ```python transfer_flagged_messages(\'/path/to/maildir\', \'/path/to/mbox\') ``` Upon execution, all messages in the Maildir mailbox that were marked as important (flagged) should be copied over to the mbox mailbox, maintaining their flagged status.","solution":"import mailbox def transfer_flagged_messages(maildir_path, mbox_path): Transfer all flagged (\'F\') messages from a Maildir mailbox to an mbox mailbox. Parameters: - maildir_path (str): The path to the Maildir mailbox. - mbox_path (str): The path to the mbox mailbox. Returns: None maildir = mailbox.Maildir(maildir_path, factory=None) mbox = mailbox.mbox(mbox_path) mbox.lock() try: for key, msg in maildir.items(): if \'F\' in msg.get_flags(): mbox_msg = mailbox.mboxMessage(msg) mbox_msg.add_flag(\'F\') mbox.add(mbox_msg) mbox.flush() finally: mbox.unlock()"},{"question":"**Objective:** Write a Python function that takes a list of tuples where each tuple contains a student\'s name (a string) and their scores in three subjects (three integers). The function should calculate the total score for each student, determine the student with the highest total score, and return their name along with their total score. **Function Signature:** ```python def top_student(students: list[tuple[str, int, int, int]]) -> tuple[str, int]: ``` **Input:** - A list of tuples. Each tuple contains: - A string representing the student\'s name. - Three integers representing the scores in three different subjects. **Output:** - A tuple containing: - A string representing the name of the student with the highest total score. - An integer representing the highest total score. **Constraints:** - The input list will have at least one student. - All scores are non-negative integers. **Example:** ```python students = [(\\"Alice\\", 85, 90, 88), (\\"Bob\\", 78, 82, 80), (\\"Charlie\\", 92, 95, 91)] print(top_student(students)) # Output: (\\"Charlie\\", 278) ``` **Requirements/Notes:** - You should not use any external packages or libraries. - Your solution should effectively utilize Python\'s built-in data structures and should be efficient. - Consider edge cases such as multiple students having the same highest score. In such cases, return the first student (in the list) achieving the highest score. **Sample Testing:** _Construct additional tests to ensure the robustness of your solution._ ```python def test_top_student(): assert top_student([(\\"Alice\\", 85, 90, 88), (\\"Bob\\", 78, 82, 80), (\\"Charlie\\", 92, 95, 91)]) == (\\"Charlie\\", 278) assert top_student([(\\"Dave\\", 70, 75, 80), (\\"Eva\\", 85, 89, 90)]) == (\\"Eva\\", 264) assert top_student([(\\"Frank\\", 100, 100, 100)]) == (\\"Frank\\", 300) assert top_student([(\\"George\\", 60, 65, 70), (\\"Hannah\\", 60, 65, 70)]) == (\\"George\\", 195) print(\\"All test cases pass\\") # Run the tests test_top_student() ```","solution":"def top_student(students): Returns the name and total score of the student with the highest total score. :param students: List of tuples, where each tuple contains a student\'s name and three subject scores. :return: Tuple containing the name of the student with the highest total score and their total score. # Initialize the best student\'s name and score. best_student = \\"\\" highest_score = 0 for student in students: name, score1, score2, score3 = student total_score = score1 + score2 + score3 if total_score > highest_score: highest_score = total_score best_student = name return (best_student, highest_score)"},{"question":"**Advanced Hashing and Key Derivation** You are tasked with designing a robust password management system using Python\'s `hashlib` module. This system should include functionality for secure password storage and password verification using modern cryptographic practices. Specifically, you should implement the following functions: # Function 1: `store_password(password: str) -> dict` This function should take a user\'s password as a string input and generate a secure storage structure using the `pbkdf2_hmac` function for key derivation. The stored output should include salt and the derived hash. - **Input**: - `password` (str): The password to be securely stored. - **Output**: - A dictionary with keys: - `salt`: base64-encoded salt value used. - `hash`: base64-encoded hash of the password. - `iterations`: the number of iterations used in the pbkdf2_hmac function. - **Constraints**: - Use `os.urandom()` for generating a random salt value. - Use SHA-256 as the hash function for `pbkdf2_hmac`. - Use 100,000 iterations for the key derivation function (or more). - Ensure proper length for salt (at least 16 bytes) and derived key (32 bytes). # Function 2: `verify_password(stored: dict, password: str) -> bool` This function should verify a given password against the stored hash and salt using the `pbkdf2_hmac` function. - **Input**: - `stored` (dict): The dictionary obtained from `store_password`, containing `salt`, `hash`, and `iterations`. - `password` (str): The password to verify. - **Output**: - `True` if the password matches, `False` otherwise. # Example Usage ```python stored_info = store_password(\\"secure_password123\\") print(stored_info) # Output: {\'salt\': \'<base64_encoded_salt>\', \'hash\': \'<base64_encoded_hash>\', \'iterations\': 100000} is_correct = verify_password(stored_info, \\"secure_password123\\") print(is_correct) # Output: True is_incorrect = verify_password(stored_info, \\"wrong_password\\") print(is_incorrect) # Output: False ``` # Additional Requirements: 1. Use the `base64` module to encode and decode the salt and hash values for storage and verification. 2. Your implementation should ensure that the system is secure and follows best cryptographic practices for password handling. Please begin by implementing these functions below: ```python import hashlib import os import base64 def store_password(password: str) -> dict: pass def verify_password(stored: dict, password: str) -> bool: pass ```","solution":"import hashlib import os import base64 def store_password(password: str) -> dict: Store a user\'s password securely using pbkdf2_hmac. Args: - password (str): The password to be securely stored. Returns: - dict: A dictionary with keys \'salt\', \'hash\', and \'iterations\'. salt = os.urandom(16) # Generate a 16-byte salt iterations = 100000 # Define the number of iterations hash_value = hashlib.pbkdf2_hmac(\'sha256\', password.encode(\'utf-8\'), salt, iterations, dklen=32) # Generate the derived key return { \'salt\': base64.b64encode(salt).decode(\'utf-8\'), \'hash\': base64.b64encode(hash_value).decode(\'utf-8\'), \'iterations\': iterations } def verify_password(stored: dict, password: str) -> bool: Verify the given password against the stored salt and hash. Args: - stored (dict): The dictionary obtained from store_password, containing \'salt\', \'hash\', and \'iterations\'. - password (str): The password to verify. Returns: - bool: True if the password matches, False otherwise. salt = base64.b64decode(stored[\'salt\'].encode(\'utf-8\')) stored_hash = base64.b64decode(stored[\'hash\'].encode(\'utf-8\')) iterations = stored[\'iterations\'] test_hash = hashlib.pbkdf2_hmac(\'sha256\', password.encode(\'utf-8\'), salt, iterations, dklen=32) return stored_hash == test_hash"},{"question":"Supervised Learning with Scikit-Learn **Objective**: This question aims to assess your ability to preprocess data, implement a supervised learning algorithm using scikit-learn, and evaluate the model\'s performance. **Scenario**: You are provided with a dataset containing information about various houses, including their features and sale prices. Your task is to build a model to predict house prices using a suitable supervised learning algorithm from scikit-learn. **Dataset**: The dataset `house_prices.csv` has the following columns: - `LotArea`: Total area of the plot in square feet. - `OverallQual`: Overall material and finish quality (integer values 1-10). - `OverallCond`: Overall condition rating (integer values 1-10). - `YearBuilt`: Original construction date. - `TotalBsmtSF`: Total square feet of basement area. - `GrLivArea`: Above grade (ground) living area square feet. - `FullBath`: Total number of full bathrooms. - `BedroomAbvGr`: Number of bedrooms above ground. - `SalePrice`: Sale price of the house (target variable). **Tasks**: 1. **Data Preprocessing**: - Load the dataset into a pandas DataFrame. - Handle any missing values if present. - Split the data into features (`X`) and the target variable (`y`). - Standardize or normalize the features if necessary. 2. **Model Implementation**: - Split the data into training and testing sets (80% training, 20% testing). - Choose a suitable supervised learning algorithm from scikit-learn for this regression task. - Train the model on the training data. 3. **Model Evaluation**: - Make predictions on the test data. - Evaluate the model using appropriate regression metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared value. **Constraints**: - You must use at least one of the algorithms from the following: - `linear_model` - `tree` - `ensemble` **Expectations**: - The code should be well-documented with comments explaining each step. - Provide a brief explanation of why you chose the specific algorithm. - Ensure your code is efficient and handles potential edge cases. **Input Format**: - The dataset file `house_prices.csv`. **Output Format**: - Numerical performance metrics of the model. - Brief summary and interpretation of the model\'s performance. **Sample Code Structure**: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Load the dataset df = pd.read_csv(\'house_prices.csv\') # Handle missing values df.fillna(df.mean(), inplace=True) # Split the data into features (X) and target (y) X = df.drop(\'SalePrice\', axis=1) y = df[\'SalePrice\'] # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Train the model model = LinearRegression() model.fit(X_train, y_train) # Make predictions y_pred = model.predict(X_test) # Evaluate the model mae = mean_absolute_error(y_test, y_pred) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) print(f\\"MAE: {mae}, MSE: {mse}, R2: {r2}\\") ``` **Submission**: Submit your complete code and a short report summarizing your approach and findings.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score def preprocess_data(file_path): Loads and preprocesses the data. df = pd.read_csv(file_path) # Handling missing values by replacing them with the mean of the column df.fillna(df.mean(), inplace=True) # Splitting the data into features (X) and target (y) X = df.drop(\'SalePrice\', axis=1) y = df[\'SalePrice\'] # Standardizing the features scaler = StandardScaler() X = scaler.fit_transform(X) return X, y def train_and_evaluate(file_path): Trains and evaluates a regression model on the provided dataset. X, y = preprocess_data(file_path) # Splitting the data into training and testing sets (80% training, 20% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Choosing and training the model (Linear Regression) model = LinearRegression() model.fit(X_train, y_train) # Making predictions on the test data y_pred = model.predict(X_test) # Evaluating the model mae = mean_absolute_error(y_test, y_pred) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return mae, mse, r2"},{"question":"# Question: Signal Handling in Python You are required to implement a function that uses the `signal` module to handle a specific signal in Python. Problem Statement: Write a Python script that does the following: 1. **Function**: Define a function `handle_interrupt_signal` that: - Takes no arguments. - Sets up a custom signal handler for the `SIGINT` signal (typically sent via `Ctrl+C` from the keyboard). - The custom signal handler should print \\"Interrupt signal received!\\" and set a global flag `interrupt_received` to `True`. 2. **Process**: In the main part of your script: - Use the `signal.alarm()` function to raise a `SIGALRM` signal after a specified number of seconds. - Configure the `SIGALRM` signal to be handled by a function that raises an `OSError` with the message \\"Timeout!\\". 3. **Loop**: - Start an infinite loop that continuously waits for signals using `signal.pause()`. - If at any point the `interrupt_received` flag is set to `True`, catch the signal and exit the loop, printing \\"Exiting due to interrupt\\". 4. **Constraints**: - You should use only the `signal` module for signal handling. - Assume the environment is Unix-based and supports all described signal features. - Do not use multithreading or multiprocessing libraries. Example: For instance, if the script is running and the user hits `Ctrl+C`, you should see: ``` Interrupt signal received! Exiting due to interrupt ``` If the specified number of seconds elapses before any `Ctrl+C` action, you should see: ``` Timeout! ``` Notes: - Make sure to handle the signals properly following Python\'s documented behavior, ensuring the signal handlers execute as intended. ```python import signal import time interrupt_received = False def handle_interrupt_signal(): global interrupt_received def signal_handler(signum, frame): global interrupt_received print(\\"Interrupt signal received!\\") interrupt_received = True signal.signal(signal.SIGINT, signal_handler) def alarm_handler(signum, frame): raise OSError(\\"Timeout!\\") # Your implementation here handle_interrupt_signal() signal.signal(signal.SIGALRM, alarm_handler) signal.alarm(10) # Set the alarm for 10 seconds while True: try: signal.pause() # Wait for signals except OSError as e: print(e) break if interrupt_received: print(\\"Exiting due to interrupt\\") break ``` **Note**: You can adjust the alarm time as needed for testing purposes.","solution":"import signal import time interrupt_received = False def handle_interrupt_signal(): global interrupt_received def signal_handler(signum, frame): global interrupt_received print(\\"Interrupt signal received!\\") interrupt_received = True signal.signal(signal.SIGINT, signal_handler) def alarm_handler(signum, frame): raise OSError(\\"Timeout!\\") # Your implementation here handle_interrupt_signal() signal.signal(signal.SIGALRM, alarm_handler) signal.alarm(10) # Set the alarm for 10 seconds while True: try: signal.pause() # Wait for signals except OSError as e: print(e) break if interrupt_received: print(\\"Exiting due to interrupt\\") break"},{"question":"You are given two datasets, `fmri` and `seaice`. Your task is to create a visualization that compares the temperature signals over time from the `fmri` dataset for different events and compares the extent of sea ice over two specific years (1980 and 2019) from the `seaice` dataset. # Requirements: 1. Load the `fmri` dataset and filter it to include only data from the \\"parietal\\" region. 2. Load the `seaice` dataset, and: - Create two new columns: `Day` (day of year) and `Year`. - Consider data from the years 1980 onwards. - Pivot the dataset so that `Day` is the index, and columns represent years, with values being the `Extent`. - Filter out only the columns for the years 1980 and 2019. - Remove any rows with missing values. 3. Using seaborn, create two plots: - The first plot visualizes the signal over time for different events in the `fmri` dataset using a line plot with error bands. - The second plot shows the extent of sea ice over the days of the year for the years 1980 and 2019. # Input: No direct input, datasets should be loaded within the code. # Output: Display the required plots. # Implementation Constraints: - You must use `seaborn.objects` for plotting. - The first plot should use the `fmri` dataset filtered by the \\"parietal\\" region and display signals over time with error bands partitioned by different events. - The second plot should use the `seaice` dataset to show ice extent for 1980 and 2019 using bands and lines. # Example: Here is an example of how you might start the implementation: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load and manipulate fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and manipulate seaice dataset seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Create first plot for fmri dataset p1 = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) p1.show() # Create second plot for seaice dataset p2 = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") p2.add(so.Band(alpha=.5, edgewidth=2)) p2.show() ``` Complete the code to satisfy the stated requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def visualize_fmri_and_seaice(): # Load and manipulate fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and manipulate seaice dataset seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Create first plot for fmri dataset p1 = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) # Show the first plot for fmri dataset p1.show() # Create second plot for seaice dataset p2 = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") p2 = p2.add(so.Band(alpha=.5, edgewidth=2)) # Show the second plot for seaice dataset p2.show()"},{"question":"# **Unsupervised Learning with Clustering** **Objective:** Use scikit-learn to perform clustering on a dataset and evaluate the performance of the clustering algorithm. **Problem Statement:** You are provided with the `Iris` dataset, a classic dataset in the field of machine learning. Your task is to implement K-Means clustering using scikit-learn to group the iris flowers into clusters. **Task Details:** 1. **Load the Dataset:** - Use scikit-learn\'s `datasets` module to load the Iris dataset. 2. **Implement K-Means Clustering:** - Apply K-Means clustering to the dataset with a predefined number of clusters (`k=3`). - Use the `KMeans` class from the `sklearn.cluster` module. - Fit the K-Means model on the Iris dataset. 3. **Evaluate the Clustering:** - Calculate the Davies-Bouldin score to measure the quality of the clustering. This score indicates the average “similarity ratio” of each cluster with respect to all the others. The lower the score, the better the clustering. - Use `davies_bouldin_score` from the `sklearn.metrics` module. 4. **Visualize the Results:** - Using matplotlib, create a scatter plot of the first two features of the Iris dataset, color-coded by the clusters found by the K-Means algorithm. - Plot the cluster centers as well. **Constraints:** - Do not use any other clustering methods for this task. - Ensure the K-Means algorithm runs efficiently and converges. **Input:** - None (The Iris dataset should be loaded using scikit-learn\'s `datasets` module). **Output:** - The Davies-Bouldin score of the clustering. - A scatter plot displaying the clusters and their centers. **Performance Requirements:** - The solution should handle the Iris dataset efficiently and produce the results in a reasonable amount of time. **Example:** ```python from sklearn import datasets from sklearn.cluster import KMeans from sklearn.metrics import davies_bouldin_score import matplotlib.pyplot as plt # Load the Iris dataset iris = datasets.load_iris() X = iris.data # Apply K-Means clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans.fit(X) # Calculate Davies-Bouldin score db_score = davies_bouldin_score(X, kmeans.labels_) print(f\'Davies-Bouldin Score: {db_score}\') # Visualize the results plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap=\'viridis\') centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c=\'red\', s=200, alpha=0.75) plt.xlabel(iris.feature_names[0]) plt.ylabel(iris.feature_names[1]) plt.title(\'K-Means Clustering of Iris Dataset\') plt.show() ``` Please ensure your solution follows this example format, adapting as needed for clarity and efficiency.","solution":"from sklearn import datasets from sklearn.cluster import KMeans from sklearn.metrics import davies_bouldin_score import matplotlib.pyplot as plt def kmeans_iris_clustering(): # Load the Iris dataset iris = datasets.load_iris() X = iris.data # Apply K-Means clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans.fit(X) # Calculate Davies-Bouldin score db_score = davies_bouldin_score(X, kmeans.labels_) print(f\'Davies-Bouldin Score: {db_score}\') # Visualize the results plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap=\'viridis\') centers = kmeans.cluster_centers_ plt.scatter(centers[:, 0], centers[:, 1], c=\'red\', s=200, alpha=0.75) plt.xlabel(iris.feature_names[0]) plt.ylabel(iris.feature_names[1]) plt.title(\'K-Means Clustering of Iris Dataset\') plt.show() return db_score"},{"question":"You are given the task of creating a command-line utility using the \\"uu\\" module to encode and decode files in uuencode format. This utility should be able to: 1. Encode a given input file to uuencode format. 2. Decode a given uuencoded input file to its original format. 3. Handle any potential errors that might arise during these processes and provide appropriate error messages to the user. # Task Implement a function `uuencode_util(action, in_file_path, out_file_path)` that takes the following parameters: - `action` (string): It can be either `\'encode\'` or `\'decode\'`, specifying the action to perform. - `in_file_path` (string): The path to the input file. - `out_file_path` (string): The path to the output file. The function should: - Encode the input file to uuencode format and save the output if the `action` is `\'encode\'`. - Decode the uuencoded input file and save the original file if the `action` is `\'decode\'`. - Handle and print user-friendly error messages for any exceptions or errors encountered during the process. Constraints - The input file path is guaranteed to be a valid existing path. - The output path should be writable. Example ```python # Example input: A text file named \'example.txt\' containing \\"Hello World\\" which we desire to encode to \'encoded.txt\' uuencode_util(\'encode\', \'example.txt\', \'encoded.txt\') # Subsequently, decode \'encoded.txt\' back to \'decoded.txt\' uuencode_util(\'decode\', \'encoded.txt\', \'decoded.txt\') ``` Expected Output - After encoding, \'encoded.txt\' should contain the uuencoded representation of \'example.txt\'. - After decoding, \'decoded.txt\' should be identical to the original \'example.txt\' file. Note Ensure that you thoroughly handle all potential exceptions, especially those related to file I/O and invalid uuencode format.","solution":"import uu import os def uuencode_util(action, in_file_path, out_file_path): Encodes or decodes a file using uuencode format. Parameters: action (str): \'encode\' to convert input file to uuencode format, \'decode\' to revert uuencoded file to original format. in_file_path (str): Path to the input file. out_file_path (str): Path to the output file. Returns: None try: if action == \'encode\': with open(in_file_path, \'rb\') as in_file, open(out_file_path, \'wb\') as out_file: uu.encode(in_file, out_file, os.path.basename(in_file_path)) elif action == \'decode\': with open(in_file_path, \'rb\') as in_file, open(out_file_path, \'wb\') as out_file: uu.decode(in_file, out_file) else: raise ValueError(\\"Invalid action. Use \'encode\' or \'decode\'.\\") except FileNotFoundError: print(f\\"Error: The file at {in_file_path} does not exist.\\") except PermissionError: print(f\\"Error: Permission denied when trying to access {out_file_path}.\\") except ValueError as ve: print(f\\"Error: {ve}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"**PyTorch Testing with Custom Function Implementation** You are tasked with implementing a PyTorch function that performs a specific arithmetic operation on two tensors and then writing tests to ensure your function works correctly using PyTorch\'s `torch.testing` utilities. **Function to Implement** Implement a function `tensor_add_multiply` that takes two tensors, adds them together, and then multiplies the result by a scalar value. ```python import torch def tensor_add_multiply(tensor1: torch.Tensor, tensor2: torch.Tensor, scalar: float) -> torch.Tensor: Adds two tensors element-wise and multiplies the result by a scalar. Parameters: - tensor1 (torch.Tensor): The first input tensor. - tensor2 (torch.Tensor): The second input tensor. - scalar (float): The scalar value to multiply with the resulting tensor. Returns: - torch.Tensor: The resulting tensor after performing the operation. # Implement the function pass ``` **Testing the Function** Write a test function `test_tensor_add_multiply` to test the `tensor_add_multiply` function using the utilities provided by the `torch.testing` module. Use the `make_tensor` function to generate tensors for your test cases, and use `assert_close` or `assert_allclose` to validate your results. ```python def test_tensor_add_multiply(): Tests the tensor_add_multiply function using PyTorch testing utilities. # Generate test tensors using make_tensor tensor1 = torch.testing.make_tensor(shape=(2, 2), dtype=torch.float32) tensor2 = torch.testing.make_tensor(shape=(2, 2), dtype=torch.float32) scalar = 3.0 # Calculate the expected result manually or using simple operations expected_result = (tensor1 + tensor2) * scalar # Get the actual result from the implemented function actual_result = tensor_add_multiply(tensor1, tensor2, scalar) # Use assert_close or assert_allclose to check if the results are as expected torch.testing.assert_close(actual_result, expected_result, rtol=1e-5, atol=1e-8) ``` **Expected Input and Output Formats** - Input: Two 2D tensors of shape (2, 2) and a scalar value. - Output: A 2D tensor of shape (2, 2) that represents the result of the operation. **Constraints** - Ensure that your function works correctly for tensors of different shapes and data types. - Your unit tests should cover edge cases, such as large and small values, to ensure robustness. **Performance Requirements** - The implementation should efficiently handle common tensor operations without unnecessary overhead. - The test function should cover varying tensor contents and verify the correctness of the results within a reasonable tolerance level.","solution":"import torch def tensor_add_multiply(tensor1: torch.Tensor, tensor2: torch.Tensor, scalar: float) -> torch.Tensor: Adds two tensors element-wise and multiplies the result by a scalar. Parameters: - tensor1 (torch.Tensor): The first input tensor. - tensor2 (torch.Tensor): The second input tensor. - scalar (float): The scalar value to multiply with the resulting tensor. Returns: - torch.Tensor: The resulting tensor after performing the operation. return (tensor1 + tensor2) * scalar"},{"question":"# Advanced Coding Assessment Objective: To assess your understanding of Python\'s runtime services, specifically focusing on data classes and context management. Problem Statement: You are required to design a system that processes and manages data entries. Each data entry contains information about an individual including their name, age, and email. The system should be able to load data entries, perform modifications, and ensure proper resource management during processing. Implement a Python class `DataProcessor` which includes the following functionalities: 1. **Data Class Definition:** - Use Python\'s `dataclasses` module to define a data class `Person` with the following fields: - `name` (string, required) - `age` (integer, required) - `email` (string, required) 2. **Context Management for Resource Handling:** - Use Python\'s `contextlib` module to implement context management for resource handling in the `DataProcessor` class. - The `DataProcessor` class should use a context manager to ensure that all resources are released after data processing. 3. **Implementation Details:** - The `DataProcessor` class should have the following methods: - `load_data(self, data: List[dict]) -> None`: Loads data from a list of dictionaries where each dictionary represents an individual. - `update_email(self, name: str, new_email: str) -> bool`: Updates the email of the individual with the given name. Returns `True` if the update was successful, `False` otherwise. - `save_data(self) -> List[dict]`: Returns the current state of data entries. 4. **Constraints:** - The context manager should ensure that data is saved before exiting the context. - No resources should be left open/unmanaged after processing. Input and Output Format: - **Input:** - The `load_data` method takes a list of dictionaries, each containing keys `name`, `age`, and `email`. - The `update_email` method takes a string `name` and a string `new_email`. - **Output:** - The `save_data` method returns a list of dictionaries reflecting the current state of the data. Example: ```python from dataclasses import dataclass from contextlib import contextmanager from typing import List, Dict @dataclass class Person: name: str age: int email: str class DataProcessor: def __init__(self): self.data = [] @contextmanager def manage_resources(self): try: yield finally: self.save_data() def load_data(self, data: List[Dict[str, str]]) -> None: self.data = [Person(**entry) for entry in data] def update_email(self, name: str, new_email: str) -> bool: for person in self.data: if person.name == name: person.email = new_email return True return False def save_data(self) -> List[Dict[str, str]]: return [dataclasses.asdict(person) for person in self.data] # Example Usage processor = DataProcessor() with processor.manage_resources(): processor.load_data([ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"email\\": \\"alice@example.com\\"}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"email\\": \\"bob@example.com\\"}, ]) processor.update_email(\\"Alice\\", \\"alice@newdomain.com\\") ``` You should implement the `DataProcessor` class adhering to the specifications above. Make sure your code handles edge cases and follows best practices for resource management.","solution":"from dataclasses import dataclass, asdict from contextlib import contextmanager from typing import List, Dict @dataclass class Person: name: str age: int email: str class DataProcessor: def __init__(self): self.data = [] @contextmanager def manage_resources(self): try: yield finally: self.save_data() def load_data(self, data: List[Dict[str, str]]) -> None: self.data = [Person(**entry) for entry in data] def update_email(self, name: str, new_email: str) -> bool: for person in self.data: if person.name == name: person.email = new_email return True return False def save_data(self) -> List[Dict[str, str]]: self.saved_data = [asdict(person) for person in self.data] return self.saved_data"},{"question":"# Persistent Storage Using Shelve: Implement a Simple User Database **Objective:** Implement a user database that can store, retrieve, update, and delete user profiles using the `shelve` module. Each user profile will be a dictionary containing the user\'s `username`, `email`, and `age`. The database operations must be carried out in a way that ensures data persistence across multiple runs of the script. **Requirements:** 1. **Initialize Database:** - Write a function `initialize_db(filename: str) -> None` that creates or opens a shelf with the provided filename. - This function should clear any existing data if the file already exists. 2. **Add User:** - Write a function `add_user(db: shelve.Shelf, username: str, email: str, age: int) -> None` that adds a new user to the database. - Ensure that usernames are unique. 3. **Retrieve User:** - Write a function `retrieve_user(db: shelve.Shelf, username: str) -> dict` that retrieves the user profile for the given username. - Raise a `KeyError` if the user does not exist. 4. **Update User:** - Write a function `update_user(db: shelve.Shelf, username: str, email: str = None, age: int = None) -> None` that updates the email and/or age of the user. - Raise a `KeyError` if the user does not exist. 5. **Delete User:** - Write a function `delete_user(db: shelve.Shelf, username: str) -> None` that deletes the user profile for the given username. - Raise a `KeyError` if the user does not exist. 6. **List All Users:** - Write a function `list_users(db: shelve.Shelf) -> list` that returns a list of all usernames in the database. 7. **Main Function:** - Write a `main` function to demonstrate the functionality: 1. Initialize the database. 2. Add a few users. 3. Retrieve and print a user profile. 4. Update a user profile. 5. Delete a user profile. 6. List and print all user profiles. **Constraints:** - Ensure that the database is properly closed after operations are done. - Use context managers (`with` statement) where applicable. **Performance Requirements:** - The implementation should handle typical use cases efficiently. For simplicity, performance guarantees are not strict, as this is primarily an educational exercise. **Example:** ```python def main(): filename = \'user_db\' initialize_db(filename) with shelve.open(filename) as db: add_user(db, \'johndoe\', \'john@example.com\', 30) add_user(db, \'janedoe\', \'jane@example.com\', 25) print(retrieve_user(db, \'johndoe\')) # {\'username\': \'johndoe\', \'email\': \'john@example.com\', \'age\': 30} update_user(db, \'johndoe\', age=31) print(retrieve_user(db, \'johndoe\')) # {\'username\': \'johndoe\', \'email\': \'john@example.com\', \'age\': 31} delete_user(db, \'janedoe\') print(list_users(db)) # [\'johndoe\'] if __name__ == \\"__main__\\": main() ``` **Note:** The example provided should not be included in the final solution submitted by the students, as it is mainly for illustrative purposes.","solution":"import shelve def initialize_db(filename: str) -> None: Initializes the database, clearing any existing data if the file already exists. with shelve.open(filename, \'c\') as db: db.clear() def add_user(db: shelve.Shelf, username: str, email: str, age: int) -> None: Adds a new user to the database. Ensures that usernames are unique. if username in db: raise ValueError(f\\"User {username} already exists\\") db[username] = {\'username\': username, \'email\': email, \'age\': age} def retrieve_user(db: shelve.Shelf, username: str) -> dict: Retrieves the user profile for the given username. Raises KeyError if user does not exist. if username in db: return db[username] else: raise KeyError(f\\"User {username} not found\\") def update_user(db: shelve.Shelf, username: str, email: str = None, age: int = None) -> None: Updates the email and/or age of the user. Raises KeyError if user does not exist. if username in db: user = db[username] if email is not None: user[\'email\'] = email if age is not None: user[\'age\'] = age db[username] = user else: raise KeyError(f\\"User {username} not found\\") def delete_user(db: shelve.Shelf, username: str) -> None: Deletes the user profile for the given username. Raises KeyError if user does not exist. if username in db: del db[username] else: raise KeyError(f\\"User {username} not found\\") def list_users(db: shelve.Shelf) -> list: Returns a list of all usernames in the database. return list(db.keys()) def main(): filename = \'user_db\' initialize_db(filename) with shelve.open(filename) as db: add_user(db, \'johndoe\', \'john@example.com\', 30) add_user(db, \'janedoe\', \'jane@example.com\', 25) print(retrieve_user(db, \'johndoe\')) # {\'username\': \'johndoe\', \'email\': \'john@example.com\', \'age\': 30} update_user(db, \'johndoe\', age=31) print(retrieve_user(db, \'johndoe\')) # {\'username\': \'johndoe\', \'email\': \'john@example.com\', \'age\': 31} delete_user(db, \'janedoe\') print(list_users(db)) # [\'johndoe\'] if __name__ == \\"__main__\\": main()"},{"question":"# Regex-based Text Processing # Objective: Your task is to write a Python function that identifies patterns in text and transforms it based on specific criteria using regular expressions. You will demonstrate proficiency with the `re` module by implementing a function that extracts and processes data from a given string. # Function Signature: ```python def process_text(input_string: str) -> list: Processes the input string to identify specified patterns and transforms it. Args: input_string (str): A string containing sentences with various information. Returns: list: A list of dictionaries, where each dictionary has extracted data with specific keys. ``` # Tasks: 1. Extract all dates from the input string in the formats `DD-MM-YYYY`, `YYYY/MM/DD`, or `Month Day, Year` (e.g., `12-05-2021`, `2021/05/12`, `March 5, 2021`). 2. Replace all instances of email addresses with the text `[REDACTED]`. The email addresses will follow the pattern `example@domain.com`. 3. Extract all sequences of words that end with `\'ly\'`. The dictionary for each sentence should have the following structure: ```python { \'dates\': list of dates found, \'modified_text\': text with email addresses redacted, \'ly_words\': list of words ending with \'ly\' } ``` # Constraints: - The input string can include multiple sentences. - Sentences can contain special characters and numbers. - Ensure efficiency considering the possible large size of the input string. # Example: ```python input_string = The event is scheduled for 12-05-2021. Contact us at example@domain.com for more details. Another meeting is on 2021/05/12. She quickly fixed the problem. His actions were ridiculously extravagant. output = process_text(input_string) print(output) # Expected output: # [ # { # \'dates\': [\'12-05-2021\'], # \'modified_text\': \'The event is scheduled for 12-05-2021. Contact us at [REDACTED] # for more details. Another meeting is on 2021/05/12. She quickly fixed # the problem. His actions were ridiculously extravagant.\', # \'ly_words\': [\'quickly\', \'ridiculously\'] # }, # { # \'dates\': [\'2021/05/12\'], # \'modified_text\': \'The event is scheduled for 12-05-2021. Contact us at [REDACTED] # for more details. Another meeting is on 2021/05/12. She quickly fixed # the problem. His actions were ridiculously extravagant.\', # \'ly_words\': [\'quickly\', \'ridiculously\'] # } # ] ``` You need to handle the patterns appropriately and ensure the function passes varied test cases by correctly identifying and transforming the text as specified.","solution":"import re def process_text(input_string: str) -> list: Processes the input string to identify specified patterns and transforms it. Args: input_string (str): A string containing sentences with various information. Returns: list: A list of dictionaries, where each dictionary has extracted data with specific keys as defined in the task. # Define regex patterns date_pattern = r\'b(?:d{2}-d{2}-d{4}|d{4}/d{2}/d{2}|(?:January|February|March|April|May|June|July|August|September|October|November|December) d{1,2}, d{4})b\' email_pattern = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' ly_words_pattern = r\'bw+lyb\' # Extract dates dates = re.findall(date_pattern, input_string) # Redact email addresses modified_text = re.sub(email_pattern, \'[REDACTED]\', input_string) # Extract words ending in \'ly\' ly_words = re.findall(ly_words_pattern, input_string) # Structure the result result = [{ \'dates\': dates, \'modified_text\': modified_text, \'ly_words\': ly_words }] return result"},{"question":"# Dynamic Module Execution with `runpy` You are required to create a Python program that utilizes the `runpy` module to dynamically execute Python modules based on user input. This exercise aims to test your understanding and ability to use the `runpy` functions effectively while handling various edge cases. Task 1. **Function Implementation**: Write a function `execute_module` that: - Takes two arguments: - `module_name`: The name of the module to be executed. This can be an absolute module name or a path to a Python script. - `use_path`: A boolean flag that indicates whether to use `run_module` (if `False`) or `run_path` (if `True`). 2. **Function Behavior**: - If `use_path` is `True`, use `runpy.run_path` to execute the module. - If `use_path` is `False`, use `runpy.run_module` to execute the module. - Capture the returned globals dictionary from `runpy` and extract the value of a specific variable named `result`. - Return the extracted value of `result`. 3. **Constraints**: - Ensure to handle exceptions properly. If the module does not exist or fails to execute, return the string `\\"Error\\"`. - The module or script being executed will always define a variable named `result` which your function should return. 4. **Input and Output**: - Input: A string representing `module_name`, and a boolean representing `use_path`. - Output: The value of the variable `result` from the executed module, or `\\"Error\\"` if an exception occurs. Example Usage ```python # A sample module `sample_module.py`: # result = 42 print(execute_module(\'sample_module\', False)) # Should print: 42 # A sample script `/path/to/sample_script.py`: # result = \\"Hello, World!\\" print(execute_module(\'/path/to/sample_script.py\', True)) # Should print: \\"Hello, World!\\" ``` Notes: - The `module_name` can either be the name of a module available in the Python environment or a complete path to a Python script file. - Handle thread-safety considerations and ensure minimal impact on global state.","solution":"import runpy def execute_module(module_name, use_path): Executes a Python module or script and returns the value of the variable \'result\'. Args: - module_name: str, the name of the module or path to the script. - use_path: bool, if True uses run_path, if False uses run_module. Returns: - The value of the variable \'result\' from the executed module or script. - \\"Error\\" if an exception occurs. try: if use_path: result_globals = runpy.run_path(module_name) else: result_globals = runpy.run_module(module_name) return result_globals.get(\'result\', \\"Error\\") except Exception: return \\"Error\\""},{"question":"# PyTorch Conditional Execution with `torch.cond` Objective This question assesses your understanding of PyTorch\'s `torch.cond` structured control flow operator, as well as your ability to implement dynamic models that execute different operations based on conditions specified using tensor values or shapes. Problem Statement You are required to implement a PyTorch module that utilizes the `torch.cond` operator to perform different computations based on the properties of the input tensor. Specifically, you need to create a model that performs the following: 1. If the sum of the input tensor elements is greater than a threshold value (5.0): - Compute the element-wise square of the tensor and then apply the cosine function to each element. 2. If the sum of the input tensor elements is less than or equal to the threshold value: - Compute the element-wise square root of the tensor (ensure all elements are non-negative) and then apply the sine function to each element. Requirements - Implement a class `DynamicConditionalModel` which extends `torch.nn.Module`. - Implement a method `forward` in the class which uses `torch.cond` to determine the control flow based on the sum of the elements of the input tensor. - The `forward` method should accept a tensor `x` as input and apply the relevant operations based on the conditions specified. Input Format - A 1D tensor `x` of arbitrary size with non-negative elements. Output Format - A tensor of the same shape as the input, containing the results of the specified operations. Constraints - The input tensor `x` will contain non-negative values only. - You must use `torch.cond` to implement the conditional logic. Example ```python import torch import torch.nn as nn class DynamicConditionalModel(nn.Module): def __init__(self): super(DynamicConditionalModel, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor) -> torch.Tensor: return torch.cos(x ** 2) def false_fn(x: torch.Tensor) -> torch.Tensor: return torch.sin(torch.sqrt(x)) return torch.cond(x.sum() > 5.0, true_fn, false_fn, (x,)) model = DynamicConditionalModel() inp1 = torch.tensor([1.0, 2.0, 1.0, 1.0]) inp2 = torch.tensor([2.0, 2.0, 2.0]) print(model(inp1)) print(model(inp2)) ``` Expected Output For `inp1` = [1.0, 2.0, 1.0, 1.0] (sum = 5.0, applying false_fn): ``` tensor([0.8415, 0.9093, 0.8415, 0.8415]) ``` For `inp2` = [2.0, 2.0, 2.0] (sum = 6.0, applying true_fn): ``` tensor([0.4081, 0.4081, 0.4081]) ``` Ensure your implementation correctly uses the `torch.cond` operation and returns the expected results based on the input tensor.","solution":"import torch import torch.nn as nn class DynamicConditionalModel(nn.Module): def __init__(self): super(DynamicConditionalModel, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: threshold = 5.0 def true_fn() -> torch.Tensor: return torch.cos(x ** 2) def false_fn() -> torch.Tensor: return torch.sin(torch.sqrt(x)) return true_fn() if x.sum() > threshold else false_fn() # Example usage: model = DynamicConditionalModel() inp1 = torch.tensor([1.0, 2.0, 1.0, 1.0]) inp2 = torch.tensor([2.0, 2.0, 2.0]) print(model(inp1)) print(model(inp2))"},{"question":"# Custom Pretty Printer with pprint Objective Your task is to create a custom pretty-printer function that leverages the `pprint` module to format and print a given nested dictionary containing various data types, including lists, tuples, sets, and other dictionaries. Your custom pretty-printer should allow specifying output characteristics such as indentation level, maximum width, list compactness, and dictionary key sorting. Problem Statement Implement a function named `custom_pretty_print` that accepts the following parameters: - `data` (dict): The nested data structure you will be pretty-printing. - `indent` (int, optional): The amount of indentation added for each nesting level. Default is 2. - `width` (int, optional): Specifies the desired maximum number of characters per line in the output. Default is 80. - `depth` (int, optional): Controls the number of nesting levels which may be printed; if the data structure being printed is deeper, the next contained level is replaced by \\"...\\". Default is None (no constraint). - `compact` (bool, optional): Impacts the way that long sequences are formatted. If True, as many items as will fit within the width will be formatted on each output line. Default is False. - `sort_dicts` (bool, optional): If True, dictionaries will be formatted with their keys sorted. If False, they will display in insertion order. Default is True. - `underscore_numbers` (bool, optional): If True, integers will be formatted with the \\"_\\" character for a thousands separator. Default is False. The function should use these parameters to configure a `PrettyPrinter` instance and print the formatted representation of the `data`. Expected Function Signature ```python def custom_pretty_print(data: dict, indent: int = 2, width: int = 80, depth: int = None, compact: bool = False, sort_dicts: bool = True, underscore_numbers: bool = False): pass ``` Example Usage ```python from pprint import pprint # For verifying output sample_data = { \'Name\': \'Alice\', \'Age\': 30, \'Hobbies\': [\'Reading\', \'Cycling\', \'Hiking\'], \'Education\': { \'Undergraduate\': {\'Institution\': \'ABC University\', \'Year\': 2012}, \'Graduate\': {\'Institution\': \'XYZ University\', \'Year\': 2016} }, \'Work\': { \'Company A\': {\'Role\': \'Developer\', \'Years\': 3}, \'Company B\': {\'Role\': \'Team Lead\', \'Years\': 2} } } # Example call custom_pretty_print(sample_data, indent=4, width=50, depth=2, compact=True, sort_dicts=False, underscore_numbers=True) # Expected output should be formatted according to provided parameters. ``` Constraints - The dictionary keys and values will be of standard Python data types (strings, integers, lists, tuples, sets, dictionaries). - The function should handle any reasonably nested dictionary without running into recursion limits. Notes - Use the `pprint` module\'s `PrettyPrinter` class for the implementation. - Ensure that your code handles edge cases like empty dictionaries and lists gracefully.","solution":"import pprint def custom_pretty_print(data: dict, indent: int = 2, width: int = 80, depth: int = None, compact: bool = False, sort_dicts: bool = True, underscore_numbers: bool = False): Pretty prints a nested dictionary according to the specified formatting parameters. :param data: The nested dictionary to be pretty-printed. :param indent: Indentation level for nested structures. :param width: Maximum width of the printed lines. :param depth: Maximum depth to print, replacing deeper levels with \'...\'. :param compact: Whether to fit as many items as possible within the width. :param sort_dicts: Whether to sort dictionary keys before printing. :param underscore_numbers: Whether to use underscores as thousands separators in integers. printer = pprint.PrettyPrinter(indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts, underscore_numbers=underscore_numbers) printer.pprint(data)"},{"question":"# Mini Chat Server using `asyncio` Streams Objective: Design and implement a mini chat server using `asyncio` streams. The server should be able to handle multiple clients simultaneously, broadcast messages from any client to all connected clients, and properly manage connection resources. Instructions: 1. Implement a function `handle_client(reader, writer)` that handles each client\'s connection. This function should: - Continuously read messages from the client. - Broadcast any non-empty message to all connected clients. - Handle client disconnection gracefully. 2. Implement a function `start_chat_server(host, port)` that starts the chat server. This function should: - Use `asyncio.start_server` to start the server on the specified host and port. - Maintain a list of connected clients. - Pass the client connections to `handle_client`. 3. Use an `asyncio.Queue` to manage messages for broadcasting. 4. Ensure proper resource management by closing client connections when they are done. Input: - `host` (string): The hostname or IP address where the server will run (e.g., `\'127.0.0.1\'`). - `port` (int): The port number on which the server will listen for incoming connections (e.g., `8888`). Output: The server should keep running indefinitely, handling multiple clients. There is no direct output from the server, but it should print server logs to stdout to indicate various events (e.g., client connected, message received, client disconnected). Constraints: - The server should handle at least 10 clients concurrently. - Each client message should be broadcast within 100 ms of being received. Example Code to Start the Server: ```python import asyncio async def handle_client(reader, writer): # Your implementation here... async def start_chat_server(host=\'127.0.0.1\', port=8888): # Your implementation here... if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8888 asyncio.run(start_chat_server(host, port)) ``` Notes: - You can assume that all messages are UTF-8 encoded strings. - Make sure to handle exceptions and errors gracefully. - Ensure that the server can correctly close connections when a client disconnects or a shutdown signal is received. This task will test your ability to manage asynchronous I/O operations, work with network connections using `asyncio` streams, and handle concurrency in Python.","solution":"import asyncio clients = [] async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"{addr} is connected\\") try: while True: data = await reader.read(100) message = data.decode() if not message: break broadcast_message = f\\"{addr}: {message}\\" print(f\\"Received {broadcast_message}\\") await broadcast(broadcast_message) except asyncio.CancelledError: pass finally: clients.remove(writer) writer.close() await writer.wait_closed() print(f\\"{addr} is disconnected\\") async def broadcast(message): for client in clients: client.write(message.encode()) await client.drain() async def start_chat_server(host=\'127.0.0.1\', port=8888): server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Server started on {addr}\') async with server: await server.serve_forever()"},{"question":"Coding Assessment Question # Objective To assess the comprehension of handling toy datasets provided by scikit-learn, preprocessing data, applying machine learning algorithms, and evaluating the resulting models. # Question You are given the Wine dataset from scikit-learn\'s toy datasets. Your task is to create a classification model to predict the class of wine based on its features. You will need to: 1. Load the Wine dataset using `sklearn.datasets.load_wine`. 2. Preprocess the data by performing the following steps: - Split the dataset into training and testing sets (80% training, 20% testing). - Standardize the features by removing the mean and scaling to unit variance. 3. Implement and fit a Logistic Regression model to the training data. 4. Evaluate the model using the testing data and calculate the following metrics: - Accuracy - Precision - Recall - F1 score 5. Display the metrics in a readable format. # Constraints - Use only the scikit-learn package for model implementation and evaluation. - Use random state `42` for reproducibility when splitting the data. # Input Format - No input is required from the user; the dataset should be loaded within the code. # Output Format - Print the accuracy, precision, recall, and F1 score of the model. # Example ``` Accuracy: 0.97 Precision: 0.96 Recall: 0.97 F1 Score: 0.96 ``` # Implementation ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def wine_classification(): # Load the Wine dataset data = load_wine() X = data.data y = data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement and fit a Logistic Regression model model = LogisticRegression(random_state=42) model.fit(X_train, y_train) # Predict the labels for the testing set y_pred = model.predict(X_test) # Calculate evaluation metrics acc = accuracy_score(y_test, y_pred) prec = precision_score(y_test, y_pred, average=\'weighted\') rec = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') # Display the metrics print(f\\"Accuracy: {acc:.2f}\\") print(f\\"Precision: {prec:.2f}\\") print(f\\"Recall: {rec:.2f}\\") print(f\\"F1 Score: {f1:.2f}\\") # Uncomment the line below to run the function # wine_classification() ```","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def wine_classification(): # Load the Wine dataset data = load_wine() X = data.data y = data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement and fit a Logistic Regression model model = LogisticRegression(random_state=42) model.fit(X_train, y_train) # Predict the labels for the testing set y_pred = model.predict(X_test) # Calculate evaluation metrics acc = accuracy_score(y_test, y_pred) prec = precision_score(y_test, y_pred, average=\'weighted\') rec = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') # Display the metrics print(f\\"Accuracy: {acc:.2f}\\") print(f\\"Precision: {prec:.2f}\\") print(f\\"Recall: {rec:.2f}\\") print(f\\"F1 Score: {f1:.2f}\\") return acc, prec, rec, f1 # Uncomment the line below to run the function # wine_classification()"},{"question":"**Title: Implementing Cross-Version Compatible Function** **Objective:** Write a Python function that reads a file, processes its content, and writes the processed content to another file. Ensure that the code is compatible with both Python 2.7 and Python 3.X versions. The function should correctly handle text and binary data and demonstrate the use of modern Python practices. **Function Specifications:** - **Function Name:** `process_file` - **Input Parameters:** - `input_file_path` (str): The path of the input file to read. - `output_file_path` (str): The path of the output file to write. - `mode` (str): The mode in which to read and write files. It can be either \'text\' or \'binary\'. - **Output:** None. The function writes the processed content to the specified output file. **Processing Requirements:** 1. If the file mode is \'text\': - Read the file content as a string. - Convert the string to uppercase. - Write the string content to the output file. 2. If the file mode is \'binary\': - Read the file content as binary data. - Reverse the binary content. - Write the binary content to the output file. **Constraints:** - Use `io.open()` for file operations. - Use `from __future__ import division`. - Handle any discrepancies between string and binary operations for ensuring compatibility. - Incorporate feature detection instead of version detection. **Example Usage:** ```python # Assuming \'input.txt\' contains \\"hello world\\" and \'input.bin\' contains b\'hello world\' process_file(\'input.txt\', \'output.txt\', \'text\') # The \'output.txt\' should contain \\"HELLO WORLD\\" process_file(\'input.bin\', \'output.bin\', \'binary\') # The \'output.bin\' should contain b\'dlrow olleh\' ``` # Implementation Details: Make sure to: - Add proper import statements to ensure compatibility between Python 2 and 3. - Implement both text and binary handling ensuring type consistency. - Use best practices for cross-version compatibility as discussed in the provided documentation. Implement the function `process_file` below: ```python def process_file(input_file_path, output_file_path, mode): # Add your implementation here pass ```","solution":"from __future__ import division import io def process_file(input_file_path, output_file_path, mode): Processes the content of the input file and writes the processed content to the output file. Parameters: input_file_path (str): The path of the input file to read. output_file_path (str): The path of the output file to write. mode (str): The mode in which to read and write files. It can be either \'text\' or \'binary\'. if mode == \'text\': with io.open(input_file_path, \'r\', encoding=\'utf-8\') as infile: content = infile.read() processed_content = content.upper() with io.open(output_file_path, \'w\', encoding=\'utf-8\') as outfile: outfile.write(processed_content) elif mode == \'binary\': with io.open(input_file_path, \'rb\') as infile: content = infile.read() processed_content = content[::-1] with io.open(output_file_path, \'wb\') as outfile: outfile.write(processed_content) else: raise ValueError(\\"Mode should be either \'text\' or \'binary\'\\")"},{"question":"# PyTorch Coding Assessment: GPU Performance Tuning **Objective:** Implement functions to use the `torch.cuda.tunable` module for tuning a GEMM (General Matrix Multiplication) operation on a CUDA-enabled device. The student should be able to configure the tuning process, execute it, save the results, and validate the configuration. **Problem Statement:** 1. **Initialization and Configuration:** - Enable the tuning module using the appropriate function. - Set the maximum tuning duration to 10 seconds. - Set the maximum number of tuning iterations to 100. 2. **Execution:** - Perform a tuning operation on a GEMM operation using an input file named `gemm_input.json`. Assume this file is already prepared with valid inputs for a GEMM operation. 3. **Save Results:** - Write the results of the tuning process to an output file named `tuning_results.json`. 4. **Validation:** - Implement a function to check and return whether tuning is enabled and whether the maximum tuning iterations and duration are set correctly. **Function Signatures:** ```python def configure_tuning() -> None: Configure the tuning module: - Enable the tuning module. - Set the maximum tuning duration to 10 seconds. - Set the maximum number of tuning iterations to 100. pass def execute_tuning() -> None: Execute the tuning process: - Perform tuning on a GEMM operation using the \'gemm_input.json\' file. - Output the results to \'tuning_results.json\'. pass def validate_configuration() -> bool: Validate the configuration: - Check if tuning is enabled. - Check if the maximum tuning duration is set to 10 seconds. - Check if the maximum tuning iterations are set to 100. Returns: bool: True if all conditions are satisfied, False otherwise. pass ``` **Constraints:** - You can assume that JSON file parsing functions (`torch.cuda.tunable.read_file` and `torch.cuda.tunable.write_file`) will handle the correct format. - Ensure exception handling for any potential errors during the function calls. **Input and Output Examples:** Given no input data, when calling `configure_tuning()`, the internal state should be set up properly, which can be later verified by calling `validate_configuration()`. Call to `execute_tuning()` should read `gemm_input.json`, perform tuning, and produce an `tuning_results.json` with results. **Your tasks:** 1. Implement `configure_tuning()`. 2. Implement `execute_tuning()`. 3. Implement `validate_configuration()`. **Note:** This task assumes that the machine used for testing has a CUDA-enabled GPU and the `torch.cuda.tunable` module is properly installed and configured.","solution":"import torch def configure_tuning() -> None: Configure the tuning module: - Enable the tuning module. - Set the maximum tuning duration to 10 seconds. - Set the maximum number of tuning iterations to 100. try: torch.cuda.tunable.enable_tuning() torch.cuda.tunable.set_max_duration(10) # duration in seconds torch.cuda.tunable.set_max_iterations(100) except Exception as e: print(f\\"An error occurred during configuration: {e}\\") def execute_tuning() -> None: Execute the tuning process: - Perform tuning on a GEMM operation using the \'gemm_input.json\' file. - Output the results to \'tuning_results.json\'. try: result = torch.cuda.tunable.tune_kernel_from_file(\'gemm_input.json\') torch.cuda.tunable.write_file(result, \'tuning_results.json\') except Exception as e: print(f\\"An error occurred during execution: {e}\\") def validate_configuration() -> bool: Validate the configuration: - Check if tuning is enabled. - Check if the maximum tuning duration is set to 10 seconds. - Check if the maximum tuning iterations are set to 100. Returns: bool: True if all conditions are satisfied, False otherwise. try: is_enabled = torch.cuda.tunable.is_tuning_enabled() max_duration = torch.cuda.tunable.get_max_duration() max_iterations = torch.cuda.tunable.get_max_iterations() return is_enabled and (max_duration == 10) and (max_iterations == 100) except Exception as e: print(f\\"An error occurred during validation: {e}\\") return False"},{"question":"# Distributed Processing and Debugging with PyTorch You are tasked with implementing and testing a distributed processing system using PyTorch and leveraging the extra debug and control handlers provided by the `torch.distributed.elastic.control_plane` module. Task Description: 1. **Implementation**: - Set up a PyTorch distributed processing environment. - Implement a distributed worker function that performs a simple computation, such as matrix multiplication. - Integrate the `worker_main` helper function from the `torch.distributed.elastic.control_plane` module to add debugging capabilities. 2. **Testing**: - Demonstrate the execution of the distributed worker function across multiple processes. - Verify that the debugging and control handlers are effectively providing the necessary control and debugging output. Requirements: - **Input**: - Rank of the node in the distributed setup, a serialized matrix to process. - **Output**: - The result of the matrix multiplication from the distributed nodes. Constraints: - Assume an available cluster setup mimicking a real-world distributed environment. - Use a maximum of 4 nodes for simplicity. - Ensure code modularity and readability. Example: Here is an abstract skeleton of your expected implementation: ```python import torch import torch.distributed as dist from torch.distributed.elastic.control_plane import worker_main def distributed_matrix_multiplication(rank, size, matrix_data): Placeholder for distributed matrix multiplication function. Incorporate worker_main for debugging and control handling. pass if __name__ == \\"__main__\\": matrix_data = ... # some initialization of matrix data world_size = 4 # Set up the distributed environment dist.init_process_group(\\"gloo\\", rank=0, world_size=world_size) for rank in range(world_size): # Call the worker function with necessary parameters distributed_matrix_multiplication(rank, world_size, matrix_data) # Finalize the distributed environment dist.destroy_process_group() ``` Ensure your implementation encapsulates the setup, execution, and tests the described distributed processing, including verifying the debug and control handlers.","solution":"import torch import torch.distributed as dist from torch.distributed.elastic.control_plane import worker_main import os def init_process(rank, world_size, fn, matrix_data): Initialize the distributed environment. os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) fn(rank, world_size, matrix_data) dist.destroy_process_group() def distributed_matrix_multiplication(rank, world_size, matrix_data): Distributed worker function that performs matrix multiplication. Incorporates worker_main for debugging and control handling. def worker_fn(): matrix = torch.tensor(matrix_data[rank], dtype=torch.float32) result = torch.matmul(matrix, matrix.t()) print(f\\"Rank {rank} result:n{result}\\") worker_main(worker_fn) if __name__ == \\"__main__\\": # Example matrix data matrix_data = [ [[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]], [[13, 14], [15, 16]] ] world_size = 4 processes = [] for rank in range(world_size): p = torch.multiprocessing.Process(target=init_process, args=(rank, world_size, distributed_matrix_multiplication, matrix_data)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"Coding Assessment: Creating Minimal Reproducible Examples and Synthetic Data Generation with scikit-learn # Problem Statement You have been hired to assist a machine learning team in diagnosing an issue they encountered with a classification model. The team is using scikit-learn for their machine learning pipeline, but they\'ve run into a warning when fitting their model. They would like a minimal reproducible example to better understand and communicate this issue. Using the guidelines outlined in the provided documentation, complete the following tasks: # Tasks 1. **Dataset Generation**: Generate a synthetic dataset appropriate for a binary classification problem. Ensure the dataset has 100 samples and 5 features. Use the `make_classification` function from scikit-learn. 2. **Model Training**: Train a `RandomForestClassifier` on your generated dataset. Use default parameters for the model. 3. **Simulating the Issue**: Simulate an issue by changing the `max_depth` parameter of the `RandomForestClassifier` to 1 and observe any warnings or errors during model fitting. 4. **Crafting a Minimal Reproducible Example**: Create a minimal, runnable code snippet that reproduces the issue detailed in task 3. The code should be concise, self-contained, and follow good practices for generating synthetic data, importing necessary libraries, and formatting for readability. # Input and Output - **Input**: Your code should not require any input from the user. All operations should be performed on the synthetic dataset generated within the script. - **Output**: Print any warnings or errors encountered during the model fitting in task 3. # Constraints - Use only the scikit-learn library for model training and dataset generation. - The code should be limited to a maximum of 50 lines. - Ensure the code is well-commented for clarity. # Example ```python import numpy as np from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier # Task 1: Generate synthetic dataset X, y = make_classification(n_samples=100, n_features=5, random_state=42) # Task 2: Train RandomForestClassifier with default parameters clf = RandomForestClassifier() clf.fit(X, y) # Task 3: Simulate issue by changing max_depth parameter clf_issue = RandomForestClassifier(max_depth=1) try: clf_issue.fit(X, y) except Warning as w: print(f\\"Warning encountered: {w}\\") # Task 4: Minimal reproducible example print(\\"Minimal Reproducible Example:\\") print( import numpy as np from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier X, y = make_classification(n_samples=100, n_features=5, random_state=42) clf_issue = RandomForestClassifier(max_depth=1) clf_issue.fit(X, y) # Expecting a warning ) ``` # Evaluation Criteria - **Correctness**: Does the code generate the synthetic dataset and simulate the warning correctly? - **Reproducibility**: Is the minimal code snippet correctly formatted and runnable in isolation? - **Clarity**: Are the comments and structure of the code clear and easy to understand? - **Conciseness**: Is the code snippet minimal and to the point? Submit your code for review. Good luck!","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier import warnings # Task 1: Generate synthetic dataset X, y = make_classification(n_samples=100, n_features=5, random_state=42) # Task 2: Train RandomForestClassifier with default parameters clf = RandomForestClassifier() clf.fit(X, y) # Task 3: Simulate issue by changing max_depth parameter and observing warnings clf_issue = RandomForestClassifier(max_depth=1) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") clf_issue.fit(X, y) if w: warning = w[-1] print(f\\"Warning encountered: {warning.message}\\") # Task 4: Minimal reproducible example print(\\"Minimal Reproducible Example:\\") print( import numpy as np from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier import warnings # Generate synthetic dataset X, y = make_classification(n_samples=100, n_features=5, random_state=42) # Train RandomForestClassifier with modified parameters clf_issue = RandomForestClassifier(max_depth=1) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") clf_issue.fit(X, y) if w: warning = w[-1] print(f\\"Warning encountered: {warning.message}\\") )"},{"question":"# Advanced Module Importer **Objective:** Create a Python wrapper function that provides enhanced module import capabilities by utilizing the underlying C API functions related to module imports as described in the provided documentation. **Task:** Implement a function `enhanced_import` in Python that performs the following tasks: 1. **Import a Module:** Use `PyImport_ImportModule` to import a module given its name. 2. **Reload a Module:** Use `PyImport_ReloadModule` to reload the module if it is already imported. 3. **Import with Execution:** Use `PyImport_ExecCodeModule` to import a module by executing its compiled bytecode. 4. **Return Module Attribute:** Return a specified attribute of the module if it exists. **Function Signature:** ```python def enhanced_import(module_name: str, attribute: str = None, reload: bool = False, bytecode: bytes = None) -> Any: pass ``` **Input:** - `module_name`: A string specifying the name of the module to be imported or reloaded. - `attribute`: An optional string specifying an attribute of the module to return. If not provided or if the attribute does not exist, return the module itself. - `reload`: A boolean flag indicating whether to reload the module if it is already imported. - `bytecode`: An optional bytes object representing the compiled bytecode of the module to be imported. **Output:** - If `attribute` is specified, return the value of the attribute from the imported/reloaded module. - If `attribute` is not specified, return the module itself. **Constraints:** - Do not use Python\'s built-in `importlib` or `__import__` functions directly for this task. - Ensure that the function performs error handling gracefully. - Make sure the function maintains thread-safety when importing modules. **Example:** ```python # Usage Example 1: Import the \'os\' module and return the \'path\' attribute path_module = enhanced_import(\'os\', attribute=\'path\') # Usage Example 2: Import and then reload the \'datetime\' module, and return the module itself datetime_module = enhanced_import(\'datetime\', reload=True) # Usage Example 3: Import a module from its bytecode bytecode = compile(\'def hello(): return \\"Hello, World!\\"\', \'hello_module\', \'exec\') hello_module = enhanced_import(\'hello_module\', bytecode=bytecode) print(hello_module.hello()) # Output: \\"Hello, World!\\" ``` Implement the `enhanced_import` function ensuring all specified capabilities are handled properly.","solution":"import importlib import sys import types def enhanced_import(module_name: str, attribute: str = None, reload: bool = False, bytecode: bytes = None): Enhanced module import function. Parameters: - module_name: str, name of the module to import. - attribute: str, optional attribute of the module to return. - reload: bool, whether to reload the module if it is already imported. - bytecode: bytes, optional compiled bytecode for the module. Returns: - The value of the specified attribute from the module, or the module itself. if bytecode is not None: # Import a module using its compiled bytecode module = types.ModuleType(module_name) exec(bytecode, module.__dict__) sys.modules[module_name] = module else: if reload and module_name in sys.modules: # Reload the module if it\'s already imported and reload is True module = importlib.reload(sys.modules[module_name]) else: # Import the module using its name module = importlib.import_module(module_name) if attribute: # Return the specified attribute if it exists if hasattr(module, attribute): return getattr(module, attribute) else: raise AttributeError(f\\"Module \'{module_name}\' has no attribute \'{attribute}\'\\") # Return the module itself return module"},{"question":"# **Coding Assessment Question** **Objective:** Your task is to create a function that programmatically ensures pip is installed in the current environment or a specified root directory using the \\"ensurepip\\" module. The function should also return the version of pip that was installed. You are required to handle various options provided by the `ensurepip.bootstrap()` method. **Function Signature:** ```python def ensure_pip_installed(root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False) -> str: pass ``` **Input:** - `root` (str): The root directory to install pip relative to. If `None`, the default install location is used. - `upgrade` (bool): Indicates whether to upgrade an existing installation of an earlier version of pip. Default is `False`. - `user` (bool): Indicates whether to install pip using the user scheme rather than globally. Default is `False`. - `altinstall` (bool): If set to `True`, the `pipX` script will not be installed. Default is `False`. - `default_pip` (bool): If set to `True`, the `pip` script will be installed in addition to the regular scripts. Default is `False`. **Output:** - Returns a string specifying the version of pip that was installed. **Constraints:** - The function should raise a `ValueError` if both `altinstall` and `default_pip` are set to `True`. **Example:** ```python # Example 1: Default installation assert ensure_pip_installed() == \\"pip X.Y.Z\\" # where X.Y.Z is the current version of pip # Example 2: Upgrading existing pip assert ensure_pip_installed(upgrade=True) == \\"pip X.Y.Z\\" # upgraded to current version # Example 3: Specifying root directory assert ensure_pip_installed(root=\\"/custom/path\\") == \\"pip X.Y.Z\\" # installed at /custom/path # Example 4: User installation assert ensure_pip_installed(user=True) == \\"pip X.Y.Z\\" # installed in user site packages # Example 5: Alternative install without default script assert ensure_pip_installed(altinstall=True) == \\"pip X.Y.Z\\" # pipX not installed # Example 6: ValueError for conflicting options try: ensure_pip_installed(altinstall=True, default_pip=True) except ValueError as e: assert str(e) == \\"altinstall and default_pip cannot both be True\\" ``` **Notes:** - Utilize the `ensurepip.version()` function to fetch the current version of pip that will be installed. - Use the `ensurepip.bootstrap()` function to perform the actual installation. - Ensure any side effects of the `ensurepip.bootstrap()` method are appropriately handled or documented.","solution":"import ensurepip import subprocess def ensure_pip_installed(root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False) -> str: Ensures that pip is installed in the current environment or a specified root directory, and returns the version of pip that was installed. if altinstall and default_pip: raise ValueError(\\"altinstall and default_pip cannot both be True\\") ensurepip_args = [] if root: ensurepip_args.extend([\'--root\', root]) if upgrade: ensurepip_args.append(\'--upgrade\') if user: ensurepip_args.append(\'--user\') if altinstall: ensurepip_args.append(\'--altinstall\') if default_pip: ensurepip_args.append(\'--default-pip\') ensurepip.bootstrap(ensurepip_args) # Fetch the current version of pip installed pip_version = subprocess.check_output([\'pip\', \'--version\']).decode().split()[1] return f\\"pip {pip_version}\\""},{"question":"**Problem Statement: Asynchronous Task Manager** You are tasked with creating an asynchronous task manager using Python\'s asyncio module. The task manager should be able to handle multiple types of tasks concurrently, ensure certain tasks do not block or cancel others, and handle timeouts effectively. **Requirements:** 1. **Coroutines**: - `task_1`: A coroutine that sleeps for a specified number of seconds and returns a message. - `task_2`: A coroutine that performs a repetitive computation while intermittently yielding control using `asyncio.sleep(0)`. - `task_3`: A coroutine that runs an IO-bound blocking call in a separate thread. 2. **Task Manager**: - A main coroutine `task_manager` to manage and run the above coroutines concurrently using `asyncio.create_task`. - Ensure `task_1` and `task_2` do not block each other. - Add a timeout to `task_3` to ensure it does not run indefinitely. 3. **Output and Handling**: - Print start and end times using `time.strftime`. - Properly handle and print exceptions if any task fails or gets canceled. - Print final results of all completed tasks. **Function Signatures**: ```python import asyncio import time async def task_1(seconds: int, name: str) -> str: # Sleep for the specified number of seconds # Return a message including the task name pass async def task_2(name: str) -> str: # Perform some computation, intermittently yielding control # Return a message including the task name pass async def task_3() -> str: # Simulate a blocking IO-bound operation running in a separate thread using asyncio.to_thread # Return a completion message pass async def task_manager(): # Create tasks for task_1, task_2, and task_3 # Ensure task_1 and task_2 run concurrently # Add a timeout for task_3 # Handle exceptions # Print final results of completed tasks pass # Run the task manager asyncio.run(task_manager()) ``` **Constraints**: 1. `task_1` should sleep for a time between 1 to 5 seconds. 2. `task_2` should perform at least 1000 iterations of computation, yielding after every 100 iterations. 3. `task_3` should simulate an IO-bound task that takes at most 2 seconds. 4. The task manager should execute all tasks concurrently and handle a timeout of 3 seconds for `task_3`. Implement the required functions to satisfy the above requirements.","solution":"import asyncio import time async def task_1(seconds: int, name: str) -> str: await asyncio.sleep(seconds) return f\\"Task {name} completed after {seconds} seconds.\\" async def task_2(name: str) -> str: for i in range(1000): if i % 100 == 0: await asyncio.sleep(0) return f\\"Task {name} completed computations.\\" async def task_3(name: str) -> str: await asyncio.to_thread(time.sleep, 2) return f\\"Task {name} completed IO-bound operation.\\" async def task_manager(): try: print(\\"Task manager started at:\\", time.strftime(\\"%X\\")) t1 = asyncio.create_task(task_1(3, \'T1\')) t2 = asyncio.create_task(task_2(\'T2\')) t3 = asyncio.create_task(task_3(\'T3\')) completed, pending = await asyncio.wait( [t1, t2, t3], timeout=3, return_when=asyncio.ALL_COMPLETED ) for t in completed: print(await t) for t in pending: t.cancel() try: await t except asyncio.CancelledError: print(f\\"{t.get_name()} was cancelled\\") except Exception as e: print(f\\"An error occurred: {e}\\") print(\\"Task manager ended at:\\", time.strftime(\\"%X\\")) # Run the task manager if __name__ == \\"__main__\\": asyncio.run(task_manager())"},{"question":"# Sparse Tensors in PyTorch: Challenge Task As neural networks become large, models with sparse connections and weights often arise, especially in graph neural networks or pruned networks. Efficiently handling these sparse structures can significantly reduce computation time and memory usage. In this task, you are to demonstrate your understanding of sparse tensors in PyTorch by implementing the following functionalities: 1. **Conversion to Sparse Format:** - Write a function `convert_to_sparse(input_tensor: torch.Tensor, layout: str) -> torch.Tensor` that takes a dense tensor and a layout type (`\'coo\'`, `\'csr\'`, `\'csc\'`, `\'bsr\'`, `\'bsc\'`) as inputs and converts it to the specified sparse tensor format. 2. **Matrix Multiplication with Sparse Tensors:** - Write a function `sparse_matrix_multiplication(A: torch.Tensor, B: torch.Tensor, layout: str) -> torch.Tensor` that performs a matrix multiplication between a dense matrix `A` and a sparse matrix `B` and returns the result as a dense matrix. The sparse matrix `B` can be in any of the above formats. Ensure you convert the dense matrix `A` to a sparse format for the multiplication if necessary. 3. **Sparse Tensor Operations:** - Write a function `sparse_tensor_operations(A: torch.Tensor, B: torch.Tensor, layout: str) -> torch.Tensor` that adds two dense matrices, but after converting one of them to a sparse tensor using the specified format. Provide element-wise addition for sparse representations. # Function Signatures ```python import torch def convert_to_sparse(input_tensor: torch.Tensor, layout: str) -> torch.Tensor: Converts a given dense tensor to the specified sparse format. Args: - input_tensor (torch.Tensor): The dense tensor to convert. - layout (str): The sparse format (\'coo\', \'csr\', \'csc\', \'bsr\', \'bsc\'). Returns: - torch.Tensor: The tensor in the specified sparse format. pass def sparse_matrix_multiplication(A: torch.Tensor, B: torch.Tensor, layout: str) -> torch.Tensor: Performs matrix multiplication between a dense matrix and a sparse matrix. Args: - A (torch.Tensor): The dense matrix. - B (torch.Tensor): The sparse matrix. - layout (str): The sparse format of matrix B (\'coo\', \'csr\', \'csc\', \'bsr\', \'bsc\'). Returns: - torch.Tensor: The result of the matrix multiplication in dense format. pass def sparse_tensor_operations(A: torch.Tensor, B: torch.Tensor, layout: str) -> torch.Tensor: Adds two dense matrices after converting one of them to a specified sparse format. Args: - A (torch.Tensor): The first dense matrix. - B (torch.Tensor): The second dense matrix to be converted and added. - layout (str): The sparse format of matrix B (\'coo\', \'csr\', \'csc\', \'bsr\', \'bsc\'). Returns: - torch.Tensor: The result of the addition in dense format. pass ``` # Evaluation Criteria Your functions will be evaluated based on: - Correctness and completeness of the conversion to sparse formats. - Efficiency and correctness of the sparse matrix multiplication. - Performance and accuracy of sparse tensor operations. - Code readability and use of PyTorch\'s sparse tensor functionalities. # Example Usage ```python dense_matrix = torch.tensor([[0, 2.], [3, 0]]) sparse_matrix = convert_to_sparse(dense_matrix, \'coo\') A = torch.tensor([[1, 0], [0, 1]], dtype=torch.float32) B = convert_to_sparse(torch.tensor([[4, 0], [5, 6]], dtype=torch.float32), \'csr\') result = sparse_matrix_multiplication(A, B, \'csr\') C = torch.tensor([[5, 5], [5, 5]], dtype=torch.float32) operation_result = sparse_tensor_operations(C, B, \'csr\') ``` # Constraints - Ensure compatibility with CUDA tensors if possible. - Consider edge cases such as completely dense or empty sparse representations. - Avoid unnecessary data conversions to maintain efficiency.","solution":"import torch def convert_to_sparse(input_tensor: torch.Tensor, layout: str) -> torch.Tensor: Converts a given dense tensor to the specified sparse format. Args: - input_tensor (torch.Tensor): The dense tensor to convert. - layout (str): The sparse format (\'coo\', \'csr\', \'csc\', \'bsr\', \'bsc\'). Returns: - torch.Tensor: The tensor in the specified sparse format. if layout == \'coo\': return input_tensor.to_sparse() elif layout == \'csr\': return input_tensor.to_sparse_csr() elif layout == \'csc\': return input_tensor.to_sparse_csc() elif layout == \'bsr\': return input_tensor.to_sparse_bsr(blocksize=(2, 2)) elif layout == \'bsc\': return input_tensor.to_sparse_bsc(blocksize=(2, 2)) else: raise ValueError(f\\"Unsupported layout: {layout}\\") def sparse_matrix_multiplication(A: torch.Tensor, B: torch.Tensor, layout: str) -> torch.Tensor: Performs matrix multiplication between a dense matrix and a sparse matrix. Args: - A (torch.Tensor): The dense matrix. - B (torch.Tensor): The sparse matrix. - layout (str): The sparse format of matrix B (\'coo\', \'csr\', \'csc\', \'bsr\', \'bsc\'). Returns: - torch.Tensor: The result of the matrix multiplication in dense format. sparse_B = convert_to_sparse(B, layout) return torch.mm(A, sparse_B.to_dense()) def sparse_tensor_operations(A: torch.Tensor, B: torch.Tensor, layout: str) -> torch.Tensor: Adds two dense matrices after converting one of them to a specified sparse format. Args: - A (torch.Tensor): The first dense matrix. - B (torch.Tensor): The second dense matrix to be converted and added. - layout (str): The sparse format of matrix B (\'coo\', \'csr\', \'csc\', \'bsr\', \'bsc\'). Returns: - torch.Tensor: The result of the addition in dense format. sparse_B = convert_to_sparse(B, layout) return A + sparse_B.to_dense()"},{"question":"You are working on a project that requires managing the contents of ZIP archives. Specifically, you need to perform several operations, such as creating a ZIP file, adding files to it, listing its contents, and extracting files. Write a function, `manage_zip_file`, that accepts a command and executes the appropriate operation. # Input: 1. **command** - A string indicating the operation to perform. It can be one of the following: - `\'create\'`: Create a new ZIP file. - `\'add\'`: Add files to an existing ZIP file. - `\'list\'`: List the contents of a ZIP file. - `\'extract\'`: Extract files from a ZIP file. 2. Additional arguments depending on the command: - For `\'create\'`: - `zip_name`: The name of the ZIP file to create. - `files`: A list of file paths to include in the ZIP file. - For `\'add\'`: - `zip_name`: The name of the ZIP file to add files to. - `files`: A list of file paths to add to the ZIP file. - For `\'list\'`: - `zip_name`: The name of the ZIP file to list contents of. - For `\'extract\'`: - `zip_name`: The name of the ZIP file to extract files from. - `extract_to`: The directory to extract files to. # Output: - For `\'create\'` and `\'add\'`: Print a success message. - For `\'list\'`: Print the names of the files in the ZIP archive. - For `\'extract\'`: Print a success message. # Constraints: - The ZIP file must be created using the `\'w\'` mode. - Files must be added using `\'a\'` mode. - Only handle uncompressed (`ZIP_STORED`) files for simplicity. # Example: ```python files_to_create = [\'file1.txt\', \'file2.txt\'] manage_zip_file(\'create\', \'example.zip\', files_to_create) files_to_add = [\'file3.txt\'] manage_zip_file(\'add\', \'example.zip\', files_to_add) manage_zip_file(\'list\', \'example.zip\') manage_zip_file(\'extract\', \'example.zip\', \'output_directory\') ``` # Implementation: ```python import zipfile def manage_zip_file(command, zip_name, files=None, extract_to=None): if command == \'create\': with zipfile.ZipFile(zip_name, \'w\', zipfile.ZIP_STORED) as zipf: for file in files: zipf.write(file) print(f\\"ZIP file \'{zip_name}\' created successfully.\\") elif command == \'add\': with zipfile.ZipFile(zip_name, \'a\', zipfile.ZIP_STORED) as zipf: for file in files: zipf.write(file) print(f\\"Files added to \'{zip_name}\' successfully.\\") elif command == \'list\': with zipfile.ZipFile(zip_name, \'r\') as zipf: print(\\"Contents of the ZIP file:\\") for file_info in zipf.namelist(): print(file_info) elif command == \'extract\': with zipfile.ZipFile(zip_name, \'r\') as zipf: zipf.extractall(extract_to) print(f\\"Files extracted from \'{zip_name}\' to \'{extract_to}\' successfully.\\") else: print(\\"Invalid command\\") # Example usage: files_to_create = [\'file1.txt\', \'file2.txt\'] manage_zip_file(\'create\', \'example.zip\', files_to_create) files_to_add = [\'file3.txt\'] manage_zip_file(\'add\', \'example.zip\', files_to_add) manage_zip_file(\'list\', \'example.zip\') manage_zip_file(\'extract\', \'example.zip\', \'output_directory\') ```","solution":"import zipfile def manage_zip_file(command, zip_name, files=None, extract_to=None): Manages ZIP file operations such as creating, adding, listing, and extracting. Args: - command (str): The operation to perform (\'create\', \'add\', \'list\', \'extract\'). - zip_name (str): The name of the ZIP file. - files (list, optional): The list of files involved in the operation. - extract_to (str, optional): The directory to extract files to. Returns: - str: Message indicating the result of the operation (for create, add, and extract commands). - list: Names of the files in the ZIP archive (for list command). if command == \'create\': with zipfile.ZipFile(zip_name, \'w\', zipfile.ZIP_STORED) as zipf: for file in files: zipf.write(file) return f\\"ZIP file \'{zip_name}\' created successfully.\\" elif command == \'add\': with zipfile.ZipFile(zip_name, \'a\', zipfile.ZIP_STORED) as zipf: for file in files: zipf.write(file) return f\\"Files added to \'{zip_name}\' successfully.\\" elif command == \'list\': with zipfile.ZipFile(zip_name, \'r\') as zipf: return zipf.namelist() elif command == \'extract\': with zipfile.ZipFile(zip_name, \'r\') as zipf: zipf.extractall(extract_to) return f\\"Files extracted from \'{zip_name}\' to \'{extract_to}\' successfully.\\" else: raise ValueError(\\"Invalid command\\")"},{"question":"# Custom Autograd Function in PyTorch Objective: Create a custom autograd Function to calculate a modified exponential function and its gradient while using saved tensors. This exercise will test your understanding of defining forward and backward functions using `torch.autograd.Function`. Problem Statement: In this exercise, you are required to implement a custom autograd function called `ExpFunction` which computes the exponential of a tensor and then performs a custom power operation on it. Specifically, the following operations should be performed: [ y = (e^x)^{power} ] where ( x ) is the input tensor and ( power ) is a given exponent. You need to implement both the forward and backward functions for this custom autograd operation. 1. **Forward pass**: - Compute ( e^x ). - Save the intermediate result for use in the backward pass. - Compute the final output ( (e^x)^{power} ). 2. **Backward pass**: - Retrieve the saved tensor. - Compute the gradient of the output with respect to the input. Input: - A tensor `x` of any shape with `requires_grad=True`. - A float `power`. Output: - A tensor `y` of the same shape as input tensor `x`. Constraints: - The implementation must correctly handle the gradient calculation. - Use `torch.autograd.function._ContextMethodMixin.save_for_backward` to save intermediate tensors. - Validate input tensor shapes and ensure `requires_grad=True`. Example: ```python import torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Function class ExpFunction(Function): @staticmethod def forward(ctx, x, power): # Save context for backward exp_x = torch.exp(x) ctx.save_for_backward(exp_x) return exp_x ** power @staticmethod def backward(ctx, grad_output): exp_x, = ctx.saved_tensors power = ctx.extra_args[0] # Assume extra_args is provided grad_input = grad_output * power * exp_x ** (power - 1) return grad_input, None # No gradient for the power parameter # Example Usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) power = 3.0 y = ExpFunction.apply(x, power) print(\\"Output:\\", y) # Expected output: tensor([ e^3, e^6, e^9 ]) y.sum().backward() print(\\"Gradient:\\", x.grad) # Expected gradients ``` Requirements: - Implement the `ExpFunction` class with both forward and backward methods. - Ensure proper saving and retrieval of intermediate tensors. - Test the function with various input tensors and power values. Notes: - Review how the autograd system tracks operations and handles gradients. - Understand how to define and use custom autograd functions in PyTorch.","solution":"import torch from torch.autograd import Function class ExpFunction(Function): @staticmethod def forward(ctx, x, power): # Save the power for backward computation ctx.power = power # Compute the exponential of x exp_x = torch.exp(x) # Save exp_x for use in the backward pass ctx.save_for_backward(exp_x) # Return the final output return exp_x**power @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor exp_x, = ctx.saved_tensors power = ctx.power # Compute the gradient with respect to the input tensor grad_input = grad_output * power * exp_x**(power - 1) * exp_x return grad_input, None # No gradient for the power parameter # Example Usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) power = 3.0 y = ExpFunction.apply(x, power) print(\\"Output:\\", y) # Examine the output y.sum().backward() print(\\"Gradient:\\", x.grad) # Expected gradients"},{"question":"**Objective**: To test the understanding of seaborn KDE plotting functionalities, including univariate and bivariate distributions, smoothing adjustments, conditional distributions, and customization options. **Problem Statement**: You have been provided with two datasets: 1. **Iris dataset**: This dataset contains information about different species of iris flowers. 2. **Geyser dataset**: This dataset contains information about waiting times between eruptions and durations for the Old Faithful geyser in Yellowstone National Park. Using seaborn, create the following plots: 1. **Univariate KDE Plots**: - Plot the kernel density estimation of the `total_bill` column from the tips dataset along the x-axis. - Plot the kernel density estimation of the `total_bill` column from the tips dataset along the y-axis. - Plot the kernel density estimation for each column in the iris dataset. 2. **Conditional Distribution with Smoothing**: - For the `total_bill` column in the tips dataset, create a conditional KDE plot with `hue` representing `time`. Adjust the bandwidth smoothing to 0.2 and create a stack plot. Additionally, normalize the stacked distribution at each grid value. 3. **Weighted KDE Plot**: - Aggregate the tips dataset by `size`, then plot a weighted KDE using the average `total_bill` weighted by the count for each size. 4. **Bivariate KDE Plot**: - Create a bivariate KDE plot for the `geyser` dataset showing the density of `waiting` vs. `duration`. - Add `hue` to represent the `kind` of eruption in the `geyser` dataset and show filled contours with fewer levels (e.g., 5 levels). 5. **Customization**: - Customize one of the univariate KDE plots to use a different palette, increase transparency to 50%, and remove the lines, only showing the filled area. **Expected Inputs**: The function should not require any input parameters and should directly use the seaborn library\'s dataset loading functions. **Output**: The function should generate and show the specified plots using seaborn\'s `sns.kdeplot` function. **Constraints**: - Ensure all plots are displayed within the function. - Use seaborn for all plots. - Handle any potential exceptions gracefully. ```python import seaborn as sns import matplotlib.pyplot as plt def create_kde_plots(): sns.set_theme() # 1. Univariate KDE Plots tips = sns.load_dataset(\\"tips\\") iris = sns.load_dataset(\\"iris\\") plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\") plt.show() plt.figure() sns.kdeplot(data=tips, y=\\"total_bill\\") plt.show() plt.figure() sns.kdeplot(data=iris) plt.show() # 2. Conditional Distribution with Smoothing plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", bw_adjust=0.2, multiple=\\"stack\\", fill=True) plt.show() plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.show() # 3. Weighted KDE Plot tips_agg = (tips.groupby(\\"size\\").agg(total_bill=(\\"total_bill\\", \\"mean\\"), n=(\\"total_bill\\", \\"count\\"))) plt.figure() sns.kdeplot(data=tips_agg, x=\\"total_bill\\", weights=\\"n\\") plt.show() # 4. Bivariate KDE Plot geyser = sns.load_dataset(\\"geyser\\") plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\") plt.show() plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, levels=5, thresh=.2) plt.show() # 5. Customization plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"size\\", fill=True, common_norm=False, palette=\\"crest\\", alpha=.5, linewidth=0) plt.show() create_kde_plots() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_kde_plots(): sns.set_theme() # 1. Univariate KDE Plots tips = sns.load_dataset(\\"tips\\") iris = sns.load_dataset(\\"iris\\") plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\") plt.title(\\"Univariate KDE Plot (X-axis: total_bill)\\") plt.show() plt.figure() sns.kdeplot(data=tips, y=\\"total_bill\\") plt.title(\\"Univariate KDE Plot (Y-axis: total_bill)\\") plt.show() plt.figure() for col in iris.columns[:-1]: # Avoiding the categorical `species` column sns.kdeplot(data=iris, x=col, label=col) plt.legend() plt.title(\\"Univariate KDE Plots for Iris Dataset\\") plt.show() # 2. Conditional Distribution with Smoothing plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", bw_adjust=0.2, multiple=\\"stack\\", fill=True) plt.title(\\"Conditional KDE Plot with Stacked Distribution\\") plt.show() plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.title(\\"Conditional KDE Plot with Normalized Stacked Distribution\\") plt.show() # 3. Weighted KDE Plot tips_agg = tips.groupby(\\"size\\").agg(total_bill=(\\"total_bill\\", \\"mean\\"), n=(\\"total_bill\\", \\"count\\")).reset_index() plt.figure() sns.kdeplot(data=tips_agg, x=\\"total_bill\\", weights=\\"n\\") plt.title(\\"Weighted KDE Plot\\") plt.show() # 4. Bivariate KDE Plot geyser = sns.load_dataset(\\"geyser\\") plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\") plt.title(\\"Bivariate KDE Plot\\") plt.show() plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, levels=5, thresh=.2) plt.title(\\"Bivariate KDE Plot with Hue and Filled Contours\\") plt.show() # 5. Customization plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"size\\", fill=True, common_norm=False, palette=\\"crest\\", alpha=.5, linewidth=0) plt.title(\\"Customized Univariate KDE Plot\\") plt.show()"},{"question":"**Pandas Text Data Manipulation Task** Given a DataFrame containing text data, perform various operations to extract, transform, and clean the text data. # Objective: Implement a function `clean_and_transform_text(df: pd.DataFrame, text_column: str) -> pd.DataFrame` that performs the following operations on the specified text column of the input DataFrame: 1. **Lowercase Conversion**: Convert all text in the specified column to lowercase. 2. **Whitespace Removal**: Strip leading and trailing whitespaces from the text. 3. **Substring Extraction**: Extract a specific part of the string using a regular expression. - First Word: Extract the first word of the string (assume words are separated by spaces). - Digits: Extract all digit sequences from the string. 4. **Replace Patterns**: Replace any occurrence of the word \'error\' with the string \'issue\' (case-insensitive). 5. **Concatenation**: Concatenate the transformed text column with another column provided as a list-like object. 6. **Dummy Variable Creation**: Create dummy variables based on a given delimiter in the text. # Input: - `df`: A pandas DataFrame containing the text data. - `text_column`: A column in the DataFrame which contains the text data. # Output: - **DataFrame**: A DataFrame with the following transformations applied: - `lowercase_text`: The original text converted to lowercase. - `cleaned_text`: Text with leading/trailing whitespaces removed. - `first_word`: Extracted first word from the text. - `digits`: Extracted digits from the text. - `pattern_replaced`: Text with \'error\' replaced by \'issue\'. - `concatenated_text`: Concatenated text from `cleaned_text` and a provided list-like object. - Dummy variables based on a delimiter \'|\' in the text. # Constraints: - The input DataFrame is guaranteed to have at least one column of string type. # Example: ```python import pandas as pd data = { \'text\': [\' Error message 101\', \'No issues found\', \' Warning: disk space low \', \'Contact support 202\'], \'additional_column\': [\'extra1\', \'extra2\', \'extra3\', \'extra4\'] } df = pd.DataFrame(data) additional_list = [\'concat1\', \'concat2\', \'concat3\', \'concat4\'] # Implement and use your function result_df = clean_and_transform_text(df, \'text\') # Expected output: # lowercase_text cleaned_text first_word digits pattern_replaced concatenated_text a b c d # Dummy variables for the example \'|\' # 0 error message 101 error message 101 error 101 issue message 101 error message 101concat1 # 1 no issues found no issues found no NaN no issues found no issues foundconcat2 # 2 warning: disk space low warning: disk space low warning NaN warning: disk space low warning: disk space low concat3 # 3 contact support 202 contact support 202 contact 202 contact support 202 contact support 202concat4 ``` # Note: - Handle missing values appropriately where applicable. - Assumption can be made that the delimiter \'|\' is used for creating dummy variables from the text column. **Solution Template** ```python import pandas as pd import numpy as np def clean_and_transform_text(df: pd.DataFrame, text_column: str) -> pd.DataFrame: # Convert to lowercase df[\'lowercase_text\'] = df[text_column].str.lower() # Strip leading and trailing whitespaces df[\'cleaned_text\'] = df[\'lowercase_text\'].str.strip() # Extract first word df[\'first_word\'] = df[\'cleaned_text\'].str.extract(r\'(w+)\') # Extract digits df[\'digits\'] = df[\'cleaned_text\'].str.extract(r\'(d+)\') # Replace patterns df[\'pattern_replaced\'] = df[\'cleaned_text\'].str.replace(\'error\', \'issue\', case=False, regex=True) # Concatenate with additional list-like object (example list passed separately) additional_list = [\'concat1\', \'concat2\', \'concat3\', \'concat4\'] # Replace with actual input df[\'concatenated_text\'] = df[\'cleaned_text\'].str.cat(additional_list, sep=\\"\\") # Create dummy variables df = df.join(df[\'cleaned_text\'].str.get_dummies(sep=\'|\')) return df # Example usage data = { \'text\': [\' Error message 101\', \'No issues found\', \' Warning: disk space low \', \'Contact support 202\'], \'additional_column\': [\'extra1\', \'extra2\', \'extra3\', \'extra4\'] } df = pd.DataFrame(data) result_df = clean_and_transform_text(df, \'text\') print(result_df) ```","solution":"import pandas as pd def clean_and_transform_text(df: pd.DataFrame, text_column: str, additional_list: list) -> pd.DataFrame: # Convert to lowercase df[\'lowercase_text\'] = df[text_column].str.lower() # Strip leading and trailing whitespaces df[\'cleaned_text\'] = df[\'lowercase_text\'].str.strip() # Extract first word df[\'first_word\'] = df[\'cleaned_text\'].str.extract(r\'(w+)\', expand=False) # Extract digits df[\'digits\'] = df[\'cleaned_text\'].str.extract(r\'(d+)\', expand=False) # Replace patterns df[\'pattern_replaced\'] = df[\'cleaned_text\'].str.replace(\'error\', \'issue\', case=False, regex=True) # Concatenate with additional list-like object df[\'concatenated_text\'] = df[\'cleaned_text\'] + additional_list # Create dummy variables df = df.join(df[\'cleaned_text\'].str.get_dummies(sep=\'|\')) return df"},{"question":"# Question: Implement and Compare Multiclass Strategies Using scikit-learn In this task, you will implement and compare two different multiclass classification strategies provided by scikit-learn: `OneVsRestClassifier` and `OneVsOneClassifier`. Step-by-Step Instructions: 1. **Load the Data**: - Utilize the famous `Iris` dataset from scikit-learn, where each sample is an iris flower, and the label is one of three species. 2. **Preprocess the Data**: - Split the dataset into training and testing sets. - Standardize the feature variables. 3. **Implement Multiclass Classifiers**: - Implement the `OneVsRestClassifier` using `LinearSVC` as the base classifier. - Implement the `OneVsOneClassifier` using `LinearSVC` as the base classifier as well. 4. **Training and Prediction**: - Train both classifiers on the training data. - Predict labels on the test data using both classifiers. 5. **Evaluate Performance**: - Calculate and print the classification accuracy for both classifiers on the test set. - Print the confusion matrix for both classifiers. Constraints: - You must use `LinearSVC` from `sklearn.svm` as the base classifier. - You should import any additional necessary libraries from scikit-learn. - Make sure to set the random state to 42 where needed to ensure reproducibility. Code Template: ```python from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier from sklearn.metrics import accuracy_score, confusion_matrix # Step 1: Load the Data iris = datasets.load_iris() X, y = iris.data, iris.target # Step 2: Preprocess the Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 3: Implement and Train the OneVsRest Classifier ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=42)) ovr_classifier.fit(X_train, y_train) # Step 4: Predict and Evaluate OneVsRest Classifier y_pred_ovr = ovr_classifier.predict(X_test) ovr_accuracy = accuracy_score(y_test, y_pred_ovr) ovr_confusion_matrix = confusion_matrix(y_test, y_pred_ovr) print(\\"OneVsRest Classifier Accuracy:\\", ovr_accuracy) print(\\"OneVsRest Classifier Confusion Matrix:n\\", ovr_confusion_matrix) # Step 3: Implement and Train the OneVsOne Classifier ovo_classifier = OneVsOneClassifier(LinearSVC(random_state=42)) ovo_classifier.fit(X_train, y_train) # Step 4: Predict and Evaluate OneVsOne Classifier y_pred_ovo = ovo_classifier.predict(X_test) ovo_accuracy = accuracy_score(y_test, y_pred_ovo) ovo_confusion_matrix = confusion_matrix(y_test, y_pred_ovo) print(\\"OneVsOne Classifier Accuracy:\\", ovo_accuracy) print(\\"OneVsOne Classifier Confusion Matrix:n\\", ovo_confusion_matrix) ``` The output should display the accuracy and confusion matrix for both the `OneVsRestClassifier` and `OneVsOneClassifier`. Expected Output Format: ``` OneVsRest Classifier Accuracy: [accuracy_score] OneVsRest Classifier Confusion Matrix: [[...], [...], [...]] OneVsOne Classifier Accuracy: [accuracy_score] OneVsOne Classifier Confusion Matrix: [[...], [...], [...]] ``` Note: - Make sure your code is well-documented and handles any potential errors gracefully. - Interpretation of the results is optional but encouraged to understand the differences between the two strategies.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier from sklearn.metrics import accuracy_score, confusion_matrix def compare_multiclass_strategies(): # Load the Data iris = datasets.load_iris() X, y = iris.data, iris.target # Preprocess the Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement and Train the OneVsRest Classifier ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=42)) ovr_classifier.fit(X_train, y_train) # Predict and Evaluate OneVsRest Classifier y_pred_ovr = ovr_classifier.predict(X_test) ovr_accuracy = accuracy_score(y_test, y_pred_ovr) ovr_confusion_matrix = confusion_matrix(y_test, y_pred_ovr) # Implement and Train the OneVsOne Classifier ovo_classifier = OneVsOneClassifier(LinearSVC(random_state=42)) ovo_classifier.fit(X_train, y_train) # Predict and Evaluate OneVsOne Classifier y_pred_ovo = ovo_classifier.predict(X_test) ovo_accuracy = accuracy_score(y_test, y_pred_ovo) ovo_confusion_matrix = confusion_matrix(y_test, y_pred_ovo) return { \\"ovr_accuracy\\": ovr_accuracy, \\"ovr_confusion_matrix\\": ovr_confusion_matrix, \\"ovo_accuracy\\": ovo_accuracy, \\"ovo_confusion_matrix\\": ovo_confusion_matrix } if __name__ == \\"__main__\\": results = compare_multiclass_strategies() print(\\"OneVsRest Classifier Accuracy:\\", results[\\"ovr_accuracy\\"]) print(\\"OneVsRest Classifier Confusion Matrix:n\\", results[\\"ovr_confusion_matrix\\"]) print(\\"OneVsOne Classifier Accuracy:\\", results[\\"ovo_accuracy\\"]) print(\\"OneVsOne Classifier Confusion Matrix:n\\", results[\\"ovo_confusion_matrix\\"])"},{"question":"# Question: Implement a Custom Logging System with Priority Filtering In this exercise, you are required to create a custom logging function that utilizes the Unix `syslog` library functions in Python. The function should accept various parameters to configure the logging system, log messages, and filter the messages based on priority. Function Signature ```python def custom_logger(messages, ident=None, logoption=0, facility=syslog.LOG_USER, mask_priority=syslog.LOG_INFO): ``` Parameters - `messages` (list of tuples): - A list where each tuple contains two elements: - `priority` (int): The priority level of the message (e.g., `syslog.LOG_ERR`). - `message` (str): The content of the message to be logged. - `ident` (str, optional): String to be prepended to every message, defaults to `None`. - `logoption` (int, optional): Bit field for log options, defaults to 0. - `facility` (int, optional): Default facility for logging, defaults to `syslog.LOG_USER`. - `mask_priority` (int, optional): Mask priority for filtering messages, defaults to `syslog.LOG_INFO`. Requirements 1. Use `openlog()` to configure logging options, including `ident`, `logoption`, and `facility`. 2. Use `setlogmask()` to set the mask priority for filtering messages. 3. Use `syslog()` to log only those messages whose priority is within the mask priority. 4. Ensure each message is tagged with the appropriate priority and format. 5. Close the logging system using `closelog()` after all messages have been processed. Example Usage ```python import syslog messages = [ (syslog.LOG_ERR, \\"Error occurred\\"), (syslog.LOG_INFO, \\"Information message\\"), (syslog.LOG_DEBUG, \\"Debugging message\\") ] custom_logger(messages, ident=\\"MyApp\\", logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL, mask_priority=syslog.LOG_ERR) ``` Additional Constraints - Ensure the function handles unexpected inputs gracefully. - The function must add a trailing newline to each message if not already present. - Do not assume that `openlog()` has been called prior to the `custom_logger()` function. Expected Output Behavior Only the messages with priority equal to or higher than `mask_priority` should be logged. In the example usage, only the message \\"Error occurred\\" should be logged to the mail facility with the process ID included. Hints - Make use of `syslog.LOG_MASK(pri)` and `syslog.LOG_UPTO(pri)` to handle mask priorities. - The `syslog` module should be imported within your function or at the start of your script. Implement the function `custom_logger` based on the given requirements and constraints.","solution":"import syslog def custom_logger(messages, ident=None, logoption=0, facility=syslog.LOG_USER, mask_priority=syslog.LOG_INFO): Custom logging function that logs messages to the Unix syslog system based on the provided priority. :param messages: List of tuples where each tuple contains (priority, message). :param ident: Optional string to be prepended to every message. :param logoption: Optional bit field for log options. :param facility: Optional default facility for logging. :param mask_priority: Optional mask priority for filtering messages. # Open the log with given ident, logoption, and facility syslog.openlog(ident=ident, logoption=logoption, facility=facility) # Set the log mask priority syslog.setlogmask(syslog.LOG_UPTO(mask_priority)) # Process and log each message if it meets the priority criteria for priority, message in messages: # Ensure the message ends with a newline if not message.endswith(\'n\'): message += \'n\' # Log the message if the priority is within the mask if priority <= mask_priority: syslog.syslog(priority, message) # Close the log syslog.closelog()"},{"question":"**Email Header Encoding and Decoding** You have been tasked with creating a function that can handle email header encoding and decoding operations using Python\'s `email.header` module. Specifically, you will write a function to encode a string with a specified character set into an RFC 2047 compliant format, simulate an email header, and then decode this header back to its original form. # Function Signature ```python def encode_and_decode_header(header_value: str, charset: str) -> str: pass ``` # Input - `header_value` (str): The header\'s initial value which might contain non-ASCII characters. A typical example could be a subject line with special characters: \\"pöstal\\". - `charset` (str): The character set used to encode the header value. Example: \\"iso-8859-1\\". # Output - Returns a decoded string that matches the original `header_value`. # Example ```python header_value = \\"pöstal\\" charset = \\"iso-8859-1\\" assert encode_and_decode_header(header_value, charset) == header_value ``` # Constraints - The function should correctly handle non-ASCII characters. - Ensure the encoded header is compliant with RFC 2047. - The function should not produce any exceptions if used with valid inputs as specified. # Detailed Requirements 1. **Encoding**: Use the `Header` class from the `email.header` module to encode the provided `header_value` with the given `charset`. 2. **Decoding**: Decode the resulting header back to its original string format using the `decode_header` function from the `email.header` module and handle the decode results accordingly. 3. **Return**: The function should return the decoded string, which must be identical to the original `header_value`. # Hint - Use the `Header` object to manage encoding efficiently. - Utilize the `decode_header` function to handle the decoding process. ```python import codecs from email.header import Header, decode_header, make_header def encode_and_decode_header(header_value: str, charset: str) -> str: # Create a Header instance header = Header(header_value, charset) # Encode the header encoded_header = str(header) # Decode the header decoded_pairs = decode_header(encoded_header) # Make a Header instance from the decoded pairs decoded_header = make_header(decoded_pairs) # Return the decoded string return str(decoded_header) # Example usage header_value = \\"pöstal\\" charset = \\"iso-8859-1\\" print(encode_and_decode_header(header_value, charset)) # should print: pöstal ``` # Note You should ensure the encoded string created by the `Header` class and the decoded string produced should match exactly with the input `header_value`.","solution":"import codecs from email.header import Header, decode_header, make_header def encode_and_decode_header(header_value: str, charset: str) -> str: # Create a Header instance header = Header(header_value, charset) # Encode the header encoded_header = str(header) # Decode the header decoded_pairs = decode_header(encoded_header) # Make a Header instance from the decoded pairs decoded_header = make_header(decoded_pairs) # Return the decoded string return str(decoded_header)"},{"question":"Objective Implement a multi-client chat server using the `selectors` module that supports basic I/O multiplexing. The server should handle multiple clients simultaneously, allowing them to send and receive messages in real-time. Requirements 1. Use the `selectors` module to implement the chat server. 2. The server should accept connections from multiple clients. 3. Each connected client should be able to send messages to the server, which will then broadcast the message to all other connected clients. 4. The server should handle client disconnects gracefully. 5. Ensure proper usage of non-blocking sockets and efficient I/O multiplexing. Input and Output - **Input**: No direct input is needed from the user. The server will listen on a specified port for incoming client connections. - **Output**: The server should print logs for important events such as new connections, messages received, messages broadcasted, and client disconnections. Constraints 1. The server should be implemented using Python\'s `selectors` module for I/O multiplexing. 2. Use non-blocking sockets to avoid blocking operations. 3. Proper error handling should be in place to handle invalid operations and client disconnects. Performance Requirements The server should handle at least 100 concurrent client connections efficiently. Detailed Instructions 1. Implement a class `ChatServer` with the following methods: - `start(host, port)`: Start the chat server on the specified host and port. - `accept(sock, mask)`: Accept a new client connection. - `read(conn, mask)`: Read data from a client connection and broadcast it. - `broadcast(message, exclude)`: Broadcast a message to all connected clients, excluding the sender. - `close_connection(conn)`: Close a client connection and unregister it. 2. Use the `selectors.DefaultSelector()` for efficient I/O multiplexing. 3. Ensure the server runs indefinitely, processing connections and I/O events. Example ```python import selectors import socket import types class ChatServer: def __init__(self): self.sel = selectors.DefaultSelector() self.clients = [] def start(self, host, port): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind((host, port)) sock.listen() print(f\'Server started on {host}:{port}\') sock.setblocking(False) self.sel.register(sock, selectors.EVENT_READ, self.accept) while True: events = self.sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) def accept(self, sock, mask): conn, addr = sock.accept() print(f\'Accepted connection from {addr}\') conn.setblocking(False) data = types.SimpleNamespace(addr=addr, inb=\'\', outb=\'\') self.sel.register(conn, selectors.EVENT_READ, self.read) self.clients.append(conn) def read(self, conn, mask): data = conn.recv(1024) if data: print(f\'Received message: {data.decode()} from {conn.getpeername()}\') self.broadcast(data.decode(), conn) else: self.close_connection(conn) def broadcast(self, message, exclude): for client in self.clients: if client != exclude: try: client.send(message.encode()) except OSError: self.close_connection(client) def close_connection(self, conn): print(f\'Closing connection to {conn.getpeername()}\') self.sel.unregister(conn) conn.close() self.clients.remove(conn) if __name__ == \\"__main__\\": server = ChatServer() server.start(\'localhost\', 65432) ``` **Note**: Make sure to test your implementation using multiple clients to ensure it handles concurrent connections and messaging correctly.","solution":"import selectors import socket import types class ChatServer: def __init__(self): self.sel = selectors.DefaultSelector() self.clients = [] def start(self, host, port): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind((host, port)) sock.listen() print(f\'Server started on {host}:{port}\') sock.setblocking(False) self.sel.register(sock, selectors.EVENT_READ, self.accept) while True: events = self.sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) def accept(self, sock, mask): conn, addr = sock.accept() print(f\'Accepted connection from {addr}\') conn.setblocking(False) self.sel.register(conn, selectors.EVENT_READ, self.read) self.clients.append(conn) def read(self, conn, mask): data = conn.recv(1024) if data: print(f\'Received message: {data.decode()} from {conn.getpeername()}\') self.broadcast(data.decode(), conn) else: self.close_connection(conn) def broadcast(self, message, exclude): for client in self.clients: if client != exclude: try: client.send(message.encode()) except OSError: self.close_connection(client) def close_connection(self, conn): print(f\'Closing connection to {conn.getpeername()}\') self.sel.unregister(conn) conn.close() self.clients.remove(conn) if __name__ == \\"__main__\\": server = ChatServer() server.start(\'localhost\', 65432)"},{"question":"# Regex-based Log File Analyzer Question You are provided with a log file from a server, and your task is to write a Python function that analyzes this log file to extract specific information using regular expressions. Log File Format The log file entries are of the following format: ``` [<timestamp>] <log_level> <message> (<user_id>) ``` - `<timestamp>`: The date and time of the log entry in the format `YYYY-MM-DD HH:MM:SS` - `<log_level>`: The severity level of the log (e.g., INFO, WARNING, ERROR) - `<message>`: A descriptive message associated with the log entry - `<user_id>`: A unique identifier for the user associated with this log entry Tasks 1. **Function**: `extract_error_logs(log_file_path: str) -> List[str]` - **Input**: Path to the log file as a string. - **Output**: A list of log messages that have the log level `ERROR`, processed from the log file. 2. **Function**: `count_logs_per_user(log_file_path: str) -> Dict[str, int]` - **Input**: Path to the log file as a string. - **Output**: A dictionary where the keys are `user_id`s and the values are the count of log entries associated with each user. 3. **Function**: `find_logs_in_timeframe(log_file_path: str, start_time: str, end_time: str) -> List[str]` - **Input**: - `log_file_path`: Path to the log file as a string. - `start_time`: The start of the timeframe as a string in format `YYYY-MM-DD HH:MM:SS`. - `end_time`: The end of the timeframe as a string in format `YYYY-MM-DD HH:MM:SS`. - **Output**: A list of log entries within the given timeframe. Constraints - Use regular expressions to parse the log entries. - Assume that the log file is small enough to fit into memory. - Log entries within a specified timeframe should be inclusive of the start and end times. Example Given the following log entries in a file at `log_file_path`: ``` [2023-01-01 10:00:00] INFO Server started (user123) [2023-01-01 10:05:00] ERROR Failed to connect to database (user456) [2023-01-01 10:10:00] WARNING Low memory (user123) [2023-01-01 10:15:00] ERROR Disk full (user789) ``` 1. `extract_error_logs(\'log_file_path\')` should return: ```python [\'Failed to connect to database\', \'Disk full\'] ``` 2. `count_logs_per_user(\'log_file_path\')` should return: ```python {\'user123\': 2, \'user456\': 1, \'user789\': 1} ``` 3. `find_logs_in_timeframe(\'log_file_path\', \'2023-01-01 10:00:00\', \'2023-01-01 10:10:00\')` should return: ```python [\'[2023-01-01 10:00:00] INFO Server started (user123)\', \'[2023-01-01 10:05:00] ERROR Failed to connect to database (user456)\', \'[2023-01-01 10:10:00] WARNING Low memory (user123)\'] ``` Notes - You can use the `datetime` module to handle date and time comparisons. - Ensure your regular expressions are robust and can handle any correctly formatted log entries.","solution":"import re from typing import List, Dict from datetime import datetime def extract_error_logs(log_file_path: str) -> List[str]: error_logs = [] log_pattern = re.compile(r\'[(.*?)]s+ERRORs+(.*?)s+((.*?))\') with open(log_file_path, \'r\') as file: for line in file: match = log_pattern.match(line) if match: error_logs.append(match.group(2)) return error_logs def count_logs_per_user(log_file_path: str) -> Dict[str, int]: user_log_counts = {} log_pattern = re.compile(r\'[(.*?)]s+(w+)s+(.*?)s+((.*?))\') with open(log_file_path, \'r\') as file: for line in file: match = log_pattern.match(line) if match: user_id = match.group(4) if user_id in user_log_counts: user_log_counts[user_id] += 1 else: user_log_counts[user_id] = 1 return user_log_counts def find_logs_in_timeframe(log_file_path: str, start_time: str, end_time: str) -> List[str]: logs_in_timeframe = [] log_pattern = re.compile(r\'[(.*?)]s+(w+)s+(.*?)s+((.*?))\') start_dt = datetime.strptime(start_time, \'%Y-%m-%d %H:%M:%S\') end_dt = datetime.strptime(end_time, \'%Y-%m-%d %H:%M:%S\') with open(log_file_path, \'r\') as file: for line in file: match = log_pattern.match(line) if match: log_time = datetime.strptime(match.group(1), \'%Y-%m-%d %H:%M:%S\') if start_dt <= log_time <= end_dt: logs_in_timeframe.append(line.strip()) return logs_in_timeframe"},{"question":"# Python Coding Assessment # Problem Statement You are tasked with creating a utility script to dynamically discover and list all submodules and their corresponding paths within a specified top-level package. Your script should be capable of handling exceptions gracefully and should print the name and path of each discovered module. # Requirements: 1. **Function Implementation**: Implement the following function: ```python def list_submodules(package_name: str) -> None: Lists all submodules and their corresponding paths within a specified top-level package. Args: package_name (str): The name of the top-level package. Returns: None ``` 2. **Function Details**: - The function should use the `pkgutil.walk_packages` function to iterate over all submodules within the specified package. - For each discovered submodule, the function should print its name and corresponding path in the following format: ``` Submodule: <module_name>, Path: <module_path> ``` - If the specified package cannot be found or any other error occurs, the function should print an appropriate error message without terminating unexpectedly. # Input: - `package_name` (string): The name of the top-level package you want to list the submodules for. # Example Usage: Suppose we have a package named `my_package`, and it has submodules `submodule1`, `submodule2`, etc. ```python >>> list_submodules(\'my_package\') Submodule: my_package.submodule1, Path: /path/to/my_package/submodule1.py Submodule: my_package.submodule2, Path: /path/to/my_package/submodule2.py ... ``` # Notes: - Insert appropriate error handling to deal with cases where the package or its paths are inaccessible. - Utilize `pkgutil.walk_packages`. This task assesses your ability to work with Python\'s module system and specifically the `pkgutil` module to dynamically interact with packages and their submodules.","solution":"import pkgutil import importlib import sys def list_submodules(package_name: str) -> None: Lists all submodules and their corresponding paths within a specified top-level package. Args: package_name (str): The name of the top-level package. Returns: None try: package = importlib.import_module(package_name) package_path = package.__path__ except ImportError as e: print(f\\"Error: Could not import package \'{package_name}\': {str(e)}\\") return for module_info in pkgutil.walk_packages(package_path, package_name + \'.\'): try: submodule = importlib.import_module(module_info.name) submodule_path = submodule.__file__ print(f\\"Submodule: {module_info.name}, Path: {submodule_path}\\") except Exception as e: print(f\\"Error: Could not import submodule \'{module_info.name}\': {str(e)}\\")"},{"question":"# Question: Analyze Customer Spending Patterns with Seaborn You have been provided with a dataset of customer spending patterns named `customer_spending.csv`. The dataset contains the following columns: - `customer_id` : Unique identifier for each customer. - `date` : Date of the transaction. - `total_spent` : Total amount spent by the customer in a single transaction. - `items_bought` : Number of items bought in a single transaction. - `store_location` : Store location where the transaction happened. - `customer_type` : Type of customer (e.g., \\"Regular\\", \\"VIP\\"). Your task is to perform the following: 1. Load the dataset and display the first few rows to understand its structure. 2. Create a scatter plot showing the relationship between `total_spent` and `items_bought`. 3. Use the `hue` parameter to differentiate data points based on `store_location`. 4. Use the `style` parameter to differentiate data points based on `customer_type`. 5. Encode the `items_bought` into the size of the markers. 6. Customize the plot by setting marker sizes between 50 and 300. 7. Ensure the legend is fully displayed by showing all unique values. 8. Save the plot to a file named `customer_spending_plot.png`. # Input Format: - A CSV file named `customer_spending.csv`. # Output Format: - A image file named `customer_spending_plot.png`. # Constraints: - Use Seaborn for visualizations. - Properly handle missing data if present. Here is a sample implementation to get you started: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'customer_spending.csv\') # Display the first few rows of the dataset print(df.head()) # Create a scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=df, x=\'total_spent\', y=\'items_bought\', hue=\'store_location\', style=\'customer_type\', size=\'items_bought\', sizes=(50, 300), legend=\'full\' ) # Save the plot to a file scatter_plot.figure.savefig(\'customer_spending_plot.png\') ``` # Instructions: Use appropriate functions and methods from Seaborn and Matplotlib to achieve the customization required. Ensure your code is efficient and handles potential edge cases such as missing values.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def analyze_customer_spending(file_path): # Load the dataset df = pd.read_csv(file_path) # Display the first few rows of the dataset print(\\"First few rows of the dataset:\\") print(df.head()) # Handle missing values by dropping rows with any missing values df.dropna(inplace=True) # Create a scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=df, x=\'total_spent\', y=\'items_bought\', hue=\'store_location\', style=\'customer_type\', size=\'items_bought\', sizes=(50, 300), legend=\'full\' ) # Title and labels plt.title(\\"Customer Spending Patterns\\") plt.xlabel(\\"Total Spent ()\\") plt.ylabel(\\"Items Bought\\") # Save the plot to a file scatter_plot.figure.savefig(\'customer_spending_plot.png\') plt.close() # Call the function with the dataset file path # analyze_customer_spending(\'customer_spending.csv\')"},{"question":"# Asynchronous File Transfer using `asyncio` Problem Statement You are required to implement an asynchronous file transfer system using Python\'s `asyncio` library. The system consists of two parts: a file server and a file client. The server should accept connections and serve files to clients, while the client should be able to download files from the server. Your task is to implement the following functionalities: 1. `FileServer`: This class will handle incoming client connections and serve the requested files. 2. `FileClient`: This class will request files from the server and write them to the local disk. # Requirements 1. `FileServer` Class - **Initialization Parameters**: - `host`: The hostname or IP address to bind the server to. - `port`: The port on which the server will listen for incoming connections. - `directory`: The directory where the files to be served are located. - **Methods**: - `start_server()`: This should start the server and begin accepting connections. - `handle_client(reader, writer)`: This should handle incoming client requests. It should read the requested filename from the client and send the file content back. 2. `FileClient` Class - **Initialization Parameters**: - `host`: The server\'s hostname or IP address to connect to. - `port`: The port on which the server is listening for incoming connections. - **Methods**: - `download_file(filename, destination)`: This should request the file `filename` from the server and save it to the `destination` path on the client’s local disk. # Constraints - Assume all files are in text format. - The server should handle multiple clients concurrently. - Properly manage exceptions and ensure all resources (e.g., open sockets) are cleaned up correctly. # Example Usage Here\'s an example of how the classes should be used: ```python import asyncio # Server side server = FileServer(host=\'127.0.0.1\', port=8888, directory=\'/path/to/files\') asyncio.run(server.start_server()) # Client side client = FileClient(host=\'127.0.0.1\', port=8888) asyncio.run(client.download_file(\'example.txt\', \'/path/to/save/example.txt\')) ``` # Input and Output Formats - **Input**: - For the server: Configuration parameters (host, port, directory). - For the client: Configuration parameters (host, port), filename to download, and destination path. - **Output**: - Server: It should run indefinitely, handling multiple clients. - Client: The requested file should be saved to the specified destination path. # Additional Notes - You are required to use `asyncio` features such as tasks, event loops, protocols, or transports where appropriate. - Ensure the server can handle clients requesting files concurrently without blocking. - Provide appropriate exception handling and ensure that all connections are properly closed after file transfers complete. # Hints - For the server, use `await asyncio.start_server()` and handle connections using a coroutine. - For the client, use `await asyncio.open_connection()` to establish the connection and `await writer.drain()` to ensure data is sent. # Submission Submit the implementation of `FileServer` and `FileClient` classes, ensuring they meet the requirements outlined above. ```python import asyncio import os class FileServer: def __init__(self, host, port, directory): self.host = host self.port = port self.directory = directory async def start_server(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) async with server: await server.serve_forever() async def handle_client(self, reader, writer): try: data = await reader.read(100) filename = data.decode().strip() file_path = os.path.join(self.directory, filename) if os.path.isfile(file_path): with open(file_path, \'r\') as file: content = file.read() writer.write(content.encode()) await writer.drain() else: writer.write(f\\"File {filename} not found\\".encode()) await writer.drain() except Exception as e: print(f\\"Error handling client: {e}\\") finally: writer.close() await writer.wait_closed() class FileClient: def __init__(self, host, port): self.host = host self.port = port async def download_file(self, filename, destination): try: reader, writer = await asyncio.open_connection(self.host, self.port) writer.write(f\\"{filename}n\\".encode()) await writer.drain() data = await reader.read() with open(destination, \'w\') as file: file.write(data.decode()) except Exception as e: print(f\\"Error downloading file: {e}\\") finally: writer.close() await writer.wait_closed() ```","solution":"import asyncio import os class FileServer: def __init__(self, host, port, directory): self.host = host self.port = port self.directory = directory async def start_server(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) async with server: await server.serve_forever() async def handle_client(self, reader, writer): try: data = await reader.read(100) filename = data.decode().strip() file_path = os.path.join(self.directory, filename) if os.path.isfile(file_path): with open(file_path, \'r\') as file: content = file.read() writer.write(content.encode()) await writer.drain() else: writer.write(f\\"File {filename} not found\\".encode()) await writer.drain() except Exception as e: print(f\\"Error handling client: {e}\\") finally: writer.close() await writer.wait_closed() class FileClient: def __init__(self, host, port): self.host = host self.port = port async def download_file(self, filename, destination): try: reader, writer = await asyncio.open_connection(self.host, self.port) writer.write(f\\"{filename}n\\".encode()) await writer.drain() data = await reader.read() with open(destination, \'w\') as file: file.write(data.decode()) except Exception as e: print(f\\"Error downloading file: {e}\\") finally: writer.close() await writer.wait_closed()"},{"question":"Objective Your task is to create a utility that benchmarks different sorting algorithms using the `timeit` module. This will test your understanding of `timeit` and ability to write code that leverages this module to compare performance accurately. Requirements and Constraints: 1. **Implement at least three different sorting algorithms**: - Insertion Sort - Merge Sort - Quick Sort 2. **Benchmark the sorting algorithms**: - Use the `timeit` module to time these algorithms. - Each algorithm should be run for an input list of 1000 random integers. - Perform 5 repetitions for each algorithm and capture timing data. 3. **Output the results**: - Display the minimum, maximum, and average execution time for each algorithm. - Ensure the results are clear and properly labeled. Input and Output - **Input**: The input list can be generated within your code and does not need to be provided by the user. - **Output**: Print the timing results to the console in a readable format. Constraints: - List size is 1000 elements. - Use `timeit.timeit` for benchmarking. - Perform timing in a way that isolates the sorting function\'s execution. Solution Template: ```python import random import timeit # Sorting Algorithms def insertion_sort(arr): for i in range(1, len(arr)): key = arr[i] j = i - 1 while j >= 0 and key < arr[j]: arr[j + 1] = arr[j] j -= 1 arr[j + 1] = key def merge_sort(arr): if len(arr) > 1: mid = len(arr) // 2 left_half = arr[:mid] right_half = arr[mid:] merge_sort(left_half) merge_sort(right_half) i = j = k = 0 while i < len(left_half) and j < len(right_half): if left_half[i] < right_half[j]: arr[k] = left_half[i] i += 1 else: arr[k] = right_half[j] j += 1 k += 1 while i < len(left_half): arr[k] = left_half[i] i += 1 k += 1 while j < len(right_half): arr[k] = right_half[j] j += 1 k += 1 def quick_sort(arr): if len(arr) <= 1: return arr else: pivot = arr[len(arr) // 2] left = [x for x in arr if x < pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x > pivot] return quick_sort(left) + middle + quick_sort(right) # Benchmark function def benchmark_sorting_algorithms(): algorithms = [\'insertion_sort\', \'merge_sort\', \'quick_sort\'] results = {} for algo in algorithms: setup_code = f\\"from __main__ import {algo}, generate_random_list\\" stmt = f\\"{algo}(generate_random_list())\\" timings = timeit.repeat(stmt=stmt, setup=setup_code, repeat=5, number=1, globals=globals()) results[algo] = { \'min_time\': min(timings), \'max_time\': max(timings), \'average_time\': sum(timings) / len(timings) } for algo, timing in results.items(): print(f\\"Algorithm: {algo}\\") print(f\\" - Min time: {timing[\'min_time\']} seconds\\") print(f\\" - Max time: {timing[\'max_time\']} seconds\\") print(f\\" - Average time: {timing[\'average_time\']} secondsn\\") def generate_random_list(size=1000): return [random.randint(0, 10000) for _ in range(size)] if __name__ == \\"__main__\\": benchmark_sorting_algorithms() ```","solution":"import random import timeit # Sorting Algorithms def insertion_sort(arr): for i in range(1, len(arr)): key = arr[i] j = i - 1 while j >= 0 and key < arr[j]: arr[j + 1] = arr[j] j -= 1 arr[j + 1] = key def merge_sort(arr): if len(arr) > 1: mid = len(arr) // 2 left_half = arr[:mid] right_half = arr[mid:] merge_sort(left_half) merge_sort(right_half) i = j = k = 0 while i < len(left_half) and j < len(right_half): if left_half[i] < right_half[j]: arr[k] = left_half[i] i += 1 else: arr[k] = right_half[j] j += 1 k += 1 while i < len(left_half): arr[k] = left_half[i] i += 1 k += 1 while j < len(right_half): arr[k] = right_half[j] j += 1 k += 1 def quick_sort(arr): if len(arr) <= 1: return arr else: pivot = arr[len(arr) // 2] left = [x for x in arr if x < pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x > pivot] return quick_sort(left) + middle + quick_sort(right) def quick_sort_wrapper(arr): sorted_arr = quick_sort(arr) for i in range(len(arr)): arr[i] = sorted_arr[i] # Utility function to generate random list def generate_random_list(size=1000): return [random.randint(0, 10000) for _ in range(size)] # Benchmark function def benchmark_sorting_algorithms(): algorithms = [ (\'insertion_sort\', insertion_sort), (\'merge_sort\', merge_sort), (\'quick_sort\', quick_sort_wrapper) ] results = {} for name, func in algorithms: setup_code = f\\"from __main__ import {func.__name__}, generate_random_list\\" stmt = f\\"{func.__name__}(generate_random_list())\\" timings = timeit.repeat(stmt=stmt, setup=setup_code, repeat=5, number=1, globals=globals()) results[name] = { \'min_time\': min(timings), \'max_time\': max(timings), \'average_time\': sum(timings) / len(timings) } for algo, timing in results.items(): print(f\\"Algorithm: {algo}\\") print(f\\" - Min time: {timing[\'min_time\']} seconds\\") print(f\\" - Max time: {timing[\'max_time\']} seconds\\") print(f\\" - Average time: {timing[\'average_time\']} secondsn\\") if __name__ == \\"__main__\\": benchmark_sorting_algorithms()"},{"question":"# Python Configuration Analysis You are tasked with writing a Python script to analyze the Python environment\'s configuration and produce a summary report. Specifically, the script should gather and display information about the installation paths, configuration variables, and platform-related details. Requirements: 1. Implement a function `generate_config_report()` which: - Determines the default installation scheme and lists the associated installation paths. - Fetches critical configuration variables and their values. - Identifies the current platform and Python version. - Checks if Python was built from source in the current environment. 2. **Implementation Details:** - Use the `sysconfig` module to fetch relevant details. - The output should be a well-formatted string summarizing all the information. - Handle cases where configuration variables or paths might be missing, and provide sensible defaults or messages. 3. **Function Signature:** ```python def generate_config_report() -> str: ``` Expected Output Format: The output should be a single string with the following structure: ``` Python Configuration Report: ----------------------------- Platform: <platform> Python Version: <version> Built from Source: <True/False> Default Installation Scheme: <scheme> Installation Paths: <path1_name>: <path1_value> <path2_name>: <path2_value> ... Configuration Variables: <variable1_name>: <variable1_value> <variable2_name>: <variable2_value> ... ``` Constraints: - The function should not require any external packages other than those provided in the standard Python library. - Ensure that your code handles different platforms (e.g., Windows, Linux, macOS) appropriately. Performance Requirements: - Your implementation should efficiently interact with the `sysconfig` module and handle its dictionary responses. Example Usage: ```python if __name__ == \\"__main__\\": report = generate_config_report() print(report) ``` Hints: - Use `sysconfig.get_scheme_names()` and `sysconfig.get_default_scheme()` to determine the installation schemes. - Utilize `sysconfig.get_path_names()` and `sysconfig.get_path(name)` to fetch paths. - Fetch configuration variables using `sysconfig.get_config_vars()`. - Retrieve platform and version details using `sysconfig.get_platform()` and `sysconfig.get_python_version()`. - Check if Python build information using `sysconfig.is_python_build()`.","solution":"import sysconfig def generate_config_report() -> str: # Platform and Python version information platform = sysconfig.get_platform() python_version = sysconfig.get_python_version() built_from_source = sysconfig.is_python_build() # Default installation scheme and paths default_scheme = sysconfig.get_default_scheme() paths_info = {name: sysconfig.get_path(name, default_scheme) for name in sysconfig.get_path_names()} # Configuration variables config_vars = sysconfig.get_config_vars() # Formatting report report = [\\"Python Configuration Report:\\", \\"-----------------------------\\"] report.append(f\\"Platform: {platform}\\") report.append(f\\"Python Version: {python_version}\\") report.append(f\\"Built from Source: {built_from_source}\\") report.append(f\\"nDefault Installation Scheme: {default_scheme}\\") report.append(\\"Installation Paths:\\") for path_name, path_value in paths_info.items(): report.append(f\\" {path_name}: {path_value}\\") report.append(\\"nConfiguration Variables:\\") for var_name, var_value in config_vars.items(): report.append(f\\" {var_name}: {var_value}\\") return \'n\'.join(report)"},{"question":"# Coding Assessment: Custom Iterator Implementation Objective Develop a custom iterator in Python that mimics the behavior of the `PyCallIter_Type` iterator described in the provided documentation. This iterator will repeatedly call a given function until a specified sentinel value is returned. Problem Statement Write a Python class `CallableIterator` that implements an iterator, which repeatedly calls a given callable object (function) with no parameters, and terminates the iteration when the callable returns a value equal to a specified sentinel value. Class Definition ```python class CallableIterator: def __init__(self, func, sentinel): Initialize the iterator with a callable and a sentinel value. Parameters: func (callable): The callable object to be repeatedly called. sentinel (any type): The value at which to stop the iteration. # Your code here def __iter__(self): Return the iterator object itself. # Your code here def __next__(self): Get the next value by calling the callable function. Returns the next value from the callable function, or raises StopIteration if the sentinel value is reached. # Your code here ``` Input and Output format - **Input:** - The constructor of `CallableIterator` will take two arguments: 1. `func`: A callable object (function) that takes no parameters. 2. `sentinel`: A value that, when returned by the callable, stops the iteration. - **Output:** - The `__next__()` method should return the next element from the callable. - If the callable returns the sentinel value, `__next__()` should raise a `StopIteration` exception. Constraints 1. The callable function, `func`, will always be a valid Python callable that requires no parameters. 2. The sentinel value can be of any comparable type. 3. The iteration should be lazy, meaning that it only computes the next value when `__next__()` is called. Example ```python def generate_numbers(): import random return random.randint(1, 10) iterator = CallableIterator(generate_numbers, 5) for number in iterator: print(number) ``` In this example, the iterator should print random numbers generated by `generate_numbers()` until it generates the number 5, at which point the iteration stops. **Note:** Your implementation should handle edge cases, such as when the sentinel value is returned on the first call to the callable function. Performance Requirements - The solution should be efficient in terms of memory and support lazy evaluation. Good luck!","solution":"class CallableIterator: def __init__(self, func, sentinel): Initialize the iterator with a callable and a sentinel value. Parameters: func (callable): The callable object to be repeatedly called. sentinel (any type): The value at which to stop the iteration. self.func = func self.sentinel = sentinel def __iter__(self): Return the iterator object itself. return self def __next__(self): Get the next value by calling the callable function. Returns the next value from the callable function, or raises StopIteration if the sentinel value is reached. result = self.func() if result == self.sentinel: raise StopIteration return result"},{"question":"**Email Payload Encoder** You are tasked with writing functions that simulate the functionality of the deprecated `email.encoders` module in Python. Your goal is to create encoding functions for an email message payload. # Function Specifications 1. **Function 1: encode_quopri** - **Input**: A dictionary `msg` with two keys: - `\'payload\'`: A string representing the message payload. - `\'Content-Transfer-Encoding\'`: Initially set to `None`. - **Output**: The same dictionary with the `\'payload\'` encoded using quoted-printable encoding, and the `\'Content-Transfer-Encoding\'` set to `\\"quoted-printable\\"`. 2. **Function 2: encode_base64** - **Input**: A dictionary `msg` with two keys: - `\'payload\'`: A string representing the message payload. - `\'Content-Transfer-Encoding\'`: Initially set to `None`. - **Output**: The same dictionary with the `\'payload\'` encoded using base64 encoding, and the `\'Content-Transfer-Encoding\'` set to `\\"base64\\"`. 3. **Function 3: encode_7or8bit** - **Input**: A dictionary `msg` with two keys: - `\'payload\'`: A string representing the message payload. - `\'Content-Transfer-Encoding\'`: Initially set to `None`. - **Output**: The same dictionary with the `\'Content-Transfer-Encoding\'` set to `\\"7bit\\"` if all characters in the payload are ASCII, otherwise set to `\\"8bit\\"`. The payload itself remains unchanged. 4. **Function 4: encode_noop** - **Input**: A dictionary `msg` with two keys: - `\'payload\'`: A string representing the message payload. - `\'Content-Transfer-Encoding\'`: Initially set to `None`. - **Output**: The same dictionary unmodified. # Constraints - You are not allowed to use any external libraries for the encoding. - Assume that the input dictionary will always contain the required keys. - Only standard ASCII and printable characters will be used in the payload for this task. # Example Usage ```python msg = { \'payload\': \'This is a test message!\', \'Content-Transfer-Encoding\': None } # Quoted-Printable encoding example encoded_msg = encode_quopri(msg) print(encoded_msg) # Output should be: # {\'payload\': \'This is a test message=21\', # \'Content-Transfer-Encoding\': \'quoted-printable\'} # Base64 encoding example encoded_msg = encode_base64(msg) print(encoded_msg) # Output should be: # {\'payload\': \'VGhpcyBpcyBhIHRlc3QgbWVzc2FnZSE=\', # \'Content-Transfer-Encoding\': \'base64\'} # 7bit or 8bit encoding example encoded_msg = encode_7or8bit(msg) print(encoded_msg) # Output should be: # {\'payload\': \'This is a test message!\', # \'Content-Transfer-Encoding\': \'7bit\'} # Noop encoding example encoded_msg = encode_noop(msg) print(encoded_msg) # Output should be: # {\'payload\': \'This is a test message!\', # \'Content-Transfer-Encoding\': None} ``` Implement these functions to demonstrate your understanding of encoding emails for transport.","solution":"def encode_quopri(msg): Encodes the payload of the message using quoted-printable encoding. def to_quoted_printable(payload): result = [] for c in payload: if ord(c) > 126 or c == \'=\' or (ord(c) < 32 and c not in \'trn\'): result.append(f\'={ord(c):02X}\') else: result.append(c) return \'\'.join(result) msg[\'payload\'] = to_quoted_printable(msg[\'payload\']) msg[\'Content-Transfer-Encoding\'] = \'quoted-printable\' return msg def encode_base64(msg): Encodes the payload of the message using base64 encoding. import base64 encoded_payload = base64.b64encode(msg[\'payload\'].encode(\'ascii\')).decode(\'ascii\') msg[\'payload\'] = encoded_payload msg[\'Content-Transfer-Encoding\'] = \'base64\' return msg def encode_7or8bit(msg): Determines whether to use \'7bit\' or \'8bit\' encoding based on the payload. Sets the \'Content-Transfer-Encoding\' accordingly, but the payload itself is unchanged. for c in msg[\'payload\']: if ord(c) > 127: msg[\'Content-Transfer-Encoding\'] = \'8bit\' return msg msg[\'Content-Transfer-Encoding\'] = \'7bit\' return msg def encode_noop(msg): Does not modify the payload or the encoding type in any way. return msg"},{"question":"**Complex Number Operations** You are required to demonstrate your understanding of the fundamental and advanced concepts in Python by working with Complex Number objects. The following tasks should be implemented: 1. Create a function `is_complex_number(obj)` to check if a given object is a complex number. 2. Develop a function `add_complex(c1, c2)` that takes two complex numbers and returns their sum. 3. Create a function `complex_properties(c)` that returns a dictionary containing the real and imaginary parts and the modulus of a given complex number. 4. Implement a class `ComplexCalculator` that supports basic mathematical operations on complex numbers (addition, subtraction) and can store a history of operations. # Function Signatures ```python def is_complex_number(obj: any) -> bool: pass def add_complex(c1: complex, c2: complex) -> complex: pass def complex_properties(c: complex) -> dict: pass class ComplexCalculator: def __init__(self): pass def add(self, c1: complex, c2: complex) -> complex: pass def subtract(self, c1: complex, c2: complex) -> complex: pass def history(self) -> list: pass ``` # Requirements 1. **is_complex_number(obj: any) -> bool** - Input: A single object of any type. - Output: `True` if the object is a complex number, otherwise `False`. 2. **add_complex(c1: complex, c2: complex) -> complex** - Input: Two complex numbers `c1` and `c2`. - Output: A complex number that is the sum of `c1` and `c2`. 3. **complex_properties(c: complex) -> dict** - Input: A single complex number `c`. - Output: A dictionary with keys: `real`, `imaginary`, and `modulus`, storing the respective properties of the complex number. 4. **ComplexCalculator** - Methods: - **__init__()**: Initializes an empty history list. - **add(self, c1: complex, c2: complex) -> complex**: Returns the sum of `c1` and `c2`, and records the operation. - **subtract(self, c1: complex, c2: complex) -> complex**: Returns the result of `c1 - c2`, and records the operation. - **history(self) -> list**: Returns a list of all operations performed. - Constraints: - Ensure that every operation (add and subtract) is recorded in the format `\\"{operation}: {c1} {operator} {c2} = {result}\\"`. # Example ```python # Example Usage print(is_complex_number(3+4j)) # Expected: True print(is_complex_number(5)) # Expected: False result = add_complex(1+2j, 3+4j) print(result) # Expected: (4+6j) props = complex_properties(3+4j) print(props) # Expected: {\'real\': 3, \'imaginary\': 4, \'modulus\': 5.0} calculator = ComplexCalculator() sum_result = calculator.add(1+2j, 3+4j) print(sum_result) # Expected: (4+6j) sub_result = calculator.subtract(5+6j, 1+2j) print(sub_result) # Expected: (4+4j) history = calculator.history() print(history) # Expected: [ # \\"addition: (1+2j) + (3+4j) = (4+6j)\\", # \\"subtraction: (5+6j) - (1+2j) = (4+4j)\\" # ] ``` # Constraints - All complex number operations must handle valid complex numbers as inputs. - The `ComplexCalculator` should maintain a history of operations in the exact format specified.","solution":"import cmath def is_complex_number(obj): Returns whether the given object is a complex number. return isinstance(obj, complex) def add_complex(c1, c2): Returns the sum of two complex numbers. return c1 + c2 def complex_properties(c): Returns a dictionary with real, imaginary and modulus properties of the complex number. return { \'real\': c.real, \'imaginary\': c.imag, \'modulus\': abs(c) } class ComplexCalculator: def __init__(self): self._history = [] def add(self, c1, c2): result = c1 + c2 self._record_history(\\"addition\\", c1, c2, result) return result def subtract(self, c1, c2): result = c1 - c2 self._record_history(\\"subtraction\\", c1, c2, result) return result def history(self): return self._history def _record_history(self, operation, c1, c2, result): if operation == \\"addition\\": operation_string = f\\"addition: {c1} + {c2} = {result}\\" elif operation == \\"subtraction\\": operation_string = f\\"subtraction: {c1} - {c2} = {result}\\" self._history.append(operation_string)"},{"question":"# Assessment Question: Working with OSS-Compatible Audio Devices using the `ossaudiodev` Module **Objective:** Write a Python program that: 1. Opens an audio device and a mixer device. 2. Configures the audio device to play sound at a sample rate of 44100 Hz, 16-bit signed format, and stereo. 3. Writes some sample audio data to the device. 4. Adjusts the master volume of the mixer device to 50%. **Specifications:** 1. **Opening Devices:** - Use `ossaudiodev.open()` to open the audio device in write mode. - Use `ossaudiodev.openmixer()` to open the mixer device. 2. **Configuring the Audio Device:** - Set the audio format to `AFMT_S16_LE` (signed 16-bit, little-endian). - Set the number of channels to 2 (stereo). - Set the sample rate to 44100 Hz. 3. **Writing Sample Audio Data:** - Generate a 1-second sine wave audio signal at 440 Hz (A4 note) and write it to the audio device. 4. **Adjusting Master Volume:** - Set the master volume control (`SOUND_MIXER_VOLUME`) to 50% on both left and right channels. **Constraints:** - You may assume the existence of necessary environment variables or default paths for audio and mixer devices. - Ensure proper error handling for device opening, configuration, and writing operations. - Use appropriate constants and methods from the `ossaudiodev` module. **Input and Output:** - The program does not take any input from the user. - The program should output informative messages about the status of each step (e.g., \\"Audio device opened\\", \\"Sample audio data written\\", \\"Master volume set to 50%\\", etc.). **Sample Code Structure:** ```python import ossaudiodev import math import array # Constants SAMPLE_RATE = 44100 FREQUENCY = 440 DURATION = 1 # in seconds FORMAT = ossaudiodev.AFMT_S16_LE CHANNELS = 2 VOLUME_PERCENTAGE = 50 def generate_sine_wave(frequency, duration, sample_rate): num_samples = int(duration * sample_rate) sine_wave = [ int(32767.0 * math.sin(2.0 * math.pi * frequency * x / sample_rate)) for x in range(num_samples) ] return array.array(\'h\', sine_wave * CHANNELS).tobytes() def main(): try: # Open audio device audio_device = ossaudiodev.open(\'w\') print(\\"Audio device opened\\") # Set audio parameters audio_device.setfmt(FORMAT) audio_device.channels(CHANNELS) audio_device.speed(SAMPLE_RATE) print(\\"Audio device configured\\") # Generate and write sample audio data audio_data = generate_sine_wave(FREQUENCY, DURATION, SAMPLE_RATE) audio_device.writeall(audio_data) print(\\"Sample audio data written\\") # Adjust master volume on mixer device mixer_device = ossaudiodev.openmixer() mixer_device.set(ossaudiodev.SOUND_MIXER_VOLUME, (VOLUME_PERCENTAGE, VOLUME_PERCENTAGE)) print(\\"Master volume set to 50%\\") except ossaudiodev.OSSAudioError as e: print(f\\"OSSAudioError: {e}\\") except OSError as e: print(f\\"OSError: {e}\\") finally: audio_device.close() mixer_device.close() print(\\"Devices closed\\") if __name__ == \\"__main__\\": main() ``` **Explanation:** - The `generate_sine_wave` function generates a sine wave audio signal. - The `main` function orchestrates the opening, configuring, writing, and volume setting for the audio and mixer devices, handling errors, and ensuring device closure. **Testing:** - To test the program, ensure it is run on a system with OSS-compatible audio hardware or software emulation (e.g., ALSA with OSS emulation on Linux).","solution":"import ossaudiodev import math import array # Constants SAMPLE_RATE = 44100 FREQUENCY = 440 DURATION = 1 # in seconds FORMAT = ossaudiodev.AFMT_S16_LE CHANNELS = 2 VOLUME_PERCENTAGE = 50 def generate_sine_wave(frequency, duration, sample_rate): num_samples = int(duration * sample_rate) sine_wave = [ int(32767.0 * math.sin(2.0 * math.pi * frequency * x / sample_rate)) for x in range(num_samples) ] # Ensure stereo by duplicating samples for each channel stereo_wave = [] for sample in sine_wave: stereo_wave.extend([sample, sample]) return array.array(\'h\', stereo_wave).tobytes() def main(): try: # Open audio device audio_device = ossaudiodev.open(\'w\') print(\\"Audio device opened\\") # Set audio parameters audio_device.setfmt(FORMAT) audio_device.channels(CHANNELS) audio_device.speed(SAMPLE_RATE) print(\\"Audio device configured\\") # Generate and write sample audio data audio_data = generate_sine_wave(FREQUENCY, DURATION, SAMPLE_RATE) audio_device.writeall(audio_data) print(\\"Sample audio data written\\") # Adjust master volume on mixer device mixer_device = ossaudiodev.openmixer() mixer_device.set(ossaudiodev.SOUND_MIXER_VOLUME, (VOLUME_PERCENTAGE, VOLUME_PERCENTAGE)) print(\\"Master volume set to 50%\\") except ossaudiodev.OSSAudioError as e: print(f\\"OSSAudioError: {e}\\") except OSError as e: print(f\\"OSError: {e}\\") finally: try: audio_device.close() print(\\"Audio device closed\\") except NameError: pass try: mixer_device.close() print(\\"Mixer device closed\\") except NameError: pass if __name__ == \\"__main__\\": main()"},{"question":"# Drag and Drop in tkinter **Objective:** Create and implement a basic drag-and-drop functionality within a tkinter application using the `tkinter.dnd` module. **Description:** You are tasked with designing a simple graphical user interface (GUI) using the `tkinter` library, where users can drag a label widget from one part of the window and drop it into another designated drop zone within the same window. The drop zone should only accept the label if it meets certain conditions. **Requirements:** 1. **Create the Main Window:** - Initialize a `tkinter` root window. - Set the window title to \\"tkinter Drag and Drop Demo\\". 2. **Create a Draggable Label:** - Add a `Label` widget with some text (e.g., \\"Drag me!\\") to the window. - Bind the `ButtonPress` event of the Label to initiate dragging. 3. **Create a Drop Zone:** - Add a `Canvas` or a designated area (e.g., a `Label` or `Frame`) to the window to serve as the drop zone. - Ensure that the drop zone has a visual distinction (e.g., a border or background color). 4. **Implement Drag-and-Drop Logic:** - Use `dnd_start()` from the `tkinter.dnd` module to initiate the drag process. - The drop zone should have a `dnd_accept` method that defines the conditions under which the label can be accepted (e.g., based on label text or other criteria). - Provide implementations for `dnd_enter`, `dnd_leave`, `dnd_commit`, and `dnd_end` methods to handle the drop process and visual feedback. 5. **Constraints:** - Ensure that the draggable label can only be dropped once. - The drop zone should give visual feedback (e.g., change color) when a valid object is dragged over it. **Expected Functionality:** When the user clicks and holds the draggable label, they should be able to move it around the window. Upon releasing the mouse button over the designated drop zone (if accepted), the label should be effectively \\"dropped\\" into that zone with some visual indication that the drop was successful. **Code Template:** Below is a basic template to help you get started: ```python import tkinter as tk import tkinter.dnd as dnd class DragDropDemo(tk.Tk): def __init__(self): super().__init__() self.title(\\"tkinter Drag and Drop Demo\\") self.draggable_label = tk.Label(self, text=\\"Drag me!\\") self.draggable_label.pack(pady=10) self.drop_zone = tk.Label(self, text=\\"Drop zone\\", bg=\\"lightgray\\", width=20, height=5) self.drop_zone.pack(pady=10) self.draggable_label.bind(\\"<ButtonPress>\\", self.start_drag) def start_drag(self, event): dnd.dnd_start(self.draggable_label, event) # Implement the required methods for drag-and-drop here def dnd_accept(self, source, event): pass def dnd_enter(self, source, event): pass def dnd_leave(self, source, event): pass def dnd_commit(self, source, event): pass def dnd_end(self, target, event): pass if __name__ == \\"__main__\\": app = DragDropDemo() app.mainloop() ``` **Note:** Your solution should demonstrate a clear understanding of event handling, widget manipulation, and the use of the `tkinter.dnd` module to enable drag-and-drop functionality.","solution":"import tkinter as tk from tkinter import dnd class DragDropDemo(tk.Tk): def __init__(self): super().__init__() self.title(\\"tkinter Drag and Drop Demo\\") self.geometry(\\"300x300\\") self.draggable_label = tk.Label(self, text=\\"Drag me!\\", bg=\\"yellow\\") self.draggable_label.pack(pady=10) self.drop_zone = tk.Label(self, text=\\"Drop zone\\", bg=\\"lightgray\\", width=20, height=5) self.drop_zone.pack(pady=10) self.draggable_label.bind(\\"<ButtonPress>\\", self.start_drag) self.drop_zone.dnd_accept = self.dnd_accept self.drop_zone.dnd_enter = self.dnd_enter self.drop_zone.dnd_leave = self.dnd_leave self.drop_zone.dnd_commit = self.dnd_commit def start_drag(self, event): dnd.dnd_start(self.draggable_label, event) def dnd_accept(self, source, event): if source is self.draggable_label: return self.drop_zone def dnd_enter(self, source, event): if source is self.draggable_label: self.drop_zone.config(bg=\'green\') def dnd_leave(self, source, event): if source is self.draggable_label: self.drop_zone.config(bg=\'lightgray\') def dnd_commit(self, source, event): if source is self.draggable_label: self.drop_zone.config(bg=\'lightblue\', text=\\"Dropped!\\") self.draggable_label.unbind(\\"<ButtonPress>\\") def dnd_end(self, target, event): pass if __name__ == \\"__main__\\": app = DragDropDemo() app.mainloop()"},{"question":"# Coding Challenge: Audio Processing with `ossaudiodev` Objective: You need to demonstrate your understanding of the `ossaudiodev` module by implementing a function that records a given number of seconds of audio input and plays it back immediately. The function should set specific audio parameters and ensure the playback parameters match the recording parameters. Function Signature: ```python def record_and_playback(seconds: int, audio_format: str, channels: int, sample_rate: int) -> None: pass ``` Input: - `seconds` (int): The number of seconds of audio to record. - `audio_format` (str): The format of the audio (e.g., `AFMT_S16_LE`). - `channels` (int): The number of audio channels (e.g., 1 for mono, 2 for stereo). - `sample_rate` (int): The audio sample rate in Hz (e.g., 44100). Constraints: - `seconds` must be a positive integer. - `audio_format` must be a valid OSS audio format as defined in the `ossaudiodev` module. - `channels` must be a positive integer (typically 1 or 2). - `sample_rate` must be a positive integer (commonly used rates include 8000, 11025, 22050, 44100, 96000). Output: The function does not return any value but performs the following actions: 1. Records audio from the default audio input device for the specified number of seconds with the provided parameters. 2. Plays back the recorded audio immediately using the same audio parameters. Steps to Implement: 1. Open the default audio device for recording using `ossaudiodev.open()`. 2. Set the audio format, channels, and sample rate using the appropriate methods (`setfmt`, `channels`, `speed`). 3. Read audio data for the specified duration. 4. Open the default audio device for playback. 5. Set the same audio parameters for playback. 6. Write the recorded audio data to the playback device. 7. Ensure proper cleanup by closing devices. Example: ```python import ossaudiodev import time def record_and_playback(seconds: int, audio_format: str, channels: int, sample_rate: int) -> None: # Open the audio device for recording dsp_in = ossaudiodev.open(\\"r\\") dsp_in.setparameters(audio_format, channels, sample_rate) # Calculate the total number of bytes to read bytes_per_second = sample_rate * channels * 2 # assuming 16-bit samples total_bytes = seconds * bytes_per_second # Read audio input audio_data = dsp_in.read(total_bytes) dsp_in.close() # Open the audio device for playback dsp_out = ossaudiodev.open(\\"w\\") dsp_out.setparameters(audio_format, channels, sample_rate) # Playback the recorded audio data dsp_out.writeall(audio_data) dsp_out.sync() dsp_out.close() ``` Test the function to ensure it records and plays back audio correctly with various parameters.","solution":"import ossaudiodev import time def record_and_playback(seconds: int, audio_format: int, channels: int, sample_rate: int) -> None: Records audio from the default audio input device for the specified number of seconds with the provided parameters and plays back the recorded audio immediately. Parameters: seconds (int): The number of seconds of audio to record. audio_format (int): The format of the audio (e.g., ossaudiodev.AFMT_S16_LE). channels (int): The number of audio channels (e.g., 1 for mono, 2 for stereo). sample_rate (int): The audio sample rate in Hz (e.g., 44100). # Open the audio device for recording dsp_in = ossaudiodev.open(\\"r\\") dsp_in.setfmt(audio_format) dsp_in.channels(channels) dsp_in.speed(sample_rate) # Calculate the total number of bytes to read bytes_per_second = sample_rate * channels * (ossaudiodev.AFMT_S16_LE // 8) # 2 bytes per sample if AFMT_S16_LE total_bytes = seconds * bytes_per_second # Read audio input audio_data = dsp_in.read(total_bytes) dsp_in.close() # Open the audio device for playback dsp_out = ossaudiodev.open(\\"w\\") dsp_out.setfmt(audio_format) dsp_out.channels(channels) dsp_out.speed(sample_rate) # Playback the recorded audio data dsp_out.write(audio_data) dsp_out.close()"},{"question":"# Custom Finder and Loader Objective Design a custom finder and loader using the `importlib` module to import Python modules from a network location. Problem Statement You are required to create a custom finder and loader that allows importing Python modules from a specified HTTP server. For the purposes of this exercise, you can assume the server serves Python files at URLs like `http://example.com/modules/<module_name>.py`. # Requirements: 1. **Custom Finder**: - Implement a custom finder that checks if a module exists on the server. - The finder should be registered as a meta path finder in `sys.meta_path`. 2. **Custom Loader**: - Implement a custom loader that fetches the module source code from the server, compiles it, and executes it to create a module. 3. **Functionality**: - You should be able to import a module, for example, `foo`, using the statement `import foo`, where the module is located at `http://example.com/modules/foo.py`. 4. **Handling Cache**: - If the module is imported successfully, it should be cached in `sys.modules`. 5. **Error Handling**: - Raise appropriate errors if the module cannot be found or if there are issues during fetching or compiling the source code. Input and Output Formats 1. **Input**: - A URL base for the HTTP server containing Python modules. - The name of the module to import (e.g., `foo`). 2. **Output**: - The module should be imported and usable in the script. Constraints - The module names will always be valid Python identifiers. - Network failures may occur, handle them gracefully. Example Suppose the server serves the following Python file at `http://example.com/modules/foo.py`: ```python # File: foo.py def hello(): return \\"Hello from foo\\" ``` You should be able to import `foo` and use the `hello` function as follows: ```python import foo print(foo.hello()) # Output: Hello from foo ``` Implementation Implement your solution in a class, `NetworkModuleFinder`. The class should have the following methods: 1. `__init__(self, base_url)`: Initialize with the base URL of the server. 2. `find_spec(self, fullname, path, target=None)`: Method to find and return the module spec. 3. `NetworkModuleLoader`: Inner class to handle loading the module. ```python import sys import importlib.abc import importlib.util import types import urllib.request class NetworkModuleFinder(importlib.abc.MetaPathFinder): def __init__(self, base_url): self.base_url = base_url.rstrip(\'/\') + \'/\' def find_spec(self, fullname, path, target=None): url = self.base_url + fullname + \'.py\' try: with urllib.request.urlopen(url) as response: if response.status == 200: return importlib.util.spec_from_loader(fullname, self.NetworkModuleLoader(url)) except urllib.error.URLError: pass return None class NetworkModuleLoader(importlib.abc.Loader): def __init__(self, url): self.url = url def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = self._get_code() exec(code, module.__dict__) def _get_code(self): with urllib.request.urlopen(self.url) as response: source = response.read().decode(\'utf-8\') return compile(source, self.url, \'exec\') # Usage example sys.meta_path.insert(0, NetworkModuleFinder(\'http://example.com/modules\')) import foo print(foo.hello()) ``` This example should help you understand how to implement and test the custom import machinery.","solution":"import sys import importlib.abc import importlib.util import types import urllib.request class NetworkModuleFinder(importlib.abc.MetaPathFinder): def __init__(self, base_url): self.base_url = base_url.rstrip(\'/\') + \'/\' def find_spec(self, fullname, path, target=None): url = self.base_url + fullname + \'.py\' try: with urllib.request.urlopen(url) as response: if response.status == 200: return importlib.util.spec_from_loader(fullname, self.NetworkModuleLoader(url)) except urllib.error.URLError: pass return None class NetworkModuleLoader(importlib.abc.Loader): def __init__(self, url): self.url = url def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = self._get_code() exec(code, module.__dict__) def _get_code(self): with urllib.request.urlopen(self.url) as response: source = response.read().decode(\'utf-8\') return compile(source, self.url, \'exec\') def install_network_module_finder(base_url): sys.meta_path.insert(0, NetworkModuleFinder(base_url))"},{"question":"# Task You are given the Titanic dataset. Your task is to create a specific violin plot using Seaborn and then analyze some aspects of the plot. Follow the instructions below to complete the task. # Instructions 1. **Data Loading and Preparation:** - Load the Titanic dataset using `seaborn`\'s `load_dataset` function. 2. **Generate the Plot:** - Create a grouped violin plot that shows the distribution of passengers\' ages (`age`) against their travel class (`class`). - Color each group by the survival status (`alive`), using different colors for those who lived and those who did not. - Split the violins to show each subgroup side by side within the same plot area. - Use \\"quart\\" as the inner value representation to indicate quartiles within each subgroup. 3. **Plot Customization:** - Add a small gap between the split violins to make them more visually distinct (`gap = 0.1`). - Adjust the KDE bandwidth for the smoothing to half its default value. 4. **Analysis Questions:** - Describe the general trends you observe in the relationship between the age and travel class of the passengers. - How do the age distributions of survivors and non-survivors differ among the different travel classes? # Expected Input and Output Formats Your function should not accept any input arguments nor return any output. The execution and results should be fully observable on a Jupyter notebook cell. # Constraints - Use the Titanic dataset provided with Seaborn. - Maintain the default Seaborn theme settings unless specified otherwise. # Performance Requirements - The plot should be generated within a reasonable time (<1 minute). # Example Code and Output ```python import seaborn as sns # Load dataset df = sns.load_dataset(\\"titanic\\") # Create Violin plot sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True, inner=\\"quart\\", gap=0.1, bw_adjust=0.5) # Display the plot import matplotlib.pyplot as plt plt.show() ``` # Analysis - **Trend Analysis:** Provide a description of the trends you observe. - **Distribution Differences:** Compare the age distributions of survivors and non-survivors across different travel classes.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_violin_plot(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Create the violin plot sns.violinplot( data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True, inner=\\"quart\\", bw=0.5 ) plt.show() # Analysis part def analyze_violin_plot(): Analyzes general trends in the violin plot for Titanic dataset. General Trends Observed: - First Class: Passengers in first class tend to be older compared to other classes. - Second Class: A moderate age range is observed, slightly younger than first class. - Third Class: Passengers tend to be younger, with the age distribution skewed towards children and younger adults. Survivorship Analysis: - First Class: Survivors tend to be of various ages, with a slight concentration of older ages. - Second Class: The difference in age distribution between survivors and non-survivors is less pronounced. - Third Class: Younger passengers, particularly children, have a higher survival rate compared to older passengers. # Execute the functions to generate plot and analyze create_violin_plot() analyze_violin_plot()"},{"question":"Coding Assessment Question # Objective The task aims to assess the understanding of the candidate on extending the TorchElastic framework to customize distributed training in PyTorch. # Problem Statement You are required to implement a custom `MetricHandler` that counts and logs the number of times each type of metric is emitted during the execution of a distributed training job using TorchElastic. Specifically, you will extend the `torch.distributed.elastic.metrics.MetricHandler` class and implement the `emit` method to update and log the count of various metrics. # Input - There are no specific inputs in the traditional sense as this is an implementation task. # Output - Your custom `MetricHandler` should log the count of each type of metric emitted whenever the `emit` method is called. # Specification 1. **MetricHandler Implementation**: - Implement a class `CountingMetricHandler` that extends `torch.distributed.elastic.metrics.MetricHandler`. - The `emit` method should take a `torch.distributed.elastic.metrics.MetricData` object as an argument and update a count of each type of metric emitted (you can assume the metric data has an attribute `name` which is a string representing the metric name). 2. **Logging Mechanism**: - Use Python\'s built-in `logging` module to log the count of each metric type. 3. **Integration**: - Write a function `configure_metrics` that configures this custom metric handler in a PyTorch Elastic Agent setup. # Constraints - The solution should be efficient with respect to time complexity considering the emit function might be called frequently. # Example Usage Below is an example of how your `CountingMetricHandler` should be used and integrated. You do not need to implement the entire distributed training script, just the handler and configuration. ```python import torch.distributed.elastic.metrics as metrics class CountingMetricHandler(metrics.MetricHandler): def __init__(self): self.metrics_count = {} def emit(self, metric_data: metrics.MetricData): metric_name = metric_data.name if metric_name in self.metrics_count: self.metrics_count[metric_name] += 1 else: self.metrics_count[metric_name] = 1 logging.info(f\\"Metric {metric_name}: Count {self.metrics_count[metric_name]}\\") def configure_metrics(): metrics.configure(CountingMetricHandler()) # Example of torch elastic agent using the custom handler if __name__ == \\"__main__\\": from torch.distributed.elastic.agent.local_elastic_agent import LocalElasticAgent from torch.distributed.elastic.utils import parse_args configure_metrics() args = parse_args(sys.argv[1:]) rdzv_handler = RendezvousHandler(...) spec = WorkerSpec( local_world_size=args.nproc_per_node, fn=trainer_entrypoint_fn, args=(trainer_entrypoint_fn_args,), rdzv_handler=rdzv_handler, max_restarts=args.max_restarts, monitor_interval=args.monitor_interval, ) agent = LocalElasticAgent(spec, start_method=\\"spawn\\") agent.run() ``` # Assessment Criteria - Correctness: Does the `CountingMetricHandler` correctly count and log each metric? - Efficiency: Is the `emit` method efficient and able to handle frequent metric emissions? - Integration: Is the custom handler correctly configured and integrated into the TorchElastic framework?","solution":"import logging from torch.distributed.elastic.metrics import MetricHandler, MetricData # Setting up logging configuration logging.basicConfig(level=logging.INFO) class CountingMetricHandler(MetricHandler): def __init__(self): self.metrics_count = {} def emit(self, metric_data: MetricData): metric_name = metric_data.name if metric_name in self.metrics_count: self.metrics_count[metric_name] += 1 else: self.metrics_count[metric_name] = 1 logging.info(f\\"Metric {metric_name}: Count {self.metrics_count[metric_name]}\\") def configure_metrics(): from torch.distributed.elastic.metrics import configure configure(CountingMetricHandler())"},{"question":"# Objective: Implement a function that performs a series of operations using the \\"operator\\" module functions. The function will process a list of integers, apply several transformations and comparisons, and return a specific result based on these operations. # Function Signature: ```python def process_numbers(nums: list, threshold: int) -> bool: Parameters: - nums (list): A list of integers. - threshold (int): An integer threshold value. Returns: - bool: The function will return True if the processed result meets a certain condition, False otherwise. ``` # Instructions: 1. Compute the absolute values of all integers in the given list using the appropriate \\"operator\\" module function. 2. Calculate the sum of all these absolute values using the appropriate \\"operator\\" module function for addition. 3. Calculate the bitwise XOR of all the absolute values in the list using the appropriate \\"operator\\" module function. 4. Check if the bitwise XOR result is an element in the original list, using the appropriate \\"operator\\" function for containment. 5. Compare the sum calculated in step 2 with the provided threshold using the appropriate \\"operator\\" module function. 6. Return `True` if both conditions are met: the XOR result is in the original list, and the sum is greater than or equal to the threshold. Otherwise, return `False`. # Example: ```python nums = [1, -3, 2] threshold = 4 # After absolute values: [1, 3, 2] # Sum of absolute values: 1 + 3 + 2 = 6 # Bitwise XOR of absolute values: 1 ^ 3 ^ 2 = 0 # Check if 0 is in [1, -3, 2] -> False # Check if sum (6) >= threshold (4) -> True # Since one condition is False, the function should return False output = process_numbers(nums, threshold) print(output) # Output should be False ``` # Notes: - You must use functions from the \\"operator\\" module to handle all specified operations. - Ensure the use of appropriate functions for each step to demonstrate the application of the \\"operator\\" module.","solution":"import operator def process_numbers(nums: list, threshold: int) -> bool: Process the list of integers, apply transformations and comparisons, and return a result based on the operations. Parameters: - nums (list): A list of integers. - threshold (int): An integer threshold value. Returns: - bool: True if the XOR result is in the original list and the sum of absolute values is greater than or equal to the threshold. False otherwise. # Compute the absolute values abs_values = list(map(operator.abs, nums)) # Calculate the sum of absolute values sum_abs_values = sum(abs_values) # Calculate the bitwise XOR of the absolute values xor_result = 0 for value in abs_values: xor_result = operator.xor(xor_result, value) # Check if the XOR result is an element in the original list is_xor_in_list = operator.contains(nums, xor_result) # Compare the sum with the threshold is_sum_ge_threshold = operator.ge(sum_abs_values, threshold) return is_xor_in_list and is_sum_ge_threshold"},{"question":"You are given a dataset containing a list of `X` values and corresponding `y` values. Your task is to fit an isotonic regression model to this data using the `IsotonicRegression` class from the sklearn library. You must handle both increasing and decreasing regression based on a user-provided parameter. Additionally, you must evaluate the model by calculating the mean squared error on the fitted data. Write a function `fit_and_evaluate_isotonic_regression(X, y, increasing=True)`: # Input: - `X`: List or array of real numbers representing the input data points. - `y`: List or array of real numbers representing the target values corresponding to each input data point in `X`. - `increasing`: Boolean parameter (default is True). If `True`, fit an increasing isotonic regression; if `False`, fit a decreasing isotonic regression. # Output: - A tuple containing: - The fitted values corresponding to the input `X`. - The mean squared error of the fitted values compared to the actual `y` values. # Constraints: - You must use the `IsotonicRegression` class from sklearn to fit the model. - The input arrays `X` and `y` will have lengths between 2 and 1000 inclusive, and the weights will be uniformly set to 1. # Example: ```python from sklearn.metrics import mean_squared_error def fit_and_evaluate_isotonic_regression(X, y, increasing=True): # Import the IsotonicRegression class from sklearn.isotonic import IsotonicRegression # Initialize the IsotonicRegression model with the specified parameter iso_reg = IsotonicRegression(increasing=increasing) # Fit the model using the input data y_fitted = iso_reg.fit_transform(X, y) # Calculate the mean squared error mse = mean_squared_error(y, y_fitted) return y_fitted, mse # Example usage: X = [0, 1, 2, 3, 4, 5] y = [1, 2, 2, 3, 4, 3] print(fit_and_evaluate_isotonic_regression(X, y)) # Output: (array([1., 2., 2., 2.75, 3.5, 3.5]), mean squared error value) print(fit_and_evaluate_isotonic_regression(X, y, increasing=False)) # Output: array of fitted values and mean squared error ``` This question will test the students\' understanding of: - Importing and using the `IsotonicRegression` class. - Setting parameters to control regression behavior. - Fitting and evaluating a regression model using sklearn tools.","solution":"def fit_and_evaluate_isotonic_regression(X, y, increasing=True): Fits an isotonic regression model to the data and evaluates it using mean squared error. Parameters: X (list or array): List or array of real numbers representing the input data points. y (list or array): List or array of real numbers representing the target values. increasing (bool): Whether to fit an increasing (default) or decreasing isotonic regression. Returns: tuple: Tuple containing the fitted values and the mean squared error. from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error # Initialize the IsotonicRegression model with the specified parameter iso_reg = IsotonicRegression(increasing=increasing) # Fit the model using the input data y_fitted = iso_reg.fit_transform(X, y) # Calculate the mean squared error mse = mean_squared_error(y, y_fitted) return y_fitted, mse"},{"question":"**Objective:** Demonstrate your understanding of the `faulthandler` module in Python by writing functions to manage fault handlers, trigger traceback dumps, and handle file descriptor issues. **Problem Statement:** 1. **Fault Handler Management:** Write a function `manage_fault_handler(enable: bool) -> bool`: - When `enable` is `True`, enable the fault handler to handle signals (`SIGSEGV`, `SIGFPE`, `SIGABRT`, `SIGBUS`, and `SIGILL`). - When `enable` is `False`, disable the fault handler. - Return `True` if the fault handler was successfully enabled or disabled, otherwise return `False`. 2. **Trackback on Timeout:** Write a function `trigger_timeout_traceback(timeout: int, repeat: bool=False, exit_after: bool=False) -> bool`: - Set up a traceback dump after `timeout` seconds. - If `repeat` is `True`, set it to repeat every `timeout` seconds. - If `exit_after` is `True`, the process should exit after dumping tracebacks. - Return `True` if the timeout was successfully set, otherwise return `False`. 3. **Handle User Signal:** Write a function `register_user_signal(signum: int, chain: bool=False) -> bool`: - Register a user signal `signum` to dump tracebacks of all threads. - If `chain` is `True`, call the previous handler after dumping the traceback. - Return `True` if the signal was successfully registered, otherwise return `False`. 4. **Cleanup on File Closure:** Write a function `handle_file_closure(file_path: str) -> None`: - Create a file at `file_path` and register handlers to dump tracebacks into this file. - Demonstrate handling of file descriptor issues by closing and reopening the file, ensuring handlers are properly re-registered. **Constraints:** - Use the `faulthandler` module and its methods effectively. - Perform signal-safe operations, avoiding functions that require dynamic memory allocation during callbacks. - Handle exceptions in a way that does not compromise the functionality of fault handlers. **Example Usage:** ```python # Example file path for test file_path = \\"/tmp/test_traceback.log\\" # Manage fault handler print(manage_fault_handler(True)) # Expected output: True print(faulthandler.is_enabled()) # Expected output: True # Trigger timeout-based traceback print(trigger_timeout_traceback(5)) # Expected output: True # Register user signal for traceback import signal print(register_user_signal(signal.SIGUSR1)) # Expected output: True # Handle file descriptor closure issues handle_file_closure(file_path) # Check /tmp/test_traceback.log file for tracebacks ``` **Submission:** Submit your Python script with function implementations. Ensure functions are well-documented with comments explaining the logic used.","solution":"import faulthandler import signal def manage_fault_handler(enable: bool) -> bool: Enable or disable the fault handler. if enable: faulthandler.enable() return faulthandler.is_enabled() else: faulthandler.disable() return not faulthandler.is_enabled() def trigger_timeout_traceback(timeout: int, repeat: bool=False, exit_after: bool=False) -> bool: Set up a traceback dump after a specified timeout. try: faulthandler.dump_traceback_later(timeout, repeat=repeat, exit=exit_after) return True except Exception as e: return False def register_user_signal(signum: int, chain: bool=False) -> bool: Register a user signal to dump tracebacks of all threads. try: faulthandler.register(signum, chain=chain) return True except Exception as e: return False def handle_file_closure(file_path: str) -> None: Handle file descriptor closure issues. with open(file_path, \'w\') as file: faulthandler.enable(file) # Now, closing the file file.close() # Reopen to re-register the handler with open(file_path, \'w\') as file: faulthandler.enable(file)"},{"question":"Objective Demonstrate your understanding of the seaborn package, specifically focusing on creating custom color palettes using the `sns.blend_palette` function and applying these palettes in seaborn visualizations. Problem Statement You are given a dataset of your choice (e.g., Iris, Penguins, Tips), and you need to create a clear and informative scatter plot visualizing two numerical variables from the dataset, colored by a categorical variable. However, you need to first create a custom color palette or colormap and apply it to the scatter plot based on the given conditions. Task 1. Load a dataset of your choice (e.g., Iris, Penguins, Tips) using seaborn. 2. Define a custom color palette using `sns.blend_palette` with at least three distinct colors. Ensure the palette includes both named colors and hex codes. 3. Visualize the relationship between any two numerical features in a scatter plot, using the custom color palette to differentiate categories of a categorical variable. 4. Save the plot to a file named `\\"custom_scatter_plot.png\\"`. Input and Output Formats - **Input:** - A dataset available in seaborn (you can select iris, penguins, or tips dataset). - A set of at least three colors (mix of named colors and hex codes). - **Output:** - A scatter plot that uses the custom color palette. - The scatter plot image saved as `\\"custom_scatter_plot.png\\"`. Constraints - Use the `sns.blend_palette` function to create the color palette. - You must use at least one named color and one hex code. - The palette must be applied to a categorical variable in the scatter plot. Example Below is a simplified example of creating a custom color palette and using it in a scatter plot: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = sns.load_dataset(\\"iris\\") # Create a custom color palette palette = sns.blend_palette([\\"#45a872\\", \\"blue\\", \\"xkcd:golden\\"]) # Create the scatter plot sns.scatterplot(data=df, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", palette=palette) # Save the plot to a file plt.savefig(\\"custom_scatter_plot.png\\") ``` Ensure that your solution goes beyond this example by carefully selecting colors, handling edge cases, and producing a well-labeled and informative scatter plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatter_plot(): # Load the dataset df = sns.load_dataset(\\"iris\\") # Create a custom color palette palette = sns.blend_palette([\\"#ff6347\\", \\"blue\\", \\"xkcd:sunflower\\"]) # Create the scatter plot scatter_plot = sns.scatterplot(data=df, x=\\"sepal_length\\", y=\\"petal_length\\", hue=\\"species\\", palette=palette) # Add informative labels and title scatter_plot.set(title=\\"Scatter Plot of Iris Dataset\\", xlabel=\\"Sepal Length\\", ylabel=\\"Petal Length\\") # Save the plot to a file plt.savefig(\\"custom_scatter_plot.png\\") # Show the plot plt.show() return # Example usage: create_custom_scatter_plot()"},{"question":"# Question: Optimizing a Scikit-Learn Algorithm Using Profiling Background You have been provided with a machine learning algorithm written in Python using scikit-learn. The algorithm works but is not performing efficiently, especially with large datasets. Your task is to profile the code, identify bottlenecks, and optimize it using appropriate techniques described in the provided documentation. Task 1. **Profiling the Code**: Use IPython\'s profiling tools to identify the main bottlenecks in the given `CustomKMeans` algorithm implementation. 2. **Optimize with Cython**: Rewrite the identified bottleneck segments using Cython to improve performance. 3. **Verification**: Ensure that the optimized code produces the same result as the original code and evaluate the performance improvement. Instructions 1. **Profiling**: - Load the dataset and measure the total execution time of the algorithm without any profiler overhead. - Use `%prun` or `line_profiler` to identify which parts of the code are the most time-consuming. 2. **Optimization**: - Extract the bottleneck segments identified and rewrite them using Cython. - Compile the Cython code and integrate it back into the main algorithm. 3. **Compare Results and Performance**: - Ensure that the optimized code yields the same clustering results as the original implementation. - Measure the execution time of the optimized code and compare it with the original. Provided Code Here is a sample implementation of a simple `CustomKMeans` algorithm: ```python import numpy as np from sklearn.datasets import load_iris class CustomKMeans: def __init__(self, n_clusters=3, max_iter=300, tol=1e-4): self.n_clusters = n_clusters self.max_iter = max_iter self.tol = tol def fit(self, X): np.random.seed(42) random_idx = np.random.permutation(X.shape[0]) self.centroids = X[random_idx[:self.n_clusters]] for _ in range(self.max_iter): self.labels_ = self._assign_labels(X) new_centroids = np.array([X[self.labels_ == i].mean(axis=0) for i in range(self.n_clusters)]) if np.all(np.abs(self.centroids - new_centroids) < self.tol): break self.centroids = new_centroids def _assign_labels(self, X): distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2) return np.argmin(distances, axis=1) if __name__ == \\"__main__\\": data = load_iris().data kmeans = CustomKMeans(n_clusters=3) kmeans.fit(data) ``` Submission Requirements 1. **Profile Report**: Include the output of your profiling session identifying the bottlenecks. 2. **Optimized Code**: Provide the Cython-optimized segment and the integrated final version of the `CustomKMeans` algorithm. 3. **Performance Comparison**: Report the execution time of both the original and optimized versions. Show that the results (cluster labels) are identical. Constraints - Ensure that the code runs efficiently, even for large datasets with tens of thousands of samples. - Your submission will be evaluated on correctness, performance improvement, and adherence to the profiling and optimization techniques described.","solution":"import numpy as np from sklearn.datasets import load_iris class CustomKMeans: def __init__(self, n_clusters=3, max_iter=300, tol=1e-4): self.n_clusters = n_clusters self.max_iter = max_iter self.tol = tol def fit(self, X): np.random.seed(42) random_idx = np.random.permutation(X.shape[0]) self.centroids = X[random_idx[:self.n_clusters]] for _ in range(self.max_iter): self.labels_ = self._assign_labels(X) new_centroids = np.array([X[self.labels_ == i].mean(axis=0) for i in range(self.n_clusters)]) if np.all(np.abs(self.centroids - new_centroids) < self.tol): break self.centroids = new_centroids def _assign_labels(self, X): distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2) return np.argmin(distances, axis=1) if __name__ == \\"__main__\\": data = load_iris().data kmeans = CustomKMeans(n_clusters=3) kmeans.fit(data)"},{"question":"**Advanced Debugging with `pdb`:** As a Python developer, understanding how to effectively use the `pdb` (Python Debugger) module is crucial for debugging complex issues in your code. In this assessment, you will create a custom debugging scenario using `pdb` to locate and fix an error in a provided script. # Task: You are given a simple Python script containing several functions. Your task is to implement a debug function using the `pdb` module that satisfies the following requirements: 1. **Set a breakpoint** at the start of a function. 2. **Step through and inspect variables** within the function to understand the cause of the error. 3. **Capture and print the call stack** when the error is encountered. 4. **Post-mortem debugging** to determine the state at the time of the error. # Provided Code (`example_script.py`): ```python def function_a(x, y): return x + function_b(y) def function_b(z): if z > 0: return function_c(z) * 2 else: return function_c(z) / 2 def function_c(w): if w == 0: raise ValueError(\\"Invalid value: w cannot be 0.\\") return 10 / w def main(): a = 3 b = 0 # This will cause an error in function_c print(f\\"Result: {function_a(a, b)}\\") if __name__ == \\"__main__\\": main() ``` # Requirements: 1. **Create a function `debug_example_script`**: - This function should execute `example_script.py` under `pdb` control. - Set a **breakpoint** at the start of `function_b`. - Use `pdb` commands to **step through** the code, **inspect variables** (`a`, `b`, `w`, etc.), and understand where and why the error occurs. - **Print the call stack** when the error is encountered. - Perform **post-mortem debugging** to inspect the state of the program at the time of the error. # Example Output: Your output should capture the debugging process indicating where you set breakpoints, the state of variables at key points, and the call stack when the error occurs. Include the final findings about the root cause of the error. The function signature should be: ```python def debug_example_script(): pass ``` Implement the `debug_example_script` function to meet these requirements. Use the `pdb` module methods and commands effectively to demonstrate your comprehension of debugging advanced Python scenarios. # Constraints: - Do not modify the provided script directly. Only use `pdb` commands to control the execution flow and gather information. - Ensure your solution is efficient and captures all necessary details for thorough debugging. **Note**: After completing the task, you should be able to determine that the script errors because `b` is set to `0`, which causes a `ValueError` in `function_c`. Your debug output should clearly show this discovery process.","solution":"import pdb import traceback def debug_example_script(): Function to debug the `example_script.py` using pdb. Sets breakpoints, steps through the code, inspects variables and provides call stack when an error is encountered. Also performs post-mortem debugging. try: # Simulate the script environment def function_a(x, y): return x + function_b(y) def function_b(z): pdb.set_trace() # Set breakpoint if z > 0: return function_c(z) * 2 else: return function_c(z) / 2 def function_c(w): if w == 0: raise ValueError(\\"Invalid value: w cannot be 0.\\") return 10 / w def main(): a = 3 b = 0 # This will cause an error in function_c print(f\\"Result: {function_a(a, b)}\\") if __name__ == \\"__main__\\": main() except Exception as e: print(\\"Exception occurred:\\") traceback.print_exc() print(\\"nStarting post-mortem debugging with pdb\\") pdb.post_mortem() # Call the debug function (Uncomment it during actual debugging) # debug_example_script()"},{"question":"Objective Your task is to create a PyTorch model, export it using `torch.export` to obtain its Export IR representation, and then manipulate this representation to add an additional operation to the graph. Problem Statement 1. Define a simple PyTorch model that takes two input tensors, `x` and `y`, and returns their sum. 2. Use `torch.export.export` to export this model, obtaining an `ExportedProgram`. 3. Modify the resulting Export IR graph by adding a new operation that multiplies the result of the addition by a constant scalar `2`. 4. Generate a Python function that, when called, executes the modified Export IR graph. Specifications 1. **Define the Model**: - The model should be a PyTorch `nn.Module` that takes two inputs `x` and `y` and returns `x + y`. 2. **Export the Model**: - Use `torch.export.export` with appropriate example inputs to export the model to an `ExportedProgram`. 3. **Modify the Export IR Graph**: - Access the `graph` attribute of the `ExportedProgram` and add a new `call_function` node that multiplies the output by 2. - Ensure that this node is inserted correctly so that the final output of the graph represents `(x + y) * 2`. 4. **Generate the Python Executor**: - Convert the modified graph back to a Python function ensuring it is executable and produces the correct results. Input - Two tensors `x` and `y`. Output - A tensor representing `(x + y) * 2`. Constraints - Assume input tensors are of compatible dimensions for the operations. - Use the information and methods described in the provided documentation to manipulate the graph. - Ensure to maintain correct node metadata as described in the documentation. Example ```python import torch from torch import nn from torch.export import export from torch.fx import GraphModule # Step 1: Define the Model class Model(nn.Module): def forward(self, x, y): return x + y # Step 2: Export the Model model = Model() example_inputs = (torch.randn(2, 3), torch.randn(2, 3)) exported_program = export(model, example_inputs) # Step 3: Modify the Export IR Graph graph = exported_program.graph graph_module = exported_program.graph_module # Create a new node that multiplies the result of addition by 2 mult_node = graph.create_node( \'call_function\', torch.mul, args=(graph.nodes[-1], 2), ) # Set the output of the graph to be the new multiplication node graph.output(mult_node) # Step 4: Generate the Python Executor new_graph_module = GraphModule(graph_module, graph) exec_func = new_graph_module.forward # Verify the result val_x = torch.tensor([[1, 2, 3], [4, 5, 6]]) val_y = torch.tensor([[7, 8, 9], [10, 11, 12]]) result = exec_func(val_x, val_y) print(result) # This should print tensor representing ((x + y) * 2) ```","solution":"import torch from torch import nn from torch.fx import symbolic_trace # Step 1: Define the Model class SimpleAddModel(nn.Module): def forward(self, x, y): return x + y # Step 2: Export the Model model = SimpleAddModel() example_inputs = (torch.randn(2, 3), torch.randn(2, 3)) traced_graph = symbolic_trace(model) # Step 3: Modify the Export IR Graph # Insert a new node that multiplies the output of the addition by 2 graph = traced_graph.graph for node in graph.nodes: if node.op == \\"output\\": with graph.inserting_before(node): new_node = graph.call_function(torch.mul, args=(list(node.args)[0], 2)) node.args = (new_node,) graph.lint() # Step 4: Generate the Python Executor # Compile the modified graph back into a usable model modified_graph_module = torch.fx.GraphModule(traced_graph, graph) def modified_model(x, y): return modified_graph_module(x, y)"},{"question":"# **Randomized Task Scheduler Assessment** **Objective:** Design a randomized task scheduler that simulates distributing tasks among a group of workers while ensuring no worker is idle for long. **Task Description:** You are required to implement a function `schedule_tasks(workers: int, tasks: int, max_task_time: int) -> dict` that: 1. **Distributes tasks randomly** among the provided number of workers. 2. **Each task** has a random completion time between 1 and `max_task_time`. 3. **Ensures fairness**: No worker should have more than 20% more tasks than the average task count. 4. Returns a dictionary structure with worker IDs as keys and lists of task completion times as values. # **Function Signature:** ```python def schedule_tasks(workers: int, tasks: int, max_task_time: int) -> dict: pass ``` # **Input:** - `workers` (int): Number of workers available. - `tasks` (int): Total number of tasks to be distributed. - `max_task_time` (int): The maximum time a task can take to complete. # **Output:** - Returns a dictionary where: - Each key represents a worker ID (from 1 to `workers`). - Each value is a list of integers representing the time each task takes. # **Constraints:** - `1 <= workers <= 100` - `1 <= tasks <= 10000` - `1 <= max_task_time <= 1000` # **Example:** ```python workers = 5 tasks = 20 max_task_time = 10 # Example Output (your output may vary due to randomness): { 1: [3, 5, 6], 2: [1, 9, 10, 2], 3: [4, 8, 2], 4: [1, 7, 9, 5], 5: [6, 8, 3] } ``` # **Requirements:** 1. Use the functions from the `random` module to accomplish the task. 2. Implement the fairness constraint where no worker should have more than 20% more tasks than the average task count (rounded). # **Testing:** Test your function with various inputs to ensure: 1. Tasks are distributed fairly. 2. Each task’s completion time is between 1 and `max_task_time`. 3. The sum of all tasks\' lists should be equal to `tasks`. **Note:** Your solution will be evaluated on correctness, adherence to constraints, and efficiency.","solution":"import random import math def schedule_tasks(workers: int, tasks: int, max_task_time: int) -> dict: # Create a dictionary to store tasks for each worker task_distribution = {i: [] for i in range(1, workers + 1)} # Calculate the maximum number of tasks any worker can have avg_tasks = tasks / workers max_tasks_per_worker = math.ceil(avg_tasks * 1.2) # Distribute tasks among workers for _ in range(tasks): while True: worker_id = random.randint(1, workers) if len(task_distribution[worker_id]) < max_tasks_per_worker: task_time = random.randint(1, max_task_time) task_distribution[worker_id].append(task_time) break return task_distribution"},{"question":"Objective: Demonstrate your understanding of the `tempfile` module in Python by implementing a function that manages temporary files and directories. Your task is to create a function that: 1. Generates a temporary directory. 2. Within this directory, creates multiple temporary files with specific content. 3. Reads back and verifies the contents of these files. 4. Uses memory spooling for small files to optimize performance. 5. Ensures all temporary resources are cleaned up properly after use. Details: Write a function `manage_temp_files(file_contents: Dict[str, str]) -> Dict[str, str]` that takes a dictionary `file_contents` where keys are filenames and values are the content to be written into the files. The function must create a temporary directory, and within this directory, create temporary files with the provided names and contents. Then, the function should read the content back from these files and return a dictionary mapping the filenames to their read content. Use memory spooling for files smaller than 1 KB. # Function Signature: ```python from typing import Dict def manage_temp_files(file_contents: Dict[str, str]) -> Dict[str, str]: pass ``` # Input: - `file_contents: Dict[str, str]`: A dictionary where keys are filenames (strings) and values are file contents (strings). # Output: - `Dict[str, str]`: A dictionary where keys are filenames (strings) and values are the content read back from the files (strings). # Constraints: - Filenames in the dictionary will be unique. - Content size of individual files will not exceed 1 MB. # Performance Requirements: - Use `SpooledTemporaryFile` for files smaller than 1 KB for better performance. - Ensure proper cleanup to avoid any leftover temporary files or directories. # Example: ```python input_files = { \\"file1.txt\\": \\"Hello World!\\", \\"file2.txt\\": \\"This is a test.\\", \\"file3.log\\": \\"Logging information...\\" } result = manage_temp_files(input_files) # This should be equivalent to: # assert result == { # \\"file1.txt\\": \\"Hello World!\\", # \\"file2.txt\\": \\"This is a test.\\", # \\"file3.log\\": \\"Logging information...\\" # } ``` # Notes: - Make use of context managers to handle file and directory creation to ensure they are properly cleaned up after use. - Test your function with files of varying sizes to ensure that memory spooling is correctly implemented for smaller files.","solution":"from typing import Dict import tempfile import os def manage_temp_files(file_contents: Dict[str, str]) -> Dict[str, str]: Manages temporary files by creating a temporary directory, writing given contents to files, reading the contents back, and returning the contents. Uses memory spooling for files smaller than 1 KB to optimize performance. Args: - file_contents (Dict[str, str]): A dictionary with filenames as keys and their contents as values. Returns: - Dict[str, str]: A dictionary with filenames as keys and the read contents as values. result = {} with tempfile.TemporaryDirectory() as temp_dir: for filename, content in file_contents.items(): file_path = os.path.join(temp_dir, filename) if len(content) < 1024: # Use SpooledTemporaryFile for small files with tempfile.SpooledTemporaryFile(max_size=1024, mode=\'w+t\') as temp_file: temp_file.write(content) temp_file.seek(0) result[filename] = temp_file.read() else: # Use regular TemporaryFile for larger files with open(file_path, \'w\') as temp_file: temp_file.write(content) with open(file_path, \'r\') as temp_file: result[filename] = temp_file.read() return result"},{"question":"**Advanced Python Coding Challenge: Manipulating and Analyzing Abstract Syntax Trees** # Problem Statement You are given a string containing Python source code. Your task is to write a function that uses the `ast` (Abstract Syntax Trees) module to parse this code, identify all the function definitions, and count the number of function arguments for each function. # Function Signature ```python def count_function_args(source_code: str) -> dict: Takes a string of Python source code, parses it, and returns a dictionary where the keys are function names and the values are the number of arguments each function takes. :param source_code: A string containing Python source code :return: A dictionary with function names as keys and the number of arguments as values ``` # Input - `source_code`: A string that contains valid Python source code. # Output - Return a dictionary where keys are function names (as strings) and values are integers representing the number of arguments each function takes. # Constraints - The source code string will contain valid Python 3.10+ code. - Assume that all function names are unique within the given source code. # Example ```python source_code = \'\'\' def foo(a, b, c): pass def bar(x, y): pass \'\'\' # Function call result = count_function_args(source_code) # Expected output { \\"foo\\": 3, \\"bar\\": 2 } ``` # Guidelines 1. Use the `ast` module to parse the source code into an Abstract Syntax Tree. 2. Traverse the syntax tree to find all function definitions (`ast.FunctionDef` nodes). 3. Extract the function names and count the number of arguments (consider both positional and default arguments). 4. Return the results in the specified dictionary format. # Advanced Considerations - Your solution should handle nested functions correctly. - Handle edge cases where functions might have no arguments. - Aim for clear and efficient code while leveraging the capabilities of the `ast` module. Good luck!","solution":"import ast def count_function_args(source_code: str) -> dict: Takes a string of Python source code, parses it, and returns a dictionary where the keys are function names and the values are the number of arguments each function takes. :param source_code: A string containing Python source code :return: A dictionary with function names as keys and the number of arguments as values tree = ast.parse(source_code) function_args_count = {} for node in ast.walk(tree): if isinstance(node, ast.FunctionDef): function_name = node.name num_args = len(node.args.args) function_args_count[function_name] = num_args return function_args_count"},{"question":"# Advanced asyncio Synchronization You are tasked with implementing an advanced asyncio-based system that models a simple producer-consumer scenario. In this scenario, producers add items to a shared buffer, and consumers remove items from the buffer. The buffer has a fixed capacity, and synchronization primitives from the asyncio library must be used to ensure safe concurrent access to the buffer. Requirements: 1. **Buffer:** A shared resource with a fixed capacity. 2. **Producers:** Tasks that add items to the buffer. They must wait if the buffer is full. 3. **Consumers:** Tasks that remove items from the buffer. They must wait if the buffer is empty. 4. **Using asyncio primitives:** Utilize `asyncio.Lock`, `asyncio.Condition`, or `asyncio.Semaphore` to manage access control. Task: Implement the following: 1. **AsyncBuffer Class:** - `__init__(self, capacity: int)`: Initializes the buffer with the specified capacity. - `async def produce(self, item: Any)`: Adds an item to the buffer. Waits if buffer is full. - `async def consume(self) -> Any`: Removes and returns an item from the buffer. Waits if buffer is empty. 2. **Example usage:** ```python import asyncio import random class AsyncBuffer: def __init__(self, capacity: int): self.capacity = capacity self.buffer = [] self.lock = asyncio.Lock() self.not_full = asyncio.Condition(self.lock) self.not_empty = asyncio.Condition(self.lock) async def produce(self, item): async with self.not_full: while len(self.buffer) >= self.capacity: await self.not_full.wait() async with self.lock: self.buffer.append(item) self.not_empty.notify() async def consume(self): async with self.not_empty: while not self.buffer: await self.not_empty.wait() async with self.lock: item = self.buffer.pop(0) self.not_full.notify() return item async def producer(buffer: AsyncBuffer, n: int): for _ in range(n): item = random.randint(1, 100) await buffer.produce(item) print(f\'Produced: {item}\') await asyncio.sleep(random.random()) async def consumer(buffer: AsyncBuffer, n: int): for _ in range(n): item = await buffer.consume() print(f\'Consumed: {item}\') await asyncio.sleep(random.random()) async def main(): buffer = AsyncBuffer(10) producers = [asyncio.create_task(producer(buffer, 20)) for _ in range(2)] consumers = [asyncio.create_task(consumer(buffer, 20)) for _ in range(2)] await asyncio.gather(*producers) await asyncio.gather(*consumers) asyncio.run(main()) ``` Constraints: - The buffer capacity must be a positive integer. - The number of items produced and consumed should be positive integers. - Prioritize efficiency and correctness of the synchronization logic. **Notes**: - The example usage provided is just a starting point. Feel free to enhance the solution for better performance or additional features. - You are encouraged to handle potential edge cases such as buffer overflows or underflows gracefully.","solution":"import asyncio from typing import Any class AsyncBuffer: def __init__(self, capacity: int): if capacity <= 0: raise ValueError(\\"Capacity must be a positive integer.\\") self.capacity = capacity self.buffer = [] self.lock = asyncio.Lock() self.not_full = asyncio.Condition(self.lock) self.not_empty = asyncio.Condition(self.lock) async def produce(self, item: Any): async with self.not_full: while len(self.buffer) >= self.capacity: await self.not_full.wait() self.buffer.append(item) self.not_empty.notify() async def consume(self) -> Any: async with self.not_empty: while not self.buffer: await self.not_empty.wait() item = self.buffer.pop(0) self.not_full.notify() return item async def producer(buffer: AsyncBuffer, n: int): for _ in range(n): item = random.randint(1, 100) await buffer.produce(item) print(f\'Produced: {item}\') await asyncio.sleep(random.random()) async def consumer(buffer: AsyncBuffer, n: int): for _ in range(n): item = await buffer.consume() print(f\'Consumed: {item}\') await asyncio.sleep(random.random()) async def main(): buffer = AsyncBuffer(10) producers = [asyncio.create_task(producer(buffer, 20)) for _ in range(2)] consumers = [asyncio.create_task(consumer(buffer, 20)) for _ in range(2)] await asyncio.gather(*producers) await asyncio.gather(*consumers) if __name__ == \\"__main__\\": import random asyncio.run(main())"},{"question":"# **Styling and Exporting a DataFrame using Pandas** Objective You are required to demonstrate your ability to style a pandas DataFrame using the `Styler` functionalities and export the styled DataFrame to an HTML file. Task 1. **Data Loading**: - Create a pandas DataFrame with the following data: ```python data = { \'A\': [1, 2, 6], \'B\': [4, 5, 6], \'C\': [7, 8, 9], \'D\': [10, 11, 12] } df = pd.DataFrame(data) ``` 2. **Style Application**: - Highlight the maximum values in each column. - Apply a background gradient to the DataFrame. - Set a table caption that says \\"Styled DataFrame Example\\". 3. **Export to HTML**: - Export the styled DataFrame to an HTML file named `styled_dataframe.html`. 4. **Verification**: - Ensure that the exported HTML file contains the applied styles and the caption. Requirements - Your code should be contained within a function named `style_and_export_dataframe()`. - The function should not take any input arguments and should save the styled DataFrame to `styled_dataframe.html`. - Make sure to import the necessary pandas package at the beginning of your code. ```python def style_and_export_dataframe(): # Your code here style_and_export_dataframe() ``` Constraints - Use only pandas built-in styling functions. - Your solution should handle the data precisely as given above. - The `styled_dataframe.html` file should be created in the same directory as your script. Example Output The function should create an HTML file (`styled_dataframe.html`) that correctly displays the DataFrame with the applied styles.","solution":"import pandas as pd def style_and_export_dataframe(): # Create the DataFrame data = { \'A\': [1, 2, 6], \'B\': [4, 5, 6], \'C\': [7, 8, 9], \'D\': [10, 11, 12] } df = pd.DataFrame(data) # Apply styles to the DataFrame styled_df = (df.style .highlight_max(axis=0) .background_gradient() .set_caption(\\"Styled DataFrame Example\\")) # Export the styled DataFrame to an HTML file styled_df.to_html(\\"styled_dataframe.html\\") # Call the function style_and_export_dataframe()"},{"question":"Objective Implement a Python class that leverages the buffer protocol to read and write data from/to a memory buffer efficiently. Task Write a class `MemoryBufferManager` that creates and manages a buffer using the `bytearray` type. The class should provide methods to: 1. Initialize the buffer with a given size. 2. Write data to the buffer at a specified position. 3. Read data from the buffer at a specified position. 4. Check if the buffer conforms to a specific format and contiguity. Class Definition and Methods ```python class MemoryBufferManager: def __init__(self, size: int): Initialize the buffer with the given size. :param size: The size of the buffer to allocate. pass def write_data(self, data: bytes, position: int): Write data to the buffer at the specified starting position. :param data: The data to write to the buffer. :param position: The position in the buffer to start writing. :raises ValueError: If the position is out of bounds or data exceeds buffer size. pass def read_data(self, size: int, position: int) -> bytes: Read data from the buffer starting at the specified position. :param size: The number of bytes to read from the buffer. :param position: The position in the buffer to start reading. :raises ValueError: If the position is out of bounds or read exceeds buffer size. :return: The data read from the buffer. pass def is_contiguous(self, c_or_f: str) -> bool: Check if the buffer is contiguous according to the specified order. :param c_or_f: \'C\' for C-contiguous, \'F\' for Fortran-contiguous. :return: True if the buffer is contiguous in the specified order, False otherwise. pass def get_buffer_info(self) -> str: Get a string representation of the buffer\'s attributes. :return: A string detailing the buffer\'s attributes such as length, readonly status, etc. pass ``` Constraints - The buffer size must be a positive integer. - Writing or reading data should not exceed the bounds of the buffer. - `is_contiguous` method should handle both \'C\' and \'F\' contiguity checks. - The buffer is initially empty (all zero bytes). Example Usage ```python # Initialize buffer with size 10 buffer_manager = MemoryBufferManager(10) # Write some data to the buffer buffer_manager.write_data(b\'hello\', 2) # Read data from the buffer print(buffer_manager.read_data(5, 2)) # Output: b\'hello\' # Check if the buffer is C-contiguous print(buffer_manager.is_contiguous(\'C\')) # Output: True # Get buffer information print(buffer_manager.get_buffer_info()) # Output: Attributes of the buffer ``` Testing Ensure your implementation handles edge cases, such as: - Writing data that exceeds the buffer size. - Reading data from an invalid position. - Checking contiguity for different orders. - Properly managing and releasing buffer resources to avoid memory leaks.","solution":"class MemoryBufferManager: def __init__(self, size: int): Initialize the buffer with the given size. :param size: The size of the buffer to allocate. if size <= 0: raise ValueError(\\"Size must be a positive integer.\\") self.buffer = bytearray(size) self.size = size def write_data(self, data: bytes, position: int): Write data to the buffer at the specified starting position. :param data: The data to write to the buffer. :param position: The position in the buffer to start writing. :raises ValueError: If the position is out of bounds or data exceeds buffer size. if position < 0 or position >= self.size: raise ValueError(f\\"Position {position} is out of bounds.\\") if position + len(data) > self.size: raise ValueError(\\"Data exceeds buffer size.\\") self.buffer[position:position + len(data)] = data def read_data(self, size: int, position: int) -> bytes: Read data from the buffer starting at the specified position. :param size: The number of bytes to read from the buffer. :param position: The position in the buffer to start reading. :raises ValueError: If the position is out of bounds or read exceeds buffer size. :return: The data read from the buffer. if position < 0 or position >= self.size: raise ValueError(f\\"Position {position} is out of bounds.\\") if position + size > self.size: raise ValueError(\\"Read exceeds buffer size.\\") return bytes(self.buffer[position:position + size]) def is_contiguous(self, c_or_f: str) -> bool: Check if the buffer is contiguous according to the specified order. :param c_or_f: \'C\' for C-contiguous, \'F\' for Fortran-contiguous. :return: True if the buffer is contiguous in the specified order, False otherwise. # For a bytearray, memory is always C-contiguous. if c_or_f == \'C\': return True elif c_or_f == \'F\': # Bytearrays are inherently not Fortran-contiguous return False else: raise ValueError(\\"Invalid contiguity type. Use \'C\' or \'F\'.\\") def get_buffer_info(self) -> str: Get a string representation of the buffer\'s attributes. :return: A string detailing the buffer\'s attributes such as length, readonly status, etc. info = ( f\\"Buffer length: {self.size}n\\" f\\"Buffer readonly: {self.buffer.readonly if hasattr(self.buffer, \'readonly\') else False}n\\" f\\"Buffer content: {list(self.buffer)}\\" ) return info"},{"question":"# Understanding Seaborn Heatmaps You are provided with a dataset `students_scores` which consists of students\' scores in different subjects. Your task is to visualize this data using a seaborn heatmap with specific customizations. Dataset `students_scores` (as pandas DataFrame) | Student | Math | English | Science | History | Geography | |---------|------|---------|---------|---------|------------| | John | 88 | 92 | 85 | 90 | 93 | | Emily | 75 | 80 | 89 | 85 | 87 | | Tom | 93 | 88 | 91 | 79 | 81 | | Lisa | 89 | 94 | 90 | 76 | 80 | | Mark | 76 | 85 | 79 | 88 | 86 | Requirements 1. **Heatmap Visualization**: - Create a heatmap to visualize these scores. - Annotate the heatmap cells with the scores. - Use a different colormap called `magma`. - Add lines between the cells with a linewidth of 0.5. - Set the color intensity limits to `vmin=70` and `vmax=100`. 2. **Customization**: - Use a separate dataframe to annotate the cells with rankings (rank from 1 to 5 with axis=0). - Remove the x and y axis labels. - Move the x-axis ticks to the top. Input - A pandas DataFrame named `students_scores` that contains students\' scores. Output - A heatmap plot visualizing the students\' scores with the specified customization. Constraints - The input DataFrame `students_scores` will have at least 5 rows and 5 columns. - All scores will be integers between 0 and 100. # Implementation Implement the function `plot_student_scores_heatmap(students_scores)` that performs the above requirements. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_student_scores_heatmap(students_scores): # Write your code below pass # Sample usage students_scores = pd.DataFrame({ \'Student\': [\'John\', \'Emily\', \'Tom\', \'Lisa\', \'Mark\'], \'Math\': [88, 75, 93, 89, 76], \'English\': [92, 80, 88, 94, 85], \'Science\': [85, 89, 91, 90, 79], \'History\': [90, 85, 79, 76, 88], \'Geography\': [93, 87, 81, 80, 86] }).set_index(\'Student\') plot_student_scores_heatmap(students_scores) ``` **Note**: This question is designed to ensure that students are familiar not just with creating heatmaps using seaborn but also with customizing these plots by modifying annotation styles, adjusting color maps and data limits, and applying final tweaks with matplotlib axes methods.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_student_scores_heatmap(students_scores): # Annotate the heatmap cells with the actual scores annotations = students_scores.astype(str) # Create the heatmap plt.figure(figsize=(10, 8)) ax = sns.heatmap(students_scores, annot=annotations, fmt=\'\', cmap=\'magma\', linewidths=0.5, vmin=70, vmax=100, cbar_kws={\'label\': \'Scores\'}) # Remove axis labels ax.set_xlabel(\'\') ax.set_ylabel(\'\') # Move x-axis ticks to the top ax.xaxis.set_ticks_position(\'top\') plt.title(\'Students Scores Heatmap\') plt.show() # Sample usage students_scores = pd.DataFrame({ \'Student\': [\'John\', \'Emily\', \'Tom\', \'Lisa\', \'Mark\'], \'Math\': [88, 75, 93, 89, 76], \'English\': [92, 80, 88, 94, 85], \'Science\': [85, 89, 91, 90, 79], \'History\': [90, 85, 79, 76, 88], \'Geography\': [93, 87, 81, 80, 86] }).set_index(\'Student\') plot_student_scores_heatmap(students_scores)"},{"question":"**Question: Comprehensive Network Analysis Tool** You have been tasked with developing a tool to analyze IP ranges and addresses for network configurations. Your tool should be capable of the following: 1. Parse a list of IP addresses and networks and categorize them as IPv4 or IPv6. 2. For each network, calculate the number of usable host addresses. 3. For each IP address, determine if it falls within any of the provided networks. 4. Return a summary report of your findings. **Input:** - `ip_list`: A list of strings, where each string represents an IP address or network. The list can contain both IPv4 and IPv6 addresses/networks. For example: `[\\"192.0.2.1\\", \\"2001:db8::1\\", \\"192.0.2.0/24\\", \\"2001:db8::/96\\"]` **Output:** - A dictionary with the following structure: ```python { \\"IPv4\\": { \\"addresses\\": [list of IPv4 addresses], \\"networks\\": { \\"network/prefix\\": { \\"num_addresses\\": int (total number of addresses in the network), \\"num_usable_addresses\\": int (excluding network and broadcast addresses) }, ... } }, \\"IPv6\\": { \\"addresses\\": [list of IPv6 addresses], \\"networks\\": { \\"network/prefix\\": { \\"num_addresses\\": int (total number of addresses in the network), \\"num_usable_addresses\\": int }, ... } }, \\"address_in_network\\": { \\"address\\": [list of networks the address falls within], ... } } ``` **Function Signature:** ```python def analyze_ip_addresses(ip_list: list) -> dict: pass ``` **Constraints and Notes:** - When counting usable addresses for IPv4 networks, exclude the network and broadcast addresses. Assume no exclusion for IPv6 networks. - The input list can have both valid and invalid IP addresses/networks. Your code should handle errors gracefully and skip invalid entries. - Performance optimizations are not the primary focus but consider edge cases like large networks. **Example:** ```python input_ips = [\\"192.0.2.1\\", \\"2001:db8::1\\", \\"192.0.2.0/24\\", \\"2001:db8::/96\\", \\"999.999.999.999\\"] output = analyze_ip_addresses(input_ips) expected_output = { \\"IPv4\\": { \\"addresses\\": [\\"192.0.2.1\\"], \\"networks\\": { \\"192.0.2.0/24\\": { \\"num_addresses\\": 256, \\"num_usable_addresses\\": 254 } } }, \\"IPv6\\": { \\"addresses\\": [\\"2001:db8::1\\"], \\"networks\\": { \\"2001:db8::/96\\": { \\"num_addresses\\": 4294967296, \\"num_usable_addresses\\": 4294967296 } } }, \\"address_in_network\\": { \\"192.0.2.1\\": [\\"192.0.2.0/24\\"], \\"2001:db8::1\\": [\\"2001:db8::/96\\"] } } ``` Write code to implement the function `analyze_ip_addresses` which adheres to the above specifications.","solution":"import ipaddress def analyze_ip_addresses(ip_list): result = { \\"IPv4\\": { \\"addresses\\": [], \\"networks\\": {} }, \\"IPv6\\": { \\"addresses\\": [], \\"networks\\": {} }, \\"address_in_network\\": {} } addresses = [] networks = [] for entry in ip_list: try: if \'/\' in entry: # network net = ipaddress.ip_network(entry, strict=False) networks.append(net) else: # single address addr = ipaddress.ip_address(entry) addresses.append(addr) except ValueError: continue # skip invalid entries for addr in addresses: if isinstance(addr, ipaddress.IPv4Address): category = \\"IPv4\\" else: category = \\"IPv6\\" result[category][\\"addresses\\"].append(str(addr)) result[\\"address_in_network\\"][str(addr)] = [] for net in networks: if addr in net: result[\\"address_in_network\\"][str(addr)].append(str(net)) for net in networks: if isinstance(net, ipaddress.IPv4Network): category = \\"IPv4\\" else: category = \\"IPv6\\" total_addresses = net.num_addresses if category == \\"IPv4\\" and total_addresses > 2: usable_addresses = total_addresses - 2 else: usable_addresses = total_addresses result[category][\\"networks\\"][str(net)] = { \\"num_addresses\\": total_addresses, \\"num_usable_addresses\\": usable_addresses } return result"},{"question":"Objective To assess your understanding of kernel density estimation using scikit-learn\'s `KernelDensity` class and your ability to apply it to real-world data. Question You are given a dataset consisting of one-dimensional data points sampled from an unknown distribution. Your task is to implement a function that: 1. Fits a kernel density estimator to the data. 2. Visualizes the original data points as a histogram. 3. Overlays the kernel density estimates on the histogram using different kernels and bandwidths. Function Signature ```python def visualize_kde(data: np.ndarray, bandwidths: list, kernels: list): Fits a kernel density estimator to the data, and visualizes both the histogram of data and KDEs. Parameters: data (np.ndarray): A 1D array of data points. bandwidths (list): A list of bandwidths to be used for KDE. kernels (list): A list of kernels to be used for KDE. Returns: None: The function should display the plot. pass ``` Input - **data**: A 1D numpy array of data points. - **bandwidths**: A list of bandwidth values (floats) to be used for the kernel density estimation. - **kernels**: A list of kernel names (strings) to be used for the kernel density estimation. (Valid kernel names include: \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\') Output - The function should display a plot with the histogram of the data points and the KDE curves for each combination of bandwidth and kernel. Constraints - Ensure the plot is clearly labeled with different colors and legend entries for each combination of bandwidth and kernel. - The kernel density estimates should be normalized. Example Usage ```python import numpy as np import matplotlib.pyplot as plt data = np.random.normal(0, 1, 1000) # Generate some random data bandwidths = [0.1, 0.5, 1.0] kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] visualize_kde(data, bandwidths, kernels) ``` Notes - You may use matplotlib for plotting. - The example usage is for illustration purposes only. Your implementation should be general and work with any provided inputs.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def visualize_kde(data: np.ndarray, bandwidths: list, kernels: list): Fits a kernel density estimator to the data, and visualizes both the histogram of data and KDEs. Parameters: data (np.ndarray): A 1D array of data points. bandwidths (list): A list of bandwidths to be used for KDE. kernels (list): A list of kernels to be used for KDE. Returns: None: The function should display the plot. # Create a range of values for plotting the KDE x_d = np.linspace(min(data), max(data), 1000) # Plot the histogram of the data points plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', label=\'Histogram\') # Fit and plot the KDE for each combination of bandwidth and kernel for bandwidth in bandwidths: for kernel in kernels: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data[:, np.newaxis]) log_dens = kde.score_samples(x_d[:, np.newaxis]) plt.plot(x_d, np.exp(log_dens), label=f\'KDE: Kernel={kernel}, Bandwidth={bandwidth}\') # Add legend and labels plt.legend() plt.xlabel(\'Data points\') plt.ylabel(\'Density\') plt.title(\'Kernel Density Estimates\') # Show the plot plt.show()"},{"question":"You are given a script that processes files based on command-line inputs. Your task is to utilize the deprecated `optparse` module in Python to handle various command-line options and arguments. Implement the following functionalities: 1. **Option Definitions**: The script should accept the following options: - `-i`, `--input`: Specifies the input file. This option is mandatory. - `-o`, `--output`: Specifies the output file. If not provided, use a default output file `output.txt`. - `-v`, `--verbose`: A flag that, if set, should enable verbose mode, printing additional details during processing. - `-m`, `--mode`: Specifies the mode of operation. It should accept the values `basic`, `intermediate`, or `advanced`. The default mode should be `basic`. 2. **Command-Line Parsing**: Parse the provided command-line arguments. Ensure that the appropriate error messages are displayed if the required arguments are missing or if the provided values do not match the expected types or choices. 3. **Help Message**: Automatically generate a help message that describes the usage of the script and its options. The help message should be correctly formatted and user-friendly. 4. **Main Functionality**: Implement a simple main function that: - Prints the parsed options and positional arguments. - If the verbose flag is set, additionally prints \\"Verbose mode enabled.\\" ```python from optparse import OptionParser def main(): usage = \\"usage: %prog [options] arg1 arg2\\" parser = OptionParser(usage=usage) # Add your options here (options, args) = parser.parse_args() # Ensure the input file option is provided if not options.input: parser.error(\\"Input file is required (-i or --input)\\") # Print the options and arguments, and handle verbose mode if __name__ == \\"__main__\\": main() ``` **Constraints**: - Do not use any external libraries apart from `optparse`. - Ensure that the script handles different ways of passing options (`-i input.txt`, `--input input.txt`, `-iinput.txt`) correctly. - The code should be well-organized and maintainable. **Sample Execution**: 1. `script.py -i input.txt -o output.txt -v --mode advanced` 2. `script.py -i input.txt arg1 arg2` (`output.txt` should be the default output file, and the mode should be `basic`) **Expected Output**: The expected output should display the parsed options and arguments, for instance: ``` Options: {\'input\': \'input.txt\', \'output\': \'output.txt\', \'verbose\': True, \'mode\': \'advanced\'} Arguments: [\'arg1\', \'arg2\'] Verbose mode enabled. ```","solution":"from optparse import OptionParser def main(): usage = \\"usage: %prog [options] arg1 arg2\\" parser = OptionParser(usage=usage) parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input\\", help=\\"specifies the input file\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output\\", default=\\"output.txt\\", help=\\"specifies the output file (default: output.txt)\\", metavar=\\"FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"enable verbose mode\\") parser.add_option(\\"-m\\", \\"--mode\\", dest=\\"mode\\", default=\\"basic\\", type=\\"choice\\", choices=[\\"basic\\", \\"intermediate\\", \\"advanced\\"], help=\\"specifies the mode of operation (choices: basic, intermediate, advanced)\\") (options, args) = parser.parse_args() # Ensure the input file option is provided if not options.input: parser.error(\\"Input file is required (-i or --input)\\") # Print the options and arguments, and handle verbose mode print(f\\"Options: {{\'input\': \'{options.input}\', \'output\': \'{options.output}\', \'verbose\': {options.verbose}, \'mode\': \'{options.mode}\'}}\\") print(f\\"Arguments: {args}\\") if options.verbose: print(\\"Verbose mode enabled.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question: Pandas Index Manipulation** # Objective The aim of this task is to assess your ability to create and manipulate pandas Index and MultiIndex objects. You will work with a MultiIndex DataFrame and perform various operations to demonstrate your understanding of pandas functionalities. # Problem Statement You are provided with sales data for different products across various regions over several quarters. Your task is to create a MultiIndex DataFrame and then manipulate it according to the given instructions. # Input 1. Lists of regions, products, and quarters: - `regions` = [\'North\', \'South\', \'East\', \'West\'] - `products` = [\'Product_A\', \'Product_B\', \'Product_C\'] - `quarters` = [\'Q1\', \'Q2\', \'Q3\', \'Q4\'] 2. A dictionary containing sales data for each product in each region for all the quarters: ```python sales_data = { \'North\': {\'Product_A\': [120, 210, 315, 450], \'Product_B\': [230, 410, 305, 260], \'Product_C\': [340, 215, 190, 380]}, \'South\': {\'Product_A\': [130, 220, 330, 460], \'Product_B\': [240, 420, 310, 270], \'Product_C\': [350, 225, 200, 390]}, \'East\': {\'Product_A\': [140, 230, 345, 470], \'Product_B\': [250, 430, 315, 280], \'Product_C\': [360, 235, 210, 400]}, \'West\': {\'Product_A\': [150, 240, 360, 480], \'Product_B\': [260, 440, 320, 290], \'Product_C\': [370, 245, 220, 410]} } ``` # Requirements 1. **Create a MultiIndex DataFrame:** Construct a DataFrame `df` with MultiIndex from `regions` and `products`, indexing rows by regions and products, and columns by quarters. Populate `df` with the given `sales_data`. 2. **Index and MultiIndex Operations:** - Demonstrate the use of at least five different attributes or methods from the pandas `Index` and `MultiIndex` classes by modifying `df` or extracting information from it. Examples include checking for uniqueness, extracting levels, and reordering. 3. **Manipulating data:** - Insert a new product \'Product_D\' for the \'North\' region with sales data `[110, 220, 330, 440]`. - Drop all data for the \'West\' region. - Rename \'Product_C\' to \'Product_C_Updated\' in the entire DataFrame. # Output The final MultiIndex DataFrame after all manipulations, named `final_df`. # Constraints - Ensure the DataFrame remains well-structured and retains the correct MultiIndex formatting after each operation. - Minimize the use of loops; utilize pandas built-in methods and functionalities wherever possible. **Example Output:** ```python # A snippet of what the final DataFrame `final_df` might look like print(final_df) # Output: # Q1 Q2 Q3 Q4 # region product # North Product_A 120 210 315 450 # Product_B 230 410 305 260 # Product_C_Updated 340 215 190 380 # Product_D 110 220 330 440 # South Product_A 130 220 330 460 # Product_B 240 420 310 270 # Product_C_Updated 350 225 200 390 # East Product_A 140 230 345 470 # Product_B 250 430 315 280 # Product_C_Updated 360 235 210 400 ``` # Performance Requirement Ensure the implementation is efficient and leverages pandas functionalities optimally without unnecessary iterations.","solution":"import pandas as pd # Given data regions = [\'North\', \'South\', \'East\', \'West\'] products = [\'Product_A\', \'Product_B\', \'Product_C\'] quarters = [\'Q1\', \'Q2\', \'Q3\', \'Q4\'] sales_data = { \'North\': {\'Product_A\': [120, 210, 315, 450], \'Product_B\': [230, 410, 305, 260], \'Product_C\': [340, 215, 190, 380]}, \'South\': {\'Product_A\': [130, 220, 330, 460], \'Product_B\': [240, 420, 310, 270], \'Product_C\': [350, 225, 200, 390]}, \'East\': {\'Product_A\': [140, 230, 345, 470], \'Product_B\': [250, 430, 315, 280], \'Product_C\': [360, 235, 210, 400]}, \'West\': {\'Product_A\': [150, 240, 360, 480], \'Product_B\': [260, 440, 320, 290], \'Product_C\': [370, 245, 220, 410]} } # Creating MultiIndex DataFrame index = pd.MultiIndex.from_product([regions, products], names=[\'region\', \'product\']) df = pd.DataFrame(index=index, columns=quarters) # Populate the DataFrame with given sales data for region in regions: for product in products: df.loc[(region, product), :] = sales_data[region][product] # Demonstrating different MultiIndex operations # Show uniqueness of the index index_unique = df.index.is_unique # Extracting levels of the index levels = df.index.levels region_levels = levels[0] product_levels = levels[1] # Reordering the levels df = df.reorder_levels([\'product\', \'region\']) # Inserting a new product \'Product_D\' for the \'North\' region new_data = {\'North\': {\'Product_D\': [110, 220, 330, 440]}} for region in new_data: for product, sales in new_data[region].items(): df.loc[(product, region), :] = sales # Dropping all data for the \'West\' region df = df.drop(\'West\', level=\'region\') # Renaming \'Product_C\' to \'Product_C_Updated\' df = df.rename(index={\'Product_C\': \'Product_C_Updated\'}) # Sort index for final presentation df = df.sort_index() final_df = df"},{"question":"Objective: You are required to create a Python script using the `random` module to simulate drawing balls from a bag. The bag contains balls of different colors, and you will perform various random operations to demonstrate your understanding of the module\'s functionalities. Task: 1. **Define a function `initialize_bag` that takes a dictionary of ball counts and returns a list representing the bag:** ```python def initialize_bag(ball_counts: dict) -> list: Initialize the bag of balls. Parameters: ball_counts (dict): A dictionary where keys are colors (str) and values are counts (int) of balls. Returns: list: A list where each element is a ball (represented by its color as a string) based on the input counts. pass ``` 2. **Define a function `draw_balls` that simulates drawing `n` balls from the bag without replacement:** ```python def draw_balls(bag: list, n: int) -> list: Draw n balls from the bag without replacement. Parameters: bag (list): The list representing the bag of balls. n (int): The number of balls to draw. Returns: list: A list of drawn balls. pass ``` 3. **Define a function `shuffle_bag` that shuffles the balls in the bag:** ```python def shuffle_bag(bag: list) -> None: Shuffle the balls in the bag. Parameters: bag (list): The list representing the bag of balls. Returns: None pass ``` 4. **Define a function `generate_statistics` that calculates the statistics for multiple draws:** ```python def generate_statistics(bag: list, n: int, draws: int) -> dict: Generate statistics for multiple draws. Parameters: bag (list): The list representing the bag of balls. n (int): The number of balls to draw per draw. draws (int): The number of draws to perform. Returns: dict: A dictionary where keys are colors and values are the average number of times each color is drawn. pass ``` 5. **Write a script that:** - Initializes a bag with 5 red, 3 blue, and 2 green balls. - Shuffles the bag. - Draws 4 balls from the bag without replacement. - Prints the drawn balls. - Generates statistics for 1000 draws of 4 balls each and prints the average counts of each color. 6. **Constraints:** - Each color in the bag has at least one ball. - The number of balls to draw in one go cannot exceed the total number of balls in the bag. Example: ```python if __name__ == \\"__main__\\": ball_counts = {\'red\': 5, \'blue\': 3, \'green\': 2} bag = initialize_bag(ball_counts) shuffle_bag(bag) drawn_balls = draw_balls(bag, 4) print(f\\"Drawn Balls: {drawn_balls}\\") statistics = generate_statistics(bag, 4, 1000) print(f\\"Average Draws: {statistics}\\") ``` Expected Output: ``` Drawn Balls: [\'red\', \'green\', \'blue\', \'red\'] Average Draws: {\'red\': 2.1, \'blue\': 1.0, \'green\': 0.9} ``` Good luck!","solution":"import random def initialize_bag(ball_counts: dict) -> list: Initialize the bag of balls. Parameters: ball_counts (dict): A dictionary where keys are colors (str) and values are counts (int) of balls. Returns: list: A list where each element is a ball (represented by its color as a string) based on the input counts. bag = [] for color, count in ball_counts.items(): bag.extend([color] * count) return bag def draw_balls(bag: list, n: int) -> list: Draw n balls from the bag without replacement. Parameters: bag (list): The list representing the bag of balls. n (int): The number of balls to draw. Returns: list: A list of drawn balls. drawn_balls = random.sample(bag, n) for ball in drawn_balls: bag.remove(ball) return drawn_balls def shuffle_bag(bag: list) -> None: Shuffle the balls in the bag. Parameters: bag (list): The list representing the bag of balls. Returns: None random.shuffle(bag) def generate_statistics(bag: list, n: int, draws: int) -> dict: Generate statistics for multiple draws. Parameters: bag (list): The list representing the bag of balls. n (int): The number of balls to draw per draw. draws (int): The number of draws to perform. Returns: dict: A dictionary where keys are colors and values are the average number of times each color is drawn. color_counts = {color: 0 for color in set(bag)} total_balls = len(bag) for _ in range(draws): temp_bag = list(bag) # Create a copy of the bag for each draw drawn_balls = draw_balls(temp_bag, n) for ball in drawn_balls: color_counts[ball] += 1 # Calculate averages average_counts = {color: count/draws for color, count in color_counts.items()} return average_counts if __name__ == \\"__main__\\": ball_counts = {\'red\': 5, \'blue\': 3, \'green\': 2} bag = initialize_bag(ball_counts) shuffle_bag(bag) drawn_balls = draw_balls(bag, 4) print(f\\"Drawn Balls: {drawn_balls}\\") statistics = generate_statistics(bag, 4, 1000) print(f\\"Average Draws: {statistics}\\")"},{"question":"# Problem: Data Processing Pipeline **Objective:** Implement a robust data processing pipeline which handles various stages of processing and is capable of managing exceptions, context handling, and asynchronous operations. **Problem Statement:** You are responsible for processing a large dataset from various sources. The processing involves multiple stages, some of which require synchronous operations, exception handling, and resource management, while others benefit from asynchronous operations for better performance. Your task is to implement a `Pipeline` class that processes data through the following steps: 1. **Loading Data**: Load the data from a file. Ensure resource management using the `with` statement. 2. **Processing Data**: Perform various data transformations and calculations. Use appropriate control flow statements to manage the logic. 3. **Saving Data**: Save the processed data back to a file. Ensure resource management. 4. **Reporting**: Provide a summary report of the processing. Make sure to handle any exceptions that occur during the process and log them. Specifications: - **Input:** - A file path (string) from which data is to be loaded. - A file path (string) to where the processed data should be saved. - Data is assumed to be in a simple format, e.g., list of integers, one per line. - **Output:** - A file with processed data saved to the specified file. - A summary of the process printed to the console including: - Number of items processed. - Any errors encountered during processing. Requirements: 1. Implement data loading and saving using `with` statements for context management. 2. Use `try-except-else-finally` blocks to handle potential errors during data loading, processing, and saving. 3. Implement one of the processing stages using asynchronous programming to showcase your understanding of `async` functions and `await` expressions. 4. Provide meaningful summaries and error logs to the console during and after the process. Constraints: - The file to be processed might be very large, handle it efficiently. - Ensure the program is robust and can handle unexpected input gracefully. Example: Given the input file `input.txt` contains: ``` 1 2 3 4 5 ``` After processing: - Save the processed data to `output.txt` - Print: ``` Data processing summary: - Items processed: 5 - Errors encountered: 0 ``` Note: You are required to demonstrate the use of compound statements, proper context management, exception handling, and asynchronous programming in Python. **Code Template**: ```python import asyncio class Pipeline: def __init__(self, input_path, output_path): self.input_path = input_path self.output_path = output_path self.errors = [] def load_data(self): try: with open(self.input_path, \'r\') as file: data = [int(line.strip()) for line in file] return data except Exception as e: self.errors.append(f\\"Error loading data: {e}\\") async def process_data(self, data): try: transformed_data = await self.async_transform(data) return transformed_data except Exception as e: self.errors.append(f\\"Error processing data: {e}\\") async def async_transform(self, data): await asyncio.sleep(1) # Simulate async operation return [x * x for x in data] def save_data(self, data): try: with open(self.output_path, \'w\') as file: for item in data: file.write(f\\"{item}n\\") except Exception as e: self.errors.append(f\\"Error saving data: {e}\\") def report(self, data): print(\\"Data processing summary:\\") print(f\\"- Items processed: {len(data)}\\") print(f\\"- Errors encountered: {len(self.errors)}\\") for error in self.errors: print(f\\" - {error}\\") async def run(self): data = self.load_data() if data: processed_data = await self.process_data(data) if processed_data: self.save_data(processed_data) self.report(data) if __name__ == \\"__main__\\": input_path = \'input.txt\' output_path = \'output.txt\' pipeline = Pipeline(input_path, output_path) asyncio.run(pipeline.run()) ``` **Explanation:** - The `Pipeline` class handles the entire data processing pipeline. - Methods `load_data` and `save_data` use `with` statements for file context management. - The `process_data` method uses asynchronous processing. - The `report` method provides a summary of the process. - Error handling is done using `try-except-else-finally` blocks. - The example code demonstrates how to handle the file I/O, asynchronous tasks, and exception management effectively.","solution":"import asyncio class Pipeline: def __init__(self, input_path, output_path): self.input_path = input_path self.output_path = output_path self.errors = [] def load_data(self): data = [] try: with open(self.input_path, \'r\') as file: data = [int(line.strip()) for line in file] except Exception as e: self.errors.append(f\\"Error loading data: {e}\\") return data async def process_data(self, data): try: transformed_data = await self.async_transform(data) return transformed_data except Exception as e: self.errors.append(f\\"Error processing data: {e}\\") async def async_transform(self, data): await asyncio.sleep(1) # Simulate async operation return [x * x for x in data] def save_data(self, data): try: with open(self.output_path, \'w\') as file: for item in data: file.write(f\\"{item}n\\") except Exception as e: self.errors.append(f\\"Error saving data: {e}\\") def report(self, data): print(\\"Data processing summary:\\") print(f\\"- Items processed: {len(data)}\\") print(f\\"- Errors encountered: {len(self.errors)}\\") for error in self.errors: print(f\\" - {error}\\") async def run(self): data = self.load_data() if data: processed_data = await self.process_data(data) if processed_data: self.save_data(processed_data) self.report(data) # The following lines would be used to run the pipeline in a real scenario, # but for unit testing, you should call the run method directly in the tests # if __name__ == \\"__main__\\": # input_path = \'input.txt\' # output_path = \'output.txt\' # pipeline = Pipeline(input_path, output_path) # asyncio.run(pipeline.run())"},{"question":"**Coding Assessment Question: Seaborn Visualization** **Objective:** Demonstrate your understanding of the `seaborn.objects` module in creating advanced data visualizations. **Problem Statement:** Using the `mpg` dataset from seaborn, create visualizations to explore the relationships between different variables related to car performance. Your task is to: 1. Create a plot that pairs the `mpg` (miles per gallon) variable with both `weight` and `horsepower`. Display these relationships using dot plots. 2. Create a second plot to visualize relationships between `displacement` as the dependent variable and `weight`, `horsepower`, and `acceleration` as independent variables. Use subplots wrapped into a 2x2 grid. 3. Customize both plots with appropriate axis labels. **Tasks:** 1. **Load the `mpg` dataset** from seaborn. 2. **Create the first plot**: Pair `mpg` against `weight` and `horsepower` using dot plots. 3. **Create the second plot**: Pair `displacement` with `weight`, `horsepower`, and `acceleration` using a 2x2 grid of subplots. 4. **Label the axes** meaningfully for both plots. **Input:** - No inputs required, the dataset is loaded directly within the code. **Output:** - Two figures displaying the specified relationships with appropriate customizations. **Constraints:** - You must use seaborn and its objects module for plotting. - Use appropriate seaborn plotting methods and ensure the plots are correctly labeled and readable. **Example Solution:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the mpg dataset mpg = load_dataset(\\"mpg\\") # Create the first plot: Pair mpg with weight and horsepower using dot plots plot1 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"weight\\", \\"horsepower\\"]) .label(x0=\\"Weight (lb)\\", x1=\\"Horsepower (HP)\\", y=\\"Miles per Gallon (MPG)\\") .add(so.Dots()) ) plot1.show() # Create the second plot: Pair displacement with weight, horsepower, and acceleration in a 2x2 grid plot2 = ( so.Plot(mpg, y=\\"displacement\\") .pair(x=[\\"weight\\", \\"horsepower\\", \\"acceleration\\"], wrap=2) .label(x0=\\"Weight (lb)\\", x1=\\"Horsepower (HP)\\", x2=\\"Acceleration (s)\\", y=\\"Displacement (cu in)\\") .add(so.Dots()) ) plot2.show() ``` **Note:** Ensure your code is well-commented and error-free. The final submission must produce two figures as described above when run.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_plots(): # Load the mpg dataset mpg = load_dataset(\\"mpg\\") # Create the first plot: Pair mpg with weight and horsepower using dot plots plot1 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"weight\\", \\"horsepower\\"]) .label(x0=\\"Weight (lb)\\", x1=\\"Horsepower (HP)\\", y=\\"Miles per Gallon (MPG)\\") .add(so.Dots()) ) plot1.show() # Create the second plot: Pair displacement with weight, horsepower, and acceleration in a 2x2 grid plot2 = ( so.Plot(mpg, y=\\"displacement\\") .pair(x=[\\"weight\\", \\"horsepower\\", \\"acceleration\\"], wrap=2) .label(x0=\\"Weight (lb)\\", x1=\\"Horsepower (HP)\\", x2=\\"Acceleration (s)\\", y=\\"Displacement (cu in)\\") .add(so.Dots()) ) plot2.show()"},{"question":"You are given a dataset of supermarket sales that contains columns for product category, sales date, units sold, and sales amount. Your task is to perform several data manipulation tasks using the pandas library to gain insights into the data. The dataset is structured as follows: ``` | ProductCategory | SalesDate | UnitsSold | SalesAmount | |-----------------|------------|-----------|-------------| | Beverages | 2023-01-01 | 10 | 30.00 | | Snacks | 2023-01-01 | 5 | 15.00 | | Beverages | 2023-01-02 | 8 | 24.00 | | Snacks | 2023-01-02 | 7 | 21.00 | | ... | ... | ... | ... | ``` # Instructions 1. **Data Aggregation:** Calculate the total units sold and total sales amount for each product category. - **Input:** A DataFrame `df` with columns as shown above. - **Output:** A DataFrame with columns `ProductCategory`, `TotalUnitsSold`, and `TotalSalesAmount`. 2. **Date Range Filtering:** Filter the data to include only sales that occurred in January 2023. - **Input:** A DataFrame `df` with columns as shown above. - **Output:** A filtered DataFrame only containing rows where `SalesDate` is within January 2023. 3. **Pivot Table Creation:** Create a pivot table that shows the total sales amount for each product category by day. - **Input:** A DataFrame `df` with columns as shown above. - **Output:** A pivot table with `SalesDate` as index, `ProductCategory` as columns, and the values as `SalesAmount`. 4. **Handling Missing Data:** There might be missing sales data for some dates. Fill any missing sales amounts with zero in the pivot table created in step 3. - **Input:** A pivot table as described in step 3. - **Output:** The same pivot table with any missing values filled with zero. # Constraints - Ensure that your code is efficient and makes use of pandas functions to achieve each task. - Document your code with comments explaining each step of the process. - Assume that the dataset is not extremely large, but consider the performance for datasets with up to 1,000,000 rows. # Performance Requirements - Your solution should complete each task in a reasonable time frame. - Avoid using loops where vectorized pandas operations are available. # Example Solution ```python import pandas as pd # Sample data creation data = { \'ProductCategory\': [\'Beverages\', \'Snacks\', \'Beverages\', \'Snacks\'], \'SalesDate\': [\'2023-01-01\', \'2023-01-01\', \'2023-01-02\', \'2023-01-02\'], \'UnitsSold\': [10, 5, 8, 7], \'SalesAmount\': [30.00, 15.00, 24.00, 21.00] } df = pd.DataFrame(data) # Task 1: Data Aggregation aggregation = df.groupby(\'ProductCategory\').agg({ \'UnitsSold\': \'sum\', \'SalesAmount\': \'sum\' }).reset_index() aggregation.columns = [\'ProductCategory\', \'TotalUnitsSold\', \'TotalSalesAmount\'] # Task 2: Date Range Filtering df[\'SalesDate\'] = pd.to_datetime(df[\'SalesDate\']) jan_sales = df[df[\'SalesDate\'].dt.month == 1] # Task 3: Pivot Table Creation pivot_table = jan_sales.pivot_table( index=\'SalesDate\', columns=\'ProductCategory\', values=\'SalesAmount\', aggfunc=\'sum\' ) # Task 4: Handling Missing Data pivot_table = pivot_table.fillna(0) print(aggregation) print(jan_sales) print(pivot_table) ```","solution":"import pandas as pd def aggregate_sales(df): Aggregate the total units sold and total sales amount for each product category. Parameters: df (DataFrame): The input DataFrame with columns `ProductCategory`, `SalesDate`, `UnitsSold`, and `SalesAmount`. Returns: DataFrame: Aggregated DataFrame with `ProductCategory`, `TotalUnitsSold`, and `TotalSalesAmount`. aggregation = df.groupby(\'ProductCategory\').agg({ \'UnitsSold\': \'sum\', \'SalesAmount\': \'sum\' }).reset_index() aggregation.columns = [\'ProductCategory\', \'TotalUnitsSold\', \'TotalSalesAmount\'] return aggregation def filter_sales_for_january(df): Filter the data to include only sales that occurred in January 2023. Parameters: df (DataFrame): The input DataFrame with columns `ProductCategory`, `SalesDate`, `UnitsSold`, and `SalesAmount`. Returns: DataFrame: Filtered DataFrame only containing rows where `SalesDate` is within January 2023. df[\'SalesDate\'] = pd.to_datetime(df[\'SalesDate\']) return df[(df[\'SalesDate\'].dt.year == 2023) & (df[\'SalesDate\'].dt.month == 1)] def create_pivot_table(df): Create a pivot table that shows the total sales amount for each product category by day. Parameters: df (DataFrame): The input DataFrame with columns `ProductCategory`, `SalesDate`, `UnitsSold`, and `SalesAmount`. Returns: DataFrame: Pivot table with `SalesDate` as index, `ProductCategory` as columns, and the values as `SalesAmount`. pivot_table = df.pivot_table( index=\'SalesDate\', columns=\'ProductCategory\', values=\'SalesAmount\', aggfunc=\'sum\' ) pivot_table = pivot_table.fillna(0) return pivot_table"},{"question":"**Objective**: Demonstrate understanding and capability in utilizing the `email.message.EmailMessage` class in Python 310. # Task You need to implement a function named `create_and_manage_email` that takes no parameters and follows these steps: 1. **Create a new email message**: - Set the `From`, `To`, `Subject`, and `Content-Type` headers. - Add a plain text body to the email. 2. **Modify the email**: - Add a new header indicating the email importance (e.g., `X-Priority` with a value of `\\"1 (Highest)\\"`). 3. **Query the email**: - Fetch and return the complete list of headers and their values as a dictionary. - Check if the email is multipart and include this information in the result. 4. **Output**: - Return the email as a string, complete with all headers and the body. # Implementation Notes - Use the `email.message.EmailMessage` class. - Ensure all headers and the body are appropriately set. - Use methods like `add_header`, `get_all` for manipulating and querying headers. - Serialize the email using the `as_string` method. # Example Your function should produce an output dictionary with the specific structure showing the headers and email content type information, along with the serialized email string. ```python def create_and_manage_email(): from email.message import EmailMessage # Step 1: Create the email message msg = EmailMessage() msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"recipient@example.com\\" msg[\'Subject\'] = \\"Test Email\\" msg.set_content(\\"This is the body of the test email.\\") # Step 2: Modify the email msg.add_header(\'X-Priority\', \'1 (Highest)\') # Step 3: Query the email headers = {key: msg[key] for key in msg.keys()} is_multipart = msg.is_multipart() # Step 4: Serialize the email email_string = msg.as_string() return { \\"headers\\": headers, \\"is_multipart\\": is_multipart, \\"email_string\\": email_string } # Output Example: result = create_and_manage_email() print(result) ``` **Constraints**: - Ensure that any added or modified headers comply with email standards. - Assume the body payload is simple text for this task. - Consider performance and clarity in your implementation. # Evaluation Criteria: - Correctness: The solution must follow all the steps and meet the outlined requirements. - Usage of the `EmailMessage` class and its methods. - Code readability and organization. - Handling of edge cases and errors (if any).","solution":"def create_and_manage_email(): from email.message import EmailMessage # Step 1: Create the email message msg = EmailMessage() msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"recipient@example.com\\" msg[\'Subject\'] = \\"Test Email\\" msg.set_content(\\"This is the body of the test email.\\") # Step 2: Modify the email msg.add_header(\'X-Priority\', \'1 (Highest)\') # Step 3: Query the email headers = {key: msg[key] for key in msg.keys()} is_multipart = msg.is_multipart() # Step 4: Serialize the email email_string = msg.as_string() return { \\"headers\\": headers, \\"is_multipart\\": is_multipart, \\"email_string\\": email_string }"},{"question":"You are given a file containing a sequence of integer values, each on a new line. Write a Python function that reads these integers from the file, stores them in an array, applies some operations on this array, and writes the final array values to a new file. Your function should: 1. Create an array of signed integers (`\'i\'` type code). 2. Read integers from the input file. 3. Insert these integers into the array. 4. Perform the following operations in order: - Count the occurrences of a specific integer in the array. - Insert a new value at a specific position. - Reverse the array. - Append a new value at the end of the array. - Perform a byte swap of all items in the array. 5. Write the final array values to the output file, each value on a new line. # Function Signature ```python def process_integer_array(input_file: str, output_file: str, count_value: int, insert_value: int, insert_position: int, append_value: int) -> int: pass ``` # Input 1. `input_file` (string): Path to the input file containing integers. 2. `output_file` (string): Path to the output file where final array values should be written. 3. `count_value` (int): The integer value whose occurrences need to be counted. 4. `insert_value` (int): The integer value to be inserted at a specified position. 5. `insert_position` (int): The position at which to insert the new integer value. 6. `append_value` (int): The integer value to be appended at the end of the array. # Output - The function should return the count of occurrences of `count_value` in the array before it is reversed. # Example Suppose the input file contains: ``` 1 2 3 4 5 3 2 1 ``` Calling the function as: ```python process_integer_array(\'input.txt\', \'output.txt\', 3, 99, 4, 7) ``` The method should: - Read these values into an array. - Count the occurrences of `3` (which should be `2`). - Insert `99` at position `4` (array becomes `[1, 2, 3, 4, 99, 5, 3, 2, 1]`). - Reverse the array (it becomes `[1, 2, 3, 5, 99, 4, 3, 2, 1]`). - Append `7` (it becomes `[1, 2, 3, 5, 99, 4, 3, 2, 1, 7]`). - Perform a byte swap (if needed based on system architecture). Finally, write the values `[1, 2, 3, 5, 99, 4, 3, 2, 1, 7]` to the output file. # Constraints - You may assume the input file is well-formed and contains only valid, separated integer values. - The position for insertion will be valid (i.e., within the current bounds of the array). - You must use the `array` module as per the provided documentation. # Notes - Ensure the solution handles large files efficiently. - Consider the system\'s endianness when performing byte swap operations.","solution":"import array def process_integer_array(input_file: str, output_file: str, count_value: int, insert_value: int, insert_position: int, append_value: int) -> int: # Step 1: Create an array of signed integers (\'i\' type code) int_array = array.array(\'i\') # Step 2: Read integers from the input file with open(input_file, \'r\') as file: for line in file: int_array.append(int(line.strip())) # Step 3: Count the occurrences of \'count_value\' in the array count_occurrences = int_array.count(count_value) # Step 4: Insert \'insert_value\' at the \'insert_position\' int_array.insert(insert_position, insert_value) # Step 5: Reverse the array int_array.reverse() # Step 6: Append \'append_value\' at the end of the array int_array.append(append_value) # Step 7: Perform a byte swap of all items in the array int_array.byteswap() # Step 8: Write the final array values to the output file with open(output_file, \'w\') as file: for value in int_array: file.write(str(value) + \'n\') # Return the count of occurrences return count_occurrences"},{"question":"# Custom Fixer Implementation As part of your journey to understand and extend the functionality of the `2to3` tool, you are required to implement a custom fixer that transforms Python 2 code to Python 3 code. Your custom fixer will specifically address transforming the use of `xrange` to `range` and ensure that the resulting code maintains the same functionality. Task 1. Write a function `transform_xrange_to_range` that takes a string containing Python 2 code and returns a transformed string where all instances of `xrange` are replaced with `range`. 2. Ensure the transformation considers the change in behavior from `xrange` (a lazy sequence) to `range` (which in Python 3 behaves like `xrange` in Python 2). 3. To handle range behavior, explicitly wrap `range` in a `list` for consistency with Python 2\'s `xrange`. Function Signature ```python def transform_xrange_to_range(python2_code: str) -> str: pass ``` Input - `python2_code` (str): A string of Python 2 source code. Output - `str`: A transformed string with all instances of `xrange` converted to `range`. Example ```python # Input: python2_code = for i in xrange(10): print(i) # Output: for i in list(range(10)): print(i) ``` Constraints - The input code may have multiple instances of `xrange`. - Ensure that the transformation preserves the logical structure and indentation of the original code. - Handle different variations such as `for i in xrange(10)`, `x = xrange(10)`, and `xrange ( 10 )`. Notes - You do not need to handle nested or multiline string literals. - Comments should be preserved as much as possible. - Assume the input adheres to Python 2 syntax. Implement the function and ensure it accurately transforms the `xrange` usage to `range` wrapped in `list`.","solution":"import re def transform_xrange_to_range(python2_code: str) -> str: Transforms Python 2 code by replacing \'xrange\' with \'list(range(...))\' to maintain the same functionality in Python 3. # Regular expression to find all instances of `xrange` xrange_pattern = re.compile(r\'bxranges*(([^)]*))\') # Replace `xrange` with `list(range(...))` transformed_code = xrange_pattern.sub(r\'list(range(1))\', python2_code) return transformed_code"},{"question":"<|Analysis Begin|> The documentation provides a brief introduction to the seaborn package, specifically focusing on how to set the visual theme and load a dataset. It then demonstrates how to create basic count plots using `sns.countplot`, including incorporating a second variable for grouping and normalizing counts to percentages. Based on the documentation, we can understand that the students should be familiar with: 1. Loading datasets provided by seaborn. 2. Setting the visual theme. 3. Creating count plots with a single categorical variable. 4. Grouping counts by a second variable. 5. Normalizing counts to show percentages. To design an assessment question, we can build upon these fundamental concepts and include a task that requires students to apply multiple aspects of seaborn plotting, including customization and potentially comparing different plots. <|Analysis End|> <|Question Begin|> # Seaborn Coding Assessment Question Objective: Demonstrate your comprehension of Seaborn by constructing and customizing a detailed visual analysis of the Titanic dataset. Assignment: Write a Python function `analyze_survival_by_class_and_sex(titanic)` that takes in the Titanic dataset and outputs a pair of count plots showcasing survival rates. You need to create count plots comparing survival rates grouped by both `class` and `sex`. Then, customize these plots by setting different styles and display them side-by-side for comparison. Requirements: 1. **Input:** The function should accept the Titanic dataset as a Pandas DataFrame. 2. **Output:** Generate and display two count plots: * The first plot should display the count of survivors and non-survivors grouped by the passenger class. * The second plot should display the count of survivors and non-survivors grouped by the passenger class and further separated by gender (`sex`). 3. **Customization:** * Set a theme for each plot (`whitegrid` for the first plot and `dark` for the second plot). * The counts in the second plot should be normalized to show percentages instead of raw counts. 4. **Constraints and Limitations:** * Use appropriate seaborn functions for loading the dataset, creating the plots, and setting the themes. * Ensure that the plots are clearly labeled and have suitable titles to indicate the information being conveyed. Example: To help you get started, here\'s an example template for the function. You need to fill in the appropriate code to meet the requirements specified: ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_survival_by_class_and_sex(titanic): # Set the theme and create the first plot sns.set_theme(style=\\"whitegrid\\") plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) # Your plotting code here # Change theme and create the second plot sns.set_theme(style=\\"dark\\") plt.subplot(1, 2, 2) # Your plotting code here # Display the plots plt.show() # Example usage: titanic = sns.load_dataset(\\"titanic\\") analyze_survival_by_class_and_sex(titanic) ``` Ensure that your function is well-documented and adheres to best practices for readability and code organization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_survival_by_class_and_sex(titanic): Generates and displays a pair of count plots showing the survival rates by class and sex in the Titanic dataset. Parameters: titanic (pd.DataFrame): The Titanic dataset as a pandas DataFrame. # Set the theme and create the first plot sns.set_theme(style=\\"whitegrid\\") plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) sns.countplot(data=titanic, x=\'class\', hue=\'survived\') plt.title(\'Survival Counts by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') # Change theme and create the second plot sns.set_theme(style=\\"dark\\") plt.subplot(1, 2, 2) sns.histplot( data=titanic, x=\'class\', hue=\'survived\', multiple=\'fill\', stat=\'percent\', binwidth=1, shrink=0.8 ) plt.title(\'Survival Percentages by Class and Sex\') plt.xlabel(\'Class\') plt.ylabel(\'Percentage\') # Display the plots plt.show() # Example usage: # titanic = sns.load_dataset(\\"titanic\\") # analyze_survival_by_class_and_sex(titanic)"},{"question":"# XML Incremental Parsing and Attribute Extraction You are tasked with implementing a SAX-based XML parser using the `xml.sax.xmlreader` module. The parser should be able to handle large XML documents incrementally, process specific elements to extract their attributes, and respond to locale settings dynamically during parsing. Requirements 1. **Parsing Function**: Implement a function `incremental_parse(xml_data: List[str], locale: str) -> List[Dict[str, str]]` that accepts: - `xml_data`: A list of strings, each representing a chunk of the XML document. - `locale`: A string representing the locale to be set for errors and warnings. The function should: - Use the `xml.sax.xmlreader.IncrementalParser` to parse the chunks incrementally. - Set the locale dynamically during parsing using `setLocale(locale)`. - Extract attributes from elements that match a specific tag (e.g., `\\"item\\"`), and return these attributes as a list of dictionaries. 2. **Attributes Extraction**: Focus on extracting attributes of elements named `\\"item\\"`. If an `item` element has attributes such as `id`, `name`, and `value`, the function should store these as dictionaries in the resulting list. Example Usage ```python xml_data = [ \'<?xml version=\\"1.0\\"?><root>\', \'<item id=\\"1\\" name=\\"item1\\" value=\\"100\\"/>\', \'<item id=\\"2\\" name=\\"item2\\" value=\\"200\\"/>\', \'</root>\' ] locale = \\"en_US\\" result = incremental_parse(xml_data, locale) print(result) ``` Expected Output ```python [ {\\"id\\": \\"1\\", \\"name\\": \\"item1\\", \\"value\\": \\"100\\"}, {\\"id\\": \\"2\\", \\"name\\": \\"item2\\", \\"value\\": \\"200\\"} ] ``` Constraints - Ensure that the function handles the end of the document correctly and resets the parser as needed. - Raise appropriate exceptions for invalid XML data or errors during parsing. Performance Requirements - Optimize for large XML documents by effectively chunking and processing data. - Minimize memory footprint by avoiding loading the entire document into memory. ```python def incremental_parse(xml_data: List[str], locale: str) -> List[Dict[str, str]]: # Implement this function following the given requirements. pass ``` Ensure your implementation adheres to the SAX `XMLReader` and `IncrementalParser` interfaces as described in the documentation.","solution":"import xml.sax import xml.sax.handler class ItemHandler(xml.sax.handler.ContentHandler): def __init__(self): self.items = [] def startElement(self, name, attrs): if name == \\"item\\": item = {k: v for k, v in attrs.items()} self.items.append(item) def incremental_parse(xml_data, locale): Incrementally parse XML data and extract attributes from \'item\' elements. Parameters: - xml_data: List of XML data chunks as strings. - locale: Locale setting for errors/warnings. Returns: - List of dictionaries containing attributes of \'item\' elements. # Create an IncrementalParser parser = xml.sax.make_parser() handler = ItemHandler() parser.setContentHandler(handler) # Process each chunk of XML data for chunk in xml_data: parser.feed(chunk) # End the parsing sequence parser.close() return handler.items"},{"question":"# Custom Managed Resource Context You are required to implement a custom context manager in Python that manages an arbitrary resource. This context manager will ensure that the resource is properly acquired and released, handling any exceptions that occur during its use. Your implementation should use the `@contextlib.contextmanager` decorator. Detailed Requirements 1. **Function Signature:** ```python from contextlib import contextmanager @contextmanager def manage_arbitrary_resource(resource_name: str): pass ``` 2. **Functionality:** - The context manager should simulate acquiring a resource (e.g., opening a file or a network connection) and yield control back to the caller. - After yielding the resource, it should ensure the resource is released properly, even if an exception occurs within the `with` block. - To simulate resource acquisition and release: - Print `\\"Acquiring resource: <resource_name>\\"` when the resource is acquired. - Print `\\"Releasing resource: <resource_name>\\"` when the resource is released. 3. **Exception Handling:** - If an exception occurs during the `with` block\'s execution, the context manager should catch it, print `\\"Exception encountered: <exception_message>\\"`, and then release the resource. Example Usage ```python if __name__ == \'__main__\': try: with manage_arbitrary_resource(\\"example_resource\\") as resource: print(\\"Using the resource...\\") # Simulate an error raise ValueError(\\"Something went wrong!\\") except Exception as e: print(f\\"Handled exception: {e}\\") print(\\"nNo error scenario:\\") with manage_arbitrary_resource(\\"another_resource\\") as resource: print(\\"Using the resource without error...\\") ``` Expected Output ``` Acquiring resource: example_resource Using the resource... Exception encountered: Something went wrong! Releasing resource: example_resource Handled exception: Something went wrong! No error scenario: Acquiring resource: another_resource Using the resource without error... Releasing resource: another_resource ``` Note: Ensure you follow the structure and instructions carefully, using the `@contextlib.contextmanager` decorator to manage the resource.","solution":"from contextlib import contextmanager @contextmanager def manage_arbitrary_resource(resource_name: str): try: # Simulate resource acquisition print(f\\"Acquiring resource: {resource_name}\\") yield resource_name except Exception as e: # Handling exception and reporting print(f\\"Exception encountered: {e}\\") raise # Re-raise the exception after handling it finally: # Ensure resource is released print(f\\"Releasing resource: {resource_name}\\")"},{"question":"# Problem: Concurrent Matrix Multiplication In this problem, you are required to implement a program that multiplies two matrices using concurrent execution to speed up the computation. You will use both threading-based and process-based parallelism to perform the matrix multiplication. Task 1. Implement the function `matrix_mult_threading(A: List[List[int]], B: List[List[int]]) -> List[List[int]]` using thread-based parallelism (`threading` module). 2. Implement the function `matrix_mult_multiprocessing(A: List[List[int]], B: List[List[int]]) -> List[List[int]]` using process-based parallelism (`multiprocessing` module). Input - Two matrices, `A` and `B`, where `A` is of size `MxN` and `B` is of size `NxP`. Both matrices will be provided as lists of lists of integers. Output - The resulting matrix after performing the multiplication of `A` and `B`, which will be of size `MxP`. Constraints - All elements in the input matrices are integers. - `1 <= M, N, P <= 500` - You must use concurrency to compute the result to demonstrate your understanding of both `threading` and `multiprocessing`. Example ```python A = [ [1, 2], [3, 4] ] B = [ [2, 0], [1, 2] ] # Expected Output: # [ # [4, 4], # [10, 8] # ] ``` Implementation Requirements 1. Your threading-based implementation should utilize `Thread` objects from the `threading` module. 2. Your multiprocessing-based implementation should utilize `Process` objects from the `multiprocessing` module. 3. Ensure to handle synchronization and ensure that the main thread or process correctly aggregates the results. Performance - Aim for your solution to be efficient in terms of time complexity by effectively utilizing concurrent computation. - Ensure that the solution handles the upper constraint limits within a reasonable amount of time. Additional Notes - You are encouraged to use locks, semaphores, or other synchronization primitives if necessary to ensure thread/process safety in your implementations. - You should write your main function to compare the execution time of both implementations for matrices of size 500x500 with random integer elements to observe the performance difference.","solution":"import threading import multiprocessing import random def matrix_mult_threading(A, B): M, N = len(A), len(A[0]) P = len(B[0]) result = [[0 for _ in range(P)] for _ in range(M)] def worker(i, j): result[i][j] = sum(A[i][k] * B[k][j] for k in range(N)) threads = [] for i in range(M): for j in range(P): thread = threading.Thread(target=worker, args=(i, j)) threads.append(thread) thread.start() for thread in threads: thread.join() return result def matrix_mult_multiprocessing(A, B): M, N = len(A), len(A[0]) P = len(B[0]) result = [[0 for _ in range(P)] for _ in range(M)] def worker(i, j, output): output.put((i, j, sum(A[i][k] * B[k][j] for k in range(N)))) output = multiprocessing.Queue() processes = [] for i in range(M): for j in range(P): process = multiprocessing.Process(target=worker, args=(i, j, output)) processes.append(process) process.start() for process in processes: process.join() while not output.empty(): i, j, value = output.get() result[i][j] = value return result"},{"question":"Objective: Implement a class hierarchy for a simple library system. This will test your understanding of class design, inheritance, and the use of iterators or generators. Requirements: 1. **Book Class** - Attributes: `title`, `author`, `year` - Method: `__str__` should return a string in the format: `\\"title\\" by author (year)` 2. **Library Class** - Attributes: `books` (a list to store books) - Methods: - `add_book(book)`: Adds a `Book` instance to the library. - `remove_book(title)`: Removes a book from the library by its title. If the book is not found, raise an appropriate exception. - `find_books_by_author(author)`: Returns a generator of all books by the specified author. - `__iter__`: Should return an iterator that iterates over all books in the library in the order they were added. - Incorporate appropriate error-handling. 3. **EnhancedLibrary Class** - Inherits from `Library` - Additional Method: - `find_books_by_year(year)`: Returns a generator of all books published in the given year. Constraints: - Only use Python standard library. - Ensure your implementation is efficient and avoids redundancy. - Consider edge cases, such as trying to remove a book that doesn\'t exist. Input and Output Formats: - No specific input/output format required. Focus on correct class and method design. Example Usage: ```python # Creating book instances book1 = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925) book2 = Book(\\"1984\\", \\"George Orwell\\", 1949) book3 = Book(\\"Animal Farm\\", \\"George Orwell\\", 1945) # Creating Library instance and adding books library = EnhancedLibrary() library.add_book(book1) library.add_book(book2) library.add_book(book3) # Finding books by author for book in library.find_books_by_author(\\"George Orwell\\"): print(book) # Finding books by year for book in library.find_books_by_year(1945): print(book) # Iterating over all books in the library for book in library: print(book) ``` Expected Output: ``` \\"1984\\" by George Orwell (1949) \\"Animal Farm\\" by George Orwell (1945) \\"Animal Farm\\" by George Orwell (1945) \\"The Great Gatsby\\" by F. Scott Fitzgerald (1925) \\"1984\\" by George Orwell (1949) \\"Animal Farm\\" by George Orwell (1945) ``` **Note:** The `print` statements are for demonstration purposes. Ensure you handle all other possible scenarios and exceptions as specified.","solution":"class Book: def __init__(self, title, author, year): self.title = title self.author = author self.year = year def __str__(self): return f\'\\"{self.title}\\" by {self.author} ({self.year})\' class Library: def __init__(self): self.books = [] def add_book(self, book): self.books.append(book) def remove_book(self, title): for book in self.books: if book.title == title: self.books.remove(book) return raise ValueError(f\'Book with title \\"{title}\\" not found in the library\') def find_books_by_author(self, author): return (book for book in self.books if book.author == author) def __iter__(self): return iter(self.books) class EnhancedLibrary(Library): def find_books_by_year(self, year): return (book for book in self.books if book.year == year)"},{"question":"**Objective:** Implement a task processing system using asyncio queues where multiple workers process tasks concurrently and handle priorities of tasks. **Problem Statement:** You are required to implement an asynchronous task processing system using `asyncio.PriorityQueue`. Your system should handle tasks with different priorities and maintain the order of execution based on these priorities. **Task Requirements:** 1. **Function Signature:** ```python async def process_tasks(tasks: list[tuple[int, str]], num_workers: int) -> list[str]: ``` 2. **Input:** - `tasks`: A list of tuples where each tuple contains an integer priority and a string task description. Lower integer values indicate higher priority. Example: `[(1, \'Task1\'), (3, \'Task3\'), (2, \'Task2\')]`. - `num_workers`: An integer representing the number of worker tasks to process the queue concurrently. 3. **Output:** - A list of strings representing the order in which tasks were processed by the workers. 4. **Constraints:** - You should use `asyncio.PriorityQueue` to manage the tasks. - Each worker should retrieve tasks from the queue, simulate processing by using `await asyncio.sleep(0.1)`, and then mark the task as done using `task_done`. - The function should return a list of task descriptions in the order they were processed. - Handle any necessary exceptions such as when trying to get from an empty queue. **Examples:** Example 1: ```python tasks = [(1, \'Task1\'), (3, \'Task3\'), (2, \'Task2\')] num_workers = 2 result = await process_tasks(tasks, num_workers) print(result) # Output could be [\'Task1\', \'Task2\', \'Task3\'] ``` Example 2: ```python tasks = [(5, \'Task5\'), (1, \'Task1\'), (3, \'Task3\'), (2, \'Task2\'), (4, \'Task4\')] num_workers = 3 result = await process_tasks(tasks, num_workers) print(result) # Output could be [\'Task1\', \'Task2\', \'Task3\', \'Task4\', \'Task5\'] ``` **Notes:** - The order of tasks with the same priority must be preserved as in the input list. - Make sure that your function appropriately handles worker cancellation and ensures all tasks are completed before returning the result list. - You can assume that the input tasks list will contain at least one task, and num_workers will be a positive integer. # Guidance: 1. Initialize a `PriorityQueue` and populate it with the provided tasks. 2. Define worker coroutines that continuously fetch from the queue until all tasks are done. 3. Use `asyncio.gather` to run all worker coroutines concurrently. 4. Ensure the `task_done` method is called for each completed task and `join` is used to wait until all tasks are done.","solution":"import asyncio from typing import List, Tuple async def worker(queue: asyncio.PriorityQueue, results: List[str]): while True: try: priority, task = await queue.get() except asyncio.CancelledError: break results.append(task) await asyncio.sleep(0.1) # Simulate task processing time queue.task_done() async def process_tasks(tasks: List[Tuple[int, str]], num_workers: int) -> List[str]: queue = asyncio.PriorityQueue() results = [] for task in tasks: queue.put_nowait(task) workers = [asyncio.create_task(worker(queue, results)) for _ in range(num_workers)] await queue.join() for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) return results"},{"question":"# Question: You are required to implement a custom collection class named `CustomSequence` that adheres to the `collections.abc.Sequence` Abstract Base Class. This class should be able to handle a sequence of elements and provide all necessary methods defined by the `Sequence` ABC. Requirements: 1. **Class Definition**: - Define a class `CustomSequence` that inherits from `collections.abc.Sequence`. 2. **Initialization**: - The class should be initialized with an iterable (list, tuple, etc.). 3. **Abstract Methods**: - Implement the required abstract methods: `__getitem__`, `__len__`. 4. **Mixin Methods**: - Ensure your class supports mixin methods by inheriting from the ABC `Sequence`. - You may choose to override any mixin methods if necessary. 5. **Additional Functionality**: - Implement a method `append` to add elements to the sequence. - Implement a method `remove` to remove the first occurrence of an element from the sequence. 6. **Input/Output**: - The constructor will receive an iterable to initialize the sequence. - Methods like `__getitem__`, `__len__`, `append`, and `remove` should work as expected for a sequence. Example: ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self.data = list(iterable) def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) def append(self, value): self.data.append(value) def remove(self, value): self.data.remove(value) # Example Usage: cs = CustomSequence([1, 2, 3, 4]) print(cs[0]) # Output: 1 print(len(cs)) # Output: 4 cs.append(5) print(len(cs)) # Output: 5 cs.remove(3) print(len(cs)) # Output: 4 print(cs[2]) # Output: 4 ``` Constraints: - The sequence should only contain elements that support comparison operations. - Ensure that your class handles invalid operations, such as removing an element not in the sequence, by raising appropriate exceptions.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self.data = list(iterable) def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) def append(self, value): self.data.append(value) def remove(self, value): self.data.remove(value) # Example Usage: cs = CustomSequence([1, 2, 3, 4]) print(cs[0]) # Output: 1 print(len(cs)) # Output: 4 cs.append(5) print(len(cs)) # Output: 5 cs.remove(3) print(len(cs)) # Output: 4 print(cs[2]) # Output: 4"},{"question":"Question: Clustering Analysis with scikit-learn You are provided with a dataset consisting of multiple features. Your task is to implement a function that preprocesses the data, applies a clustering algorithm, evaluates its performance, and visualizes the clustering result. # Function Signature ```python def clustering_analysis(data: np.ndarray, n_clusters: int) -> Tuple[float, plt.Figure]: Perform clustering analysis on the given dataset. Parameters: - data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. - n_clusters (int): The number of clusters to form. Returns: - silhouette_score (float): The silhouette score indicating the quality of the clustering. - figure (plt.Figure): A matplotlib figure object visualizing the clustering result. ``` # Input - `data`: A 2D numpy array of shape (n_samples, n_features) representing the dataset. - `n_clusters`: An integer specifying the number of clusters to form. # Output - `silhouette_score`: A float indicating the silhouette score of the clustering. - `figure`: A matplotlib figure object displaying the clustering result. # Constraints - Ensure that the provided data is scaled properly before applying the clustering algorithm. - Use KMeans clustering algorithm from scikit-learn. - The silhouette score should be calculated using `sklearn.metrics.silhouette_score`. - Visualize the clusters in a 2D scatter plot (using the first two principal components if the data has more than 2 features). # Example ```python import numpy as np from matplotlib import pyplot as plt # Example data data = np.array([ [1.0, 2.1], [1.5, 1.8], [5.1, 5.2], [6.3, 5.8] ]) n_clusters = 2 silhouette, figure = clustering_analysis(data, n_clusters) print(f\\"Silhouette Score: {silhouette}\\") figure.show() ``` This example should result in a silhouette score being printed and a scatter plot showing the clusters. # Notes - Ensure the data is scaled using `StandardScaler` from scikit-learn before clustering. - Use `PCA` for dimensionality reduction if the data has more than 2 features for visualization purposes. - The scatter plot should have different colors representing different clusters. Make sure to include any necessary imports in your function implementation.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler from sklearn.metrics import silhouette_score from sklearn.decomposition import PCA import matplotlib.pyplot as plt def clustering_analysis(data: np.ndarray, n_clusters: int) -> tuple: Perform clustering analysis on the given dataset. Parameters: - data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. - n_clusters (int): The number of clusters to form. Returns: - silhouette_score (float): The silhouette score indicating the quality of the clustering. - figure (plt.Figure): A matplotlib figure object visualizing the clustering result. # Step 1: Scale the data scaler = StandardScaler() scaled_data = scaler.fit_transform(data) # Step 2: Apply KMeans clustering kmeans = KMeans(n_clusters=n_clusters) cluster_labels = kmeans.fit_predict(scaled_data) # Step 3: Calculate the silhouette score score = silhouette_score(scaled_data, cluster_labels) # Step 4: Visualize the clustering result if data.shape[1] > 2: pca = PCA(n_components=2) reduced_data = pca.fit_transform(scaled_data) else: reduced_data = scaled_data # Create the scatter plot fig, ax = plt.subplots() scatter = ax.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap=\'viridis\') legend1 = ax.legend(*scatter.legend_elements(), title=\\"Clusters\\") ax.add_artist(legend1) return score, fig"},{"question":"**Question: Temporary File and Directory Management in Python** You are tasked with writing a Python function, `manage_temp_files_and_dirs`, that performs the following operations: 1. Creates a temporary directory. 2. Within this temporary directory, creates a temporary file and writes multiple lines of text data into it. 3. Reads back the content of this temporary file and returns it. 4. Ensures that all temporary files and directories are properly deleted after the operations, regardless of whether exceptions occur during the file manipulation. To achieve this: - Make use of `tempfile.TemporaryDirectory` for creating the temporary directory. - Use `tempfile.NamedTemporaryFile` for creating the temporary file within this directory. - Ensure you utilize context managers for proper automatic cleanup. Your implementation should ensure: - The file is created and visible in the temporary directory. - Proper handling of file I/O operations. - Proper cleanup of resources to avoid leaving any temporary files or directories behind. **Function Signature:** ```python def manage_temp_files_and_dirs(text_lines: list) -> str: pass ``` **Parameters:** - `text_lines` (list of str): A list of text lines to be written into the temporary file. **Returns:** - `str`: The content read back from the temporary file. **Example Usage:** ```python text_lines = [\\"Hello, World!\\", \\"Python is great!\\", \\"Goodbye!\\"] content = manage_temp_files_and_dirs(text_lines) assert content == \\"Hello, World!nPython is great!nGoodbye!n\\" ``` **Constraints:** - You must use the `tempfile` module for managing the temporary files and directories. - Ensure that the temporary file is explicitly created within the temporary directory. Write your implementation in the provided function signature. Make sure your code handles exceptions gracefully to ensure that temporary files and directories are always cleaned up.","solution":"import tempfile import os def manage_temp_files_and_dirs(text_lines): Creates a temporary directory and a file within it, writes the provided text lines to the file, reads back the content and returns it. Ensures proper cleanup of temporary files and directories. Parameters: text_lines (list of str): A list of text lines to write into the temporary file. Returns: str: Content read back from the temporary file. content = \\"\\" try: with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = os.path.join(temp_dir, \'tempfile.txt\') with open(temp_file_path, \'w\') as temp_file: temp_file.writelines(line + \'n\' for line in text_lines) with open(temp_file_path, \'r\') as temp_file: content = temp_file.read() except Exception as e: print(f\\"An error occurred: {e}\\") return content"},{"question":"# XML Data Manipulation Challenge You are given an XML document containing information about a collection of books. Your task is to parse this XML, extract specific pieces of information, and apply several modifications. You will write a function called `process_books_xml(input_file: str, output_file: str) -> None` that performs the following tasks: 1. **Parse the XML**: Import the XML data from the file specified by `input_file`. 2. **Extract Data**: Iterate over all book elements and collect: - The titles of books published after the year 2000. - All author names associated with books published after the year 2000. 3. **Modify Data**: - Increment the price of every book by 10%. - Add a new attribute `\\"discounted\\": \\"no\\"` to each book element. 4. **Write Modified XML**: Save the modified XML data to the specified `output_file`, ensuring that the XML is nicely indented for readability. Input - `input_file` is the path to the input XML file. - `output_file` is the path to the output XML file where modified XML should be saved. Output - The function should not return any value. It should write the modified XML data to `output_file`. Constraints - Assume the XML follows this general structure: ```xml <?xml version=\\"1.0\\"?> <bookstore> <book category=\\"children\\"> <title lang=\\"en\\">Harry Potter</title> <author>J K. Rowling</author> <year>2003</year> <price>29.99</price> </book> <book category=\\"web\\"> <title lang=\\"en\\">Learning XML</title> <author>Erik T. Ray</author> <year>2000</year> <price>39.95</price> </book> ... </bookstore> ``` Example Usage Assuming `books.xml` contains above sample XML structure: ```python process_books_xml(\'books.xml\', \'modified_books.xml\') ``` Hints - Utilize `xml.etree.ElementTree` for parsing and modifying the XML. - You can use `ElementTree.parse()` and `Element.getroot()` for parsing and tree traversal. - Use the `ElementTree.iter()` method for iterative operations. - To write the modified tree back to a file, use `ElementTree.write()` with appropriate parameters. Your function should handle the following cases: 1. Books with multiple authors. 2. Books without a price field (ignore price update for such books). 3. Books without a year field (ignore such books when filtering for data extraction).","solution":"import xml.etree.ElementTree as ET def process_books_xml(input_file: str, output_file: str) -> None: Process the input XML file, extract necessary information, modify data, and save to output file. Args: - input_file (str): Path to the input XML file. - output_file (str): Path to the output XML file. tree = ET.parse(input_file) root = tree.getroot() titles_after_2000 = [] authors_after_2000 = set() for book in root.iter(\'book\'): year_element = book.find(\'year\') if year_element is not None and int(year_element.text) > 2000: title = book.find(\'title\').text titles_after_2000.append(title) for author in book.findall(\'author\'): authors_after_2000.add(author.text) price_element = book.find(\'price\') if price_element is not None: new_price = round(float(price_element.text) * 1.1, 2) price_element.text = str(new_price) book.set(\\"discounted\\", \\"no\\") # Write the modified XML to the output file tree.write(output_file, encoding=\'utf-8\', xml_declaration=True) # Print the extracted information print(\\"Titles of books published after 2000:\\", titles_after_2000) print(\\"Authors of books published after 2000:\\", authors_after_2000)"},{"question":"# Secure Login System Simulation **Objective:** Design a Python function to simulate a secure login system using the \\"getpass\\" module. The function should demonstrate an understanding of secure password handling and basic user authentication. **Task:** 1. Implement a function called `secure_login_system(users)` where: - `users` is a dictionary containing usernames as keys and their respective passwords as values. 2. The function should: - Prompt the user for their username using `getpass.getuser()`. - Prompt the user for their password using `getpass.getpass(prompt)`. - Validate the entered username and password against the provided dictionary. - Notify the user of the login status (success or failure). **Input:** - A dictionary `users` where keys are usernames and values are passwords (strings). **Output:** - Print \\"Login successful\\" if the username and password match. - Print \\"Invalid username or password\\" if they do not match. **Constraints:** - Handle potential exceptions raised by the getpass module gracefully. - Ensure that passwords are handled securely without being echoed during input. **Performance Requirements:** - Optimize for clarity and correct functionality over performance. **Example Usage:** ```python def secure_login_system(users): import getpass try: username = getpass.getuser() password = getpass.getpass(\'Enter your password: \') if username in users and users[username] == password: print(\\"Login successful\\") else: print(\\"Invalid username or password\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Sample dictionary of users users = { \'user1\': \'password123\', \'user2\': \'passw0rd\' } secure_login_system(users) ``` **Note:** - This function simulates a secure login system environment. It assumes that the dictionary `users` is obtained securely and is kept confidential within the scope of the function.","solution":"def secure_login_system(users): import getpass try: username = input(\\"Enter your username: \\") password = getpass.getpass(\'Enter your password: \') if username in users and users[username] == password: print(\\"Login successful\\") else: print(\\"Invalid username or password\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Custom Calendar Class Implementation You are tasked with implementing a custom calendar class in Python using the `calendar` module. This class should encapsulate several advanced features to demonstrate your command of the module. **Objective:** Create a class `CustomCalendar` that: 1. Extends the `calendar.Calendar` class. 2. Adds additional functionalities not directly available in the base class. **Requirements:** 1. **Initialization:** - The class should allow the setting of the first weekday during initialization. - Example: `CustomCalendar(firstweekday=0)` 2. **Weekday Name Schedule:** - Implement a method `weekday_name_schedule()` that returns the names of the weekdays starting with the configured first weekday of the calendar. - Example: ```python cal = CustomCalendar(firstweekday=2) print(cal.weekday_name_schedule()) ``` Output: ```python [\'Wednesday\', \'Thursday\', \'Friday\', \'Saturday\', \'Sunday\', \'Monday\', \'Tuesday\'] ``` 3. **Month with Events:** - Implement a method `events_month(year, month, events)` that takes a year, a month, and a dictionary of events where the keys are day numbers and the values are lists of events for those days. This method should return a version of a text calendar for the specified year and month with events listed beneath each corresponding day. - Example: ```python events = {3: [\'Event 1\', \'Event 2\'], 15: [\'Event 3\']} cal = CustomCalendar() print(cal.events_month(2023, 10, events)) ``` Output: ``` October 2023 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 - Event 1 - Event 2 Event 3 ``` 4. **Leap Year Counter:** - Implement a method `count_leap_years(start_year, end_year)` that returns the number of leap years between the given start and end years (inclusive). - Example: ```python cal = CustomCalendar() print(cal.count_leap_years(2000, 2020)) ``` Output: ```python 6 ``` 5. **Week Number Calculation:** - Implement a method `week_number(year, month, day)` that calculates the week number of a given date (1 being the first week of the month). - Example: ```python cal = CustomCalendar() print(cal.week_number(2023, 10, 15)) ``` Output: ```python 3 ``` **Constraints:** - You must use the methods and functionalities provided by the `calendar` module. - Ensure the output is formatted as specified in the examples. - Handle edge cases such as invalid dates or empty event dictionaries gracefully with appropriate error messages. This task will assess your understanding of the `calendar` module and your ability to implement a class structure with specific requirements.","solution":"import calendar class CustomCalendar(calendar.Calendar): def __init__(self, firstweekday=0): super().__init__(firstweekday) def weekday_name_schedule(self): Returns the names of the weekdays starting with the configured first weekday. days = list(calendar.day_name) return days[self.firstweekday:] + days[:self.firstweekday] def events_month(self, year, month, events): Returns a text calendar for the specified year and month with events listed under corresponding days. if not isinstance(events, dict): raise ValueError(\\"Events must be provided as a dictionary with day numbers as keys.\\") text_cal = calendar.TextCalendar(self.firstweekday) month_cal = text_cal.formatmonth(year, month) # Split the calendar into lines and parse events lines = month_cal.split(\'n\') for day, event_list in events.items(): if not (1 <= day <= 31): raise ValueError(\\"Invalid day number in events.\\") for i, line in enumerate(lines): if line.strip().split().count(str(day)) > 0: for event in event_list: lines.insert(i + 1, f\\" - {event}\\") return \\"n\\".join(lines) def count_leap_years(self, start_year, end_year): Returns the number of leap years between start_year and end_year (inclusive). leap_years = 0 for year in range(start_year, end_year + 1): if calendar.isleap(year): leap_years += 1 return leap_years def week_number(self, year, month, day): Calculates the week number of a given date (1 being the first week of the month). if not (1 <= month <= 12) or not (1 <= day <= 31): raise ValueError(\\"Invalid date provided.\\") # Get the first day of the month and the weekday of that day first_day_of_month_weekday, _ = calendar.monthrange(year, month) days_in_month = calendar.monthcalendar(year, month) week_number = 1 for week in days_in_month: if day in week: return week_number week_number += 1 raise ValueError(\\"Invalid date. Date does not exist in the given month.\\") # Example usage cal = CustomCalendar(firstweekday=2) print(cal.weekday_name_schedule()) events = {3: [\'Event 1\', \'Event 2\'], 15: [\'Event 3\']} print(cal.events_month(2023, 10, events)) print(cal.count_leap_years(2000, 2020)) print(cal.week_number(2023, 10, 15))"},{"question":"# IP Network Management You are tasked with writing a function that takes a list of IP address strings and a network string, determines how many of those IP addresses belong to the given network, and returns the count of these addresses. Additionally, you should extract and return the netmask as a string. Function Signature ```python def count_ips_in_network(ip_list: list, network: str) -> tuple: pass ``` Input - `ip_list`: A list of strings, where each string represents an IP address (e.g., `[\\"192.0.2.1\\", \\"192.0.2.2\\", \\"192.0.3.1\\"]`). - `network`: A string representing an IP network in CIDR notation (e.g., `\\"192.0.2.0/24\\"`). Output - A tuple with two elements: - An integer representing the number of IP addresses from the list that are within the given network. - A string representing the netmask of the network in standard decimal format for IPv4 or exploded format for IPv6. Constraints - The number of IP addresses in `ip_list` will not exceed 1000. - All IP addresses and network strings are valid and well-formed. - The function should handle both IPv4 and IPv6 addresses. Example ```python ip_list = [\\"192.0.2.1\\", \\"192.0.2.2\\", \\"192.0.3.1\\"] network = \\"192.0.2.0/24\\" result = count_ips_in_network(ip_list, network) print(result) # Output: (2, \\"255.255.255.0\\") ``` Explanation In the example, there are three IP addresses. Only two of the addresses (\\"192.0.2.1\\" and \\"192.0.2.2\\") belong to the network \\"192.0.2.0/24\\". The netmask for this network is \\"255.255.255.0\\". # Notes - You should use the `ipaddress` module to handle IP address and network manipulations. - Ensure your solution is efficient and handles the provided constraints gracefully.","solution":"import ipaddress def count_ips_in_network(ip_list, network): Counts how many IP addresses in ip_list are within the given network and returns the count and the network\'s netmask. Parameters: ip_list (list): A list of IP address strings. network (str): A string representing an IP network in CIDR notation (e.g., \\"192.0.2.0/24\\"). Returns: tuple: A tuple with an integer counting the IP addresses in the network and the netmask as a string. net = ipaddress.ip_network(network) count = sum(1 for ip in ip_list if ipaddress.ip_address(ip) in net) return count, str(net.netmask)"},{"question":"# Custom Autograd Function for Complex Numbers In this exercise, you are required to implement a custom autograd function in PyTorch that performs element-wise multiplication of two input tensors, and then test it using complex numbers. This task aims to evaluate your understanding of both the autograd mechanics and handling complex number differentiation. Task 1. Define a custom autograd function `ComplexMultiply` that: - During the forward pass, multiplies two input tensors element-wise. - During the backward pass, computes the gradients correctly by leveraging Wirtinger derivatives for complex numbers. 2. Create a test case to ensure that your custom autograd function handles complex tensors correctly and computes gradients as expected. Detailed steps: 1. Implement the `ComplexMultiply` class: - It should inherit from `torch.autograd.Function`. - Define the `forward` static method. - Define the `backward` static method. 2. Use this custom function in a simple neural network to multiply complex tensors and compute a loss: - Initialize two complex input tensors with `requires_grad=True`. - Perform the forward operation using your custom function. - Define a real-valued loss, compute gradients using `.backward()`, and verify the gradients. # Example Below is a skeletal structure to get you started: ```python import torch class ComplexMultiply(torch.autograd.Function): @staticmethod def forward(ctx, input1, input2): # Save tensors for backward pass ctx.save_for_backward(input1, input2) return input1 * input2 @staticmethod def backward(ctx, grad_output): input1, input2 = ctx.saved_tensors # Compute the gradient for each input grad_input1 = grad_output * input2.conj() grad_input2 = grad_output * input1.conj() return grad_input1, grad_input2 # Testing the custom autograd function if __name__ == \\"__main__\\": # Initialize complex tensors input1 = torch.randn(5, dtype=torch.cfloat, requires_grad=True) input2 = torch.randn(5, dtype=torch.cfloat, requires_grad=True) # Apply custom autograd function result = ComplexMultiply.apply(input1, input2) # Define a simple real-valued loss (sum of absolute values squared) loss = result.abs().pow(2).sum() # Backward pass loss.backward() # Print gradients print(f\\"Gradient for input1: {input1.grad}\\") print(f\\"Gradient for input2: {input2.grad}\\") ``` Expected Output - Ensure that the gradient values are correctly computed and match the expected derivative values according to Wirtinger calculus. Constraints 1. You are to handle complex numbers correctly. 2. Your implementation should ensure no in-place operations that might corrupt the computation graph. Note - The test case should validate the correctness of gradients for varying inputs. - You might want to validate against PyTorch’s built-in operations for correctness. Good luck!","solution":"import torch class ComplexMultiply(torch.autograd.Function): @staticmethod def forward(ctx, input1, input2): # Save tensors for backward pass ctx.save_for_backward(input1, input2) return input1 * input2 @staticmethod def backward(ctx, grad_output): input1, input2 = ctx.saved_tensors # Compute the gradient for each input grad_input1 = grad_output * input2.conj() grad_input2 = grad_output * input1.conj() return grad_input1, grad_input2 # Testing the custom autograd function if __name__ == \\"__main__\\": # Initialize complex tensors input1 = torch.randn(5, dtype=torch.cfloat, requires_grad=True) input2 = torch.randn(5, dtype=torch.cfloat, requires_grad=True) # Apply custom autograd function result = ComplexMultiply.apply(input1, input2) # Define a simple real-valued loss (sum of absolute values squared) loss = result.abs().pow(2).sum() # Backward pass loss.backward() # Print gradients print(f\\"Gradient for input1: {input1.grad}\\") print(f\\"Gradient for input2: {input2.grad}\\")"},{"question":"You are tasked with implementing a function that compresses and subsequently decompresses a given byte string using the `zlib` module. The function should ensure that the decompressed data matches the original input data. This exercise will test your understanding of both the compression and decompression functionalities provided by the `zlib` module. # Function Signature ```python def compress_and_decompress(data: bytes, compression_level: int = -1) -> bytes: pass ``` # Input - `data`: A byte string (`bytes`) that needs to be compressed and then decompressed. - `compression_level`: An optional integer (`int`) representing the compression level. Must be between `0` and `9` with `-1` as the default, representing the default compression. # Output - Return a byte string (`bytes`) that is the result of decompressing the compressed data. It should match the original input byte string if decompression is successful without any data loss. # Constraints - The function should handle empty byte strings. - Values for `compression_level` should strictly lie in the range `0` to `9` or be `-1`. - Make sure to handle potential errors/exceptions during compression and decompression. # Example ```python original_data = b\'This is a test string to compress and decompress using zlib.\' compressed_and_decompressed_data = compress_and_decompress(original_data, compression_level=6) assert compressed_and_decompressed_data == original_data print(\\"Data integrity maintained after compression and decompression.\\") ``` # Additional Information - Use the `zlib.compress` function to compress the data. - Use the `zlib.decompress` function to decompress the data. Implement the function `compress_and_decompress` according to the specifications given. Ensure your solution handles various edge cases, such as empty input and different compression levels.","solution":"import zlib def compress_and_decompress(data: bytes, compression_level: int = -1) -> bytes: Compresses and then decompresses the given byte string using the zlib module. Parameters: data (bytes): The data to be compressed and decompressed. compression_level (int): The compression level (between 0 and 9 or -1 for default). Returns: bytes: The decompressed byte string which should be identical to the original. if not isinstance(data, bytes): raise TypeError(\\"Data must be a byte string.\\") if not isinstance(compression_level, int): raise TypeError(\\"Compression level must be an integer.\\") if compression_level < -1 or compression_level > 9: raise ValueError(\\"Compression level must be between 0 and 9, or -1 for default compression.\\") compressed_data = zlib.compress(data, level=compression_level) decompressed_data = zlib.decompress(compressed_data) return decompressed_data"},{"question":"Objective You are required to implement a function that processes numeric data and performs various operations using Python\'s `array` module. This exercise will test your understanding of array type codes, array methods, and manipulation of data using arrays. Problem Statement You have been given a list of integers and a few operations to perform on this list using the `array` module. Your task is to implement a function `process_array(operations: List[Tuple[str, Union[int, List[int]]]]) -> List[int]`, which will process the operations in sequence and return the final state of the array as a list. The operations are provided as a list of tuples, where each tuple contains an operation name followed by the required arguments. The supported operations are: 1. **\\"create\\"**: Create an array with the given type code and initializer list of integers. Example: `(\\"create\\", (\'i\', [1, 2, 3]))` 2. **\\"append\\"**: Append an integer to the array. Example: `(\\"append\\", 4)` 3. **\\"extend\\"**: Extend the array with a list of integers. Example: `(\\"extend\\", [5, 6, 7])` 4. **\\"insert\\"**: Insert an integer at a specific index in the array. Example: `(\\"insert\\", (1, 0))` (insert 0 at index 1) 5. **\\"remove\\"**: Remove the first occurrence of an integer from the array. Example: `(\\"remove\\", 2)` 6. **\\"pop\\"**: Remove and return the element at the specified index (default is the last element). Example: `(\\"pop\\", 1)` (remove element at index 1) 7. **\\"reverse\\"**: Reverse the array. Example: `(\\"reverse\\",)` 8. **\\"byteswap\\"**: Perform a byteswap of the array items. Example: `(\\"byteswap\\",)` Constraints 1. The array type code is always valid and relates to an integer type (e.g., \'b\', \'B\', \'h\', \'H\', \'i\', \'I\', \'l\', \'L\', \'q\', \'Q\'). 2. The initializer list for \\"create\\" operation contains valid integers. 3. The operations list will always start with a \\"create\\" operation. 4. Removal operations (\\"remove\\") will always work on existing elements. 5. There will be no operations that result in invalid indices. Function Signature ```python from typing import List, Tuple, Union def process_array(operations: List[Tuple[str, Union[int, List[int]]]]) -> List[int]: pass ``` Example ```python operations = [ (\\"create\\", (\'i\', [1, 2, 3, 4])), (\\"append\\", 5), (\\"extend\\", [6, 7]), (\\"insert\\", (1, 0)), (\\"remove\\", 3), (\\"pop\\", 2), (\\"reverse\\",), (\\"byteswap\\",) ] output = process_array(operations) print(output) # Output may vary depending on the system endianess ``` Notes - You may assume that the array operations will always be valid. - The byteswap operation may affect the endianness of the numbers, and the output may vary based on the system’s architecture. You do not need to consider the system endianness for this exercise.","solution":"from typing import List, Tuple, Union import array def process_array(operations: List[Tuple[str, Union[str, int, List[int], Tuple[int, int]]]]) -> List[int]: arr = None for operation in operations: if operation[0] == \\"create\\": type_code, initializer = operation[1] arr = array.array(type_code, initializer) elif operation[0] == \\"append\\": arr.append(operation[1]) elif operation[0] == \\"extend\\": arr.extend(operation[1]) elif operation[0] == \\"insert\\": index, value = operation[1] arr.insert(index, value) elif operation[0] == \\"remove\\": arr.remove(operation[1]) elif operation[0] == \\"pop\\": arr.pop(operation[1]) elif operation[0] == \\"reverse\\": arr.reverse() elif operation[0] == \\"byteswap\\": arr.byteswap() return arr.tolist()"},{"question":"# Coding Assessment: Implementing an Out-of-Core Learning Pipeline Objective The task is to implement an out-of-core learning pipeline using scikit-learn, demonstrating your understanding of streaming instances, feature extraction, and incremental learning. Problem Statement You are given a large dataset stored in multiple CSV files, each containing a portion of the data. The dataset is too large to fit into your computer\'s main memory (RAM) all at once. You need to build a machine learning pipeline that reads the data in chunks, extracts features using a stateless vectorizer, and trains an incremental learning algorithm. Requirements 1. **Reading Data**: Implement a generator function that reads data in mini-batches from CSV files. 2. **Feature Extraction**: Use `HashingVectorizer` from scikit-learn to convert text data into a numeric format. 3. **Incremental Learning**: Use `SGDClassifier` from scikit-learn to incrementally train a classification model. Input and Output Formats - **Input**: - Folder path containing multiple CSV files. - Mini-batch size for reading the data. - CSV files each containing two columns: \\"text\\" (string) and \\"label\\" (int). - **Output**: - Trained `SGDClassifier` model after processing all data. Constraints - Each CSV file must fit into memory. - Mini-batch size must be a positive integer. Performance Requirements - Efficiently read and process data in mini-batches to avoid using excessive memory. Example ```python import os import pandas as pd from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def data_streamer(folder_path, batch_size): # Generator that yields mini-batches from CSV files in the folder files = os.listdir(folder_path) for file_name in files: file_path = os.path.join(folder_path, file_name) df = pd.read_csv(file_path) for i in range(0, len(df), batch_size): yield df.iloc[i:i + batch_size] def build_out_of_core_pipeline(folder_path, batch_size): vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() data_gen = data_streamer(folder_path, batch_size) for mini_batch in data_gen: X = vectorizer.transform(mini_batch[\'text\']) y = mini_batch[\'label\'] classifier.partial_fit(X, y, classes=[0, 1]) return classifier # Example usage: # folder_path = \\"path_to_csv_files\\" # batch_size = 1000 # model = build_out_of_core_pipeline(folder_path, batch_size) ``` Submission Submit a Python script that: 1. Implements the `data_streamer` function. 2. Implements the `build_out_of_core_pipeline` function. 3. Demonstrates the pipeline with at least three small CSV files for testing purposes.","solution":"import os import pandas as pd from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def data_streamer(folder_path, batch_size): Generator that yields mini-batches from CSV files in the folder. :param folder_path: Path to the folder containing CSV files. :param batch_size: Number of rows to yield per mini-batch. files = os.listdir(folder_path) for file_name in files: file_path = os.path.join(folder_path, file_name) df = pd.read_csv(file_path) for i in range(0, len(df), batch_size): yield df.iloc[i:i + batch_size] def build_out_of_core_pipeline(folder_path, batch_size): Builds and trains an out-of-core learning pipeline using SGDClassifier. :param folder_path: Path to the folder containing CSV files. :param batch_size: Number of rows to process at a time. :return: Trained SGDClassifier model. vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() data_gen = data_streamer(folder_path, batch_size) for mini_batch in data_gen: X = vectorizer.transform(mini_batch[\'text\']) y = mini_batch[\'label\'] classifier.partial_fit(X, y, classes=[0, 1]) return classifier"},{"question":"# Create a Custom Named Tuple in Python **Objective:** You are required to create a custom named tuple in Python called `Student`, which stores the following details for a student: 1. Name (string) 2. Age (integer) 3. Grades (a tuple of integers representing grades in different subjects) Additionally, you need to implement a function that performs various operations on a list of `Student` named tuples. **Function Specification:** 1. **Function Name:** `process_students` 2. **Parameters:** - `students`: A list of `Student` named tuples. 3. **Output:** - Returns a dictionary with the following keys and values: - `\'total_students\'`: Total number of students (integer). - `\'average_grades\'`: A tuple containing the average grade for each subject across all students (tuple of floats). - `\'oldest_student\'`: The name of the oldest student (string). - `\'unique_ages\'`: A set of all unique ages of the students (set of integers). **Constraints:** - Implement this using the `collections.namedtuple` to define the `Student`. - You can assume that the grades tuple for each student has the same length. **Performance Requirements:** - The function should efficiently handle up to 10,000 students with up to 10 subjects in each grades tuple. **Example:** ```python from collections import namedtuple # Define the Student named tuple Student = namedtuple(\'Student\', [\'name\', \'age\', \'grades\']) def process_students(students): # Your implementation here pass # Example usage students = [ Student(name=\'Alice\', age=20, grades=(85, 92, 78)), Student(name=\'Bob\', age=22, grades=(79, 85, 88)), Student(name=\'Charlie\', age=21, grades=(92, 90, 85)), ] result = process_students(students) print(result) # Example output format: {\'total_students\': 3, \'average_grades\': (85.33, 89, 83.67), \'oldest_student\': \'Bob\', \'unique_ages\': {20, 21, 22}} ``` **Notes:** - Ensure that the average grades are calculated to two decimal places. - Do not use any external libraries other than `collections.namedtuple`.","solution":"from collections import namedtuple # Define the Student named tuple Student = namedtuple(\'Student\', [\'name\', \'age\', \'grades\']) def process_students(students): Processes a list of Student named tuples and returns relevant statistics. Parameters: students (list): A list of Student named tuples. Returns: dict: A dictionary with statistics about the students. total_students = len(students) if total_students == 0: return { \'total_students\': 0, \'average_grades\': (), \'oldest_student\': None, \'unique_ages\': set() } oldest_student = max(students, key=lambda s: s.age).name unique_ages = {student.age for student in students} # Transpose the grades to sum them by subject grade_sums = [0] * len(students[0].grades) for student in students: for i, grade in enumerate(student.grades): grade_sums[i] += grade average_grades = tuple(round(total / total_students, 2) for total in grade_sums) return { \'total_students\': total_students, \'average_grades\': average_grades, \'oldest_student\': oldest_student, \'unique_ages\': unique_ages }"},{"question":"# Advanced Python OOP Challenge You are tasked with implementing a mini-library that deals with custom instance method and method objects similar to Python\'s internal API. Specifically, you need to implement the following functionalities: 1. **Check Instance Method**: A function `is_instance_method` that checks if a given object is an instance method. 2. **Create Instance Method**: A function `create_instance_method` that creates a new instance method object from a given function. 3. **Get Instance Method Function**: A function `get_instance_method_function` that retrieves the function associated with an instance method. 4. **Check Method**: A function `is_method` that checks if a given object is a method. 5. **Create Method**: A function `create_method` that creates a new method object from a given function and instance. 6. **Get Method Function**: A function `get_method_function` that retrieves the function associated with a method. 7. **Get Method Self**: A function `get_method_self` that retrieves the instance associated with a method. Function Specifications: - `is_instance_method(obj: Any) -> bool` - **Input**: `obj` of any type. - **Output**: Returns `True` if `obj` is an instance method, otherwise `False`. - `create_instance_method(func: Callable) -> Callable` - **Input**: `func` is a callable function. - **Output**: Returns a new instance method object. - `get_instance_method_function(im: Callable) -> Callable` - **Input**: `im` is an instance method. - **Output**: Returns the function object associated with the instance method. - `is_method(obj: Any) -> bool` - **Input**: `obj` of any type. - **Output**: Returns `True` if `obj` is a method, otherwise `False`. - `create_method(func: Callable, self_instance: Any) -> Callable` - **Input**: `func` is a callable function, `self_instance` is the instance to which the method is bound. - **Output**: Returns a new method object. - `get_method_function(meth: Callable) -> Callable` - **Input**: `meth` is a method. - **Output**: Returns the function object associated with the method. - `get_method_self(meth: Callable) -> Any` - **Input**: `meth` is a method. - **Output**: Returns the instance associated with the method. Constraints: - Do not use the built-in Python methods `types.MethodType`. - Ensure proper handling of inputs and outputs as specified. - The methods created should mimic the behavior of Python\'s internal API as detailed in the documentation. # Example Usage: ```python def demo_func(): print(\\"This is a demo function\\") class DemoClass: def method(self): print(\\"This is a method\\") instance = DemoClass() # Example: Instance Method inst_method = create_instance_method(demo_func) print(is_instance_method(inst_method)) # Should return True print(get_instance_method_function(inst_method) is demo_func) # Should return True # Example: Method method = create_method(DemoClass.method, instance) print(is_method(method)) # Should return True print(get_method_function(method) is DemoClass.method) # Should return True print(get_method_self(method) is instance) # Should return True ``` Implement the functions according to the specifications and ensure they pass the provided example usages.","solution":"def is_instance_method(obj): Checks if the given object is an instance method. return hasattr(obj, \'__self__\') and obj.__self__ is not None def create_instance_method(func): Creates a new instance method object from a given function. class Dummy: pass return func.__get__(Dummy(), Dummy) def get_instance_method_function(im): Retrieves the function associated with an instance method. return im.__func__ def is_method(obj): Checks if the given object is a method. return hasattr(obj, \'__self__\') and callable(obj) def create_method(func, self_instance): Creates a new method object from a given function and instance. return func.__get__(self_instance, self_instance.__class__) def get_method_function(meth): Retrieves the function associated with a method. return meth.__func__ def get_method_self(meth): Retrieves the instance associated with a method. return meth.__self__"},{"question":"# Coding Assessment Question Objective Implement a function to compare two directory trees and generate a detailed report on their differences, including any extra or missing files, as well as differences in the contents of common files. The function should also print out any errors encountered during the comparison. Task Write a Python function `compare_directory_trees(dir1: str, dir2: str) -> dict` that compares the directory trees rooted at `dir1` and `dir2`. # Input - `dir1`: A string representing the path to the first directory. - `dir2`: A string representing the path to the second directory. # Output - A dictionary with the following structure: ```python { \\"left_only\\": [], # List of files and subdirectories only in dir1 \\"right_only\\": [], # List of files and subdirectories only in dir2 \\"same_files\\": [], # List of files with identical content in both directories \\"diff_files\\": [], # List of files with differing content in both directories \\"errors\\": [] # List of files that could not be compared due to errors } ``` # Constraints - You may assume that you have read permissions for all files and directories. - Use shallow comparisons for file content comparison (i.e., comparing files based on metadata such as file type, size, and modification time). # Example ```python dir1 = \\"/path/to/directory1\\" dir2 = \\"/path/to/directory2\\" result = compare_directory_trees(dir1, dir2) ``` The `result` dictionary should contain lists of files and directories as described above. Any errors encountered during the comparison should be listed in the `errors` field. Implementation Notes - Use the `filecmp` module, specifically the `dircmp` class, to perform the directory comparisons. - Recursively compare subdirectories and include the results in the final output dictionary. - Print any errors encountered during the comparison process. Additional Information Refer to the `filecmp` module documentation for details on how to use the `dircmp` class and its methods. # Hints - Utilize the `subdirs` attribute of the `dircmp` class to handle recursive comparison of subdirectories. - The `dircmp` class provides attributes such as `left_only`, `right_only`, `same_files`, `diff_files`, and `funny_files` that will be useful for constructing the output dictionary.","solution":"import os from filecmp import dircmp def compare_directory_trees(dir1: str, dir2: str) -> dict: Compare two directory trees and generate a report on their differences. Parameters: - dir1 (str): Path to the first directory. - dir2 (str): Path to the second directory. Returns: - dict: A dictionary with differences between the two directories. def compare_dirs(dcmp, result): result[\'left_only\'].extend(dcmp.left_only) result[\'right_only\'].extend(dcmp.right_only) result[\'same_files\'].extend(dcmp.same_files) result[\'diff_files\'].extend(dcmp.diff_files) for error in dcmp.funny_files: result[\'errors\'].append(f\\"Error in \'{os.path.join(dcmp.left, error)}\' if in left or \'{os.path.join(dcmp.right, error)}\' if in right\\") for sub_dcmp in dcmp.subdirs.values(): compare_dirs(sub_dcmp, result) result = { \'left_only\': [], \'right_only\': [], \'same_files\': [], \'diff_files\': [], \'errors\': [] } try: dcmp = dircmp(dir1, dir2) compare_dirs(dcmp, result) except Exception as e: result[\'errors\'].append(str(e)) return result"},{"question":"# Custom Importer Implementation Objective: Implement a custom module importer using Python\'s `importlib`. You are required to create a class that imports a Python module from a specified file path, even if that module is not present in the standard location or Python path. Your implementation should consider error handling and demonstrate importing a function from the module and executing it. Detailed Requirements: 1. **Class Name**: `CustomImporter` 2. **Method**: `import_and_execute` - **Input**: - `module_path` (str): The path to the module file (e.g., `/path/to/module.py`). - `func_name` (str): The name of the function to import and execute (e.g., `my_function`). - **Output**: - The result of the executed function. - **Constraints**: - Assume the function to be executed does not require arguments for this task. - The module file specified must exist at the provided path, and the function must be defined within the module. - If the module or function cannot be found or imported, raise an appropriate exception with a meaningful error message. Example: ```python # Suppose this is in \'/path/to/module.py\' def my_function(): return \\"Hello from my_function!\\" # Usage of CustomImporter class importer = CustomImporter() result = importer.import_and_execute(\'/path/to/module.py\', \'my_function\') print(result) # Output should be \\"Hello from my_function!\\" ``` Notes: - You can use `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec` to achieve the custom import. - Ensure you handle file and module import errors gracefully. Performance Requirements: - The solution should not load or execute unnecessary modules and should avoid manipulating the global module namespace.","solution":"import importlib.util import os class CustomImporter: def import_and_execute(self, module_path, func_name): Imports a module from a specified file path and executes a function from that module. :param module_path: Path to the module file (e.g., /path/to/module.py) :param func_name: Name of the function to import and execute (e.g., my_function) :return: The result of the executed function :raises ImportError: If the module or function cannot be imported if not os.path.isfile(module_path): raise FileNotFoundError(f\\"The specified module file does not exist: {module_path}\\") module_name = os.path.splitext(os.path.basename(module_path))[0] # Load the module from the specified file location spec = importlib.util.spec_from_file_location(module_name, module_path) if spec is None: raise ImportError(f\\"Could not create a module spec for {module_name} from {module_path}\\") module = importlib.util.module_from_spec(spec) try: spec.loader.exec_module(module) except Exception as e: raise ImportError(f\\"Could not load the module from {module_path}\\") from e if not hasattr(module, func_name): raise ImportError(f\\"Function {func_name} is not found in the module\\") func = getattr(module, func_name) return func()"},{"question":"# Advanced Coding Task: Implementing a Fraction Calculator Using Decimal with Specific Precision **Objective**: Your task is to create a fraction calculator that can handle basic arithmetic operations (+, -, *, /) using the `decimal` module to ensure precision, especially with fractional parts. The calculator should support operations on fractions where the results are displayed with a specified number of decimal places. **Instructions**: 1. Implement a function `calculate_fraction(operation: str, fraction1: str, fraction2: str, precision: int) -> str` that takes: - `operation`: A string representing the arithmetic operation. Accepted values are \'add\', \'subtract\', \'multiply\', \'divide\'. - `fraction1` and `fraction2`: Strings representing the fractions in the form \\"numerator/denominator\\" (e.g., \\"3/4\\"). - `precision`: An integer indicating the number of decimal places for result. 2. Use the `decimal` module to ensure precise arithmetic operations and handle exceptional cases. 3. The function should: - Parse `fraction1` and `fraction2` into their respective numerators and denominators. - Perform the specified arithmetic operation using the `decimal` module. - Return the result as a string rounded to the specified number of decimal places. - Handle cases where denominators are zero and return an appropriate message. - Handle invalid input gracefully. **Example**: ```python # Example 1 operation = \\"add\\" fraction1 = \\"1/3\\" fraction2 = \\"1/4\\" precision = 4 result = calculate_fraction(operation, fraction1, fraction2, precision) print(result) # \\"0.5833\\" # Example 2 operation = \\"divide\\" fraction1 = \\"1/2\\" fraction2 = \\"4/5\\" precision = 6 result = calculate_fraction(operation, fraction1, fraction2, precision) print(result) # \\"0.625000\\" # Example 3 operation = \\"multiply\\" fraction1 = \\"0/1\\" fraction2 = \\"1/0\\" precision = 2 result = calculate_fraction(operation, fraction1, fraction2, precision) print(result) # \\"Error: Division by zero.\\" ``` **Constraints**: - You may assume the fractions provided are valid (consist of integers in the format \\"numerator/denominator\\"). - Precision will be a non-negative integer. - Ensure your function is efficient and handles edge cases (like division by zero) appropriately. **Hints**: - Use `Decimal` for arithmetic and pay attention to the context precision for rounding. - Consider creating helper functions for parsing the fraction and handling the `decimal` context setup. **Performance**: - Your implementation should efficiently handle typical input sizes and operations, keeping precision management in focus.","solution":"from decimal import Decimal, getcontext def calculate_fraction(operation: str, fraction1: str, fraction2: str, precision: int) -> str: getcontext().prec = precision + 5 # Set precision slightly higher to handle intermediate steps def parse_fraction(fraction): numerator, denominator = map(int, fraction.split(\'/\')) return Decimal(numerator) / Decimal(denominator) try: frac1 = parse_fraction(fraction1) frac2 = parse_fraction(fraction2) except ZeroDivisionError: return \\"Error: Division by zero.\\" except Exception: return \\"Error: Invalid input.\\" result = None try: if operation == \\"add\\": result = frac1 + frac2 elif operation == \\"subtract\\": result = frac1 - frac2 elif operation == \\"multiply\\": result = frac1 * frac2 elif operation == \\"divide\\": result = frac1 / frac2 else: return \\"Error: Unknown operation.\\" except ZeroDivisionError: return \\"Error: Division by zero.\\" result = result.quantize(Decimal(\'1.\' + \'0\' * precision)) return str(result)"},{"question":"**Question: Implementing a Dynamic Package Loader and Explorer** In this task, you will create two functions leveraging the `pkgutil` module: `extend_package_search_path()` and `find_submodules()`. 1. **extend_package_search_path(package_name: str, additional_paths: list) -> list:** This function should dynamically extend the search path of a given package by using `pkgutil.extend_path`. It takes a package\'s name and a list of additional directory paths that you want to add to the package\'s existing search path. * **Input:** - `package_name`: A string representing the name of the package. - `additional_paths`: A list of strings, each representing a file system path. * **Output:** - A list representing the new extended search path of the package. * **Constraints:** - The `package_name` should be an existing package. - Each path in `additional_paths` should be a valid directory. - The function should raise appropriate exceptions if the inputs are invalid. 2. **find_submodules(package_name: str) -> list:** This function should list all submodules of a given package. Use `pkgutil.iter_modules` to discover submodules. * **Input:** - `package_name`: A string representing the name of the package. * **Output:** - A list of strings, each representing the name of a submodule within the package. * **Constraints:** - The `package_name` should be an existing package. - If the package contains no submodules, return an empty list. * **Example Usage:** ```python # Assuming \'numpy\' is an installed package and you want to extend its path extended_path = extend_package_search_path(\\"numpy\\", [\\"/additional/libs/numpy\\"]) print(extended_path) # Should print the new extended path list which includes the additional /libs/numpy # Assuming \'collections\' is a built-in package submodules = find_submodules(\\"collections\\") print(submodules) # Should print submodules like [\'abc\', \'_collections_abc\', etc.] ``` * **Notes:** - You may need to import necessary modules such as `sys` and use them appropriately. - Proper error handling through exceptions will be appreciated. - Make sure to import and use `pkgutil` and any other standard modules needed to complete the task. Implement the functions `extend_package_search_path` and `find_submodules` below: ```python def extend_package_search_path(package_name: str, additional_paths: list) -> list: # Your code here pass def find_submodules(package_name: str) -> list: # Your code here pass ```","solution":"import pkgutil import sys import os def extend_package_search_path(package_name: str, additional_paths: list) -> list: Extends the search path of the given package by adding the provided additional paths. Args: package_name (str): The name of the package. additional_paths (list): List of additional paths to add. Returns: list: The new extended search path of the package. if not isinstance(package_name, str): raise TypeError(\\"package_name must be a string\\") if not isinstance(additional_paths, list) or not all(isinstance(path, str) for path in additional_paths): raise TypeError(\\"additional_paths must be a list of strings\\") # Check if the given package is valid loader = pkgutil.find_loader(package_name) if loader is None: raise ImportError(f\\"Package \'{package_name}\' not found\\") # Verify all additional paths are directories for path in additional_paths: if not os.path.isdir(path): raise ValueError(f\\"Path \'{path}\' is not a valid directory\\") # Extend the package\'s search path package = __import__(package_name) new_path = list(package.__path__) for path in additional_paths: new_path.append(path) package.__path__ = pkgutil.extend_path(new_path, package_name) return list(package.__path__) def find_submodules(package_name: str) -> list: Lists all submodules of the given package. Args: package_name (str): The name of the package. Returns: list: List of submodule names within the package. if not isinstance(package_name, str): raise TypeError(\\"package_name must be a string\\") loader = pkgutil.find_loader(package_name) if loader is None: raise ImportError(f\\"Package \'{package_name}\' not found\\") package = __import__(package_name) submodules = [name for _, name, ispkg in pkgutil.iter_modules(package.__path__)] return submodules"},{"question":"# Advanced Seaborn Plotting Background Seaborn is a powerful visualization library for Python that allows the creation of informative and attractive statistical graphics. The `seaborn.objects` submodule enhances this by providing a more flexible and composable approach to building complex plots with multiple layers. This task will assess your ability to leverage these features to create meaningful visualizations. Task You are given two datasets: `penguins` and `diamonds`. Your task is to create a complex plot comparing specific aspects of these datasets using the `seaborn.objects` submodule. 1. **Penguins Dataset**: Visualize the distribution of body mass (`body_mass_g`) for each species (`species`) by: - Using `Dots` to represent individual data points with jitter to reduce overlap. - Showing the interquartile range (IQR) using `Range` and shifting the range slightly to avoid overlapping with the dots. 2. **Diamonds Dataset**: Visualize the distribution of carat values (`carat`) across different clarity levels (`clarity`) by: - Using `Dots` to represent individual data points with jitter. - Highlighting the IQR with `Range` and shifting it to avoid overlapping with the dots. Implementation Implement the function `create_complex_plots(penguins, diamonds)` that takes in two datasets and generates two separate plots using seaborn: ```python import seaborn.objects as so def create_complex_plots(penguins, diamonds): Creates and displays complex plots for penguins and diamonds datasets using seaborn.objects. Parameters: - penguins (DataFrame): The penguins dataset. - diamonds (DataFrame): The diamonds dataset. Returns: None # Part 1: Penguins Dataset Plot penguin_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) penguin_plot.show() # Part 2: Diamonds Dataset Plot diamond_plot = ( so.PPlot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) diamond_plot.show() # Load datasets (Already provided in the example) from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Call the function to create and show the plots create_complex_plots(penguins, diamonds) ``` Constraints - Ensure the plots are clear and readable, avoiding any significant overlap between different plot components. - Use the `so.Jitter()` and `so.Shift()` transformations as specified to enhance the visualization. Expected Output Two plots: 1. A plot showing the distribution of body mass in grams for each penguin species with dots and interquartile ranges. 2. A plot showing the distribution of carat weights across clarity levels with dots and interquartile ranges. Notes - You are encouraged to explore the seaborn objects documentation and experiment to understand the nuances of these features better. - Make sure your plotting environments support displaying the plots immediately. If using a Jupyter notebook, ensure `%matplotlib inline` or relevant magic commands are used.","solution":"import seaborn.objects as so import matplotlib.pyplot as plt def create_complex_plots(penguins, diamonds): Creates and displays complex plots for penguins and diamonds datasets using seaborn.objects. Parameters: - penguins (DataFrame): The penguins dataset. - diamonds (DataFrame): The diamonds dataset. Returns: None # Part 1: Penguins Dataset Plot penguin_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) penguin_plot.show() # Part 2: Diamonds Dataset Plot diamond_plot = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) diamond_plot.show()"},{"question":"# Email Management Utility using `poplib` Module Objective Design and implement a Python utility that interacts with a POP3 server to manage emails. Specifically, the utility should: 1. Connect to a POP3 server using both plain and SSL connections. 2. Authenticate the user. 3. Retrieve and list the email messages. 4. Allow the user to delete specific emails. 5. Handle errors gracefully. Function Signature ```python def manage_emails(server: str, username: str, password: str, use_ssl: bool = False) -> None: Connect to a POP3 server, authenticate using provided credentials, and allow the user to manage emails. :param server: The address of the POP3 server. :param username: The username for authentication. :param password: The password for authentication. :param use_ssl: Boolean indicating whether to use SSL for the connection. Defaults to False. :return: None ``` Requirements 1. If `use_ssl` is `True`, use the `POP3_SSL` class to connect to the server; otherwise, use the `POP3` class. 2. Authenticate the user with the provided `username` and `password`. 3. Retrieve the list of email messages and display the message number and size. 4. Prompt the user to select an email to delete by message number. 5. Allow the user to quit the process by typing \'quit\'. 6. Ensure the mailbox is properly unlocked and the connection is closed after operations. 7. Handle connection and authentication errors, and raise `poplib.error_proto` as necessary. 8. Implement basic error checking as shown in the example. Example Usage ```python manage_emails(\\"pop.example.com\\", \\"username\\", \\"password\\", use_ssl=True) ``` Constraints - The function does not return any value; all interactions are through console input and output. - Assume a maximum of 1000 emails in the mailbox. Performance Requirements - The utility should handle the connection and disconnection securely and efficiently. - When listing messages, avoid retrieving the full content until the user selects a message to delete. Hints - Use `POP3.list()` to get the list of messages. - Use `POP3.dele()` to delete a specific message. - Use proper error handling and cleanup (`POP3.quit()`) to ensure the connection is terminated correctly.","solution":"import poplib from getpass import getpass def manage_emails(server: str, username: str, password: str, use_ssl: bool = False) -> None: Connect to a POP3 server, authenticate using provided credentials, and allow the user to manage emails. :param server: The address of the POP3 server. :param username: The username for authentication. :param password: The password for authentication. :param use_ssl: Boolean indicating whether to use SSL for the connection. Defaults to False. :return: None try: if use_ssl: mail = poplib.POP3_SSL(server) else: mail = poplib.POP3(server) mail.user(username) mail.pass_(password) email_num, total_size = mail.stat() print(f\\"Number of emails: {email_num}, Total size: {total_size} bytes\\") response, messages, _ = mail.list() print(\\"Messages list:\\") for msg in messages: print(msg.decode()) while True: user_input = input(\\"nEnter message number to delete or \'quit\' to exit: \\").strip() if user_input.lower() == \'quit\': break try: msg_num = int(user_input) if 1 <= msg_num <= email_num: mail.dele(msg_num) print(f\\"Message {msg_num} marked for deletion.\\") else: print(f\\"Invalid message number. Please enter a number between 1 and {email_num}.\\") except ValueError: print(\\"Invalid input. Please enter a valid message number or \'quit\' to exit.\\") mail.quit() print(\\"Connection closed.\\") except poplib.error_proto as e: print(f\\"POP3 error occurred: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are assigned the task of creating a utility function for managing ZIP archives. Your function will need to compress multiple files into a ZIP archive and then provide a report containing details about the archived files. Specifically, the function should take in multiple file paths, compress them using the specified compression method, and then generate a report listing each compressed file name, its original size, and its compressed size. # Function Signature ```python def compress_files(file_paths: list, zip_name: str, compression_method: int) -> str: Compress multiple files into a ZIP archive and generate a report. Parameters: file_paths (list): A list of file paths to be compressed. zip_name (str): The name of the output ZIP archive. compression_method (int): The compression method to use (zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA). Returns: str: A string report listing each file name, its original size, and its compressed size. # Example Usage ```python file_paths = [\'file1.txt\', \'file2.txt\', \'image.png\'] zip_name = \'archive.zip\' compression_method = zipfile.ZIP_DEFLATED report = compress_files(file_paths, zip_name, compression_method) print(report) ``` # Constraints 1. You need to handle invalid file paths by skipping them and recording an error message in the report. 2. If the specified ZIP archive file already exists, it should be overwritten. 3. You should use context managers to ensure that files and ZIP archives are properly closed after operations. 4. You must handle various compression methods specified by the `compression_method` parameter. 5. If the specified compression method is not supported, raise a `ValueError`. # Detailed Requirements 1. **Input Parameters:** - `file_paths`: List of file paths (strings) that need to be compressed. - `zip_name`: Name of the output ZIP archive. - `compression_method`: The numerical constant for the specified compression method (`ZIP_STORED`, `ZIP_DEFLATED`, `ZIP_BZIP2`, `ZIP_LZMA`). 2. **Output:** - A report string listing each file compressed with its original size and compressed size. - Format for each line in the report: \\"Filename: {file_name}, Original Size: {original_size} bytes, Compressed Size: {compressed_size} bytes\\". - If a file is not found or inaccessible, include an error message: \\"Filename: {file_name} could not be compressed due to an error.\\" 3. **Example Report:** ``` Filename: file1.txt, Original Size: 12345 bytes, Compressed Size: 6789 bytes Filename: file2.txt, Original Size: 54321 bytes, Compressed Size: 9876 bytes Filename: image.png could not be compressed due to an error ``` # Hints - Use the `zipfile.ZipFile` class along with its `write` method to add files to the archive. - Utilize the `os.path` module to check file existence and calculate file sizes. - Ensure the use of proper exception handling to manage file I/O errors and unsupported compression methods. Happy coding!","solution":"import os import zipfile def compress_files(file_paths: list, zip_name: str, compression_method: int) -> str: Compress multiple files into a ZIP archive and generate a report. Parameters: file_paths (list): A list of file paths to be compressed. zip_name (str): The name of the output ZIP archive. compression_method (int): The compression method to use (zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA). Returns: str: A string report listing each file name, its original size, and its compressed size. # Check if the compression method is supported if compression_method not in [zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA]: raise ValueError(\\"Unsupported compression method\\") report_lines = [] # Create the ZIP archive with zipfile.ZipFile(zip_name, \'w\', compression=compression_method) as zipf: for file_path in file_paths: try: if not os.path.isfile(file_path): raise FileNotFoundError(f\\"File {file_path} not found.\\") original_size = os.path.getsize(file_path) zipf.write(file_path, os.path.basename(file_path)) compressed_size = zipf.getinfo(os.path.basename(file_path)).compress_size report_lines.append(f\\"Filename: {file_path}, Original Size: {original_size} bytes, Compressed Size: {compressed_size} bytes\\") except Exception as e: report_lines.append(f\\"Filename: {file_path} could not be compressed due to an error: {str(e)}\\") return \\"n\\".join(report_lines)"},{"question":"# Data Compression and Archiving Task **Objective:** Implement a function that takes a directory path and creates a compressed archive (zip format) of all files inside the specified directory. The function should also be able to extract the contents of a provided archive back into a directory. **Function Signatures:** ```python def create_archive(directory_path: str, archive_path: str) -> None: Creates a compressed zip archive of all files in the specified directory. Parameters: - directory_path (str): The path to the directory to compress. - archive_path (str): The output path for the created zip archive. Returns: - None pass def extract_archive(archive_path: str, extract_path: str) -> None: Extracts the contents of a zip archive into the specified directory. Parameters: - archive_path (str): The path to the zip archive to decompress. - extract_path (str): The directory to extract the contents into. Returns: - None pass ``` **Input/Output:** - `create_archive` function: - **Input:** - `directory_path` (str): Path to the directory to be compressed. - `archive_path` (str): Path where the zip archive will be saved. - **Output:** None (Creates the zip archive at **archive_path**) - `extract_archive` function: - **Input:** - `archive_path` (str): Path to the compressed zip archive. - `extract_path` (str): Path to the directory where files should be extracted. - **Output:** None (Extracts files into **extract_path**) **Constraints:** - Ensure that the `create_archive` function includes all files and subdirectories within the specified directory. - The `extract_archive` function should handle cases where the archive might not exist or is corrupted gracefully by raising appropriate exceptions with descriptive messages. - Aim to optimize the operations to handle large directories or archives efficiently in terms of time and space. **Sample Usage:** ```python # Create a zip archive of the \'example_dir\' directory and save it as \'example.zip\' create_archive(\'example_dir\', \'example.zip\') # Extract the contents of \'example.zip\' into the \'extracted_dir\' directory extract_archive(\'example.zip\', \'extracted_dir\') ``` Implement these two functions to demonstrate your understanding of working with compressed files in Python.","solution":"import zipfile import os def create_archive(directory_path: str, archive_path: str) -> None: Creates a compressed zip archive of all files in the specified directory. Parameters: - directory_path (str): The path to the directory to compress. - archive_path (str): The output path for the created zip archive. Returns: - None with zipfile.ZipFile(archive_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, _, files in os.walk(directory_path): for file in files: full_path = os.path.join(root, file) relative_path = os.path.relpath(full_path, start=directory_path) zipf.write(full_path, relative_path) def extract_archive(archive_path: str, extract_path: str) -> None: Extracts the contents of a zip archive into the specified directory. Parameters: - archive_path (str): The path to the zip archive to decompress. - extract_path (str): The directory to extract the contents into. Returns: - None try: with zipfile.ZipFile(archive_path, \'r\') as zipf: zipf.extractall(extract_path) except zipfile.BadZipFile: raise Exception(\\"The provided zip file is corrupted or not a valid zip file.\\") except FileNotFoundError: raise Exception(\\"The provided zip file does not exist.\\")"},{"question":"**Objective:** Your task is to implement a Python function `process_pth_file(pth_file_path)` that reads a path configuration file (\\".pth\\" file) and returns a list of valid directories to be added to \\"sys.path\\", excluding any lines that are comments or that refer to non-existing paths. # Function Signature ```python def process_pth_file(pth_file_path: str) -> list: pass ``` # Input - `pth_file_path` (str): The file path of the \\".pth\\" file to be processed. This file will contain paths to be added to \\"sys.path\\". # Output - A list of strings where each string is a valid directory path to be added to \\"sys.path\\", sorted in the order they appear in the \\".pth\\" file. # Constraints 1. The function should skip over any lines that are empty or start with the \\"#\\" character (comments). 2. The function should check if each path in the \\".pth\\" file exists in the filesystem. Only existing directory paths should be included in the output list. 3. If a line starts with \\"import\\", the function should raise a ValueError indicating that executable code is not supported in this context. # Example Usage Suppose the content of `sample.pth` is as follows: ``` # Example .pth file /path/to/existing/dir1 /path/to/non_existing/dir /path/to/existing/dir2 import os ``` Calling the function: ```python print(process_pth_file(\'sample.pth\')) ``` Should produce the output: ```python [\'/path/to/existing/dir1\', \'/path/to/existing/dir2\'] ``` # Notes - Use the `os.path.exists()` function to check if a directory path exists. - Ensure the function handles file reading and any potential exceptions. **Tip**: - Think about edge cases where the \\".pth\\" file could be empty or contain only comments. # Additional Information - You may assume that the input file paths are always valid file paths that the function has permission to read. Happy Coding!","solution":"import os def process_pth_file(pth_file_path: str) -> list: try: with open(pth_file_path, \'r\') as file: lines = file.readlines() except Exception as e: raise e valid_paths = [] for line in lines: stripped_line = line.strip() if not stripped_line or stripped_line.startswith(\'#\'): continue if stripped_line.startswith(\'import\'): raise ValueError(\\"Executable code is not supported in this context.\\") if os.path.exists(stripped_line): valid_paths.append(stripped_line) return valid_paths"},{"question":"**Coding Assessment Question:** # Objective: Implement a `Calculator` class that uses underlying C-based Python numeric operations for various arithmetic, bitwise, and type conversion tasks. This will demonstrate your understanding of the numeric operations provided by the package. # Description: 1. **Class**: `Calculator` 2. **Methods**: - `add(self, a, b)`: Returns the sum of `a` and `b`. - `subtract(self, a, b)`: Returns the result of `a - b`. - `multiply(self, a, b)`: Returns the product of `a` and `b`. - `divide(self, a, b)`: Returns the floating-point division of `a` by `b`. - `floor_divide(self, a, b)`: Returns the floor division of `a` by `b`. - `modulus(self, a, b)`: Returns the remainder when `a` is divided by `b`. - `power(self, a, b)`: Returns `a` raised to the power `b`. - `negate(self, a)`: Returns the negation of `a`. - `bitwise_and(self, a, b)`: Returns the bitwise AND of `a` and `b`. - `bitwise_or(self, a, b)`: Returns the bitwise OR of `a` and `b`. - `bitwise_xor(self, a, b)`: Returns the bitwise XOR of `a` and `b`. - `left_shift(self, a, b)`: Returns the result of left shifting `a` by `b` positions. - `right_shift(self, a, b)`: Returns the result of right shifting `a` by `b` positions. - `convert_to_int(self, a)`: Converts `a` to an integer. - `convert_to_float(self, a)`: Converts `a` to a float. - `numeric_to_base(self, a, base)`: Converts an integer `a` to its string representation in the given base (2, 8, 10, or 16). # Constraints: - All input values will be valid and will not result in NULL returns from the underlying functions. - You must use the C-based numeric operations wherever applicable. - Performance should handle typical arithmetic calculations within reasonable time frames. # Input: - For operations involving two numbers like add, subtract, etc., `a` and `b` will be the two operands. - For single operand operations like negate, `a` will be the operand. # Output: - Each method should return the appropriate result of the operation as specified. # Example: ```python calc = Calculator() print(calc.add(10, 5)) # Should return 15 print(calc.divide(10, 2)) # Should return 5.0 print(calc.bitwise_and(5, 3)) # Should return 1 print(calc.convert_to_int(5.7)) # Should return 5 print(calc.numeric_to_base(10, 2)) # Should return \'0b1010\' ``` # Implementation: ```python class Calculator: def add(self, a, b): # Use PyNumber_Add equivalent pass def subtract(self, a, b): # Use PyNumber_Subtract equivalent pass def multiply(self, a, b): # Use PyNumber_Multiply equivalent pass def divide(self, a, b): # Use PyNumber_TrueDivide equivalent pass def floor_divide(self, a, b): # Use PyNumber_FloorDivide equivalent pass def modulus(self, a, b): # Use PyNumber_Remainder equivalent pass def power(self, a, b): # Use PyNumber_Power equivalent pass def negate(self, a): # Use PyNumber_Negative equivalent pass def bitwise_and(self, a, b): # Use PyNumber_And equivalent pass def bitwise_or(self, a, b): # Use PyNumber_Or equivalent pass def bitwise_xor(self, a, b): # Use PyNumber_Xor equivalent pass def left_shift(self, a, b): # Use PyNumber_Lshift equivalent pass def right_shift(self, a, b): # Use PyNumber_Rshift equivalent pass def convert_to_int(self, a): # Use PyNumber_Long equivalent pass def convert_to_float(self, a): # Use PyNumber_Float equivalent pass def numeric_to_base(self, a, base): # Use PyNumber_ToBase equivalent pass ```","solution":"class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): return a / b def floor_divide(self, a, b): return a // b def modulus(self, a, b): return a % b def power(self, a, b): return a ** b def negate(self, a): return -a def bitwise_and(self, a, b): return a & b def bitwise_or(self, a, b): return a | b def bitwise_xor(self, a, b): return a ^ b def left_shift(self, a, b): return a << b def right_shift(self, a, b): return a >> b def convert_to_int(self, a): return int(a) def convert_to_float(self, a): return float(a) def numeric_to_base(self, a, base): if base == 2: return bin(a) elif base == 8: return oct(a) elif base == 10: return str(a) elif base == 16: return hex(a) else: raise ValueError(\\"Base must be 2, 8, 10, or 16\\")"},{"question":"Objective: You are to demonstrate your understanding of `sklearn.datasets` package by creating a function that loads a specific dataset, preprocesses the data, and performs a basic analysis. Task: Write a function `process_and_analyze_iris()` that: 1. Loads the Iris dataset using the dataset loader from `sklearn.datasets`. 2. Normalizes the feature data using `StandardScaler` from `sklearn.preprocessing`. 3. Performs a Principal Component Analysis (PCA) to reduce the dataset to 2 principal components using `PCA` from `sklearn.decomposition`. 4. Returns the transformed dataset in the form of a Pandas DataFrame, where the feature columns are labeled `PC1` and `PC2`, and includes a column for the target values labeled `Target`. Expected Input: The function does not take any input arguments. Expected Output: A Pandas DataFrame with the following columns: - `PC1`: The first principal component. - `PC2`: The second principal component. - `Target`: The target values from the Iris dataset. Constraints: 1. You must use the `load_iris()` function from `sklearn.datasets` to load the dataset. 2. You must use `StandardScaler` to normalize the data. 3. You must use `PCA` to reduce the data to 2 components. 4. The Pandas DataFrame should have appropriate column names. Example of Expected Output: ``` PC1 PC2 Target 0 -2.2647 0.4803 0 1 -2.0809 0.6741 0 2 -2.3642 0.3417 0 3 -2.2996 0.5976 0 4 -2.3895 0.6461 0 ... ``` Code Implementation: ```python import pandas as pd from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA def process_and_analyze_iris(): # Load Iris dataset iris = load_iris() X = iris.data y = iris.target # Normalize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply PCA to reduce to 2 components pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Create a Pandas DataFrame with appropriate column names df = pd.DataFrame(X_pca, columns=[\'PC1\', \'PC2\']) df[\'Target\'] = y return df # Example usage: # df_iris = process_and_analyze_iris() # print(df_iris.head()) ```","solution":"import pandas as pd from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA def process_and_analyze_iris(): # Load Iris dataset iris = load_iris() X = iris.data y = iris.target # Normalize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply PCA to reduce to 2 components pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Create a Pandas DataFrame with appropriate column names df = pd.DataFrame(X_pca, columns=[\'PC1\', \'PC2\']) df[\'Target\'] = y return df"},{"question":"**Python Coding Assessment Question: Custom URL Opener with Authentication and Error Handling** --- # Objective: Implement a custom URL opener that fetches content from a given URL while handling HTTP authentication and managing error responses. # Task: You need to create a Python function `fetch_url_content` that performs the following operations: 1. Authenticates with the server using HTTP Basic Authentication. 2. Fetches the content from the specified URL. 3. Handles specific HTTP error responses (e.g., 404 Not Found, 403 Forbidden). 4. Returns the content of the URL if the request is successful or an appropriate error message if it fails. # Requirements: 1. Use the `urllib.request` module. 2. Implement a custom HTTP opener using `OpenerDirector` and `HTTPBasicAuthHandler`. 3. Handle the following HTTP status codes: - 200: Return the content of the response. - 403: Return \\"Error: Forbidden\\". - 404: Return \\"Error: Not Found\\". # Function Signature: ```python def fetch_url_content(url: str, username: str, password: str) -> str: ``` # Input: - `url`: A string representing the URL to fetch. - `username`: A string representing the username for HTTP Basic Authentication. - `password`: A string representing the password for HTTP Basic Authentication. # Output: - A string containing the content of the URL if the request is successful, or an error message (\\"Error: Forbidden\\" or \\"Error: Not Found\\") if it fails. # Example: ```python url = \\"http://example.com\\" username = \\"user\\" password = \\"pass\\" # Expected output if the URL content is fetched successfully print(fetch_url_content(url, username, password)) # Expected output if the URL returns a 403 Forbidden status # Output: \\"Error: Forbidden\\" # Expected output if the URL returns a 404 Not Found status # Output: \\"Error: Not Found\\" ``` # Constraints: - Use proper error handling for network-related issues. - Ensure the function works for URLs requiring HTTP Basic Authentication. --- **Notes:** - The implementation should utilize `urllib.request` for making the HTTP requests. - You may use the provided sample authentication and error handling examples as a reference. **Good Luck!**","solution":"import urllib.request from urllib.error import HTTPError def fetch_url_content(url: str, username: str, password: str) -> str: Fetches content from a given URL using HTTP Basic Authentication. Parameters: - url (str): The URL to fetch content from. - username (str): The username for HTTP Basic Authentication. - password (str): The password for HTTP Basic Authentication. Returns: - str: Content of the URL or an error message. try: # Create a password manager password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() # Add the username and password password_mgr.add_password(None, url, username, password) # Create an auth handler handler = urllib.request.HTTPBasicAuthHandler(password_mgr) # Create an opener with the auth handler opener = urllib.request.build_opener(handler) # Use the opener to fetch the url response = opener.open(url) if response.status == 200: return response.read().decode() except HTTPError as e: if e.code == 403: return \\"Error: Forbidden\\" elif e.code == 404: return \\"Error: Not Found\\" else: return f\\"Error: HTTP Error {e.code}\\" except Exception as e: return f\\"Error: {str(e)}\\""},{"question":"# Question Context You are developing a Python application that processes user inputs and executes code dynamically. To ensure robust error handling and useful debugging information, you want to enhance the application\'s exception handling mechanism using the `traceback` module. Task Write a function `run_code()` that takes a single string argument `source_code`. This function should execute the provided source code string, handle any exceptions that arise, and format the exception information for display as a user-friendly multiline string. Your implementation should: 1. Execute the provided source code using `exec()`. 2. Catch any exceptions that occur during execution. 3. Use the `traceback` module to format the caught exception and the corresponding stack trace. Function Signature ```python def run_code(source_code: str) -> str: pass ``` Input - `source_code` (str): A string containing Python code that will be dynamically executed. Output - Returns a string with the formatted exception information and stack trace if an exception occurs. - If no exception occurs, return \\"Execution successful.\\" Constraints - Only built-in Python functionalities are available for execution. - The provided code should not involve any blocking operations such as infinite loops or actual system-level calls for security reasons. Example ```python source_code = def faulty_function(): return 1 / 0 faulty_function() print(run_code(source_code)) ``` **Output:** ``` Traceback (most recent call last): File \\"<string>\\", line 5, in <module> File \\"<string>\\", line 3, in faulty_function ZeroDivisionError: division by zero ``` Use the `traceback` module to extract and format exception details for the cases when errors occur. This will demonstrate your understanding of handling and presenting them effectively.","solution":"import traceback def run_code(source_code: str) -> str: Executes the given source code and returns formatted exception information if an error occurs. If no exception occurs, returns \\"Execution successful.\\" try: exec(source_code) return \\"Execution successful.\\" except Exception: return traceback.format_exc()"},{"question":"# Question: Implement a DataFrame Modification Checker **Objective:** Implement a function `check_modification(df: pd.DataFrame) -> pd.DataFrame` which demonstrates the effects of the Copy-on-Write (CoW) mechanism in pandas. This function should show how modifications to DataFrames and Series behave differently under the CoW rules. **Function Signature:** ```python def check_modification(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** - `df`: A pandas DataFrame with at least two columns. For example: ```python df = pd.DataFrame({ \\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6] }) ``` **Output:** - Returns a new DataFrame after demonstrating CoW principles. This DataFrame should be different from the original DataFrame if and only if a copy was made due to an operation triggered by CoW. **Steps:** 1. Create a subset of `df` using column selection or slicing. 2. Modify the subset and show whether modifying the subset changes the original DataFrame. 3. Demonstrate chaining assignment error by modifying a DataFrame using chained indexing. 4. Show how to correctly modify the DataFrame without violating CoW principles (using `loc` or another proper method). 5. Demonstrate the creation of a read-only NumPy array from the DataFrame and attempt modifying it to see the error. **Constraints:** - You must use pandas version 3.0 or higher. - Your function should adhere to CoW rules strictly and raise exceptions if any chained assignment is performed. **Example:** ```python import pandas as pd def check_modification(df: pd.DataFrame) -> pd.DataFrame: # Create a subset of the original DataFrame subset = df[\\"A\\"] # Modify the subset and show the impact on original DataFrame (if any) subset.iloc[0] = 100 print(\\"Original DataFrame after subset modification:\\") print(df) # Demonstrate chaining assignment which should raise an error try: df[\\"A\\"][df[\\"B\\"] > 5] = 100 except Exception as e: print(f\\"Chaining assignment error: {e}\\") # Correctly modify DataFrame using loc df.loc[df[\\"B\\"] > 5, \\"A\\"] = 200 print(\\"DataFrame after using loc:\\") print(df) # Demonstrate read-only NumPy array arr = df.to_numpy() print(\\"Attempt to modify read-only NumPy array:\\") try: arr[0, 0] = 999 except Exception as e: print(f\\"Read-only array modification error: {e}\\") return df # Example usage: df = pd.DataFrame({\\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6]}) result_df = check_modification(df) print(\\"Final DataFrame:\\") print(result_df) ``` In the example above, the function `check_modification` performs several operations that demonstrate the effects of CoW principles.","solution":"import pandas as pd def check_modification(df: pd.DataFrame) -> pd.DataFrame: # Create a subset of the original DataFrame subset = df[\\"A\\"].copy() # Using copy to ensure Copy-on-Write mechanism # Modify the subset and show the impact on original DataFrame (if any) subset.iloc[0] = 100 print(\\"Original DataFrame after subset modification:\\") print(df) # Demonstrate chaining assignment which should be avoided try: df[\\"A\\"][df[\\"B\\"] > 5] = 100 except Exception as e: print(f\\"Chaining assignment error: {e}\\") # Correctly modify DataFrame using loc df.loc[df[\\"B\\"] > 5, \\"A\\"] = 200 print(\\"DataFrame after using loc:\\") print(df) # Demonstrate read-only NumPy array arr = df.to_numpy() print(\\"Attempt to modify read-only NumPy array:\\") try: arr.setflags(write=False) arr[0, 0] = 999 except Exception as e: print(f\\"Read-only array modification error: {e}\\") return df # Example usage df = pd.DataFrame({\\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6]}) result_df = check_modification(df) print(\\"Final DataFrame:\\") print(result_df)"},{"question":"**Title: Predicting Diabetes Progression using Scikit-Learn** **Background:** You are provided with a dataset containing medical information of patients, which includes various features and a target variable indicating the diabetes progression. Your task is to design and implement a machine learning solution to predict the progression of diabetes using supervised learning techniques in scikit-learn. **Dataset:** The dataset is similar to the Diabetes dataset available in scikit-learn. **Features:** - Age: age of the patient - Sex: gender of the patient - BMI: body mass index - BP: average blood pressure - S1 to S6: six blood serum measurements **Target:** - A continuous variable representing the diabetes progression one year after baseline metrics were recorded. **Tasks:** 1. **Data Preprocessing:** - Handle any missing values in the dataset. - Standardize the features by removing the mean and scaling to unit variance. 2. **Model Training:** - Implement at least three different supervised learning models from the scikit-learn library (e.g., Linear Regression, Decision Tree Regressor, Random Forest Regressor). - Train each model on the dataset. 3. **Model Evaluation:** - Evaluate the performance of each model using Mean Squared Error (MSE) and R-squared (R²) metrics. - Perform cross-validation to assess the consistency of your model performance. 4. **Model Interpretation:** - Provide a brief interpretation of which model performs best and why, based on your evaluation metrics. **Constraints:** - Use only libraries provided in scikit-learn for implementing the models. - Ensure your code is optimized for performance and readability. **Expected Input and Output:** - Input: A dataset in a pandas DataFrame format, with columns representing the features and the target. - Output: A dictionary containing the evaluation metrics (MSE and R²) for each of the three models, and a brief interpretation of the results. **Function Signature:** ```python def predict_diabetes_progression(data: pd.DataFrame) -> dict: data: pd.DataFrame - DataFrame containing the dataset with features and target variable returns: dict - A dictionary containing the evaluation metrics for each model and a brief interpretation of the results pass ``` **Example Usage:** ```python import pandas as pd # Assume \'data\' is a Pandas DataFrame containing the dataset results = predict_diabetes_progression(data) print(results) ``` **Notes:** - Include proper documentation within your code. - Make sure to split the data into training and testing sets before model training and evaluation.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, r2_score def predict_diabetes_progression(data: pd.DataFrame) -> dict: # Split the data into features and target variable X = data.drop(columns=[\\"target\\"]) y = data[\\"target\\"] # Handle missing values by filling them with the mean of each column X.fillna(X.mean(), inplace=True) # Standardize the features scaler = StandardScaler() X = scaler.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the models to be used models = { \\"Linear Regression\\": LinearRegression(), \\"Decision Tree Regressor\\": DecisionTreeRegressor(random_state=42), \\"Random Forest Regressor\\": RandomForestRegressor(random_state=42) } results = {} for model_name, model in models.items(): # Train the model model.fit(X_train, y_train) # Predict on the test set y_pred = model.predict(X_test) # Calculate evaluation metrics mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) # Perform cross-validation cv_mse = cross_val_score(model, X, y, cv=5, scoring=\'neg_mean_squared_error\').mean() cv_r2 = cross_val_score(model, X, y, cv=5, scoring=\'r2\').mean() # Store the results results[model_name] = { \\"MSE\\": mse, \\"R²\\": r2, \\"CV_MSE\\": -cv_mse, \\"CV_R²\\": cv_r2 } best_model = max(results, key=lambda x: results[x][\\"R²\\"]) best_model_interpretation = f\\"The best performing model is {best_model} with an R² of {results[best_model][\'R²\']:.4f}.\\" # Add interpretation to the results results[\\"Interpretation\\"] = best_model_interpretation return results"},{"question":"# PyTorch Storage Manipulation In this assignment, you will demonstrate your understanding of PyTorch\'s storage and its interaction with tensors. You will perform a series of tasks to manipulate tensor storage directly and verify the results. # Objectives 1. Create a tensor and clone its untyped storage. 2. Modify the cloned storage and apply it to the original tensor. 3. Implement a function that takes two tensors and checks if they share the same storage. # Tasks 1. Implement the function `modify_storage` which takes a tensor `t` and modifies its underlying storage by filling it with a specified value. - **Input**: - A tensor `t` (e.g., `torch.ones(3)`). - An integer value `fill_value` to fill the storage (e.g., `0`). - **Output**: - The modified tensor `t`, with its storage filled with `fill_value`. ```python def modify_storage(t: torch.Tensor, fill_value: int) -> torch.Tensor: # Your code here ``` 2. Implement the function `check_shared_storage` which checks whether two tensors share the same underlying untyped storage. - **Input**: - Two tensors `t1` and `t2`. - **Output**: - `True` if they share the same storage, and `False` otherwise. ```python def check_shared_storage(t1: torch.Tensor, t2: torch.Tensor) -> bool: # Your code here ``` # Example ```python import torch # Task 1 - Modify Storage t = torch.ones(3) t = modify_storage(t, 0) print(t) # Expected output: tensor([0., 0., 0.]) # Task 2 - Check Shared Storage a = torch.ones((2, 2)) b = a.view(-1) print(check_shared_storage(a, b)) # Expected output: True c = a.clone() print(check_shared_storage(a, c)) # Expected output: False ``` # Constraints 1. Do not use tensor-specific methods like `t.fill_()` or `t.clone()`. Instead, use storage-specific methods. 2. Ensure your solution is efficient and adheres to best practices. # Submission Submit your solution as a `.py` file containing the two functions `modify_storage` and `check_shared_storage`.","solution":"import torch def modify_storage(t: torch.Tensor, fill_value: int) -> torch.Tensor: Modifies the underlying storage of tensor `t` by filling it with `fill_value`. storage = t.storage() storage.fill_(fill_value) return t def check_shared_storage(t1: torch.Tensor, t2: torch.Tensor) -> bool: Checks whether two tensors share the same underlying untyped storage. return t1.storage().data_ptr() == t2.storage().data_ptr()"},{"question":"<|Analysis Begin|> The provided documentation describes the `contextvars` module in Python. This module provides APIs to manage, store, and access context-local state. The primary classes and functions discussed are: 1. **ContextVar**: A class used to declare new context variables. - Important methods: `get`, `set`, and `reset`. 2. **Token**: A class that represents tokens returned by `ContextVar.set()`, which can revert the variable to its previous value using `ContextVar.reset()`. 3. **copy_context()**: A function to get a copy of the current context. 4. **Context**: A class representing a collection of context variables. - Important methods: `run`, `copy`, `get`, `keys`, `values`, `items`, etc. The main concepts covered in the documentation include: - Declaring and managing context variables. - Manipulating context variables and their values. - Using contexts to contain the changes made to context variables. - Integration with `asyncio`. Given the advanced concepts described, a coding question encapsulating these features would assess a student\'s understanding of context management, context variables, and their application in asynchronous programming. <|Analysis End|> <|Question Begin|> # Context-Aware Task Management You are tasked with implementing a simple context-aware task management system using the `contextvars` module. This system will track and manage tasks, each having its own context with specific variables. This is particularly useful for scenarios where context needs to be isolated and manipulated independently, such as in web servers handling multiple client requests concurrently. Requirements: 1. Create a class `TaskManager` that will manage tasks. 2. `TaskManager` should use context variables to store task-specific information. 3. Implement the following methods in the `TaskManager` class: - `add_task(task_id: str, initial_data: dict) -> None`: Adds a new task with the provided `task_id` and initial data. Each task should have its own context. - `update_task(task_id: str, key: str, value: any) -> None`: Updates the specified context variable (`key`) for the given `task_id` with the new `value`. - `get_task_data(task_id: str) -> dict`: Returns all the context variables and their values for the specified `task_id`. 4. Handle the case where operations are attempted on a non-existent task elegantly (e.g., by raising appropriate exceptions). Constraints: - The implementation should use the `contextvars.ContextVar` and `contextvars.Context` for managing contexts. Example Usage: ```python # Example of adding and updating tasks task_manager = TaskManager() # Adding tasks task_manager.add_task(\\"task1\\", {\\"username\\": \\"alice\\", \\"status\\": \\"pending\\"}) task_manager.add_task(\\"task2\\", {\\"username\\": \\"bob\\", \\"status\\": \\"in-progress\\"}) # Updating task context task_manager.update_task(\\"task1\\", \\"status\\", \\"completed\\") # Retrieving task data print(task_manager.get_task_data(\\"task1\\")) # Expected: {\'username\': \'alice\', \'status\': \'completed\'} print(task_manager.get_task_data(\\"task2\\")) # Expected: {\'username\': \'bob\', \'status\': \'in-progress\'} ``` Note: The `TaskManager` should ensure that context variables for different tasks do not interfere with each other. This is crucial for maintaining the isolation of task-specific data.","solution":"import contextvars class TaskManager: def __init__(self): self._task_contexts = {} self._task_vars = {} def add_task(self, task_id: str, initial_data: dict) -> None: if task_id in self._task_contexts: raise ValueError(f\\"Task ID {task_id} already exists\\") ctx = contextvars.Context() self._task_contexts[task_id] = ctx task_vars = {} for key, value in initial_data.items(): var = contextvars.ContextVar(key) var.set(value) task_vars[key] = var self._task_vars[task_id] = task_vars def update_task(self, task_id: str, key: str, value: any) -> None: if task_id not in self._task_contexts: raise ValueError(f\\"Task ID {task_id} does not exist\\") task_vars = self._task_vars[task_id] if key not in task_vars: var = contextvars.ContextVar(key) task_vars[key] = var else: var = task_vars[key] var.set(value) def get_task_data(self, task_id: str) -> dict: if task_id not in self._task_contexts: raise ValueError(f\\"Task ID {task_id} does not exist\\") task_vars = self._task_vars[task_id] return {key: var.get() for key, var in task_vars.items()}"},{"question":"You are provided with a dataset of car characteristics. Your task is to visualize the relationship between two continuous variables from this dataset using seaborn\'s scatter plot, with points colored based on a categorical variable. Additionally, you will customize the color palette using the `sns.husl_palette()` function to better distinguish different categories. Dataset Assume you have a pandas DataFrame named `car_data` with the following columns: - `horsepower`: Continuous variable representing the horsepower of the car. - `weight`: Continuous variable representing the weight of the car. - `origin`: Categorical variable representing the origin of the car (e.g., \'USA\', \'Europe\', \'Asia\'). Requirements 1. **Input/Output Format**: - Function Name: `visualize_car_data` - Input: `car_data` (pandas DataFrame) - Output: Display a scatter plot using seaborn. 2. **Task**: - Create a scatter plot where the x-axis represents `horsepower` and the y-axis represents `weight`. - Color the points based on the `origin` column using a custom color palette generated by `sns.husl_palette()`. The palette should have as many colors as there are unique values in `origin`. - Adjust the lightness and saturation of the palette to `.7` and `.7` respectively. - Display a legend that clearly shows which color corresponds to which category in `origin`. 3. **Constraints**: - Use seaborn and matplotlib only for plotting. - Ensure the plot is clear and aesthetically pleasing. 4. **Performance Requirements**: - Efficient handling of typical small to medium-sized datasets (up to 10,000 rows). Example ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Example data data = { \'horsepower\': [95, 123, 150, 170, 102, 110, 132], \'weight\': [2000, 2200, 2500, 3000, 2100, 2150, 2400], \'origin\': [\'USA\', \'Europe\', \'Asia\', \'USA\', \'Europe\', \'Asia\', \'USA\'] } car_data = pd.DataFrame(data) def visualize_car_data(car_data): # Your implementation here # Call the function visualize_car_data(car_data) ``` Expected outcome is a displayed scatter plot with appropriately colored points based on the `origin` column using a custom `husl_palette`.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_car_data(car_data): Visualizes the relationship between horsepower and weight of cars, using a scatter plot colored by the origin of the cars. Parameters: car_data (pd.DataFrame): DataFrame containing the columns \'horsepower\', \'weight\', and \'origin\'. Output: Displays a seaborn scatter plot. unique_origins = car_data[\'origin\'].nunique() palette = sns.husl_palette(unique_origins, l=.7, s=.7) # Create the scatter plot sns.scatterplot(data=car_data, x=\'horsepower\', y=\'weight\', hue=\'origin\', palette=palette) # Show the plot plt.legend(title=\'Origin\') plt.title(\'Car Weight vs. Horsepower\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Weight\') plt.show()"},{"question":"**Question: Implement a Log File Parser using Regular Expressions** # Objective: Write a Python function `parse_logfile` that uses regular expressions from the `re` module to parse a log file and extract specific information. # Description: You are given a log file containing server logs. Each log entry in the file is formatted as follows: ``` <IP> - - [<timestamp>] \\"<request>\\" <status> <size> \\"<referer>\\" \\"<user_agent>\\" ``` Here is an example of an entry: ``` 192.168.0.1 - - [10/Oct/2023:13:55:36 -0700] \\"GET /index.html HTTP/1.1\\" 200 1043 \\"-\\" \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\\" ``` # Task: Write a function `parse_logfile(filepath)` which: 1. Opens and reads a log file from the given `filepath`. 2. Extracts and prints a summary of: - The total number of log entries. - The number of log entries per unique IP address. - The number of GET requests and POST requests. - The number of successful responses (status code 200) and the number of client errors (status codes 4xx). # Input: - `filepath` (string): Path to the log file. # Output: The function should print the following: ``` Total Log Entries: <total_entries> Unique IPs: <number_of_unique_ips> GET requests: <number_of_get_requests> POST requests: <number_of_post_requests> Successful responses: <number_of_successful_responses> Client Errors: <number_of_client_errors> ``` # Constraints and Limitations: - Assume the log file is well-formatted. - Optimize for readability and efficiency where possible. # Requirements: - Use the `re` module for regular expressions to parse the log entries. - Handle the file operations in a safe manner (i.e., with proper opening and closing of files). # Example Usage: ```python def parse_logfile(filepath): import re from collections import defaultdict # Pattern to match each line in the log file log_pattern = re.compile(r\'(d{1,3}.){3}d{1,3} - - [(.*?)] \\"(.*?)\\" (d{3}) (d+|-) \\"(.*?)\\" \\"(.*?)\\"\') # Counters and trackers total_entries = 0 unique_ips = defaultdict(int) get_requests = 0 post_requests = 0 successful_responses = 0 client_errors = 0 with open(filepath, \'r\') as file: for line in file: match = log_pattern.match(line) if match: total_entries += 1 ip = match.group(0) request = match.group(3) status_code = int(match.group(4)) unique_ips[ip] += 1 if request.startswith(\'GET\'): get_requests += 1 elif request.startswith(\'POST\'): post_requests += 1 if status_code == 200: successful_responses += 1 elif 400 <= status_code < 500: client_errors += 1 print(f\'Total Log Entries: {total_entries}\') print(f\'Unique IPs: {len(unique_ips)}\') print(f\'GET requests: {get_requests}\') print(f\'POST requests: {post_requests}\') print(f\'Successful responses: {successful_responses}\') print(f\'Client Errors: {client_errors}\') # Example call # parse_logfile(\'server_log.txt\') ``` **Note:** Assuming `server_log.txt` contains the log entries as mentioned in the example.","solution":"import re from collections import defaultdict def parse_logfile(filepath): Parse a log file and extract specific information using regular expressions. :param filepath: path to the log file # Pattern to match each line in the log file log_pattern = re.compile(r\'(?P<ip>d{1,3}(?:.d{1,3}){3}) - - [(?P<timestamp>.*?)] \\"(?P<request>.*?)\\" (?P<status>d{3}) (?P<size>d+|-) \\"(?P<referer>.*?)\\" \\"(?P<user_agent>.*?)\\"\') # Counters and trackers total_entries = 0 unique_ips = defaultdict(int) get_requests = 0 post_requests = 0 successful_responses = 0 client_errors = 0 with open(filepath, \'r\') as file: for line in file: match = log_pattern.match(line) if match: total_entries += 1 ip = match.group(\\"ip\\") request = match.group(\\"request\\") status_code = int(match.group(\\"status\\")) unique_ips[ip] += 1 if request.startswith(\'GET\'): get_requests += 1 elif request.startswith(\'POST\'): post_requests += 1 if status_code == 200: successful_responses += 1 elif 400 <= status_code < 500: client_errors += 1 print(f\'Total Log Entries: {total_entries}\') print(f\'Unique IPs: {len(unique_ips)}\') print(f\'GET requests: {get_requests}\') print(f\'POST requests: {post_requests}\') print(f\'Successful responses: {successful_responses}\') print(f\'Client Errors: {client_errors}\')"},{"question":"**Coding Assessment Question:** Imagine you are tasked with simulating and analyzing some statistical properties of certain random tensors using PyTorch. Your objective is to demonstrate your understanding of PyTorch\'s random module by completing the following tasks: # Task: 1. **Set the Random Seed:** - Set the random seed to `42` to ensure reproducibility of your results. 2. **Generate Random Tensors:** - Create two random tensors `A` and `B` of shape (3, 4) with values drawn from a uniform distribution over `[0, 1)`. 3. **Operations on Random Tensors:** - Compute the element-wise sum of tensors `A` and `B`. - Compute the dot product of tensors `A` and `B` (by treating them as flattened vectors). 4. **Statistical Analysis:** - Calculate the mean and standard deviation of the resulting tensors from the above operations. 5. **Conditional Modification:** - Create a new tensor `C` by setting all values in tensor `A` that are less than `0.5` to zero, and all other values to one. Implement the function `random_tensors_analysis` to accomplish the above tasks. # Function Definition: ```python def random_tensors_analysis(): Executes the tasks described above and returns the results. Returns: - A_sum_B (torch.Tensor): The element-wise sum of tensors A and B. - dot_product (torch.Tensor): The dot product of tensors A and B. - stats (dict): A dictionary containing the mean and standard deviation {\'mean\': val, \'std\': val} for both the summed and dot product tensors. - C (torch.Tensor): The tensor `C` created by applying the conditional modification on tensor `A`. # Your implementation here ``` # Expected Output: - `A_sum_B`: A tensor of shape (3, 4) representing the element-wise sum of `A` and `B`. - `dot_product`: A scaler value representing the dot product of the flattened `A` and `B` tensors. - `stats`: A dictionary with keys `mean` and `std`, containing the mean and standard deviation of both summed and dot product tensors. - `C`: A tensor of shape (3, 4) after being conditionally modified. # Constraints: - Do not use any loops (for, while, etc.) in your implementation. - You must use PyTorch for all tensor operations. - Ensure the function is reproducible by setting the random seed as specified. # Performance Requirements: Ensure the function executes efficiently and avoids unnecessary computations or memory usage.","solution":"import torch def random_tensors_analysis(): Executes the tasks described above and returns the results. Returns: - A_sum_B (torch.Tensor): The element-wise sum of tensors A and B. - dot_product (torch.Tensor): The dot product of tensors A and B. - stats (dict): A dictionary containing the mean and standard deviation {\'mean\': val, \'std\': val} for both the summed and dot product tensors. - C (torch.Tensor): The tensor `C` created by applying the conditional modification on tensor `A`. # Set the random seed. torch.manual_seed(42) # Generate random tensors A and B. A = torch.rand(3, 4) B = torch.rand(3, 4) # Compute the element-wise sum of tensors A and B. A_sum_B = A + B # Compute the dot product of tensors A and B (flatten them first). flat_A = A.flatten() flat_B = B.flatten() dot_product = torch.dot(flat_A, flat_B) # Calculate mean and standard deviation of A_sum_B. mean_sum = A_sum_B.mean().item() std_sum = A_sum_B.std().item() # Calculate mean and standard deviation of dot_product. mean_dot = dot_product.item() std_dot = 0 # Dot product of two vectors is a scalar, hence std is 0. # Create a new tensor C by setting all values in A < 0.5 to zero, else to one. C = torch.where(A < 0.5, torch.tensor(0.0), torch.tensor(1.0)) # Prepare the stats dictionary. stats = { \'sum\': {\'mean\': mean_sum, \'std\': std_sum}, \'dot\': {\'mean\': mean_dot, \'std\': std_dot} } return A_sum_B, dot_product, stats, C"},{"question":"# MIME Email Message Generator Problem Statement: You are provided with an email message object that requires serialization for transport via standard protocols (e.g., SMTP). Your task is to implement a function `generate_serialized_email` that uses the `email.generator` module to convert the provided email message object into a serialized byte stream. The function must handle both MIME and non-MIME messages and should ensure the output conforms to standard email formatting requirements. Function Signature: ```python def generate_serialized_email(msg, unixfrom: bool = False, mangle_from: bool = None, maxheaderlen: int = None, cte_type: str = \\"8bit\\") -> bytes: pass ``` Input: - `msg`: An `EmailMessage` object that represents the email to be serialized. - `unixfrom` (bool): When `True`, includes the Unix \\"From \\" envelope header. Defaults to `False`. - `mangle_from` (bool): If `True`, adds a \\">\\" character in front of any line starting with \\"From \\". If unspecified, follows the message policy. - `maxheaderlen` (int): Maximum allowed header length. If `None`, follows the message policy. If `0`, does not rewrap headers at all. - `cte_type` (str): Content transfer encoding type. Either \\"7bit\\" or \\"8bit\\". Defaults to \\"8bit\\". Output: - Returns a `bytes` object representing the serialized email message. Constraints: - The function must handle both ASCII and non-ASCII characters appropriately. - Ensure the serialized message maintains compliance with email RFC standards. - Optimize for performance given the constraints of typical email sizes. Example Usage: ```python from email.message import EmailMessage # Create an example email message msg = EmailMessage() msg.set_content(\\"This is a test email.\\") msg[\'Subject\'] = \'Test\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Generate serialized email serialized_email = generate_serialized_email(msg, unixfrom=True, mangle_from=True, maxheaderlen=78, cte_type=\\"8bit\\") # Output the result print(serialized_email) ``` Notes: - You may use the classes and methods from the `email.generator` module to implement the function. - Take care of proper content transfer encoding based on the `cte_type` parameter. - Ensure that the email message remains fully compliant with relevant standards throughout serialization.","solution":"from email.generator import BytesGenerator from email.policy import default def generate_serialized_email(msg, unixfrom: bool = False, mangle_from: bool = None, maxheaderlen: int = None, cte_type: str = \\"8bit\\") -> bytes: Serializes the email message object to a byte stream following standard email formatting requirements. Arguments: - msg: An `EmailMessage` object that represents the email to be serialized. - unixfrom (bool): When `True`, includes the Unix \\"From \\" envelope header. Defaults to `False`. - mangle_from (bool): If `True`, adds a \\">\\" character in front of any line starting with \\"From \\". If unspecified, follows the message policy. - maxheaderlen (int): Maximum allowed header length. If `None`, follows the message policy. If `0`, does not rewrap headers at all. - cte_type (str): Content transfer encoding type. Either \\"7bit\\" or \\"8bit\\". Defaults to \\"8bit\\". Returns: - A `bytes` object representing the serialized email message. from io import BytesIO with BytesIO() as out: policy = default.clone(cte_type=cte_type) if maxheaderlen is not None: policy = policy.clone(max_line_length=maxheaderlen) gen = BytesGenerator(out, policy=policy, mangle_from_=mangle_from) gen.flatten(msg, unixfrom=unixfrom) return out.getvalue()"},{"question":"You are provided with a dataset containing image features of different types of fruits and their colors. Each sample has two target variables: `Type` and `Color`. Your task is to implement a multiclass-multioutput classification model using scikit-learn that can predict both the type of the fruit and its color simultaneously. This exercise will help test your comprehension of handling multioutput problems using scikit-learn. # Data Format The provided dataset contains the following columns: - `feature1`, `feature2`, ..., `featureN`: Features of the fruit image. - `Type`: Type of fruit (categorical) with classes: Apple, Banana, Orange. - `Color`: Color of fruit (categorical) with classes: Red, Yellow, Green. # Requirements 1. Implement a function `multioutput_fruit_classifier(X_train, y_train, X_test)` which accepts: - `X_train`: A DataFrame with shape (n_samples, n_features) containing the training features. - `y_train`: A DataFrame with shape (n_samples, 2) containing two columns: `Type` and `Color`, representing the training targets. - `X_test`: A DataFrame with shape (m_samples, n_features) containing the test features. 2. The function should return: - `y_pred`: A DataFrame with shape (m_samples, 2) containing the predictions for `Type` and `Color`. 3. Use `MultiOutputClassifier` combined with `RandomForestClassifier` to build the model. # Example Usage ```python import pandas as pd from sklearn.model_selection import train_test_split # Sample Data data = pd.DataFrame({ \'feature1\': [0.1, 0.2, 0.15, 0.3, 0.33], \'feature2\': [1.1, 1.2, 1.15, 1.3, 1.33], \'Type\': [\'Apple\', \'Banana\', \'Apple\', \'Orange\', \'Banana\'], \'Color\': [\'Red\', \'Yellow\', \'Green\', \'Yellow\', \'Green\'] }) # Splitting Data X = data[[\'feature1\', \'feature2\']] y = data[[\'Type\', \'Color\']] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Calling the function y_pred = multioutput_fruit_classifier(X_train, y_train, X_test) print(y_pred) ``` # Constraints - Use `MultiOutputClassifier` with `RandomForestClassifier` as the base estimator. - Ensure reproducibility by setting random states where required. - The solution should handle missing values and data preprocessing where appropriate. - Performance is not the main constraint. Focus on correctness and implementation of multioutput capabilities. # Hints - Refer to `sklearn.multioutput.MultiOutputClassifier` for implementing multioutput classification. - You may need to preprocess the target variables using `LabelEncoder` or similar for encoding categorical labels to numerical values.","solution":"import pandas as pd from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder def multioutput_fruit_classifier(X_train, y_train, X_test): Trains a MultiOutputClassifier with RandomForestClassifier as the base estimator on the training data and uses it to predict \'Type\' and \'Color\' on the test data. Parameters: - X_train: DataFrame with shape (n_samples, n_features) containing the training features. - y_train: DataFrame with shape (n_samples, 2) containing the training targets (\'Type\' and \'Color\'). - X_test: DataFrame with shape (m_samples, n_features) containing the test features. Returns: - y_pred: DataFrame with shape (m_samples, 2) containing the predictions for \'Type\' and \'Color\'. # Encoding the target variables le_type = LabelEncoder() le_color = LabelEncoder() y_train_encoded = y_train.copy() y_train_encoded[\'Type\'] = le_type.fit_transform(y_train[\'Type\']) y_train_encoded[\'Color\'] = le_color.fit_transform(y_train[\'Color\']) # Defining the model model = MultiOutputClassifier(RandomForestClassifier(random_state=42)) # Fitting the model model.fit(X_train, y_train_encoded) # Predicting the values y_pred_encoded = model.predict(X_test) # Converting predictions to DataFrame y_pred = pd.DataFrame(y_pred_encoded, columns=[\'Type\', \'Color\']) # Decoding the predicted labels back to original classes y_pred[\'Type\'] = le_type.inverse_transform(y_pred[\'Type\']) y_pred[\'Color\'] = le_color.inverse_transform(y_pred[\'Color\']) return y_pred"},{"question":"# Unsupervised Learning Algorithm Comparison Your task is to implement a function that applies multiple unsupervised learning algorithms to a dataset and compares their performance using a set of metrics. You will use `scikit-learn`\'s clustering and decomposition modules for this exercise. Function Signature ```python def compare_unsupervised_algorithms(data: np.ndarray, n_clusters: int) -> Dict[str, float]: Applies multiple unsupervised learning algorithms to the input data and compares their performance. Parameters: data (np.ndarray): A 2D numpy array where each row represents a sample and each column represents a feature. n_clusters (int): The number of clusters to form for clustering algorithms. Returns: Dict[str, float]: A dictionary where the keys are the names of the algorithms and the values are the silhouette scores of the clustering results. pass ``` Instructions 1. **Data Input**: - The input `data` is a 2D numpy array of shape (n_samples, n_features). - The input `n_clusters` is an integer specifying the number of clusters. 2. **Algorithms to Implement**: - KMeans from `sklearn.cluster`. - Agglomerative Clustering from `sklearn.cluster`. - Gaussian Mixture from `sklearn.mixture`. - PCA (Principal Component Analysis) for dimensionality reduction followed by KMeans. 3. **Performance Metric**: - Use the silhouette score from `sklearn.metrics` to evaluate the quality of the clusters. Higher scores indicate more defined clusters. 4. **Procedure**: - For each algorithm: - Perform clustering on the dataset. - If using PCA, first reduce the data to `n_clusters` dimensions before clustering. - Compute the silhouette score. - Return a dictionary with algorithm names as keys and their respective silhouette scores as values. Example ```python import numpy as np from sklearn.datasets import load_iris from sklearn.cluster import KMeans, AgglomerativeClustering from sklearn.mixture import GaussianMixture from sklearn.decomposition import PCA from sklearn.metrics import silhouette_score def compare_unsupervised_algorithms(data: np.ndarray, n_clusters: int) -> Dict[str, float]: results = {} # KMeans kmeans = KMeans(n_clusters=n_clusters) kmeans_labels = kmeans.fit_predict(data) kmeans_score = silhouette_score(data, kmeans_labels) results[\'KMeans\'] = kmeans_score # Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=n_clusters) agglomerative_labels = agglomerative.fit_predict(data) agglomerative_score = silhouette_score(data, agglomerative_labels) results[\'AgglomerativeClustering\'] = agglomerative_score # Gaussian Mixture gm = GaussianMixture(n_components=n_clusters) gm_labels = gm.fit_predict(data) gm_score = silhouette_score(data, gm_labels) results[\'GaussianMixture\'] = gm_score # PCA + KMeans pca = PCA(n_components=n_clusters) data_reduced = pca.fit_transform(data) pca_kmeans = KMeans(n_clusters=n_clusters) pca_kmeans_labels = pca_kmeans.fit_predict(data_reduced) pca_kmeans_score = silhouette_score(data, pca_kmeans_labels) results[\'PCA + KMeans\'] = pca_kmeans_score return results # Load sample data iris = load_iris() data = iris.data n_clusters = 3 # Execute function result = compare_unsupervised_algorithms(data, n_clusters) print(result) ``` This question will test the student\'s ability to understand, implement, and compare various fundamental and advanced unsupervised learning techniques using `scikit-learn`.","solution":"import numpy as np from typing import Dict from sklearn.cluster import KMeans, AgglomerativeClustering from sklearn.mixture import GaussianMixture from sklearn.decomposition import PCA from sklearn.metrics import silhouette_score def compare_unsupervised_algorithms(data: np.ndarray, n_clusters: int) -> Dict[str, float]: results = {} # KMeans kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans_labels = kmeans.fit_predict(data) kmeans_score = silhouette_score(data, kmeans_labels) results[\'KMeans\'] = kmeans_score # Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=n_clusters) agglomerative_labels = agglomerative.fit_predict(data) agglomerative_score = silhouette_score(data, agglomerative_labels) results[\'AgglomerativeClustering\'] = agglomerative_score # Gaussian Mixture gm = GaussianMixture(n_components=n_clusters, random_state=42) gm_labels = gm.fit_predict(data) gm_score = silhouette_score(data, gm_labels) results[\'GaussianMixture\'] = gm_score # PCA + KMeans pca = PCA(n_components=n_clusters, random_state=42) data_reduced = pca.fit_transform(data) pca_kmeans = KMeans(n_clusters=n_clusters, random_state=42) pca_kmeans_labels = pca_kmeans.fit_predict(data_reduced) pca_kmeans_score = silhouette_score(data, pca_kmeans_labels) results[\'PCA + KMeans\'] = pca_kmeans_score return results"},{"question":"**Lexical Analyzer Implementation** # Objective: Implement a function called `lexical_analyzer` that processes a Python source code string and returns a list of tokens as per Python\'s lexical analysis rules. # Function Signature: ```python def lexical_analyzer(source_code: str) -> List[str]: ``` # Input: - `source_code`: A string containing Python source code. The string may contain comments, strings, numbers, identifiers, keywords, operators, and delimiters. # Output: - A list of strings where each string is a token identified in the `source_code`. # Constraints: - The source code string will have a maximum length of 1000 characters. - The function should handle different types of line endings (`n`, `rn`, `r`). - It should correctly identify and handle different types of comments, strings (single, double, and triple-quoted), and whitespace. - It should correctly handle encoding declarations if present in the source code. # Example: ```python source_code = # This is a comment x = 10 + 20 # Addition y = 0x1A + 0b1010 tokens = lexical_analyzer(source_code) # Example output: # [ # \'COMMENT\', \'NEWLINE\', # \'IDENTIFIER\', \'WHITESPACE\', \'OPERATOR\', \'WHITESPACE\', \'INTEGER\', \'WHITESPACE\', \'OPERATOR\', \'WHITESPACE\', \'INTEGER\', \'COMMENT\', \'NEWLINE\', # \'IDENTIFIER\', \'WHITESPACE\', \'OPERATOR\', \'WHITESPACE\', \'HEXINTEGER\', \'WHITESPACE\', \'OPERATOR\', \'WHITESPACE\', \'BININTEGER\', \'NEWLINE\' # ] ``` # Notes: - Identify and classify different tokens as per the rules provided in the documentation. - Handle implicit and explicit line joining. - Ignore blank lines and correctly interpret indentation. - The `COMMENT` token should represent Python comments. - The `NEWLINE` token should represent the end of a logical line. - The `WHITESPACE` token should represent sequences of spaces, tabs, or formfeeds existing between tokens. - Handle different numeric literals (decimal, binary, octal, hexadecimal) appropriately. # Challenge: Implement the function `lexical_analyzer` to comply with the requirements and constraints provided above. Ensure that the function accurately identifies and classifies tokens based on the types mentioned (comments, newline, whitespace, identifiers, operators, delimiters, string literals, numeric literals). Proper handling of edge cases and different termination sequences is crucial for this task. **Good Luck!**","solution":"import re from typing import List def lexical_analyzer(source_code: str) -> List[str]: # Define regex patterns for different token types token_specification = [ (\'COMMENT\', r\'#.*\'), (\'NEWLINE\', r\'n\'), (\'WHITESPACE\', r\'[ t]+\'), (\'STRING\', r\'\\".*?\\"|\'\'.*?\'\'|\'[^\']*\'|\\"[^\\"]*\\"\'), (\'HEXINTEGER\', r\'0x[0-9A-Fa-f]+\'), (\'BININTEGER\', r\'0b[01]+\'), (\'OCTINTEGER\', r\'0o[0-7]+\'), (\'INTEGER\', r\'d+\'), (\'IDENTIFIER\', r\'[A-Za-z_][A-Za-z0-9_]*\'), (\'OPERATOR\', r\'[+-*/%&|<>!=]=?|==|//|<<|>>|**\'), (\'DELIMITER\', r\'[(),:;[]{}@]\'), ] # Creating regex pattern from all token specifications token_spec = \'|\'.join(\'(?P<%s>%s)\' % pair for pair in token_specification) tokenizer = re.compile(token_spec) line_number = 1 line_start = 0 tokens = [] for mo in tokenizer.finditer(source_code): kind = mo.lastgroup value = mo.group(kind) if kind == \'NEWLINE\': line_start = mo.end() line_number += 1 tokens.append(kind) return tokens"},{"question":"Question: Custom Serialization of PyTorch Module Your task is to create a custom utility to serialize and deserialize a PyTorch `torch.nn.Module` with optimizations for view relationships and storage sizes. This utility should handle models with complex architectures and tensor views efficiently. Specifically, you need to ensure that the size of the saved file is minimized without losing any view relationships among tensors and should be able to restore the module\'s state accurately. # Specifications 1. **Function to Save Module State with Optimizations:** - Function Name: `save_module_state` - Input: `torch.nn.Module instance`, `file_path (str)` - Operation: Save the module\'s state to the specified file path, optimizing for storage size by cloning tensors where necessary while preserving view relationships. - Output: None 2. **Function to Load Optimized Module State:** - Function Name: `load_module_state` - Input: `torch.nn.Module instance`, `file_path (str)` - Operation: Load the module\'s state from the specified file path into the provided module instance. - Output: None 3. **Constraints:** - The solution should work for modules with arbitrary nesting and complexity. - The solution must minimize the storage size for individual tensors that share the storage with larger ones while preserving view relationships. - Both functions must handle any exceptions and ensure the integrity of the saved and loaded data. 4. **Performance Requirements:** - Loading and saving should be efficient considering typical model sizes encountered in deep learning. # Example Usage ```python import torch class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.fc1 = torch.nn.Linear(10, 5) self.fc2 = torch.nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.fc1(x)) return self.fc2(x) def save_module_state(module, file_path): # Your implementation here pass def load_module_state(module, file_path): # Your implementation here pass # Initialize model and save its state model = MyModel() save_module_state(model, \'mymodel_optimized.pt\') # Load state into a new instance of the model new_model = MyModel() load_module_state(new_model, \'mymodel_optimized.pt\') # Verify that state was loaded correctly assert torch.allclose(model.fc1.weight, new_model.fc1.weight) assert torch.allclose(model.fc2.weight, new_model.fc2.weight) ``` **Note**: Ensure to write a robust solution with error handling and appropriate usage of PyTorch\'s serialization functions.","solution":"import torch def save_module_state(module, file_path): Save the module\'s state to the specified file path, optimizing for storage size while preserving view relationships among tensors. state_dict = module.state_dict() # We will use torch.save and torch.load, which handle all necessary optimizations internally. with open(file_path, \'wb\') as f: torch.save(state_dict, f) def load_module_state(module, file_path): Load the module\'s state from the specified file path into the provided module instance. with open(file_path, \'rb\') as f: state_dict = torch.load(f) module.load_state_dict(state_dict)"},{"question":"# **Complex Sorting Function Implementation** Problem Statement You are given a list of dictionaries where each dictionary represents a student\'s record with the following key-value pairs: - `name` (str): The student\'s name. - `grade` (str): The student\'s grade (A, B, C, etc.). - `age` (int): The student\'s age. You need to implement a function `sort_students` that takes two arguments: 1. A list of student records (each record is a dictionary). 2. A list of tuples specifying the sorting keys and their respective orders (each tuple contains a string key and a boolean for reverse order). The function should sort the given list of students based on the specified keys and orders using multiple sorting passes. Each key can be either `name`, `grade`, or `age`, and the boolean value indicates whether the order should be descending (True) or ascending (False). Function Signature ```python def sort_students(students: List[Dict[str, Union[str, int]]], sort_keys: List[Tuple[str, bool]]) -> List[Dict[str, Union[str, int]]]: pass ``` Input - `students` (List[Dict[str, Union[str, int]]]): A list of dictionaries where each dictionary represents a student\'s record. - `sort_keys` (List[Tuple[str, bool]]): A list of tuples specifying the sort key and order. Output - (List[Dict[str, Union[str, int]]]): A list of dictionaries sorted based on the specified keys and orders. Constraints - The `students` list can contain up to 1000 student records. - The `name` will be a string of up to 50 characters. - The `grade` will be a string of a single uppercase alphabet (A, B, C, etc.). - The `age` will be an integer between 6 and 20. - You can assume that the `sort_keys` list will not be empty and will contain valid keys (name, grade, age). Example ```python students = [ {\\"name\\": \\"john\\", \\"grade\\": \\"A\\", \\"age\\": 15}, {\\"name\\": \\"jane\\", \\"grade\\": \\"B\\", \\"age\\": 12}, {\\"name\\": \\"dave\\", \\"grade\\": \\"B\\", \\"age\\": 10}, {\\"name\\": \\"jake\\", \\"grade\\": \\"A\\", \\"age\\": 20}, ] sort_keys = [(\\"grade\\", True), (\\"age\\", False)] sorted_students = sort_students(students, sort_keys) print(sorted_students) ``` Expected Output ```python [ {\\"name\\": \\"jake\\", \\"grade\\": \\"A\\", \\"age\\": 20}, {\\"name\\": \\"john\\", \\"grade\\": \\"A\\", \\"age\\": 15}, {\\"name\\": \\"dave\\", \\"grade\\": \\"B\\", \\"age\\": 10}, {\\"name\\": \\"jane\\", \\"grade\\": \\"B\\", \\"age\\": 12}, ] ``` Additional Notes - Make use of the `operator` module functions `itemgetter` or appropriate key functions to achieve the sorting. - Ensure that the sort is stable, meaning that records with equal keys retain their relative order.","solution":"from typing import List, Dict, Union, Tuple def sort_students(students: List[Dict[str, Union[str, int]]], sort_keys: List[Tuple[str, bool]]) -> List[Dict[str, Union[str, int]]]: Sorts a list of student records based on multiple keys and their specified orders. :param students: List of dictionaries where each dictionary represents a student\'s record. :param sort_keys: List of tuples specifying the sorting keys and their respective orders. :return: Sorted list of student records. for key, reverse in reversed(sort_keys): students.sort(key=lambda x: x[key], reverse=reverse) return students"},{"question":"Coding Assessment Question # Objective Implement a function that encodes a list of strings into various Unicode codecs and then decodes them, storing the results in a dictionary. # Task You are tasked with writing two functions: 1. `encode_strings(strings: List[str], codec: str) -> List[bytes]`: This function takes a list of strings and a codec name as input and returns a list of encoded bytes using the specified codec. 2. `decode_strings(encoded_strings: List[bytes], codec: str) -> List[str]`: This function takes a list of encoded bytes and a codec name as input and returns a list of decoded strings using the specified codec. Additionally, write a function `process_strings(strings: List[str], codecs: List[str]) -> Dict[str, List[str]]` that takes a list of strings and a list of codec names. For each codec, encode the strings, then decode them, and store the results in a dictionary where the keys are the codec names and the values are the decoded strings. # Input Format - `encode_strings(strings, codec)`: - `strings`: A list of strings (`List[str]`) - `codec`: A string specifying the codec name (`str`) - `decode_strings(encoded_strings, codec)`: - `encoded_strings`: A list of bytes (`List[bytes]`) - `codec`: A string specifying the codec name (`str`) - `process_strings(strings, codecs)`: - `strings`: A list of strings (`List[str]`) - `codecs`: A list of codec names (`List[str]`) # Output Format - `encode_strings(strings, codec)`: - Returns a list of encoded bytes (`List[bytes]`) - `decode_strings(encoded_strings, codec)`: - Returns a list of decoded strings (`List[str]`) - `process_strings(strings, codecs)`: - Returns a dictionary with codec names as keys and lists of decoded strings as values (`Dict[str, List[str]]`) # Example ```python def encode_strings(strings, codec): # Your code here def decode_strings(encoded_strings, codec): # Your code here def process_strings(strings, codecs): # Your code here strings = [\\"hello\\", \\"world\\"] codecs = [\\"utf-8\\", \\"utf-16\\"] encoded_bytes = encode_strings(strings, \\"utf-8\\") # Example output: [b\'hello\', b\'world\'] decoded_strings = decode_strings(encoded_bytes, \\"utf-8\\") # Example output: [\\"hello\\", \\"world\\"] result = process_strings(strings, codecs) # Example output: { # \\"utf-8\\": [\\"hello\\", \\"world\\"], # \\"utf-16\\": [\\"hello\\", \\"world\\"] # } ``` # Constraints 1. Handle exceptions where codecs might fail during encoding/decoding. 2. Assume input strings are valid and do not require sanitization. 3. Ensure the function performance is optimal for a list of up to 1000 strings and up to 10 codecs. # Notes - Refer to the `codecs` module for supported codec names in Python. - Ensure proper error handling for unsupported or invalid codecs. Document your code and provide basic test cases to validate your implementation.","solution":"from typing import List, Dict def encode_strings(strings: List[str], codec: str) -> List[bytes]: Encode a list of strings using the specified codec. try: return [s.encode(codec) for s in strings] except Exception as e: raise ValueError(f\\"Encoding failed: {e}\\") def decode_strings(encoded_strings: List[bytes], codec: str) -> List[str]: Decode a list of encoded bytes using the specified codec. try: return [b.decode(codec) for b in encoded_strings] except Exception as e: raise ValueError(f\\"Decoding failed: {e}\\") def process_strings(strings: List[str], codecs: List[str]) -> Dict[str, List[str]]: Encode and decode a list of strings using various codecs and return a dictionary with the codec names as keys and decoded strings as values. result = {} for codec in codecs: encoded_strings = encode_strings(strings, codec) decoded_strings = decode_strings(encoded_strings, codec) result[codec] = decoded_strings return result"},{"question":"# Context Management with Python\'s `contextlib` Objective: Implement a custom context manager using `contextlib`\'s `contextmanager` decorator that maintains a stack of resources and ensures their proper release. Additionally, implement a mechanism that supports conditional resource acquisition and cleanup. # Instructions: 1. Using the `contextlib.contextmanager` decorator, implement a context manager named `resource_manager` that: - Acquires a resource. - Yields control back to the block within the `with` statement. - Ensures the resource is released once the block is exited, even if exceptions are raised. 2. Implement another context manager named `conditional_resource_manager` utilizing `contextlib.ExitStack`, which: - Accepts a list of resources. - Manages the entry and exit of each resource using `ExitStack`. # Function Specifications: `resource_manager` - **Input**: A string representing the resource name. - **Output**: Yields the resource. `conditional_resource_manager` - **Input**: A list of pairs `(resource_name, condition)`, where `resource_name` is a string and `condition` is a boolean. - **Output**: Yields a list of acquired resources\' names (strings). # Example Usage: ```python from contextlib import contextmanager from contextlib import ExitStack # Implement `resource_manager` @contextmanager def resource_manager(resource_name): # Code to acquire the resource print(f\\"Acquiring {resource_name}\\") try: yield resource_name finally: # Code to release the resource print(f\\"Releasing {resource_name}\\") # Implement `conditional_resource_manager` @contextmanager def conditional_resource_manager(resources): with ExitStack() as stack: acquired_resources = [] for resource_name, condition in resources: if condition: acquired_resources.append(stack.enter_context(resource_manager(resource_name))) yield acquired_resources # Example usage of `conditional_resource_manager` resources = [(\\"resource1\\", True), (\\"resource2\\", False), (\\"resource3\\", True)] with conditional_resource_manager(resources) as acquired: print(\\"Inside context with resources:\\", acquired) # Expected output (order may vary based on Python version): # Acquiring resource1 # Acquiring resource3 # Inside context with resources: [\'resource1\', \'resource3\'] # Releasing resource3 # Releasing resource1 ``` # Constraints: - Your implementation should correctly acquire and release resources based on their conditions. - Handle exceptions appropriately to ensure all acquired resources are released. # Evaluation: Your solution will be evaluated on: - Correct implementation of `resource_manager` and `conditional_resource_manager`. - Proper use of `contextlib.contextmanager` and `contextlib.ExitStack`. - Ability to meet the provided constraints and example usage.","solution":"from contextlib import contextmanager, ExitStack @contextmanager def resource_manager(resource_name): Context manager that acquires and releases a resource. Args: resource_name (str): The name of the resource to acquire. Yields: str: The name of the acquired resource. # Code to acquire the resource print(f\\"Acquiring {resource_name}\\") try: yield resource_name finally: # Code to release the resource print(f\\"Releasing {resource_name}\\") @contextmanager def conditional_resource_manager(resources): Context manager that conditionally acquires resources based on provided conditions. Args: resources (list): A list of tuples (resource_name, condition) where resource_name is a string and condition is a boolean. Yields: list: A list of acquired resource names. with ExitStack() as stack: acquired_resources = [] for resource_name, condition in resources: if condition: acquired_resources.append(stack.enter_context(resource_manager(resource_name))) yield acquired_resources"},{"question":"Objective: To demonstrate an understanding of Argument Clinic by converting a provided C function to use Argument Clinic for argument parsing and handling default values. Instructions: 1. You are provided with a simple C function that takes multiple arguments and parses them using `PyArg_ParseTuple`. 2. Your task is to convert this function to use Argument Clinic. 3. Ensure that the generated code handles argument parsing, has appropriate docstrings, and uses any necessary converters and default values. Provided C Function: Here is the existing C code you need to convert: ```c static PyObject * example_function(PyObject *self, PyObject *args) { const char *message; int repeat; if (!PyArg_ParseTuple(args, \\"si\\", &message, &repeat)) return NULL; for (int i = 0; i < repeat; i++) { printf(\\"%sn\\", message); } Py_RETURN_NONE; } static PyMethodDef ExampleMethods[] = { {\\"example_function\\", example_function, METH_VARARGS, \\"Prints a message a specified number of times.\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef examplemodule = { PyModuleDef_HEAD_INIT, \\"example\\", NULL, -1, ExampleMethods }; PyMODINIT_FUNC PyInit_example(void) { return PyModule_Create(&examplemodule); } ``` Task: 1. Create an Argument Clinic block to replace the manual argument parsing in `example_function`. 2. Ensure the `message` argument defaults to \\"Hello, world!\\" if not provided. 3. Add a per-parameter docstring for `repeat` that explains it must be an integer specifying how many times to repeat the message. 4. Incorporate these changes into the function, compile, and test to ensure functionality. Expected Output Your final .c file should contain: 1. The Argument Clinic block with inputs and specified converters. 2. The generated function code handling the argument parsing. 3. The updated method definition macro `_EXAMPLE_FUNCTION_METHODDEF`. # Your Solution: ```c /*[clinic input] example.example_function message: str = \\"Hello, world!\\" The message to be printed. repeat: int Integer specifying how many times to repeat the message. / Prints a message a specified number of times. [clinic start generated code]*/ /*[clinic end generated code: output=f273d95285bbcaac input=db4c29146efc5e63]*/ #include \\"clinic/examplemodule.c.h\\" static PyObject * example_function_impl(PyObject *module, const char *message, int repeat) { for (int i = 0; i < repeat; i++) { printf(\\"%sn\\", message); } Py_RETURN_NONE; } static PyMethodDef ExampleMethods[] = { _EXAMPLE_FUNCTION_METHODDEF {NULL, NULL, 0, NULL} }; static struct PyModuleDef examplemodule = { PyModuleDef_HEAD_INIT, \\"example\\", NULL, -1, ExampleMethods }; PyMODINIT_FUNC PyInit_example(void) { return PyModule_Create(&examplemodule); } ``` Note: 1. Do not modify the generated code manually. 2. Compile and run tests ensuring the default value works and the function performs as expected.","solution":"def example_function(message=\\"Hello, world!\\", repeat=1): Prints a message a specified number of times. Parameters: message (str): The message to be printed. Defaults to \\"Hello, world!\\". repeat (int): Integer specifying how many times to repeat the message. for _ in range(repeat): print(message)"},{"question":"# Advanced Coding Assessment: Function Object Manipulation Objective Demonstrate your understanding of Python function objects by implementing a set of utilities that inspect and manipulate function objects to modify their behavior. Task You are required to implement a function named `create_custom_function` that takes the following parameters: 1. `code_source` (str): A string of Python code defining the function body. 2. `name` (str): The name of the function. 3. `globals_dict` (dict): A dictionary representing the global namespace in which the function should be executed. 4. `defaults` (tuple): A tuple of default argument values. 5. `annotations` (dict): A dictionary of argument annotations. 6. `qualname` (str, optional): The qualified name of the function. The function should return a new function object that behaves according to the given parameters. Additionally, implement the following helper functions to inspect the created function: 1. `get_function_code(func)`: Returns the code object associated with the function `func`. 2. `get_function_globals(func)`: Returns the globals dictionary associated with the function `func`. 3. `get_function_annotations(func)`: Returns the annotations of the function `func`. 4. `get_function_defaults(func)`: Returns the default argument values of the function `func`. Constraints - The `code_source` should be a valid Python function body (without the `def` statement). - The `globals_dict` must contain all necessary imports and global variables required by `code_source`. - The `defaults` tuple should match the function\'s parameter signature. - The `annotations` dictionary should match the function\'s parameter annotations. - The `qualname` is optional but, if provided, must be a valid string. Example Usage ```python code_source = def custom_func(a, b=2): A custom function. return a + b globals_dict = {} name = \\"custom_func\\" defaults = (2,) annotations = {\'a\': int, \'b\': int, \'return\': int} qualname = \\"my_module.custom_func\\" new_func = create_custom_function(code_source, name, globals_dict, defaults, annotations, qualname) # Inspecting the function print(get_function_code(new_func)) # Prints the code object of new_func print(get_function_globals(new_func)) # Prints the globals dictionary of new_func print(get_function_annotations(new_func)) # Prints the annotations of new_func print(get_function_defaults(new_func)) # Prints the default argument values of new_func print(new_func(3)) # Should return 5 ``` Requirements - Write clean, readable, and well-documented code. - Handle edge cases gracefully, such as empty strings or mismatched defaults and annotations. Submission Submit your implementation along with any assumptions or design choices you made during development.","solution":"import types def create_custom_function(code_source, name, globals_dict, defaults, annotations, qualname=None): Create a custom function with the provided parameters. Args: - code_source (str): A string of Python code defining the function body. - name (str): The name of the function. - globals_dict (dict): A dictionary representing the global namespace in which the function should be executed. - defaults (tuple): A tuple of default argument values. - annotations (dict): A dictionary of argument annotations. - qualname (str, optional): The qualified name of the function. Returns: - Function object defined by the code_source. # Create a namespace for executing the function code locals_dict = {} # Compile the code to obtain a code object func_code = compile(code_source, \\"<string>\\", \\"exec\\") # Execute the code within the provided global namespace to populate locals_dict exec(func_code, globals_dict, locals_dict) # Extract the function from locals_dict. Assuming the function in code_source is named `name`. func = locals_dict[name] # Set defaults, annotations, and qualname func.__defaults__ = defaults func.__annotations__ = annotations if qualname: func.__qualname__ = qualname return func def get_function_code(func): Returns the code object associated with the function `func`. return func.__code__ def get_function_globals(func): Returns the globals dictionary associated with the function `func`. return func.__globals__ def get_function_annotations(func): Returns the annotations of the function `func`. return func.__annotations__ def get_function_defaults(func): Returns the default argument values of the function `func`. return func.__defaults__"},{"question":"You are tasked with creating an XML parser that processes an XML document containing book information. Each book has a title, an author, a publication year, and an optional description. The goal of this task is to implement custom handlers for the start and end of elements, as well as for character data, to extract and print the information for each book as it is parsed. # Requirements 1. **Function Definition**: Implement a function `parse_books(xml_data: str) -> None` that takes a string containing XML data and prints out the parsed book information. 2. **Handlers**: - `StartElementHandler`: Called at the start of each XML element. Initialize a new dictionary for a book when a `<book>` element is encountered. - `EndElementHandler`: Called at the end of each XML element. Print the book\'s information when a `</book>` element is encountered. - `CharacterDataHandler`: Called for character data within an element. Capture the text content for elements like title, author, year, and description. 3. **Parser Setup**: - Create the XML parser instance using `ParserCreate`. - Set up the three handlers (`StartElementHandler`, `EndElementHandler`, `CharacterDataHandler`) for the parser. # Input Format - `xml_data`: A string containing XML data. # Output Format - No return value. - The function should print the book information in the following format: ``` Title: <title> Author: <author> Year: <year> Description: <description> (if available) ``` # Example Given the XML data: ```xml <library> <book> <title>Ender\'s Game</title> <author>Orson Scott Card</author> <year>1985</year> <description>A science fiction novel.</description> </book> <book> <title>1984</title> <author>George Orwell</author> <year>1949</year> </book> </library> ``` The function `parse_books` should output: ``` Title: Ender\'s Game Author: Orson Scott Card Year: 1985 Description: A science fiction novel. Title: 1984 Author: George Orwell Year: 1949 ``` # Constraints - The XML data is assumed to be well-formed. - Elements are provided in a straightforward nested structure as shown in the example. # Additional Notes - Ensure proper error handling for any potential issues during parsing using `ExpatError`. - The parser should handle multiple book entries within a single library. Implement the function `parse_books(xml_data: str) -> None` accordingly.","solution":"from xml.parsers.expat import ParserCreate, ExpatError def parse_books(xml_data): current_book = None current_element = None books = [] def start_element(name, attrs): nonlocal current_book, current_element if name == \'book\': current_book = {} current_element = name def end_element(name): nonlocal current_book, current_element if name == \'book\': books.append(current_book) current_book = None current_element = None def char_data(data): nonlocal current_book, current_element if current_book is not None and current_element is not None: if current_element in current_book: current_book[current_element] += data.strip() else: current_book[current_element] = data.strip() parser = ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data) except ExpatError as e: print(f\\"XML parsing error: {e}\\") return for book in books: print(f\\"Title: {book.get(\'title\', \'\')}\\") print(f\\"Author: {book.get(\'author\', \'\')}\\") print(f\\"Year: {book.get(\'year\', \'\')}\\") if \'description\' in book: print(f\\"Description: {book.get(\'description\', \'\')}\\")"},{"question":"# Large Dataset Processing with Pandas Objective In this coding task, you are required to write a function that processes a large dataset in an efficient manner using pandas. The dataset is split into multiple Parquet files, each containing data for a different time interval. Your task is to implement a function that reads the data from these files, optimizes the memory usage, and performs a specific analysis. Problem Statement You need to implement a function `process_large_dataset(directory: str) -> pd.Series` that loads, optimizes, and analyzes a large dataset containing timeseries data stored in several Parquet files in a given directory. Details: 1. **Data Format**: - Each Parquet file contains a DataFrame with columns: \\"timestamp\\", \\"name\\", \\"id\\", \\"x\\", \\"y\\". - The \\"timestamp\\" column contains datetime values. - The \\"name\\" column contains strings with few unique values. - The \\"id\\" column contains integer values. - The \\"x\\" and \\"y\\" columns contain float values. 2. **Optimization**: - Convert the \\"name\\" column to a categorical type to optimize memory usage. - Downcast the \\"id\\" column to the smallest possible unsigned integer type. - Downcast \\"x\\" and \\"y\\" columns to the smallest possible float type. 3. **Analysis**: - Calculate the value counts for the \\"name\\" column across all files. Input: - `directory` (str): A path to the directory containing the Parquet files. Output: - Returns a pandas Series containing the value counts of names across all the Parquet files, with the names as index and counts as values (integers). Constraints: - Each individual file fits into memory, but the combined dataset may not. Example: ```python def process_large_dataset(directory: str) -> pd.Series: import pathlib import pandas as pd files = pathlib.Path(directory).glob(\\"*.parquet\\") counts = pd.Series(dtype=int) for path in files: df = pd.read_parquet(path) df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") counts = counts.add(df[\\"name\\"].value_counts(), fill_value=0) return counts.astype(int) # Example Usage: # result = process_large_dataset(\\"data/timeseries\\") # print(result) ``` Note: Ensure to test your function on multiple files to verify its performance and correctness.","solution":"import pandas as pd import pathlib def process_large_dataset(directory: str) -> pd.Series: Reads, optimizes, and analyzes a large dataset containing timeseries data stored in Parquet files. Parameters: - directory (str): A path to the directory containing the Parquet files. Returns: - pd.Series: A series containing the value counts of names across all the Parquet files. files = pathlib.Path(directory).glob(\\"*.parquet\\") counts = pd.Series(dtype=int) for path in files: df = pd.read_parquet(path) df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") # Accumulate value counts for the \\"name\\" column counts = counts.add(df[\\"name\\"].value_counts(), fill_value=0) return counts.astype(int)"},{"question":"<|Analysis Begin|> The provided documentation pertains to the `random` module in Python. This module is used for generating pseudo-random numbers, including integers, floating-point numbers, and those from various probability distributions (e.g., normal, exponential). It includes a variety of functions for these purposes, allowing for both uniform and weighted selections from sequences, shuffling lists, and sampling without replacement, among other tasks. The `random` module employs the Mersenne Twister algorithm as its core generator, which is well-tested and known for its high-quality pseudo-random number generation. However, it should not be used for cryptographic purposes, for which the `secrets` module is recommended. Key functions and classes in the module include `random()`, `uniform()`, `randrange()`, `randint()`, `choice()`, `shuffle()`, `sample()`, and `choices()`, among others. The module also allows for state management, enabling reproducibility of random sequences across runs. I have identified that the information provided is sufficient to design a challenging, clear, and self-contained coding assessment question based on the `random` module. <|Analysis End|> <|Question Begin|> **Assessment Question: Implementing a Custom Random Number Generator** # Objective Your task is to implement a custom random number generator that can generate random floating-point numbers between 0 (inclusive) and a given upper bound (exclusive) utilizing the principles from the `random` module. # Background Python’s `random` module uses the Mersenne Twister pseudorandom number generator. For this task, you will create your own custom generator using basic principles of random number generation. You will be provided with a simple random number generator function that you will integrate into your solution. # Task Overview 1. **Implement a CustomRandom class**: - This class should provide methods for generating random floating-point numbers. - It should follow the same principles as the `random()` function in python’s `random` module. 2. **Implement the following methods in the CustomRandom class**: - `__init__(self, seed=None)`: Initialize the random number generator. Optionally set its seed. - `random(self)`: Generate a random floating-point number in the range [0.0, 1.0). - `uniform(self, upper_bound)`: Generate a random floating-point number in the range [0.0, upper_bound). # Input and Output Specifications - `__init__(self, seed=None)`: Optional integer `seed` for initializing the generator. If not provided, use the current system time. - `random(self)`: No parameters. Returns a float between 0.0 (inclusive) and 1.0 (exclusive). - `uniform(self, upper_bound)`: Takes a positive float `upper_bound` and returns a float between 0.0 (inclusive) and `upper_bound` (exclusive). # Constraints 1. You may use the built-in `time.time()` function to get the current time for seeding purposes if no seed is provided. 2. Do not use any functions from the `random` module in your implementation. 3. Assume `upper_bound` in the `uniform` method will always be a positive float greater than 0. # Example Usage ```python # Initialize CustomRandom with a seed generator = CustomRandom(seed=123) # Generate a random number between 0 and 1 print(generator.random()) # Output: some float between 0 and 1 # Generate a random number between 0 and 10 print(generator.uniform(10.0)) # Output: some float between 0 and 10 ``` # Code Template ```python import time class CustomRandom: def __init__(self, seed=None): if seed is not None: self.seed = seed else: self.seed = int(time.time() * 1000) # Use current time in milliseconds def random(self): # Implement a simple random number generator self.seed = (self.seed * 1103515245 + 12345) % (2**31) return self.seed / (2**31) def uniform(self, upper_bound): return self.random() * upper_bound # Example Usage if __name__ == \\"__main__\\": generator = CustomRandom(seed=123) print(generator.random()) print(generator.uniform(10.0)) ``` Note: The multiplication factor and the addend in the `random` method are part of a simple Linear Congruential Generator (LCG). The modulo value `(2**31)` helps in maintaining the periodicity of the sequence of random numbers.","solution":"import time class CustomRandom: def __init__(self, seed=None): if seed is not None: self.seed = seed else: self.seed = int(time.time() * 1000) # Use current time in milliseconds def random(self): # Implement a simple random number generator using Linear Congruential Generator (LCG) method self.seed = (self.seed * 1664525 + 1013904223) % (2**32) return self.seed / 2**32 def uniform(self, upper_bound): return self.random() * upper_bound"},{"question":"# Custom Iterator Implementation You are required to implement two custom iterators in Python, both of which mimic the behavior described in the `PySeqIter_Type` and `PyCallIter_Type` documentation. # Part 1: Sequence Iterator Implement a class `SeqIterator` that creates an iterator for any given sequence object (`list`, `tuple`, `string`, etc.) that supports the `__getitem__()` method. Requirements: - **Initialization**: The constructor should accept a sequence object. - **Functionality**: Implement the `__iter__()` and `__next__()` methods to make your class an iterator. - The iteration ends when the sequence raises an `IndexError`. Example Usage: ```python seq = [1, 2, 3, 4] seq_iter = SeqIterator(seq) for value in seq_iter: print(value) ``` **Expected Output:** ``` 1 2 3 4 ``` # Part 2: Callable-based Sentinel Iterator Implement a class `CallIterator` that creates an iterator from a callable object and a sentinel value. Requirements: - **Initialization**: The constructor should accept a callable object (which can be called with no parameters) and a sentinel value. - **Functionality**: Implement the `__iter__()` and `__next__()` methods to make your class an iterator. - The iteration ends when the callable returns a value equal to the sentinel value. Example Usage: ```python def counter(): counter.count += 1 return counter.count counter.count = 0 sentinel = 5 call_iter = CallIterator(counter, sentinel) for value in call_iter: print(value) ``` **Expected Output:** ``` 1 2 3 4 ``` # Constraints: - Do not use the built-in `iter()` function for solving these parts directly; you need to implement the functionality from scratch. - Ensure that your implementation correctly handles the end of iteration as described. # Submission: Provide the implementation of `SeqIterator` and `CallIterator` classes.","solution":"class SeqIterator: def __init__(self, seq): if not hasattr(seq, \'__getitem__\'): raise TypeError(\\"Provided object does not support __getitem__\\") self._seq = seq self._index = 0 def __iter__(self): return self def __next__(self): try: value = self._seq[self._index] except IndexError: raise StopIteration self._index += 1 return value class CallIterator: def __init__(self, func, sentinel): if not callable(func): raise TypeError(\\"Provided object is not callable\\") self._func = func self._sentinel = sentinel def __iter__(self): return self def __next__(self): value = self._func() if value == self._sentinel: raise StopIteration return value"},{"question":"**Objective:** You are tasked with demonstrating your understanding of seaborn\'s figure aesthetics customization. You need to create a function that generates matplotlib plots with customized seaborn aesthetics based on provided parameters. **Problem Statement:** Write a function `customized_plot()` that generates a customized seaborn plot based on the following input parameters: - `data`: a 2D NumPy array of numerical values. - `plot_type`: a string indicating the type of plot to create (`\'box\'` for boxplot, `\'violin\'` for violin plot). - `theme`: a string indicating the seaborn theme to use (one of `\'darkgrid\'`, `\'whitegrid\'`, `\'dark\'`, `\'white\'`, `\'ticks\'`). - `style`: a string indicating the figure style to use (one of `\'darkgrid\'`, `\'whitegrid\'`, `\'dark\'`, `\'white\'`, `\'ticks\'`). - `context`: a string indicating the plot context to use (one of `\'paper\'`, `\'notebook\'`, `\'talk\'`, `\'poster\'`). - `remove_spine`: a boolean indicating whether to remove the top and right spines from the plot. Your function should create the plot using the specified customization parameters, display it, and save the plot as an image named `customized_plot.png` in the current working directory. **Function Signature:** ```python def customized_plot(data: np.ndarray, plot_type: str, theme: str, style: str, context: str, remove_spine: bool) -> None: pass ``` **Input Constraints:** - `data` is a 2D NumPy array where each row represents a series of values. - `plot_type` is a string and must be either `\'box\'` or `\'violin\'`. - `theme`, `style`, and `context` are strings and must be one of the specified options. - `remove_spine` is a boolean value. **Output:** - The function should display the customized plot and save it as `customized_plot.png`. **Example:** ```python import numpy as np data = np.random.normal(size=(20, 6)) + np.arange(6) / 2 customized_plot(data, \'box\', \'darkgrid\', \'white\', \'notebook\', True) ``` In this example, the function will create a boxplot with the theme `\'darkgrid\'`, style `\'white\'`, and context `\'notebook\'`. It will also remove the top and right spines from the plot. The plot will be displayed and saved as `customized_plot.png`. **Notes:** - Use `seaborn` for setting themes, styles, and contexts. - Use matplotlib\'s `plt.savefig` method to save the plot. - Ensure that any changes made temporarily using the `with` statement do not affect subsequent plots.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def customized_plot(data: np.ndarray, plot_type: str, theme: str, style: str, context: str, remove_spine: bool) -> None: Generates a customized seaborn plot based on the provided parameters. # Validate plot_type if plot_type not in [\'box\', \'violin\']: raise ValueError(\\"Invalid plot_type. Choose either \'box\' or \'violin\'.\\") # Validate theme, style, and context valid_options = [\'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\'] if theme not in valid_options: raise ValueError(\\"Invalid theme. Choose from \'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\'.\\") if style not in valid_options: raise ValueError(\\"Invalid style. Choose from \'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\'.\\") valid_contexts = [\'paper\', \'notebook\', \'talk\', \'poster\'] if context not in valid_contexts: raise ValueError(\\"Invalid context. Choose from \'paper\', \'notebook\', \'talk\', \'poster\'.\\") # Set the seaborn style and context with sns.axes_style(style), sns.plotting_context(context): sns.set_theme(style=theme) # Create the plot if plot_type == \'box\': sns.boxplot(data=data) elif plot_type == \'violin\': sns.violinplot(data=data) # Remove spines if specified if remove_spine: sns.despine() # Save and show the plot plt.savefig(\'customized_plot.png\') plt.show()"},{"question":"**Objective**: Design and implement a function using PyTorch nested tensors to process sequences of variable lengths, manipulate their shapes, and perform arithmetic operations. # Description: You are given a list of 1-dimensional tensors, each of different lengths. Your task is to: 1. Construct a nested tensor (NJT) using this list. 2. Perform a given arithmetic operation on each tensor within the NJT. 3. Manipulate the shape of each tensor by adding an extra dimension. 4. Convert the resultant NJT back to a padded tensor with a specified padding value. # Function Signature: ```python import torch def process_variable_length_sequences(tensors: list, operation: str, padding_value: float) -> torch.Tensor: Function to process sequences of variable lengths. Parameters: tensors (list of torch.Tensor): A list of 1-dimensional tensors of varying lengths. operation (str): An arithmetic operation to perform on each tensor (\'add\', \'mul\', \'sub\', \'div\'). padding_value (float): The value to use for padding when converting the NJT to a padded tensor. Returns: torch.Tensor: A padded dense tensor after processing the NJT. pass ``` # Input: - `tensors`: A list of 1-dimensional `torch.Tensor` objects. Each tensor can have a different length. - `operation`: A string representing an arithmetic operation to perform on each tensor. It can be \'add\' (addition), \'mul\' (multiplication), \'sub\' (subtraction), or \'div\' (division). - `padding_value`: A floating-point value to use for padding when converting the NJT to a dense tensor. # Output: - A padded dense tensor (of type `torch.Tensor`) with each input tensor processed and transformed based on the specified operation, reshaped with an additional dimension, and padded with the given `padding_value`. # Constraints: - Ensure that the function handles edge cases such as empty lists and incompatible operations. - You may assume that the arithmetic operations are applied element-wise. - The function should work for tensors placed on either CPU or GPU. # Example: ```python import torch # Example input tensors = [torch.tensor([1, 2, 3]), torch.tensor([4, 5])] operation = \'mul\' padding_value = -1.0 # Expected output after processing: # tensor([[[ 2, 4, 6], # [ 1, -1, -1], # [-1, -1, -1]], # [[ 8, 10, -1], # [-1, -1, -1], # [-1, -1, -1]]]) result = process_variable_length_sequences(tensors, operation, padding_value) print(result) ``` The `process_variable_length_sequences` function should implement the following steps: 1. Create an NJT using the provided list of tensors. 2. Apply the specified arithmetic operation to each tensor within the NJT. 3. Add an extra dimension to each tensor. 4. Convert the NJT back to a padded dense tensor with the padding value specified. **Note:** Make sure to handle any potential errors, such as mismatched shapes or unsupported operations, gracefully within your implementation.","solution":"import torch import torch.nn.utils.rnn as rnn_utils def process_variable_length_sequences(tensors, operation, padding_value): Function to process sequences of variable lengths. Parameters: tensors (list of torch.Tensor): A list of 1-dimensional tensors of varying lengths. operation (str): An arithmetic operation to perform on each tensor (\'add\', \'mul\', \'sub\', \'div\'). padding_value (float): The value to use for padding when converting the NJT to a padded tensor. Returns: torch.Tensor: A padded dense tensor after processing the NJT. if not tensors: return torch.tensor([]) # Return empty tensor for empty input operations = { \'add\': torch.add, \'mul\': torch.mul, \'sub\': torch.sub, \'div\': torch.div } if operation not in operations: raise ValueError(f\\"Unsupported operation: {operation}\\") op_func = operations[operation] # Convert to nested tensor (list of tensors is `Nested`) nested_tensors = rnn_utils.pad_sequence(tensors, batch_first=True, padding_value=padding_value) result_tensor = [] for tensor in tensors: # Perform the operation (for simplicity, use scalar 2 as example) if operation == \'div\': tensor = op_func(tensor, 2) # Avoid division by zero issues else: tensor = op_func(tensor, 2) # Add extra dimension tensor = tensor.unsqueeze(dim=0) result_tensor.append(tensor) # Convert back to padded tensor with given padding_value padded_result = rnn_utils.pad_sequence([t.squeeze(0) for t in result_tensor], batch_first=True, padding_value=padding_value) return padded_result"},{"question":"**Problem Statement** You are required to implement a function that enumerates over all files and directories under a specified directory structure and performs certain operations dependent on the file types. This will involve using the `pathlib` module for filesystem path manipulations. **Function Signature:** ```python def analyze_directory_structure(path: str) -> dict: pass ``` **Input:** - `path` (str): A string representing the path to the root directory to be analyzed. **Output:** - `result` (dict): A dictionary with the following structure: ```python { \\"total_files\\": int, \\"total_directories\\": int, \\"total_size_bytes\\": int, \\"file_types\\": dict } ``` - `total_files` (int): The total number of files encountered in the directory. - `total_directories` (int): The total number of directories encountered. - `total_size_bytes` (int): The total size of all files, measured in bytes. - `file_types` (dict): A dictionary where the keys are file extensions and the values are counts of files with that extension. **Constraints:** 1. The function should handle deep and nested directory structures. 2. Symlinks should be resolved to avoid infinite loops and accurately count sizes/files. 3. For performance considerations, use efficient directory traversal techniques. **Examples:** ```python # Example 1 result = analyze_directory_structure(\\"/path/to/directory\\") print(result) # Output: {\'total_files\': 10, \'total_directories\': 4, \'total_size_bytes\': 1048576, \'file_types\': {\'.txt\': 3, \'.py\': 2, \'.jpg\': 5}} # Example 2 result = analyze_directory_structure(\\"C:/example/directory\\") print(result) # Output: {\'total_files\': 15, \'total_directories\': 7, \'total_size_bytes\': 2097152, \'file_types\': {\'.docx\': 7, \'.xlsx\': 4, \'.pdf\': 4}} ``` **Guidelines:** 1. Use the `pathlib` module extensively for all path manipulations. 2. Ensure that permissions and symlink issues are handled gracefully to avoid exceptions or missed files. 3. Write unit tests to validate your solution against various edge cases like empty directories, large file sizes, and deep nesting. Good luck, and showcase your understanding of `pathlib` and efficient filesystem operations!","solution":"from pathlib import Path def analyze_directory_structure(path: str) -> dict: Analyzes the directory structure and returns detailed information. Parameters: path (str): The path to the root directory to be analyzed. Returns: dict: A dictionary containing total files, directories, size, and file types. total_files = 0 total_directories = 0 total_size_bytes = 0 file_types = {} root_path = Path(path) # Ensuring the given path is valid if not root_path.exists() or not root_path.is_dir(): raise ValueError(f\\"The provided path \'{path}\' is not a valid directory.\\") for p in root_path.rglob(\'*\'): if p.is_file(): total_files += 1 total_size_bytes += p.stat().st_size file_extension = p.suffix if file_extension in file_types: file_types[file_extension] += 1 else: file_types[file_extension] = 1 elif p.is_dir(): total_directories += 1 return { \\"total_files\\": total_files, \\"total_directories\\": total_directories, \\"total_size_bytes\\": total_size_bytes, \\"file_types\\": file_types }"},{"question":"# Custom Scikit-Learn Estimator **Objective:** Implement a custom scikit-learn estimator that classifies data using a simplified K-Nearest Neighbors algorithm. This exercise aims to assess your understanding of scikit-learn conventions and the development of compliant estimators. **Description:** You need to create a class called `CustomKNNClassifier` that mimics a basic K-Nearest Neighbors classifier. Follow the scikit-learn estimator guidelines to make your class compatible with scikit-learn pipelines and model selection tools. **Requirements:** 1. **Class Definition:** - Define a class named `CustomKNNClassifier` that inherits from `BaseEstimator` and `ClassifierMixin`. 2. **Initialization (`__init__`):** - The class should have one hyperparameter `n_neighbors` with a default value of 3. 3. **Fit Method:** - Implement the `fit` method that takes `X` (array-like of shape `(n_samples, n_features)`) and `y` (array-like of shape `(n_samples,)`) as input. - The method should validate the input shapes and assign the input data to instance variables. 4. **Predict Method:** - Implement the `predict` method that takes `X` (array-like of shape `(n_samples, n_features)`) as input and outputs predicted labels. - The method should calculate the Euclidean distance between input samples and the training data. - For each input sample, predict the label based on majority voting among the `n_neighbors` nearest neighbors from the training data. 5. **Attributes:** - Ensure that the class maintains an attribute `n_features_in_` to store the number of features seen during fit. - Store the classes seen during fit in an attribute `classes_`. **Constraints:** - `n_neighbors` should be a positive integer. - Input arrays `X` and `y` provided to `fit` should have the same number of samples. - Raise a `ValueError` if any of these constraints are violated. **Example Usage:** ```python from sklearn.utils.estimator_checks import check_estimator # Example data X_train = [[1, 2], [3, 4], [5, 6], [7, 8]] y_train = [0, 1, 0, 1] X_test = [[2, 3], [6, 7]] # Initialize and fit the classifier knn = CustomKNNClassifier(n_neighbors=2) knn.fit(X_train, y_train) # Predict pred = knn.predict(X_test) print(pred) # Output should be a list of predicted labels e.g., [0, 1] # Check compatibility with scikit-learn check_estimator(CustomKNNClassifier()) # This should pass without errors ``` **Submission:** Submit a single Python file `custom_knn.py` containing the `CustomKNNClassifier` class implementation. This task assesses your ability to follow scikit-learn estimator guidelines, understand class inheritance, and implement machine learning algorithms.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from sklearn.neighbors import NearestNeighbors class CustomKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=3): self.n_neighbors = n_neighbors def fit(self, X, y): # Validate the inputs X, y = check_X_y(X, y) if self.n_neighbors <= 0: raise ValueError(\\"n_neighbors must be a positive integer\\") self.classes_ = np.unique(y) self.n_features_in_ = X.shape[1] self.X_ = X self.y_ = y return self def predict(self, X): # Check that fit has been called check_is_fitted(self) # Validate the input X = check_array(X) if X.shape[1] != self.n_features_in_: raise ValueError(\\"Number of features of the input must be equal to the number of features in the training data\\") neighbors = NearestNeighbors(n_neighbors=self.n_neighbors) neighbors.fit(self.X_) distances, indices = neighbors.kneighbors(X) y_pred = [] for idx in indices: # Find the most common class among the nearest neighbors nearest_labels = self.y_[idx] most_common = np.argmax(np.bincount(nearest_labels)) y_pred.append(most_common) return np.array(y_pred)"},{"question":"Objective In this assignment, you will demonstrate your understanding of the `unicodedata` module in Python by implementing a function that can analyze and normalize Unicode strings. Problem Statement Write a Python function `analyze_unicode_string(unistr: str) -> dict` that takes a Unicode string and returns a dictionary with the following information for each character in the string: 1. The Unicode name of the character. 2. The general category of the character. 3. The decimal value of the character (if applicable). 4. The digit value of the character (if applicable). 5. The numeric value of the character (if applicable). 6. The bidirectional class of the character. 7. Whether the character is mirrored in bidirectional text. 8. The decomposition mapping of the character. 9. The East Asian width of the character. 10. The canonical combining class of the character. In addition, the function should also return the following information for the entire Unicode string: 1. Whether the string is in NFC, NFD, NFKC, and NFKD normal forms. 2. The normalized forms of the string (NFC, NFD, NFKC, NFKD). Input - `unistr` (str): A Unicode string. Output - A dictionary containing the detailed analysis of each character in the string and the normalization information for the entire string. Example ```python import unicodedata def analyze_unicode_string(unistr: str) -> dict: result = {\'characters\': [], \'normalization\': {}} for char in unistr: char_info = { \'character\': char, \'name\': unicodedata.name(char, \'N/A\'), \'category\': unicodedata.category(char), \'decimal\': unicodedata.decimal(char, \'N/A\'), \'digit\': unicodedata.digit(char, \'N/A\'), \'numeric\': unicodedata.numeric(char, \'N/A\'), \'bidirectional\': unicodedata.bidirectional(char), \'mirrored\': unicodedata.mirrored(char), \'decomposition\': unicodedata.decomposition(char), \'east_asian_width\': unicodedata.east_asian_width(char), \'combining\': unicodedata.combining(char) } result[\'characters\'].append(char_info) normalization_forms = [\'NFC\', \'NFD\', \'NFKC\', \'NFKD\'] for form in normalization_forms: result[\'normalization\'][f\'is_{form}\'] = unicodedata.is_normalized(form, unistr) result[\'normalization\'][form] = unicodedata.normalize(form, unistr) return result # Example usage unicode_string = \\"Café\\" analysis_result = analyze_unicode_string(unicode_string) print(analysis_result) ``` Constraints - The input string `unistr` should be a valid Unicode string. Performance Requirements - Ensure that the function runs efficiently, even for long strings.","solution":"import unicodedata def analyze_unicode_string(unistr: str) -> dict: result = {\'characters\': [], \'normalization\': {}} for char in unistr: char_info = { \'character\': char, \'name\': unicodedata.name(char, \'N/A\'), \'category\': unicodedata.category(char), \'decimal\': unicodedata.decimal(char, \'N/A\'), \'digit\': unicodedata.digit(char, \'N/A\'), \'numeric\': unicodedata.numeric(char, \'N/A\'), \'bidirectional\': unicodedata.bidirectional(char), \'mirrored\': unicodedata.mirrored(char), \'decomposition\': unicodedata.decomposition(char), \'east_asian_width\': unicodedata.east_asian_width(char), \'combining\': unicodedata.combining(char) } result[\'characters\'].append(char_info) normalization_forms = [\'NFC\', \'NFD\', \'NFKC\', \'NFKD\'] for form in normalization_forms: result[\'normalization\'][f\'is_{form}\'] = unicodedata.is_normalized(form, unistr) result[\'normalization\'][form] = unicodedata.normalize(form, unistr) return result"},{"question":"Task You are given a Python script in the form of a string. Your task is to write a function that takes this script, analyzes its content using Python\'s `ast` module, and transforms it by doubling all the numeric constants in the script. Finally, the function will return the transformed script in its original string format. Function Signature ```python def double_numeric_constants(script: str) -> str: ``` Input - `script`: A string representing a valid Python script. The script will contain variables, expressions, functions, and potentially more complex structures. Output - A string representing the transformed Python script where all numeric constants have been doubled. Constraints - You may assume the input script is valid Python code. - The script will not contain any syntax errors. - Use the `ast` module to parse, manipulate, and unparse the script. Example Given the script: ```python a = 3 b = a + 4 def foo(x): return x * 2 ``` The output should be: ```python a = 6 b = a + 8 def foo(x): return x * 4 ``` Instructions 1. Parse the input Python script using `ast.parse`. 2. Traverse the AST and double all numeric constants (`ast.Constant` nodes with a numeric value). 3. Use `ast.unparse` to convert the modified AST back to a source code string. 4. Return the transformed script as the result. Implementation Tips - Utilize `ast.NodeTransformer` to create a custom transformer class. - Ensure all necessary attributes like `lineno` and `col_offset` are correctly handled in the transformed AST nodes. - Use `ast.fix_missing_locations` if necessary to ensure the transformed AST is valid.","solution":"import ast class DoubleNumericConstantsTransformer(ast.NodeTransformer): def visit_Constant(self, node): if isinstance(node.value, (int, float)): return ast.Constant(node.value * 2, kind=None) return node def double_numeric_constants(script: str) -> str: tree = ast.parse(script) transformer = DoubleNumericConstantsTransformer() tree = transformer.visit(tree) ast.fix_missing_locations(tree) return ast.unparse(tree) # Example usage script = a = 3 b = a + 4 def foo(x): return x * 2 transformed_script = double_numeric_constants(script) print(transformed_script)"},{"question":"**Title: Advanced Iterator and Functional Utility Construction** **Objective:** Implement a utility function that processes a large dataset using iterators, generator functions, and relevant tools from the `itertools` and `functools` modules to demonstrate proficiency in Python\'s functional programming features. --- **Problem Statement:** You are provided with a large dataset, which is a stream of potentially infinite user activity logs. Each log record is a tuple `(timestamp, user_id, action)`. The `timestamp` is a string representing the time of activity, `user_id` is an integer representing the unique ID of a user, and `action` is a string representing the user\'s action. Implement a function `process_activity_logs(logs)` that performs the following tasks using iterators and functional programming techniques: 1. **Filter**: Remove logs where the action is \\"logout\\". 2. **Transformation**: For the remaining logs, convert the `timestamp` from string to a Python `datetime` object. 3. **Grouping**: Group the logs by `user_id`. 4. **Aggregation**: For each group, count the number of distinct actions taken by the user. Finally, return an iterator that yields tuples in the form `(user_id, action_count_dict)` where `action_count_dict` is a dictionary mapping each distinct action to its count for the user. --- **Function Signature**: ```python from typing import Iterator, Tuple, Dict from datetime import datetime def process_activity_logs(logs: Iterator[Tuple[str, int, str]]) -> Iterator[Tuple[int, Dict[str, int]]]: pass ``` **Input:** - `logs`: An iterator of tuples `(timestamp, user_id, action)` representing user activity logs. **Output:** - An iterator of tuples `(user_id, action_count_dict)`. **Constraints:** - The `timestamp` string is in the format `\'%Y-%m-%d %H:%M:%S\'`. - The dataset can be very large and should be processed efficiently using iterators and generators to ensure scalability. --- **Example:** ```python logs = iter([ (\'2023-01-01 10:00:00\', 1, \'login\'), (\'2023-01-01 10:05:00\', 1, \'click\'), (\'2023-01-01 10:10:00\', 1, \'logout\'), (\'2023-01-01 10:15:00\', 2, \'login\'), (\'2023-01-01 10:20:00\', 2, \'click\') ]) result = process_activity_logs(logs) for user_log in result: print(user_log) # Expected Output: # (1, {\'login\': 1, \'click\': 1}) # (2, {\'login\': 1, \'click\': 1}) ``` **Note:** - You should avoid materializing the entire dataset in memory. - Utilize built-in functions, generator expressions, and the itertools and functools modules to solve this task.","solution":"from typing import Iterator, Tuple, Dict from datetime import datetime import itertools from collections import defaultdict def process_activity_logs(logs: Iterator[Tuple[str, int, str]]) -> Iterator[Tuple[int, Dict[str, int]]]: # Step 1: Filter logs where action is not \\"logout\\" filtered_logs = (log for log in logs if log[2] != \'logout\') # Step 2: Transform timestamp to datetime object transformed_logs = ((datetime.strptime(log[0], \'%Y-%m-%d %H:%M:%S\'), log[1], log[2]) for log in filtered_logs) # Step 3: Group logs by user_id sorted_logs = sorted(transformed_logs, key=lambda x: x[1]) grouped_logs = itertools.groupby(sorted_logs, key=lambda x: x[1]) # Step 4: Aggregate logs to count distinct actions for each user_id for user_id, user_logs in grouped_logs: action_count_dict = defaultdict(int) for _, _, action in user_logs: action_count_dict[action] += 1 yield user_id, dict(action_count_dict)"},{"question":"# HTML Entities to Unicode Conversion In this task, you are required to write a function `convert_html_entities_to_unicode` that transforms a string containing HTML character entities into a string with the corresponding Unicode characters. Function Signature ```python def convert_html_entities_to_unicode(s: str) -> str: pass ``` Input - `s` (str): A string potentially containing HTML character entities (e.g., `&lt;`, `&gt;`, `&amp;`, etc.). Output - (str): A new string where all HTML character entities are replaced by their corresponding Unicode characters. Constraints - The input string `s` will have a maximum length of 1000 characters. - Only valid HTML entities present in the `html.entities.html5` dictionary should be considered. Example ```python # Example 1 input_str = \\"Hello &gt; World\\" assert convert_html_entities_to_unicode(input_str) == \\"Hello > World\\" # Example 2 input_str = \\"5 &lt; 10 &amp; 20 &gt; 15\\" assert convert_html_entities_to_unicode(input_str) == \\"5 < 10 & 20 > 15\\" ``` Notes 1. You can utilize the `html.entities.html5` dictionary to get the equivalent Unicode character for an HTML entity. 2. The function should account for the possibility that HTML entities may or may not end with a semicolon and handle both cases appropriately. 3. Do not use external libraries for this task, only pure Python and the provided dictionaries. Good luck!","solution":"import html def convert_html_entities_to_unicode(s: str) -> str: Converts HTML entities in the string s to their corresponding Unicode characters. return html.unescape(s)"},{"question":"Objective Assess your understanding of the Seaborn library, specifically the `sns.pointplot` functionality, by creating and customizing point plots with multiple layers of complexity. Dataset We will use the `penguins` dataset available in the Seaborn library. Ensure you have the necessary library and dataset loaded. Task 1. **Load the `penguins` dataset** using Seaborn. 2. **Create a point plot** that shows the average `bill_length_mm` for each `island` differentiated by `species`. 3. **Enhance the plot** by: - Adding a second layer of grouping based on `sex` and representing this layer using different markers and linestyles. - Using error bars to represent the standard deviation of each distribution. 4. **Dodge** the points along the categorical axis to reduce overplotting. 5. **Customize the appearance** of the plot by: - Changing the color to `\\".6\\"`. - Setting the marker to `\\"D\\"`. - Removing the default error bars and adding custom error bars that represent the 95% confidence interval. 6. **Format the tick labels** for the `island` variable to uppercase letters. Input and Output Format There are no specific input and output formats. This task involves generating and displaying the required plot. Constraints and Limitations - Ensure the plot is clear and well-labeled. - Adhere to the guidelines for plot customization. - Use in-line comments to explain each customization step. Requirements - Your solution should run efficiently and make use of Seaborn\'s in-built functionalities. - Code readability and proper documentation will be considered during evaluation. Performance Requirements - The code should run within a reasonable time frame (less than a few seconds for execution). - The plot should be easily interpretable and visually distinct. Example Here is a baseline example to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the basic point plot sns.pointplot(data=penguins, x=\\"island\\", y=\\"bill_length_mm\\", hue=\\"species\\") # Customize and enhance the plot as per the task requirements # Your additional customization code goes here # Display the plot plt.show() ``` Adapt and expand upon this example to fulfill the entire task. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Function to calculate the 95% confidence interval def ci_95(data): return 1.96 * np.std(data) / np.sqrt(len(data)) # Create the point plot with enhancements def create_custom_pointplot(): plt.figure(figsize=(10, 6)) sns.pointplot( data=penguins, x=\\"island\\", y=\\"bill_length_mm\\", hue=\\"species\\", markers=[\\"o\\", \\"s\\", \\"D\\"], # Different markers for different species linestyles=[\\"-\\", \\"--\\", \\"-.\\"], # Different linestyles for different species dodge=True, color=\\".6\\", # Color marker=\\"D\\", # Marker type ci=\'sd\', # Using standard deviation for error bars as an intermediary step ) # Customize tick labels to uppercase letters plt.xticks(ticks=range(len(penguins[\'island\'].unique())), labels=[label.upper() for label in penguins[\'island\'].unique()]) # Manually add the custom 95% confidence interval error bars for island in penguins[\'island\'].unique(): island_data = penguins[penguins[\'island\'] == island] for species in penguins[\'species\'].unique(): species_data = island_data[island_data[\'species\'] == species] for sex in species_data[\'sex\'].unique(): sex_data = species_data[species_data[\'sex\'] == sex] if not sex_data.empty: mean_bill_length = sex_data[\'bill_length_mm\'].mean() conf_interval = ci_95(sex_data[\'bill_length_mm\']) x = list(penguins[\'island\'].unique()).index(island) - 0.15 + 0.30 * list(penguins[\'species\'].unique()).index(species) plt.errorbar(x, mean_bill_length, yerr=conf_interval, fmt=\'none\', c=\'red\', capsize=5) plt.title(\'Average Bill Length by Island and Species with 95% CI\') plt.show() create_custom_pointplot()"},{"question":"# PyTorch ProcessGroupNCCL Configuration Checker Objective You are required to write a PyTorch helper function that reads and validates configuration settings for `ProcessGroupNCCL` based on the provided environment variables. This requires you to ensure that specific rules and constraints are adhered to for a robust distributed training setup. Function Specifications 1. **Function Name**: `validate_nccl_config` 2. **Input Parameters**: None. 3. **Output**: - A dictionary containing the environment variables and their values. - A list of any errors or warnings observed based on the validation rules. 4. **Validation Rules**: - If `TORCH_NCCL_DUMP_ON_TIMEOUT` is set, `TORCH_NCCL_TRACE_BUFFER_SIZE` must be larger than 0. - If `TORCH_NCCL_ENABLE_MONITORING` is set to `1`, `TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC` must be defined and greater than 0. - `TORCH_NCCL_TRACE_BUFFER_SIZE` should not be negative. - `TORCH_NCCL_COORD_CHECK_MILSEC` and `TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC` must be positive integers if defined. Constraints - The function must handle undefined environment variables gracefully by assigning default values or reporting their absence. - You may use the `os` module to access environment variables. - Assume default values for any undefined environment variables if described in the documentation snippet. Example Usage ```python import os # Example environment variables setup for illustration os.environ[\'TORCH_NCCL_DUMP_ON_TIMEOUT\'] = \'1\' os.environ[\'TORCH_NCCL_TRACE_BUFFER_SIZE\'] = \'4096\' os.environ[\'TORCH_NCCL_ENABLE_MONITORING\'] = \'1\' os.environ[\'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\'] = \'20\' output, errors = validate_nccl_config() print(\\"Configuration:\\", output) print(\\"Errors/Warnings:\\", errors) ``` Expected Output For the given example input, the expected output would be: ``` Configuration: { \'TORCH_NCCL_DUMP_ON_TIMEOUT\': 1, \'TORCH_NCCL_TRACE_BUFFER_SIZE\': 4096, \'TORCH_NCCL_ENABLE_MONITORING\': 1, \'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\': 20, ... } Errors/Warnings: [] ``` If `TORCH_NCCL_TRACE_BUFFER_SIZE` had been set to `-1`, it would be reported in the errors or warnings. Notes - Implementations can be tested with different sets of environment variables to ensure compliance with the specified validation rules. - You may expand the validation rules based on additional inferred constraints from the given documentation snippet.","solution":"import os def validate_nccl_config(): config = { \'TORCH_NCCL_DUMP_ON_TIMEOUT\': os.getenv(\'TORCH_NCCL_DUMP_ON_TIMEOUT\', None), \'TORCH_NCCL_TRACE_BUFFER_SIZE\': int(os.getenv(\'TORCH_NCCL_TRACE_BUFFER_SIZE\', \'0\')), \'TORCH_NCCL_ENABLE_MONITORING\': os.getenv(\'TORCH_NCCL_ENABLE_MONITORING\', None), \'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\': int(os.getenv(\'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\', \'0\')), \'TORCH_NCCL_COORD_CHECK_MILSEC\': int(os.getenv(\'TORCH_NCCL_COORD_CHECK_MILSEC\', \'0\')), \'TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC\': int(os.getenv(\'TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC\', \'0\')), } errors_warnings = [] if config[\'TORCH_NCCL_DUMP_ON_TIMEOUT\'] is not None and config[\'TORCH_NCCL_TRACE_BUFFER_SIZE\'] <= 0: errors_warnings.append(\\"TORCH_NCCL_TRACE_BUFFER_SIZE must be larger than 0 if TORCH_NCCL_DUMP_ON_TIMEOUT is set.\\") if config[\'TORCH_NCCL_ENABLE_MONITORING\'] == \'1\' and config[\'TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\'] <= 0: errors_warnings.append(\\"TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC must be defined and greater than 0 if TORCH_NCCL_ENABLE_MONITORING is set to 1.\\") if config[\'TORCH_NCCL_TRACE_BUFFER_SIZE\'] < 0: errors_warnings.append(\\"TORCH_NCCL_TRACE_BUFFER_SIZE should not be negative.\\") if config[\'TORCH_NCCL_COORD_CHECK_MILSEC\'] < 0: errors_warnings.append(\\"TORCH_NCCL_COORD_CHECK_MILSEC must be a positive integer if defined.\\") if config[\'TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC\'] < 0: errors_warnings.append(\\"TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC must be a positive integer if defined.\\") return config, errors_warnings"},{"question":"Coding Assessment Question # Objective: Design a testing suite for a `Store` class, which handles products using the `unittest.mock` module. This question will test your ability to utilize the `mock` library for unit testing under various conditions such as call verification and complex mock behavior. # Task: You are provided with a class `Store` and a function in your codebase where you need to ensure that the `Store` operations are thoroughly tested using `unittest.mock`. `Store` Class: ```python class Store: def __init__(self): self.products = [] def add_product(self, product): self.products.append(product) def remove_product(self, product): self.products.remove(product) def get_inventory(self): return self.products def find_product(self, product_name): for product in self.products: if product[\'name\'] == product_name: return product return None def process_order(self, order): for product_name, qty in order.items(): product = self.find_product(product_name) if product and product[\'quantity\'] >= qty: product[\'quantity\'] -= qty else: raise ValueError(f\\"Product {product_name} is out of stock or insufficient quantity\\") ``` # Requirements: 1. **Mock the `Store` Class:** - Mock the `add_product` and `remove_product` methods. - Verify that `add_product` is called once with the correct product. - Verify that `remove_product` is called with the correct product. 2. **Patch Method (`find_product`):** - Patch the `find_product` method for testing `process_order`. - Ensure that `process_order` calls `find_product` the correct number of times with the expected arguments. - Simulate a scenario where `find_product` should return `None`. 3. **Track Calls and Return Values:** - Use the mock object to track calls to `get_inventory`. - Mock the return value of the `get_inventory` method to return a specific product list. - Verify that the mocked return value is correctly returned in the test. 4. **Exception Handling:** - Test `process_order` to ensure it raises a `ValueError` when there is insufficient product quantity or the product is not found. 5. **Mocking Dictionary and Iterables:** - Mock a dictionary scenario within `process_order` using side_effect to return different values for the `find_product` method. # Constraints: - Use Python 3.10. - Utilize `unittest.mock` capabilities extensively. - Ensure that the tests are self-contained and runnable independently. # Example Test Skeleton: ```python import unittest from unittest.mock import Mock, patch, call class StoreTests(unittest.TestCase): def test_add_product(self): # Implement your test here pass def test_remove_product(self): # Implement your test here pass def test_find_product(self): # Implement your test here pass def test_get_inventory(self): # Implement your test here pass def test_process_order_success(self): # Implement your test here pass def test_process_order_failure(self): # Implement your test here pass if __name__ == \'__main__\': unittest.main() ``` # Submission: - Implement the test cases in the `StoreTests` class as per the requirements above. - Ensure all tests pass successfully when the `Store` class is correctly implemented.","solution":"class Store: def __init__(self): self.products = [] def add_product(self, product): self.products.append(product) def remove_product(self, product): self.products.remove(product) def get_inventory(self): return self.products def find_product(self, product_name): for product in self.products: if product[\'name\'] == product_name: return product return None def process_order(self, order): for product_name, qty in order.items(): product = self.find_product(product_name) if product and product[\'quantity\'] >= qty: product[\'quantity\'] -= qty else: raise ValueError(f\\"Product {product_name} is out of stock or insufficient quantity\\")"},{"question":"# PyTorch Advanced Tensors and Operations Objective: To assess the understanding of PyTorch fundamental and advanced concepts, including tensor creation, random sampling, and mathematical operations. Problem Statement: Implement a function `generate_and_transform` in PyTorch that performs the following steps: 1. **Tensor Generation**: - Create two tensors `A` and `B` of shape (n, m) filled with values drawn from a normal distribution with mean 0 and standard deviation 1. - Create a tensor `C` of shape (m, n) with values uniformly sampled between 0 and 1. 2. **Matrix Operations**: - Compute the element-wise product of `A` and `B` to get tensor `D`. - Compute the matrix multiplication of `D` and `C` to get tensor `E`. 3. **Reduction Operations**: - Compute the row-wise mean of tensor `E`. - Compute the column-wise mean of tensor `E`. 4. **Transformation**: - Apply the sigmoid function element-wise to tensor `E`. 5. **Return**: - The final transformed tensor `E`, the row-wise means, and the column-wise means. Function Signature: ```python import torch def generate_and_transform(n: int, m: int) -> (torch.Tensor, torch.Tensor, torch.Tensor): Generates tensors A and B using normal distribution, tensor C with uniform distribution, performs specified transformations and returns the transformed tensor along with row-wise and column-wise means. Parameters: n (int): Number of rows. m (int): Number of columns. Returns: tuple: A tuple containing: - Transformed tensor (torch.Tensor) - Row-wise means (torch.Tensor) - Column-wise means (torch.Tensor) pass ``` Constraints: - You must use PyTorch tensor operations for this task. - Ensure that your implementation efficiently handles the tensor operations in terms of both time and space complexity. Example: ```python n, m = 3, 4 E, row_means, col_means = generate_and_transform(n, m) print(\\"Transformed Tensor (E):\\", E) print(\\"Row-wise Means:\\", row_means) print(\\"Column-wise Means:\\", col_means) ``` The example values can vary as the tensors `A`, `B`, and `C` are randomly generated. Notes: - The normal distribution for tensors `A` and `B` can be generated using `torch.randn`. - The uniform distribution for tensor `C` can be generated using `torch.rand`. - The element-wise product can be computed using the `*` operator. - The matrix multiplication can be computed using `torch.matmul`. - Row-wise and column-wise means can be computed using the `mean` method with specified dimensions. - The sigmoid function is available in PyTorch as `torch.sigmoid`. Good luck and happy coding!","solution":"import torch def generate_and_transform(n: int, m: int) -> (torch.Tensor, torch.Tensor, torch.Tensor): Generates tensors A and B using normal distribution, tensor C with uniform distribution, performs specified transformations and returns the transformed tensor along with row-wise and column-wise means. Parameters: n (int): Number of rows. m (int): Number of columns. Returns: tuple: A tuple containing: - Transformed tensor (torch.Tensor) - Row-wise means (torch.Tensor) - Column-wise means (torch.Tensor) # Generate tensors A and B using normal distribution (mean=0, std=1) A = torch.randn(n, m) B = torch.randn(n, m) # Generate tensor C using uniform distribution (between 0 and 1) C = torch.rand(m, n) # Element-wise product of A and B to get tensor D D = A * B # Matrix multiplication of D and C to get tensor E E = torch.matmul(D, C) # Compute row-wise mean of tensor E row_means = E.mean(dim=1) # Compute column-wise mean of tensor E col_means = E.mean(dim=0) # Apply the sigmoid function element-wise to tensor E E_transformed = torch.sigmoid(E) return E_transformed, row_means, col_means"},{"question":"Objective: Your task is to implement a function that generates a comparative plot using seaborn, demonstrating the use of different error bar methodologies on a randomly generated dataset. Function Signature: ```python def generate_comparative_errorbar_plot(random_seed: int, sample_size: int) -> None: Generates a comparative plot with various error bar methodologies (standard deviation, standard error, percentile interval, confidence interval). Parameters: - random_seed (int): The seed for the random number generator to ensure reproducibility. - sample_size (int): The number of data points to generate for the dataset. Returns: - None: The function should display the plot directly. ``` Input: - `random_seed` (int): An integer to seed the random number generator. - `sample_size` (int): An integer indicating the number of data points. Output: - The function should not return anything, but should display a matplotlib figure with 4 subplots arranged in a 2x2 grid. - Each subplot should contain a point plot with a different error bar methodology: - Top-left: Standard deviation (`errorbar=\\"sd\\"`) - Top-right: Standard error (`errorbar=\\"se\\"`) - Bottom-left: Percentile Interval (default 95%) (`errorbar=\\"pi\\"`) - Bottom-right: Confidence Interval (default 95%) (`errorbar=\\"ci\\"`) Constraints: - You should use seaborn and matplotlib for plotting. - Ensure that each subplot uses the same dataset for consistency. - Use appropriate labels and titles to differentiate the error bar methodologies in each subplot. Example: Calling `generate_comparative_errorbar_plot(42, 100)` should display a comparative plot with the specified configurations. Additional Notes: - Pay attention to reproducibility by setting the random seed as indicated. - Ensure that the plot is not cluttered and that each subplot is clearly distinguishable.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def generate_comparative_errorbar_plot(random_seed: int, sample_size: int) -> None: Generates a comparative plot with various error bar methodologies (standard deviation, standard error, percentile interval, confidence interval). Parameters: - random_seed (int): The seed for the random number generator to ensure reproducibility. - sample_size (int): The number of data points to generate for the dataset. Returns: - None: The function should display the plot directly. # Set random seed for reproducibility np.random.seed(random_seed) # Generate random data data = np.random.normal(size=sample_size) df = pd.DataFrame(data, columns=[\'value\']) df[\'category\'] = np.random.choice([\'A\', \'B\', \'C\', \'D\'], size=sample_size) # Create a 2x2 subplot layout fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Top-left: Standard deviation sns.pointplot(x=\'category\', y=\'value\', data=df, errorbar=\'sd\', ax=axes[0, 0]) axes[0, 0].set_title(\'Standard Deviation\') # Top-right: Standard error sns.pointplot(x=\'category\', y=\'value\', data=df, errorbar=\'se\', ax=axes[0, 1]) axes[0, 1].set_title(\'Standard Error\') # Bottom-left: Percentile Interval (default 95%) sns.pointplot(x=\'category\', y=\'value\', data=df, errorbar=\'pi\', ax=axes[1, 0]) axes[1, 0].set_title(\'Percentile Interval (95%)\') # Bottom-right: Confidence Interval (default 95%) sns.pointplot(x=\'category\', y=\'value\', data=df, errorbar=\'ci\', ax=axes[1, 1]) axes[1, 1].set_title(\'Confidence Interval (95%)\') # Adjust layout for better display plt.tight_layout() plt.show()"},{"question":"# Pandas Categorical Data Analysis You are provided with sales data for a small retail company. The company records each sale with the following attributes: - `ProductID`: ID of the product sold. - `Category`: Category of the product, such as electronics, clothing, etc. - `Region`: The region in which the sale was made. - `SalesAmount`: Amount of the sale. The data is listed below as dictionaries for each field. Your task is to perform the following steps using pandas and categorical data types: 1. **Create a DataFrame** from the provided dictionaries. 2. **Convert the `Category` and `Region` columns** to categorical data types with specified order: - `Category`: ordered as `[\'Electronics\', \'Clothing\', \'Groceries\', \'Home\']` - `Region`: ordered as `[\'North\', \'South\', \'East\', \'West\']` 3. Add a new category `\'Stationery\'` to the `Category` column. 4. Remove the unused category `\'Stationery\'` from the `Category` column. 5. **Reorder the `Category` column** to `[\'Clothing\', \'Home\', \'Electronics\', \'Groceries\']`. 6. Fill any missing values in the `SalesAmount` column with the mean sales amount. **Input Format:** ```python product_ids = [1, 2, 3, 4, 5, 6] categories = [\'Electronics\', \'Clothing\', \'Groceries\', \'Home\', \'Groceries\', \'Clothing\'] regions = [\'North\', \'South\', \'East\', \'North\', \'West\', \'South\'] sales_amounts = [250.0, 150.0, None, 300.0, 175.0, 200.0] ``` **Output:** 1. DataFrame after converting columns to categorical data types and adding the new category. 2. DataFrame after removing unused categories and reordering the `Category` column. 3. DataFrame after filling missing values in the `SalesAmount` column. # Below is the expected format of the DataFrame: | ProductID | Category | Region | SalesAmount | |-----------|--------------|--------|-------------| | 1 | Electronics | North | 250.0 | | 2 | Clothing | South | 150.0 | | 3 | Groceries | East | ... | ... # Code Implementation Write your code in the `categorical_analysis` function: ```python import pandas as pd import numpy as np from pandas.api.types import CategoricalDtype def categorical_analysis(product_ids, categories, regions, sales_amounts): # Step 1: Create DataFrame data = {\'ProductID\': product_ids, \'Category\': categories, \'Region\': regions, \'SalesAmount\': sales_amounts} df = pd.DataFrame(data) # Step 2: Convert columns to categorical with specified order category_dtype = CategoricalDtype(categories=[\'Electronics\', \'Clothing\', \'Groceries\', \'Home\'], ordered=True) region_dtype = CategoricalDtype(categories=[\'North\', \'South\', \'East\', \'West\'], ordered=True) df[\'Category\'] = df[\'Category\'].astype(category_dtype) df[\'Region\'] = df[\'Region\'].astype(region_dtype) # Step 3: Add new category df[\'Category\'] = df[\'Category\'].cat.add_categories([\'Stationery\']) # Output step 3 result print(df) # Step 4: Remove unused category df[\'Category\'] = df[\'Category\'].cat.remove_categories([\'Stationery\']) # Step 5: Reorder categories df[\'Category\'] = df[\'Category\'].cat.reorder_categories([\'Clothing\', \'Home\', \'Electronics\', \'Groceries\'], ordered=True) # Output step 5 result print(df) # Step 6: Fill missing SalesAmount with mean df[\'SalesAmount\'] = df[\'SalesAmount\'].fillna(df[\'SalesAmount\'].mean()) # Output final result print(df) return df # Sample data product_ids = [1, 2, 3, 4, 5, 6] categories = [\'Electronics\', \'Clothing\', \'Groceries\', \'Home\', \'Groceries\', \'Clothing\'] regions = [\'North\', \'South\', \'East\', \'North\', \'West\', \'South\'] sales_amounts = [250.0, 150.0, None, 300.0, 175.0, 200.0] # Function call df_result = categorical_analysis(product_ids, categories, regions, sales_amounts) print(df_result) ``` Ensure to create methods/functions as needed and add comments to make your code clean and understandable. **Good Luck!**","solution":"import pandas as pd import numpy as np from pandas.api.types import CategoricalDtype def categorical_analysis(product_ids, categories, regions, sales_amounts): # Step 1: Create DataFrame data = {\'ProductID\': product_ids, \'Category\': categories, \'Region\': regions, \'SalesAmount\': sales_amounts} df = pd.DataFrame(data) # Step 2: Convert columns to categorical with specified order category_dtype = CategoricalDtype(categories=[\'Electronics\', \'Clothing\', \'Groceries\', \'Home\'], ordered=True) region_dtype = CategoricalDtype(categories=[\'North\', \'South\', \'East\', \'West\'], ordered=True) df[\'Category\'] = df[\'Category\'].astype(category_dtype) df[\'Region\'] = df[\'Region\'].astype(region_dtype) # Step 3: Add new category df[\'Category\'] = df[\'Category\'].cat.add_categories([\'Stationery\']) # Step 4: Remove unused category df[\'Category\'] = df[\'Category\'].cat.remove_categories([\'Stationery\']) # Step 5: Reorder categories df[\'Category\'] = df[\'Category\'].cat.reorder_categories([\'Clothing\', \'Home\', \'Electronics\', \'Groceries\'], ordered=True) # Step 6: Fill missing SalesAmount with mean df[\'SalesAmount\'] = df[\'SalesAmount\'].fillna(df[\'SalesAmount\'].mean()) return df"},{"question":"Objective: Demonstrate your understanding of Python\'s `warnings` module by creating a complex function that issues, filters, and handles multiple types of warnings. Problem Statement: Write a function named `manage_warnings` that takes a list of tuples where each tuple contains: 1. A warning message (string). 2. A warning category (one of the standard Python warning categories such as UserWarning, DeprecationWarning, etc.). 3. A filter action (one of `default`, `ignore`, `always`, `module`, `once`). 4. A boolean flag `temporary` that indicates if the filter should be applied temporarily using a context manager. The function should: 1. Issue the warning according to the message and category provided in each tuple. 2. Apply the specified filter action to control how warnings are displayed or suppressed. 3. If the `temporary` flag is `True`, apply the filter action only within a context manager so that it doesn\'t affect subsequent warnings. 4. If the `temporary` flag is `False`, directly apply the filter action to affect the warning module\'s global state. Expected Function Signature: ```python import warnings def manage_warnings(warning_list): Issues and manages warnings based on the given list of tuples. Parameters: warning_list (List[Tuple[str, Type[Warning], str, bool]]): A list of tuples containing - message (str): The warning message. - category (Warning): The warning category. - action (str): The filter action. - temporary (bool): Whether the filter action should be temporary. Returns: None pass ``` Constraints: - The `message` parameter must be a non-empty string. - The `category` parameter must be a valid warning category subclass of `Warning`. - The `action` parameter must be one of `default`, `ignore`, `always`, `module`, or `once`. - The `temporary` parameter must be a boolean. Input & Output: - The function will take a list of tuples. Each tuple contains: - `message` (str): The warning message. - `category` (Warning): The warning category class. - `action` (str): The filter action. - `temporary` (bool): Whether the filter action is temporary. - The function will not return any value, but will issue warnings and apply filters accordingly. Example: ```python from warnings import UserWarning, DeprecationWarning # Example input warning_list = [ (\\"This is a user warning\\", UserWarning, \\"default\\", False), (\\"This is a deprecation warning\\", DeprecationWarning, \\"ignore\\", True) ] manage_warnings(warning_list) # Expected Behavior: # - The first warning should be issued and displayed according to the default filter settings. # - The second warning should not be displayed because the filter is temporarily set to ignore it. ``` Notes: - Make sure to reset any global state changes after applying non-temporary filters to ensure subsequent tests/warnings are not affected. - Ensure proper handling of erroneous inputs by raising appropriate exceptions.","solution":"import warnings def manage_warnings(warning_list): Issues and manages warnings based on the given list of tuples. Parameters: warning_list (List[Tuple[str, Type[Warning], str, bool]]): A list of tuples containing - message (str): The warning message. - category (Warning): The warning category. - action (str): The filter action. - temporary (bool): Whether the filter action should be temporary. Returns: None for message, category, action, temporary in warning_list: if not message or not issubclass(category, Warning) or action not in [\'default\', \'ignore\', \'always\', \'module\', \'once\'] or not isinstance(temporary, bool): raise ValueError(\\"Invalid input values provided.\\") if temporary: with warnings.catch_warnings(): warnings.simplefilter(action, category) warnings.warn(message, category) else: warnings.simplefilter(action, category) warnings.warn(message, category)"},{"question":"**Coding Assessment Question:** In this assessment, you are required to process text data and extract specific information using regular expressions. This task will test your understanding of various regular expression concepts, including matching characters, repeating patterns, capturing groups, and using the \\"re\\" module functions. # Problem Statement: Write a Python function `extract_emails_and_dates(text: str) -> dict` that extracts all email addresses and dates from the input text and returns them in a dictionary. # Input: * A single string `text` containing email addresses and dates. # Output: * A dictionary with two keys: \'emails\' and \'dates\'. * The value for \'emails\' is a list of all email addresses found in the text. * The value for \'dates\' is a list of all dates found in the text. * Dates are written in the format: `day-month-year`, where day and year are numerical (day can be one or two digits, year is four digits), and month can be either numerical (one or two digits) or the full month name (both lower case and title case are acceptable). # Constraints: * The input text may contain multiple email addresses and date formats. * Email addresses follow the usual format `username@domain.tld`. * You may assume the text does not contain any malformed email addresses or dates. # Example: ```python text = contact: john.doe@example.com, jane_doe123@sample.net meeting dates: 23-June-2022, 5-07-2021, 12-december-2020 expected_output = { \'emails\': [\'john.doe@example.com\', \'jane_doe123@sample.net\'], \'dates\': [\'23-June-2022\', \'5-07-2021\', \'12-december-2020\'] } assert extract_emails_and_dates(text) == expected_output ``` # Function Signature: ```python def extract_emails_and_dates(text: str) -> dict: # Your code here ``` # Requirements: * Use the \\"re\\" module to implement the function. * Your implementation should use regular expressions to find and extract the required patterns. * Do not use any third-party libraries. # Tips: * You can use the `re.findall()` method to find all occurrences of a pattern in the text. * Create separate regular expressions for email addresses and dates. * Consider using raw strings (prefix \'r\') for regular expressions for better readability and to avoid escaping issues. Good luck, and happy coding!","solution":"import re def extract_emails_and_dates(text: str) -> dict: Function that extracts all email addresses and dates from the input text. Args: text (str): Input text containing email addresses and dates. Returns: dict: A dictionary with two keys: - \'emails\': List of all email addresses found in the text. - \'dates\': List of all dates found in the text. email_pattern = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' date_pattern = r\'bd{1,2}-(?:d{1,2}|[a-zA-Z]+)-d{4}b\' emails = re.findall(email_pattern, text) dates = re.findall(date_pattern, text) return { \'emails\': emails, \'dates\': dates }"},{"question":"**Objective:** Write a custom PyTorch autograd function using `torch.autograd.Function` that computes the following: 1. Forward pass of matrix multiplication followed by an element-wise sine operation. 2. Backward pass to compute the gradients of the input matrices. 3. Optional: Add `vmap` support for batch processing. # Requirements 1. The `forward` method should support two input matrices and compute `result = sin(A * B)`, where `*` is the matrix multiplication. 2. The `setup_context` method should save any necessary intermediate values required for the backward pass. 3. The `backward` method should compute the gradients with respect to the input matrices `A` and `B`. 4. Optionally, implement the `vmap` method to provide support for batching. # Constraints - The input matrices `A` and `B` will have compatible shapes for matrix multiplication. - Ensure that the operations in your backward and `vmap` (if implemented) methods are transformable using PyTorch operations. # Input and Output Formats **Input:** - Two 2D tensors `A` and `B` representing the matrices to be multiplied. **Output:** - A single 2D tensor representing the result of the element-wise sine of the matrix product. # Example ```python import torch import torch.autograd.Function class SineMatrixMultiplication(torch.autograd.Function): @staticmethod def forward(A, B): device = A.device result = torch.mm(A, B) sine_result = torch.sin(result) return sine_result, result @staticmethod def setup_context(ctx, inputs, output): A, B = inputs _, result = output ctx.save_for_backward(A, B, result) @staticmethod def backward(ctx, grad_output, _): A, B, result = ctx.saved_tensors grad_A = torch.mm(grad_output * torch.cos(result), B.t()) grad_B = torch.mm(A.t(), grad_output * torch.cos(result)) return grad_A, grad_B @staticmethod def vmap(info, in_dims, A, B): A_bdim, B_bdim = in_dims A = A.movedim(A_bdim, 0) B = B.movedim(B_bdim, 0) result = SineMatrixMultiplication.apply(A, B) return result, 0 # Both outputs are batched along dimension 0 def sine_matrix_multiplication(A, B): result, _ = SineMatrixMultiplication.apply(A, B) return result # Forward pass test A = torch.randn(3, 2, requires_grad=True) B = torch.randn(2, 3, requires_grad=True) output = sine_matrix_multiplication(A, B) print(output) # Backward pass test output.sum().backward() print(A.grad) print(B.grad) # Vmap test (optional) batch_A = torch.randn(10, 3, 2, requires_grad=True) batch_B = torch.randn(10, 2, 3, requires_grad=True) batch_output = torch.vmap(sine_matrix_multiplication)(batch_A, batch_B) print(batch_output) ``` # Notes 1. Ensure that the forward, setup_context, and backward methods are implemented correctly. 2. Test your function with both simple and batched inputs to ensure correctness. 3. Add comments to elaborate on the logic wherever necessary.","solution":"import torch class SineMatrixMultiplication(torch.autograd.Function): @staticmethod def forward(ctx, A, B): result = torch.mm(A, B) sine_result = torch.sin(result) ctx.save_for_backward(A, B, result) return sine_result @staticmethod def backward(ctx, grad_output): A, B, result = ctx.saved_tensors grad_A = torch.mm(grad_output * torch.cos(result), B.t()) grad_B = torch.mm(A.t(), grad_output * torch.cos(result)) return grad_A, grad_B def sine_matrix_multiplication(A, B): return SineMatrixMultiplication.apply(A, B)"},{"question":"# Objective: You are required to write a Python function that automates the preparation of a source distribution for a given Python project. Your function must handle the creation of the `MANIFEST.in` file and invoke the `sdist` command to generate specific types of source distributions. # Task: Write a function `prepare_source_distribution` that accepts three parameters: 1. `formats`: A list of formats in which to create the archive (e.g., `[\\"gztar\\", \\"zip\\"]`). 2. `include_patterns`: A list of glob patterns specifying the files to include in the distribution (e.g., `[\\"*.py\\", \\"*.txt\\"]`). 3. `exclude_patterns`: A list of glob patterns specifying the files or directories to exclude from the distribution (e.g., [`\\".git/*\\", \\"build/*\\"]`). Your function should: 1. Create a `MANIFEST.in` file in the current directory with the specified include and exclude patterns. 2. Generate the source distribution archives in the given formats using the `sdist` command. # Constraints: - The function should not use any external packages not present in the standard Python library. - The `MANIFEST.in` file should be created or overwritten each time the function is called. - Each pattern in `include_patterns` should be written as a separate line in `MANIFEST.in` prefixed with `include`. - Each pattern in `exclude_patterns` should be written as a separate line in `MANIFEST.in` prefixed with `prune`. # Input: - `formats`: List of strings. - `include_patterns`: List of strings. - `exclude_patterns`: List of strings. # Output: - No return value. The function performs its task by creating the necessary files and invoking the `sdist` command. # Example: ```python def prepare_source_distribution(formats, include_patterns, exclude_patterns): pass formats = [\\"gztar\\", \\"zip\\"] include_patterns = [\\"*.py\\", \\"README.md\\"] exclude_patterns = [\\".git/*\\", \\"test/*\\"] prepare_source_distribution(formats, include_patterns, exclude_patterns) ``` # Notes: 1. Assume the current directory is the root of the project. 2. Ensure robust error handling for cases such as invalid format names. Test your function thoroughly to ensure it works as expected in various scenarios.","solution":"import os import subprocess def prepare_source_distribution(formats, include_patterns, exclude_patterns): Prepares the source distribution for a given Python project. Parameters: - formats (list): A list of formats in which to create the archive (e.g., [\\"gztar\\", \\"zip\\"]). - include_patterns (list): A list of glob patterns specifying the files to include in the distribution (e.g., [\\"*.py\\", \\"*.txt\\"]). - exclude_patterns (list): A list of glob patterns specifying the files or directories to exclude from the distribution (e.g., [\\".git/*\\", \\"build/*\\"]). Returns: - None # Step 1: Create the MANIFEST.in file with open(\\"MANIFEST.in\\", \\"w\\") as manifest_file: for pattern in include_patterns: manifest_file.write(f\\"include {pattern}n\\") for pattern in exclude_patterns: manifest_file.write(f\\"prune {pattern}n\\") # Step 2: Generate the source distribution archives in the given formats command = [\\"python\\", \\"setup.py\\", \\"sdist\\"] for fmt in formats: valid_formats = [\\"gztar\\", \\"ztar\\", \\"tar\\", \\"zip\\"] if fmt not in valid_formats: raise ValueError(f\\"Invalid format: {fmt}. Valid formats are: {valid_formats}\\") command.append(f\\"--formats={fmt}\\") subprocess.run(command, check=True)"},{"question":"<|Analysis Begin|> The `email.utils` module from Python provides miscellaneous utilities to deal with email messages, mainly their headers. This module includes various functions to handle dates, addresses, and to format or parse these email components according to different RFC standards. Here\'s a summary of the key functionalities: 1. **Time Handling Utilities**: - Functions to get local time (`localtime`), parse dates (`parsedate`, `parsedate_tz`, `parsedate_to_datetime`), format dates (`formatdate`, `format_datetime`), and convert parsed date tuples to timestamps (`mktime_tz`). 2. **Message-ID and Encoding Utilities**: - Functions to create unique message IDs (`make_msgid`), encode and decode according to RFC2231 (`encode_rfc2231`, `decode_rfc2231`, `collapse_rfc2231_value`). 3. **String Handling Utilities**: - Functions to quote and unquote strings (`quote`, `unquote`). 4. **Address Parsing and Formatting Utilities**: - Functions to parse and format email addresses (`parseaddr`, `formataddr`, `getaddresses`). This analysis makes it clear that the module provides a range of utilities dealing with email-specific data handling, especially addressing and dates, within the context defined by the relevant RFCs. Considering these functionalities, let\'s design an assessment question that requires using multiple functions from the `email.utils` module to solve a comprehensive problem. <|Analysis End|> <|Question Begin|> # **Coding Assessment Question** **Title**: Email Header Processing and Formatting **Objective**: To understand and utilize various utilities from the `email.utils` module to process, parse, format, and validate email headers and dates. **Problem Statement**: You are developing a small utility to process email headers and generate formatted output for an email client application. The utility must perform the following functions: 1. **Parse and Format Email Addresses**: - Parse a list of email addresses provided in a field such as \\"To\\" or \\"Cc\\". - Format the parsed addresses into a string suitable for display. 2. **Generate Message-ID**: - Create a unique message ID for each email being processed. 3. **Handle Date and Time**: - Parse a given string representing an email\'s date. - Format the parsed date into a standardized RFC 2822 formatted string. # **Requirements**: 1. Implement the function `process_email_headers` which takes the following parameters: - `addresses` (list of str): A list of email addresses in the format \\"Name <email@example.com>\\". - `raw_date` (str): A string representing the date of the email. - `idstring` (Optional[str]): A string to strengthen the uniqueness of the message ID. - `domain` (Optional[str]): The domain to use for generating the message ID. 2. The function should return a dictionary with the following keys: - `parsed_addresses` (list of tuple): A list of tuples with the real names and email addresses. - `formatted_addresses` (str): A single string of formatted email addresses suitable for display. - `message_id` (str): A unique message ID for the email. - `formatted_date` (str): A date string formatted according to RFC 2822. # **Constraints**: - The input date will always be in a valid RFC 2822 format. - The list of addresses will always be non-empty. # **Function Signature**: ```python def process_email_headers(addresses: list[str], raw_date: str, idstring: Optional[str] = None, domain: Optional[str] = None) -> dict: pass ``` # **Example**: ```python addresses = [\\"Alice <alice@example.com>\\", \\"Bob <bob@example.net>\\"] raw_date = \\"Mon, 20 Nov 1995 19:12:08 -0500\\" idstring = \\"example\\" domain = \\"mail.example.com\\" result = process_email_headers(addresses, raw_date, idstring, domain) # expected output structure: # { # \'parsed_addresses\': [(\'Alice\', \'alice@example.com\'), (\'Bob\', \'bob@example.net\')], # \'formatted_addresses\': \'Alice <alice@example.com>, Bob <bob@example.net>\', # \'message_id\': \'<some_unique_id@mail.example.com>\', # \'formatted_date\': \'Mon, 20 Nov 1995 19:12:08 -0500\' # } ``` Use the documentation provided for `email.utils` to correctly implement and make use of the following functions: - `parseaddr()` - `formataddr()` - `make_msgid()` - `parsedate()` - `formatdate()` # **Notes**: - Pay attention to function return formats and make sure they adhere to the expected RFC standards. - You may import any standard Python modules/libraries as needed to accomplish your solution.","solution":"from typing import Optional from email.utils import parseaddr, formataddr, make_msgid, parsedate_to_datetime, format_datetime def process_email_headers(addresses: list[str], raw_date: str, idstring: Optional[str] = None, domain: Optional[str] = None) -> dict: Process email headers to parse, format addresses, generate message ID and format date. Args: addresses (list of str): List of email addresses in \\"Name <email@example.com>\\" format. raw_date (str): Raw date string in RFC 2822 format. idstring (Optional[str]): Optional string to strengthen message ID uniqueness. domain (Optional[str]): Optional domain for message ID. Returns: dict: Dictionary with parsed addresses, formatted addresses, message ID, and formatted date. # Parse and format the email addresses parsed_addresses = [parseaddr(addr) for addr in addresses] formatted_addresses = \\", \\".join([formataddr(addr) for addr in parsed_addresses]) # Generate message ID message_id = make_msgid(idstring=idstring, domain=domain) # Parse and format date parsed_datetime = parsedate_to_datetime(raw_date) formatted_date = format_datetime(parsed_datetime) return { \'parsed_addresses\': parsed_addresses, \'formatted_addresses\': formatted_addresses, \'message_id\': message_id, \'formatted_date\': formatted_date }"},{"question":"**Question: Implement a Function to Fetch and Save Webpage Content** Write a Python function `fetch_webpage_content` that takes in a URL and an optional dictionary of query parameters. The function should: 1. Build the full URL using the base URL and the query parameters, if provided. 2. Add a custom `User-Agent` header to the request. 3. Make a GET request to the server and handle potential exceptions that may occur. 4. For certain HTTP status codes, handle redirects appropriately. 5. Save the response content to a file in a given directory. Your function should follow these specifications: - **Function Signature**: ```python def fetch_webpage_content(base_url: str, params: dict = None, save_dir: str = \'.\') -> str: ``` - `base_url` (str): The base URL to which the GET request is made. - `params` (dict, optional): A dictionary of query parameters to be appended to the URL. Defaults to `None`. - `save_dir` (str, optional): The directory where the fetched content should be saved. Defaults to the current directory (`.`). - **Returns**: - `file_path` (str): The path to the saved file containing the webpage content. - **Constraints**: - If an error occurs (URLError or HTTPError), the function should return a message like `\\"Error: <error_message>\\"`. - The User-Agent should be set to `\'Mozilla/5.0 (Python urllib)\'`. - Handle redirects (HTTP status codes in the range 300-399) to fetch the final content. - **Examples**: ```python url = \\"http://www.example.com\\" params = {\'search\': \'python\', \'page\': 1} directory = \\"/path/to/save\\" file_path = fetch_webpage_content(url, params, directory) print(file_path) # Should print the path to the saved file or an error message ``` - **Note**: - Use the `urllib.parse` module for encoding query parameters. - Use the `urllib.request` module for making HTTP requests and handling exceptions. - Handle directories and file paths appropriately.","solution":"import os import urllib.parse import urllib.request import urllib.error def fetch_webpage_content(base_url: str, params: dict = None, save_dir: str = \'.\') -> str: Fetches the content of a webpage from the given URL with optional query parameters and saves it to a file in the specified directory. Parameters: base_url (str): The base URL to which the GET request is made. params (dict, optional): A dictionary of query parameters to be appended to the URL. Defaults to None. save_dir (str, optional): The directory where the fetched content should be saved. Defaults to the current directory (\'.\'). Returns: file_path (str): The path to the saved file containing the webpage content or an error message. # Construct the full URL with query parameters if provided if params: query_string = urllib.parse.urlencode(params) full_url = f\\"{base_url}?{query_string}\\" else: full_url = base_url # Add custom User-Agent header headers = {\'User-Agent\': \'Mozilla/5.0 (Python urllib)\'} req = urllib.request.Request(full_url, headers=headers) try: with urllib.request.urlopen(req) as response: # Handle redirects (status codes 300-399) if 300 <= response.status < 400: redirected_url = response.geturl() req = urllib.request.Request(redirected_url, headers=headers) with urllib.request.urlopen(req) as final_response: webpage_content = final_response.read() else: webpage_content = response.read() # Ensure save_dir exists and is a directory if not os.path.exists(save_dir): os.makedirs(save_dir) # Generate a file name from the URL, replace invalid characters file_name = urllib.parse.quote(full_url, safe=\'\').replace(\'%\', \'_\') + \'.html\' file_path = os.path.join(save_dir, file_name) # Save content to a file with open(file_path, \'wb\') as f: f.write(webpage_content) return file_path except (urllib.error.URLError, urllib.error.HTTPError) as e: return f\\"Error: {str(e)}\\""},{"question":"Question # Problem Statement Your task is to implement a PyTorch function that demonstrates efficient handling and management of multiple CUDA devices. The implementation should include the creation of tensors on different CUDA devices, performing tensor operations, transferring data between devices, and benchmarking the performance difference for these operations while leveraging CUDA streams for efficient execution. # Requirements 1. **Tensor Creation and Transfer**: - Create tensors on two different CUDA devices. - Perform tensor operations on both devices. - Transfer tensors between these devices. 2. **Stream Management**: - Use CUDA streams to handle asynchronous execution. - Synchronize streams where necessary. 3. **Benchmarking**: - Measure and compare the time taken for the tensor operations and transfers without explicit stream management vs. with CUDA stream and synchronization. 4. **Device-Agnostic Initialization**: - Ensure that your function can handle cases where CUDA is not available and falls back to CPU execution. # Function Signature ```python import torch def multi_device_tensor_operations(): Function to demonstrate efficient handling and management of multiple CUDA devices. Steps: 1. Check for CUDA availability. 2. Create tensors on different CUDA devices. 3. Perform tensor operations and transfers, with and without using CUDA streams. 4. Benchmark the time taken for different operations. Returns: A dictionary containing the time taken for each operation type as follows: { \'operation_without_stream\': float, \'operation_with_stream\': float, ... } # Your code here # Example usage: # times = multi_device_tensor_operations() # print(times) ``` # Expected Output The function should return a dictionary with the measured times for the operations, comparing scenarios with and without using CUDA streams. For example: ```python { \'tensor_creation_device0\': 0.001, \'tensor_creation_device1\': 0.002, \'operation_without_stream\': 0.004, \'operation_with_stream\': 0.003, ... } ``` # Constraints 1. Ensure that the function works correctly and efficiently even when CUDA is not available. 2. Perform proper synchronization where necessary to ensure correctness of benchmarked results. 3. Use appropriate CUDA memory management techniques to avoid memory leaks or excessive memory utilization. # Notes - You can use `torch.cuda.Event` to accurately measure the elapsed time. - Make sure to handle scenarios where CUDA is not available by gracefully falling back to CPU execution.","solution":"import torch import time def multi_device_tensor_operations(): Function to demonstrate efficient handling and management of multiple CUDA devices. Steps: 1. Check for CUDA availability. 2. Create tensors on different CUDA devices. 3. Perform tensor operations and transfers with and without using CUDA streams. 4. Benchmark the time taken for different operations. Returns: A dictionary containing the time taken for each operation type. results = {} # Check for CUDA availability if not torch.cuda.is_available() or torch.cuda.device_count() < 2: results[\'error\'] = \'CUDA is not available or less than 2 CUDA devices found. Falling back to CPU.\' return results device0 = torch.device(\'cuda:0\') device1 = torch.device(\'cuda:1\') # Function to measure time for operations def measure_time(operation): start_time = torch.cuda.Event(enable_timing=True) end_time = torch.cuda.Event(enable_timing=True) start_time.record() operation() end_time.record() torch.cuda.synchronize() # Wait for the events to be recorded elapsed_time = start_time.elapsed_time(end_time) / 1000.0 # Convert to seconds return elapsed_time # Tensor creation def create_tensors(): t0 = torch.ones((1000, 1000), device=device0) t1 = torch.ones((1000, 1000), device=device1) return t0, t1 # Transfer tensor def transfer_tensors(tensor, dst_device): return tensor.to(dst_device) t0, t1 = create_tensors() # Benchmark operations results[\'tensor_creation_device0\'] = measure_time(lambda: torch.ones((1000, 1000), device=device0)) results[\'tensor_creation_device1\'] = measure_time(lambda: torch.ones((1000, 1000), device=device1)) # Operations without stream def operations_without_stream(): t2 = t0 + t0 t3 = t1 * t1 t4 = transfer_tensors(t2, device1) results[\'operation_without_stream\'] = measure_time(operations_without_stream) # Operations with streams stream0 = torch.cuda.Stream(device=device0) stream1 = torch.cuda.Stream(device=device1) def operations_with_stream(): with torch.cuda.stream(stream0): t2 = t0 + t0 with torch.cuda.stream(stream1): t3 = t1 * t1 stream1.synchronize() t4 = transfer_tensors(t2, device1) results[\'operation_with_stream\'] = measure_time(operations_with_stream) return results # Example usage: # times = multi_device_tensor_operations() # print(times)"},{"question":"# Coding Assessment: Advanced ZIP File Operations Objective You are required to write a Python script using the `zipfile` module to accomplish the following tasks: 1. Create a ZIP archive from a given list of files and directories. 2. List the contents of the created ZIP archive. 3. Extract specific files from the ZIP archive to a specified directory. 4. Implement appropriate exception handling to manage potential errors during these operations. Input - A list of file and directory paths to be added to the ZIP archive. - A list of filenames to be extracted from the ZIP archive. - The target directory path where the files need to be extracted. Output - A ZIP archive containing the specified files and directories. - Console output listing the contents of the ZIP archive. - Extracted files in the specified target directory. Constraints - Use only the built-in `zipfile` module for ZIP file operations. - Ensure the script handles non-existent files/directories, invalid ZIP archives, read/write permissions issues, and other common errors gracefully. - Assume that the files specified for extraction are present in the ZIP archive. Performance Requirements - The solution should efficiently handle large files using the `ZIP64` extensions if needed. - Ensure that the script manages memory and disk usage effectively to avoid exhaustion issues. Example ```python import os import zipfile def create_zip_archive(paths, zip_filename): Creates a ZIP file with the specified filename from the given list of paths. :param paths: List of file and directory paths to be included in the ZIP archive. :param zip_filename: Name of the ZIP archive to be created. :return: None pass def list_zip_contents(zip_filename): Lists the contents of the specified ZIP archive. :param zip_filename: Name of the ZIP archive. :return: List of filenames in the ZIP archive. pass def extract_files(zip_filename, filenames, extract_to): Extracts the specified files from the ZIP archive to the target directory. :param zip_filename: Name of the ZIP archive. :param filenames: List of filenames to extract. :param extract_to: Target directory path for extraction. :return: None pass # Sample usage: paths_to_zip = [\'file1.txt\', \'dir/file2.txt\'] zip_filename = \'archive.zip\' files_to_extract = [\'file1.txt\', \'file2.txt\'] extract_target_dir = \'extracted_files/\' create_zip_archive(paths_to_zip, zip_filename) print(\\"ZIP file contents:\\", list_zip_contents(zip_filename)) extract_files(zip_filename, files_to_extract, extract_target_dir) ``` Tasks 1. Implement the `create_zip_archive` function to create a ZIP file from the given list of file and directory paths. 2. Implement the `list_zip_contents` function to return a list of filenames contained in the ZIP archive. 3. Implement the `extract_files` function to extract the specified files from the ZIP archive to the target directory. 4. Add appropriate exception handling to manage errors such as `FileNotFoundError`, `zipfile.BadZipFile`, and `PermissionError`. Submission Submit your Python script file with the functions implemented and tested using the provided example usage.","solution":"import os import zipfile def create_zip_archive(paths, zip_filename): Creates a ZIP file with the specified filename from the given list of paths. :param paths: List of file and directory paths to be included in the ZIP archive. :param zip_filename: Name of the ZIP archive to be created. :return: None try: with zipfile.ZipFile(zip_filename, \'w\', zipfile.ZIP_DEFLATED) as zipf: for path in paths: if os.path.isfile(path): zipf.write(path, os.path.relpath(path, start=os.path.dirname(paths[0]))) elif os.path.isdir(path): for root, _, files in os.walk(path): for file in files: full_path = os.path.join(root, file) zipf.write(full_path, os.path.relpath(full_path, start=os.path.dirname(paths[0]))) else: print(f\\"Path \'{path}\' does not exist and will be skipped.\\") except Exception as e: print(f\\"Error creating ZIP archive {zip_filename}: {e}\\") def list_zip_contents(zip_filename): Lists the contents of the specified ZIP archive. :param zip_filename: Name of the ZIP archive. :return: List of filenames in the ZIP archive. try: with zipfile.ZipFile(zip_filename, \'r\') as zipf: return zipf.namelist() except FileNotFoundError: print(f\\"ZIP file \'{zip_filename}\' not found.\\") except zipfile.BadZipFile: print(f\\"Bad ZIP file \'{zip_filename}\'.\\") except Exception as e: print(f\\"Error reading ZIP file {zip_filename}: {e}\\") return [] def extract_files(zip_filename, filenames, extract_to): Extracts the specified files from the ZIP archive to the target directory. :param zip_filename: Name of the ZIP archive. :param filenames: List of filenames to extract. :param extract_to: Target directory path for extraction. :return: None try: with zipfile.ZipFile(zip_filename, \'r\') as zipf: for filename in filenames: try: zipf.extract(filename, extract_to) except KeyError: print(f\\"File \'{filename}\' not found in the ZIP archive.\\") except FileNotFoundError: print(f\\"ZIP file \'{zip_filename}\' not found.\\") except zipfile.BadZipFile: print(f\\"Bad ZIP file \'{zip_filename}\'.\\") except PermissionError: print(f\\"Permission denied when trying to extract \'{zip_filename}\' to \'{extract_to}\'.\\") except Exception as e: print(f\\"Error extracting files from {zip_filename}: {e}\\") # Sample usage: # paths_to_zip = [\'file1.txt\', \'dir/file2.txt\'] # zip_filename = \'archive.zip\' # files_to_extract = [\'file1.txt\', \'file2.txt\'] # extract_target_dir = \'extracted_files/\' # create_zip_archive(paths_to_zip, zip_filename) # print(\\"ZIP file contents:\\", list_zip_contents(zip_filename)) # extract_files(zip_filename, files_to_extract, extract_target_dir)"},{"question":"**Bytearray Manipulation Challenge** # Objective: Implement a Python function that performs various operations on `bytearray` objects using the provided C API functions and macros. # Description: You are required to implement a Python function `bytearray_operations(data: list) -> bytes` that takes a list of tuples as input. Each tuple contains two elements: an operation string and a value. The function should perform the specified operations on an internal `bytearray` object and finally return the contents of the `bytearray` as a `bytes` object. # Operations: The function should support the following operations: 1. `\\"create\\"`: Create a new `bytearray` object from the given string. - Example: `(\\"create\\", \\"hello world\\")` 2. `\\"concat\\"`: Concatenate the bytearray with another bytearray created from the given string. - Example: `(\\"concat\\", \\"!!!\\")` 3. `\\"resize\\"`: Resize the bytearray to the given length. - Example: `(\\"resize\\", 5)` 4. `\\"to_string\\"`: Convert the `bytearray` to a `bytes` object and return it. - Example: `(\\"to_string\\", None)` # Constraints: - The input list will contain at least one operation. - The operations will be provided in a valid sequence. - The `\\"create\\"` operation will always be the first operation. - The lengths and sizes will be within reasonable limits for practical use. # Example: ```python def bytearray_operations(data): # Example implementation using C API functions and macros pass # Example usage data = [ (\\"create\\", \\"hello\\"), (\\"concat\\", \\" world\\"), (\\"resize\\", 11), (\\"to_string\\", None) ] result = bytearray_operations(data) print(result) # Output should be: b\'hello world0\' ``` # Implementation Requirements: - Use the provided C API functions and macros where applicable. - Ensure proper error handling for C API function returns (e.g., handle NULL return values). - The final result should be a `bytes` object containing the contents of the `bytearray`. # Notes: - Pay attention to memory management and reference counting when using the C API. - Make sure to append a null byte when converting to a `bytes` object.","solution":"def bytearray_operations(data): Perform operations on a bytearray as specified and return the updated bytearray as bytes. ba = None for operation, val in data: if operation == \\"create\\": ba = bytearray(val, \'utf-8\') # Create a new bytearray from the given string elif operation == \\"concat\\": ba += bytearray(val, \'utf-8\') # Concatenate the bytearray with another bytearray created from the given string elif operation == \\"resize\\": ba = ba[:val] # Resize the bytearray to the given length elif operation == \\"to_string\\": ba += b\'0\' # Append a null byte return bytes(ba) # Convert the bytearray to a bytes object and return it return bytes(ba)"},{"question":"You are tasked with analyzing a company\'s business operations which span across multiple business periods measured in days, weeks, months, and quarters. You need to create a summary of key dates within a given range based on business operations schedules. Write a function `business_summary(start_date: str, end_date: str) -> pd.DataFrame` that: 1. Takes two input parameters, `start_date` and `end_date`, both strings in the \'YYYY-MM-DD\' format. These define the period you are interested in. 2. Calculates the following business-specific key dates: - Business day (every workday from Monday to Friday) - Start and end of each month - Start and end of each quarter - Start and end of the year 3. Outputs a Pandas DataFrame summarizing the key dates, with the following columns: - `Date`: The specific date in `YYYY-MM-DD` format - `Type`: Description of the date, e.g., \\"Business Day\\", \\"Month Start\\", \\"Quarter End\\", etc. Constraints: - Assume the week operates from Monday to Friday (standard business days). - If a month or quarter does not have a start or end within the provided range, it should not appear in the summary. - Utilize pandas\' `tseries.offsets` to accomplish this task. Example: ```python >>> import pandas as pd >>> summary = business_summary(\\"2023-01-01\\", \\"2023-03-31\\") >>> print(summary) # Output will be a DataFrame with columns \\"Date\\" and \\"Type\\" showing all business days, month start/end dates, quarter start/end dates within the specified range. ``` Additional Notes: - Make sure to consider edge cases where the `start_date` or `end_date` might themselves be key dates like month-end, quarter-end, etc. - Efficiency considerations: handle the date range efficiently to avoid performance bottlenecks. Feel free to use helper functions or additional custom logic to keep your main function concise and readable.","solution":"import pandas as pd import numpy as np def business_summary(start_date: str, end_date: str) -> pd.DataFrame: start = pd.to_datetime(start_date) end = pd.to_datetime(end_date) # Generate business days business_days = pd.date_range(start=start, end=end, freq=\'B\') business_df = pd.DataFrame({\'Date\': business_days, \'Type\': \'Business Day\'}) # Generate month starts and ends month_starts = pd.date_range(start=start, end=end, freq=\'MS\') month_ends = pd.date_range(start=start, end=end, freq=\'M\') month_start_df = pd.DataFrame({\'Date\': month_starts, \'Type\': \'Month Start\'}) month_end_df = pd.DataFrame({\'Date\': month_ends, \'Type\': \'Month End\'}) # Generate quarter starts and ends quarter_starts = pd.date_range(start=start, end=end, freq=\'QS\') quarter_ends = pd.date_range(start=start, end=end, freq=\'Q\') quarter_start_df = pd.DataFrame({\'Date\': quarter_starts, \'Type\': \'Quarter Start\'}) quarter_end_df = pd.DataFrame({\'Date\': quarter_ends, \'Type\': \'Quarter End\'}) # Generate year start and end year_starts = pd.date_range(start=start, end=end, freq=\'AS\') year_ends = pd.date_range(start=start, end=end, freq=\'A\') year_start_df = pd.DataFrame({\'Date\': year_starts, \'Type\': \'Year Start\'}) year_end_df = pd.DataFrame({\'Date\': year_ends, \'Type\': \'Year End\'}) # Concatenate all DataFrames summary_df = pd.concat([ business_df, month_start_df, month_end_df, quarter_start_df, quarter_end_df, year_start_df, year_end_df ]) # Ensuring that the dates are within the given range again summary_df = summary_df[(summary_df[\'Date\'] >= start) & (summary_df[\'Date\'] <= end)] summary_df = summary_df.sort_values(by=\'Date\').reset_index(drop=True) return summary_df"},{"question":"**Task: XML Data Manipulation** In this task, you are required to write a function make_xml_safe which takes a dictionary of attribute key-value pairs, and returns a string representing these attributes safely wrapped within an XML tag. Your function should use `xml.sax.saxutils.escape` for escaping special XML characters, and `xml.sax.saxutils.quoteattr` for preparing attribute values correctly. # Function Definition: ```python def make_xml_safe(tag: str, attributes: dict) -> str: Given a dictionary of attribute key-value pairs, return a string representing these attributes safely wrapped within an XML tag. :param tag: The name of the tag :param attributes: A dictionary of attribute key-value pairs :return: A string representing the tag with safely escaped attributes ``` # Input: - `tag` (str): The name of the XML tag. - `attributes` (dict): A dictionary where keys and values are strings representing XML attribute names and values, respectively. # Output: - Return a string representing the XML tag with the attributes safely escaped and quoted. # Example: ```python tag = \\"person\\" attributes = { \\"name\\": \\"John Doe & Co.\\", \\"occupation\\": \\"Developer <Python>\\" } print(make_xml_safe(tag, attributes)) ``` Expected output: ```xml <person name=\\"John Doe &amp; Co.\\" occupation=\\"Developer &lt;Python&gt;\\"/> ``` # Constraints: - The keys and values in the attributes dictionary will always be strings. - The tag name will always be a valid string for XML. The purpose of this question is to assess your understanding of the xml.sax.saxutils module and your ability to manipulate XML data safely and correctly. To solve this problem, you need to: 1. Iterate through the attributes dictionary to escape and quote each value. 2. Construct an attribute string that includes the safely processed attributes. 3. Form the final tag that combines the tag name and the attribute string.","solution":"import xml.sax.saxutils def make_xml_safe(tag: str, attributes: dict) -> str: Given a dictionary of attribute key-value pairs, return a string representing these attributes safely wrapped within an XML tag. :param tag: The name of the tag :param attributes: A dictionary of attribute key-value pairs :return: A string representing the tag with safely escaped attributes attr_str = \' \'.join(f\'{xml.sax.saxutils.escape(k)}={xml.sax.saxutils.quoteattr(v)}\' for k, v in attributes.items()) return f\'<{tag} {attr_str}/>\'"},{"question":"**Problem Statement:** You are required to implement a file-based session logger using the `atexit` module. Every time the program runs, it should log specific events to a session file and ensure the session file saves upon normal termination. The session file should store the events in chronological order with timestamps. **Functionality Requirements:** 1. Implement a function `log_event(event: str) -> None` that logs an event with the current timestamp to a session file. 2. The session file should be named `session.log` and each event should be stored in the format: `\\"[timestamp] event\\"`, where `timestamp` is the current time in `YYYY-MM-DD HH:MM:SS` format. 3. Ensure that all logged events are automatically saved to the `session.log` file upon normal interpreter termination without requiring explicit calls to save. 4. You must use the `atexit` module to ensure the events are saved upon program termination. 5. Implement a function `clear_events() -> None` to clear all previously logged events from the `session.log` file. **Input and Output Formats:** - `log_event(event: str) -> None`: The input parameter is a string `event` representing the event to log. There is no return value. - `clear_events() -> None`: There are no input parameters and there is no return value. **Constraints:** - You may assume that the `datetime` module is available for use. - The function should handle file operations gracefully, ensuring no data loss or corruption. # Example Usage: ```python import time # Initialize logging clear_events() log_event(\\"Session started\\") time.sleep(1) log_event(\\"Event 1 occurred\\") time.sleep(2) log_event(\\"Event 2 occurred\\") # The events will be saved automatically upon program termination ``` Upon running the example, the contents of `session.log` should be: ``` [2023-03-15 14:35:01] Session started [2023-03-15 14:35:02] Event 1 occurred [2023-03-15 14:35:04] Event 2 occurred ``` # Implementation Notes: - Make sure to use the `atexit` module to register the function that saves the log events to the file. - Use the `datetime` module to get the current timestamp. - Handle file operations properly, ensuring the session file is always in a consistent state.","solution":"import datetime import atexit # In-memory list to hold events events = [] def log_event(event: str) -> None: Log an event with the current timestamp. timestamp = datetime.datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") events.append(f\\"[{timestamp}] {event}\\") def save_events() -> None: Save all logged events to the session.log file. with open(\\"session.log\\", \\"a\\") as log_file: for event in events: log_file.write(event + \\"n\\") events.clear() def clear_events() -> None: Clear all previously logged events from the session.log file. with open(\\"session.log\\", \\"w\\") as log_file: log_file.truncate(0) # Register save_events to be called upon program termination atexit.register(save_events)"},{"question":"**Objective:** Write a Python function that reads a pickled file, disassembles its contents, and returns a summary of the pickled objects, including their types and sizes. The function should also optimize the pickle string and return the difference in size before and after optimization. **Function Signature:** ```python def analyze_and_optimize_pickle(file_path: str) -> dict: pass ``` **Input:** - `file_path` (str): Path to the pickled file. **Output:** - A dictionary with the following structure: ```python { \\"object_summary\\": [ {\\"type\\": \\"type_name\\", \\"size\\": size_in_bytes, \\"position\\": position_in_pickle} ... ], \\"original_size\\": original_pickled_size_in_bytes, \\"optimized_size\\": optimized_pickled_size_in_bytes, \\"size_difference\\": size_difference_in_bytes } ``` **Constraints:** - The pickled file can contain any valid pickle bytestream. - File must be read in binary mode. **Performance Requirements:** - The function should handle pickled files up to 10MB efficiently. **Example:** Suppose you have a pickled file `example.pickle` containing a tuple (1, {\\"a\\": [1, 2]}, \\"test\\"). The function should: 1. Disassemble the file using `pickletools.dis()` or `pickletools.genops()` to gather information about each object type, size, and position. 2. Optimize the pickle string using `pickletools.optimize()`. 3. Return the summary information and size differences. **Hint:** - You can use the `sys.getsizeof()` function to determine the size of objects in bytes. ```python import pickletools import os import sys def analyze_and_optimize_pickle(file_path: str) -> dict: with open(file_path, \\"rb\\") as f: pickled_data = f.read() # Generate operation summary opcodes = pickletools.genops(pickled_data) object_summary = [] for opcode, arg, pos in opcodes: object_summary.append({ \\"type\\": type(arg).__name__, \\"size\\": sys.getsizeof(arg), \\"position\\": pos }) original_size = len(pickled_data) optimized_data = pickletools.optimize(pickled_data) optimized_size = len(optimized_data) size_difference = original_size - optimized_size summary = { \\"object_summary\\": object_summary, \\"original_size\\": original_size, \\"optimized_size\\": optimized_size, \\"size_difference\\": size_difference } return summary ``` **Explanation:** 1. The function reads the pickled file. 2. It disassembles the pickle using `pickletools.genops` to collect type, size, and position data. 3. It optimizes the pickle string and calculates the size before and after optimization. 4. Finally, it assembles and returns the summary dictionary.","solution":"import pickletools import os import sys def analyze_and_optimize_pickle(file_path: str) -> dict: with open(file_path, \\"rb\\") as f: pickled_data = f.read() # Generate operation summary opcodes = pickletools.genops(pickled_data) object_summary = [] for opcode, arg, pos in opcodes: if arg is not None: # some opcodes may not have args object_summary.append({ \\"type\\": type(arg).__name__, \\"size\\": sys.getsizeof(arg), \\"position\\": pos }) original_size = len(pickled_data) optimized_data = pickletools.optimize(pickled_data) optimized_size = len(optimized_data) size_difference = original_size - optimized_size summary = { \\"object_summary\\": object_summary, \\"original_size\\": original_size, \\"optimized_size\\": optimized_size, \\"size_difference\\": size_difference } return summary"},{"question":"# Question: Implementing a Simple Key-Value Store with Persistence You are tasked with implementing a simple key-value store with persistent storage using the `shelve` module in Python. The key-value store should support the following operations: - `set(key: str, value: any)`: Stores the given key-value pair. - `get(key: str) -> any`: Retrieves the value associated with the given key, or raises a `KeyError` if the key does not exist. - `delete(key: str)`: Deletes the key-value pair associated with the given key, or raises a `KeyError` if the key does not exist. - `exists(key: str) -> bool`: Returns `True` if the key exists, `False` otherwise. - `all_keys() -> list[str]`: Returns a list of all keys in the store. Your implementation must meet the following requirements: - Use the `shelve` module for persistent storage. - Store the data in a file named `store.db`. - Ensure that the file is properly closed after all operations. - Should handle mutable objects correctly. For example, appending to a list stored as a value should persist the change. **Constraints:** - Keys are always strings. - Values can be any serializable Python object. - Do not use the `writeback` parameter. - Assume the `store.db` file can be created in the current working directory. Here is the function signature you need to implement: ```python import shelve class KeyValueStore: def __init__(self, filename=\'store.db\'): self.filename = filename def set(self, key: str, value: any): with shelve.open(self.filename) as db: db[key] = value def get(self, key: str) -> any: with shelve.open(self.filename) as db: if key in db: return db[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def delete(self, key: str): with shelve.open(self.filename) as db: if key in db: del db[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def exists(self, key: str) -> bool: with shelve.open(self.filename) as db: return key in db def all_keys(self) -> list[str]: with shelve.open(self.filename) as db: return list(db.keys()) ``` Implement the `KeyValueStore` class with the specified methods. Ensure that modifications to mutable objects held in the store are properly persisted without using `writeback=True`. Provide test cases demonstrating the functionality of your implementation.","solution":"import shelve class KeyValueStore: def __init__(self, filename=\'store.db\'): self.filename = filename def set(self, key: str, value: any): with shelve.open(self.filename) as db: db[key] = value def get(self, key: str) -> any: with shelve.open(self.filename) as db: if key in db: return db[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def delete(self, key: str): with shelve.open(self.filename) as db: if key in db: del db[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def exists(self, key: str) -> bool: with shelve.open(self.filename) as db: return key in db def all_keys(self) -> list[str]: with shelve.open(self.filename) as db: return list(db.keys())"},{"question":"# XML Data Processing Using `xml.dom.pulldom` You are provided with an XML document containing information about employees in a company. Each employee has an ID, name, salary, and department. Your task is to parse the XML data, identify employees with a salary greater than a specified amount, and return their details in a structured format. Input - A string containing XML data with the following structure: ```xml <company> <employee id=\\"1\\"> <name>John Doe</name> <salary>60000</salary> <department>Engineering</department> </employee> <employee id=\\"2\\"> <name>Jane Smith</name> <salary>45000</salary> <department>Marketing</department> </employee> <employee id=\\"3\\"> <name>Emily Johnson</name> <salary>75000</salary> <department>Sales</department> </employee> <!-- More employee records --> </company> ``` - An integer `threshold` representing the salary threshold. Output - A list of dictionaries, where each dictionary contains the details of an employee with a salary greater than the specified `threshold`. The keys of the dictionary should be `id`, `name`, `salary`, and `department`. Constraints - The input XML string will contain valid XML data. - The threshold value will be a non-negative integer. Example # Input ```python xml_data = \'\'\' <company> <employee id=\\"1\\"> <name>John Doe</name> <salary>60000</salary> <department>Engineering</department> </employee> <employee id=\\"2\\"> <name>Jane Smith</name> <salary>45000</salary> <department>Marketing</department> </employee> <employee id=\\"3\\"> <name>Emily Johnson</name> <salary>75000</salary> <department>Sales</department> </employee> </company> \'\'\' threshold = 50000 ``` # Output ```python [ {\'id\': \'1\', \'name\': \'John Doe\', \'salary\': \'60000\', \'department\': \'Engineering\'}, {\'id\': \'3\', \'name\': \'Emily Johnson\', \'salary\': \'75000\', \'department\': \'Sales\'} ] ``` Function Signature ```python def get_high_salary_employees(xml_data: str, threshold: int) -> list: pass ``` **Notes:** - Use the `xml.dom.pulldom` module to parse and process the XML data. - Ensure efficient handling of the XML data to meet performance requirements.","solution":"from xml.dom import pulldom def get_high_salary_employees(xml_data: str, threshold: int) -> list: high_salary_employees = [] doc = pulldom.parseString(xml_data) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'employee\': doc.expandNode(node) emp_id = node.getAttribute(\'id\') name = node.getElementsByTagName(\'name\')[0].firstChild.data salary = int(node.getElementsByTagName(\'salary\')[0].firstChild.data) department = node.getElementsByTagName(\'department\')[0].firstChild.data if salary > threshold: high_salary_employees.append({ \'id\': emp_id, \'name\': name, \'salary\': str(salary), # Convert salary back to string for consistency \'department\': department }) return high_salary_employees"},{"question":"# Problem: Sorted Logs Management You are given a list of log entries from a system, each entry is represented as a tuple `(timestamp, log_message)`, where `timestamp` is a string representing the time when the log was created in the format `YYYY-MM-DD HH:MM:SS`. Implement a `LogManager` class with the following functionality: 1. **Initialization**: * The `LogManager` should initialize with an empty list of logs. 2. **Insert Log**: * Method: `insert_log(self, log_entry: Tuple[str, str]) -> None` * Inserts a new log entry into the logs while maintaining the list sorted by `timestamp`. 3. **Find Closest Log**: * Method: `find_closest_log(self, target_timestamp: str) -> Tuple[str, str]` * Finds and returns the closest log entry to the given `target_timestamp`. If two log entries are equally close, return the one that appears earlier in the log list. Input - `log_entry`: A tuple of two strings, `(timestamp, log_message)`. - `target_timestamp`: A string representing the timestamp in the format `YYYY-MM-DD HH:MM:SS`. Output - `insert_log` method should not return anything. - `find_closest_log` method should return a tuple of two strings `(timestamp, log_message)`. Constraints - Timestamps are unique. - You may assume that all `timestamp` inputs are valid and follow the format `YYYY-MM-DD HH:MM:SS`. - The `find_closest_log` method should be optimized to run in O(log n) time for searching. Example ```python # Example Usage log_manager = LogManager() log_manager.insert_log((\\"2023-01-01 12:00:00\\", \\"System start\\")) log_manager.insert_log((\\"2023-01-01 12:30:00\\", \\"Error: Something happened\\")) log_manager.insert_log((\\"2023-01-01 13:00:00\\", \\"System restart\\")) closest_log = log_manager.find_closest_log(\\"2023-01-01 12:45:00\\") print(closest_log) # Output: (\\"2023-01-01 12:30:00\\", \\"Error: Something happened\\") log_manager.insert_log((\\"2023-01-01 12:15:00\\", \\"Warning: High memory usage\\")) closest_log = log_manager.find_closest_log(\\"2023-01-01 12:20:00\\") print(closest_log) # Output: (\\"2023-01-01 12:15:00\\", \\"Warning: High memory usage\\") ``` Note: - Implement binary search and insertion using the `bisect` module. - Use the `key` parameter effectively to manage and search logs by timestamps.","solution":"from typing import List, Tuple import bisect from datetime import datetime class LogManager: def __init__(self): self.logs = [] def insert_log(self, log_entry: Tuple[str, str]) -> None: # Convert timestamp to datetime object for sorting and comparison timestamp = datetime.strptime(log_entry[0], \\"%Y-%m-%d %H:%M:%S\\") # Insert maintaining sorted order bisect.insort(self.logs, (timestamp, log_entry[1])) def find_closest_log(self, target_timestamp: str) -> Tuple[str, str]: # Convert target timestamp to datetime object for comparison target_dt = datetime.strptime(target_timestamp, \\"%Y-%m-%d %H:%M:%S\\") pos = bisect.bisect_left(self.logs, (target_dt, \'\')) # Find position to insert # Determining the closest log closest_log = None if pos == 0: closest_log = self.logs[0] # Closest is the first log elif pos == len(self.logs): closest_log = self.logs[-1] # Closest is the last log else: before = self.logs[pos - 1] after = self.logs[pos] if (target_dt - before[0]) <= (after[0] - target_dt): closest_log = before else: closest_log = after # Return in original format return (closest_log[0].strftime(\\"%Y-%m-%d %H:%M:%S\\"), closest_log[1])"},{"question":"**Concurrent File Processing** You are tasked with processing multiple large text files concurrently to generate statistical data about the frequency of words in these files. To achieve optimal performance in this task, you will need to use the `concurrent.futures` module. Specifically, you will use the `ThreadPoolExecutor` and `ProcessPoolExecutor` classes to demonstrate their capabilities. **Problem Statement:** Write a Python function named `concurrent_word_frequency` that accepts a list of file paths and returns a dictionary where the keys are the words and the values are their cumulative frequencies across all files. Implement the function using both `ThreadPoolExecutor` and `ProcessPoolExecutor`, and ensure that your solution is efficient and well-structured. **Function Signatures:** ```python def concurrent_word_frequency(files: List[str], use_threads: bool = True) -> Dict[str, int]: pass ``` **Inputs:** - `files` (List[str]): A list of file paths to process. - `use_threads` (bool): A boolean flag to indicate whether to use threads (True) or processes (False) for concurrency. Default is True. **Output:** - Returns a dictionary (Dict[str, int]) where the keys are words and the values are their frequency counts across all files. **Constraints:** 1. You must handle large files efficiently. 2. Ensure proper synchronization to avoid race conditions. 3. Your solution should leverage the `concurrent.futures` module only. 4. Given the nature of text processing, consider lines or chunks of the file to avoid memory overload. **Example:** Suppose you have three text files with the following contents: - `file1.txt`: \\"apple banana apple\\" - `file2.txt`: \\"banana orange\\" - `file3.txt`: \\"apple orange banana\\" Running the function as follows: ```python files = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] result = concurrent_word_frequency(files, use_threads=True) print(result) ``` Should output: ```python {\'apple\': 3, \'banana\': 3, \'orange\': 2} ``` **Additional Notes:** - You need to import required modules (os, concurrent.futures, etc.). - Think about exception handling and file reading efficiency. - Consider the differences in managing concurrency with threads vs. processes, especially how they manage I/O and CPU-bound tasks.","solution":"import os from typing import List, Dict from collections import defaultdict from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor def count_words_in_file(file_path: str) -> Dict[str, int]: word_count = defaultdict(int) try: with open(file_path, \'r\', encoding=\'utf-8\') as file: for line in file: words = line.split() for word in words: word_count[word] += 1 except Exception as e: print(f\\"Error processing file {file_path}: {e}\\") return word_count def merge_word_counts(counts: List[Dict[str, int]]) -> Dict[str, int]: cumulative_count = defaultdict(int) for count in counts: for word, frequency in count.items(): cumulative_count[word] += frequency return cumulative_count def concurrent_word_frequency(files: List[str], use_threads: bool = True) -> Dict[str, int]: executor_class = ThreadPoolExecutor if use_threads else ProcessPoolExecutor with executor_class() as executor: future_to_file = {executor.submit(count_words_in_file, file): file for file in files} results = [] for future in future_to_file: try: result = future.result() results.append(result) except Exception as e: print(f\\"Error retrieving result from file {future_to_file[future]}: {e}\\") return merge_word_counts(results)"},{"question":"# Composite Mathematical Processor Using the Python modules `math`, `decimal`, and `statistics`, write a function `composite_math_processor(numbers: List[float]) -> dict` that performs the following operations on a list of input floats (numbers): 1. **Arithmetic Mean**: - Calculate the arithmetic mean of the numbers using the `statistics` module. 2. **Standard Deviation**: - Compute the standard deviation of the numbers using the `statistics` module. 3. **Logarithms**: - For each number, calculate its natural logarithm (`log`) using the `math` module. - If a number is less than or equal to zero, return `None` for that number. 4. **Decimal Representation and Rounding**: - Represent each number as a `decimal.Decimal` object, and round it to 2 decimal places using the `decimal` module. The function should return a dictionary with the following keys: - `\\"mean\\"`: The arithmetic mean of the numbers. - `\\"std_dev\\"`: The standard deviation of the numbers. - `\\"logarithms\\"`: A list of natural logarithms for each number or `None` for non-positive numbers. - `\\"rounded_decimals\\"`: A list of decimal representations rounded to 2 decimal places. # Constraints: - Assume the input list will always have at least one number. - Focus on correct usage of `math.log`, `decimal.Decimal`, `round`, and statistics functions. # Example: ```python from typing import List import math import decimal import statistics def composite_math_processor(numbers: List[float]) -> dict: mean = statistics.mean(numbers) std_dev = statistics.stdev(numbers) logarithms = [math.log(num) if num > 0 else None for num in numbers] decimal_numbers = [round(decimal.Decimal(num), 2) for num in numbers] return { \\"mean\\": mean, \\"std_dev\\": std_dev, \\"logarithms\\": logarithms, \\"rounded_decimals\\": decimal_numbers } # Example input: numbers = [10.5, 3.4, -2.1, 0, 8.8] # Example output: # { # \\"mean\\": 4.12, # \\"std_dev\\": 4.93, # \\"logarithms\\": [2.3513752571634776, 1.2237754316221157, None, None, 2.174751721484161], # \\"rounded_decimals\\": [Decimal(\'10.50\'), Decimal(\'3.40\'), Decimal(\'-2.10\'), Decimal(\'0.00\'), Decimal(\'8.80\')] # } ``` Ensure your solution is efficient and correctly handles edge cases.","solution":"from typing import List import math import decimal import statistics def composite_math_processor(numbers: List[float]) -> dict: mean = statistics.mean(numbers) std_dev = statistics.stdev(numbers) logarithms = [math.log(num) if num > 0 else None for num in numbers] decimal_numbers = [round(decimal.Decimal(num), 2) for num in numbers] return { \\"mean\\": mean, \\"std_dev\\": std_dev, \\"logarithms\\": logarithms, \\"rounded_decimals\\": decimal_numbers }"},{"question":"# Question: You are given a dataset containing car information named `cars.csv` with the following columns: - `mpg`: Miles per gallon. - `cylinders`: Number of cylinders. - `displacement`: Engine displacement (in cubic inches). - `horsepower`: Engine horsepower. - `weight`: Vehicle weight (in lbs). - `acceleration`: Time taken to accelerate from 0 to 60 mph (in seconds). - `model_year`: Model year (eg. 70 for 1970). - `origin`: Country of origin (1=USA, 2=Europe, 3=Japan). - `name`: Car name. Your task is to conduct a thorough analysis and visualization of the dataset using the `seaborn` library to answer and illustrate the following: 1. **Scatter Plot**: Visualize the relationship between `horsepower` and `mpg`. Use different colors to distinguish cars based on their `origin`. 2. **Facet Grid**: Create a faceted scatter plot (using columns) to show this relationship for each value of `cylinders`. 3. **Line Plot with Aggregation**: Create a line plot to show the trend of the average `mpg` per `model_year`. Include a 95% confidence interval. 4. **Customization**: Customize the scatter plot by adding marker style differentiation for `cylinders` and adjust the palette to make the plot more accessible. 5. **Advanced Faceting**: Create a faceted line plot to show the average `mpg` trend per `model_year` for each `origin` (using rows) and further facet by `cylinders` (using columns). Set the height and aspect ratio appropriately. # Input: - `cars.csv`: The dataset file available in the same directory as your script. # Output: - Different plots saved as images and displayed inline for visual validation. **Requirements:** - The script should use the `seaborn` library. - The script should handle any necessary data preprocessing steps, such as handling missing values or data formatting, before visualization. - Save all plots as image files (e.g., `scatter_plot.png`, `faceted_plot.png`, etc.) and display them inline using Jupyter notebook or an equivalent setup. # Constraints: - The dataset should be loaded directly from the provided CSV file. - Ensure that visualizations convey clear distinctions and are aesthetically pleasing. **Performance:** - Ensure that the script runs efficiently, especially when dealing with large datasets.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load dataset def load_data(filepath): return pd.read_csv(filepath) # Scatter Plot: Visualize the relationship between `horsepower` and `mpg` with `origin` as hue def scatter_plot(data): plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'horsepower\', y=\'mpg\', hue=\'origin\', palette=\'Set1\') plt.title(\'Horsepower vs MPG by Origin\') plt.savefig(\'scatter_plot.png\') plt.show() # Facet Grid: Faceted scatter plot for each value of `cylinders` def facet_grid_plot(data): g = sns.FacetGrid(data, col=\'cylinders\', hue=\'origin\', palette=\'Set1\', height=5, aspect=1) g.map(sns.scatterplot, \'horsepower\', \'mpg\').add_legend() g.fig.suptitle(\'Horsepower vs MPG by Cylinders and Origin\', y=1.02) plt.savefig(\'faceted_plot.png\') plt.show() # Line Plot with Aggregation: Trend of average `mpg` per `model_year` def line_plot(data): plt.figure(figsize=(10, 6)) sns.lineplot(data=data, x=\'model_year\', y=\'mpg\', ci=95, estimator=\'mean\') plt.title(\'Average MPG per Model Year\') plt.savefig(\'line_plot.png\') plt.show() # Scatter Plot Customization: Marker style differentiation for `cylinders` and adjusted palette def customized_scatter_plot(data): plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'horsepower\', y=\'mpg\', hue=\'origin\', style=\'cylinders\', palette=\'Dark2\', markers=True) plt.title(\'Customized Horsepower vs MPG by Origin and Cylinders\') plt.savefig(\'customized_scatter_plot.png\') plt.show() # Advanced Faceting: Faceted line plot showing trend of average `mpg` per `model_year` for each `origin` and further facet by `cylinders` def advanced_facet_grid(data): g = sns.FacetGrid(data, row=\'origin\', col=\'cylinders\', height=3, aspect=1.2) g.map(sns.lineplot, \'model_year\', \'mpg\', estimator=\'mean\', ci=95) g.add_legend() g.fig.suptitle(\'Average MPG Trend per Model Year by Origin and Cylinders\', y=1.02) plt.savefig(\'advanced_faceted_line_plot.png\') plt.show()"},{"question":"# ZIP File Manipulation and Validation You are tasked with creating a utility function that leverages the `zipfile` module to perform multiple operations on ZIP files. The function should create a ZIP file, add files to it, list the contents, and verify the integrity of the information. Specifically, you need to: 1. Create a new ZIP file and add several files from a specified directory. 2. List the contents of the ZIP file. 3. Extract all files to a specified extraction directory. 4. Verify the integrity of the ZIP file to ensure there are no corrupted files. # Function Signature ```python def zipfile_utility(source_dir: str, zip_filename: str, extract_dir: str) -> dict: Perform various operations on a ZIP file. Parameters: - source_dir (str): The directory containing files to be added to the ZIP file. - zip_filename (str): The path where the ZIP file will be created. - extract_dir (str): The directory where files will be extracted. Returns: - dict: A dictionary containing: - \'contents\': List of file names in the created ZIP file. - \'integrity\': Boolean indicating whether the ZIP file passed the integrity check. pass ``` # Constraints - Assume `source_dir` contains only files (no subdirectories). - The `zip_filename` should be a new file and not overwrite any existing file. - Handle exceptions gracefully and ensure resources are closed properly. - Use `zipfile.ZIP_DEFLATED` for file compression. # Requirements - You must use the `zipfile` module. - The function should first list the files in `source_dir`, add them to a new ZIP file (`zip_filename`), extract all files to `extract_dir`, and finally validate the integrity of the ZIP file. - The `integrity` check should be based on the `testzip()` method of the `ZipFile` class. # Example Usage ```python source_dir = \'path/to/source/files\' zip_filename = \'path/to/output/archive.zip\' extract_dir = \'path/to/extract/directory\' result = zipfile_utility(source_dir, zip_filename, extract_dir) print(result) ``` **Expected Output Format**: ```python { \'contents\': [\'file1.txt\', \'file2.txt\', \'file3.txt\'], \'integrity\': True } ``` This function should help assess your ability to manipulate ZIP files using the `zipfile` module, handle files and directories, and ensure data integrity.","solution":"import os import zipfile def zipfile_utility(source_dir: str, zip_filename: str, extract_dir: str) -> dict: Perform various operations on a ZIP file. Parameters: - source_dir (str): The directory containing files to be added to the ZIP file. - zip_filename (str): The path where the ZIP file will be created. - extract_dir (str): The directory where files will be extracted. Returns: - dict: A dictionary containing: - \'contents\': List of file names in the created ZIP file. - \'integrity\': Boolean indicating whether the ZIP file passed the integrity check. contents = [] # Step 1: Add files to a new ZIP file with zipfile.ZipFile(zip_filename, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, _, files in os.walk(source_dir): for file in files: file_path = os.path.join(root, file) zipf.write(file_path, arcname=file) contents.append(file) # Step 2: Verify the contents with zipfile.ZipFile(zip_filename, \'r\') as zipf: zip_contents = zipf.namelist() # Step 3: Extract all files with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extractall(extract_dir) # Step 4: Verify integrity with zipfile.ZipFile(zip_filename, \'r\') as zipf: integrity = zipf.testzip() is None return { \'contents\': contents, \'integrity\': integrity }"},{"question":"Objective: The goal of this task is to test your understanding of file handling and formatted string operations in Python. You will be required to read data from a file, process the data, and then write the output back to another file using various string formatting techniques. Problem Statement: You are provided with a text file called `data.txt` that contains names and scores of students in the following format: ``` John, 85 Jane, 92 Doe, 78 ``` Each line contains a student\'s name and their score separated by a comma. Your task is to write a function `process_student_scores(input_file, output_file)` that: 1. Reads the data from the `data.txt` file. 2. Processes the data to calculate the average score. 3. Writes the formatted output to a new file specified by `output_file`. The output file should contain: * Each student\'s name formatted to a width of 10 characters. * Each student\'s score right-justified within a width of 5 characters. * The average score formatted to two decimal places. Input: - `input_file` (str): The name of the input file (e.g., `data.txt`). - `output_file` (str): The name of the output file (e.g., `processed_data.txt`). Output: The output file should contain the formatted contents with neatly aligned columns and the average score at the bottom, similar to the following: ``` John ==> 85 Jane ==> 92 Doe ==> 78 Average Score: 85.00 ``` Constraints: 1. The input file may contain an arbitrary number of students. 2. Each line in the input file follows the format `Name, Score`. 3. The scores are all non-negative integers. Function Signature: ```python def process_student_scores(input_file: str, output_file: str) -> None: pass ``` Example: Suppose the `data.txt` contains the following data: ``` Alice, 90 Bob, 84 Charlie, 95 ``` Calling `process_student_scores(\'data.txt\', \'processed_data.txt\')` should create a file `processed_data.txt` with the content: ``` Alice ==> 90 Bob ==> 84 Charlie ==> 95 Average Score: 89.67 ``` Notes: - Use formatted string literals (f-strings) or the `str.format()` method to achieve the required formatting. - Ensure the file is properly closed after reading or writing operations are complete. - Use appropriate error handling for file operations (e.g., file not found).","solution":"def process_student_scores(input_file: str, output_file: str) -> None: try: with open(input_file, \'r\') as file: lines = file.readlines() total_score = 0 student_count = len(lines) students = [] for line in lines: name, score = line.strip().split(\', \') score = int(score) students.append((name, score)) total_score += score average_score = total_score / student_count if student_count > 0 else 0 with open(output_file, \'w\') as file: for name, score in students: file.write(f\\"{name:<10} ==> {score:>5}n\\") file.write(\\"n\\") file.write(f\\"Average Score: {average_score:.2f}n\\") except FileNotFoundError: print(f\\"The file {input_file} does not exist.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective: Write a Python function `send_email_notification` that sends an email notification using a specified email service. Then, write a set of unit tests for this function using the `unittest.mock` library to mock the email service and make assertions about how it is used. Function Description: **Function Name:** `send_email_notification` **Parameters:** - `email_service`: An instance of an email service class with a method `send_email(to_address, subject, body)`. - `to_address` (str): The recipient\'s email address. - `subject` (str): The email subject. - `body` (str): The content of the email. **Output:** - Returns `True` if the email is sent successfully. - Returns `False` if any exception occurs during sending the email. **Constraints:** - The `send_email` method of the email service instance may raise an exception (e.g., `ValueError`). ```python def send_email_notification(email_service, to_address, subject, body): try: email_service.send_email(to_address, subject, body) return True except Exception: return False ``` Unit Test Requirements: Write a test class `TestSendEmailNotification` that includes the following tests: 1. **Test Successful Email Sending**: - Use the `patch` function to mock the `send_email` method. - Ensure that `send_email_notification` returns `True` when `send_email` is called successfully. 2. **Test Email Sending Failure**: - Use the `patch` function to mock the `send_email` method. - Ensure that `send_email_notification` returns `False` when `send_email` raises an exception. 3. **Test Method Called with Correct Arguments**: - Check that the `send_email` method is called with the correct `to_address`, `subject`, and `body` arguments. 4. **Test Using create_autospec**: - Use `create_autospec` to create a mock of the email service. - Ensure `send_email_notification` enforces correct method signature for `send_email`. Example Test Class: ```python import unittest from unittest.mock import patch, create_autospec class TestEmailService: def send_email(self, to_address, subject, body): pass class TestSendEmailNotification(unittest.TestCase): @patch(\'path.to.your.module.TestEmailService.send_email\') def test_successful_email_sending(self, mock_send_email): mock_send_email.return_value = None email_service = TestEmailService() result = send_email_notification(email_service, \'test@example.com\', \'Test Subject\', \'Test Body\') self.assertTrue(result) @patch(\'path.to.your.module.TestEmailService.send_email\') def test_email_sending_failure(self, mock_send_email): mock_send_email.side_effect = ValueError(\'Failed to send\') email_service = TestEmailService() result = send_email_notification(email_service, \'test@example.com\', \'Test Subject\', \'Test Body\') self.assertFalse(result) @patch(\'path.to.your.module.TestEmailService.send_email\') def test_method_called_with_correct_arguments(self, mock_send_email): email_service = TestEmailService() send_email_notification(email_service, \'test@example.com\', \'Test Subject\', \'Test Body\') mock_send_email.assert_called_once_with(\'test@example.com\', \'Test Subject\', \'Test Body\') def test_using_create_autospec(self): email_service = create_autospec(TestEmailService) send_email_notification(email_service, \'test@example.com\', \'Test Subject\', \'Test Body\') email_service.send_email.assert_called_once_with(\'test@example.com\', \'Test Subject\', \'Test Body\') if __name__ == \'__main__\': unittest.main() ``` Note: Replace `path.to.your.module` with the actual import path where the `send_email_notification` function and `TestEmailService` class are defined.","solution":"def send_email_notification(email_service, to_address, subject, body): try: email_service.send_email(to_address, subject, body) return True except Exception: return False"},{"question":"Coding Assessment Question # Objective Implement a function that reads a list of records from a text file, formats each record using f-strings, and writes the formatted records back to another text file. # Problem Statement Create a function `process_records(input_file, output_file)` that reads from `input_file`, formats each record using f-strings, and writes the results to `output_file`. Each record in the input file is on a separate line, with fields separated by commas. The format of the record is: ``` Name,Age,Score ``` For example: ``` John Doe,23,67 Jane Smith,30,85 ``` The function should format each record into a string that looks like: ``` {Name} is {Age} years old and scored {Score} points. ``` For example: ``` John Doe is 23 years old and scored 67 points. Jane Smith is 30 years old and scored 85 points. ``` # Input - `input_file`: The path to a text file containing records, one per line. - `output_file`: The path to a text file where the formatted records will be written. # Output The function should not return anything. It should write the formatted records to the `output_file`. # Constraints - Each record in the input file is correctly formatted (no need to handle malformed records). - The name field does not contain commas. # Example ```python def process_records(input_file, output_file): pass # Example usage input_content = John Doe,23,67 Jane Smith,30,85 with open(\'input.txt\', \'w\') as f: f.write(input_content) process_records(\'input.txt\', \'output.txt\') with open(\'output.txt\') as f: output_content = f.read() print(output_content) ``` Output: ``` John Doe is 23 years old and scored 67 points. Jane Smith is 30 years old and scored 85 points. ``` # Notes - Use the `with` statement to handle file operations. - Utilize f-strings for formatting the output records. - Ensure proper closing of input and output files.","solution":"def process_records(input_file, output_file): Reads records from input_file, formats them, and writes them to output_file. with open(input_file, \'r\') as infile: lines = infile.readlines() formatted_records = [] for line in lines: name, age, score = line.strip().split(\',\') formatted_record = f\\"{name} is {age} years old and scored {score} points.\\" formatted_records.append(formatted_record) with open(output_file, \'w\') as outfile: for record in formatted_records: outfile.write(record + \'n\')"},{"question":"# Python 310 Floating Point Object Manipulation **Objective:** You are tasked with implementing several functions that interact with floating-point objects using the `python310` package. These functions will allow you to create, check, and manipulate floating-point objects. **Function 1: `create_float_from_string`** - **Input**: A string representation of a floating-point number. - **Output**: A `PyFloatObject` created from the string. - **Constraint**: If the input string cannot be converted to a floating-point number, return `None`. **Function 2: `create_float_from_double`** - **Input**: A double (floating-point number). - **Output**: A `PyFloatObject` created from the double. **Function 3: `is_float_object`** - **Input**: A Python object. - **Output**: `True` if the object is a `PyFloatObject` or a subtype, `False` otherwise. **Function 4: `is_exact_float_object`** - **Input**: A Python object. - **Output**: `True` if the object is exactly a `PyFloatObject` (not a subtype), `False` otherwise. **Function 5: `get_float_as_double`** - **Input**: A `PyFloatObject`. - **Output**: The floating-point value as a double. - **Constraint**: Return `-1.0` if the input object is not a valid `PyFloatObject`. **Function 6: `get_float_info`** - **Output**: A dictionary containing the precision, minimum, and maximum values of a float. **Function 7: `get_max_float`** - **Output**: The maximum representable finite float. **Function 8: `get_min_float`** - **Output**: The minimum normalized positive float. # Example Output ```python str_float = \\"123.45\\" double_float = 123.45 # Function 1 pyfloat_from_str = create_float_from_string(str_float) assert str(create_float_from_string(\\"abc\\")) == \\"None\\" # Function 2 pyfloat_from_double = create_float_from_double(double_float) # Function 3 assert is_float_object(pyfloat_from_str) == True assert is_float_object(\\"not a float\\") == False # Function 4 assert is_exact_float_object(pyfloat_from_double) == True assert is_exact_float_object(123.45) == False # Function 5 assert get_float_as_double(pyfloat_from_double) == 123.45 assert get_float_as_double(\\"not a float\\") == -1.0 # Function 6 float_info = get_float_info() assert \'precision\' in float_info assert \'min\' in float_info assert \'max\' in float_info # Function 7 assert get_max_float() == 1.7976931348623157e+308 # Function 8 assert get_min_float() == 2.2250738585072014e-308 ``` # Instructions 1. Implement the above functions. 2. Ensure that your code properly handles edge cases and invalid inputs as specified. 3. Test your functions with the example outputs provided.","solution":"import sys class PyFloatObject: def __init__(self, value): self.value = value def create_float_from_string(s): Creates and returns a PyFloatObject from a string representation of a floating-point number. If the string cannot be converted to a float, returns None. try: value = float(s) return PyFloatObject(value) except ValueError: return None def create_float_from_double(d): Creates and returns a PyFloatObject from a double (floating-point number). return PyFloatObject(d) def is_float_object(obj): Checks if the given object is a PyFloatObject or a subtype. return isinstance(obj, PyFloatObject) def is_exact_float_object(obj): Checks if the given object is exactly a PyFloatObject (not a subtype). return type(obj) is PyFloatObject def get_float_as_double(obj): Returns the floating-point value as a double if the input object is a valid PyFloatObject, otherwise returns -1.0. if isinstance(obj, PyFloatObject): return obj.value return -1.0 def get_float_info(): Returns a dictionary containing the precision, minimum, and maximum values of a float. return { \'precision\': sys.float_info.epsilon, \'min\': sys.float_info.min, \'max\': sys.float_info.max } def get_max_float(): Returns the maximum representable finite float. return sys.float_info.max def get_min_float(): Returns the minimum normalized positive float. return sys.float_info.min"},{"question":"**Title**: Implement a Custom Neural Network Layer with Dynamic Type Handling using TorchScript **Objective**: Design and implement a custom layer for a neural network that uses TorchScript to handle dynamic input types flexibly. The layer should accept inputs of different types and perform specific operations based on the type of the input data. The implementation will also involve type annotations and custom class definitions. **Question**: You are required to implement a custom layer `CustomDynamicLayer` in PyTorch using TorchScript. This layer should dynamically handle inputs of different types (int, float, torch.Tensor) and perform specific operations based on the type. The required operations are as follows: 1. **Integers**: Increment the integer by 1. 2. **Floats**: Add 0.5 to the float. 3. **Tensors**: Multiply the tensor by 2. # Function Signature ```python import torch from typing import Union, Any @torch.jit.script class CustomDynamicLayer: def __init__(self): pass def forward(self, input: Any) -> Union[int, float, torch.Tensor]: # Your implementation here pass # Example usage: # layer = CustomDynamicLayer() # print(layer.forward(5)) # Should output 6 # print(layer.forward(3.7)) # Should output 4.2 # print(layer.forward(torch.tensor([1, 2, 3]))) # Should output tensor([2, 4, 6]) ``` # Constraints: 1. Implement the `forward` method such that it handles different input types (int, float, torch.Tensor). 2. Use appropriate type annotations to ensure compatibility with TorchScript. 3. Include any necessary type checks and operations within the `forward` method. 4. Your implementation should be TorchScript compilable. 5. The class should be decorated with `@torch.jit.script` to ensure it meets TorchScript\'s type-checking rules. **Performance Requirements**: Ensure that the implementation efficiently handles tensor operations and complies with the static typing constraints of TorchScript. **Evaluation Criteria**: - Correctness of the type handling and operations. - Proper use of type annotations and TorchScript syntax. - Efficiency and readability of the code.","solution":"import torch from typing import Union, Any @torch.jit.script class CustomDynamicLayer: def __init__(self): pass def forward(self, input: Any) -> Union[int, float, torch.Tensor]: if isinstance(input, int): return input + 1 elif isinstance(input, float): return input + 0.5 elif isinstance(input, torch.Tensor): return input * 2 else: raise TypeError(\\"Unsupported input type\\") # Example usage: # layer = CustomDynamicLayer() # print(layer.forward(5)) # Should output 6 # print(layer.forward(3.7)) # Should output 4.2 # print(layer.forward(torch.tensor([1, 2, 3]))) # Should output tensor([2, 4, 6])"},{"question":"Objective: You are tasked with implementing a class using the `hmac` module to securely handle user authentication tokens. The class should be able to generate a token using a given secret key and validate tokens by comparing them against the expected digest. Requirements: 1. **Class Implementation:** Implement a class `AuthToken` with the following methods: - `__init__(self, secret_key: bytes, digestmod: str)`: Initializes the object with a secret key and the name of a hash algorithm (e.g., `\'sha256\'`). - `generate_token(self, user_id: str) -> str`: Generates an HMAC token for the given `user_id`. The token should be a hexadecimal string. - `validate_token(self, user_id: str, token: str) -> bool`: Validates whether the given `token` matches the HMAC digest for the `user_id` using the initialized secret key and hash algorithm. 2. **Input and Output Formats:** - `__init__(self, secret_key: bytes, digestmod: str)`: - `secret_key`: A `bytes` object representing the secret key. - `digestmod`: A `str` representing the name of the hash algorithm. - `generate_token(self, user_id: str) -> str`: - `user_id`: A `str` representing the user ID. - Returns a `str` representing the generated HMAC token in hexadecimal format. - `validate_token(self, user_id: str, token: str) -> bool`: - `user_id`: A `str` representing the user ID. - `token`: A `str` representing the token to be validated. - Returns a `bool` indicating whether the token is valid (`True`) or not (`False`). 3. **Constraints:** - The secret key should be securely stored and never exposed directly. - The `digestmod` must be a valid hash algorithm available in the `hashlib` module. - The `user_id` should be a non-empty string. - The comparison of tokens must be performed in a way that prevents timing attacks. Example: ```python # Example usage of the AuthToken class # Initialize the AuthToken object auth = AuthToken(secret_key=b\'supersecretkey\', digestmod=\'sha256\') # Generate a token for a user user_id = \'user123\' token = auth.generate_token(user_id) print(f\'Token for {user_id}: {token}\') # Validate the token is_valid = auth.validate_token(user_id, token) print(f\'Token valid: {is_valid}\') ``` In this example: 1. We create an `AuthToken` object with a secret key and `\'sha256\'` hash algorithm. 2. We generate a token for a user with ID `\'user123\'`. 3. We validate the token to ensure it matches the expected digest. Implement the `AuthToken` class in Python to achieve the above functionality using the `hmac` module.","solution":"import hmac import hashlib class AuthToken: def __init__(self, secret_key: bytes, digestmod: str): self.secret_key = secret_key self.digestmod = digestmod def generate_token(self, user_id: str) -> str: digest = hmac.new(self.secret_key, user_id.encode(), self.digestmod) return digest.hexdigest() def validate_token(self, user_id: str, token: str) -> bool: expected_token = self.generate_token(user_id) return hmac.compare_digest(expected_token, token)"},{"question":"Objective: You are tasked with creating a Python program that analyzes a given directory containing textual data files and performs the following operations: 1. **Directory Management**: Identify and work with files in a specific directory. 2. **Pattern Matching**: Use regular expressions to extract specific patterns from the text files. 3. **Data Aggregation and Calculation**: Compute statistical metrics based on the extracted data. Description: Write a Python function `analyze_directory(directory_path: str) -> dict` that: 1. Takes a directory path as input. 2. Searches for all `.txt` files in the given directory. 3. Reads each `.txt` file and uses a regular expression to extract all words that contain a specific pattern (e.g., words that start with a vowel). 4. Counts the occurrences of each word that matches the pattern. 5. Computes the mean, median, and variance of the word occurrences across all files. 6. Returns a dictionary containing the count of each matched word and the computed statistical metrics. Input: - `directory_path` (str): A string representing the path to the directory containing the text files. Output: - A dictionary with the following structure: ```python { \\"word_counts\\": {\\"word1\\": count1, \\"word2\\": count2, ...}, \\"mean\\": float, \\"median\\": float, \\"variance\\": float } ``` Constraints: - The directory contains only text files and no subdirectories. - Each `.txt` file is formatted as plain text. - Use the `os`, `glob`, `re`, and `statistics` modules to complete the task. Example: Suppose the directory `/example_dir` contains three files with the following contents: - `file1.txt`: \\"apple orange banana apple\\" - `file2.txt`: \\"orange apple apple orange\\" - `file3.txt`: \\"banana orange orange apple apple\\" If the pattern specifies words starting with a vowel, the function should return: ```python { \\"word_counts\\": {\\"apple\\": 6, \\"orange\\": 5}, \\"mean\\": 5.5, \\"median\\": 5.5, \\"variance\\": 0.5 } ``` # Implementation: Implement the `analyze_directory` function in Python according to the specifications above.","solution":"import os import glob import re import statistics def analyze_directory(directory_path): word_pattern = re.compile(r\'b[aeiouAEIOU]w*b\') # Words starting with a vowel word_counts = {} # Retrieve all .txt files in the given directory txt_files = glob.glob(os.path.join(directory_path, \'*.txt\')) # Process each text file for file_path in txt_files: with open(file_path, \'r\') as file: content = file.read() # Find all words matching the pattern words = word_pattern.findall(content) for word in words: word_counts[word] = word_counts.get(word, 0) + 1 # Calculate the mean, median, and variance of word occurrences counts = list(word_counts.values()) if counts: mean = statistics.mean(counts) median = statistics.median(counts) variance = statistics.variance(counts) if len(counts) > 1 else 0.0 else: mean = median = variance = 0.0 return { \\"word_counts\\": word_counts, \\"mean\\": mean, \\"median\\": median, \\"variance\\": variance }"},{"question":"You are given a machine learning problem where you need to compute the gradient of a loss function with respect to the model parameters for each sample in the batch efficiently. # Task Implement a function `compute_batch_gradients` that takes in a dataset and model weights, and returns the per-sample gradients of the loss function with respect to the model parameters. Input - `weights`: A tensor of shape `(feature_size,)` representing the model weights. - `examples`: A tensor of shape `(batch_size, feature_size)` representing the input dataset. - `targets`: A tensor of shape `(batch_size,)` representing the target values. Output - A tensor of shape `(batch_size, feature_size)` representing the gradients of the loss function with respect to the model weights for each sample. Requirements 1. Define a model function `model(weights, feature_vec)` that performs a linear operation followed by a ReLU activation. 2. Define a loss function `compute_loss(weights, example, target)` that computes the Mean Squared Error (MSE) between the model output and the target. 3. Use `torch.func.grad` and `torch.func.vmap` to calculate the per-sample gradients. # Constraints - You must use `torch.func.grad` and `torch.func.vmap` to compute the per-sample gradients. # Performance Requirements - The function should be efficient enough to handle batches of size up to `1024`. # Example ```python import torch from torch.func import vmap, grad def model(weights, feature_vec): # Very simple linear model with activation assert feature_vec.dim() == 1 return feature_vec.dot(weights).relu() def compute_loss(weights, example, target): y = model(weights, example) return ((y - target) ** 2).mean() # MSELoss def compute_batch_gradients(weights, examples, targets): inputs = (weights, examples, targets) grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs) return grad_weight_per_example # Example usage batch_size, feature_size = 3, 5 weights = torch.randn(feature_size, requires_grad=True) examples = torch.randn(batch_size, feature_size) targets = torch.randn(batch_size) gradients = compute_batch_gradients(weights, examples, targets) print(gradients) ``` In this example, `gradients` will contain the gradients of the loss function with respect to the model weights for each sample in the batch.","solution":"import torch from torch.func import grad, vmap def model(weights, feature_vec): Simple linear model with ReLU activation return torch.nn.functional.relu(feature_vec.matmul(weights)) def compute_loss(weights, example, target): Computes Mean Squared Error (MSE) loss between model prediction and target y = model(weights, example) return ((y - target) ** 2).mean() def compute_batch_gradients(weights, examples, targets): Computes per-sample gradients of the loss function with respect to the model weights # vec_grad computes the gradient of the loss function w.r.t weights for each sample vec_grad = vmap(grad(compute_loss), in_dims=(None, 0, 0)) return vec_grad(weights, examples, targets)"},{"question":"**Objective:** Demonstrate your knowledge of creating and customizing color palettes using the seaborn library. **Problem Statement:** You are tasked with generating a visually appealing color palette for a data visualization project. Specifically, you will create a sequential color palette that transitions from light gray to a specified color, modify it to meet certain design specifications, and use this palette to style a simple data visualization. **Requirements:** 1. **Function Implementation:** - Implement a function named `generate_palette` that takes the following parameters: - `color`: A string representing the target color (can be specified by name, hex code, or HUSL system). - `n_colors`: An integer specifying the number of colors in the palette (default should be 6). - `as_cmap`: A boolean indicating whether to return the palette as a continuous colormap (default should be `False`). - The function should return the generated color palette. ```python def generate_palette(color, n_colors=6, as_cmap=False): import seaborn as sns # Generate the color palette palette = sns.light_palette(color, n_colors=n_colors, as_cmap=as_cmap) return palette ``` 2. **Data Visualization:** - Implement a function named `visualize_data` that takes the following parameters: - `data`: A 2D list representing data to be visualized. - `palette`: The color palette generated by the `generate_palette` function. - The function should create a heatmap using seaborn\'s `heatmap` function, styled with the specified color palette. ```python def visualize_data(data, palette): import seaborn as sns import matplotlib.pyplot as plt # Create a heatmap sns.heatmap(data, cmap=palette) plt.show() ``` **Example Usage:** ```python # Generate a color palette with 8 colors transitioning to \'seagreen\' palette = generate_palette(\'seagreen\', n_colors=8) # Sample data for visualization data = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] # Visualize the data using the generated palette visualize_data(data, palette) ``` **Constraints:** - The input color for `generate_palette` can be specified in any of the formats supported by seaborn (`str` for name or hex code, `tuple` for HUSL system). - The number of colors `n_colors` should be a positive integer. - The `data` parameter for `visualize_data` should be a 2D list of numerical values. **Deliverables:** - The implementation of both `generate_palette` and `visualize_data` functions meeting the described requirements. - An example script showcasing the usage of these functions to generate a color palette and visualize sample data.","solution":"def generate_palette(color, n_colors=6, as_cmap=False): import seaborn as sns # Generate the color palette palette = sns.light_palette(color, n_colors=n_colors, as_cmap=as_cmap) return palette def visualize_data(data, palette): import seaborn as sns import matplotlib.pyplot as plt # Create a heatmap sns.heatmap(data, cmap=palette) plt.show()"},{"question":"# System Information Gathering In this coding assessment, you are required to implement a function that uses the `platform` module to gather and return comprehensive information about the system on which the code is being executed. Your function should use multiple functions from the `platform` module to collect the data. Function Signature ```python def get_system_info() -> dict: ``` Expected Input The function does not take any input parameters. Expected Output The function should return a dictionary with the following keys and corresponding values: - `\\"architecture\\"`: A tuple containing the bit architecture and linkage format of the Python interpreter executable. - `\\"machine\\"`: The machine type. - `\\"node\\"`: The computer\'s network name. - `\\"platform\\"`: A string identifying the underlying platform. - `\\"processor\\"`: The processor name. - `\\"python_build\\"`: A tuple containing the Python build number and date. - `\\"python_compiler\\"`: The compiler used to compile Python. - `\\"python_implementation\\"`: The Python implementation. - `\\"python_version\\"`: The Python version as a string in the format \'major.minor.patchlevel\'. - `\\"release\\"`: The system\'s release version. - `\\"system\\"`: The system/OS name. - `\\"version\\"`: The system\'s release version description. - `\\"uname\\"`: A named tuple with attributes \'system\', \'node\', \'release\', \'version\', \'machine\', and \'processor\'. Constraints - The function should handle cases where the information is unavailable by returning appropriate default values as documented in the `platform` module. Example ```python def get_system_info() -> dict: # Your implementation here pass info = get_system_info() print(info) ``` Expected output (example, may vary based on actual system): ```python { \\"architecture\\": (\\"64bit\\", \\"\\"), \\"machine\\": \\"x86_64\\", \\"node\\": \\"hostname\\", \\"platform\\": \\"Linux-5.4.0-42-generic-x86_64-with-glibc2.29\\", \\"processor\\": \\"x86_64\\", \\"python_build\\": (\\"default\\", \\"Jul 1 2020 17:38:17\\"), \\"python_compiler\\": \\"GCC 9.3.0\\", \\"python_implementation\\": \\"CPython\\", \\"python_version\\": \\"3.8.5\\", \\"release\\": \\"5.4.0-42-generic\\", \\"system\\": \\"Linux\\", \\"version\\": \\"#46-Ubuntu SMP Wed Jun 24 10:55:06 UTC 2020\\", \\"uname\\": uname_result(system=\'Linux\', node=\'hostname\', release=\'5.4.0-42-generic\', version=\'#46-Ubuntu SMP Wed Jun 24 10:55:06 UTC 2020\', machine=\'x86_64\', processor=\'x86_64\') } ```","solution":"import platform def get_system_info() -> dict: Gather and return comprehensive information about the system. info = { \\"architecture\\": platform.architecture(), \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build\\": platform.python_build(), \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_version\\": platform.python_version(), \\"release\\": platform.release(), \\"system\\": platform.system(), \\"version\\": platform.version(), \\"uname\\": platform.uname() } return info"},{"question":"Coding Assessment Question # Objective: Demonstrate comprehension of the `fileinput` module in Python, including handling multiple input files, using context managers, and performing in-place file modifications. # Problem Statement: You are given multiple text files containing numeric data (one number per line). Implement a Python function `process_files` that reads from these files, calculates the sum of numbers from all files combined, and replaces each number in the files with its running cumulative sum. # Requirements: 1. **Function Signature**: Your function should be implemented as: ```python def process_files(file_paths: list) -> None: ``` 2. **Input**: - `file_paths`: A list of strings where each string is a path to a text file. 3. **Output**: - The function returns `None`. The in-place modification of input files is the main objective. 4. **Constraints**: - Each file contains one number per line, and all numbers are integers. - Ensure that file handling is clean, preventing any resource leaks (use of context managers is recommended). - Handle empty files gracefully. 5. **Performance**: - The function should be efficient in terms of both time and space complexity. # Example: Given three files: - `file1.txt` with content: ``` 1 2 3 ``` - `file2.txt` with content: ``` 4 5 ``` - `file3.txt` with content: ``` 6 ``` After calling `process_files([\'file1.txt\', \'file2.txt\', \'file3.txt\'])`, the files should be modified as follows: - `file1.txt`: ``` 1 3 6 ``` - `file2.txt`: ``` 10 15 ``` - `file3.txt`: ``` 21 ``` # Guidelines: - Use the `fileinput` module for reading and modifying files. - Ensure the operations are performed in a single pass to maintain efficiency. - Implement helper functions if necessary for better code organization. # Hints: 1. Utilize `fileinput.input` with `inplace=True` to handle in-place file modifications. 2. Track running cumulative sum while iterating over each line. 3. Consider edge cases such as empty files and files containing non-integer values. Good luck!","solution":"import fileinput def process_files(file_paths): cumulative_sum = 0 with fileinput.input(files=file_paths, inplace=True) as f: for line in f: if line.strip(): num = int(line.strip()) cumulative_sum += num print(cumulative_sum)"},{"question":"# Question: Analyzing Function Inputs using the `inspect` Module Write a function named `retrieve_function_info` that takes another function as its argument and returns a dictionary containing detailed information about the function. The returned dictionary should include: - `name`: The name of the function. - `doc`: The documentation string of the function or `None` if not present. - `source`: The source code of the function as a string. - `parameters`: A dictionary where keys are parameter names and values are dictionaries with: - `annotation`: The annotation of the parameter or `None` if not present. - `default`: The default value of the parameter or `None` if not present. - `kind`: The kind of the parameter (e.g., positional or keyword-only). You can use the `inspect` module to help with retrieving this information. Function signature: ```python def retrieve_function_info(func: callable) -> dict: ``` Example: ```python def example_function(a: int, b: str = \\"default\\", *args, **kwargs) -> None: This is an example function. print(a, b) # Expected Output retrieve_function_info(example_function) # { # \\"name\\": \\"example_function\\", # \\"doc\\": \\"This is an example function.\\", # \\"source\\": \\"def example_function(a: int, b: str = \'default\', *args, **kwargs) -> None:n \\"\\"\\"n This is an example function.n \\"\\"\\"n print(a, b)\\", # \\"parameters\\": { # \\"a\\": {\\"annotation\\": int, \\"default\\": None, \\"kind\\": \\"POSITIONAL_OR_KEYWORD\\"}, # \\"b\\": {\\"annotation\\": str, \\"default\\": \\"default\\", \\"kind\\": \\"POSITIONAL_OR_KEYWORD\\"}, # \\"args\\": {\\"annotation\\": None, \\"default\\": None, \\"kind\\": \\"VAR_POSITIONAL\\"}, # \\"kwargs\\": {\\"annotation\\": None, \\"default\\": None, \\"kind\\": \\"VAR_KEYWORD\\"} # } # } ``` # Constraints - The function must handle all valid Python functions, including those with complex signatures such as `*args` and `**kwargs`. - The function should raise a `TypeError` if the input is not a Python function. Hints - Use `inspect.getdoc()`, `inspect.getsource()`, and `inspect.signature()` to retrieve relevant information. - The `Parameter` object from the `inspect` module can help in understanding each parameter\'s details.","solution":"import inspect def retrieve_function_info(func: callable) -> dict: if not callable(func): raise TypeError(\\"Input is not a callable function.\\") func_info = {} func_info[\'name\'] = func.__name__ func_info[\'doc\'] = inspect.getdoc(func) func_info[\'source\'] = inspect.getsource(func) params = inspect.signature(func).parameters parameters = {} for name, param in params.items(): parameters[name] = { \'annotation\': param.annotation if param.annotation is not param.empty else None, \'default\': param.default if param.default is not param.empty else None, \'kind\': param.kind.name } func_info[\'parameters\'] = parameters return func_info"},{"question":"# Asynchronous Programming with Asyncio In this coding exercise, demonstrate your understanding of Python\'s asyncio package by implementing an asynchronous task manager. Your task manager will manage multiple tasks with varying completion times, handle timeouts and cancellations, and gather results. # Task: 1. Write an asynchronous function `async_task` that takes a name (string) and a duration (integer, representing seconds). The function should print a start message, await the specified duration using `await asyncio.sleep(duration)`, and then print a completion message. 2. Write an asynchronous function `task_manager` that: - Accepts a list of tasks. Each task is represented as a tuple with the task name and duration. - Schedules all tasks using `asyncio.create_task`. - Waits until all tasks complete, using `asyncio.gather`. - Manages a timeout of 5 seconds for each task using `asyncio.wait_for`. Handle tasks that do not complete within the timeout by cancelling them and printing a cancellation message. - Handles cancellations gracefully, making sure to print if a task was cancelled. - Aggregates and returns the results of the tasks that completed successfully within the timeout period. 3. Write an `asyncio.run` call to execute `task_manager` with a sample list of tasks. # Expected input and output formats: Input: - A list of tasks in the format `[(\'Task1\', 3), (\'Task2\', 7), (\'Task3\', 2)]` Output: - Console prints: - Start and completion/cancellation messages for each task. - Return: - A list with the names of tasks that completed within the timeout period. # Constraints: - Each task should respect the 5-second timeout period. - Ensure that tasks are properly cancelled and handled if they exceed the timeout. # Example: ```python import asyncio async def async_task(name: str, duration: int): print(f\\"{name} started\\") try: await asyncio.sleep(duration) print(f\\"{name} completed\\") except asyncio.CancelledError: print(f\\"{name} was cancelled\\") raise async def task_manager(tasks: list): task_objs = [] for name, duration in tasks: task = asyncio.create_task(async_task(name, duration)) task_objs.append(task) completed_results = [] try: results = await asyncio.gather(*[ asyncio.wait_for(task, timeout=5) for task in task_objs ], return_exceptions=True) for result in results: if isinstance(result, asyncio.CancelledError): continue completed_results.append(result) except asyncio.TimeoutError: pass return completed_results tasks = [(\'Task1\', 3), (\'Task2\', 7), (\'Task3\', 2)] results = asyncio.run(task_manager(tasks)) print(\\"Completed tasks:\\", results) ``` # Performance requirements: - Ensure all tasks are scheduled concurrently for efficient execution. - Tasks running longer than 5 seconds should be properly handled without blocking other tasks.","solution":"import asyncio async def async_task(name: str, duration: int): Asynchronous task that waits for a given duration and handles timeout. print(f\\"{name} started\\") try: await asyncio.sleep(duration) print(f\\"{name} completed\\") return name except asyncio.CancelledError: print(f\\"{name} was cancelled\\") raise async def task_manager(tasks: list): Manages multiple asynchronous tasks with a timeout and collects results of the tasks that completed within the timeout period. task_objs = [] for name, duration in tasks: task = asyncio.create_task(async_task(name, duration)) task_objs.append(task) completed_results = [] try: results = await asyncio.gather(*[ asyncio.wait_for(task, timeout=5) for task in task_objs ], return_exceptions=True) for result in results: if isinstance(result, str): completed_results.append(result) except asyncio.TimeoutError: pass return completed_results # Example usage if __name__ == \\"__main__\\": tasks = [(\'Task1\', 3), (\'Task2\', 7), (\'Task3\', 2)] results = asyncio.run(task_manager(tasks)) print(\\"Completed tasks:\\", results)"},{"question":"# Question **Task:** Implement a simplified interactive Python environment that can execute user-provided code snippets. This mini interpreter should: 1. Continuously read user input (code snippets) until the user types \\"exit()\\" to quit. 2. Handle and display syntax errors appropriately. 3. Execute valid complete code snippets and display any output or errors. 4. Require more input for incomplete lines and prompt the user accordingly. **Requirements:** 1. Implement a class `MiniInterpreter` with the following methods: - **`__init__(self)`**: Initialize the interpreter environment. - **`run(self)`**: The main method to start the interactive interpreter loop. - **`execute(self, code: str) -> bool`**: Method to compile and execute a given code snippet. - **`display_error(self, error: Exception)`**: Method to display syntax or runtime errors. 2. The `run` method should continuously prompt the user for input, using `sys.ps1` and `sys.ps2` for primary and secondary prompts, respectively. The interpreter should identify and handle incomplete code snippets, in need of further input, and should only execute when a complete valid command is provided. 3. Use the provided *template* for the `MiniInterpreter` class: ```python import code import sys class MiniInterpreter: def __init__(self): self.console = code.InteractiveConsole() def run(self): while True: try: user_input = input(sys.ps1) if user_input.strip() == \\"exit()\\": print(\\"Exiting the interpreter.\\") break more_input_required = self.console.push(user_input) while more_input_required: user_input = input(sys.ps2) if user_input.strip() == \\"exit()\\": print(\\"Exiting the interpreter.\\") return more_input_required = self.console.push(user_input) except (EOFError, KeyboardInterrupt): print(\\"nExiting the interpreter.\\") break def execute(self, code: str) -> bool: try: return self.console.push(code) except Exception as e: self.display_error(e) return False def display_error(self, error: Exception): self.console.showtraceback() # Example execution entry point if __name__ == \\"__main__\\": interpreter = MiniInterpreter() interpreter.run() ``` **Additional Notes:** - The `execute` method should return `False` if the input code is complete and executed or invalid; it should return `True` if more input is required. - Handle and display exceptions using the `display_error` method. You can test your interpreter by running scripts that input various Python commands interactively, ensuring they execute correctly and handle errors appropriately.","solution":"import code import sys class MiniInterpreter: def __init__(self): self.console = code.InteractiveConsole() sys.ps1 = \\">>> \\" sys.ps2 = \\"... \\" def run(self): while True: try: user_input = input(sys.ps1) if user_input.strip() == \\"exit()\\": print(\\"Exiting the interpreter.\\") break more_input_required = self.console.push(user_input) while more_input_required: user_input = input(sys.ps2) if user_input.strip() == \\"exit()\\": print(\\"Exiting the interpreter.\\") return more_input_required = self.console.push(user_input) except (EOFError, KeyboardInterrupt): print(\\"nExiting the interpreter.\\") break def execute(self, code: str) -> bool: try: return self.console.push(code) except Exception as e: self.display_error(e) return False def display_error(self, error: Exception): self.console.showtraceback() # Example execution entry point if __name__ == \\"__main__\\": interpreter = MiniInterpreter() interpreter.run()"},{"question":"# Question: Manipulating Function Objects in Python You are given a task to manipulate Python function objects dynamically. You should write a Python code performing the following operations: 1. **Create a Function**: Create a function `multiply` that takes two arguments `a` and `b` and returns their product. 2. **Retrieve Function Code Object**: Retrieve and print the code object associated with the `multiply` function. 3. **Retrieve Globals Dictionary**: Retrieve and print the globals dictionary associated with the `multiply` function. 4. **Set and Retrieve Default Arguments**: Set a default value of 10 for the second argument `b` of the `multiply` function and then retrieve and print the default argument. 5. **Set and Retrieve Annotations**: Add type annotations to the `multiply` function where both `a` and `b` are of type `int` and the return type is also `int`, and then retrieve and print the annotations. 6. **Closure Creation**: Create a closure for the function where the `multiply` function accesses an inherited variable from an enclosing scope. Demonstrate and print this by enclosing a variable `factor` in an outer function and having `multiply` use this `factor`. Provide the implementation for: - Creating the function. - Functions to retrieve and print the code object, globals dictionary, default arguments, and annotations. - Demonstrating closure with an example. Write concise, clear, and correct functions in Python that achieve the above tasks. Example of the closure task: ```python def create_multiplier(factor): def multiplier(a): return a * factor return multiplier multiply_by_5 = create_multiplier(5) print(multiply_by_5(10)) # Should print 50 ``` Constraints: - You must demonstrate the use of closures effectively. - All retrievals (code object, globals dictionary, etc.) should be printed in a human-readable way. Expected Output: The output of your main function should demonstrate the steps and print outputs clearly for verification.","solution":"def multiply(a, b): Returns the product of a and b. return a * b def get_code_object(func): Returns the code object associated with the given function. return func.__code__ def get_globals_dict(func): Returns the globals dictionary associated with the given function. return func.__globals__ def set_default_args(func, defaults): Sets the default arguments for the given function. func.__defaults__ = defaults def get_default_args(func): Returns the default arguments of the given function. return func.__defaults__ def set_annotations(func, annotations): Sets the annotations for the given function. func.__annotations__ = annotations def get_annotations(func): Returns the annotations of the given function. return func.__annotations__ def create_closure(factor): Creates a closure where the multiply function uses an enclosed variable `factor`. def closure_multiply(a): return a * factor return closure_multiply # Example Usage: # Retrieve and print the code object associated with the multiply function code_obj = get_code_object(multiply) print(\\"Code object:\\", code_obj) # Retrieve and print the globals dictionary associated with the multiply function globals_dict = get_globals_dict(multiply) print(\\"Globals dictionary:\\", globals_dict) # Set the default arguments for the multiply function and print them set_default_args(multiply, (10,)) default_args = get_default_args(multiply) print(\\"Default arguments:\\", default_args) # Set and retrieve annotations set_annotations(multiply, {\'a\': int, \'b\': int, \'return\': int}) annotations = get_annotations(multiply) print(\\"Annotations:\\", annotations) # Demonstrate closure creation multiplier_with_factor_5 = create_closure(5) closure_result = multiplier_with_factor_5(10) print(\\"Closure result with factor 5 and input 10:\\", closure_result)"},{"question":"**Objective:** Implement and compare the performance of three different clustering algorithms (K-Means, DBSCAN, and Agglomerative Clustering) on a given dataset. **Problem Statement:** You are provided with a dataset `data.csv` which contains a set of points in 2D space. Your task is to: 1. Load the data from the file. 2. Implement clustering using the following algorithms: - K-Means - DBSCAN - Agglomerative Clustering 3. For each algorithm, determine the following: - Number of clusters found - Silhouette score for the clustering result 4. Plot the clusters and their centroids (for K-Means), core samples (for DBSCAN), or anything similar that makes sense for Agglomerative Clustering. 5. Write a brief (1-2 paragraph) analysis of the results, comparing the effectiveness of each algorithm on this dataset. **Requirements:** 1. The implementation should be done in Python using the scikit-learn library. 2. Input format: - The dataset file `data.csv` containing two columns: `x` and `y`. 3. Output format: - Number of clusters and silhouette score for each algorithm. - A plot for the clustering results of each algorithm. - A brief analysis comparing the clustering results. **Constraints:** - You should use the default parameters for each algorithm unless specified otherwise. - Ensure the code is efficient and handles the computational complexity within reasonable limits. **Dataset Example (`data.csv`):** ```csv x,y 1.0,2.0 1.5,1.8 1.2,2.1 5.0,8.0 5.5,8.1 5.2,8.2 6.0,1.0 7.5,1.2 7.2,0.8 ``` **Expected Output:** Print the number of clusters and silhouette score for each algorithm, plot the results, and provide a brief written analysis. **Sample Code Skeleton:** ```python import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import silhouette_score def load_data(file_path): # Load the dataset data = pd.read_csv(file_path) return data def kmeans_clustering(data): # Implement K-Means clustering kmeans = KMeans(n_clusters=3) labels = kmeans.fit_predict(data) score = silhouette_score(data, labels) return labels, kmeans.cluster_centers_, score def dbscan_clustering(data): # Implement DBSCAN clustering dbscan = DBSCAN(eps=0.5, min_samples=5) labels = dbscan.fit_predict(data) score = silhouette_score(data, labels) return labels, score def agglomerative_clustering(data): # Implement Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=3) labels = agglomerative.fit_predict(data) score = silhouette_score(data, labels) return labels, score def plot_clusters(data, labels, title, centers=None): # Plot the clusters plt.scatter(data[\'x\'], data[\'y\'], c=labels, cmap=\'viridis\') if centers is not None: plt.scatter(centers[:, 0], centers[:, 1], s=300, c=\'red\', marker=\'X\') plt.title(title) plt.show() def main(): file_path = \'data.csv\' data = load_data(file_path) kmeans_labels, kmeans_centers, kmeans_score = kmeans_clustering(data) print(f\'K-Means: {len(set(kmeans_labels))} clusters, Silhouette Score: {kmeans_score}\') plot_clusters(data, kmeans_labels, \'K-Means Clustering\', kmeans_centers) dbscan_labels, dbscan_score = dbscan_clustering(data) print(f\'DBSCAN: {len(set(dbscan_labels))} clusters, Silhouette Score: {dbscan_score}\') plot_clusters(data, dbscan_labels, \'DBSCAN Clustering\') agglomerative_labels, agglomerative_score = agglomerative_clustering(data) print(f\'Agglomerative: {len(set(agglomerative_labels))} clusters, Silhouette Score: {agglomerative_score}\') plot_clusters(data, agglomerative_labels, \'Agglomerative Clustering\') analyze_results(kmeans_score, dbscan_score, agglomerative_score) def analyze_results(kmeans_score, dbscan_score, agglomerative_score): print(\\"K-Means Clustering performed with a silhouette score of \\", kmeans_score) print(\\"DBSCAN Clustering performed with a silhouette score of \\", dbscan_score) print(\\"Agglomerative Clustering performed with a silhouette score of \\", agglomerative_score) print(\\"From these results, it can be observed that...\\") if __name__ == \\"__main__\\": main() ```","solution":"import pandas as pd import matplotlib.pyplot as plt from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import silhouette_score def load_data(file_path): Load the dataset from a CSV file. data = pd.read_csv(file_path) return data def kmeans_clustering(data): Perform K-Means clustering on the data. kmeans = KMeans(n_clusters=3, random_state=42) labels = kmeans.fit_predict(data) score = silhouette_score(data, labels) return labels, kmeans.cluster_centers_, score def dbscan_clustering(data): Perform DBSCAN clustering on the data. dbscan = DBSCAN(eps=0.5, min_samples=2) labels = dbscan.fit_predict(data) if len(set(labels)) > 1: score = silhouette_score(data, labels) else: score = -1 # Score is not defined for one cluster return labels, score def agglomerative_clustering(data): Perform Agglomerative Clustering on the data. agglomerative = AgglomerativeClustering(n_clusters=3) labels = agglomerative.fit_predict(data) score = silhouette_score(data, labels) return labels, score def plot_clusters(data, labels, title, centers=None): Plot the clusters resulting from the clustering algorithm. plt.scatter(data[\'x\'], data[\'y\'], c=labels, cmap=\'viridis\', marker=\'o\') if centers is not None: plt.scatter(centers[:, 0], centers[:, 1], s=300, c=\'red\', marker=\'X\') plt.title(title) plt.xlabel(\'X\') plt.ylabel(\'Y\') plt.show() def main(): file_path = \'data.csv\' data = load_data(file_path) kmeans_labels, kmeans_centers, kmeans_score = kmeans_clustering(data) print(f\'K-Means: {len(set(kmeans_labels))} clusters, Silhouette Score: {kmeans_score}\') plot_clusters(data, kmeans_labels, \'K-Means Clustering\', kmeans_centers) dbscan_labels, dbscan_score = dbscan_clustering(data) print(f\'DBSCAN: {len(set(dbscan_labels))} clusters, Silhouette Score: {dbscan_score}\') plot_clusters(data, dbscan_labels, \'DBSCAN Clustering\') agglomerative_labels, agglomerative_score = agglomerative_clustering(data) print(f\'Agglomerative: {len(set(agglomerative_labels))} clusters, Silhouette Score: {agglomerative_score}\') plot_clusters(data, agglomerative_labels, \'Agglomerative Clustering\') analyze_results(kmeans_score, dbscan_score, agglomerative_score) def analyze_results(kmeans_score, dbscan_score, agglomerative_score): Provide an analysis of the clustering results. print(\\"K-Means Clustering performed with a silhouette score of \\", kmeans_score) print(\\"DBSCAN Clustering performed with a silhouette score of \\", dbscan_score) print(\\"Agglomerative Clustering performed with a silhouette score of \\", agglomerative_score) print(\\"From these results, it can be observed that...\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: XML Data Manipulation and Querying with `xml.etree.ElementTree` In this exercise, you will use the `xml.etree.ElementTree` module to parse, manipulate, and query an XML document. Given an XML file `books.xml` with the following structure: ```xml <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies.</description> </book> <!-- more book elements --> </catalog> ``` You must implement a function `process_books(xml_file)` that performs the following tasks: 1. Parse the XML document from the given file `xml_file`. 2. Extract all books with a price greater than 10. 3. Modify the description of these books by appending the string `\\" (Expensive Book)\\"`. 4. Write the modified XML content to a new file named `processed_books.xml`. Your implementation should include the following considerations: - Use XPath to efficiently query elements. - Handle XML elements and attributes correctly. - Ensure the output XML is well-formed and retains the original structure apart from the modifications. # Function Signature ```python def process_books(xml_file: str) -> None: ``` # Input - `xml_file` (str): The path to the input XML file containing the catalog of books. # Output - The function should create an output file named `processed_books.xml` with the modified content. # Example Given `books.xml`: ```xml <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies.</description> </book> </catalog> ``` Output in `processed_books.xml`: ```xml <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML. (Expensive Book)</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies.</description> </book> </catalog> ``` # Constraints - Assume the XML file is well-formed and the structure is as described. - Only books with prices strictly greater than 10 should have their descriptions modified. - Your function should not return anything. Implement the `process_books` function to solve this problem.","solution":"import xml.etree.ElementTree as ET def process_books(xml_file: str) -> None: # Parse the XML document tree = ET.parse(xml_file) root = tree.getroot() # Find all book elements where the price is greater than 10 for book in root.findall(\'book\'): price = float(book.find(\'price\').text) if price > 10: description = book.find(\'description\') if description is not None: description.text += \\" (Expensive Book)\\" # Write the modified XML to a new file tree.write(\'processed_books.xml\', xml_declaration=True, encoding=\'utf-8\')"},{"question":"# Coding Challenge: Custom Codec Implementation The provided documentation describes various functions related to codec registration, codec lookup, and error handling. In this challenge, you will demonstrate your understanding of these concepts by implementing a custom codec and encoding/decoding operations in simulated environments. Task 1. **Register a Custom Codec**: - Create a custom codec named `reverse_codec` that simply reverses any given string input during encoding and decoding. - Ensure that your codec can be registered and unregistered using the APIs provided. 2. **Encoding and Decoding**: - Implement encoding and decoding functions using your custom codec. - Handle error scenarios such as unknown encodings or invalid input types gracefully, using appropriate error handling methods. 3. **Perform a Series of Operations**: - Register your custom codec. - Encode the string `\\"hello world\\"` using your custom codec. - Decode the encoded string back to its original form. - Unregister your custom codec after the operations are complete. Constraints - The codec should work with string objects only. - You should use the APIs outlined in the documentation to register/unregister codecs, and to perform encoding/decoding. - Error handling should use the `PyCodec_ReplaceErrors` method that replaces problematic characters with a `?`. Input Your functions may not take input directly but should demonstrate the specified operations in a self-contained manner. Test cases will be used to validate your implementation. Output Return the decoded string after performing all operations. Example ```python # Example steps (your implementation should encapsulate these). # Register reverse_codec PyCodec_Register(reverse_codec) # Encode \\"hello world\\" encoded_string = PyCodec_Encode(\\"hello world\\", \\"reverse_codec\\", \\"replace_errors\\") # Decode the encoded string decoded_string = PyCodec_Decode(encoded_string, \\"reverse_codec\\", \\"replace_errors\\") # Unregister reverse_codec PyCodec_Unregister(reverse_codec) # Output decoded string print(decoded_string) # Should output: \\"hello world\\" ``` Implementation Provide a Python script or function definitions that accomplish the above tasks.","solution":"import codecs class ReverseCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): try: if not isinstance(input, str): raise TypeError(\'input should be a string\') return (input[::-1], len(input)) except Exception as e: return (\'?\', len(input)) def decode(self, input, errors=\'strict\'): try: if not isinstance(input, str): raise TypeError(\'input should be a string\') return (input[::-1], len(input)) except Exception as e: return (\'?\', len(input)) def reverse_codec(name): if name == \'reverse_codec\': return codecs.CodecInfo( name=\'reverse_codec\', encode=ReverseCodec().encode, decode=ReverseCodec().decode, ) return None def register_reverse_codec(): codecs.register(reverse_codec) def unregister_reverse_codec(): codecs.unregister(reverse_codec) def encode_string(input_string): return codecs.encode(input_string, \'reverse_codec\', \'replace\') def decode_string(encoded_string): return codecs.decode(encoded_string, \'reverse_codec\', \'replace\') def perform_operations(): register_reverse_codec() encoded_string = encode_string(\\"hello world\\") decoded_string = decode_string(encoded_string) unregister_reverse_codec() return decoded_string"},{"question":"# Advanced Python Coding Assessment **Understanding Coroutines and Asynchronous Programming in Python** **Objective:** This question is designed to assess your understanding of asynchronous programming using coroutines in Python. You will implement a simple asynchronous function and a coroutine to demonstrate these concepts. **Question:** Write an asynchronous function `fetch_data` using the `async` and `await` keywords, and a coroutine function `process_data`. The `fetch_data` function should simulate fetching data from a remote server by sleeping for a specified time and then returning a dictionary with the fetched data. The `process_data` coroutine should then process this data and return the result. **Detailed Requirements:** 1. **`fetch_data` function:** - **Input**: Integer `delay` - the number of seconds to simulate the data fetching delay. - **Output**: Dictionary `{ \'status\': \'success\', \'data\': \'sample data\' }` - The function should use `await asyncio.sleep(delay)` to simulate the delay. 2. **`process_data` coroutine:** - **Input**: Integer `delay` - **Output**: String - processed result of fetched data. - The coroutine should: - Await the completion of `fetch_data(delay)`. - Process the returned data (for simplicity, concatenate the \'data\' value with the string \\" processed\\"). - Return the processed result string. ```python import asyncio async def fetch_data(delay: int) -> dict: Simulate fetching data from a remote server. Parameters: delay (int): Number of seconds to delay. Returns: dict: A dictionary containing status and data. await asyncio.sleep(delay) return {\'status\': \'success\', \'data\': \'sample data\'} async def process_data(delay: int) -> str: Process fetched data after awaiting fetch_data. Parameters: delay (int): Number of seconds to delay for fetching data. Returns: str: Processed result of fetched data. data = await fetch_data(delay) return data[\'data\'] + \' processed\' # Example usage (for testing purposes only, not part of the required submission) # asyncio.run(process_data(2)) ``` **Constraints:** - The `delay` parameter will be a non-negative integer. - You must use the `async`/`await` syntax. - You should not use any external libraries other than `asyncio`. **Performance Requirements:** - The solution should handle scenarios with delays up to 10 seconds gracefully. Ensure your code is well-documented and includes type annotations. You may write additional helper functions if necessary, but the primary focus should be on implementing and using asynchronous programming constructs.","solution":"import asyncio async def fetch_data(delay: int) -> dict: Simulate fetching data from a remote server. Parameters: delay (int): Number of seconds to delay. Returns: dict: A dictionary containing status and data. await asyncio.sleep(delay) return {\'status\': \'success\', \'data\': \'sample data\'} async def process_data(delay: int) -> str: Process fetched data after awaiting fetch_data. Parameters: delay (int): Number of seconds to delay for fetching data. Returns: str: Processed result of fetched data. data = await fetch_data(delay) return data[\'data\'] + \' processed\' # Example usage (for testing purposes only, not part of the required submission) # asyncio.run(process_data(2))"},{"question":"**Objective**: Create a function that demonstrates the use of seaborn\'s `sns.axes_style` to modify plot aesthetics and use temporary style changes. **Function Implementation**: The function should be named `plot_with_styles` and should perform the following tasks: - Accept two string arguments, `default_style` and `temporary_style`, which represent seaborn style parameters. - Plot a sample barplot using the `default_style`. - Within a context manager, temporarily change the style to `temporary_style` and plot another barplot. **Input**: - `default_style` (string): The seaborn style to be used as the default. - `temporary_style` (string): The seaborn style to be applied temporarily within a context manager. **Output**: - The function should not return anything. - It should display two barplots: - The first barplot using `default_style`. - The second barplot using `temporary_style`. **Constraints**: - Use the following data for barplots: ```python x = [1, 2, 3, 4] y = [10, 20, 15, 25] ``` **Example**: ```python def plot_with_styles(default_style: str, temporary_style: str) -> None: # Implementation here # Usage example: plot_with_styles(\\"dark\\", \\"whitegrid\\") ``` **Note**: Ensure that the function handles invalid style names gracefully by using a try-except block and defaulting to seaborn\'s default style if an invalid style name is provided.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_with_styles(default_style: str, temporary_style: str) -> None: x = [1, 2, 3, 4] y = [10, 20, 15, 25] # Apply the default style try: sns.set_style(default_style) except ValueError: sns.set_style(\\"darkgrid\\") # Default to seaborn\'s default style if invalid # Plot with default style plt.figure() sns.barplot(x=x, y=y) plt.title(f\'Default Style: {default_style}\') plt.show() # Within a context manager, temporarily change the style try: with sns.axes_style(temporary_style): plt.figure() sns.barplot(x=x, y=y) plt.title(f\'Temporary Style: {temporary_style}\') plt.show() except ValueError: with sns.axes_style(\\"darkgrid\\"): # Default to seaborn\'s default style if invalid plt.figure() sns.barplot(x=x, y=y) plt.title(\'Temporary Style: darkgrid\') plt.show()"},{"question":"# Group Database Information Analysis You are required to write a Python function that takes a list of group names and returns a dictionary where each key is a group name and the value is a list of group member usernames. This function should make use of the `grp` module functions provided by Python. Function Signature ```python def get_group_members(group_names: List[str]) -> Dict[str, List[str]]: pass ``` # Input * `group_names` – A list of group names (List of strings). # Output * A dictionary where: * Each key is a group name. * Each value is the list of member usernames. # Constraints * If a group name in the list is not found in the group database, ignore that group. * The function should handle cases where the group has no members. * Performance should be considered for large lists of group names. # Example ```python group_names = [\'staff\', \'sudo\', \'docker\'] print(get_group_members(group_names)) ``` Expected Output (depending on the group database entries): ```python { \'staff\': [\'user1\', \'user2\', \'user3\'], \'sudo\': [\'admin1\', \'admin2\'], \'docker\': [] } ``` # Notes * Use the appropriate functions from the `grp` module to fetch group details. * Handle exceptions appropriately if the group is not found. * Ensure that the function processes each group in the input list and retrieves members if the group exists.","solution":"import grp def get_group_members(group_names): Returns a dictionary where each key is a group name and the value is a list of group member usernames. group_members = {} for group_name in group_names: try: group_info = grp.getgrnam(group_name) group_members[group_name] = group_info.gr_mem except KeyError: # If the group is not found, skip it continue return group_members"},{"question":"**Understanding and Manipulating Environment Variables using the `os` Module** Objective: Write a Python script that manipulates environment variables using the `os` module to demonstrate your understanding of the module’s functionality and its interaction with environment settings. Task: 1. Write a function `print_environment() -> None` that: * Prints all current environment variables to the console. Each variable should be displayed in the format: `KEY=VALUE`. 2. Write a function `add_environment_variable(key: str, value: str) -> None` that: * Adds a new environment variable or updates an existing one to the environment with the given key and value. * Prints a success message: `Environment variable \'key\' set to \'value\'`. 3. Write a function `remove_environment_variable(key: str) -> None` that: * Removes the environment variable with the specified key from the environment. * Prints a success message: `Environment variable \'key\' removed`. * If the variable does not exist, print: `Environment variable \'key\' does not exist`. 4. Write a main function `main()` that: * Demonstrates the following sequence: - Prints the initial set of environment variables. - Adds a new environment variable `MY_VAR` with the value `PYTHON310`. - Verifies and prints the modified environment variables. - Removes `MY_VAR`. - Verifies and prints the final set of environment variables. Input: * Various calls to the functions as shown in the sequence in the `main()` function. Output: * Print the appropriate success or error messages and the environment variables as specified. Constraints: * Ensure compatibility across different operating systems. * Use the `os` module functions to interact with environment variables. Example: ```python import os def print_environment() -> None: for key, value in os.environ.items(): print(f\\"{key}={value}\\") def add_environment_variable(key: str, value: str) -> None: os.environ[key] = value print(f\\"Environment variable \'{key}\' set to \'{value}\'.\\") def remove_environment_variable(key: str) -> None: if key in os.environ: del os.environ[key] print(f\\"Environment variable \'{key}\' removed.\\") else: print(f\\"Environment variable \'{key}\' does not exist.\\") def main(): print(\\"Initial environment variables:\\") print_environment() add_environment_variable(\\"MY_VAR\\", \\"PYTHON310\\") print(\\"nModified environment variables:\\") print_environment() remove_environment_variable(\\"MY_VAR\\") print(\\"nFinal environment variables:\\") print_environment() if __name__ == \\"__main__\\": main() ``` Please implement the functions as specified and ensure they work correctly across different platforms.","solution":"import os def print_environment() -> None: for key, value in os.environ.items(): print(f\\"{key}={value}\\") def add_environment_variable(key: str, value: str) -> None: os.environ[key] = value print(f\\"Environment variable \'{key}\' set to \'{value}\'.\\") def remove_environment_variable(key: str) -> None: if key in os.environ: del os.environ[key] print(f\\"Environment variable \'{key}\' removed.\\") else: print(f\\"Environment variable \'{key}\' does not exist.\\") def main(): print(\\"Initial environment variables:\\") print_environment() add_environment_variable(\\"MY_VAR\\", \\"PYTHON310\\") print(\\"nModified environment variables:\\") print_environment() remove_environment_variable(\\"MY_VAR\\") print(\\"nFinal environment variables:\\") print_environment() if __name__ == \\"__main__\\": main()"},{"question":"# Question In this assessment, you will demonstrate your understanding of Seaborn by creating and customizing several plots. Follow the steps below to complete the task. Task 1. **Load Data:** - Load the \\"tips\\" dataset provided by Seaborn. 2. **Single Plot Visualization:** - Create a scatter plot showing the relationship between `total_bill` and `tip`, with points colored by `time` (Lunch or Dinner). 3. **Distribution Visualization:** - Create a figure-level plot (`displot`) to visualize the distribution of `total_bill` separated by `time` and faceted by `day`, using histograms. Place the legend outside the plot, and set the height to 5 and the aspect to 0.8 for each subplot. Use appropriate custom labels for the axes. 4. **Joint Distribution:** - Create a joint plot (`jointplot`) to visualize the relationship between `total_bill` and `tip`, colored by `sex`. Use a `kde` kind plot for the joint distribution. Expected Input and Output - **Input:** No direct input is required from the user other than running the script. - **Output:** The script should display the required plots according to the specifications. Constraints and Performance - Use Seaborn for plotting, and ensure the plots are labeled and styled appropriately. - The operations should be efficiently handled by Seaborn and Matplotlib. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Task 2: Scatter Plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.title(\'Scatter Plot of Total Bill vs Tip\') plt.show() # Task 3: Distribution Plot g = sns.displot(data=tips, x=\\"total_bill\\", hue=\\"time\\", col=\\"day\\", multiple=\\"stack\\", kind=\\"hist\\", height=5, aspect=0.8) g.set_axis_labels(\\"Total Bill\\", \\"Count\\") plt.show() # Task 4: Joint Plot sns.jointplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", kind=\\"kde\\") plt.show() ``` This code will display the required plots step-by-step, demonstrating an understanding of Seaborn\'s plotting capabilities and customization options.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_plots(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Task 2: Scatter Plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.title(\'Scatter Plot of Total Bill vs Tip\') plt.show() # Task 3: Distribution Plot g = sns.displot( data=tips, x=\\"total_bill\\", hue=\\"time\\", col=\\"day\\", multiple=\\"stack\\", kind=\\"hist\\", height=5, aspect=0.8 ) g.set_axis_labels(\\"Total Bill\\", \\"Count\\") plt.show() # Task 4: Joint Plot sns.jointplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", kind=\\"kde\\") plt.show()"},{"question":"# MIME Email Message Construction and Manipulation Background You are the developer of an email client application. To give the users a richer experience, your application needs to support sending complex MIME email messages with different types of content such as text, images, and attachments. Task Write a Python function that constructs a MIME email message with the following structure: - The email should have a subject and from/to addresses. - The email body should include both plain text and HTML text. - The email should have an attached image (MIME type `image`). - The email should have an attached application file (MIME type `application`). Your function should take the text body, HTML body, image file path, and application file path as inputs and output the serialized MIME email message as a string. Input: 1. `subject` (str): The subject of the email. 2. `from_address` (str): The sender\'s email address. 3. `to_address` (str): The recipient\'s email address. 4. `text_body` (str): The plain text content of the email. 5. `html_body` (str): The HTML content of the email. 6. `image_path` (str): A file path to the image to be attached. 7. `application_path` (str): A file path to the application file to be attached. Output: - A serialized MIME email message (str). Constraints: - Assume the image and application files are accessible and valid. - The function should handle MIME types appropriately. - The function should add necessary headers such as `Subject`, `From`, `To`, and `MIME-Version`. Example: ```python result = create_mime_email( subject=\\"Test Email\\", from_address=\\"sender@example.com\\", to_address=\\"recipient@example.com\\", text_body=\\"This is the plain text part of the email.\\", html_body=\\"<html><body><h1>This is the HTML part of the email.</h1></body></html>\\", image_path=\\"path/to/image.png\\", application_path=\\"path/to/file.pdf\\" ) print(result) ``` Your function should be implemented as follows: ```python from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication import os def create_mime_email(subject, from_address, to_address, text_body, html_body, image_path, application_path): # Initialize the root message and fill in the headers msg = MIMEMultipart(\'related\') msg[\'Subject\'] = subject msg[\'From\'] = from_address msg[\'To\'] = to_address # Create the body with both plain text and HTML parts msg_alternative = MIMEMultipart(\'alternative\') msg.attach(msg_alternative) # Attach plain text part text_part = MIMEText(text_body, \'plain\') msg_alternative.attach(text_part) # Attach HTML part html_part = MIMEText(html_body, \'html\') msg_alternative.attach(html_part) # Attach the image file with open(image_path, \'rb\') as img_file: img_data = img_file.read() image_part = MIMEImage(img_data, name=os.path.basename(image_path)) msg.attach(image_part) # Attach the application file with open(application_path, \'rb\') as app_file: app_data = app_file.read() application_part = MIMEApplication(app_data, name=os.path.basename(application_path)) msg.attach(application_part) # Serialize message to string return msg.as_string() ```","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication import os def create_mime_email(subject, from_address, to_address, text_body, html_body, image_path, application_path): Constructs a MIME email message with plain text, HTML text, an image attachment, and an application file attachment. # Initialize the root message and fill in the headers msg = MIMEMultipart(\'related\') msg[\'Subject\'] = subject msg[\'From\'] = from_address msg[\'To\'] = to_address # Create the body with both plain text and HTML parts msg_alternative = MIMEMultipart(\'alternative\') msg.attach(msg_alternative) # Attach plain text part text_part = MIMEText(text_body, \'plain\') msg_alternative.attach(text_part) # Attach HTML part html_part = MIMEText(html_body, \'html\') msg_alternative.attach(html_part) # Attach the image file with open(image_path, \'rb\') as img_file: img_data = img_file.read() image_part = MIMEImage(img_data, name=os.path.basename(image_path)) msg.attach(image_part) # Attach the application file with open(application_path, \'rb\') as app_file: app_data = app_file.read() application_part = MIMEApplication(app_data, name=os.path.basename(application_path)) msg.attach(application_part) # Serialize message to string return msg.as_string()"},{"question":"**Problem Statement:** You are responsible for creating a simple Python interpreter emulator that can read code from a file and execute it. Your task includes handling both expression inputs using `eval()` and complete statements using `exec()`. # Input You will be provided with a file (in txt format) containing Python code. The file may include: - Single or multiple lines of code. - Expressions or statements. - Empty lines or lines with only whitespace, which should be ignored. The path to the file will be the only argument to your function. # Task 1. Read the file and ignore any empty lines. 2. Detect whether each line is an expression or a statement: - If it is an expression, evaluate it using `eval()`. - If it is a statement, execute it using `exec()`. 3. Collect the result of each evaluated expression and return these results in a list. 4. Ensure your function filters out invalid lines with appropriate error handling to prevent the entire program from crashing due to malformed inputs. # Example The input file `sample_code.txt` contains the following lines: ``` x = 10 y = 20 x + y y * 2 ``` # Expected Output ``` [30, 40] ``` # Function Signature ```python def interpret_code(file_path: str) -> list: pass ``` # Constraints - You must use `eval()` for expressions and `exec()` for statements. - Do not print any output from within the function; only return the results as specified. - You can assume the file will have a maximum size of 1 MB for handling purposes. **Helpful Information:** - Expressions return a value and typically can be included as part of larger expressions. Example: `3 + 4`, `my_func()`. - Statements perform an action, but they do not return a value that can be used. Example: `x = 10`, `for i in range(5): print(i)`. # Notes - You may use `try..except` blocks to handle any runtime errors gracefully. - Think about how to differentiate between expressions and statements; you may assume that expressions do not involve assignment or control flow operations like loops or conditionals.","solution":"def interpret_code(file_path: str) -> list: Interpret the code from the given file, evaluating expressions with eval() and statements with exec(). Collect and return results of evaluated expressions. results = [] with open(file_path, \'r\') as file: code_lines = file.readlines() # Define a local execution and evaluation context local_vars = {} for line in code_lines: line = line.strip() if not line: continue try: # Try to evaluate the line as an expression result = eval(line, {}, local_vars) results.append(result) except: # If eval fails, treat the line as a statement try: exec(line, {}, local_vars) except Exception as e: print(f\\"Error executing line \'{line}\': {e}\\") return results"},{"question":"**Question: Implement a dictionary lookup service with caching and partial application** You are tasked with creating a dictionary lookup service that fetches data from a dictionary. Given the latency associated with repeated lookups, you will implement caching to improve performance. Additionally, you will use partial functions to configure and customize the lookup behavior. # Requirements: 1. **Dictionary Cache Implementation**: - Implement a class `DictLookupCache` that encapsulates a dictionary and provides a method `lookup(key)` to fetch the value associated with `key`. - Use `functools.cache` (Python 3.9+) to cache the results of lookups to avoid repeated dictionary access. 2. **Partial Function for Custom Lookup**: - Implement a function `get_dict_lookup_with_default(dictionary, default)` that returns a partially applied function for dictionary lookups with a default value if the key is not found. - Utilize `functools.partial` to achieve this. # Input Format: - For class `DictLookupCache`: * A dictionary and a list of keys to look up: `my_dict = {\'a\':1, \'b\':2, \'c\':3}` and `keys = [\'a\', \'b\', \'z\', \'a\']` - For `get_dict_lookup_with_default` function: * A dictionary, a key to look up, and a default value: `my_dict = {\'a\':1, \'b\':2, \'c\':3}`, `key = \'z\'`, `default = \'not found\'` # Output Format: - For class `DictLookupCache`, calling `lookup(\'a\')` repeatedly should reflect caching improvements. - For `get_dict_lookup_with_default` function, invoking the partially applied function with a specific key should return the corresponding value or the default value if the key is not found. # Constraints: - Dictionary keys are strings and values are integers. - Function calls will be within reasonable limits for caching purposes (e.g., less than 1000 calls). # Example: ```python # Implementing the class DictLookupCache class DictLookupCache: def __init__(self, dictionary): self.dictionary = dictionary @functools.cache def lookup(self, key): return self.dictionary.get(key, \'Key not found\') # Testing DictLookupCache my_dict = {\'a\': 1, \'b\': 2, \'c\': 3} keys = [\'a\', \'b\', \'z\', \'a\'] cache = DictLookupCache(my_dict) for key in keys: print(cache.lookup(key)) # Should print 1, 2, \'Key not found\', 1 and demonstrate cache usage # Implementing the function using functools.partial def get_dict_lookup_with_default(dictionary, default): def lookup_func(dictionary, key, default): return dictionary.get(key, default) return functools.partial(lookup_func, dictionary, default=default) # Testing the partial function my_dict = {\'a\': 1, \'b\': 2, \'c\': 3} lookup_with_default = get_dict_lookup_with_default(my_dict, \'not found\') print(lookup_with_default(\'a\')) # Should print 1 print(lookup_with_default(\'z\')) # Should print \'not found\' ```","solution":"from functools import lru_cache, partial class DictLookupCache: def __init__(self, dictionary): self.dictionary = dictionary @lru_cache(maxsize=None) def lookup(self, key): return self.dictionary.get(key, \'Key not found\') def get_dict_lookup_with_default(dictionary, default): def lookup_func(dictionary, key, default): return dictionary.get(key, default) return partial(lookup_func, dictionary, default=default)"},{"question":"In Python, the introspection and manipulation of objects can be done at high-level using the features like `getattr`, `setattr`, and `hasattr`. Your task is to create a class `ReflectionUtils` that will provide methods for these functionalities along with masking some behaviors mimicking C API. Class: `ReflectionUtils` Implement the following static methods in `ReflectionUtils`: 1. **has_attribute(obj: object, attr_name: str) -> bool:** - This method should return `True` if the object `obj` has an attribute named `attr_name`, and `False` otherwise. - Equivalent to `hasattr(obj, attr_name)`. 2. **get_attribute(obj: object, attr_name: str) -> object:** - This method should return the value of the attribute named `attr_name` from the object `obj`. - If the attribute does not exist, it should raise an `AttributeError`. 3. **set_attribute(obj: object, attr_name: str, value: object) -> None:** - This method should set the value of the attribute named `attr_name` to `value` in the object `obj`. - Equivalent to `setattr(obj, attr_name, value)`. 4. **delete_attribute(obj: object, attr_name: str) -> None:** - This method should delete the attribute named `attr_name` in the object `obj`. - If the attribute does not exist, it should raise an `AttributeError`. 5. **compare_objects(obj1: object, obj2: object, operator: str) -> bool:** - This method should perform a comparison between `obj1` and `obj2` using the operator specified as a string in `operator`. - The operator string can be one of the following: `\\"==\\"`, `\\"!=\\"`, `\\"<\\"`, `\\"<=\\"`, `\\">\\"`, `\\">=\\"`. - It should return `True` or `False` based on the comparison result. - Raise `ValueError` if an unsupported operator is provided. Use the following structure for your implementation: ```python class ReflectionUtils: @staticmethod def has_attribute(obj: object, attr_name: str) -> bool: pass @staticmethod def get_attribute(obj: object, attr_name: str) -> object: pass @staticmethod def set_attribute(obj: object, attr_name: str, value: object) -> None: pass @staticmethod def delete_attribute(obj: object, attr_name: str) -> None: pass @staticmethod def compare_objects(obj1: object, obj2: object, operator: str) -> bool: pass ``` # Input/Output Requirements: 1. **has_attribute(obj: object, attr_name: str) -> bool** - **Input:** - obj: Any Python object - attr_name: A string representing the attribute name - **Output:** - A boolean indicating if `obj` has the attribute `attr_name`. 2. **get_attribute(obj: object, attr_name: str) -> object** - **Input:** - obj: Any Python object - attr_name: A string representing the attribute name - **Output:** - The attribute value. Raises `AttributeError` if attribute does not exist. 3. **set_attribute(obj: object, attr_name: str, value: object) -> None** - **Input:** - obj: Any Python object - attr_name: A string representing the attribute name - value: The new value for the attribute - **Output:** - None 4. **delete_attribute(obj: object, attr_name: str) -> None** - **Input:** - obj: Any Python object - attr_name: A string representing the attribute name - **Output:** - None. Raises `AttributeError` if attribute does not exist. 5. **compare_objects(obj1: object, obj2: object, operator: str) -> bool** - **Input:** - obj1: Any Python object - obj2: Any Python object - operator: A string representing the comparison operator - **Output:** - A boolean indicating the result of the comparison. Raises `ValueError` if unsupported operator provided. # Constraints: - Assume that the attributes being checked or manipulated are valid Python identifiers. - The comparison operators will strictly be one of the following: `\\"==\\"`, `\\"!=\\"`, `\\"<\\"`, `\\"<=\\"`, `\\">\\"`, `\\">=\\"`. You may assume that the input types will be correctly handled as specified. **Examples:** ```python # Example Usage class Sample: def __init__(self): self.x = 10 obj = Sample() assert ReflectionUtils.has_attribute(obj, \\"x\\") == True assert ReflectionUtils.get_attribute(obj, \\"x\\") == 10 ReflectionUtils.set_attribute(obj, \\"y\\", 20) assert ReflectionUtils.get_attribute(obj, \\"y\\") == 20 ReflectionUtils.delete_attribute(obj, \\"y\\") assert ReflectionUtils.has_attribute(obj, \\"y\\") == False assert ReflectionUtils.compare_objects(10, 20, \\"<\\") == True assert ReflectionUtils.compare_objects(10, 20, \\"==\\") == False # Handling exceptions try: ReflectionUtils.delete_attribute(obj, \\"y\\") except AttributeError as e: print(e) # Should print an appropriate error message try: ReflectionUtils.compare_objects(10, 20, \\"//\\") except ValueError as e: print(e) # Should print an appropriate error message ``` Good luck!","solution":"class ReflectionUtils: @staticmethod def has_attribute(obj: object, attr_name: str) -> bool: Checks if the object has the specified attribute. return hasattr(obj, attr_name) @staticmethod def get_attribute(obj: object, attr_name: str) -> object: Gets the value of the specified attribute from the object. if hasattr(obj, attr_name): return getattr(obj, attr_name) else: raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") @staticmethod def set_attribute(obj: object, attr_name: str, value: object) -> None: Sets the value of the specified attribute on the object. setattr(obj, attr_name, value) @staticmethod def delete_attribute(obj: object, attr_name: str) -> None: Deletes the specified attribute from the object. if hasattr(obj, attr_name): delattr(obj, attr_name) else: raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") @staticmethod def compare_objects(obj1: object, obj2: object, operator: str) -> bool: Compares two objects using the specified operator. if operator == \\"==\\": return obj1 == obj2 elif operator == \\"!=\\": return obj1 != obj2 elif operator == \\"<\\": return obj1 < obj2 elif operator == \\"<=\\": return obj1 <= obj2 elif operator == \\">\\": return obj1 > obj2 elif operator == \\">=\\": return obj1 >= obj2 else: raise ValueError(f\\"Unsupported comparison operator: {operator}\\")"},{"question":"# Coding Challenge: Advanced Sequence Manipulation with Custom Context Manager **Objective**: Demonstrate your mastery of Python3.10 sequence operations, context managers, and custom classes. # Problem Statement: You are tasked with creating a custom sequence class `AdvancedList` that mimics some behavior of a built-in list and can apply transformations to its elements within a context manager. # Detailed Requirements: 1. **Class `AdvancedList` Implementation**: - Initialize with an iterable. - Mimic list operations: support indexing, slicing, `len()`, and item assignment. - Include a method `filter(self, condition_callable)` that filters elements according to a provided condition function. 2. **Context Manager `Transformation`**: - Apply a transformation function to each element of the `AdvancedList` within the context block. - Ensure the transformation is only applied temporarily within the `with` block and reverts after exiting. # Function Signatures: ```python class AdvancedList: def __init__(self, iterable): Initializes the AdvancedList with the iterable\'s elements. pass def __getitem__(self, index): Supports indexing and slicing. pass def __setitem__(self, index, value): Supports item assignment. pass def __len__(self): Returns the number of elements. pass def filter(self, condition_callable): Filters elements based on the condition_callable. pass class Transformation: def __init__(self, advanced_list, transform_callable): Initializes the context manager with the list and transformation function. pass def __enter__(self): Applies the transformation to the list elements. pass def __exit__(self, exc_type, exc_val, exc_tb): Reverts the transformation upon exiting the context. pass # Example usage: adv_list = AdvancedList(range(10)) adv_list.filter(lambda x: x % 2 == 0) # Should filter even numbers print(list(adv_list)) # Output: [0, 2, 4, 6, 8] with Transformation(adv_list, lambda x: x * 10): print(list(adv_list)) # Output: [0, 20, 40, 60, 80] print(list(adv_list)) # Reverted back: [0, 2, 4, 6, 8] ``` # Constraints: 1. Do not use any external libraries. 2. The `AdvancedList` should support negative indices and slicing. 3. Ensure the context manager does not permanently alter the list elements, changes must only be visible within the context. # Notes: - Ensure to handle edge cases like out-of-bound indices, invalid transformations. - Performance considerations: Ensure the operations are efficient to handle reasonably large sizes of lists. Develop this solution, demonstrate it with examples showing all functionalities, and test edge cases to validate your implementation.","solution":"class AdvancedList: def __init__(self, iterable): Initializes the AdvancedList with the iterable\'s elements. self._elements = list(iterable) def __getitem__(self, index): Supports indexing and slicing. return self._elements[index] def __setitem__(self, index, value): Supports item assignment. self._elements[index] = value def __len__(self): Returns the number of elements. return len(self._elements) def filter(self, condition_callable): Filters elements based on the condition_callable. self._elements = [elem for elem in self._elements if condition_callable(elem)] class Transformation: def __init__(self, advanced_list, transform_callable): Initializes the context manager with the list and transformation function. self.advanced_list = advanced_list self.transform_callable = transform_callable self.original_elements = None def __enter__(self): Applies the transformation to the list elements. self.original_elements = self.advanced_list._elements[:] self.advanced_list._elements = [self.transform_callable(elem) for elem in self.advanced_list._elements] return self.advanced_list def __exit__(self, exc_type, exc_val, exc_tb): Reverts the transformation upon exiting the context. self.advanced_list._elements = self.original_elements # Example usage: adv_list = AdvancedList(range(10)) adv_list.filter(lambda x: x % 2 == 0) # Should filter even numbers print(list(adv_list)) # Output: [0, 2, 4, 6, 8] with Transformation(adv_list, lambda x: x * 10): print(list(adv_list)) # Output: [0, 20, 40, 60, 80] print(list(adv_list)) # Reverted back: [0, 2, 4, 6, 8]"},{"question":"**Problem Statement:** You are tasked with writing a Python function `fetch_user_info(criteria, value)` to fetch user information from the Unix password database using the `pwd` module. The function takes in two parameters: 1. `criteria`: A string which can be either `\\"uid\\"` or `\\"name\\"`, denoting the search criteria. 2. `value`: An integer or a string, based on the criteria. If the criteria is `\\"uid\\"`, value will be an integer (numerical user ID). If the criteria is `\\"name\\"`, value will be a string (username). The function should return the user information in the form of a dictionary with the following keys: - `\\"pw_name\\"` - `\\"pw_passwd\\"` - `\\"pw_uid\\"` - `\\"pw_gid\\"` - `\\"pw_gecos\\"` - `\\"pw_dir\\"` - `\\"pw_shell\\"` If a user matching the criteria is not found, the function should raise a `ValueError` with the message `\\"User not found\\"`. Additionally, write a helper function `list_all_users()` that returns a list of all users in the password database. Each user should be represented by the same dictionary structure as described above. # Input Constraints: - `criteria` is guaranteed to be either `\\"uid\\"` or `\\"name\\"`. - `value` will be an appropriate type as per the specified criteria. - The system has a Unix-based operating system where the `pwd` module is functional. # Function Signatures: ```python def fetch_user_info(criteria: str, value) -> dict: pass def list_all_users() -> list: pass ``` # Example Usage: ```python # Example user data might be: # pw_name: \\"exampleuser\\", pw_passwd: \\"*\\", pw_uid: 1001, pw_gid: 1001, pw_gecos: \\"Example User\\", pw_dir: \\"/home/exampleuser\\", pw_shell: \\"/bin/bash\\" # Fetch user info by UID try: user_info = fetch_user_info(\\"uid\\", 1001) print(user_info) except ValueError as e: print(e) # Fetch user info by name try: user_info = fetch_user_info(\\"name\\", \\"exampleuser\\") print(user_info) except ValueError as e: print(e) # List all users all_users = list_all_users() print(all_users) ``` # Example Output: ```python # Expected output for fetching user info by UID { \\"pw_name\\": \\"exampleuser\\", \\"pw_passwd\\": \\"*\\", \\"pw_uid\\": 1001, \\"pw_gid\\": 1001, \\"pw_gecos\\": \\"Example User\\", \\"pw_dir\\": \\"/home/exampleuser\\", \\"pw_shell\\": \\"/bin/bash\\" } # Expected output for fetching user info by name { \\"pw_name\\": \\"exampleuser\\", \\"pw_passwd\\": \\"*\\", \\"pw_uid\\": 1001, \\"pw_gid\\": 1001, \\"pw_gecos\\": \\"Example User\\", \\"pw_dir\\": \\"/home/exampleuser\\", \\"pw_shell\\": \\"/bin/bash\\" } # Expected output for listing all users may vary [ { \\"pw_name\\": \\"exampleuser\\", \\"pw_passwd\\": \\"*\\", \\"pw_uid\\": 1001, \\"pw_gid\\": 1001, \\"pw_gecos\\": \\"Example User\\", \\"pw_dir\\": \\"/home/exampleuser\\", \\"pw_shell\\": \\"/bin/bash\\" }, ... ] ``` **Note:** The actual values and the number of users depend on the system the code is run on. # Additional Notes: - Make sure to import the `pwd` module at the beginning of your script. - Handle any exceptions appropriately to avoid crashes.","solution":"import pwd def fetch_user_info(criteria, value): Fetches user information based on the search criteria. Args: criteria: str - Either \\"uid\\" or \\"name\\" to indicate the search basis. value: int or str - The user id (if criteria is \\"uid\\") or the username (if criteria is \\"name\\"). Returns: dict - A dictionary containing the user information. Raises: ValueError: If the user is not found. try: if criteria == \\"uid\\": user_info = pwd.getpwuid(value) elif criteria == \\"name\\": user_info = pwd.getpwnam(value) else: raise ValueError(\\"Invalid criteria\\") return { \\"pw_name\\": user_info.pw_name, \\"pw_passwd\\": user_info.pw_passwd, \\"pw_uid\\": user_info.pw_uid, \\"pw_gid\\": user_info.pw_gid, \\"pw_gecos\\": user_info.pw_gecos, \\"pw_dir\\": user_info.pw_dir, \\"pw_shell\\": user_info.pw_shell, } except KeyError: raise ValueError(\\"User not found\\") def list_all_users(): Lists all users in the Unix password database. Returns: list - A list of dictionaries containing user information. users = pwd.getpwall() user_list = [] for user_info in users: user_list.append({ \\"pw_name\\": user_info.pw_name, \\"pw_passwd\\": user_info.pw_passwd, \\"pw_uid\\": user_info.pw_uid, \\"pw_gid\\": user_info.pw_gid, \\"pw_gecos\\": user_info.pw_gecos, \\"pw_dir\\": user_info.pw_dir, \\"pw_shell\\": user_info.pw_shell, }) return user_list"},{"question":"# Question: Detecting and Fixing a Data Preprocessing Issue in a Regression Task with scikit-learn You are given a dataset that simulates a regression problem. The dataset contains some missing values and needs to be preprocessed before fitting a machine learning model. Your task is to process this dataset and fit a `GradientBoostingRegressor` model to it. However, there is a specific preprocessing issue that causes a warning during the model fitting process. Your objective is to: 1. **Detect the preprocessing issue**. 2. **Resolve the issue**. 3. **Fit the model** and evaluate its performance. The `GradientBoostingRegressor` should be trained with specific parameters and you need to compare the performance before and after fixing the issue. Steps: 1. **Data Generation:** Generate a synthetic dataset using `make_regression` with `n_samples=1000` and `n_features=20`. Introduce some missing values randomly. 2. **Data Preprocessing:** - Fill or handle the missing values appropriately. - Standardize the dataset using `StandardScaler`. 3. **Model Training:** - First, fit a `GradientBoostingRegressor` without handling the preprocessing issue and note any warnings. - Correct the identified issue in the previous step and refit the model. 4. **Model Evaluation:** - Evaluate the model\'s performance using R-squared score on a test set, both before and after fixing the preprocessing issue. Requirements: - **Input:** No input is required as the data is generated within the script. - **Output:** Print R-squared scores both before and after fixing the issue. - Follow good coding practices by isolating the problem in the first attempt before presenting the fix. **Note:** Ensure the code can be run in a single execution block without external data dependencies. Example Workflow: ```python from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split import numpy as np import pandas as pd # Step 1: Data Generation X, y = make_regression(n_samples=1000, n_features=20) X[np.random.choice([True, False], X.shape)] = np.nan # Introducing random missing values # Step 2: Data Preprocessing df = pd.DataFrame(X) df.fillna(df.mean(), inplace=True) # Simple strategy to fill missing values, adjust as per issue detection scaler = StandardScaler() X_scaled = scaler.fit_transform(df.to_numpy()) # Step 3: Model Training - Without issue handling X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42) gbdt = GradientBoostingRegressor(random_state=0) gbdt.fit(X_train, y_train) initial_score = gbdt.score(X_test, y_test) print(f\'Initial R-squared score: {initial_score}\') # Fix the preprocessing issue here if identified # Placeholder for the fix - may need detecting and filling missing values, scaling etc. # Step 4: Model Training - After issue handling # Repeat after fixing the issue gbdt = GradientBoostingRegressor(random_state=0) gbdt.fit(X_train, y_train) final_score = gbdt.score(X_test, y_test) print(f\'Final R-squared score: {final_score}\') ``` Your task is to implement the above workflow correctly, identify any preprocessing issues, and resolve them to improve model performance.","solution":"import numpy as np import pandas as pd from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split # Step 1: Data Generation def generate_data(): X, y = make_regression(n_samples=1000, n_features=20, random_state=42) # Introduce some random missing values mask = np.random.rand(*X.shape) < 0.1 X[mask] = np.nan return X, y # Step 2: Data Preprocessing def preprocess_data(X): df = pd.DataFrame(X) df.fillna(df.mean(), inplace=True) # Handle missing values scaler = StandardScaler() X_scaled = scaler.fit_transform(df.to_numpy()) # Standardize the dataset return X_scaled # Evaluate the model def evaluate_model(X_train, y_train, X_test, y_test): # Step 3: Model Training gbdt = GradientBoostingRegressor(random_state=0) gbdt.fit(X_train, y_train) score = gbdt.score(X_test, y_test) return score # Putting it all together X, y = generate_data() X_preprocessed = preprocess_data(X) X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.33, random_state=42) initial_score = evaluate_model(X_train, y_train, X_test, y_test) print(f\'R-squared score after preprocessing: {initial_score}\')"},{"question":"**Question: Advanced Visualization with Seaborn\'s FacetGrid** --- # Problem Statement You are required to create an advanced multi-plot visualization using Seaborn\'s `FacetGrid`. The dataset to be used is the `tips` dataset, which can be loaded using Seaborn. The visualization should answer the following questions: 1. How do the distribution of total bill amounts vary across different days and times (Lunch/Dinner)? 2. How does the tipping behavior differ across genders for each day of the week? 3. Can you annotate each plot with the number of observations it contains? 4. Add a reference line showing the median tip amount across all data in each plot. You are expected to write a function that constructs this visualization with the specified features. # Input and Output - The function has no input parameters. - The function should output (display) the generated `FacetGrid` plot. # Constraints - Use the `tips` dataset provided by Seaborn. - Grid plots should be faceted by `day` (columns) and `time` (rows). - The scatter plots should represent `total_bill` on the x-axis and `tip` on the y-axis. - Each plot should have a legend indicating male and female data points. - Annotate each plot with the number of observations (points). - Add a horizontal reference line at the median tip amount. # Function Signature ```python def create_facetgrid_plot() -> None: # your code here ``` # Example Calling `create_facetgrid_plot()` should generate and display a grid plot as specified. # Hints - Use `sns.load_dataset` to load the `tips` dataset. - Use `FacetGrid` with `col`, `row`, and `hue` parameters to achieve the desired grid layout and coloring. - Employ `map_dataframe` for the creation of scatter plots and annotations. - Use the `refline` method to add reference lines to each plot. - Customize the grid for better visual clarity with appropriate `set_axis_labels`, `set_titles`, and `tight_layout`. # Solution Template ```python import seaborn as sns import matplotlib.pyplot as plt def create_facetgrid_plot() -> None: # Load the dataset tips = sns.load_dataset(\\"tips\\") # Initialize the FacetGrid g = sns.FacetGrid(tips, col=\\"day\\", row=\\"time\\", hue=\\"sex\\", margin_titles=True) # Map the scatterplot g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Add legend g.add_legend() # Annotate each plot with the number of observations def annotate(data, **kws): n = len(data) ax = plt.gca() ax.text(.1, .6, f\\"N = {n}\\", transform=ax.transAxes) g.map_dataframe(annotate) # Add a reference line at the median tip amount median_tip = tips[\\"tip\\"].median() g.refline(y=median_tip) # Set axis labels and titles g.set_axis_labels(\\"Total bill ()\\", \\"Tip ()\\") g.set_titles(col_template=\\"{col_name} day\\", row_template=\\"{row_name} time\\") g.tight_layout() # Display the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_facetgrid_plot() -> None: # Load the dataset tips = sns.load_dataset(\\"tips\\") # Initialize the FacetGrid g = sns.FacetGrid(tips, col=\\"day\\", row=\\"time\\", hue=\\"sex\\", margin_titles=True) # Map the scatterplot g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Add legend g.add_legend() # Annotate each plot with the number of observations def annotate(data, **kws): n = len(data) ax = plt.gca() ax.text(.1, .6, f\\"N = {n}\\", transform=ax.transAxes) g.map_dataframe(annotate) # Add a reference line at the median tip amount median_tip = tips[\\"tip\\"].median() g.refline(y=median_tip) # Set axis labels and titles g.set_axis_labels(\\"Total bill ()\\", \\"Tip ()\\") g.set_titles(col_template=\\"{col_name} day\\", row_template=\\"{row_name} time\\") g.tight_layout() # Display the plot plt.show()"},{"question":"# Conditional Execution with `torch.cond` In this task, you will leverage the `torch.cond` operator to create a PyTorch model that conditionally applies different transformations to an input tensor based on its properties. Problem Statement Create a PyTorch module named `ConditionalTensorTransform`. This module will perform one of the following operations on its input tensor `x` based on the sum of its elements: 1. If the sum of the elements in `x` is greater than `5.0`, the module should return the tensor with each element squared. 2. If the sum of the elements in `x` is less than or equal to `5.0`, the module should return the tensor with each element cubed. Function Signature The module should have the following structure: ```python import torch class ConditionalTensorTransform(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: # Define true and false branch functions def true_fn(x: torch.Tensor): # When condition is True: elements are squared return x.pow(2) def false_fn(x: torch.Tensor): # When condition is False: elements are cubed return x.pow(3) # Use torch.cond to apply conditional logic return torch.cond(x.sum() > 5.0, true_fn, false_fn, (x,)) ``` Input - A single input tensor `x` of any shape containing floating-point values. Output - A tensor where transformation has been applied based on the sum of elements in `x`. Detailed Requirements 1. Implement the `ConditionalTensorTransform` module including: - An `__init__` method. - A `forward` method that employs `torch.cond` for conditional logic. - Two helper functions within the `forward` method: `true_fn` for squaring the elements and `false_fn` for cubing the elements. 2. Ensure that the module is tested by running it with different input tensors and verifying the transformations. Example: ```python import torch # Instantiate the module model = ConditionalTensorTransform() # Provide test inputs input_tensor1 = torch.tensor([1.0, 1.0, 1.0]) input_tensor2 = torch.tensor([2.0, 3.0, 1.5]) # Compute outputs output_tensor1 = model(input_tensor1) output_tensor2 = model(input_tensor2) # Verify results print(output_tensor1) # Expected: tensor elements cubed -> tensor([1.0, 1.0, 1.0]) print(output_tensor2) # Expected: tensor elements squared -> tensor([4.0, 9.0, 2.25]) ``` The provided examples and expected results demonstrate how to use the module, perform the conditional checks, and confirm the functionality.","solution":"import torch class ConditionalTensorTransform(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: # Define true and false branch functions def true_fn(x: torch.Tensor): # When condition is True: elements are squared return x.pow(2) def false_fn(x: torch.Tensor): # When condition is False: elements are cubed return x.pow(3) # Use conditional logic based on the sum of x if x.sum() > 5.0: return true_fn(x) else: return false_fn(x)"},{"question":"**Coding Assessment Question: Python\'s `sysconfig` Module** **Objective:** Assess your understanding of Python\'s `sysconfig` module by requiring you to implement a function that leverages this module to gather and return specific Python installation configuration information. **Problem Statement:** You are tasked with writing a function `get_python_config_info()` that uses the `sysconfig` module to gather and return a summary of the Python installation configuration. The function should return a dictionary with the following keys and their corresponding values: 1. `\\"platform\\"`: The current platform name using `sysconfig.get_platform()`. 2. `\\"python_version\\"`: The current Python version in the \\"MAJOR.MINOR\\" format using `sysconfig.get_python_version()`. 3. `\\"default_scheme\\"`: The default installation scheme using `sysconfig.get_default_scheme()`. 4. `\\"paths\\"`: A dictionary of all installation paths corresponding to the default installation scheme using `sysconfig.get_paths()`. 5. `\\"all_variables\\"`: A dictionary of all configuration variables using `sysconfig.get_config_vars()`. **Function Signature:** ```python def get_python_config_info() -> dict: pass ``` **Expected Input/Output:** - The function does not take any input arguments. - The function returns a dictionary with the specified configuration information. **Constraints:** - The function should only use the `sysconfig` module to gather the required information. - Ensure that the function handles cases where expected data might not be available (e.g., returning `None` if a variable is not found). **Example:** ```python import sysconfig def get_python_config_info() -> dict: config_info = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"paths\\": sysconfig.get_paths(), \\"all_variables\\": sysconfig.get_config_vars(), } return config_info # Example usage info = get_python_config_info() print(info) # Expected Output: # { # \\"platform\\": \\"linux-x86_64\\", # \\"python_version\\": \\"3.10\\", # \\"default_scheme\\": \\"posix_prefix\\", # \\"paths\\": { # ... (installation paths) # }, # \\"all_variables\\": { # ... (all configuration variables) # } # } ``` **Notes:** - The actual output will depend on the Python installation and platform you are running. - Ensure that the returned dictionary keys match exactly as specified.","solution":"import sysconfig def get_python_config_info(): Returns a dictionary summarizing the Python installation configuration. return { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"paths\\": sysconfig.get_paths(), \\"all_variables\\": sysconfig.get_config_vars() }"},{"question":"# Advanced DataFrame Memory and Boolean Handling with Pandas You are given a pandas DataFrame containing a variety of data types. Your task is to write a function that: 1. Analyzes the memory usage of the DataFrame with a detailed report. 2. Properly handles missing values in integer and boolean columns. 3. Safely applies a User Defined Function (UDF) to alter the DataFrame without causing mutation issues. 4. Evaluates a conditional check on the DataFrame using best practices for boolean handling. Function Signature ```python import pandas as pd import numpy as np def analyze_and_modify_dataframe(df: pd.DataFrame) -> str: This function performs a detailed analysis and modification on the given DataFrame. Parameters: df (pd.DataFrame): The input DataFrame to analyze and modify. Returns: str: A report detailing the memory usage and modifications performed on the DataFrame. # Your implementation here ``` Detailed Requirements 1. **Memory Usage Analysis**: - Print the memory usage report of the DataFrame including a “deep” memory usage analysis. - Return the total memory used by the DataFrame in bytes. 2. **Handling Missing Values**: - Identify integer columns in the DataFrame and convert them to a nullable integer type (e.g., `Int64Dtype`). - Identify boolean columns and ensure they can handle None values by converting them to an appropriate type. 3. **Safely Applying UDF**: - Implement a UDF that doubles the value of numeric columns and apply this UDF to the DataFrame. Ensure that the UDF does not mutate the original DataFrame during iteration. 4. **Boolean Condition Handling**: - Implement a check to see if any value in the DataFrame is greater than 100. Print an appropriate message based on the result. Example Usage ```python dtypes = [\\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"complex128\\", \\"object\\", \\"bool\\"] n = 5000 data = {t: np.random.randint(0, 150, size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df[\\"categorical\\"] = df[\\"object\\"].astype(\\"category\\") report = analyze_and_modify_dataframe(df) print(report) ``` Constraints and Notes: - You should handle potential performance implications when performing deep memory usage analysis. - Ensure that no mutation of the DataFrame occurs during UDF application. - Test your function with varied DataFrames to handle edge cases effectively.","solution":"import pandas as pd import numpy as np def analyze_and_modify_dataframe(df: pd.DataFrame) -> str: This function performs a detailed analysis and modification on the given DataFrame. Parameters: df (pd.DataFrame): The input DataFrame to analyze and modify. Returns: str: A report detailing the memory usage and modifications performed on the DataFrame. report = \\"\\" # Part 1: Memory Usage Analysis memory_usage = df.memory_usage(deep=True).sum() memory_report = df.memory_usage(deep=True) report += f\\"Memory Usage Report (deep=True):n{memory_report}n\\" report += f\\"Total memory usage: {memory_usage} bytesnn\\" # Part 2: Handling Missing Values # Convert integer columns to nullable integer type for col in df.select_dtypes(include=[\'int64\', \'int32\', \'int16\', \'int8\']).columns: df[col] = df[col].astype(\'Int64\') # Convert boolean columns to handle None values for col in df.select_dtypes(include=[\'bool\']).columns: df[col] = df[col].astype(\'boolean\') report += \\"Integer and Boolean columns converted to nullable types to handle None values.nn\\" # Part 3: Safely Applying UDF def double_value(val): if isinstance(val, (int, float)): return val * 2 return val df_transformed = df.applymap(double_value) report += \\"Applied UDF to double values of numeric columns.nn\\" # Part 4: Boolean Condition Handling if (df_transformed.select_dtypes(include=[np.number]) > 100).any().any(): report += \\"There are values greater than 100 in the DataFrame.n\\" else: report += \\"No value greater than 100 found in the DataFrame.n\\" return report"},{"question":"You are given two datasets: `diamonds` and `mpg`. Use seaborn\'s `seaborn.objects` interface to create customized visualizations demonstrating your understanding of scaling. Your task involves creating two distinct plots with specific customization requirements related to scales. # Part 1: Diamonds Dataset Create a scatter plot to visualize the relationship between `carat` and `price` using the `diamonds` dataset with the following customizations: 1. Use `so.Dots()` to add points to the plot. 2. Use a log scale for the y-axis (price). 3. Color the points according to the `clarity` attribute using the `crest` palette from seaborn. # Part 2: MPG Dataset Create a scatter plot to visualize the relationship between `weight` and `acceleration` using the `mpg` dataset with the following customizations: 1. Use `so.Dot()` to add points to the plot, with `cylinders` determining the color of the dots. 2. Include a nonlinear (sqrt) transform for the x-axis (`weight`). 3. Use a qualitative color palette to distinguish between different `cylinders` values. # Expected Input and Output **Input:** None directly. The datasets should be loaded internally using seaborn\'s `load_dataset` function. **Output:** Display two seaborn plots with the specified customizations. Ensure the plots are clear and the requirements are met. # Constraints 1. Use seaborn\'s `seaborn.objects` interface. 2. Follow the specified customizations precisely. 3. Ensure that the plots have clear axis labels and legends where applicable. # Performance Requirements There are no specific performance requirements; however, the plots should render within a reasonable time frame to be useful for exploratory data analysis. # Example ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets diamonds = load_dataset(\\"diamonds\\") mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Part 1: Diamonds Dataset p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") p1.add(so.Dots()).scale(y=\\"log\\") p1.add(so.Dots(), color=\\"clarity\\").scale(color=\\"crest\\") p1.show() # Part 2: MPG Dataset p2 = so.Plot(mpg, x=\\"weight\\", y=\\"acceleration\\", color=\\"cylinders\\") p2.add(so.Dot()) p2.scale(x=so.Continuous(trans=\\"sqrt\\")) p2.scale(color=\\"deep\\") p2.show() ``` This example lays out the structure and ideas for the solution but leaves room for additional customization and improvements.","solution":"import seaborn.objects as so import seaborn as sns def plot_diamonds(): # Load diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create scatter plot for diamonds dataset p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots(), color=\\"clarity\\").scale(y=\\"log\\").scale(color=\\"crest\\") return p1 def plot_mpg(): # Load mpg dataset mpg = sns.load_dataset(\\"mpg\\") # Create scatter plot for mpg dataset p2 = so.Plot(mpg, x=\\"weight\\", y=\\"acceleration\\", color=\\"cylinders\\").add(so.Dot()).scale(x=so.Continuous(trans=\\"sqrt\\")).scale(color=sns.color_palette(\\"Set2\\", as_cmap=True)) return p2"},{"question":"**Objective:** You are required to use seaborn\'s capabilities to create a complex visual representation of categorical data. This assessment will test your ability to use various types of categorical plots, adjust their parameters, and combine different plots to provide comprehensive insights into the dataset. **Problem Description:** You are given a dataset containing information about different vehicles, including their fuel type, transmission type, and fuel efficiency (miles per gallon). Your task is to visualize this data using seaborn to obtain and combine multiple views that effectively communicate the data\'s distribution and central tendencies. **Dataset:** The dataset `vehicles.csv` has the following columns: - `fuel_type`: Type of fuel used (e.g., Petrol, Diesel, Electric) - `transmission`: Type of transmission (e.g., Automatic, Manual) - `mpg`: Miles per gallon (fuel efficiency) **Tasks:** 1. Load the dataset into a pandas DataFrame. 2. Create a categorical scatterplot (swarmplot) to show the distribution of `mpg` for each `fuel_type`. 3. Create a violin plot for the same data, showing `mpg` distributions across `fuel_type` with inner boxplots. Use a hue semantic for `transmission`. 4. Combine the above violin plot with a swarmplot overlay. 5. Create a bar plot to show the average `mpg` for each combination of `fuel_type` and `transmission`, along with error bars showing the 95% confidence interval. 6. Use `FacetGrid` to create faceted plots of `mpg` distributions (using box plots) for each transmission type (`transmission`) faceted by `fuel_type` categories. **Constraints:** - Ensure that the code is well-documented with comments explaining each major step. - Use appropriate titles, labels, and legends to make the plots self-explanatory. **Submission:** Submit your code as a Python script or Jupyter notebook, ensuring that the script runs without errors and produces the required plots. Include the visualizations in your notebook submission. **Expected Output:** - A script that loads the `vehicles.csv` dataset and generates the plots described in the tasks. - Clear and informative plots with titles, axis labels, and legends where necessary. - Code is clean and well-commented. **Example:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load dataset vehicles = pd.read_csv(\'vehicles.csv\') # Task 2: Categorical scatterplot (Swarmplot) plt.figure(figsize=(10, 6)) sns.swarmplot(data=vehicles, x=\'fuel_type\', y=\'mpg\') plt.title(\'Swarmplot of MPG across Fuel Types\') plt.show() # Task 3 & 4: Violin plot with inner boxplots + swarmplot overlay plt.figure(figsize=(10, 6)) g = sns.violinplot(data=vehicles, x=\'fuel_type\', y=\'mpg\', hue=\'transmission\', inner=\'box\', palette=\'muted\') sns.swarmplot(data=vehicles, x=\'fuel_type\', y=\'mpg\', color=\'k\', size=3) plt.title(\'Violin Plot with Swarm Overlay: MPG across Fuel Types\') plt.show() # Task 5: Bar plot with confidence intervals plt.figure(figsize=(10, 6)) sns.barplot(data=vehicles, x=\'fuel_type\', y=\'mpg\', hue=\'transmission\', ci=\'sd\') plt.title(\'Average MPG with Confidence Intervals across Fuel Types and Transmissions\') plt.show() # Task 6: Faceted box plots using FacetGrid g = sns.catplot(data=vehicles, x=\'mpg\', y=\'fuel_type\', hue=\'transmission\', kind=\'box\', col=\'transmission\', sharey=False) g.fig.subplots_adjust(top=0.9) # Adjust subplot to fit title g.fig.suptitle(\'Faceted Box Plots of MPG by Fuel Type across Transmissions\') plt.show() ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_dataset(filename): Load the dataset from a CSV file into a pandas DataFrame. return pd.read_csv(filename) def plot_swarmplot(vehicles): Create a categorical scatterplot (swarmplot) to show the distribution of mpg for each fuel_type. plt.figure(figsize=(10, 6)) sns.swarmplot(data=vehicles, x=\'fuel_type\', y=\'mpg\') plt.title(\'Swarmplot of MPG across Fuel Types\') plt.xlabel(\'Fuel Type\') plt.ylabel(\'Miles per Gallon (MPG)\') plt.show() def plot_violin_swarm(vehicles): Create a violin plot with inner boxplots and overlay it with a swarmplot, showing mpg distributions across fuel_type with hue as transmission. plt.figure(figsize=(10, 6)) g = sns.violinplot(data=vehicles, x=\'fuel_type\', y=\'mpg\', hue=\'transmission\', inner=\'box\', palette=\'muted\') sns.swarmplot(data=vehicles, x=\'fuel_type\', y=\'mpg\', color=\'k\', size=3) plt.title(\'Violin Plot with Swarm Overlay: MPG across Fuel Types\') plt.xlabel(\'Fuel Type\') plt.ylabel(\'Miles per Gallon (MPG)\') plt.show() def plot_barplot(vehicles): Create a bar plot to show the average mpg for each combination of fuel_type and transmission along with error bars showing the 95% confidence interval. plt.figure(figsize=(10, 6)) sns.barplot(data=vehicles, x=\'fuel_type\', y=\'mpg\', hue=\'transmission\', ci=\'sd\') plt.title(\'Average MPG with Confidence Intervals across Fuel Types and Transmissions\') plt.xlabel(\'Fuel Type\') plt.ylabel(\'Miles per Gallon (MPG)\') plt.show() def plot_facetgrid(vehicles): Use FacetGrid to create faceted plots of mpg distributions (using box plots) for each transmission type faceted by fuel_type categories. g = sns.catplot(data=vehicles, x=\'mpg\', y=\'fuel_type\', hue=\'transmission\', kind=\'box\', col=\'transmission\', sharey=False) g.fig.subplots_adjust(top=0.9) # Adjust subplot to fit title g.fig.suptitle(\'Faceted Box Plots of MPG by Fuel Type across Transmissions\') plt.show()"},{"question":"# Seaborn Advanced Visualization Task You are provided with a dataset named `tips` which records information about restaurant tips, including total bill amount, tip amount, the day of the week, the time of day, and additional attributes. You are required to analyze and visualize this dataset focusing on exploring the trends in tipping behavior over the days of the week and times of the day. Dataset Format: - **total_bill**: Total bill amount (float) - **tip**: Tip amount (float) - **sex**: Gender of the person paying the bill (string) - **smoker**: Whether the person is a smoker or non-smoker (string) - **day**: Day of the week (string) - **time**: Time of day (Lunch or Dinner, string) - **size**: Size of the group (integer) Task: 1. **Load the dataset**: Start by loading the `tips` dataset using Seaborn\'s `load_dataset` function. 2. **Data Transformation**: - Group the data to calculate the average `tip` for each combination of `day` and `time`. - Create a new column that calculates the standard deviation of the tips for each group. 3. **Visualization**: - Create a plot using Seaborn\'s object-oriented interface to visualize the trends in average tips over different days of the week, segmented by `time`. - Add error bands to your plot to represent the standard deviation of the tips. 4. **Customization**: - Ensure that the plot has distinct colors for lunch and dinner. - Add appropriate labels and titles to the plot. Expected Input and Output: - **Input**: No input from the user. The dataset `tips` is loaded within the script. - **Output**: A Seaborn plot visualized with the specified customizations. Constraints: - You must use the Seaborn `objects` API (`so.Plot`) to achieve the task. - Handle any missing data in a manner that does not interrupt the plot generation. Here is a template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # 1. Load the dataset tips = load_dataset(\'tips\') # 2. Data Transformation # Group the data by day and time, calculate the mean and standard deviation of tips grouped = tips.groupby([\'day\', \'time\']).agg(mean_tip=(\'tip\', \'mean\'), std_tip=(\'tip\', \'std\')).reset_index() # 3. Visualization # Create a plot using seaborn\'s object interface p = so.Plot(grouped, x=\'day\', y=\'mean_tip\', color=\'time\') p.add(so.Band(y=\'mean_tip\', ymin=grouped[\'mean_tip\']-grouped[\'std_tip\'], ymax=grouped[\'mean_tip\']+grouped[\'std_tip\'], alpha=0.3, edgewidth=1)) p.add(so.Line(linewidth=2)) # 4. Customization # Add necessary customization such as labels and title p.label(x=\\"Day of the week\\", y=\\"Average Tip\\", title=\\"Average Tips by Day and Time with Error Bands\\") p.show() ``` Ensure that your final plot reflects the described requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def visualize_tips(): # 1. Load the dataset tips = load_dataset(\'tips\') # 2. Data Transformation # Group the data by day and time, calculate the mean and standard deviation of tips grouped = tips.groupby([\'day\', \'time\']).agg(mean_tip=(\'tip\', \'mean\'), std_tip=(\'tip\', \'std\')).reset_index() # 3. Visualization # Create a plot using seaborn\'s object interface p = so.Plot(grouped, x=\'day\', y=\'mean_tip\', color=\'time\').scale(color=\'colorblind\') p.add(so.Band(y=\'mean_tip\', ymin=grouped[\'mean_tip\'] - grouped[\'std_tip\'], ymax=grouped[\'mean_tip\'] + grouped[\'std_tip\'], alpha=0.3, edgewidth=1)) p.add(so.Line(linewidth=2)) # 4. Customization # Add necessary customization such as labels and title p.label(x=\\"Day of the week\\", y=\\"Average Tip\\", title=\\"Average Tips by Day and Time with Error Bands\\") p.show()"},{"question":"**Problem Statement:** You are required to implement a simplified task scheduling system using Python\'s `queue` module. The system should be able to handle tasks submitted from multiple sources, prioritize them, and process them using worker threads. The scheduling system must support the following features: 1. Tasks can be submitted with a priority (1 being the highest priority). 2. Tasks are processed by multiple worker threads. 3. The system should be thread-safe and handle synchronization correctly. **Specifications:** 1. Implement the `TaskScheduler` class with the following methods: - `__init__(self, num_workers)`: Initializes the scheduler with a given number of worker threads. - `submit_task(self, priority: int, task: callable)`: Submits a new task with the specified priority. The task should be a callable (function) that takes no arguments. - `start(self)`: Starts the worker threads and begins processing tasks. - `stop(self)`: Stops all worker threads gracefully after completing the current tasks. 2. Use the `PriorityQueue` for managing the tasks and a combination of `Queue` operations to ensure tasks are processed correctly by worker threads. 3. Ensure that the system handles the synchronization between threads correctly, avoiding race conditions and ensuring the proper completion of tasks. **Input and Output:** - No direct input or output; the class methods will be used to interact with the scheduling system. **Constraints:** - The maximum number of worker threads will be 10. - The `stop` method should ensure that all tasks are processed before stopping the threads. **Example Usage:** ```python import time def task1(): print(\\"Task 1 is being processed\\") time.sleep(1) print(\\"Task 1 is completed\\") def task2(): print(\\"Task 2 is being processed\\") time.sleep(2) print(\\"Task 2 is completed\\") def task3(): print(\\"Task 3 is being processed\\") time.sleep(1) print(\\"Task 3 is completed\\") if __name__ == \'__main__\': scheduler = TaskScheduler(num_workers=3) scheduler.submit_task(priority=2, task=task1) scheduler.submit_task(priority=1, task=task2) scheduler.submit_task(priority=3, task=task3) scheduler.start() scheduler.stop() ``` In this example, `task2` should be processed first because it has the highest priority, followed by `task1` and `task3`. **Performance Requirements:** - The system should efficiently handle and process up to 100 tasks. **Hints:** - Use `queue.PriorityQueue` to manage the tasks. - Implement a thread-safe method to start and stop the worker threads. - Ensure each worker thread retrieves tasks from the priority queue and processes them. - Use synchronization mechanisms provided by the `queue` module to handle task completions.","solution":"import threading import queue class TaskScheduler: def __init__(self, num_workers): self.num_workers = num_workers self.tasks = queue.PriorityQueue() self.threads = [] self.stopped = threading.Event() def submit_task(self, priority: int, task: callable): self.tasks.put((priority, task)) def worker(self): while not self.stopped.is_set() or not self.tasks.empty(): try: priority, task = self.tasks.get(timeout=1) task() self.tasks.task_done() except queue.Empty: continue def start(self): self.stopped.clear() for _ in range(self.num_workers): thread = threading.Thread(target=self.worker) thread.start() self.threads.append(thread) def stop(self): self.stopped.set() for thread in self.threads: thread.join()"},{"question":"**Advanced Email Handling with Python** # Problem Statement: You are required to implement a function `send_custom_email` that sends a customized email using the Python `email` package. The email should contain various parts, including plain text, an HTML version, and an image embedded within the HTML content. Additionally, save a copy of the email message to disk before sending it. # Function Signature: ```python def send_custom_email(subject: str, text_content: str, html_content: str, sender: str, recipients: list, image_path: str, output_path: str) -> None: pass ``` # Input: - `subject` (str): The subject of the email. - `text_content` (str): The plain text version of the email content. - `html_content` (str): The HTML version of the email content which should include a placeholder for the image using the format `<img src=\\"cid:image_cid\\" />`. - `sender` (str): The sender\'s email address. - `recipients` (list): A list of recipient email addresses. - `image_path` (str): The file path of the image to be embedded within the HTML content. - `output_path` (str): The file path where the composed email should be saved before sending. # Output: - The function does not return anything. It sends the email and saves a copy locally. # Constraints: - The `image_path` must point to a valid image file on the disk. - The `output_path` must be a valid path where the email should be saved. - The email should include a plain text part, an HTML part with an embedded image, and all the necessary headers should be properly formatted. # Example: ```python subject = \\"Special Event Invitation\\" text_content = \\"Join us for a special event this weekend!\\" html_content = <html> <body> <p>Join us for a <b>special event</b> this weekend!</p> <img src=\\"cid:image_cid\\" /> </body> </html> sender = \\"organizer@example.com\\" recipients = [\\"attendee1@example.com\\", \\"attendee2@example.com\\"] image_path = \\"event_image.jpg\\" output_path = \\"saved_email.msg\\" send_custom_email(subject, text_content, html_content, sender, recipients, image_path, output_path) ``` # Explanation: The example call to `send_custom_email` should compose an email with the specified subject and content. The HTML content will have an embedded image specified by `image_path`. The email is saved to `output_path` and sent to the recipients. # Notes: - You must use the `email` package to construct the email message. - Use proper email structures (MIMEType) as demonstrated in the documentation. - The function should handle possible exceptions (e.g., file not found, invalid email addresses) gracefully.","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email import encoders from email.utils import formatdate import os def send_custom_email(subject: str, text_content: str, html_content: str, sender: str, recipients: list, image_path: str, output_path: str) -> None: # Create the root message and fill in the from, to, and subject headers msg = MIMEMultipart(\'related\') msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = \\", \\".join(recipients) msg[\'Date\'] = formatdate(localtime=True) # Encapsulate the plain and HTML versions of the message body in an alternative part msg_alternative = MIMEMultipart(\'alternative\') msg.attach(msg_alternative) # Attach the plain text message part1 = MIMEText(text_content, \'plain\') msg_alternative.attach(part1) # Attach the HTML message with the image placeholder part2 = MIMEText(html_content, \'html\') msg_alternative.attach(part2) # Attach the image if os.path.isfile(image_path): with open(image_path, \'rb\') as img_fp: msg_image = MIMEImage(img_fp.read()) encoders.encode_base64(msg_image) msg_image.add_header(\'Content-ID\', \'<image_cid>\') msg.attach(msg_image) else: raise FileNotFoundError(f\\"Image file not found: {image_path}\\") # Save the email to a file with open(output_path, \'w\') as f: f.write(msg.as_string()) # Send the email using a local SMTP server (for demonstration purposes) try: with smtplib.SMTP(\'localhost\') as s: s.sendmail(sender, recipients, msg.as_string()) except Exception as e: print(f\\"An error occurred while sending email: {e}\\")"},{"question":"# Custom Import Mechanism in Python You are tasked with creating a custom import mechanism using Python\'s import system functionalities. The goal is to write a function that selectively imports specific submodules from within a package and caches these imports to avoid repeated imports. Instructions 1. Implement the function `custom_importer` that takes two arguments: - `package_name`: A string representing the fully qualified name of the package. - `submodules`: A list of strings, each representing a submodule within the package to be imported. 2. The function should: - Import the specified submodules from the provided package. - Cache these imported submodules to avoid repeated imports using `sys.modules`. - Return a dictionary where the keys are the submodules\' names and the values are the imported module objects. 3. Make sure that the function handles the following scenarios: - If a submodule does not exist, it should skip the import for that particular submodule and continue with the rest. - If a submodule is already imported and cached, it should retrieve it from the cache instead of importing it again. Constraints - You may assume that the package and its submodules (if they exist) are correctly installed and accessible. - Do not use any third-party libraries for this implementation. - Ensure that the function works with both regular packages and namespace packages. Example Consider a package `example_pkg` with the following structure: ``` example_pkg/ __init__.py submodule1.py submodule2.py ``` If you call: ```python result = custom_importer(\'example_pkg\', [\'submodule1\', \'submodule2\', \'submodule3\']) ``` - `submodule1` and `submodule2` should be imported and included in the result. - `submodule3` does not exist, so it should be skipped. - Subsequent calls to import `submodule1` or `submodule2` should retrieve these submodules from the cache. The expected output of the example call should be a dictionary: ```python { \'submodule1\': <module \'example_pkg.submodule1\' from \'.../example_pkg/submodule1.py\'>, \'submodule2\': <module \'example_pkg.submodule2\' from \'.../example_pkg/submodule2.py\'> } ``` Implementation Provide the implementation for the `custom_importer` function below: ```python import sys import importlib def custom_importer(package_name, submodules): result = {} for submodule in submodules: full_module_name = f\\"{package_name}.{submodule}\\" if full_module_name in sys.modules: result[submodule] = sys.modules[full_module_name] else: try: module = importlib.import_module(full_module_name) result[submodule] = module sys.modules[full_module_name] = module except ModuleNotFoundError: continue return result # Example Usage: # result = custom_importer(\'example_pkg\', [\'submodule1\', \'submodule2\', \'submodule3\']) # print(result) ``` Ensure to test your implementation with various scenarios to confirm its correctness and robustness.","solution":"import sys import importlib def custom_importer(package_name, submodules): Imports specified submodules from a given package and caches them. Parameters: - package_name: A string representing the fully qualified name of the package. - submodules: A list of strings, each representing a submodule within the package to be imported. Returns: A dictionary where the keys are the submodules\' names and the values are the imported module objects. result = {} for submodule in submodules: full_module_name = f\\"{package_name}.{submodule}\\" if full_module_name in sys.modules: result[submodule] = sys.modules[full_module_name] else: try: module = importlib.import_module(full_module_name) result[submodule] = module sys.modules[full_module_name] = module except ModuleNotFoundError: continue return result"},{"question":"# PyTorch Coding Assessment Question Objective Implement a custom PyTorch module that uses data-dependent control flow via `torch.cond`. You will create a PyTorch module that takes a tensor as input and performs different transformations based on the sum of the tensor elements. Problem Statement You are tasked with creating a PyTorch module called `SumBasedConditional`. This module will use the `torch.cond` operator to decide between two different operations on the input tensor based on whether the sum of its elements is greater than a specified threshold. Requirements 1. **Module Definition**: Define a class `SumBasedConditional` that inherits from `torch.nn.Module`. 2. **Constructor**: - The class should have an `__init__` method that accepts a `threshold` parameter (a float) and initializes it. 3. **Forward Method**: - Implement the `forward` method which takes a tensor `x` as input. - Use `torch.cond` to apply one of two operations depending on whether the sum of the elements in `x` is greater than `threshold`. - Define the two operations as follows: * **True Function**: If the condition is true (sum is greater than `threshold`), the tensor `x` should have its elements squared. * **False Function**: If the condition is false (sum is less than or equal to `threshold`), the tensor `x` should have its elements cubed. Constraints - You must use `torch.cond` to implement the conditional logic. - You may assume the input tensor will always be a one-dimensional tensor of floating-point numbers. - You should not use any external libraries other than PyTorch. Expected Input and Output Formats - **Input**: A one-dimensional tensor `x` of floating-point numbers. - **Output**: A tensor of the same shape as `x` with the appropriate conditional transformation applied. Example ```python import torch class SumBasedConditional(torch.nn.Module): def __init__(self, threshold: float): super(SumBasedConditional, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return x ** 2 def false_fn(x: torch.Tensor): return x ** 3 return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x,)) # Example usage: model = SumBasedConditional(threshold=10.0) input_tensor = torch.tensor([1.0, 2.0, 3.0]) output_tensor = model(input_tensor) print(output_tensor) # Output will be: tensor([ 1., 8., 27.]) since sum([1, 2, 3]) ≤ 10 ``` Note: The example usage provided is indicative. Your implementation should be able to handle different input tensors and thresholds as specified. Ensure your code is clear, well-commented, and follows best practices.","solution":"import torch import torch.nn as nn class SumBasedConditional(nn.Module): def __init__(self, threshold: float): super(SumBasedConditional, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x): return x ** 2 def false_fn(x): return x ** 3 return torch.where(x.sum() > self.threshold, true_fn(x), false_fn(x))"},{"question":"# Pandas DataFrame Manipulation and Visualization Problem Statement You are given a CSV file named `sales_data.csv` containing sales data for a retail store. This file has the following columns: - `Date`: The date of the sale (format: YYYY-MM-DD) - `Product`: The name of the product sold - `Category`: The category of the product - `Quantity`: The quantity of the product sold - `Price`: The price of the product sold Using pandas, your task is to perform the following operations: 1. **Loading Data**: Load the data into a pandas DataFrame. 2. **Data Cleaning**: - Check for any missing values and handle them appropriately. 3. **Data Manipulation**: - Add a new column `Total_Sales` which is the product of `Quantity` and `Price`. - Group the data by `Category` and `Date`, and calculate the total sales for each category on each date. 4. **Data Visualization**: - Create a line plot showing the total sales over time for each category. - Create a bar plot showing the total sales for each product category. 5. **Advanced Visualization** (Optional but highly recommended): - Create a scatter plot to analyze the relationship between `Quantity` and `Price` for different product categories. Color the points based on the category. Function Signature ```python import pandas as pd def sales_analysis(csv_file: str) -> None: # Your code here pass ``` Constraints - Use pandas for data manipulation. - Use matplotlib or any compatible pandas plotting backend for visualization. - Ensure your plots are well-labeled and have informative titles. Expected Output - A pandas DataFrame with the required transformations and additional columns. - Several plots visualizing different aspects of the sales data. Example Usage ```python sales_analysis(\'sales_data.csv\') ``` Performance Requirements - Your solution should be efficient with a linear or near-linear time complexity with respect to the number of rows in the dataset. # Notes - Ensure that the visualizations are clear and appropriately labeled. - Your code should be well-documented with comments explaining each step.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def sales_analysis(csv_file: str) -> pd.DataFrame: Perform sales data analysis including loading, cleaning, manipulation, and visualization. Args: - csv_file (str): Path to the CSV file containing sales data. Returns: - pd.DataFrame: DataFrame after data manipulations. # Load the data df = pd.read_csv(csv_file) # Data Cleaning: Handle missing values by filling them with 0 df.fillna(0, inplace=True) # Data Manipulation: Add a new column `Total_Sales` df[\'Total_Sales\'] = df[\'Quantity\'] * df[\'Price\'] # Group the data by `Category` and `Date`, and calculate the total sales grouped_df = df.groupby([\'Category\', \'Date\']).agg({\'Total_Sales\': \'sum\'}).reset_index() # Visualization # Line plot for total sales over time for each category plt.figure(figsize=(12, 8)) sns.lineplot(data=grouped_df, x=\'Date\', y=\'Total_Sales\', hue=\'Category\') plt.title(\'Total Sales Over Time by Category\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.legend(title=\'Category\') plt.tight_layout() plt.show() # Bar plot for total sales for each product category category_sales = df.groupby(\'Category\')[\'Total_Sales\'].sum().reset_index() plt.figure(figsize=(10, 6)) sns.barplot(data=category_sales, x=\'Category\', y=\'Total_Sales\') plt.title(\'Total Sales by Product Category\') plt.xlabel(\'Category\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.tight_layout() plt.show() # Scatter plot for Quantity vs Price colored by Category plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'Quantity\', y=\'Price\', hue=\'Category\') plt.title(\'Quantity vs Price by Category\') plt.xlabel(\'Quantity\') plt.ylabel(\'Price\') plt.legend(title=\'Category\') plt.tight_layout() plt.show() return df # To call the function and see visualizations, you would use: # sales_analysis(\'sales_data.csv\')"},{"question":"**Objective**: Demonstrate your understanding of Python\'s error handling, script execution, and interactive startup file concepts. **Problem Statement**: Create a Python script called `custom_interactive.py` that performs the following tasks: 1. **Setup**: - Creates a file called `startup.py` in the current directory containing the following Python commands: ```python import os import sys print(\\"Startup file executed.\\") ``` 2. **Exception Handling**: - The script should prompt the user to input a Python expression repeatedly until they decide to exit by entering `exit`. - If a `KeyboardInterrupt` (i.e., pressing `Ctrl+C`) is detected, the script should catch this exception and print `\\"Execution interrupted by user\\"`, then continue execution. - If any other exception occurs, print the error type and message, and continue executing. 3. **Executing with Startup File**: - Before entering the input loop, the script should read and execute the `startup.py` file using the environment variable method described in the documentation. Ensure any variables or imports in `startup.py` are accessible in the main script. 4. **Customizing Site Initialization**: - Within the script, create a `usercustomize.py` file in the user site-packages directory that prints `\\"User customization applied.\\"` when executed. **Constraints**: - The script should be compatible with both Unix-based systems and Windows. - Ensure correct handling of file paths and environment variables across platforms. **Input/Output Format**: - **Input**: User inputs Python expressions interactively. - **Output**: - Print `\\"Startup file executed.\\"` when `startup.py` runs. - Print `\\"User customization applied.\\"` when `usercustomize.py` is executed. - Print the result of evaluating the user input expression. - Print `\\"<ErrorType>: <ErrorMessage>\\"` for any errors that occur, except `KeyboardInterrupt`, which should print `\\"Execution interrupted by user\\"`. **Example Execution**: ``` python3 custom_interactive.py Startup file executed. User customization applied. > 1 + 1 2 > print(\\"Hello, world!\\") Hello, world! > int(\'abc\') ValueError: invalid literal for int() with base 10: \'abc\' > Ctrl+C Execution interrupted by user > exit Program exited. ``` Create the `custom_interactive.py` script to fulfill these requirements.","solution":"import os import sys import site def create_startup_file(): with open(\'startup.py\', \'w\') as f: f.write(\'import osn\') f.write(\'import sysn\') f.write(\'print(\\"Startup file executed.\\")n\') def create_usercustomize_file(): user_site_path = site.getusersitepackages() if not os.path.exists(user_site_path): os.makedirs(user_site_path) with open(os.path.join(user_site_path, \'usercustomize.py\'), \'w\') as f: f.write(\'print(\\"User customization applied.\\")n\') def execute_startup_file(): startup_path = os.path.abspath(\'startup.py\') os.environ[\'PYTHONSTARTUP\'] = startup_path with open(startup_path, \'r\') as f: exec(f.read(), globals()) def main(): create_startup_file() create_usercustomize_file() execute_startup_file() while True: try: user_input = input(\'> \') if user_input.lower() == \'exit\': print(\'Program exited.\') break result = eval(user_input) if result is not None: print(result) except KeyboardInterrupt: print(\\"nExecution interrupted by user\\") except Exception as e: print(f\\"{type(e).__name__}: {e}\\") if __name__ == \'__main__\': main()"},{"question":"Background In this task, you will work with the `colorsys` module to perform color conversions. Understanding different color systems and converting between them is crucial in many multimedia applications, such as image processing, computer graphics, and design. Problem Statement You need to write a Python function that receives an RGB color value and converts it to the HLS (Hue, Lightness, Saturation) color system. The function should then determine if the converted HLS value represents a color that is more \\"light\\" or \\"dark\\". Specifically, a color is considered \\"light\\" if its lightness value is greater than 0.5; otherwise, it is considered \\"dark\\". Detailed Requirements: 1. Implement a function called `rgb_to_light_or_dark` with the following signature: ```python def rgb_to_light_or_dark(rgb: tuple) -> str: ``` 2. The function takes one input: - `rgb` (tuple): A tuple of three elements representing the red, green, and blue components of the color. Each component is a float between 0 and 1, inclusive. 3. The function returns a string: - `\\"light\\"` if the lightness component of the HLS conversion is greater than 0.5. - `\\"dark\\"` otherwise. 4. Use the `colorsys.rgb_to_hls` function from the `colorsys` module for the conversion. Example ```python # Example 1 rgb = (0.1, 0.2, 0.3) print(rgb_to_light_or_dark(rgb)) # Output: \\"dark\\" # Example 2 rgb = (0.7, 0.8, 0.9) print(rgb_to_light_or_dark(rgb)) # Output: \\"light\\" # Example 3 rgb = (0.5, 0.5, 0.5) print(rgb_to_light_or_dark(rgb)) # Output: \\"dark\\" ``` Constraints - Ensure that the RGB values provided are within the valid range [0, 1]. - Consider performance efficiency, though for the given constraints it should not be a major concern. Additional Notes - You can assume the `colorsys` module is available and do not need to write the conversion functions yourself. - Aim to create clean and readable code with appropriate variable names and comments where necessary.","solution":"import colorsys def rgb_to_light_or_dark(rgb: tuple) -> str: Converts an RGB color value to HLS and determines if the color is \'light\' or \'dark\' based on the lightness value. Parameters: rgb (tuple): A tuple containing the red, green, and blue components of the color, each between 0 and 1 inclusive. Returns: str: \'light\' if the lightness value > 0.5, otherwise \'dark\'. try: # Ensure the rgb values are valid assert all(0 <= component <= 1 for component in rgb), \\"RGB values should be between 0 and 1\\" # Convert RGB to HLS hls = colorsys.rgb_to_hls(*rgb) lightness = hls[1] # Determine if the color is light or dark return \\"light\\" if lightness > 0.5 else \\"dark\\" except AssertionError as e: raise ValueError(e)"},{"question":"# Mocking and Patching in Unit Tests **Objective:** Demonstrate your understanding of the `unittest.mock` package in Python by creating and using mock objects, configuring them, and making assertions about their usage. You will also use the `patch()` function to replace real objects with mock objects within the scope of the test and utilize the autospec feature. **Instructions:** 1. **Mock Creation and Configuration:** - Create a mock object for a hypothetical class `DatabaseClient` that has two methods `connect()` and `fetch_data()`. - Configure the mock such that the `connect()` method returns `True` and the `fetch_data()` method returns a list of dictionaries representing rows of data. 2. **Assert Methods:** - Make assertions to verify that the methods of the mock object were called with the correct arguments. - Check how many times the `connect()` method was called. 3. **Patching:** - Use the `patch()` function to replace the `DatabaseClient` class in a module `data_processor` with a mock within the scope of a test function. - Ensure that the mock object created by `patch()` has the correct specification (i.e., the same methods and attributes as `DatabaseClient`). 4. **Advanced Configuration:** - Use a `side_effect` on the `fetch_data()` method to simulate raising an exception when a specific parameter is passed. - Use the `autospec` feature to ensure that the mock methods have the same signature as the real methods. **Expected Input and Output:** Here is a hypothetical module `data_processor`: ```python # data_processor.py class DatabaseClient: def connect(self, host, port): # Simulates connecting to a database pass def fetch_data(self, query): # Simulates fetching data from a database pass def process_data(db_client): db_client.connect(\'localhost\', 5432) try: data = db_client.fetch_data(\'SELECT * FROM table\') except Exception as e: return str(e) return data ``` **Your task is to:** 1. Write a unit test for the `process_data` function using the `unittest.mock` package. 2. Replace the `DatabaseClient` with a mock to control the behavior of `connect()` and `fetch_data()`. 3. Use assertions to ensure the methods are called correctly and handle exceptions properly. **Constraints:** - You must use the `unittest.mock` package. - The `connect()` method should be called exactly once. - The `fetch_data()` method should raise an exception when queried with `\'SELECT * FROM error_table\'`. **Performance Requirements:** - The tests should be efficient and not rely on actual database connections or network calls. # Example Unit Test: ```python import unittest from unittest.mock import patch, Mock, call from data_processor import process_data, DatabaseClient class TestDataProcessor(unittest.TestCase): @patch(\'data_processor.DatabaseClient\', autospec=True) def test_process_data(self, MockDatabaseClient): # Setup the mock configuration mock_db_client = MockDatabaseClient.return_value mock_db_client.connect.return_value = True mock_db_client.fetch_data.side_effect = lambda query: ( raise Exception(\\"Database error\\") if query == \'SELECT * FROM error_table\' else [{\'id\': 1, \'data\': \'value\'}] ) # Call the function to test result = process_data(mock_db_client) # Assertions mock_db_client.connect.assert_called_once_with(\'localhost\', 5432) mock_db_client.fetch_data.assert_called_once_with(\'SELECT * FROM table\') self.assertEqual(result, [{\'id\': 1, \'data\': \'value\'}]) # Test exception handling result_with_error = process_data(mock_db_client) self.assertEqual(result_with_error, \'Database error\') if __name__ == \'__main__\': unittest.main() ```","solution":"import unittest from unittest.mock import Mock, patch, call class DatabaseClient: def connect(self, host, port): # Simulates connecting to a database pass def fetch_data(self, query): # Simulates fetching data from a database pass def process_data(db_client): db_client.connect(\'localhost\', 5432) try: data = db_client.fetch_data(\'SELECT * FROM table\') except Exception as e: return str(e) return data"},{"question":"You are tasked with implementing a utility that reads data from multiple URLs and processes the data concurrently. To ensure proper resource management, you need to use asynchronous context managers. # Requirements 1. **Context Manager**: Implement an asynchronous context manager to handle acquiring and releasing network connections. 2. **Exception Handling**: Properly handle any exceptions that occur during data acquisition or processing to ensure resources are properly released. 3. **Concurrency**: Use asyncio to manage concurrent data acquisition and processing. # Implementation Details 1. **AsyncContextManager**: - Create a class `AsyncNetworkManager` that inherits from `contextlib.AbstractAsyncContextManager`. - The `__aenter__` method should simulate acquiring a network connection. - The `__aexit__` method should ensure that the network connection is closed, even if an exception is raised within the context. 2. **Data Acquisition Function**: - Implement an asynchronous function `fetch_data` that takes a URL and uses `AsyncNetworkManager` to manage the connection. Simulate a network fetch with `asyncio.sleep`. 3. **Concurrent Processing**: - Implement an asynchronous function `process_urls` that takes a list of URLs, uses `asyncio.gather` to fetch data concurrently, and handles exceptions appropriately. ```python import asyncio from contextlib import AbstractAsyncContextManager class AsyncNetworkManager(AbstractAsyncContextManager): async def __aenter__(self): print(\\"Acquiring connection\\") await asyncio.sleep(1) # Simulate network delay return self async def __aexit__(self, exc_type, exc, tb): if exc: print(f\\"Error: {exc}\\") print(\\"Releasing connection\\") await asyncio.sleep(1) # Simulate network release async def fetch_data(url): async with AsyncNetworkManager(): print(f\\"Fetching data from {url}\\") await asyncio.sleep(2) # Simulate data fetching return f\\"Data from {url}\\" async def process_urls(urls): tasks = [fetch_data(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) for result in results: if isinstance(result, Exception): print(f\\"Encountered exception: {result}\\") else: print(f\\"Successfully retrieved: {result}\\") # URLs for testing urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] # Uncomment to run # asyncio.run(process_urls(urls)) ``` # Input - A list of URLs (strings). # Output - It should print statements indicating the acquiring and releasing of connections and the success or failure of data fetching. # Constraints - Simulated network operations (`asyncio.sleep`) should be used to demonstrate asynchronous context management. - All URLs should be processed concurrently. - Properly handle all exceptions and ensure resources are cleaned up. # Performance Requirements - Ensure that all connections and fetch operations are managed asynchronously to maximize concurrency.","solution":"import asyncio from contextlib import AbstractAsyncContextManager class AsyncNetworkManager(AbstractAsyncContextManager): async def __aenter__(self): print(\\"Acquiring connection\\") await asyncio.sleep(1) # Simulate network delay return self async def __aexit__(self, exc_type, exc, tb): if exc: print(f\\"Error: {exc}\\") print(\\"Releasing connection\\") await asyncio.sleep(1) # Simulate network release async def fetch_data(url): async with AsyncNetworkManager(): print(f\\"Fetching data from {url}\\") await asyncio.sleep(2) # Simulate data fetching return f\\"Data from {url}\\" async def process_urls(urls): tasks = [fetch_data(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) for result in results: if isinstance(result, Exception): print(f\\"Encountered exception: {result}\\") else: print(f\\"Successfully retrieved: {result}\\") # URLs for testing urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] # Uncomment to run # asyncio.run(process_urls(urls))"},{"question":"Objective Write a Python function using pandas that performs the following operations on a given DataFrame: 1. **Memory Analysis**: Analyze and print the memory usage of the DataFrame using both standard and deep memory analysis. 2. **Boolean Operations**: Implement a function that receives a DataFrame and a condition, and returns a DataFrame filtered by the condition using proper Boolean indexing without causing a ValueError. 3. **Applying UDF**: Apply a user-defined function that increments every integer value by 1 without mutating the original DataFrame. 4. **Missing Values**: Replace all missing values in the DataFrame with the mean value of their respective columns. 5. **Byte Conversion**: Convert the given DataFrame to native system byte order if it is in a different byte order. Input and Output Formats - **Input**: A DataFrame `df` and a Boolean condition `condition`. - **Output**: Various outputs as described below. Constraints 1. Assume the DataFrame can be large. 2. Handle non-integer values appropriately in UDF. 3. Ensure thread-safety while applying the UDF. Specifications 1. **Memory Analysis:** ```python def memory_analysis(df): Prints the memory usage of the DataFrame in both standard and deep modes. Parameters: df (pd.DataFrame): Input DataFrame. ``` 2. **Boolean Operations:** ```python def filter_dataframe(df, condition): Filters the DataFrame based on a given condition using proper Boolean indexing. Parameters: df (pd.DataFrame): Input DataFrame. condition (str): A condition to filter the DataFrame Returns: pd.DataFrame: Filtered DataFrame. ``` 3. **Applying UDF:** ```python def apply_udf_safely(df): Applies a UDF to increment each integer value by 1, without mutating the original DataFrame. Parameters: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with updated values. ``` 4. **Missing Values:** ```python def replace_missing_values(df): Replaces all missing values in the DataFrame with the mean value of their respective columns. Parameters: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with missing values replaced. ``` 5. **Byte Conversion:** ```python def convert_byte_order(df): Converts the DataFrame to native system byte order if it is different. Parameters: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame converted to native byte order. ``` **Implement these functions in a single script. Make sure each function\'s output demonstrates its correct functioning.**","solution":"import pandas as pd import numpy as np def memory_analysis(df): Prints the memory usage of the DataFrame in both standard and deep modes. Parameters: df (pd.DataFrame): Input DataFrame. print(\\"Memory usage (standard):\\") print(df.memory_usage(deep=False)) print(\\"nMemory usage (deep):\\") print(df.memory_usage(deep=True)) def filter_dataframe(df, condition): Filters the DataFrame based on a given condition using proper Boolean indexing. Parameters: df (pd.DataFrame): Input DataFrame. condition (str): A condition to filter the DataFrame Returns: pd.DataFrame: Filtered DataFrame. filtered_df = df.query(condition) return filtered_df def apply_udf_safely(df): Applies a UDF to increment each integer value by 1, without mutating the original DataFrame. Parameters: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with updated values. def increment(value): if pd.api.types.is_integer(value): return value + 1 return value return df.applymap(increment) def replace_missing_values(df): Replaces all missing values in the DataFrame with the mean value of their respective columns. Parameters: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with missing values replaced. return df.apply(lambda x: x.fillna(x.mean()) if x.dtype.kind in \'biufc\' else x) def convert_byte_order(df): Converts the DataFrame to native system byte order if it is different. Parameters: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame converted to native byte order. def to_native_type(series): if series.dtype.byteorder not in (\'<\', \'=\'): return series.astype(series.dtype.newbyteorder(\'N\')) return series return df.apply(to_native_type)"},{"question":"**Coding Assessment Question: Configuration File Validator** You have been provided with a system that relies on a configuration file to setup various parameters. Your task is to implement a function `validate_config(file_path: str) -> bool` that reads a configuration file and validates its contents according to specific rules. # Configuration File Structure The configuration file is an INI file with the following sections: 1. `[DATABASE]`: Contains settings for database connection. - `host`: Must be a non-empty string. - `port`: Must be an integer between 1024 and 65535. - `user`: Must be a non-empty string. - `password`: Must be a non-empty string. 2. `[APPLICATION]`: Contains application settings. - `debug`: Must be `True` or `False`. - `theme`: Must be one of the following strings: `light`, `dark`. # Input - `file_path` (str): A string representing the path to the configuration file. # Output - Returns `True` if the configuration file is valid according to the specified rules, `False` otherwise. # Constraints - You may assume that the configuration file, if present, is syntactically correct (i.e., there are no parsing errors). - If any required section or option is missing, or if any value fails to meet the criteria mentioned, the configuration is considered invalid. # Example Suppose the provided configuration file `config.ini` contains: ``` [DATABASE] host = localhost port = 5432 user = admin password = secret [APPLICATION] debug = True theme = dark ``` ```python print(validate_config(\'config.ini\')) # Output: True ``` And if the file contains: ``` [DATABASE] host = port = 5432 user = admin password = [APPLICATION] debug = True theme = dark ``` ```python print(validate_config(\'config.ini\')) # Output: False ``` # Implementation Details You should use the `configparser` module to accomplish this task. Below is a skeleton of the `validate_config` function to get you started: ```python import configparser def validate_config(file_path: str) -> bool: config = configparser.ConfigParser() config.read(file_path) # Validate DATABASE section if \'DATABASE\' not in config: return False db_settings = config[\'DATABASE\'] if not db_settings.get(\'host\'): return False port = db_settings.getint(\'port\', fallback=None) if port is None or not (1024 <= port <= 65535): return False if not db_settings.get(\'user\'): return False if not db_settings.get(\'password\'): return False # Validate APPLICATION section if \'APPLICATION\' not in config: return False app_settings = config[\'APPLICATION\'] debug = app_settings.getboolean(\'debug\', fallback=None) if debug is None: return False theme = app_settings.get(\'theme\') if theme not in [\'light\', \'dark\']: return False return True ```","solution":"import configparser def validate_config(file_path: str) -> bool: config = configparser.ConfigParser() config.read(file_path) # Validate DATABASE section if \'DATABASE\' not in config: return False db_settings = config[\'DATABASE\'] if not db_settings.get(\'host\'): return False try: port = db_settings.getint(\'port\') if not (1024 <= port <= 65535): return False except ValueError: return False if not db_settings.get(\'user\'): return False if not db_settings.get(\'password\'): return False # Validate APPLICATION section if \'APPLICATION\' not in config: return False app_settings = config[\'APPLICATION\'] debug = app_settings.get(\'debug\') if debug not in [\'True\', \'False\']: return False theme = app_settings.get(\'theme\') if theme not in [\'light\', \'dark\']: return False return True"},{"question":"# Advanced Coding Assessment: Asynchronous Task Management with asyncio Problem Statement: You have been tasked with creating an asynchronous job scheduler using Python\'s asyncio library. The scheduler should be able to: 1. Schedule multiple asynchronous tasks concurrently. 2. Control the execution order of tasks based on priority. 3. Synchronize the completion of tasks using asyncio primitives. 4. Handle task timeouts and retries. Task Requirements: 1. Implement a `JobScheduler` class with the following methods: - `add_job(coroutine, priority)`: Adds an asynchronous task to the scheduler with a given priority. Lower priority numbers indicate higher priority. - `run()`: Runs all scheduled tasks concurrently while respecting their priority. Higher priority tasks should start earlier. - `clear_scheduler()`: Clears all tasks from the scheduler. 2. Implement the following functionalities in the `JobScheduler`: - Use appropriate asyncio synchronization primitives (like locks) to ensure that tasks do not interfere with one another when accessing shared resources. - Implement task timeouts: If a task exceeds a predefined execution time, it should be cancelled and retried up to a maximum number of retries. Input: - A series of asynchronous coroutines provided as input to the `add_job` method. - Each coroutine can simulate asynchronous I/O operations (e.g., using `asyncio.sleep`). Output: - Print the result of each task upon completion. - Print messages indicating if a task is retried or cancelled due to timeout. Constraints: - No direct input/output operations other than required printing within the scheduler are allowed. - Ensure all tasks complete within a reasonable timeframe (set a global timeout in the `run` method). Example Usage: ```python import asyncio import random class JobScheduler: def __init__(self): # Initialize your job scheduler pass def add_job(self, coroutine, priority): # Add a job to the scheduler with the given priority pass def run(self): # Run all scheduled jobs respecting their priority pass def clear_scheduler(self): # Clear all scheduled jobs pass # Example asynchronous task async def sample_task(name, duration): await asyncio.sleep(duration) print(f\'Task {name} completed after {duration} seconds\') # Sample usage scheduler = JobScheduler() scheduler.add_job(sample_task(\'Task1\', 2), priority=1) scheduler.add_job(sample_task(\'Task2\', 1), priority=2) scheduler.add_job(sample_task(\'Task3\', 3), priority=1) # Run the scheduler await scheduler.run() ``` Ensure your `JobScheduler` correctly handles priority, task synchronization, and timeouts.","solution":"import asyncio import time class JobScheduler: def __init__(self, max_retries=3, timeout=5): self.jobs = [] self.max_retries = max_retries self.timeout = timeout def add_job(self, coroutine, priority): self.jobs.append((priority, coroutine)) def clear_scheduler(self): self.jobs.clear() def retry_with_timeout(self, func, *args, **kwargs): retries = 0 while retries < self.max_retries: try: return asyncio.wait_for(func(*args, **kwargs), timeout=self.timeout) except asyncio.TimeoutError: print(f\\"Task {func.__name__} timed out. Retrying {retries + 1}/{self.max_retries}...\\") retries += 1 return None async def run_task(self, coroutine): try: result = await self.retry_with_timeout(coroutine) if result is not None: print(f\\"Task {coroutine.__name__} completed successfully.\\") else: print(f\\"Task {coroutine.__name__} failed after {self.max_retries} attempts.\\") except asyncio.CancelledError: print(f\\"Task {coroutine.__name__} was cancelled.\\") async def run(self): tasks = [] self.jobs.sort(key=lambda job: job[0]) # Sort by priority for _, coroutine in self.jobs: tasks.append(self.run_task(coroutine)) await asyncio.gather(*tasks) self.clear_scheduler() # Example usage async def sample_task(name, duration): await asyncio.sleep(duration) print(f\'Task {name} completed after {duration} seconds\') # Sample usage scheduler = JobScheduler(timeout=2) scheduler.add_job(sample_task(\'Task1\', 1), priority=1) scheduler.add_job(sample_task(\'Task2\', 1), priority=2) scheduler.add_job(sample_task(\'Task3\', 3), priority=1)"},{"question":"# Objective Write a Python function that programmatically imports a module and checks if a specified attribute or method is available within that module. If the module or the attribute does not exist, the function should handle these cases gracefully. # Function Signature ```python def check_and_import(module_name: str, attribute_name: str) -> bool: Imports the specified module and checks if the specified attribute exists in the module. Parameters: - module_name (str): The name of the module to import. - attribute_name (str): The name of the attribute or method to check for in the module. Returns: - bool: True if the module and attribute exist, False otherwise. ``` # Input - `module_name` (str): A string representing the name of the module to be imported. - `attribute_name` (str): A string representing the name of the attribute or method to check for in the module. # Output - The function should return `True` if both the module and the specified attribute exist. - The function should return `False` if either the module or the attribute does not exist. # Constraints - The function must handle any exceptions that arise from attempting to import the module or access the attribute. - Standard Python modules should be used; no external libraries are allowed. # Examples ```python # Example 1 # Trying to import a module \'math\' and check for attribute \'sqrt\' print(check_and_import(\'math\', \'sqrt\')) # Expected output: True # Example 2 # Trying to import a module \'math\' and check for attribute \'unknown_attr\' print(check_and_import(\'math\', \'unknown_attr\')) # Expected output: False # Example 3 # Trying to import a non-existent module \'nonexistent\' and check for any attribute print(check_and_import(\'nonexistent\', \'any_attr\')) # Expected output: False ``` # Additional Notes - Students should use the `importlib` module effectively to achieve the import functionality programmatically. - They should handle exceptions such as `ModuleNotFoundError` and `AttributeError` to manage cases where the module or attribute does not exist. This question tests students\' understanding of module imports, exception handling, and dynamic attribute checking in Python.","solution":"import importlib def check_and_import(module_name: str, attribute_name: str) -> bool: Imports the specified module and checks if the specified attribute exists in the module. Parameters: - module_name (str): The name of the module to import. - attribute_name (str): The name of the attribute or method to check for in the module. Returns: - bool: True if the module and attribute exist, False otherwise. try: module = importlib.import_module(module_name) return hasattr(module, attribute_name) except ModuleNotFoundError: return False except AttributeError: return False"},{"question":"# **Functional Programming Challenge with Python** **Problem Statement:** You are given a list of dictionaries where each dictionary represents a student with the following fields: `\'name\'`, `\'marks\'`, and `\'age\'`. Your task is to create three functions using functional programming constructs that achieve the following: 1. **Filter and sort students**: Filter out students who score less than 50 marks and sort the remaining students by their names alphabetically. 2. **Calculate average age**: Calculate and return the average age of the filtered and sorted list of students. 3. **Format the list of students**: Format the list into a list of strings where each string is of the format `\\"Name: XYZ, Marks: ABC, Age: PQR\\"`. Implement the following functions using functional programming constructs: 1. `filter_and_sort_students(students: List[Dict[str, Union[str, int]]]) -> List[Dict[str, Union[str, int]]]:` 2. `calculate_average_age(sorted_students: List[Dict[str, Union[str, int]]]) -> float:` 3. `format_students_list(sorted_students: List[Dict[str, Union[str, int]]]) -> List[str]:` **Input:** - A list of dictionaries, with each dictionary containing: - `\'name\'`: string - `\'marks\'`: integer - `\'age\'`: integer **Output:** 1. For `filter_and_sort_students`: a list of dictionaries, filtered and sorted as specified. 2. For `calculate_average_age`: a float representing the average age. 3. For `format_students_list`: a list of formatted strings. **Constraints:** - The list of students may have at most 10,000 entries. - Each student dictionary will have valid keys with appropriate data types as specified. - The age of a student is a positive integer. **Example:** ```python students = [ {\'name\': \'John Doe\', \'marks\': 75, \'age\': 20}, {\'name\': \'Jane Doe\', \'marks\': 45, \'age\': 21}, {\'name\': \'Alice\', \'marks\': 85, \'age\': 22}, {\'name\': \'Bob\', \'marks\': 55, \'age\': 19} ] filtered_sorted_students = filter_and_sort_students(students) # [{\'name\': \'Alice\', \'marks\': 85, \'age\': 22}, {\'name\': \'Bob\', \'marks\': 55, \'age\': 19}, {\'name\': \'John Doe\', \'marks\': 75, \'age\': 20}] average_age = calculate_average_age(filtered_sorted_students) # 20.33 formatted_list = format_students_list(filtered_sorted_students) # [\'Name: Alice, Marks: 85, Age: 22\', \'Name: Bob, Marks: 55, Age: 19\', \'Name: John Doe, Marks: 75, Age: 20\'] ``` **Additional Requirements:** - Use `map()`, `filter()`, and `sorted()` functions where appropriate. - Do not use iterative constructs like loops (`for`, `while`, etc.) for the primary logic.","solution":"from typing import List, Dict, Union def filter_and_sort_students(students: List[Dict[str, Union[str, int]]]) -> List[Dict[str, Union[str, int]]]: Filters out students with marks less than 50 and sorts the remaining students by their name alphabetically. filtered_students = filter(lambda student: student[\'marks\'] >= 50, students) sorted_students = sorted(filtered_students, key=lambda student: student[\'name\']) return sorted_students def calculate_average_age(sorted_students: List[Dict[str, Union[str, int]]]) -> float: Calculates the average age of the filtered and sorted list of students. if not sorted_students: return 0.0 total_age = sum(map(lambda student: student[\'age\'], sorted_students)) return total_age / len(sorted_students) def format_students_list(sorted_students: List[Dict[str, Union[str, int]]]) -> List[str]: Formats the list of students into a list of strings. Each string is of the format: \\"Name: XYZ, Marks: ABC, Age: PQR\\" return list(map(lambda student: f\\"Name: {student[\'name\']}, Marks: {student[\'marks\']}, Age: {student[\'age\']}\\", sorted_students))"},{"question":"# Coding Challenge: Model Export and Optimization Using `torch.export` Objective: Your task is to create a PyTorch model, export it using the `torch.export` functionality, and perform specific modifications and optimizations on the exported model. Requirements: 1. Implement a PyTorch neural network model that includes at least two layers from the following options: `Conv2d`, `Linear`, `ReLU`, `BatchNorm2d`. 2. Export the model into a graph-based intermediate representation using `torch.export.export`. 3. Modify the exported graph to replace a specific layer (e.g., the activation function `ReLU`) with a custom operation. 4. Verify the correctness of the exported, modified model by comparing its output with the original model before export. Instructions: 1. Define a class `MyModel` extending `torch.nn.Module` with the required layers. 2. Export the model using `torch.export.export`. 3. Implement a custom operation (e.g., a modified ReLU function that applies a threshold). 4. Apply this custom operation within the exported model graph. 5. Validate that the outputs of both the original and modified models are equivalent when given the same input tensor. Code Implementation: ```python import torch from torch.export import export, ExportedProgram import torch.nn.functional as F # Step 1: Define the model class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1) self.bn1 = torch.nn.BatchNorm2d(16) self.relu = torch.nn.ReLU() self.fc = torch.nn.Linear(16 * 32 * 32, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = torch.flatten(x, start_dim=1) x = self.fc(x) return x # Step 2: Export the model example_input = torch.randn(1, 3, 32, 32) model = MyModel() exported_program: ExportedProgram = export(model, example_input) # Step 3: Implement a custom ReLU function with a threshold def custom_relu(x, threshold=0.1): return torch.maximum(x, torch.tensor(threshold, dtype=x.dtype)) # Step 4: Modify the exported graph def replace_relu_with_custom(graph_module: ExportedProgram): for node in graph_module.graph.nodes: if node.op == \'call_function\' and node.target == torch.ops.aten.relu.default: with graph_module.graph.inserting_after(node): new_node = graph_module.graph.call_function(custom_relu, args=(node.args[0],)) node.replace_all_uses_with(new_node) graph_module.graph.erase_node(node) graph_module.recompile() replace_relu_with_custom(exported_program.model) # Step 5: Verify the correctness of the modified model modified_model = exported_program.model original_output = model(example_input) modified_output = modified_model(example_input) assert torch.allclose(original_output, modified_output, atol=1e-6), \\"Outputs are not equivalent!\\" print(\\"The modified exported model matches the original model\'s output.\\") ``` Constraints: - The comparison in step 5 should ensure a tolerance to account for potential minor numerical differences due to the custom operation. - Use `torch.allclose` with `atol=1e-6` for output comparison. Deliverables: - A Python script implementing the entire pipeline from model definition to validation of the modified exported model.","solution":"import torch from torch.fx import symbolic_trace # Step 1: Define the model class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1) self.bn1 = torch.nn.BatchNorm2d(16) self.relu = torch.nn.ReLU() self.fc = torch.nn.Linear(16 * 32 * 32, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = torch.flatten(x, start_dim=1) x = self.fc(x) return x # Step 2: Export the model using torch.fx model = MyModel() example_input = torch.randn(1, 3, 32, 32) traced_model = symbolic_trace(model) # Step 3: Implement a custom ReLU function with a threshold def custom_relu(x, threshold=0.1): return torch.maximum(x, torch.tensor(threshold, dtype=x.dtype)) # Step 4: Modify the exported graph to replace ReLU with custom ReLU class CustomReLUTracer(torch.fx.Interpreter): def call_function(self, target, args, kwargs): if target == torch.nn.functional.relu: return custom_relu(*args, **kwargs) return super().call_function(target, args, kwargs) custom_traced_model = CustomReLUTracer(traced_model).run(example_input) # Step 5: Verify the correctness of the modified model original_output = model(example_input) modified_output = custom_traced_model assert torch.allclose(original_output, modified_output, atol=1e-6), \\"Outputs are not equivalent!\\" print(\\"The modified exported model matches the original model\'s output.\\")"},{"question":"# Question: Working with Label Transformers in Scikit-learn You are given a dataset comprising samples with both singular labels and multiple labels. Your task is to implement a function `process_labels` that performs the following operations using scikit-learn\'s preprocessing classes: 1. **Label Binarization**: Use `LabelBinarizer` to transform a list of multiclass labels into a label indicator matrix. 2. **Multilabel Binarization**: Use `MultiLabelBinarizer` to transform a list of multilabel data into a binary indicator array and then revert it back to the original list of sets of labels. 3. **Label Encoding**: Use `LabelEncoder` to normalize a list of non-numerical labels (cities) to a numerical format and then revert them back to the original format. Your function should take three inputs: - `multiclass_labels` (List[int]): A list of integer labels. - `multilabel_data` (List[List[int]]): A list of lists, where each sublist represents a set of labels for one sample. - `city_labels` (List[str]): A list of non-numerical labels representing city names. The function should return a tuple containing: - The label indicator matrix obtained from `LabelBinarizer`. - The binary indicator array obtained from `MultiLabelBinarizer`. - The original format of the multilabel data after inverse transformation. - The encoded numerical labels obtained from `LabelEncoder`. - The original city names after inverse transformation from their encoded labels. Function Signature ```python def process_labels(multiclass_labels: List[int], multilabel_data: List[List[int]], city_labels: List[str]) -> Tuple[np.ndarray, np.ndarray, List[List[int]], np.ndarray, List[str]]: pass ``` Example ```python multiclass_labels = [1, 2, 6, 4, 2] multilabel_data = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] city_labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] result = process_labels(multiclass_labels, multilabel_data, city_labels) # result should be: # ( # array([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 0, 1], # [0, 0, 1, 0], # [0, 1, 0, 0]]), # array([[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]]), # [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]], # array([1, 1, 2, 0]), # [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] # ) ``` Constraints - You are allowed to use only the preprocessing classes from scikit-learn mentioned in the documentation. - The inputs will always be non-empty and properly formatted as described. Ensure your function handles the transformations and inverse transformations correctly and efficiently.","solution":"from typing import List, Tuple import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def process_labels(multiclass_labels: List[int], multilabel_data: List[List[int]], city_labels: List[str]) -> Tuple[np.ndarray, np.ndarray, List[List[int]], np.ndarray, List[str]]: # Label Binarization lb = LabelBinarizer() multiclass_label_matrix = lb.fit_transform(multiclass_labels) # Multilabel Binarization mlb = MultiLabelBinarizer() multilabel_binary_indicator = mlb.fit_transform(multilabel_data) inverse_multilabel_data = mlb.inverse_transform(multilabel_binary_indicator) # Label Encoding le = LabelEncoder() encoded_city_labels = le.fit_transform(city_labels) decoded_city_labels = le.inverse_transform(encoded_city_labels) return (multiclass_label_matrix, multilabel_binary_indicator, list(map(list, inverse_multilabel_data)), encoded_city_labels, list(decoded_city_labels))"},{"question":"Objective You are required to demonstrate your understanding of seaborn\'s advanced plotting functionality using the `seaborn.objects` module. Task Using the provided `mpg` dataset, complete the following tasks: 1. **Create a Pair Plot with Custom Labels**: - Plot the `acceleration` against both `displacement` and `weight`. - Add appropriate labels to `x0`, `x1`, and `y`. 2. **Show Multiple Pairwise Relationships**: - Plot pairwise relationships for `displacement`, `weight`, `horsepower`, and `acceleration`. - Ensure that each variable is paired against every other variable. 3. **Combine Pairing with Faceting**: - Create a grid of plots showing `weight` against both `horsepower` and `acceleration`. - Use faceting to create separate plots for different `origin` values. 4. **Customize the Grid Layout**: - Plot `mpg` against `displacement`, `weight`, `horsepower`, and `cylinders`. - Wrap these plots in a 2-column grid. You are expected to use the seaborn\'s `so.Plot` class for all tasks. Input - The input dataset `mpg` can be loaded using `mpg = seaborn.load_dataset(\\"mpg\\")`. Output Your code should produce the following plots: - A pair plot with custom labels. - A grid of plots showing multiple pairwise relationships. - A faceted plot combined with variable pairing. - A grid layout plot. Example Solution ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Task 1: Create a Pair Plot with Custom Labels ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", y=\\"Acceleration\\") .add(so.Dots()) ) # Task 2: Show Multiple Pairwise Relationships ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .add(so.Dots()) ) # Task 3: Combine Pairing with Faceting ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ) # Task 4: Customize the Grid Layout ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .add(so.Dots()) ) ``` Write your solution code below: ```python # Your code here ``` Ensure that your plots are correctly rendered and customized as per the given requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset mpg = load_dataset(\\"mpg\\") # Task 1: Create a Pair Plot with Custom Labels def plot_pair_with_custom_labels(): p = ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", y=\\"Acceleration (0-60 mph in seconds)\\") .add(so.Dots()) ) p.show() # Task 2: Show Multiple Pairwise Relationships def plot_multiple_pairwise_relationships(): p = ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .add(so.Dots()) ) p.show() # Task 3: Combine Pairing with Faceting def plot_faceted_pair_grid(): p = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ) p.show() # Task 4: Customize the Grid Layout def plot_customized_grid_layout(): p = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .add(so.Dots()) ) p.show()"},{"question":"# Exception Handling and Custom Exception Creation in Python In this exercise, you are required to demonstrate your understanding of Python\'s exception handling mechanisms and custom exception creation. You will create a set of functions that simulate a simple banking system, handle exceptions properly, and create custom exceptions where necessary. Instructions: 1. **Define Custom Exceptions:** - Create a custom exception named `InsufficientFundsError` which should be raised when an account does not have enough balance for a withdrawal. - Create a custom exception named `AccountNotFoundError` which should be raised when an account is not found in the banking system. 2. **Implement Banking Functions:** - `create_account(account_data, account_id, initial_balance)`: Adds a new account to `account_data` with the given `account_id` and `initial_balance`. Raise a `ValueError` if `initial_balance` is negative. - `get_balance(account_data, account_id)`: Returns the balance of the account with the given `account_id`. If the account does not exist, raise `AccountNotFoundError`. - `deposit(account_data, account_id, amount)`: Adds the specified `amount` to the balance of the account with the given `account_id`. Raise a `ValueError` if `amount` is not positive. - `withdraw(account_data, account_id, amount)`: Subtracts the specified `amount` from the balance of the account with the given `account_id`. Raise a `ValueError` if `amount` is not positive, and raise `InsufficientFundsError` if the balance is insufficient for the withdrawal. 3. **Chaining Exceptions in `transfer()` Function:** - `transfer(account_data, source_account_id, destination_account_id, amount)`: Transfers the specified `amount` from `source_account_id` to `destination_account_id`. - Handle exceptions within this function to ensure: - `AccountNotFoundError` is raised if either the source or destination account does not exist. - `InsufficientFundsError` is raised if the source account does not have enough balance for the transfer. - Use exception chaining to maintain the original context of the exception. Below is the skeleton code to get you started: ```python class InsufficientFundsError(Exception): pass class AccountNotFoundError(Exception): pass def create_account(account_data, account_id, initial_balance): if initial_balance < 0: raise ValueError(\\"Initial balance cannot be negative!\\") account_data[account_id] = initial_balance def get_balance(account_data, account_id): if account_id not in account_data: raise AccountNotFoundError(f\\"Account {account_id} not found!\\") return account_data[account_id] def deposit(account_data, account_id, amount): if amount <= 0: raise ValueError(\\"Deposit amount must be positive!\\") if account_id not in account_data: raise AccountNotFoundError(f\\"Account {account_id} not found!\\") account_data[account_id] += amount def withdraw(account_data, account_id, amount): if amount <= 0: raise ValueError(\\"Withdrawal amount must be positive!\\") if account_id not in account_data: raise AccountNotFoundError(f\\"Account {account_id} not found!\\") if account_data[account_id] < amount: raise InsufficientFundsError(f\\"Not enough funds in account {account_id}!\\") account_data[account_id] -= amount def transfer(account_data, source_account_id, destination_account_id, amount): try: withdraw(account_data, source_account_id, amount) deposit(account_data, destination_account_id, amount) except AccountNotFoundError as e: raise AccountNotFoundError(f\\"Transfer failed: {e}\\") from e except InsufficientFundsError as e: raise InsufficientFundsError(f\\"Transfer failed: {e}\\") from e # Example Usage: account_data = {} create_account(account_data, \'acc1\', 500) create_account(account_data, \'acc2\', 300) try: transfer(account_data, \'acc1\', \'acc2\', 200) except Exception as e: print(e) print(account_data) ``` Expected Function Behavior: - `create_account({\'acc1\': 500}, \'acc2\', 300)` should add an account `acc2` with a balance of 300 to the dictionary. - `get_balance({\'acc1\': 500}, \'acc1\')` should return 500. - `deposit({\'acc1\': 500}, \'acc1\', 100)` should increase the balance of `acc1` to 600. - `withdraw({\'acc1\': 500}, \'acc1\', 300)` should decrease the balance of `acc1` to 200, or raise `InsufficientFundsError` if the balance is insufficient. - `transfer({\'acc1\': 500, \'acc2\': 300}, \'acc1\', \'acc2\', 200)` should transfer 200 from `acc1` to `acc2`, raising appropriate exceptions if conditions are not met. Ensure your code is well-structured, handles exceptions gracefully, and includes any necessary comments to explain your logic.","solution":"class InsufficientFundsError(Exception): pass class AccountNotFoundError(Exception): pass def create_account(account_data, account_id, initial_balance): if initial_balance < 0: raise ValueError(\\"Initial balance cannot be negative!\\") account_data[account_id] = initial_balance def get_balance(account_data, account_id): if account_id not in account_data: raise AccountNotFoundError(f\\"Account {account_id} not found!\\") return account_data[account_id] def deposit(account_data, account_id, amount): if amount <= 0: raise ValueError(\\"Deposit amount must be positive!\\") if account_id not in account_data: raise AccountNotFoundError(f\\"Account {account_id} not found!\\") account_data[account_id] += amount def withdraw(account_data, account_id, amount): if amount <= 0: raise ValueError(\\"Withdrawal amount must be positive!\\") if account_id not in account_data: raise AccountNotFoundError(f\\"Account {account_id} not found!\\") if account_data[account_id] < amount: raise InsufficientFundsError(f\\"Not enough funds in account {account_id}!\\") account_data[account_id] -= amount def transfer(account_data, source_account_id, destination_account_id, amount): try: withdraw(account_data, source_account_id, amount) deposit(account_data, destination_account_id, amount) except AccountNotFoundError as e: raise AccountNotFoundError(f\\"Transfer failed: {e}\\") from e except InsufficientFundsError as e: raise InsufficientFundsError(f\\"Transfer failed: {e}\\") from e"},{"question":"**Tarfile Module Coding Challenge** # Objective Demonstrate your proficiency with the `tarfile` module by performing a variety of tasks involving reading, writing, and manipulating tar archive files. # Problem Description You are given a compressed tar archive `data.tar.gz` containing various file types (regular files, directories, symbolic links). Your task is to implement functions that: 1. **Extract all files safely** from the tar archive to a specified directory. 2. **List all files and directories**, along with their sizes, from the tar archive. 3. **Create a new tar archive** from a list of source files and directories. 4. **Add a filter** to the `extractall` method to ignore extraction of symbolic links and device files during extraction. # Input and Output Format Function 1: extract_safely(tarfile_path: str, extract_path: str) -> None - **Inputs:** - `tarfile_path` (str): The path to the compressed tar archive file (`data.tar.gz`). - `extract_path` (str): The directory where files should be extracted. - **Functionality:** - Extract all files from the tar archive to `extract_path` ensuring symbolic links and device files are ignored. - **Output:** None Function 2: list_contents(tarfile_path: str) -> List[Tuple[str, int]] - **Inputs:** - `tarfile_path` (str): The path to the compressed tar archive file (`data.tar.gz`). - **Outputs:** - A list of tuples where each tuple contains: - `name` (str): The name of the file/directory in the archive. - `size` (int): The size of the file in bytes. Function 3: create_tar_archive(source_paths: List[str], tarfile_path: str) -> None - **Inputs:** - `source_paths` (List[str]): A list of file and directory paths to be added to the tar archive. - `tarfile_path` (str): The path to the tar archive file to be created. - **Output:** None # Constraints - The tar archive file can be large (up to several gigabytes), so your implementation should be efficient in both time and space. - Handle exceptions gracefully and ensure the program can output informative error messages in case of exceptions. - Ensure that the directory structure is preserved during extraction and archive creation. # Example ```python # Example usage: extract_safely(\'data.tar.gz\', \'/tmp/extracted_files\') contents = list_contents(\'data.tar.gz\') print(contents) # [(\'file1.txt\', 1234), (\'dir1/file2.txt\', 5678), ...] create_tar_archive([\'/path/to/file1.txt\', \'/path/to/dir2\'], \'new_archive.tar.gz\') ``` # Note Refer to the `tarfile` module documentation for detailed information on methods and their usage. Ensure proper use of context management (`with` statement) to handle file operations safely.","solution":"import tarfile import os from typing import List, Tuple def extract_safely(tarfile_path: str, extract_path: str) -> None: Extracts the contents of a tar archive to a specified directory, ignoring symbolic links and device files. with tarfile.open(tarfile_path, \'r:gz\') as tar: def is_valid_member(member): if member.islnk() or member.isdev(): return False return True safe_members = [m for m in tar.getmembers() if is_valid_member(m)] tar.extractall(path=extract_path, members=safe_members) def list_contents(tarfile_path: str) -> List[Tuple[str, int]]: Lists all files and directories inside a tar archive along with their sizes. Args: - tarfile_path: path to the tar archive file. Returns: - A list of tuples containing file/directory name and size. with tarfile.open(tarfile_path, \'r:gz\') as tar: contents = [(member.name, member.size) for member in tar.getmembers()] return contents def create_tar_archive(source_paths: List[str], tarfile_path: str) -> None: Creates a tar archive from a list of source file and directory paths. Args: - source_paths: list of paths to files and directories to include in the archive. - tarfile_path: path to the tar archive file to be created. with tarfile.open(tarfile_path, \'w:gz\') as tar: for path in source_paths: tar.add(path, arcname=os.path.basename(path))"},{"question":"# Asynchronous Task with Futures You are tasked with implementing a function that schedules multiple asynchronous tasks using `asyncio.Future` and returns their results in order of execution. Your function should demonstrate the use of various `Future` object methods to manage task execution and result retrieval. Function Signature: ```python import asyncio from typing import List async def fetch_data(futures: List[asyncio.Future], delay_seconds: List[int], values: List[int]) -> List[int]: ``` Given: 1. `futures` - A list of `asyncio.Future` objects. 2. `delay_seconds` - A list of integers where each integer represents the number of seconds a corresponding task should wait before setting its result. 3. `values` - A list of integers where each integer represents the value to be set as the result of the corresponding future. Task: 1. Each `Future` should simulate a delay (using `asyncio.sleep`) from the corresponding `delay_seconds` list before setting its result from the `values` list. 2. Your function should return a list of results from all the futures in the order they complete. Detailed Requirements: 1. After the initial setup, each Future should be set with its result using `set_result` method after the specified delay. 2. Use `asyncio.gather` to wait for all the futures to complete. 3. Return a list of all results in the order they were completed. Example: ```python import asyncio async def fetch_data(futures: List[asyncio.Future], delay_seconds: List[int], values: List[int]) -> List[int]: async def set_result_after_delay(future: asyncio.Future, delay: int, value: int): await asyncio.sleep(delay) future.set_result(value) for future, delay, value in zip(futures, delay_seconds, values): asyncio.create_task(set_result_after_delay(future, delay, value)) results = await asyncio.gather(*futures) return results # Example usage async def main(): loop = asyncio.get_running_loop() futures = [loop.create_future() for _ in range(3)] delays = [2, 1, 3] values = [100, 200, 300] results = await fetch_data(futures, delays, values) print(results) # Output should be: [200, 100, 300] asyncio.run(main()) ``` **Constraints:** - The length of `futures`, `delay_seconds`, and `values` lists will always be equal. - The elements in `delay_seconds` are non-negative integers. - The elements in `values` are integers. Demonstrate the function implementation based on the provided example for clarity.","solution":"import asyncio from typing import List async def fetch_data(futures: List[asyncio.Future], delay_seconds: List[int], values: List[int]) -> List[int]: async def set_result_after_delay(future: asyncio.Future, delay: int, value: int): await asyncio.sleep(delay) future.set_result(value) for future, delay, value in zip(futures, delay_seconds, values): asyncio.create_task(set_result_after_delay(future, delay, value)) results = await asyncio.gather(*futures) return results # Example usage async def main(): loop = asyncio.get_running_loop() futures = [loop.create_future() for _ in range(3)] delays = [2, 1, 3] values = [100, 200, 300] results = await fetch_data(futures, delays, values) print(results) # Output should be: [100, 200, 300] # Run the example # asyncio.run(main()) # Uncomment this line to run the example outside the test environment"},{"question":"# Custom Exception Handling and Chaining in Python You are tasked with writing a Python program that simulates a library management system. This system should handle various errors gracefully using both built-in and custom exceptions. You should demonstrate your understanding of exception inheritance, raising exceptions with detailed messages, and handling them appropriately. Requirements: 1. **Define Custom Exceptions**: - `BookNotFoundError`: Raised when a book is not found in the library. - `DuplicateBookError`: Raised when trying to add a book that already exists in the library. 2. **Library Class**: - Implement a class `Library` with the following methods: - `add_book(self, title)`: Adds a book to the library. Raises `DuplicateBookError` if the book already exists. - `remove_book(self, title)`: Removes a book from the library. Raises `BookNotFoundError` if the book does not exist. - `find_book(self, title)`: Returns the book if found, otherwise raises `BookNotFoundError`. 3. **Exception Handling**: - Implement a function `manage_library()` that performs a series of operations on the `Library` class: 1. Adds a list of books to the library. 2. Removes a list of books from the library. 3. Tries to find a specific book. - Handle exceptions using try-except blocks and ensure that meaningful messages are printed to the user. - Demonstrate exception chaining by raising a new exception within an exception handler and using the `raise ... from` syntax. Constraints: - The `library` should be represented as a list of book titles. - Book titles are case-insensitive. Example: ```python class BookNotFoundError(Exception): pass class DuplicateBookError(Exception): pass class Library: def __init__(self): self.books = [] def add_book(self, title): # Your implementation here def remove_book(self, title): # Your implementation here def find_book(self, title): # Your implementation here def manage_library(): library = Library() try: library.add_book(\\"Python 101\\") library.add_book(\\"Python 101\\") # Should raise DuplicateBookError except DuplicateBookError as e: print(f\\"Error: {e}\\") try: library.remove_book(\\"Advanced Python\\") # Should raise BookNotFoundError except BookNotFoundError as e: print(f\\"Error: {e}\\") try: library.find_book(\\"Python Tricks\\") # Should raise BookNotFoundError except BookNotFoundError as e: print(f\\"Error: {e}\\") # Demonstrate exception chaining: try: raise ValueError(\\"General error occurred\\") # Simulate a general error except ValueError as e: raise RuntimeError(\\"Runtime error occurred\\") from e # Chained exception if __name__ == \\"__main__\\": manage_library() ``` Fill in the implementation of the `Library` class methods and handle the exceptions as described. Input Structure: - No input from the user. Use hard-coded book manipulation to demonstrate functionality. Output Structure: - Print statements within exception handlers showcasing the error handling and chained exceptions.","solution":"class BookNotFoundError(Exception): pass class DuplicateBookError(Exception): pass class Library: def __init__(self): self.books = set() # Using a set for efficient membership testing. def add_book(self, title): title = title.lower() if title in self.books: raise DuplicateBookError(f\\"The book \'{title}\' already exists in the library.\\") self.books.add(title) def remove_book(self, title): title = title.lower() if title not in self.books: raise BookNotFoundError(f\\"The book \'{title}\' was not found in the library.\\") self.books.remove(title) def find_book(self, title): title = title.lower() if title not in self.books: raise BookNotFoundError(f\\"The book \'{title}\' was not found in the library.\\") return title def manage_library(): library = Library() try: library.add_book(\\"Python 101\\") library.add_book(\\"Python 101\\") # Should raise DuplicateBookError except DuplicateBookError as e: print(f\\"Error: {e}\\") try: library.remove_book(\\"Advanced Python\\") # Should raise BookNotFoundError except BookNotFoundError as e: print(f\\"Error: {e}\\") try: library.find_book(\\"Python Tricks\\") # Should raise BookNotFoundError except BookNotFoundError as e: print(f\\"Error: {e}\\") # Demonstrate exception chaining: try: try: raise ValueError(\\"General error occurred\\") # Simulate a general error except ValueError as e: raise RuntimeError(\\"Runtime error occurred\\") from e # Chained exception except RuntimeError as e: print(f\\"Chained Error: {e}\\") print(f\\"Original Error: {e.__cause__}\\") if __name__ == \\"__main__\\": manage_library()"},{"question":"# Custom Event Loop Policy and Child Watcher Implementation Problem Statement You are tasked with creating a custom event loop policy and attaching a custom child watcher to it in Python using the `asyncio` library. Specifically, you must: 1. Create a custom event loop policy that logs each time an event loop is retrieved. 2. Create a custom child watcher that logs when a child process handler is added and removed. 3. Ensure that these custom implementations conform to the behaviors outlined in the `AbstractEventLoopPolicy` and `AbstractChildWatcher` classes, respectively. Task 1. **Custom Event Loop Policy**: - Subclass `asyncio.DefaultEventLoopPolicy` to create `LoggingEventLoopPolicy`. - Override `get_event_loop` method to log a message each time an event loop is gotten. - Ensure that `get_event_loop` returns a valid event loop object. 2. **Custom Child Watcher**: - Subclass `asyncio.ThreadedChildWatcher` to create `LoggingChildWatcher`. - Override `add_child_handler` method to log a message each time a handler is added. - Override `remove_child_handler` method to log a message each time a handler is removed. 3. **Setup**: - Set the process-wide policy to the custom `LoggingEventLoopPolicy`. - Set the current child watcher to the custom `LoggingChildWatcher`. Implementation Requirements 1. **LoggingEventLoopPolicy Class**: - Inherits from `asyncio.DefaultEventLoopPolicy`. - Logs a message each time `get_event_loop` is called. 2. **LoggingChildWatcher Class**: - Inherits from `asyncio.ThreadedChildWatcher`. - Logs messages for `add_child_handler` and `remove_child_handler`. 3. **Usage**: - Demonstrate the use of these classes in a simple asyncio program that spawns a few subprocesses and retrieves event loops. Input No direct input is required for the function implementations themselves. Use the classes in a script to showcase their functionality. Output The classes should produce log messages indicating when each significant event occurs (e.g., getting an event loop, adding/removing a child handler). Example Here is a basic outline to help you get started: ```python import asyncio class LoggingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Event loop retrieved\\") return loop class LoggingChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): super().add_child_handler(pid, callback, *args) print(f\\"Added child handler for PID {pid}\\") def remove_child_handler(self, pid): result = super().remove_child_handler(pid) print(f\\"Removed child handler for PID {pid}\\") return result # Set custom policy and watcher asyncio.set_event_loop_policy(LoggingEventLoopPolicy()) asyncio.get_event_loop_policy().get_child_watcher().attach_loop(asyncio.get_event_loop()) asyncio.set_child_watcher(LoggingChildWatcher()) # Example asyncio code with subprocesses async def main(): proc = await asyncio.create_subprocess_shell(\'echo \\"Hello World\\"\') await proc.wait() if __name__ == \\"__main__\\": asyncio.run(main()) ``` Your task is to fill in the remaining detailed implementation parts and ensure compliance with the provided requirements.","solution":"import asyncio class LoggingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Event loop retrieved\\") return loop class LoggingChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): super().add_child_handler(pid, callback, *args) print(f\\"Added child handler for PID {pid}\\") def remove_child_handler(self, pid): result = super().remove_child_handler(pid) print(f\\"Removed child handler for PID {pid}\\") return result # Set custom policy and watcher asyncio.set_event_loop_policy(LoggingEventLoopPolicy()) asyncio.set_child_watcher(LoggingChildWatcher()) # Example asyncio code with subprocesses async def main(): proc = await asyncio.create_subprocess_shell(\'echo \\"Hello World\\"\') await proc.wait() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question: Implementing a Custom GenericAlias Class In this task, you are required to implement a custom Python class similar to Python\'s `GenericAlias`. This class will help you understand type hinting and generic types. Your class should mimic the behavior of the `GenericAlias` object as described below. Task Implement a class `MyGenericAlias` which accepts an origin type and arguments during initialization and sets these as attributes. Additionally, ensure the following behavior: - If the provided arguments are not a tuple, automatically convert them into a single-element tuple. - The class should have attributes `__origin__`, `__args__`, and `__parameters__`. - The `__parameters__` attribute should be constructed lazily from `__args__`. Requirements 1. **Initialization**: - Constructor signature: `__init__(self, origin, args)` - Set `self.__origin__` to the provided origin. - If `args` is not a tuple, convert it to a single-element tuple and assign it to `self.__args__`. - If `args` is already a tuple, assign it directly to `self.__args__`. 2. **Lazy Attribute Construction**: - The `__parameters__` attribute should be constructed only when accessed, by processing `self.__args__`. Example Usage Here is how your class should be used and behave: ```python # Example of creating MyGenericAlias objects alias1 = MyGenericAlias(list, int) print(alias1.__origin__) # Output: <class \'list\'> print(alias1.__args__) # Output: (<class \'int\'>,) alias2 = MyGenericAlias(dict, (str, int)) print(alias2.__origin__) # Output: <class \'dict\'> print(alias2.__args__) # Output: (<class \'str\'>, <class \'int\'>) # Lazy construction of __parameters__ print(alias1.__parameters__) # Output: a constructed form from __args__ print(alias2.__parameters__) # Output: a constructed form from __args__ ``` Constraints - Handle different types of `origin` and `args`. - Performance should be considered; ensure lazy evaluation for the `__parameters__` attribute. Evaluation Your implementation will be evaluated on correctness, handling of edge cases, and performance. Ensure you follow good coding practices and provide comments where necessary.","solution":"class MyGenericAlias: def __init__(self, origin, args): self.__origin__ = origin if not isinstance(args, tuple): self.__args__ = (args,) else: self.__args__ = args self.__parameters__ = None @property def __parameters__(self): if self._parameters is None: self._parameters = self._construct_parameters() return self._parameters @__parameters__.setter def __parameters__(self, value): self._parameters = value def _construct_parameters(self): # Lazy construction of parameters based on __args__ return tuple(arg for arg in self.__args__)"},{"question":"**Title**: Implementing and Managing Cached Functions with `functools` **Objective**: Demonstrate your understanding of caching mechanisms provided by the `functools` module by implementing a project which uses `@functools.lru_cache` and `@functools.cache` decorators. **Problem Statement**: **Part 1**: Implement a Fibonacci function with an LRU cache. 1. Write a function `fib_lru(n)` that calculates the nth Fibonacci number using an `@functools.lru_cache` decorator. 2. The LRU cache should have a maxsize of 100. 3. The function should return the nth Fibonacci number. **Part 2**: Implement a factorial function with a simple cache. 1. Write a function `factorial_cache(n)` that calculates the factorial of `n` using an `@functools.cache` decorator. 2. The cache should store previously computed values without any size limit. 3. The function should return `n!`. **Part 3**: Testing and Evaluating Cache Performance 1. Write a function `evaluate_cache_performance()` that: * Tests both functions (`fib_lru` and `factorial_cache`) for multiple values. * Returns a dictionary with the cache performance metrics (hits, misses, maxsize, and currsize) for each function. 2. The output should be a dictionary of the form: ```python { \\"fib_lru\\": { \\"hits\\": int, \\"misses\\": int, \\"maxsize\\": int, \\"currsize\\": int }, \\"factorial_cache\\": { \\"hits\\": int, \\"misses\\": int, \\"maxsize\\": None, # Since this cache is unbounded \\"currsize\\": int } } ``` # Constraints: 1. Use Python 3.8 or higher. 2. The functions should only use the `functools` module for caching. 3. Ensure your functions handle large inputs efficiently by utilizing the caching mechanisms. **Input and Output Formats**: * `fib_lru(n: int) -> int` * `factorial_cache(n: int) -> int` * `evaluate_cache_performance() -> dict` # Example: ```python @lru_cache(maxsize=100) def fib_lru(n: int) -> int: if n < 2: return n return fib_lru(n - 1) + fib_lru(n - 2) @cache def factorial_cache(n: int) -> int: if n == 0: return 1 return n * factorial_cache(n - 1) def evaluate_cache_performance() -> dict: results = {} # Test fib_lru for values 10, 20, 30, 40 for i in [10, 20, 30, 40]: fib_lru(i) fib_info = fib_lru.cache_info() results[\'fib_lru\'] = { \\"hits\\": fib_info.hits, \\"misses\\": fib_info.misses, \\"maxsize\\": fib_info.maxsize, \\"currsize\\": fib_info.currsize } # Test factorial_cache for values 5, 10, 15, 20 for i in [5, 10, 15, 20]: factorial_cache(i) factorial_info = factorial_cache.cache_info() results[\'factorial_cache\'] = { \\"hits\\": factorial_info.hits, \\"misses\\": factorial_info.misses, \\"maxsize\\": None, \\"currsize\\": factorial_info.currsize } return results # Running example print(fib_lru(10)) print(factorial_cache(5)) print(evaluate_cache_performance()) ``` # Notes: 1. Make sure to test the functions individually and optimize for better performance by leveraging the caching mechanisms. 2. Thoroughly validate the output of `evaluate_cache_performance` to ensure the caching metrics are accurate.","solution":"import functools @functools.lru_cache(maxsize=100) def fib_lru(n: int) -> int: Returns the nth Fibonacci number using LRU cache with a max size of 100. if n < 2: return n return fib_lru(n - 1) + fib_lru(n - 2) @functools.cache def factorial_cache(n: int) -> int: Returns the factorial of n using a simple cache. if n == 0: return 1 return n * factorial_cache(n - 1) def evaluate_cache_performance() -> dict: results = {} # Test fib_lru for values 10, 20, 30, 40 for i in [10, 20, 30, 40]: fib_lru(i) fib_info = fib_lru.cache_info() results[\'fib_lru\'] = { \\"hits\\": fib_info.hits, \\"misses\\": fib_info.misses, \\"maxsize\\": fib_info.maxsize, \\"currsize\\": fib_info.currsize } # Test factorial_cache for values 5, 10, 15, 20 for i in [5, 10, 15, 20]: factorial_cache(i) factorial_info = factorial_cache.cache_info() results[\'factorial_cache\'] = { \\"hits\\": factorial_info.hits, \\"misses\\": factorial_info.misses, \\"maxsize\\": None, \\"currsize\\": factorial_info.currsize } return results"},{"question":"# Python Coding Assessment: Advanced Date and Time Manipulations Objective: To assess your understanding of the `datetime` module in Python, particularly in working with `datetime` objects, `timedelta`, and `timezone` classes. This question will test your ability to create functions that manipulate and utilize these classes effectively. **Problem Statement**: You\'ve been tasked to create a user-friendly scheduling application that requires the following functionality: 1. **Calculate age**: - Write a function `calculate_age(birth_date: datetime.date) -> int` that takes a birth date (date object) and returns the age in years. 2. **Time until next birthday**: - Write a function `time_until_next_birthday(birth_date: datetime.date) -> datetime.timedelta` that takes a birth date (date object) and returns a timedelta object representing the time until the next birthday from the current date. 3. **Convert between timezones**: - Write a function `convert_timezone(dt: datetime.datetime, new_timezone: datetime.timezone) -> datetime.datetime` that takes a naive or aware datetime object and a target timezone, returning the datetime in the new timezone. 4. **Format datetime**: - Write a function `format_date(dt: datetime.datetime, format_str: str) -> str` that takes a datetime object and a format string, returning the formatted date string according to the specified format. **Input and Output format**: 1. **calculate_age(birth_date: datetime.date) -> int** - **Input**: `birth_date` (date object in the format `YYYY-MM-DD`) - **Output**: An integer representing the age in years. 2. **time_until_next_birthday(birth_date: datetime.date) -> timedelta** - **Input**: `birth_date` (date object in the format `YYYY-MM-DD`) - **Output**: A `timedelta` object representing the time remaining until the next birthday. 3. **convert_timezone(dt: datetime.datetime, new_timezone: datetime.timezone) -> datetime.datetime** - **Input**: `dt` (datetime object with or without timezone), `new_timezone` (timezone object) - **Output**: A datetime object adjusted to the new timezone. 4. **format_date(dt: datetime.datetime, format_str: str) -> str** - **Input**: `dt` (datetime object), `format_str` (string as per the `strftime` format directives) - **Output**: A string representing the formatted date and time. **Examples**: ```python from datetime import datetime, date, timedelta, timezone # Example for calculate_age print(calculate_age(date(1990, 8, 14))) # Output will depend on the current date # Example for time_until_next_birthday print(time_until_next_birthday(date(1990, 8, 14))) # Output will be a timedelta object # Example for convert_timezone dt = datetime(2023, 10, 25, 12, 0, 0) # naive datetime new_tz = timezone(timedelta(hours=5, minutes=30)) # UTC+05:30 print(convert_timezone(dt, new_tz)) # Output will be datetime adjusted to UTC+05:30 # Example for format_date dt = datetime(2023, 10, 25, 12, 0, 0) format_string = \\"%Y-%m-%d %H:%M:%S\\" print(format_date(dt, format_string)) # Output: \'2023-10-25 12:00:00\' ``` **Constraints**: - Ensure that dates are valid Gregorian calendar dates. - `calculate_age` assumes birth date is never in the future. - The format string for `format_date` should follow the `strftime` and `strptime` behavior as per the documentation. - Take care of time zone transitions such as daylight saving time changes. Implement the four functions as specified above to complete the assessment question.","solution":"from datetime import datetime, date, timedelta, timezone def calculate_age(birth_date: date) -> int: Returns the age based on the birth date provided. today = date.today() age = today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day)) return age def time_until_next_birthday(birth_date: date) -> timedelta: Returns the time remaining until the next birthday. today = date.today() next_birthday = date(today.year, birth_date.month, birth_date.day) if today > next_birthday: next_birthday = date(today.year + 1, birth_date.month, birth_date.day) return next_birthday - today def convert_timezone(dt: datetime, new_timezone: timezone) -> datetime: Converts the input datetime to the specified new timezone. if dt.tzinfo is None: dt = dt.replace(tzinfo=timezone.utc) return dt.astimezone(new_timezone) def format_date(dt: datetime, format_str: str) -> str: Returns the formatted date string based on the format string provided. return dt.strftime(format_str)"},{"question":"# Advanced Functional Programming Challenge Scenario You are working on a data analysis project that involves processing large sequences of numbers. To ensure your solution is efficient and concise, you decide to utilize functional programming techniques provided by Python\'s `itertools`, `functools`, and `operator` modules. Task Write a Python function `process_and_filter_data(data, multiplier, adder)` that takes three parameters: 1. `data` (List[int]): A list of integers. 2. `multiplier` (int): An integer value to be used for multiplicative transformation. 3. `adder` (int): An integer value to be used for additive transformation. Your function should: 1. **Filter**: Remove any even numbers from the list. 2. **Transform**: For the remaining numbers, first multiply each number by the `multiplier` using `operator.mul`, and then add the `adder` using `operator.add`. 3. **Reduction**: Compute the cumulative product of these transformed numbers using `functools.reduce` with `operator.mul`. Constraints - The length of the data list will not exceed (10^5). - All numbers in the list are non-negative. - The function should be optimized for performance. Input Format ```python def process_and_filter_data(data: List[int], multiplier: int, adder: int) -> int: # Your implementation here ``` Output Format Returns the cumulative product (an integer) of the transformed numbers after filtering and transformation. Example ```python from typing import List from functools import reduce from itertools import filterfalse import operator def process_and_filter_data(data: List[int], multiplier: int, adder: int) -> int: # Step 1: Filter out even numbers filtered_data = filterfalse(lambda x: x % 2 == 0, data) # Step 2: Transform the remaining numbers transformed_data = map(lambda x: operator.add(operator.mul(x, multiplier), adder), filtered_data) # Step 3: Compute the cumulative product result = reduce(operator.mul, transformed_data, 1) # Start with the multiplicative identity return result # Example Usage: data = [1, 2, 3, 4, 5] multiplier = 2 adder = 3 print(process_and_filter_data(data, multiplier, adder)) # Output example, depending on the data and operations. ``` Explanation 1. The number list `[1, 2, 3, 4, 5]` first filters out the even numbers, resulting in `[1, 3, 5]`. 2. Each remaining number is then transformed: `1 * 2 + 3 = 5`, `3 * 2 + 3 = 9`, `5 * 2 + 3 = 13`. 3. Finally, their cumulative product is calculated: `5 * 9 * 13`. **Note**: The example provided in the question prompt may not be directly evaluated; you should test your implementation with various data inputs and edge cases.","solution":"from typing import List from functools import reduce from itertools import filterfalse import operator def process_and_filter_data(data: List[int], multiplier: int, adder: int) -> int: Processes and filters the data according to the specified rules. Parameters: data (List[int]): A list of integers. multiplier (int): An integer value to be used for multiplicative transformation. adder (int): An integer value to be used for additive transformation. Returns: int: The cumulative product of the transformed numbers after filtering. # Step 1: Filter out even numbers filtered_data = filterfalse(lambda x: x % 2 == 0, data) # Step 2: Transform the remaining numbers transformed_data = map(lambda x: operator.add(operator.mul(x, multiplier), adder), filtered_data) # Step 3: Compute the cumulative product result = reduce(operator.mul, transformed_data, 1) # Start with the multiplicative identity return result"},{"question":"# Coding Assessment **Objective**: Demonstrate your understanding of Python\'s asyncio module by implementing a task that performs multiple concurrent operations and manages task synchronization and timeouts. **Problem Statement**: You are required to implement an asynchronous simulation of a content delivery network (CDN). The CDN is designed to fetch pieces of data from different servers concurrently and combine them into a final result. Each piece of data has a different retrieval time, and for efficiency, we need to manage these retrievals using asyncio. **Function Signature**: ```python import asyncio async def fetch_piece(server_id: int, delay: int) -> str: Simulates fetching a piece of data from a server. Args: - server_id (int): The ID of the server to fetch from. - delay (int): The simulated time in seconds it takes to fetch the data. Returns: - str: The piece of data fetched from the server. The data fetched should be in the format \\"Data from server {server_id}\\". async def combine_results(servers: list, timeout: int) -> str: Fetches data from multiple servers and combines the results. Args: - servers (list): A list of tuples where each tuple contains a server ID and delay time. Example: [(1, 3), (2, 1), (3, 2)] - timeout (int): The maximum time to wait for all pieces to be fetched, in seconds. Returns: - str: The combined result of all fetched pieces, or a message indicating that the operation timed out. The combined result should be a string concatenation of all fetched data pieces. If the operation times out before fetching all pieces, return \\"Operation timed out\\". ``` **Instructions**: 1. Implement the `fetch_piece` function, which simulates fetching a piece of data from a server. This function should use `asyncio.sleep` to simulate the delay and return a string \\"Data from server {server_id}\\" once the delay is over. 2. Implement the `combine_results` function, which: - Takes a list of servers (each represented by a tuple of server ID and delay time) and a timeout value. - Concurrently fetches data from all the specified servers using the `fetch_piece` function. - Combines the results of all fetched pieces into a single string. - Ensures all tasks complete within the given timeout. If the timeout is reached before all pieces are fetched, it should return \\"Operation timed out\\". 3. Use `asyncio.gather` and `asyncio.wait_for` to manage the concurrent tasks and enforce the timeout. **Constraints**: - You must use the asyncio module to manage concurrency. - The `combine_results` function must handle the timeout properly. **Example**: ```python servers = [(1, 3), (2, 1), (3, 2)] timeout = 5 combined = await combine_results(servers, timeout) print(combined) # Expected: \\"Data from server 1Data from server 2Data from server 3\\" timeout = 2 combined = await combine_results(servers, timeout) print(combined) # Expected: \\"Operation timed out\\" ``` Your implementation should showcase competence in using asyncio\'s task management, timeouts, and concurrent execution of coroutines.","solution":"import asyncio async def fetch_piece(server_id: int, delay: int) -> str: Simulates fetching a piece of data from a server. Args: - server_id (int): The ID of the server to fetch from. - delay (int): The simulated time in seconds it takes to fetch the data. Returns: - str: The piece of data fetched from the server. The data fetched should be in the format \\"Data from server {server_id}\\". await asyncio.sleep(delay) return f\\"Data from server {server_id}\\" async def combine_results(servers: list, timeout: int) -> str: Fetches data from multiple servers and combines the results. Args: - servers (list): A list of tuples where each tuple contains a server ID and delay time. Example: [(1, 3), (2, 1), (3, 2)] - timeout (int): The maximum time to wait for all pieces to be fetched, in seconds. Returns: - str: The combined result of all fetched pieces, or a message indicating that the operation timed out. The combined result should be a string concatenation of all fetched data pieces. If the operation times out before fetching all pieces, return \\"Operation timed out\\". tasks = [fetch_piece(server_id, delay) for server_id, delay in servers] try: results = await asyncio.wait_for(asyncio.gather(*tasks), timeout) return \'\'.join(results) except asyncio.TimeoutError: return \\"Operation timed out\\""},{"question":"You are tasked with writing a Python script that simulates the process of integrating tracing functionality in a simplified, educational version of CPython. The goal is to understand how tracing of function calls and their returns could be implemented. **Objective**: Implement a tracing mechanism for Python functions using decorators. Your solution should annotate function entry and exit points in a similar manner to how DTrace/SystemTap might do. # Requirements: 1. **Decorator Implementation**: - Write a Python decorator `@trace` that: - Prints a message when a function is entered. - Prints a message when the function is about to return. - Includes the function name, filename, and line number of the call in the print messages. 2. **Application and Testing**: - Apply this decorator to multiple functions in a script. - Ensure that nested function calls are properly traced with appropriate indentation to reflect the call hierarchy. # Input Format: - The functions and the script where these functions will be executed. # Output Format: - Console output that shows the function call/return hierarchy including the function names, filenames, and line numbers. # Constraints: - Assume all functions are defined in a single file. - You may not use any special CPython or system-level debugging tools (Focus on pure Python implementation). # Example: ```python import inspect def trace(func): def wrapper(*args, **kwargs): frame = inspect.currentframe().f_back print(f\\"Entering {func.__name__} in {frame.f_code.co_filename} at line {frame.f_lineno}\\") result = func(*args, **kwargs) print(f\\"Exiting {func.__name__} in {frame.f_code.co_filename} at line {frame.f_lineno}\\") return result return wrapper @trace def start(): function_1() function_2() @trace def function_1(): function_3() @trace def function_2(): function_1() @trace def function_3(): pass start() ``` # Expected Output: ``` Entering start in <your_file_name>.py at line <line_number> Entering function_1 in <your_file_name>.py at line <line_number> Entering function_3 in <your_file_name>.py at line <line_number> Exiting function_3 in <your_file_name>.py at line <line_number> Exiting function_1 in <your_file_name>.py at line <line_number> Entering function_2 in <your_file_name>.py at line <line_number> Entering function_1 in <your_file_name>.py at line <line_number> Entering function_3 in <your_file_name>.py at line <line_number> Exiting function_3 in <your_file_name>.py at line <line_number> Exiting function_1 in <your_file_name>.py at line <line_number> Exiting function_2 in <your_file_name>.py at line <line_number> Exiting start in <your_file_name>.py at line <line_number> ``` **Note**: Ensure the filename and line numbers match the actual location in your script.","solution":"import inspect import os call_depth = 0 def trace(func): def wrapper(*args, **kwargs): global call_depth # Get calling frame for getting filename and line number frame = inspect.currentframe().f_back file_name = os.path.basename(frame.f_code.co_filename) line_no = frame.f_lineno # Print message before function execution print(f\\"{\' \' * call_depth}Entering {func.__name__} in {file_name} at line {line_no}\\") call_depth += 1 try: result = func(*args, **kwargs) finally: call_depth -= 1 print(f\\"{\' \' * call_depth}Exiting {func.__name__} in {file_name} at line {line_no}\\") return result return wrapper @trace def start(): function_1() function_2() @trace def function_1(): function_3() @trace def function_2(): function_1() @trace def function_3(): pass"},{"question":"# Principal Component Analysis (PCA) for Dimensionality Reduction **Objective:** Your task is to implement a function that applies Principal Component Analysis (PCA) for dimensionality reduction on a given dataset. You will be given a dataset with high-dimensional features, and you need to reduce the dimensionality to a specified number of components. **Function Specification:** ```python def apply_pca(data: np.ndarray, n_components: int) -> np.ndarray: Applies Principal Component Analysis (PCA) to reduce the dimensionality of the given dataset. Parameters: data (np.ndarray): A 2D array where each row represents a sample and each column represents a feature. n_components (int): The number of principal components to keep. Returns: np.ndarray: A 2D array of the transformed data with reduced dimensionality. pass ``` **Input:** - `data`: A 2D numpy array (shape: [num_samples, num_features]) representing the dataset with high-dimensional features. - `n_components`: An integer specifying the number of principal components to keep. **Output:** - A 2D numpy array with reduced dimensionality (shape: [num_samples, n_components]). **Constraints:** - The number of components `n_components` must be less than or equal to the number of original features in the dataset. - You may assume that the input data is centered (mean subtracted). **Example:** ```python import numpy as np # Sample data (4 samples, 3 features) data = np.array([[2.5, 2.4, 0.8], [0.5, 0.7, -0.1], [2.2, 2.9, 0.9], [1.9, 2.2, 1.1]]) n_components = 2 # Expected output is a 2D array with reduced dimensionality (4 samples, 2 components) reduced_data = apply_pca(data, n_components) print(reduced_data) ``` **Notes:** - You should use the `sklearn.decomposition.PCA` class to implement this function. - The function should handle errors gracefully, providing informative messages if the input is invalid. - Ensure that the output format is retained. **Scoring:** - Correct implementation of PCA. - Proper handling of edge cases and constraints. - Code efficiency and readability. Hint: Refer to the scikit-learn documentation for more details on the `PCA` class.","solution":"import numpy as np from sklearn.decomposition import PCA def apply_pca(data: np.ndarray, n_components: int) -> np.ndarray: Applies Principal Component Analysis (PCA) to reduce the dimensionality of the given dataset. Parameters: data (np.ndarray): A 2D array where each row represents a sample and each column represents a feature. n_components (int): The number of principal components to keep. Returns: np.ndarray: A 2D array of the transformed data with reduced dimensionality. if not isinstance(data, np.ndarray): raise ValueError(\\"Input data must be a numpy array\\") if not isinstance(n_components, int) or n_components <= 0: raise ValueError(\\"Number of components must be a positive integer\\") if n_components > data.shape[1]: raise ValueError(\\"Number of components must be less than or equal to the number of features\\") pca = PCA(n_components=n_components) transformed_data = pca.fit_transform(data) return transformed_data"},{"question":"**Objective:** The goal of this question is to assess your understanding of handling annotations in Python 3.10. You will be required to implement various functions that adhere to best practices for accessing and managing the `__annotations__` attribute. Problem Statement Implement a function `validate_and_get_annotations` that takes an object and returns its annotations dictionary. If the object does not support annotations or annotations are not defined, it should return an empty dictionary. Your implementation should follow the best practices provided in the documentation. Additionally, implement a function `unstringize_annotations` that takes an object and returns a dictionary with keys as the annotation names and values as the evaluated annotation types, if they are stringized. If unstringizing fails, the original string annotation should be retained. **Function Signatures:** ```python def validate_and_get_annotations(obj: Any) -> dict: pass def unstringize_annotations(obj: Any) -> dict: pass ``` **Input:** 1. `obj`: The object whose annotations need to be accessed. This can be any Python object such as a function, class, or module. **Output:** 1. For `validate_and_get_annotations` - A dictionary containing annotation names and their respective types. 2. For `unstringize_annotations` - A dictionary containing annotation names and their respective evaluated types, with unevaluated string annotations included. **Constraints:** 1. You should use `inspect.get_annotations()` for accessing annotations when possible. 2. Use appropriate methods to handle cases where annotations might not be defined or are inherited. 3. Avoid directly manipulating `__annotations__`. **Examples:** ```python def example_function(a: \'int\' = 10, b: \'str\' = \'hello\') -> \'None\': pass # Example usage of validate_and_get_annotations print(validate_and_get_annotations(example_function)) # Expected Output: {\'a\': \'int\', \'b\': \'str\', \'return\': \'None\'} # Example usage of unstringize_annotations print(unstringize_annotations(example_function)) # Expected Output: {\'a\': int, \'b\': str, \'return\': None} ``` **Notes:** - You might need to use the `eval` function to convert stringized annotations to actual types. - Handle any exceptions that might occur during `eval` gracefully, potentially logging or ignoring specific ones. Good luck!","solution":"import inspect def validate_and_get_annotations(obj): Return the annotations of an object if they exist, or an empty dictionary if they don\'t. :param obj: Any Python object such as a function, class, or module. :return: A dictionary containing annotation names and types. try: return inspect.get_annotations(obj) except AttributeError: return {} except TypeError: return {} def unstringize_annotations(obj): Evaluate string annotations to actual types if possible. :param obj: Any Python object such as a function, class, or module. :return: A dictionary containing annotation names and evaluated types. annotations = validate_and_get_annotations(obj) evaluated_annotations = {} for key, value in annotations.items(): if isinstance(value, str): try: evaluated_annotations[key] = eval(value, vars(obj)) except (NameError, SyntaxError): evaluated_annotations[key] = value else: evaluated_annotations[key] = value return evaluated_annotations"},{"question":"# Synchronization Challenge with Asyncio You have been tasked with managing an online shared resource platform where multiple users can interact with shared data. To ensure data integrity and proper synchronization between tasks, you will employ various `asyncio` synchronization primitives (`Lock`, `Event`, `Condition`, `Semaphore`). Task Write a Python script to simulate a scenario where multiple users can read from and write to a shared resource. The requirement is to use `asyncio` synchronization primitives to manage the access and modification of the shared resource. Requirements 1. Implement an `AsyncResourceManager` class that uses `asyncio.Lock` to provide exclusive access for writing to the shared resource. 2. Use `asyncio.Event` to signal when the shared resource is updated by any writer. 3. Implement functions `read_resource` and `write_resource` for reading and writing the shared resource respectively. 4. Simulate 3 writer tasks and 5 reader tasks: - Writers should wait for a random time between 1 to 3 seconds before writing to the resource. - Readers should continuously check for updates and print the new value of the resource when it changes. 5. Use `asyncio.Semaphore` to limit the number of concurrent readers to 3. Class and Function Definitions ```python import asyncio import random class AsyncResourceManager: def __init__(self): self.lock = asyncio.Lock() self.event = asyncio.Event() self.read_semaphore = asyncio.Semaphore(3) self.resource = None async def write_resource(self, value): async with self.lock: self.resource = value print(f\\"Resource written with value: {value}\\") self.event.set() async def read_resource(self): async with self.read_semaphore: while True: await self.event.wait() async with self.lock: print(f\\"Resource read with value: {self.resource}\\") # Clear the event and wait for the next update self.event.clear() break async def writer(manager, writer_id): while True: await asyncio.sleep(random.randint(1, 3)) value = f\\"Writer-{writer_id}: {random.randint(100, 999)}\\" await manager.write_resource(value) async def reader(manager, reader_id): while True: await manager.read_resource() await asyncio.sleep(random.uniform(0.1, 0.5)) async def main(): manager = AsyncResourceManager() writers = [asyncio.create_task(writer(manager, i)) for i in range(1, 4)] readers = [asyncio.create_task(reader(manager, i)) for i in range(1, 6)] await asyncio.gather(*writers, *readers) if __name__ == \\"__main__\\": asyncio.run(main()) ``` Submission Submit the script with the implemented `AsyncResourceManager` class and necessary coroutine functions. Ensure proper usage of `asyncio.Lock`, `asyncio.Event`, and `asyncio.Semaphore` to meet the requirements. Code correctness, efficiency, and adherence to asyncio best practices will be part of the evaluation criteria.","solution":"import asyncio import random class AsyncResourceManager: def __init__(self): self.lock = asyncio.Lock() self.event = asyncio.Event() self.read_semaphore = asyncio.Semaphore(3) self.resource = None async def write_resource(self, value): async with self.lock: self.resource = value print(f\\"Resource written with value: {value}\\") self.event.set() async def read_resource(self): async with self.read_semaphore: while True: await self.event.wait() async with self.lock: print(f\\"Resource read with value: {self.resource}\\") # Clear the event and wait for the next update self.event.clear() break async def writer(manager, writer_id): while True: await asyncio.sleep(random.randint(1, 3)) value = f\\"Writer-{writer_id}: {random.randint(100, 999)}\\" await manager.write_resource(value) async def reader(manager, reader_id): while True: await manager.read_resource() await asyncio.sleep(random.uniform(0.1, 0.5)) async def main(): manager = AsyncResourceManager() writers = [asyncio.create_task(writer(manager, i)) for i in range(1, 4)] readers = [asyncio.create_task(reader(manager, i)) for i in range(1, 6)] await asyncio.gather(*writers, *readers) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"<|Analysis Begin|> The `os` module in Python provides a way to use operating system-dependent functionality, which includes interaction with the file system, management of environment variables, process and thread functions, and many others. Key features documented here include: 1. **File and Directory operations**: Creating, changing, and querying directories, handling paths. 2. **Environment Variables**: Getting and setting environment variables. 3. **Process Management**: Forking processes, executing programs, sending signals. 4. **File Descriptor Operations**: Low-level file I/O operations. 5. **System Information**: Gathering information about the operating system. 6. **Random Numbers**: Generating random bytes. The documentation provides details on various functions associated with these features, exceptions that can be raised, as well as notes on platform-specific behaviors. Given the extensive functionality covered in the `os` module, a comprehensive coding assessment question should target one or more key functions, requiring the student to demonstrate their understanding of file and directory manipulation, environment variables, process management, or system information retrieval. <|Analysis End|> <|Question Begin|> # Python Coding Assessment: File Manipulation and Process Management Objective To evaluate your understanding of file and directory operations and process management using the `os` module in Python. Question Create a Python script that does the following: 1. **Read an Environment Variable**: - Read the environment variable `TARGET_DIR`. If it does not exist, print a message indicating this and exit the script. 2. **List Files and Directories**: - List all the files and subdirectories in the directory specified by `TARGET_DIR`. - For each file, print its name, size, and the last modification time. - For each directory, print its name and indicate that it is a directory. 3. **File Operations**: - Create a new file named `report.txt` in the `TARGET_DIR`. - Write a summary of the files and directories (name, type, size/last modification time) to `report.txt`. 4. **Process Management**: - Spawn a new process that lists all files in `report.txt` (use `os.spawnlp` or a similar function). Ensure the process waits for completion and properly handle any errors. - Print the process\'s exit code once it completes. Function Details You are free to create helper functions if necessary, but your script must have a clear structure and handle possible errors gracefully (e.g., non-existent directories, file access issues). Input and Output Formats - **Input**: The environment variable `TARGET_DIR`. - **Output**: - If the environment variable does not exist, print an error message. - Otherwise, list details of files and directories within `TARGET_DIR`. - Create a file `report.txt` in `TARGET_DIR` with directory contents summary. - Process and print the exit code of the spawned process. Constraints and Assumptions - Assume that the directory specified by `TARGET_DIR` is accessible and contains files and subdirectories. - Handle both relative and absolute paths for `TARGET_DIR`. Example 1. **Environment variable setup**: ```bash export TARGET_DIR=\'/path/to/directory\' ``` 2. **Expected Output** (print to the console and save to `report.txt`): ```plaintext Listing contents of directory: /path/to/directory Files: - file1.txt: size=1204 bytes, modified=2023-01-01 12:34:56 - file2.log: size=2048 bytes, modified=2023-01-02 13:47:30 Directories: - subdir1: directory - subdir2: directory Summary written to report.txt Spawned process exit code: 0 ``` Here is the expected script layout: ```python import os import time def main(): # Step 1: Read environment variable target_dir = os.getenv(\'TARGET_DIR\') if not target_dir: print(\\"Environment variable \'TARGET_DIR\' is not set. Exiting.\\") return # Step 2: List files and directories try: items = os.listdir(target_dir) file_details = [] dir_details = [] for item in items: item_path = os.path.join(target_dir, item) if os.path.isdir(item_path): dir_details.append(item) else: stat_info = os.stat(item_path) file_details.append((item, stat_info.st_size, time.ctime(stat_info.st_mtime))) # Print and prepare report content report_content = \\"Files:n\\" for file in file_details: print(f\\" - {file[0]}: size={file[1]} bytes, modified={file[2]}\\") report_content += f\\" - {file[0]}: size={file[1]} bytes, modified={file[2]}n\\" report_content += \\"Directories:n\\" for directory in dir_details: print(f\\" - {directory}: directory\\") report_content += f\\" - {directory}: directoryn\\" # Step 3: Write to report.txt report_path = os.path.join(target_dir, \'report.txt\') with open(report_path, \'w\') as report_file: report_file.write(report_content) print(f\\"Summary written to {report_path}\\") except Exception as e: print(f\\"An error occurred: {e}\\") return # Step 4: Spawn new process to list report contents try: exit_code = os.spawnlp(os.P_WAIT, \'cat\', \'cat\', report_path) print(f\\"Spawned process exit code: {exit_code}\\") except Exception as e: print(f\\"Failed to spawn process: {e}\\") if __name__ == \\"__main__\\": main() ```","solution":"import os import time def list_directory_contents(target_dir): try: items = os.listdir(target_dir) file_details = [] dir_details = [] for item in items: item_path = os.path.join(target_dir, item) if os.path.isdir(item_path): dir_details.append(item) else: stat_info = os.stat(item_path) file_details.append((item, stat_info.st_size, time.ctime(stat_info.st_mtime))) return file_details, dir_details except FileNotFoundError: print(f\\"Directory \'{target_dir}\' does not exist.\\") return [], [] except PermissionError: print(f\\"Permission denied to access directory \'{target_dir}\'.\\") return [], [] except Exception as e: print(f\\"An error occurred: {e}\\") return [], [] def write_report(target_dir, file_details, dir_details): report_path = os.path.join(target_dir, \'report.txt\') with open(report_path, \'w\') as report_file: report_file.write(\\"Files:n\\") for file in file_details: report_file.write(f\\" - {file[0]}: size={file[1]} bytes, modified={file[2]}n\\") report_file.write(\\"Directories:n\\") for directory in dir_details: report_file.write(f\\" - {directory}: directoryn\\") return report_path def main(): # Step 1: Read environment variable target_dir = os.getenv(\'TARGET_DIR\') if not target_dir: print(\\"Environment variable \'TARGET_DIR\' is not set. Exiting.\\") return # Step 2: List files and directories file_details, dir_details = list_directory_contents(target_dir) # Print details print(\\"nFiles:\\") for file in file_details: print(f\\" - {file[0]}: size={file[1]} bytes, modified={file[2]}\\") print(\\"Directories:\\") for directory in dir_details: print(f\\" - {directory}: directory\\") # Step 3: Write to report.txt report_path = write_report(target_dir, file_details, dir_details) print(f\\"Summary written to {report_path}\\") # Step 4: Spawn new process to list report contents try: exit_code = os.spawnlp(os.P_WAIT, \'cat\', \'cat\', report_path) print(f\\"Spawned process exit code: {exit_code}\\") except OSError as e: print(f\\"Failed to spawn process: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Class Assessment Problem Statement You are required to design a Python class that models a simple library system. The library should be able to: 1. Add new books. 2. Remove books by their title. 3. Get the list of all books. 4. Iterate over the books. Each book should have the following attributes: - `title` (string) - `author` (string) - `publication_year` (integer) - `available` (boolean) - indicating if the book is available for borrowing or not. Use class variables to keep track of the total number of books and the number of available books. Requirements 1. Define a class `Book` representing individual books with the above attributes. 2. Define a class `Library` which has: - A class variable to keep track of the total number of books in the library. - A class variable to keep track of the number of available books. - An instance variable for storing a list of books. 3. Implement the following methods for the `Library` class: - `add_book(self, book: Book)`: Adds a new book to the library. - `remove_book(self, title: str) -> bool`: Removes a book by title. It should return `True` if the removal was successful, otherwise `False`. - `get_books(self) -> list[Book]`: Returns a list of all books in the library. - `__iter__(self)`: Makes the library class iterable. - A private method for updating the available books count whenever a book is added or removed. 4. Use a generator expression inside some method of the `Library` class to illustrate its ability to process the library\'s book list. 5. Ensure any direct use of class variables from outside the class should follow proper encapsulation principles making use of private methods or variables where necessary. Input and Output - You don\'t have to handle user input or output, just ensure the classes and methods are correctly defined. - See the example below for usage. Example Here is an example usage of the `Library` and `Book` classes: ```python # Define Book and Library classes as per requirements here. # Sample books book1 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, True) book2 = Book(\\"1984\\", \\"George Orwell\\", 1949, True) book3 = Book(\\"Moby Dick\\", \\"Herman Melville\\", 1851, False) # Create Library instance and add books library = Library() library.add_book(book1) library.add_book(book2) library.add_book(book3) # Get list of all books books = library.get_books() for book in books: print(book.title) # Remove a book by title if library.remove_book(\\"1984\\"): print(\\"Book removed successfully\\") else: print(\\"Book not found\\") # Iterate over books in the library for book in library: print(f\\"{book.title} by {book.author}\\") ``` Ensure your code adheres to the principles outlined and tests the behavior expected as described.","solution":"class Book: def __init__(self, title, author, publication_year, available=True): self.title = title self.author = author self.publication_year = publication_year self.available = available class Library: total_books = 0 total_available_books = 0 def __init__(self): self.books = [] def _update_counts(self): Library.total_books = len(self.books) Library.total_available_books = sum(book.available for book in self.books) def add_book(self, book): self.books.append(book) self._update_counts() def remove_book(self, title): for i, book in enumerate(self.books): if book.title == title: del self.books[i] self._update_counts() return True return False def get_books(self): return self.books def __iter__(self): return (book for book in self.books)"},{"question":"Write a function `organize_files_by_extension(input_dir)` that organizes files in a given directory by their extensions. For each type of extension found in the `input_dir`, create a sub-directory named after the extension (without the leading dot), and move all files of that extension into the corresponding sub-directory. If a sub-directory for an extension already exists, simply move the files into it without creating a new one. Ensure no files are overwritten in this operation. Function Signature ```python def organize_files_by_extension(input_dir: str) -> None: pass ``` Input - `input_dir` (str): A string representing the full path of the input directory that contains the files to be organized. Output - The function does not need to return any value. The `input_dir` should be modified in place. Constraints - The input directory will only contain files (no subdirectories). - The function should handle both relative and absolute paths. - You can assume that the input directory will always exist and you have permissions to read and write within it. Example Assume `input_dir` contains the following files: ``` file1.txt file2.txt file3.jpg file4.png file5.png file6 ``` After calling `organize_files_by_extension(input_dir)`, the directory structure should be: ``` txt/ file1.txt file2.txt jpg/ file3.jpg png/ file4.png file5.png file6 ``` Notes - Use functions from the `os.path` module to handle paths. - Handle exceptions and edge cases gracefully, such as if a file has no extension.","solution":"import os import shutil def organize_files_by_extension(input_dir: str) -> None: Organizes files in the given directory by their extensions. # List all files in the given directory files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))] for file in files: # Split the filename into name and extension base, ext = os.path.splitext(file) # Skip files without an extension if not ext: continue # Remove the leading dot from the extension for the directory name ext_dir = ext.lstrip(\'.\') # Create the target directory if it doesn\'t exist target_dir = os.path.join(input_dir, ext_dir) if not os.path.exists(target_dir): os.makedirs(target_dir) # Move the file to the target directory src_path = os.path.join(input_dir, file) dest_path = os.path.join(target_dir, file) # Ensure no file is overwritten if not os.path.exists(dest_path): shutil.move(src_path, dest_path)"},{"question":"# Question: Signal Processing with PyTorch Windows You are given a 1-dimensional PyTorch tensor representing a signal. Your task is to apply different window functions from the `torch.signal.windows` module to this signal and analyze the results. Specifically, you need to perform the following steps: 1. Implement a function `apply_window(signal: torch.Tensor, window_type: str, **kwargs) -> torch.Tensor` that takes in a signal tensor and a window type as inputs. The window type is a string that can be one of the following: \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'hamming\', \'hann\', \'kaiser\', or \'nuttall\'. Depending on the window type, apply the corresponding window function to the signal. The `kwargs` parameter allows for any additional arguments required by specific window functions (e.g., the `beta` parameter for the `kaiser` window). 2. Implement a function `analyze_window_effect(signal: torch.Tensor, window_type: str, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]` that applies the specified window to the signal using the `apply_window` function and then computes the Fourier Transform of the original and windowed signals. The function should return the Fourier Transforms of both the original and the windowed signals. 3. Use the implemented functions to analyze the effect of each window function on a sample signal. Plot the original signal, the windowed signal, and their respective Fourier Transforms for at least three different window types from the list provided. Expected Input and Output Formats - `apply_window(signal: torch.Tensor, window_type: str, **kwargs) -> torch.Tensor`: - `signal`: A 1-dimensional PyTorch tensor of shape `(n,)`. - `window_type`: A string specifying the type of window to apply. - `kwargs`: Additional parameters required for specific window functions. - **Returns**: A 1-dimensional PyTorch tensor of shape `(n,)` representing the windowed signal. - `analyze_window_effect(signal: torch.Tensor, window_type: str, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]`: - `signal`: A 1-dimensional PyTorch tensor of shape `(n,)`. - `window_type`: A string specifying the type of window to apply. - `kwargs`: Additional parameters required for specific window functions. - **Returns**: A tuple containing two 1-dimensional PyTorch tensors of shape `(n,)` representing the Fourier Transforms of the original and the windowed signals. Constraints - You should handle edge cases such as invalid window types or inappropriate additional parameters by raising appropriate exceptions. - You are allowed to use PyTorch\'s `torch.fft` module for computing Fourier Transforms. Performance Requirements - The implementation should be efficient and leverage PyTorch\'s tensor operations to the fullest extent possible. - Aim to minimize unnecessary data copying and transformations to ensure good performance on larger signals. Example Usage ```python import torch import torch.signal.windows as windows import matplotlib.pyplot as plt # Sample signal: a simple sine wave n = 1024 t = torch.linspace(0, 1, steps=n) signal = torch.sin(2 * torch.pi * 10 * t) # Apply Hamming window windowed_signal = apply_window(signal, \'hamming\') # Analyze effect original_fft, windowed_fft = analyze_window_effect(signal, \'hamming\') # Plot results plt.figure(figsize=(12, 8)) plt.subplot(3, 1, 1) plt.plot(signal.numpy(), label=\'Original Signal\') plt.legend() plt.subplot(3, 1, 2) plt.plot(windowed_signal.numpy(), label=\'Windowed Signal (Hamming)\') plt.legend() plt.subplot(3, 1, 3) plt.plot(original_fft.abs().numpy(), label=\'Original Signal FFT\') plt.plot(windowed_fft.abs().numpy(), label=\'Windowed Signal FFT (Hamming)\') plt.legend() plt.show() ``` Complete the functions `apply_window` and `analyze_window_effect` as described to achieve this functionality.","solution":"import torch from typing import Any, Tuple def apply_window(signal: torch.Tensor, window_type: str, **kwargs) -> torch.Tensor: Applies a specified window function to the input signal. Parameters: signal (torch.Tensor): The input 1-dimensional signal tensor. window_type (str): The type of window function to apply. kwargs (Any): Additional arguments required for specific window functions. Returns: torch.Tensor: The windowed signal. n = len(signal) if window_type == \'bartlett\': window = torch.signal.windows.bartlett(n) elif window_type == \'blackman\': window = torch.signal.windows.blackman(n) elif window_type == \'cosine\': window = torch.signal.windows.cosine(n) elif window_type == \'exponential\': window = torch.signal.windows.exponential(n, **kwargs) elif window_type == \'gaussian\': window = torch.signal.windows.gaussian(n, **kwargs) elif window_type == \'hamming\': window = torch.signal.windows.hamming(n) elif window_type == \'hann\': window = torch.signal.windows.hann(n) elif window_type == \'kaiser\': window = torch.signal.windows.kaiser(n, **kwargs) elif window_type == \'nuttall\': window = torch.signal.windows.nuttall(n) else: raise ValueError(f\\"Invalid window type: {window_type}\\") return signal * window def analyze_window_effect(signal: torch.Tensor, window_type: str, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]: Applies a specified window to the input signal and computes the Fourier Transform. Parameters: signal (torch.Tensor): The input 1-dimensional signal tensor. window_type (str): The type of window function to apply. kwargs (Any): Additional arguments required for specific window functions. Returns: Tuple[torch.Tensor, torch.Tensor]: Two tensors containing the Fourier Transform of the original and windowed signal. windowed_signal = apply_window(signal, window_type, **kwargs) original_fft = torch.fft.fft(signal) windowed_fft = torch.fft.fft(windowed_signal) return original_fft, windowed_fft"},{"question":"Objective You are required to write a function that takes a batch of square matrices and performs multiple linear algebra operations, using PyTorch\'s `torch.linalg` module. Problem Statement Write a function `batch_matrix_operations` that performs the following operations on each matrix in the given batch: 1. **Matrix Norm**: Compute the Frobenius norm of each matrix. 2. **Matrix Inverse**: Compute the inverse of each matrix. If a matrix is singular (non-invertible), handle the case gracefully by returning a zero matrix of the same shape. 3. **Eigenvalues**: Compute the eigenvalues of each matrix. 4. **QR Decomposition**: Perform a QR decomposition of each matrix. Return both the Q and R matrices. Input - A 3D tensor of shape `(batch_size, n, n)` representing `batch_size` number of `n x n` matrices. Output A tuple containing: 1. A 1D tensor of shape `(batch_size,)` containing the Frobenius norm of each matrix. 2. A 3D tensor of shape `(batch_size, n, n)` containing the inverse of each matrix (zero matrix if not invertible). 3. A 2D tensor of shape `(batch_size, n)` containing the eigenvalues of each matrix. 4. Two 3D tensors of shape `(batch_size, n, n)` containing the Q and R matrices obtained from the QR decomposition of each matrix. Function Signature ```python import torch def batch_matrix_operations(matrices: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: pass ``` Example ```python import torch input_matrices = torch.randn((2, 3, 3)) norms, inverses, eigenvalues, Q_matrices, R_matrices = batch_matrix_operations(input_matrices) print(\\"Norms:\\", norms) print(\\"Inverses:\\", inverses) print(\\"Eigenvalues:\\", eigenvalues) print(\\"Q matrices:\\", Q_matrices) print(\\"R matrices:\\", R_matrices) ``` **Constraints**: - You can assume the input will always be a batch of square matrices. - You should use PyTorch\'s `torch.linalg` module for all operations. - Handle singular matrix cases efficiently without causing exceptions. Performance Requirements Your implementation should be able to handle batches of up to `1000` matrices of size `100 x 100` efficiently.","solution":"import torch from typing import Tuple def batch_matrix_operations(matrices: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: batch_size, n, _ = matrices.shape # Computing the Frobenius norm for each matrix norms = torch.linalg.norm(matrices, ord=\'fro\', dim=(1, 2)) # Initializing tensors for results inverses = torch.zeros_like(matrices) eigenvalues = torch.zeros(batch_size, n) Q_matrices = torch.zeros_like(matrices) R_matrices = torch.zeros_like(matrices) for i in range(batch_size): matrix = matrices[i] try: inv_matrix = torch.linalg.inv(matrix) except torch.linalg.LinAlgError: inv_matrix = torch.zeros_like(matrix) eigen_vals = torch.linalg.eigvals(matrix).real Q, R = torch.linalg.qr(matrix) # Storing the results in appropriate tensors inverses[i] = inv_matrix eigenvalues[i] = eigen_vals Q_matrices[i] = Q R_matrices[i] = R return norms, inverses, eigenvalues, Q_matrices, R_matrices"},{"question":"# Unique User Session Identifier Generator In a distributed web application, each user session must be uniquely identifiable across multiple servers to ensure data consistency and avoid collisions. Your task is to implement a unique identifier generator using the `uuid` module. The generator should support generating different versions of UUIDs and provide information about the generated UUIDs. You need to implement a function `generate_session_id(version, *args, **kwargs)` that generates a UUID based on the specified version and additional arguments. The function details are as follows: Function Signature ```python def generate_session_id(version: int, *args, **kwargs) -> dict: ``` Parameters - `version` (int): The UUID version to generate (1, 3, 4, or 5). - `*args` and `**kwargs`: Additional arguments required for generating the specific version of UUID. Returns - A dictionary with the following keys: - `uuid`: The generated UUID (as a string). - `version`: The version of the UUID (as an integer). - `is_safe`: Indicates whether the UUID was generated in a multiprocessing-safe way (as a string from \'safe\', \'unsafe\', \'unknown\'). - (`namespace` and `name` keys should also be included for version 3 and 5 UUIDs to return the provided namespace and name used in generation). Constraints - For `version` 1, no additional arguments are needed. - For `version` 3 and `version` 5, `args` should include a `namespace` (one of `uuid.NAMESPACE_DNS`, `uuid.NAMESPACE_URL`, `uuid.NAMESPACE_OID`, `uuid.NAMESPACE_X500`) and `name` (a string). - For `version` 4, no additional arguments are needed. Example Usage ```python # Example 1: Generate a version 1 UUID result = generate_session_id(1) assert result[\'version\'] == 1 # Example 2: Generate a version 3 UUID result = generate_session_id(3, uuid.NAMESPACE_DNS, \'example.com\') assert result[\'version\'] == 3 assert result[\'namespace\'] == uuid.NAMESPACE_DNS assert result[\'name\'] == \'example.com\' # Example 3: Generate a version 4 UUID result = generate_session_id(4) assert result[\'version\'] == 4 # Example 4: Generate a version 5 UUID result = generate_session_id(5, uuid.NAMESPACE_URL, \'https://example.com\') assert result[\'version\'] == 5 assert result[\'namespace\'] == uuid.NAMESPACE_URL assert result[\'name\'] == \'https://example.com\' ``` Notes Use the `uuid` module as described in the provided documentation to implement the function. Make sure to handle invalid `version` inputs gracefully by raising a `ValueError`.","solution":"import uuid def generate_session_id(version: int, *args, **kwargs) -> dict: Generates a UUID based on the specified version and additional arguments. Parameters: - version (int): The UUID version to generate (1, 3, 4, or 5). - *args: Additional arguments required for generating the specific version of UUID. Returns: - A dictionary with the generated UUID and additional information. if version not in (1, 3, 4, 5): raise ValueError(\\"Invalid UUID version. Only versions 1, 3, 4, and 5 are supported.\\") if version == 1: generated_uuid = uuid.uuid1() elif version == 3: if len(args) < 2: raise ValueError(\\"UUID version 3 requires a namespace and a name.\\") namespace, name = args[0], args[1] generated_uuid = uuid.uuid3(namespace, name) elif version == 4: generated_uuid = uuid.uuid4() elif version == 5: if len(args) < 2: raise ValueError(\\"UUID version 5 requires a namespace and a name.\\") namespace, name = args[0], args[1] generated_uuid = uuid.uuid5(namespace, name) result = { \'uuid\': str(generated_uuid), \'version\': generated_uuid.version, \'is_safe\': str(generated_uuid.is_safe) } if version in (3, 5): result[\'namespace\'] = namespace result[\'name\'] = name return result"},{"question":"**Objective**: Implement and manipulate bytearrays using the given functions to demonstrate comprehension of memory management and buffer protocols in Python. # Problem Statement: Write a Python function that performs the following tasks: 1. **Create a ByteArray**: - Accept a string input and convert it to a bytearray using the appropriate API function. 2. **Concatenate Byte Arrays**: - Accept another string input and convert it to a bytearray. - Concatenate the new bytearray to the original bytearray. 3. **Resize Byte Array**: - Resize the concatenated bytearray to a specified length. 4. **Extract Substring**: - Extract and return a substring from the resized bytearray as a normal string. # Function Signature: ```python def manipulate_bytearray(initial_string: str, add_string: str, resize_length: int, start: int, end: int) -> str: initial_string : str : The initial string to be converted to a bytearray. add_string : str : The string to be converted to a bytearray and concatenated. resize_length : int : The length to which the concatenated bytearray is resized. start : int : The start index (inclusive) for extracting the substring. end : int : The end index (exclusive) for extracting the substring. Returns ------- str : The extracted substring from the resized bytearray. ``` # Constraints: - All input strings will be ASCII strings. - The resize length will not be smaller than the substring indices. - The substring indices will be valid with respect to the resized bytearray length. # Example: ```python # Example usage of the function. initial_string = \\"Hello\\" add_string = \\" World\\" resize_length = 15 start = 3 end = 8 result = manipulate_bytearray(initial_string, add_string, resize_length, start, end) print(result) # Expected output: lo Wo ``` # Requirements: 1. The function should use the provided bytearray API functions to perform the tasks. 2. Handle memory management and type checks appropriately within the function. 3. The solution should not use any imports or standard Python bytearray methods outside the ones provided in the documentation. Provide a well-documented solution covering the above requirements, ensuring code readability and following best practices for error handling and type checking.","solution":"def manipulate_bytearray(initial_string: str, add_string: str, resize_length: int, start: int, end: int) -> str: initial_string : str : The initial string to be converted to a bytearray. add_string : str : The string to be converted to a bytearray and concatenated. resize_length : int : The length to which the concatenated bytearray is resized. start : int : The start index (inclusive) for extracting the substring. end : int : The end index (exclusive) for extracting the substring. Returns ------- str : The extracted substring from the resized bytearray. # Step 1: Convert the initial string to a bytearray. byte_array = bytearray(initial_string, \'ascii\') # Step 2: Convert the additional string to a bytearray and concatenate. add_byte_array = bytearray(add_string, \'ascii\') byte_array.extend(add_byte_array) # Step 3: Resize the concatenated bytearray. byte_array = byte_array[:resize_length] # Step 4: Extract and return the substring from the resized bytearray. substring = byte_array[start:end] return substring.decode(\'ascii\')"},{"question":"# PyTorch Multiprocessing Task **Objective**: Implement a parallel processing function using `torch.multiprocessing` to distribute computational tasks across multiple processes and efficiently share the results. **Task Description**: You are required to: 1. Create a function `parallel_square_sum` that computes the square of each element in a given tensor and returns the sum of these squares. 2. This computation must be executed in parallel using multiple processes. 3. Properly handle the shared tensors between processes to ensure they are not leaked or held longer than necessary. **Function Signature**: ```python import torch import torch.multiprocessing as mp def parallel_square_sum(tensor: torch.Tensor, num_processes: int) -> float: pass ``` **Input**: - `tensor` (torch.Tensor): A 1-dimensional tensor containing the input data. - `num_processes` (int): The number of processes to spawn for parallel computation. **Output**: - `float`: The sum of the squares of each element in the input tensor. **Constraints**: - Use the `spawn` start method for creating subprocesses. - Ensure efficient memory management, avoiding any resource leaks. - Use Python 3 and ensure code compatibility with CUDA tensors. # Example Usage ```python tensor = torch.tensor([1.0, 2.0, 3.0, 4.0]) result = parallel_square_sum(tensor, 2) print(result) # Expected output: 30.0 (1^2 + 2^2 + 3^2 + 4^2 = 30) ``` # Guidelines 1. **Subprocess Management**: - Split the tensor into chunks for each subprocess. - Perform the square and sum operations within each subprocess. - Use `torch.multiprocessing.Queue` for aggregating results from subprocesses. 2. **Memory Management**: - Create tensor clones as needed to prevent direct sharing issues. - Properly clean up shared resources after computation. 3. **Error Handling**: - Handle and propagate subprocess errors gracefully. - Ensure that the main process waits for all subprocesses to complete. # Hints - Use `mp.Queue` to collect results from subprocesses. - For best memory management practices, refer to the section on sharing CUDA tensors in the PyTorch multiprocessing documentation.","solution":"import torch import torch.multiprocessing as mp def worker(tensor_chunk, queue): Worker function to compute the square and then the sum for a chunk of tensor. sum_of_squares = torch.sum(tensor_chunk ** 2).item() queue.put(sum_of_squares) def parallel_square_sum(tensor: torch.Tensor, num_processes: int) -> float: if num_processes < 1: raise ValueError(\\"num_processes must be at least 1\\") # Initialize a queue to collect results from subprocesses queue = mp.Queue() processes = [] tensor_chunks = tensor.chunk(num_processes) # Spawn processes for chunk in tensor_chunks: p = mp.Process(target=worker, args=(chunk, queue)) p.start() processes.append(p) # Collect results from all processes total_sum = 0.0 for _ in range(num_processes): total_sum += queue.get() # Ensure all processes have finished for p in processes: p.join() return total_sum # Uncomment the below lines to test the function manually. # tensor = torch.tensor([1.0, 2.0, 3.0, 4.0]) # result = parallel_square_sum(tensor, 2) # print(result) # Should print: 30.0 (1^2 + 2^2 + 3^2 + 4^2)"},{"question":"# Question **Objective**: Design and implement an asynchronous chat server using Python\'s asyncio package. This server should manage multiple clients, handle messages asynchronously, and include custom error handling. **Requirements**: 1. **Server Initialization**: The server must start an event loop and accept connections on a specified host and port. 2. **Client Handling**: Each client connection should be managed in a separate asynchronous task. Connections should be kept open until the client disconnects. 3. **Message Broadcast**: When a client sends a message, the message should be broadcasted to all connected clients. 4. **Error Handling**: Implement a custom exception handler for the event loop. This handler should log the error details and continue running the server. 5. **Thread Pool Executor**: Use a thread pool executor to offload the logging of messages to a separate thread to simulate a blocking I/O operation. **Implementation Details**: - **Input**: - `host` (string): The hostname for the server to bind to. - `port` (int): The port number for the server to listen on. - **Output**: - None. The server will continue running until an external stop signal (like Ctrl+C) is received. **Constraints**: - Use Python 3.7 or higher. - The server should handle at least 10 concurrent client connections. # Function Signature ```python import asyncio from concurrent.futures import ThreadPoolExecutor async def start_server(host: str, port: int): # Your implementation here pass ``` # Example Usage ```python if __name__ == \'__main__\': # Starting the server on localhost and port 12345 asyncio.run(start_server(\'127.0.0.1\', 12345)) ``` **Hints**: - Use `loop.create_server()` to initialize the server. - Use `loop.run_in_executor()` to offload logging to a separate thread. - Implement a custom exception handler using `loop.set_exception_handler()`. # Evaluation Criteria - Correct implementation of the event loop and server initialization. - Proper handling of client connections and message broadcasting. - Effective use of thread pool executor for logging. - Robust custom exception handler that logs exceptions without stopping the server.","solution":"import asyncio from concurrent.futures import ThreadPoolExecutor class ChatServer: def __init__(self, host, port): self.host = host self.port = port self.clients = [] async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"New connection from {addr}\\") self.clients.append(writer) try: while True: data = await reader.read(100) if not data: break message = data.decode() await self.broadcast(message, writer) except Exception as e: print(f\\"Error: {e}\\") finally: print(f\\"Connection closed from {addr}\\") self.clients.remove(writer) writer.close() await writer.wait_closed() async def broadcast(self, message, writer): for client in self.clients: if client != writer: client.write(message.encode()) await client.drain() async def start_server(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) addr = server.sockets[0].getsockname() print(f\\"Serving on {addr}\\") async with server: await server.serve_forever() async def start_server(host: str, port: int): chat_server = ChatServer(host, port) await chat_server.start_server() if __name__ == \'__main__\': try: asyncio.run(start_server(\'127.0.0.1\', 12345)) except KeyboardInterrupt: print(\\"Server stopped manually.\\")"},{"question":"**Question: Secure Password Generator** You are tasked with creating a function that generates a secure random password meeting specific criteria using the \\"secrets\\" module in Python. **Function Signature:** ```python def generate_secure_password(length: int, min_lower: int, min_upper: int, min_digits: int, min_special: int) -> str: pass ``` **Input:** - `length` (int): The total length of the password. - `min_lower` (int): The minimum number of lowercase alphabetic characters required in the password. - `min_upper` (int): The minimum number of uppercase alphabetic characters required in the password. - `min_digits` (int): The minimum number of digit characters required in the password. - `min_special` (int): The minimum number of special characters required in the password. Special characters are considered to be `!@#%^&*()-_=+[]{}|;:,.<>?/`. **Output:** - Returns a string representing the generated password. **Constraints:** - The sum of `min_lower`, `min_upper`, `min_digits`, and `min_special` must be less than or equal to `length`. - The password length must be greater than or equal to 1. - The password must be randomly generated using the \\"secrets\\" module to ensure cryptographic security. **Example:** ```python assert len(generate_secure_password(12, 2, 2, 2, 2)) == 12 assert any(c.islower() for c in generate_secure_password(12, 2, 2, 2, 2)) assert any(c.isupper() for c in generate_secure_password(12, 2, 2, 2, 2)) assert any(c.isdigit() for c in generate_secure_password(12, 2, 2, 2, 2)) assert any(c in \'!@#%^&*()-_=+[]{}|;:,.<>?/\' for c in generate_secure_password(12, 2, 2, 2, 2)) ``` **Notes:** - Ensure that the function uses the \\"secrets\\" module exclusively for generating the random characters. - The remaining number of characters (after accounting for the minimum requirements) should be randomly chosen from all valid characters.","solution":"import secrets import string def generate_secure_password(length: int, min_lower: int, min_upper: int, min_digits: int, min_special: int) -> str: Generates a secure password of the given length containing at least the specified number of lowercase letters, uppercase letters, digits, and special characters. if (min_lower + min_upper + min_digits + min_special) > length or length < 1: raise ValueError(\\"Invalid input requirements: total character requirements exceed length or length is less than 1.\\") # Character pools lower_chars = string.ascii_lowercase upper_chars = string.ascii_uppercase digit_chars = string.digits special_chars = \\"!@#%^&*()-_=+[]{}|;:,.<>?/\\" # Generating required numbers of each type of character password_characters = [ secrets.choice(lower_chars) for _ in range(min_lower) ] + [ secrets.choice(upper_chars) for _ in range(min_upper) ] + [ secrets.choice(digit_chars) for _ in range(min_digits) ] + [ secrets.choice(special_chars) for _ in range(min_special) ] # Generating remaining characters remaining_length = length - len(password_characters) all_chars = lower_chars + upper_chars + digit_chars + special_chars password_characters += [ secrets.choice(all_chars) for _ in range(remaining_length) ] # Shuffle to ensure randomness secrets.SystemRandom().shuffle(password_characters) return \'\'.join(password_characters)"},{"question":"You are required to implement a Python function that fetches the HTML content from a list of URLs, handles HTTP basic authentication if needed, and returns the clean text (no HTML tags) from the fetched content. You will use the `urllib.request` module for these operations. # Function Signature: ```python def fetch_clean_text_from_urls(urls: list, auth: dict = None) -> dict: pass ``` # Parameters: - `urls` (list): A list of URLs (strings) to fetch data from. - `auth` (dict): A dictionary containing authentication information with keys `realm`, `uri`, `user`, and `passwd`. Defaults to `None`. # Returns: - A dictionary where the keys are the URLs and the values are the clean text content retrieved from those URLs. # Constraints: - Handling only HTTP and HTTPS URLs. - If authentication details are provided, all URLs should use the same authentication credentials. - You must handle any HTTP errors gracefully and return an empty string for URLs that resulted in errors. - For simplicity, assume the encoding of all responses is `utf-8`. # Example: ```python urls = [ \\"http://example.com\\", \\"https://example.org\\" ] auth = { \\"realm\\": \\"ExampleRealm\\", \\"uri\\": \\"https://example.org\\", \\"user\\": \\"username\\", \\"passwd\\": \\"password\\" } result = fetch_clean_text_from_urls(urls, auth) print(result) ``` # Sample Output: ```python { \\"http://example.com\\": \\"Example Domain This domain is for use in illustrative examples in documents.\\", \\"https://example.org\\": \\"Example Domain This domain is for use in illustrative examples in documents.\\" } ``` # Notes: - You may use the `html.parser` module to strip HTML tags and extract clean text. - Your function should leverage the `urllib.request` module extensively to demonstrate understanding of its functionality. - Ensure you handle cases with and without authentication details properly. - Implement appropriate error handling to manage issues such as unreachable URLs or authentication failures.","solution":"import urllib.request import urllib.error from urllib.request import HTTPBasicAuthHandler, HTTPPasswordMgrWithDefaultRealm, build_opener, install_opener, urlopen from html.parser import HTMLParser # Custom HTMLParser to extract text from HTML class MyHTMLParser(HTMLParser): def __init__(self): super().__init__() self.reset() self.strict = False self.convert_charrefs = True self.fed = [] def handle_data(self, d): self.fed.append(d) def get_data(self): return \' \'.join(self.fed) def strip_html_tags(html): parser = MyHTMLParser() parser.feed(html) return parser.get_data() def fetch_clean_text_from_urls(urls, auth=None): result = {} auth_handler = None if auth: password_mgr = HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, auth[\'uri\'], auth[\'user\'], auth[\'passwd\']) auth_handler = HTTPBasicAuthHandler(password_mgr) opener = build_opener(auth_handler) install_opener(opener) for url in urls: try: with urlopen(url) as response: html = response.read().decode(\'utf-8\') clean_text = strip_html_tags(html) result[url] = clean_text except urllib.error.HTTPError as e: print(f\\"HTTP Error: {e.code} for URL: {url}\\") result[url] = \\"\\" except urllib.error.URLError as e: print(f\\"URL Error: {e.reason} for URL: {url}\\") result[url] = \\"\\" except Exception as e: print(f\\"General Error: {e} for URL: {url}\\") result[url] = \\"\\" return result"},{"question":"**Coding Assessment Question:** # File Encoder/Decoder You are required to implement a command-line tool that leverages the `uu` module to encode and decode files in the uuencode format. Your tool should provide functionalities to encode a file to uuencoded format and decode a uuencoded file back to its original form. Requirements: 1. **Encoding Functionality** - Implement a function `encode_file(input_path: str, output_path: str, use_backtick: bool = False) -> None` that: - Takes the path of an input file (`input_path`) to be uuencoded. - Takes the path where the encoded file will be written (`output_path`). - Takes an optional parameter (`use_backtick`) that, when set to `True`, uses backticks instead of spaces for zeros in the encoded file. - Properly handles any exceptions that may occur during the encoding process. 2. **Decoding Functionality** - Implement a function `decode_file(encoded_path: str, output_path: str, file_mode: int = 0o666, suppress_warnings: bool = False) -> None` that: - Takes the path of the uuencoded input file (`encoded_path`). - Takes the path where the decoded file will be written (`output_path`). - Takes an optional parameter (`file_mode`) that specifies the file permissions of the output file. - Takes an optional parameter (`suppress_warnings`) that, when set to `True`, suppresses any warnings related to bad encoding. - Properly handles any exceptions that may occur during the decoding process. Example Usage: ```python # Encode a file try: encode_file(\'input.txt\', \'encoded.txt\', use_backtick=True) print(\\"File encoded successfully.\\") except Exception as e: print(f\\"Encoding failed: {e}\\") # Decode a file try: decode_file(\'encoded.txt\', \'output.txt\', file_mode=0o755, suppress_warnings=True) print(\\"File decoded successfully.\\") except Exception as e: print(f\\"Decoding failed: {e}\\") ``` Constraints: - You must use the `uu` module functions `uu.encode` and `uu.decode` to perform the encoding and decoding operations. - Handle the `uu.Error` exception specifically during the decoding process to demonstrate proper exception handling. - Ensure your code is compatible with Python 3.10. - Assume that the input files specified by paths exist and are accessible. Performance Requirements: - Your solution should handle large files efficiently without running into memory issues. Submission: - A Python script (.py file) containing the implementation of `encode_file` and `decode_file` functions. - Include any additional code required for testing your functions as described in the example usage.","solution":"import uu import os def encode_file(input_path: str, output_path: str, use_backtick: bool = False) -> None: Encodes a file in uuencode format. Args: input_path (str): The path to the input file to be encoded. output_path (str): The path to the output encoded file. use_backtick (bool): If True, encoding will use backticks instead of spaces for zeros. try: with open(input_path, \'rb\') as infile, open(output_path, \'wb\') as outfile: uu.encode(infile, outfile, name=os.path.basename(input_path), mode=0o666, backtick=use_backtick) except Exception as e: print(f\\"An error occurred during encoding: {e}\\") def decode_file(encoded_path: str, output_path: str, file_mode: int = 0o666, suppress_warnings: bool = False) -> None: Decodes a uuencoded file back to its original format. Args: encoded_path (str): The path to the uuencoded input file. output_path (str): The path to the output decoded file. file_mode (int): The file permissions for the output file. suppress_warnings (bool): If True, suppress warnings related to bad encoding. try: with open(encoded_path, \'rb\') as infile, open(output_path, \'wb\') as outfile: uu.decode(infile, outfile, mode=file_mode, quiet=suppress_warnings) except uu.Error as e: print(f\\"UU decoding error: {e}\\") except Exception as e: print(f\\"An error occurred during decoding: {e}\\")"},{"question":"<|Analysis Begin|> The provided documentation is quite limited, focusing specifically on the `sns.plotting_context()` function in the seaborn library. This function is used to set or get the plotting context parameters to control the scale of plot elements. There are a few examples of how to call the function with different arguments and how to use it as a context manager. This information, while useful, is quite narrow and may not be sufficient for a comprehensive coding assessment. However, I can still craft a question around the specifics of the `sns.plotting_context()` function to assess understanding of this feature within seaborn. <|Analysis End|> <|Question Begin|> # Question: Using `seaborn.plotting_context` in Data Visualization You are given a dataset containing information about car performance. Your task is to create two different visualizations using seaborn, applying different plotting contexts to highlight the effect of context management in seaborn. Specifically, you need to: 1. Load the `mpg` dataset from seaborn\'s built-in datasets. 2. Create a scatter plot of horsepower vs. miles per gallon (mpg) using the default plotting context. 3. Create another scatter plot of horsepower vs. miles per gallon (mpg) using the `talk` plotting context. 4. Put both plots side-by-side for comparison. **Input:** There are no direct inputs from the user. The dataset is loaded from seaborn\'s built-in datasets. **Output:** Two plots side-by-side showing the effect of different plotting contexts. **Constraints:** - Use the seaborn library for loading datasets and creating plots. - Use the `sns.plotting_context()` function to change plotting contexts. Here is a template to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt def create_comparison_plots(): # Load the dataset df = sns.load_dataset(\'mpg\') # Create the first plot with default context sns.set_context(\\"notebook\\") plt.subplot(1, 2, 1) sns.scatterplot(x=\'horsepower\', y=\'mpg\', data=df) plt.title(\'Default Context\') # Create the second plot with \'talk\' context with sns.plotting_context(\\"talk\\"): plt.subplot(1, 2, 2) sns.scatterplot(x=\'horsepower\', y=\'mpg\', data=df) plt.title(\'Talk Context\') # Show the plots plt.tight_layout() plt.show() # Call the function to create plots create_comparison_plots() ``` Make sure to include the necessary imports and provide appropriate documentation/comments within your code to explain your implementation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_comparison_plots(): Create two scatter plots of horsepower vs. miles per gallon (mpg) in different contexts. The first plot uses the default plotting context. The second plot uses the \'talk\' plotting context. The plots are displayed side-by-side for comparison. # Load the dataset df = sns.load_dataset(\'mpg\') # Create a figure for the subplots fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6)) # Create the first plot with default context sns.set_context(\\"notebook\\") sns.scatterplot(x=\'horsepower\', y=\'mpg\', data=df, ax=ax1) ax1.set_title(\'Default Context\') # Create the second plot with \'talk\' context with sns.plotting_context(\\"talk\\"): sns.scatterplot(x=\'horsepower\', y=\'mpg\', data=df, ax=ax2) ax2.set_title(\'Talk Context\') # Adjust layout and show the plots plt.tight_layout() plt.show() # Call the function to create plots create_comparison_plots()"},{"question":"**Advanced Seaborn Plotting Using `seaborn.objects`** # Objective: Create a comprehensive visualization of the penguins dataset using the advanced plotting features of `seaborn.objects`. This assessment will measure your ability to load and manipulate datasets, create multi-layered plots, and customize visuals. # Instructions: 1. **Data Preparation:** - Load the penguins dataset using `seaborn.load_dataset()`. - Handle any missing values by removing rows with `NaN` values in any of the columns used for plotting. 2. **Plot Creation:** - Create a multi-layered plot showing the distribution of `body_mass_g` for each `species`, color-coded by `sex`. - Use a `Dot` mark to represent individual data points. - Add error bars displaying the standard deviation using the `Range` mark. - Incorporate `facet` functionality to create separate plots for each `island`. 3. **Customization:** - Customize the appearance of the points and error bars (e.g., point size, line width). - Ensure that the plots are well-labeled and visually distinguishable. # Input: The `penguins` dataset provided by seaborn. # Output: A visual plot showing the distribution of `body_mass_g` by `species` and `sex`, faceted by `island`. # Constraints: - Ensure that the solution is efficient and handles the dataset correctly. - Use appropriate seaborn methods as demonstrated in the documentation. # Performance Requirements: - The code should execute within a reasonable time frame for this dataset size. # Example Code: ```python import seaborn.objects as so from seaborn import load_dataset # Load and clean the dataset penguins = load_dataset(\\"penguins\\").dropna() # Create the plot ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .scale(color_color=\'dark:sex\', marker_size=5, linewidth=1.5) .label(x=\\"Body Mass (g)\\", y=\\"Species\\", color=\\"Sex\\") .show() ) ``` **Note:** - Make sure to understand and utilize the `so.Plot` structure well. - Customize according to the requirements mentioned.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load and clean the dataset penguins = load_dataset(\\"penguins\\").dropna() # Create the plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .scale(color=\\"dark:sex\\", marker_size=5, linewidth=1.5) .label(x=\\"Body Mass (g)\\", y=\\"Species\\", color=\\"Sex\\") ) return plot"},{"question":"# Python Coding Assessment: Advanced URL Fetching and Handling Objective Your task is to write a Python function that fetches data from a given URL, handles different HTTP methods and exceptions, includes specific headers in the request, and processes the response. This will test your understanding of the `urllib` package and your ability to handle web-related tasks in Python. Detailed Description 1. **Function Name & Signature**: ```python def fetch_data(url: str, method: str = \'GET\', data: dict = None, headers: dict = None) -> str: ``` 2. **Parameters**: - `url` (str): The URL to fetch data from. - `method` (str): The HTTP method to use (\'GET\', \'POST\', etc.). Default is \'GET\'. - `data` (dict): The data to send in the request. This should be used for \'POST\' requests. Default is `None`. - `headers` (dict): A dictionary of headers to include in the request. Default is `None`. 3. **Behavior**: - If `method` is \'GET\', the function should append the `data` to the URL as query parameters. - If `method` is \'POST\', the function should encode `data` and include it in the request body. - Add default headers and combine them with any additional headers provided in the `headers` parameter. - Handle common HTTP errors (400-599) by printing a relevant message. - Handle other exceptions like network errors. 4. **Return Value**: - The function should return the raw HTML content of the fetched page as a string. 5. **Example**: ```python url = \'http://example.com/search\' data = {\'query\': \'Python\'} headers = {\'User-Agent\': \'Mozilla/5.0\'} result = fetch_data(url, method=\'GET\', data=data, headers=headers) print(result) ``` 6. **Constraints**: - You may assume that the URL is valid and reachable. - You should not use any third-party libraries for HTTP requests (only use built-in `urllib`). 7. **Performance**: - Ensure that the function is efficient in terms of memory usage, especially when handling large responses. Sample Input and Output **Input:** ```python url = \'http://httpbin.org/get\' data = {\'param1\': \'value1\'} headers = {\'User-Agent\': \'CustomUserAgent/1.0\'} response = fetch_data(url, method=\'GET\', data=data, headers=headers) ``` **Output:** ```plaintext \'{n \\"args\\": {n \\"param1\\": \\"value1\\"n },n \\"headers\\": {n \\"User-Agent\\": \\"CustomUserAgent/1.0\\"n },n ... n}\' ``` **Input:** ```python url = \'http://httpbin.org/post\' data = {\'param1\': \'value1\'} headers = {\'User-Agent\': \'CustomUserAgent/1.0\'} response = fetch_data(url, method=\'POST\', data=data, headers=headers) ``` **Output:** ```plaintext \'{n \\"args\\": {},n \\"data\\": \\"\\",n \\"files\\": {},n \\"form\\": {n \\"param1\\": \\"value1\\"n },n \\"headers\\": {n \\"User-Agent\\": \\"CustomUserAgent/1.0\\"n },n ... n}\' ``` Implementation Notes - You will use `urllib.request.Request` and `urllib.request.urlopen` for making HTTP requests. - Use `urllib.parse.urlencode` to encode data into query parameters for GET requests. - Handle appropriate exceptions using `try` and `except` blocks.","solution":"import urllib.request import urllib.parse import urllib.error def fetch_data(url: str, method: str = \'GET\', data: dict = None, headers: dict = None) -> str: Fetch data from the given URL using the specified HTTP method and parameters. if headers is None: headers = {} default_headers = { \'User-Agent\': \'Mozilla/5.0\' } # Combine default headers with user-specified headers combined_headers = {**default_headers, **headers} if data: if method == \'GET\': encoded_data = urllib.parse.urlencode(data) url = f\\"{url}?{encoded_data}\\" data = None elif method == \'POST\': encoded_data = urllib.parse.urlencode(data).encode() data = encoded_data request = urllib.request.Request(url, data=data, headers=combined_headers, method=method) try: with urllib.request.urlopen(request) as response: return response.read().decode() except urllib.error.HTTPError as e: return f\\"HTTPError: {e.code} - {e.reason}\\" except urllib.error.URLError as e: return f\\"URLError: {e.reason}\\" except Exception as e: return f\\"Exception: {str(e)}\\""},{"question":"# Advanced Python Coding Assessment: Asynchronous Synchronization **Objective:** Implement a function using various asyncio synchronization primitives (`Lock`, `Event`, `Condition`, `Semaphore`, `BoundedSemaphore`) to solve a producer-consumer problem in an asynchronous environment. Problem Statement You are required to implement a simplified Producer-Consumer scenario using `asyncio` synchronization primitives. The producer generates data items and puts them into a bounded buffer, while the consumer takes items from the buffer and processes them. Use appropriate `asyncio` synchronization primitives to ensure thread-safe operations and coordinated task executions. Function Specification Implement the following functions: 1. **producer(buffer: asyncio.Queue, produced_event: asyncio.Event) -> None** - Continuously produce data items and put them into the buffer. - Set the `produced_event` event when data is added to the buffer. 2. **consumer(buffer: asyncio.Queue, produced_event: asyncio.Event) -> None** - Continuously consume data items from the buffer. - Wait for the `produced_event` event before attempting to consume an item. 3. **main() -> None** - Create a bounded buffer using `asyncio.Queue`. - Instantiate the `producer` and `consumer` coroutines and run them concurrently. - Ensure proper synchronization using asyncio primitives. Constraints - The buffer size should be limited to 10 items. - The producer should produce 50 items in total. - Each item is a simple integer (e.g., `1, 2, 3, ...`). - Ensure proper locking and signaling mechanisms to avoid race conditions. Example ```python import asyncio async def producer(buffer: asyncio.Queue, produced_event: asyncio.Event) -> None: # Implementation here ... async def consumer(buffer: asyncio.Queue, produced_event: asyncio.Event) -> None: # Implementation here ... async def main() -> None: buffer = asyncio.Queue(maxsize=10) produced_event = asyncio.Event() producer_task = asyncio.create_task(producer(buffer, produced_event)) consumer_task = asyncio.create_task(consumer(buffer, produced_event)) await asyncio.gather(producer_task, consumer_task) asyncio.run(main()) ``` **Input and Output** - *Input*: None. - *Output*: Print statements from the producer and consumer indicating the produced and consumed items. Performance and Constraints - Ensure the program can handle the concurrency efficiently without deadlocks or race conditions. - The buffer should not exceed its maximum size at any point. - The tasks should cleanly terminate after producing and consuming all items. # Submission Submit your solution with the complete implementation of the `producer`, `consumer`, and `main` functions. Ensure your code is well-commented and adheres to the specified constraints and requirements.","solution":"import asyncio async def producer(buffer: asyncio.Queue, produced_event: asyncio.Event) -> None: for i in range(1, 51): await buffer.put(i) print(f\\"Produced {i}\\") produced_event.set() await asyncio.sleep(0.1) # Signal the end of production await buffer.put(None) async def consumer(buffer: asyncio.Queue, produced_event: asyncio.Event) -> None: while True: # Wait for the produced event signal await produced_event.wait() item = await buffer.get() if item is None: break print(f\\"Consumed {item}\\") await asyncio.sleep(0.2) produced_event.clear() async def main() -> None: buffer = asyncio.Queue(maxsize=10) produced_event = asyncio.Event() producer_task = asyncio.create_task(producer(buffer, produced_event)) consumer_task = asyncio.create_task(consumer(buffer, produced_event)) await asyncio.gather(producer_task, consumer_task) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Environment Variables for PyTorch MPS Backend You are tasked with optimizing the performance of a PyTorch model running on an Apple device that supports Metal Performance Shaders (MPS). To do this, you need to set specific environment variables that help manage memory allocation and logging in PyTorch. Task 1. **Implement a function `set_mps_env_vars`**: This function should programmatically set the following environment variables based on the inputs provided: - `PYTORCH_DEBUG_MPS_ALLOCATOR` - `PYTORCH_MPS_LOG_PROFILE_INFO` - `PYTORCH_MPS_TRACE_SIGNPOSTS` - `PYTORCH_MPS_HIGH_WATERMARK_RATIO` - `PYTORCH_MPS_LOW_WATERMARK_RATIO` - `PYTORCH_MPS_FAST_MATH` - `PYTORCH_MPS_PREFER_METAL` - `PYTORCH_ENABLE_MPS_FALLBACK` Function Signature ```python def set_mps_env_vars(debug_allocator: int, log_profile: int, trace_signposts: int, high_watermark_ratio: float, low_watermark_ratio: float, fast_math: int, prefer_metal: int, enable_fallback: int) -> None: pass ``` Parameters - `debug_allocator` (int): Set to `1` to enable verbose logging for the MPS allocator. - `log_profile` (int): Set log options bitmask for profiling (refer to `LogOptions` enum). - `trace_signposts` (int): Set profile and signpost bitmasks (refer to `ProfileOptions` and `SignpostTypes` enums). - `high_watermark_ratio` (float): Set high watermark ratio for MPS allocator (default is 1.7). - `low_watermark_ratio` (float): Set low watermark ratio for MPS allocator (defaults depend on memory type). - `fast_math` (int): Set to `1` to enable fast math for MPS metal kernels. - `prefer_metal` (int): Set to `1` to prefer using metal kernels over MPS Graph APIs. - `enable_fallback` (int): Set to `1` to enable fallback operations to CPU when MPS does not support them. Returns - `None` 2. **Testing Your Function**: Write a separate function to test `set_mps_env_vars` that: - Sets the environment variables using the function. - Verifies that the environment variables are correctly set by reading them back. Function Signature ```python def test_set_mps_env_vars(): pass ``` Steps - Call `set_mps_env_vars` with sample input values. - Verify each environment variable to ensure it has been set correctly. - Print a success message if all variables are set correctly, otherwise, print an error message. Constraints - The range for `high_watermark_ratio` and `low_watermark_ratio` should be between 0.0 and 2.0. - All integer inputs must be either 0 or 1. Example Usage ```python set_mps_env_vars(1, 1, 1, 1.7, 1.4, 1, 1, 1) test_set_mps_env_vars() ``` Requirements - Your solution should correctly set and verify the environment variables. - Ensure to handle edge cases such as invalid input values by raising appropriate exceptions.","solution":"import os def set_mps_env_vars(debug_allocator: int, log_profile: int, trace_signposts: int, high_watermark_ratio: float, low_watermark_ratio: float, fast_math: int, prefer_metal: int, enable_fallback: int) -> None: Set environment variables to optimize the performance of a PyTorch model running on an Apple device that supports Metal Performance Shaders (MPS). Parameters: - debug_allocator (int): Enable verbose logging for the MPS allocator (0 or 1). - log_profile (int): Log options bitmask for profiling (0 or 1). - trace_signposts (int): Profile and signpost bitmasks (0 or 1). - high_watermark_ratio (float): High watermark ratio for MPS allocator (0.0 to 2.0). - low_watermark_ratio (float): Low watermark ratio for MPS allocator (0.0 to 2.0). - fast_math (int): Enable fast math for MPS metal kernels (0 or 1). - prefer_metal (int): Prefer using metal kernels over MPS Graph APIs (0 or 1). - enable_fallback (int): Enable fallback operations to CPU when MPS does not support them (0 or 1). Returns: - None # Check for invalid input values def validate_int(value): if value not in [0, 1]: raise ValueError(f\\"Invalid integer value: {value}. Must be 0 or 1.\\") def validate_float(value): if not (0.0 <= value <= 2.0): raise ValueError(f\\"Invalid float value: {value}. Must be between 0.0 and 2.0.\\") validate_int(debug_allocator) validate_int(log_profile) validate_int(trace_signposts) validate_float(high_watermark_ratio) validate_float(low_watermark_ratio) validate_int(fast_math) validate_int(prefer_metal) validate_int(enable_fallback) os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = str(debug_allocator) os.environ[\'PYTORCH_MPS_LOG_PROFILE_INFO\'] = str(log_profile) os.environ[\'PYTORCH_MPS_TRACE_SIGNPOSTS\'] = str(trace_signposts) os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = str(high_watermark_ratio) os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = str(low_watermark_ratio) os.environ[\'PYTORCH_MPS_FAST_MATH\'] = str(fast_math) os.environ[\'PYTORCH_MPS_PREFER_METAL\'] = str(prefer_metal) os.environ[\'PYTORCH_ENABLE_MPS_FALLBACK\'] = str(enable_fallback)"},{"question":"Objective: Design a function that analyzes audio fragments and performs a series of operations to normalize the fragment and then convert it to a specified encoding format. Task: You are provided with two audio fragments that were recorded separately but need to be analyzed together to determine their normalization factor and then converted to a new encoding format. 1. **Normalize the Audio Fragments:** - Find the average RMS (root-mean-square) value of both fragments combined. - Use this average RMS value to normalize each fragment individually by computing the necessary multiplicative factor. The normalization factor for each fragment should ensure that the RMS value of the normalized fragment is equal to the combined average RMS. 2. **Convert the Normalized Fragments:** - Convert the normalized fragments from their original format to the specified encoding format (a-LAW or u-LAW). - Ensure that the conversion handles the bytes width appropriately for both the input and the output formats. Function Signature: ```python def normalize_and_convert(fragment1: bytes, fragment2: bytes, width: int, encoding: str) -> tuple: Normalize two audio fragments and convert them to the specified encoding format. Args: fragment1 (bytes): The first audio fragment. fragment2 (bytes): The second audio fragment. width (int): The byte width of the original fragments (1, 2, 3, or 4). encoding (str): The target encoding format (\\"alaw\\" or \\"ulaw\\"). Returns: tuple: A tuple containing two elements - the converted normalized first fragment and the converted normalized second fragment. pass ``` Input Constraints: - `fragment1` and `fragment2` are non-empty bytes-like objects containing the audio data. - `width` is one of the values {1, 2, 3, 4}. - `encoding` is either \\"alaw\\" or \\"ulaw\\". Output: - The function should return a tuple with the converted normalized versions of `fragment1` and `fragment2`. Example: ```python # Example usage of the function f1 = b\'x01x02x03x04x05\' f2 = b\'x06x07x08x09x0A\' width = 2 encoding = \\"alaw\\" normalized_f1, normalized_f2 = normalize_and_convert(f1, f2, width, encoding) print(normalized_f1) # Should print the normalized and a-LAW encoded fragment1 print(normalized_f2) # Should print the normalized and a-LAW encoded fragment2 ``` Notes: - Use `audioop.rms` to calculate the RMS value. - Use `audioop.mul` to apply the normalization factor. - Use `audioop.lin2alaw` or `audioop.lin2ulaw` for conversion based on the encoding specified. - Ensure proper error handling for unsupported encoding formats and invalid input widths.","solution":"import audioop def normalize_and_convert(fragment1: bytes, fragment2: bytes, width: int, encoding: str) -> tuple: Normalize two audio fragments and convert them to the specified encoding format. Args: fragment1 (bytes): The first audio fragment. fragment2 (bytes): The second audio fragment. width (int): The byte width of the original fragments (1, 2, 3, or 4). encoding (str): The target encoding format (\\"alaw\\" or \\"ulaw\\"). Returns: tuple: A tuple containing two elements - the converted normalized first fragment and the converted normalized second fragment. if encoding not in [\\"alaw\\", \\"ulaw\\"]: raise ValueError(\\"Unsupported encoding format. Choose \'alaw\' or \'ulaw\'.\\") if width not in [1, 2, 3, 4]: raise ValueError(\\"Invalid width. Must be 1, 2, 3, or 4.\\") # Calculate RMS of both fragments rms1 = audioop.rms(fragment1, width) rms2 = audioop.rms(fragment2, width) # Calculate average RMS combined_avg_rms = (rms1 + rms2) / 2 # Normalize fragments norm_factor1 = combined_avg_rms / rms1 if rms1 != 0 else 1 norm_factor2 = combined_avg_rms / rms2 if rms2 != 0 else 1 normalized_fragment1 = audioop.mul(fragment1, width, norm_factor1) normalized_fragment2 = audioop.mul(fragment2, width, norm_factor2) # Convert to specified encoding format if encoding == \\"alaw\\": converted_fragment1 = audioop.lin2alaw(normalized_fragment1, width) converted_fragment2 = audioop.lin2alaw(normalized_fragment2, width) elif encoding == \\"ulaw\\": converted_fragment1 = audioop.lin2ulaw(normalized_fragment1, width) converted_fragment2 = audioop.lin2ulaw(normalized_fragment2, width) return (converted_fragment1, converted_fragment2)"},{"question":"**Networking Chat Application Using Sockets and Asynchronous I/O** # Objective Implement a simple chat server-client system using the `asyncio` and `socket` modules. The server should handle simultaneous connections from multiple clients and facilitate message broadcasting to all connected clients in real-time. # Requirements 1. **Server**: - The server should accept incoming connections and handle multiple clients simultaneously. - Implement non-blocking I/O using `asyncio`. - On receiving a message from a client, it should broadcast the message to all connected clients. - Log the messages with a timestamp and the client\'s address. 2. **Client**: - The client should connect to the server and be able to send messages. - Should receive and display any messages broadcast by the server. - Implement non-blocking I/O using `asyncio`. # Constraints - **Server**: - Should run on localhost and a specific port (e.g., 12345). - **Client**: - Should connect to localhost at the specified port. # Performance Requirements - The server should handle at least 10 simultaneous client connections without significant latency. # Input and Output Formats - **Server**: - Input: Messages from clients via socket connections. - Output: Broadcast messages to all clients and logs to a local file. - **Client**: - Input: Messages from the user via standard input. - Output: Receives and displays broadcast messages from the server. # Hints - Use `asyncio` for handling asynchronous connections and message broadcasting. - Utilize Python\'s `socket` library to establish the connection. - Remember to handle exceptions and cleanup to avoid crashes on sudden client disconnection. # Sample Code Structure ```python # Server Implementation import asyncio import socket import time clients = [] async def handle_client(client_socket, addr): while True: try: message = await loop.sock_recv(client_socket, 1024) if not message: break message = message.decode(\'utf-8\') timestamp = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime()) log_message = f\\"{timestamp} - {addr}: {message}\\" print(log_message) for client in clients: if client != client_socket: await loop.sock_sendall(client, message.encode(\'utf-8\')) except: break client_socket.close() clients.remove(client_socket) print(f\\"Client {addr} disconnected\\") async def server(loop): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', 12345)) server_socket.listen(5) server_socket.setblocking(False) while True: client_socket, addr = await loop.sock_accept(server_socket) print(f\\"Client {addr} connected\\") clients.append(client_socket) loop.create_task(handle_client(client_socket, addr)) # Client Implementation async def client(loop): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) await loop.sock_connect(client_socket, (\'localhost\', 12345)) async def send_message(): while True: message = await loop.run_in_executor(None, input, \\"You: \\") await loop.sock_sendall(client_socket, message.encode(\'utf-8\')) async def receive_message(): while True: try: message = await loop.sock_recv(client_socket, 1024) if not message: break message = message.decode(\'utf-8\') print(f\\"rReceived: {message}nYou: \\", end=\\"\\") except: break loop.create_task(send_message()) loop.create_task(receive_message()) if __name__ == \\"__main__\\": loop = asyncio.get_event_loop() loop.create_task(server(loop)) loop.run_forever() ``` **Note**: The above sample code is for structural guidance only. Students are expected to build and refine their functional implementation.","solution":"import asyncio import socket import time clients = [] async def handle_client(client_socket, addr): while True: try: message = await asyncio.get_event_loop().sock_recv(client_socket, 1024) if not message: break message = message.decode(\'utf-8\') timestamp = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime()) log_message = f\\"{timestamp} - {addr}: {message}\\" print(log_message) for client in clients: if client != client_socket: await asyncio.get_event_loop().sock_sendall(client, message.encode(\'utf-8\')) except Exception as e: print(f\\"Exception: {e}\\") break client_socket.close() clients.remove(client_socket) print(f\\"Client {addr} disconnected\\") async def server(loop): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'localhost\', 12345)) server_socket.listen(5) server_socket.setblocking(False) while True: client_socket, addr = await loop.sock_accept(server_socket) print(f\\"Client {addr} connected\\") clients.append(client_socket) loop.create_task(handle_client(client_socket, addr)) if __name__ == \\"__main__\\": loop = asyncio.get_event_loop() try: loop.create_task(server(loop)) loop.run_forever() except KeyboardInterrupt: print(\\"Server is shutting down...\\") finally: loop.close()"},{"question":"**Objective**: Demonstrate your understanding of the `inspect` module in Python by writing a function that inspects all the methods of a given class and retrieves their signatures and documentation. **Problem Statement**: Write a function `get_class_methods_info(cls: type) -> dict` that takes a class `cls` as input and returns a dictionary. The dictionary should have the names of all the methods of the class (including inherited methods) as keys. The value for each key should be a tuple `(signature, docstring)`, where `signature` is the method\'s signature obtained using the `inspect.signature` function and `docstring` is the method\'s documentation string obtained using `inspect.getdoc`. # Expected Input and Output Formats Input: - `cls`: A Python class object. Output: - A dictionary where each key is the name of a method in the class and the value is a tuple containing the method\'s signature and its documentation string. # Constraints: - You may assume that the input class does not have methods with conflicting names. - You should include methods from the class and its superclasses. # Example: ```python import inspect class Base: def base_method(self, x): Base method docstring. pass class Derived(Base): def derived_method(self, y): Derived method docstring. pass result = get_class_methods_info(Derived) print(result) ``` # Expected Output: ```python { \'base_method\': (\'(self, x)\', \'Base method docstring.\'), \'derived_method\': (\'(self, y)\', \'Derived method docstring.\') } ``` # Notes: - Use the `inspect.getmembers` function with a suitable predicate to fetch the methods of the class. - Use the `inspect.signature` function to retrieve the method signatures. - Use the `inspect.getdoc` function to retrieve the method documentation. - You do not need to handle exceptions or invalid input for this task. Implement the function `get_class_methods_info` below: ```python import inspect def get_class_methods_info(cls: type) -> dict: # Your code here pass # Example usage class Base: def base_method(self, x): Base method docstring. pass class Derived(Base): def derived_method(self, y): Derived method docstring. pass result = get_class_methods_info(Derived) print(result) ```","solution":"import inspect def get_class_methods_info(cls: type) -> dict: Inspects all methods of the given class and retrieves their signatures and documentation. Args: cls (type): The class to inspect. Returns: dict: Dictionary with method names as keys and tuples (signature, docstring) as values. methods_info = {} for name, method in inspect.getmembers(cls, predicate=inspect.isfunction): signature = str(inspect.signature(method)) docstring = inspect.getdoc(method) methods_info[name] = (signature, docstring) return methods_info"},{"question":"Objective Demonstrate your understanding of the `ipaddress` module by creating and manipulating IP address objects and performing specific operations with them. Problem Statement You are given a list of IP address strings and a list of IP networks in CIDR notation. Your task is to: 1. Convert the IP address strings to `ipaddress` objects. 2. Convert the IP networks to `ipaddress` network objects. 3. Determine and return the number of IP addresses within each network. 4. Identify which network each IP address belongs to and return this mapping. Write a function `process_ip_addresses_and_networks` that takes two parameters: - `ip_addresses` (List of strings): A list containing IP address strings (both IPv4 and IPv6). - `networks` (List of strings): A list of network definitions in CIDR notation (both IPv4 and IPv6). The function should return a tuple of two elements: 1. A dictionary with network strings as keys and the number of addresses in each network (excluding the network and broadcast addresses for IPv4) as values. 2. A dictionary mapping each IP address to the network it belongs to. Example ```python from typing import List, Dict, Tuple import ipaddress def process_ip_addresses_and_networks(ip_addresses: List[str], networks: List[str]) -> Tuple[Dict[str, int], Dict[str, str]]: result = ({}, {}) Your code here return result # Example Inputs ips = [\'192.168.1.1\', \'2001:db8::1\', \'192.168.1.255\'] nets = [\'192.168.1.0/24\', \'2001:db8::/64\'] # Expected Outputs: # Network Information: {\'192.168.1.0/24\': 254, \'2001:db8::/64\': 18446744073709551614} # IP to Network Mapping: {\'192.168.1.1\': \'192.168.1.0/24\', \'2001:db8::1\': \'2001:db8::/64\', \'192.168.1.255\': \'192.168.1.0/24\'} network_info, ip_to_network_mapping = process_ip_addresses_and_networks(ips, nets) print(network_info) print(ip_to_network_mapping) ``` # Constraints - The IP address strings and network definitions will always be valid. - The list of IP addresses and networks will not be empty. - Performance considerations must be taken into account for large lists of IP addresses and networks. # Additional Notes - Utilize the `ipaddress` module to handle IP and network operations. - Ensure to exclude the network and broadcast addresses when counting the number of usable addresses in IPv4 networks.","solution":"from typing import List, Dict, Tuple import ipaddress def process_ip_addresses_and_networks(ip_addresses: List[str], networks: List[str]) -> Tuple[Dict[str, int], Dict[str, str]]: # Convert networks to ipaddress.IPv4Network or ipaddress.IPv6Network objects and prepare the result dictionary network_objects = [ipaddress.ip_network(net) for net in networks] network_info = {} for net in network_objects: # Calculate the number of usable addresses for the network if net.version == 4: usable_ips = net.num_addresses - 2 if net.num_addresses > 2 else 1 else: usable_ips = net.num_addresses - 2 network_info[str(net)] = usable_ips # Convert IP addresses to ipaddress.IPv4Address or ipaddress.IPv6Address objects and determine their networks ip_to_network_mapping = {} for ip_str in ip_addresses: ip = ipaddress.ip_address(ip_str) for net in network_objects: if ip in net: ip_to_network_mapping[ip_str] = str(net) break return network_info, ip_to_network_mapping"},{"question":"Objective: Write a Python script using the \\"unittest\\" framework to test the functionality of a custom class `Account`. The `Account` class should handle basic bank account operations such as deposit, withdraw, and balance inquiry. Requirements: 1. Implement the `Account` class with the following methods: - `__init__(self, account_number, initial_balance=0)`: Initialize the account with an account number and an optional initial balance. - `deposit(self, amount)`: Add `amount` to the account balance. - `withdraw(self, amount)`: Subtract `amount` from the account balance if sufficient funds are available; otherwise, raise a `ValueError` with the message \\"Insufficient funds\\". - `get_balance(self)`: Return the current account balance. 2. Implement the following test cases using `unittest`: - Test that an account is initialized correctly with an account number and initial balance. - Test that deposits correctly increase the account balance. - Test that withdrawals correctly decrease the account balance. - Test that withdrawals do not proceed if there are insufficient funds. - Test that the account balance is correctly reported. Input and Output: - There is no direct input/output; the functionality will be verified through unit tests. - Ensure all tests pass without any assertion errors. Constraints: - The initial balance and deposit/withdraw amounts will be non-negative floats. - The account number will be a non-empty string. Performance Requirements: - The solution should efficiently handle operations and checks within typical constraints for monetary transactions. Example Code Structure: ```python class Account: def __init__(self, account_number, initial_balance=0): pass def deposit(self, amount): pass def withdraw(self, amount): pass def get_balance(self): pass import unittest class TestAccount(unittest.TestCase): def setUp(self): pass def test_initialization(self): pass def test_deposit(self): pass def test_withdraw(self): pass def test_insufficient_funds(self): pass def test_get_balance(self): pass if __name__ == \'__main__\': unittest.main() ``` Implement the required methods in the `Account` class and complete the test cases in the `TestAccount` class to fully validate the functionality.","solution":"class Account: def __init__(self, account_number, initial_balance=0): Initialize the account with an account number and an optional initial balance. :param account_number: str, the account number :param initial_balance: float, initial balance of the account (default is 0) self.account_number = account_number self.balance = initial_balance def deposit(self, amount): Add `amount` to the account balance. :param amount: float, the amount to deposit self.balance += amount def withdraw(self, amount): Subtract `amount` from the account balance if sufficient funds are available; otherwise, raise a `ValueError`. :param amount: float, the amount to withdraw :raises ValueError: if the balance is insufficient for the withdrawal if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): Return the current account balance. :return: float, the current balance of the account return self.balance"},{"question":"Question: Advanced String Formatting and Conversion # Objective: Your task is to implement two functions that demonstrate your understanding of the `PyOS_snprintf` and `PyOS_vsnprintf` functionalities, as well as string to double conversion and double to string conversion provided by Python310. # Part 1: Diversified Messages Formatter Implement a function `format_messages` that takes a list of tuples, each containing a string format and a list of values, and returns a list of formatted strings. The formatting should adhere to the constraints provided by the `PyOS_snprintf` function. Function Signature: ```python def format_messages(messages: List[Tuple[str, List[Any]]]) -> List[str]: ``` Input: - `messages`: A list of tuples. Each tuple contains: - `format_str`: A string containing format specifiers. - `values`: A list of values to be substituted into the format string. Output: - A list of strings where each string is formatted according to its corresponding format string and values. Constraints: - Ensure the output strings are not truncated and handle potential errors gracefully as per the behavior of `PyOS_snprintf`. # Part 2: Double Conversion with Error Handling Implement a function `convert_and_format_double` that converts a string to a double, applies some mathematical transformation to it, and then returns the string representation of the transformed value with specified formatting. Function Signature: ```python def convert_and_format_double(s: str, format_code: str, precision: int) -> str: ``` Input: - `s`: A string representation of a floating-point number. - `format_code`: A character from the set {\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'} specifying the format. - `precision`: An integer specifying the required precision for the format. Output: - A string representation of the transformed floating-point number according to the specified format. Constraints: - The conversion must handle the following edge cases: - Invalid string representation. - Overflow situations. - Proper freeing of allocated memory. - The transformation to be applied is: if the number is less than 100, multiply it by 2, otherwise, square it. # Example Usage: ```python # Example for format_messages messages = [ (\\"%s is %d years old\\", [\\"Alice\\", 30]), (\\"Price: %.2f\\", [49.99]) ] output = format_messages(messages) # Expected: [\\"Alice is 30 years old\\", \\"Price: 49.99\\"] # Example for convert_and_format_double s = \\"43.5\\" format_code = \\"f\\" precision = 2 output = convert_and_format_double(s, format_code, precision) # Expected: \\"87.00\\" (since 43.5 * 2 = 87.0, formatted to 2 decimal places) ``` Implement both functions, ensuring they handle errors according to the guidelines provided in the documentation.","solution":"from typing import List, Tuple, Any def format_messages(messages: List[Tuple[str, List[Any]]]) -> List[str]: Formats a list of messages with given format strings and values. :param messages: List of tuples, each containing a format string and a list of values :return: List of formatted strings formatted_messages = [] for format_str, values in messages: try: formatted_message = format_str % tuple(values) except (TypeError, ValueError) as e: formatted_message = f\\"Error in formatting: {e}\\" formatted_messages.append(formatted_message) return formatted_messages def convert_and_format_double(s: str, format_code: str, precision: int) -> str: Converts a string to a double, applies transformations, and formats it. :param s: A string representation of a floating-point number :param format_code: A character specifying the format :param precision: An integer specifying the required precision for the format :return: A string representation of the transformed floating-point number try: num = float(s) except ValueError: return \\"Invalid input\\" if num < 100: num *= 2 else: num **= 2 format_str = f\\"{{:.{precision}{format_code}}}\\" try: formatted_num = format_str.format(num) except (ValueError, OverflowError): return \\"Formatting error\\" return formatted_num"},{"question":"# Task You are given a deep learning model implemented in PyTorch and you are required to perform GPU profiling to identify and optimize its performance bottlenecks using TorchInductor. Follow the steps below: 1. **Setup the Environment:** - Implement a function to set the relevant TorchInductor environment variables: `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1`, `TORCHINDUCTOR_BENCHMARK_KERNEL=1`. 2. **Run Profiling:** - Run the provided model and collect the profiling data. - Ensure that the data includes detailed information about individual kernel execution times. 3. **Parse Profiling Data:** - Implement a function to parse the profiling data, identifying the top 3 kernels that take the most GPU time. 4. **Optimize the Model:** - Implement a function that attempts to optimize the model by re-running the most time-consuming kernel with `TORCHINDUCTOR_MAX_AUTOTUNE=1`. - Compare the execution times before and after optimization. Use the following template for your implementation: ```python import os import subprocess from typing import List, Tuple def set_environment_variables(): Set the relevant TorchInductor environment variables. os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' def run_model(model_script: str): Run the given model script and collect profiling data. Args: - model_script (str): Path to the Python script containing the model definition and execution. Returns: - profiling_log (str): Path to the profiling log file. profiling_log = \\"/tmp/profiling_log.txt\\" with open(profiling_log, \'w\') as log_file: subprocess.run([\\"python\\", model_script], stdout=log_file, stderr=subprocess.STDOUT) return profiling_log def parse_profiling_data(profiling_log: str) -> List[Tuple[str, float]]: Parse the profiling log to identify the top 3 kernels that take the most GPU time. Args: - profiling_log (str): Path to the profiling log file. Returns: - List[Tuple[str, float]]: A list of tuples where each tuple contains (kernel_name, percent_gpu_time). top_kernels = [] # Parse the profiling log to extract information about the top 3 kernels. # ... return top_kernels def optimize_kernel(kernel_name: str): Optimize the given kernel using max autotune. Args: - kernel_name (str): Name of the kernel to be optimized. Returns: - execution_time_before (float): Execution time of the kernel before optimization. - execution_time_after (float): Execution time of the kernel after optimization. # Find the kernel script path from the profiling log. kernel_script_path = f\\"/tmp/{kernel_name}.py\\" # Run the kernel script to benchmark and get the execution time before optimization. result_before = subprocess.run([\\"python\\", kernel_script_path], capture_output=True, text=True) execution_time_before = parse_execution_time(result_before.stdout) # Set TORCHINDUCTOR_MAX_AUTOTUNE to optimize the kernel os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' result_after = subprocess.run([\\"python\\", kernel_script_path], capture_output=True, text=True) execution_time_after = parse_execution_time(result_after.stdout) return execution_time_before, execution_time_after def parse_execution_time(output: str) -> float: Parse the execution time from the output of the kernel script. Args: - output (str): Output string of the kernel script execution. Returns: - float: Parsed execution time. # Extract the execution time from the output string. # ... return execution_time if __name__ == \\"__main__\\": set_environment_variables() # Run the model and collect profiling data. profiling_log = run_model(\\"model.py\\") # Parse the profiling data to identify the top 3 kernels. top_kernels = parse_profiling_data(profiling_log) # Optimize the top kernel and compare execution times. for kernel_name, _ in top_kernels: execution_time_before, execution_time_after = optimize_kernel(kernel_name) print(f\\"Kernel: {kernel_name}\\") print(f\\"Execution time before optimization: {execution_time_before} ms\\") print(f\\"Execution time after optimization: {execution_time_after} ms\\") ``` # Input - `model.py`: A Python script that defines and runs the model in PyTorch. # Output - Print the execution times for the top 3 kernels before and after optimization. Note: Ensure that you have the necessary permissions and libraries installed to execute the profiling and optimization scripts.","solution":"import os import subprocess from typing import List, Tuple def set_environment_variables(): Set the relevant TorchInductor environment variables. os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' def run_model(model_script: str) -> str: Run the given model script and collect profiling data. Args: - model_script (str): Path to the Python script containing the model definition and execution. Returns: - profiling_log (str): Path to the profiling log file. profiling_log = \\"/tmp/profiling_log.txt\\" with open(profiling_log, \'w\') as log_file: subprocess.run([\\"python\\", model_script], stdout=log_file, stderr=subprocess.STDOUT) return profiling_log def parse_profiling_data(profiling_log: str) -> List[Tuple[str, float]]: Parse the profiling log to identify the top 3 kernels that take the most GPU time. Args: - profiling_log (str): Path to the profiling log file. Returns: - List[Tuple[str, float]]: A list of tuples where each tuple contains (kernel_name, percent_gpu_time). top_kernels = [] kernel_times = {} with open(profiling_log, \'r\') as log_file: for line in log_file: if \\"Kernel:\\" in line: parts = line.split() kernel_name = parts[1] gpu_time = float(parts[-1].strip(\'%\')) kernel_times[kernel_name] = gpu_time sorted_kernels = sorted(kernel_times.items(), key=lambda x: x[1], reverse=True) top_kernels = sorted_kernels[:3] return top_kernels def optimize_kernel(kernel_name: str) -> Tuple[float, float]: Optimize the given kernel using max autotune. Args: - kernel_name (str): Name of the kernel to be optimized. Returns: - execution_time_before (float): Execution time of the kernel before optimization. - execution_time_after (float): Execution time of the kernel after optimization. kernel_script_path = f\\"/tmp/{kernel_name}.py\\" result_before = subprocess.run([\\"python\\", kernel_script_path], capture_output=True, text=True) execution_time_before = parse_execution_time(result_before.stdout) os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' result_after = subprocess.run([\\"python\\", kernel_script_path], capture_output=True, text=True) execution_time_after = parse_execution_time(result_after.stdout) return execution_time_before, execution_time_after def parse_execution_time(output: str) -> float: Parse the execution time from the output of the kernel script. Args: - output (str): Output string of the kernel script execution. Returns: - float: Parsed execution time. for line in output.splitlines(): if \\"Execution time\\" in line: return float(line.split()[-2]) return 0.0 if __name__ == \\"__main__\\": set_environment_variables() profiling_log = run_model(\\"model.py\\") top_kernels = parse_profiling_data(profiling_log) for kernel_name, _ in top_kernels: execution_time_before, execution_time_after = optimize_kernel(kernel_name) print(f\\"Kernel: {kernel_name}\\") print(f\\"Execution time before optimization: {execution_time_before} ms\\") print(f\\"Execution time after optimization: {execution_time_after} ms\\")"},{"question":"# Python Logging Configuration Task Objective: Implement a Python function that configures the logging system using a dictionary format and a file format. The function should take a configuration dictionary and a file path, set up the logging based on the input, and create appropriate log entries. Task Description: 1. **Function Definition:** - Define a function `configure_logging(dict_config: dict, file_path: str) -> None`. 2. **Parameters:** - `dict_config`: A dictionary containing the logging configuration. - `file_path`: Path to a file containing logging configuration in `configparser` (INI) format. 3. **Requirements:** - The function should first configure logging using the dictionary schema provided in `dict_config`. - Following this, it should configure logging based on the INI file located at `file_path`. - Handle any errors gracefully, and log them appropriately using the default logging configuration. - Ensure that the function respects existing loggers unless otherwise specified. 4. **Constraints:** - Assume `dict_config` and the file at `file_path` are well-formed as per the schema and format provided in the documentation. - Ensure that the function handles invalid configurations gracefully by catching and logging exceptions. 5. **Example:** ```python dict_config = { \'version\': 1, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'standard\', \'level\': \'INFO\', \'stream\': \'ext://sys.stdout\', }, }, \'loggers\': { \'my_logger\': { \'handlers\': [\'console\'], \'level\': \'INFO\', \'propagate\': False }, } } file_path = \'logging.ini\' # Assume a valid INI file exists at this path. configure_logging(dict_config, file_path) # Test the configured logging logger = logging.getLogger(\'my_logger\') logger.info(\'This should appear in both configurations\') ``` Create the required INI configuration file and test the `configure_logging` function accordingly. The function should configure logging in a way that appending configurations from the file should respect the initial dictionary configurations unless overridden. Submission: Submit the `configure_logging` function along with the `logging.ini` file used for testing.","solution":"import logging.config import configparser def configure_logging(dict_config: dict, file_path: str) -> None: Configures logging using both dictionary and INI file. Parameters: dict_config (dict): Logging configuration dictionary. file_path (str): Path to the INI configuration file. try: # Configure logging using dictionary config logging.config.dictConfig(dict_config) logger = logging.getLogger(__name__) logger.info(\\"Logging configured using dictionary\\") # Read the INI configuration file config = configparser.ConfigParser() config.read(file_path) # Configure logging using the INI file logging.config.fileConfig(file_path) logger.info(\\"Logging configured using INI file\\") except Exception as e: logging.basicConfig(level=logging.ERROR) logger = logging.getLogger(__name__) logger.error(f\\"Error configuring logging: {e}\\")"},{"question":"**Question:** You are given the task of analyzing the penguins dataset using seaborn\'s `objects` module. Your goal is to create a set of visualizations that will help understand the relationships between various features of the penguins. # Objective 1. **Create a faceted plot of bill length vs. bill depth:** - Facet the plot by species (columns) and sex (rows). - Each subplot should have Dots representing the data points. - The axes should be shared only across rows and not across columns. 2. **Create a paired plot to compare bill length and bill depth:** - Include two subplots showing the relation between bill length and depth for all penguins. - The x-axes should be shared across all subplots. # Input and Output Formats - **Input:** The penguins dataset is preloaded into the variable `penguins`. - **Output:** The output should be two seaborn plot objects. # Code: Implement the function `create_plots()`: ```python import seaborn.objects as so from seaborn import load_dataset def create_plots(penguins): # Create a faceted plot faceted_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dots()) .share(x=\\"col\\", y=\\"row\\") # Share axes only across rows, not across columns ) # Create a paired plot paired_plot = ( so.Plot(penguins, y=\\"bill_depth_mm\\") .pair(x=[\\"bill_length_mm\\", \\"bill_length_mm\\"]) .add(so.Dots()) .share(x=True) # Share x-axis across all subplots ) return faceted_plot, paired_plot # Load the dataset penguins = load_dataset(\\"penguins\\") # Generate the plots faceted_plot, paired_plot = create_plots(penguins) ``` # Constraints: - Utilize seaborn\'s `objects` module for creating the plots. - Ensure that the plots meet the specified sharing configurations. - Handle any missing data appropriately if encountered in the dataset. # Points for Evaluation: - Correct usage of seaborn functions to create faceted and paired plots. - Proper configuration of axis sharing for each subplot. - Code quality: readability, comments, and structure. Good luck, and happy plotting!","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plots(penguins): # Create a faceted plot faceted_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dots()) .share(y=True) # Share y-axis only across rows ) # Create a paired plot paired_plot = ( so.Plot(penguins, y=\\"bill_depth_mm\\") .pair(x=[\\"bill_length_mm\\", \\"bill_length_mm\\"]) .add(so.Dots()) .share(x=True) # Share x-axis across all subplots ) return faceted_plot, paired_plot # Load the dataset penguins = load_dataset(\\"penguins\\") # Generate the plots faceted_plot, paired_plot = create_plots(penguins)"},{"question":"# Question **Objective:** Write a Python function that leverages the `pwd` module to fetch and display specific user account information. Your function should demonstrate comprehension of using this module for typical tasks such as fetching records by user ID or username and working with the password database entries. **Task:** Implement a function named `fetch_user_details` with the following specification: **Function Signature:** ```python def fetch_user_details(identifier: str) -> dict: ``` **Parameters:** - `identifier` (str): A string that could either be a username or a user ID. **Returns:** - dict: A dictionary containing user information with keys `\\"username\\"`, `\\"user_id\\"`, `\\"group_id\\"`, `\\"user_home\\"`, `\\"user_shell\\"`, and `\\"user_info\\"`. **Behavior:** 1. If the `identifier` is numeric, consider it as a user ID and fetch the user details using `pwd.getpwuid`. 2. Otherwise, treat it as a username and fetch the user details using `pwd.getpwnam`. 3. Handle scenarios where the given `identifier` does not correspond to any user by raising a `ValueError` with an appropriate message. 4. Return a dictionary with user details (see format below). **Example Output Dictionary Format:** ```python { \\"username\\": \\"johndoe\\", \\"user_id\\": 1000, \\"group_id\\": 1000, \\"user_home\\": \\"/home/johndoe\\", \\"user_shell\\": \\"/bin/bash\\", \\"user_info\\": \\"John Doe\\" } ``` **Constraints:** - The function should handle exceptions appropriately and raise a `ValueError` if the user entry for the given identifier does not exist. - Make sure the function is functional on Unix systems and adheres to the characteristics described in the documentation. **Example Usage:** ```python fetch_user_details(\\"1000\\") # Output: {\\"username\\": \\"johndoe\\", \\"user_id\\": 1000, \\"group_id\\": 1000, \\"user_home\\": \\"/home/johndoe\\", \\"user_shell\\": \\"/bin/bash\\", \\"user_info\\": \\"John Doe\\"} fetch_user_details(\\"johndoe\\") # Output: {\\"username\\": \\"johndoe\\", \\"user_id\\": 1000, \\"group_id\\": 1000, \\"user_home\\": \\"/home/johndoe\\", \\"user_shell\\": \\"/bin/bash\\", \\"user_info\\": \\"John Doe\\"} ``` **Note:** Remember to import the `pwd` module in your implementation.","solution":"import pwd def fetch_user_details(identifier: str) -> dict: Fetch and return user details from the password database using either username or user ID. Parameters: identifier (str): A string that could either be a username or a user ID. Returns: dict: A dictionary containing user information. Raises: ValueError: If the user entry does not exist. try: if identifier.isdigit(): user_info = pwd.getpwuid(int(identifier)) else: user_info = pwd.getpwnam(identifier) return { \\"username\\": user_info.pw_name, \\"user_id\\": user_info.pw_uid, \\"group_id\\": user_info.pw_gid, \\"user_home\\": user_info.pw_dir, \\"user_shell\\": user_info.pw_shell, \\"user_info\\": user_info.pw_gecos } except KeyError: raise ValueError(f\\"User with identifier \'{identifier}\' does not exist.\\")"},{"question":"# Asyncio Subprocess Management You are tasked with creating a Python function that asynchronously runs multiple shell commands concurrently and processes their output. Each command\'s output should be collected, decoded, and returned. If any command fails (i.e., returns a non-zero exit code), the function should raise an exception indicating which command failed. Function Signature ```python import asyncio from typing import List, Tuple async def run_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: Run multiple shell commands asynchronously and collect their outputs. Args: - commands (List[str]): A list of commands to run as shell commands. Returns: - List[Tuple[str, int, str, str]]: A list of tuples, each containing: - The command executed. - The return code of the command. - The stdout output of the command. - The stderr output of the command. Raises: - RuntimeError: If any command exits with a non-zero status. ``` Constraints - **Input Constraints**: - Each command in the `commands` list is a valid shell command as a string. - **Output Constraints**: - Each return tuple consists of the command string, its status code, its stdout output, and its stderr output. Requirements - Implement the function using `asyncio.create_subprocess_shell` for running the commands. - Handle stdout and stderr streams to prevent deadlocks. - Raise a `RuntimeError` with a message indicating which command failed if any command exits with a non-zero status. - Ensure the output is properly decoded from bytes to a string format. Example Usage ```python commands = [ \\"echo \'hello world\'\\", \\"ls /nonexistent\\", \\"echo \'this is a test\'\\" ] async def main(): try: results = await run_commands(commands) for cmd, returncode, stdout, stderr in results: print(f\\"Command: {cmd}\\") print(f\\"Return code: {returncode}\\") print(f\\"stdout: {stdout}\\") print(f\\"stderr: {stderr}\\") except RuntimeError as e: print(f\\"Runtime error: {e}\\") asyncio.run(main()) # Expected Output: # Command: echo \'hello world\' # Return code: 0 # stdout: hello world # stderr: # Command: ls /nonexistent # Runtime error: Command \'ls /nonexistent\' failed with exit code 1 ``` Test your function thoroughly to ensure it behaves correctly under various scenarios and edge cases, such as empty commands, commands with significant output, and commands that fail.","solution":"import asyncio from typing import List, Tuple async def run_single_command(command: str) -> Tuple[str, int, str, str]: Helper function to run a single shell command asynchronously and collect its output. process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return (command, process.returncode, stdout.decode().strip(), stderr.decode().strip()) async def run_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: Run multiple shell commands asynchronously and collect their outputs. Args: - commands (List[str]): A list of commands to run as shell commands. Returns: - List[Tuple[str, int, str, str]]: A list of tuples, each containing: - The command executed. - The return code of the command. - The stdout output of the command. - The stderr output of the command. Raises: - RuntimeError: If any command exits with a non-zero status. tasks = [run_single_command(cmd) for cmd in commands] results = await asyncio.gather(*tasks) for command, returncode, stdout, stderr in results: if returncode != 0: raise RuntimeError(f\\"Command \'{command}\' failed with exit code {returncode}\\") return results"}]'),D={name:"App",components:{PoemCard:S},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],O={key:0},M={key:1};function N(i,e,l,m,s,r){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>s.searchQuery=o),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>s.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),n(b,null,w(r.displayedPoems,(o,f)=>(a(),v(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),n("div",R,' No results found for "'+u(s.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[s.isLoading?(a(),n("span",M,"Loading...")):(a(),n("span",O,"See more"))],8,q)):d("",!0)])}const L=p(D,[["render",N],["__scopeId","data-v-c5a348f0"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/44.md","filePath":"chatai/44.md"}'),U={name:"chatai/44.md"},H=Object.assign(U,{setup(i){return(e,l)=>(a(),n("div",null,[x(L)]))}});export{Y as __pageData,H as default};
