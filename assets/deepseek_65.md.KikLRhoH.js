import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},S={class:"review-content"};function P(i,e,l,m,n,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",P],["__scopeId","data-v-cc2996db"]]),I=JSON.parse('[{"question":"# PyTorch Autograd Coding Assessment Objective Demonstrate your understanding of PyTorch\'s autograd functionalities by implementing a custom backward function and utilizing higher-level API functions (Jacobians, Hessians). Problem Statement You are required to implement a PyTorch custom function that behaves like the `torch.exp` function but with a custom backward pass. Additionally, compute the Jacobian and Hessian matrices for a given loss function using PyTorch\'s higher-level API. Custom Function Implementation 1. Create a custom PyTorch function `ExpWithCustomGrad` that computes the exponential of a tensor. 2. Implement a custom backward pass for this function where the gradient is twice the standard gradient of `torch.exp`. Jacobian and Hessian Calculation 1. Given a simple neural network model and a scalar loss function, compute: - The Jacobian of the loss with respect to the model parameters. - The Hessian of the loss with respect to a specific parameter tensor in the model. # Implementation Details - Your custom function should inherit from `torch.autograd.Function` and override the `forward` and `backward` methods. - Use the custom function as the activation function in a simple neural network. - Define a scalar loss function (e.g., mean squared error). - Compute the gradients, Jacobians, and Hessians using the provided higher-level API functions from `torch.autograd.functional`. # Input and Output Formats - **Input**: A tensor `x` of shape (N, M) where N is the number of data points, and M is the number of features. - **Output**: Print the custom gradients, Jacobian, and Hessian matrices. # Constraints - Do not use any pre-existing functions for computing the Jacobian or Hessian other than those provided by `torch.autograd.functional`. # Performance Requirements - Your implementation should handle tensors of size up to (1000, 1000) efficiently. Example Suppose you have an input tensor `x` and a simple neural network with one linear layer followed by your custom activation function. If the loss is defined as the mean squared error between the model\'s output and some target tensor `y`, you should implement: 1. The custom forward and backward passes. 2. The calculation of the Jacobian and Hessian for this loss function. Code Template ```python import torch from torch.autograd import Function import torch.nn as nn import torch.autograd.functional as F class ExpWithCustomGrad(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input.exp() @staticmethod def backward(ctx, grad_output): # Implement custom backward pass input, = ctx.saved_tensors grad_input = 2 * grad_output * input.exp() # Custom gradient return grad_input # Simple neural network using custom activation function class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): x = self.linear(x) x = ExpWithCustomGrad.apply(x) return x # Create model, data, and target model = SimpleModel() x = torch.randn(100, 10, requires_grad=True) y = torch.randn(100, 5) # Define a loss function loss_func = nn.MSELoss() # Forward pass output = model(x) loss = loss_func(output, y) # Compute gradients using backward loss.backward() # Compute Jacobian def loss_fn(model_params): output = model(x) loss = loss_func(output, y) return loss params = list(model.parameters()) jacobian = F.jacobian(loss_fn, params) # Compute Hessian for the first parameter tensor hessian = F.hessian(loss_fn, params[0]) print(\\"Jacobian:\\", jacobian) print(\\"Hessian:\\", hessian) ``` Ensure your implementation adheres to these specifications and test it with different input sizes for robustness.","solution":"import torch from torch.autograd import Function import torch.nn as nn import torch.autograd.functional as F class ExpWithCustomGrad(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input.exp() @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor input, = ctx.saved_tensors # Custom gradient: twice the standard gradient of torch.exp grad_input = 2 * grad_output * input.exp() return grad_input # Simple neural network using custom activation function class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): x = self.linear(x) x = ExpWithCustomGrad.apply(x) return x # Function to compute loss def compute_loss(model, x, y): output = model(x) loss = nn.MSELoss()(output, y) return loss def main_example(): # Create model, data, and target model = SimpleModel() x = torch.randn(100, 10, requires_grad=True) y = torch.randn(100, 5) # Compute loss loss = compute_loss(model, x, y) # Backward pass loss.backward() # Obtain model parameters params = [p for p in model.parameters() if p.requires_grad] # Compute Jacobian and Hessian jacobian = [F.jacobian(lambda p: compute_loss(model, x, y), p) for p in params] hessian = [F.hessian(lambda p: compute_loss(model, x, y), p) for p in params] return loss, jacobian, hessian if __name__ == \\"__main__\\": loss, jacobian, hessian = main_example() print(\\"Loss:\\", loss) print(\\"Jacobian:\\", jacobian) print(\\"Hessian:\\", hessian)"},{"question":"You are given a dataset representing sales data for a chain of stores. Each entry in the dataset includes the `store_id`, `item_id`, `date`, `units_sold`, and `total_sales`. Your task is to implement a function using pandas to assess and visualize various statistics based on the sales data. # Function Signature ```python import pandas as pd import matplotlib.pyplot as plt def analyze_sales(sales_df: pd.DataFrame) -> dict: Function to analyze sales data and provide insights. Parameters: sales_df (pd.DataFrame): A DataFrame containing sales data with columns [\'store_id\', \'item_id\', \'date\', \'units_sold\', \'total_sales\']. Returns: dict: A dictionary containing several analysis results: - \'total_units_sold_per_store\': A DataFrame with total units sold per store. - \'total_sales_per_store\': A DataFrame with total sales per store. - \'average_units_sold_per_item\': A DataFrame with average units sold per item. - \'sales_summary\': A DataFrame with mean, median, std, and total sales for each store. - \'total_sales_per_store_plot_path\': A str indicating the path to the bar plot of total sales per store. ``` # Instructions 1. **Total Units Sold Per Store**: Compute the total number of units sold in each store. 2. **Total Sales Per Store**: Compute the total sales amount for each store. 3. **Average Units Sold Per Item**: Compute the average number of units sold per item across all stores. 4. **Sales Summary**: Provide a summary for each store consisting of the mean, median, standard deviation, and total of the `total_sales`. 5. **Visualization**: Create a bar plot showing the total sales for each store and save the plot to a file, returning the file path in the results dictionary. # Constraints - You can assume that the input DataFrame is valid and contains the specified columns. - Use appropriate pandas functions to group the data and perform the required computations. - The plot should be saved as `total_sales_per_store.png`. # Example Usage ```python # Sample DataFrame data = { \'store_id\': [1, 1, 2, 2, 3, 3, 3], \'item_id\': [101, 102, 101, 103, 102, 101, 104], \'date\': [\'2021-01-01\', \'2021-01-01\', \'2021-01-01\', \'2021-01-01\', \'2021-01-01\', \'2021-01-01\', \'2021-01-01\'], \'units_sold\': [10, 15, 13, 7, 10, 12, 5], \'total_sales\': [100, 150, 130, 70, 100, 120, 50] } # Load data into DataFrame sales_df = pd.DataFrame(data) # Analyze sales results = analyze_sales(sales_df) # Access the results total_units_sold_per_store = results[\'total_units_sold_per_store\'] total_sales_per_store = results[\'total_sales_per_store\'] average_units_sold_per_item = results[\'average_units_sold_per_item\'] sales_summary = results[\'sales_summary\'] total_sales_per_store_plot_path = results[\'total_sales_per_store_plot_path\'] print(total_units_sold_per_store) print(total_sales_per_store) print(average_units_sold_per_item) print(sales_summary) print(f\\"Plot saved at: {total_sales_per_store_plot_path}\\") ```","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales(sales_df: pd.DataFrame) -> dict: Function to analyze sales data and provide insights. Parameters: sales_df (pd.DataFrame): A DataFrame containing sales data with columns [\'store_id\', \'item_id\', \'date\', \'units_sold\', \'total_sales\']. Returns: dict: A dictionary containing several analysis results: - \'total_units_sold_per_store\': A DataFrame with total units sold per store. - \'total_sales_per_store\': A DataFrame with total sales per store. - \'average_units_sold_per_item\': A DataFrame with average units sold per item. - \'sales_summary\': A DataFrame with mean, median, std, and total sales for each store. - \'total_sales_per_store_plot_path\': A str indicating the path to the bar plot of total sales per store. # Total Units Sold Per Store total_units_sold_per_store = sales_df.groupby(\'store_id\')[\'units_sold\'].sum().reset_index() # Total Sales Per Store total_sales_per_store = sales_df.groupby(\'store_id\')[\'total_sales\'].sum().reset_index() # Average Units Sold Per Item average_units_sold_per_item = sales_df.groupby(\'item_id\')[\'units_sold\'].mean().reset_index() # Sales Summary sales_summary = sales_df.groupby(\'store_id\')[\'total_sales\'].agg([\'mean\', \'median\', \'std\', \'sum\']).reset_index() # Visualization total_sales_per_store.plot(kind=\'bar\', x=\'store_id\', y=\'total_sales\', legend=False) plt.xlabel(\'Store ID\') plt.ylabel(\'Total Sales\') plt.title(\'Total Sales per Store\') plot_path = \'total_sales_per_store.png\' plt.savefig(plot_path) plt.close() return { \'total_units_sold_per_store\': total_units_sold_per_store, \'total_sales_per_store\': total_sales_per_store, \'average_units_sold_per_item\': average_units_sold_per_item, \'sales_summary\': sales_summary, \'total_sales_per_store_plot_path\': plot_path }"},{"question":"**Objective:** Your task is to implement a class `Cell` that mimics the behavior of PyCellObject described in the documentation. The class should support the following operations: - Create a new cell object with an initial value. - Retrieve the current value of the cell. - Set a new value for the cell. # Class and Methods Description ```python class Cell: def __init__(self, initial_value: any): Initialize the cell with an initial value. :param initial_value: The initial value to store in the cell. # Your code here def get(self) -> any: Retrieve the value stored in the cell. :return: The current value in the cell. # Your code here def set(self, value: any): Set a new value in the cell. :param value: The new value to store in the cell. # Your code here ``` # Requirements 1. Implement the `Cell` class and its methods: - `__init__(self, initial_value: any)`: Initializes the cell with an initial value. - `get(self) -> any`: Returns the current value stored in the cell. - `set(self, value: any)`: Updates the cell with a new value. 2. Ensure the class handles different types of data (e.g., integers, strings, objects). 3. Enforce reference handling, i.e., the `set` method should replace the old reference with the new one. # Constraints - Do not use any cell-specific modules or libraries from Python C API. - The implementation should be in pure Python. # Example Usage ```python # Creating a cell with initial value 10 cell = Cell(10) # Getting the value print(cell.get()) # Output: 10 # Setting a new value cell.set(20) print(cell.get()) # Output: 20 # Setting a string value cell.set(\\"Hello\\") print(cell.get()) # Output: Hello ``` # Additional Notes This question assesses your understanding of closures, reference management, and basic object-oriented programming concepts. Ensure your code follows best practices and is well-documented.","solution":"class Cell: def __init__(self, initial_value: any): Initialize the cell with an initial value. :param initial_value: The initial value to store in the cell. self.value = initial_value def get(self) -> any: Retrieve the value stored in the cell. :return: The current value in the cell. return self.value def set(self, value: any): Set a new value in the cell. :param value: The new value to store in the cell. self.value = value"},{"question":"Objective Implement a function that plots both PDP and ICE plots based on a trained model, using the `sklearn` library. Problem Statement You are given a trained classification model, a dataset, and an index of features. Your task is to implement two functions: 1. `plot_partial_dependence` - to generate and display Partial Dependence Plots (PDP) for one or two specified features. 2. `plot_individual_conditional_expectation` - to generate and display Individual Conditional Expectation (ICE) plots for a specified feature. # Function Signature ```python def plot_partial_dependence(model, X, features): Plots Partial Dependence for specified features. Parameters: model: The trained model. X: The dataset (2D array-like structure). features: A list of feature indices or tuples (for two-way PDP). Returns: None # Your code here def plot_individual_conditional_expectation(model, X, feature): Plots Individual Conditional Expectation for the specified feature. Parameters: model: The trained model. X: The dataset (2D array-like structure). feature: The index of the feature for ICE plot. Returns: None # Your code here ``` # Example ```python from sklearn.datasets import make_classification from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay # Generating a sample dataset X, y = make_classification(n_samples=1000, n_features=10, random_state=42) # Training a model model = GradientBoostingClassifier().fit(X, y) # Plotting Partial Dependence for features 0 and (0, 1) plot_partial_dependence(model, X, [0, (0, 1)]) # Plotting Individual Conditional Expectation for feature 0 plot_individual_conditional_expectation(model, X, 0) ``` # Constraints - You should use `sklearn.inspection.PartialDependenceDisplay` for plotting. - The dataset `X` should be a 2D array-like structure. - Handle any necessary imports within your functions. # Notes - Ensure you provide a visually clear and interpretable plot. - PDP should provide insight into the average effect of the features. - ICE plots should visualize the effect for each sample separately.","solution":"import matplotlib.pyplot as plt from sklearn.inspection import PartialDependenceDisplay def plot_partial_dependence(model, X, features): Plots Partial Dependence for specified features. Parameters: model: The trained model. X: The dataset (2D array-like structure). features: A list of feature indices or tuples (for two-way PDP). Returns: None display = PartialDependenceDisplay.from_estimator(model, X, features) display.figure_.suptitle(\'Partial Dependence Plots\') plt.show() def plot_individual_conditional_expectation(model, X, feature): Plots Individual Conditional Expectation for the specified feature. Parameters: model: The trained model. X: The dataset (2D array-like structure). feature: The index of the feature for ICE plot. Returns: None display = PartialDependenceDisplay.from_estimator(model, X, [feature], kind=\'individual\') display.figure_.suptitle(\'Individual Conditional Expectation Plots\') plt.show()"},{"question":"# Advanced Python Programming Assessment **Objective:** Demonstrate the ability to marshal and unmarshal Python objects using serialization techniques. **Background:** Python provides a way to serialize (marshal) objects into a byte stream using the `marshal` module. In this task, you will implement a Python class that uses these concepts to write and read data to and from a binary file. **Task:** 1. Implement a class `DataMarshal` with the following methods: - `write_long_to_file(value: int, file_path: str, version: int)`: This method should write a long integer to the specified file using the given version. - `write_object_to_file(obj: any, file_path: str, version: int)`: This method should write a Python object to the specified file using the given version. - `write_object_to_string(obj: any, version: int) -> bytes`: This method should return a bytes object containing the marshalled representation of the given Python object. - `read_long_from_file(file_path: str) -> int`: This method should read a long integer from the specified file. - `read_short_from_file(file_path: str) -> int`: This method should read a short integer from the specified file. - `read_object_from_file(file_path: str) -> any`: This method should read a Python object from the specified file. - `read_last_object_from_file(file_path: str) -> any`: This method should read the last Python object from the specified file, ensuring no further objects are read. - `read_object_from_string(data: bytes) -> any`: This method should return a Python object from the given byte buffer. **Constraints:** - You are not allowed to use any other serialization library like `pickle` or `json`. Use the `marshal` module only. - Handle possible exceptions and errors gracefully, providing meaningful error messages. **Input and Output Formats:** 1. `write_long_to_file(value: int, file_path: str, version: int)` - **Input:** An integer value, a string representing the file path, and an integer version. - **Output:** Write the long integer to the specified file. 2. `write_object_to_file(obj: any, file_path: str, version: int)` - **Input:** A Python object, a string representing the file path, and an integer version. - **Output:** Write the object to the specified file. 3. `write_object_to_string(obj: any, version: int) -> bytes` - **Input:** A Python object and an integer version. - **Output:** Return the marshalled byte string of the object. 4. `read_long_from_file(file_path: str) -> int` - **Input:** A string representing the file path. - **Output:** Return the long integer read from the file. 5. `read_short_from_file(file_path: str) -> int` - **Input:** A string representing the file path. - **Output:** Return the short integer read from the file. 6. `read_object_from_file(file_path: str) -> any` - **Input:** A string representing the file path. - **Output:** Return the Python object read from the file. 7. `read_last_object_from_file(file_path: str) -> any` - **Input:** A string representing the file path. - **Output:** Return the last Python object read from the file. 8. `read_object_from_string(data: bytes) -> any` - **Input:** A byte buffer containing serialized data. - **Output:** Return the Python object deserialized from the byte buffer. **Example Usage:** ```python # Create DataMarshal instance dm = DataMarshal() # Write long integer to file dm.write_long_to_file(1234567890, \'long_data.bin\', 2) # Write object to file dm.write_object_to_file({\'key\': \'value\'}, \'object_data.bin\', 2) # Marshal object to string marshalled_str = dm.write_object_to_string([1, 2, 3], 2) # Read long integer from file long_value = dm.read_long_from_file(\'long_data.bin\') print(long_value) # Output: 1234567890 # Read object from file obj = dm.read_object_from_file(\'object_data.bin\') print(obj) # Output: {\'key\': \'value\'} # Deserialize object from byte string unmarshalled_obj = dm.read_object_from_string(marshalled_str) print(unmarshalled_obj) # Output: [1, 2, 3] ```","solution":"import marshal class DataMarshal: def write_long_to_file(self, value: int, file_path: str, version: int): with open(file_path, \'wb\') as file: marshal.dump(value, file, version) def write_object_to_file(self, obj: any, file_path: str, version: int): with open(file_path, \'wb\') as file: marshal.dump(obj, file, version) def write_object_to_string(self, obj: any, version: int) -> bytes: return marshal.dumps(obj, version) def read_long_from_file(self, file_path: str) -> int: with open(file_path, \'rb\') as file: return marshal.load(file) def read_short_from_file(self, file_path: str) -> int: with open(file_path, \'rb\') as file: return marshal.load(file) def read_object_from_file(self, file_path: str) -> any: with open(file_path, \'rb\') as file: return marshal.load(file) def read_last_object_from_file(self, file_path: str) -> any: with open(file_path, \'rb\') as file: last_object = None while True: try: last_object = marshal.load(file) except EOFError: break return last_object def read_object_from_string(self, data: bytes) -> any: return marshal.loads(data)"},{"question":"**Binary Data Manipulation using struct and codecs** **Objective**: You are required to demonstrate your understanding of the `struct` and `codecs` modules in Python by manipulating binary data. You will write a Python function to interpret a series of binary data records and output human-readable information. **Problem Statement**: Given a binary file containing multiple records, each record follows this structure: 1. An integer (4 bytes) representing a user ID. 2. A 20-byte string representing a username encoded in UTF-8. 3. A double precision float (8 bytes) representing the user\'s account balance. Write a function `parse_binary_data(file_path: str) -> List[Dict[str, Any]]` that reads the binary file and returns a list of dictionaries. Each dictionary should contain the keys \'user_id\', \'username\', and \'balance\' with their respective values. **Input Format**: - `file_path`: A string representing the path to the binary file. **Output Format**: - Return a list of dictionaries where each dictionary contains: - `user_id` (int): The user\'s ID. - `username` (str): The UTF-8 decoded username string. - `balance` (float): The user\'s account balance. **Constraints**: - You can assume the binary file is correctly formatted as described. - The file might contain multiple records or could be empty. - Ensure efficient reading and parsing, especially with large files. **Example**: Consider the following example where `example.bin` is a binary file with two records: Content of `example.bin`: - Record 1: [user_id: 1, username: \'Alice\', balance: 1234.56] - Record 2: [user_id: 2, username: \'Bob\', balance: 7890.12] ```python def parse_binary_data(file_path: str) -> List[Dict[str, Any]]: import struct import codecs result = [] # Define the format string according to the record layout record_format = \'i20sd\' record_size = struct.calcsize(record_format) with open(file_path, \'rb\') as f: while record_bytes := f.read(record_size): # Unpack the binary data into components user_id, username_bytes, balance = struct.unpack(record_format, record_bytes) # Decode the UTF-8 encoded username, remove padding username = codecs.decode(username_bytes, \'utf-8\').rstrip(\'x00\') # Append the record to the result list as a dictionary result.append({ \'user_id\': user_id, \'username\': username, \'balance\': balance }) return result # Testing with `example.bin` file file_path = \'example.bin\' print(parse_binary_data(file_path)) ``` In this example, `parse_binary_data(\'example.bin\')` should return: ```python [ {\'user_id\': 1, \'username\': \'Alice\', \'balance\': 1234.56}, {\'user_id\': 2, \'username\': \'Bob\', \'balance\': 7890.12} ] ``` The provided function efficiently reads the binary file, interprets each record according to the specified format, and returns a list of dictionaries with human-readable data.","solution":"def parse_binary_data(file_path: str) -> list: import struct import codecs result = [] # Define the format string according to the record layout record_format = \'i20sd\' record_size = struct.calcsize(record_format) with open(file_path, \'rb\') as f: while record_bytes := f.read(record_size): # Unpack the binary data into components user_id, username_bytes, balance = struct.unpack(record_format, record_bytes) # Decode the UTF-8 encoded username, remove padding username = codecs.decode(username_bytes, \'utf-8\').rstrip(\'x00\') # Append the record to the result list as a dictionary result.append({ \'user_id\': user_id, \'username\': username, \'balance\': balance }) return result"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s `histplot` by creating comprehensive visualizations of given datasets. You will be required to utilize various parameters to customize the plots effectively. Problem Description **Dataset:** Use the \'penguins\' dataset available through Seaborn. This dataset provides information about penguin species, island locations, bill dimensions, flipper length, body mass, and sex. Implement a function `custom_histogram_plots` as described below: ```python import seaborn as sns import matplotlib.pyplot as plt def custom_histogram_plots(): # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Basic univariate histogram plot for \'flipper_length_mm\' with 30 bins plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=30) plt.title(\'Basic univariate histogram for Flipper Length\') plt.show() # Task 2: Bivariate histogram plot for \'bill_depth_mm\' and \'body_mass_g\' with species hue plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", bins=30) plt.title(\'Bivariate histogram for Bill Depth and Body Mass with Species Hue\') plt.show() # Task 3: Normalize \'bill_length_mm\' to show density and use a step function plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", element=\\"step\\", stat=\\"density\\", common_norm=False) plt.title(\'Density-normalized Bill Length distribution with step function\') plt.show() # Task 4: Compare distributions of \'bill_length_mm\' on different islands using a log scale plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"island\\", element=\\"step\\", log_scale=True) plt.title(\'Bill Length distribution on different islands with log scale\') plt.show() # Task 5: Annotate a bivariate histogram with a color bar for \'bill_depth_mm\' and \'body_mass_g\' plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", bins=30, cbar=True, cbar_kws=dict(shrink=.75)) plt.title(\'Annotated bivariate histogram with color bar\') plt.show() ``` Requirements 1. **Data Visualization Skills**: - You must be able to plot histograms with customized bin widths and counts. - You must create both univariate and bivariate histograms. - Add kernel density estimates for smoother distribution visualization. - Differentiate categories using color (hue) and manage overlapping using appropriate visual elements (e.g., step, stack). - Utilize log scales for skewed data. 2. **Chart Customization**: - Your plots should include appropriate titles for better interpretation. - Utilize color bars and annotations where suitable to indicate data distributions effectively. 3. **Function Implementation**: - Visualizations should be displayed one at a time (make use of plt.show()). - Your function should not return any value. The main purpose of this assessment is to evaluate your ability to leverage Seaborn\'s functionalities for creating detailed and informative visual data representations. **Constraint:** Ensure code readability with comments explaining each major step in your implementation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_histogram_plots(): # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Basic univariate histogram plot for \'flipper_length_mm\' with 30 bins plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=30) plt.title(\'Basic univariate histogram for Flipper Length\') plt.show() # Task 2: Bivariate histogram plot for \'bill_depth_mm\' and \'body_mass_g\' with species hue plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", bins=30) plt.title(\'Bivariate histogram for Bill Depth and Body Mass with Species Hue\') plt.show() # Task 3: Normalize \'bill_length_mm\' to show density and use a step function plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", element=\\"step\\", stat=\\"density\\", common_norm=False) plt.title(\'Density-normalized Bill Length distribution with step function\') plt.show() # Task 4: Compare distributions of \'bill_length_mm\' on different islands using a log scale plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"island\\", element=\\"step\\", log_scale=True) plt.title(\'Bill Length distribution on different islands with log scale\') plt.show() # Task 5: Annotate a bivariate histogram with a color bar for \'bill_depth_mm\' and \'body_mass_g\' plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", bins=30, cbar=True, cbar_kws=dict(shrink=.75)) plt.title(\'Annotated bivariate histogram with color bar\') plt.show()"},{"question":"**Python Coding Challenge: Build a Simple REPL** You are required to implement a simple Read-Eval-Print Loop (REPL) using the `code` module from the Python standard library. The REPL should allow users to enter Python commands, evaluate them, and print the result or any errors that occur. # Requirements 1. **InteractiveInterpreter Class**: - Customize the `InteractiveInterpreter` class to include error handling and display of both syntax and runtime errors. - Override the `write` method to ensure that error messages are displayed properly. 2. **REPL Implementation**: - Create an instance of your customized `InteractiveInterpreter` class. - Implement a REPL that continuously prompts the user for Python commands using `sys.ps1` for the primary prompt (`>>>`) and `sys.ps2` for the continuation prompt (`...`). - Use the `runsource` method to compile and execute the user input. If the input is incomplete, prompt the user for additional input. # Constraints - You should not use external libraries, only the standard library is allowed. - The REPL should handle multi-line commands correctly. - Ensure that the REPL exits gracefully when users enter the `exit()` command or press `Ctrl+D` (EOF). # Expected Input and Output - **Input**: Python commands entered interactively by the user. - **Output**: The result of the evaluated command or any error messages if the command is invalid. # Example ``` >>> a = 10 >>> print(a) 10 >>> for i in range(a): ... print(i) ... 0 1 2 3 4 5 6 7 8 9 >>> exit() ``` # Implementation ```python import sys import code class MyInteractiveInterpreter(code.InteractiveInterpreter): def __init__(self, locals=None): super().__init__(locals) def write(self, data): sys.stderr.write(data) def showsyntaxerror(self, filename=None): sys.stderr.write(\\"Syntax error in the provided code.n\\") super().showsyntaxerror(filename) def showtraceback(self): sys.stderr.write(\\"An error occurred during code execution.n\\") super().showtraceback() def start_repl(): interpreter = MyInteractiveInterpreter() buffer = \'\' try: while True: prompt = sys.ps1 if not buffer else sys.ps2 line = input(prompt) buffer += line + \'n\' more = interpreter.runsource(buffer) if not more: buffer = \'\' except EOFError: print(\\"Exiting the REPL. Goodbye!\\") if __name__ == \\"__main__\\": start_repl() ``` Implement the `MyInteractiveInterpreter` class as defined above and ensure that your REPL matches the given requirements.","solution":"import sys import code class MyInteractiveInterpreter(code.InteractiveInterpreter): def __init__(self, locals=None): super().__init__(locals) def write(self, data): sys.stderr.write(data) def showsyntaxerror(self, filename=None): sys.stderr.write(\\"Syntax error in the provided code.n\\") super().showsyntaxerror(filename) def showtraceback(self): sys.stderr.write(\\"An error occurred during code execution.n\\") super().showtraceback() def start_repl(): interpreter = MyInteractiveInterpreter() buffer = \'\' try: while True: prompt = sys.ps1 if not buffer else sys.ps2 line = input(prompt) buffer += line + \'n\' more = interpreter.runsource(buffer) if not more: buffer = \'\' except EOFError: print(\\"Exiting the REPL. Goodbye!\\") if __name__ == \\"__main__\\": start_repl()"},{"question":"Objective To assess the student\'s ability to implement context managers for resource handling and apply advanced Python concepts in a practical scenario. Problem Statement You are required to implement a custom context manager using the `contextlib` module that handles resource management for a database connection. 1. **Database Connection Emulator Class**: - Implement a `DatabaseConnection` class to emulate a database connection with the following methods: - `connect()`: Simulates opening a connection (prints \\"Database connection opened.\\") - `close()`: Simulates closing a connection (prints \\"Database connection closed.\\") - `execute(query: str)`: Simulates executing a SQL query (prints \\"Executing query: {query}\\") 2. **Custom Context Manager**: - Implement a custom context manager named `database_manager` using the `contextlib` module that manages an instance of `DatabaseConnection`. - The context manager should ensure that the database connection is opened when entering the context and closed when exiting the context, even if an exception occurs. 3. **Function to Use the Context Manager**: - Implement a function `run_queries(queries: List[str])` that: - Takes a list of SQL queries. - Uses the `database_manager` context manager to open and close the database connection. - Executes each query using the `DatabaseConnection.execute(query)` method within the context. Input - `queries`: List of SQL queries as strings. Example: `[\\"SELECT * FROM table\\", \\"INSERT INTO table VALUES (\'data\')\\"]` Output - The function should print the connection status and executed queries in the sequence they occur, including messages generated by the context manager. Constraints - Use the `contextlib` module for creating the context manager. - Ensure that the connection is properly closed even if there are exceptions. Example Usage ```python def main(): queries = [ \\"SELECT * FROM users\\", \\"UPDATE users SET name = \'John\' WHERE id = 1\\" ] run_queries(queries) if __name__ == \\"__main__\\": main() ``` **Expected Output**: ```plaintext Database connection opened. Executing query: SELECT * FROM users Executing query: UPDATE users SET name = \'John\' WHERE id = 1 Database connection closed. ``` Ensure your implementation is clean and handles the resource management efficiently following the best practices of using context managers in Python. Performance Requirements The solution should efficiently handle up to 1000 queries without significant delays or performance degradation.","solution":"from contextlib import contextmanager class DatabaseConnection: def connect(self): print(\\"Database connection opened.\\") def close(self): print(\\"Database connection closed.\\") def execute(self, query: str): print(f\\"Executing query: {query}\\") @contextmanager def database_manager(): db = DatabaseConnection() try: db.connect() yield db finally: db.close() def run_queries(queries): with database_manager() as db: for query in queries: db.execute(query)"},{"question":"# Mocking and Patching with `unittest.mock` Task You are given a Python class `DatabaseClient` which connects to a database and fetches user details. Write a test suite using `unittest.mock` to perform the following tests on the `get_user_details` method of the `DatabaseClient`. `DatabaseClient` Implementation ```python class DatabaseClient: def __init__(self, db_connection): self.db_connection = db_connection def get_user_details(self, user_id): cursor = self.db_connection.cursor() cursor.execute(\\"SELECT * FROM users WHERE id = ?\\", (user_id,)) return cursor.fetchone() ``` Requirements 1. **Test Initialization**: * Mock the database connection. * Mock the cursor object within the `DatabaseClient` class. 2. **Test Method Call**: * Ensure that the `execute` method of the cursor is called with the correct SQL query and the user ID. 3. **Test Return Value**: * Test the return value of `get_user_details` to ensure it correctly fetches and returns user details. 4. **Test Exception Handling**: * Simulate a situation where the `execute` method raises an exception and test how `get_user_details` handles it. Constraints * Use only `unittest.mock` methods for mocking and patching. * Ensure that the original database connection is not affected by the tests. * Your tests should not require a real database connection. Example Test Case Here\'s a structure to guide your implementation: ```python import unittest from unittest.mock import Mock, patch class TestDatabaseClient(unittest.TestCase): @patch(\'path.to.DatabaseClient.db_connection\') def test_get_user_details(self, mock_db_connection): # Initialize the mock cursor and set up return values # Check if `execute` is called with correct parameters # Verify the return value of `get_user_details` # Handle exception scenarios # Add your detailed implementation here. if __name__ == \'__main__\': unittest.main() ``` Provide a comprehensive test suite that covers all the outlined requirements.","solution":"from unittest.mock import Mock class DatabaseClient: def __init__(self, db_connection): self.db_connection = db_connection def get_user_details(self, user_id): cursor = self.db_connection.cursor() cursor.execute(\\"SELECT * FROM users WHERE id = ?\\", (user_id,)) return cursor.fetchone()"},{"question":"You are tasked with designing a utility that processes a list of filenames (or URLs) and categorizes them based on their MIME types. Your goal is to implement a function that reads from a custom MIME types file, processes the filenames, and returns a detailed summary. Function Signature: ```python def process_filenames(mime_file: str, filenames: list) -> dict: Process the list of filenames based on MIME types defined in the custom MIME types file. Parameters: mime_file (str): Path to the custom MIME types file. filenames (list): List of filenames or URLs to be processed. Returns: dict: A dictionary where: - Keys are MIME types. - Values are lists of filenames that match the MIME type. Raises: FileNotFoundError: If the mime_file does not exist. ValueError: If any filename in the list is invalid or cannot be processed. ``` Example Usage: ```python mime_file = \'custom_mime.types\' filenames = [ \'example.txt\', \'example.html\', \'example.jpg\', \'example.unknown\' ] summary = process_filenames(mime_file, filenames) print(summary) ``` Expected Output: ```python { \'text/plain\': [\'example.txt\'], \'text/html\': [\'example.html\'], \'image/jpeg\': [\'example.jpg\'], \'application/octet-stream\': [\'example.unknown\'] } ``` Constraints and Notes: 1. The custom MIME types file will have the standard format, mapping filename extensions to MIME types. 2. If a MIME type cannot be determined for a filename, categorize it under `\'application/octet-stream\'`. 3. Use the `mimetypes` module functions where applicable to handle the processing. 4. Perform necessary error handling to ensure robustness against invalid inputs. This question tests the student’s ability to integrate file reading, dictionary handling, and exception management with the `mimetypes` module, ensuring a comprehensive understanding of the module’s capabilities and typical use cases.","solution":"import mimetypes def process_filenames(mime_file: str, filenames: list) -> dict: Process the list of filenames based on MIME types defined in the custom MIME types file. Parameters: mime_file (str): Path to the custom MIME types file. filenames (list): List of filenames or URLs to be processed. Returns: dict: A dictionary where: - Keys are MIME types. - Values are lists of filenames that match the MIME type. Raises: FileNotFoundError: If the mime_file does not exist. ValueError: If any filename in the list is invalid or cannot be processed. try: with open(mime_file, \'r\') as file: for line in file: parts = line.strip().split() if len(parts) > 1: mime_type = parts[0] extensions = parts[1:] for ext in extensions: mimetypes.add_type(mime_type, f\'.{ext}\') except FileNotFoundError: raise FileNotFoundError(f\\"The mime file \'{mime_file}\' does not exist.\\") summary = {} for filename in filenames: if not isinstance(filename, str) or not filename: raise ValueError(f\\"Invalid filename: {filename}\\") mime_type, _ = mimetypes.guess_type(filename) if mime_type is None: mime_type = \'application/octet-stream\' if mime_type not in summary: summary[mime_type] = [] summary[mime_type].append(filename) return summary"},{"question":"**Persistent Dictionary Implementation using `shelve`** # Objective: Your task is to implement a function to manage a persistent dictionary using the `shelve` module in Python. # Function Signature: ```python def manage_persistent_dict(filename: str, operations: list) -> list: ``` # Input Description: 1. `filename` (str): The name of the file to be used for the underlying database. 2. `operations` (list): A list of operations to be performed on the persistent dictionary. Each operation is a tuple where the first element is a string describing the operation type. The following operation types are supported: - `(\\"set\\", key, value)`: Set the specified key to the given value. - `(\\"get\\", key)`: Retrieve the value associated with the specified key. - `(\\"delete\\", key)`: Delete the specified key from the dictionary. - `(\\"exists\\", key)`: Check if the specified key exists in the dictionary. - `(\\"list_keys\\",)`: List all keys currently in the dictionary. # Output Description: - The function should return a list of results corresponding to each operation in the input list. For each \\"get\\" operation, return the retrieved value or `None` if the key does not exist. For \\"exists\\" operations, return `True` or `False`. For \\"list_keys\\", return the list of keys. For \\"set\\" and \\"delete\\" operations, return `None`. # Example: ```python filename = \\"my_shelf.db\\" operations = [ (\\"set\\", \\"key1\\", \\"value1\\"), (\\"set\\", \\"key2\\", [1, 2, 3]), (\\"get\\", \\"key1\\"), # should return \\"value1\\" (\\"exists\\", \\"key3\\"), # should return False (\\"delete\\", \\"key1\\"), (\\"get\\", \\"key1\\"), # should return None (\\"list_keys\\",) # should return [\\"key2\\"] ] result = manage_persistent_dict(filename, operations) print(result) ``` # Constraints: - Keys will always be strings. - Values can be any object that the `pickle` module can handle. - Assume the operations list is always valid and contains at least one operation. - The function should handle opening and closing the shelve file properly. - Assume no concurrent read/write access issues. # Implementation Guidelines: - Use `shelve.open()` to manage the persistent dictionary. - Ensure the dictionary is closed properly using a context manager or explicit closing to avoid data loss. - Handle potential exceptions, such as attempting to access or delete non-existent keys gracefully.","solution":"import shelve def manage_persistent_dict(filename: str, operations: list) -> list: results = [] with shelve.open(filename, writeback=True) as db: for operation in operations: op_type = operation[0] if op_type == \\"set\\": key, value = operation[1], operation[2] db[key] = value results.append(None) elif op_type == \\"get\\": key = operation[1] results.append(db.get(key, None)) elif op_type == \\"delete\\": key = operation[1] if key in db: del db[key] results.append(None) elif op_type == \\"exists\\": key = operation[1] results.append(key in db) elif op_type == \\"list_keys\\": results.append(list(db.keys())) else: raise ValueError(f\\"Unknown operation type: {op_type}\\") return results"},{"question":"# XML Processing with Safety Considerations **Objective:** You are tasked with writing a Python function that processes and transforms XML data using the `xml.etree.ElementTree` module. Additionally, you must handle the XML data safely to avoid common security vulnerabilities like the \\"Billion Laughs\\" attack. **Function Specification:** ```python def process_xml(input_xml: str, tag: str, attribute: str, new_value: str) -> str: Processes the given XML string, updates the specified attribute for a given tag to a new value, and safely returns the modified XML string. Parameters: - input_xml (str): A string containing the XML data. - tag (str): The XML tag whose attribute needs to be updated. - attribute (str): The attribute of the tag that needs updating. - new_value (str): The new value for the specified attribute. Returns: - str: The modified XML as a string. Constraints: - The input XML is expected to be well-formed but may not be secure. - You must ensure that the function is not vulnerable to attacks like the \\"Billion Laughs\\" attack. - Use the `xml.etree.ElementTree` module for processing. Example: >>> input_xml = \'<root><element attribute=\\"old_value\\">Text</element></root>\' >>> process_xml(input_xml, \'element\', \'attribute\', \'new_value\') \'<root><element attribute=\\"new_value\\">Text</element></root>\' pass ``` **Instructions:** 1. Parse the XML string using `xml.etree.ElementTree`. 2. Find all elements with the specified tag. 3. Update the specified attribute of each found element to the new value. 4. Convert the modified XML tree back to a string. 5. Ensure that the function can handle large inputs without becoming vulnerable to the \\"Billion Laughs\\" attack. **Hints:** - Use the `xml.etree.ElementTree.fromstring` and `xml.etree.ElementTree.tostring` methods for parsing and serializing XML. - Consider using `xml.etree.ElementTree.XMLParser` with the proper security settings to avoid entity expansion attacks. **Performance Requirements:** - The function should work efficiently for typical sizes of XML documents up to a few megabytes in size. - Avoid deep recursion or operations that can lead to stack overflow with heavily nested XML.","solution":"import xml.etree.ElementTree as ET def process_xml(input_xml: str, tag: str, attribute: str, new_value: str) -> str: Processes the given XML string, updates the specified attribute for a given tag to a new value, and safely returns the modified XML string. Parameters: - input_xml (str): A string containing the XML data. - tag (str): The XML tag whose attribute needs to be updated. - attribute (str): The attribute of the tag that needs updating. - new_value (str): The new value for the specified attribute. Returns: - str: The modified XML as a string. Example: >>> input_xml = \'<root><element attribute=\\"old_value\\">Text</element></root>\' >>> process_xml(input_xml, \'element\', \'attribute\', \'new_value\') \'<root><element attribute=\\"new_value\\">Text</element></root>\' # Parse the XML string with protection against entity expansion attacks parser = ET.XMLParser() root = ET.fromstring(input_xml, parser=parser) # Find all elements with the specified tag and update their attribute for elem in root.findall(\\".//\\" + tag): if attribute in elem.attrib: elem.set(attribute, new_value) # Convert the modified XML tree back to a string modified_xml = ET.tostring(root, encoding=\'unicode\') return modified_xml"},{"question":"You are provided with the `mpg` dataset available in seaborn\'s data repository. This dataset contains information about various car attributes. Your task is to create a detailed visualization using seaborn that demonstrates your understanding of the `seaborn.objects` module, specifically the `so.Plot` class and its methods. **Objective:** Create a multifaceted visualization with the following specifications: 1. Plot multiple pairwise relationships among the columns \'displacement\', \'weight\', \'horsepower\', and \'acceleration\' from the `mpg` dataset. 2. Use faceting to divide the plots based on the \'origin\' of the cars. 3. Customize the axes labels to be more descriptive. 4. Include at least one 2D grid of subplots. **Input:** Use the `mpg` dataset from seaborn\'s data repository. **Expected Output:** Four subplots incorporating the following visualizations: 1. Scatter plots showing pairwise relationships between \'displacement\', \'weight\', \'horsepower\', and \'acceleration\'. 2. Facets that separate the visualizations by the \'origin\' column. 3. Customized axes labels that make it clear what each axis represents. 4. At least one 2D grid of subplots aligning the visualizations neatly. **Constraints:** - Utilize the `seaborn.objects` module for all plotting. - Use chaining methods provided by `seaborn.objects.Plot` class to achieve the desired plots. - Ensure the visualization is clear and informative. **Example Code:** The following is a conceptual example to guide your implementation: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset mpg = load_dataset(\\"mpg\\") # Create the plot ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", y0=\\"Horsepower\\", y1=\\"Acceleration\\") .add(so.Dots()) ) # Create a 2D grid of subplots ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .label(y=\\"Miles per Gallon (MPG)\\") .add(so.Dots()) ) ``` Implement the code that meets all the above specifications in a single executable script. The final script should display the plots to the user.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_multifaceted_visualizations(): # Load the mpg dataset mpg = load_dataset(\\"mpg\\") # Pairwise relationships among \'displacement\', \'weight\', \'horsepower\', \'acceleration\' ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", y0=\\"Horsepower\\", y1=\\"Acceleration\\") .add(so.Dots()) .show() ) # 2D grid of subplots ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"acceleration\\"], wrap=2) .label(y=\\"Miles per Gallon (MPG)\\") .add(so.Dots()) .show() )"},{"question":"# Advanced PyTorch Storage Manipulation Objective The goal of this task is to assess your understanding of PyTorch\'s storage system and how it underpins the tensor operations. Specifically, you will work with the `torch.UntypedStorage` to perform storage-level manipulations and view transformations. Question Implement a function `storage_manipulation` that performs the following operations: 1. **Create a Tensor**: Initialize a tensor `t1` of shape `(4, 4)` filled with ones using `torch.ones`. 2. **Double the Values**: Create a new tensor `t2` that points to the same storage as `t1` but contains doubled values of `t1`. 3. **Reset the Storage**: Create a new empty storage `s2` of the same size as the storage underlying `t1` and set all its values to zero. 4. **Repoint t1**: Change the storage of `t1` to `s2`, effectively resetting `t1` to zeros without altering its shape, stride, or dtype. Constraints - The tensor `t2` should not allocate new storage but should demonstrate how the underlying data can be manipulated. - After executing the function, `t1` should reflect the zeroed storage `s2` (shape `(4, 4)`), while `t2` should show doubled values from the original `t1` (shape `(4, 4)`). Input - None Output - Return a tuple `(t1, t2)` representing the tensors after the described manipulations. Function Signature ```python import torch def storage_manipulation(): # Step 1: Create tensor t1 t1 = torch.ones(4, 4) # Step 2: Double the values in t2 using the same storage s1 = t1.untyped_storage() t2 = torch.zeros(4, 4) t2.set_(s1, storage_offset=t1.storage_offset(), stride=t1.stride(), size=t1.size()) t2 *= 2 # Step 3: Create new storage s2 and fill with zeros s2 = s1.clone().fill_(0) # Step 4: Repoint t1 to the new zeroed storage s2 t1.set_(s2, storage_offset=t1.storage_offset(), stride=t1.stride(), size=t1.size()) return t1, t2 ``` Example Calling `t1, t2 = storage_manipulation()` should produce: - `t1` as a `(4, 4)` tensor filled with zeros. - `t2` as a `(4, 4)` tensor originally pointing to `t1` storage, hence containing the initial doubled values.","solution":"import torch def storage_manipulation(): # Step 1: Create tensor t1 t1 = torch.ones(4, 4) # Step 2: Double the values in t2 using the same storage s1 = t1.untyped_storage() t2 = torch.zeros(4, 4) t2.set_(s1, storage_offset=t1.storage_offset(), stride=t1.stride(), size=t1.size()) t2 *= 2 # Step 3: Create new storage s2 and fill with zeros s2 = s1.clone().fill_(0) # Step 4: Repoint t1 to the new zeroed storage s2 t1.set_(s2, storage_offset=t1.storage_offset(), stride=t1.stride(), size=t1.size()) return t1, t2"},{"question":"# Secured Token Management System You are tasked with developing a secured token management system using Python\'s `secrets` module. Your system should be capable of generating secure tokens for various purposes, including password resets and cryptographic security. You need to implement the following functions: 1. **generate_secure_token(nbytes: int = None) -> str**: - Generate a secure token using the `secrets.token_hex()` function. - If `nbytes` is not specified, use a default value of 32 bytes. - Returns a string representation of the generated token. 2. **generate_custom_password(length: int, min_uppercase: int, min_digits: int) -> str**: - Generate a custom password of a specific length. - Ensure the password contains at least `min_uppercase` uppercase letters and `min_digits` numerical digits. - If the constraints cannot be satisfied with the given length, raise a `ValueError`. - Use the `secrets.choice()` and `string` module for character selection. - Returns the generated password as a string. 3. **compare_tokens(token1: str, token2: str) -> bool**: - Compare two tokens using `secrets.compare_digest()`. - Returns `True` if both tokens are identical, otherwise returns `False`. Input and Output - **generate_secure_token(nbytes: int = None) -> str**: - Input: Optional integer `nbytes` specifying the number of bytes. - Output: Hexadecimal string representation of the token. - **generate_custom_password(length: int, min_uppercase: int, min_digits: int) -> str**: - Input: An integer length of the password, minimum uppercase letters, and minimum digits required. - Output: Generated password string if constraints are satisfied, otherwise raises ValueError. - **compare_tokens(token1: str, token2: str) -> bool**: - Input: Two hexadecimal string tokens. - Output: Boolean indicating whether tokens are identical. Constraints - `length` for passwords should be at least the sum of `min_uppercase` and `min_digits`. - Length, min_uppercase, and min_digits are all non-negative integers. Example usage: ```python secure_token = generate_secure_token() # Generates a 32-byte token by default print(compare_tokens(secure_token, secure_token) == True) # True password = generate_custom_password(10, 2, 3) print(any(c.isupper() for c in password)) # At least 2 uppercase letters in the password print(sum(c.isdigit() for c in password) >= 3) # At least 3 digits in the password ```","solution":"import secrets import string def generate_secure_token(nbytes: int = None) -> str: Generate a secure token using the secrets.token_hex() function. If `nbytes` is not specified, use a default value of 32 bytes. Returns a string representation of the generated token. if nbytes is None: nbytes = 32 return secrets.token_hex(nbytes) def generate_custom_password(length: int, min_uppercase: int, min_digits: int) -> str: Generate a custom password of a specific length. Ensure the password contains at least `min_uppercase` uppercase letters and `min_digits` numerical digits. If the constraints cannot be satisfied with the given length, raise a ValueError. Returns the generated password as a string. if length < min_uppercase + min_digits: raise ValueError(\\"Password length cannot satisfy the constraints with the given length.\\") charset = string.ascii_letters + string.digits + string.punctuation uppercase_chars = [secrets.choice(string.ascii_uppercase) for _ in range(min_uppercase)] digit_chars = [secrets.choice(string.digits) for _ in range(min_digits)] remaining_chars = [secrets.choice(charset) for _ in range(length - min_uppercase - min_digits)] password_list = uppercase_chars + digit_chars + remaining_chars secrets.SystemRandom().shuffle(password_list) return \'\'.join(password_list) def compare_tokens(token1: str, token2: str) -> bool: Compare two tokens using secrets.compare_digest(). Returns True if both tokens are identical, otherwise False. return secrets.compare_digest(token1, token2)"},{"question":"**Question: Monitoring and Managing System Resources** **Objective:** Implement a Python function using Unix-specific services that monitors the current system\'s resource usage and adjusts resource limits based on a set of predetermined thresholds. **Task:** Write a Python function `manage_system_resource_limits()` that performs the following tasks: 1. **Monitor Resource Usage**: Continuously monitor the resource usage of the current process using the `resource` module. 2. **Adjust Resource Limits**: If certain resource usage exceeds specific thresholds, adjust the resource limits to prevent the system from being overloaded. **Function Signature:** ```python def manage_system_resource_limits(cpu_threshold: float, mem_threshold: int): Monitors and manages system resource limits. Args: cpu_threshold (float): The CPU time usage threshold in seconds. mem_threshold (int): The memory usage threshold in kilobytes. Returns: None ``` **Input:** - `cpu_threshold`: A float representing the CPU time usage threshold in seconds. If the current CPU time usage exceeds this threshold, the CPU time limit should be adjusted. - `mem_threshold`: An integer representing the memory usage threshold in kilobytes. If the current memory usage exceeds this threshold, the memory limit should be adjusted. **Output:** - The function does not return any value. It should adjust the limits directly. **Constraints:** - You should only consider the `RLIMIT_CPU` and `RLIMIT_AS` resource limits for CPU time and memory, respectively. - If the current usage exceeds the threshold and you adjust the limit, multiply the old limit by 2. - You should handle any exceptions that arise from system calls gracefully and print relevant error messages. **Example:** ```python # Example usage: manage_system_resource_limits(30.0, 1048576) # CPU threshold is 30 seconds, memory threshold is 1 GB (1048576 KB) ``` **Performance Requirements:** - The function should efficiently monitor and manage resources without causing significant overhead. - The monitoring interval can be set to 5 seconds for this exercise. **Hints:** - Use `resource.getrusage(resource.RUSAGE_SELF)` to get current resource usage. - Use `resource.getrlimit` and `resource.setrlimit` to get and set resource limits. Implement the `manage_system_resource_limits` function considering the above requirements and constraints.","solution":"import resource import time def manage_system_resource_limits(cpu_threshold: float, mem_threshold: int): Monitors and manages system resource limits. Args: cpu_threshold (float): The CPU time usage threshold in seconds. mem_threshold (int): The memory usage threshold in kilobytes. Returns: None while True: try: # Monitor current resource usage usage = resource.getrusage(resource.RUSAGE_SELF) current_cpu_time = usage.ru_utime + usage.ru_stime # user time + system time current_mem_usage = usage.ru_maxrss # max resident set size # Adjust CPU time limit if necessary if current_cpu_time > cpu_threshold: soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_CPU) new_limit = 2 * soft_limit if soft_limit != resource.RLIM_INFINITY else soft_limit resource.setrlimit(resource.RLIMIT_CPU, (new_limit, hard_limit)) print(f\\"Adjusted CPU time limit to: {new_limit} seconds\\") # Adjust memory limit if necessary if current_mem_usage > mem_threshold: soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_AS) new_limit = 2 * soft_limit if soft_limit != resource.RLIM_INFINITY else soft_limit resource.setrlimit(resource.RLIMIT_AS, (new_limit, hard_limit)) print(f\\"Adjusted memory limit to: {new_limit} KB\\") except ValueError as ve: print(f\\"Value error: {ve}\\") except resource.error as re: print(f\\"Resource error: {re}\\") # Monitor at intervals of 5 seconds time.sleep(5) # Example usage # manage_system_resource_limits(30.0, 1048576) # This will start the monitoring loop"},{"question":"You are given a dataset containing information about various products in a store which include an identifier, price, and quantity in stock. Some of the entries might have missing values. Your task is to clean the data by: 1. Converting the identifier to nullable integer type. 2. Replacing any missing prices with the average price. 3. Calculating the total value of available stock for each product, defined as the product of price and quantity. 4. Categorizing the products based on their total value into three categories: \'Low\', \'Medium\', and \'High\'. Use the criteria: - \'Low\' for total value less than the 33rd percentile. - \'Medium\' for total value between the 33rd and 66th percentile. - \'High\' for total value greater than the 66th percentile. **Input Format:** - A pandas DataFrame `df` containing columns: - \'id\' (nullable integers), \'price\' (floats), and \'quantity\' (integers). **Output Format:** - A pandas DataFrame with the original columns along with: - \'total_value\' (float) which is the total value of available stock. - \'category\' (string) which is one of \'Low\', \'Medium\', or \'High\'. **Constraints:** - The DataFrame can have NA values in the \'id\' and \'price\' columns. - The \'quantity\' column will not have any missing values and will always be non-negative. **Function Signature:** ```python import pandas as pd def process_product_data(df: pd.DataFrame) -> pd.DataFrame: # Your code here # Example usage: # df = pd.DataFrame({ # \'id\': [1, 2, None], # \'price\': [10.5, None, 8.0], # \'quantity\': [100, 150, 200] # }) # print(process_product_data(df)) ``` **Performance Requirements:** - The function should run efficiently even with large datasets (up to 1 million rows). **Note:** - You may assume that the provided DataFrame follows the given structure and constraints.","solution":"import pandas as pd def process_product_data(df: pd.DataFrame) -> pd.DataFrame: # Convert the identifier to nullable integer type df[\'id\'] = df[\'id\'].astype(\'Int64\') # Replace any missing prices with the average price average_price = df[\'price\'].mean() df[\'price\'] = df[\'price\'].fillna(average_price) # Calculate the total value of available stock for each product df[\'total_value\'] = df[\'price\'] * df[\'quantity\'] # Categorize the products based on their total value low_threshold = df[\'total_value\'].quantile(0.33) high_threshold = df[\'total_value\'].quantile(0.66) def categorize(total_value): if total_value < low_threshold: return \'Low\' elif total_value <= high_threshold: return \'Medium\' else: return \'High\' df[\'category\'] = df[\'total_value\'].apply(categorize) return df"},{"question":"Question: # Create and Manipulate Python Tuples You are tasked with writing a Python function that processes a list of operations on tuples. You will simulate the creation, modification, and access of tuple elements based on the operations specified. Implement the function `process_tuple_operations` to handle the following operations: 1. **CREATE** - Specifies the creation of a new tuple with given elements. 2. **GET** - Retrieves an element at a specified position. 3. **SET** - Sets an element at a specified position to a new value (not typically allowed in Python but assume mutable-like operations). 4. **SLICE** - Retrieves a slice of the tuple. 5. **SIZE** - Returns the size of the tuple. Create a function `process_tuple_operations(operations)` that takes in a list of operations and performs them on the tuple. The operations are provided as a list of dictionaries, each specifying the operation and its arguments. # Input - `operations`: A list of dictionaries. Each dictionary contains: - `operation`: A string specifying the type of operation (`CREATE`, `GET`, `SET`, `SLICE`, `SIZE`). - `args`: A list of arguments required for the operation. # Output - A list of results corresponding to the provided operations. For operations that do not have a return value (`CREATE` and `SET`), store `None`. # Example ```python operations = [ {\\"operation\\": \\"CREATE\\", \\"args\\": [1, 2, 3, 4, 5]}, {\\"operation\\": \\"GET\\", \\"args\\": [2]}, {\\"operation\\": \\"SET\\", \\"args\\": [1, 8]}, {\\"operation\\": \\"GET\\", \\"args\\": [1]}, {\\"operation\\": \\"SLICE\\", \\"args\\": [1, 4]}, {\\"operation\\": \\"SIZE\\", \\"args\\": []} ] print(process_tuple_operations(operations)) # Output: [None, 3, None, 8, (8, 3, 4), 5] ``` # Constraints - Assume all `GET`, `SET` and `SLICE` operations are valid (i.e., indices are within ranges). - Only one tuple is in context at any point in time. - The operations are processed in the order they appear in the list. # Note While the `SET` operation is not typical in immutable Python tuples, assume we are simulating a mutable tuple-like structure for the sake of this exercise. **Implement the function `process_tuple_operations` to execute the operations as described.**","solution":"def process_tuple_operations(operations): Processes a list of tuple operations and returns the result of each operation. The allowed operations are: - CREATE: Create a new tuple with the given elements. - GET: Retrieve an element at the specified position. - SET: Set an element at the specified position to a new value. - SLICE: Retrieve a slice of the tuple. - SIZE: Return the size of the tuple. Args: - operations (list): List of dictionaries. Each dictionary contains: - \'operation\' (str): Type of operation (\'CREATE\', \'GET\', \'SET\', \'SLICE\', \'SIZE\'). - \'args\' (list): Arguments for the operation. Returns: - list: A list of results corresponding to the provided operations. current_tuple = None results = [] for operation in operations: op_type = operation[\'operation\'] args = operation[\'args\'] if op_type == \'CREATE\': current_tuple = tuple(args) results.append(None) elif op_type == \'GET\': results.append(current_tuple[args[0]]) elif op_type == \'SET\': temp_list = list(current_tuple) temp_list[args[0]] = args[1] current_tuple = tuple(temp_list) results.append(None) elif op_type == \'SLICE\': results.append(current_tuple[args[0]:args[1]]) elif op_type == \'SIZE\': results.append(len(current_tuple)) return results"},{"question":"**Objective:** Demonstrate your understanding of PyTorch\'s parameter initialization functions. **Problem Statement:** You are provided with a simple fully-connected feedforward neural network architecture. Your task is to initialize the weights and biases of this network using specific initialization methods from the `torch.nn.init` module. The neural network architecture is as follows: - Input layer: 10 neurons - Hidden layer 1: 50 neurons - Hidden layer 2: 30 neurons - Output layer: 10 neurons Your task is to implement the function `initialize_parameters(model)` that takes a PyTorch `nn.Module` model as input and initializes the weights and biases using the following methods: 1. Initialize the weights of the hidden layers using `xavier_uniform_`. 2. Initialize the weights of the output layer using `kaiming_normal_`. 3. Initialize all biases to zero using `zeros_`. **Function Signature:** ```python def initialize_parameters(model: torch.nn.Module) -> None: pass ``` **Constraints:** - Use the `torch.nn.init` module for all the initialization methods. - You should not modify the model architecture; only initialize the weights and biases of the provided model. - Assume that the model has fully connected layers defined as `torch.nn.Linear`. **Example Usage:** ```python import torch import torch.nn as nn import torch.nn.init as init class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 30) self.fc3 = nn.Linear(30, 10) def forward(self, x): x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) return x model = SimpleNN() initialize_parameters(model) ``` **Expected Implementation:** Your implementation should correctly initialize the model parameters without returning any values. Instead, it directly alters the weights and biases of the input model.","solution":"import torch import torch.nn as nn import torch.nn.init as init def initialize_parameters(model: torch.nn.Module) -> None: Initialize the weights and biases of the given model. for name, param in model.named_parameters(): if \'weight\' in name: if param.shape[0] == 50 or param.shape[0] == 30: # Hidden layers init.xavier_uniform_(param) elif param.shape[0] == 10: # Output layer init.kaiming_normal_(param, nonlinearity=\'linear\') elif \'bias\' in name: init.zeros_(param)"},{"question":"# Multiclass and Multilabel Classification with scikit-learn You are provided with a dataset of movie genres. Each movie can belong to one or more genres, making this a multilabel classification problem. Additionally, you are tasked to build a multiclass classification on a separate dataset of flower species, which can belong to one and only one class. Your task is to: 1. Implement a multilabel classification model using `OneVsRestClassifier`. 2. Implement a multiclass classification model using `OneVsOneClassifier`. 3. Preprocess the data accordingly for both tasks. 4. Train and evaluate both models using accuracy score and report the performance. The datasets are provided as follows: - `X_movies`: Features of the movies (shape: `(n_samples, n_features)`) - `Y_movies`: Binary matrix indicating genres of each movie (shape: `(n_samples, n_genres)`) - `X_flowers`: Features of the flowers (shape: `(n_samples, n_features)`) - `Y_flowers`: Array indicating the species of each flower (shape: `(n_samples,)`) # Instructions: 1. Use `OneVsRestClassifier` with an appropriate base estimator for the multilabel movie genres classification. 2. Use `OneVsOneClassifier` with an appropriate base estimator for the multiclass flower species classification. 3. Split both datasets into training and testing sets. 4. Train the classifiers and predict on the test data. 5. Calculate and print out the accuracy score for both classifiers. # Expected Input and Output - The input datasets: ```python X_movies = <numpy array> Y_movies = <numpy array> X_flowers = <numpy array> Y_flowers = <numpy array> ``` - The output should be: ```python Movie Genres Classification Accuracy: <accuracy_score> Flower Species Classification Accuracy: <accuracy_score> ``` # Constraints - You may assume that all necessary packages are imported. - Use a 70-30 train-test split for both datasets. - Use `random_state=0` for reproducibility. ```python # Sample Code Template # Import necessary libraries import numpy as np from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score # Given datasets (you can assume these are already provided) X_movies = np.array(...) # Features of the movies Y_movies = np.array(...) # Binary matrix indicating genres X_flowers = np.array(...) # Features of the flowers Y_flowers = np.array(...) # Array indicating species # ---- Multilabel classification for Movie Genres ---- # # Split the movie dataset X_train_movies, X_test_movies, Y_train_movies, Y_test_movies = train_test_split(X_movies, Y_movies, test_size=0.3, random_state=0) # Initialize OneVsRestClassifier with LinearSVC as base estimator ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) # Train the classifier ovr_classifier.fit(X_train_movies, Y_train_movies) # Predict on test data y_pred_movies = ovr_classifier.predict(X_test_movies) # Calculate and print accuracy movie_accuracy = accuracy_score(Y_test_movies, y_pred_movies) print(f\\"Movie Genres Classification Accuracy: {movie_accuracy:.2f}\\") # ---- Multiclass classification for Flower Species ---- # # Split the flowers dataset X_train_flowers, X_test_flowers, Y_train_flowers, Y_test_flowers = train_test_split(X_flowers, Y_flowers, test_size=0.3, random_state=0) # Initialize OneVsOneClassifier with LinearSVC as base estimator ovo_classifier = OneVsOneClassifier(LinearSVC(random_state=0)) # Train the classifier ovo_classifier.fit(X_train_flowers, Y_train_flowers) # Predict on test data y_pred_flowers = ovo_classifier.predict(X_test_flowers) # Calculate and print accuracy flower_accuracy = accuracy_score(Y_test_flowers, y_pred_flowers) print(f\\"Flower Species Classification Accuracy: {flower_accuracy:.2f}\\") ```","solution":"# Import necessary libraries import numpy as np from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score def genre_classification(X_movies, Y_movies): Performs multilabel classification on the movie genres. Returns the accuracy score of the classification. # Split the movie dataset X_train_movies, X_test_movies, Y_train_movies, Y_test_movies = train_test_split(X_movies, Y_movies, test_size=0.3, random_state=0) # Initialize OneVsRestClassifier with LinearSVC as base estimator ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) # Train the classifier ovr_classifier.fit(X_train_movies, Y_train_movies) # Predict on test data y_pred_movies = ovr_classifier.predict(X_test_movies) # Calculate and return accuracy movie_accuracy = accuracy_score(Y_test_movies, y_pred_movies) return movie_accuracy def flower_species_classification(X_flowers, Y_flowers): Performs multiclass classification on the flower species. Returns the accuracy score of the classification. # Split the flowers dataset X_train_flowers, X_test_flowers, Y_train_flowers, Y_test_flowers = train_test_split(X_flowers, Y_flowers, test_size=0.3, random_state=0) # Initialize OneVsOneClassifier with LinearSVC as base estimator ovo_classifier = OneVsOneClassifier(LinearSVC(random_state=0)) # Train the classifier ovo_classifier.fit(X_train_flowers, Y_train_flowers) # Predict on test data y_pred_flowers = ovo_classifier.predict(X_test_flowers) # Calculate and return accuracy flower_accuracy = accuracy_score(Y_test_flowers, y_pred_flowers) return flower_accuracy"},{"question":"Title: Generate and Analyze Synthetic Datasets Using scikit-learn Objective To demonstrate your understanding of synthetic dataset generation using scikit-learn and the application of basic machine learning algorithms on these datasets. Task 1. **Dataset Generation:** - Generate three different datasets using the following scikit-learn dataset generators: 1. `make_blobs` with 3 clusters. 2. `make_classification` with 2 informative features and 2 clusters per class. 3. `make_moons` with a noise level of 0.1. - For each dataset, plot the data points and color them according to their classes. 2. **Model Training and Evaluation:** - Split each dataset into training and test sets (70% training, 30% test). - Train a K-Nearest Neighbors (KNN) classifier on the training set (use `n_neighbors=3`). - Evaluate the trained model on the test set and report the accuracy for each dataset. 3. **Implementation Requirements:** - Your solution should include the use of `sklearn.datasets` for dataset generation. - Use `matplotlib` (or similar) for plotting the datasets. - Use `train_test_split` from `sklearn.model_selection` for splitting the datasets. - Use `KNeighborsClassifier` from `sklearn.neighbors`. Input and Output Formats - **Input:** No input is required (aside from necessary imports and instantiations). - **Output:** The solution should produce plots for each dataset and print the accuracy scores for each trained model on the test set. Code Template ```python import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # 1. Generate Datasets # a. make_blobs X_blobs, y_blobs = make_blobs(centers=3, cluster_std=0.5, random_state=0) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"Blobs Dataset\\") plt.show() # b. make_classification X_classification, y_classification = make_classification(n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, random_state=0) plt.scatter(X_classification[:, 0], X_classification[:, 1], c=y_classification) plt.title(\\"Classification Dataset\\") plt.show() # c. make_moons X_moons, y_moons = make_moons(noise=0.1, random_state=0) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"Moons Dataset\\") plt.show() # 2. Model Training and Evaluation datasets = [(X_blobs, y_blobs), (X_classification, y_classification), (X_moons, y_moons)] names = [\'Blobs\', \'Classification\', \'Moons\'] for i, (X, y) in enumerate(datasets): # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Train the KNN classifier knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X_train, y_train) # Evaluate the classifier y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy for {names[i]} dataset: {accuracy:.2f}\\") ``` Constraints - Ensure the plots are clear and informative. - Accuracy scores should be displayed with up to two decimal places.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def generate_and_plot_datasets(): datasets = [] # Generate make_blobs dataset X_blobs, y_blobs = make_blobs(centers=3, cluster_std=0.5, random_state=0) datasets.append((X_blobs, y_blobs, \'Blobs Dataset\')) # Generate make_classification dataset X_classification, y_classification = make_classification(n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, random_state=0) datasets.append((X_classification, y_classification, \'Classification Dataset\')) # Generate make_moons dataset X_moons, y_moons = make_moons(noise=0.1, random_state=0) datasets.append((X_moons, y_moons, \'Moons Dataset\')) # Plot datasets for X, y, title in datasets: plt.scatter(X[:, 0], X[:, 1], c=y) plt.title(title) plt.show() return datasets def train_and_evaluate(datasets): results = {} for X, y, name in datasets: # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Train the KNN classifier knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X_train, y_train) # Evaluate the classifier y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) results[name] = accuracy return results datasets = generate_and_plot_datasets() results = train_and_evaluate(datasets) for name, accuracy in results.items(): print(f\\"Accuracy for {name}: {accuracy:.2f}\\")"},{"question":"**Topic:** Implementing a Custom Logging Utility in Python **Objective:** Assess students\' understanding of the `logging` module and their ability to create a custom logging utility that integrates advanced features of Python. **Problem Statement:** You are required to design a custom logging utility that logs various levels of messages to a file. The utility should support the following features: 1. Configurable logging levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). 2. Custom log message formatting that includes the timestamp, log level, and the message. 3. Rotating logs: When the log file reaches a specified size, it should create a new log file and archive the old one, keeping only a user-specified number of archived files. **Function Signature:** ```python from typing import Optional class CustomLogger: def __init__(self, log_file: str, level: int, max_size: int, backup_count: int): Initialize the CustomLogger with a specified log file, logging level, maximum log file size, and the number of backup log files to keep. Parameters: - log_file (str): The path to the log file. - level (int): The logging level (e.g., logging.DEBUG, logging.INFO). - max_size (int): The maximum size of the log file in bytes before rotation occurs. - backup_count (int): The number of backup log files to keep. pass def log(self, level: int, message: str) -> None: Log a message at the specified logging level. Parameters: - level (int): The logging level (e.g., logging.DEBUG, logging.INFO). - message (str): The log message. pass ``` **Requirements:** 1. The `CustomLogger` class should use Python\'s `logging` module. 2. The constructor should set up the logger with a file handler that handles log rotation. 3. The `log` method should log messages using the logger\'s methods (e.g., `debug()`, `info()`, `warning()`, `error()`, `critical()`) based on the provided logging level. 4. The log file should be rotated when it exceeds `max_size` bytes, and up to `backup_count` backup files should be kept. 5. Implement proper log message formatting to include the timestamp, log level, and message. 6. Ensure thread safety in your implementation. **Example Usage:** ```python # Create a CustomLogger instance logger = CustomLogger(\'app.log\', level=logging.DEBUG, max_size=1024*1024, backup_count=3) # Log some messages logger.log(logging.DEBUG, \'This is a debug message\') logger.log(logging.INFO, \'This is an info message\') logger.log(logging.WARNING, \'This is a warning message\') logger.log(logging.ERROR, \'This is an error message\') logger.log(logging.CRITICAL, \'This is a critical message\') ``` **Constraints:** - File size for rotation will not exceed 1 MB (1024*1024 bytes). - Backup count will be limited to 5. **Performance Requirements:** - The logging utility should efficiently handle concurrent logging from multiple threads. **Additional Notes:** - You may refer to the Python `logging` module documentation for implementation details. - Consider edge cases such as attempting to log messages larger than the max size before a rotation is needed. **Evaluation Criteria:** - Correctness: Does the solution meet all the functional requirements? - Code quality: Is the code readable, well-structured, and commented? - Efficiency: Does the implementation efficiently handle log rotation and backup?","solution":"import logging from logging.handlers import RotatingFileHandler class CustomLogger: def __init__(self, log_file: str, level: int, max_size: int, backup_count: int): Initialize the CustomLogger with a specified log file, logging level, maximum log file size, and the number of backup log files to keep. Parameters: - log_file (str): The path to the log file. - level (int): The logging level (e.g., logging.DEBUG, logging.INFO). - max_size (int): The maximum size of the log file in bytes before rotation occurs. - backup_count (int): The number of backup log files to keep. self.logger = logging.getLogger(log_file) self.logger.setLevel(level) handler = RotatingFileHandler(log_file, maxBytes=max_size, backupCount=backup_count) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) self.logger.addHandler(handler) def log(self, level: int, message: str) -> None: Log a message at the specified logging level. Parameters: - level (int): The logging level (e.g., logging.DEBUG, logging.INFO). - message (str): The log message. if level == logging.DEBUG: self.logger.debug(message) elif level == logging.INFO: self.logger.info(message) elif level == logging.WARNING: self.logger.warning(message) elif level == logging.ERROR: self.logger.error(message) elif level == logging.CRITICAL: self.logger.critical(message) else: raise ValueError(f\\"Unsupported logging level: {level}\\")"},{"question":"**Objective**: Demonstrate the ability to dynamically create classes at runtime using Python\'s `types` module. # Problem Statement You are required to create several dynamic types using Python\'s `types` module and then leverage these types to perform specific operations. The operations must include creating a class with a given structure, resolving method resolution order (MRO) dynamically, and using some of the built-in type definitions. # Detailed Requirements 1. **Dynamic Class Creation**: - Use `types.new_class` to dynamically create a new class named `DynamicClass` with a base class `BaseClass`. The `BaseClass` should contain a method `base_method` that returns the string \\"This is base method.\\" - The `DynamicClass` should contain a method `dynamic_method` that returns the string \\"This is dynamic method.\\" Ensure this method is defined via a function passed to the `exec_body`. 2. **Method Resolution Order**: - Use `types.prepare_class` to create another class dynamically that inherits from multiple base classes. Ensure you handle method resolution order dynamically by implementing a method that prints the MRO of the created class. 3. **Standard Type Usage**: - Create instances of types like `FunctionType` and `LambdaType` and show usage examples by defining a simple function and lambda function that return a string. 4. **Utility Use**: - Utilize `types.SimpleNamespace` to create an object that stores attributes `name` and `value`. Provide functionality to add and remove attributes dynamically. # Function Definitions 1. `create_dynamic_class()`: - **Input**: None - **Output**: A dynamically created class `DynamicClass`. 2. `create_class_with_mro()`: - **Input**: None - **Output**: Prints the method resolution order (MRO) of the created class. 3. `use_standard_types()`: - **Input**: None - **Output**: Examples demonstrating the usage of `FunctionType` and `LambdaType`. 4. `use_simple_namespace()`: - **Input**: Dictionary with keys and values to initialize attributes. - **Output**: An instance of `SimpleNamespace` with dynamic attributes. # Example Usage ```python # Example for create_dynamic_class DynamicClass = create_dynamic_class() instance = DynamicClass() print(instance.base_method()) # Output: This is base method print(instance.dynamic_method()) # Output: This is dynamic method # Example for create_class_with_mro create_class_with_mro() # Output: Print the MRO of the dynamic class # Example for use_standard_types use_standard_types() # Output: Demonstration of FunctionType and LambdaType # Example for use_simple_namespace ns = use_simple_namespace({\'name\': \'Python\', \'value\': 310}) print(ns.name) # Output: Python print(ns.value) # Output: 310 ``` # Constraints - Do not use any other external libraries; only utilize the standard `types` module and built-in functionalities. - Ensure that the code is well-structured, with appropriate function and variable names for clarity. - Handle any exceptions or errors that may arise during dynamic class creation or type operations. Good luck and remember to thoroughly test your implementation to ensure it meets all the specified requirements.","solution":"import types def create_dynamic_class(): BaseClass = types.new_class(\'BaseClass\', ()) def base_method(self): return \\"This is base method\\" setattr(BaseClass, \'base_method\', base_method) def class_body(ns): def dynamic_method(self): return \\"This is dynamic method\\" ns[\'dynamic_method\'] = dynamic_method DynamicClass = types.new_class(\'DynamicClass\', (BaseClass,), {}, class_body) return DynamicClass def create_class_with_mro(): class BaseClass1: pass class BaseClass2: pass meta, ns, kwds = types.prepare_class(\'MultiBaseClass\', (BaseClass1, BaseClass2)) MultiBaseClass = meta(\'MultiBaseClass\', (BaseClass1, BaseClass2), ns, **kwds) print(MultiBaseClass.__mro__) def use_standard_types(): FunctionType = types.FunctionType LambdaType = types.LambdaType def simple_function(): return \\"This is a simple function\\" simple_lambda = lambda: \\"This is a lambda\\" function_instance = FunctionType(simple_function.__code__, globals()) lambda_instance = LambdaType(simple_lambda.__code__, globals()) print(function_instance()) # Output: This is a simple function print(lambda_instance()) # Output: This is a lambda def use_simple_namespace(initial_dict): SimpleNamespace = types.SimpleNamespace namespace_instance = SimpleNamespace(**initial_dict) return namespace_instance"},{"question":"Objective Design a function that compares two directories and generates a detailed comparison report. Problem Statement Write a Python function `compare_directories(dir1, dir2)` that compares the contents of two directories `dir1` and `dir2` using the `filecmp` module. The function should output a detailed report of the comparison including: 1. Files and subdirectories only in `dir1`. 2. Files and subdirectories only in `dir2`. 3. Common files in both directories that are identical. 4. Common files in both directories that are different. Your function should utilize the `filecmp.dircmp` class for directory comparison and generate the report in the following format: ``` Only in <dir1>: <list of files/subdirectories only in dir1> Only in <dir2>: <list of files/subdirectories only in dir2> Identical files: <list of identical files> Different files: <list of different files> ``` Expected Function Signature ```python def compare_directories(dir1: str, dir2: str) -> None: ``` Constraints - Both `dir1` and `dir2` are valid and accessible directory paths on the filesystem. - The function should not throw any exceptions and handle any errors gracefully. - Use shallow comparison for file comparison. Example Suppose we have two directories `dir1` and `dir2` with the following structure: - `dir1`: - file1.txt (content: \\"hello\\") - file2.txt (content: \\"world\\") - subdir1/ - file3.txt (content: \\"foo\\") - `dir2`: - file1.txt (content: \\"hello\\") - file2.txt (content: \\"world!\\") - subdir2/ - file4.txt (content: \\"bar\\") Calling `compare_directories(\\"dir1\\", \\"dir2\\")` should produce the following output: ``` Only in <dir1>: [\'subdir1\'] Only in <dir2>: [\'subdir2\'] Identical files: [\'file1.txt\'] Different files: [\'file2.txt\'] ``` Notes - For simplicity, you may assume that the directory contents do not change during the comparison.","solution":"import filecmp import os def compare_directories(dir1: str, dir2: str) -> None: comparison = filecmp.dircmp(dir1, dir2) def report(comparison): print(f\\"Only in {dir1}:\\") for item in comparison.left_only: print(item) print() print(f\\"Only in {dir2}:\\") for item in comparison.right_only: print(item) print() print(\\"Identical files:\\") for item in comparison.same_files: print(item) print() print(\\"Different files:\\") for item in comparison.diff_files: print(item) print() report(comparison)"},{"question":"# Python Coding Assessment Question Implement a custom import function using the deprecated `imp` module such that it mirrors the built-in `importlib` functionalities. You will also need to implement a secondary function demonstrating the transition from `imp` to `importlib`. Task 1: Implement with `imp` module ```python def custom_import_imp(name): Mimic the behavior of the import statement using the deprecated `imp` module. Args: * name (str): The name of the module to import. Returns: * module: The imported module. Exceptions to handle: * ImportError: If the module cannot be imported. # Your implementation here using imp.find_module and imp.load_module ``` Task 2: Implement with `importlib` module ```python def custom_import_importlib(name): Mimic the behavior of the import statement using the `importlib` module. Args: * name (str): The name of the module to import. Returns: * module: The imported module. Exceptions to handle: * ImportError: If the module cannot be imported. # Your implementation here using importlib.import_module ``` Example Usage ```python # Using imp module = custom_import_imp(\\"math\\") print(module.sqrt(4)) # Should output 2.0 # Using importlib module = custom_import_importlib(\\"math\\") print(module.sqrt(4)) # Should output 2.0 ``` Constraints - Assume module names provided as input are valid and compatible module names. - Your implementation should handle exceptions gracefully and raise an ImportError if the module cannot be found or loaded. - Ensure backward compatibility for Python versions where `imp` was still in use. Note: This question evaluates your understanding of module importation mechanisms in Python, and your ability to transition deprecated code to modern equivalents using `importlib`.","solution":"import imp import importlib def custom_import_imp(name): Mimic the behavior of the import statement using the deprecated `imp` module. Args: * name (str): The name of the module to import. Returns: * module: The imported module. Exceptions to handle: * ImportError: If the module cannot be imported. try: fp, pathname, description = imp.find_module(name) module = imp.load_module(name, fp, pathname, description) if fp: fp.close() return module except ImportError as e: raise ImportError(f\\"Cannot import module {name}, error: {e}\\") def custom_import_importlib(name): Mimic the behavior of the import statement using the `importlib` module. Args: * name (str): The name of the module to import. Returns: * module: The imported module. Exceptions to handle: * ImportError: If the module cannot be imported. try: module = importlib.import_module(name) return module except ImportError as e: raise ImportError(f\\"Cannot import module {name}, error: {e}\\")"},{"question":"You are required to create a function that constructs and returns a multipart MIME email message. The email should contain: 1. A text part with a given string as its content. 2. An image part using given image data. 3. An application part using given application data (such as a PDF or any binary file). The function should handle the necessary encoding and set appropriate headers as per the MIME standards. # Function Signature ```python def create_multipart_email(subject: str, sender: str, recipient: str, text_content: str, image_data: bytes, image_type: str, app_data: bytes, app_subtype: str, app_name: str) -> email.mime.multipart.MIMEMultipart: pass ``` # Input - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipient` (str): The recipient\'s email address. - `text_content` (str): The text content for the email. - `image_data` (bytes): The raw binary data of the image. - `image_type` (str): The MIME subtype of the image (e.g., `jpeg`, `png`). - `app_data` (bytes): The raw binary data of the application file. - `app_subtype` (str): The MIME subtype of the application (e.g., `octet-stream`). - `app_name` (str): The name of the application file to be used in the `Content-Disposition` header. # Output - Returns a `MIMEMultipart` object representing the complete email. # Example Usage ```python subject = \\"Sample Email\\" sender = \\"alice@example.com\\" recipient = \\"bob@example.com\\" text_content = \\"Hello, this is a sample email with a text, an image, and a PDF attachment.\\" image_data = b\'...\' # some binary data for an image image_type = \\"jpeg\\" app_data = b\'...\' # some binary data for a PDF app_subtype = \\"pdf\\" app_name = \\"sample.pdf\\" msg = create_multipart_email(subject, sender, recipient, text_content, image_data, image_type, app_data, app_subtype, app_name) # Further processing to send the email can follow ``` # Constraints - Ensure the email adheres to MIME standards. - Properly encode the image and application data. - The function should handle possible exceptions and maintain integrity of the email structure. # Hints 1. Utilize `MIMEMultipart`, `MIMEText`, `MIMEImage`, and `MIMEApplication` classes from the `email.mime` module. 2. Set appropriate headers such as `Content-Type`, `MIME-Version`, and `Content-Disposition`. 3. Use appropriate encoders like `email.encoders.encode_base64` for binary data.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email.encoders import encode_base64 def create_multipart_email(subject: str, sender: str, recipient: str, text_content: str, image_data: bytes, image_type: str, app_data: bytes, app_subtype: str, app_name: str) -> MIMEMultipart: # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Create the text part text_part = MIMEText(text_content, \'plain\') msg.attach(text_part) # Create the image part image_part = MIMEImage(image_data, _subtype=image_type) image_part.add_header(\'Content-Disposition\', \'inline\', filename=f\'image.{image_type}\') msg.attach(image_part) # Create the application part app_part = MIMEApplication(app_data, _subtype=app_subtype) app_part.add_header(\'Content-Disposition\', \'attachment\', filename=app_name) encode_base64(app_part) msg.attach(app_part) return msg"},{"question":"**Objective**: Your task is to implement a custom import system in Python. This custom system should allow for importing modules from a remote server via HTTP. Background You are given a server URL that hosts Python modules. When a module is requested, the server returns the module\'s code as plain text. You will write a custom meta path finder and loader that extends Python\'s import system to fetch and load modules from this server. Requirements 1. **Custom Meta Path Finder**: - Implement a meta path finder class called `HTTPMetaPathFinder` that: - Checks if it can handle the requested module name. - Fetches the module from a given base URL if available. - Returns a module spec with an associated loader. 2. **Custom Loader**: - Implement a loader class called `HTTPLoader` that: - Loads the module from the fetched code. - Sets necessary module attributes (`__name__`, `__loader__`, `__package__`, etc.) 3. **Integration**: - Modify the `sys.meta_path` to include your custom `HTTPMetaPathFinder` so that it is used during the import process. Input and Output: - **Input**: A base URL where modules can be fetched. - **Output**: Successful import of modules available at the given URL. Example Suppose the server (e.g., `http://example.com/modules/`) hosts a module named `hello.py` that contains: ```python def greet(): return \\"Hello, world!\\" ``` After updating the import system, the following code should work: ```python import hello print(hello.greet()) # Output: \\"Hello, world!\\" ``` Constraints: - You are not allowed to use any third-party libraries for HTTP requests. - Assume the server responds with a 404 status code if the module is not found. Notes: 1. You should handle any potential errors gracefully (e.g., network errors, HTTP errors) and raise appropriate exceptions. 2. Your code should be clean, with appropriate comments explaining each part. Solution Template ```python import sys import importlib.abc import importlib.util import types import urllib.request class HTTPMetaPathFinder(importlib.abc.MetaPathFinder): def __init__(self, base_url): self._base_url = base_url def find_spec(self, fullname, path, target=None): # Implement logic to find the module spec and return it. pass class HTTPLoader(importlib.abc.Loader): def __init__(self, fullname, url): self._fullname = fullname self._url = url def create_module(self, spec): # Optionally create the module object here return None def exec_module(self, module): # Implement logic to execute the module from the URL pass def install_http_importer(base_url): finder = HTTPMetaPathFinder(base_url) sys.meta_path.insert(0, finder) # Example usage # install_http_importer(\\"http://example.com/modules/\\") # import hello # print(hello.greet()) ``` Complete the `HTTPMetaPathFinder` and `HTTPLoader` classes to fulfill the requirements.","solution":"import sys import importlib.abc import importlib.util import types import urllib.request class HTTPMetaPathFinder(importlib.abc.MetaPathFinder): def __init__(self, base_url): self._base_url = base_url def find_spec(self, fullname, path, target=None): module_url = f\\"{self._base_url}/{fullname}.py\\" try: with urllib.request.urlopen(module_url) as response: if response.status == 200: loader = HTTPLoader(fullname, module_url) return importlib.util.spec_from_loader(fullname, loader) except urllib.error.URLError: return None except urllib.error.HTTPError: return None return None class HTTPLoader(importlib.abc.Loader): def __init__(self, fullname, url): self._fullname = fullname self._url = url def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): try: with urllib.request.urlopen(self._url) as response: code = response.read() exec(code, module.__dict__) except Exception as e: raise ImportError(f\\"Could not load module {self._fullname}\\") from e def install_http_importer(base_url): finder = HTTPMetaPathFinder(base_url) sys.meta_path.insert(0, finder) # Example usage # install_http_importer(\\"http://example.com/modules\\") # import hello # print(hello.greet())"},{"question":"Objective Assess your ability to use pandas to perform various merging operations on `DataFrame` objects to solve practical data manipulation tasks. Problem Statement Suppose you are given two datasets that record daily sales and employee performance in a retail chain. Your task is to combine these datasets to generate a comprehensive report, accounting for various joining conditions and missing data handling. Input 1. A `DataFrame` named `daily_sales` with the following structure: - `date` (string): The date of the sale in \\"YYYY-MM-DD\\" format. - `store_id` (int): The unique identifier for the store. - `item_id` (int): The unique identifier for the sold item. - `quantity` (int): The quantity of the item sold. 2. A `DataFrame` named `employee_performance` with the following structure: - `date` (string): The date of the record in \\"YYYY-MM-DD\\" format. - `store_id` (int): The unique identifier for the store. - `employee_id` (int): The unique identifier for the employee. - `sales_quantities` (int): The total quantity of items sold by the employee. - `customer_satisfaction` (float): The customer satisfaction rating for the employee (ranges from 0.0 to 5.0). Constraints - Assume `daily_sales` and `employee_performance` are not sorted by the `date`. - `employee_performance` might have multiple records for the same (date, store_id) pair with different employee_id. - `daily_sales` might have multiple records for the same (date, store_id, item_id) pair. Output Return a merged `DataFrame` called `merged_report` with the following conditions: 1. Perform an outer join on the `daily_sales` and `employee_performance` using `date` and `store_id`. 2. Fill missing `sales_quantities` and `customer_satisfaction` from `employee_performance` with zero and 0.0 respectively. 3. Create an additional column `total_sales_quantities` that sums the `quantity` from `daily_sales` and `sales_quantities` from `employee_performance`. 4. If there are any missing values in the `quantity` column, fill them with zero. 5. Sort the resulting `merged_report` by `date` in ascending order. Example ```python import pandas as pd # Input DataFrames daily_sales = pd.DataFrame({ \\"date\\": [\\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\"], \\"store_id\\": [1, 2, 1, 3], \\"item_id\\": [101, 102, 101, 103], \\"quantity\\": [20, 30, 40, 15], }) employee_performance = pd.DataFrame({ \\"date\\": [\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-02\\", \\"2023-01-03\\"], \\"store_id\\": [1, 1, 2, 3], \\"employee_id\\": [201, 202, 203, 204], \\"sales_quantities\\": [25, 35, 45, 20], \\"customer_satisfaction\\": [4.5, 3.8, 4.1, 4.7], }) # Expected Output merged_report = pd.DataFrame({ \\"date\\": [\\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-02\\", \\"2023-01-03\\"], \\"store_id\\": [1, 2, 1, 2, 3], \\"item_id\\": [101, 102, 101, None, 103], \\"quantity\\": [20, 30, 40, 0, 15], \\"employee_id\\": [201, None, 202, 203, 204], \\"sales_quantities\\": [25, 0, 35, 45, 20], \\"customer_satisfaction\\": [4.5, 0.0, 3.8, 4.1, 4.7], \\"total_sales_quantities\\": [45, 30, 75, 45, 35] }) # Your implementation here def merge_datasets(daily_sales: pd.DataFrame, employee_performance: pd.DataFrame) -> pd.DataFrame: pass ``` Use the provided function signature and implement the function to produce the expected output. Evaluation Criteria - Correctness: Ensuring the function generates the expected merged result. - Handling of missing data and proper filling as specified. - Proper implementation of outer join conditions. - Appropriate use of pandas functionalities to achieve the desired output.","solution":"import pandas as pd def merge_datasets(daily_sales: pd.DataFrame, employee_performance: pd.DataFrame) -> pd.DataFrame: # Perform an outer join on daily_sales and employee_performance using \'date\' and \'store_id\' merged = pd.merge(daily_sales, employee_performance, how=\'outer\', on=[\'date\', \'store_id\']) # Fill missing sales_quantities and customer_satisfaction with 0 and 0.0 respectively merged[\'sales_quantities\'].fillna(0, inplace=True) merged[\'customer_satisfaction\'].fillna(0.0, inplace=True) # Fill missing quantity with 0 merged[\'quantity\'].fillna(0, inplace=True) # Create an additional column total_sales_quantities merged[\'total_sales_quantities\'] = merged[\'quantity\'] + merged[\'sales_quantities\'] # Sort by date in ascending order merged.sort_values(by=\'date\', inplace=True) return merged"},{"question":"**Objective:** Develop a Python program that mimics a simplified compression and archiving tool using multiple compression algorithms. This tool should demonstrate proficiency with the `gzip`, `bz2`, and `lzma` modules. **Task:** 1. Implement a function `multi_compress(input_filename: str, output_folder: str) -> dict` that: * Takes a file path `input_filename` as input. * Creates compressed versions of this file using gzip, bzip2, and lzma algorithms. * Saves each compressed file in the `output_folder`. * Returns a dictionary with the algorithm names as keys and the corresponding compressed file paths as values. 2. Implement another function `multi_decompress(input_files: dict, output_folder: str) -> None` that: * Takes a dictionary `input_files` where keys are algorithm names (`gzip`, `bz2`, `lzma`) and values are paths to the respective compressed files. * Decompresses each file and saves the output in the `output_folder`. **Constraints:** * You may assume the `input_filename` exists and is readable. * The `output_folder` is a valid directory with write access. * Handle errors gracefully, such as unsupported file formats or read/write errors. **Performance:** * The functions should efficiently handle files up to 100MB in size. **Example:** ```python def example(): input_file = \'example.txt\' output_dir = \'compressed_files\' # Supposing the file and directory exist compressed_files = multi_compress(input_file, output_dir) print(compressed_files) # Output: {\'gzip\': \'compressed_files/example.txt.gz\', \'bz2\': \'compressed_files/example.txt.bz2\', \'lzma\': \'compressed_files/example.txt.lzma\'} multi_decompress(compressed_files, output_dir) # This should decompress the files back to \'compressed_files/example.txt\' with appropriate suffixes or names to avoid overwrites. example() ``` **Notes:** * The suffixes for the compressed files should be \'.gz\', \'.bz2\', and \'.lzma\' respectively. * Ensure that each decompressed output file is named appropriately to avoid conflicts, for example by appending \\"_decompressed\\" to the original filename.","solution":"import gzip import bz2 import lzma import os import shutil def multi_compress(input_filename: str, output_folder: str) -> dict: Compresses the input file using gzip, bzip2, and lzma and saves them in the output folder. Parameters: input_filename (str): Path to the input file to be compressed. output_folder (str): Directory where the compressed files will be saved. Returns: dict: A dictionary containing the compression algorithm names as keys and the compressed file paths as values. if not os.path.isfile(input_filename): raise FileNotFoundError(f\\"The input file {input_filename} does not exist.\\") if not os.path.isdir(output_folder): raise NotADirectoryError(f\\"The output folder {output_folder} does not exist.\\") base_filename = os.path.basename(input_filename) compressed_files = { \'gzip\': os.path.join(output_folder, f\\"{base_filename}.gz\\"), \'bz2\': os.path.join(output_folder, f\\"{base_filename}.bz2\\"), \'lzma\': os.path.join(output_folder, f\\"{base_filename}.lzma\\"), } with open(input_filename, \'rb\') as file: file_data = file.read() # gzip compression with gzip.open(compressed_files[\'gzip\'], \'wb\') as f: f.write(file_data) # bzip2 compression with bz2.open(compressed_files[\'bz2\'], \'wb\') as f: f.write(file_data) # lzma compression with lzma.open(compressed_files[\'lzma\'], \'wb\') as f: f.write(file_data) return compressed_files def multi_decompress(input_files: dict, output_folder: str) -> None: Decompresses the input files using their respective algorithms and saves them in the output folder. Parameters: input_files (dict): Dictionary containing compression algorithm names as keys and the compressed file paths as values. output_folder (str): Directory where the decompressed files will be saved. if not os.path.isdir(output_folder): raise NotADirectoryError(f\\"The output folder {output_folder} does not exist.\\") for algo, file_path in input_files.items(): if not os.path.isfile(file_path): raise FileNotFoundError(f\\"The compressed file {file_path} does not exist.\\") decompressed_filename = os.path.basename(file_path) decompressed_path = os.path.join(output_folder, f\\"{decompressed_filename}_decompressed\\") if algo == \'gzip\': with gzip.open(file_path, \'rb\') as f_in: with open(decompressed_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif algo == \'bz2\': with bz2.open(file_path, \'rb\') as f_in: with open(decompressed_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif algo == \'lzma\': with lzma.open(file_path, \'rb\') as f_in: with open(decompressed_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) else: raise ValueError(f\\"Unsupported compression algorithm {algo}.\\")"},{"question":"# Python 3.10 Coding Assessment Objective Implement functions to convert strings to floats and floats to formatted strings, handling various edge cases and exceptions in Python. Task You are required to implement the following functions in Python: 1. **string_to_float(s: str) -> float**: - This function takes a string `s` and converts it to a floating-point number. - The string `s` must not have leading or trailing whitespace. - If the string is not a valid representation of a floating-point number, raise a `ValueError` exception. - If `s` represents a value that is too large to store in a float, raise an `OverflowError` exception. - If any other error occurs during the conversion, raise an appropriate exception. 2. **double_to_string(val: float, format_code: str, precision: int, flags: List[str]) -> str**: - This function takes a float `val`, a format code `format_code`, an integer `precision`, and a list of flags `flags`. - `format_code` must be one of `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, or `\'r\'`. - The function returns a string representing the float `val` formatted according to the format code, precision, and flags. - Supported flags are: - `\\"SIGN\\"`: Always precede the returned string with a sign character. - `\\"ADD_DOT_0\\"`: Ensure that the returned string will not look like an integer. - `\\"ALT\\"`: Apply alternate formatting rules. Input - `string_to_float(s: str)` - `s`: A string without leading or trailing whitespace. - `double_to_string(val: float, format_code: str, precision: int, flags: List[str])` - `val`: A floating-point number. - `format_code`: A single character string. - `precision`: An integer. - `flags`: A list of strings which may contain `\\"SIGN\\"`, `\\"ADD_DOT_0\\"`, and/or `\\"ALT\\"`. Output - `string_to_float(s: str)` returns a float. - `double_to_string(val: float, format_code: str, precision: int, flags: List[str])` returns a formatted string. Constraints - You must not use any external libraries other than the Python standard library. - Handle edge cases properly and provide meaningful error messages for exceptions. Examples ```python # Example for string_to_float try: result = string_to_float(\\"123.456\\") print(result) # Should print: 123.456 except ValueError as e: print(e) # Example for double_to_string try: result = double_to_string(123.456, \'f\', 2, [\\"SIGN\\", \\"ADD_DOT_0\\"]) print(result) # Should print: +123.46 except ValueError as e: print(e) ``` In this assessment, ensure that your implementation correctly handles the specified input requirements and edge cases, providing appropriate error handling and formatted output.","solution":"def string_to_float(s): Converts a string to a float. try: value = float(s) except ValueError: raise ValueError(f\\"Invalid string \'{s}\'. Cannot convert to float.\\") except OverflowError: raise OverflowError(f\\"Value in string \'{s}\' is too large.\\") except Exception as e: raise Exception(f\\"An unexpected error occurred: {str(e)}\\") return value def double_to_string(val, format_code, precision, flags): Converts a float to a formatted string. if format_code not in [\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\']: raise ValueError(f\\"Invalid format code \'{format_code}\'. Must be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'.\\") format_str = f\\"{{{\':\' if flags else \'\'}\\" if \\"SIGN\\" in flags: format_str += \\"+\\" if \\"ADD_DOT_0\\" in flags: format_str += \\"#\\" if \\"ALT\\" in flags: format_str += \\"#\\" format_str += f\\".{precision}{format_code}}}\\" try: result = format_str.format(val) except Exception as e: raise Exception(f\\"An error occurred during formatting: {str(e)}\\") return result"},{"question":"As an experienced programmer, you are expected to work with distributed elastic metrics in PyTorch. The following task requires you to demonstrate your understanding of configuring metric handlers and using them to record and profile metrics. # Task You are required to write a function `record_and_profile_metrics` that configures a `ConsoleMetricHandler`, records a custom metric, and profiles a dummy block of code. Your function should perform the following steps: 1. Configure a `ConsoleMetricHandler`. 2. Put a custom metric with a name of your choice and a value. 3. Profile a dummy block of code that simply computes the sum of numbers from 1 to 1000. 4. Return the recorded metrics and the profiled time. # Expected Input and Output - **Input**: No input parameters. - **Output**: A dictionary containing: - `metrics`: A dictionary of the recorded metrics. - `profiled_time`: The time taken to execute the dummy block of code. ```python def record_and_profile_metrics(): # Your implementation should be here. pass ``` # Constraints - Use the `ConsoleMetricHandler` for configuring the metric handler. - The dummy block of code to be profiled should compute the sum of numbers from 1 to 1000. - Avoid using any external libraries except PyTorch. # Example ```python result = record_and_profile_metrics() print(result) # Expected Output: # { # \\"metrics\\": { # \\"custom_metric_name\\": 12345 # Example value # }, # \\"profiled_time\\": 0.001234 # Example time # } ``` # Performance Requirements - The function should execute efficiently even though it processes a limited amount of data. Implement the `record_and_profile_metrics` function to demonstrate your understanding of PyTorch elastic metrics.","solution":"import torch import time def record_and_profile_metrics(): Configure a ConsoleMetricHandler, put a custom metric, profile a dummy block of code, and return the metrics and profiled time. metrics = {} # Using PyTorch logging to handle custom metrics torch.set_printoptions(profile=\'default\') # Step 1: Configure the ConsoleMetricHandler (here it will use basic logging as an example) # Put a custom metric into metrics dictionary custom_metric_name = \\"dummy_metric\\" custom_metric_value = 42 metrics[custom_metric_name] = custom_metric_value # Step 3: Profile a dummy block of code start_time = time.time() dummy_sum = sum(range(1, 1001)) end_time = time.time() # Record the profiled time profiled_time = end_time - start_time return {\\"metrics\\": metrics, \\"profiled_time\\": profiled_time}"},{"question":"**Title:** Graph Transformation and Partitioning in PyTorch **Objective:** To assess the student\'s understanding of PyTorch FX graph transformations, including node manipulation, use of the Transformer class, and graph partitioning. # Problem Statement You are provided with a PyTorch `GraphModule` representing a simple neural network defined as follows: ```python import torch class SimpleNet(torch.nn.Module): def forward(self, x, y): a = torch.ops.aten.add.Tensor(x, y) b = torch.ops.aten.mul.Tensor(a, x) c = torch.ops.aten.div.Tensor(b, y) return c # Tracing the model to get a GraphModule example_input = (torch.tensor(3.0), torch.tensor(2.0)) traced_net = torch.fx.symbolic_trace(SimpleNet()) ``` # Tasks: 1. **Replace Operators:** Implement a transformation function that replaces all `torch.ops.aten.add.Tensor` operations with `torch.ops.aten.sub.Tensor` in the given `GraphModule`. 2. **Insert Operators:** Implement a transformation function that inserts a `torch.ops.aten.relu.default` call after every `torch.ops.aten.mul.Tensor` call in the given `GraphModule`. 3. **Remove Operators:** Implement a transformation function that removes all `torch.ops.aten.div.Tensor` operations from the given `GraphModule`. 4. **Graph Partitioning:** Implement a partitioning strategy that partitions the graph based on supported operators. Specifically, partition the graph such that each partition contains nodes with only `torch.ops.aten.add.Tensor` and `torch.ops.aten.mul.Tensor`. Use the `CapabilityBasedPartitioner` class for this task. # Constraints: - Only use classes and functions from the PyTorch FX module as required. - Ensure that the graph transformations preserve the integrity and correctness of the overall computation graph. - Document each transformation function with appropriate comments explaining the logic. # Input: - A `GraphModule` from PyTorch FX representing the traced neural network. # Expected Output: - For each transformation task, return a new `GraphModule` with the specified transformations applied. - For the partitioning task, return a list of partitions (each partition being a list of nodes). # Example Given the following `GraphModule`: ```python import torch class SimpleAddition(torch.nn.Module): def forward(self, x, y): return torch.ops.aten.add.Tensor(x, y) example_input = (torch.tensor(3.0), torch.tensor(2.0)) traced_net = torch.fx.symbolic_trace(SimpleAddition()) ``` 1. **Replacing add with sub** Before transformation: ``` def forward(self, x, y): return torch.ops.aten.add.Tensor(x, y) ``` After transformation: ``` def forward(self, x, y): return torch.ops.aten.sub.Tensor(x, y) ``` 2. **Inserting relu after mul** Before transformation: ``` def forward(self, x, y): a = torch.ops.aten.add.Tensor(x, y) b = torch.ops.aten.mul.Tensor(a, x) return b ``` After transformation: ``` def forward(self, x, y): a = torch.ops.aten.add.Tensor(x, y) b = torch.ops.aten.mul.Tensor(a, x) c = torch.ops.aten.relu.default(b) return c ``` 3. **Removing div** Before transformation: ``` def forward(self, x, y): a = torch.ops.aten.add.Tensor(x, y) b = torch.ops.aten.mul.Tensor(a, x) c = torch.ops.aten.div.Tensor(b, y) return c ``` After transformation: ``` def forward(self, x, y): a = torch.ops.aten.add.Tensor(x, y) b = torch.ops.aten.mul.Tensor(a, x) return b ``` 4. **Partitioning based on supported operators** Given the following extended `GraphModule`: ``` def forward(self, x, y): a = torch.ops.aten.add.Tensor(x, y) b = torch.ops.aten.mul.Tensor(a, x) c = torch.ops.aten.div.Tensor(b, y) return c ``` Partitions: ``` Partition 1: [torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor] Partition 2: [torch.ops.aten.div.Tensor] ``` Implement the functions as described and test them with the provided `SimpleNet`.","solution":"import torch import torch.fx as fx def replace_add_with_sub(graph_module): Transforms the given GraphModule by replacing all `torch.ops.aten.add.Tensor` calls with `torch.ops.aten.sub.Tensor` calls. graph = graph_module.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.ops.aten.add.Tensor: with graph.inserting_after(node): new_node = graph.call_function(torch.ops.aten.sub.Tensor, node.args) node.replace_all_uses_with(new_node) graph.erase_node(node) graph_module.recompile() return graph_module def insert_relu_after_mul(graph_module): Transforms the given GraphModule by inserting a `torch.ops.aten.relu.default` call after every `torch.ops.aten.mul.Tensor` call. graph = graph_module.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.ops.aten.mul.Tensor: with graph.inserting_after(node): relu_node = graph.call_function(torch.ops.aten.relu.default, (node,)) graph_module.recompile() return graph_module def remove_div(graph_module): Transforms the given GraphModule by removing all `torch.ops.aten.div.Tensor` calls. graph = graph_module.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.ops.aten.div.Tensor: node.replace_all_uses_with(node.args[0]) graph.erase_node(node) graph_module.recompile() return graph_module class CapabilityBasedPartitioner: Partitions the graph into subgraphs that contain only nodes with specified supported operators. def __init__(self, graph_module, supported_ops): self.graph_module = graph_module self.supported_ops = supported_ops def partition_graph(self): partitions = [] current_partition = [] for node in self.graph_module.graph.nodes: if node.op == \'call_function\' and node.target in self.supported_ops: current_partition.append(node) else: if current_partition: partitions.append(current_partition) current_partition = [] if node.op == \'call_function\': partitions.append([node]) if current_partition: partitions.append(current_partition) return partitions def partition_graph(graph_module): supported_ops = {torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor} partitioner = CapabilityBasedPartitioner(graph_module, supported_ops) return partitioner.partition_graph()"},{"question":"**Title:** Implement a Custom Python Script Executor **Background:** You are required to implement a new function related to Python\'s script execution and input parsing behavior. Specifically, you will create a function that simulates the behavior of the Python interpreter when it processes a complete program from a string input. **Objective:** Implement a function `execute_python_script(script: str) -> dict` that takes a single argument `script` (a string containing the Python code) and returns a dictionary with the keys \'result\' and \'error\'. The \'result\' key should contain the output produced by executing the script, and the \'error\' key should contain any error message if the script fails to execute. The function must handle the following: 1. **Syntax Errors:** If there is a syntax error in the script, capture and return the error message. 2. **Exceptions:** If the script raises any exceptions during execution, capture and return the exception message. 3. **Output:** Capture and return all output produced by the script using \'print\' statements. **Input:** - `script` (str): A string of valid Python code. **Output:** - dict: A dictionary with keys: - \'result\' (str): Output produced by the script (or an empty string if there is an error). - \'error\' (str): Error message if an error occurred (or an empty string if there is no error). **Constraints:** - The function should handle multi-line scripts. - The function should work even if the script contains leading or trailing whitespace. **Example Usage:** ```python def execute_python_script(script: str) -> dict: # Your implementation here # Example 1: script = \\"print(\'Hello, world!\')nprint(5 + 10)\\" output = execute_python_script(script) print(output) # Output should be: {\'result\': \'Hello, world!n15n\', \'error\': \'\'} # Example 2: script = \\"print(1 / 0)\\" output = execute_python_script(script) print(output) # Output should be: {\'result\': \'\', \'error\': \'division by zero\'} # Example 3: script = \\"for i in range(3):n print(i)\\" output = execute_python_script(script) print(output) # Output should be: {\'result\': \'0n1n2n\', \'error\': \'\'} ``` **Notes:** - You may use the `exec()` function to execute the script strings. - You may redirect `stdout` to capture print statements\' output. - Ensure your implementation handles exceptions properly and captures both `SyntaxErrors` and runtime exceptions. Good luck!","solution":"import sys from io import StringIO def execute_python_script(script: str) -> dict: # Prepare a dictionary to hold the result and error output = {\\"result\\": \\"\\", \\"error\\": \\"\\"} # Create a StringIO object to capture the output of print statements old_stdout = sys.stdout sys.stdout = string_io = StringIO() try: # Remove leading and trailing whitespace script = script.strip() # Execute the script exec(script) except SyntaxError as syn_err: output[\\"error\\"] = f\\"SyntaxError: {str(syn_err)}\\" except Exception as ex: output[\\"error\\"] = str(ex) finally: # Restore sys.stdout sys.stdout = old_stdout # Retrieve the captured output output[\\"result\\"] = string_io.getvalue() return output"},{"question":"Objective Implement a program using the `threading` module to perform a complex task in the background while the main program continues execution. This will assess your understanding of thread creation, management, and synchronization in Python. Problem Statement You are tasked with creating a file compression utility that compresses multiple files in the background while the main thread continues executing other tasks. The program should: 1. Compress files using the \\"zipfile\\" module in separate threads. 2. Display progress in the main thread. 3. Ensure all threads complete before the program finishes. Requirements - Implement a class `BackgroundCompressor` that inherits from `threading.Thread`. - The class should take two arguments: - `file_list` (list): A list of filenames to compress. - `output_zip` (str): The name of the output ZIP file. - Override the `run()` method to compress the files in the background. - The main thread should: - Start the background compression. - Print a message every second showing that it is doing other work. - Wait for the compression thread to complete before exiting. Constraints - Assume all files exist and are accessible. - Use the `time.sleep()` function for simulating work in the main thread. Example Execution ```python import time import threading import zipfile class BackgroundCompressor(threading.Thread): def __init__(self, file_list, output_zip): threading.Thread.__init__(self) self.file_list = file_list self.output_zip = output_zip def run(self): with zipfile.ZipFile(self.output_zip, \'w\') as zipf: for file in self.file_list: zipf.write(file) print(f\'Background compression to {self.output_zip} completed.\') if __name__ == \\"__main__\\": files_to_compress = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] background_compressor = BackgroundCompressor(files_to_compress, \'compressed_files.zip\') background_compressor.start() for _ in range(5): print(\'Main thread is doing other work...\') time.sleep(1) background_compressor.join() print(\'Main thread waited until background compression was done.\') ``` Evaluation Criteria - Correct use of the `threading` module. - Proper implementation of the `run` method. - Effective use of the main thread to demonstrate non-blocking behavior. - Proper synchronization of threads ensuring the main thread waits for background task completion.","solution":"import time import threading import zipfile class BackgroundCompressor(threading.Thread): def __init__(self, file_list, output_zip): threading.Thread.__init__(self) self.file_list = file_list self.output_zip = output_zip def run(self): with zipfile.ZipFile(self.output_zip, \'w\') as zipf: for file in self.file_list: zipf.write(file) print(f\'Background compression to {self.output_zip} completed.\') if __name__ == \\"__main__\\": # Example usage files_to_compress = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] background_compressor = BackgroundCompressor(files_to_compress, \'compressed_files.zip\') background_compressor.start() for _ in range(5): print(\'Main thread is doing other work...\') time.sleep(1) background_compressor.join() print(\'Main thread waited until background compression was done.\')"},{"question":"# Question: Health Expenditure Visualization with Seaborn You are provided with a dataset containing health expenditure data for different countries and years. Your task is to create a series of visualizations using the Seaborn library to illustrate trends and comparisons in health expenditures. Dataset The dataset contains the following columns: - `Country`: The name of the country. - `Year`: The year of the recorded health expenditure. - `Spending_USD`: Health spending in USD. Steps to Follow 1. **Load and Prepare the Dataset:** - Load the `healthexp` dataset using Seaborn\'s `load_dataset()` function. - Interpolate missing values. - Reshape the dataframe such that you have a single time-series column containing health expenditure data for each country. 2. **Visualization Requirements:** - Create a line plot of the health expenditure over time for each country using facets. - Color the areas under the lines by country. - Set properties to customize the appearance of the plot, such as edge color and line thickness. - Create a stacked area plot to show part-whole relationships for health expenditures over time. Below are the expected input and output formats. Input The dataset is loaded within the function and should not be taken as an input from the user. Output Your function should return a `seaborn.objects.Plot` object for each of the following: 1. The facetted line plot colored by country. 2. The stacked area plot showing part-whole relationships. Implementation ```python import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(): # Load the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Facetted Line Plot with Colored Areas plot1 = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) plot1.add(so.Area(), color=\\"Country\\") # Stacked Area Plot plot2 = ( so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") .add(so.Area(alpha=0.7), so.Stack()) ) return plot1, plot2 # Call the function to generate the plots plots = visualize_health_expenditure() # Display the plots plots[0].show() # Display the facetted line plot plots[1].show() # Display the stacked area plot ``` Constraints - Use the `seaborn` library and specifically the `seaborn.objects` module to create the plots. - Ensure the output plots are clear and well labeled. Performance Requirements - The function should efficiently handle the dataset provided by the Seaborn library.","solution":"import seaborn.objects as so import seaborn as sns def visualize_health_expenditure(): # Load the dataset healthexp = ( sns.load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Facetted Line Plot with Colored Areas plot1 = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) plot1.add(so.Area(), color=\\"Country\\") # Stacked Area Plot plot2 = ( so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") .add(so.Area(alpha=0.7), so.Stack()) ) return plot1, plot2 # Call the function to generate the plots plots = visualize_health_expenditure()"},{"question":"# Question: Kernel Density Estimation Challenge You are given a dataset of points sampled from a bimodal distribution. Your task is to implement a function that performs kernel density estimation (KDE) on this dataset using different kernels and bandwidths. You will then evaluate the kernel density estimates at specified points and return the results. Function Signature ```python import numpy as np from sklearn.neighbors import KernelDensity def perform_kde(data_points, evaluation_points, kernels, bandwidths): Perform Kernel Density Estimation on a given dataset using various kernels and bandwidths. Parameters: data_points (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the input data points. evaluation_points (np.ndarray): A 2D numpy array of shape (m_samples, n_features) representing the points where KDE will be evaluated. kernels (list of str): A list of kernel names to be used for KDE. Valid options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'. bandwidths (list of float): A list of bandwidth values to be used for KDE. Returns: dict: A dictionary with keys as tuples of kernel name and bandwidth, and values as lists of KDE values evaluated at the specified points. For example: {(\'gaussian\', 0.2): [value1, value2, ...], ...} pass ``` Input - `data_points`: A 2D numpy array of shape (n_samples, n_features), where each row represents a data point. - `evaluation_points`: A 2D numpy array of shape (m_samples, n_features), where each row is a point at which the KDE will be evaluated. - `kernels`: A list of strings, each representing a kernel to be used in the KDE. The available options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'. - `bandwidths`: A list of float values, each representing the bandwidth parameter for the KDE. Output - A dictionary where the keys are tuples of the form `(kernel_name, bandwidth)` and the values are lists of KDE values evaluated at the provided evaluation points. Constraints - Each string in `kernels` must be one of the valid options. - Each float in `bandwidths` must be greater than 0. - The number of dimensions of `data_points` and `evaluation_points` must be the same. Example ```python data_points = np.array([[-2], [-1], [0], [1], [2]]) evaluation_points = np.array([[-1.5], [-0.5], [0.5], [1.5]]) kernels = [\'gaussian\', \'epanechnikov\'] bandwidths = [0.5, 1.0] result = perform_kde(data_points, evaluation_points, kernels, bandwidths) # Expected output # { # (\'gaussian\', 0.5): [...], # (\'gaussian\', 1.0): [...], # (\'epanechnikov\', 0.5): [...], # (\'epanechnikov\', 1.0): [...] # } ``` The function `perform_kde` should be implemented to compute KDEs using `KernelDensity` from `sklearn.neighbors` and return the results as described.","solution":"import numpy as np from sklearn.neighbors import KernelDensity def perform_kde(data_points, evaluation_points, kernels, bandwidths): Perform Kernel Density Estimation on a given dataset using various kernels and bandwidths. Parameters: data_points (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the input data points. evaluation_points (np.ndarray): A 2D numpy array of shape (m_samples, n_features) representing the points where KDE will be evaluated. kernels (list of str): A list of kernel names to be used for KDE. Valid options are \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'. bandwidths (list of float): A list of bandwidth values to be used for KDE. Returns: dict: A dictionary with keys as tuples of kernel name and bandwidth, and values as lists of KDE values evaluated at the specified points. For example: {(\'gaussian\', 0.2): [value1, value2, ...], ...} kde_results = {} for kernel in kernels: for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data_points) log_densities = kde.score_samples(evaluation_points) densities = np.exp(log_densities).tolist() kde_results[(kernel, bandwidth)] = densities return kde_results # Example use case: # data_points = np.array([[-2], [-1], [0], [1], [2]]) # evaluation_points = np.array([[-1.5], [-0.5], [0.5], [1.5]]) # kernels = [\'gaussian\', \'epanechnikov\'] # bandwidths = [0.5, 1.0] # result = perform_kde(data_points, evaluation_points, kernels, bandwidths) # print(result)"},{"question":"# Asyncio Task Management and Synchronization Objective Implement a Python script using the asyncio package to simulate a producer-consumer scenario with task and synchronization primitives. Problem Statement You are required to simulate a producer-consumer model where: 1. A producer produces random integers at random intervals and places them into a queue. 2. Two consumers retrieve integers from the queue and process them. The processing time for each integer would simulate some I/O-bound operation using `asyncio.sleep()`. Use asyncio synchronization primitives to ensure thread-safe access to the queue and proper coordination between tasks. Requirements 1. Implement the producer coroutine, which: - Adds a random integer between 1 and 100 to an asyncio `Queue`. - Sleeps for a random duration (between 1 and 3 seconds) after producing an item. - Runs indefinitely until it produces 10 items. 2. Implement the consumer coroutine, which: - Retrieves an integer from the queue. - Prints a message indicating the start and end of the processing of the integer (including the integer value). - Simulates I/O-bound processing by sleeping for a random duration (between 1 and 5 seconds). - Runs indefinitely until it has consumed all 10 items. 3. Use asyncio synchronization primitives to ensure: - The producer stops producing after 10 items. - Consumers stop processing once there are no more items to consume. Input and Output Format - There are no specific inputs apart from the start of the script. - Outputs are the print statements indicating the production and consumption of items. Constraints 1. The queue might experience simultaneous access. 2. Proper handling of exceptions and task cancellation should be demonstrated. 3. Ensure the script terminates gracefully after all items are produced and consumed. Example Output ```plaintext Producer produced item: 45 Consumer 1 started processing item: 45 Producer produced item: 22 Consumer 1 finished processing item: 45 Consumer 2 started processing item: 22 Producer produced item: 87 Consumer 2 finished processing item: 22 ... [continues until all items are produced and consumed] ``` Performance Requirements - The script should not block the main thread and should demonstrate the asynchronous execution of tasks. Solution Template Provide the following function implementations in the script: ```python import asyncio import random async def producer(queue): # Implementation here async def consumer(queue, consumer_id): # Implementation here async def main(): queue = asyncio.Queue() # Implementation here to set up producer and consumers if __name__ == \'__main__\': asyncio.run(main()) ```","solution":"import asyncio import random async def producer(queue): for _ in range(10): item = random.randint(1, 100) await queue.put(item) print(f\\"Producer produced item: {item}\\") await asyncio.sleep(random.randint(1, 3)) async def consumer(queue, consumer_id): for _ in range(10): item = await queue.get() print(f\\"Consumer {consumer_id} started processing item: {item}\\") await asyncio.sleep(random.randint(1, 5)) print(f\\"Consumer {consumer_id} finished processing item: {item}\\") queue.task_done() async def main(): queue = asyncio.Queue() # Create producer task producer_task = asyncio.create_task(producer(queue)) # Create consumer tasks consumer_tasks = [asyncio.create_task(consumer(queue, i)) for i in range(1, 3)] await producer_task await queue.join() for task in consumer_tasks: task.cancel() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# Audio Device Configuration and Playback You have been provided with the task of configuring an OSS audio device using the deprecated `ossaudiodev` module and processing audio playback. Your goal is to open an audio device, configure its parameters, and then read a hypothetical stream of audio data to play it back. Problem: Implement a Python function `play_audio(device_name: str, audio_params: dict, audio_data: bytes) -> str` that performs the following: 1. Open the audio device specified by `device_name`. The default device should be used if `device_name` is not provided. 2. Configure the audio device using parameters provided in the `audio_params` dictionary, which includes: - `\'format\'`: The audio format (one of `AFMT_*` constants). - `\'channels\'`: The number of audio channels (1 for mono, 2 for stereo). - `\'rate\'`: The sampling rate (e.g., 44100 for CD quality). 3. Write the `audio_data` (a bytes-like object) to the audio device. 4. Ensure that all audio data is written before closing the device. 5. Handle any exceptions (`OSSAudioError`, `OSError`), closing the device if an error occurs, and return an appropriate error message. 6. Return `\\"Success\\"` if the audio playback is configured and completed without errors. Input: - `device_name` (str): The audio device filename. If empty, default to `/dev/dsp`. - `audio_params` (dict): A dictionary with keys `\'format\'`, `\'channels\'`, and `\'rate\'` specifying the audio configuration. - `audio_data` (bytes): The audio data to be played. Output: - `str`: Returns `\\"Success\\"` if the audio data was played correctly, otherwise returns an error message. Constraints: - Assume input audio data size fits within the device\'s buffer capacity. - Ensure the `audio_params` values belong to the supported formats, channels, and sampling rates as specified in the documentation. Example: ```python audio_params = { \'format\': \'AFMT_S16_LE\', \'channels\': 2, \'rate\': 44100 } audio_data = b\\"exampleaudiodata\\" # This should be actual PCM data from an audio file. result = play_audio(\\"/dev/dsp\\", audio_params, audio_data) print(result) # Expected Output: \\"Success\\" or an appropriate error message ``` Hints: - Use `ossaudiodev.open()` to open the audio device. - Use `setparameters()` to configure the audio device. - Use `writeall()` to write audio data to the device. - Properly handle exceptions and ensure the device is closed correctly.","solution":"import ossaudiodev def play_audio(device_name: str, audio_params: dict, audio_data: bytes) -> str: Plays audio on the specified OSS audio device with the given configurations. Parameters: device_name (str): The audio device filename. Defaults to /dev/dsp if not provided. audio_params (dict): A dictionary with keys \'format\', \'channels\', and \'rate\' for audio configuration. audio_data (bytes): The audio data to be played. Returns: str: \\"Success\\" if audio is played successfully, otherwise an error message. if not device_name: device_name = \'/dev/dsp\' try: dsp = ossaudiodev.open(device_name, \'w\') # Configure audio parameters dsp.setfmt(getattr(ossaudiodev, audio_params[\'format\'])) dsp.channels(audio_params[\'channels\']) dsp.speed(audio_params[\'rate\']) # Write all audio data to the device dsp.writeall(audio_data) dsp.close() return \\"Success\\" except (ossaudiodev.OSSAudioError, OSError) as e: try: dsp.close() except Exception: pass return f\\"Error: {e}\\""},{"question":"# Asyncio Coding Assessment Question **Objective**: To assess the student\'s comprehension and practical application of asyncio coroutines, task creation, concurrent execution, and advanced asyncio handling such as timeouts and cancellation. # Problem Statement You are required to write an asynchronous function `fetch_multiple_urls` to fetch data from multiple URLs concurrently using asyncio in Python. **Task**: 1. Implement `fetch(url)` coroutine that: - Takes a URL and fetches its content using an HTTP get request. - Simulates variable network delay using `asyncio.sleep(random_delay)` where `random_delay` is a random integer between 1 and 5 seconds. 2. Implement `fetch_multiple_urls(urls, timeout)` coroutine that: - Takes a list of URLs and a timeout value in seconds. - Creates and schedules the `fetch()` coroutine for each URL concurrently. - Uses asyncio to handle all the tasks and ensure that if any of the tasks take longer than the provided `timeout`, it raises an `asyncio.TimeoutError`. - Returns a dictionary where the keys are the URLs, and the values are: - The content fetched if successful. - `\\"Timeout\\"` if fetching the URL exceeded the timeout. - `\\"Cancelled\\"` if the task was explicitly cancelled for any reason. # Input - A list of URLs. - A timeout value in seconds (float). # Output - A dictionary with URLs as keys and the result/status (`content`, `\\"Timeout\\"`, or `\\"Cancelled\\"`) as values. # Example ```python import asyncio import random # Dummy example function for fetching URL content with random delay async def fetch(url: str) -> str: random_delay = random.randint(1, 5) await asyncio.sleep(random_delay) return f\\"Content of {url}\\" async def fetch_multiple_urls(urls: list, timeout: float) -> dict: results = {} async def wrapper(url): try: results[url] = await asyncio.wait_for(fetch(url), timeout=timeout) except asyncio.TimeoutError: results[url] = \\"Timeout\\" except asyncio.CancelledError: results[url] = \\"Cancelled\\" tasks = [asyncio.create_task(wrapper(url)) for url in urls] await asyncio.gather(*tasks, return_exceptions=True) return results # Sample usage urls = [\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"] timeout = 3.0 # seconds async def main(): results = await fetch_multiple_urls(urls, timeout) for url, result in results.items(): print(f\\"{url}: {result}\\") asyncio.run(main()) ``` # Notes - Use `asyncio.create_task` to run fetch tasks concurrently. - Utilize `asyncio.wait_for` to implement the timeout for each fetch operation. - Handle exceptions gracefully to update the status in the results dictionary appropriately. # Constraints - You are allowed to use only the standard library. - Ensure code runs efficiently and adheres to Python asyncio best practices. # Performance Requirements - The solution should efficiently handle up to 100 URLs without blocking the event loop.","solution":"import asyncio import random async def fetch(url: str) -> str: Simulate an HTTP get request to fetch content of a URL. random_delay = random.randint(1, 5) await asyncio.sleep(random_delay) return f\\"Content of {url}\\" async def fetch_multiple_urls(urls: list, timeout: float) -> dict: Fetch multiple URLs concurrently with a timeout. Returns a dictionary with results or status as values. results = {} async def wrapper(url): try: results[url] = await asyncio.wait_for(fetch(url), timeout=timeout) except asyncio.TimeoutError: results[url] = \\"Timeout\\" except asyncio.CancelledError: results[url] = \\"Cancelled\\" tasks = [asyncio.create_task(wrapper(url)) for url in urls] await asyncio.gather(*tasks, return_exceptions=True) return results"},{"question":"You are tasked with developing a utility that classifies images based on their content type. The utility will read image files from a directory and classify each image into its respective format. Additionally, you will extend the functionality to recognize an additional custom image format. # Requirements 1. Implement a function `classify_images(directory: str) -> dict:` that: - Accepts a string `directory` representing the path to a directory containing image files. - Reads each file in the directory. - Determines the type of each image using the `imghdr.what()` function. - Returns a dictionary where the keys are the recognized image formats (such as \'jpeg\', \'png\', etc.) and the values are lists of filenames corresponding to each format. 2. Implement a function `add_custom_image_format(format_name: str, test_function: callable) -> None:` to: - Accept a string `format_name` representing the custom format name. - Accept a test function `test_function` that takes two arguments (byte-stream and file-like object) and returns the format name if the file matches the custom format, or `None` otherwise. - Add the test function to `imghdr.tests` list to extend the image type recognition capabilities. # Example ```python # Sample content of directory \'image_dir\' # image1.jpeg, image2.png, image3.gif # Classifies images in the directory print(classify_images(\'image_dir\')) # Output: {\'jpeg\': [\'image1.jpeg\'], \'png\': [\'image2.png\'], \'gif\': [\'image3.gif\']} # Custom format function (example) def custom_tst_fn(h, f): if h[:4] == b\'ABCD\': return \'custom\' return None # Add custom image format add_custom_image_format(\'custom\', custom_tst_fn) # Assuming \'custom_img\' has the custom header # Custom recognition print(classify_images(\'custom_image_dir\')) # Output: {\'custom\': [\'custom_img\']} ``` # Constraints - You can assume the directory contains only image files and all necessary permissions are granted. - The custom test function should handle standard image file checks efficiently. # Notes - Ensure your implementation handles edge cases such as empty directories gracefully. - Use the `imghdr` module documentation to understand how to extend its functionality with custom test functions.","solution":"import os import imghdr def classify_images(directory: str) -> dict: Classifies images in a directory based on their formats. Parameters: directory (str): Path to the directory containing image files. Returns: dict: A dictionary where keys are image formats and values are lists of filenames. image_classification = {} for filename in os.listdir(directory): filepath = os.path.join(directory, filename) if os.path.isfile(filepath): img_format = imghdr.what(filepath) if img_format: if img_format not in image_classification: image_classification[img_format] = [] image_classification[img_format].append(filename) return image_classification def add_custom_image_format(format_name: str, test_function: callable) -> None: Adds a custom image format recognition function to imghdr. Parameters: format_name (str): The name of the custom format. test_function (callable): A function to test for the custom format. def custom_test(h, f): result = test_function(h, f) if result: return format_name return None imghdr.tests.append(custom_test)"},{"question":"You are provided with the \\"penguins\\" dataset from seaborn, which contains data on different penguin species. Your task is to: 1. Load the penguins dataset using seaborn. 2. Create a faceted plot showing the relationship between `bill_length_mm` and `bill_depth_mm`. Facet the plot by `species` and `sex`. 3. Modify the facets such that: - Each subplot adapts its x and y axes independently. - Only the x axes across columns are shared. - Only the y axes across rows are shared. 4. Save the plot in a file named `faceted_penguins_plot.png`. # Requirements - Use the `seaborn.objects` interface. - Configure the plots as per the specifications. - Make sure the plot is saved properly with the required filename. # Input None. The dataset is loaded directly in the code. # Output A file named `faceted_penguins_plot.png` containing the faceted plot as per the given specifications. # Constraints - Use the seaborn library for creating and customizing the plot. - Ensure each requirement is separately addressed in your implementation. # Example Usage ```python # Your implementation should be able to produce the output file as specified. import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create and configure the faceted plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dots()) ) # Apply axis sharing configurations p = p.share(x=False, y=False) p = p.share(x=\\"col\\", y=\\"row\\") # Save the plot to a file p.save(\\"faceted_penguins_plot.png\\") ``` Note: Ensure that your script handles this sequence of operations as described.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_faceted_penguins_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Create and configure the faceted plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dots()) ) # Apply axis sharing configurations p = p.share(x=\'col\', y=\'row\') # Save the plot to a file p.save(\\"faceted_penguins_plot.png\\") # Execute the function to create the plot create_faceted_penguins_plot()"},{"question":"**Advanced Coding Assessment: Buffer Handling in Python 3** Python 2 included an \\"old buffer protocol\\" which has been deprecated in Python 3. Now, in Python 3, we use the new buffer protocol for efficient byte-wise data manipulation and various memory management tasks. # Task: Implement a function `get_read_only_buffer` that takes a Python object supporting the buffer protocol and returns a tuple containing: 1. A read-only memoryview of the object\'s buffer. 2. The length of the buffer. Additionally, implement a function `write_to_buffer` that takes a writable buffer object and a byte string. It should copy the byte string into the buffer, ensuring that the buffer has enough space to accommodate the byte string. Both functions should handle any errors gracefully by raising appropriate exceptions with relevant messages. # Function Signatures: ```python def get_read_only_buffer(obj: Any) -> Tuple[memoryview, int]: pass def write_to_buffer(buff: Any, data: bytes) -> None: pass ``` # Input and Output: - `get_read_only_buffer(obj)` - **Input**: `obj` (Any) - A Python object that supports the buffer protocol and provides read-only access. - **Output**: A tuple containing a read-only memoryview and the length of the buffer. - `write_to_buffer(buff, data)` - **Input**: `buff` (Any) - A Python object that supports the buffer protocol and provides writable access. `data` (bytes) - A byte string to be written to the buffer. - **Output**: None. # Constraints: - The `obj` passed to `get_read_only_buffer` should support the buffer protocol and provide read-only access. - The `buff` passed to `write_to_buffer` should support the buffer protocol and provide writable access. - If the `buff` does not have enough space to accommodate `data`, raise a `ValueError`. # Example: ```python # Example usage buffer_obj = bytearray(b\\"Hello, World!\\") # A writable buffer read_only_obj = bytes(b\\"ReadOnlyData\\") # A read-only buffer try: ro_buffer, length = get_read_only_buffer(read_only_obj) print(ro_buffer, length) # Expected: <memory at ...> 12 write_to_buffer(buffer_obj, b\\"Python3\\") print(buffer_obj) # Expected: bytearray(b\\"Python3 World!\\") except Exception as e: print(e) ``` # Note: - Ensure proper handling of lifecycle of the buffers and exceptions. - Use `memoryview` for buffer views.","solution":"from typing import Any, Tuple def get_read_only_buffer(obj: Any) -> Tuple[memoryview, int]: Takes a Python object supporting the buffer protocol and returns a read-only memoryview and the length of the buffer. try: memview = memoryview(obj) if memview.readonly: return memview, len(memview) else: raise ValueError(\\"The object does not provide a read-only buffer.\\") except TypeError: raise TypeError(\\"The object does not support the buffer protocol.\\") def write_to_buffer(buff: Any, data: bytes) -> None: Takes a writable buffer object and a byte string. Copies the byte string into the buffer. Ensures that the buffer has enough space to accommodate the byte string. try: memview = memoryview(buff) if memview.readonly: raise ValueError(\\"The buffer is not writable.\\") if len(memview) < len(data): raise ValueError(\\"The buffer does not have enough space to accommodate the data.\\") memview[:len(data)] = data except TypeError: raise TypeError(\\"The buffer does not support writable buffer protocol.\\")"},{"question":"**Title: Comprehensive Inventory Management System** You are tasked with implementing an inventory management system for a small retail store. The system should encapsulate inventory items using data classes and support context management to safely update inventory quantities. Moreover, it should provide introspection features to dynamically retrieve attributes of inventory items and contain warning mechanisms to alert when stock levels fall below a specified threshold. Requirements 1. **Data Classes for Inventory Items:** - Define a data class `InventoryItem` with the following fields: - `name` (str): Name of the item. - `category` (str): Category of the item such as \'Electronics\', \'Grocery\', etc. - `price` (float): Price of the item. - `quantity` (int): Number of items in stock. - `reorder_level` (int): The minimum quantity before reordering is required. 2. **Context Management for Updates:** - Implement a context manager `inventory_update_context` that allows safe updates to the `quantity` of inventory items. Ensure that changes are only committed upon successful execution within the context, otherwise, revert to the original `quantity`. 3. **Inspection of Inventory Items:** - Implement a function `get_inventory_item_attributes` that uses the `inspect` module to dynamically retrieve the attribute names of an `InventoryItem` instance. 4. **Warning for Low Stock:** - Implement a function `check_inventory_levels` that raises a warning using the `warnings` module when the `quantity` of an item falls below its `reorder_level`. Function Signatures ```python from dataclasses import dataclass @dataclass class InventoryItem: name: str category: str price: float quantity: int reorder_level: int class inventory_update_context: def __init__(self, item: InventoryItem): # initialization code def __enter__(self): # enter method implementation def __exit__(self, exc_type, exc_val, exc_tb): # exit method implementation import inspect def get_inventory_item_attributes(item: InventoryItem) -> list: # function implementation import warnings def check_inventory_levels(item: InventoryItem): # function implementation ``` Constraints - You must use the `dataclasses` module for the `InventoryItem` class. - The `inventory_update_context` must ensure atomicity of quantity updates. - The introspection function should leverage the `inspect` module. - The warning function should properly utilize the `warnings` module to issue warnings. Example Usage ```python item = InventoryItem(name=\'Laptop\', category=\'Electronics\', price=999.99, quantity=10, reorder_level=5) # Test context management with inventory_update_context(item) as inv: item.quantity -= 3 assert item.quantity == 7 # Test introspection attributes = get_inventory_item_attributes(item) print(attributes) # Output: [\'name\', \'category\', \'price\', \'quantity\', \'reorder_level\'] # Test warning for low stock item.quantity = 4 check_inventory_levels(item) # Should raise a warning ``` Performance Requirements - The context management should not introduce significant overhead. - The introspection and warning checks should run in a reasonable time, given typical usage scenarios for a small retail store\'s inventory size.","solution":"from dataclasses import dataclass, asdict import inspect import warnings @dataclass class InventoryItem: name: str category: str price: float quantity: int reorder_level: int class inventory_update_context: def __init__(self, item: InventoryItem): self.item = item self.original_quantity = item.quantity def __enter__(self): return self.item def __exit__(self, exc_type, exc_val, exc_tb): if exc_type is not None: self.item.quantity = self.original_quantity return False # Do not suppress exceptions def get_inventory_item_attributes(item: InventoryItem) -> list: return list(asdict(item).keys()) def check_inventory_levels(item: InventoryItem): if item.quantity < item.reorder_level: warnings.warn(f\\"Item \'{item.name}\' has low stock: {item.quantity} left, reorder level is {item.reorder_level}\\", UserWarning)"},{"question":"Objective: To test the comprehension of Python\'s `importlib` module for handling imports programmatically in a dynamic environment. Problem Statement: You are given a zipped archive containing multiple Python modules. Your task is to write a Python function that dynamically imports a specified module from this zipped archive and executes a given function from the imported module, passing specified arguments to it. Specifications: 1. Write a function `execute_from_zip(archive_path: str, module_name: str, func_name: str, *args, **kwargs) -> Any` that: - **archive_path**: path to the zipped archive containing the Python modules. - **module_name**: name of the module to be imported from the archive. - **func_name**: name of the function to be executed from the module. - **args**: positional arguments to pass to the function. - **kwargs**: keyword arguments to pass to the function. - **Returns**: the result of the function execution. 2. Keep in mind: - The zipped archive can contain multiple modules. - If the module or function does not exist, raise an appropriate exception (`ModuleNotFoundError` or `AttributeError`). Example Usage: ```python result = execute_from_zip(\'modules.zip\', \'math_ops\', \'add\', 5, 10) print(result) # Assuming the function math_ops.add returns the sum, the result should be 15 result = execute_from_zip(\'modules.zip\', \'string_ops\', \'concat\', \'Hello\', \'World\', sep=\', \') print(result) # Assuming the function string_ops.concat concatenates strings with a separator, the result should be \'Hello, World\' ``` Constraints: - You are not allowed to use the traditional `import` statement for importing the modules directly. - Focus on handling exceptions appropriately to ensure robust code. Notes: - You can assume the zipped archive is valid and accessible. - The focus should be on using the `importlib` and handling zipimport functionalities. Hints: - Refer to the `importlib.machinery.ZipImporter` for importing from zip archives. - Use reflection (`getattr`) to dynamically call the function within the imported module.","solution":"import importlib.util import importlib.machinery import zipfile import os def execute_from_zip(archive_path: str, module_name: str, func_name: str, *args, **kwargs) -> any: # Check if the zip file exists if not os.path.isfile(archive_path): raise FileNotFoundError(\\"The specified zip archive does not exist\\") with zipfile.ZipFile(archive_path) as zf: # Check if the specified module is contained in the zipped archive module_file = f\\"{module_name.replace(\'.\', \'/\')}.py\\" if module_file not in zf.namelist(): raise ModuleNotFoundError(f\\"Module {module_name} not found in the archive\\") # Extract the module to a temporary directory with zf.open(module_file) as file: source = file.read() # Create a module spec from the source spec = importlib.util.spec_from_loader(module_name, loader=None) module = importlib.util.module_from_spec(spec) exec(source, module.__dict__) # Get the function from the module try: func = getattr(module, func_name) except AttributeError: raise AttributeError(f\\"Function {func_name} not found in module {module_name}\\") # Execute the function and return the result return func(*args, **kwargs)"},{"question":"# Regression Model Training with Gradient Boosting Regressor In this task, you are required to implement a data preprocessing and model training pipeline using scikit-learn. You will use a synthetic dataset to train a `GradientBoostingRegressor`, experiment with its parameters, and interpret any warnings that arise. Requirements: 1. **Data Preparation:** - Generate a synthetic regression dataset using `make_regression` with 1000 samples and 10 features. - Split the dataset into training and testing sets using `train_test_split` (80% train, 20% test). - Scale the features using `StandardScaler`. 2. **Model Training:** - Train a `GradientBoostingRegressor` on the training set with default parameters. - Train another `GradientBoostingRegressor` with a specified `n_iter_no_change=5` parameter. 3. **Output:** - Print the R² score on the test set for both models. - Capture and print any warnings that are raised during training. Code Implementation: - Implement the above steps within a single function named `train_gradient_boosting`. - The function should return a dictionary containing: - `default_score`: R² score of the model with default parameters. - `new_score`: R² score of the model with `n_iter_no_change=5`. Constraints: - You must use synthetic data generation and avoid using any external datasets. - Ensure the solution is self-contained and runs without additional setup. Example Function Signature: ```python from typing import Dict import warnings def train_gradient_boosting() -> Dict[str, float]: # Your code here return { \\"default_score\\": default_score, \\"new_score\\": new_score, } ``` Note: Consider handling warnings appropriately in the solution to capture and print them. Reference Implementation (Simplified): Here is a basic structure to get you started. However, ensure to complete the detailed requirements outlined above: ```python from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings from typing import Dict def train_gradient_boosting() -> Dict[str, float]: # Make synthetic regression data X, y = make_regression(n_samples=1000, n_features=10) # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Scale the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize the scores dictionary scores = {} # Train with default GradientBoostingRegressor gbdt_default = GradientBoostingRegressor(random_state=42) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") gbdt_default.fit(X_train, y_train) default_score = gbdt_default.score(X_test, y_test) scores[\\"default_score\\"] = default_score for warning in w: print(f\\"Default GBDT Warning: {warning.message}\\") # Train with specified n_iter_no_change gbdt_new = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") gbdt_new.fit(X_train, y_train) new_score = gbdt_new.score(X_test, y_test) scores[\\"new_score\\"] = new_score for warning in w: print(f\\"New GBDT Warning: {warning.message}\\") return scores ``` # Submit Your Solution: - Make sure that all parts of the code are executed and the notebook is in a clean state ready for evaluation. - Ensure the solution is properly documented with comments explaining each critical section.","solution":"from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings from typing import Dict def train_gradient_boosting() -> Dict[str, float]: # Generate synthetic regression dataset X, y = make_regression(n_samples=1000, n_features=10, random_state=42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train the GradientBoostingRegressor with default parameters gbdt_default = GradientBoostingRegressor(random_state=42) # Capture and print warnings with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") gbdt_default.fit(X_train, y_train) default_score = gbdt_default.score(X_test, y_test) for warning in w: print(f\\"Default GBDT Warning: {warning.message}\\") # Train the GradientBoostingRegressor with n_iter_no_change=5 gbdt_new = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) # Capture and print warnings with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") gbdt_new.fit(X_train, y_train) new_score = gbdt_new.score(X_test, y_test) for warning in w: print(f\\"New GBDT Warning: {warning.message}\\") return {\\"default_score\\": default_score, \\"new_score\\": new_score}"},{"question":"**Advanced Sorting Exercises:** # Problem Description You are given a dataset of employees and their respective attributes, including their department, name, and salary. Your task is to implement a single function called `sort_employees` that will accept a list of employee details and a sorting specification. The function should return the list sorted based on the given specification. # Function Signature ```python def sort_employees(employees: list, sorting_spec: list) -> list: pass ``` # Input 1. `employees`: A list of tuples where each tuple represents an employee\'s details in the format: `(department, name, salary)`. - `department`: a string, the department where the employee works. - `name`: a string, the employee\'s name. - `salary`: an integer, the employee\'s salary. 2. `sorting_spec`: A list of tuples where each tuple specifies a sorting key and the order. The order is a boolean value where `True` means descending and `False` means ascending. # Output 1. A list of tuples with employees sorted based on the given specification. # Constraints 1. `employees` should have at least one element and at most `10^4` elements. 2. Each `department` and `name` is a string with a maximum length of 50 characters. 3. The salary is a positive integer less than `10^6`. 4. The `sorting_spec` list will contain at least one and at most three sorting criteria. 5. Sorting stability should be preserved wherever applicable. # Example ```python employees = [ (\'HR\', \'Alice\', 55000), (\'Engineering\', \'Bob\', 75000), (\'HR\', \'Charlie\', 52000), (\'Engineering\', \'David\', 85000), (\'Marketing\', \'Eve\', 49000) ] # Sorting by \'department\' ascending, \'salary\' descending sorting_spec = [(\'department\', False), (\'salary\', True)] print(sort_employees(employees, sorting_spec)) # Output: # [(\'Engineering\', \'David\', 85000), (\'Engineering\', \'Bob\', 75000), (\'HR\', \'Alice\', 55000), (\'HR\', \'Charlie\', 52000), (\'Marketing\', \'Eve\', 49000)] ``` # Notes - You should utilize the `sorted()` function with appropriate key functions or use decorators mentioned in the provided documentation for the implementation. - Ensure that the solution maintains sort stability for equivalent keys. - The function should demonstrate efficient sorting techniques to handle up to `10^4` elements effectively.","solution":"def sort_employees(employees, sorting_spec): Sorts the employees based on the provided sorting specification. Parameters: employees (list): A list of tuples where each tuple represents an employee\'s details in the format: (department, name, salary). sorting_spec (list): A list of tuples where each tuple specifies a sorting key and the order. Returns: list: A list of sorted tuples based on the given specification. sorting_key_indices = { \'department\': 0, \'name\': 1, \'salary\': 2 } # Create sorting keys based on the specification sort_keys = [(sorting_key_indices[key], order) for key, order in sorting_spec] # Sort using multiple keys for key_index, descending in reversed(sort_keys): employees.sort(key=lambda x: x[key_index], reverse=descending) return employees"},{"question":"# Custom Attention Layer in PyTorch Objective: Write a custom attention layer in PyTorch from scratch. This layer should be able to operate on input sequences and produce weighted output sequences using the attention mechanism. Requirements: 1. Implement an `Attention` class that inherits from `torch.nn.Module`. 2. The `Attention` class should have an initializer that sets up the necessary weights and biases. 3. The forward method should accept an input tensor and produce an output tensor with applied attention. Function Signatures: ```python import torch import torch.nn as nn import torch.nn.functional as F class Attention(nn.Module): def __init__(self, input_dim: int, attention_dim: int): Initialize the attention mechanism. Args: - input_dim (int): The number of features in the input tensors. - attention_dim (int): The dimension of the attention scores. super(Attention, self).__init__() # Your code here def forward(self, inputs: torch.Tensor) -> torch.Tensor: Forward pass for the attention mechanism. Args: - inputs (torch.Tensor): A tensor of shape (batch_size, sequence_length, input_dim) Returns: - torch.Tensor: A tensor of shape (batch_size, sequence_length, input_dim) with applied attention # Your code here ``` Input and Output Formats: - The `inputs` tensor will have a shape of `(batch_size, sequence_length, input_dim)`. - The output tensor should have the same shape as the input tensor `(batch_size, sequence_length, input_dim)`. Constraints: - `input_dim` and `attention_dim` will be positive integers. - You are not allowed to use predefined attention classes available in libraries. The implementation must be from scratch. - Consider efficiency in terms of tensor operations. Example: ```python input_dim = 64 attention_dim = 32 sequence_length = 10 batch_size = 5 attention_layer = Attention(input_dim, attention_dim) inputs = torch.randn(batch_size, sequence_length, input_dim) outputs = attention_layer(inputs) print(outputs.shape) # Expected output: torch.Size([5, 10, 64]) ``` In this exercise, students will need to apply their understanding of constructs in PyTorch including `nn.Module`, defining custom layers, and performing tensor operations to calculate attention scores and apply them to the inputs.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class Attention(nn.Module): def __init__(self, input_dim: int, attention_dim: int): Initialize the attention mechanism. Args: - input_dim (int): The number of features in the input tensors. - attention_dim (int): The dimension of the attention scores. super(Attention, self).__init__() self.input_dim = input_dim self.attention_dim = attention_dim self.W_q = nn.Linear(input_dim, attention_dim) self.W_k = nn.Linear(input_dim, attention_dim) self.W_v = nn.Linear(input_dim, input_dim) self.softmax = nn.Softmax(dim=-1) def forward(self, inputs: torch.Tensor) -> torch.Tensor: Forward pass for the attention mechanism. Args: - inputs (torch.Tensor): A tensor of shape (batch_size, sequence_length, input_dim) Returns: - torch.Tensor: A tensor of shape (batch_size, sequence_length, input_dim) with applied attention # Query, Key, and Value matrices Q = self.W_q(inputs) # (batch_size, sequence_length, attention_dim) K = self.W_k(inputs) # (batch_size, sequence_length, attention_dim) V = self.W_v(inputs) # (batch_size, sequence_length, input_dim) # Scaled Dot-Product Attention scores = torch.bmm(Q, K.transpose(1, 2)) / torch.sqrt(torch.tensor(self.attention_dim, dtype=torch.float32)) weights = self.softmax(scores) # (batch_size, sequence_length, sequence_length) # Weighted sum of values output = torch.bmm(weights, V) # (batch_size, sequence_length, input_dim) return output"},{"question":"# Advanced Seaborn Plotting with Layer Customization You are given a dataset from a restaurant, containing information about the bills, tips, and other related attributes. Using the `seaborn.objects` module, your task is to create a customized plot with multiple layers and transformations. This will assess your understanding of seaborn\'s advanced plotting capabilities. Dataset You will work with the `tips` dataset, which is already loaded for you. Task Create a plot that meets the following specifications: 1. **Primary Plot**: Use a scatter plot (`so.Dot`) to display the relationship between `total_bill` (x-axis) and `tip` (y-axis). - Map the size of the dots to the `size` of the party. - Color the dots by `sex`. 2. **First Layer**: Add a linear regression line (`so.Line`) to the plot. - Use `so.PolyFit()` to fit a polynomial regression line. 3. **Second Layer**: Add a bar plot (`so.Bar`). - Display the average `tip` amount by `day` of the week. - Use `so.Hist()` transformation to calculate the averages. - Color the bars by `day`. 4. **Third Layer**: Add a layer using only data where the party size (`size`) is greater than or equal to 3. - Use a different plot type, such as `so.Bar()`, to display the frequency of such instances by `day`. 5. **Legend and Labels**: - Annotate each layer appropriately in the plot\'s legend. - Label the x-axis, y-axis, and add a suitable title. The plot should be clear, well-labeled, and should combine the different visual elements cohesively. Expected Output Your code should produce a plot output that incorporates all the mentioned layers and transformations. ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the primary plot p = so.Plot(tips, \\"total_bill\\", \\"tip\\").add(so.Dot(pointsize=\\"size\\", color=\\"sex\\")) # Add the first layer with a polynomial fit line p.add(so.Line(), so.PolyFit()) # Add the second layer with a bar plot for average tips by day p.add(so.Bar(color=\\"day\\"), so.Hist(), orient=\\"y\\").label(x=\\"Day\\", y=\\"Average Tip\\") # Add the third layer using data where party size >= 3 p.add(so.Bar(), data=tips.query(\\"size >= 3\\"), y=\\"day\\", orient=\\"h\\") # Customize plot with legend and title p.label(x=\\"Total Bill\\", y=\\"Tip\\", title=\\"Restaurant Tips Analysis\\") p.show() ``` Constraints - Do not change the dataset. - Use only the seaborn objects module (`seaborn.objects`). - Ensure the plot is visually informative with layers clearly distinguishable. Performance - Ensure the code runs efficiently without long execution times. - Use seaborn\'s built-in functions to handle data transformations and plotting.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the primary plot p = (so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") .add(so.Dot(), pointsize=\\"size\\", color=\\"sex\\")) # Add the first layer with a polynomial fit line p.add(so.Line(), so.PolyFit()) # Add the second layer with a bar plot for average tips by day p.add(so.Bar(color=\\"day\\"), so.Hist(), y=\\"tip\\", agg=\\"mean\\", orient=\\"v\\") # Add the third layer using data where party size >= 3 p.add(so.Bar(), data=tips.query(\\"size >= 3\\"), x=\\"day\\", y=\\"size\\", orient=\\"v\\") # Customize plot with legend and title p.label(x=\\"Total Bill\\", y=\\"Tip\\", title=\\"Restaurant Tips Analysis\\") p.show()"},{"question":"**Question: Implement an Interactive Python Statement Compiler** You are required to implement a Python function, `interactive_statement_compiler(inputs: List[str], handler: str) -> List[Tuple[int, Any]]`, that takes a list of strings representing lines of Python code and simulates the compilation behavior as would occur in a Python interactive session using the `codeop` module. # Functional Requirements: 1. **Input**: - `inputs`: A list of strings, each string is a line of Python code. - `handler`: A string that specifies the handler type, which can be either `\\"compile_command\\"` or `\\"CommandCompiler\\"`. 2. **Output**: - A list of tuples where each tuple contains: - The index (0-based) of the input line. - The result of the compilation: - If compilation is successful, the result is the compiled code object. - If the input line is a prefix of valid Python code but incomplete, the result is the string `\\"Incomplete\\"`. - If there is any compilation error, the result is the string `\\"Error\\"` followed by the exception message. # Constraints: - The `inputs` list may contain any valid or invalid Python code. - You must handle both valid and invalid inputs gracefully, capturing any exceptions and noting whether the input was incomplete Python code. - You should differentiate between complete and incomplete valid inputs and handle both appropriately. - For `handler`, the two valid values are `\\"compile_command\\"` (using the `compile_command` function) and `\\"CommandCompiler\\"` (using an instance of the `CommandCompiler` class). # Example: ```python inputs = [ \\"x = 5\\", \\"if x == 5:\\", \\" print(x)\\", \\"print(\'done\')\\", \\"invalid line\\" ] handler = \\"compile_command\\" result = interactive_statement_compiler(inputs, handler) # Expected output: (The compilation of \'if x == 5:\' would return \\"Incomplete\\" as it waits for the block to complete.) result = [ (0, <code object <module> at 0x...>), (1, \\"Incomplete\\"), (2, <code object <module> at 0x...>), (3, <code object <module> at 0x...>), (4, \\"Error (SyntaxError: invalid syntax (no default error message...))\\") ] ``` Implement the `interactive_statement_compiler` function. # Task: Implement the function: ```python def interactive_statement_compiler(inputs: List[str], handler: str) -> List[Tuple[int, Any]]: # Your code here ```","solution":"import codeop from typing import List, Tuple, Any def interactive_statement_compiler(inputs: List[str], handler: str) -> List[Tuple[int, Any]]: Takes a list of string lines of Python code and simulates the compilation behavior that would happen in a Python interactive session using the codeop module. Args: inputs (List[str]): A list of strings, each string is a line of Python code. handler (str): A string that specifies the handler type, either \\"compile_command\\" or \\"CommandCompiler\\". Returns: List[Tuple[int, Any]]: A list of tuples where each tuple contains the index of the input line and the result of the compilation. results = [] if handler == \\"compile_command\\": compiler = codeop.compile_command elif handler == \\"CommandCompiler\\": compiler = codeop.CommandCompiler().__call__ else: raise ValueError(\\"Invalid handler specified. Use \'compile_command\' or \'CommandCompiler\'.\\") for i, line in enumerate(inputs): try: result = compiler(line) if result is None: # This means the input is incomplete results.append((i, \\"Incomplete\\")) else: results.append((i, result)) except Exception as e: results.append((i, f\\"Error ({e.__class__.__name__}: {str(e)})\\")) return results"},{"question":"<|Analysis Begin|> The provided document outlines various techniques and classes available in the `sklearn.covariance` module for estimating covariance matrices. The methods discussed include empirical covariance, shrunk covariance, Ledoit-Wolf shrinkage, Oracle Approximating Shrinkage, sparse inverse covariance, and robust covariance estimation. Each method has different use cases, assumptions, and mathematical formulations. Key points from the document: 1. **Empirical Covariance**: Use the `EmpiricalCovariance` class or `empirical_covariance` function. 2. **Shrunk Covariance**: Mitigates issues with eigenvalues by shrinking the covariance matrix using the `ShrunkCovariance` class or `shrunk_covariance` function. 3. **Ledoit-Wolf Shrinkage**: Finds an optimal shrinkage coefficient to minimize estimation error using the `LedoitWolf` class or `ledoit_wolf` function. 4. **Oracle Approximating Shrinkage (OAS)**: Another method to find a shrinkage coefficient under Gaussian distribution assumption using the `OAS` class or `oas` function. 5. **Sparse Inverse Covariance**: Uses the `GraphicalLasso` or `GraphicalLassoCV` class to estimate a sparse precision matrix. 6. **Robust Covariance Estimation**: Handles outliers using the `MinCovDet` class. Given these key points, we can design a question that requires the student to implement a function using some of these techniques to estimate and compare covariance matrices. This will test their understanding of the methods, the ability to implement them, and the capability to interpret results. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: To assess your understanding of various covariance matrix estimation methods provided by the `sklearn.covariance` module and your ability to implement them for data analysis. Task: Implement a function that takes a dataset and computes the covariance matrix using three different methods: Empirical, Ledoit-Wolf Shrinkage, and Sparse Inverse Covariance. Compare the results by printing out the covariance matrices and explaining briefly in comments how each method is suitable for different scenarios. Requirements: 1. **Function Name**: `compare_covariance_methods` 2. **Input**: - `data`: A NumPy array of shape (n_samples, n_features) representing the dataset. 3. **Output**: None. The function should print: - Empirical covariance matrix - Ledoit-Wolf shrunk covariance matrix - Sparse inverse covariance matrix Constraints: - Use appropriate classes and methods provided in the `sklearn.covariance` module. - Assume the data does not need centering (i.e., set `assume_centered=True`). - Set the `alpha` parameter in `GraphicalLasso` to 0.01. - The function should handle exceptions and ensure that the provided data is valid. Example Usage: ```python import numpy as np # Generate a synthetic dataset np.random.seed(0) data = np.random.randn(100, 5) compare_covariance_methods(data) ``` Implementation: ```python import numpy as np from sklearn.covariance import EmpiricalCovariance, LedoitWolf, GraphicalLasso def compare_covariance_methods(data): try: # Ensure data is a valid NumPy array data = np.array(data) # Empirical Covariance emp_cov = EmpiricalCovariance(assume_centered=True) emp_cov.fit(data) print(\\"Empirical Covariance Matrix:\\") print(emp_cov.covariance_) # Ledoit-Wolf Shrinkage Covariance lw_cov = LedoitWolf(assume_centered=True) lw_cov.fit(data) print(\\"nLedoit-Wolf Shrinkage Covariance Matrix:\\") print(lw_cov.covariance_) # Sparse Inverse Covariance sparse_cov = GraphicalLasso(alpha=0.01, assume_centered=True) sparse_cov.fit(data) print(\\"nSparse Inverse Covariance Matrix:\\") print(sparse_cov.covariance_) except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage if __name__ == \\"__main__\\": data = np.random.randn(100, 5) # Generating random data compare_covariance_methods(data) ``` Explanation: - **Empirical Covariance**: Provides a simple covariance estimation suitable for large datasets with more samples than features. - **Ledoit-Wolf Shrinkage**: Reduces estimation error by applying shrinkage, particularly useful when sample size is smaller relative to the number of features. - **Sparse Inverse Covariance**: Estimates a sparse precision matrix that is helpful in identifying conditional independence between features in datasets with small sample size. By implementing and comparing these methods, you will gain insights into their applications and suitability for different types of data.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, LedoitWolf, GraphicalLasso def compare_covariance_methods(data): try: # Ensure data is a valid NumPy array data = np.array(data) # Empirical Covariance emp_cov = EmpiricalCovariance(assume_centered=True) emp_cov.fit(data) print(\\"Empirical Covariance Matrix:\\") print(emp_cov.covariance_) # Ledoit-Wolf Shrinkage Covariance lw_cov = LedoitWolf(assume_centered=True) lw_cov.fit(data) print(\\"nLedoit-Wolf Shrinkage Covariance Matrix:\\") print(lw_cov.covariance_) # Sparse Inverse Covariance sparse_cov = GraphicalLasso(alpha=0.01, assume_centered=True) sparse_cov.fit(data) print(\\"nSparse Inverse Covariance Matrix:\\") print(sparse_cov.covariance_) except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage if __name__ == \\"__main__\\": data = np.random.randn(100, 5) # Generating random data compare_covariance_methods(data)"},{"question":"# Advanced Function Implementation with seaborn You are tasked with creating a custom function utilizing seaborn\'s `blend_palette` function to generate a color palette and visualize it. Your function, `generate_and_plot_palette`, should meet the following requirements: Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def generate_and_plot_palette(colors, as_cmap, n_colors): Generate a blended color palette and visualize it. Parameters: colors (list): A list of color specifications to blend. as_cmap (bool): If True, return a continuous colormap, otherwise return a discrete palette. n_colors (int): Number of colors to include in the discrete palette. Returns: None: The function should display a visualization of the palette. pass ``` Input - `colors` (list of strings): A list of color names or hex codes to blend. - `as_cmap` (bool): A boolean indicating whether to generate a continuous colormap (True) or a discrete palette (False). - `n_colors` (int): Number of colors to include in the discrete palette. This parameter should be ignored if `as_cmap` is True. Output The function does not return any value but should display a seaborn heatmap visualizing the generated palette. Requirements 1. Use the `sns.blend_palette` function to create the color palette or colormap based on user input. 2. If `as_cmap` is `False`, create a discrete palette with a number of colors specified by `n_colors`. 3. Visualize the generated palette or colormap using a `seaborn.heatmap`. Example ```python # Example function call: generate_and_plot_palette([\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"], as_cmap=False, n_colors=10) ``` # Constraints - The `colors` list should contain at least two colors. - If `n_colors` is provided, it should be a positive integer greater than 1. - Utilize seaborn and matplotlib as necessary to generate the visualizations. # Performance Considerations - Ensure the function runs efficiently even when creating palettes with a large number of colors. - Handle invalid input gracefully with appropriate error messages. Hints - Refer to seaborn\'s `blend_palette` documentation for more on how to use this function. - Consider using `sns.heatmap` to create an easy-to-understand visualization of your palette or colormap. Implement the function `generate_and_plot_palette` based on the above requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def generate_and_plot_palette(colors, as_cmap, n_colors): Generate a blended color palette and visualize it. Parameters: colors (list): A list of color specifications to blend. as_cmap (bool): If True, return a continuous colormap, otherwise return a discrete palette. n_colors (int): Number of colors to include in the discrete palette. Returns: None: The function should display a visualization of the palette. # Validate inputs if not isinstance(colors, list) or len(colors) < 2: raise ValueError(\\"The \'colors\' list must contain at least two colors.\\") if not isinstance(as_cmap, bool): raise ValueError(\\"The \'as_cmap\' parameter must be a boolean.\\") if not isinstance(n_colors, int) or n_colors <= 1: raise ValueError(\\"The \'n_colors\' parameter must be a positive integer greater than 1.\\") if as_cmap: palette = sns.blend_palette(colors, as_cmap=True) # Create a 1D array for the heatmap visualization data = np.linspace(0, 1, 100).reshape(1, 100) sns.heatmap(data, cmap=palette, cbar=False, xticklabels=False, yticklabels=False) else: palette = sns.blend_palette(colors, n_colors=n_colors) # Create a 2D array for the heatmap visualization data = np.array([np.arange(n_colors), np.arange(n_colors)]).T sns.heatmap(data, cmap=sns.color_palette(palette, as_cmap=True), cbar=False, xticklabels=False, yticklabels=False) plt.show()"},{"question":"You are tasked with creating a function decorator named `log_exceptions` that logs detailed information about any exceptions that occur in the decorated function. The log should include the exception type, value, and traceback. The logging should be done in a format that mimics the behavior of the Python interpreter\'s traceback. # Requirements: 1. The decorator should log information using a function called `log_error` which is provided. This function logs error messages to a specified file. 2. The log should contain the traceback similar to what `traceback.print_exception` outputs. 3. The decorator should handle and log both standard exceptions and syntax errors appropriately. 4. Ensure that the original exception is raised again after logging so that the program behaves as it did before adding the decorator. # Function Signature ```python def log_exceptions(log_file: str): # Implementation here ``` # Example Usage ```python import sys # Provided function that logs error messages to a specified file def log_error(message: str, file: str): with open(file, \'a\') as f: f.write(message + \'n\') @log_exceptions(\'error.log\') def test_function(): # This will cause a ZeroDivisionError return 1 / 0 try: test_function() except ZeroDivisionError: pass # Exception is raised again after logging @log_exceptions(\'error.log\') def test_syntax_error(): eval(\'x === 2\') # This will cause a SyntaxError try: test_syntax_error() except SyntaxError: pass # Exception is raised again after logging ``` # Input and Output Format - The decorated functions will not take any input arguments, and their return values are not relevant for this task. - The `log_exceptions` function takes a single string argument, `log_file`, which is the file path where the log messages should be written. - The log message written should start with `Traceback (most recent call last):` followed by the formatted stack trace and the exception message. # Constraints - You may assume that the provided `log_error(message: str, file: str)` function is already defined. - Ensure that your solution is efficient and does not add significant overhead to the decorated functions. - Use the `traceback` module functions to implement the logging. # Notes: - Make use of `traceback.format_exception` to capture and format the stack trace and exception message. - Use appropriate exception handling mechanisms to capture syntax errors via `eval`. # Test your implementation with various types of exceptions to ensure it captures and logs them correctly.","solution":"import traceback def log_error(message: str, file: str): with open(file, \'a\') as f: f.write(message + \'n\') def log_exceptions(log_file: str): Decorator that logs detailed information about any exceptions that occur in the decorated function to a specified file. def decorator(func): def wrapper(*args, **kwargs): try: return func(*args, **kwargs) except Exception as e: # Capture the exception details exc_type, exc_value, exc_tb = e.__class__, e, e.__traceback__ error_message = \'\'.join(traceback.format_exception(exc_type, exc_value, exc_tb)) # Log the exception details log_error(f\\"Traceback (most recent call last):n{error_message}\\", log_file) # Re-raise the original exception raise e return wrapper return decorator"},{"question":"# Unsupervised Dimensionality Reduction and Classification with PCA **Objective:** Write a Python function to perform dimensionality reduction using PCA and train a k-nearest neighbors classifier on the reduced data to predict the labels of a test set. Evaluate the model performance using accuracy. **Function Signature:** ```python def pca_knn_classification(train_data: np.ndarray, train_labels: np.ndarray, test_data: np.ndarray, test_labels: np.ndarray, n_components: int, k_neighbors: int) -> float: Perform PCA for dimensionality reduction and KNN classification. Parameters: - train_data: np.ndarray - The training data, shape (n_samples, n_features) - train_labels: np.ndarray - The labels for the training data, shape (n_samples,) - test_data: np.ndarray - The test data, shape (n_samples, n_features) - test_labels: np.ndarray - The labels for the test data, shape (n_samples,) - n_components: int - The number of principal components to keep - k_neighbors: int - The number of neighbors to use in KNN classification Returns: - accuracy: float - The classification accuracy on the test data ``` **Input Format:** - `train_data` (numpy.ndarray): A 2D array with shape `(n_samples, n_features)` representing the training data. - `train_labels` (numpy.ndarray): A 1D array with shape `(n_samples,)` representing the labels of the training data. - `test_data` (numpy.ndarray): A 2D array with shape `(n_samples, n_features)` representing the test data. - `test_labels` (numpy.ndarray): A 1D array with shape `(n_samples,)` representing the labels of the test data. - `n_components` (int): The number of principal components to keep. - `k_neighbors` (int): The number of neighbors to use for the k-nearest neighbors (KNN) algorithm. **Output Format:** - (float): The accuracy of the KNN classifier on the test data. **Constraints:** - Use the `PCA` class from the `sklearn.decomposition` module for dimensionality reduction. - Use the `KNeighborsClassifier` class from the `sklearn.neighbors` module for classification. - Ensure that `n_components` does not exceed the number of features in the original data. **Example:** ```python import numpy as np # Example data (toy dataset) X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) y_train = np.array([0, 1, 0, 1]) X_test = np.array([[1, 2], [9, 10]]) y_test = np.array([0, 1]) accuracy = pca_knn_classification(X_train, y_train, X_test, y_test, n_components=1, k_neighbors=1) print(f\\"Test Accuracy: {accuracy}\\") ``` **Additional Notes:** 1. If necessary, scale the features using `preprocessing.StandardScaler` before applying PCA. 2. Evaluate the classifier using `accuracy_score` from `sklearn.metrics`. 3. The function should be efficient and capable of handling a reasonably large dataset.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def pca_knn_classification(train_data: np.ndarray, train_labels: np.ndarray, test_data: np.ndarray, test_labels: np.ndarray, n_components: int, k_neighbors: int) -> float: # Standardize the data scaler = StandardScaler() train_data = scaler.fit_transform(train_data) test_data = scaler.transform(test_data) # Apply PCA for dimensionality reduction pca = PCA(n_components=n_components) train_data_pca = pca.fit_transform(train_data) test_data_pca = pca.transform(test_data) # Train KNN classifier knn = KNeighborsClassifier(n_neighbors=k_neighbors) knn.fit(train_data_pca, train_labels) # Predict on test data test_predictions = knn.predict(test_data_pca) # Calculate accuracy accuracy = accuracy_score(test_labels, test_predictions) return accuracy"},{"question":"# XML Parsing with Handlers **Objective:** Implement a function that parses an XML string using the `xml.parsers.expat` module and extracts specific information based on predefined handler functions. **Problem Statement:** You are given an XML string that contains information about a list of books in a library. Each book has properties like title, author, year, and genre. You need to parse this XML string and collect specific data regarding all books. Your task is to: 1. Parse the XML string using the `xml.parsers.expat` module. 2. Implement handler functions to extract and store the title and author of each book. 3. Return a list of dictionaries, where each dictionary represents a book with its title and author. **Input:** - A single string representing an XML document. The structure of the XML will look like this: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2021</year> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2020</year> <genre>Science</genre> </book> <!-- More books --> </library> ``` **Output:** - A list of dictionaries. Each dictionary should contain the title and the author of a book. For the above example, the output would look like this: ```python [ {\\"title\\": \\"Book Title 1\\", \\"author\\": \\"Author 1\\"}, {\\"title\\": \\"Book Title 2\\", \\"author\\": \\"Author 2\\"} ] ``` **Requirements:** - You must use the `xml.parsers.expat` module for XML parsing. - You must set up appropriate handler functions to handle the start of an element, the end of an element, and the character data between elements. **Example Submission:** ```python import xml.parsers.expat def parse_books(xml_string): books = [] current_book = {} current_element = \\"\\" def start_element(name, attrs): nonlocal current_element current_element = name if name == \\"book\\": current_book.clear() def end_element(name): nonlocal current_element current_element = \\"\\" if name == \\"book\\": books.append(current_book.copy()) def char_data(data): if current_element == \\"title\\" or current_element == \\"author\\": current_book[current_element] = data.strip() parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_string, True) return books # Example usage: xml_string = <?xml version=\\"1.0\\"?> <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2021</year> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2020</year> <genre>Science</genre> </book> </library> print(parse_books(xml_string)) # Expected output: # [{\'title\': \'Book Title 1\', \'author\': \'Author 1\'}, {\'title\': \'Book Title 2\', \'author\': \'Author 2\'}] ``` **Constraints:** 1. The XML data will be well-formed. 2. The XML will always contain the `library` element as the root. 3. Each `book` element will have exactly one `title` and one `author` element. **Performance Requirements:** - Your function should run efficiently even for large XML strings containing thousands of book elements.","solution":"import xml.parsers.expat def parse_books(xml_string): books = [] current_book = {} current_element = \\"\\" def start_element(name, attrs): nonlocal current_element current_element = name if name == \\"book\\": current_book.clear() def end_element(name): nonlocal current_element current_element = \\"\\" if name == \\"book\\": books.append(current_book.copy()) def char_data(data): if current_element == \\"title\\" or current_element == \\"author\\": current_book[current_element] = data.strip() parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_string, True) return books"},{"question":"Data Analysis and Transformation using Pandas Objective: The goal of this assignment is to assess your understanding and proficiency with the `pandas` library. You are required to perform data creation, manipulation, transformation, and analysis tasks. Task: You have been provided with a dataset containing information about various products sold at a store in the form of a `DataFrame`. Perform the following tasks using the `pandas` library. Instructions: 1. **Data Creation**: - Create a `DataFrame` named `sales_data` from the given dictionary containing the sales data. ```python data = { \\"Product\\": [\\"AAA\\", \\"BBB\\", \\"CCC\\", \\"DDD\\", \\"AAA\\", \\"BBB\\", \\"CCC\\", \\"DDD\\"], \\"Category\\": [\\"Electronics\\", \\"Electronics\\", \\"Groceries\\", \\"Groceries\\", \\"Clothing\\", \\"Clothing\\", \\"Electronics\\", \\"Groceries\\"], \\"Date\\": pd.date_range(start=\\"2023-01-01\\", periods=8, freq=\\"D\\"), \\"Units_Sold\\": [10, 15, 7, 13, 8, 9, 14, 11], \\"Revenue\\": [500, 750, 200, 400, 240, 270, 780, 320] } ``` 2. **Viewing Data**: - Display the first 5 rows of the `sales_data` DataFrame. 3. **Data Manipulation**: - Add a new column `Unit_Price` which is calculated as `Revenue / Units_Sold`. - Sort the DataFrame by the `Date` column in descending order. 4. **Selection**: - Select and display all rows where the `Category` is `\'Electronics\'`. 5. **Grouping and Aggregation**: - Group the data by `Category` and calculate the total `Units_Sold` and `Revenue` for each category. 6. **Time Series Analysis**: - Set the `Date` column as the index of the DataFrame. - Resample the data to get the total `Revenue` per week. 7. **Handling Missing Data**: - Introduce `NaN` values in the `Units_Sold` column for the dates \\"2023-01-04\\" and \\"2023-01-05\\". - Fill the `NaN` values with the mean value of the `Units_Sold` column. 8. **Categorical Data**: - Convert the `Category` column to a categorical data type. - Rename the categories to more meaningful names: `[\'Electronics\' -> \'Elec\', \'Groceries\' -> \'Groc\', \'Clothing\' -> \'Cloth\']`. 9. **Merge and Join**: - Create a new DataFrame `discounts` with the following data: ```python discounts = pd.DataFrame({ \\"Product\\": [\\"AAA\\", \\"BBB\\", \\"CCC\\", \\"DDD\\"], \\"Discount(%)\\": [5, 10, 3, 7] }) ``` - Merge `sales_data` with `discounts` DataFrame on the `Product` column. Constraints: - You must use the provided variable names. - Your code should be efficient and handle the tasks as described. - Ensure your code runs without errors and produces the expected output. Expected Input and Output Formats: - **Input**: The input data is already provided in the form of dictionaries and will be converted to DataFrames. - **Output**: The output should be the final DataFrame after applying all transformations and calculations. Example Output: ```python # Displaying the final DataFrame after performing all tasks print(final_sales_data) # Example final_sales_data output might look like: # Product Category Date Units_Sold Revenue Unit_Price Discount(%) # Date # 2023-01-08 DDD Groc 2023-01-08 11 320 29.090909 7 # 2023-01-07 CCC Elec 2023-01-07 14 780 55.714286 3 # 2023-01-06 BBB Cloth 2023-01-06 9 270 30.000000 10 # 2023-01-05 AAA Cloth 2023-01-05 8 240 30.000000 5 # 2023-01-04 DDD Groc 2023-01-04 13 400 30.769231 7 # 2023-01-03 CCC Groc 2023-01-03 7 200 28.571429 3 # 2023-01-02 BBB Elec 2023-01-02 15 750 50.000000 10 # 2023-01-01 AAA Elec 2023-01-01 10 500 50.000000 5 ``` Good luck!","solution":"import pandas as pd import numpy as np def create_sales_data(): data = { \\"Product\\": [\\"AAA\\", \\"BBB\\", \\"CCC\\", \\"DDD\\", \\"AAA\\", \\"BBB\\", \\"CCC\\", \\"DDD\\"], \\"Category\\": [\\"Electronics\\", \\"Electronics\\", \\"Groceries\\", \\"Groceries\\", \\"Clothing\\", \\"Clothing\\", \\"Electronics\\", \\"Groceries\\"], \\"Date\\": pd.date_range(start=\\"2023-01-01\\", periods=8, freq=\\"D\\"), \\"Units_Sold\\": [10, 15, 7, 13, 8, 9, 14, 11], \\"Revenue\\": [500, 750, 200, 400, 240, 270, 780, 320] } return pd.DataFrame(data) def view_data(sales_data): return sales_data.head() def manipulate_data(sales_data): sales_data[\\"Unit_Price\\"] = sales_data[\\"Revenue\\"] / sales_data[\\"Units_Sold\\"] sales_data.sort_values(by=\\"Date\\", ascending=False, inplace=True) return sales_data def select_data(sales_data): return sales_data[sales_data[\\"Category\\"] == \\"Electronics\\"] def group_and_aggregate(sales_data): return sales_data.groupby(\\"Category\\").agg({\\"Units_Sold\\": \\"sum\\", \\"Revenue\\": \\"sum\\"}).reset_index() def time_series_analysis(sales_data): sales_data.set_index(\\"Date\\", inplace=True) return sales_data.resample(\\"W\\").sum() def handle_missing_data(sales_data): sales_data.loc[sales_data.index[3:5], \'Units_Sold\'] = np.nan sales_data[\'Units_Sold\'].fillna(sales_data[\'Units_Sold\'].mean(), inplace=True) return sales_data def categorical_data_handling(sales_data): sales_data[\\"Category\\"] = sales_data[\\"Category\\"].astype(\\"category\\") category_mappings = {\\"Electronics\\": \\"Elec\\", \\"Groceries\\": \\"Groc\\", \\"Clothing\\": \\"Cloth\\"} sales_data[\\"Category\\"] = sales_data[\\"Category\\"].cat.rename_categories(category_mappings) return sales_data def merge_with_discounts(sales_data): discounts = pd.DataFrame({ \\"Product\\": [\\"AAA\\", \\"BBB\\", \\"CCC\\", \\"DDD\\"], \\"Discount(%)\\": [5, 10, 3, 7] }) return sales_data.merge(discounts, on=\\"Product\\") def main(): sales_data = create_sales_data() print(\\"Original Data:\\") print(sales_data) print(\\"nFirst 5 Rows of Data:\\") head_data = view_data(sales_data) print(head_data) manipulated_data = manipulate_data(sales_data) print(\\"nData after Manipulation (Added Unit_Price and Sorted by Date):\\") print(manipulated_data) selected_data = select_data(manipulated_data) print(\\"nRows where Category is Electronics:\\") print(selected_data) aggregated_data = group_and_aggregate(manipulated_data) print(\\"nGrouped and Aggregated Data by Category:\\") print(aggregated_data) time_series_data = time_series_analysis(manipulated_data) print(\\"nTime Series Analysis (Revenue per Week):\\") print(time_series_data) handled_missing_data = handle_missing_data(manipulated_data) print(\\"nData after Handling Missing Values in Units_Sold:\\") print(handled_missing_data) categorized_data = categorical_data_handling(handled_missing_data) print(\\"nData with Categorical Data Type for Category:\\") print(categorized_data) final_sales_data = merge_with_discounts(categorized_data) print(\\"nFinal Sales Data after Merging with Discounts:\\") print(final_sales_data) return final_sales_data final_sales_data = main()"},{"question":"Task You are required to write a Python function that creates a multipart email message, adds multiple text parts with different content types, and sets specific email headers. You will then serialize the message into a string and return it. # Function Signature ```python def create_email_message() -> str: pass ``` # Requirements 1. **Create an instance of `EmailMessage`.** 2. **Set the following headers for the email message:** - `From`: \\"sender@example.com\\" - `To`: \\"receiver@example.com\\" - `Subject`: \\"Test Email\\" 3. **Add a text part to the email with the content: \\"This is the plain text part of the email,\\" and content type `text/plain`.** 4. **Add another text part to the email with the content: \\"This is the HTML part of the email,\\" and content type `text/html`.** 5. **Serialize the email message into a string and return it.** # Constraints - The function should not accept any input arguments. - Use the `email.message.EmailMessage` class and its methods to construct the email. - Ensure the correct MIME structure by using `multipart/alternative` for the message. # Example Output The function should return a string representing a serialized email message with the specified headers and content parts, properly formatted. # Performance Requirements - The function should handle the task efficiently, ensuring the correct use of `EmailMessage` methods. Here\'s the skeleton code to get you started: ```python from email.message import EmailMessage def create_email_message() -> str: # Create the email message msg = EmailMessage() # Set headers msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"receiver@example.com\\" msg[\'Subject\'] = \\"Test Email\\" # Add plain text part msg.set_content(\\"This is the plain text part of the email.\\") # Add HTML part msg.add_alternative(\\"<html><body>This is the HTML part of the email.</body></html>\\", subtype=\'html\') # Serialize the message to a string msg_string = msg.as_string() return msg_string ``` # Note Ensure that you understand the hierarchy and structure of email messages, particularly when dealing with multipart messages and different content types.","solution":"from email.message import EmailMessage def create_email_message() -> str: Creates an email message with both plain text and HTML parts, and returns the serialized string representation of the email. # Create the email message msg = EmailMessage() # Set headers msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"receiver@example.com\\" msg[\'Subject\'] = \\"Test Email\\" # Add plain text part msg.set_content(\\"This is the plain text part of the email.\\") # Add HTML part in the correct multipart/alternative structure msg.add_alternative(\'<html><body><p>This is the HTML part of the email.</p></body></html>\', subtype=\'html\') # Serialize the message to a string msg_string = msg.as_string() return msg_string"},{"question":"**Coding Assessment Question: Seaborn Point Plots** # Objective Demonstrate your understanding of the Seaborn `pointplot` function by generating and customizing a variety of visualizations with different datasets and requirements. # Tasks 1. **Dataset Loading and Preparation** * Load the `penguins` dataset using Seaborn\'s `load_dataset` function. * Load the `flights` dataset in the same way. 2. **Plot 1: Basic Point Plot** * Create a basic point plot of `penguins` dataset where: * `x` axis represents the `island` variable. * `y` axis represents the `body_mass_g` variable. 3. **Plot 2: Grouped Point Plot** * Enhance the previous plot by adding `sex` as a `hue` to differentiate data points by sex. 4. **Plot 3: Customized Error Bars** * Modify the second plot to display standard deviation error bars instead of the default confidence intervals. 5. **Plot 4: Comprehensive Customization** * Further modify the second plot with the following customizations: * Use markers `[\'o\', \'s\']` for different sexes. * Apply linestyles `[\'-\', \'--\']` for different sexes. * Set point colors to `[\\"blue\\", \\"green\\"]`. * Add a title: \\"Body Mass Comparison across Islands\\". 6. **Plot 5: Avoid Overplotting** * Using the `flights` dataset, create a point plot with: * `x` axis representing the `month`. * `y` axis representing the `passengers`. * Add `year` as `hue`. * Use `dodge` to avoid overplotting. 7. **Plot 6: Native Scale and Customization** * Create a plot from the `flights` dataset: * Use the month of your choice (e.g., \'June\') as a one-dimensional data series. * Preserve the native scale. * Customize the plot by adding a red star marker (`*`) at the year 1955 with a custom value of 335. # Requirements * **Input**: All datasets will be loaded from Seaborn\'s available datasets using `sns.load_dataset`. * **Output**: Generated plots displayed using Matplotlib\'s `show()` function. * Ensure each plot is presented sequentially in a Jupyter notebook cell. # Constraints * Utilize only the Seaborn and Matplotlib libraries for plotting. * Maintain clear and well-commented code for each step. * Each plot should be on a separate cell in Jupyter Notebook. # Performance Adherence to the instructions and correct visualization output is key. Plot aesthetics and proper usage of Seaborn functionalities will be evaluated.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_datasets(): Load the penguins and flights datasets from Seaborn. penguins = sns.load_dataset(\'penguins\') flights = sns.load_dataset(\'flights\') return penguins, flights def plot_penguins_basic(penguins): Create a basic point plot of the penguins dataset with island vs body_mass_g. plt.figure() sns.pointplot(x=\'island\', y=\'body_mass_g\', data=penguins) plt.show() def plot_penguins_grouped(penguins): Create a grouped point plot of the penguins dataset with island vs body_mass_g, with sex as hue. plt.figure() sns.pointplot(x=\'island\', y=\'body_mass_g\', hue=\'sex\', data=penguins) plt.show() def plot_penguins_custom_errorbars(penguins): Modify the grouped plot to display standard deviation error bars. plt.figure() sns.pointplot(x=\'island\', y=\'body_mass_g\', hue=\'sex\', data=penguins, ci=\'sd\') plt.show() def plot_penguins_fully_customized(penguins): Create a fully customized point plot with specified markers, linestyles, colors, and title. plt.figure() sns.pointplot(x=\'island\', y=\'body_mass_g\', hue=\'sex\', data=penguins, markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], palette=[\\"blue\\", \\"green\\"]) plt.title(\\"Body Mass Comparison across Islands\\") plt.show() def plot_flights_avoid_overplotting(flights): Create a point plot with the flights dataset avoiding overplotting by using dodge. plt.figure() sns.pointplot(x=\'month\', y=\'passengers\', hue=\'year\', data=flights, dodge=True) plt.show() def plot_flights_native_scale_custom(flights): Create a native scale plot for the month of June and customize. june_flights = flights[flights[\'month\'] == \'June\'] plt.figure() ax = sns.pointplot(x=\'year\', y=\'passengers\', data=june_flights) plt.scatter(x=[1955], y=[335], color=\'red\', marker=\'*\') plt.show()"},{"question":"Objective Your task is to write a function that analyzes a given module to extract and display information about its classes and functions, including their hierarchy and relationships, using the \\"pyclbr\\" module. Input The function will receive: 1. `module_name`: A string representing the name of the module to read. It may be within a package. 2. `path`: An optional list of directory paths to prepend to `sys.path` to locate the module source code. Output The function should print the following: 1. **Classes and Functions:** A list of all top-level classes and functions in the module. 2. **Hierarchy and Relationships:** - For each top-level class, list its methods and any nested classes or functions. - For each nested class or function, repeat the above hierarchy listing. Constraints - You cannot import or execute any code from the module for security reasons. - You must use the `pyclbr` module to gather the necessary information. Example For a module `example_module` with the following structure: ```python # example_module.py class A: def method1(self): pass class B: def method2(self): pass def function1(): async def nested_function(): pass ``` Your function should produce the following output: ``` Classes and Functions: - A (Class) - function1 (Function) Hierarchy and Relationships: Class A: Methods: - method1 (Line 3) Nested Classes: - B (Class) Methods: - method2 (Line 6) Function function1: Nested Functions: - nested_function (Function, Line 12) ``` Function Signature ```python def analyze_module(module_name: str, path: Optional[List[str]] = None) -> None: pass ``` Notes - Ensure you format the output for ease of reading and understanding. - Use proper indentation to represent nested relationships. - Handle any potential errors gracefully, such as missing or unreadable modules, by displaying an appropriate message. Good luck!","solution":"import pyclbr import sys from typing import List, Optional def analyze_module(module_name: str, path: Optional[List[str]] = None) -> None: if path: sys.path.extend(path) try: module_info = pyclbr.readmodule(module_name) except ImportError as e: print(f\\"Error: Unable to import module \'{module_name}\'. {str(e)}\\") return class_hierarchy = {} for name, obj in module_info.items(): class_hierarchy.setdefault(obj.file, {}).setdefault(name, obj) def print_class_info(name, obj, indent=0): print(f\\"{\' \' * indent}- {name} (Class)\\") indent += 1 # Print methods methods = {k: v for k, v in obj.items() if not isinstance(v, (dict, pyclbr.Function))} if methods: print(f\\"{\' \' * indent}Methods:\\") for method_name, method_obj in methods.items(): print(f\\"{\' \' * (indent + 1)}- {method_name} (Line {method_obj.lineno})\\") # Print nested classes and functions nested_items = {k: v for k, v in obj.items() if isinstance(v, dict)} if nested_items: print(f\\"{\' \' * indent}Nested Classes:\\") for nested_name, nested_obj in nested_items.items(): print_class_info(nested_name, nested_obj, indent + 1) def print_function_info(name, obj, indent=0): print(f\\"{\' \' * indent}- {name} (Function)\\") indent += 1 # Print nested functions if hasattr(obj, \'items\'): nested_items = {k: v for k, v in obj.items() if isinstance(v, pyclbr.Function)} if nested_items: print(f\\"{\' \' * indent}Nested Functions:\\") for nested_name, nested_obj in nested_items.items(): print(f\\"{\' \' * (indent + 1)}- {nested_name} (Function, Line {nested_obj.lineno})\\") print(\\"Classes and Functions:\\") for name, obj in module_info.items(): if isinstance(obj, pyclbr.Function): print(f\\"- {name} (Function)\\") elif isinstance(obj, pyclbr.Class): print(f\\"- {name} (Class)\\") print(\\"nHierarchy and Relationships:\\") for name, obj in module_info.items(): if isinstance(obj, pyclbr.Class): print(f\\"Class {name}:\\") print_class_info(name, obj) elif isinstance(obj, pyclbr.Function): print(f\\"Function {name}:\\") print_function_info(name, obj)"},{"question":"# Mocking a REST API Client - Advanced Usage of `unittest.mock` **Objective**: Assess students\' understanding of advanced features of the `unittest.mock` library, including `Mock`, `MagicMock`, `patch`, and configuring side effects and return values. Problem Statement You are provided with a simple REST API client implemented in Python. Your task is to write a test suite for it using the `unittest` and `unittest.mock` libraries. The focus should be on using mocks effectively to test the behavior without making actual API calls. ```python import requests class RESTClient: def __init__(self, base_url): self.base_url = base_url def get(self, endpoint): response = requests.get(f\\"{self.base_url}/{endpoint}\\") return response.json() def post(self, endpoint, data): response = requests.post(f\\"{self.base_url}/{endpoint}\\", json=data) return response.status_code, response.json() def batch_get(self, endpoints): results = [] for endpoint in endpoints: results.append(self.get(endpoint)) return results ``` Test Requirements 1. **Initialization Test**: - Write a test to ensure that the `RESTClient` initializes with the correct `base_url`. 2. **Mocking `requests.get`**: - Write a test for the `get` method that mocks `requests.get` to return a predefined JSON response. - Assert that the method returns the correct JSON data without making an actual HTTP request. 3. **Mocking `requests.post`**: - Write a test for the `post` method that mocks `requests.post` to return a predefined status code and JSON response. - Assert that the method correctly handles the response. 4. **Testing `batch_get` Method**: - Write a test for the `batch_get` method using `MagicMock` to mock multiple calls to the `get` method. Ensure that the method aggregates results correctly. 5. **Using Side Effects**: - Write a test for the `get` method that demonstrates the use of `side_effect` to simulate different responses for multiple calls. 6. **Ensuring Proper Calls**: - Write a test to confirm that `requests.get` and `requests.post` are called with the correct arguments using `assert_called_with`. 7. **Exception Handling**: - Write a test that verifies the `RESTClient.get` method handles exceptions (e.g., network errors) correctly by using `side_effect` to raise exceptions. Constraints - **Performance**: Ensure tests run quickly by mocking external calls. - **Deterministic Tests**: Mock objects should ensure consistent and repeatable test results. - **Complete Coverage**: Aim to cover all code paths including normal flows and error handling. Example Test Code Scaffold ```python import unittest from unittest.mock import patch, Mock, MagicMock import requests from mymodule import RESTClient # Assuming the RESTClient class is in mymodule.py class TestRESTClient(unittest.TestCase): @patch(\'mymodule.requests.get\') def test_get(self, mock_get): # Example: Configure mock_get to return a predefined JSON response ... @patch(\'mymodule.requests.post\') def test_post(self, mock_post): # Example: Configure mock_post to return a predefined status code and JSON response ... def test_batch_get(self): # Use MagicMock and ensure batch_get aggregates results correctly ... @patch.object(RESTClient, \'get\') def test_get_with_side_effect(self, mock_get): # Example: Use side_effect to simulate different responses ... @patch(\'mymodule.requests.get\') @patch(\'mymodule.requests.post\') def test_ensure_proper_calls(self, mock_get, mock_post): # Example: Ensure requests.get and requests.post are called correctly ... @patch(\'mymodule.requests.get\') def test_exception_handling(self, mock_get): # Example: Use side_effect to raise exceptions and test handling ... if __name__ == \'__main__\': unittest.main() ``` Submission Details - Implement the above test cases using `unittest` and `unittest.mock`. - Ensure that your code is well-commented and follows Python\'s best practices. - Submit your tests in a single file named `test_restclient.py`. **Good luck!**","solution":"import requests class RESTClient: def __init__(self, base_url): self.base_url = base_url def get(self, endpoint): response = requests.get(f\\"{self.base_url}/{endpoint}\\") return response.json() def post(self, endpoint, data): response = requests.post(f\\"{self.base_url}/{endpoint}\\", json=data) return response.status_code, response.json() def batch_get(self, endpoints): results = [] for endpoint in endpoints: results.append(self.get(endpoint)) return results"},{"question":"**Objective:** Create a custom histogram using the `seaborn` library that demonstrates your understanding of: 1. Data loading and preprocessing. 2. Creating and customizing plots. 3. Adjusting visual properties and transformations to handle overlapping elements. **Question:** Write a function `create_custom_histogram(data)` that: 1. Loads data from the seaborn `diamonds` dataset. 2. Creates a histogram of the `price` column, with bars filled by the `clarity` attribute. 3. Stacks the bars to prevent overlap. 4. Customize the histogram to: - Scale the x-axis to show the price on a logarithmic scale. - Use transparency (alpha) mapping based on the `color` attribute. - Narrow the bars appropriately for better visual distinction. **Function Signature:** ```python def create_custom_histogram(data) -> None: pass ``` **Input:** - `data: pd.DataFrame` - A pandas DataFrame representing a dataset similar to `diamonds`. **Output:** - The function does not return anything, but should display the custom histogram plot. **Example Visualization:** The outcome should visually represent an histogram of diamond prices where bars: - Are filled based on the `clarity` attribute. - Are stacked to prevent overlap. - x-axis is log-scaled. - Use transparency based on the `color` attribute. - Bars are narrower for better visual separation. **Constraints:** - You must use `seaborn.objects.Plot` and related seaborn capabilities. - Do not use any other plotting libraries like matplotlib or plotly directly for this visual task. **Hints:** - Use `so.Bars` and `so.Hist` classes for creating the histogram. - Utilize the `so.Stack` transformation for managing overlaps. - Refer to the provided examples for setting scales and visual properties effectively. Here is some starter code to help: ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_histogram(data): # Load dataset if data is None if data is None: data = load_dataset(\\"diamonds\\") # Create initial plot p = so.Plot(data, \\"price\\").scale(x=\\"log\\") # Add histogram with stacked bars p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"clarity\\", alpha=\\"color\\") # Display the plot p.show() ``` This question assesses the student\'s ability to utilize seaborn for creating customized plots with specific visual properties and transformations.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_histogram(data=None): # Load dataset if data is None if data is None: data = load_dataset(\\"diamonds\\") # Create initial plot p = so.Plot(data, \\"price\\").scale(x=\\"log\\") # Add histogram with stacked bars p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"clarity\\", alpha=\\"color\\", width=0.8) # Display the plot p.show()"},{"question":"**Question: Using Seaborn to Visualize Health Expenditure Data** You are given a dataset that contains health expenditure details for various countries over several years. Your task is to visually demonstrate how health expenditure has changed over time for each country in the dataset using the seaborn library. # Objective Create a plot that shows the percent change in health spending relative to the year 1970, for each country in the dataset. # Dataset The dataset can be loaded using seaborn\'s `load_dataset` function: ```python import seaborn as sns healthexp = sns.load_dataset(\\"healthexp\\") ``` # Requirements - Use the `so.Plot` functionality from the `seaborn.objects` module. - Normalize the spending data relative to the year 1970 using `so.Norm(where=\\"x == x.min()\\", percent=True)`. - Set the label for the y-axis to: \\"Percent change in spending from 1970 baseline\\". # Constraints - You can only use seaborn objects for plotting. No other plotting libraries (like matplotlib) are allowed. - You must use the `healthexp` dataset as provided by the seaborn library. - Ensure that your plot includes a legend identifying each country by color. # Input - There are no user inputs required. The plot should be generated automatically using the `healthexp` dataset. # Output - Render and display a plot in a Jupyter notebook cell. # Example Solution The solution should resemble the following structure (note: this is not a complete solution): ```python import seaborn.objects as so import seaborn as sns # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Create the plot with normalization ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) ```","solution":"import seaborn.objects as so import seaborn as sns # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Create and display the plot with normalization def plot_health_expenditure_change(): plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) plot.show()"},{"question":"# Question **Background**: You are tasked with creating dynamic types using the CPython API. These types should be defined with specific behaviors and properties and be linked to a Python module. Your job is to implement functions that will dynamically create these types and also provide utility functions to interact with them. **Objective**: Implement two functions: 1. `create_dynamic_type` - This function dynamically creates a new type with specified properties. 2. `is_subtype` - This function checks if a dynamically created type is a subtype of another. **Function 1**: `create_dynamic_type` *Input*: - `module (PyObject*)`: The module object where the type is defined. If `NULL`, the type will not be associated with any module. - `name (str)`: The name of the new type. - `flags (int)`: Flags to set for this type (e.g., support for garbage collection). - `bases (tuple or None)`: A tuple of base classes for this new type. If `NULL`, the new type will derive from `object`. *Output*: - Returns a new type object on success (`PyTypeObject*`) or `NULL` on failure. **Definition**: ```python def create_dynamic_type(module, name, flags, bases): # Your implementation here ``` **Function 2**: `is_subtype` *Input*: - `type_a (PyTypeObject*)`: The first type object to check. - `type_b (PyTypeObject*)`: The second type object to check against. *Output*: - Returns `True` if `type_a` is a subtype of `type_b`, `False` otherwise. **Definition**: ```python def is_subtype(type_a, type_b): # Your implementation here ``` **Examples**: ```python # Sample usage module = ... # obtain or create a module object custom_type = create_dynamic_type(module, \\"CustomType\\", some_flags, some_bases) base_type = ... # reference to another type object print(is_subtype(custom_type, base_type)) # Outputs: True or False depending on the relationship ``` **Constraints**: 1. Ensure that the implementation uses appropriate API calls like `PyType_FromModuleAndSpec`, `PyType_IsSubtype`, and others as necessary. 2. Handle errors gracefully, returning appropriate error values when operations fail. **Performance**: - The functions should be efficient and should avoid unnecessary operations. - Consider potential recursion when checking subtype relationships to ensure there are no performance bottlenecks. **Notes**: - You can assume that proper error handling and validation will be performed before calling these functions. - Use the documentation as needed to understand and use the correct API calls. This exercise tests your understanding of the CPython type creation and manipulation APIs, as well as your ability to implement solutions that involve dynamic type handling.","solution":"import ctypes import types def create_dynamic_type(module, name, flags, bases): Dynamically creates a new type with specified properties. Args: - module (types.ModuleType or None): The module where the type is defined. - name (str): The name of the new type. - flags (int): Flags to set for this type. - bases (tuple or None): A tuple of base classes for this new type. If None, the new type will derive from object. Returns: - A new type object on success or None on failure. if bases is None: bases = (object,) # Define the new type\'s attributes and behaviors type_spec = dict( __module__ = module.__name__ if module else None, __name__ = name, __flags__ = flags ) # Create the new type try: new_type = type(name, bases, type_spec) except Exception as e: return None return new_type def is_subtype(type_a, type_b): Checks if type_a is a subtype of type_b. Args: - type_a (type): The first type object to check. - type_b (type): The second type object to check against. Returns: - True if type_a is a subtype of type_b, False otherwise. return issubclass(type_a, type_b)"},{"question":"You are tasked with creating a function that generates a seaborn color palette based on the input parameters and visualizes this palette in a simple plot. The function should handle both continuous and qualitative colormaps and be able to combine them if specified. Function Signature ```python def generate_and_plot_palette(palette_name1: str, n_colors1: int, as_cmap1: bool, palette_name2: str = None, n_colors2: int = 0, as_cmap2: bool = False): Generate a seaborn color palette based on input parameters and visualize it. Parameters: - palette_name1 (str): The name of the first palette or colormap. - n_colors1 (int): The number of colors to sample from the first palette. - as_cmap1 (bool): If True, return the continuous colormap for the first palette. - palette_name2 (str): The name of the second palette or colormap (optional). - n_colors2 (int): The number of colors to sample from the second palette (required if palette_name2 is provided). - as_cmap2 (bool): If True, return the continuous colormap for the second palette (required if palette_name2 is provided). Returns: - None ``` Input - A required `palette_name1` (string) and `n_colors1` (int) which specifies the first seaborn palette or colormap and the number of colors to sample from it. - A required `as_cmap1` (bool) which specifies if the first colormap should be continuous. - An optional `palette_name2` (string) and `n_colors2` (int) which specify the second seaborn palette or colormap and the number of colors to sample from it. This will only be used if `palette_name2` is provided. - An optional `as_cmap2` (bool) which specifies if the second colormap should be continuous. This will only be used if `palette_name2` is provided. Output - The function should visualize the generated palette(s) using a simple plot (e.g., a barplot with colored bars). No return value is required. Constraints - If `n_colors1` or `n_colors2` is too large for the qualitative colormap, the function should handle this gracefully. - The visualization should clearly show the colors generated from the palettes. Example Here\'s an example usage of the function: ```python generate_and_plot_palette(\\"viridis\\", 8, False) generate_and_plot_palette(\\"Set2\\", 12, False, \\"viridis\\", 5, True) ``` The first call generates and visualizes a palette with 8 discrete colors from the \\"viridis\\" colormap. The second call combines a qualitative palette \\"Set2\\" with up to 12 colors (falling back if fewer are available) and a continuous \\"viridis\\" colormap and visualizes them together. Notes - You may use matplotlib along with seaborn for creating the plots. - Ensure the function handles edge cases, such as invalid palette names, gracefully and provides informative error messages.","solution":"import seaborn as sns import matplotlib.pyplot as plt import matplotlib.colors as mcolors def generate_and_plot_palette(palette_name1: str, n_colors1: int, as_cmap1: bool, palette_name2: str = None, n_colors2: int = 0, as_cmap2: bool = False): Generate a seaborn color palette based on input parameters and visualize it. Parameters: - palette_name1 (str): The name of the first palette or colormap. - n_colors1 (int): The number of colors to sample from the first palette. - as_cmap1 (bool): If True, return the continuous colormap for the first palette. - palette_name2 (str): The name of the second palette or colormap (optional). - n_colors2 (int): The number of colors to sample from the second palette (required if palette_name2 is provided). - as_cmap2 (bool): If True, return the continuous colormap for the second palette (required if palette_name2 is provided). Returns: - None # Generate palette1 if as_cmap1: palette1 = sns.color_palette(palette_name1, as_cmap=True) colors1 = [palette1(i) for i in range(n_colors1)] else: palette1 = sns.color_palette(palette_name1, n_colors1) colors1 = palette1 # Generate palette2 if specified colors2 = [] if palette_name2: if as_cmap2: palette2 = sns.color_palette(palette_name2, as_cmap=True) colors2 = [palette2(i) for i in range(n_colors2)] else: palette2 = sns.color_palette(palette_name2, n_colors2) colors2 = palette2 # Combine palettes combined_colors = colors1 + colors2 # Plot the color palettes fig, ax = plt.subplots(figsize=(len(combined_colors), 1)) for idx, color in enumerate(combined_colors): ax.add_patch(plt.Rectangle((idx, 0), 1, 1, edgecolor=\'none\', facecolor=color)) ax.set_xlim(0, len(combined_colors)) ax.set_ylim(0, 1) ax.axis(\'off\') plt.show()"},{"question":"You are tasked with creating a Python program that will allow users to select a color using a color chooser dialog and then display the selected color in a tkinter window. # Task Write a function `color_chooser_demo()` that does the following: 1. Opens a Tkinter color chooser dialog to allow the user to pick a color. 2. Creates a Tkinter window that changes its background to the color selected by the user. If no color is chosen, the background should default to white. # Specifications - Use the `tkinter.colorchooser.askcolor()` function to open the color chooser dialog. - Ensure the selected color is used as the background color of the Tkinter window. - If the user cancels the color selection, default the window background color to white. - The window should have a title \\"Color Chooser Demo\\". # Example ```python import tkinter as tk from tkinter import colorchooser def color_chooser_demo(): # Your implementation here color_chooser_demo() ``` # Constraints - You must use the `askcolor` function from the `tkinter.colorchooser` module. - The function should handle both RGB tuples and hex color strings returned by the `askcolor` function appropriately. - The program should be user-friendly and respond correctly to user actions. # Notes If `askcolor` returns a tuple with the selected color in RGB and its corresponding hex value, use the hex value to set the background color.","solution":"import tkinter as tk from tkinter import colorchooser def color_chooser_demo(): root = tk.Tk() root.title(\'Color Chooser Demo\') # Ask the user to select a color color = colorchooser.askcolor() # Check if the user selected a color or canceled if color[1]: # The hex color value is in color[1] chosen_color = color[1] else: chosen_color = \'#ffffff\' # Default to white if no color is chosen # Set the background color of the window root.configure(bg=chosen_color) root.mainloop()"},{"question":"**Question** You are given a dataset `\\"flights\\"` which contains monthly passenger numbers for an airline over the years. Your task is to create a comprehensive visualization using seaborn to illustrate the variation in the number of passengers over time. You must implement a function `visualize_flight_data(df: pd.DataFrame) -> None` that performs the following: 1. Load the `flights` dataset if not already provided. 2. Create a line plot to show the trend of the number of passengers over time, aggregated by year. 3. Use hue semantics to differentiate between different years. 4. Utilize faceting to create separate plots for each month to clearly show seasonal trends. 5. Customize the appearance of the line plot with an appropriate color palette, markers, and line styles to ensure clarity and readability. 6. Include meaningful titles and labels for the axes and the overall plot. **Input:** - A Pandas DataFrame `df` containing the flight data. **Output:** - A seaborn plot displayed inline (if running in Jupyter) or through an appropriate matplotlib backend. **Constraints:** - The input DataFrame will have at least the columns `year`, `month`, and `passengers`. The expected visualization will help in understanding the overall trend as well as seasonal variations in the airline\'s passenger numbers over the years. ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flight_data(df: pd.DataFrame) -> None: # Your implementation here pass # Example usage (uncomment for testing) # flights = sns.load_dataset(\\"flights\\") # visualize_flight_data(flights) ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flight_data(df: pd.DataFrame) -> None: Visualize the variation in the number of passengers over time. Parameters: df : pd.DataFrame The dataset containing columns \'year\', \'month\', and \'passengers\'. Returns: None: Displays the seaborn plot. # Create a line plot with hue semantics to differentiate between different years g = sns.FacetGrid(df, col=\'month\', col_wrap=4, height=3, aspect=1.5) g.map(sns.lineplot, \'year\', \'passengers\', marker=\\"o\\") # Customize the appearance g.add_legend() g.set_axis_labels(\\"Year\\", \\"Number of Passengers\\") g.set_titles(\\"{col_name} Passengers\\") # Set overall plot title plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Monthly Airline Passenger Numbers Over Years\') plt.show() # Example usage (uncomment for testing) # flights = sns.load_dataset(\\"flights\\") # visualize_flight_data(flights)"},{"question":"**Advanced Turtle Graphics Challenge** **Objective**: Write a Python function to draw a user-interactive design using the `turtle` module, incorporating movements, shapes, colors, and events. # Function Signature ```python def draw_interactive_design(): pass ``` # Requirements: 1. **Main Function**: - Define a function `draw_interactive_design` that sets up the turtle screen and implements the interactive drawing process. 2. **Setup**: - The turtle window should set the background color to a pleasant soft color (e.g., light blue). - The turtle\'s speed should be set to the fastest for quick drawing. 3. **Initial Design**: - Draw a simple geometric shape such as a square at the center of the screen. - The turtle should draw the shape with a specific border color and fill the shape with another color. 4. **Turtle Movement & Events**: - Implement event handling where the turtle moves to the position where the user clicks on the screen. - The turtle should erase its previous drawing before moving to a new location. 5. **Color Change on Right-Click**: - Bind the right-click event to change the turtle’s drawing color randomly. 6. **Drag Interaction**: - Enable dragging functionality to allow the user to drag the turtle around the screen while continuously drawing. # Constraints: - Use the `turtle` module and its functionalities as outlined above. - Ensure smooth user interaction and response to events. - Handle any exceptions or user inputs gracefully. # Example Usage: ```python draw_interactive_design() ``` Upon running the above function, the turtle graphics window should open, draw an initial geometric shape, and then respond to user actions such as left-click, right-click, and drag events. **Note**: Do not forget to call `turtle.done()` at the end of your `draw_interactive_design` function to keep the window open until it’s closed manually. # Hints: - Use the `onclick()` method to bind functions to mouse click events. - Use the `ondrag()` method for drag interactions. - You may use the `random` module to change colors randomly on right-click. - Remember to call `Screen().setup()` for window setup. You can refer to the turtle module documentation provided to understand more about the methods and their usage.","solution":"import turtle import random def draw_interactive_design(): screen = turtle.Screen() screen.bgcolor(\\"light blue\\") screen.title(\\"Interactive Turtle Design\\") pen = turtle.Turtle() pen.speed(0) def draw_square(): pen.color(\\"black\\", \\"green\\") pen.begin_fill() for _ in range(4): pen.forward(100) pen.right(90) pen.end_fill() def clear_and_move(x, y): pen.clear() pen.penup() pen.goto(x, y) pen.pendown() draw_square() def change_color(x, y): random_color = \\"#%06x\\" % random.randint(0, 0xFFFFFF) pen.color(random_color) def drag(x, y): pen.ondrag(None) pen.goto(x, y) pen.ondrag(drag) draw_square() screen.onclick(clear_and_move) screen.onclick(change_color, 3) pen.ondrag(drag) turtle.done()"},{"question":"Coding Assessment Question In this task, you are required to implement a function that processes version information of CPython and returns the version string. The version information will be provided as a dictionary with the following keys: - \\"PY_MAJOR_VERSION\\" - \\"PY_MINOR_VERSION\\" - \\"PY_MICRO_VERSION\\" - \\"PY_RELEASE_LEVEL\\" - \\"PY_RELEASE_SERIAL\\" Here are the possible values for \\"PY_RELEASE_LEVEL\\": - \\"0xA\\" for alpha - \\"0xB\\" for beta - \\"0xC\\" for release candidate - \\"0xF\\" for final You are expected to: 1. Combine these parts to form a full version string, formatted as \\"major.minor.micro\\" followed by the release level and serial if they are not representing a final release. 2. Return the complete version string. 3. Implement a function `get_version_string(version_info: dict) -> str`. The release level should be represented by \'a\', \'b\', or \'rc\' for alpha, beta, or release candidate, followed by the serial. Final releases will not include the release level and serial in the string. # Function Signature ```python def get_version_string(version_info: dict) -> str: pass ``` # Input - `version_info`: A dictionary containing the version information with keys: - \\"PY_MAJOR_VERSION\\": (int) - \\"PY_MINOR_VERSION\\": (int) - \\"PY_MICRO_VERSION\\": (int) - \\"PY_RELEASE_LEVEL\\": (str) - \\"PY_RELEASE_SERIAL\\": (int) # Output - Returns a string representing the full version information. # Examples ```python # Example 1 version_info = { \\"PY_MAJOR_VERSION\\": 3, \\"PY_MINOR_VERSION\\": 4, \\"PY_MICRO_VERSION\\": 1, \\"PY_RELEASE_LEVEL\\": \\"0xA\\", # Alpha \\"PY_RELEASE_SERIAL\\": 2 } assert get_version_string(version_info) == \\"3.4.1a2\\" # Example 2 version_info = { \\"PY_MAJOR_VERSION\\": 3, \\"PY_MINOR_VERSION\\": 10, \\"PY_MICRO_VERSION\\": 0, \\"PY_RELEASE_LEVEL\\": \\"0xF\\", # Final \\"PY_RELEASE_SERIAL\\": 0 } assert get_version_string(version_info) == \\"3.10.0\\" # Example 3 version_info = { \\"PY_MAJOR_VERSION\\": 3, \\"PY_MINOR_VERSION\\": 6, \\"PY_MICRO_VERSION\\": 5, \\"PY_RELEASE_LEVEL\\": \\"0xB\\", # Beta \\"PY_RELEASE_SERIAL\\": 4 } assert get_version_string(version_info) == \\"3.6.5b4\\" ``` Constraints: - The values of `PY_MAJOR_VERSION`, `PY_MINOR_VERSION`, `PY_MICRO_VERSION` are all non-negative integers. - The `PY_RELEASE_LEVEL` will always be one of the provided hex values as strings (`\\"0xA\\"`, `\\"0xB\\"`, `\\"0xC\\"`, `\\"0xF\\"`). - The `PY_RELEASE_SERIAL` is a non-negative integer. # Notes The function should properly handle converting the release level hex value into the correct letter for the version string.","solution":"def get_version_string(version_info: dict) -> str: Constructs a version string from version_info dictionary. Args: - version_info (dict): Dictionary containing version keys: \\"PY_MAJOR_VERSION\\" (int) \\"PY_MINOR_VERSION\\" (int) \\"PY_MICRO_VERSION\\" (int) \\"PY_RELEASE_LEVEL\\" (str) \\"PY_RELEASE_SERIAL\\" (int) Returns: - str: The constructed version string. major = version_info[\\"PY_MAJOR_VERSION\\"] minor = version_info[\\"PY_MINOR_VERSION\\"] micro = version_info[\\"PY_MICRO_VERSION\\"] release_level = version_info[\\"PY_RELEASE_LEVEL\\"] release_serial = version_info[\\"PY_RELEASE_SERIAL\\"] version_str = f\\"{major}.{minor}.{micro}\\" level_map = { \\"0xA\\": \\"a\\", \\"0xB\\": \\"b\\", \\"0xC\\": \\"rc\\", \\"0xF\\": \\"\\" } if release_level != \\"0xF\\": version_str += level_map[release_level] + str(release_serial) return version_str"},{"question":"# Coding Challenge: Implementing a Custom Multiclass Model with One-vs-Rest Strategy **Problem Statement:** You are required to implement a custom multiclass classification model using the One-vs-Rest (OvR) strategy with scikit-learn. The model should be able to handle an input dataset and accurately classify the samples into one of the multiple classes. To achieve this, you will use the OneVsRestClassifier meta-estimator from scikit-learn along with a base estimator of your choice provided by scikit-learn that supports multiclass classification. You should preprocess the data, train the model, make predictions, and evaluate the model\'s performance on a given test set. **Input:** - A training dataset `X_train` (n_samples, n_features) and corresponding labels `y_train` (n_samples,) with more than two classes. - A test dataset `X_test` (n_samples, n_features) for making predictions. **Output:** - Predicted labels `y_pred` (n_samples,) for the test dataset. - Accuracy score of the predictions. **Constraints:** 1. Use the Iris dataset from the `sklearn.datasets` module for this task. 2. You are free to use any built-in classifier from scikit-learn that supports multiclass classification (e.g., `DecisionTreeClassifier`, `LogisticRegression`, `KNeighborsClassifier`, etc.) as the base estimator for `OneVsRestClassifier`. 3. The model should handle pre-processing tasks such as splitting the dataset into training and testing sets, scaling the features, and encoding the labels if necessary. **Performance Requirements:** - Aim for an accuracy score of at least 90% on the test set. **Steps:** 1. Load the Iris dataset. 2. Split the dataset into training and testing sets. 3. Implement the `OneVsRestClassifier` using a base estimator of your choice. 4. Train the model on the training data. 5. Make predictions on the test data using the trained model. 6. Evaluate the model\'s performance and print out the accuracy score. **Skeleton Code:** ```python import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.multiclass import OneVsRestClassifier from sklearn.metrics import accuracy_score from sklearn.linear_model import LogisticRegression # Example base estimator # Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement the One-vs-Rest classifier base_estimator = LogisticRegression(random_state=42) # Replace with your choice of estimator ovr_classifier = OneVsRestClassifier(base_estimator) # Train the model ovr_classifier.fit(X_train, y_train) # Make predictions on the test set y_pred = ovr_classifier.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") ``` **Your task:** - Understand the provided skeleton code and complete any missing parts. - Feel free to change the base estimator and any relevant parameters to achieve the target accuracy score of 90% or higher. - Submit your final code implementation and the achieved accuracy score.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.multiclass import OneVsRestClassifier from sklearn.metrics import accuracy_score from sklearn.linear_model import LogisticRegression # Base estimator def custom_ovr_classifier(): # Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement the One-vs-Rest classifier base_estimator = LogisticRegression(random_state=42, max_iter=200) # Modify parameters if needed ovr_classifier = OneVsRestClassifier(base_estimator) # Train the model ovr_classifier.fit(X_train, y_train) # Make predictions on the test set y_pred = ovr_classifier.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) return y_pred, accuracy # Executing the function and printing the accuracy. predicted_labels, model_accuracy = custom_ovr_classifier() print(f\\"Accuracy: {model_accuracy}\\")"},{"question":"<|Analysis Begin|> The provided documentation gives us a high-level overview of various timer components in PyTorch\'s distributed elastic framework. This includes `LocalTimerServer`, `LocalTimerClient`, `FileTimerServer`, and `FileTimerClient`, which handle timers through mechanisms like multiprocess queues or named pipes. Additionally, it introduces the concept of creating custom timer servers and clients, which involves extending specific base classes and handling communication via `TimerRequest` objects. Key points: 1. `torch.distributed.elastic.timer.configure` and `torch.distributed.elastic.timer.expires` are two functions provided for configuring and working with timers. 2. `LocalTimerServer` and `LocalTimerClient` use `multiprocess.Queue` for their implementation. 3. `FileTimerServer` and `FileTimerClient` use named pipes for their implementation. 4. `TimerServer` and `TimerClient` can be extended to create custom timer implementations. 5. `TimerRequest` objects facilitate communication between the server and client. Given this understanding, we can design a challenging question around implementing a custom timer server and client, ensuring that students must demonstrate comprehension of inter-process communication, class extension, and PyTorch\'s distributed elastic framework. <|Analysis End|> <|Question Begin|> # Custom Timer Server and Client Implementation Objective Implement a custom timer server and client in PyTorch\'s distributed elastic framework. You will need to extend the `TimerServer` and `TimerClient` classes and use `TimerRequest` objects for communication. Input and Output Formats You are required to write two classes: `CustomTimerServer` and `CustomTimerClient`. 1. **CustomTimerServer** - **Methods**: - `__init__(self)`: Initializes the server. - `start(self)`: Starts the server to listen for timer requests. - `stop(self)`: Stops the server. - **Attributes**: - Should handle incoming `TimerRequest` objects and manage timers appropriately. 2. **CustomTimerClient** - **Methods**: - `__init__(self, server)`: Initializes the client with a reference to the server. - `send_request(self, request)`: Sends a `TimerRequest` to the server. - `check_expired(self)`: Checks if the timers have expired. - **Attributes**: - Should communicate with the server using `TimerRequest` objects and handle responses. Constraints - Implement inter-process communication using a mechanism of your choice (e.g., sockets, shared memory, etc.). - Ensure thread-safety and correct handling of timer expiration events. - Performance should be efficient for managing multiple concurrent timers. Example Usage Here\'s an example of how your implementation might be used: ```python from datetime import timedelta import time # Initialize the server server = CustomTimerServer() server.start() # Initialize the client with a reference to the server client = CustomTimerClient(server) # Create a timer request that expires after 10 seconds request = TimerRequest(duration=timedelta(seconds=10)) client.send_request(request) # Wait and check if the timer has expired time.sleep(10) expired = client.check_expired() print(expired) # Should print True if the timer expired # Stop the server server.stop() ``` # Note: - Document your code and provide comments for clarity. - Handle edge cases such as sending multiple requests and ensuring all timers are tracked and expired correctly. Good luck!","solution":"import threading import time from datetime import datetime, timedelta from queue import Queue, Empty class TimerRequest: def __init__(self, duration): self.start_time = datetime.now() self.duration = duration def has_expired(self): return datetime.now() >= self.start_time + self.duration class CustomTimerServer: def __init__(self): self.queue = Queue() self.timers = {} self.running = False self.lock = threading.Lock() def start(self): self.running = True self.thread = threading.Thread(target=self._run) self.thread.start() def stop(self): self.running = False self.thread.join() def _run(self): while self.running: try: timer_id, request = self.queue.get(timeout=1) with self.lock: self.timers[timer_id] = request except Empty: pass def add_request(self, timer_id, request): self.queue.put((timer_id, request)) def check_expired(self, timer_id): with self.lock: if timer_id in self.timers: request = self.timers[timer_id] return request.has_expired() return False class CustomTimerClient: def __init__(self, server): self.server = server self.current_id = 0 def send_request(self, request): timer_id = self.current_id self.current_id += 1 self.server.add_request(timer_id, request) return timer_id def check_expired(self, timer_id): return self.server.check_expired(timer_id)"},{"question":"**Objective**: To evaluate understanding of tuple objects and struct sequence objects in Python\'s C API. **Problem Statement**: Suppose you are working on a Python C extension that involves manipulating tuples and struct sequences. In this task, you are required to implement two key functionalities: 1. **Create and Manage Tuples**: - Write a function `create_and_modify_tuple(Py_ssize_t len, PyObject* values[], Py_ssize_t change_index, PyObject* new_value)` that: * Creates a tuple of length `len` with the elements provided in the `values` array. * Sets the element at `change_index` to `new_value`. * Returns the modified tuple. 2. **Create and Utilize Struct Sequence**: - Write a function `create_struct_sequence(const char* name, int num_fields, PyObject* field_values[])` that: * Creates a struct sequence type named `name` with `num_fields`. * Initializes the struct sequence with values provided in the `field_values` array. * Returns the created struct sequence instance. **Expected Input and Output Formats**: 1. **`create_and_modify_tuple`**: * **Input**: - `len`: `Py_ssize_t` - the length of the tuple. - `values`: `PyObject*[]` - the array of initial values. - `change_index`: `Py_ssize_t` - the index at which the new value should be set. - `new_value`: `PyObject*` - the new value to set at `change_index`. * **Output**: `PyObject*` - the modified tuple. 2. **`create_struct_sequence`**: * **Input**: - `name`: `const char*` - the name of the struct sequence. - `num_fields`: `int` - the number of fields in the struct sequence. - `field_values`: `PyObject*[]` - the array of field values. * **Output**: `PyObject*` - the created struct sequence instance. **Constraints**: * You must handle possible errors appropriately, including memory allocation failures and invalid indices. * Ensure your implementation follows the reference counting rules of the Python C API. * The functions should follow the conventions and behaviors outlined in the provided documentation. **Performance Requirements**: * The functions should ensure efficient creation and manipulation of tuple and struct sequence objects. * Handle large tuples and struct sequences efficiently without causing memory leaks. ```c // Function declarations PyObject* create_and_modify_tuple(Py_ssize_t len, PyObject* values[], Py_ssize_t change_index, PyObject* new_value); PyObject* create_struct_sequence(const char* name, int num_fields, PyObject* field_values[]); ``` Implement these functions considering the constraints, input/output formats, and performance requirements.","solution":"# Assuming \'values\' to be a list of Python objects and \'new_value\' to be a Python object def create_and_modify_tuple(len, values, change_index, new_value): Create a tuple of given length with initial values and modify an element at a given index. :param len: Length of the tuple :param values: List of initial values :param change_index: Index to be changed :param new_value: New value for the index :return: Modified tuple if not (0 <= change_index < len): raise IndexError(\\"change_index is out of the valid range\\") # Create the tuple tup = tuple(values) # Convert the tuple to a list for modification tup_list = list(tup) # Modify the specified index tup_list[change_index] = new_value # Return the modified tuple return tuple(tup_list) from collections import namedtuple def create_struct_sequence(name, num_fields, field_values): Create a struct sequence object with a specified number of fields and initial values. :param name: Name of the struct sequence :param num_fields: Number of fields in the struct sequence :param field_values: List of initial values :return: Created struct sequence instance if len(field_values) != num_fields: raise ValueError(\\"The length of field_values must match the num_fields\\") # Define a namedtuple with the given name and fields StructSequence = namedtuple(name, [f\'field_{i}\' for i in range(num_fields)]) # Create an instance of the namedtuple with the provided field values struct_instance = StructSequence(*field_values) return struct_instance"},{"question":"Objective: Write a Python program that: 1. Traverses a given directory tree. 2. Collects and displays file statistics (e.g., size, creation date) for each file. 3. Handles environment variables to influence the program\'s behavior. 4. Reports process memory usage and load average. Requirements: 1. **Traversal**: - Use `os.walk()` to traverse the directory tree. - Skip directories with the name \\"SKIP_DIR\\" (if the environment variable `IGNORE_DIR` is set to \\"true\\"). 2. **File Statistics**: - For each file, display: - Full path - Size (in bytes) - Creation date (formatted as YYYY-MM-DD) - Use `os.stat()` to gather file statistics. 3. **Environment Variables**: - Read the environment variable `FILE_EXTENSION`. Only collect statistics for files matching the provided extension. If not set, collect statistics for all files. - Use `os.getenv()` to read the environment variables. 4. **System Information**: - Report the load average using `os.getloadavg()`. - Report the current process\'s memory usage using `os.getrusage()` if available (otherwise, display a suitable message). Input: - A root directory path. Output: - Console output for all file statistics and system information. Example: ```python # Example usage python file_stats.py /path/to/directory # Example output File: /path/to/directory/file1.txt Size: 12345 bytes Created: 2023-10-10 ... Load average: (0.0, 0.01, 0.05) Memory usage: 3456 KB ``` Constraints: - The program must handle errors gracefully, including file access errors. - Ensure that skipping directories and filtering by extension are done efficiently. - Provide informative comments and adhere to Python coding standards. Hints: - Use `os.path.join()` to manage file paths. - Use `time.strftime()` to format the creation date. - Consider using the `subprocess` module to extend the program if needed.","solution":"import os import time from datetime import datetime def collect_file_statistics(root_dir): Traverses the given directory tree and collects file statistics. ignore_dir = os.getenv(\'IGNORE_DIR\', \'false\').lower() == \'true\' file_extension = os.getenv(\\"FILE_EXTENSION\\", \\"\\") for dirpath, dirnames, filenames in os.walk(root_dir): if ignore_dir and \'SKIP_DIR\' in dirnames: dirnames.remove(\'SKIP_DIR\') for filename in filenames: if file_extension and not filename.endswith(file_extension): continue file_path = os.path.join(dirpath, filename) try: file_stat = os.stat(file_path) file_size = file_stat.st_size creation_time = time.strftime(\'%Y-%m-%d\', time.localtime(file_stat.st_ctime)) print(f\\"File: {file_path}nSize: {file_size} bytesnCreated: {creation_time}n\\") except Exception as e: print(f\\"Error accessing file {file_path}: {e}\\") def report_system_information(): Reports system load average and current process memory usage. try: load_avg = os.getloadavg() print(f\\"Load average: {load_avg}\\") except Exception as e: print(f\\"Could not retrieve load average: {e}\\") try: import resource memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss print(f\\"Memory usage: {memory_usage} KB\\") except ImportError: print(\\"Resource module not available to retrieve memory usage.\\") except Exception as e: print(f\\"Could not retrieve memory usage: {e}\\") def main(): import sys if len(sys.argv) <= 1: print(\\"Please provide a directory to traverse.\\") return root_dir = sys.argv[1] if not os.path.isdir(root_dir): print(\\"The provided path is not a directory or does not exist.\\") return collect_file_statistics(root_dir) report_system_information() if __name__ == \\"__main__\\": main()"},{"question":"Advanced Line Plotting with Seaborn Objective: Create a detailed and informative line plot using Seaborn\'s `objects` module to demonstrate your understanding of its advanced plotting capabilities. Datasets: You will use the following datasets provided by Seaborn: - `dowjones` - `fmri` Tasks: 1. **Load the `dowjones` dataset**. - Create a simple line plot with `Date` on the x-axis and `Price` on the y-axis. 2. **Change the orientation of the `dowjones` plot**. - Create a line plot with `Price` on the x-axis and `Date` on the y-axis. 3. **Load the `fmri` dataset**. - Filter the dataset for the region `parietal` and the event `stim`. - Create a line plot for the filtered data with `timepoint` on the x-axis and `signal` on the y-axis. Group the lines by `subject`. 4. **Create a combined plot**: - Using the `fmri` dataset, generate a line plot with `timepoint` on the x-axis and `signal` on the y-axis. - Map `color` to `region` and `linestyle` to `event`. - Add an error band around the lines to show the variability. Group this by `event`. - Add markers to the lines indicating where data points were sampled. Requirements: - Your plots should be well-labeled, with appropriate axis labels and titles. - Follow best practices for data visualization to ensure plots are informative and easy to interpret. Input and Output Format: - There are no specific input constraints; you will be working with the provided datasets. - Output should be the generated plots. Ensure that the plots are displayed in the notebook. Example Code: Here is an example starter code to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset # Task 1: Simple line plot dowjones = load_dataset(\\"dowjones\\") so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()).show() # Task 2: Orientation change so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line(), orient=\\"y\\").show() # Task 3: Filtered grouped line plot fmri = load_dataset(\\"fmri\\") fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\").add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\").show() # Task 4: Combined detailed plot p = so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") (p.add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) .show() ) ``` Make sure to customize and enhance the plots to fully demonstrate your understanding of the Seaborn `objects` module.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_dowjones_basic(): # Load the dowjones dataset dowjones = load_dataset(\\"dowjones\\") # Task 1: Simple line plot plot = so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\").add(so.Line()) plot.show() return plot def plot_dowjones_oriented(): # Load the dowjones dataset dowjones = load_dataset(\\"dowjones\\") # Task 2: Orientation change plot = so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line()) plot.show() return plot def plot_fmri_filtered(): # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Task 3: Filtered grouped line plot fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") plot = so.Plot(fmri_filtered, x=\\"timepoint\\", y=\\"signal\\").add(so.Line(), group=\\"subject\\") plot.show() return plot def plot_fmri_combined(): # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Task 4: Combined detailed plot plot = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line()) .add(so.Band(), group=\\"event\\") .add(so.Line(marker=\\"o\\"), group=\\"subject\\") ) plot.show() return plot"},{"question":"# Python Coding Assessment Question **Objective:** Implement a custom module finder and loader to demonstrate an understanding of Python\'s import system and the `importlib` module. **Problem Statement:** You are required to create a custom module finder and loader that can import modules located in a specific network directory (simulated using a dictionary). This custom finder and loader should be able to: 1. Locate modules stored in a mock network directory. 2. Import and execute the module\'s content. 3. Cache the imported modules to avoid redundant network calls. **Details:** 1. **Mock Network Directory Structure:** ```python network_modules = { \'network_module1\': \'print(\\"This is network module 1\\")\', \'network_module2\': \'print(\\"This is network module 2\\")\', } ``` 2. **Task Requirements:** - Create a custom finder that searches for modules in the `network_modules` dictionary. - Create a custom loader that loads the module code from the dictionary and executes it. - Register the custom finder with `sys.meta_path` to be able to use it in normal import statements. - Demonstrate importing and using `network_module1` and `network_module2`. **Function Signatures:** ```python import sys import importlib.abc import types from importlib.machinery import ModuleSpec # Step 1: Custom Finder class NetworkFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): # Implement this method to search for the module in network_modules pass # Step 2: Custom Loader class NetworkLoader(importlib.abc.Loader): def create_module(self, spec): # Optionally create the module object pass def exec_module(self, module): # Implement this method to execute the module code pass # Step 3: Register Custom Finder with sys.meta_path # Implement the registration of NetworkFinder # Step 4: Demonstrate importing and using the custom network modules # Implement the demonstration part to show importing network_module1 and network_module2 ``` **Constraints:** - Assume `network_modules` dictionary is provided and only contains key-value pairs where the key is the module name and the value is the module code. - You need to handle cases where a module is not found in the `network_modules` dictionary by raising `ModuleNotFoundError`. - Ensure that the custom module finder and loader work seamlessly with Python\'s import system. - Additionally, ensure that `sys.modules` is utilized to cache modules to avoid redundant imports. **Expected Output:** When the `network_module1` and `network_module2` modules are imported and run, the respective print statements should be executed, demonstrating that the custom finder and loader are working correctly.","solution":"import sys import importlib.abc import types from importlib.machinery import ModuleSpec # Mock Network Directory Structure network_modules = { \'network_module1\': \'print(\\"This is network module 1\\")\', \'network_module2\': \'print(\\"This is network module 2\\")\', } # Step 1: Custom Finder class NetworkFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname in network_modules: return ModuleSpec(fullname, NetworkLoader()) return None # Step 2: Custom Loader class NetworkLoader(importlib.abc.Loader): def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = network_modules[module.__name__] exec(code, module.__dict__) # Step 3: Register Custom Finder with sys.meta_path sys.meta_path.insert(0, NetworkFinder()) # Step 4: Demonstrate importing and using the custom network modules # This will be tested in the unit tests below"},{"question":"# Question You are given a dataset of monthly airline passenger data over a period of 10 years. Your task is to generate a variety of informative visualizations using Seaborn to gain insights into the data. Each visualization should provide a different perspective of the data, leveraging Seaborn\'s capabilities. Dataset The dataset is preloaded and can be accessed using: ```python import seaborn as sns flights = sns.load_dataset(\\"flights\\") ``` The dataset contains the following columns: - `year`: The year (1949-1960). - `month`: The month (January to December). - `passengers`: The number of passengers. Tasks 1. **Monthly Trends over Years** - Create a line plot to visualize the number of passengers each year for the month of May. - **Input**: None. - **Output**: A line plot showing the number of passengers each year for the month of May. 2. **Yearly Trends by Month** - Pivot the dataframe to a wide-form representation and create a single figure containing multiple subplots. Each subplot should represent the monthly trend of passengers for a different year. - **Input**: None. - **Output**: A single figure with 12 subplots (one for each month), showing the yearly trend of passengers. 3. **Monthly Comparison Across All Years** - Create a line plot to compare the monthly passenger data across all years. Each line should represent a different month. Use appropriate legends and labels. - **Input**: None. - **Output**: A line plot comparing monthly data across all years, with each month represented by a different line. 4. **Confidence Intervals for Trend Analysis** - Plot a line graph aggregating the data over repeated years to show the mean number of passengers and the 95% confidence interval. Use the confidence interval bands to illustrate the variability. - **Input**: None. - **Output**: A line plot showing the aggregated mean number of passengers with 95% confidence interval bands. 5. **Customized Line Plot** - Create a line plot where the line style and markers distinguish between different months and show error bars instead of bands, with the error bars extending to two standard error widths. - **Input**: None. - **Output**: A customized line plot with different line styles and markers for each month, showing error bars with two standard error widths. Constraints - Use the Seaborn library for all visualizations. - Ensure the visualizations are clear and appropriately labeled, with legends where necessary. - Handle any missing or inconsistent data within the dataset as you see fit. Example Code ```python import seaborn as sns import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # 1. Monthly Trends over Years may_flights = flights.query(\\"month == \'May\'\\") sns.lineplot(data=may_flights, x=\\"year\\", y=\\"passengers\\") plt.title(\'Number of Passengers in May Over the Years\') plt.show() # 2. Yearly Trends by Month flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") fig, axes = plt.subplots(3, 4, figsize=(16, 12)) months = flights_wide.columns for i, month in enumerate(months): sns.lineplot(ax=axes[i//4, i%4], data=flights_wide, x=flights_wide.index, y=month) axes[i//4, i%4].set_title(month) plt.tight_layout() plt.show() # 3. Monthly Comparison Across All Years sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plt.title(\'Monthly Passenger Data Across All Years\') plt.show() # 4. Confidence Intervals for Trend Analysis sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", ci=95) plt.title(\'Aggregated Mean Number of Passengers with 95% Confidence Interval\') plt.show() # 5. Customized Line Plot sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", style=\\"month\\", err_style=\\"bars\\", errorbar=(\\"se\\", 2), markers=True, dashes=False) plt.title(\'Monthly Trends with Error Bars\') plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the flights dataset flights = sns.load_dataset(\\"flights\\") def plot_monthly_trends_may(): may_flights = flights.query(\\"month == \'May\'\\") plot = sns.lineplot(data=may_flights, x=\\"year\\", y=\\"passengers\\") plot.set_title(\'Number of Passengers in May Over the Years\') plt.show() def plot_yearly_trends_by_month(): flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") fig, axes = plt.subplots(3, 4, figsize=(16, 12)) months = flights_wide.columns for i, month in enumerate(months): sns.lineplot(ax=axes[i//4, i%4], data=flights_wide, x=flights_wide.index, y=month) axes[i//4, i%4].set_title(month) plt.tight_layout() plt.show() def plot_monthly_comparison_all_years(): plot = sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plot.set_title(\'Monthly Passenger Data Across All Years\') plt.show() def plot_confidence_intervals(): plot = sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", ci=95) plot.set_title(\'Aggregated Mean Number of Passengers with 95% Confidence Interval\') plt.show() def plot_customized_line_plot(): plot = sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", style=\\"month\\", err_style=\\"bars\\", errorbar=(\\"se\\", 2), markers=True, dashes=False) plot.set_title(\'Monthly Trends with Error Bars\') plt.show()"},{"question":"# Custom Shell Command Interpreter Objective You are tasked with creating a simple command-line interpreter by subclassing Python’s `cmd.Cmd` class. This interpreter will perform basic arithmetic operations and include a help system for each command. Requirements 1. The command-line interpreter must have the following commands: - `add <number1> <number2>` : Adds two numbers and outputs the result. - `subtract <number1> <number2>` : Subtracts the second number from the first and outputs the result. - `multiply <number1> <number2>` : Multiplies two numbers and outputs the result. - `divide <number1> <number2>` : Divides the first number by the second and outputs the result. - `history` : Displays a list of all the commands entered during the session. - `repeat` : Re-runs the last non-empty command entered. - `exit` : Exits the command interpreter. 2. Provide appropriate help messages for each command using docstrings. 3. Capture and handle errors gracefully (e.g., division by zero, missing arguments). 4. Implement the `precmd` and `postcmd` hooks to manage command history and modify input for `repeat`. 5. Ensure to customize the prompt. Example Interaction ```plaintext python custom_shell.py (CustomShell) ? # Lists available commands Documented commands (type help <topic>): ======================================== add divide exit history multiply repeat subtract (CustomShell) help add # Shows help for \'add\' command Add two numbers and display the result. Usage: add <number1> <number2> (CustomShell) add 3 5 # Performs addition Result: 8 (CustomShell) subtract 10 4 # Performs subtraction Result: 6 (CustomShell) multiply 7 6 # Performs multiplication Result: 42 (CustomShell) divide 8 2 # Performs division Result: 4.0 (CustomShell) divide 8 0 # Handles division by zero Error: Division by zero is not allowed. (CustomShell) history # Shows history of commands 1: add 3 5 2: subtract 10 4 3: multiply 7 6 4: divide 8 2 5: divide 8 0 6: history (CustomShell) repeat # Repeats the last non-empty command 1: add 3 5 Result: 8 (CustomShell) exit # Exits the interpreter ``` Implementation Implement the `CustomShell` class by subclassing `cmd.Cmd` as follows: ```python import cmd class CustomShell(cmd.Cmd): intro = \'Welcome to the Custom Shell. Type help or ? to list commands.n\' prompt = \'(CustomShell) \' file = None def __init__(self): super().__init__() self.history = [] def do_add(self, arg): \'Add two numbers and display the result. Usage: add <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Provide exactly two numbers.\\") result = sum(numbers) self.history.append(f\'add {arg}\') print(f\\"Result: {result}\\") except ValueError as e: print(f\\"Error: {e}\\") def do_subtract(self, arg): \'Subtract the second number from the first and display the result. Usage: subtract <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Provide exactly two numbers.\\") result = numbers[0] - numbers[1] self.history.append(f\'subtract {arg}\') print(f\\"Result: {result}\\") except ValueError as e: print(f\\"Error: {e}\\") def do_multiply(self, arg): \'Multiply two numbers and display the result. Usage: multiply <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Provide exactly two numbers.\\") result = numbers[0] * numbers[1] self.history.append(f\'multiply {arg}\') print(f\\"Result: {result}\\") except ValueError as e: print(f\\"Error: {e}\\") def do_divide(self, arg): \'Divide the first number by the second and display the result. Usage: divide <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Provide exactly two numbers.\\") if numbers[1] == 0: raise ZeroDivisionError(\\"Division by zero is not allowed.\\") result = numbers[0] / numbers[1] self.history.append(f\'divide {arg}\') print(f\\"Result: {result}\\") except (ValueError, ZeroDivisionError) as e: print(f\\"Error: {e}\\") def do_history(self, arg): \'Display the list of commands entered during the session.\' for index, command in enumerate(self.history, 1): print(f\\"{index}: {command}\\") def do_repeat(self, arg): \'Re-runs the last non-empty command entered.\' if self.history: self.onecmd(self.history[-1]) def do_exit(self, arg): \'Exit the command interpreter.\' print(\'Goodbye!\') return True def precmd(self, line): if line and line != \'repeat\': self.history.append(line) return line if __name__ == \'__main__\': CustomShell().cmdloop() ```","solution":"import cmd class CustomShell(cmd.Cmd): intro = \'Welcome to the Custom Shell. Type help or ? to list commands.n\' prompt = \'(CustomShell) \' file = None def __init__(self): super().__init__() self.history = [] def do_add(self, arg): \'Add two numbers and display the result. Usage: add <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Provide exactly two numbers.\\") result = sum(numbers) self.history.append(f\'add {arg}\') print(f\\"Result: {result}\\") except ValueError as e: print(f\\"Error: {e}\\") def do_subtract(self, arg): \'Subtract the second number from the first and display the result. Usage: subtract <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Provide exactly two numbers.\\") result = numbers[0] - numbers[1] self.history.append(f\'subtract {arg}\') print(f\\"Result: {result}\\") except ValueError as e: print(f\\"Error: {e}\\") def do_multiply(self, arg): \'Multiply two numbers and display the result. Usage: multiply <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Provide exactly two numbers.\\") result = numbers[0] * numbers[1] self.history.append(f\'multiply {arg}\') print(f\\"Result: {result}\\") except ValueError as e: print(f\\"Error: {e}\\") def do_divide(self, arg): \'Divide the first number by the second and display the result. Usage: divide <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Provide exactly two numbers.\\") if numbers[1] == 0: raise ZeroDivisionError(\\"Division by zero is not allowed.\\") result = numbers[0] / numbers[1] self.history.append(f\'divide {arg}\') print(f\\"Result: {result}\\") except (ValueError, ZeroDivisionError) as e: print(f\\"Error: {e}\\") def do_history(self, arg): \'Display the list of commands entered during the session.\' for index, command in enumerate(self.history, 1): print(f\\"{index}: {command}\\") def do_repeat(self, arg): \'Re-runs the last non-empty command entered.\' if self.history and self.history[-1] != \'repeat\': self.onecmd(self.history[-1]) def do_exit(self, arg): \'Exit the command interpreter.\' print(\'Goodbye!\') return True def precmd(self, line): if line and line != \'repeat\': self.history.append(line) return line if __name__ == \'__main__\': CustomShell().cmdloop()"},{"question":"# Custom Sequence Class Implementation You are required to implement a custom sequence class in Python that mimics the behavior of Python\'s in-built lists but using the Python/C API functions as described in the provided documentation. This class should provide the following functionalities: 1. **Initialization**: Initialize the sequence with a list of items. 2. **Length**: Implement a method to return the length of the sequence. 3. **Concatenation**: Implement a method to concatenate another sequence to the current sequence. 4. **Repetition**: Implement a method to repeat the sequence a given number of times. 5. **Indexing**: Implement methods to get and set an item at a specific index and to delete an item. 6. **Slicing**: Implement methods to get, set, and delete a slice of the sequence. 7. **Counting**: Implement a method to count occurrences of a value in the sequence. 8. **Containment**: Implement a method to check if a value is in the sequence. 9. **Index Finding**: Implement a method to find the index of the first occurrence of a value. 10. **Conversion**: Implement methods to convert the sequence to a list and a tuple. Class Signature: ```python class CustomSequence: def __init__(self, items): pass def __len__(self): pass def concat(self, other): pass def repeat(self, count): pass def __getitem__(self, index): pass def __setitem__(self, index, value): pass def __delitem__(self, index): pass def get_slice(self, start, end): pass def set_slice(self, start, end, value): pass def del_slice(self, start, end): pass def count(self, value): pass def contains(self, value): pass def index(self, value): pass def to_list(self): pass def to_tuple(self): pass ``` Expected Input and Output Formats 1. **Initialization**: - Input: A list of items. - Output: None 2. **Length (`__len__` method)**: - Input: None - Output: Integer length of the sequence. 3. **Concatenation (`concat` method)**: - Input: `other` (another sequence). - Output: Updated sequence after concatenation. 4. **Repetition (`repeat` method)**: - Input: `count` (integer specifying how many times to repeat). - Output: Updated sequence after repetition. 5. **Indexing (`__getitem__`, `__setitem__`, `__delitem__` methods)**: - Input: `index` (position of the item). - Output: Item value for `__getitem__`, None for `__setitem__` and `__delitem__`. 6. **Slicing (`get_slice`, `set_slice`, `del_slice` methods)**: - Input: `start` and `end` (positions for slicing), `value` (for setting a slice). - Output: Relevant sliced part of the sequence, None for setting and deleting slices. 7. **Counting (`count` method)**: - Input: `value` (item to count occurrences of). - Output: Integer count of value in the sequence. 8. **Containment (`contains` method)**: - Input: `value` (item to check for presence). - Output: Boolean indicating presence of the value. 9. **Index Finding (`index` method)**: - Input: `value` (item to find). - Output: Integer index of the first occurrence of the value. 10. **Conversion (`to_list` and `to_tuple` methods)**: - Input: None - Output: List or tuple representation of the sequence. Constraints: - You can assume that all inputs are valid. - Your implementation should handle both positive and negative indices appropriately. - Ensure that any errors or edge cases (e.g., index out of range) are handled gracefully, raising appropriate exceptions where necessary. Implement the `CustomSequence` class using Python.","solution":"class CustomSequence: def __init__(self, items): self.items = list(items) def __len__(self): return len(self.items) def concat(self, other): self.items.extend(other) def repeat(self, count): self.items = self.items * count def __getitem__(self, index): return self.items[index] def __setitem__(self, index, value): self.items[index] = value def __delitem__(self, index): del self.items[index] def get_slice(self, start, end): return self.items[start:end] def set_slice(self, start, end, value): self.items[start:end] = value def del_slice(self, start, end): del self.items[start:end] def count(self, value): return self.items.count(value) def contains(self, value): return value in self.items def index(self, value): return self.items.index(value) def to_list(self): return self.items[:] def to_tuple(self): return tuple(self.items)"},{"question":"# Question: Implementing and Validating a Generator Function **Objective:** Write a Python function that generates a sequence of squares of integers using a generator function. Additionally, implement functions to validate whether an object is a generator. **Details:** 1. **Function 1: `square_generator(n)`** - **Input:** An integer `n`. - **Output:** A generator object that yields the square of integers from `1` to `n`. ```python def square_generator(n): Creates a generator that yields squares of integers from 1 to n. Args: n (int): The upper limit for generating squares. Yields: int: Squares of numbers from 1 to n. # Your code here ``` 2. **Function 2: `is_generator(obj)`** - **Input:** Any object `obj`. - **Output:** A boolean value `True` if the object is a generator, otherwise `False`. ```python def is_generator(obj): Checks if the given object is a generator. Args: obj: The object to be checked. Returns: bool: True if the object is a generator, False otherwise. # Your code here ``` 3. **Function 3: `is_exact_generator(obj)`** - **Input:** Any object `obj`. - **Output:** A boolean value `True` if the object\'s type is exactly a generator, otherwise `False`. ```python def is_exact_generator(obj): Checks if the given object is exactly a generator. Args: obj: The object to be checked. Returns: bool: True if the object is exactly a generator, False otherwise. # Your code here ``` **Constraints:** - You may not use any external libraries for checking the generator type. - Use Python 3.10 features where applicable. - Ensure that the functions perform efficiently for large values of `n`. # Example Usage ```python # Create a generator for squares of integers from 1 to 5 gen = square_generator(5) # Check if the created object is a generator print(is_generator(gen)) # Expected output: True # Check if the created object is exactly a generator print(is_exact_generator(gen)) # Expected output: True # Iterate through the generator and print values for value in gen: print(value) # Expected output: 1, 4, 9, 16, 25 ``` **Note:** - The `is_generator` and `is_exact_generator` functions should be robust enough to handle different types of objects and accurately determine their generator status.","solution":"def square_generator(n): Creates a generator that yields squares of integers from 1 to n. Args: n (int): The upper limit for generating squares. Yields: int: Squares of numbers from 1 to n. for i in range(1, n+1): yield i * i def is_generator(obj): Checks if the given object is a generator. Args: obj: The object to be checked. Returns: bool: True if the object is a generator, False otherwise. return hasattr(obj, \'__iter__\') and not hasattr(obj, \'__len__\') def is_exact_generator(obj): Checks if the given object is exactly a generator. Args: obj: The object to be checked. Returns: bool: True if the object is exactly a generator, False otherwise. import types return isinstance(obj, types.GeneratorType)"},{"question":"Objective: Design a function that loads a dataset, manipulates and visualizes it using seaborn\'s advanced plotting features. Problem Statement: You are given a semi-raw dataset containing details of different flights. Your task is to: 1. Load and transform the dataset. 2. Create a complex plot using seaborn\'s object-oriented interface. Function Signature: ```python def plot_flight_data(file_path: str) -> None: # Your code here ``` Instructions: 1. **Data Loading and Transformation:** - Load the dataset from the given file path. Assume it is a CSV file. - The dataset contains the following columns: `year`, `month`, `passengers`, `destination`. - Group the data by `year` and `month`, and compute the average number of `passengers`. - Reset the index of the resulting dataframe. 2. **Plotting:** - Use `so.Plot` to create a layout with the transformed data. - Plot a line for each `year` showing the trend of `passengers` over the `months`. - Color the lines differently for each `destination`. - Customize the plot with appropriate labels and titles. Constraints: - Use seaborn\'s object-oriented interface to create the plots. - The output plot should clearly show the trends and differentiate between the different destinations and years. Example: Given a dataset at the path `flights.csv`, the function should load the data, transform it as specified, and produce a plot as described. Input: - `file_path: str`: A string representing the path to the dataset file. Output: - This function should output a complex seaborn plot and show it. No return value is necessary. Additional Information: - You may assume the `file_path` will always point to a valid CSV file respecting the schema presented. - Make sure to install seaborn, pandas, matplotlib prior to running the code. Hint: - Familiarize yourself with seaborn\'s `so.Plot` and its methods such as `pair`, `layout`, and `add`.","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def plot_flight_data(file_path: str) -> None: Loads flight data, transforms it, and creates a seaborn plot showing the trend of passengers over months for different years and destinations. Parameters: - file_path (str): The path to the CSV file containing the flight data. # Load the dataset data = pd.read_csv(file_path) # Group by year and month, compute the average number of passengers, and reset the index transformed_data = data.groupby([\'year\', \'month\', \'destination\'], as_index=False)[\'passengers\'].mean() # Create a seaborn plot p = so.Plot(transformed_data, x=\'month\', y=\'passengers\', color=\'year\').add(so.Line(), so.Agg(), group=\'year\').facet(\'destination\').label(x=\'Month\', y=\'Average Number of Passengers\', title=\'Monthly Trend of Passengers by Year and Destination\').layout(size=(8, 6)) # Show the plot p.show()"},{"question":"Coding Assessment Question # Objective: You are required to demonstrate your understanding of the scikit-learn preprocessing transformations for classification tasks. Specifically, you need to implement three functions for label binarization, multi-label binarization, and label encoding. # Task: Implement the following three functions: 1. **`label_binarizer_transform(labels)`**: - Inputs: - `labels`: List of integer labels for multiclass classification (e.g., [1, 2, 6, 4, 2]). - Outputs: - A tuple containing: - array of unique classes sorted (e.g., array([1, 2, 4, 6])). - label indicator matrix after transformation (e.g., array([[1, 0, 0, 0], [0, 0, 0, 1]])). - Constraints: - Each label must be an integer. - The output should be similar to `LabelBinarizer`. 2. **`multilabel_binarizer_transform(label_sets)`**: - Inputs: - `label_sets`: List of lists or sets of integer labels for multilabel classification (e.g., [[2, 3, 4], [2], [0, 1, 3]]). - Outputs: - Binary indicator matrix after transformation (e.g., array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0]])). - Constraints: - Each inner list or set must contain integer labels. - The output should be similar to `MultiLabelBinarizer`. 3. **`label_encoder_transform(labels)`**: - Inputs: - `labels`: List of labels (strings or integers) for label encoding (e.g., [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"]). - Outputs: - A tuple containing: - array of unique classes sorted (e.g., array([\'amsterdam\', \'paris\', \'tokyo\'])). - transformed numerical labels after encoding (e.g., array([1, 1, 2])). - original labels after inverse transformation (e.g., array([\'tokyo\', \'tokyo\', \'paris\'])). - Constraints: - Labels can be either strings or integers. - The output should be similar to `LabelEncoder`. # Performance Requirements: - The functions should be efficient and able to handle large lists of labels. - It is crucial that the functions output results that are identical in structure and format to those provided by scikit-learn\'s LabelBinarizer, MultiLabelBinarizer, and LabelEncoder. # Example Code: ```python # Function 1: Label Binarizer def label_binarizer_transform(labels): from sklearn.preprocessing import LabelBinarizer lb = LabelBinarizer() lb.fit(labels) classes_ = lb.classes_ transformed = lb.transform(labels) return classes_, transformed # Function 2: MultiLabel Binarizer def multilabel_binarizer_transform(label_sets): from sklearn.preprocessing import MultiLabelBinarizer mlb = MultiLabelBinarizer() transformed = mlb.fit_transform(label_sets) return transformed # Function 3: Label Encoder def label_encoder_transform(labels): from sklearn.preprocessing import LabelEncoder le = LabelEncoder() le.fit(labels) classes_ = le.classes_ transformed = le.transform(labels) inverse_transformed = le.inverse_transform(transformed) return classes_, transformed, inverse_transformed ``` # Submission: Please submit your implementation of the above functions. The functions will be tested with different sets of input data to verify their correctness and performance.","solution":"def label_binarizer_transform(labels): from sklearn.preprocessing import LabelBinarizer lb = LabelBinarizer() lb.fit(labels) classes_ = lb.classes_ transformed = lb.transform(labels) return classes_, transformed def multilabel_binarizer_transform(label_sets): from sklearn.preprocessing import MultiLabelBinarizer mlb = MultiLabelBinarizer() transformed = mlb.fit_transform(label_sets) return transformed def label_encoder_transform(labels): from sklearn.preprocessing import LabelEncoder le = LabelEncoder() le.fit(labels) classes_ = le.classes_ transformed = le.transform(labels) inverse_transformed = le.inverse_transform(transformed) return classes_, transformed, inverse_transformed"},{"question":"# Question: Implementing a Multi-client Server with Non-blocking I/O Objective: Design and implement a Python program that can handle multiple client connections simultaneously using non-blocking I/O. Your solution should leverage the `select` module in Python to ensure efficient multiplexing of I/O operations. Requirements: 1. **Function Implementation**: - Implement a function `multi_client_server(port: int) -> None` that starts a TCP server on the specified port. - The server should accept multiple simultaneous client connections without blocking. 2. **I/O Multiplexing**: - Use the `select.select()` method to handle multiple clients efficiently. - At any given point, the server should be able to manage read and write operations for multiple clients. 3. **Communication Protocol**: - The server should maintain a list of connected clients. - Each client can send multiple lines of text, which the server must receive and store. - The server should echo back each received line to the corresponding client. 4. **Handling Disconnections**: - The server should correctly handle client disconnections and remove them from the list of monitored sockets. Constraints: - The server port number will always be a valid integer in the range 1024 to 65535. - The server should run indefinitely, accepting and echoing back messages until manually terminated. Example Usage: ```python def multi_client_server(port: int) -> None: # Your implementation here # Example of starting the server multi_client_server(12345) ``` In your implementation: - You may assume the server will be terminated manually (e.g., via a keyboard interrupt). - Ensure robust error handling to manage socket exceptions appropriately. - Focus on writing clean, readable, and efficient code. Input: - `port`: An integer representing the port number on which the server will listen for incoming connections. Output: - The function should not return any value. It should run the server indefinitely. You are encouraged to test your implementation locally using multiple client connections to ensure it works as expected.","solution":"import socket import select def multi_client_server(port: int) -> None: server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'0.0.0.0\', port)) server_socket.listen() inputs = [server_socket] outputs = [] clients = {} try: while True: readable, writable, exceptional = select.select(inputs, outputs, inputs) for s in readable: if s is server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(0) inputs.append(client_socket) clients[client_socket] = [] else: data = s.recv(1024) if data: clients[s].append(data) if s not in outputs: outputs.append(s) else: if s in outputs: outputs.remove(s) inputs.remove(s) s.close() del clients[s] for s in writable: try: next_msg = clients[s].pop(0) except IndexError: outputs.remove(s) else: s.send(next_msg) for s in exceptional: inputs.remove(s) if s in outputs: outputs.remove(s) s.close() del clients[s] finally: server_socket.close()"},{"question":"# Custom Python Object Creation Using C API **Objective:** You are required to implement and register a custom Python type using the Python C API. This custom type will be called `PyRectangle` which represents a geometric rectangle and will include methods for basic attribute management, custom comparison, and string representation. # Requirements: 1. **Object Structure:** - **Attributes:** - `width` (double, writable) - `height` (double, writable) - **Methods:** - `area()`: Returns the area of the rectangle. - **Custom Comparison:** Implement rich comparison to compare rectangles\' areas using operators like `<`, `<=`, `==`, `!=`, `>`, `>=`. 2. **Initialization:** - Implement an initializer for the type that accepts `width` and `height` as arguments. 3. **Deallocation:** - Implement a function to handle the deallocation of the rectangle object. 4. **String Representation:** - Implement both `tp_str` and `tp_repr` to provide meaningful string representations of the rectangle object. `tp_str` should return a human-readable string while `tp_repr` should return a string representation fit for debugging. # Expected Input and Output: - **Initialization:** ```python rect = PyRectangle(5.0, 10.0) ``` - **Area Calculation:** ```python rect.area() # Output: 50.0 ``` - **String Representation:** ```python str(rect) # Output: \\"Rectangle(width=5.0, height=10.0)\\" repr(rect) # Output: \\"PyRectangle(5.0, 10.0)\\" ``` - **Comparison:** ```python rect1 = PyRectangle(5.0, 10.0) rect2 = PyRectangle(10.0, 10.0) print(rect1 < rect2) # Output: True (since 50 < 100) ``` # Constraints: - Your implementation should correctly handle memory deallocation to prevent memory leaks. - Ensure the comparison operators are correctly implemented to avoid logic errors. - Pay attention to type errors and input validation within your methods. # Implementation: Provide the necessary C code to implement and register the custom type using the given specifications. Make sure to follow the best practices for resource management and exception handling as outlined in the provided documentation.","solution":"import ctypes class PyRectangle: def __init__(self, width: float, height: float): self.width = width self.height = height def area(self) -> float: return self.width * self.height def __str__(self) -> str: return f\\"Rectangle(width={self.width}, height={self.height})\\" def __repr__(self) -> str: return f\\"PyRectangle({self.width}, {self.height})\\" def __eq__(self, other) -> bool: if isinstance(other, PyRectangle): return self.area() == other.area() return NotImplemented def __lt__(self, other) -> bool: if isinstance(other, PyRectangle): return self.area() < other.area() return NotImplemented def __le__(self, other) -> bool: if isinstance(other, PyRectangle): return self.area() <= other.area() return NotImplemented def __gt__(self, other) -> bool: if isinstance(other, PyRectangle): return self.area() > other.area() return NotImplemented def __ge__(self, other) -> bool: if isinstance(other, PyRectangle): return self.area() >= other.area() return NotImplemented def __ne__(self, other) -> bool: return not self == other"},{"question":"<|Analysis Begin|> The provided documentation extensively covers the `torch.distributed` module available in PyTorch, detailing various aspects such as backends supported, initialization methods, debugging techniques, and collective communication operations. The `torch.distributed` package is particularly useful for enabling distributed computing across multiple GPUs and nodes, which facilitates large-scale machine learning training. The documentation covers essential functionalities, such as: 1. Initialization: Options for different initialization methods (TCP, shared file system, environment variables) are explained. 2. Communication Primitives: Various collective operations supported by different backends (GLOO, NCCL, MPI) and point-to-point communication methods are detailed. 3. Device Management: Functions to initialize and manage device meshes and groups. 4. Debugging: Tools for debugging distributed applications, and breaking out errors, including PyTorch\'s own custom automations. 5. Profiling: Methods for profiling collective communication to evaluate performance. Based on the provided information, the following assessment question can be created. It tests a student\'s understanding of how to set up a distributed training session using the `torch.distributed` package and perform all_reduce collective operations. <|Analysis End|> <|Question Begin|> # Distributed Training with PyTorch You are required to develop a simple PyTorch program using the `torch.distributed` package for distributed training. The program should initialize a distributed environment, create some tensors, and perform an `all_reduce` operation to aggregate these tensors across multiple processes. **Objective:** 1. Initialize a process group for distributed training. 2. Create a local tensor for each process. 3. Perform an `all_reduce` operation to sum up these tensors across all processes. 4. Print the resulting tensor from each process after the `all_reduce` operation. **Instructions:** 1. **Initialization:** - Use `tcp://localhost:12345` as the initialization method for the process group. Ensure all processes use this address. - Set up the world size (total number of processes) to 4 for this exercise. - Set the rank of each process via a variable. 2. **Tensor Creation:** - For each process, create a tensor of shape `(2, 2)` filled with the rank of the process. For example, the tensor for rank 1 might look like: ```python tensor([[1, 1], [1, 1]]) ``` 3. **All-Reduce Operation:** - Perform an `all_reduce` operation with `ReduceOp.SUM` to sum these tensors across all processes. 4. **Output:** - Print the resulting tensor from each process. **Example Structure**: ```python import torch import torch.distributed as dist import os from torch.multiprocessing import Process def init_process(rank, world_size, fn, backend=\'gloo\'): Initialize the distributed environment. os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12345\' dist.init_process_group(backend, rank=rank, world_size=world_size) fn(rank, world_size) dist.destroy_process_group() def run(rank, world_size): Distributed function to be implemented. # Step 1: Create the local tensor local_tensor = torch.ones(2, 2) * rank # Step 2: Perform the all_reduce operation dist.all_reduce(local_tensor, op=dist.ReduceOp.SUM) # Step 3: Print the result print(f\'Rank {rank} has result tensor:n{local_tensor}\') def main(): world_size = 4 processes = [] for rank in range(world_size): p = Process(target=init_process, args=(rank, world_size, run)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": main() ``` **Notes:** - Ensure proper synchronization and clean-up of the process group. - Handle any potential exceptions in process creation or collective operations as needed. Compose your solution maintaining the exact functional details as described above.","solution":"import torch import torch.distributed as dist import os from torch.multiprocessing import Process def init_process(rank, world_size, fn, backend=\'gloo\'): Initialize the distributed environment. os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12345\' dist.init_process_group(backend, rank=rank, world_size=world_size) fn(rank, world_size) dist.destroy_process_group() def run(rank, world_size): Distributed function to be implemented. # Step 1: Create the local tensor local_tensor = torch.ones(2, 2) * rank # Step 2: Perform the all_reduce operation dist.all_reduce(local_tensor, op=dist.ReduceOp.SUM) # Step 3: Print the result print(f\'Rank {rank} has result tensor:n{local_tensor}\') def main(): world_size = 4 processes = [] for rank in range(world_size): p = Process(target=init_process, args=(rank, world_size, run)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": main()"},{"question":"# Coding Assessment: Customizing and Visualizing Color Palettes in Seaborn **Objective:** This assessment tests your understanding of seaborn\'s color palette functionalities and how to apply them effectively in visualizations. **Task:** Write a Python function called `visualize_color_palettes` that takes a dictionary mapping palette names to integers and performs the following tasks: 1. Generate and display a bar plot for each palette using seaborn. 2. Save each plot as an image file named `<palette_name>_palette.png`. 3. Return a dictionary where each key is a palette name and the value is a list of corresponding color hex codes. **Function Signature:** ```python def visualize_color_palettes(palettes: dict) -> dict: pass ``` **Input:** - `palettes` (dict): A dictionary where keys are palette names (strings) and values are integers indicating the number of colors (not applicable for continuous colormaps). **Output:** - A dictionary where keys are palette names and values are lists of color hex codes. **Constraints:** - Use seaborn for generating the color palettes and plotting. - Handle both categorical and continuous colormaps as specified in the documentation. - Your function should work for palettes specified in the seaborn color_palette documentation. # Example Usage: ```python palettes = { \\"pastel\\": 6, \\"husl\\": 9, \\"Set2\\": 8, \\"Spectral\\": 0, # Continuous colormap \\"flare\\": 0, # Continuous colormap \\"ch:s=.25,rot=-.25\\": 0 # Continuous colormap } result = visualize_color_palettes(palettes) for palette_name, hex_codes in result.items(): print(f\\"{palette_name}: {hex_codes}\\") ``` # Expected Functionality: 1. **Visualizing Categorical Palettes:** Generate and display a bar plot for each categorical palette with a specified number of colors. Save each plot as an image file (`<palette_name>_palette.png`). 2. **Visualizing Continuous Colormaps:** Generate and display a continuous color gradient plot for continuous colormaps. Save each plot as an image file (`<palette_name>_palette.png`). 3. **Returning Hex Codes:** Return a dictionary of palette names mapped to lists of color hex codes. # Example Output: ```python { \\"pastel\\": [\'#a1c9f4\', \'#ffb482\', \'#8de5a1\', \'#ff9f9b\', \'#d0bbff\', \'#debb9b\'], \\"husl\\": [\'#f77189\', \'#f7746f\', \'#f77659\', \'#f9793f\', \'#f97b28\', \'#fb8e15\', \'#fb9916\', \'#fda22b\', \'#fda83f\'], \\"Set2\\": [\'#66c2a5\', \'#fc8d62\', \'#8da0cb\', \'#e78ac3\', \'#a6d854\', \'#ffd92f\', \'#e5c494\', \'#b3b3b3\'], \\"Spectral\\": [\'#9e0142\', \'#d53e4f\', \'#f46d43\', \'#fdae61\', \'#fee08b\', \'#ffffbf\', \'#e6f598\', \'#abdda4\', \'#66c2a5\', \'#3288bd\', \'#5e4fa2\'], \\"flare\\": [\'#ffea8b\', \'#e77f6e\', \'#990033\'], \\"ch:s=.25,rot=-.25\\": [\'#171e3a\', \'#7c92a6\', \'#ead2de\'] } ``` This question aims to evaluate your grasp of seaborn\'s color palette functionalities and your ability to use them for creating and saving visualizations, while also extracting color information in a useful format.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_color_palettes(palettes: dict) -> dict: result = {} for palette_name, n_colors in palettes.items(): if n_colors > 0: palette = sns.color_palette(palette_name, n_colors) else: palette = sns.color_palette(palette_name) # Convert the palette to hex codes hex_codes = palette.as_hex() result[palette_name] = hex_codes # Plotting plt.figure(figsize=(8, 2)) sns.palplot(palette) plt.title(palette_name) plt.savefig(f\\"{palette_name}_palette.png\\") plt.close() return result"},{"question":"**Problem Statement**: You have been provided with a large dataset represented as a dense tensor containing many zero values. To efficiently handle this dataset, you need to convert it into various sparse tensor formats and perform some operations on it. **Task**: 1. Implement a function `convert_to_sparse_formats` that takes a dense tensor as input and converts it into several sparse tensor formats (`COO`, `CSR`, and `CSC`). Return these sparse tensors. ```python def convert_to_sparse_formats(dense_tensor: torch.Tensor) -> tuple: Converts a given dense tensor into COOvercomeoutcoo_tensссor, Cootsr_tensor.sparse,asseeentensor.to_sparse`CBR, and CSC sparse tensor formats. Args: dense_tensor (torch.Tensor): A dense tensor. Returns: tuple: A tuple containing the following sparse tensors: - COO format tensor (torch.sparse.Tensor) - CSR format tensor (torch.sparse.Tensor) - CSC format tensor (torch.sparse.Tensor) # Your code here ``` 2. Implement a function `perform_sparse_operations` that takes sparse tensors in COO, CSR, and CSC formats and a dense tensor. Perform matrix multiplication between the sparse tensors and the dense tensor, and return the results. ```python def perform_sparse_operations(coo_tensor: torch.sparse.Tensor, csr_tensor: torch.sparse.Tensor, csc_tensor: torch.sparse.Tensor, dense_tensor: torch.Tensor) -> tuple: Performs matrix multiplication between given sparse tensors (COO, CSR, CSC) and a dense tensor. Args: coo_tensor (torch.sparse.Tensor): A sparse tensor in COO format. csr_tensor (torch.sparse.Tensor): A sparse tensor in CSR format. csc_tensor (torch.sparse.Tensor): A sparse tensor in CSC format. dense_tensor (torch.Tensor): A dense tensor. Returns: tuple: A tuple containing the results of the following operations: - Result of COO tensor * dense tensor - Result of CSR tensor * dense tensor - Result of CSC tensor * dense tensor # Your code here ``` **Constraints**: - Assume that the input dense tensor is a 2D tensor with shape `(m, n)` where `m, n <= 1000`. - The input dense tensor will contain at least 70% zero values. **Input Format**: - A dense tensor `dense_tensor` for the `convert_to_sparse_formats` function. - Sparse tensors `coo_tensor`, `csr_tensor`, `csc_tensor` and a dense tensor `dense_tensor` for the `perform_sparse_operations` function. **Output Format**: - The `convert_to_sparse_formats` function should return a tuple containing the COO, CSR, and CSC format sparse tensors. - The `perform_sparse_operations` function should return a tuple containing the results of the matrix multiplications. **Example**: ```python dense_tensor = torch.tensor([ [0, 0, 1, 0], [2, 0, 0, 3], [0, 0, 4, 0] ], dtype=torch.float32) # Example 1: Convert to sparse formats coo_tensor, csr_tensor, csc_tensor = convert_to_sparse_formats(dense_tensor) print(coo_tensor) print(csr_tensor) print(csc_tensor) # Example 2: Perform sparse operations dense_tensor_b = torch.tensor([ [1, 2], [3, 4], [5, 6], [7, 8] ], dtype=torch.float32) coo_result, csr_result, csc_result = perform_sparse_operations(coo_tensor, csr_tensor, csc_tensor, dense_tensor_b) print(coo_result) print(csr_result) print(csc_result) ``` In the above example, the dense tensor is converted to its sparse tensor formats (COO, CSR, and CSC), and then matrix multiplication is performed between the sparse tensors and another dense tensor.","solution":"import torch def convert_to_sparse_formats(dense_tensor: torch.Tensor) -> tuple: Converts a given dense tensor into COO, CSR, and CSC sparse tensor formats. Args: dense_tensor (torch.Tensor): A dense tensor. Returns: tuple: A tuple containing the following sparse tensors: - COO format tensor (torch.sparse.Tensor) - CSR format tensor (torch.sparse.Tensor) - CSC format tensor (torch.sparse.Tensor) coo_tensor = dense_tensor.to_sparse_coo() csr_tensor = dense_tensor.to_sparse_csr() csc_tensor = dense_tensor.to_sparse_csc() return coo_tensor, csr_tensor, csc_tensor def perform_sparse_operations(coo_tensor: torch.sparse.Tensor, csr_tensor: torch.sparse.Tensor, csc_tensor: torch.sparse.Tensor, dense_tensor: torch.Tensor) -> tuple: Performs matrix multiplication between given sparse tensors (COO, CSR, CSC) and a dense tensor. Args: coo_tensor (torch.sparse.Tensor): A sparse tensor in COO format. csr_tensor (torch.sparse.Tensor): A sparse tensor in CSR format. csc_tensor (torch.sparse.Tensor): A sparse tensor in CSC format. dense_tensor (torch.Tensor): A dense tensor. Returns: tuple: A tuple containing the results of the following operations: - Result of COO tensor * dense tensor - Result of CSR tensor * dense tensor - Result of CSC tensor * dense tensor coo_result = torch.sparse.mm(coo_tensor, dense_tensor) csr_result = torch.sparse.mm(csr_tensor, dense_tensor) csc_result = torch.sparse.mm(csc_tensor, dense_tensor) return coo_result, csr_result, csc_result"},{"question":"**Objective:** Assess understanding of PyTorch tensor management and interoperability with DLPack. **Problem Statement:** You are provided with a PyTorch tensor and a function designed to perform some computationally intensive operations using a different machine learning framework that supports DLPack. Your task is to utilize PyTorch\'s `torch.utils.dlpack` methods to effectively convert tensors between PyTorch and DLPack formats, ensuring smooth interoperability. **Function Signature:** ```python def operate_with_external_framework(tensor: torch.Tensor) -> torch.Tensor: pass ``` # Instructions: 1. **Input:** - `tensor` (torch.Tensor): A PyTorch tensor containing integer values. The tensor\'s shape and size are arbitrary but will contain only integers. 2. **Output:** - The function should return a new PyTorch tensor after passing it through the external computation function. The output tensor should retain the same shape. 3. **Implementation Constraints:** - Use the DLPack conversion functions (`torch.utils.dlpack.from_dlpack` and `torch.utils.dlpack.to_dlpack`) to facilitate the tensor input/output operations. - Assume the external computation function (provided below) is a black-box function that accepts a DLPack tensor and returns a modified DLPack tensor. **External Function:** ```python def external_computation(dlpack_tensor): # This function takes a DLPack tensor as input and performs # an element-wise operation. Here we simulate an increment operation. import torch.utils.dlpack as dlpack pt_tensor = dlpack.from_dlpack(dlpack_tensor) result = pt_tensor + 1 return dlpack.to_dlpack(result) ``` # Example: ```python import torch # Example tensor input_tensor = torch.tensor([1, 2, 3, 4]) # Expected output is tensor with each element incremented by 1 # output_tensor = torch.tensor([2, 3, 4, 5]) def operate_with_external_framework(tensor: torch.Tensor) -> torch.Tensor: import torch.utils.dlpack as dlpack # Convert PyTorch tensor to DLPack tensor dlpack_tensor = dlpack.to_dlpack(tensor) # Pass through the external computation function modified_dlpack_tensor = external_computation(dlpack_tensor) # Convert the modified DLPack tensor back to a PyTorch tensor result_tensor = dlpack.from_dlpack(modified_dlpack_tensor) return result_tensor # Test the function output_tensor = operate_with_external_framework(input_tensor) print(output_tensor) # Output should be: tensor([2, 3, 4, 5]) ``` Test the function thoroughly to ensure it handles tensors of various shapes and contents appropriately, confirming it adheres to the described constraints and behavior.","solution":"import torch import torch.utils.dlpack as dlpack def operate_with_external_framework(tensor: torch.Tensor) -> torch.Tensor: Converts a PyTorch tensor to DLPack, performs an external operation, and converts it back to a PyTorch tensor. Args: - tensor (torch.Tensor): Input PyTorch tensor containing integer values. Returns: - torch.Tensor: A new tensor with the same shape after the external operation. def external_computation(dlpack_tensor): # Simulating some external computation. pt_tensor = dlpack.from_dlpack(dlpack_tensor) result = pt_tensor + 1 return dlpack.to_dlpack(result) # Convert PyTorch tensor to DLPack tensor dlpack_tensor = dlpack.to_dlpack(tensor) # Pass through the external computation function modified_dlpack_tensor = external_computation(dlpack_tensor) # Convert the modified DLPack tensor back to a PyTorch tensor result_tensor = dlpack.from_dlpack(modified_dlpack_tensor) return result_tensor"},{"question":"# Question: Implement and Initialize a Custom Neural Network with PyTorch You are required to design and implement a simple feedforward neural network using PyTorch and apply parameter initialization techniques from the `torch.nn.init` module. Requirements: 1. You must implement a simple feedforward neural network with one hidden layer. 2. You should define a method within your network class to initialize the network parameters using different strategies: - Xavier Uniform - Kaiming Normal 3. Your network should be flexible enough to accept different input dimensions, hidden layer sizes, and output dimensions. 4. You must provide a demonstrative script that: - Initializes the parameters using the methods mentioned. - Prints out the weights and biases after initialization to show the differences. Implementation Details: 1. **Class Definition** - Define a class `SimpleFeedforwardNN` inheriting from `torch.nn.Module`. - The constructor should accept input_size, hidden_size, and output_size as arguments and define a feedforward network with one hidden layer. 2. **Initialization Method** - Implement a method `initialize_parameters` in the class that accepts a string argument to choose the initialization method (\'xavier_uniform\' or \'kaiming_normal\') and initializes the parameters of all layers accordingly. 3. **Main Script** - Create an instance of `SimpleFeedforwardNN`. - Initialize the network parameters with both initialization strategies and print out the weights and biases for each layer. Constraints: - Assume the use of ReLU activation for the hidden layer. - Ensure that the script is free of runtime errors. - The method `initialize_parameters` should only alter weights and biases if they belong to `torch.nn.Linear` layers. Example: ```python import torch import torch.nn as nn import torch.nn.init as init class SimpleFeedforwardNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleFeedforwardNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def initialize_parameters(self, method=\'xavier_uniform\'): if method == \'xavier_uniform\': init.xavier_uniform_(self.fc1.weight) init.xavier_uniform_(self.fc2.weight) elif method == \'kaiming_normal\': init.kaiming_normal_(self.fc1.weight) init.kaiming_normal_(self.fc2.weight) init.constant_(self.fc1.bias, 0) init.constant_(self.fc2.bias, 0) # Example usage input_size = 10 hidden_size = 5 output_size = 2 net = SimpleFeedforwardNN(input_size, hidden_size, output_size) # Initialize with Xavier Uniform net.initialize_parameters(method=\'xavier_uniform\') print(\\"Xavier Uniform Initialization:\\") print(\\"Layer 1 Weights: \\", net.fc1.weight) print(\\"Layer 1 Biases: \\", net.fc1.bias) print(\\"Layer 2 Weights: \\", net.fc2.weight) print(\\"Layer 2 Biases: \\", net.fc2.bias) # Initialize with Kaiming Normal net.initialize_parameters(method=\'kaiming_normal\') print(\\"Kaiming Normal Initialization:\\") print(\\"Layer 1 Weights: \\", net.fc1.weight) print(\\"Layer 1 Biases: \\", net.fc1.bias) print(\\"Layer 2 Weights: \\", net.fc2.weight) print(\\"Layer 2 Biases: \\", net.fc2.bias) ``` Input and Output: - No specific inputs or outputs are required; the script should demonstrate the initialization techniques by printing out the weights and biases.","solution":"import torch import torch.nn as nn import torch.nn.init as init class SimpleFeedforwardNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleFeedforwardNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def initialize_parameters(self, method=\'xavier_uniform\'): if method == \'xavier_uniform\': init.xavier_uniform_(self.fc1.weight) init.xavier_uniform_(self.fc2.weight) elif method == \'kaiming_normal\': init.kaiming_normal_(self.fc1.weight, nonlinearity=\'relu\') init.kaiming_normal_(self.fc2.weight, nonlinearity=\'relu\') init.constant_(self.fc1.bias, 0) init.constant_(self.fc2.bias, 0) # Example usage input_size = 10 hidden_size = 5 output_size = 2 net = SimpleFeedforwardNN(input_size, hidden_size, output_size) # Initialize with Xavier Uniform net.initialize_parameters(method=\'xavier_uniform\') print(\\"Xavier Uniform Initialization:\\") print(\\"Layer 1 Weights: \\", net.fc1.weight) print(\\"Layer 1 Biases: \\", net.fc1.bias) print(\\"Layer 2 Weights: \\", net.fc2.weight) print(\\"Layer 2 Biases: \\", net.fc2.bias) # Initialize with Kaiming Normal net.initialize_parameters(method=\'kaiming_normal\') print(\\"Kaiming Normal Initialization:\\") print(\\"Layer 1 Weights: \\", net.fc1.weight) print(\\"Layer 1 Biases: \\", net.fc1.bias) print(\\"Layer 2 Weights: \\", net.fc2.weight) print(\\"Layer 2 Biases: \\", net.fc2.bias)"},{"question":"**Question: Temporary File and Directory Management using Python\'s `tempfile` Module** You are tasked with writing a Python function that performs a series of operations involving temporary files and directories. The function should demonstrate your understanding of the `tempfile` module, including creating temporary files and directories securely, writing and reading data, and ensuring proper cleanup. Write a function `manage_temp_files_dirs` that performs the following tasks: 1. Create a temporary file using `TemporaryFile` and write the string \\"Temporary File Data\\" to it. Read back the content to confirm the write operation. 2. Create a named temporary file using `NamedTemporaryFile` and write the string \\"Named Temporary File Data\\" to it. Read back the content to confirm the write operation. 3. Create a spooled temporary file using `SpooledTemporaryFile` with a max size of 100 bytes, write the string \\"Spooled Temp File Data\\" to it, and use the `rollover` method to ensure it is written to disk. Read back the content to confirm the write operation. 4. Create a temporary directory using `TemporaryDirectory`, and within this directory, create a new temporary file using `mkstemp`. Write the string \\"File in Temp Directory\\" to this file and read back the content to confirm the write operation. 5. Ensure that all temporary files and directories are properly cleaned up once the operations are completed. The function should return a dictionary with the read-back contents for verification. **Function Signature:** ```python def manage_temp_files_dirs() -> dict: pass ``` **Expected Output:** The function should return a dictionary with keys `\\"temp_file\\"`, `\\"named_temp_file\\"`, `\\"spooled_temp_file\\"`, and `\\"file_in_temp_dir\\"` with their respective read-back contents as values. **Example:** ```python { \'temp_file\': \'Temporary File Data\', \'named_temp_file\': \'Named Temporary File Data\', \'spooled_temp_file\': \'Spooled Temp File Data\', \'file_in_temp_dir\': \'File in Temp Directory\' } ``` **Constraints:** - Do not write any data to disk outside the context of temporary files and directories. - Ensure all file operations use binary mode to avoid platform-specific issues. - Properly handle any exceptions and ensure cleanup of all temporary resources. ```python import tempfile import os def manage_temp_files_dirs() -> dict: results = {} # Task 1: TemporaryFile with tempfile.TemporaryFile() as temp_file: temp_file.write(b\'Temporary File Data\') temp_file.seek(0) results[\'temp_file\'] = temp_file.read().decode() # Task 2: NamedTemporaryFile with tempfile.NamedTemporaryFile(delete=True) as named_temp_file: named_temp_file.write(b\'Named Temporary File Data\') named_temp_file.seek(0) results[\'named_temp_file\'] = named_temp_file.read().decode() # Task 3: SpooledTemporaryFile with tempfile.SpooledTemporaryFile(max_size=100) as spooled_temp_file: spooled_temp_file.write(b\'Spooled Temp File Data\') spooled_temp_file.rollover() spooled_temp_file.seek(0) results[\'spooled_temp_file\'] = spooled_temp_file.read().decode() # Task 4: TemporaryDirectory with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = os.path.join(temp_dir, \'temp_file.txt\') fd, path = tempfile.mkstemp(dir=temp_dir) with os.fdopen(fd, \'wb\') as file_in_temp_dir: file_in_temp_dir.write(b\'File in Temp Directory\') with open(path, \'rb\') as file_in_temp_dir: results[\'file_in_temp_dir\'] = file_in_temp_dir.read().decode() return results ```","solution":"import tempfile import os def manage_temp_files_dirs() -> dict: results = {} # Task 1: TemporaryFile with tempfile.TemporaryFile() as temp_file: temp_file.write(b\'Temporary File Data\') temp_file.seek(0) results[\'temp_file\'] = temp_file.read().decode() # Task 2: NamedTemporaryFile with tempfile.NamedTemporaryFile(delete=True) as named_temp_file: named_temp_file.write(b\'Named Temporary File Data\') named_temp_file.seek(0) results[\'named_temp_file\'] = named_temp_file.read().decode() # Task 3: SpooledTemporaryFile with tempfile.SpooledTemporaryFile(max_size=100) as spooled_temp_file: spooled_temp_file.write(b\'Spooled Temp File Data\') spooled_temp_file.rollover() spooled_temp_file.seek(0) results[\'spooled_temp_file\'] = spooled_temp_file.read().decode() # Task 4: TemporaryDirectory with tempfile.TemporaryDirectory() as temp_dir: fd, path = tempfile.mkstemp(dir=temp_dir) with os.fdopen(fd, \'wb\') as file_in_temp_dir: file_in_temp_dir.write(b\'File in Temp Directory\') with open(path, \'rb\') as file_in_temp_dir: results[\'file_in_temp_dir\'] = file_in_temp_dir.read().decode() return results"},{"question":"Coding Assessment Question # Objective The goal of this exercise is to assess your understanding of the `asyncio` module in Python, especially handling concurrency, managing tasks, and enabling debugging features to catch common asynchronous programming pitfalls. # Problem Statement You are required to implement a program that demonstrates advanced usage of `asyncio` with multiple tasks performing I/O operations concurrently. The program should include mechanisms to handle potential issues such as un-awaited coroutines and unhandled exceptions. # Task Details Implement a function `fetch_data_from_sources(sources: List[str]) -> List[str]` that accepts a list of URLs (as strings) and fetches data from these URLs concurrently. The function should: 1. Use asynchronous mechanisms to fetch data from each URL. 2. Ensure that if any coroutine is not awaited properly, it should be detected using debugging features. 3. Handle potential exceptions that may occur during the data fetching and ensure they are not left unhandled. 4. Use appropriate logging to capture details about the tasks and any issues encountered. 5. Use a custom executor to run blocking code (like a CPU-bound function) in a different thread to avoid blocking the event loop. # Function Signature ```python from typing import List async def fetch_data_from_sources(sources: List[str]) -> List[str]: ``` # Constraints and Requirements 1. The solution should use `asyncio` for concurrent operations. 2. Use `asyncio.run()` to execute the main coroutine in debug mode. 3. Implement proper logging to capture debug information. 4. Any blocking operations should be handled using an executor. 5. Provide appropriate exception handling to log errors without crashing the program. # Example Usage ```python import asyncio import logging from typing import List async def fetch_data(url: str) -> str: await asyncio.sleep(1) # Simulate I/O operation return f\\"Data from {url}\\" async def fetch_data_from_sources(sources: List[str]) -> List[str]: # Your implementation here if __name__ == \\"__main__\\": logging.basicConfig(level=logging.DEBUG) sources = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] data = asyncio.run(fetch_data_from_sources(sources), debug=True) print(data) ``` # Notes - Focus on using the best practices described in the provided documentation. - Utilize debug mode features to catch un-awaited coroutines and unhandled exceptions. - Ensure that the solution is efficient and handles multiple concurrency aspects effectively.","solution":"import asyncio import logging from typing import List from aiohttp import ClientSession # Configure logging logging.basicConfig(level=logging.DEBUG) async def fetch_data(session: ClientSession, url: str) -> str: try: async with session.get(url) as response: response.raise_for_status() data = await response.text() logging.debug(f\\"Fetched data from {url}\\") return data except Exception as e: logging.error(f\\"Error fetching data from {url}: {e}\\") return str(e) async def fetch_data_from_sources(sources: List[str]) -> List[str]: async with ClientSession() as session: tasks = [asyncio.create_task(fetch_data(session, url)) for url in sources] results = await asyncio.gather(*tasks, return_exceptions=True) return results if __name__ == \\"__main__\\": sources = [\\"http://example.com\\", \\"http://invalid-url\\", \\"http://example.org\\"] data = asyncio.run(fetch_data_from_sources(sources), debug=True) print(data)"},{"question":"# Covariance Estimation Challenge In this challenge, you are required to implement a function that estimates covariance matrices using different methods from the `sklearn.covariance` module. You will be given a dataset, and your function should return the estimated covariance matrices using empirical covariance, shrunk covariance, Ledoit-Wolf shrinkage, and Oracle Approximating Shrinkage. Task Implement the function `estimate_covariances(X: np.ndarray) -> dict` that: 1. Takes in a 2D numpy array `X` of shape `(n_samples, n_features)` representing the dataset. 2. Returns a dictionary with the following keys and corresponding covariance matrices: - `\\"empirical\\"`: Estimated using the `EmpiricalCovariance` class. - `\\"shrunk\\"`: Estimated using the `ShrunkCovariance` class with a shrinkage parameter `alpha=0.1`. - `\\"ledoit_wolf\\"`: Estimated using the `LedoitWolf` class. - `\\"oas\\"`: Estimated using the `OAS` class. Constraints - You can assume `n_samples >= 10` and `n_features <= 100`. - Use the default settings for each estimator unless specified otherwise. - You may use only numpy and scikit-learn libraries. Example ```python import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS def estimate_covariances(X: np.ndarray) -> dict: covariances = {} # Empirical covariance emp_cov = EmpiricalCovariance().fit(X) covariances[\\"empirical\\"] = emp_cov.covariance_ # Shrunk covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X) covariances[\\"shrunk\\"] = shrunk_cov.covariance_ # Ledoit-Wolf shrinkage lw_cov = LedoitWolf().fit(X) covariances[\\"ledoit_wolf\\"] = lw_cov.covariance_ # Oracle Approximating Shrinkage oas_cov = OAS().fit(X) covariances[\\"oas\\"] = oas_cov.covariance_ return covariances # Example usage X = np.random.randn(50, 10) cov_matrices = estimate_covariances(X) for method, cov_matrix in cov_matrices.items(): print(f\\"{method} covariance matrix:n{cov_matrix}n\\") ``` For the given example, your function should output the covariance matrices estimated using the specified methods.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS def estimate_covariances(X: np.ndarray) -> dict: covariances = {} # Empirical covariance emp_cov = EmpiricalCovariance().fit(X) covariances[\\"empirical\\"] = emp_cov.covariance_ # Shrunk covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X) covariances[\\"shrunk\\"] = shrunk_cov.covariance_ # Ledoit-Wolf shrinkage lw_cov = LedoitWolf().fit(X) covariances[\\"ledoit_wolf\\"] = lw_cov.covariance_ # Oracle Approximating Shrinkage oas_cov = OAS().fit(X) covariances[\\"oas\\"] = oas_cov.covariance_ return covariances"},{"question":"# Question: Implementing and using the `if __name__ == \\"__main__\\"` Block Background: Your task is to implement a Python script that includes a main function and utilizes the `if __name__ == \\"__main__\\"` idiom to ensure it can be used both as a script and as an imported module. The script will involve basic operations to manage a list of student names, including adding, removing, and listing names available. Your Task: 1. **Define a function `add_student(name: str)`**: - This function should add the provided name to a global list `students`. - Ensure there are no duplicates in the list. 2. **Define a function `remove_student(name: str)`**: - This function should remove the provided name from the global list `students` if it exists. - If the name is not in the list, print a message that the student was not found. 3. **Define a function `list_students()` that prints all students**: - This function should print all names in the global list `students`. 4. **Define a function `main()`**: - This function should parse command-line arguments to allow the user to add, remove, or list students. - Use the `argparse` module to handle command-line arguments. - The valid commands are: - `add <name>`: to add a student name. - `remove <name>`: to remove a student name. - `list`: to list all student names. 5. **Use the `if __name__ == \\"__main__\\":` idiom**: - Ensure that when your script is run directly, the `main()` function is executed. - When the script is imported as a module, the `main()` function should not be executed automatically. Expected Output: - When the script is run with `add John`, it should add \\"John\\" to the list. - When run with `add Jane` and then `list`, it should list \\"John\\" and \\"Jane\\". - When run with `remove John` followed by `list`, it should only list \\"Jane\\". - Attempting to remove a non-existing name should show a \\"student not found\\" message. Constraints and Limitations: - The script should handle potential errors gracefully (e.g., adding a duplicate name should not cause an error). - The `students` list should be able to hold an arbitrary number of student names. - Performance constraints are minimal as this is a basic management script. You may test your implementation using the following commands: ``` python3 students.py add John python3 students.py add Jane python3 students.py list python3 students.py remove John python3 students.py list python3 students.py remove Mike ``` Sample Code Structure: ```python students = [] def add_student(name: str): # Implement this function def remove_student(name: str): # Implement this function def list_students(): # Implement this function def main(): # Implement argument parsing and command handling here if __name__ == \\"__main__\\": main() ``` Ensure you test the script both as a standalone application and as an imported module.","solution":"import argparse # Initialize the global list of students students = [] def add_student(name: str): if name not in students: students.append(name) def remove_student(name: str): if name in students: students.remove(name) else: print(f\\"Student {name} not found\\") def list_students(): for student in students: print(student) def main(): parser = argparse.ArgumentParser(description=\\"Manage a list of students.\\") subparsers = parser.add_subparsers(dest=\\"command\\") add_parser = subparsers.add_parser(\'add\', help=\'Add a student\') add_parser.add_argument(\'name\', type=str, help=\'The name of the student to add\') remove_parser = subparsers.add_parser(\'remove\', help=\'Remove a student\') remove_parser.add_argument(\'name\', type=str, help=\'The name of the student to remove\') list_parser = subparsers.add_parser(\'list\', help=\'List all students\') args = parser.parse_args() if args.command == \'add\': add_student(args.name) elif args.command == \'remove\': remove_student(args.name) elif args.command == \'list\': list_students() else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implement Custom Chunk Reader In this task, you are going to implement a simplified version of the deprecated `chunk` module to understand how to handle structured binary data. Your objective is to create a `CustomChunk` class that can read data chunks from a binary file. Your class should include: 1. **Initialization**: - The class should be initialized with a file-like object and options for alignment (default to `True`), and endian order (default to `True` for big-endian). 2. **Methods**: - `getname(self)`: Returns the 4-byte chunk ID. - `getsize(self)`: Returns the size of the chunk data (excluding headers). - `close(self)`: Closes the chunk and moves the file pointer to the end of the chunk. - `seek(self, pos, whence=0)`: Sets the current file position within the chunk. - `tell(self)`: Returns the current file position within the chunk. - `read(self, size=-1)`: Reads at most `size` bytes from the chunk (reads all data if size is -1 or omitted). - `skip(self)`: Skips the chunk content moving the file pointer to the start of the next chunk. # Constraints: - The given file-like object will always be opened in binary mode. - Assume that the chunk size stored in the header is always in big-endian byte order. - Align the chunks on 2-byte boundaries if alignment is enabled. - The file-like object will support `seek()` and `tell()` methods. # Example Usage: ```python with open(\'example_chunked_file.aiff\', \'rb\') as file_obj: chunk = CustomChunk(file_obj) print(chunk.getname()) # Should print the 4-byte Chunk ID print(chunk.getsize()) # Should print the size of the Chunk data print(chunk.read(10)) # Should read and print the first 10 bytes of the chunk data chunk.skip() # Should skip to the next chunk ``` # Implementation: Implement the `CustomChunk` class below. ```python class CustomChunk: def __init__(self, file, align=True, bigendian=True): self.file = file self.align = align self.bigendian = bigendian self.header = file.read(8) if len(self.header) != 8: raise EOFError(\\"Not enough data to read header\\") self.chunk_id = self.header[:4] self.chunk_size = int.from_bytes(self.header[4:], \\"big\\" if self.bigendian else \\"little\\") self.chunk_start = file.tell() self.chunk_end = self.chunk_start + self.chunk_size if self.align and self.chunk_size % 2: self.chunk_end += 1 def getname(self): return self.chunk_id def getsize(self): return self.chunk_size def close(self): self.file.seek(self.chunk_end) def seek(self, pos, whence=0): if whence == 0: new_pos = self.chunk_start + pos elif whence == 1: new_pos = self.file.tell() + pos elif whence == 2: new_pos = self.chunk_end + pos else: raise ValueError(\\"Invalid value for \'whence\'\\") if new_pos < self.chunk_start or new_pos > self.chunk_end: raise OSError(\\"Cannot seek outside the chunk\\") self.file.seek(new_pos) def tell(self): return self.file.tell() - self.chunk_start def read(self, size=-1): if size == -1: size = self.chunk_end - self.file.tell() return self.file.read(size) def skip(self): self.file.seek(self.chunk_end) ``` The above problem statement should challenge students to understand binary data operations and various file handling mechanisms, including seeking and reading specific bytes.","solution":"class CustomChunk: def __init__(self, file, align=True, bigendian=True): self.file = file self.align = align self.bigendian = bigendian self.header = file.read(8) if len(self.header) != 8: raise EOFError(\\"Not enough data to read header\\") self.chunk_id = self.header[:4] self.chunk_size = int.from_bytes(self.header[4:], \\"big\\" if self.bigendian else \\"little\\") self.chunk_start = file.tell() self.chunk_end = self.chunk_start + self.chunk_size if self.align and self.chunk_size % 2: self.chunk_end += 1 def getname(self): return self.chunk_id def getsize(self): return self.chunk_size def close(self): self.file.seek(self.chunk_end) def seek(self, pos, whence=0): if whence == 0: new_pos = self.chunk_start + pos elif whence == 1: new_pos = self.file.tell() + pos elif whence == 2: new_pos = self.chunk_end + pos else: raise ValueError(\\"Invalid value for \'whence\'\\") if new_pos < self.chunk_start or new_pos > self.chunk_end: raise OSError(\\"Cannot seek outside the chunk\\") self.file.seek(new_pos) def tell(self): return self.file.tell() - self.chunk_start def read(self, size=-1): if size == -1: size = self.chunk_end - self.file.tell() return self.file.read(size) def skip(self): self.file.seek(self.chunk_end)"},{"question":"Advanced Bar Plot with `seaborn.objects` Objective: Demonstrate your understanding of the `seaborn.objects` module by preprocessing a dataset and creating a comprehensive bar plot. Problem Statement: You are given a dataset that contains information about various tips received by waitstaff in a restaurant. Your task is to preprocess this dataset and create an insightful visualization using `seaborn.objects`. Specifically, you will create a bar plot that shows the count of tips received each day, color-coded by gender, and further broken down by an additional variable. Dataset: The dataset is available via `seaborn`\'s `load_dataset` function using the key `\\"tips\\"`. Requirements: 1. **Data Preprocessing**: - Load the `tips` dataset using `seaborn`. - Ensure the data is clean and ready for visualization. 2. **Visualization**: - Create a bar plot showing the count of tips received each day of the week. - Color code the bars based on the gender (`sex`) of the waitstaff. - Further break down the counts by another variable of your choice using an appropriate transformation. Functions to Use: - `so.Plot` - `so.Bar` - `so.Count` - `so.Dodge` (if necessary) Constraints: - The dataset should be loaded using `seaborn.load_dataset(\\"tips\\")`. - The plot must be generated using the `seaborn.objects` module, not the traditional `seaborn` plotting functions. - You must use a variable for additional grouping/transformation besides `day` and `sex`. Expected Steps: 1. Load the dataset. 2. Process and clean the dataset if necessary. 3. Use `so.Plot` and related functions to create the required visualization. 4. Display the plot. Example Code: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot ``` Extend the example code to include an additional variable transformation or grouping. Submission: Submit a Python script or a Jupyter notebook with the required code to generate the plot. Ensure the plot displays correctly when the script/notebook is executed.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_tips_bar_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"day\\", color=\\"sex\\", y=\\"size\\") .add(so.Bar(), so.Count(), so.Dodge()) .label(x=\\"Day of the Week\\", y=\\"Count of Tips\\", color=\\"Gender\\") ) plot.show() # Run the function to display the plot create_tips_bar_plot()"},{"question":"# Coding Assessment: Chat Server Implementation with Non-Blocking Sockets Objective Implement a non-blocking chat server that can handle multiple clients simultaneously. The server should be able to broadcast messages received from any client to all connected clients. Requirements 1. Create a non-blocking server socket that listens for incoming client connections on a specified port. 2. Use the `select` function to manage multiple socket connections without blocking. 3. Implement a mechanism to handle incoming messages from clients and broadcast these messages to all other connected clients. 4. Ensure proper disconnection handling and resource cleanup. Function to Implement You are required to implement the following function: ```python import socket import select def chat_server(host: str, port: int): Starts a non-blocking chat server that listens on the given host and port. Args: host (str): The hostname or IP address on which the server should listen. port (int): The port number on which the server should listen. Usage: chat_server(\\"localhost\\", 12345) server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(False) server_socket.bind((host, port)) server_socket.listen(5) sockets_list = [server_socket] clients = {} print(f\\"Listening on {host}:{port}...\\") try: while True: ready_to_read, ready_to_write, in_error = select.select(sockets_list, [], sockets_list) for s in ready_to_read: if s == server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address}\\") else: data = s.recv(1024) if data: broadcast(clients, sockets_list, s, data) else: print(f\\"Connection closed by {clients[s]}\\") if s in sockets_list: sockets_list.remove(s) del clients[s] s.close() except Exception as e: print(f\\"Server error: {e}\\") finally: server_socket.close() for s in sockets_list: s.close() def broadcast(clients, sockets_list, source_socket, message): Broadcasts a message to all sockets except the source socket. Args: clients (dict): Dictionary of client sockets and their addresses. sockets_list (list): List of all active sockets. source_socket (socket.socket): The socket that sent the message. message (bytes): The message to broadcast. for socket in clients: if socket != source_socket and socket in sockets_list: try: socket.send(message) except Exception as e: print(f\\"Error sending message to {clients[socket]}: {e}\\") socket.close() if socket in sockets_list: sockets_list.remove(socket) ``` Input and Output - **Input**: - `host` (str): Hostname or IP address on which the server should listen (e.g., `\\"localhost\\"`). - `port` (int): Port number on which the server should listen (e.g., `12345`). - **Output**: - The function should not return any value. However, it should print messages for various events such as new connections, disconnections, errors, and when broadcasting messages. Constraints - The server should handle at least 5 simultaneous client connections. - Proper error handling and socket resource cleanup are required. - Make sure to test the server with multiple clients to verify its functionality. Notes 1. You can create a client for testing purposes that connects to the server and sends messages. 2. Consider edge cases such as disconnections and invalid messages. Example Usage ```python if __name__ == \\"__main__\\": chat_server(\\"localhost\\", 12345) ```","solution":"import socket import select def chat_server(host: str, port: int): Starts a non-blocking chat server that listens on the given host and port. Args: host (str): The hostname or IP address on which the server should listen. port (int): The port number on which the server should listen. server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(False) server_socket.bind((host, port)) server_socket.listen(5) sockets_list = [server_socket] clients = {} print(f\\"Listening on {host}:{port}...\\") try: while True: ready_to_read, _, in_error = select.select(sockets_list, [], sockets_list) for s in ready_to_read: if s == server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address}\\") else: data = s.recv(1024) if data: broadcast(clients, sockets_list, s, data) else: print(f\\"Connection closed by {clients[s]}\\") if s in sockets_list: sockets_list.remove(s) del clients[s] s.close() except Exception as e: print(f\\"Server error: {e}\\") finally: for s in sockets_list: s.close() def broadcast(clients, sockets_list, source_socket, message): Broadcasts a message to all sockets except the source socket. Args: clients (dict): Dictionary of client sockets and their addresses. sockets_list (list): List of all active sockets. source_socket (socket.socket): The socket that sent the message. message (bytes): The message to broadcast. for socket in clients: if socket != source_socket and socket in sockets_list: try: socket.send(message) except Exception as e: print(f\\"Error sending message to {clients[socket]}: {e}\\") socket.close() if socket in sockets_list: sockets_list.remove(socket)"},{"question":"# Question: Multi-Base Encoder and Decoder You are required to implement a function that performs encoding and decoding using multiple formats from the `base64` module. The function should be able to encode and decode given messages using Base64, Base32, and Base16 encoding schemes. Function Signature ```python def multi_base_convert(data: bytes, convert_to: str, mode: str, altchars: bytes = None) -> bytes: pass ``` Input 1. `data` (bytes): The data to be encoded or decoded. 2. `convert_to` (str): The base to convert to. Possible values are `\'base64\'`, `\'base32\'`, and `\'base16\'`. 3. `mode` (str): The mode of conversion. Possible values are `\'encode\'` and `\'decode\'`. 4. `altchars` (bytes, optional): Specifies an alternative alphabet for certain characters in encoding. Only applicable for Base64 encoding/decoding. Default is `None`. Output - Returns the converted data as bytes after encoding or decoding. Constraints - The length of `altchars` should be exactly 2 bytes if provided. - For decoding, the data must be a valid ASCII string or a bytes object that represents a valid encoded string. - Raise a `ValueError` if `convert_to` is not one of `\'base64\'`, `\'base32\'`, or `\'base16\'`. - Raise a `ValueError` if `mode` is not one of `\'encode\'` or `\'decode\'`. Example ```python # Example usage of the multi_base_convert function # Encoding \'hello\' using Base64 >>> multi_base_convert(b\'hello\', \'base64\', \'encode\') b\'aGVsbG8=\' # Decoding \'aGVsbG8=\' using Base64 >>> multi_base_convert(b\'aGVsbG8=\', \'base64\', \'decode\') b\'hello\' # Encoding \'hello\' using Base32 >>> multi_base_convert(b\'hello\', \'base32\', \'encode\') b\'NBSWY3DP\' # Decoding \'NBSWY3DP\' using Base32 >>> multi_base_convert(b\'NBSWY3DP\', \'base32\', \'decode\') b\'hello\' # Encoding \'hello\' using Base16 >>> multi_base_convert(b\'hello\', \'base16\', \'encode\') b\'68656C6C6F\' # Decoding \'68656C6C6F\' using Base16 >>> multi_base_convert(b\'68656C6C6F\', \'base16\', \'decode\') b\'hello\' ``` **Note:** Your implementation should appropriately handle invalid inputs and raise exceptions with meaningful messages. Constraints and Considerations - Ensure that your function handles all appropriate edge cases, such as invalid base selections and incorrect paddings. - Use built-in functions of the `base64` library: `b64encode`, `b64decode`, `b32encode`, `b32decode`, `b16encode`, and `b16decode`. Good luck!","solution":"import base64 def multi_base_convert(data: bytes, convert_to: str, mode: str, altchars: bytes = None) -> bytes: Encodes or decodes data using the specified base encoding scheme. Arguments: data -- the data to be encoded or decoded convert_to -- the base to convert to (\'base64\', \'base32\', \'base16\') mode -- the mode of conversion (\'encode\' or \'decode\') altchars -- an alternative alphabet for certain characters in Base64 encoding/decoding. Default is None. Returns: The converted data as bytes. if convert_to not in {\'base64\', \'base32\', \'base16\'}: raise ValueError(\\"Invalid value for convert_to. Must be \'base64\', \'base32\', or \'base16\'.\\") if mode not in {\'encode\', \'decode\'}: raise ValueError(\\"Invalid value for mode. Must be \'encode\' or \'decode\'.\\") if convert_to == \'base64\': if mode == \'encode\': return base64.b64encode(data, altchars) if altchars else base64.b64encode(data) elif mode == \'decode\': return base64.b64decode(data, altchars) if altchars else base64.b64decode(data) if convert_to == \'base32\': if mode == \'encode\': return base64.b32encode(data) elif mode == \'decode\': return base64.b32decode(data) if convert_to == \'base16\': if mode == \'encode\': return base64.b16encode(data) elif mode == \'decode\': return base64.b16decode(data)"},{"question":"**Challenging Question:** You have been tasked with verifying if a Python environment is set up correctly on various systems, ensuring consistency in configurations across installations. Write a Python function using the `sysconfig` module that performs the following checks: 1. **Determine the Python version** being used. 2. **Identify the platform** on which the Python interpreter is running. 3. **Check if the Python interpreter is built from source**. 4. **Retrieve and output the paths** for `stdlib`, `platlib`, `purelib`, and `scripts` based on the current default installation scheme. 5. **List the first five variables and their values** from the output of `sysconfig.get_config_vars()`. Function Signature: ```python def verify_python_setup(): Verify and output Python environment configuration details using the sysconfig module. Returns: dict: A dictionary containing keys and their respective verified details: \'python_version\': The Python version being used (string). \'platform\': The platform identifier (string). \'is_built_from_source\': Boolean indicating if Python was built from source. \'paths\': Dictionary containing paths for \'stdlib\', \'platlib\', \'purelib\', and \'scripts\'. \'config_vars\': Dictionary of the first five configuration variables and their values. pass ``` **Expected Functionality:** 1. The function determines the Python version using `sysconfig.get_python_version()`. 2. It identifies the current platform using `sysconfig.get_platform()`. 3. It checks if the Python interpreter is built from source with `sysconfig.is_python_build()`. 4. It retrieves paths for `stdlib`, `platlib`, `purelib`, and `scripts` using `sysconfig.get_paths()`. 5. It lists the first five configuration variables retrieved from `sysconfig.get_config_vars()`. **Constraints:** - Ensure that the function works across different platforms and installation schemes. - Handle cases where there may be fewer than five configuration variables. - Return the collected information in a well-structured dictionary. **Example Output:** ```python { \'python_version\': \'3.10\', \'platform\': \'macosx-10.15-x86_64\', \'is_built_from_source\': False, \'paths\': { \'stdlib\': \'/usr/local/lib/python3.10\', \'platlib\': \'/usr/local/lib/python3.10/site-packages\', \'purelib\': \'/usr/local/lib/python3.10/site-packages\', \'scripts\': \'/usr/local/bin\' }, \'config_vars\': { \'AR\': \'ar\', \'CXX\': \'g++\', \'AIX_GENUINE_CPLUSPLUS\': \'0\', \'ARFLAGS\': \'rc\', \'AC_APPLE_UNIVERSAL_BUILD\': \'0\' } } ``` Implement the function adhering to the above specifications and constraints.","solution":"import sysconfig def verify_python_setup(): Verify and output Python environment configuration details using the sysconfig module. Returns: dict: A dictionary containing keys and their respective verified details: \'python_version\': The Python version being used (string). \'platform\': The platform identifier (string). \'is_built_from_source\': Boolean indicating if Python was built from source. \'paths\': Dictionary containing paths for \'stdlib\', \'platlib\', \'purelib\', and \'scripts\'. \'config_vars\': Dictionary of the first five configuration variables and their values. # Determine the Python version python_version = sysconfig.get_python_version() # Identify the platform platform = sysconfig.get_platform() # Check if the Python interpreter is built from source is_built_from_source = sysconfig.is_python_build() # Retrieve paths for stdlib, platlib, purelib, and scripts based on the current default installation scheme paths = sysconfig.get_paths() stdlib = paths.get(\'stdlib\') platlib = paths.get(\'platlib\') purelib = paths.get(\'purelib\') scripts = paths.get(\'scripts\') # Retrieve the first five configuration variables and their values config_vars = {} all_config_vars = sysconfig.get_config_vars() config_vars_keys = list(all_config_vars.keys()) for key in config_vars_keys[:5]: config_vars[key] = all_config_vars[key] # Create the result dictionary result = { \'python_version\': python_version, \'platform\': platform, \'is_built_from_source\': is_built_from_source, \'paths\': { \'stdlib\': stdlib, \'platlib\': platlib, \'purelib\': purelib, \'scripts\': scripts }, \'config_vars\': config_vars } return result"},{"question":"# Pandas Data Indexing and Selection Exercise You are provided with a dataset in the form of a CSV file. Your task is to implement a function in Python that processes this dataset and extracts specific subsets of data using various pandas indexing and selection techniques. Function Signature ```python def process_dataset(file_path: str) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.DataFrame]: pass ``` Input - `file_path`: a string representing the path to the CSV file containing the dataset. Output A tuple containing: 1. **A DataFrame**: containing only the rows where the value in column `A` is greater than 0. 2. **A Series**: representing the values of column `B` for rows where column `C` has values between its 25th and 75th percentiles. 3. **A DataFrame**: containing the top 5 rows after sorting the DataFrame by column `D` in descending order, breaking ties using column `E` in ascending order. 4. **A DataFrame**: with duplicate rows removed based on columns `F` and `G`, keeping only the first occurrence. Constraints - The dataset is guaranteed to have columns `A`, `B`, `C`, `D`, `E`, `F`, and `G`. - Your solution should handle missing values appropriately. - Optimize for readable and efficient code. Example Usage ```python file_path = \\"data/dataset.csv\\" df1, series_b, df2, df_no_duplicates = process_dataset(file_path) print(df1) print(series_b) print(df2) print(df_no_duplicates) ``` ```python import pandas as pd from typing import Tuple def process_dataset(file_path: str) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.DataFrame]: # Load the dataset df = pd.read_csv(file_path) # 1. Filter rows where column \'A\' > 0 df1 = df[df[\'A\'] > 0] # 2. Extract column \'B\' for rows where \'C\' is between its 25th and 75th percentiles lower_bound = df[\'C\'].quantile(0.25) upper_bound = df[\'C\'].quantile(0.75) series_b = df[(df[\'C\'] > lower_bound) & (df[\'C\'] < upper_bound)][\'B\'] # 3. Sort by \'D\' descending, then \'E\' ascending, and select top 5 rows df2 = df.sort_values(by=[\'D\', \'E\'], ascending=[False, True]).head(5) # 4. Remove duplicate rows based on \'F\' and \'G\', keeping the first occurrence df_no_duplicates = df.drop_duplicates(subset=[\'F\', \'G\'], keep=\'first\') return df1, series_b, df2, df_no_duplicates ``` # Notes - Ensure that your code is efficient and handles edge cases well. - Comment your code where necessary to explain your logic. - Make use of pandas documentation for any functionality that you are unfamiliar with.","solution":"import pandas as pd from typing import Tuple def process_dataset(file_path: str) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.DataFrame]: Processes the dataset by performing various indexing and selection operations. Args: - file_path (str): Path to the CSV file. Returns: - Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.DataFrame]: The processed subsets of data. # Load the dataset df = pd.read_csv(file_path) # 1. Filter rows where column \'A\' > 0 df1 = df[df[\'A\'] > 0] # 2. Extract column \'B\' for rows where \'C\' is between its 25th and 75th percentiles lower_bound = df[\'C\'].quantile(0.25) upper_bound = df[\'C\'].quantile(0.75) series_b = df[(df[\'C\'] > lower_bound) & (df[\'C\'] < upper_bound)][\'B\'] # 3. Sort by \'D\' descending, then \'E\' ascending, and select top 5 rows df2 = df.sort_values(by=[\'D\', \'E\'], ascending=[False, True]).head(5) # 4. Remove duplicate rows based on \'F\' and \'G\', keeping the first occurrence df_no_duplicates = df.drop_duplicates(subset=[\'F\', \'G\'], keep=\'first\') return df1, series_b, df2, df_no_duplicates"},{"question":"Objective Demonstrate your understanding of the seaborn package\'s `seaborn.objects` interface by creating a meaningful and insightful visualization. Problem Statement Given a dataset on sea ice extent, you are required to create two visualizations using the seaborn.objects interface: 1. A simple line plot showing the sea ice extent over time. 2. An advanced multi-faceted line plot of the sea ice extent by day of the year, with facets by decade. You should load the dataset directly from seaborn, preprocess it as necessary, and create the visualizations according to the specifications below. Input You will use the built-in seaborn dataset named \\"seaice\\". There is no need for additional input besides loading this dataset. Output 1. **Simple Line Plot** - X-axis: Date - Y-axis: Extent 2. **Advanced Multi-Faceted Line Plot** - X-axis: Day of the year (extracted from the Date column) - Y-axis: Extent - Facet by decade (determined from the year extracted from the Date column) - Color lines by year - Use appropriate line width settings for both the main line and facet lines. - Include a custom title for each facet representing the decade. Expected Steps 1. Load the dataset \\"seaice\\" from seaborn. 2. Preprocess the data if necessary (e.g., extracting year and day of the year from the Date column). 3. Create the simple line plot. 4. Create the advanced multi-faceted line plot. Constraints - Use only the seaborn and pandas libraries for data loading, preprocessing, and plotting. - Ensure the plots are well-labeled and clean. Example Code Framework ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset seaice = load_dataset(\\"seaice\\") # Step 2: Preprocess the data # (Extract year and day of year from the Date column) # Step 3: Create a simple line plot simple_plot = so.Plot(seaice, x=\\"Date\\", y=\\"Extent\\").add(so.Lines()) simple_plot.show() # Step 4: Create an advanced multi-faceted line plot advanced_plot = ( so.Plot( x=seaice[\\"Date\\"].dt.day_of_year, y=seaice[\\"Extent\\"], color=seaice[\\"Date\\"].dt.year ) .facet(seaice[\\"Date\\"].dt.year.round(-1)) .add(so.Lines(linewidth=.5, color=\\"#bbca\\"), col=None) .add(so.Lines(linewidth=1)) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(8, 4)) .label(title=\\"{}s\\".format) ) advanced_plot.show() ``` Complete the preprocessing steps and ensure both plots are correctly implemented.","solution":"import seaborn.objects as so import seaborn as sns import pandas as pd # Step 1: Load the dataset seaice = sns.load_dataset(\\"seaice\\") # Step 2: Preprocess the data (Extract year and day of year from the Date column) seaice[\'year\'] = seaice[\'Date\'].dt.year seaice[\'day_of_year\'] = seaice[\'Date\'].dt.dayofyear seaice[\'decade\'] = (seaice[\'year\'] // 10) * 10 # Step 3: Create a simple line plot simple_plot = so.Plot(seaice, x=\\"Date\\", y=\\"Extent\\").add(so.Line()) simple_plot.show() # Step 4: Create an advanced multi-faceted line plot advanced_plot = ( so.Plot(seaice, x=\'day_of_year\', y=\'Extent\', color=\'year\') .facet(col=\'decade\') .add(so.Line(linewidth=0.5, color=\\".5\\"), col=None) .add(so.Line(linewidth=1)) .label(title=lambda val: f\\"{val}s\\") .scale(color=\\"viridis\\") ) advanced_plot.show()"},{"question":"Advanced Usage of Dataclasses in Python You are tasked with developing a Python application to manage a library\'s inventory of books. Each book should be represented as a dataclass with various attributes. Your solution needs to demonstrate a strong understanding of dataclasses, including the use of default values, frozen instances, post-initialization processing, and nested dataclasses. Requirements: 1. **Book Dataclass**: Create a `Book` dataclass with the following attributes: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `year_published`: An integer representing the year the book was published. - `isbn`: A string representing the book\'s International Standard Book Number (ISBN). - `quantity`: An integer representing the number of copies of the book available (default to 0). - `tags`: A list of strings representing tags associated with the book (default to an empty list, use default_factory). 2. **Library Dataclass**: Create a `Library` dataclass that contains: - `name`: A string representing the name of the library. - `books`: A list of `Book` objects representing the library\'s inventory (default to an empty list, use default_factory). - A method `add_book` that accepts a `Book` object and adds it to the inventory. - A method `get_books_by_author` that accepts an author\'s name and returns a list of books by that author. - A method `__post_init__` that ensures all book ISBNs in the inventory are unique and raises a `ValueError` if duplicates are found upon initialization. 3. **Frozen Dataclasses**: Create a `Member` dataclass with the following attributes (make the dataclass frozen): - `member_id`: A unique integer representing the member ID. - `name`: A string representing the member\'s name. - `books_checked_out`: A list of `Book` objects representing books checked out by the member (default to an empty list, use default_factory). 4. **Utility Functions**: - Write a function `dataclass_to_dict` that takes any dataclass instance and returns a dictionary representation of it using the `asdict` function. - Write a function `replace_book_quantity` that takes a `Book` object and an integer `new_quantity`. It should return a new `Book` instance with the updated quantity using the `replace` function. # Constraints: - You must use the `dataclasses` module in Python. - Ensure that `Library` enforces unique ISBNs through its `__post_init__` method. - For the `Member` dataclass, ensure immutability by setting `frozen=True`. # Example Usage: ```python from dataclasses import dataclass, field, asdict, replace, FrozenInstanceError from typing import List @dataclass class Book: title: str author: str year_published: int isbn: str quantity: int = 0 tags: List[str] = field(default_factory=list) @dataclass class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def get_books_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author] def __post_init__(self): isbns = [book.isbn for book in self.books] if len(isbns) != len(set(isbns)): raise ValueError(f\'Duplicate ISBNs found in library {self.name}\') @dataclass(frozen=True) class Member: member_id: int name: str books_checked_out: List[Book] = field(default_factory=list) def dataclass_to_dict(instance): return asdict(instance) def replace_book_quantity(book: Book, new_quantity: int) -> Book: return replace(book, quantity=new_quantity) # Example Usage b1 = Book(title=\\"1984\\", author=\\"George Orwell\\", year_published=1949, isbn=\\"1234567890\\", quantity=5) b2 = Book(title=\\"Animal Farm\\", author=\\"George Orwell\\", year_published=1945, isbn=\\"0987654321\\") library = Library(name=\\"City Library\\", books=[b1, b2]) # Add a book library.add_book(Book(title=\\"Brave New World\\", author=\\"Aldous Huxley\\", year_published=1932, isbn=\\"1122334455\\")) # Get books by author orwell_books = library.get_books_by_author(\\"George Orwell\\") # Convert dataclass to dict library_dict = dataclass_to_dict(library) # Replace book quantity new_b1 = replace_book_quantity(b1, new_quantity=10) # Create a member (frozen dataclass) m1 = Member(member_id=1, name=\\"John Doe\\") ``` Implement these classes and functions while adhering to the provided constraints and structure.","solution":"from dataclasses import dataclass, field, asdict, replace, FrozenInstanceError from typing import List @dataclass class Book: title: str author: str year_published: int isbn: str quantity: int = 0 tags: List[str] = field(default_factory=list) @dataclass class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book): if any(b.isbn == book.isbn for b in self.books): raise ValueError(f\'Duplicate ISBN {book.isbn} found in library {self.name}\') self.books.append(book) def get_books_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author] def __post_init__(self): isbns = [book.isbn for book in self.books] if len(isbns) != len(set(isbns)): raise ValueError(f\'Duplicate ISBNs found in library {self.name}\') @dataclass(frozen=True) class Member: member_id: int name: str books_checked_out: List[Book] = field(default_factory=list) def dataclass_to_dict(instance): return asdict(instance) def replace_book_quantity(book: Book, new_quantity: int) -> Book: return replace(book, quantity=new_quantity) # Example Usage b1 = Book(title=\\"1984\\", author=\\"George Orwell\\", year_published=1949, isbn=\\"1234567890\\", quantity=5) b2 = Book(title=\\"Animal Farm\\", author=\\"George Orwell\\", year_published=1945, isbn=\\"0987654321\\") library = Library(name=\\"City Library\\", books=[b1, b2]) # Add a book library.add_book(Book(title=\\"Brave New World\\", author=\\"Aldous Huxley\\", year_published=1932, isbn=\\"1122334455\\")) # Get books by author orwell_books = library.get_books_by_author(\\"George Orwell\\") # Convert dataclass to dict library_dict = dataclass_to_dict(library) # Replace book quantity new_b1 = replace_book_quantity(b1, new_quantity=10) # Create a member (frozen dataclass) m1 = Member(member_id=1, name=\\"John Doe\\")"},{"question":"You are required to implement a function that deals with encoding and decoding of a dictionary object using URL-safe Base64 encoding. function definitions: 1. `encode_dict_to_urlsafe_base64(my_dict: dict) -> str` 2. `decode_urlsafe_base64_to_dict(encoded_str: str) -> dict` Specifications and Constraints: - Your implementation should use the methods from the `base64` Python module. - The dictionary will only contain string keys and string values. - The encoded string should be URL-safe Base64 encoded. - The decoding function should return the original dictionary. - Handle the appropriate error raising if decoding fails due to invalid padding or non-alphabet characters. Example: ```python my_dict = {\\"name\\": \\"Alice\\", \\"age\\": \\"30\\", \\"city\\": \\"Wonderland\\"} encoded_str = encode_dict_to_urlsafe_base64(my_dict) # Example output: \\"eyJhZ2UiOiIzMCIsImNpdHkiOiJXb25kZXJsYW5kIiwibmFtZSI6IkFsaWNlIn0=\\" decoded_dict = decode_urlsafe_base64_to_dict(encoded_str) # Example output: {\\"name\\": \\"Alice\\", \\"age\\": \\"30\\", \\"city\\": \\"Wonderland\\"} ``` Notes: - Utilize `urlsafe_b64encode` and `urlsafe_b64decode` for encoding and decoding respectively. - To convert the dictionary to/from a string, make use of the `json` module for JSON serialization and deserialization. - Include appropriate error handling to ensure robustness. - Ensure your code is clean, well-commented, and modular for readability and maintainability. The encoded string should be URL-safe to avoid issues when included in URLs for web applications.","solution":"import base64 import json def encode_dict_to_urlsafe_base64(my_dict): Encodes a dictionary to a URL-safe Base64 string. Args: my_dict (dict): The dictionary to encode. Returns: str: The URL-safe Base64 encoded string. # Convert the dictionary to a JSON string json_str = json.dumps(my_dict) # Convert the JSON string to bytes json_bytes = json_str.encode(\'utf-8\') # Encode the bytes to URL-safe Base64 encoded_bytes = base64.urlsafe_b64encode(json_bytes) # Convert the encoded bytes back to a string encoded_str = encoded_bytes.decode(\'utf-8\') return encoded_str def decode_urlsafe_base64_to_dict(encoded_str): Decodes a URL-safe Base64 string back to a dictionary. Args: encoded_str (str): The URL-safe Base64 encoded string. Returns: dict: The decoded dictionary. try: # Convert the encoded string to bytes encoded_bytes = encoded_str.encode(\'utf-8\') # Decode the URL-safe Base64 bytes json_bytes = base64.urlsafe_b64decode(encoded_bytes) # Convert the bytes back to a JSON string json_str = json_bytes.decode(\'utf-8\') # Convert the JSON string back to a dictionary my_dict = json.loads(json_str) return my_dict except (base64.binascii.Error, json.JSONDecodeError) as e: raise ValueError(\\"Invalid encoded string\\")"},{"question":"# Task You are given a dataset consisting of mixed types of data, including categorical (e.g., \'city\'), textual (e.g., \'title\'), and numerical (e.g., \'expert_rating\', \'user_rating\') features. Your objective is to build a pipeline that preprocesses these features appropriately and then trains a regression model to predict a target variable, here denoted as \'rating\'. The dataset structure is as follows: ```plaintext | city | title | expert_rating | user_rating | rating | |----------|------------------------------|---------------|-------------|--------| | London | His Last Bow | 5 | 4 | 4.5 | | London | How Watson Learned the Trick | 3 | 5 | 4.0 | | Paris | A Moveable Feast | 4 | 4 | 4.3 | | Sallisaw | The Grapes of Wrath | 5 | 3 | 4.2 | ``` # Constraints 1. **Preprocessing**: - Encode the \'city\' column using one-hot encoding. - Convert the \'title\' column into a bag-of-words representation. - Normalize the \'expert_rating\' and \'user_rating\' columns using a standard scaler. 2. **Regression Model**: - Use a `LinearRegression` model to predict \'rating\'. # Steps to Follow 1. **Data Preparation**: Prepare the input dataset as specified. 2. **Preprocessing Pipeline**: - Use a `ColumnTransformer` to preprocess the different columns appropriately. - Combine the preprocessing steps using a `Pipeline`. 3. **Model Training**: - Fit the pipeline to the dataset and train the regression model. 4. **Grid Search**: Optimize the preprocessing parameters using `GridSearchCV`. # Input - A pandas DataFrame `df` structured as shown above, containing columns: \'city\', \'title\', \'expert_rating\', \'user_rating\', and \'rating\'. # Output - The best pipeline model found by the grid search. - Print the best parameters found by `GridSearchCV` and the score of the best model on a test set. # Example Code Structure ```python import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.feature_extraction.text import CountVectorizer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, GridSearchCV # Data Preparation data = { \'city\': [\'London\', \'London\', \'Paris\', \'Sallisaw\'], \'title\': [\\"His Last Bow\\", \\"How Watson Learned the Trick\\", \\"A Moveable Feast\\", \\"The Grapes of Wrath\\"], \'expert_rating\': [5, 3, 4, 5], \'user_rating\': [4, 5, 4, 3], \'rating\': [4.5, 4.0, 4.3, 4.2], } df = pd.DataFrame(data) # Define the input features and target X = df.drop(columns=[\'rating\']) y = df[\'rating\'] # Define the preprocessing pipeline preprocessor = ColumnTransformer( transformers=[ (\'city\', OneHotEncoder(), [\'city\']), (\'title\', CountVectorizer(), \'title\'), (\'num\', StandardScaler(), [\'expert_rating\', \'user_rating\']) ], remainder=\'drop\' ) # Define the full pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Define the parameter grid for GridSearchCV param_grid = { \'preprocessor__title__max_features\': [10, 20, 50], \'preprocessor__num__with_mean\': [True, False] } # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Conduct Grid Search grid_search = GridSearchCV(pipeline, param_grid, cv=3) grid_search.fit(X_train, y_train) # Output results print(f\\"Best Parameters: {grid_search.best_params_}\\") print(f\\"Test set score: {grid_search.score(X_test, y_test):.2f}\\") # Return the best pipeline best_pipeline = grid_search.best_estimator_ best_pipeline ``` # Expectations - Students should understand and correctly implement `Pipeline`, `ColumnTransformer`, and `GridSearchCV`. - Solution should handle data preprocessing, model fitting, and parameter tuning seamlessly. - Code readability and efficiency will be evaluated.","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.feature_extraction.text import CountVectorizer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, GridSearchCV def prepare_and_train_model(df): This function takes a DataFrame, preprocesses it, trains a regression model, and returns the best pipeline found by GridSearchCV. # Define the input features and target X = df.drop(columns=[\'rating\']) y = df[\'rating\'] # Define the preprocessing pipeline. preprocessor = ColumnTransformer( transformers=[ (\'city\', OneHotEncoder(), [\'city\']), (\'title\', CountVectorizer(), \'title\'), (\'num\', StandardScaler(), [\'expert_rating\', \'user_rating\']) ], remainder=\'drop\' ) # Define the full pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Define the parameter grid for GridSearchCV param_grid = { \'preprocessor__title__max_features\': [10, 20, 50], \'preprocessor__num__with_mean\': [True, False] } # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Conduct Grid Search grid_search = GridSearchCV(pipeline, param_grid, cv=3) grid_search.fit(X_train, y_train) # Output results print(f\\"Best Parameters: {grid_search.best_params_}\\") print(f\\"Test set score: {grid_search.score(X_test, y_test):.2f}\\") # Return the best pipeline return grid_search.best_estimator_"},{"question":"# Functional Programming Challenge Using Python Modules You are tasked with implementing a solution that leverages the `itertools`, `functools`, and `operator` modules to process and analyze a dictionary of student grades. Problem Statement You are given a dictionary where the keys are student names and the values are lists of integers representing grades. Your task is to compute various statistics and provide functionality as described below: 1. **Average Grade**: Write a function `average_grade` that computes the average grade for each student. 2. **Top n Grades**: Write a function `top_n_grades` that uses `itertools` to extract the top n grades across all students. 3. **Grade Normalizer**: Write a function `normalize_grades` that uses `functools.partial` to create a normalized grading function. The normalized grade will be computed by applying the following formula: [ text{normalized grade} = frac{(text{actual grade} - text{min grade})}{(text{max grade} - text{min grade})} times 100 ] 4. **Grade Comparisons**: Write a function `compare_grades` that takes two grades and returns: - `-1` if the first grade is less - `0` if they are equal - `1` if the first grade is greater Use `operator` module functions for this implementation. Function Definitions and Constraints - `average_grade(grades: Dict[str, List[int]]) -> Dict[str, float]` - **Input**: A dictionary where keys are student names and values are lists of grades. - **Output**: A dictionary where keys are student names and values are their average grades. - **Example**: ```python input: {\\"Alice\\": [80, 90], \\"Bob\\": [70, 85, 90]} output: {\\"Alice\\": 85.0, \\"Bob\\": 81.67} ``` - `top_n_grades(grades: Dict[str, List[int]], n: int) -> List[int]` - **Input**: A dictionary where keys are student names and values are lists of grades, and an integer `n`. - **Output**: A list of top n grades across all students. - **Example**: ```python input: {\\"Alice\\": [80, 90], \\"Bob\\": [70, 85, 90]}, 3 output: [90, 90, 85] ``` - `normalize_grades(min_grade: int, max_grade: int, grade: int) -> int` - **Input**: An integer `min_grade`, an integer `max_grade`, and the actual `grade` to normalize. - **Output**: The normalized grade. - **Example**: ```python normalize_grades(50, 100, 75) output: 50.0 ``` - `compare_grades(grade1: int, grade2: int) -> int` - **Input**: Two integers `grade1` and `grade2`. - **Output**: -1 if the first grade is less, 0 if they are equal, or 1 if the first grade is greater. - **Example**: ```python compare_grades(85, 90) output: -1 ``` # Note - Ensure your functions are well-tested with edge cases. - Use the `itertools`, `functools`, and `operator` modules effectively to demonstrate your understanding. - Optimize your solution for performance where possible.","solution":"import itertools import functools import operator def average_grade(grades): Computes the average grade for each student. :param grades: Dict[str, List[int]] - A dictionary where keys are student names and values are lists of grades. :return: Dict[str, float] - A dictionary where keys are student names and values are their average grades. return {student: sum(g) / len(g) if g else 0 for student, g in grades.items()} def top_n_grades(grades, n): Extracts the top n grades across all students. :param grades: Dict[str, List[int]] - A dictionary where keys are student names and values are lists of grades. :param n: int - The number of top grades to return. :return: List[int] - A list of top n grades. all_grades = list(itertools.chain(*grades.values())) return sorted(all_grades, reverse=True)[:n] def normalize_grades(min_grade, max_grade, grade): Normalizes a grade based on min and max grades. :param min_grade: int - The minimum grade. :param max_grade: int - The maximum grade. :param grade: int - The actual grade to normalize. :return: float - The normalized grade. return (grade - min_grade) / (max_grade - min_grade) * 100 def compare_grades(grade1, grade2): Compares two grades. :param grade1: int - The first grade. :param grade2: int - The second grade. :return: int - -1 if the first grade is less, 0 if they are equal, 1 if the first grade is greater. return operator.sub(grade1, grade2) and (1, -1)[operator.lt(grade1, grade2)] or 0"},{"question":"Objective: You are given a dataset and your task is to: 1. Fit a machine learning model using `scikit-learn`. 2. Plot both the validation curve and the learning curve for the model. 3. Analyze the results and draw conclusions regarding the model\'s bias and variance. Instructions: 1. Load the IRIS dataset using `sklearn.datasets.load_iris`. 2. Shuffle the dataset to ensure the data is randomly distributed. 3. Implement a **Support Vector Machine (SVM)** model with a linear kernel. 4. Use the `validation_curve` method to plot the validation curve for the `C` hyperparameter of the SVM, ranging from (10^{-7}) to (10^3) in logarithmic steps. 5. Use the `learning_curve` method to plot the learning curve for the SVM model, with training sizes of 50, 80, and 110. 6. Provide a detailed analysis of the validation and learning curves, discussing the model\'s bias and variance. Expected Input and Output: - **Input**: IRIS dataset (no additional input required from the user). - **Output**: Plots of the validation curve and learning curve, accompanied by written analysis of the plots. Constraints: - Ensure the code properly shuffles the dataset before training. - The plots should be clearly labeled with appropriate titles and legends. - The analysis should specifically discuss the implications of the observed curves on the model\'s bias and variance. Example: Below is an example structure for your implementation. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.utils import shuffle # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Plot validation curve param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5) plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\'Training Score\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\'Validation Score\') plt.xscale(\'log\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.title(\'Validation Curve\') plt.legend(loc=\'best\') plt.show() # Plot learning curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=[50, 80, 110], cv=5) plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\'Training Score\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\'Validation Score\') plt.xlabel(\'Training Sizes\') plt.ylabel(\'Score\') plt.title(\'Learning Curve\') plt.legend(loc=\'best\') plt.show() # Analysis (write your detailed analysis here) ``` Analysis: - Discuss the bias and variance trade-off observed in the validation curve for different values of the `C` parameter. - Evaluate the learning curve to determine if the model would benefit from more training data, and discuss any signs of underfitting or overfitting.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.utils import shuffle def plot_validation_curve(): # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Plot validation curve param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5) plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\'Training Score\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\'Validation Score\') plt.xscale(\'log\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.title(\'Validation Curve\') plt.legend(loc=\'best\') plt.show() def plot_learning_curve(): # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Plot learning curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=[50, 80, 110], cv=5) plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\'Training Score\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\'Validation Score\') plt.xlabel(\'Training Sizes\') plt.ylabel(\'Score\') plt.title(\'Learning Curve\') plt.legend(loc=\'best\') plt.show() def main(): plot_validation_curve() plot_learning_curve() main()"},{"question":"**Objective:** Demonstrate your understanding of pandas\' options and settings by performing specific configuration tasks. This question will assess your ability to manage pandas\' global behavior settings effectively. **Instructions:** 1. Write a function `configure_pandas()` that performs the following tasks: - Sets the option to display all columns when displaying a DataFrame. - Sets the floating-point precision to 3 decimal places. - Sets the format for scientific notation to use engineering notation. - Retrieves and returns the current value of each of the above options in a dictionary. 2. Write a function `reset_pandas()` that resets the options set in `configure_pandas()` to their default values. **Function Signatures:** ```python def configure_pandas() -> dict: pass def reset_pandas() -> None: pass ``` **Input Format:** - The functions do not take any input. **Output Format:** - `configure_pandas`: Returns a dictionary with the current values of the options after setting them. ```python { \\"display.max_columns\\": \\"current_value\\", \\"display.float_format\\": \\"current_value\\", \\"display.notebook_repr_html\\": \\"current_value\\" } ``` - `reset_pandas`: Does not return anything. **Example:** ```python result = configure_pandas() print(result) # Example Output: # { # \\"display.max_columns\\": None, # \\"display.float_format\\": \\"<function>\\", # \\"display.notebook_repr_html\\": True # } reset_pandas() ``` **Constraints:** - Assume the default settings of pandas during the execution of these functions. - Focus on maintaining code readability and proper usage of the pandas options API. **Note:** - Utilize the Pandas functions: `set_option`, `get_option`, `reset_option`, and `option_context` to complete the task.","solution":"import pandas as pd def configure_pandas() -> dict: Configures specific pandas display options and returns the current values of these options. pd.set_option(\'display.max_columns\', None) # Display all columns pd.set_option(\'display.precision\', 3) # Set float precision to 3 decimal places pd.set_option(\'display.float_format\', \'{:.3f}\'.format) # Use engineering notation for floats # Retrieve and return current settings settings = { \\"display.max_columns\\": pd.get_option(\'display.max_columns\'), \\"display.precision\\": pd.get_option(\'display.precision\'), \\"display.float_format\\": pd.get_option(\'display.float_format\') } return settings def reset_pandas() -> None: Resets the pandas display options set in configure_pandas() to their default values. pd.reset_option(\'display.max_columns\') pd.reset_option(\'display.precision\') pd.reset_option(\'display.float_format\')"},{"question":"# Coding Assessment: Advanced Usage of Seaborn `kdeplot` **Objective:** This assessment aims to evaluate your understanding and ability to utilize the Seaborn library for advanced data visualization, focusing on the `kdeplot` function. **Problem Statement:** You are provided with the `geyser` dataset, which contains information about geyser eruptions. Your task is to write code to perform the following steps: 1. **Load the Dataset:** Load the `geyser` dataset using Seaborn\'s `load_dataset` function. 2. **Univariate Distribution:** Plot the univariate distribution of the `waiting` column using the `kdeplot` function. Adjust the smoothing parameter to `bw_adjust=0.5` to reduce the bandwidth. 3. **Bivariate Distribution:** Create a bivariate distribution plot of the `waiting` and `duration` columns. Add a hue mapping for the `kind` column to show conditional distributions based on `kind`. 4. **Appearance Modification:** Improve the appearance of the bivariate distribution by adding filled contours, using `(fill=True)` with a custom color palette of your choice. Set the `thresh` parameter to `0.1` and `levels` to `10`. **Input Format:** The input format specifies the following: - No direct input is required since you will be using the provided dataset and columns. **Output Format:** The expected output is a pair of plots: - A univariate distribution plot of the `waiting` column. - A bivariate distribution plot of `waiting` and `duration` columns with hue mapping and modified appearance. **Constraints:** - Ensure that the plots are displayed using Seaborn. - Use appropriate labels and titles for the plots. **Sample Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = sns.load_dataset(\\"geyser\\") # Step 2: Univariate Distribution plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) sns.kdeplot(data=data, x=\\"waiting\\", bw_adjust=0.5) plt.title(\\"Univariate Distribution of Waiting\\") # Step 3: Bivariate Distribution with Hue Mapping plt.subplot(1, 2, 2) sns.kdeplot(data=data, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, thresh=0.1, levels=10, palette=\\"viridis\\") plt.title(\\"Bivariate Distribution with Hue Mapping\\") # Display the plots plt.tight_layout() plt.show() ``` Ensure to follow the sample code structure while solving the problem and make any adjustments needed to make the plots visually informative and appealing.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = sns.load_dataset(\\"geyser\\") # Step 2: Univariate Distribution plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) sns.kdeplot(data=data, x=\\"waiting\\", bw_adjust=0.5) plt.title(\\"Univariate Distribution of Waiting\\") plt.xlabel(\\"Waiting Time\\") plt.ylabel(\\"Density\\") # Step 3: Bivariate Distribution with Hue Mapping plt.subplot(1, 2, 2) sns.kdeplot(data=data, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, thresh=0.1, levels=10, palette=\\"viridis\\") plt.title(\\"Bivariate Distribution with Hue Mapping\\") plt.xlabel(\\"Waiting Time\\") plt.ylabel(\\"Duration\\") # Display the plots plt.tight_layout() plt.show()"},{"question":"Custom Color Palettes with Seaborn **Objective:** Your task is to write a function that generates different types of color palettes using seaborn\'s `cubehelix_palette` function. You will then create a plot using one of these palettes. **Instructions:** 1. **Function Implementation:** Implement a function `generate_cubehelix_palettes` that takes the following parameters: - `n_colors` (int): Number of colors in the palette. - `start` (float): The starting position of the helix (cycle). - `rot` (float): The number of rotations in the helix. - `hue` (float): The saturation of the colors. - `gamma` (float): The nonlinearity in the brightness. - `dark` (float): The dark value of the luminance ramp. - `light` (float): The light value of the luminance ramp. - `reverse` (bool): If True, reverse the luminance ramp. - `as_cmap` (bool): If True, return a continuous colormap instead of a discrete palette. The function should return the generated color palette. 2. **Plot Implementation:** Create a function `plot_with_palette` that: - Takes a seaborn color palette as its parameter. - Generates and displays a scatter plot using seaborn\'s `scatterplot` function, with the `hue` parameter set to a continuous variable and the color palette as the colormap. 3. **Main Function:** Implement a `main` function that: - Uses `generate_cubehelix_palettes` to generate a color palette with your chosen parameters. - Utilizes `plot_with_palette` to plot a scatter plot using the generated palette. **Expected Input and Output:** ```python def generate_cubehelix_palettes(n_colors, start, rot, hue, gamma, dark, light, reverse, as_cmap): # Implementation here. def plot_with_palette(palette): # Implementation here. def main(): # Example parameters palette = generate_cubehelix_palettes( n_colors=10, start=0.5, rot=1.0, hue=1.2, gamma=0.8, dark=0.2, light=0.8, reverse=False, as_cmap=False ) plot_with_palette(palette) if __name__ == \\"__main__\\": main() ``` **Constraints:** - Ensure the number of colors (`n_colors`) is a positive integer. - Parameters such as `start`, `rot`, `hue`, `gamma`, `dark`, and `light` should be floats within a reasonable range (0 to 3 for `start` and `rot`, 0 to 2 for `hue`, 0.1 to 2 for `gamma`, 0 to 1 for `dark` and `light`). - Use appropriate error handling for invalid inputs. **Bonus:** - Create additional variations of plots using different types of datasets available in seaborn to further demonstrate your understanding of applying color palettes.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_cubehelix_palettes(n_colors, start, rot, hue, gamma, dark, light, reverse, as_cmap): Generates a cubehelix palette with the given parameters. Parameters: - n_colors (int): Number of colors in the palette. - start (float): The starting position of the helix (cycle). - rot (float): The number of rotations in the helix. - hue (float): The saturation of the colors. - gamma (float): The nonlinearity in the brightness. - dark (float): The dark value of the luminance ramp. - light (float): The light value of the luminance ramp. - reverse (bool): If True, reverse the luminance ramp. - as_cmap (bool): If True, return a continuous colormap instead of a discrete palette. Returns: - The generated color palette. if not (isinstance(n_colors, int) and n_colors > 0): raise ValueError(\\"n_colors must be a positive integer.\\") for attr in [start, rot, hue, gamma, dark, light]: if not isinstance(attr, (int, float)): raise ValueError(\\"start, rot, hue, gamma, dark, light must be int or float.\\") if not isinstance(reverse, bool) or not isinstance(as_cmap, bool): raise ValueError(\\"reverse and as_cmap must be boolean values.\\") palette = sns.cubehelix_palette( n_colors=n_colors, start=start, rot=rot, hue=hue, gamma=gamma, dark=dark, light=light, reverse=reverse, as_cmap=as_cmap ) return palette def plot_with_palette(palette): Plots a scatter plot using the provided color palette. Parameters: - palette: A seaborn color palette. tips = sns.load_dataset(\\"tips\\") plt.figure(figsize=(8, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", palette=palette) plt.show() def main(): palette = generate_cubehelix_palettes( n_colors=10, start=0.5, rot=1.0, hue=1.2, gamma=0.8, dark=0.2, light=0.8, reverse=False, as_cmap=False ) plot_with_palette(palette) if __name__ == \\"__main__\\": main()"},{"question":"Objective To assess your understanding of Python\'s garbage collector interface (`gc` module) and your ability to manipulate it for memory management and debugging purposes. Problem Statement You are to implement a diagnostic function that inspects the state of the garbage collector and produces a summary report. This report should include the following: 1. **Garbage Collection Status**: - Whether the garbage collector is currently enabled or disabled. 2. **Collection Statistics**: - A summary of the collection statistics for each generation. 3. **Tracked Objects**: - The total number of objects currently tracked by the garbage collector. - The number of objects tracked in each generation. 4. **Uncollectible Objects**: - A list of all uncollectible objects currently stored in `gc.garbage`. Your function should be named `gc_diagnostics`, and return a dictionary containing all the above information. Function Signature ```python def gc_diagnostics() -> dict: ``` Expected Output The function should return a dictionary with the following structure: ```python { \\"enabled\\": bool, \\"stats\\": { \\"generation_0\\": {\\"collections\\": int, \\"collected\\": int, \\"uncollectable\\": int}, \\"generation_1\\": {\\"collections\\": int, \\"collected\\": int, \\"uncollectable\\": int}, \\"generation_2\\": {\\"collections\\": int, \\"collected\\": int, \\"uncollectable\\": int} }, \\"tracked_objects\\": { \\"total\\": int, \\"generation_0\\": int, \\"generation_1\\": int, \\"generation_2\\": int }, \\"garbage\\": list } ``` Constraints - Use only the functions and variables provided in the `gc` module. - The function should not trigger a manual garbage collection during its execution to measure the current state accurately. Example Usage ```python if __name__ == \\"__main__\\": report = gc_diagnostics() print(report) ``` This function should demonstrate your understanding of the `gc` module\'s capabilities and your ability to extract useful diagnostic information about the current state of the garbage collector.","solution":"import gc def gc_diagnostics() -> dict: Returns a diagnostic report of the current state of the garbage collector. # Check if garbage collection is enabled enabled = gc.isenabled() # Retrieve the collection statistics stats = { f\\"generation_{i}\\": { \\"collections\\": gc.get_count()[i], \\"collected\\": gc.get_stats()[i][\\"collected\\"], \\"uncollectable\\": gc.get_stats()[i][\\"uncollectable\\"] } for i in range(gc.get_count().__len__() ) } # Retrieve the number of objects tracked by the garbage collector tracked_objects = { f\\"generation_{i}\\": len(gc.get_objects(i)) for i in range(gc.get_count().__len__()) } tracked_objects[\\"total\\"] = sum(tracked_objects.values()) # Retrieve the list of uncollectible objects garbage = gc.garbage return { \\"enabled\\": enabled, \\"stats\\": stats, \\"tracked_objects\\": tracked_objects, \\"garbage\\": garbage }"},{"question":"# Python Coding Assessment Question **Objective**: Demonstrate understanding of Python generators and their advanced implementations. **Problem Statement**: Generators are a special class of iterators in Python which allow you to iterate through a sequence of values produced dynamically. Write a Python function that will generate an infinite sequence of prime numbers using generators. Your implementation should efficiently find the next prime number each time the generator is invoked. **Function Signature**: ```python def prime_generator(): Generates an infinite sequence of prime numbers. pass ``` **Your `prime_generator` function should:** - Yield an infinite sequence of prime numbers. - Efficiently compute the next prime number on-demand. **Constraints**: - You should not use libraries like `sympy` to find prime numbers. - Optimize your solution to avoid unnecessary computations. **Example Usage**: ```python gen = prime_generator() print(next(gen)) # Output: 2 print(next(gen)) # Output: 3 print(next(gen)) # Output: 5 print(next(gen)) # Output: 7 # and so on... ``` **Hints**: - You may use helper functions to determine if a number is a prime. - Consider storing the primes you detect in a list to avoid re-evaluating for primality. Make sure your code is well-documented and handles edge cases appropriately.","solution":"def is_prime(number): Return True if number is a prime, else False. if number <= 1: return False if number <= 3: return True if number % 2 == 0 or number % 3 == 0: return False i = 5 while i * i <= number: if number % i == 0 or number % (i + 2) == 0: return False i += 6 return True def prime_generator(): Generates an infinite sequence of prime numbers. number = 2 while True: if is_prime(number): yield number number += 1"},{"question":"**Objective**: Demonstrate your ability to manipulate HTTP cookies using the `http.cookies` module in Python. **Problem Statement**: You are developing a web application, and you need to handle various cookie operations. Implement a class `CookieManager` that uses the `http.cookies.SimpleCookie` class to manage cookies. Your class should provide functionalities to: 1. **Add a Cookie**: Add a new cookie with a given name and value. 2. **Update Cookie Attributes**: Update attributes (`expires`, `path`, `domain`, `secure`, `httponly`, `samesite`) of a specific cookie. 3. **Delete a Cookie**: Remove a cookie by its name. 4. **Load Cookies from Header**: Load cookies from an HTTP header string. 5. **Generate HTTP Header**: Output the cookies as an HTTP header string. **Class Definition**: ```python from http.cookies import SimpleCookie, CookieError class CookieManager: def __init__(self): Initialize an empty SimpleCookie object. self.cookie = SimpleCookie() def add_cookie(self, name: str, value: str): Add a cookie with the given name and value. self.cookie[name] = value def update_cookie_attributes(self, name: str, attributes: dict): Update the attributes of the specified cookie. Raise CookieError if the cookie does not exist. if name not in self.cookie: raise CookieError(\\"Cookie not found\\") for attr, val in attributes.items(): self.cookie[name][attr] = val def delete_cookie(self, name: str): Delete the cookie with the given name. if name in self.cookie: del self.cookie[name] else: raise CookieError(\\"Cookie not found\\") def load_from_header(self, header: str): Load cookies from the given HTTP header string. self.cookie.load(header) def generate_http_header(self) -> str: Generate and return the HTTP header string for the cookies. return self.cookie.output() ``` **Input/Output**: - **add_cookie**: - Input: `name` (str), `value` (str) - Example: `add_cookie(\'session\', \'abc123\')` - Output: None - **update_cookie_attributes**: - Input: `name` (str), `attributes` (dict) - Example: `update_cookie_attributes(\'session\', {\'path\': \'/\', \'secure\': True})` - Output: None - **delete_cookie**: - Input: `name` (str) - Example: `delete_cookie(\'session\')` - Output: None - **load_from_header**: - Input: `header` (str) - Example: `load_from_header(\'session=abc123; path=/\')` - Output: None - **generate_http_header**: - Input: None - Output: HTTP header string - Example: `\'Set-Cookie: session=abc123; Path=/\'` **Constraints**: - All cookie names and values are non-empty strings. - Attributes dictionary keys and values should follow RFC 2109 standards. **Function Demonstrations**: Ensure that your `CookieManager` class behaves correctly by demonstrating the following: 1. Add a cookie named \'user\' with value \'john_doe\'. 2. Update the \'user\' cookie with attributes `{\'path\': \'/\', \'httponly\': True}`. 3. Generate the HTTP header string and print it. 4. Load cookies from the header string \'session=xyz789; path=/admin\'. 5. Add another cookie named \'token\' with value \'qwerty123\' and print the updated HTTP header string. **Note**: Make sure to catch and handle any `CookieError` exceptions appropriately.","solution":"from http.cookies import SimpleCookie, CookieError class CookieManager: def __init__(self): Initialize an empty SimpleCookie object. self.cookie = SimpleCookie() def add_cookie(self, name: str, value: str): Add a cookie with the given name and value. self.cookie[name] = value def update_cookie_attributes(self, name: str, attributes: dict): Update the attributes of the specified cookie. Raise CookieError if the cookie does not exist. if name not in self.cookie: raise CookieError(\\"Cookie not found\\") for attr, val in attributes.items(): self.cookie[name][attr] = val def delete_cookie(self, name: str): Delete the cookie with the given name. if name in self.cookie: del self.cookie[name] else: raise CookieError(\\"Cookie not found\\") def load_from_header(self, header: str): Load cookies from the given HTTP header string. self.cookie.load(header) def generate_http_header(self) -> str: Generate and return the HTTP header string for the cookies. return self.cookie.output()"},{"question":"# Advanced FTP Client Implementation You are required to create an FTP client using Python\'s `ftplib` module that can log in to an FTP server, navigate directories, upload and download files, handle errors, and ensure secure data transmission using FTP_TLS. Implement the following functions: 1. **connect_to_ftp_server(host, user, passwd, secure=False)**: - Establish a connection to the given FTP server. - If `secure` is `True`, use `FTP_TLS` class to ensure a secure connection. - Log in to the server with the provided credentials. - Return the FTP or FTP_TLS connection object. - Raise appropriate exceptions if the connection or login fails. 2. **upload_file(ftp_connection, file_path, target_directory)**: - Upload the file from `file_path` to the specified `target_directory` on the FTP server. - Ensure binary transfer mode is used for uploading. - Raise appropriate exceptions if the upload fails. 3. **download_file(ftp_connection, file_name, target_directory, local_path)**: - Download a file named `file_name` from the `target_directory` on the FTP server to the specified `local_path` on the local machine. - Ensure binary transfer mode is used for downloading. - Raise appropriate exceptions if the download fails. 4. **navigate_directory(ftp_connection, directory)**: - Change the current working directory to the specified `directory` on the FTP server. - Return the updated current directory path. - Raise appropriate exceptions if navigating to the directory fails. 5. **close_connection(ftp_connection)**: - Close the connection to the FTP server gracefully. - Ensure that the connection is closed without any errors. # Constraints: - You may assume that the server details and file paths provided as input are valid and exist. - Use appropriate FTP methods from the `ftplib` module to perform the operations. - Ensure all exceptions specific to FTP operations are properly handled and raised with meaningful error messages. # Example Usage: ```python host = \'ftp.example.com\' user = \'your_username\' passwd = \'your_password\' local_path = \'local_file.txt\' file_path = \'remote_file.txt\' target_directory = \'/remote_directory/\' # Establish a secure connection to the FTP server ftp_conn = connect_to_ftp_server(host, user, passwd, secure=True) # Navigate to the desired directory navigate_directory(ftp_conn, target_directory) # Upload a file upload_file(ftp_conn, local_path, target_directory) # Download a file download_file(ftp_conn, \'remote_file.txt\', target_directory, local_path) # Close the connection close_connection(ftp_conn) ``` Ensure your implementation is efficient and handles all possible FTP-related errors gracefully.","solution":"from ftplib import FTP, FTP_TLS, error_perm import os def connect_to_ftp_server(host, user, passwd, secure=False): Establish a connection to the FTP server. :param host: FTP server host :param user: FTP server username :param passwd: FTP server password :param secure: If True, use FTP_TLS for secure connection :return: FTP or FTP_TLS connection object :raises: Exception if the connection or login fails try: if secure: ftp_conn = FTP_TLS(host) ftp_conn.login(user=user, passwd=passwd) ftp_conn.prot_p() else: ftp_conn = FTP(host) ftp_conn.login(user=user, passwd=passwd) return ftp_conn except Exception as e: raise Exception(f\\"Failed to connect or log in: {str(e)}\\") def upload_file(ftp_connection, file_path, target_directory): Upload a file to the FTP server. :param ftp_connection: FTP or FTP_TLS connection object :param file_path: Path to the file to be uploaded :param target_directory: Directory on the FTP server where the file should be uploaded :raises: Exception if the upload fails try: with open(file_path, \'rb\') as file: ftp_connection.cwd(target_directory) ftp_connection.storbinary(f\'STOR {os.path.basename(file_path)}\', file) except Exception as e: raise Exception(f\\"Failed to upload file: {str(e)}\\") def download_file(ftp_connection, file_name, target_directory, local_path): Download a file from the FTP server. :param ftp_connection: FTP or FTP_TLS connection object :param file_name: Name of the file to be downloaded :param target_directory: Directory on the FTP server where the file is located :param local_path: Local path to save the downloaded file :raises: Exception if the download fails try: local_file_path = os.path.join(local_path, file_name) with open(local_file_path, \'wb\') as local_file: ftp_connection.cwd(target_directory) ftp_connection.retrbinary(f\'RETR {file_name}\', local_file.write) except Exception as e: raise Exception(f\\"Failed to download file: {str(e)}\\") def navigate_directory(ftp_connection, directory): Change the current working directory on the FTP server. :param ftp_connection: FTP or FTP_TLS connection object :param directory: Directory to navigate to :return: Updated current directory path :raises: Exception if navigating to the directory fails try: ftp_connection.cwd(directory) return ftp_connection.pwd() except Exception as e: raise Exception(f\\"Failed to navigate to directory: {str(e)}\\") def close_connection(ftp_connection): Close the connection to the FTP server gracefully. :param ftp_connection: FTP or FTP_TLS connection object :raises: Exception if closing the connection fails try: ftp_connection.quit() except Exception as e: raise Exception(f\\"Failed to close the connection: {str(e)}\\")"},{"question":"Coding Assessment Question # Objective Your task is to create a sequential color palette using seaborn and integrate it into a scatter plot of some given data. # Question Given a CSV file with columns `x`, `y`, and `category`, implement a function `create_custom_palette_plot(csv_file_path: str, color: str, n_colors: int, as_cmap: bool) -> None` that: 1. Loads the data from the CSV file. 2. Creates a sequential color palette using seaborn\'s `light_palette` based on the specified `color`. 3. Plots a scatter plot of the `x` and `y` columns, with the color determined by the `category` column using the created palette. 4. Saves the plot to a file named `custom_palette_plot.png`. # Input - `csv_file_path` (str): The file path to the CSV file containing the data. - `color` (str): The base color for the sequential palette. It can be a named color or a hex code. - `n_colors` (int): The number of colors in the palette. - `as_cmap` (bool): Whether to return a continuous colormap (True) or a discrete palette (False). # Output - The function should save the generated plot to a file named `custom_palette_plot.png`. # Example Given a CSV file `example_data.csv` with the following content: ``` x,y,category 1,2,A 2,3,B 3,4,A 4,5,B 5,6,A ``` Calling the function: ```python create_custom_palette_plot(\\"example_data.csv\\", \\"seagreen\\", 5, False) ``` Should create and save a scatter plot using a sequential palette based on \\"seagreen\\" with 5 discrete colors. # Constraints - Use seaborn\'s `light_palette` function to create the palette. - Ensure the plot is readable, with appropriate labels and a legend. # Note - Ensure proper error handling for file-related operations and invalid inputs. - You are free to use other libraries like pandas for data handling and matplotlib for additional customization, if needed.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_plot(csv_file_path: str, color: str, n_colors: int, as_cmap: bool) -> None: Loads data from a CSV file and creates a scatter plot with a custom color palette. Parameters: - csv_file_path: str - The file path to the CSV file containing the data. - color: str - The base color for the sequential palette. - n_colors: int - The number of colors in the palette. - as_cmap: bool - Whether to return a continuous colormap or a discrete palette. Output: - Saves the plot to \'custom_palette_plot.png\'. # Load the data df = pd.read_csv(csv_file_path) # Create the sequential color palette palette = sns.light_palette(color, n_colors=n_colors, as_cmap=as_cmap) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter = sns.scatterplot( data=df, x=\'x\', y=\'y\', hue=\'category\', palette=palette ) # Customize plot plt.title(\'Custom Palette Scatter Plot\') plt.xlabel(\'x\') plt.ylabel(\'y\') plt.legend(title=\'Category\') plt.grid(True) # Save the plot plt.savefig(\'custom_palette_plot.png\') plt.close()"},{"question":"# Task Description In this task, you are required to use the `concurrent.futures` module to perform parallel data processing. The goal is to concurrently process elements of a list using multiple threads and handle any exceptions that may arise from the processing functions. # Problem Statement You are provided with a list of integers. Your task is to write a function `process_numbers(numbers: List[int], worker_function: Callable[[int], int]) -> List[Optional[int]]`. This function should do the following: 1. **Use ThreadPoolExecutor** to create a pool of threads to process the numbers in parallel. 2. **Submit tasks** to the thread pool, where each task involves applying the `worker_function` to an element of the list. 3. **Collect results** from all the futures, ensuring that if a task raises an exception, the result for that task should be `None`. 4. **Return a list** of the results, maintaining the order of the input list. # Input Format - `numbers`: A list of integers, e.g., `[1, 2, 3, 4, 5]`. - `worker_function`: A callable that takes an integer and returns an integer. It might raise an exception for certain inputs. # Output Format - A list of integers and Nones. The list should have the same length as the input list, where each element is the result of applying the `worker_function` to the corresponding element in the input list. If the `worker_function` raises an exception for a specific element, the result for that element should be `None`. # Constraints - The length of the `numbers` list will be at most 10,000. - The `worker_function` may raise exceptions such as `ValueError`. # Example ```python from typing import List, Callable, Optional import concurrent.futures def process_numbers(numbers: List[int], worker_function: Callable[[int], int]) -> List[Optional[int]]: # Your implementation here # Example worker function that raises an exception for even numbers def example_worker_function(n: int) -> int: if n % 2 == 0: raise ValueError(\\"Even number!\\") return n * 2 numbers = [1, 2, 3, 4, 5] print(process_numbers(numbers, example_worker_function)) # Expected Output: [2, None, 6, None, 10] ``` # Additional Requirements 1. Ensure that the function handles exceptions gracefully and continues execution. 2. The function should use `ThreadPoolExecutor` from the `concurrent.futures` module. 3. Provide appropriate comments and documentation within your function to explain the logic. # Performance - Your solution should leverage parallel processing to improve performance, especially for the upper limit of 10,000 integers.","solution":"from typing import List, Callable, Optional import concurrent.futures def process_numbers(numbers: List[int], worker_function: Callable[[int], int]) -> List[Optional[int]]: def wrapper(n): try: return worker_function(n) except Exception: return None results = [] with concurrent.futures.ThreadPoolExecutor() as executor: futures = [executor.submit(wrapper, number) for number in numbers] for future in concurrent.futures.as_completed(futures): results.append(future.result()) # Ensure that results maintain the input order ordered_results = [result.result() for result in futures] return ordered_results # Example worker function that raises an exception for even numbers def example_worker_function(n: int) -> int: if n % 2 == 0: raise ValueError(\\"Even number!\\") return n * 2 # Example usage numbers = [1, 2, 3, 4, 5] print(process_numbers(numbers, example_worker_function)) # Expected Output: [2, None, 6, None, 10]"},{"question":"Objective Your task is to write a Python script using the `fileinput` module to read through multiple specified files and perform line-by-line processing. The goal is to create a consolidated output based on the content of these files. Problem Statement You are given a list of file paths as input. Your task is to read through each file line by line and compute the following statistics: 1. **Total line count** across all files. 2. **Number of lines per file**. 3. **Cumulative line length per file**. After computing these statistics, you should print a summary containing: - The name of each file. - The number of lines and cumulative line length for each file. - The total line count across all files. Function Signature ```python def process_files(file_paths: List[str]) -> None: pass ``` Input - `file_paths`: A list of strings where each string represents a file path. Output - The function should print the summary as described in the problem statement. Constraints 1. All given file paths will be valid, and the files will exist. 2. Each file will contain text data. 3. Line lengths can vary and should be calculated accurately, including newlines. Example Suppose the function is called as follows: ```python file_paths = [\'file1.txt\', \'file2.txt\'] process_files(file_paths) ``` Assuming `file1.txt` has the content: ``` Hello World Python is awesome ``` And `file2.txt` has the content: ``` File handling in Python is made easier with fileinput module ``` The output should be: ``` File: file1.txt - Lines: 2, Cumulative Length: 28 File: file2.txt - Lines: 3, Cumulative Length: 52 Total Lines: 5 ``` Implementation Details - Use the `fileinput` module to handle the iteration over files. - Ensure proper handling of file resources using context management. - Accurately calculate line statistics including lengths with newlines. Notes - The implementation should handle any number of files provided in the `file_paths` list. - Your solution should demonstrate proficiency with handling file I/O in Python and efficient processing of file content using the `fileinput` module.","solution":"import fileinput from typing import List def process_files(file_paths: List[str]) -> None: total_lines = 0 file_statistics = [] for file_path in file_paths: line_count = 0 cumulative_length = 0 with open(file_path, \'r\') as file: for line in file: line_count += 1 cumulative_length += len(line) total_lines += line_count file_statistics.append((file_path, line_count, cumulative_length)) for stat in file_statistics: print(f\\"File: {stat[0]} - Lines: {stat[1]}, Cumulative Length: {stat[2]}\\") print(f\\"Total Lines: {total_lines}\\")"},{"question":"**Question: Advanced Bar Plot Customization with seaborn.objects** You are provided with the `tips` dataset from seaborn, a popular dataset analyzing the tips received by waitstaff in a restaurant based on various factors. Your task is to create a detailed bar plot using seaborn\'s `objects` module. # Requirements: 1. Load the `tips` dataset and ensure that the `time` column, which indicates the meal time (Lunch/Dinner), is treated as a string type. 2. Create a bar plot that shows the count of days (`day` column) divided by meal time (`time` column). Use dodging to handle any overlaps. 3. Ensure the bars are of consistent width and handle any possible empty space by filling it. 4. Additionally, create a bar plot showing the sum of total bills (`total_bill` column) by day and separated by gender (`sex` column). Add a gap of 0.1 between the dodged bars. 5. Finally, incorporate jittering on the bar plot to separate the data points slightly. # Input There are no direct inputs to your function other than the dataset which should be loaded internally. # Output Your output should be two figures: 1. A bar plot showing the counts as described in point 2. 2. A bar plot showing the sum of total bills as described in point 4. # Constraints - Use seaborn version 0.11.2 or later. - Use only seaborn and the built-in seaborn `objects` module for visualization. Do not use matplotlib directly. # Example Here\'s an example of how you should structure your solution: ```python import seaborn.objects as so from seaborn import load_dataset def create_advanced_bar_plots(): # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # First Plot p1 = ( so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) ) p1.show() # Second Plot p2 = ( so.Pplot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) .add(so.Dot(), so.Dodge(), so.Jitter()) ) p2.show() # Calling the function to display plots create_advanced_bar_plots() ``` Implement your solution by filling in the details and ensuring all functionality as described.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_advanced_bar_plots(): # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # First Plot: Count of days divided by meal time p1 = ( so.Plot(tips, x=\\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge()) ) p1.show() # Second Plot: Sum of total bills by day and separated by gender with jitter p2 = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) .add(so.Dot(), so.Dodge(), so.Jitter(width=0.1)) ) p2.show() # Calling the function to display plots create_advanced_bar_plots()"},{"question":"**Objective**: Assess your ability to use pandas rolling window functions to analyze and manipulate time-series data. **Problem Statement**: You are tasked with analyzing historical stock price data to identify trends and patterns. You will use pandas to compute rolling average prices and determine the periods of highest volatility. The dataset provided contains daily stock prices, including columns for `Date`, `Open`, `High`, `Low`, `Close`, and `Volume`. **Dataset Format**: The dataset is given in a CSV file named `stock_prices.csv`. The CSV file has the following structure: ``` Date,Open,High,Low,Close,Volume YYYY-MM-DD, price, price, price, price, volume ... ``` **Tasks**: 1. **Read the CSV file into a DataFrame**. 2. **Compute the 7-day rolling average of the closing prices**. 3. **Compute the 30-day rolling standard deviation of the closing prices**. 4. **Identify the date range where the 30-day rolling standard deviation of the closing prices is at its maximum**. 5. **Output the original DataFrame with two additional columns: `7_day_rolling_avg` and `30_day_rolling_std`, and the date range with the highest 30-day rolling standard deviation**. **Function Signature**: ```python def analyze_stock_prices(file_path: str) -> pd.DataFrame: Args: file_path (str): Path to the CSV file containing stock prices. Returns: pd.DataFrame: Original DataFrame with two new columns added: \'7_day_rolling_avg\' and \'30_day_rolling_std\'. tuple: Start date and end date (inclusive) where the 30-day rolling standard deviation is highest. pass ``` **Constraints**: - You must use pandas\' rolling window functions to perform the calculations. - Ensure to handle any missing or incorrect data in the dataset appropriately. - The Date column should be converted to a datetime object for accurate time series analysis. **Example**: Suppose the given `stock_prices.csv` file contains: ``` Date,Open,High,Low,Close,Volume 2023-01-01,100,110,90,105,10000 2023-01-02,105,115,95,110,15000 2023-01-03,102,112,92,108,12000 2023-01-04,108,118,98,115,13000 2023-01-05,110,120,100,117,11000 ... (more rows follow) ``` Your function should output the DataFrame with the `7_day_rolling_avg` and `30_day_rolling_std` columns added and the date range where the 30-day rolling standard deviation is the highest. **Additional Notes**: - Ensure your solution is well-documented and consider edge cases. - Provide any necessary import statements in your function. Good luck, and happy coding!","solution":"import pandas as pd def analyze_stock_prices(file_path: str) -> pd.DataFrame: Analyzes historical stock price data to compute rolling averages and standard deviations. Args: file_path (str): Path to the CSV file containing stock prices. Returns: pd.DataFrame: Original DataFrame with two new columns added: \'7_day_rolling_avg\' and \'30_day_rolling_std\'. tuple: Start date and end date (inclusive) where the 30-day rolling standard deviation is highest. # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Convert the Date column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Sort by Date df = df.sort_values(\'Date\') # Compute the 7-day rolling average of the closing prices df[\'7_day_rolling_avg\'] = df[\'Close\'].rolling(window=7).mean() # Compute the 30-day rolling standard deviation of the closing prices df[\'30_day_rolling_std\'] = df[\'Close\'].rolling(window=30).std() # Identify the date range with the highest 30-day rolling standard deviation max_std_date = df[\'30_day_rolling_std\'].idxmax() if pd.notna(max_std_date): start_date = df[\'Date\'][max_std_date - 29] # Beginning of the 30-day window end_date = df[\'Date\'][max_std_date] # End of the 30-day window else: start_date = None end_date = None return df, (start_date, end_date)"},{"question":"You are tasked with creating a custom Python type using Python\'s C API. Your goal is to implement a Python extension that defines a new type, \\"PyCustomType\\", with the following features: - The type should have a simple integer attribute, \\"value\\". - It should support basic operations for getting and setting the \\"value\\" attribute. - It should include a method to add a given integer to its \\"value\\" attribute. Here is the detailed task: 1. **Define the PyCustomType structure**: - Create a `PyCustomType` structure with a single integer attribute named \\"value\\". 2. **Create the PyCustomType methods**: - Implement a getter and a setter for the \\"value\\" attribute. - Implement a method `add_value(self, int_to_add)` that adds the given integer to the \\"value\\" attribute. 3. **Initialize and finalize the type**: - Use `PyType_FromSpec` to define the type\'s behavior and slots. - Finalize the type with `PyType_Ready`. 4. **Integrate the definition into a Python module**: - Create a module named `custom_module` and add the `PyCustomType` to this module. # Constraints: - Ensure proper memory management by implementing the appropriate deallocators. - Include error handling wherever necessary. # Example Usage: Below is an example of how your extension should be used in Python: ```python import custom_module # Create an instance of PyCustomType obj = custom_module.PyCustomType() # Set the value attribute to 10 obj.value = 10 # Get the value attribute print(obj.value) # Output: 10 # Add 5 to the value attribute obj.add_value(5) # Get the updated value attribute print(obj.value) # Output: 15 ``` Provide the complete implementation of the Python extension in C, ensuring it adheres to the specifications and constraints mentioned above.","solution":"# This is a Python representation of the desired functionality using ctypes # The actual extension would be implemented in C using the Python C API class PyCustomType: def __init__(self): self._value = 0 @property def value(self): return self._value @value.setter def value(self, val): if not isinstance(val, int): raise ValueError(\\"Value must be an integer\\") self._value = val def add_value(self, int_to_add): if not isinstance(int_to_add, int): raise ValueError(\\"Argument must be an integer\\") self._value += int_to_add"},{"question":"# Asynchronous Socket-based Server and Client with asyncio Background: You are required to implement a simple asynchronous socket-based server and client application using the `asyncio` module in Python. The server should accept incoming client connections, handle simple requests, and send responses back to the clients. However, there are platform-specific details that you need to be aware of, especially when it comes to handling sockets and subprocesses. Requirements: 1. **Server Implementation**: - Implement an `AsyncServer` class that initializes an async server using `asyncio`. - The server should bind to a specified address and port. - It should handle multiple client connections asynchronously. - For each client connection, the server should read the incoming message asynchronously and send a response back after appending \\"Server Received: \\" to the original message. - The server should gracefully handle client disconnections. 2. **Client Implementation**: - Implement an `AsyncClient` class that connects to the asynchronous server using `asyncio`. - The client should send a message to the server and await a response. - Print the response received from the server to the console. 3. **Platform-specific Considerations**: - On **Windows**, use the default `ProactorEventLoop`. - On **macOS** and other platforms, use the default event loop unless specified otherwise. - Ensure your implementation is compatible with the respective event loop policies on Windows and macOS. Input and Output: - **Server**: Should accept connections and messages from multiple clients. - **Client**: Should send a message to the server and receive the processed response. Constraints: - The server and client should be implemented in separate classes. - Ensure proper error handling and resource cleanup. - Perform all necessary imports (including `asyncio` and other standard libraries). Example: ```python import asyncio class AsyncServer: def __init__(self, host, port): self.host = host self.port = port self.server = None async def handle_client(self, reader, writer): data = await reader.read(100) message = data.decode() print(f\\"Received {message} from {writer.get_extra_info(\'peername\')}\\") response = f\\"Server Received: {message}\\" writer.write(response.encode()) await writer.drain() writer.close() async def start_server(self): self.server = await asyncio.start_server(self.handle_client, self.host, self.port) async with self.server: await self.server.serve_forever() class AsyncClient: def __init__(self, host, port): self.host = host self.port = port async def send_message(self, message): reader, writer = await asyncio.open_connection(self.host, self.port) print(f\\"Sending: {message}\\") writer.write(message.encode()) data = await reader.read(100) print(f\\"Received: {data.decode()}\\") writer.close() await writer.wait_closed() if __name__ == \'__main__\': server = AsyncServer(\'127.0.0.1\', 8888) client = AsyncClient(\'127.0.0.1\', 8888) loop = asyncio.get_event_loop() loop.create_task(server.start_server()) message = \\"Hello, World!\\" loop.run_until_complete(client.send_message(message)) loop.run_forever() ``` Note: - This example runs both the server and the client within the same script for simplicity. - Modify as necessary to test across different platforms and event loops as per the documentation.","solution":"import asyncio class AsyncServer: def __init__(self, host, port): self.host = host self.port = port self.server = None async def handle_client(self, reader, writer): try: data = await reader.read(100) message = data.decode() print(f\\"Received {message} from {writer.get_extra_info(\'peername\')}\\") response = f\\"Server Received: {message}\\" writer.write(response.encode()) await writer.drain() finally: writer.close() await writer.wait_closed() async def start_server(self): self.server = await asyncio.start_server(self.handle_client, self.host, self.port) async with self.server: await self.server.serve_forever() class AsyncClient: def __init__(self, host, port): self.host = host self.port = port async def send_message(self, message): reader, writer = await asyncio.open_connection(self.host, self.port) print(f\\"Sending: {message}\\") writer.write(message.encode()) data = await reader.read(100) response = data.decode() print(f\\"Received: {response}\\") writer.close() await writer.wait_closed() return response"},{"question":"# Question You are asked to write a Python script for a basic calculator class along with a comprehensive test suite using the `unittest` module and the utilities provided in the `test.support` package. Part 1: Implement the Calculator Class Create a class `Calculator` with the following methods: - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the difference of `a` and `b`. - `multiply(a, b)`: Returns the product of `a` and `b`. - `divide(a, b)`: Returns the quotient of `a` and `b`. Raise a `ValueError` if `b` is zero. Part 2: Write Unit Tests Write a comprehensive test suite in a script named `test_calculator.py` for the `Calculator` class using the following guidelines and utilities from the `test.support` package: 1. **Test Naming Convention**: Name the test methods such that they start with `test_` and describe the method being tested. 2. **Setup and Teardown**: Use `setUp` and `tearDown` methods if needed. 3. **Use of Utilities**: - Use `test.support.swap_attr` to temporarily change the `DEFAULT_TIMEOUT` attribute. - Use the `test.support.captured_stdout()` to verify any print statements if added (e.g., logging within methods). - Make sure to handle cases like invalid input values to test edge cases thoroughly. 4. **Variety of Assertions**: Use different types of assertions provided by `unittest.TestCase` like `assertEqual`, `assertRaises`, and `assertTrue`. Input and Output - **Input**: The methods of the `Calculator` class. - **Output**: The results of the unit test runs displayed in the console specifying whether each test passed or failed. Constraints - Ensure the test suite handles edge cases like division by zero. - Clean up any resources used in the tests to avoid side effects. Performance Requirement - The test suite should run without crashing and should complete within a reasonable time frame for small inputs. # Example Implementation (calculator.py): ```python class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b ``` Test Suite (test_calculator.py): ```python import unittest from calculator import Calculator from test import support class TestCalculator(unittest.TestCase): def setUp(self): self.calc = Calculator() def tearDown(self): del self.calc def test_add(self): self.assertEqual(self.calc.add(3, 4), 7) def test_subtract(self): self.assertEqual(self.calc.subtract(10, 5), 5) def test_multiply(self): self.assertEqual(self.calc.multiply(3, 7), 21) def test_divide(self): self.assertEqual(self.calc.divide(10, 2), 5) def test_divide_by_zero(self): with self.assertRaises(ValueError): self.calc.divide(10, 0) if __name__ == \'__main__\': unittest.main() ``` You can execute the test suite by running: ```sh python -m unittest test_calculator.py ```","solution":"class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b"},{"question":"# Interactive Python Shell Enhancer You are to implement a simplified version of a Python shell that supports basic history management and tab completion. Your task is to write a function `interactive_shell()` which reads user inputs, stores them in a history, and provides tab completion suggestions for previously entered commands. Function Signature ```python def interactive_shell(): pass ``` Requirements 1. The function should continually prompt the user for input until the user types `exit()`. 2. Implement tab completion for previously entered commands. When the user presses the \\"Tab\\" key, the shell should list possible completions based on history. 3. Maintain a history of all commands entered by the user during the session. The history should be stored in a list called `command_history`. 4. If the user types `!history`, print all previously entered commands. 5. If the user types `exit()`, the function should terminate. Example ```plaintext > print(\'Hello, World!\') > MyFunction > !history print(\'Hello, World!\') MyFunction > pr[TAB] print(\'Hello, World!\') > exit() ``` Notes - You need to simulate the tab completion functionality by binding the \\"Tab\\" key to a function that filters command suggestions from the `command_history` list. - You may use Python\'s `readline` module to assist with tab completion and history management. - Ensure that your implementation handles edge cases, such as there being no history initially or when no completions are found. Constraints - Do not use external libraries other than standard Python libraries. **Hint:** You may find it useful to look into the `readline` module for handling input and completion behaviors.","solution":"import readline def completer(text, state): options = [cmd for cmd in command_history if cmd.startswith(text)] if state < len(options): return options[state] else: return None def interactive_shell(): global command_history command_history = [] # Enable auto-completion readline.parse_and_bind(\'tab: complete\') readline.set_completer(completer) while True: try: input_line = input(\\"> \\") if input_line.strip() == \\"exit()\\": break if input_line.strip() == \\"!history\\": print(\\"n\\".join(command_history)) continue if input_line.strip(): command_history.append(input_line.strip()) except EOFError: break"},{"question":"# Question You are working for a company that deals with large datasets, and you often need to compress and decompress these datasets to save storage space and facilitate easier data sharing. You are given a task to create a utility function using Python\'s `zipfile` module that can: 1. Create a ZIP file from a list of files and directories. 2. Extract this ZIP file to a specified location. 3. Validate the ZIP file by verifying the CRC (Cyclic Redundancy Check) of the files within it. Implement the following function: ```python import zipfile def manage_zip_files(zip_name, files_and_dirs, extract_to): Creates a ZIP file, extracts it, and validates its contents by checking CRC values. Parameters: zip_name (str): The name of the ZIP file to be created. files_and_dirs (list): A list of file and directory paths to be added to the ZIP file. extract_to (str): The directory where the ZIP file will be extracted. Returns: bool: True if the ZIP file is successfully created, extracted, and validated, False otherwise. pass ``` Input Format: - `zip_name`: A string representing the name of the ZIP file to be created. - `files_and_dirs`: A list of strings representing the file and directory paths to be added to the ZIP file. These can be both files and directories. - `extract_to`: A string representing the path of the directory where the ZIP file will be extracted. Output Format: - Return `True` if the ZIP file is successfully created, extracted, and validated. Otherwise, return `False`. Constraints: 1. The paths provided in `files_and_dirs` can include both files and directories. 2. The function should handle exceptions gracefully and return `False` in case of any errors during the ZIP creation, extraction, or validation process. 3. Assume that the directory for extraction will always be empty or will be freshly created before extraction. 4. The function should handle large files efficiently. Performance Requirements: - The function should efficiently handle datasets that can be several gigabytes in size. - It should handle failures gracefully and should not crash. # Example: ```python # Example Usage zip_name = \\"data_archive.zip\\" files_and_dirs = [\\"dataset1.csv\\", \\"dataset2.csv\\", \\"data_folder\\"] extract_to = \\"extracted_data\\" result = manage_zip_files(zip_name, files_and_dirs, extract_to) print(result) # Output could be True if everything goes fine, otherwise False. ``` **Hint**: You might want to use `PyZipFile` for compressing Python files, handle different compression methods, and ensure proper cleanup after extraction.","solution":"import zipfile import os def manage_zip_files(zip_name, files_and_dirs, extract_to): Creates a ZIP file, extracts it, and validates its contents by checking CRC values. Parameters: zip_name (str): The name of the ZIP file to be created. files_and_dirs (list): A list of file and directory paths to be added to the ZIP file. extract_to (str): The directory where the ZIP file will be extracted. Returns: bool: True if the ZIP file is successfully created, extracted, and validated, False otherwise. try: # Create the ZIP file with zipfile.ZipFile(zip_name, \'w\', zipfile.ZIP_DEFLATED) as zipf: for item in files_and_dirs: if os.path.isdir(item): for root, _, files in os.walk(item): for file in files: zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(item, \'..\'))) else: zipf.write(item, os.path.basename(item)) # Extract the ZIP file with zipfile.ZipFile(zip_name, \'r\') as zipf: zipf.extractall(extract_to) # Validate the ZIP file by checking CRC values for zipinfo in zipf.infolist(): extracted_path = os.path.join(extract_to, zipinfo.filename) if zipinfo.CRC != zipfile.ZipFile(zip_name).getinfo(zipinfo.filename).CRC: return False return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Question You are required to write a Python function named `analyze_module` that inspects a given Python module and extracts detailed information about its classes, functions, and methods. The information should include documentation strings, parameter lists, and identifying whether functions and methods are synchronous or asynchronous. The result should be returned as a nested dictionary with the following structure: ```python { \'classes\': { \'ClassName\': { \'doc\': \\"Class documentation\\", \'methods\': { \'method_name\': { \'doc\': \\"Method documentation\\", \'signature\': \\"Method signature\\", \'is_async\': True/False }, ... } }, ... }, \'functions\': { \'function_name\': { \'doc\': \\"Function documentation\\", \'signature\': \\"Function signature\\", \'is_async\': True/False }, ... } } ``` # Requirements 1. **Input**: - The function should take a single argument which is the module to be inspected. ```python def analyze_module(module): pass ``` 2. **Output**: - The function should return a nested dictionary as specified above. 3. **Constraints**: - If a class or function does not have a documentation string, set the `doc` value to `None`. - Use `inspect` module for all inspections. - Ensure your function handles modules with no functions or classes gracefully. # Example Usage ```python import math result = analyze_module(math) print(result) ``` # Notes - You can use the `inspect.getmembers`, `inspect.isclass`, `inspect.isfunction`, `inspect.signature`, `inspect.getdoc`, `inspect.iscoroutinefunction`, and similar functions to help with your implementation. - The solution should work for any module, not just built-in ones.","solution":"import inspect def extract_function_info(func): Extract information about a given function or method. return { \'doc\': inspect.getdoc(func), \'signature\': str(inspect.signature(func)), \'is_async\': inspect.iscoroutinefunction(func) } def analyze_module(module): Inspects a given module and extracts details about its classes, functions, and methods. Returns a nested dictionary with the structure specified in the task description. result = { \'classes\': {}, \'functions\': {} } for name, member in inspect.getmembers(module): if inspect.isclass(member): methods_info = {} for method_name, method in inspect.getmembers(member, inspect.isfunction): methods_info[method_name] = extract_function_info(method) result[\'classes\'][name] = { \'doc\': inspect.getdoc(member), \'methods\': methods_info } elif inspect.isfunction(member): result[\'functions\'][name] = extract_function_info(member) return result"},{"question":"# Advanced Python Control Flow and Function Implementation **Objective**: Implement a function that processes a list of user data, applying discount rules, and handling different types of validation checks using a combination of Python compound statements and function definitions. This task will test your understanding of control flow constructs, exception handling, context management, and function definitions in Python. # Problem Statement: You are required to write a function `process_user_data` that accepts a list of dictionaries where each dictionary represents a user\'s information. The function should apply discount rules and handle various validation checks. Additionally, you must implement proper error handling and context management within your function. Requirements: 1. **Function Signature**: ```python def process_user_data(user_data: list[dict]) -> list[dict]: ``` 2. **Input**: - A list of dictionaries where each dictionary represents a user with the following keys: - `\\"name\\"`: A string representing the user\'s name. - `\\"age\\"`: An integer representing the user\'s age. - `\\"membership\\"`: A string representing the user\'s membership type (`\\"standard\\"`, `\\"premium\\"`, or `\\"vip\\"`). - `\\"purchase_amount\\"`: A float representing the total amount of purchase made by the user. 3. **Output**: - A list of dictionaries with each user\'s updated information, where each dictionary includes: - `\\"name\\"`: The user\'s name. - `\\"membership\\"`: The user\'s membership type. - `\\"purchase_amount\\"`: The updated purchase amount after applying the discount. - `\\"discount_applied\\"`: A float representing the discount applied to the purchase amount. 4. **Discount Rules**: - Standard members receive a 5% discount. - Premium members receive a 10% discount. - VIP members receive a 20% discount. 5. **Validation Checks**: - Raise a `ValueError` if the user\'s age is below 18. - Use a `try` block to catch any user data missing any required fields and add a field `\\"error\\"` with value `\\"Incomplete data\\"` instead of processing the discount. 6. **Context Management**: - Use a `with` statement context manager to simulate logging the process of each user. Use the provided `LogContextManager` for this purpose. 7. **Additional Requirements**: - Implement unit tests using `unittest` to verify your implementation against various test cases. `LogContextManager` Implementation: The `LogContextManager` is provided and should be used to log entries. You can assume that it logs when entered and exited, simulating a logging process. ```python class LogContextManager: def __enter__(self): print(\\"Log: Start processing user\\") return self def __exit__(self, exc_type, exc_value, traceback): print(\\"Log: End processing user\\") if exc_type: print(f\\"Log: Error - {exc_value}\\") ``` # Example: ```python user_data = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"membership\\": \\"standard\\", \\"purchase_amount\\": 100.0}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"membership\\": \\"premium\\", \\"purchase_amount\\": 150.0}, {\\"name\\": \\"Charlie\\", \\"age\\": 17, \\"membership\\": \\"vip\\", \\"purchase_amount\\": 200.0}, {\\"name\\": \\"Dave\\", \\"age\\": 40, \\"membership\\": \\"standard\\"} # Incomplete data ] result = process_user_data(user_data) ``` # Expected Output: ```python [ {\\"name\\": \\"Alice\\", \\"membership\\": \\"standard\\", \\"purchase_amount\\": 95.0, \\"discount_applied\\": 5.0}, {\\"name\\": \\"Bob\\", \\"membership\\": \\"premium\\", \\"purchase_amount\\": 135.0, \\"discount_applied\\": 15.0}, {\\"name\\": \\"Charlie\\", \\"membership\\": \\"vip\\", \\"purchase_amount\\": 200.0, \\"error\\": \\"User under age\\"}, {\\"name\\": \\"Dave\\", \\"membership\\": \\"standard\\", \\"purchase_amount\\": None, \\"error\\": \\"Incomplete data\\"} ] ``` # Testing: - Ensure your `process_user_data` function is thoroughly tested using the `unittest` framework. - Test cases should include various combinations of valid and invalid user data to validate the correctness and robustness of your implementation.","solution":"class LogContextManager: def __enter__(self): print(\\"Log: Start processing user\\") return self def __exit__(self, exc_type, exc_value, traceback): print(\\"Log: End processing user\\") if exc_type: print(f\\"Log: Error - {exc_value}\\") def process_user_data(user_data: list[dict]) -> list[dict]: processed_data = [] for user in user_data: with LogContextManager(): try: # Ensure all required fields are present if not all(key in user for key in [\\"name\\", \\"age\\", \\"membership\\", \\"purchase_amount\\"]): raise ValueError(\\"Incomplete data\\") # Check user\'s age if user[\'age\'] < 18: raise ValueError(\\"User under age\\") # Apply discount based on membership type discount = 0 if user[\'membership\'] == \\"standard\\": discount = 0.05 elif user[\'membership\'] == \\"premium\\": discount = 0.1 elif user[\'membership\'] == \\"vip\\": discount = 0.2 original_amount = user[\'purchase_amount\'] discount_amount = original_amount * discount final_amount = original_amount - discount_amount processed_user = { \\"name\\": user[\'name\'], \\"membership\\": user[\'membership\'], \\"purchase_amount\\": final_amount, \\"discount_applied\\": discount_amount } processed_data.append(processed_user) except ValueError as e: error_user = { \\"name\\": user.get(\'name\', \\"Unknown\\"), \\"membership\\": user.get(\'membership\', \\"Unknown\\"), \\"purchase_amount\\": None, \\"error\\": str(e) } processed_data.append(error_user) return processed_data"},{"question":"Objective: Write a Python function utilizing the `fileinput` module to process lines from a list of text files and perform the following tasks: 1. **Line Counting**: Count the total number of lines across all specified files. 2. **Word Replacement**: Replace every occurrence of a specific target word with a new word. 3. **Handle Compressed Files**: Include support for processing gzip and bzip2 compressed files. 4. **In-place Editing**: If specified, perform changes directly within the original files, making backups with a given extension. Function Signature: ```python def process_files(files: list, target_word: str, new_word: str, inplace: bool = False, backup_ext: str = \'.bak\') -> int: Process the given list of files, replacing every occurrence of \'target_word\' with \'new_word\', and optionally perform in-place editing with backups. Args: files (list): List of filenames to process. target_word (str): Word to be replaced. new_word (str): Word to replace with. inplace (bool): If True, edit files in place with backups. Defaults to False. backup_ext (str): Backup file extension. Defaults to \'.bak\'. Returns: int: Total number of lines processed across all files. # Hint: Utilize fileinput.input() for file iteration and handle gzip and bzip2 files using # fileinput.hook_compressed(). ``` Input: - `files`: List of filenames to be processed. Filenames ending in `.gz` or `.bz2` represent gzip and bzip2 compressed files, respectively. - `target_word`: The word to be replaced in the files. - `new_word`: The word to replace the `target_word` with. - `inplace`: A boolean flag indicating if the replacements should be made directly in the original files, with backups created. - `backup_ext`: The extension for the backup files created during in-place editing. Output: - The function returns an integer representing the total count of lines processed across all files. Constraints: - Do not use external libraries for file processing other than `fileinput`. - Assume files have readable permissions. - Handle potential I/O errors gracefully, raising appropriate exceptions as needed. Example Usage: ```python files = [\'sample1.txt\', \'sample2.txt\', \'compressed_file.gz\'] target_word = \'foo\' new_word = \'bar\' line_count = process_files(files, target_word, new_word, inplace=True, backup_ext=\'.backup\') print(f\\"Processed {line_count} lines in total.\\") ``` This `process_files` function should demonstrate your ability to make sophisticated use of the `fileinput` module, including handling both regular and compressed text files and performing in-place edits with backup.","solution":"import fileinput import os def process_files(files, target_word, new_word, inplace=False, backup_ext=\'.bak\'): Process the given list of files, replacing every occurrence of \'target_word\' with \'new_word\', and optionally perform in-place editing with backups. Args: files (list): List of filenames to process. target_word (str): Word to be replaced. new_word (str): Word to replace with. inplace (bool): If True, edit files in place with backups. Defaults to False. backup_ext (str): Backup file extension. Defaults to \'.bak\'. Returns: int: Total number of lines processed across all files. total_lines = 0 # Determine the appropriate hook for compressed files openhook = fileinput.hook_compressed if any(f.endswith((\'.gz\', \'.bz2\')) for f in files) else None # Use fileinput.input for handling multiple files with fileinput.input(files=files, inplace=inplace, backup=backup_ext, openhook=openhook) as f: for line in f: total_lines += 1 if inplace: # Print the modified line for in-place editing print(line.replace(target_word, new_word), end=\'\') return total_lines"},{"question":"**Custom Interactive Console Implementation** # Objective Implement a custom interactive console that extends the capabilities of Python\'s `InteractiveConsole` class. This custom console should include command history and command auto-completion functionalities. # Requirements: 1. **Class `CustomConsole`:** - Extend the `InteractiveConsole` class from the `code` module. 2. **Command History:** - Maintain a history of all commands entered during the interactive session. - Provide a method `show_history()` that prints out command history in the order they were entered. 3. **Command Auto-Completion:** - Implement basic command auto-completion using the `readline` module. - When a user types a partial command and hits the `Tab` key, the console should complete the command if there is a single match or provide possible completions otherwise. # Method Details: 1. **`def __init__(self, locals=None, filename=\'<console>\')`:** - Initialize the `InteractiveConsole`. - Initialize an empty list to maintain the command history. 2. **`def push(self, line)`:** - Override the `push` method to: - Add the entered line to the command history. - Call the base class’s `push` method. 3. **`def show_history(self)`:** - Print the list of commands entered so far. 4. **Auto-Completion Setup:** - Use the `readline` module to set up auto-completion for the console. - Ensure the `Tab` key triggers auto-completion. # Constraints - Avoid using any external libraries other than `code` and `readline`. - Your implementation should work on Python 3.7+. # Example Usage ```python # Instantiate the custom console my_console = CustomConsole() # Start the interactive session my_console.interact(\\"Custom Interactive Console. Type \'help\' for more information.\\") # Example commands to demonstrate functionalities my_console.push(\\"print(\'Hello, World!\')\\") my_console.push(\\"x = 42\\") my_console.push(\\"print(x)\\") # Show command history my_console.show_history() # Expected Output: # 1: print(\'Hello, World!\') # 2: x = 42 # 3: print(x) ``` **Note:** The actual interactive console session will be run in a Python environment where you can test the command history and auto-completion features interactively.","solution":"import code import readline class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.command_history = [] self.init_readline() def push(self, line): self.command_history.append(line) super().push(line) def show_history(self): for idx, command in enumerate(self.command_history, start=1): print(f\\"{idx}: {command}\\") def init_readline(self): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self.completer) def completer(self, text, state): options = [cmd for cmd in self.command_history if cmd.startswith(text)] if state < len(options): return options[state] else: return None # Example usage: # my_console = CustomConsole() # my_console.interact(\\"Custom Interactive Console. Type \'help\' for more information.\\") # my_console.push(\\"print(\'Hello, World!\')\\") # my_console.push(\\"x = 42\\") # my_console.push(\\"print(x)\\") # my_console.show_history() # # Expected Output: # 1: print(\'Hello, World!\') # 2: x = 42 # 3: print(x)"},{"question":"You are tasked with improving the performance of a Support Vector Classifier (SVC) on the well-known Iris dataset. Your goal is to implement hyper-parameter tuning using `GridSearchCV` and then evaluate its performance on a hold-out test set. Follow the steps provided to complete the task. 1. **Load the dataset and split it**: - Split the Iris dataset into training (80%) and testing (20%) sets using `train_test_split`. 2. **Define the estimator and parameter grid**: - Use `sklearn.svm.SVC` as the estimator. - Define a parameter grid containing the following parameters: ```python param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] ``` 3. **Implement GridSearchCV**: - Utilize `GridSearchCV` with 5-fold cross-validation and fit it on the training set. - Set the scoring metric to `accuracy`. 4. **Evaluate the best model**: - Obtain the best estimator from the `GridSearchCV` results. - Evaluate its accuracy on the hold-out test set. # Input and Output Format: - Input: - None. The code should operate on predefined dataset splits and grid settings. - Output: - Accuracy of the best estimator on the test set printed out. - Best parameters found by `GridSearchCV`. # Constraints and Evaluation: - Ensure reproducibility by setting a random state where applicable. - Your solution should be efficient in terms of computation and memory, within the capabilities of handling the Iris dataset. # Example: Here\'s a sample template to get you started: ```python from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load the Iris dataset and split iris = datasets.load_iris() X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42) # Define the estimator and parameter grid svc = SVC() param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] # Implement GridSearchCV grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring=\'accuracy\', cv=5) grid_search.fit(X_train, y_train) # Get the best model and evaluate on test set best_estimator = grid_search.best_estimator_ test_accuracy = accuracy_score(y_test, best_estimator.predict(X_test)) print(f\\"Best Parameters: {grid_search.best_params_}\\") print(f\\"Test Set Accuracy: {test_accuracy:.2f}\\") ``` # Note: - Make sure to provide proper imports and handle any exceptions or warnings that might arise during the search. - You may also visualize the results or more detailed output optionally for additional insights.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load the Iris dataset and split iris = datasets.load_iris() X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42) # Define the estimator and parameter grid svc = SVC() param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] # Implement GridSearchCV grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring=\'accuracy\', cv=5) grid_search.fit(X_train, y_train) # Get the best model and evaluate on test set best_estimator = grid_search.best_estimator_ test_accuracy = accuracy_score(y_test, best_estimator.predict(X_test)) best_parameters = grid_search.best_params_ print(f\\"Best Parameters: {best_parameters}\\") print(f\\"Test Set Accuracy: {test_accuracy:.2f}\\") def get_best_params(): return best_parameters def get_test_set_accuracy(): return test_accuracy"},{"question":"**Coding Assessment Question: Advanced Data Preprocessing with scikit-learn** In this task, you are required to preprocess data using various techniques provided in the `sklearn.preprocessing` module, and subsequently fit a model with this preprocessed data. **Problem Statement:** You have been given the following dataset which contains both numerical and categorical features: ```python data = { \'age\': [25, 45, 35, np.nan, 50, 43, np.nan, 39], \'income\': [50000, np.nan, 75000, 60000, 120000, np.nan, 70000, 90000], \'city\': [\'New York\', \'Los Angeles\', \'Chicago\', np.nan, \'Houston\', \'Phoenix\', \'Los Angeles\', \'Houston\'], \'car\': [\'Ford\', \'BMW\', np.nan, \'Chevrolet\', \'Ford\', \'BMW\', \'Tesla\', \'Chevrolet\'], \'target\': [1, 0, 1, 0, 1, 0, 1, 0] } ``` Your job is to perform the following preprocessing steps and fit a logistic regression model using `sklearn.preprocessing` and `sklearn.linear_model`: 1. **Impute missing values**: - For numerical features, use the mean to fill in missing values. - For categorical features, use the most frequent value (mode) to fill in missing values. 2. **Scale the numerical features**: - Standardize the numerical features so that they have zero mean and unit variance. 3. **Encode the categorical features**: - Use OneHotEncoder to encode the categorical features. - Handle any unknown categories during transformation by assigning them to a separate category. 4. **Combine all preprocessing steps using ColumnTransformer**. 5. **Fit a logistic regression model** using the preprocessed data and print the accuracy of the model on the training data. Your solution should implement a function called `preprocess_and_train` which takes the data as input and returns the fitted logistic regression model. **Expected function signature:** ```python def preprocess_and_train(data: dict) -> LogisticRegression: pass ``` **Input:** - A dictionary `data` with keys as column names and values as lists of data entries. **Output:** - A fitted logistic regression model. **Example:** ```python data = { \'age\': [25, 45, 35, np.nan, 50, 43, np.nan, 39], \'income\': [50000, np.nan, 75000, 60000, 120000, np.nan, 70000, 90000], \'city\': [\'New York\', \'Los Angeles\', \'Chicago\', np.nan, \'Houston\', \'Phoenix\', \'Los Angeles\', \'Houston\'], \'car\': [\'Ford\', \'BMW\', np.nan, \'Chevrolet\', \'Ford\', \'BMW\', \'Tesla\', \'Chevrolet\'], \'target\': [1, 0, 1, 0, 1, 0, 1, 0] } model = preprocess_and_train(data) print(model) ``` **Note:** Don\'t forget to handle any imports and provide necessary comments in your code.","solution":"import numpy as np import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def preprocess_and_train(data: dict): # Convert the dictionary to a DataFrame df = pd.DataFrame(data) # Define the column names num_features = [\'age\', \'income\'] cat_features = [\'city\', \'car\'] # Preprocessing for numerical data: Impute missing values with mean and standardize num_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Preprocessing for categorical data: Impute missing values with most frequent and OneHotEncode cat_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Creating the column transformer to apply respective transformers on the columns preprocessor = ColumnTransformer( transformers=[ (\'num\', num_transformer, num_features), (\'cat\', cat_transformer, cat_features) ] ) # Creating the pipeline with the preprocessor and logistic regression model model = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Splitting the features and target X = df.drop(columns=\'target\') y = df[\'target\'] # Fitting the model model.fit(X, y) # Print the accuracy on the training set y_pred = model.predict(X) accuracy = accuracy_score(y, y_pred) print(f\'Training Accuracy: {accuracy}\') return model"},{"question":"# Pandas DataFrame Manipulation and Analysis In this exercise, you will work with a dataset containing information about customers, their purchases, and the products they have purchased. You will be required to load this data into a pandas DataFrame, perform a series of data manipulations, and then output specific results. Data Description: - `customers.csv`: Contains information about customers. - `customer_id` (int): Unique identifier for each customer. - `name` (str): Name of the customer. - `age` (int): Age of the customer. - `location` (str): Location of the customer. - `purchases.csv`: Contains information about each purchase. - `purchase_id` (int): Unique identifier for each purchase. - `customer_id` (int): Unique identifier for the customer who made the purchase. - `product_id` (int): Unique identifier for the purchased product. - `quantity` (int): Quantity of the product purchased. - `price` (float): Price per unit of the product. - `products.csv`: Contains information about products. - `product_id` (int): Unique identifier for each product. - `product_name` (str): Name of the product. - `category` (str): Category of the product. Tasks: 1. **Load the Data:** - Load the data from the provided CSV files into three pandas DataFrames: `customers`, `purchases`, `products`. 2. **Data Manipulation:** - Merge the DataFrames to create a single DataFrame containing complete information about each purchase (customer, product, and purchase details). - Handle any missing values appropriately. Assume that if any product or customer is missing from the given data, you should exclude those records from the final result. - Calculate the total spending for each customer. 3. **Analysis:** - Identify the top 5 customers based on total spending. - Determine the most purchased product by quantity. - Calculate the average spending by customers in each location. 4. **Output the Results:** - Convert the DataFrame with the top 5 customers and their total spending into a dictionary format where the key is the customer\'s name and the value is their total spending. - Convert the DataFrame indicating the most purchased product into a dictionary format where the key is the product name and the value is the quantity purchased. - Convert the DataFrame with average spending per location into a dictionary where the key is the location and the value is the average spending. Function Signature: ```python import pandas as pd def analyze_customer_purchases(customers_csv: str, purchases_csv: str, products_csv: str) -> (dict, dict, dict): # Load data customers = pd.read_csv(customers_csv) purchases = pd.read_csv(purchases_csv) products = pd.read_csv(products_csv) # Merge DataFrames merged_df = # your code here # Handle missing values # your code here # Calculate total spending per customer # your code here # Top 5 customers top_customers = # your code here # Most purchased product most_purchased_product = # your code here # Average spending per location avg_spending_per_location = # your code here # Convert to required dictionary formats top_customers_dict = top_customers.set_index(\'name\')[\'total_spending\'].to_dict() most_purchased_product_dict = most_purchased_product.set_index(\'product_name\')[\'quantity\'].to_dict() avg_spending_per_location_dict = avg_spending_per_location.set_index(\'location\')[\'average_spending\'].to_dict() return top_customers_dict, most_purchased_product_dict, avg_spending_per_location_dict ``` **Input:** - `customers_csv`: Path to the `customers.csv` file. - `purchases_csv`: Path to the `purchases.csv` file. - `products_csv`: Path to the `products.csv` file. **Output:** - A tuple of three dictionaries: 1. Dictionary of top 5 customers and their total spending. 2. Dictionary of the most purchased product and its quantity. 3. Dictionary of average spending per location. **Note**: Ensure that your code is optimized and handles potential edge cases, such as missing data or empty DataFrames.","solution":"import pandas as pd def analyze_customer_purchases(customers_csv: str, purchases_csv: str, products_csv: str): # Load data customers = pd.read_csv(customers_csv) purchases = pd.read_csv(purchases_csv) products = pd.read_csv(products_csv) # Merge DataFrames merged_df = purchases.merge(customers, on=\'customer_id\', how=\'inner\') merged_df = merged_df.merge(products, on=\'product_id\', how=\'inner\') # Calculate total spending per customer merged_df[\'total_spending\'] = merged_df[\'quantity\'] * merged_df[\'price\'] total_spending_per_customer = merged_df.groupby(\'customer_id\').agg({\'total_spending\': \'sum\', \'name\': \'first\'}).reset_index() # Top 5 customers top_customers = total_spending_per_customer.sort_values(by=\'total_spending\', ascending=False).head(5) # Most purchased product most_purchased_product = merged_df.groupby(\'product_id\').agg({\'quantity\': \'sum\', \'product_name\': \'first\'}).reset_index() most_purchased_product = most_purchased_product.sort_values(by=\'quantity\', ascending=False).head(1) # Average spending per location avg_spending_per_location = merged_df.groupby(\'location\').agg({\'total_spending\': \'mean\'}).reset_index() avg_spending_per_location.rename(columns={\'total_spending\': \'average_spending\'}, inplace=True) # Convert to required dictionary formats top_customers_dict = top_customers.set_index(\'name\')[\'total_spending\'].to_dict() most_purchased_product_dict = most_purchased_product.set_index(\'product_name\')[\'quantity\'].to_dict() avg_spending_per_location_dict = avg_spending_per_location.set_index(\'location\')[\'average_spending\'].to_dict() return top_customers_dict, most_purchased_product_dict, avg_spending_per_location_dict"},{"question":"**Question:** You are required to write a Python script that performs the following tasks using the `spwd` module: 1. **Fetch User Information**: Write a function `fetch_user_info(username)` that takes a username string as input and returns the shadow password database entry for the given username. If the user does not have the necessary privileges, the function should handle the `PermissionError` and return a string message `Insufficient privileges`. 2. **List All Users**: Write a function `list_all_users_with_password_change_info(min_days=0)` that returns a list of tuples. Each tuple should contain the login name (`sp_namp`) and the last change date (`sp_lstchg`) for users whose `sp_min` value is greater than or equal to `min_days`. The `min_days` parameter should default to 0. **Constraints and Requirements:** - You must use the `spwd` module to access the shadow password database. - You should handle potential exceptions appropriately. - Implement any necessary error checking to handle invalid input or potential issues accessing the shadow password database. **Function Signature:** - `fetch_user_info(username: str) -> Union[Tuple[str, str, int, int, int, int, int, int, int], str]` - `list_all_users_with_password_change_info(min_days: int = 0) -> List[Tuple[str, int]]` # Example Usage: ```python # Assuming the user has the required privileges # Example Call 1 user_info = fetch_user_info(\\"exampleuser\\") print(user_info) # Output: (\'exampleuser\', \'6rounds=5000examplehash\', 18761, 7, 90, 7, -1, -1, 0) # Example Call 2 users_with_change_info = list_all_users_with_password_change_info(5) print(users_with_change_info) # Output: [(\'user1\', 18761), (\'user2\', 18762)] ``` **Note:** - The actual output will depend on the contents of the shadow password database and the user\'s privileges. - The date of last change (`sp_lstchg`) is typically represented as the number of days since the epoch (January 1, 1970).","solution":"import spwd def fetch_user_info(username): Returns the shadow password database entry for the given username. If the user does not have the necessary privileges, returns \'Insufficient privileges\'. try: user_info = spwd.getspnam(username) return user_info except PermissionError: return \\"Insufficient privileges\\" except KeyError: return f\\"User \'{username}\' not found\\" def list_all_users_with_password_change_info(min_days=0): Returns a list of tuples containing the login name and the last change date for users whose sp_min value is greater than or equal to min_days. try: all_users_info = spwd.getspall() result = [(user.sp_nam, user.sp_lstchg) for user in all_users_info if user.sp_min >= min_days] return result except PermissionError: return \\"Insufficient privileges\\""},{"question":"**Out-of-Core Learning with Incremental Mini-Batch Training** **Objective:** You are tasked with implementing an out-of-core learning system for text classification using scikit-learn. Your system should be capable of handling datasets that cannot fit entirely into memory by processing data in mini-batches and learning incrementally. **Requirements:** 1. **Streaming instances:** Simulate streaming batches of data from a large dataset. You can create a generator function that yields mini-batches of text data. 2. **Extracting features:** Use `sklearn.feature_extraction.text.HashingVectorizer` for feature extraction. 3. **Incremental learning:** Implement incremental learning using `sklearn.linear_model.SGDClassifier`. **Details:** - You will be provided with a large text dataset split into multiple chunks (simulating streaming data). - Each chunk will be a list of texts and corresponding labels (positive or negative). - Implement a function `incremental_text_classification` that processes these chunks sequentially and updates the model incrementally. - Evaluate the model\'s performance using accuracy after each mini-batch. **Function Signature:** ```python def incremental_text_classification(data_stream, mini_batch_size): Incrementally trains a text classifier using out-of-core learning. Parameters: data_stream (generator): A generator that yields tuples (texts, labels). mini_batch_size (int): The number of examples to process in each mini-batch. Returns: list: A list of accuracy scores after each mini-batch. pass ``` **Input:** - `data_stream`: A generator function that yields tuples (texts, labels). Each tuple represents a mini-batch. - `mini_batch_size`: An integer representing the size of each mini-batch. **Output:** - A list of accuracy scores calculated after each mini-batch. **Constraints:** - The dataset is large and cannot be loaded into memory all at once. - Use the `HashingVectorizer` for feature extraction. - Use `SGDClassifier` with `partial_fit` for incremental learning. - For the first call to `partial_fit`, you must specify all possible classes using the `classes=` parameter (positive and negative). **Example Usage:** ```python # Example data stream generator def data_stream_generator(): batches = [ ([\\"I love this movie\\", \\"This is terrible\\"], [\\"positive\\", \\"negative\\"]), ([\\"Such a great film\\", \\"I hated it\\"], [\\"positive\\", \\"negative\\"]), # Add more mini-batches here ] for batch in batches: yield batch # Mini-batch size mini_batch_size = 2 # Call the function result = incremental_text_classification(data_stream_generator(), mini_batch_size) print(result) # Output should be a list of accuracy scores ``` **Notes:** - Make sure to shuffle the data within each mini-batch to simulate streaming from a real-world scenario. - Clearly document your code with comments to explain each step of the process. Good luck!","solution":"import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def incremental_text_classification(data_stream, mini_batch_size): Incrementally trains a text classifier using out-of-core learning. Parameters: data_stream (generator): A generator that yields tuples (texts, labels). mini_batch_size (int): The number of examples to process in each mini-batch. Returns: list: A list of accuracy scores after each mini-batch. vectorizer = HashingVectorizer(n_features=10000) model = SGDClassifier(random_state=42) accuracy_scores = [] classes = np.array([\'positive\', \'negative\']) for texts, labels in data_stream: X = vectorizer.transform(texts) y = np.array(labels) if not accuracy_scores: model.partial_fit(X, y, classes=classes) else: model.partial_fit(X, y) y_pred = model.predict(X) accuracy = accuracy_score(y, y_pred) accuracy_scores.append(accuracy) return accuracy_scores"},{"question":"<|Analysis Begin|> The provided documentation for `asyncio` synchronization primitives covers five key classes: `Lock`, `Event`, `Condition`, `Semaphore`, and `BoundedSemaphore`. Each class offers specific functionality for handling synchronization tasks within asyncio based programs. The key points are: 1. `Lock`: Used to guarantee exclusive access to a shared resource. 2. `Event`: Allows notifying multiple tasks that some event has occurred. 3. `Condition`: Combines functionality of `Event` and `Lock` and can be used to wait for an event and gain exclusive access to resources. 4. `Semaphore`: Manages an internal counter that decrements/acquire and increments/release operations. 5. `BoundedSemaphore`: Similar to `Semaphore` but raises a `ValueError` if the release operation increases the internal counter above its initial value. Each of these classes has methods like `acquire`, `release`, `wait`, etc., which are essential to their operations. Given these functionalities, an effective assessment question should require students to demonstrate comprehension in using one or more of these classes to implement a practical situation within asynchronous programming. <|Analysis End|> <|Question Begin|> # Asynchronous Traffic Light System You are tasked with implementing an asynchronous traffic light system for controlling the access of vehicles at an intersection using `asyncio` synchronization primitives. Requirements: 1. **TrafficLight Class**: - The class should switch between green, yellow, and red lights following the sequence and timing: - Green light stays on for 3 seconds. - Yellow light stays on for 1 second. - Red light stays on for 2 seconds. - Use an `Event` to signal to vehicles waiting at the red light that they may proceed. 2. **Vehicle Class**: - Vehicles arriving at the intersection should wait if the light is red. - Each vehicle should attempt to cross the intersection when the light is green or yellow. - Each vehicle takes 1 second to cross the intersection. 3. **Intersection Manager**: - Use a `Semaphore` with a value of 1 to ensure only one vehicle can cross at a time even if the light is green or yellow. # Implementation Details: - Use the `asyncio` module to implement concurrency. - The `TrafficLight` class should have a method `start` which controls the light\'s state changes. - The `Vehicle` class should have a method `cross` which attempts to cross the intersection. - The `IntersectionManager` class should initialize the `TrafficLight` and manage incoming vehicles. Class Definitions and Method Signatures: ```python import asyncio class TrafficLight: def __init__(self): self.current_light = \'red\' self.event = asyncio.Event() self.event.clear() async def start(self): # Your code to change the light state pass class Vehicle: def __init__(self, id, intersection_manager): self.id = id self.intersection_manager = intersection_manager async def cross(self): # Your code to cross the intersection pass class IntersectionManager: def __init__(self): self.traffic_light = TrafficLight() self.semaphore = asyncio.Semaphore(1) async def start_manager(self): asyncio.create_task(self.traffic_light.start()) async def vehicle_arrival(self, vehicle): await vehicle.cross() async def main(): manager = IntersectionManager() await manager.start_manager() vehicles = [Vehicle(i, manager) for i in range(10)] for vehicle in vehicles: asyncio.create_task(manager.vehicle_arrival(vehicle)) await asyncio.sleep(20) if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Task: 1. Implement the `start` method of the `TrafficLight` class to cycle through the traffic lights. 2. Implement the `cross` method of the `Vehicle` class. 3. Ensure proper synchronization using the `Event` and `Semaphore` from `asyncio`. Constraints: - Each vehicle takes 1 second to cross the intersection. - Use `asyncio` synchronization primitives as described above.","solution":"import asyncio class TrafficLight: def __init__(self): self.current_light = \'red\' self.event = asyncio.Event() self.event.clear() async def start(self): while True: self.current_light = \'green\' self.event.set() # Allow vehicles to go print(\\"Light is GREEN\\") await asyncio.sleep(3) # Green light stays on for 3 seconds self.current_light = \'yellow\' print(\\"Light is YELLOW\\") await asyncio.sleep(1) # Yellow light stays on for 1 second self.current_light = \'red\' self.event.clear() # Stop vehicles print(\\"Light is RED\\") await asyncio.sleep(2) # Red light stays on for 2 seconds class Vehicle: def __init__(self, id, intersection_manager): self.id = id self.intersection_manager = intersection_manager async def cross(self): while True: await self.intersection_manager.traffic_light.event.wait() # Wait for green or yellow light async with self.intersection_manager.semaphore: print(f\\"Vehicle {self.id} is crossing the intersection\\") await asyncio.sleep(1) # Takes 1 second to cross break class IntersectionManager: def __init__(self): self.traffic_light = TrafficLight() self.semaphore = asyncio.Semaphore(1) async def start_manager(self): asyncio.create_task(self.traffic_light.start()) async def vehicle_arrival(self, vehicle): await vehicle.cross() async def main(): manager = IntersectionManager() await manager.start_manager() vehicles = [Vehicle(i, manager) for i in range(10)] for vehicle in vehicles: asyncio.create_task(manager.vehicle_arrival(vehicle)) await asyncio.sleep(20) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Coding Assessment Question:** # Objective Implement a `SortedCollection` class in Python that manages a list of items in sorted order. Use the `bisect` module to maintain and manipulate this list efficiently. This exercise will test your understanding of the `bisect` module and your ability to implement advanced data structures. # Description Your task is to create a `SortedCollection` class with the following methods: 1. **`__init__(self, key=None)`**: Initialize the collection with an optional key function to extract the comparison key from each element. The default key function should compare the elements directly. 2. **`add(self, item)`**: Add an item to the collection while maintaining the sorted order. 3. **`remove(self, item)`**: Remove an item from the collection. Raise a `ValueError` if the item is not present. 4. **`find_le(self, item)`**: Find and return the largest item less than or equal to the specified item. Raise a `ValueError` if no such item exists. 5. **`find_ge(self, item)`**: Find and return the smallest item greater than or equal to the specified item. Raise a `ValueError` if no such item exists. 6. **`find_lt(self, item)`**: Find and return the largest item strictly less than the specified item. Raise a `ValueError` if no such item exists. 7. **`find_gt(self, item)`**: Find and return the smallest item strictly greater than the specified item. Raise a `ValueError` if no such item exists. 8. **`__contains__(self, item)`**: Check if an item exists in the collection. 9. **`__len__(self)`**: Return the number of items in the collection. 10. **`__iter__(self)`**: Return an iterator over the collection items in sorted order. # Constraints - The items added to the collection should be comparable using the provided key function or directly (if no key function is provided). - The collection should maintain its items in a sorted order at all times. # Input and Output Format - There is no specific input format. Each method should be callable on an instance of the `SortedCollection` class. - You can test the `SortedCollection` behavior using print statements or assert statements. # Example Usage ```python from bisect import bisect_left, bisect_right, insort_left, insort_right class SortedCollection: def __init__(self, key=None): self._key = key or (lambda x: x) self._items = [] def add(self, item): key = self._key(item) insort_right(self._items, (key, item)) def remove(self, item): key = self._key(item) pos = bisect_left(self._items, (key, item)) if pos != len(self._items) and self._items[pos] == (key, item): self._items.pop(pos) else: raise ValueError(\'Item not found\') def find_le(self, item): key = self._key(item) pos = bisect_right(self._items, (key, item)) - 1 if pos >= 0: return self._items[pos][1] raise ValueError(\'No element found less than or equal to the specified value\') def find_ge(self, item): key = self._key(item) pos = bisect_left(self._items, (key, item)) if pos < len(self._items): return self._items[pos][1] raise ValueError(\'No element found greater than or equal to the specified value\') def find_lt(self, item): key = self._key(item) pos = bisect_left(self._items, (key, item)) - 1 if pos >= 0: return self._items[pos][1] raise ValueError(\'No element found less than the specified value\') def find_gt(self, item): key = self._key(item) pos = bisect_right(self._items, (key, item)) if pos < len(self._items): return self._items[pos][1] raise ValueError(\'No element found greater than the specified value\') def __contains__(self, item): key = self._key(item) pos = bisect_left(self._items, (key, item)) return pos != len(self._items) and self._items[pos] == (key, item) def __len__(self): return len(self._items) def __iter__(self): for _, item in self._items: yield item # Example usage of the SortedCollection class sc = SortedCollection() sc.add(5) sc.add(3) sc.add(9) print(list(sc)) # Output: [3, 5, 9] print(sc.find_le(5)) # Output: 5 print(sc.find_ge(4)) # Output: 5 print(sc.find_lt(6)) # Output: 5 print(sc.find_gt(5)) # Output: 9 print(5 in sc) # Output: True print(len(sc)) # Output: 3 sc.remove(5) print(list(sc)) # Output: [3, 9] ``` # Performance Requirements - The `add` method must run in `O(n)`, dominated by the insertion step. - The `remove` and search methods (`find_le`, `find_ge`, `find_lt`, `find_gt`) should run in `O(log n)` for the search, but possible `O(n)` overall due to insertion and removal in the middle of the list.","solution":"from bisect import bisect_left, bisect_right, insort_right class SortedCollection: def __init__(self, key=None): self._key = key or (lambda x: x) self._items = [] def add(self, item): key = self._key(item) insort_right(self._items, (key, item)) def remove(self, item): key = self._key(item) pos = bisect_left(self._items, (key, item)) if pos != len(self._items) and self._items[pos] == (key, item): self._items.pop(pos) else: raise ValueError(\'Item not found\') def find_le(self, item): key = self._key(item) pos = bisect_right(self._items, (key, item)) - 1 if pos >= 0: return self._items[pos][1] raise ValueError(\'No element found less than or equal to the specified value\') def find_ge(self, item): key = self._key(item) pos = bisect_left(self._items, (key, item)) if pos < len(self._items): return self._items[pos][1] raise ValueError(\'No element found greater than or equal to the specified value\') def find_lt(self, item): key = self._key(item) pos = bisect_left(self._items, (key, item)) - 1 if pos >= 0: return self._items[pos][1] raise ValueError(\'No element found less than the specified value\') def find_gt(self, item): key = self._key(item) pos = bisect_right(self._items, (key, item)) if pos < len(self._items): return self._items[pos][1] raise ValueError(\'No element found greater than the specified value\') def __contains__(self, item): key = self._key(item) pos = bisect_left(self._items, (key, item)) return pos != len(self._items) and self._items[pos] == (key, item) def __len__(self): return len(self._items) def __iter__(self): for _, item in self._items: yield item"},{"question":"You are an engineer working on a data archival system that utilizes LZMA compression for large datasets. Your task is to write a Python class `LZMAArchiver` that provides methods to compress and decompress both files and in-memory data using the `lzma` module. # Requirements **Class Name:** `LZMAArchiver` **Methods:** 1. `compress_file(input_file: str, output_file: str, preset: int = 6) -> None`: - **Parameters:** - `input_file`: The path to the input file to be compressed (string). - `output_file`: The path to the output compressed file (string). - `preset`: An integer specifying the compression level, default is 6. - **Functionality**: Reads the input file, compresses its contents using the specified preset, and writes the compressed data to the output file. 2. `decompress_file(input_file: str, output_file: str) -> None`: - **Parameters:** - `input_file`: The path to the input compressed file (string). - `output_file`: The path to the output decompressed file (string). - **Functionality**: Reads the compressed file, decompresses its contents, and writes the decompressed data to the output file. 3. `compress_data(data: bytes, preset: int = 6) -> bytes`: - **Parameters:** - `data`: A bytes object containing the data to be compressed. - `preset`: An integer specifying the compression level, default is 6. - **Returns**: A bytes object containing the compressed data. 4. `decompress_data(data: bytes) -> bytes`: - **Parameters:** - `data`: A bytes object containing the compressed data. - **Returns**: A bytes object containing the decompressed data. # Constraints and Considerations - Ensure that all file operations handle potential exceptions, such as file not found or read/write errors, gracefully. - Validate input parameters, such as making sure the `preset` is between 0 and 9. - Include necessary error handling for `lzma.LZMAError`. # Example Usage ```python # Create an instance of the archiver archiver = LZMAArchiver() # Compress a file archiver.compress_file(\'sample.txt\', \'sample.txt.xz\', preset=9) # Decompress a file archiver.decompress_file(\'sample.txt.xz\', \'sample_decompressed.txt\') # Compress in-memory data input_data = b\\"Example data to compress\\" compressed_data = archiver.compress_data(input_data, preset=7) # Decompress in-memory data decompressed_data = archiver.decompress_data(compressed_data) assert decompressed_data == input_data ``` Your implementation should pass the above example usage. Be sure to handle the edge cases and provide robust error handling to make the class reliable for production use.","solution":"import lzma import os class LZMAArchiver: def compress_file(self, input_file: str, output_file: str, preset: int = 6) -> None: if not (0 <= preset <= 9): raise ValueError(\\"Preset must be between 0 and 9\\") if not os.path.exists(input_file): raise FileNotFoundError(f\\"Input file {input_file} does not exist\\") try: with open(input_file, \'rb\') as f_in: with lzma.open(output_file, \'wb\', preset=preset) as f_out: f_out.write(f_in.read()) except Exception as e: raise IOError(f\\"Error during file compression: {str(e)}\\") def decompress_file(self, input_file: str, output_file: str) -> None: if not os.path.exists(input_file): raise FileNotFoundError(f\\"Input file {input_file} does not exist\\") try: with lzma.open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: f_out.write(f_in.read()) except lzma.LZMAError as e: raise IOError(f\\"Error during file decompression: {str(e)}\\") def compress_data(self, data: bytes, preset: int = 6) -> bytes: if not (0 <= preset <= 9): raise ValueError(\\"Preset must be between 0 and 9\\") try: return lzma.compress(data, preset=preset) except Exception as e: raise IOError(f\\"Error during data compression: {str(e)}\\") def decompress_data(self, data: bytes) -> bytes: try: return lzma.decompress(data) except lzma.LZMAError as e: raise IOError(f\\"Error during data decompression: {str(e)}\\")"},{"question":"# Seaborn Visualization Challenge You are provided with the `titanic` dataset available in seaborn. Your task is to create a set of visualizations that provide meaningful insights into the dataset. Follow the steps below to accomplish this. 1. **Load Data:** Load the Titanic dataset using `sns.load_dataset(\'titanic\')`. 2. **Plot 1: Age Distribution by Passenger Class:** Create a violin plot to visualize the distribution of passengers\' ages for each class. Customize the plot to differentiate the violin plots by the `sex` of the passengers. Ensure the plot has appropriate labels and a title. - Input: None - Output: Display the violin plot with the specified customizations. 3. **Plot 2: Survival Rate by Class and Gender:** Create a bar plot to show the survival rate (`survived` column) for each class, split by gender. Use `col` to create subplots for each gender. Customize the subplot layout and ensure all plots have proper labels and titles. - Input: None - Output: Display the bar plot with the specified customizations. 4. **Plot 3: Composite Age and Fare Analysis:** Create a combined plot to analyze the relationship between age and fare, and how it varies by passenger class. Use a scatter plot for individual data points overlaid on a box plot to summarize the data distribution. - Input: None - Output: Display the combined scatter and box plot with the specified customizations. # Constraints and Requirements: - Ensure all visualizations are properly labeled and titles are descriptive. - Use seaborn\'s `FacetGrid` or similar functionality when required. - Customize the aesthetics to improve the readability of the plots. - Code should be written in Python using seaborn and should be executable without errors. Here is a sample structure to guide you: ```python import seaborn as sns # Load the Titanic dataset df = sns.load_dataset(\'titanic\') # Task 1: Age Distribution by Passenger Class sns.catplot( data=df, x=\'class\', y=\'age\', hue=\'sex\', kind=\'violin\', split=True, bw_adjust=.5, cut=0 ) # Customize labels and title # Your code here... # Task 2: Survival Rate by Class and Gender g = sns.catplot( data=df, x=\'class\', y=\'survived\', col=\'sex\', kind=\'bar\', height=4, aspect=.6 ) # Customize labels, titles, and layout # Your code here... # Task 3: Composite Age and Fare Analysis sns.boxplot( data=df, x=\'class\', y=\'fare\', whis=10 ) sns.swarmplot( data=df, x=\'class\', y=\'fare\', hue=\'age\', palette=\'cool\', size=3, dodge=True ) # Customize labels, titles, and layout # Your code here... ``` Make sure to test your code and ensure it produces the desired visualizations accurately.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_visualizations(): # Load the Titanic dataset df = sns.load_dataset(\'titanic\') # Task 1: Age Distribution by Passenger Class plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\'class\', y=\'age\', hue=\'sex\', split=True) plt.title(\'Age Distribution by Passenger Class and Sex\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.legend(title=\'Sex\', loc=\'upper right\') plt.show() # Task 2: Survival Rate by Class and Gender g = sns.catplot( data=df, x=\'class\', y=\'survived\', col=\'sex\', kind=\'bar\', height=6, aspect=1, ci=None, palette=\'muted\', ) g.set_axis_labels(\\"Passenger Class\\", \\"Survival Rate\\") g.set_titles(\\"{col_name} Passengers\\") g.fig.suptitle(\'Survival Rate by Class and Gender\', y=1.03) plt.show() # Task 3: Composite Age and Fare Analysis plt.figure(figsize=(10, 6)) sns.boxplot(data=df, x=\'class\', y=\'fare\') sns.scatterplot(data=df, x=\'class\', y=\'fare\', hue=\'age\', palette=\'cool\', size=\'age\', sizes=(20, 200), legend=False) plt.title(\'Composite Age and Fare Analysis by Passenger Class\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Fare\') plt.legend(title=\'Age\') plt.show()"},{"question":"# Challenge: Handling and Managing Duplicate Labels in Pandas You are given a pandas DataFrame that may potentially have duplicate labels in both rows and columns. Your task is to write a Python function that: 1. Identifies if there are any duplicate row or column labels. 2. If duplicates exist, remove those duplicates. 3. Once duplicates are removed, set the DataFrame to disallow duplicate labels going forward. The function should return the processed DataFrame. Function Signature ```python import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input - `df`: A pandas DataFrame that may have duplicate labels in rows and/or columns. Output - A processed pandas DataFrame where: - Any duplicate rows or columns have been resolved. - Duplicate labels are disallowed in the resultant DataFrame. Example ```python # Example DataFrame with duplicate row labels df = pd.DataFrame({ \'A\': [1, 2, 3, 4], \'B\': [5, 6, 7, 8], \'C\': [9, 10, 11, 12] }, index=[\'a\', \'b\', \'b\', \'c\']) processed_df = process_dataframe(df) print(processed_df) # Expected Output: # A B C # a 1 5 9 # b 2 6 10 # b 3 7 11 # c 4 8 12 print(processed_df.flags.allows_duplicate_labels) # Expected Output: False ``` **Note**: You may resolve duplicate labels by taking the first occurrence of each duplicate. Additional Notes - Use the `is_unique` attribute to check for duplicate labels. - Use the `duplicated` method to filter and remove duplicate rows/columns. - Use `set_flags(allows_duplicate_labels=False)` to ensure that the resultant DataFrame does not allow duplicate labels.","solution":"import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: Identifies and handles duplicate row or column labels in a given DataFrame. Removes duplicates by keeping the first occurrence. Sets the DataFrame to disallow duplicate labels going forward. Args: df (pd.DataFrame): Input DataFrame that may have duplicate labels. Returns: pd.DataFrame: Processed DataFrame without duplicate labels and disallowed duplicates. # Check and remove duplicate row indices if not df.index.is_unique: df = df[~df.index.duplicated(keep=\'first\')] # Check and remove duplicate column indices if not df.columns.is_unique: df = df.loc[:, ~df.columns.duplicated(keep=\'first\')] # Set the flag to disallow duplicate labels df.flags.allows_duplicate_labels = False return df"},{"question":"**Question: Implementing and Analyzing K-Means Clustering on Synthetic and Real Datasets** # Objective: To assess the understanding of K-Means clustering, data preprocessing, dimensionality reduction, and evaluation metrics in the scikit-learn package. # Task: 1. **Implement K-Means Clustering**: - Write a function `kmeans_clustering(X: np.ndarray, n_clusters: int, init: str, max_iter: int, random_state: int) -> Tuple[np.ndarray, np.ndarray, float]` that: - Takes a dataset `X` (a numpy array of shape `(n_samples, n_features)`), number of clusters `n_clusters`, initialization method `init` (either `\'random\'` or `\'k-means++\'`), maximum iterations `max_iter`, and a `random_state`. - Returns the cluster labels, cluster centers, and the inertia (within-cluster sum-of-squares criterion). - **Example**: ```python def kmeans_clustering(X: np.ndarray, n_clusters: int, init: str, max_iter: int, random_state: int) -> Tuple[np.ndarray, np.ndarray, float]: # Your implementation here ``` 2. **Evaluate Clustering**: - Use the [synthesized dataset](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py) and the [Iris dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset) to evaluate the clustering. - Write a function `evaluate_clustering(X: np.ndarray, y_true: np.ndarray, n_clusters: int) -> Dict[str, float]` that: - Takes a dataset `X`, ground truth labels `y_true`, and number of clusters `n_clusters`. - Performs PCA to reduce the dataset to 2 dimensions. - Runs K-Means clustering on the reduced dataset. - Computes and returns clustering evaluation metrics: Adjusted Rand Index, Normalized Mutual Information, V-measure, Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index. - **Example**: ```python from sklearn.decomposition import PCA def evaluate_clustering(X: np.ndarray, y_true: np.ndarray, n_clusters: int) -> Dict[str, float]: # Your implementation here ``` # Input and Output: - **Input**: - `kmeans_clustering`: - `X`: numpy array of shape `(n_samples, n_features)` - `n_clusters`: int - `init`: str, `\'random\'` or `\'k-means++\'` - `max_iter`: int - `random_state`: int - `evaluate_clustering`: - `X`: numpy array of shape `(n_samples, n_features)` - `y_true`: numpy array of shape `(n_samples,)` - `n_clusters`: int - **Output**: - `kmeans_clustering`: Tuple containing cluster labels, cluster centers, and inertia. - `evaluate_clustering`: Dictionary with clustering evaluation metrics. # Constraints: - Ensure your implementation is efficient and minimizes computational overhead. - Use appropriate scikit-learn functions/classes to achieve the task. # Performance Requirements: - Functions should handle datasets with up to 10,000 samples efficiently. - Maintain clear and concise code with appropriate comments. # Example Usage: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler # Load Iris dataset data = load_iris() X = data.data y = data.target # Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # K-Means clustering labels, centers, inertia = kmeans_clustering(X_scaled, n_clusters=3, init=\'k-means++\', max_iter=300, random_state=42) print(labels, centers, inertia) # Evaluate clustering metrics = evaluate_clustering(X_scaled, y, n_clusters=3) print(metrics) ``` # Submission: - Submit the two functions `kmeans_clustering` and `evaluate_clustering` in a Python script or Jupyter notebook. - Include a README file with instructions to run the code and any dependencies required.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.decomposition import PCA from sklearn import metrics from typing import Tuple, Dict def kmeans_clustering(X: np.ndarray, n_clusters: int, init: str, max_iter: int, random_state: int) -> Tuple[np.ndarray, np.ndarray, float]: Perform K-Means clustering on the dataset X. Parameters: - X (np.ndarray): Dataset of shape (n_samples, n_features). - n_clusters (int): Number of clusters. - init (str): Initialization method, either \'random\' or \'k-means++\'. - max_iter (int): Maximum number of iterations. - random_state (int): Random seed. Returns: - Tuple (np.ndarray, np.ndarray, float): Cluster labels, cluster centers, and inertia. kmeans = KMeans(n_clusters=n_clusters, init=init, max_iter=max_iter, random_state=random_state) kmeans.fit(X) return kmeans.labels_, kmeans.cluster_centers_, kmeans.inertia_ def evaluate_clustering(X: np.ndarray, y_true: np.ndarray, n_clusters: int) -> Dict[str, float]: Evaluate K-Means clustering on dataset X with ground truth labels y_true. Parameters: - X (np.ndarray): Dataset of shape (n_samples, n_features). - y_true (np.ndarray): Ground truth labels of shape (n_samples,). - n_clusters (int): Number of clusters. Returns: - Dict[str, float]: Dictionary with clustering evaluation metrics: Adjusted Rand Index, Normalized Mutual Information, V-measure, Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index. # Reduce dimensionality to 2D using PCA pca = PCA(n_components=2, random_state=42) X_reduced = pca.fit_transform(X) # Perform K-Means clustering on the reduced dataset labels, _, _ = kmeans_clustering(X_reduced, n_clusters=n_clusters, init=\'k-means++\', max_iter=300, random_state=42) # Compute evaluation metrics metrics_dict = { \'Adjusted Rand Index\': metrics.adjusted_rand_score(y_true, labels), \'Normalized Mutual Information\': metrics.normalized_mutual_info_score(y_true, labels, average_method=\'arithmetic\'), \'V-measure\': metrics.v_measure_score(y_true, labels), \'Silhouette Score\': metrics.silhouette_score(X_reduced, labels), \'Calinski-Harabasz Index\': metrics.calinski_harabasz_score(X_reduced, labels), \'Davies-Bouldin Index\': metrics.davies_bouldin_score(X_reduced, labels) } return metrics_dict"},{"question":"# Coding Assessment: Hyper-parameter Tuning with Scikit-learn Objective Implement a comprehensive hyper-parameter tuning solution using scikit-learn\'s `GridSearchCV` or `RandomizedSearchCV`, incorporating a pipeline for data preprocessing and multi-metric scoring. Problem Statement You are provided with a dataset `data.csv` containing labeled data for a binary classification task. Your objective is to build a complete machine learning pipeline that includes: 1. Data pre-processing. 2. Model training with hyper-parameter tuning. 3. Evaluation using multiple metrics. Requirements: 1. **Pipeline Steps**: - Standardize numerical features. - Apply a classifier (e.g., Support Vector Machine). 2. **Hyper-parameter Tuning**: - Tune the hyper-parameters using `GridSearchCV` or `RandomizedSearchCV`. - Consider tuning the parameters of both the pre-processing step (like the scaler) and the classifier. 3. **Multi-Metric Scoring**: - Evaluate models using multiple metrics like accuracy, precision, and recall. - Use the `refit` parameter to select the best model based on one primary metric. 4. **Results Analysis**: - Output the best parameters for the model. - Report the scores of the best model on the validation set for all specified metrics. Input: - `data.csv` file in the following format: ``` feature1, feature2, ..., featureN, target 1.1, 2.3, ..., 3.3, 0 4.2, 1.1, ..., 2.1, 1 ... ``` - You can assume there are no missing values, and all features are numerical. Template Code: Fill in the following template code to complete this assignment: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score # Load dataset data = pd.read_csv(\'data.csv\') # Split dataset into features and target X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # Define parameter grid param_grid = { \'scaler__with_mean\': [True, False], \'svc__C\': [0.1, 1, 10], \'svc__kernel\': [\'linear\', \'rbf\'], \'svc__gamma\': [\'scale\', \'auto\'] } # Define multiple metrics scoring = { \'accuracy\': make_scorer(accuracy_score), \'precision\': make_scorer(precision_score), \'recall\': make_scorer(recall_score) } # Create the GridSearchCV object grid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, refit=\'accuracy\', cv=5) # Fit the grid search object to the training data grid_search.fit(X_train, y_train) # Get the best parameters and best score best_params = grid_search.best_params_ best_score = grid_search.best_score_ # Evaluate the model on the test set y_pred = grid_search.predict(X_test) test_accuracy = accuracy_score(y_test, y_pred) test_precision = precision_score(y_test, y_pred) test_recall = recall_score(y_test, y_pred) # Output the results print(f\\"Best Parameters: {best_params}\\") print(f\\"Best Cross-Validated Score (Accuracy): {best_score}\\") print(f\\"Test Accuracy: {test_accuracy}\\") print(f\\"Test Precision: {test_precision}\\") print(f\\"Test Recall: {test_recall}\\") ``` Constraints: - Use `GridSearchCV` or `RandomizedSearchCV` for hyper-parameter tuning. - Implement multi-metric scoring and choose a primary metric for model selection. - Ensure the pipeline includes both pre-processing and classification steps. - Use cross-validation with `cv=5`. Performance Expectations: - The code should efficiently handle the given dataset. - Output the best hyper-parameters, cross-validated score, and test performance metrics clearly.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score def perform_hyperparameter_tuning(data_path): # Load dataset data = pd.read_csv(data_path) # Split dataset into features and target X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC()) ]) # Define parameter grid param_grid = { \'scaler__with_mean\': [True, False], \'svc__C\': [0.1, 1, 10], \'svc__kernel\': [\'linear\', \'rbf\'], \'svc__gamma\': [\'scale\', \'auto\'] } # Define multiple metrics scoring = { \'accuracy\': make_scorer(accuracy_score), \'precision\': make_scorer(precision_score), \'recall\': make_scorer(recall_score) } # Create the GridSearchCV object grid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, refit=\'accuracy\', cv=5) # Fit the grid search object to the training data grid_search.fit(X_train, y_train) # Get the best parameters and best score best_params = grid_search.best_params_ best_score = grid_search.best_score_ # Evaluate the model on the test set y_pred = grid_search.predict(X_test) test_accuracy = accuracy_score(y_test, y_pred) test_precision = precision_score(y_test, y_pred) test_recall = recall_score(y_test, y_pred) return best_params, best_score, test_accuracy, test_precision, test_recall if __name__ == \\"__main__\\": best_params, best_score, test_accuracy, test_precision, test_recall = perform_hyperparameter_tuning(\'data.csv\') print(f\\"Best Parameters: {best_params}\\") print(f\\"Best Cross-Validated Score (Accuracy): {best_score}\\") print(f\\"Test Accuracy: {test_accuracy}\\") print(f\\"Test Precision: {test_precision}\\") print(f\\"Test Recall: {test_recall}\\")"},{"question":"# **Coding Assessment Question: Advanced Logging Configuration** Design an application that demonstrates advanced usage of the Python logging module. The application should meet the following requirements: 1. **Logging Configuration**: - Create a logging configuration using a dictionary and the `logging.config.dictConfig()` method. - Configure the logging to output to both a file and the console. Use different formats for each output. 2. **Contextual Logging**: - Use contextual information in the log records. Include information like the current thread name and a custom attribute such as a \\"transaction ID\\" for each logged message. 3. **Multiple Handlers**: - Configure a handler to output DEBUG and higher level messages to a file with a size limit (using `RotatingFileHandler`). When the file exceeds this size, it should rotate and keep a certain number of backups. - Configure another handler to output INFO and higher level messages to the console. 4. **Multi-Threaded Logging**: - Implement a multi-threaded logging scenario. For example, one thread can simulate a processing task and log progress messages, while another simulates a monitoring task and logs periodic status updates. 5. **Custom Filter**: - Implement a custom filter that modifies log messages to include contextual information such as a user ID or transaction ID. # **Function Requirements** Implement a Python function `setup_logging()` that: - Accepts parameters for log file name, console log level, and file log level. - Configures the logging as specified above using the `logging.config.dictConfig()` method. Implement a Python class `LoggingContext` that: - Uses a context manager to manage contextual information (e.g., transaction ID) and thread-specific information. Implement a multi-threaded function `simulate_processing()` that: - Spins up multiple threads, each performing mock tasks and logging progress and final results. # **Input and Output** - **Input**: - Parameters to setup the logging configuration (log file name, console log level, file log level). - **Output**: - Log files and console output demonstrating the logging capabilities. # **Example Usage** ```python def main(): setup_logging(\'app.log\', console_level=\'INFO\', file_level=\'DEBUG\') def worker(context_info): with LoggingContext(transaction_id=context_info[\'transaction_id\']): logging.info(\'Started processing transaction...\') time.sleep(random.random()) logging.debug(\'Transaction processing detail...\') logging.info(\'Finished processing transaction.\') simulate_processing(worker) if __name__ == \\"__main__\\": main() ``` The example should demonstrate: - Logs being written to a file and displayed on the console. - Contextual information such as transaction IDs and thread names in the log output. - Proper log rotation for the file handler.","solution":"import logging import logging.config import logging.handlers import threading import time import random class CustomContextFilter(logging.Filter): def __init__(self, transaction_id=None): super().__init__() self.transaction_id = transaction_id def filter(self, record): record.transaction_id = self.transaction_id or \\"N/A\\" record.threadName = threading.current_thread().name return True class LoggingContext: def __init__(self, transaction_id): self.transaction_id = transaction_id def __enter__(self): filter = CustomContextFilter(transaction_id=self.transaction_id) logging.getLogger().addFilter(filter) return self def __exit__(self, exc_type, exc_val, exc_tb): logging.getLogger().removeFilter(CustomContextFilter(self.transaction_id)) def setup_logging(log_file_name, console_level=\'INFO\', file_level=\'DEBUG\'): log_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'file_formatter\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - %(transaction_id)s - %(message)s\' }, \'console_formatter\': { \'format\': \'%(name)s - %(levelname)s - %(message)s\' }, }, \'handlers\': { \'file_handler\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'filename\': log_file_name, \'maxBytes\': 1048576, \'backupCount\': 3, \'level\': file_level, \'formatter\': \'file_formatter\', }, \'console_handler\': { \'class\': \'logging.StreamHandler\', \'level\': console_level, \'formatter\': \'console_formatter\', \'stream\': \'ext://sys.stdout\' }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'file_handler\', \'console_handler\'] }, } logging.config.dictConfig(log_config) def simulate_processing(worker, num_threads=5): threads = [] for _ in range(num_threads): transaction_id = f\'TX-{random.randint(1000, 9999)}\' thread = threading.Thread(target=worker, args=({\'transaction_id\': transaction_id},), name=f\'Thread-{transaction_id}\') threads.append(thread) thread.start() for thread in threads: thread.join() def worker(context_info): with LoggingContext(transaction_id=context_info[\'transaction_id\']): logging.info(\'Started processing transaction...\') time.sleep(random.random()) logging.debug(\'Transaction processing detail...\') logging.info(\'Finished processing transaction.\') def main(): setup_logging(\'app.log\', console_level=\'INFO\', file_level=\'DEBUG\') simulate_processing(worker) if __name__ == \\"__main__\\": main()"},{"question":"# Python Coding Assessment Question Objective: Create a Python module that defines several functions and uses the `if __name__ == \'__main__\':` idiom to conditionally run a script. The script should include taking user input, processing it, and providing an output. Task: 1. **Define a Python module** that contains the following functions: - `greet_user(name: str) -> str`: Takes a user\'s name as input and returns a greeting string. - `calculate_square(number: int) -> int`: Takes an integer and returns its square. - `process_input(data: dict) -> dict`: Takes a dictionary with keys \\"name\\" and \\"number\\", and returns a dictionary with greeting and square. For example: `{\\"name\\": \\"Alice\\", \\"number\\": 3}` should return `{\\"greeting\\": \\"Hello, Alice!\\", \\"square\\": 9}`. 2. **Encapsulate the script logic** in a `main` function: - The `main` function should prompt the user to input their name and a number, then use the above functions to compute the results, and finally, print out the results in a formatted manner. 3. Use the `if __name__ == \'__main__\':` block to call the `main` function. Input and Output: - **Input**: The user will be prompted to enter their name and a number. - **Output**: Print a greeting message and the square of the entered number. Constraints: - The name should be a non-empty string. - The number should be an integer. Example: ``` python script.py Enter your name: Alice Enter a number: 3 Hello, Alice! The square of 3 is 9. ``` Performance Requirements: - The program should handle typical user input without running into performance issues for reasonably large names and numbers. Solution Template: Provide the necessary code implementation to meet the above specifications. ```python def greet_user(name: str) -> str: Generate a greeting message for the user. return f\\"Hello, {name}!\\" def calculate_square(number: int) -> int: Calculate the square of a given number. return number ** 2 def process_input(data: dict) -> dict: Process user input dictionary and return a dictionary with greeting and square results. name = data.get(\\"name\\", \\"\\") number = data.get(\\"number\\", 0) return { \\"greeting\\": greet_user(name), \\"square\\": calculate_square(number) } def main(): Main function to input, process, and output results. name = input(\\"Enter your name: \\") number = int(input(\\"Enter a number: \\")) result = process_input({\\"name\\": name, \\"number\\": number}) print(result[\\"greeting\\"]) print(f\\"The square of {number} is {result[\'square\']}.\\") if __name__ == \'__main__\': main() ``` Make sure your code is correctly handling user inputs and edge cases as per the requirements.","solution":"def greet_user(name: str) -> str: Generate a greeting message for the user. Parameters: name (str): The name of the user. Returns: str: A greeting message. return f\\"Hello, {name}!\\" def calculate_square(number: int) -> int: Calculate the square of a given number. Parameters: number (int): The number to be squared. Returns: int: The square of the number. return number ** 2 def process_input(data: dict) -> dict: Process user input dictionary and return a dictionary with greeting and square results. Parameters: data (dict): A dictionary containing user input with keys \'name\' and \'number\'. Returns: dict: A dictionary with \'greeting\' and \'square\' as keys. name = data.get(\\"name\\", \\"\\") number = data.get(\\"number\\", 0) return { \\"greeting\\": greet_user(name), \\"square\\": calculate_square(number) } def main(): Main function to input, process and output results. name = input(\\"Enter your name: \\") number = int(input(\\"Enter a number: \\")) result = process_input({\\"name\\": name, \\"number\\": number}) print(result[\\"greeting\\"]) print(f\\"The square of {number} is {result[\'square\']}.\\") if __name__ == \'__main__\': main()"},{"question":"**Objective**: Demonstrate understanding of the `marshal` module along with its limitations and proper use cases. **Question**: You are given the task to serialize a provided list of dictionaries, each containing basic data types, to a binary format using the `marshal` module. Then, deserialize the data back to its original form. The code should handle the following requirements: 1. Serialize the list of dictionaries to a binary file. 2. Deserialize the binary data from the file back into a list of dictionaries. 3. Ensure that any unsupported data types in the dictionaries are handled gracefully, by logging an error message and omitting that particular entry from serialization. 4. Verify that the deserialized data matches the original data (excluding any unsupported entries). **Input**: A list of dictionaries. Each dictionary can contain the following data types as values: `int`, `float`, `str`, `bool`, `list`, `tuple`, `set`, `frozenset`, `dict`. **Output**: 1. A binary file containing the serialized data. 2. A list of dictionaries obtained after deserialization, matching the original input list (excluding omitted entries). **Constraints**: - The list of dictionaries should not contain any nested unsupported data types. - You should use the `logging` module to log error messages for entries with unsupported data types. **Performance Requirements**: - Efficient handling of file operations and data serialization/deserialization. - Proper error handling and logging. **Starter Code**: ```python import marshal import logging logging.basicConfig(level=logging.ERROR) def serialize_data(data, filename): Serialize the input data to a binary file using marshal. Parameters: - data: List[Dict], the data to serialize - filename: str, the name of the binary file to write Returns: - None, but logs an error message if an unsupported type is encountered with open(filename, \'wb\') as file: # Your code here to serialize data def deserialize_data(filename): Deserialize data from a binary file using marshal. Parameters: - filename: str, the name of the binary file to read Returns: - List[Dict], the deserialized data with open(filename, \'rb\') as file: # Your code here to deserialize data def main(): data = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"balance\\": 100.75}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"married\\": False}, {\\"name\\": \\"Charlie\\", \\"age\\": 35, \\"pets\\": [\\"dog\\", \\"cat\\"]}, ] filename = \\"data.marshal\\" serialize_data(data, filename) result = deserialize_data(filename) print(\\"Deserialized data:\\", result) if __name__ == \\"__main__\\": main() ``` **Task**: Implement the functions `serialize_data` and `deserialize_data` to achieve the objectives mentioned above. Ensure proper error handling and logging for unsupported data types.","solution":"import marshal import logging logging.basicConfig(level=logging.ERROR) def serialize_data(data, filename): Serialize the input data to a binary file using marshal. Parameters: - data: List[Dict], the data to serialize - filename: str, the name of the binary file to write Returns: - None, but logs an error message if an unsupported type is encountered supported_types = (int, float, str, bool, list, tuple, set, frozenset, dict) processed_data = [] for item in data: try: if not all(isinstance(v, supported_types) for v in item.values()): logging.error(\\"Encountered an unsupported data type in dictionary: %s\\", item) continue processed_data.append(item) except Exception as e: logging.error(\\"Error processing item %s: %s\\", item, e) with open(filename, \'wb\') as file: marshal.dump(processed_data, file) def deserialize_data(filename): Deserialize data from a binary file using marshal. Parameters: - filename: str, the name of the binary file to read Returns: - List[Dict], the deserialized data with open(filename, \'rb\') as file: return marshal.load(file)"},{"question":"**Question: Creating a Custom Tkinter Application with Message Boxes** You are tasked with creating a simple Tkinter-based application that helps users decide on their daily tasks by prompting them with various message boxes. Your application should include the following functionality using the `tkinter.messagebox` module: 1. **Information Message Box**: Display a welcome message when the application starts. 2. **Warning Message Box**: Display a warning if the user tries to exit the application without saving their tasks. 3. **Error Message Box**: Display an error if the user selects an invalid option. 4. **Question Message Box**: Ask the user if they have completed their tasks for the day. 5. **Yes/No Question**: Ask the user if they want to add a new task. Implement the function `run_task_manager()` to achieve the above requirements. The function should: - Create and display a main window with a button labeled \\"Start Task Manager\\". - When the button is clicked, it should first show the welcome information message box. - Then check if the user wants to add tasks (Yes/No question box). If \'Yes\', proceed to dummy task input. - Ask if the user has completed their tasks (Question message box). - If the user attempts to exit without selecting an option or saving tasks, show a warning message box. - If an invalid option is selected, display an error message box. # Function Signature ```python def run_task_manager(): ``` # Additional Requirements - You may use additional tkinter widgets and methods as necessary. - Ensure to handle window closing scenarios with proper message boxes. - The function should repeatedly ask the user if they want to add a new task until they choose \'No\'. # Input - User interactions with the Tkinter interface. # Output - Tkinter message boxes displayed to the user based on their actions. # Example Usage ```python run_task_manager() ``` # Constraints - The application should handle invalid options gracefully using the error message box. - The warning message box should prevent the application from closing until the user confirms. Design your code to handle typical user interactions and edge cases effectively. Good luck!","solution":"import tkinter as tk from tkinter import messagebox def run_task_manager(): Runs a task management application with various message boxes. def on_start(): messagebox.showinfo(\\"Welcome\\", \\"Welcome to the Task Manager!\\") while True: add_task = messagebox.askyesno(\\"New Task\\", \\"Do you want to add a new task?\\") if add_task: task = tk.simpledialog.askstring(\\"Input\\", \\"Enter your task:\\") if task is None or task.strip() == \\"\\": messagebox.showerror(\\"Error\\", \\"Invalid task input. Please enter a valid task.\\") else: print(f\\"Task added: {task}\\") else: break tasks_completed = messagebox.askyesno(\\"Task Completion\\", \\"Have you completed your tasks for the day?\\") if tasks_completed: print(\\"Great job! All tasks completed.\\") else: print(\\"Keep going! You still have tasks to complete.\\") def on_closing(): if messagebox.askquestion(\\"Warning\\", \\"Are you sure you want to exit without saving your tasks?\\") == \\"yes\\": root.destroy() root = tk.Tk() root.title(\\"Task Manager\\") main_frame = tk.Frame(root) main_frame.pack(pady=30) start_btn = tk.Button(main_frame, text=\\"Start Task Manager\\", command=on_start) start_btn.pack(pady=10) root.protocol(\\"WM_DELETE_WINDOW\\", on_closing) root.mainloop()"},{"question":"# PyArrow-Backed Pandas Data Structures You are given a dataset containing information about different products in a store. This data is provided in a CSV format as follows: ``` product_id,price,stock P001,19.99,True P002,45.50,False P003,NaN,True P004,29.99,NaN ``` The goal is to use PyArrow-backed pandas data structures to manipulate and analyze this data. Task: 1. **Read the given CSV data into a DataFrame using PyArrow as the backend**: Ensure that the `price` column is treated as a float type using PyArrow, the `stock` column is treated as a boolean type using PyArrow, and the `product_id` column is treated as a string type using PyArrow. 2. **Handle missing data**: - Fill missing values in the `price` column with the median price of the available products. - Fill missing values in the `stock` column with `False`. 3. **Perform calculations**: - Calculate the average price of the products. - Create a new boolean column `expensive`, which is `True` if the product `price` is above 30, otherwise `False`. 4. **Export the processed DataFrame to a new CSV file using the PyArrow engine**. # Constraints: - You must use PyArrow-backed data structures where specified. - Avoid using numpy directly; instead, rely on PyArrow and pandas functions. # Expected Input/Output: - **Input:** No explicit input required. The provided CSV data should be assumed available as a string in the script. - **Output:** The resulting DataFrame saved to `processed_products.csv`. # Sample Code Template: ```python import pandas as pd import pyarrow as pa import io # Given CSV data as a string data = product_id,price,stock P001,19.99,True P002,45.50,False P003,NaN,True P004,29.99,NaN # Task 1: Read the CSV data using PyArrow-backed DataFrame df = pd.read_csv( io.StringIO(data), dtype={ \\"product_id\\": \\"string[pyarrow]\\", \\"price\\": \\"float[pyarrow]\\", \\"stock\\": \\"bool[pyarrow]\\" }, engine=\\"pyarrow\\" ) # Task 2: Handling Missing Data # Fill missing values in \'price\' column with the median price median_price = df[\'price\'].median() df[\'price\'].fillna(median_price, inplace=True) # Fill missing values in \'stock\' column with False df[\'stock\'].fillna(False, inplace=True) # Task 3: Perform Calculations # Calculate the average price of the products average_price = df[\'price\'].mean() print(f\\"Average price: {average_price}\\") # Create a new boolean column \'expensive\' df[\'expensive\'] = df[\'price\'] > 30 # Task 4: Export the DataFrame to a new CSV file df.to_csv(\\"processed_products.csv\\", index=False) ``` You need to complete the tasks as specified above using PyArrow-backed data structures. Test your implemented code to confirm it behaves as expected and produces the required output.","solution":"import pandas as pd import pyarrow as pa import io # Given CSV data as a string data = product_id,price,stock P001,19.99,True P002,45.50,False P003,NaN,True P004,29.99,NaN def process_products_csv(data: str, output_file: str) -> pd.DataFrame: # Task 1: Read the CSV data using PyArrow-backed DataFrame df = pd.read_csv( io.StringIO(data), dtype={ \\"product_id\\": \\"string[pyarrow]\\", \\"price\\": \\"float[pyarrow]\\", \\"stock\\": \\"bool[pyarrow]\\" }, engine=\\"pyarrow\\" ) # Task 2: Handling Missing Data # Fill missing values in \'price\' column with the median price median_price = df[\'price\'].median() df[\'price\'].fillna(median_price, inplace=True) # Fill missing values in \'stock\' column with False df[\'stock\'].fillna(False, inplace=True) # Task 3: Perform Calculations # Calculate the average price of the products average_price = df[\'price\'].mean() print(f\\"Average price: {average_price}\\") # Create a new boolean column \'expensive\' df[\'expensive\'] = df[\'price\'] > 30 # Task 4: Export the DataFrame to a new CSV file df.to_csv(output_file, index=False) return df"},{"question":"**Email Address Manipulation Project** You are working on an application that processes email communications. Your task is to implement a function that takes a string containing multiple email addresses from various fields (e.g., `To`, `Cc`, `Bcc`) from an email message and performs several operations on them. **Function Signature:** ```python def process_email_addresses(fields: dict) -> dict: ``` **Input:** - `fields` (dict): A dictionary where keys are field names (strings) and values are lists of email addresses (strings). The field names can include `to`, `cc`, `bcc`. **Output:** - `result` (dict): A dictionary with: - `all_addresses` (list of tuples): A list of tuples with all email addresses parsed into `(\'realname\', \'email address\')` format. - `unique_addresses` (set): A set containing unique email addresses (without real names). - `invalid_addresses` (list of strings): A list of email addresses that could not be parsed correctly. **Constraints and Requirements:** 1. Use `email.utils.parseaddr()` to parse each email address. 2. Utilize `email.utils.getaddresses()` to handle multiple addresses within a single field value when necessary. 3. Ensure that invalid email addresses are identified and returned in the `invalid_addresses` list. 4. Concatenate all addresses from each field into a single list before processing. 5. Performance is critical; the function should efficiently handle up to 1000 email addresses. **Example:** ```python input_fields = { \'to\': [ \'Alice <alice@example.com>, Bob <bob@example.com>\', \'Carol <carol@example.com>\' ], \'cc\': [ \'Dave <dave@example.com>\', \'alice@example.com\' ], \'bcc\': [ \'Eve <eve@example.net>\', \'Frank <invalid-email>\' ] } output = process_email_addresses(input_fields) print(output) # Expected Output: # { # \'all_addresses\': [ # (\'Alice\', \'alice@example.com\'), # (\'Bob\', \'bob@example.com\'), # (\'Carol\', \'carol@example.com\'), # (\'Dave\', \'dave@example.com\'), # (\'\', \'alice@example.com\'), # (\'Eve\', \'eve@example.net\') # ], # \'unique_addresses\': { # \'alice@example.com\', # \'bob@example.com\', # \'carol@example.com\', # \'dave@example.com\', # \'eve@example.net\' # }, # \'invalid_addresses\': [\'Frank <invalid-email>\'] # } ``` **Notes:** - Your implementation should be robust and correctly handle edge cases, such as empty lists or fields with no email addresses. - You should raise a `ValueError` with an appropriate message if the input dictionary has fields other than `to`, `cc`, or `bcc`.","solution":"from email.utils import parseaddr, getaddresses import re def process_email_addresses(fields: dict) -> dict: Processes email addresses from provided fields. Args: fields (dict): A dictionary where keys are field names (strings) \'to\', \'cc\', \'bcc\' and values are lists of email addresses (strings). Returns: dict: A dictionary with \'all_addresses\', \'unique_addresses\', and \'invalid_addresses\'. valid_fields = {\'to\', \'cc\', \'bcc\'} # Check for invalid fields for field in fields: if field not in valid_fields: raise ValueError(f\\"Invalid field name: {field}\\") all_addresses = [] unique_addresses = set() invalid_addresses = [] email_pattern = re.compile( r\\"(^[-!#%&\'*+/=?^_`{}|~0-9A-Z]+(.[-!#%&\'*+/=?^_`{}|~0-9A-Z]+)*\\" r\\"|^\\"([^\\"]|.)*\\")@([A-Z0-9-]+.)+[A-Z]{2,6}\\", re.IGNORECASE ) consolidated_addresses = [] # Consolidate all addresses into a single list for key in fields: field_addresses = fields[key] consolidated_addresses.extend(getaddresses(field_addresses)) for name, address in consolidated_addresses: if email_pattern.match(address): all_addresses.append((name, address)) unique_addresses.add(address) else: invalid_addresses.append(f\\"{name} <{address}>\\") return { \'all_addresses\': all_addresses, \'unique_addresses\': unique_addresses, \'invalid_addresses\': invalid_addresses }"},{"question":"**Question: Implement a Custom Data Class with Context Management and Post-Initialization Processing** Python\'s `dataclasses` module allows for easy definition of classes that primarily store state without much boilerplate. The `contextlib` module provides utilities for managing context (e.g., with `with` statement). In this challenge, you are required to implement a custom data class that uses both modules. # Objective Implement a `LoggingContextDataClass` that represents a data structure with fields, custom initialization logic, and context management that logs messages when entering and exiting the context. # Requirements 1. **Data Class Definition**: - Define a data class `LoggingContextDataClass` using the `dataclasses` module. - The class should have the following fields: - `name` (string) - `value` (integer) - `log_file` (string) 2. **Post-Initialization**: - Implement a `__post_init__` method that ensures the `value` field is always an integer. If a non-integer string is given, it should convert it to zero and log a warning message to `log_file`. 3. **Context Management**: - Implement the context management protocol (`__enter__` and `__exit__` methods). When the context is entered, it should log a message indicating the context has started. Upon exiting, it should log a message that the context has ended. 4. **Logging Functionality**: - Create a helper function `log_message` that takes a message and the `log_file` to which it appends the message. # Input and Output - **Initialization**: An instance of `LoggingContextDataClass` can be created as follows: ```python obj = LoggingContextDataClass(name=\\"example\\", value=\\"not_a_number\\", log_file=\\"log.txt\\") ``` The above should correct the `value` to `0` and log a warning to `log.txt`. - **Context Usage**: ```python with obj as context: # Perform some operations pass ``` This should log the entry and exit messages to `log.txt`. # Example ``` # Test the LoggingContextDataClass obj = LoggingContextDataClass(name=\\"test\\", value=\\"10\\", log_file=\\"test_log.txt\\") # Using the object with a context manager with obj: print(f\\"Using context for {obj.name}\\") # Check the log file for correct messages ``` # Constraints - You must use the `dataclasses` module for the data class implementation. - Ensure the logging messages are distinct and contain timestamps. - You should handle potential exceptions within the `__exit__` method and log accordingly. # Implementation ```python import dataclasses import contextlib from datetime import datetime def log_message(message: str, log_file: str): with open(log_file, \\"a\\") as f: f.write(f\\"{datetime.now()}: {message}n\\") @dataclasses.dataclass class LoggingContextDataClass: name: str value: int log_file: str def __post_init__(self): if not isinstance(self.value, int): log_message(f\\"Warning: value was not an integer, setting to 0\\", self.log_file) self.value = 0 def __enter__(self): log_message(f\\"Entering context for {self.name}\\", self.log_file) return self def __exit__(self, exc_type, exc_value, traceback): if exc_type: log_message(f\\"Exception occurred: {exc_value}\\", self.log_file) log_message(f\\"Exiting context for {self.name}\\", self.log_file) ``` In this question, students need to demonstrate their understanding of data classes, post-initialization processing, and context management in Python, along with practical logging implementation.","solution":"import dataclasses from contextlib import contextmanager from datetime import datetime def log_message(message: str, log_file: str): with open(log_file, \\"a\\") as f: f.write(f\\"{datetime.now()}: {message}n\\") @dataclasses.dataclass class LoggingContextDataClass: name: str value: int log_file: str def __post_init__(self): if not isinstance(self.value, int): log_message(f\\"Warning: value was not an integer, setting to 0\\", self.log_file) self.value = 0 def __enter__(self): log_message(f\\"Entering context for {self.name}\\", self.log_file) return self def __exit__(self, exc_type, exc_value, traceback): if exc_type: log_message(f\\"Exception occurred: {exc_value}\\", self.log_file) log_message(f\\"Exiting context for {self.name}\\", self.log_file)"},{"question":"Objective Demonstrate your understanding of the seaborn library, specifically focusing on the generation and customization of color palettes using the `husl_palette` function. Additionally, use the generated color palettes to create a meaningful plot. Question You are required to implement a function `custom_plot_with_palettes(data, num_colors, lightness, saturation, hue_start)`. This function performs the following steps: 1. **Generate a HUSL Palette**: - Use the `sns.husl_palette` function to generate a palette. - The palette should be generated with: - `num_colors`: An integer specifying the number of colors in the palette. - `lightness`: A float specifying the lightness of the palette colors (range 0 to 1). - `saturation`: A float specifying the saturation of the palette colors (range 0 to 1). - `hue_start`: A float specifying the start point for hue sampling (range 0 to 1). 2. **Create a Plot**: - Use seaborn to create a scatter plot using the palette: - Plot `sepal_length` vs `sepal_width` from the provided data. - Color the points based on the `species` column using the generated palette. 3. **Plot Customization**: - Set an appropriate title for the plot including details like the number of colors, lightness, and saturation. - Include legends, axis labels, and any other customization to make the plot informative. Expected Input and Output - **Input**: - `data`: A DataFrame containing iris dataset with columns `sepal_length`, `sepal_width`, and `species`. - `num_colors`: Integer, specifying the number of colors in the palette. - `lightness`: Float, the lightness for the palette colors (0 to 1). - `saturation`: Float, the saturation for the palette colors (0 to 1). - `hue_start`: Float, the starting point for hue sampling (0 to 1). - **Output**: A seaborn scatter plot displaying the `sepal_length` and `sepal_width` data points colored by `species` and the palette described by the input parameters. Performance Requirements - The function should execute efficiently for a typical iris dataset size, with minimal latency when generating plots and palettes. Constraints - Ensure to validate the input parameters such that `num_colors` is a positive integer, and `lightness`, `saturation`, and `hue_start` are floats within the range [0, 1]. Example Usage ```python import seaborn as sns import pandas as pd # Load iris dataset data = sns.load_dataset(\'iris\') # Call the function custom_plot_with_palettes(data, num_colors=5, lightness=0.5, saturation=0.7, hue_start=0.2) ``` The expected output would be a scatter plot displayed according to the description above, with the seaborn customization utilizing the generated HUSL palette.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plot_with_palettes(data, num_colors, lightness, saturation, hue_start): Create a scatter plot using a custom HUSL palette. Parameters: - data (DataFrame): A pandas DataFrame containing \'sepal_length\', \'sepal_width\', and \'species\' columns. - num_colors (int): Number of colors in the palette. - lightness (float): Lightness of the palette colors in the range [0, 1]. - saturation (float): Saturation of the palette colors in the range [0, 1]. - hue_start (float): Starting point for hue sampling in the range [0, 1]. # Input validation if not (0 <= lightness <= 1): raise ValueError(\\"Lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"Saturation must be between 0 and 1\\") if not (0 <= hue_start <= 1): raise ValueError(\\"Hue start must be between 0 and 1\\") if num_colors <= 0: raise ValueError(\\"Number of colors must be a positive integer\\") # Generate the HUSL palette palette = sns.husl_palette(n_colors=num_colors, l=lightness*100, s=saturation*100, h=hue_start*360) # Create the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette, s=100) plt.title(f\'Sepal Length vs Sepal Width with {num_colors}-color HUSL palettenLightness: {lightness}, Saturation: {saturation}\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.show()"},{"question":"**Objective:** To assess your ability to customize seaborn plots using advanced configuration settings provided by the seaborn.objects interface. **Problem Statement:** You are given a dataset representing the performance metrics of various machine learning models, and you need to visualize this data using seaborn. Specifically, you are required to: 1. Load the dataset into a pandas DataFrame. 2. Create a scatter plot showing the relationship between two metrics (e.g., `accuracy` and `f1_score`). 3. Customize the plot to have a specific theme and display configuration. **Dataset:** The dataset is provided as a CSV file named `model_performance.csv` with the following columns: - `model_name`: the name of the machine learning model. - `accuracy`: accuracy score of the model. - `precision`: precision score of the model. - `recall`: recall score of the model. - `f1_score`: F1 score of the model. # Task 1. **Read the dataset** into a pandas DataFrame. 2. **Create a scatter plot** with `accuracy` on the x-axis and `f1_score` on the y-axis. 3. **Apply theme configurations**: - Set the plot\'s background color to white. - Use the \\"whitegrid\\" style from seaborn. - Synchronize the plot with matplotlib\'s global state parameters. 4. **Adjust display settings**: - Set the display format to SVG. - Disable HiDPI scaling. - Set the scaling factor of the plotted image to 0.7. 5. **Return the plot** object for display. **Input:** - The input will be the path to the CSV file: `model_performance.csv`. **Output:** - The output must be a seaborn plot object displayed inline with the settings applied as described. **Constraints:** - Ensure that all configurations are applied using the `so.Plot.config` interface properly. ```python import pandas as pd import seaborn.objects as so import matplotlib as mpl from seaborn import axes_style def plot_model_performance(csv_path: str): # Step 1: Read dataset df = pd.read_csv(csv_path) # Step 2: Create scatter plot plot = so.Plot(df, x=\'accuracy\', y=\'f1_score\').add(so.Dot()) # Step 3: Apply theme configurations so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) so.Plot.config.theme.update(mpl.rcParams) # Step 4: Adjust display settings so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.7 # Display plot return plot # Example usage (assumes CSV file is available at the specified path) # plot = plot_model_performance(\\"model_performance.csv\\") # plot.show() ``` **Additional Notes:** - Ensure you have the necessary libraries installed (pandas, seaborn, matplotlib). - Verify the CSV file path and column names before running the function.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_model_performance(csv_path: str): # Step 1: Read dataset df = pd.read_csv(csv_path) # Step 2: Create scatter plot with seaborn sns.set_theme(style=\\"whitegrid\\") plt.figure(figsize=(10, 6)) plot = sns.scatterplot(x=\'accuracy\', y=\'f1_score\', data=df) # Step 3: Apply theme configurations plot.set_facecolor(\'white\') # Step 4: Adjust display settings plt.rcParams[\'figure.dpi\'] = 100 # Disable HiDPI scaling plt.rcParams[\'figure.figsize\'] = (10 * 0.7, 6 * 0.7) # Scale image size # Return the plot for display return plot"},{"question":"# PyTorch Fine-Grained Tracing Control with TorchDynamo Objective: To validate your understanding of PyTorch\'s TorchDynamo APIs for fine-grained tracing control by implementing and controlling the behavior of a neural network model with parts that need to bypass Dynamo tracing. Problem Statement: You are tasked with creating a simple neural network model using PyTorch and implementing functions that utilize TorchDynamo APIs to control the compilation of specific parts of the model. You will implement and test the following functions and verify their effects on the model. Task Specifications: 1. **Create a simple neural network model** using `torch.nn.Module`. The model should include at least three layers including `torch.nn.Linear` and `torch.nn.ReLU`. 2. **Implement the following functions** based on the provided model: - `compile_disable(model)`: Disables TorchDynamo tracing on the entire model. - `disallow_add_op_in_graph(model)`: Disallows the `torch.add` operation within the model\'s computation graph. - `allow_custom_op_in_graph(op)`: Uses `torch.compile.allow_in_graph` to allow a custom operation (such as a custom autograd function) in the TorchDynamo graph. 3. **Testing the implementation**: - Demonstrate the `compile_disable` function by creating a sample input tensor, compiling the model using `torch.compile`, and showing that a specific part of the model execution does not undergo tracing. - Demonstrate the `disallow_add_op_in_graph` function showing that `torch.add` operations in the model execution graph will break the graph and run in eager mode. - Demonstrate the use of `allow_custom_op_in_graph(op)` with a simple custom operation within the model execution and verify successful tracing compilation. Input and Output Formats: - **Input**: - A PyTorch model created using `torch.nn.Module`. - Operations and tensors as required for each function. - **Output**: - Print statements demonstrating the expected behavior for each of the implemented functions. - You can use dummy data to demonstrate the effects as visual verification is typically required. ```python import torch import torch.nn as nn # Sample neural network model class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.layer1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.layer2 = nn.Linear(20, 10) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x # Function to disable tracing on the model def compile_disable(model): @torch.compiler.disable def forward_disabled(x): return model(x) return forward_disabled # Function to disallow the \'add\' operation in the Graph def disallow_add_op_in_graph(model): def forward(x): torch._dynamo.disallow_in_graph(torch.ops.aten.add) return model(x) return forward # Function to allow a custom operation in the Graph def allow_custom_op_in_graph(op): @torch.compile.allow_in_graph def custom_op(x): return op(x) return custom_op if __name__ == \\"__main__\\": model = SampleModel() x = torch.randn(10) # Example usage demonstrating disabling compilation model_disabled = compile_disable(model) print(\\"Output with compilation disabled:\\", model_disabled(x)) # Example usage demonstrating disallowing \'add\' op in graph model_disallowed_add = disallow_add_op_in_graph(model) print(\\"Output with \'add\' op disallowed in graph:\\", model_disallowed_add(x)) # Placeholder for a custom operation example class CustomOp(torch.autograd.Function): @staticmethod def forward(ctx, x): return x * 2 @staticmethod def backward(ctx, grad_output): return grad_output * 2 custom_op_instance = CustomOp.apply custom_op_instance_allowed = allow_custom_op_in_graph(custom_op_instance) print(\\"Output with custom op allowed in graph:\\", custom_op_instance_allowed(x)) ``` Constraints: 1. Ensure that the `torch.compiler` and `torch._dynamo` APIs are used accurately as described. 2. Pay careful attention to the behavior changes induced by these APIs on the model\'s compilation and execution. # Note: Please use `torchdynamo_fine_grain_tracing` functionalities as described in the provided documentation.","solution":"import torch import torch.nn as nn import torch._dynamo as dynamo # Sample neural network model class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.layer1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.layer2 = nn.Linear(20, 10) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x # Function to disable tracing on the model def compile_disable(model): @dynamo.disable def forward_disabled(x): return model(x) return forward_disabled # Function to disallow the \'add\' operation in the Graph def disallow_add_op_in_graph(model): def forward(x): dynamo.disallow_in_graph(torch.ops.aten.add) return model(x) return forward # Function to allow a custom operation in the Graph def allow_custom_op_in_graph(op): @dynamo.allow_in_graph def custom_op(x): return op(x) return custom_op # Placeholder for a custom operation example class CustomOp(torch.autograd.Function): @staticmethod def forward(ctx, x): return x * 2 @staticmethod def backward(ctx, grad_output): return grad_output * 2 custom_op_instance = CustomOp.apply if __name__ == \\"__main__\\": model = SampleModel() x = torch.randn(1, 10) # Example usage demonstrating disabling compilation model_disabled = compile_disable(model) print(\\"Output with compilation disabled:\\", model_disabled(x)) # Example usage demonstrating disallowing \'add\' op in graph model_disallowed_add = disallow_add_op_in_graph(model) print(\\"Output with \'add\' op disallowed in graph:\\", model_disallowed_add(x)) # Example usage of custom operation with allowed in graph custom_op_instance_allowed = allow_custom_op_in_graph(custom_op_instance) print(\\"Output with custom op allowed in graph:\\", custom_op_instance_allowed(x))"},{"question":"Context: You are tasked with developing a simple tool that maintains individual user configurations in a multithreaded application. To achieve this, you will utilize `ContextVar` to store and reset user configurations during each request processing. Additionally, you will need to ensure that changes to the configuration do not affect other requests. Task: 1. **Implement a function `process_user_request(user_id: int, config_changes: dict) -> dict`**: - This function will simulate processing a user\'s request by: - Setting user-specific configurations using `ContextVar`. - Applying configuration changes provided in `config_changes`. - Returning the updated configurations for the user. 2. **Implement a function `handle_multiple_requests(requests: list) -> list`**: - This function will receive a list of requests, where each request is a tuple containing `user_id` and `config_changes`. - Use the `ThreadPoolExecutor` to process each request concurrently. - Collect and return the results for all requests. Input: - `user_id` (int): The unique identifier for the user. - `config_changes` (dict): Dictionary of configuration changes for the user. - `requests` (list): List of tuples where each tuple contains `user_id` and `config_changes`. Output: - `process_user_request` returns a dictionary with the updated configuration for the user. - `handle_multiple_requests` returns a list of updated configurations for all requests. Example: ```python # Example Request requests = [ (1, {\\"theme\\": \\"dark mode\\"}), (2, {\\"notifications\\": \\"disabled\\"}), (1, {\\"theme\\": \\"light mode\\", \\"notifications\\": \\"enabled\\"}) ] # Expected Output [ {\\"theme\\": \\"dark mode\\", \\"notifications\\": \\"enabled\\"}, {\\"theme\\": \\"default\\", \\"notifications\\": \\"disabled\\"}, {\\"theme\\": \\"light mode\\", \\"notifications\\": \\"enabled\\"} ] ``` # Constraints: - You may assume the default configurations for any user are `{\\"theme\\": \\"default\\", \\"notifications\\": \\"enabled\\"}`. - Ensure that each user\'s configurations remain isolated from others, even in concurrent settings. # Notes: - Use `ContextVar` to store and manage the configurations. - Make use of the `Token` class to reset configurations as needed. - Handle any necessary context management to ensure thread safety. # Implementation: ```python from contextvars import ContextVar from concurrent.futures import ThreadPoolExecutor # Initialize ContextVar for user configurations user_config_var = ContextVar(\'user_config\', default={\\"theme\\": \\"default\\", \\"notifications\\": \\"enabled\\"}) def process_user_request(user_id: int, config_changes: dict) -> dict: # Your implementation here pass def handle_multiple_requests(requests: list) -> list: # Your implementation here pass ```","solution":"from contextvars import ContextVar from concurrent.futures import ThreadPoolExecutor # Initialize ContextVar for user configurations user_config_var = ContextVar(\'user_config\', default={\\"theme\\": \\"default\\", \\"notifications\\": \\"enabled\\"}) def process_user_request(user_id: int, config_changes: dict) -> dict: # Get the current configuration current_config = user_config_var.get() # Update the configuration with changes updated_config = current_config.copy() updated_config.update(config_changes) # Set the updated configuration in ContextVar token = user_config_var.set(updated_config) try: # Return the updated configuration for the user return user_config_var.get() finally: # Reset the configuration back to original user_config_var.reset(token) def handle_multiple_requests(requests: list) -> list: results = [] def handle_single_request(request): user_id, config_changes = request return process_user_request(user_id, config_changes) with ThreadPoolExecutor() as executor: results = list(executor.map(handle_single_request, requests)) return results"},{"question":"**Memory Management in Python** As a Python developer, you are tasked with writing a set of functions to manage memory allocation for a list of integers using the Python/C memory management API functions described in the provided documentation. The aim is to dynamically allocate, resize, and deallocate a memory block used to store these integers, ensuring efficient memory management and avoiding memory corruption. # Requirements: 1. **Memory Allocation (`allocate_memory`):** - Allocate memory for an initial list of integers. - Function Signature: `allocate_memory(size: int) -> ctypes.POINTER(ctypes.c_int)` - Arguments: - `size`: The number of integers for which memory is to be allocated. - Returns: - A pointer to the allocated memory. 2. **Memory Resizing (`resize_memory`):** - Resize the previously allocated memory block to accommodate a different number of integers. - Function Signature: `resize_memory(mem_pointer: ctypes.POINTER(ctypes.c_int), new_size: int) -> ctypes.POINTER(ctypes.c_int)` - Arguments: - `mem_pointer`: A pointer to the currently allocated memory. - `new_size`: The new number of integers to accommodate. - Returns: - A pointer to the resized memory block. 3. **Memory Deallocation (`free_memory`):** - Free the allocated memory to prevent memory leaks. - Function Signature: `free_memory(mem_pointer: ctypes.POINTER(ctypes.c_int)) -> None` - Arguments: - `mem_pointer`: A pointer to the memory to be freed. # Constraints: - Zero-byte allocation must return a distinct non-`NULL` pointer if possible. - The allocation and deallocation functions must be used consistently to avoid memory corruption. # Example: ```python import ctypes # Allocating memory for 10 integers ptr = allocate_memory(10) # Resizing memory to accommodate 20 integers ptr = resize_memory(ptr, 20) # Freeing the allocated memory free_memory(ptr) ``` # Performance Requirements: - Efficient handling and resizing of memory blocks such that the minimum amount of reallocation occurs. - Proper management of memory to avoid leaks and undefined behavior. # Additional Notes: - Utilize the `ctypes` library to interface with the C memory management API functions. - Ensure to handle edge cases such as allocation of zero bytes or null pointers. Implement the required functions by utilizing the provided Python/C API functions for memory management.","solution":"import ctypes def allocate_memory(size: int) -> ctypes.POINTER(ctypes.c_int): Allocate memory for a list of integers. Args: - size (int): The number of integers for which memory is to be allocated. Returns: - ctypes.POINTER(ctypes.c_int): A pointer to the allocated memory. if size <= 0: size = 1 # Allocate at least one byte if requested size is zero or negative return ctypes.cast(ctypes.create_string_buffer(size * ctypes.sizeof(ctypes.c_int)), ctypes.POINTER(ctypes.c_int)) def resize_memory(mem_pointer: ctypes.POINTER(ctypes.c_int), new_size: int) -> ctypes.POINTER(ctypes.c_int): Resize the previously allocated memory block to accommodate a different number of integers. Args: - mem_pointer (ctypes.POINTER(ctypes.c_int)): A pointer to the currently allocated memory. - new_size (int): The new number of integers to accommodate. Returns: - ctypes.POINTER(ctypes.c_int): A pointer to the resized memory block. if new_size <= 0: new_size = 1 # Allocate at least one byte if requested size is zero or negative return ctypes.cast(ctypes.create_string_buffer(new_size * ctypes.sizeof(ctypes.c_int)), ctypes.POINTER(ctypes.c_int)) def free_memory(mem_pointer: ctypes.POINTER(ctypes.c_int)) -> None: Free the allocated memory to prevent memory leaks. Args: - mem_pointer (ctypes.POINTER(ctypes.c_int)): A pointer to the memory to be freed. # Memory allocated by ctypes does not require explicit free as it is managed by Python\'s garbage collector pass"},{"question":"# Question: Create and Serialize a Complex Multipart Email **Objective:** You are required to create a complex multipart email message using the `email.mime` module. The email should contain text content, an image, an audio clip, and an attachment (application type). Then, serialize the email to a string format for sending. **Requirements:** 1. The email should have the following structure: - A plain text message as the main content. - An attachment in the form of an application (e.g., a PDF file). - An embedded image. - An embedded audio clip. 2. The serialized email should be in a format that can be sent using an SMTP server. **Input:** 1. Plain text content as a string. 2. Image data as bytes. 3. Audio data as bytes. 4. Application data (e.g., PDF) as bytes. 5. Appropriate MIME subtypes for the image, audio, and application. **Output:** A string representing the serialized multipart email. **Constraints:** - You may assume that the input data for the image, audio, and application are valid and are provided as bytes. - Ensure that all standard email headers (like `From`, `To`, `Subject`) are included in the email. **Performance Requirements:** The implementation should efficiently handle the creation and serialization of the email without any unnecessary overhead. **Example Usage:** ```python from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email.mime.application import MIMEApplication from email.mime.multipart import MIMEMultipart from email import encoders import base64 def create_complex_email(text_content, image_data, image_subtype, audio_data, audio_subtype, app_data, app_subtype, sender, recipient, subject): # Create the root multipart message msg = MIMEMultipart() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Attach the plain text message text_part = MIMEText(text_content, \'plain\') msg.attach(text_part) # Attach the image image_part = MIMEImage(image_data, _subtype=image_subtype) msg.attach(image_part) # Attach the audio audio_part = MIMEAudio(audio_data, _subtype=audio_subtype) msg.attach(audio_part) # Attach the application data app_part = MIMEApplication(app_data, _subtype=app_subtype) encoders.encode_base64(app_part) app_part.add_header(\'Content-Disposition\', \'attachment\', filename=\\"attachment.pdf\\") msg.attach(app_part) # Serialize the email to a string email_str = msg.as_string() return email_str # Example inputs text_content = \\"Hello, this is a test email with multiple attachments!\\" image_data = base64.b64decode(\\"...encoded image data...\\") image_subtype = \\"png\\" audio_data = base64.b64decode(\\"...encoded audio data...\\") audio_subtype = \\"wav\\" app_data = base64.b64decode(\\"...encoded application data...\\") app_subtype = \\"octet-stream\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" subject = \\"Test Email\\" # Call the function email_str = create_complex_email(text_content, image_data, image_subtype, audio_data, audio_subtype, app_data, app_subtype, sender, recipient, subject) print(email_str) ``` Implement the `create_complex_email` function to achieve the described functionality.","solution":"from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email.mime.application import MIMEApplication from email.mime.multipart import MIMEMultipart from email import encoders def create_complex_email(text_content, image_data, image_subtype, audio_data, audio_subtype, app_data, app_subtype, sender, recipient, subject): Creates a complex multipart email with text, image, audio, and attachment components. Parameters: - text_content (str): Plain text content of the email. - image_data (bytes): Byte data of the image. - image_subtype (str): MIME subtype of the image (e.g., \\"png\\"). - audio_data (bytes): Byte data of the audio clip. - audio_subtype (str): MIME subtype of the audio (e.g., \\"wav\\"). - app_data (bytes): Byte data of the application file (e.g., PDF). - app_subtype (str): MIME subtype of the application (e.g., \\"pdf\\"). - sender (str): Email address of the sender. - recipient (str): Email address of the recipient. - subject (str): Subject of the email. Returns: - str: Serialized multipart email ready for sending via SMTP. # Create the root multipart message msg = MIMEMultipart() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Attach the plain text message text_part = MIMEText(text_content, \'plain\') msg.attach(text_part) # Attach the image image_part = MIMEImage(image_data, _subtype=image_subtype) msg.attach(image_part) # Attach the audio audio_part = MIMEAudio(audio_data, _subtype=audio_subtype) msg.attach(audio_part) # Attach the application data app_part = MIMEApplication(app_data, _subtype=app_subtype) encoders.encode_base64(app_part) app_part.add_header(\'Content-Disposition\', \'attachment\', filename=\\"attachment.pdf\\") msg.attach(app_part) # Serialize the email to a string email_str = msg.as_string() return email_str"},{"question":"# Custom Cookie Manager using `http.cookies` Your task is to implement a custom cookie manager using the `http.cookies` module to handle a user\'s session. This custom manager should be able to perform the following operations: 1. **Add a Cookie**: Add a new cookie with a specified name and value. 2. **Get a Cookie**: Retrieve the value of a cookie by its name. 3. **Delete a Cookie**: Delete a cookie by its name. 4. **Load Cookies from a Header String**: Load cookies from an HTTP header string into the manager. 5. **Generate Cookie Header**: Generate an HTTP header string representing all cookies in the manager. # Function Signature: You should implement the following class with the specified methods: ```python from http import cookies class CustomCookieManager: def __init__(self): self.cookie = cookies.SimpleCookie() def add_cookie(self, name: str, value: str) -> None: pass def get_cookie(self, name: str) -> str: pass def delete_cookie(self, name: str) -> None: pass def load_from_header(self, header: str) -> None: pass def generate_header(self) -> str: pass ``` # Detailed Requirements: 1. **`add_cookie(name: str, value: str) -> None`** - Adds a cookie with the given `name` and `value`. - If a cookie with that name already exists, it should update its value. 2. **`get_cookie(name: str) -> str`** - Returns the value of the cookie with the provided `name`. - If the cookie does not exist, raise a `KeyError`. 3. **`delete_cookie(name: str) -> None`** - Deletes the cookie with the given `name`. - If the cookie does not exist, raise a `KeyError`. 4. **`load_from_header(header: str) -> None`** - Parses an HTTP header string and loads cookies into the manager. - Example header string: `\\"name1=value1; name2=value2\\"` 5. **`generate_header() -> str`** - Generates an HTTP header string for all currently stored cookies. - Example output: `\\"Set-Cookie: name1=value1; name2=value2\\"` # Constraints: - Cookie names and values will only consist of valid characters (`string.ascii_letters`, `string.digits`, and `!#%&\'*+-.^_`|~`). - Assume inputs are well-formed; you do not need to handle incorrect formats. - Use `SimpleCookie` to store cookies internally. # Example Usage: ```python manager = CustomCookieManager() manager.add_cookie(\\"session_id\\", \\"abc123\\") assert manager.get_cookie(\\"session_id\\") == \\"abc123\\" manager.delete_cookie(\\"session_id\\") try: manager.get_cookie(\\"session_id\\") except KeyError: print(\\"Cookie not found\\") header = \\"name1=value1; name2=value2\\" manager.load_from_header(header) assert manager.get_cookie(\\"name1\\") == \\"value1\\" assert manager.generate_header() == \\"Set-Cookie: name1=value1; name2=value2\\" ``` Implement the `CustomCookieManager` class to fulfill the requirements.","solution":"from http import cookies class CustomCookieManager: def __init__(self): self.cookie = cookies.SimpleCookie() def add_cookie(self, name: str, value: str) -> None: self.cookie[name] = value def get_cookie(self, name: str) -> str: if name in self.cookie: return self.cookie[name].value else: raise KeyError(f\\"Cookie \'{name}\' not found\\") def delete_cookie(self, name: str) -> None: if name in self.cookie: del self.cookie[name] else: raise KeyError(f\\"Cookie \'{name}\' not found\\") def load_from_header(self, header: str) -> None: self.cookie.load(header) def generate_header(self) -> str: cookies_list = [f\\"{key}={value.value}\\" for key, value in self.cookie.items()] return f\\"Set-Cookie: {\'; \'.join(cookies_list)}\\""},{"question":"Coding Assessment Question You are tasked with building a classifier that aids in medical diagnostics for detecting a specific type of tumor. The dataset is heavily imbalanced, with the majority class being non-cancerous samples. Your objective is to maximize the recall (sensitivity) of the model to ensure that as many true positive cases (cancerous samples) are detected as possible, even at the cost of accepting more false positives. Implement a class that: 1. Trains a classifier on the provided dataset. 2. Tunes the decision threshold to maximize recall. 3. Evaluates the final model performance with the tuned threshold using both precision and recall metrics. # Dataset Assume you are given a dataset `data` in the form of a pandas DataFrame with the following structure: - Features: `feature_1`, `feature_2`, ..., `feature_n`. - Target: `target`, with values 0 (non-cancerous) and 1 (cancerous). # Requirements 1. **Input**: pandas DataFrame `data`. 2. **Output**: A tuple containing (precision, recall) of the model evaluated on a test set. 3. **Constraints**: - Use a DecisionTreeClassifier as the base model. - Use stratified 5-fold cross-validation for tuning the threshold. - Set the random state to 42 for reproducibility. - You are expected to split the data into training and testing sets with a test size of 20%. # Function Signature ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import make_scorer, recall_score, precision_score from sklearn.model_selection import TunedThresholdClassifierCV def tune_and_evaluate(data: pd.DataFrame) -> (float, float): pass ``` # Example Usage ```python import pandas as pd data = pd.read_csv(\'tumor_data.csv\') precision, recall = tune_and_evaluate(data) print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") ``` # Implementation Steps 1. **Data Splitting**: - Split the dataset into training and testing sets. 2. **Model Training**: - Write a sub-function or inline code to create a `DecisionTreeClassifier` and fit it on the training data. 3. **Threshold Tuning**: - Use `TunedThresholdClassifierCV` to tune the threshold to maximize recall. 4. **Model Evaluation**: - Evaluate the model on the test set to get the precision and recall scores. - Return these scores as output. # Additional Notes - Ensure your implementation accounts for potential overfitting by correctly using cross-validation techniques. - Recall that maximizing recall means prioritizing the detection of as many positive cases as possible, despite accepting more false positives.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import make_scorer, recall_score, precision_score, precision_recall_curve import numpy as np class TunedThresholdClassifierCV: def __init__(self, estimator, cv, random_state=42): self.estimator = estimator self.cv = cv self.random_state = random_state self.threshold = 0.5 def fit(self, X, y): skf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state) recalls = [] thresholds = [] for train_index, val_index in skf.split(X, y): X_train, X_val = X[train_index], X[val_index] y_train, y_val = y[train_index], y[val_index] self.estimator.fit(X_train, y_train) probas = self.estimator.predict_proba(X_val)[:, 1] precision, recall, threshold = precision_recall_curve(y_val, probas) recalls.extend(recall) thresholds.extend(threshold) recalls = np.array(recalls) thresholds = np.array(thresholds) # Pick the threshold which gives the highest recall self.threshold = thresholds[np.argmax(recalls)] def predict(self, X): probas = self.estimator.predict_proba(X)[:, 1] return (probas >= self.threshold).astype(int) def tune_and_evaluate(data: pd.DataFrame) -> (float, float): X = data.drop(columns=[\'target\']).values y = data[\'target\'].values # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42) # Initialize base classifier base_classifier = DecisionTreeClassifier(random_state=42) # Initialize tuned threshold classifier cv = 5 tuned_threshold_classifier = TunedThresholdClassifierCV(estimator=base_classifier, cv=cv, random_state=42) # Fit the model using cross-validation to tune the threshold tuned_threshold_classifier.fit(X_train, y_train) # Predict the test set y_pred = tuned_threshold_classifier.predict(X_test) # Compute precision and recall precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) return precision, recall"},{"question":"**Objective**: The goal of this exercise is to assess your ability to use Python\'s `re` module for processing and extracting information from text files using regular expressions. **Problem Statement**: You have been provided a text log from a web server, where each line represents an HTTP request. Your task is to extract and analyze specific pieces of information from each request using regular expressions. Each log entry follows this format: ``` [ip_address] - - [datetime] \\"HTTP_method /path/to/resource HTTP_version\\" status_code response_size ``` # Example Log Entries: ``` 192.168.1.1 - - [10/Oct/2023:13:55:36] \\"GET /index.html HTTP/1.1\\" 200 1043 123.45.67.89 - - [10/Oct/2023:13:56:12] \\"POST /submit HTTP/1.1\\" 404 512 254.123.0.56 - - [10/Oct/2023:14:01:42] \\"GET /images/photo.jpg HTTP/2.0\\" 200 2048 ``` # Task: 1. Write a function `extract_log_info(log_line: str) -> dict` that takes a single log entry as input and returns a dictionary with the extracted data: - `ip_address`: The IP address of the client. - `datetime`: The date and time of the request. - `method`: The HTTP method (e.g., GET, POST). - `path`: The requested path (e.g., /index.html). - `status_code`: The status code of the response. - `response_size`: The size of the response in bytes. # Expected Input: The function `extract_log_info` should be able to handle a string representing a single log entry. # Expected Output: The function should return a dictionary with keys `ip_address`, `datetime`, `method`, `path`, `status_code`, and `response_size`. # Constraints: - You can assume that the log entries are well-formed and follow the given pattern. # Example: ```python log_line = \'192.168.1.1 - - [10/Oct/2023:13:55:36] \\"GET /index.html HTTP/1.1\\" 200 1043\' result = extract_log_info(log_line) expected_result = { \'ip_address\': \'192.168.1.1\', \'datetime\': \'10/Oct/2023:13:55:36\', \'method\': \'GET\', \'path\': \'/index.html\', \'status_code\': \'200\', \'response_size\': \'1043\' } assert result == expected_result ``` # Guidelines: - Use Python\'s `re` module to write a regular expression that matches the given log entry format. - Utilize capturing groups to extract the necessary components from the log entry. - Ensure your function returns the data in the specified structure. # Implementation: ```python import re from typing import Dict def extract_log_info(log_line: str) -> Dict[str, str]: pattern = r\'(?P<ip_address>S+) - - [(?P<datetime>[^]]+)] \\"(?P<method>[A-Z]+) (?P<path>S+) HTTP/[0-9.]+\\" (?P<status_code>d+) (?P<response_size>d+)\' match = re.match(pattern, log_line) if match: return match.groupdict() else: return {} # Example usage: log_line = \'192.168.1.1 - - [10/Oct/2023:13:55:36] \\"GET /index.html HTTP/1.1\\" 200 1043\' print(extract_log_info(log_line)) ``` # Additional Notes: - The function should handle edge cases where the patterns might not match exactly, and return an empty dictionary in such cases. - You can add further log analysis based on this function, such as counting request types, analyzing response codes, etc.","solution":"import re from typing import Dict def extract_log_info(log_line: str) -> Dict[str, str]: Extracts information from a single web server log entry. Args: log_line (str): A string representing a single log entry. Returns: Dict[str, str]: A dictionary containing extracted log details. pattern = r\'(?P<ip_address>S+) - - [(?P<datetime>[^]]+)] \\"(?P<method>[A-Z]+) (?P<path>S+) HTTP/[0-9.]+\\" (?P<status_code>d+) (?P<response_size>d+)\' match = re.match(pattern, log_line) if match: return match.groupdict() else: return {}"},{"question":"# Seaborn Coding Assessment Objective: Demonstrate your understanding of Seaborn\'s `seaborn.objects` module by creating, customizing, and interpreting plots. Problem Statement: You are given the Anscombe\'s quartet dataset, which contains four different datasets (I, II, III, and IV), each having a similar statistical profile. Your task is to: 1. Load this dataset using Seaborn. 2. Create a faceted plot where each dataset in the quartet is shown in its subplot. 3. Apply a linear regression line to each subplot. 4. Customize the appearance of your plots using Seaborn\'s and Matplotlib\'s theme and style parameters. Specifically, customize the following: - Change the background color of the plots to white and edge color to slategray. - Set the linewidth of the lines in the plot to 4. - Use the \\"ticks\\" style from Seaborn. 5. Combine multiple parameter dictionaries for styling. 6. Configure the default theme for all `Plot` instances to use the \\"white\\" axes style. Input: The Anscombe\'s quartet dataset is intrinsic to Seaborn and should be loaded using `seaborn.load_dataset(\\"anscombe\\")`. Output: A faceted plot with the aforementioned customizations applied. Constraints: - Use the `seaborn.objects` module for plotting. - Ensure your code is clean and well-documented. Required Libraries: - `seaborn` - `matplotlib` Example: Here is a partial code snippet to get you started with loading the dataset: ```python import seaborn.objects as so from seaborn import load_dataset # Load the Anscombe\'s quartet dataset anscombe = load_dataset(\\"anscombe\\") # Create the initial plot with faceting p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Apply custom themes and styles here ``` Use this snippet as a starting point and implement the complete functionality as per the problem statement. Good luck!","solution":"import seaborn.objects as so import seaborn as sns from matplotlib import pyplot as plt # Load the Anscombe\'s quartet dataset anscombe = sns.load_dataset(\\"anscombe\\") # Customize the plot\'s appearance custom_style = { \'axes.edgecolor\': \'slategray\', \'axes.facecolor\': \'white\', \'lines.linewidth\': 4, \'axes.grid\': False } sns.set_theme(style=\\"ticks\\", rc=custom_style) # Create a faceted plot with a linear regression line p = ( so.Plot(anscombe, x=\\"x\\", y=\\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) .layout(size=(10, 10)) # Adjust size of the entire plot ) p.show()"},{"question":"Background In Python, the `asyncio` module facilitates writing concurrent code using the `async/await` syntax. One of the core components of `asyncio` is the `Future` object, which represents a placeholder for a result that is initially unknown, but which is expected to become available at some point. Objective Write an asynchronous function `chain_futures(futures: List[asyncio.Future]) -> List[Any]` that chains multiple `Future` objects together in such a way that the completion of one triggers the start of the next one in the list. The function should return the results from all `Future` objects once they are all complete. Detailed Requirements 1. **Input:** - A list of `asyncio.Future` objects, `futures`, where each `Future` completes with a result after some delay. 2. **Output:** - A list containing the results of each `Future` object in the order in which they were provided. 3. **Constraints:** - Each `Future` object needs to be awaited in sequence. The next `Future` should not start until the previous one is done. - Do not assume the order of completion of the `Future` objects – only the given order of `futures` should be maintained. 4. **Additional Requirements:** - Use the provided functions and methods from the `asyncio.Future` documentation to implement your solution. - You are not allowed to use any shortcut methods that await all `Future` objects simultaneously (like `asyncio.gather`). Example ```python import asyncio async def set_future_result(fut, delay, result): await asyncio.sleep(delay) fut.set_result(result) async def main(): loop = asyncio.get_running_loop() fut1 = loop.create_future() loop.create_task(set_future_result(fut1, 1, \'result1\')) fut2 = loop.create_future() loop.create_task(set_future_result(fut2, 2, \'result2\')) fut3 = loop.create_future() loop.create_task(set_future_result(fut3, 3, \'result3\')) results = await chain_futures([fut1, fut2, fut3]) print(results) # Expected output: [\'result1\', \'result2\', \'result3\'] # Test with asyncio.run(main()) ``` Note: Make sure your solution adheres to Python 3.10 syntax and `asyncio` functionalities.","solution":"import asyncio from typing import List, Any async def chain_futures(futures: List[asyncio.Future]) -> List[Any]: results = [] for future in futures: result = await future results.append(result) return results"},{"question":"# XML SAX Parser Implementation You are tasked with implementing a custom SAX parser for processing XML documents in Python using the `xml.sax.xmlreader` package. Your goal is to create a class that parses an XML document, extracts specific elements and attributes, and processes the data. Follow the guidelines and requirements below. Requirements: 1. Implement a Python class `CustomSAXParser` that inherits from `xml.sax.xmlreader.XMLReader`. 2. The class should have methods to: - Parse an XML file and process its contents. - Feed chunks of XML data to the parser if it is an incremental parser. - Handle and log specific SAX events such as the start and end of elements and character data. 3. Your implementation should correctly manage ContentHandler, ErrorHandler, and other handlers as needed. 4. The parser should extract and print the names and values of all attributes for a specified element passed to the parser. For example, if the element specified is \\"item\\", the parser should print all attribute names and values whenever an \\"item\\" element is encountered. 5. (Optional) Support both full-document parsing and incremental parsing. Input Format: - The XML file to be parsed will be provided as a string containing the file path or URL. - The element whose attributes need to be printed will be provided as a string. Output Format: - Print the attribute names and values for each specified element encountered during parsing. Constraints: - You may assume the XML documents are well-formed and valid. - Performance is crucial for large XML files, so design your parser for efficiency. Example: ```python <catalog> <item id=\\"1\\" name=\\"item1\\" price=\\"10.00\\"/> <item id=\\"2\\" name=\\"item2\\" price=\\"20.00\\"/> <item id=\\"3\\" name=\\"item3\\" price=\\"30.00\\"/> </catalog> ``` If the specified element is \\"item\\", the output should be: ``` Element: item Attributes: {\'id\': \'1\', \'name\': \'item1\', \'price\': \'10.00\'} Element: item Attributes: {\'id\': \'2\', \'name\': \'item2\', \'price\': \'20.00\'} Element: item Attributes: {\'id\': \'3\', \'name\': \'item3\', \'price\': \'30.00\'} ``` Implementation: Write the `CustomSAXParser` class below: ```python import xml.sax.xmlreader class CustomSAXParser(xml.sax.xmlreader.XMLReader): def __init__(self): super().__init__() self.content_handler = None self.error_handler = None self.dtd_handler = None self.entity_resolver = None self.specified_element = \\"\\" def parse(self, source): # Implement parsing logic here pass def feed(self, data): # Implement incremental parsing logic here pass def close(self): # Implement close logic for incremental parsing pass def getContentHandler(self): return self.content_handler def setContentHandler(self, handler): self.content_handler = handler def getErrorHandler(self): return self.error_handler def setErrorHandler(self, handler): self.error_handler = handler # Implement other necessary methods and handlers # Define additional handlers such as ContentHandler, ErrorHandler, etc., as needed ``` **Note:** Ensure your implementation adheres to the guidelines and methods described in the provided `xml.sax.xmlreader` documentation.","solution":"import xml.sax class CustomContentHandler(xml.sax.ContentHandler): def __init__(self, specified_element): self.specified_element = specified_element self.is_specified_element = False def startElement(self, name, attrs): if name == self.specified_element: print(f\\"Element: {name}\\") print(f\\"Attributes: {dict(attrs.items())}\\") self.is_specified_element = True def endElement(self, name): if name == self.specified_element: self.is_specified_element = False def characters(self, content): pass class CustomSAXParser(xml.sax.xmlreader.XMLReader): def __init__(self): super().__init__() self.content_handler = None self.error_handler = None self.dtd_handler = None self.entity_resolver = None self.parser = xml.sax.make_parser() def parse(self, source, specified_element): self.content_handler = CustomContentHandler(specified_element) self.parser.setContentHandler(self.content_handler) self.parser.parse(source) # Incremental parsing methods def feed(self, data): self.parser.feed(data) def close(self): self.parser.close() def getContentHandler(self): return self.content_handler def setContentHandler(self, handler): self.content_handler = handler def getErrorHandler(self): return self.error_handler def setErrorHandler(self, handler): self.error_handler = handler def getDTDHandler(self): return self.dtd_handler def setDTDHandler(self, handler): self.dtd_handler = handler def getEntityResolver(self): return self.entity_resolver def setEntityResolver(self, handler): self.entity_resolver = handler"},{"question":"**Objective:** Implement a Python script that performs the following tasks using the `os` module: 1. Create a specified directory if it does not exist. 2. Within this directory, create a file and write specific environment variables and their values into this file. 3. Create a child process that reads the content of this file and outputs it to the standard output. 4. Ensure that the environment variable changes are made visible to the child process. **Description:** 1. **Directory and File Creation:** - Your script should create a directory with a specific path provided as an argument. - Within this directory, create a file named `env_info.txt`. 2. **Environment Variables Handling:** - Set three environment variables: `USER_NAME`, `COURSE_NAME`, and `COURSE_CODE` with values provided as arguments. - Write these environment variables and their values into `env_info.txt` in the following format: ``` USER_NAME: <value> COURSE_NAME: <value> COURSE_CODE: <value> ``` 3. **Process Creation and Output:** - Create a child process that reads the content of `env_info.txt` and prints it to the standard output. - Ensure the child process inherits the environment variables set by the parent process. **Function Signature:** ```python def manage_environment_and_process(dir_path: str, user_name: str, course_name: str, course_code: str) -> None: pass ``` **Input Arguments:** - `dir_path` (str): The path of the directory to be created. - `user_name` (str): The value to set for the `USER_NAME` environment variable. - `course_name` (str): The value to set for the `COURSE_NAME` environment variable. - `course_code` (str): The value to set for the `COURSE_CODE` environment variable. **Constraints:** - Assume the `dir_path` is a valid path and the script has permissions to create directories and files. - The environment variables are simple strings without special characters. **Example:** Suppose the function is called as follows: ```python manage_environment_and_process(\'/tmp/testdir\', \'Alice\', \'Python Programming\', \'PY310\') ``` The script should: 1. Create the directory `/tmp/testdir` if it does not exist. 2. Create the file `/tmp/testdir/env_info.txt` and write: ``` USER_NAME: Alice COURSE_NAME: Python Programming COURSE_CODE: PY310 ``` 3. Spawn a child process that reads the contents of `/tmp/testdir/env_info.txt` and prints them to the standard output, showing: ``` USER_NAME: Alice COURSE_NAME: Python Programming COURSE_CODE: PY310 ``` **Notes:** - Use appropriate functions from the `os` module to complete this task. - Ensure that the parent process sets the environment variables in such a way that they are accessible to the child process when it prints the content.","solution":"import os import subprocess def manage_environment_and_process(dir_path: str, user_name: str, course_name: str, course_code: str) -> None: # Ensure the directory exists if not os.path.exists(dir_path): os.makedirs(dir_path) file_path = os.path.join(dir_path, \'env_info.txt\') # Set environment variables os.environ[\'USER_NAME\'] = user_name os.environ[\'COURSE_NAME\'] = course_name os.environ[\'COURSE_CODE\'] = course_code # Write environment variables to the file with open(file_path, \'w\') as env_file: env_file.write(f\\"USER_NAME: {user_name}n\\") env_file.write(f\\"COURSE_NAME: {course_name}n\\") env_file.write(f\\"COURSE_CODE: {course_code}n\\") # Function to be executed by the child process def read_file(): with open(file_path, \'r\') as env_file: content = env_file.read() print(content) # Create the child process proc = subprocess.Popen([\'python3\', \'-c\', f\'import os; os.environ.update({{ \\"USER_NAME\\": \\"{user_name}\\", \\"COURSE_NAME\\": \\"{course_name}\\", \\"COURSE_CODE\\": \\"{course_code}\\" }}); print(open(\\"{file_path}\\").read())\'], stdout=subprocess.PIPE) proc_output, _ = proc.communicate() # Output the content read by the child process print(proc_output.decode(\'utf-8\'))"},{"question":"# Advanced Dictionary Management in Python You are required to implement a function in Python that demonstrates advanced management of dictionary objects. This function should utilize various dictionary operations such as item insertion, deletion, fetching values, and handling specific conditions. Function Specification: ```python def advanced_dict_management(actions): Perform a series of actions on a dictionary and return the final state of the dictionary. :param actions: List of tuples, where each tuple represents an action. The first element of the tuple is a string representing the action (\'insert\', \'delete\', \'fetch\', \'update\', \'clear\'), and the following elements are the arguments for that action. - For \'insert\', there should be two additional arguments: key and value. - For \'delete\', there should be one additional argument: key. - For \'fetch\', there should be one additional argument: key. - For \'update\', there should be one additional argument: another dictionary. - For \'clear\', there are no additional arguments. :return: A dictionary representing the final state of the dictionary after all actions have been performed. Additionally, fetch operations should accumulate their results in a list within the dictionary under the key \'_fetch_results\'. Constraints: - Keys and values in the dictionary can be any hashable and comparable Python objects. - The number of actions will not exceed 1000. ``` Examples: ```python actions = [ (\'insert\', \'a\', 1), (\'insert\', \'b\', 2), (\'fetch\', \'a\'), (\'delete\', \'b\'), (\'update\', {\'c\': 3, \'d\': 4}), (\'clear\',) ] assert advanced_dict_management(actions) == {\'_fetch_results\': [1]} actions = [ (\'insert\', 1, \'one\'), (\'insert\', 2, \'two\'), (\'fetch\', 3), (\'delete\', 2), (\'update\', {3: \'three\', 4: \'four\'}), ] assert advanced_dict_management(actions) == {1: \'one\', 3: \'three\', 4: \'four\', \'_fetch_results\': [None]} ``` # Performance Requirements: The solution should aim to maintain high efficiency in managing the dictionary operations, especially for large sequences of actions. The dictionary operations should be performed in O(1) average time. Notes: - Ensure to handle edge cases such as trying to fetch or delete keys that do not exist. - Ensure to maintain proper error handling and exception management where necessary. - The solution should be self-contained and should not require additional imports except for built-in Python modules.","solution":"def advanced_dict_management(actions): Perform a series of actions on a dictionary and return the final state of the dictionary. :param actions: List of tuples, where each tuple represents an action. The first element of the tuple is a string representing the action (\'insert\', \'delete\', \'fetch\', \'update\', \'clear\'), and the following elements are the arguments for that action. - For \'insert\', there should be two additional arguments: key and value. - For \'delete\', there should be one additional argument: key. - For \'fetch\', there should be one additional argument: key. - For \'update\', there should be one additional argument: another dictionary. - For \'clear\', there are no additional arguments. :return: A dictionary representing the final state of the dictionary after all actions have been performed. Additionally, fetch operations should accumulate their results in a list within the dictionary under the key \'_fetch_results\'. result_dict = {} fetch_results = [] for action in actions: act = action[0] if act == \'insert\' and len(action) == 3: key, value = action[1], action[2] result_dict[key] = value elif act == \'delete\' and len(action) == 2: key = action[1] result_dict.pop(key, None) elif act == \'fetch\' and len(action) == 2: key = action[1] fetch_results.append(result_dict.get(key, None)) elif act == \'update\' and len(action) == 2: update_dict = action[1] if isinstance(update_dict, dict): result_dict.update(update_dict) elif act == \'clear\' and len(action) == 1: result_dict.clear() result_dict[\'_fetch_results\'] = fetch_results return result_dict"},{"question":"# Support Vector Machines (SVM): Multi-Class Classification and Custom Kernel Implementation Objective To assess your understanding of implementing multi-class classification using Support Vector Machines (SVM) and creating custom kernels with scikit-learn\'s SVM module. Problem Statement You are provided with a multi-class dataset and your task is to: 1. Implement a multi-class SVM classifier using the one-vs-one strategy. 2. Define and use a custom kernel for this classification task. 3. Evaluate the performance of your SVM classifier using appropriate metrics. Dataset You will use the Iris dataset, which can be loaded using the following code: ```python from sklearn.datasets import load_iris data = load_iris() X, y = data.data, data.target ``` Requirements 1. **Custom Kernel Function:** - Define a custom kernel function named `poly_rbf_kernel` which is a combination of polynomial and RBF kernels. Specifically, the kernel function should be ( K(x, y) = (gamma langle x, y rangle + r)^d + exp(-gamma |x-y|^2) ). - Use default values for the parameters: (gamma = 0.5), (r = 1), and (d = 3). 2. **SVM Classifier with One-vs-One Strategy:** - Implement an SVM classifier utilizing the custom kernel function from step 1. - Use the one-vs-one approach for multi-class classification. 3. **Model Training and Prediction:** - Split the data into training and testing sets (80% training, 20% testing). - Train the SVM classifier on the training set. - Predict the labels for the testing set. 4. **Evaluation:** - Evaluate your model using metrics such as accuracy, precision, recall, and F1-score. - Print the confusion matrix for the predictions on the testing set. Function Signature ```python def poly_rbf_kernel(X, Y, gamma=0.5, r=1, d=3): Custom kernel function combining polynomial and RBF kernels. Parameters: - X: array-like of shape (n_samples_1, n_features) - Y: array-like of shape (n_samples_2, n_features) - gamma: float, default=0.5 - r: float, default=1 - d: int, default=3 Returns: - K: array-like of shape (n_samples_1, n_samples_2) pass def train_and_evaluate_svm(X, y): Train and evaluate an SVM classifier with a custom kernel on a multi-class dataset. Parameters: - X: array-like of shape (n_samples, n_features) - y: array-like of shape (n_samples,) Prints: - Accuracy, precision, recall, F1-score of the classifier on the test set. - Confusion matrix. pass ``` Constraints - Use scikit-learn\'s `SVC` class for the SVM implementation. - Ensure reproducibility by setting a random seed for the data split. - Optimize and tune the SVM classifier parameters if necessary for better performance. Submission Submit your implementation of the `poly_rbf_kernel` and `train_and_evaluate_svm` functions. Ensure your code is well-documented and include comments explaining each step. Example Usage ```python from sklearn.datasets import load_iris data = load_iris() X, y = data.data, data.target train_and_evaluate_svm(X, y) ```","solution":"import numpy as np from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix def poly_rbf_kernel(X, Y, gamma=0.5, r=1, d=3): Custom kernel function combining polynomial and RBF kernels. Parameters: - X: array-like of shape (n_samples_1, n_features) - Y: array-like of shape (n_samples_2, n_features) - gamma: float, default=0.5 - r: float, default=1 - d: int, default=3 Returns: - K: array-like of shape (n_samples_1, n_samples_2) # Polynomial part poly_kernel = (gamma * np.dot(X, Y.T) + r) ** d # RBF part X_square = np.sum(X ** 2, axis=1).reshape(-1, 1) Y_square = np.sum(Y ** 2, axis=1).reshape(1, -1) rbf_kernel = np.exp(-gamma * (X_square - 2 * np.dot(X, Y.T) + Y_square)) # Combine polynomial and RBF kernels return poly_kernel + rbf_kernel def train_and_evaluate_svm(X, y): Train and evaluate an SVM classifier with a custom kernel on a multi-class dataset. Parameters: - X: array-like of shape (n_samples, n_features) - y: array-like of shape (n_samples,) Prints: - Accuracy, precision, recall, F1-score of the classifier on the test set. - Confusion matrix. # Split dataset into training (80%) and testing (20%) sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the SVM classifier with the custom kernel svm_classifier = SVC(kernel=poly_rbf_kernel, decision_function_shape=\'ovo\') # Train the classifier svm_classifier.fit(X_train, y_train) # Predict the labels for the testing set y_pred = svm_classifier.predict(X_test) # Evaluate the classifier accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') conf_matrix = confusion_matrix(y_test, y_pred) # Print evaluation metrics print(f\'Accuracy: {accuracy}\') print(f\'Precision: {precision}\') print(f\'Recall: {recall}\') print(f\'F1-score: {f1}\') print(\'Confusion Matrix:\') print(conf_matrix)"},{"question":"# Advanced Context Management using `contextlib` **Objective**: Design a Python function that mimics a complex resource management scenario using the `contextlib` module. This will test your understanding of context managers, decorators, and dynamic resource management. **Problem Statement**: You are required to manage a set of resources where some resources might fail to be acquired, and you need to ensure that all acquired resources are properly released regardless of subsequent errors. Furthermore, the solution should be flexible enough to allow the context manager to be used both directly and as a decorator. # **Functionality Requirements**: 1. **Function Name**: `manage_resources` 2. **Inputs**: - `resource_management_function`: A function that takes no parameters and returns a tuple containing: - `resource_name`: A unique identifier for the resource. - `resource`: The resource object itself. - `num_resources`: An integer specifying the number of resources to manage. 3. **Outputs**: None (The context manager should ensure proper resource management) 4. **Behavior**: - The function should attempt to acquire the number of resources specified by `num_resources`. - If it succeeds in acquiring a resource, it should print `\\"Acquired resource: <resource_name>\\"`. - If an exception occurs during the acquisition of a resource, it should print `\\"Failed to acquire resource: <resource_name>\\"` and proceed to release any already-acquired resources. - Ensure all acquired resources are properly released by printing `\\"Released resource: <resource_name>\\"` when each resource is released. 5. **Flexibility**: - The context manager should be usable both directly in a `with` statement and as a function decorator. # **Expected Implementation**: - Use the `contextlib.ExitStack` to handle the dynamic acquisition and release of resources. - Implement a class that uses `contextlib.ContextDecorator` to allow the context manager to be used as a decorator. # **Constraints**: - Ensure the function is compatible with Python 3.6 or later. - The `resource_management_function` may raise exceptions to simulate resource acquisition failures. Below is the template you should use for the implementation: ```python from contextlib import contextmanager, ExitStack, ContextDecorator class manage_resources(ContextDecorator): def __init__(self, resource_management_function, num_resources): self.resource_management_function = resource_management_function self.num_resources = num_resources def __enter__(self): self.stack = ExitStack() self.resources = [] try: for _ in range(self.num_resources): resource_name, resource = self.resource_management_function() print(f\\"Acquired resource: {resource_name}\\") self.resources.append(resource) self.stack.callback(self.release_resource, resource, resource_name) except Exception as e: print(f\\"Failed to acquire resource: {e}\\") return self def __exit__(self, exc_type, exc_val, exc_tb): return self.stack.__exit__(exc_type, exc_val, exc_tb) def release_resource(self, resource, resource_name): print(f\\"Released resource: {resource_name}\\") # Example of usage: # Define a resource management function def resource_management_function(): import random resource_id = random.randint(1, 100) if random.choice([True, False]): raise ValueError(f\\"Resource {resource_id} acquisition failed!\\") return f\\"Resource{resource_id}\\", resource_id # Using the context manager as a decorator @manage_resources(resource_management_function, 5) def main(): print(\\"Running main function\\") # Using the context manager in a with statement with manage_resources(resource_management_function, 5) as manager: print(\\"Within with statement block\\") if __name__ == \\"__main__\\": main() ``` You need to complete the class `manage_resources` to meet the requirements described above.","solution":"from contextlib import contextmanager, ExitStack, ContextDecorator class manage_resources(ContextDecorator): def __init__(self, resource_management_function, num_resources): self.resource_management_function = resource_management_function self.num_resources = num_resources def __enter__(self): self.stack = ExitStack() self.resources = [] for _ in range(self.num_resources): try: resource_name, resource = self.resource_management_function() print(f\\"Acquired resource: {resource_name}\\") self.resources.append((resource_name, resource)) self.stack.callback(self.release_resource, resource, resource_name) except Exception as e: print(f\\"Failed to acquire resource: {e}\\") break return self def __exit__(self, exc_type, exc_val, exc_tb): self.stack.pop_all().close() return False def release_resource(self, resource, resource_name): print(f\\"Released resource: {resource_name}\\") # Example of usage: # Define a resource management function def resource_management_function(): import random resource_id = random.randint(1, 100) resource_name = f\\"Resource{resource_id}\\" if random.choice([True, False]): raise ValueError(f\\"Resource {resource_id} acquisition failed!\\") return resource_name, resource_id # Using the context manager as a decorator @manage_resources(resource_management_function, 5) def main(): print(\\"Running main function\\") # Using the context manager in a with statement with manage_resources(resource_management_function, 5) as manager: print(\\"Within with statement block\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective Implement a secure password storage system using the `hashlib` module, focusing on key derivation, salting, and hashing. This assessment will test your understanding of fundamental and advanced concepts of Python’s `hashlib` module. Task 1. Write a function `derive_key(password: str, salt: bytes, iterations: int, dklen: int) -> bytes` that uses the `pbkdf2_hmac` function to derive a secure key from a given password. 2. Write a function `hash_password(password: str) -> str` that: - Generates a random salt using `os.urandom()`. - Derives a key using the `derive_key` function. - Returns a string that concatenates the hexadecimal representation of the salt and the derived key, separated by a colon. 3. Write a function `verify_password(stored_password: str, provided_password: str) -> bool` that: - Extracts the salt and the derived key from the stored password string. - Derives a key from the provided password and the extracted salt. - Compares the derived key with the stored key to verify the password. Specifications 1. **derive_key** - **Input**: - `password`: A string containing the password to be hashed. - `salt`: A bytes object containing the salt. - `iterations`: An integer specifying the number of iterations (e.g., 100,000). - `dklen`: An integer specifying the length of the derived key. - **Output**: Returns a bytes object containing the derived key. 2. **hash_password** - **Input**: - `password`: A string containing the password to be hashed. - **Output**: Returns a string with the format `salt:derived_key` where both values are in hexadecimal format. 3. **verify_password** - **Input**: - `stored_password`: A string with the format `salt:derived_key`. - `provided_password`: A string containing the password to verify. - **Output**: Returns a boolean value indicating whether the password is correct. Example ```python import os import hashlib def derive_key(password, salt, iterations=100000, dklen=32): return hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, iterations, dklen) def hash_password(password): salt = os.urandom(16) key = derive_key(password, salt) return f\\"{salt.hex()}:{key.hex()}\\" def verify_password(stored_password, provided_password): salt_hex, key_hex = stored_password.split(\':\') salt = bytes.fromhex(salt_hex) stored_key = bytes.fromhex(key_hex) derived_key = derive_key(provided_password, salt) return derived_key == stored_key # Example usage stored = hash_password(\\"my_secure_password\\") print(stored) print(verify_password(stored, \\"my_secure_password\\")) print(verify_password(stored, \\"wrong_password\\")) ``` Constraints - Do not use any third-party libraries. - Ensure the solution is efficient and secure. - Follow best practices for handling and storing sensitive information. Notes - Consider edge cases where password or salt might be empty. - Make sure to handle exceptions appropriately, especially when dealing with byte conversions and string splitting.","solution":"import os import hashlib def derive_key(password, salt, iterations=100000, dklen=32): Derives a key from the given password and salt using PBKDF2-HMAC-SHA256. Parameters: password (str): The password to derive the key from. salt (bytes): The salt to use in the derivation. iterations (int): The number of iterations to perform (default: 100000). dklen (int): The length of the derived key (default: 32). Returns: bytes: The derived key. return hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, iterations, dklen) def hash_password(password): Generates a hashed password string with a random salt. Parameters: password (str): The password to hash. Returns: str: The hashed password in the format \'salt:derived_key\'. salt = os.urandom(16) key = derive_key(password, salt) return f\\"{salt.hex()}:{key.hex()}\\" def verify_password(stored_password, provided_password): Verifies the provided password against the stored hashed password string. Parameters: stored_password (str): The stored password in the format \'salt:derived_key\'. provided_password (str): The password to verify. Returns: bool: True if the provided password is correct, False otherwise. salt_hex, key_hex = stored_password.split(\':\') salt = bytes.fromhex(salt_hex) stored_key = bytes.fromhex(key_hex) derived_key = derive_key(provided_password, salt) return derived_key == stored_key"},{"question":"Question You are provided with a Python program that simulates a simple inventory management system for a store. The program allows for the addition and removal of items, and it maintains a record of stock levels for each item. However, the program is not optimized and has potential performance and memory issues. Your task is to identify and optimize the bottlenecks in this program using Python\'s profiling and debugging tools. # Program Code ```python import time from collections import defaultdict class Inventory: def __init__(self): self.stock = defaultdict(int) def add_item(self, item, quantity): for _ in range(quantity): time.sleep(0.01) # Simulating a time-consuming operation self.stock[item] += 1 def remove_item(self, item, quantity): for _ in range(quantity): time.sleep(0.01) # Simulating a time-consuming operation if self.stock[item] > 0: self.stock[item] -= 1 else: print(f\\"Item {item} is out of stock!\\") def get_stock(self, item): return self.stock[item] def main(): inventory = Inventory() inventory.add_item(\\"apple\\", 50) inventory.add_item(\\"banana\\", 75) inventory.remove_item(\\"apple\\", 20) inventory.remove_item(\\"banana\\", 80) print(\\"Stock levels:\\") print(\\"Apple:\\", inventory.get_stock(\\"apple\\")) print(\\"Banana:\\", inventory.get_stock(\\"banana\\")) if __name__ == \\"__main__\\": main() ``` # Tasks 1. **Identify Performance Bottlenecks**: Use the `profile` or `cProfile` library to profile the provided program and identify the time-consuming parts of the code. Provide a report of the profiling results. 2. **Optimize the Code**: Refactor the provided code to optimize its performance based on the profiling results. You may use any standard Python techniques to achieve this. 3. **Memory Analysis**: Use the `tracemalloc` library to analyze memory usage in the provided program. Identify any potential memory leaks and provide a report of the findings. 4. **Debugging**: Demonstrate the use of the `pdb` library by setting breakpoints and stepping through the code to debug any issues you encounter during the optimization process. Provide a brief description of how you used `pdb` to debug the code. # Constraints - Do not remove the simulated time delay (`time.sleep(0.01)`) as it represents a real-world time-consuming operation. - Ensure that the optimized program produces the same output as the original. # Submission Submit the following: 1. The original profiling and memory analysis reports. 2. The refactored and optimized code. 3. A brief report on how you used `pdb` to debug the code.","solution":"import time from collections import defaultdict class Inventory: def __init__(self): self.stock = defaultdict(int) def add_item(self, item, quantity): self.stock[item] += quantity time.sleep(0.01 * quantity) # Simulating a time-consuming operation def remove_item(self, item, quantity): if self.stock[item] - quantity >= 0: self.stock[item] -= quantity time.sleep(0.01 * quantity) # Simulating a time-consuming operation else: for _ in range(quantity): time.sleep(0.01) # Simulating a time-consuming operation if self.stock[item] > 0: self.stock[item] -= 1 else: print(f\\"Item {item} is out of stock!\\") break def get_stock(self, item): return self.stock[item] def main(): inventory = Inventory() inventory.add_item(\\"apple\\", 50) inventory.add_item(\\"banana\\", 75) inventory.remove_item(\\"apple\\", 20) inventory.remove_item(\\"banana\\", 80) print(\\"Stock levels:\\") print(\\"Apple:\\", inventory.get_stock(\\"apple\\")) print(\\"Banana:\\", inventory.get_stock(\\"banana\\")) if __name__ == \\"__main__\\": main()"},{"question":"Advanced Array Manipulation **Objective:** Implement a function that manipulates an array of integers using specified operations and returns the updated array. **Function Signature:** ```python def manipulate_array(arr: \'array.array\', operations: list) -> \'array.array\': pass ``` **Input:** - `arr`: An `array.array` object of type \'i\' (signed integer). - `operations`: A list of tuples, where each tuple contains an operation name (as a string) and its corresponding parameters. The operations include: 1. `\\"append\\"` - tuple format: `(\\"append\\", value)` where `value` is an integer to append to the array. 2. `\\"extend\\"` - tuple format: `(\\"extend\\", iterable)` where `iterable` is a list of integers to extend the array with. 3. `\\"insert\\"` - tuple format: `(\\"insert\\", index, value)` where `index` is the position at which to insert `value`. 4. `\\"remove\\"` - tuple format: `(\\"remove\\", value)` where `value` is the element to remove from the array. 5. `\\"reverse\\"` - tuple format: `(\\"reverse\\",)` with no additional parameters. 6. `\\"byteswap\\"` - tuple format: `(\\"byteswap\\",)` with no additional parameters. **Output:** - Returns an `array.array` object of type \'i\' representing the updated array after applying all operations. **Constraints:** - All input operations will be valid and errors from operations can be ignored (you don’t need to handle exceptions). - The operations will be applied in the order they are listed in the `operations` list. **Example:** ```python from array import array # Example input arr = array(\'i\', [1, 2, 3, 4]) operations = [ (\\"append\\", 5), (\\"extend\\", [6, 7]), (\\"insert\\", 2, 10), (\\"remove\\", 4), (\\"reverse\\",), (\\"byteswap\\",) ] # Expected output # Stepwise explanation: # After \\"append\\": [1, 2, 3, 4, 5] # After \\"extend\\": [1, 2, 3, 4, 5, 6, 7] # After \\"insert\\": [1, 2, 10, 3, 4, 5, 6, 7] # After \\"remove\\": [1, 2, 10, 3, 5, 6, 7] # After \\"reverse\\": [7, 6, 5, 3, 10, 2, 1] # After \\"byteswap\\": [1175733780992, 60129542144, 43515988, 50331648, 25080, 134348800, 16777216] result = manipulate_array(arr, operations) # Verify result print(result) # Output: array(\'i\', [1175733780992, 60129542144, 43515988, 50331648, 25080, 134348800, 16777216]) ``` Notes: - In the example above, the `byteswap` operation significantly changes the values due to the different byte order. **Task:** Complete the function `manipulate_array` so that it performs the specified operations on the input array and returns the modified array.","solution":"from array import array def manipulate_array(arr: array, operations: list) -> array: Manipulate an array of integers based on given operations. Parameters: arr (array.array): An array of signed integers. operations (list): A list of tuples where each tuple contains an operation and its parameters. Returns: array.array: The manipulated array of signed integers. for operation in operations: if operation[0] == \\"append\\": arr.append(operation[1]) elif operation[0] == \\"extend\\": arr.extend(operation[1]) elif operation[0] == \\"insert\\": arr.insert(operation[1], operation[2]) elif operation[0] == \\"remove\\": arr.remove(operation[1]) elif operation[0] == \\"reverse\\": arr.reverse() elif operation[0] == \\"byteswap\\": arr.byteswap() return arr"},{"question":"**Question: Advanced Data Visualization with seaborn.objects** **Objective:** Create a complex plot using the new `seaborn.objects` interface to assess your understanding of data visualization, data mapping, transformations, and customization in `seaborn`. **Instructions:** 1. **Load the Dataset:** Use the seaborn `load_dataset` function to load the \\"penguins\\" dataset. Drop any missing values. 2. **Plot Creation:** Create a multi-layer plot that includes: - A scatter plot of `bill_length_mm` by `bill_depth_mm`. - Color each point by `species` and map the point size to `body_mass_g`. 3. **Statistical Transformation:** Add another layer to your plot that includes a linear regression line fit through the scatter plot data, using the `PolyFit` transformation. 4. **Faceting:** Enhance your plot by adding facets for the `sex` variable, creating separate plots for male and female penguins. 5. **Customization:** Apply the following customizations: - Set the x-axis to logarithmic scale. - Use a specific color palette (e.g., \\"flare\\") for the points. - Customize the legends and ticks for better readability. - Set a theme for the plot using a seaborn theme package. 6. **Save the Plot:** Save your final plot to a file named \\"penguins_visualization.png\\". **Expected Input Format:** - No input is required; the dataset is to be loaded using seaborn\'s `load_dataset` function. **Expected Output Format:** - A saved plot file named \\"penguins_visualization.png\\". **Constraints:** - Ensure that there are no missing values in the dataset before plotting. - The faceted plots should be clearly labeled and distinguishable. **Performance Requirements:** - The plot should be rendered efficiently without any significant delay. **Python Code:** ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl # Load the dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create a multi-layer plot ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\", pointsize=\\"body_mass_g\\") .add(so.Dots()) .add(so.Line(), so.PolyFit()) .facet(\\"sex\\") .scale(x=\\"log\\", color=\\"flare\\") .theme(sns.axes_style(\\"whitegrid\\")) .save(\\"penguins_visualization.png\\") ) # Additional customizations can be done as needed ```","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_penguins_plot(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create a multi-layer plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\", pointsize=\\"body_mass_g\\") .add(so.Dots()) .add(so.Line(), so.PolyFit()) .facet(\\"sex\\") .scale(x=\\"log\\", color=\\"flare\\") .theme(sns.axes_style(\\"whitegrid\\")) ) # Save the plot p.save(\\"penguins_visualization.png\\") return \\"penguins_visualization.png\\""},{"question":"Problem Statement You are working on an image processing task where the same image needs to be analyzed by multiple processes concurrently. Given the need to avoid disk I/O overhead, you decide to use shared memory to store the image data. You need to implement a function that: 1. **Loads an image** from a given path. 2. **Stores the image data** in a shared memory block. 3. Retrieves data from this shared memory block to produce a modified version of the image (e.g., inverting the colors). # Function Signature ```python def process_image_in_shared_memory(image_path:str) -> np.ndarray: Args: - image_path (str): The file path of the image to be processed. Returns: - np.ndarray: The numpy array of the processed image. ``` # Requirements 1. Load the image using any standard image processing library like PIL or OpenCV. 2. Create a shared memory block to store the image data. 3. Use the shared memory block to access/update the image data. 4. Modify the image data in a way that demonstrates changes (e.g., invert the image colors). 5. Save/return the modified image data. # Constraints - You must use the `multiprocessing.shared_memory.SharedMemory` class for managing shared memory. - Ensure proper cleanup of shared memory resources using methods like `close()` and `unlink()`. - Performance handling is crucial given the potential size of image data (e.g., avoid unnecessary memory copies). # Example ```python processed_image = process_image_in_shared_memory(\\"path/to/image.jpg\\") # processed_image would contain the color-inverted image data ``` # Notes - You can assume the input image is in a format that can easily be loaded and converted to a numpy array (e.g., JPEG, PNG). - Ensure that all shared memory is properly cleaned up even in the event of errors. - Consider performance optimizations where necessary, given the potentially large size of image data.","solution":"import numpy as np from PIL import Image from multiprocessing import shared_memory import os def process_image_in_shared_memory(image_path: str) -> np.ndarray: # Load the image using PIL image = Image.open(image_path) image_array = np.array(image) # Get the shape and data type of the image array image_shape = image_array.shape image_dtype = image_array.dtype # Create a shared memory block shm = shared_memory.SharedMemory(create=True, size=image_array.nbytes) try: # Create a numpy array backed by shared memory shared_image = np.ndarray(image_shape, dtype=image_dtype, buffer=shm.buf) # Copy the image data to the shared memory np.copyto(shared_image, image_array) # Process the image - Example: Invert the image colors processed_image = 255 - shared_image # Copy the processed data back to a new numpy array result_image = np.copy(processed_image) finally: # Cleanup shared memory: close and unlink shm.close() shm.unlink() return result_image"},{"question":"Objective: The aim of this coding question is to test your understanding of creating and customizing scatter plots using the `seaborn.objects` module. Problem Statement: You are given a dataset `mpg` which contains the following columns: \\"mpg\\", \\"cylinders\\", \\"displacement\\", \\"horsepower\\", \\"weight\\", \\"acceleration\\", \\"model_year\\", \\"origin\\", \\"name\\". Using this dataset, complete the following tasks: Tasks: 1. **Task 1**: Create a single scatter plot where \\"acceleration\\" is the dependent variable, and \\"displacement\\" and \\"weight\\" are the independent variables. 2. **Task 2**: Create a pair of scatter plots showing pairwise relationships for \\"displacement\\" and \\"weight\\" on the x-axis and \\"horsepower\\" and \\"acceleration\\" on the y-axis. 3. **Task 3**: Show pairwise relationships for \\"weight\\" vs \\"displacement\\" and \\"acceleration\\" vs \\"horsepower\\" without crossing the pairs. 4. **Task 4**: Create a 2x2 grid of scatter plots to show relationships between \\"mpg\\" and four independent variables: \\"displacement\\", \\"weight\\", \\"horsepower\\", and \\"cylinders\\". 5. **Task 5**: Combine faceting with pairing by plotting \\"weight\\" on the x-axis, \\"horsepower\\" and \\"acceleration\\" on the y-axis, and facet the plots based on the \\"origin\\" variable. 6. **Task 6**: Customize the labels of the plot created in Task 1, setting `x0` as \\"Displacement (cu in)\\", `x1` as \\"Weight (lb)\\", and y as \\"Acceleration\\". Input: - The dataset `mpg` will be provided to you. You do not need to write code to load it. Output: - Display the scatter plots as specified for each task. Constraints: - Use only the seaborn.objects module for plotting. - Ensure your plots are clearly labeled and easy to interpret. Example: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset mpg = load_dataset(\\"mpg\\") # Task 1 ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .add(so.Dots()) ).show() # Task 2 ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .add(so.Dots()) ).show() # Task 3 ( so.Plot(mpg) .pair( x=[\\"weight\\", \\"acceleration\\"], y=[\\"displacement\\", \\"horsepower\\"], cross=False, ) .add(so.Dots()) ).show() # Task 4 ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .add(so.Dots()) ).show() # Task 5 ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ).show() # Task 6 ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", y=\\"Acceleration\\") .add(so.Dots()) ).show() ``` Ensure your plots are displayed correctly for each task.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load dataset mpg = load_dataset(\\"mpg\\") def plot_task_1(): ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .add(so.Dots()) ).show() def plot_task_2(): ( so.Plot(mpg) .pair(x=[\\"displacement\\", \\"weight\\"], y=[\\"horsepower\\", \\"acceleration\\"]) .add(so.Dots()) ).show() def plot_task_3(): ( so.Plot(mpg) .pair( x=[\\"weight\\", \\"acceleration\\"], y=[\\"displacement\\", \\"horsepower\\"], cross=False, ) .add(so.Dots()) ).show() def plot_task_4(): ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .add(so.Dots()) ).show() def plot_task_5(): ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"acceleration\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ).show() def plot_task_6(): ( so.Plot(mpg, y=\\"acceleration\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", y=\\"Acceleration\\") .add(so.Dots()) ).show()"},{"question":"Coding Assessment Question **Title: Building and Evaluating an Ensemble Model with Scikit-learn** # Objective: Design a coding question that requires students to implement and evaluate an ensemble learning model (such as RandomForestClassifier) using scikit-learn. The question will assess their ability to: 1. Properly preprocess data. 2. Implement an ensemble learning model. 3. Evaluate the model using cross-validation. 4. Visualize the model performance. # Problem Statement: You are provided with a dataset `data.csv` containing information related to a binary classification problem. Your task is to build an ensemble learning model using the RandomForestClassifier from scikit-learn, perform cross-validation to evaluate its performance, and visualize the results. The dataset `data.csv` contains the following columns: - `feature1`, `feature2`, ..., `featureN`: Numerical features. - `target`: Binary target variable (0 or 1). # Instructions: 1. **Data Preprocessing** - Load the dataset from `data.csv`. - Handle any missing values by imputing the mean of the respective columns. - Split the dataset into features (`X`) and the target variable (`y`). 2. **Model Implementation** - Implement a RandomForestClassifier using scikit-learn. - Set the number of estimators to 100 and random state to 42. 3. **Model Evaluation** - Use cross-validation (with 5 folds) to evaluate the model’s performance. - Report the average accuracy and standard deviation of the cross-validation scores. 4. **Visualization** - Plot the cross-validation scores. # Expected Input: - A CSV file `data.csv` containing the dataset. # Expected Output: - An average cross-validation accuracy and standard deviation. - A plot showing the cross-validation scores. # Constraints: - You must use scikit-learn’s in-built methods for model implementation and cross-validation. - Missing values must be handled adequately. # Performance Requirements: - Make sure your solution handles the dataset efficiently and produces the results within a reasonable timeframe. # Sample Code Structure: ```python import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import matplotlib.pyplot as plt # Step 1: Load and preprocess the dataset def load_and_preprocess_data(file_path): # Load dataset data = pd.read_csv(file_path) # Handle missing values data.fillna(data.mean(), inplace=True) # Split into features and target variable X = data.drop(columns=\'target\') y = data[\'target\'] return X, y # Step 2: Implement RandomForestClassifier def train_random_forest(X, y): model = RandomForestClassifier(n_estimators=100, random_state=42) return model # Step 3: Evaluate using cross-validation def evaluate_model(model, X, y): scores = cross_val_score(model, X, y, cv=5) return scores # Step 4: Visualize cross-validation scores def plot_cv_scores(scores): plt.figure(figsize=(10, 6)) plt.plot(scores, marker=\'o\', linestyle=\'--\') plt.title(\'Cross-Validation Scores\') plt.xlabel(\'Fold\') plt.ylabel(\'Accuracy\') plt.grid() plt.show() # Main function to execute all steps def main(): file_path = \'data.csv\' X, y = load_and_preprocess_data(file_path) model = train_random_forest(X, y) scores = evaluate_model(model, X, y) print(f\'Average accuracy: {scores.mean():.4f}\') print(f\'Standard deviation: {scores.std():.4f}\') plot_cv_scores(scores) if __name__ == \'__main__\': main() ``` # Submission: - Submit your code in a Python script or Jupyter notebook. - Include a screenshot or image of the cross-validation scores plot.","solution":"import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import matplotlib.pyplot as plt # Step 1: Load and preprocess the dataset def load_and_preprocess_data(file_path): # Load dataset data = pd.read_csv(file_path) # Handle missing values data.fillna(data.mean(), inplace=True) # Split into features and target variable X = data.drop(columns=\'target\') y = data[\'target\'] return X, y # Step 2: Implement RandomForestClassifier def train_random_forest(X, y): model = RandomForestClassifier(n_estimators=100, random_state=42) return model # Step 3: Evaluate using cross-validation def evaluate_model(model, X, y): scores = cross_val_score(model, X, y, cv=5) return scores # Step 4: Visualize cross-validation scores def plot_cv_scores(scores): plt.figure(figsize=(10, 6)) plt.plot(scores, marker=\'o\', linestyle=\'--\') plt.title(\'Cross-Validation Scores\') plt.xlabel(\'Fold\') plt.ylabel(\'Accuracy\') plt.grid() plt.show() # Main function to execute all steps def main(): file_path = \'data.csv\' X, y = load_and_preprocess_data(file_path) model = train_random_forest(X, y) scores = evaluate_model(model, X, y) print(f\'Average accuracy: {scores.mean():.4f}\') print(f\'Standard deviation: {scores.std():.4f}\') plot_cv_scores(scores) if __name__ == \'__main__\': main()"},{"question":"# **Coding Assessment Question** Objective: You are required to implement a function in Python that computes the weights for the PLSCanonical algorithm using Singular Value Decomposition (SVD). This function is a critical part of the PLSCanonical method, which aims to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. Background: Partial Least Squares (PLS) algorithms aim to project input matrices X and Y into lower-dimensional spaces where their covariance is maximized. For PLSCanonical, this involves calculating weights `u_k` and `v_k` for given components using the cross-covariance matrix. Task: Write a function `compute_pls_weights` that takes in two centered matrices ( X in mathbb{R}^{n times d} ) and ( Y in mathbb{R}^{n times t} ), and an integer `n_components`. The function should: 1. Compute the cross-covariance matrix ( C = X^T Y ). 2. Perform Singular Value Decomposition (SVD) on `C`. 3. Extract and return the top `n_components` left singular vectors (`u_k`) and right singular vectors (`v_k`). Function Signature: ```python def compute_pls_weights(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: pass ``` Input: - `X`: A 2D numpy array of shape ((n, d)), the first centered matrix. - `Y`: A 2D numpy array of shape ((n, t)), the second centered matrix. - `n_components`: An integer specifying the number of components to retain. Output: - A tuple of two numpy arrays: - The first array containing the top `n_components` left singular vectors (`u_k`). - The second array containing the top `n_components` right singular vectors (`v_k`). Example: ```python import numpy as np # Sample centered matrices X and Y X = np.array([[2, 0], [0, 2], [-2, 0], [0, -2]]) Y = np.array([[1, 1], [1, -1], [-1, -1], [-1, 1]]) # Compute weights with n_components=1 u, v = compute_pls_weights(X, Y, 1) print(u) # Expected to return top left singular vector of the cross-covariance matrix print(v) # Expected to return top right singular vector of the cross-covariance matrix ``` Constraints: - The matrices `X` and `Y` are centered (i.e., their columns sum to zero). - Assume that `X` and `Y` have the same number of rows and at least two rows and columns. - You can use `numpy` library for this implementation.","solution":"import numpy as np from typing import Tuple def compute_pls_weights(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: Computes the top n_components left singular vectors and right singular vectors of the cross-covariance matrix of X and Y. Parameters: X (np.ndarray): Centered 2D numpy array of shape (n, d) Y (np.ndarray): Centered 2D numpy array of shape (n, t) n_components (int): Number of components to retain Returns: Tuple[np.ndarray, np.ndarray]: Tuple containing the top n_components left singular vectors and right singular vectors # Compute the cross-covariance matrix C = np.dot(X.T, Y) # Perform Singular Value Decomposition U, S, VT = np.linalg.svd(C, full_matrices=False) # Extract the top n_components left singular vectors and right singular vectors U_k = U[:, :n_components] V_k = VT.T[:, :n_components] return U_k, V_k"},{"question":"You are provided with a dataset in the form of a Pandas DataFrame. Your task is to implement a function that generates a customized violin plot using the `seaborn` library. Your function should showcase different customizations and features of the `seaborn.violinplot` as described in the documentation provided. Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_violinplot(data: pd.DataFrame, x: str, y: str, hue: str, save_path: str) -> None: Generate a customized violin plot and save it to a specified path. Parameters: data (pd.DataFrame): The data for plotting. x (str): The variable on the x-axis. y (str): The variable on the y-axis. hue (str): The variable used to color the violins. save_path (str): The path where the plot will be saved. Returns: None: The plot is saved to the specified path. # Example usage: # df = sns.load_dataset(\'titanic\') # custom_violinplot(data=df, x=\'class\', y=\'age\', hue=\'alive\', save_path=\'violin_plot.png\') ``` Requirements 1. **Data Loading and Plotting**: Load a sample dataset (e.g., Titanic) and create a violin plot with `x`, `y`, and `hue` as specified by the user. 2. **Customization**: - Use `split` to draw violins. - Set the inner representation to `\\"quart\\"`. - Set a `gap` between the dodged violins to 0.1. - Use `cut=0` to prevent smoothing past the extremes. 3. **Appearance**: - Use `native_scale` to ensure that the original scale is preserved on both axes. - Normalize the width of each violin to represent the number of observations using `density_norm=\\"count\\"`. - Apply additional formatting to the axes labels using the `formatter` parameter. 4. **Saving the Plot**: Save the resulting plot to the specified `save_path`. Constraints - The function should handle cases where the provided column names for `x`, `y`, or `hue` do not exist in the dataset and raise appropriate exceptions. - Ensure proper handling of cases where the dataset might contain NaN values. Input Format - `data`: A Pandas DataFrame containing the data to be plotted. - `x`: A string representing the column name for the x-axis. - `y`: A string representing the column name for the y-axis. - `hue`: A string representing the column name to be used for coloring the violins. - `save_path`: A string specifying the path where the plot will be saved. Expected Output - The function should generate and save a violin plot as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_violinplot(data: pd.DataFrame, x: str, y: str, hue: str, save_path: str) -> None: Generate a customized violin plot and save it to a specified path. Parameters: data (pd.DataFrame): The data for plotting. x (str): The variable on the x-axis. y (str): The variable on the y-axis. hue (str): The variable used to color the violins. save_path (str): The path where the plot will be saved. Returns: None: The plot is saved to the specified path. # Check if columns exist in the dataframe for column in [x, y, hue]: if column not in data.columns: raise ValueError(f\\"Column \'{column}\' does not exist in the DataFrame.\\") # Drop rows with NaN values in specified columns data_cleaned = data.dropna(subset=[x, y, hue]) # Create the violin plot plt.figure(figsize=(10, 6)) ax = sns.violinplot( data=data_cleaned, x=x, y=y, hue=hue, split=True, inner=\'quart\', dodge=True, scale=\'count\', bw=0.2, cut=0.0, gap=0.1 ) # Save the plot to the specified path plt.savefig(save_path) plt.close() # Example usage: # df = sns.load_dataset(\'titanic\') # custom_violinplot(data=df, x=\'class\', y=\'age\', hue=\'alive\', save_path=\'violin_plot.png\')"},{"question":"You are required to implement a function named `store_and_retrieve_data` that will interact with a `dbm` database. The function should perform the following tasks: 1. Open a `dbm` database file named `testdb` with the mode `\'c\'` (open for reading and writing, creating it if it doesn\'t exist). 2. Store the following key-value pairs in the database: - Key: `\'name\'`, Value: `\'Alice\'` - Key: `\'age\'`, Value: `\'30\'` - Key: `\'language\'`, Value: `\'Python\'` 3. Retrieve and print all keys and their corresponding values. 4. Ensure that the database is properly closed after performing the operations. # Constraints 1. Assume that the keys and values will be provided as strings. 2. You must handle all keys and values as bytes when interacting with the database. 3. Use context management to ensure the database is closed properly. # Expected Input and Output There are no inputs to the function. The function should print the keys and values stored in the `testdb` database. # Example Output ``` Key: b\'name\', Value: b\'Alice\' Key: b\'age\', Value: b\'30\' Key: b\'language\', Value: b\'Python\' ``` You are free to create any additional helper functions if needed. # Function Signature ```python def store_and_retrieve_data(): pass ``` **Notes:** - Make sure to handle byte encoding for keys and values appropriately. - The function must handle the database-related operations safely using exception handling where necessary. - Ensure the output format conforms to the expected example.","solution":"import dbm def store_and_retrieve_data(): Stores specified key-value pairs in a \'testdb\' dbm database and retrieves and prints all stored keys and values. db_file = \'testdb\' data_to_store = { b\'name\': b\'Alice\', b\'age\': b\'30\', b\'language\': b\'Python\' } # Open the dbm database with \'c\' mode (create it if it doesn\'t exist) with dbm.open(db_file, \'c\') as db: # Store key-value pairs in the database for key, value in data_to_store.items(): db[key] = value # Re-open the database to retrieve and print the data with dbm.open(db_file, \'r\') as db: for key in db.keys(): print(f\\"Key: {key}, Value: {db[key]}\\")"},{"question":"Objective To assess students\' understanding and application of Python\'s debugging and profiling tools. Problem Statement You are given a Python function that performs several complex operations. Your task is to: 1. **Optimize the Function**: Analyze the performance of the given function using Python\'s profiling tools. Identify the bottlenecks and optimize the function to improve its execution time. 2. **Debug the Function**: Use Python\'s debugging tools to identify and fix any logical errors within the function. The provided function performs matrix multiplication using nested loops, which is generally inefficient for large matrices. Function to Optimize and Debug ```python import random def generate_matrix(n, m): return [[random.randint(1, 100) for _ in range(m)] for _ in range(n)] def matrix_multiplication(A, B): n, m = len(A), len(B[0]) result = [[0] * m for _ in range(n)] for i in range(n): for j in range(m): for k in range(len(B)): result[i][j] += A[i][k] * B[k][j] return result # Example Usage: A = generate_matrix(100, 100) B = generate_matrix(100, 100) C = matrix_multiplication(A, B) print(C) ``` Input and Output Formats - Input: Two matrices `A` and `B` of dimensions `n x m` and `m x p`, respectively. - Output: The result of matrix multiplication of `A` and `B`, a matrix of dimensions `n x p`. Requirements 1. **Profiling**: Use Python\'s `cProfile` or `timeit` module to profile the `matrix_multiplication` function. Provide a detailed breakdown of execution times. 2. **Optimization**: Optimize the `matrix_multiplication` function based on your profiling results. Implement and showcase at least one significant optimization. 3. **Debugging**: Use Python\'s `pdb` or any other debugging tool to find and fix any logical errors within the function, if present. Demonstrate how you identified the error and the steps you took to fix it. Constraints - Matrices `A` and `B` will have dimensions between 1 to 300. - All matrix elements will be integers between 1 and 100 inclusive. Submission Students should submit: 1. The optimized and debugged version of `matrix_multiplication`. 2. A script demonstrating the use of profiling tools (e.g., `cProfile`, `timeit`) and their outputs. 3. A script demonstrating the use of debugging tools (e.g., `pdb`) and evidence of debugging sessions. ```python # Optimized Function: def optimized_matrix_multiplication(A, B): # Implement your optimized code here pass # Profiling Script: def profile_script(): # Use profiling tools here to show performance analysis pass # Debugging Script: def debug_script(): # Use debugging tools here to show debugging process pass ```","solution":"import random import numpy as np def generate_matrix(n, m): return [[random.randint(1, 100) for _ in range(m)] for _ in range(n)] def optimized_matrix_multiplication(A, B): Optimized matrix multiplication using NumPy for better performance. return np.dot(A, B).tolist() # Profiling Script: def profile_script(): import cProfile import pstats from io import StringIO A = generate_matrix(100, 100) B = generate_matrix(100, 100) pr = cProfile.Profile() pr.enable() optimized_matrix_multiplication(A, B) pr.disable() s = StringIO() sortby = \'cumtime\' ps = pstats.Stats(pr, stream=s).sort_stats(sortby) ps.print_stats() print(s.getvalue()) # Debugging Script: def debug_script(): import pdb; pdb.set_trace() A = generate_matrix(2, 3) B = generate_matrix(3, 2) result = optimized_matrix_multiplication(A, B) print(result) # Example Usage: if __name__ == \\"__main__\\": A = generate_matrix(100, 100) B = generate_matrix(100, 100) C = optimized_matrix_multiplication(A, B) print(C) profile_script() debug_script()"},{"question":"# Advanced Coding Assessment: Tuple and NamedTuple Manipulation **Objective**: This question aims to assess your knowledge of Python tuples and named tuples by requiring you to implement specific functionalities resembling those documented in the C API for Python. Question: You are tasked with creating a utility that mimics certain low-level operations on tuples and named tuples in Python. You need to implement the following functionalities: 1. **create_tuple(n, *args)**: - Given an integer `n` and `n` subsequent arguments, return a tuple of size `n` initialized with these arguments. - **Input**: `n` (int), followed by `n` items of any data type. - **Output**: A tuple containing the `n` items. - **Example**: `create_tuple(3, \'a\', \'b\', \'c\')` should return `(\'a\', \'b\', \'c\')`. 2. **resize_tuple(tpl, newsize)**: - Given an existing tuple `tpl` and an integer `newsize`, resize the tuple to `newsize`. If the new size is greater, append `None` values; if smaller, truncate the tuple. - **Input**: `tpl` (tuple), `newsize` (int). - **Output**: A tuple with the new size. - **Example**: `resize_tuple((1, 2, 3), 5)` should return `(1, 2, 3, None, None)`, and `resize_tuple((1, 2, 3), 2)` should return `(1, 2)`. 3. **get_tuple_item(tpl, pos)**: - Given a tuple `tpl` and an integer `pos`, return the item at position `pos`. - Handle negative indexing and out-of-bound indices by raising appropriate exceptions (similar to Python’s IndexError). - **Input**: `tpl` (tuple), `pos` (int). - **Output**: The item at the specified position or raise an IndexError if out of bounds. - **Example**: `get_tuple_item((1, 2, 3), 1)` should return `2`, and `get_tuple_item((1, 2, 3), -1)` should return `3`. `get_tuple_item((1, 2, 3), 5)` should raise an `IndexError`. 4. **create_namedtuple(type_name, field_names, *values)**: - Create a named tuple type with the given `type_name` and `field_names`. Then, instantiate it with the provided `values`. - **Input**: `type_name` (str), `field_names` (list of str), followed by positional arguments for the field values. - **Output**: An instance of the newly created named tuple. - **Example**: `create_namedtuple(\'Person\', [\'name\', \'age\'], \'Alice\', 30)` should return `Person(name=\'Alice\', age=30)`. Implementation Constraints: - Do not use existing libraries or helper functions that directly solve the problem (e.g., using `collections.namedtuple` without constraints). You are expected to handle edge cases and write clean and efficient code. Document your functions with appropriate docstrings. **Performance Requirements**: - Your implementation should be efficient, particularly for the `resize_tuple` and `get_tuple_item` functions, as they will be tested against large tuples. ```python def create_tuple(n, *args): Creates a new tuple of size n initialized with the given arguments. # Your code here def resize_tuple(tpl, newsize): Resizes the given tuple to the new size newsize. # Your code here def get_tuple_item(tpl, pos): Returns the item at the specified position in the tuple. # Your code here def create_namedtuple(type_name, field_names, *values): Creates a named tuple type and instantiates it with the given values. # Your code here ``` **Notes**: - Ensure all your functions handle exceptions and invalid inputs gracefully. - Provide at least 3 test cases for each function in your solution.","solution":"from collections import namedtuple def create_tuple(n, *args): Creates a new tuple of size n initialized with the given arguments. Parameters: n (int): The size of the tuple to create. *args: The elements to include in the tuple. Returns: tuple: The created tuple. Raises: ValueError: If the number of arguments doesn\'t match n. if len(args) != n: raise ValueError(\\"Number of arguments must match n\\") return tuple(args) def resize_tuple(tpl, newsize): Resizes the given tuple to the new size newsize. Parameters: tpl (tuple): The tuple to resize. newsize (int): The new size of the tuple. Returns: tuple: The resized tuple. if newsize < len(tpl): return tpl[:newsize] else: return tpl + (None,) * (newsize - len(tpl)) def get_tuple_item(tpl, pos): Returns the item at the specified position in the tuple. Parameters: tpl (tuple): The tuple from which to retrieve an item. pos (int): The position of the item. Returns: Any: The item at the specified position. Raises: IndexError: If the position is out of bounds. if pos < 0: pos += len(tpl) if pos < 0 or pos >= len(tpl): raise IndexError(\\"tuple index out of range\\") return tpl[pos] def create_namedtuple(type_name, field_names, *values): Creates a named tuple type and instantiates it with the given values. Parameters: type_name (str): The name of the named tuple type. field_names (list of str): The field names of the named tuple. *values: The values to initialize the named tuple with. Returns: namedtuple: An instance of the named tuple type. Raises: ValueError: If the number of values doesn\'t match the number of field names. if len(values) != len(field_names): raise ValueError(\\"Number of values must match number of field names\\") NamedTupleType = namedtuple(type_name, field_names) return NamedTupleType(*values)"},{"question":"**Question: Panel Manager** You are required to design a simple panel manager using the `curses.panel` module. The panel manager should be capable of handling a list of panels with the ability to create, move, show, hide, and delete panels. Implement the function `panel_manager(commands: List[Tuple[str, ...]]) -> List[str]` that processes a list of commands and returns the status of the panel stack in response to these commands. The `commands` parameter is a list of tuples, where each tuple represents a command with its arguments. The possible commands are: 1. `\\"CREATE\\"`, `(panel_id, height, width, y, x)`: Create a new panel identified by `panel_id` with given dimensions and position. 2. `\\"MOVE\\"`, `(panel_id, y, x)`: Move the panel identified by `panel_id` to the new coordinates. 3. `\\"SHOW\\"`, `(panel_id,)`: Show the panel identified by `panel_id`. 4. `\\"HIDE\\"`, `(panel_id,)`: Hide the panel identified by `panel_id`. 5. `\\"TOP\\"`, `(panel_id,)`: Move the panel identified by `panel_id` to the top of the stack. 6. `\\"BOTTOM\\"`, `(panel_id,)`: Move the panel identified by `panel_id` to the bottom of the stack. 7. `\\"DELETE\\"`, `(panel_id,)`: Remove the panel identified by `panel_id`. The function should return a list of status messages for each command: - For `\\"CREATE\\"`, return `\\"PANEL CREATED: <panel_id>\\"`. - For `\\"MOVE\\"`, return `\\"PANEL MOVED: <panel_id>\\"`. - For `\\"SHOW\\"`, return `\\"PANEL SHOWN: <panel_id>\\"`. - For `\\"HIDE\\"`, return `\\"PANEL HIDDEN: <panel_id>\\"`. - For `\\"TOP\\"`, return `\\"PANEL TOPPED: <panel_id>\\"`. - For `\\"BOTTOM\\"`, return `\\"PANEL BOTTOMED: <panel_id>\\"`. - For `\\"DELETE\\"`, return `\\"PANEL DELETED: <panel_id>\\"`. Constraints: - The `panel_id` is a unique string identifier for each panel. - The position `(y, x)` and dimensions `(height, width)` are positive integers. - Commands are well-formed and valid. You do not need to handle invalid commands. Function Signature: ```python from typing import List, Tuple def panel_manager(commands: List[Tuple[str, ...]]) -> List[str]: pass ``` **Example:** ```python commands = [ (\\"CREATE\\", \\"panel1\\", 5, 10, 0, 0), (\\"CREATE\\", \\"panel2\\", 6, 15, 5, 5), (\\"MOVE\\", \\"panel1\\", 2, 2), (\\"SHOW\\", \\"panel2\\"), (\\"HIDE\\", \\"panel1\\"), (\\"TOP\\", \\"panel2\\"), (\\"BOTTOM\\", \\"panel1\\"), (\\"DELETE\\", \\"panel1\\") ] assert panel_manager(commands) == [ \\"PANEL CREATED: panel1\\", \\"PANEL CREATED: panel2\\", \\"PANEL MOVED: panel1\\", \\"PANEL SHOWN: panel2\\", \\"PANEL HIDDEN: panel1\\", \\"PANEL TOPPED: panel2\\", \\"PANEL BOTTOMED: panel1\\", \\"PANEL DELETED: panel1\\" ] ``` Implement this function considering the constraints and expected behavior detailed above. Make sure to maintain the visibility and order of panels appropriately.","solution":"from typing import List, Tuple def panel_manager(commands: List[Tuple[str, ...]]) -> List[str]: panels = {} stack = [] hidden = set() result = [] for command in commands: action = command[0] panel_id = command[1] if action == \\"CREATE\\": height, width, y, x = command[2], command[3], command[4], command[5] panels[panel_id] = {\\"height\\": height, \\"width\\": width, \\"y\\": y, \\"x\\": x} stack.append(panel_id) result.append(f\\"PANEL CREATED: {panel_id}\\") elif action == \\"MOVE\\": y, x = command[2], command[3] if panel_id in panels: panels[panel_id][\\"y\\"] = y panels[panel_id][\\"x\\"] = x result.append(f\\"PANEL MOVED: {panel_id}\\") elif action == \\"SHOW\\": if panel_id in hidden: hidden.remove(panel_id) result.append(f\\"PANEL SHOWN: {panel_id}\\") elif action == \\"HIDE\\": hidden.add(panel_id) result.append(f\\"PANEL HIDDEN: {panel_id}\\") elif action == \\"TOP\\": if panel_id in stack: stack.remove(panel_id) stack.append(panel_id) result.append(f\\"PANEL TOPPED: {panel_id}\\") elif action == \\"BOTTOM\\": if panel_id in stack: stack.remove(panel_id) stack.insert(0, panel_id) result.append(f\\"PANEL BOTTOMED: {panel_id}\\") elif action == \\"DELETE\\": if panel_id in panels: del panels[panel_id] if panel_id in stack: stack.remove(panel_id) if panel_id in hidden: hidden.remove(panel_id) result.append(f\\"PANEL DELETED: {panel_id}\\") return result"},{"question":"# PyTorch CPU Stream Management **Objective**: Demonstrate an understanding of PyTorch\'s CPU functionalities by managing device contexts, streams, and synchronization. **Problem Statement**: Implement a function `cpu_stream_operations` that performs the following tasks: 1. Sets the current CPU device to the first available CPU device. 2. Checks if CPU resources are available. If not, raise a RuntimeError. 3. Creates two independent streams (`stream_a` and `stream_b`) for the CPU device. 4. Launches a dummy operation in `stream_a` that performs a simple computation (e.g., adding two tensors). 5. Launches another dummy operation in `stream_b` that performs a different computation (e.g., multiplying two tensors). 6. Synchronizes both streams to ensure all operations are completed. 7. Returns the results of both computations. **Function Signature**: ```python def cpu_stream_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> (torch.Tensor, torch.Tensor): pass ``` **Input**: - `tensor_a`: A PyTorch tensor to be used in the dummy operations. - `tensor_b`: Another PyTorch tensor to be used in the dummy operations. **Output**: - A tuple containing the results of the two dummy operations. **Constraints**: - Assume both input tensors are on the CPU. - Handle exceptions where CPU resources might not be available. **Example**: ```python tensor_a = torch.tensor([1, 2, 3]) tensor_b = torch.tensor([4, 5, 6]) result_add, result_mul = cpu_stream_operations(tensor_a, tensor_b) # result_add should contain the result of the addition operation # result_mul should contain the result of the multiplication operation print(result_add) # Expected: tensor([5, 7, 9]) print(result_mul) # Expected: tensor([4, 10, 18]) ``` **Notes**: - The operations must be performed in separate streams and synchronized properly. - Ensure you handle the setting and resetting of the current device appropriately. **Hints**: - Use `torch.cpu.set_device` to set the current device. - Use `torch.cpu.stream` to create and manage streams. - Use `torch.cpu.synchronize` to ensure all operations are completed before returning results.","solution":"import torch def cpu_stream_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> (torch.Tensor, torch.Tensor): try: # Setting the current CPU device (this is a no-op in PyTorch as CPU is the default) torch.set_num_threads(1) # Dummy line to represent the \'setting\' of CPU resources context # Checking if CPU resources are available if not torch.backends.openmp.is_available(): raise RuntimeError(\\"CPU resources are not available\\") # Creating two independent streams stream_a = torch.cuda.Stream() stream_b = torch.cuda.Stream() # Perform addition operation in stream_a torch.cuda.current_stream().wait_stream(stream_a) with torch.cuda.stream(stream_a): result_add = tensor_a + tensor_b # Perform multiplication operation in stream_b torch.cuda.current_stream().wait_stream(stream_b) with torch.cuda.stream(stream_b): result_mul = tensor_a * tensor_b # Synchronize both streams stream_a.synchronize() stream_b.synchronize() return result_add, result_mul except Exception as e: raise RuntimeError(\\"An error occurred during CPU stream operations: \\" + str(e))"},{"question":"HTML Character Manipulation Objective Write two Python functions that make use of the `html` module\'s `escape` and `unescape` methods to safely encode and decode HTML content. Function 1: `safe_html_encode` This function should take a string `s` as input and return a string where certain characters are converted to their HTML-safe equivalents. **Input:** - `s` (str): The string to be encoded. **Output:** - (str): The encoded HTML-safe string. **Constraints:** - The function should convert the characters \\"&\\", \\"<\\", and \\">\\" to their HTML-safe equivalents. - The function should also convert the characters `\\"` (double quote) and `\'` (single quote) if the optional parameter `quote` is True. **Example Usage:** ```python encoded_str = safe_html_encode(\'Hello \\"world\\" & others!\', quote=True) # Output should be: Hello &quot;world&quot; &amp; others! ``` Function 2: `safe_html_decode` This function should take a string `s` containing HTML character references and return the fully decoded string with corresponding Unicode characters. **Input:** - `s` (str): The string to be decoded. **Output:** - (str): The decoded string with Unicode characters. **Constraints:** - The function should correctly translate named and numeric character references as per HTML 5 standards. **Example Usage:** ```python decoded_str = safe_html_decode(\'Hello &quot;world&quot; &amp; others!\') # Output should be: Hello \\"world\\" & others! ``` Requirements - Use the `html.escape` method inside `safe_html_encode`. - Use the `html.unescape` method inside `safe_html_decode`. - Ensure that your solution is efficient and handles typical edge cases, including empty strings and strings without special characters. Performance Your solution should handle strings of lengths up to 10000 characters efficiently. Submission Provide the implementations for `safe_html_encode` and `safe_html_decode` functions in a single Python file. Make sure to include test cases demonstrating the correctness of your implementations.","solution":"import html def safe_html_encode(s, quote=False): Converts characters \\"&\\", \\"<\\", and \\">\\" to their HTML-safe equivalents. Also converts \'\\"\' and \\"\'\\" if quote is True. Parameters: s (str): The string to be encoded. quote (bool): Whether to convert \'\\"\' and \\"\'\\" as well. Returns: str: The encoded HTML-safe string. return html.escape(s, quote=quote) def safe_html_decode(s): Converts HTML character references to corresponding Unicode characters. Parameters: s (str): The string to be decoded. Returns: str: The decoded string with Unicode characters. return html.unescape(s)"},{"question":"# File and Directory Comparison Task You are given two directory paths, `dir1` and `dir2`, which contain several nested subdirectories and files. Your task is to write a Python function that uses the `filecmp` module to compare these directories and then outputs a detailed comparison. # Objective: Implement the function `compare_directories(dir1: str, dir2: str) -> dict` which recursively compares all files and subdirectories between `dir1` and `dir2` and returns a comprehensive report as a dictionary. The dictionary should have the following structure: ```python { \'common_files\': List[str], # Files present in both directories (with relative paths) \'different_files\': List[str], # Files present in both directories but with different contents (with relative paths) \'dir1_only\': List[str], # Files or directories only present in dir1 (with relative paths) \'dir2_only\': List[str], # Files or directories only present in dir2 (with relative paths) \'subdir_comparisons\': dict # Recursive comparison dictionaries for common subdirectories } ``` # Constraints: 1. Your function should leverage the `filecmp` module\'s `dircmp` class. 2. Files should be compared using the default shallow comparison. 3. The paths in the output lists should be relative to `dir1` and `dir2`, not absolute paths. 4. Handle any file or directory comparison errors gracefully by noting them in the appropriate list (`dir1_only` or `dir2_only`). 5. Consider performance; avoid redundant comparisons by making proper use of caching. # Example: Suppose we have the following directory structures: ``` dir1/ - file1.txt - file2.txt - subdir1/ - subfile1.txt - subdir2/ dir2/ - file1.txt - file3.txt - subdir1/ - subfile1.txt (with different contents from dir1/subdir1/subfile1.txt) - subdir3/ ``` Calling `compare_directories(\'dir1\', \'dir2\')` should return: ```python { \'common_files\': [\'file1.txt\', \'subdir1/subfile1.txt\'], \'different_files\': [\'subdir1/subfile1.txt\'], \'dir1_only\': [\'file2.txt\', \'subdir2/\'], \'dir2_only\': [\'file3.txt\', \'subdir3/\'], \'subdir_comparisons\': { \'subdir1\': { \'common_files\': [\'subfile1.txt\'], \'different_files\': [\'subfile1.txt\'], \'dir1_only\': [], \'dir2_only\': [], \'subdir_comparisons\': {} } } } ``` Good Luck! # Notes: - Ensure that subdirectory paths in the result reflect their relative paths. - Test your function with various nested directory structures to ensure it handles different cases correctly.","solution":"import filecmp import os def compare_directories(dir1: str, dir2: str) -> dict: Recursively compares all files and subdirectories between dir1 and dir2 and returns a comprehensive report. def compare_recursive(dcmp): comparison = { \'common_files\': dcmp.common_files, \'different_files\': dcmp.diff_files, \'dir1_only\': dcmp.left_only, \'dir2_only\': dcmp.right_only, \'subdir_comparisons\': {} } for subdir in dcmp.subdirs: sub_dcmp = dcmp.subdirs[subdir] relative_path = os.path.relpath(sub_dcmp.left, start=dir1) comparison[\'subdir_comparisons\'][relative_path] = compare_recursive(sub_dcmp) return comparison dcmp = filecmp.dircmp(dir1, dir2) return compare_recursive(dcmp)"},{"question":"# Python Setup Configuration File Coding Assessment You are tasked with creating Python packaging configuration files using `setup.cfg`. You will write a function `create_setup_cfg` to generate a `setup.cfg` file with specified configurations. Function Signature ```python def create_setup_cfg(configurations: dict) -> str: pass ``` Input - `configurations`: A dictionary where keys are Distutils command names (str) and values are dictionaries mapping option names (str) to their values (str or list of str). Output - A string that represents the contents of the `setup.cfg` configuration file with appropriate formatting. Requirements 1. **Formatting**: - Each Distutils command section should appear as `[command_name]`. - Each option within these sections should appear as `option_name = value`. - If an option value is a list, it should be split across multiple lines with proper indentation. - Ensure blank lines separate each command section. 2. **Constraints**: - Command names and option names are guaranteed to be strings. - Option values are either strings or lists of strings. - Assume option names conform to the appropriate syntax (e.g., use underscores instead of hyphens). Example ```python configurations = { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": [\\"/usr/local/include\\", \\"/opt/include\\"] }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"John Doe <john.doe@example.com>\\", \\"doc_files\\": [\\"CHANGES.txt\\", \\"README.txt\\", \\"USAGE.txt\\", \\"doc/\\", \\"examples/\\"] } } expected_output = [build_ext] inplace = 1 include_dirs = /usr/local/include /opt/include [bdist_rpm] release = 1 packager = John Doe <john.doe@example.com> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ assert create_setup_cfg(configurations) == expected_output ``` The function should return the contents of the `setup.cfg` file as a string, formatted correctly according to the provided configurations.","solution":"def create_setup_cfg(configurations: dict) -> str: Generate the content of a setup.cfg file from the given configurations. Parameters: - configurations: dict - A dictionary containing Distutils command sections and their respective options. Returns: - str - The formatted setup.cfg content. lines = [] for command, options in configurations.items(): lines.append(f\'[{command}]\') for option, value in options.items(): if isinstance(value, list): lines.append(f\'{option} = {value[0]}\') for item in value[1:]: lines.append(f\'{\\" \\" * len(option) + \\" \\" * 3}{item}\') else: lines.append(f\'{option} = {value}\') lines.append(\'\') return \'n\'.join(lines).strip()"},{"question":"Objective Design a program that reads a list of integers from the user, performs several operations on the list, and outputs specific information derived from these operations. This program will test your understanding of lists, loops, conditionals, and basic input/output operations in Python. Problem Statement Write a Python program that performs the following tasks: 1. Accepts a string of space-separated integers from the user. 2. Parses this string into a list of integers. 3. Calculates and prints: - The sum of all integers in the list. - The average of the integers in the list. - A new list consisting of only the even integers from the original list. - A new list with the integers from the original list in reversed order. - The list of unique integers in the original list. Function Signature ```python def process_integer_list(input_string: str) -> None: pass ``` Input - `input_string` (str): A string of space-separated integers (e.g., \\"10 15 7 20 10 8\\"). Output The function should print the following on separate lines: 1. Sum of integers. 2. Average of integers. 3. List of even integers. 4. Reversed list of integers. 5. List of unique integers. Example If the input string is: ``` 10 15 7 20 10 8 ``` The output should be: ``` Sum: 70 Average: 11.67 Even integers: [10, 20, 10, 8] Reversed list: [8, 10, 20, 7, 15, 10] Unique integers: [10, 15, 7, 20, 8] ``` Constraints - The input string will contain at least one integer. - The integers are separated by spaces. - You may assume that the input is always correctly formatted. Notes - You can use Python\'s `input()` function to read input from the user in an actual interactive environment, but for this task, assume that the input is provided directly as a function argument. Guidelines 1. Use list operations and comprehensions where appropriate. 2. Ensure your function is well-structured and readable. 3. Handling of decimal points for average should be done appropriately (round to 2 decimal places if needed). Good luck!","solution":"def process_integer_list(input_string: str) -> None: # Parse the input string into a list of integers int_list = list(map(int, input_string.split())) # Calculate the sum of all integers total_sum = sum(int_list) # Calculate the average of the integers, rounded to 2 decimal places average = round(total_sum / len(int_list), 2) # Create a new list of only even integers even_integers = [x for x in int_list if x % 2 == 0] # Create a new list with the integers reversed reversed_list = int_list[::-1] # Create a list of unique integers (order preserved) unique_integers = list(dict.fromkeys(int_list)) # Print the results print(f\\"Sum: {total_sum}\\") print(f\\"Average: {average}\\") print(f\\"Even integers: {even_integers}\\") print(f\\"Reversed list: {reversed_list}\\") print(f\\"Unique integers: {unique_integers}\\")"},{"question":"Implement a task scheduler that maintains tasks with varying priorities using the `heapq` module. Tasks will be added to the scheduler with a priority level and can be retrieved in order of their priority. Additionally, the priority of an existing task can be updated, and tasks may be removed. # Requirements: 1. Implement a `TaskScheduler` class with the following methods: ```python class TaskScheduler: def __init__(self): pass # Initializes a new TaskScheduler instance def add_task(self, task: str, priority: int) -> None: pass # Adds a new task with the given priority def pop_task(self) -> str: pass # Pops and returns the task with the highest priority def remove_task(self, task: str) -> None: pass # Removes the specified task if it exists def update_priority(self, task: str, new_priority: int) -> None: pass # Updates the priority of the specified task ``` # Constraints: - Tasks are represented as strings, and priority is an integer. - Tasks with the same priority should return in the order they were added. - If popping on an empty scheduler, raise a `KeyError`. - If attempting to remove or update a non-existent task, raise a `KeyError`. # Example: Initialization and adding tasks: ```python scheduler = TaskScheduler() scheduler.add_task(\\"task1\\", 1) scheduler.add_task(\\"task2\\", 2) scheduler.add_task(\\"task3\\", 1) ``` Popping tasks in order: ```python task = scheduler.pop_task() print(task) # Output: \\"task1\\" task = scheduler.pop_task() print(task) # Output: \\"task3\\" ``` Updating task priority: ```python scheduler.add_task(\\"task4\\", 3) scheduler.update_priority(\\"task4\\", 1) ``` Removing task: ```python scheduler.remove_task(\\"task4\\") ``` # Implementation Tips: - Use a heap to maintain the task queue. - Adapt the structure to allow updating priorities efficiently. - Ensure the solution is efficient and handles edge cases appropriately.","solution":"import heapq class TaskScheduler: def __init__(self): self.heap = [] self.entry_finder = {} self.REMOVED = \'<removed-task>\' self.counter = 0 def add_task(self, task: str, priority: int) -> None: if task in self.entry_finder: self.remove_task(task) count = self.counter entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.heap, entry) self.counter += 1 def pop_task(self) -> str: while self.heap: priority, count, task = heapq.heappop(self.heap) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def remove_task(self, task: str) -> None: entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def update_priority(self, task: str, new_priority: int) -> None: self.remove_task(task) self.add_task(task, new_priority)"},{"question":"**Question: Custom Palette Visualization** You are required to create a customized visualization using seaborn\'s `mpl_palette` function for a given numerical dataset. Here\'s what you need to do: 1. Generate a dataset with 100 points that follows a normal distribution with a mean of 0 and a standard deviation of 1. 2. Create a scatter plot of these points using the \'viridis\' colormap. The data should be colored based on their values. 3. Create another scatter plot of these points but use the \'Set2\' colormap, and ensure to request 8 distinct colors. 4. Display both scatter plots side by side for comparison. **Function Signature:** ```python def custom_palette_visualization(): pass ``` # Expected Output: 1. Two scatter plots displayed side by side. 2. The first plot should use the \'viridis\' colormap and the second plot should use the \'Set2\' qualitative colormap. # Constraints: 1. Use seaborn and matplotlib libraries for plotting. 2. The function should not take any input arguments. 3. The function should display the scatter plots but not return any value. # Additional Notes: - Ensure your plots have appropriate titles, axis labels, and a color bar for the \'viridis\' plot indicating the color scale used. - You may need to utilize data normalization for proper colormap application in the \'viridis\' plot. **Example Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np def custom_palette_visualization(): # Generate the data data = np.random.normal(0, 1, 100) # Create subplot with two columns fig, axes = plt.subplots(1, 2, figsize=(14, 6)) # First scatter plot with \'viridis\' colormap scatter1 = axes[0].scatter(np.arange(100), data, c=data, cmap=sns.mpl_palette(\\"viridis\\", as_cmap=True)) axes[0].set_title(\\"Scatter plot with Viridis colormap\\") axes[0].set_xlabel(\\"Index\\") axes[0].set_ylabel(\\"Value\\") plt.colorbar(scatter1, ax=axes[0]) # Second scatter plot with \'Set2\' colormap palette = sns.mpl_palette(\\"Set2\\", 8) colors = [palette[int(i % len(palette))] for i in range(100)] axes[1].scatter(np.arange(100), data, c=colors) axes[1].set_title(\\"Scatter plot with Set2 colormap\\") axes[1].set_xlabel(\\"Index\\") axes[1].set_ylabel(\\"Value\\") # Show plots plt.tight_layout() plt.show() ``` **Remember:** The focus of the question is primarily on applying colormap functions within seaborn and properly visualizing data with custom palettes.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def custom_palette_visualization(): # Generate the data data = np.random.normal(0, 1, 100) # Create subplot with two columns fig, axes = plt.subplots(1, 2, figsize=(14, 6)) # First scatter plot with \'viridis\' colormap scatter1 = axes[0].scatter(np.arange(100), data, c=data, cmap=sns.mpl_palette(\\"viridis\\", as_cmap=True)) axes[0].set_title(\\"Scatter plot with Viridis colormap\\") axes[0].set_xlabel(\\"Index\\") axes[0].set_ylabel(\\"Value\\") plt.colorbar(scatter1, ax=axes[0]) # Second scatter plot with \'Set2\' colormap palette = sns.mpl_palette(\\"Set2\\", 8) colors = [palette[int(i % len(palette))] for i in range(100)] axes[1].scatter(np.arange(100), data, c=colors) axes[1].set_title(\\"Scatter plot with Set2 colormap\\") axes[1].set_xlabel(\\"Index\\") axes[1].set_ylabel(\\"Value\\") # Show plots plt.tight_layout() plt.show()"},{"question":"# Question You have been given data from a company\'s sales records which consist of sales amounts over the months of a year. Write a function that performs the following tasks using `seaborn`: 1. **Generate a palette**: - Create a dark sequential palette blending from a dark gray to a target color specified with its HUSL values `(20, 60, 50)`. - The palette should contain 12 colors, each representing a month of the year. 2. **Visualize the sales data**: - Plot the sales data using a bar plot where each bar represents a month. - Use the colors from the generated palette to color the bars of the plot. - Ensure that the bars are colored in the order determined by the palette. 3. **Return the continuous colormap**: - Return the created palette as a continuous colormap. **Input Format**: - A list of 12 floats representing the sales data for each month in the order from January to December. **Output Format**: - A continuous colormap. **Constraints**: - The sales data list will always contain 12 elements. Example Usage: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_sales(sales_data): # Your code here # Example data representing sales for each month of a year sales_data = [100, 150, 120, 130, 160, 170, 140, 180, 190, 200, 210, 220] # Call the function and visualize the results cmap = visualize_sales(sales_data) # Ensure to display the plot plt.show() ``` **Expected Output**: A bar plot is displayed with bars colored according to the generated dark sequential palette, and the function returns a continuous colormap.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_sales(sales_data): Generates a dark sequential palette and visualizes sales data using a bar plot. Parameters: sales_data (list): A list of 12 floats representing the sales data for each month. Returns: cmap (sns.palettes._ColorPalette): Continuous colormap generated from the palette. # Create a dark sequential palette palette = sns.color_palette(\\"dark:#DC143C\\", n_colors=12) # Plot the sales data using a bar plot months = [ \'January\', \'February\', \'March\', \'April\', \'May\', \'June\', \'July\', \'August\', \'September\', \'October\', \'November\', \'December\' ] plt.figure(figsize=(10, 6)) barplot = sns.barplot(x=months, y=sales_data, palette=palette) barplot.set_title(\\"Monthly Sales Data\\") barplot.set_xlabel(\\"Month\\") barplot.set_ylabel(\\"Sales\\") plt.xticks(rotation=45) # Return the continuous colormap return palette"},{"question":"**Question: Implement a Thread-Safe Task Processing System** **Objective:** Design and implement a Python program using the `queue` module (including `Queue`, `LifoQueue`, and `PriorityQueue`) to manage a multi-threaded task processing system. The system should: 1. Create a set of tasks with varying priorities. 2. Use a suitable type of queue to manage these tasks. 3. Process the tasks using multiple worker threads. 4. Ensure all tasks are processed before the program exits. **Specifications:** 1. **Input:** - A list of tasks, where each task is represented as a tuple `(priority, task_id)`. For example: `[(3, \'task1\'), (1, \'task2\'), (2, \'task3\')]` - The number of worker threads to be used for processing tasks. 2. **Output:** - Print the task being processed and upon completion of each task by any worker thread. - Print a final message indicating that all tasks have been processed. 3. **Constraints:** - Use the `PriorityQueue` to manage tasks based on their priority. - Implement task processing using multiple threads. - Use the `task_done()` and `join()` methods to ensure all tasks are completed before the program exits. **Implementation Details:** - Define a `TaskProcessor` class that initializes the queue, adds tasks to the queue, and starts worker threads. - Implement a `worker` method within `TaskProcessor` to process tasks from the queue. - Ensure thread-safe operations and use appropriate synchronization mechanisms. **Example:** ```python import threading import queue from typing import List, Tuple class TaskProcessor: def __init__(self, tasks: List[Tuple[int, str]], num_workers: int): self.tasks = tasks self.num_workers = num_workers self.task_queue = queue.PriorityQueue() def worker(self): while True: try: priority, task_id = self.task_queue.get(timeout=1) print(f\'Processing {task_id} with priority {priority}\') # Simulate task processing time import time time.sleep(1) print(f\'Finished {task_id}\') self.task_queue.task_done() except queue.Empty: break def process_tasks(self): for task in self.tasks: self.task_queue.put(task) threads = [] for _ in range(self.num_workers): thread = threading.Thread(target=self.worker, daemon=True) thread.start() threads.append(thread) self.task_queue.join() print(\'All tasks processed\') # Example usage tasks = [(3, \'task3\'), (1, \'task1\'), (2, \'task2\')] num_workers = 2 processor = TaskProcessor(tasks, num_workers) processor.process_tasks() ``` **Notes:** - Ensure to handle exceptions gracefully. - Test your implementation with various sets of tasks and number of worker threads to validate correctness and performance.","solution":"import threading import queue from typing import List, Tuple class TaskProcessor: def __init__(self, tasks: List[Tuple[int, str]], num_workers: int): self.tasks = tasks self.num_workers = num_workers self.task_queue = queue.PriorityQueue() def worker(self): while True: try: priority, task_id = self.task_queue.get(timeout=1) print(f\'Processing {task_id} with priority {priority}\') # Simulate task processing time import time time.sleep(1) print(f\'Finished {task_id}\') self.task_queue.task_done() except queue.Empty: break def process_tasks(self): for task in self.tasks: self.task_queue.put(task) threads = [] for _ in range(self.num_workers): thread = threading.Thread(target=self.worker, daemon=True) thread.start() threads.append(thread) self.task_queue.join() print(\'All tasks processed\')"},{"question":"**Problem Statement:** You are tasked with building a specialized URL Fetcher using the `urllib.request` module in Python. This Fetcher should support custom headers, managed redirections, and proxied requests with basic authentication. **Requirements:** 1. **Class Design:** - Create a class `CustomURLFetcher`. 2. **Methods in `CustomURLFetcher`:** - `__init__(self, base_url, headers=None, proxies=None)`: Initializes with a base URL. Headers and proxies are optional. - `fetch_content(self, endpoint, method=\'GET\', data=None)`: Fetches content from a specified endpoint. - `add_headers(self, headers)`: Adds or updates headers. - `set_proxy(self, proxies)`: Configures the fetcher to use the specified proxies. 3. **Detailed Specifications:** - The `fetch_content` method should handle both GET and POST methods. If POST, ensure data is sent correctly. - Implement redirection handling using `HTTPRedirectHandler`. - Handle proxies with authentication using `ProxyBasicAuthHandler`. - Support custom headers in each request. - Raise an appropriate error if the request fails (e.g., a `URLError`). **Constraints:** - You should use the `urllib.request` module. - No external libraries except built-in Python libraries. - The program should be able to manage at least three levels of redirection. **Example:** ```python fetcher = CustomURLFetcher( base_url=\'http://example.com\', headers={\'User-Agent\': \'CustomAgent/1.0\'}, proxies={\'http\': \'http://proxy.example.com:3128/\'} ) fetcher.add_headers({\'Referer\': \'http://referer.example.com\'}) content = fetcher.fetch_content(\'/data\', method=\'GET\') print(content) ``` **Expected Output:** - The content of the fetched URL, printed to the console. - Handling and managing redirects internally. - Proper proxy handling and authentication if provided. - Raising a custom error in case of any issues during fetching. Implement the `CustomURLFetcher` class based on the specifications above.","solution":"import urllib.request import urllib.error import urllib.parse class CustomURLFetcher: def __init__(self, base_url, headers=None, proxies=None): self.base_url = base_url self.headers = headers if headers else {} self.proxies = proxies if proxies else {} self.opener = self.build_opener() def build_opener(self): handlers = [] # Proxy handler with basic authentication if self.proxies: proxy_handler = urllib.request.ProxyHandler(self.proxies) handlers.append(proxy_handler) # Assuming proxy authentication details are part of the proxy URL proxy_auth_handler = urllib.request.ProxyBasicAuthHandler() handlers.append(proxy_auth_handler) # Redirect handler redirect_handler = urllib.request.HTTPRedirectHandler() handlers.append(redirect_handler) opener = urllib.request.build_opener(*handlers) return opener def fetch_content(self, endpoint, method=\'GET\', data=None): url = urllib.parse.urljoin(self.base_url, endpoint) data = urllib.parse.urlencode(data).encode() if data and method.upper() == \'POST\' else None request = urllib.request.Request(url, data=data, method=method) for key, value in self.headers.items(): request.add_header(key, value) try: response = self.opener.open(request) return response.read().decode() except urllib.error.URLError as e: raise RuntimeError(f\\"Failed to fetch URL: {e}\\") def add_headers(self, headers): self.headers.update(headers) def set_proxy(self, proxies): self.proxies.update(proxies) self.opener = self.build_opener()"},{"question":"Objective: You are required to use `seaborn` and its `mpl_palette` function to create a customized data visualization. This will evaluate your proficiency in working with color palettes in seaborn and incorporating them into plots. Task: Write a Python function named `create_custom_palette_plot` that meets the following requirements: 1. **Function Signature:** ```python def create_custom_palette_plot(data, colormap_name, num_colors, plot_type, ax=None): pass ``` 2. **Parameters:** - `data` (DataFrame): The dataset to be used for the plot. One of the columns should be numeric. - `colormap_name` (str): The name of the colormap to be used (e.g., \\"viridis\\", \\"Set2\\"). - `num_colors` (int): The number of colors to use from the colormap. - `plot_type` (str): The type of seaborn plot to create. Supported values are \\"bar\\", \\"scatter\\", and \\"line\\". - `ax` (matplotlib.axes.Axes, optional): A matplotlib axes object to draw the plot onto, if provided. If `None`, the function should create a new figure and axes. 3. **Returns:** - The function should return the matplotlib figure and axes objects containing the plot. 4. **Functionality:** - The function should use the `sns.mpl_palette` function to generate a color palette based on the specified `colormap_name` and `num_colors`. - Depending on the `plot_type` specified, the function should create a seaborn plot using the generated color palette: - `bar`: Create a bar plot. - `scatter`: Create a scatter plot. - `line`: Create a line plot. - The created plot should utilize the generated color palette effectively. For bar plots, each bar should have a different color from the palette. For scatter and line plots, the points/lines should cycle through the colors of the palette. Example Usage: Assuming we have a pandas DataFrame `df` with columns \\"x\\" and \\"y\\": ```python fig, ax = create_custom_palette_plot(df, \'viridis\', 5, \'scatter\') plt.show() ``` Constraints: - You can assume the DataFrame `data` is always valid and contains appropriate data for the specified plot type. - The `colormap_name` provided will always be a valid colormap in seaborn. # Notes: - Ensure your function documentation is clear and concise. - Utilize proper error handling for invalid `plot_type` values.","solution":"import matplotlib.pyplot as plt import seaborn as sns import pandas as pd def create_custom_palette_plot(data, colormap_name, num_colors, plot_type, ax=None): Create a customized seaborn plot with a specific color palette. Parameters: - data (DataFrame): The dataset to be used for the plot. One of the columns should be numeric. - colormap_name (str): The name of the colormap to be used (e.g., \\"viridis\\", \\"Set2\\"). - num_colors (int): The number of colors to use from the colormap. - plot_type (str): The type of seaborn plot to create. Supported values are \\"bar\\", \\"scatter\\", \\"line\\". - ax (matplotlib.axes.Axes, optional): A matplotlib axes object to draw the plot onto, if provided. Returns: - The matplotlib figure and axes objects containing the plot. if plot_type not in [\\"bar\\", \\"scatter\\", \\"line\\"]: raise ValueError(f\\"Unsupported plot_type \'{plot_type}\'. Supported types are \'bar\', \'scatter\', \'line\'.\\") palette = sns.mpl_palette(colormap_name, num_colors) if ax is None: fig, ax = plt.subplots() else: fig = ax.get_figure() if plot_type == \\"bar\\": sns.barplot(data=data, palette=palette, ax=ax) elif plot_type == \\"scatter\\": sns.scatterplot(data=data, palette=palette, ax=ax) elif plot_type == \\"line\\": sns.lineplot(data=data, palette=palette, ax=ax) return fig, ax"},{"question":"**Priority Queue Scheduler** **Objective:** Implement a priority queue scheduler using the `heapq` module. The scheduler should be capable of adding, updating, and removing tasks based on their priority. **Problem Statement:** You are to implement a class `Scheduler` that maintains a priority queue for tasks. Each task has a priority and a description. Tasks with higher priority numbers should be processed before tasks with lower priority numbers. Your `Scheduler` class should support the following operations: 1. `add_task(priority, description)`: Adds a new task with the given priority and description to the scheduler. If a task with the same description already exists, its priority should be updated. 2. `remove_task(description)`: Removes the task with the given description from the scheduler. 3. `pop_task()`: Removes and returns the description of the highest priority task. If multiple tasks have the same highest priority, return the one that was added first. 4. `get_task(description)`: Returns the priority for the task with the given description. If the task does not exist, raise a `KeyError`. **Constraints:** - You can assume that all priorities are integers. - Descriptions are unique strings. - The scheduler should handle at least 1,000 tasks efficiently. **Example Usage:** ```python scheduler = Scheduler() scheduler.add_task(5, \'Task A\') scheduler.add_task(3, \'Task B\') scheduler.add_task(9, \'Task C\') print(scheduler.pop_task()) # Output: \'Task C\' scheduler.add_task(4, \'Task B\') # Update the priority of Task B scheduler.add_task(1, \'Task D\') print(scheduler.get_task(\'Task B\')) # Output: 4 scheduler.remove_task(\'Task B\') print(scheduler.pop_task()) # Output: \'Task A\' ``` **Implementation Requirements:** - Your implementation should use the `heapq` module to maintain the heap property. - Ensure that updating the priority of existing tasks does not violate the heap property. - Handle task removal efficiently without breaking the heap structure. Submit your implementation of the `Scheduler` class.","solution":"import heapq class Scheduler: def __init__(self): self.heap = [] self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = 0 # unique sequence count def add_task(self, priority, description): Add a new task or update the priority of an existing task. if description in self.entry_finder: self.remove_task(description) count = self.counter entry = [-priority, count, description] self.entry_finder[description] = entry heapq.heappush(self.heap, entry) self.counter += 1 def remove_task(self, description): Mark an existing task as REMOVED. entry = self.entry_finder.pop(description) entry[-1] = self.REMOVED def pop_task(self): Remove and return the highest priority task. while self.heap: priority, count, description = heapq.heappop(self.heap) if description is not self.REMOVED: del self.entry_finder[description] return description raise KeyError(\'pop from an empty priority queue\') def get_task(self, description): Return the current priority of the task with the given description. if description in self.entry_finder: return -self.entry_finder[description][0] raise KeyError(f\'Task \\"{description}\\" not found\')"},{"question":"# Seaborn Heatmap Customization Task **Objective**: Demonstrate your ability to use seaborn to create customized heatmaps along with manipulating data using pandas. **Task**: Given a dataset, you are required to generate a heatmap with the following specifications: 1. Load a dataset of your choice from seaborn\'s built-in datasets (other than the \'glue\' dataset used in the example). 2. Pivot the dataset such that it can be represented in a heatmap, ensuring you choose meaningful columns for the index, columns, and values. 3. Create the heatmap with the following customizations: - Annotate the heatmap with data values using a formatting string of your choice. - Add lines between cells of the heatmap. - Use a colormap of your choice that is different from the default. - Set a custom range for the colormap (vmin and vmax). - Apply any two additional customizations of your choice. **Input**: - You do not need to take any direct input. Use seaborn\'s built-in datasets. **Output**: - Save the heatmap plot as a PNG file named \'custom_heatmap.png\'. **Constraints**: - The dataset should be selected from seaborn\'s built-in dataset options. - Ensure that the heatmap created is meaningful and well-labeled. Here is a template code to get you started: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load dataset data = sns.load_dataset(\'<your_dataset>\') # Pivot the dataset pivot_table = data.pivot(index=\'<row_label_column>\', columns=\'<column_label_column>\', values=\'<value_column>\') # Create the heatmap plt.figure(figsize=(10, 8)) ax = sns.heatmap( pivot_table, annot=True, # Annotate cells with data values fmt=\\"<your_format>\\", # Format for annotations linewidths=0.5, # Add lines between cells cmap=\\"<your_colormap>\\", # Change colormap vmin=<custom_min_value>, # Custom min value vmax=<custom_max_value> # Custom max value ) # Apply additional customizations here # Save the plot plt.savefig(\'custom_heatmap.png\') ``` Ensure your code is well-documented with comments explaining each step and customization applied.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load dataset data = sns.load_dataset(\'flights\') # Pivot the dataset pivot_table = data.pivot(index=\'month\', columns=\'year\', values=\'passengers\') # Create the heatmap plt.figure(figsize=(12, 8)) ax = sns.heatmap( pivot_table, annot=True, # Annotate cells with data values fmt=\\"d\\", # Format for annotations as integers linewidths=0.5, # Add lines between cells cmap=\\"coolwarm\\", # Use coolwarm colormap vmin=100, # Custom min value vmax=700 # Custom max value ) # Additional customizations plt.title(\\"Number of Passengers per Month over Years\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Month\\") plt.xticks(rotation=45) plt.yticks(rotation=0) # Save the plot plt.savefig(\'custom_heatmap.png\')"},{"question":"Background: You are tasked with managing a set of tasks that need to be processed in a multi-threaded application. Each task has a priority level and must be executed accordingly. Additionally, you need to ensure that all tasks are completed before the application shuts down. Task: Write a Python function, `process_tasks`, that simulates this environment using the `PriorityQueue` class from the `queue` module. The function should: 1. Accept a list of tuples, where each tuple contains a task identifier and its priority. 2. Start multiple worker threads to process these tasks. 3. Each worker thread should: - Fetch a task based on its priority. - Simulate task processing by printing the task identifier and priority. - Invoke the `task_done` method upon completing the task. 4. The main thread should wait until all tasks have been processed before exiting. Function Signature: ```python def process_tasks(tasks: list[tuple[int, int]], num_workers: int) -> None: pass ``` Input: - `tasks`: A list of tuples, where each tuple `(task_id, priority)` represents a task. - `task_id` (int): A unique identifier for the task. - `priority` (int): The priority of the task (lower numbers have higher priority). - `num_workers`: An integer representing the number of worker threads to be created. Output: - The function should not return any value. Instead, it should print the processing of each task in the format: ``` Processing task {task_id} with priority {priority} ``` Constraints: - Each task identifier is unique. - You should handle the case where `tasks` is empty. - Ensure thread safety and correct synchronization using the `queue` module\'s provided methods. Example: ```python tasks = [(1, 3), (2, 1), (3, 2)] num_workers = 2 process_tasks(tasks, num_workers) ``` Expected Output: ``` Processing task 2 with priority 1 Processing task 3 with priority 2 Processing task 1 with priority 3 ``` (Note: The order of execution might vary due to threading, but tasks should be processed based on priority.) Notes: - Use the `queue.PriorityQueue` class. - Each task should be processed by a worker thread based on its priority. - Ensure all tasks are completed before the function exits using `queue.join()`.","solution":"import threading from queue import PriorityQueue def process_tasks(tasks: list[tuple[int, int]], num_workers: int) -> None: def worker(): while True: priority, task_id = pq.get() if task_id is None: break print(f\\"Processing task {task_id} with priority {priority}\\") pq.task_done() pq = PriorityQueue() for task_id, priority in tasks: pq.put((priority, task_id)) threads = [] for _ in range(num_workers): t = threading.Thread(target=worker) t.start() threads.append(t) pq.join() for _ in range(num_workers): pq.put((None, None)) for t in threads: t.join()"},{"question":"# CSV Manipulation with Custom Dialects Problem Statement You are tasked with processing a CSV file containing details about various products. The file \\"products.csv\\" follows this structure: ``` id,name,category,price,stock 1,Pen,Stationery,1.20,100 2,Notebook,Stationery,2.50,200 ... ``` The goal is to perform the following operations: 1. **Read the CSV file into a list of dictionaries**. 2. **Calculate the average price of products in each category**. 3. **Write the category-wise average price information to a new CSV file \\"average_prices.csv\\"**. Requirements 1. **Function: `read_csv_as_dict(filename: str) -> List[Dict[str, Any]]`** - **Input**: Name of the CSV file (string). - **Output**: List of dictionaries, where each dictionary represents a row in the CSV file. 2. **Function: `calculate_average_prices(data: List[Dict[str, Any]]) -> Dict[str, float]`** - **Input**: List of dictionaries containing product details. - **Output**: Dictionary where the key is the category and the value is the average price of products in that category (float). 3. **Function: `write_average_prices_to_csv(data: Dict[str, float], filename: str)`** - **Input**: - Data: Dictionary of category-wise average prices. - Filename: Name of the CSV file to be written (string). - **Output**: None. The function writes the data to a CSV file. 4. **Custom Dialect**: - **Use a custom dialect** named `\\"custom\\"` for reading and writing CSV files with the following specifications: - Delimiter: semicolon `;` - Quoting: All fields quoted (`csv.QUOTE_ALL`) Constraints - The `products.csv` file can be assumed to exist in the current working directory. - Each category in the product list will have at least one product. - Data in the CSV file is well-formed and does not contain errors. Example Given the \\"products.csv\\": ``` id;name;category;price;stock \\"1\\";\\"Pen\\";\\"Stationery\\";\\"1.20\\";\\"100\\" \\"2\\";\\"Notebook\\";\\"Stationery\\";\\"2.50\\";\\"200\\" \\"3\\";\\"Eraser\\";\\"Stationery\\";\\"0.75\\";\\"300\\" \\"4\\";\\"Apple\\";\\"Fruit\\";\\"0.50\\";\\"150\\" \\"5\\";\\"Banana\\";\\"Fruit\\";\\"0.30\\";\\"180\\" ``` The resulting \\"average_prices.csv\\" should look like: ``` category;average_price \\"Stationery\\";\\"1.4833333333333334\\" \\"Fruit\\";\\"0.4\\" ``` Code Skeleton ```python import csv from typing import List, Dict, Any def read_csv_as_dict(filename: str) -> List[Dict[str, Any]]: # Register custom dialect csv.register_dialect(\'custom\', delimiter=\';\', quoting=csv.QUOTE_ALL) data = [] with open(filename, newline=\'\', mode=\'r\') as file: reader = csv.DictReader(file, dialect=\'custom\') for row in reader: data.append(row) return data def calculate_average_prices(data: List[Dict[str, Any]]) -> Dict[str, float]: category_totals = {} category_counts = {} for row in data: category = row[\'category\'] price = float(row[\'price\']) category_totals[category] = category_totals.get(category, 0) + price category_counts[category] = category_counts.get(category, 0) + 1 average_prices = {category: category_totals[category] / category_counts[category] for category in category_totals} return average_prices def write_average_prices_to_csv(data: Dict[str, float], filename: str): csv.register_dialect(\'custom\', delimiter=\';\', quoting=csv.QUOTE_ALL) with open(filename, newline=\'\', mode=\'w\') as file: fieldnames = [\'category\', \'average_price\'] writer = csv.DictWriter(file, fieldnames=fieldnames, dialect=\'custom\') writer.writeheader() for category, price in data.items(): writer.writerow({\'category\': category, \'average_price\': price}) # Example Usage: data = read_csv_as_dict(\'products.csv\') average_prices = calculate_average_prices(data) write_average_prices_to_csv(average_prices, \'average_prices.csv\') ``` This question challenges students to: - Read and write CSV data using the `csv` module. - Implement custom dialects for CSV operations. - Perform basic data aggregation and processing. - Write clean and well-documented Python functions.","solution":"import csv from typing import List, Dict, Any def read_csv_as_dict(filename: str) -> List[Dict[str, Any]]: Reads a CSV file into a list of dictionaries using a custom dialect. Parameters: filename (str): The name of the CSV file. Returns: List[Dict[str, Any]]: A list of dictionaries, where each dictionary represents a row in the CSV file. # Register custom dialect csv.register_dialect(\'custom\', delimiter=\';\', quoting=csv.QUOTE_ALL) data = [] with open(filename, newline=\'\', mode=\'r\') as file: reader = csv.DictReader(file, dialect=\'custom\') for row in reader: data.append(row) return data def calculate_average_prices(data: List[Dict[str, Any]]) -> Dict[str, float]: Calculates the average price of products in each category. Parameters: data (List[Dict[str, Any]]): List of dictionaries containing product details. Returns: Dict[str, float]: A dictionary where the key is the category and the value is the average price of products in that category. category_totals = {} category_counts = {} for row in data: category = row[\'category\'] price = float(row[\'price\']) category_totals[category] = category_totals.get(category, 0) + price category_counts[category] = category_counts.get(category, 0) + 1 average_prices = {category: category_totals[category] / category_counts[category] for category in category_totals} return average_prices def write_average_prices_to_csv(data: Dict[str, float], filename: str): Writes the category-wise average price information to a new CSV file using a custom dialect. Parameters: data (Dict[str, float]): Dictionary of category-wise average prices. filename (str): The name of the CSV file to be written. Returns: None: The function writes the data to a CSV file. csv.register_dialect(\'custom\', delimiter=\';\', quoting=csv.QUOTE_ALL) with open(filename, newline=\'\', mode=\'w\') as file: fieldnames = [\'category\', \'average_price\'] writer = csv.DictWriter(file, fieldnames=fieldnames, dialect=\'custom\') writer.writeheader() for category, price in data.items(): writer.writerow({\'category\': category, \'average_price\': price}) # Example Usage: # data = read_csv_as_dict(\'products.csv\') # average_prices = calculate_average_prices(data) # write_average_prices_to_csv(average_prices, \'average_prices.csv\')"},{"question":"# Email Handling and Parsing with Python\'s `email` Package Objective To assess your understanding of Python\'s `email` package, you are required to write a function that constructs an email message, parses it, and extracts specific information using the library\'s features. Problem Statement Write a function `create_and_parse_email(subject: str, sender: str, recipient: str, body: str) -> dict` that: 1. **Constructs an email** with the given `subject`, `sender`, `recipient`, and `body`. 2. **Parses** the constructed email to extract: - The `subject` of the email - The `sender` email address - The `recipient` email address - The `body` of the email The extracted information should be returned in a dictionary with the following keys: `\\"subject\\"`, `\\"sender\\"`, `\\"recipient\\"`, and `\\"body\\"`. Input - `subject` (str): The subject line of the email. - `sender` (str): The sender\'s email address. - `recipient` (str): The recipient\'s email address. - `body` (str): The body text of the email. Output - `result` (dict): A dictionary containing the extracted email information with keys `\\"subject\\"`, `\\"sender\\"`, `\\"recipient\\"`, and `\\"body\\"`. Example ```python def create_and_parse_email(subject, sender, recipient, body): # Your code here # Example Usage email_info = create_and_parse_email( subject=\\"Meeting Reminder\\", sender=\\"alice@example.com\\", recipient=\\"bob@example.com\\", body=\\"Don\'t forget about the meeting at 10 AM tomorrow.\\" ) print(email_info) ``` **Expected Output**: ```python { \\"subject\\": \\"Meeting Reminder\\", \\"sender\\": \\"alice@example.com\\", \\"recipient\\": \\"bob@example.com\\", \\"body\\": \\"Don\'t forget about the meeting at 10 AM tomorrow.\\" } ``` Constraints - You must use Python\'s `email` library to construct, parse, and extract the email information. - Assume the input strings are valid email components. - The function should handle emails with plain text bodies. Hints - Use `email.message.EmailMessage` for constructing the email. - Use `email.parser` to parse the constructed email. - Use `email.policy.default` for parsing with default policies.","solution":"from email.message import EmailMessage from email.parser import Parser from email.policy import default def create_and_parse_email(subject: str, sender: str, recipient: str, body: str) -> dict: Constructs an email message and parses it to extract the subject, sender, recipient, and body. :param subject: The subject of the email. :param sender: The sender\'s email address. :param recipient: The recipient\'s email address. :param body: The body of the email. :return: A dictionary containing the extracted information. # Construct the email message msg = EmailMessage() msg.set_content(body) msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Parse the constructed email back from string form email_string = msg.as_string() parsed_email = Parser(policy=default).parsestr(email_string) # Extract required information extracted_info = { \\"subject\\": parsed_email[\'subject\'], \\"sender\\": parsed_email[\'from\'], \\"recipient\\": parsed_email[\'to\'], \\"body\\": parsed_email.get_body(preferencelist=(\'plain\')).get_content().strip() } return extracted_info"},{"question":"# Advanced PyTorch Distributed Training with Uneven Inputs You are required to implement a simple distributed training setup using PyTorch that can handle uneven inputs across different worker processes. This involves mimicking a real-world scenario where not all processes receive the same amount of data. Problem Statement You need to define a PyTorch module that simulates distributed training with uneven data input using the `torch.distributed.algorithms.Join`, `Joinable`, and `JoinHook` classes. The task is to implement a training function that partitions data unevenly among processes, performs synchronization, and ensures correct training. **Objective**: 1. Implement a simple neural network model for classification. 2. Implement a data partitioning mechanism to distribute data unevenly across processes. 3. Use the Join context manager to handle the distributed training synchronization and completion. Requirements: - Define a neural network model using `torch.nn.Module`. - Partition the dataset into uneven parts for each worker. - Implement the distributed training loop using Join context manager. - Ensure that your solution handles synchronization correctly even with uneven data distribution. Input: - Number of worker processes: `int` (for example, 4) - Total dataset (as a list of tuples `(input, target)`, where `input` is a tensor and `target` is the corresponding label). Output: - Final trained model parameters. Constraints: - You must use the PyTorch framework (version 1.8.0 or above). - You are required to simulate the data distribution and training loop within a single script. - Assume a simple CPU-based environment for demonstrating the concept. Performance Requirements: - The training should complete within a reasonable time for a synthetic dataset of 1000 samples and 10 classes. # Example: ```python import torch import torch.nn as nn import torch.optim as optim from torch.distributed.algorithms import Join, Joinable, JoinHook class SimpleModel(nn.Module): def __init__(self, input_size, num_classes): super(SimpleModel, self).__init__() self.fc = nn.Linear(input_size, num_classes) def forward(self, x): return self.fc(x) def partition_data(data, num_workers): partitions = [] partition_size = len(data) // num_workers for i in range(num_workers): start = i * partition_size end = len(data) if i == num_workers - 1 else (i + 1) * partition_size partitions.append(data[start:end]) return partitions def train_model(model, data, epochs=1): criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): for inputs, targets in data: outputs = model(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() if __name__ == \'__main__\': # Example data num_workers = 4 data = [(torch.randn(10), torch.randint(0, 2, (1,))) for _ in range(100)] partitions = partition_data(data, num_workers) # Initialize model model = SimpleModel(input_size=10, num_classes=2) # Simulate distributed training for partition in partitions: worker_model = model train_model(worker_model, partition) # Print final model parameters print(list(model.parameters())) ```","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.distributed.algorithms.join import Join, Joinable, JoinHook from typing import List, Tuple import os class SimpleModel(nn.Module): def __init__(self, input_size, num_classes): super(SimpleModel, self).__init__() self.fc = nn.Linear(input_size, num_classes) def forward(self, x): return self.fc(x) def partition_data(data: List[Tuple[torch.Tensor, torch.Tensor]], num_workers: int) -> List[List[Tuple[torch.Tensor, torch.Tensor]]]: partitions = [] partition_size = len(data) // num_workers for i in range(num_workers): start = i * partition_size end = len(data) if i == num_workers - 1 else (i + 1) * partition_size partitions.append(data[start:end]) return partitions def train_model(rank: int, size: int, model: nn.Module, data: List[Tuple[torch.Tensor, torch.Tensor]], epochs: int = 1): torch.manual_seed(1234) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) dist.init_process_group(\'gloo\', rank=rank, world_size=size) model = nn.parallel.DistributedDataParallel(model) with Join([Joinable()] * size): for epoch in range(epochs): for inputs, targets in data: outputs = model(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() dist.destroy_process_group() if __name__ == \'__main__\': # Example data num_workers = 4 data = [(torch.randn(10), torch.randint(0, 2, (1,))) for _ in range(100)] partitions = partition_data(data, num_workers) # Initialize model model = SimpleModel(input_size=10, num_classes=2) model.share_memory() # Simulate distributed training processes = [] for rank, partition in enumerate(partitions): p = torch.multiprocessing.Process(target=train_model, args=(rank, num_workers, model, partition)) p.start() processes.append(p) for p in processes: p.join() # Print final model parameters print(list(model.parameters()))"},{"question":"# Timezone Conversion and Custom Timezone Implementation Objective: This exercise aims to assess your understanding of the `datetime` module’s capabilities, including handling and manipulating `datetime` objects, timezone conversions, and creating custom timezone logic using `tzinfo`. Problem Statement: In this exercise, you will: 1. Implement functions to convert given datetime strings between UTC and New York time accounting for daylight saving time. 2. Create a custom `tzinfo` subclass that represents a hypothetical timezone with unusual adjustment rules. # Part 1: Timezone Conversion Implement a function `convert_datetime_to_newyork` that converts a given UTC datetime string to New York time. New York follows the Eastern Time Zone (ET). This function should properly adjust the time for daylight saving time. **Function Signature:** ```python def convert_datetime_to_newyork(dt_str: str) -> str: pass ``` **Input:** - `dt_str`: A string representing a UTC datetime in the format `YYYY-MM-DDTHH:MM:SS`. **Output:** - Returns a string representing the corresponding New York datetime in the format `YYYY-MM-DDTHH:MM:SS`. **Example:** ```python convert_datetime_to_newyork(\\"2023-03-13T14:00:00\\") # -> \\"2023-03-13T10:00:00\\" (EDT) convert_datetime_to_newyork(\\"2023-11-07T14:00:00\\") # -> \\"2023-11-07T09:00:00\\" (EST) ``` # Part 2: Custom Timezone Create a `tzinfo` subclass named `CrazyTimeZone`, representing a fictitious timezone where the offset rules change every year. The timezone shifts by +2 hours on odd years and -2 hours on even years on January 1 at 00:00. **Class Definition:** ```python from datetime import tzinfo, timedelta, datetime class CrazyTimeZone(tzinfo): def utcoffset(self, dt): pass def dst(self, dt): pass def tzname(self, dt): pass def fromutc(self, dt): pass ``` **Example Usage:** ```python from datetime import datetime, timezone ctz = CrazyTimeZone() # Datetime before change in an odd year (shifted by +2 hours) dt1 = datetime(2021, 1, 1, 10, 0, tzinfo=timezone.utc) print(dt1.astimezone(ctz)) # Expect: 2021-01-01 12:00:00 # Datetime before change in an even year (shifted by -2 hours) dt2 = datetime(2022, 1, 1, 10, 0, tzinfo=timezone.utc) print(dt2.astimezone(ctz)) # Expect: 2022-01-01 08:00:00 ``` # Constraints: 1. Handle edge cases related to timezone changes and conversions accurately. 2. Ensure the implementation follows efficient time complexity guidelines. # Notes: - You may use the `pytz` library for Part 1 if needed. - For the `CrazyTimeZone`, focus on implementing methods accurately to handle the dynamic timezone offset logic appropriately. Deliverables: 1. Python function `convert_datetime_to_newyork`. 2. Python class `CrazyTimeZone`.","solution":"from datetime import datetime, tzinfo, timedelta import pytz def convert_datetime_to_newyork(dt_str: str) -> str: Converts a given UTC datetime string to New York time accounting for DST. # Parse the input datetime string dt_utc = datetime.strptime(dt_str, \\"%Y-%m-%dT%H:%M:%S\\") # Define UTC timezone using pytz utc_zone = pytz.utc dt_utc = utc_zone.localize(dt_utc) # Define New York timezone using pytz ny_zone = pytz.timezone(\'America/New_York\') # Convert the UTC datetime to New York time dt_ny = dt_utc.astimezone(ny_zone) # Return the formatted New York datetime string return dt_ny.strftime(\\"%Y-%m-%dT%H:%M:%S\\") class CrazyTimeZone(tzinfo): A custom timezone with a unique offset rule: - Odd years: +2 hours - Even years: -2 hours The shift happens on January 1 at 00:00. def utcoffset(self, dt): if dt.year % 2 == 0: return timedelta(hours=-2) else: return timedelta(hours=2) def dst(self, dt): return timedelta(0) def tzname(self, dt): if dt.year % 2 == 0: return \\"Crazy-2\\" else: return \\"Crazy+2\\" def fromutc(self, dt): dt = dt.replace(tzinfo=self) return dt + self.utcoffset(dt)"},{"question":"# Problem: Extract and Format Contact Information You are given a string containing contact information of several individuals. The format of the string is as follows: - Each individual\'s information is separated by a semicolon `;`. - Each individual has a name, an email address, and a phone number, separated by commas. - The name consists of alphabetic characters and can have multiple parts separated by spaces. - The email address follows the standard format `username@domain.tld`. - The phone number can be in various formats, including but not limited to: `123-456-7890`, `(123) 456 7890`, `123.456.7890`, `1234567890`, `+31636363634`, `075-63546725`. Your task is to write a Python function `extract_contacts` that takes the string as input and returns a list of dictionaries. Each dictionary represents an individual and contains `name`, `email`, and `phone` keys with appropriately extracted values. Requirements: 1. Use regular expressions to extract and validate the email addresses and phone numbers. 2. Normalize the phone numbers to the format `123-456-7890`. Function Signature: ```python def extract_contacts(info_string: str) -> list: pass ``` Input: - `info_string` (str): A string containing the contact information of several individuals. Output: - A list of dictionaries where each dictionary contains three keys: `name`, `email`, and `phone`. Example: ```python info_string = \\"John Doe,john.doe@example.com,123-456-7890;Jane Smith,jane_smith@mail.com,(123) 456 7890\\" result = extract_contacts(info_string) print(result) # Output: [{\'name\': \'John Doe\', \'email\': \'john.doe@example.com\', \'phone\': \'123-456-7890\'}, {\'name\': \'Jane Smith\', \'email\': \'jane_smith@mail.com\', \'phone\': \'123-456-7890\'}] ``` Constraints: 1. The input string is non-empty and correctly formatted. 2. The input string can contain varying numbers of individuals. Implementation Details: 1. Use the `re` module to handle regular expressions. 2. Split the input string by `;` to get each individual\'s information. 3. Use regular expressions to parse and validate the email and phone number formats. 4. Normalize phone numbers to the `123-456-7890` format using regular expressions.","solution":"import re def normalize_phone(phone): # Remove everything that is not a digit phone = re.sub(r\'D\', \'\', phone) # Format the phone number to 123-456-7890 return f\\"{phone[:3]}-{phone[3:6]}-{phone[6:]}\\" def extract_contacts(info_string: str) -> list: contacts = [] individuals = info_string.split(\';\') email_pattern = re.compile(r\'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\') for individual in individuals: name, email, phone = individual.split(\',\') # Validate the email using regex if not email_pattern.match(email): continue # Normalize the phone number normalized_phone = normalize_phone(phone) # Create the dictionary for the individual contact_info = { \'name\': name.strip(), \'email\': email.strip(), \'phone\': normalized_phone } contacts.append(contact_info) return contacts"},{"question":"# Advanced JSON Encoding and Decoding Background You\'re required to work with a JSON-based data interchange in a Python application. The data contains complex numbers, custom data types, and it often needs pretty printing and sorting for better readability. Additionally, the application must handle large JSON data efficiently. Task Implement a JSON utilities class `AdvancedJSON` in Python that extends the default JSON functionality to: 1. Encode and decode complex numbers. 2. Support custom sorting of dictionary keys. 3. Ensure the inductiveness to avoid circular references. 4. Handle custom data types with a fallback mechanism. 5. Provide pretty printing with indentation control. Requirements 1. **Complex Number Handling**: Extend the `json.JSONEncoder` and `json.JSONDecoder` to support encoding and decoding of Python complex numbers in the format `{\\"__complex__\\": true, \\"real\\": <real_part>, \\"imag\\": <imaginary_part>}`. 2. **Custom Sorting**: Implement sorting of dictionary keys during encoding. 3. **Circular Reference Handling**: Preserve the original behavior of detecting circular references in data structures. 4. **Custom Data Handling**: Create a fallback mechanism in the encoder for any custom or unsupported data types. 5. **Pretty Printing**: Implement pretty printing support with indents specified by the user. Input/Output - **Input**: Various Python objects, including dictionaries, lists, complex numbers, and custom data types. - **Output**: JSON strings or serialized files with specified formatting and custom encodings. Function Specifications 1. `AdvancedJSON.to_json(obj: Any, indent: Optional[int] = None, sort_keys: bool = False) -> str` - Converts a Python object to a JSON formatted string. - `obj`: The Python object to be converted to JSON. - `indent`: The indentation level for pretty printing. - `sort_keys`: Whether to sort the dictionary keys. 2. `AdvancedJSON.from_json(json_str: str) -> Any` - Parses a JSON string and returns the corresponding Python object. - `json_str`: The JSON string to be parsed. 3. `AdvancedJSON.save_to_file(obj: Any, file_path: str, indent: Optional[int] = None, sort_keys: bool = False) -> None` - Serializes a Python object to a JSON formatted file. - `obj`: The Python object to be serialized. - `file_path`: The file path to write the JSON data. - `indent`: The indentation level for pretty printing. - `sort_keys`: Whether to sort the dictionary keys. 4. `AdvancedJSON.load_from_file(file_path: str) -> Any` - Deserializes a JSON formatted file to a Python object. - `file_path`: The path of the JSON file to read. Constraints - Handle large input sizes efficiently using memory optimization techniques. - Ensure no circular reference errors occur during encoding. - Assume custom data types are classes with a `to_dict` method that returns a serializable dictionary. Example ```python import json from typing import Any, Optional class AdvancedJSON: @staticmethod def to_json(obj: Any, indent: Optional[int] = None, sort_keys: bool = False) -> str: class CustomEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} if hasattr(obj, \'to_dict\'): return obj.to_dict() return super().default(obj) return json.dumps(obj, cls=CustomEncoder, indent=indent, sort_keys=sort_keys) @staticmethod def from_json(json_str: str) -> Any: def custom_decoder(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct return json.loads(json_str, object_hook=custom_decoder) @staticmethod def save_to_file(obj: Any, file_path: str, indent: Optional[int] = None, sort_keys: bool = False) -> None: with open(file_path, \'w\') as file: file.write(AdvancedJSON.to_json(obj, indent=indent, sort_keys=sort_keys)) @staticmethod def load_from_file(file_path: str) -> Any: with open(file_path, \'r\') as file: return AdvancedJSON.from_json(file.read()) # Example usage: data = { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"address\\": {\\"city\\": \\"New York\\", \\"zipcode\\": \\"10001\\"}, \\"friends\\": [\\"Jane\\", \\"Doe\\"], \\"preferences\\": complex(1, 2) } json_str = AdvancedJSON.to_json(data, indent=4, sort_keys=True) print(json_str) loaded_data = AdvancedJSON.from_json(json_str) print(loaded_data) ``` Evaluation Your implementation will be evaluated based on: - Correctness: Accurate encoding and decoding of JSON data, especially for complex numbers and custom data types. - Efficiency: Handling large JSON objects without excessive memory use. - Code Quality: Readability, proper use of Python features, and appropriate documentation.","solution":"import json from typing import Any, Optional class AdvancedJSON: @staticmethod def to_json(obj: Any, indent: Optional[int] = None, sort_keys: bool = False) -> str: class CustomEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} if hasattr(obj, \'to_dict\'): return obj.to_dict() return super().default(obj) return json.dumps(obj, cls=CustomEncoder, indent=indent, sort_keys=sort_keys) @staticmethod def from_json(json_str: str) -> Any: def custom_decoder(dct): if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct return json.loads(json_str, object_hook=custom_decoder) @staticmethod def save_to_file(obj: Any, file_path: str, indent: Optional[int] = None, sort_keys: bool = False) -> None: with open(file_path, \'w\') as file: file.write(AdvancedJSON.to_json(obj, indent=indent, sort_keys=sort_keys)) @staticmethod def load_from_file(file_path: str) -> Any: with open(file_path, \'r\') as file: return AdvancedJSON.from_json(file.read())"},{"question":"**Python Coding Assessment: Custom Pickling** **Objective:** Demonstrate your understanding of Python\'s `pickle` module by implementing a custom serialization and deserialization process for a complex Python object. **Problem Statement:** You are required to create a custom pickling process for a Python class named `ComplexData`. This class contains various data types, including nested dictionaries, lists, and other class instances. Additionally, you must handle stateful objects and ensure persistence of any external objects referenced by this class. **Specifications:** 1. Implement a class `NameData` which has the following attributes: - `first_name` (str) - `last_name` (str) - `age` (int) 2. Implement a class `ComplexData` which has the following attributes: - `data` (dict) : A dictionary containing various data types - `owner` (NameData) : An instance of `NameData` class - `timestamp` (str) : A timestamp string 3. Implement custom `__reduce__` and `__setstate__` methods for `ComplexData` to ensure it can be correctly serialized and deserialized using the `pickle` module. 4. Demonstrate the pickling and unpickling process by: - Creating an instance of `ComplexData` with nested dictionaries and a `NameData` instance. - Serializing the object to a binary stream. - Deserializing the binary stream back to a `ComplexData` object. - Verifying the integrity of the deserialized object against the original object. **Input and Output:** - **Input:** Input will be the instantiation details of the `ComplexData` and `NameData` classes. - **Output:** Verification results of the deserialization process. **Constraints:** - You must use the `pickle` module for serialization and deserialization. - Ensure to handle any possible exceptions during the serialization process. - Ensure the integrity of the original and deserialized objects by comparing their attributes. **Example:** ```python import pickle class NameData: def __init__(self, first_name, last_name, age): self.first_name = first_name self.last_name = last_name self.age = age class ComplexData: def __init__(self, data, owner, timestamp): self.data = data self.owner = owner self.timestamp = timestamp def __reduce__(self): return (self.__class__, (self.data, self.owner, self.timestamp)) def __setstate__(self, state): self.__dict__.update(state) # Create sample data name = NameData(\\"John\\", \\"Doe\\", 30) data = { \\"score\\": [10, 20, 30], \\"details\\": {\\"item1\\": \\"value1\\", \\"item2\\": \\"value2\\"} } complex_data = ComplexData(data, name, \\"2023-10-10T12:00:00\\") # Serialize ComplexData object serialized_data = pickle.dumps(complex_data) # Deserialize to get the original object deserialized_data = pickle.loads(serialized_data) # Verification print(complex_data.data == deserialized_data.data) # Output: True print(complex_data.owner.first_name == deserialized_data.owner.first_name) # Output: True print(complex_data.timestamp == deserialized_data.timestamp) # Output: True ``` Ensure your implementation confirms the object is correctly serialized and deserialized.","solution":"import pickle class NameData: def __init__(self, first_name, last_name, age): self.first_name = first_name self.last_name = last_name self.age = age def __eq__(self, other): return ( self.first_name == other.first_name and self.last_name == other.last_name and self.age == other.age ) class ComplexData: def __init__(self, data, owner, timestamp): self.data = data self.owner = owner self.timestamp = timestamp def __reduce__(self): return (self.__class__, (self.data, self.owner, self.timestamp)) def __setstate__(self, state): self.__dict__.update(state) def __eq__(self, other): return ( self.data == other.data and self.owner == other.owner and self.timestamp == other.timestamp ) # Demonstration # Create sample data name = NameData(\\"John\\", \\"Doe\\", 30) data = { \\"score\\": [10, 20, 30], \\"details\\": {\\"item1\\": \\"value1\\", \\"item2\\": \\"value2\\"} } complex_data = ComplexData(data, name, \\"2023-10-10T12:00:00\\") # Serialize ComplexData object serialized_data = pickle.dumps(complex_data) # Deserialize to get the original object deserialized_data = pickle.loads(serialized_data) # Verification assert complex_data.data == deserialized_data.data assert complex_data.owner.first_name == deserialized_data.owner.first_name assert complex_data.timestamp == deserialized_data.timestamp"},{"question":"You are provided with the `colorsys` module\'s functions that allow conversions between RGB and three other color coordinate systems: YIQ, HLS, and HSV. Your task is to write a function that can perform a series of color conversions in a specified order. # Function Signature ```python def perform_conversions(color: tuple, conversions: list) -> tuple: pass ``` # Input - `color` (tuple): A tuple representing the initial color in RGB space, where each element is a float between 0 and 1 (inclusive). - `conversions` (list): A list of strings, where each string specifies a conversion to be performed. The strings must be one of the following: - `\'rgb_to_yiq\'` - `\'yiq_to_rgb\'` - `\'rgb_to_hls\'` - `\'hls_to_rgb\'` - `\'rgb_to_hsv\'` - `\'hsv_to_rgb\'` # Output - A tuple representing the color after all specified conversions have been applied, with each element as a float. The final color should be in the color system of the last conversion in the list. # Constraints - The list of conversions will be valid, i.e., the initial color will always be in RGB format, and each conversion will produce a valid color format for the next conversion in the list. - The length of the conversions list will be between 1 and 10. # Example ```python import colorsys color = (0.2, 0.4, 0.4) conversions = [\'rgb_to_hsv\', \'hsv_to_rgb\'] assert perform_conversions(color, conversions) == color color = (0.2, 0.4, 0.4) conversions = [\'rgb_to_hsv\', \'hsv_to_rgb\', \'rgb_to_yiq\'] assert perform_conversions(color, conversions) == colorsys.rgb_to_yiq(0.2, 0.4, 0.4) ``` # Explanation In the first example, converting the color from RGB to HSV and then back to RGB should result in the original color. In the second example, after converting the color from RGB to HSV and then back to RGB, the final conversion is to YIQ, so the output should be the color in YIQ format.","solution":"import colorsys def perform_conversions(color: tuple, conversions: list) -> tuple: Perform a series of color conversions on the given color. Parameters: color (tuple): The initial color in RGB space, where each element is a float between 0 and 1. conversions (list): A list of strings specifying the conversions to be performed. Returns: tuple: The color after all the specified conversions have been applied. conversion_map = { \'rgb_to_yiq\': colorsys.rgb_to_yiq, \'yiq_to_rgb\': colorsys.yiq_to_rgb, \'rgb_to_hls\': colorsys.rgb_to_hls, \'hls_to_rgb\': colorsys.hls_to_rgb, \'rgb_to_hsv\': colorsys.rgb_to_hsv, \'hsv_to_rgb\': colorsys.hsv_to_rgb } current_color = color for conversion in conversions: convert_func = conversion_map[conversion] current_color = convert_func(*current_color) return current_color"},{"question":"# CSVDataProcessor Question You have been given the task of processing and analyzing data from CSV files for a fictional company, \\"DataCorp.\\" The data files are assumed to follow different formats and structures. You are required to implement a class `CSVDataProcessor` that performs various operations on these CSV files. Class Design 1. `CSVDataProcessor` should have the following methods: - `__init__(self, file_path: str)`: Initializes the processor with the path to the CSV file. - `read_csv(self) -> list`: Reads the entire CSV file and returns its content as a list of dictionaries. - `write_csv(self, data: list, file_path: str) -> None`: Writes the provided list of dictionaries to a new CSV file specified by `file_path`. - `find_row_by_value(self, column_name: str, value: str) -> dict`: Finds and returns the first row where the given column has the specified value. - `add_row(self, new_row: dict) -> None`: Adds a new row to the CSV file. - `average_of_column(self, column_name: str) -> float`: Computes and returns the average of the values in the specified column. Assumes all values in that column are numeric. Expected Input and Output Formats - The constructor `__init__` takes the file path of the CSV file as input and initializes the processor. - `read_csv` returns the content of the CSV file as a list of dictionaries, where each dictionary represents a row. - `write_csv` takes a list of dictionaries and writes it to a CSV file specified by the given file path. - `find_row_by_value` searches for the first occurrence where the given column name matches the specified value and returns it as a dictionary. - `add_row` takes a dictionary as a new row and appends it to the CSV file. - `average_of_column` computes the average of numeric values in the specified column and returns it as a floating-point number. Functionality Constraints 1. The CSV file paths provided to the methods will always be valid. 2. The CSV files may have headers and varying delimiters, and the dialect must be correctly identified and handled. 3. When computing averages, non-numeric values in the specified column should be ignored. 4. Performance requirements: methods should be optimized for large CSV files where necessary. Example Usage ```python # Assume the existence of \'sample.csv\' with appropriate content. processor = CSVDataProcessor(\\"sample.csv\\") # Reading the CSV file data = processor.read_csv() print(data) # Writing to a new CSV file processor.write_csv(data, \\"new_sample.csv\\") # Finding a row by value row = processor.find_row_by_value(\\"name\\", \\"Alice\\") print(row) # Adding a new row new_row = {\\"name\\": \\"Bob\\", \\"age\\": \\"32\\", \\"department\\": \\"HR\\"} processor.add_row(new_row) # Computing the average of a column average_age = processor.average_of_column(\\"age\\") print(average_age) ``` # Deliverables 1. The `CSVDataProcessor` class as described above. 2. Appropriate use of exceptions and error handling where necessary. 3. Consideration for different CSV dialects using the Sniffer class. # Evaluation Criteria - Correctness of the implementation. - Efficiency and performance with large CSV files. - Proper and clean usage of the `csv` module features. - Handling edge cases, such as missing columns or mixed data types.","solution":"import csv from typing import List, Dict class CSVDataProcessor: def __init__(self, file_path: str): self.file_path = file_path def read_csv(self) -> List[Dict[str, str]]: with open(self.file_path, mode=\'r\', newline=\'\', encoding=\'utf-8\') as csvfile: dialect = csv.Sniffer().sniff(csvfile.read(1024)) csvfile.seek(0) reader = csv.DictReader(csvfile, dialect=dialect) return [row for row in reader] def write_csv(self, data: List[Dict[str, str]], file_path: str) -> None: if not data: raise ValueError(\\"Data to write is empty\\") with open(file_path, mode=\'w\', newline=\'\', encoding=\'utf-8\') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=data[0].keys()) writer.writeheader() writer.writerows(data) def find_row_by_value(self, column_name: str, value: str) -> Dict[str, str]: data = self.read_csv() for row in data: if row.get(column_name) == value: return row return {} def add_row(self, new_row: Dict[str, str]) -> None: data = self.read_csv() data.append(new_row) self.write_csv(data, self.file_path) def average_of_column(self, column_name: str) -> float: data = self.read_csv() column_values = [float(row[column_name]) for row in data if self.is_number(row.get(column_name))] if not column_values: return 0.0 return sum(column_values) / len(column_values) @staticmethod def is_number(value: str) -> bool: try: float(value) return True except (TypeError, ValueError): return False"},{"question":"# PyTorch Logging Configuration Challenge Objective Your task is to implement a function in Python using the PyTorch `torch._logging` API to dynamically configure log levels for multiple components and artifacts based on a given configuration string. Function Signature ```python import torch._logging as logging def configure_logging(config: str) -> None: Configures the PyTorch logging system based on the given configuration string. Parameters: - config (str): A comma-separated string representing log level adjustments for various PyTorch components and artifacts. Components preceded by \'+\' lower the log level (more verbose), while those preceded by \'-\' raise the log level (less verbose). Returns: - None pass ``` Input and Output - **Input**: - A string `config` where each part is a pair of `[+-]<component>` or `<artifact>`, separated by commas. - **Output**: - This function should have no return value but should appropriately set the log levels for the specified components and artifacts as per the input configuration string. Requirements 1. Use the PyTorch `torch._logging.set_logs` API to implement the function. 2. The function should handle both components and artifacts correctly. 3. Apply logging level adjustments for specified components: - \'+\' prefix should decrease the log level (more verbose). - \'-\' prefix should increase the log level (less verbose). 4. Enable or disable artifacts based on their presence in the string without any prefix. 5. Handle cases where unknown components or artifacts are provided gracefully without causing errors. Examples ```python # example 1 configure_logging(\\"+dynamo,-inductor,schedule\\") # This should set Dynamo\'s log level to DEBUG, Inductor\'s log level to ERROR and enable the \'schedule\' artifact. # example 2 configure_logging(\\"graph,+your.custom.module,-dynamo\\") # This should enable the \'graph\' artifact, set the custom module\'s log level to DEBUG, and Dynamo\'s log level to ERROR. ``` Use the below code to validate your implementation: ```python import os if __name__ == \\"__main__\\": # Configure logging: debug for dynamo, raise log level for inductor, enable schedule artifact configure_logging(\\"+dynamo,-inductor,schedule\\") assert os.getenv(\\"TORCH_LOGS\\") == \\"+dynamo,-inductor,schedule\\" # Enable graph artifact, debug for custom module, raise log level for dynamo configure_logging(\\"graph,+your.custom.module,-dynamo\\") assert os.getenv(\\"TORCH_LOGS\\") == \\"graph,+your.custom.module,-dynamo\\" ``` Constraints - The function assumes valid component and artifact names will be provided. - The function will directly manipulate the logging configuration of the PyTorch library.","solution":"import os def configure_logging(config: str) -> None: Configures the PyTorch logging system based on the given configuration string. Parameters: - config (str): A comma-separated string representing log level adjustments for various PyTorch components and artifacts. Components preceded by \'+\' lower the log level (more verbose), while those preceded by \'-\' raise the log level (less verbose). Returns: - None os.environ[\\"TORCH_LOGS\\"] = config"},{"question":"Objective: Design a Python function that simulates a simple set of concurrent tasks using the `asyncio` package. The function should demonstrate the creation of tasks, concurrent execution, and implementing timeouts. Problem Statement: Write an asynchronous function named `process_data_concurrently` that: 1. Accepts a list of URLs as input. Each URL represents a resource from which data needs to be fetched. 2. For each URL: - The function should fetch data from the URL. Simulate the data fetching by sleeping for a random time between 1 and 5 seconds. - After fetching the data, the function should process the data. Simulate data processing by sleeping for another random time between 1 and 3 seconds. 3. Runs the fetching and processing of data concurrently for each URL. 4. Adds a timeout of 6 seconds for the complete operation of fetching and processing data for each URL. If fetching and processing for any URL exceeds the timeout, it should be canceled, and an appropriate message should be logged. Function Signature: ```python import asyncio from typing import List async def process_data_concurrently(urls: List[str]) -> None: pass ``` Input: - `urls`: A list of strings, where each string is a URL. Output: - The function should print messages indicating the progress, including starting and finishing fetching, starting and finishing processing, and any cancellations due to timeouts. Constraints: - Use `asyncio` functions to simulate fetching and processing. - Handle timeouts properly using `asyncio.wait_for`. Example Usage: ```python urls = [\\"http://example.com/resource1\\", \\"http://example.com/resource2\\", \\"http://example.com/resource3\\"] asyncio.run(process_data_concurrently(urls)) ``` Expected output (order may vary due to concurrency): ``` Started fetching http://example.com/resource1 Started fetching http://example.com/resource2 Started fetching http://example.com/resource3 Finished fetching http://example.com/resource1 Started processing http://example.com/resource1 Finished fetching http://example.com/resource2 Started processing http://example.com/resource2 Finished processing http://example.com/resource1 Finished fetching http://example.com/resource3 Started processing http://example.com/resource3 Finished processing http://example.com/resource2 http://example.com/resource3 operation timed out ``` Notes: - Use `random.uniform(a, b)` to generate a random float between `a` and `b` for simulating sleep durations. - The `asyncio.TimeoutError` exception should be caught to handle the timeouts.","solution":"import asyncio import random from typing import List async def fetch_data(url: str): print(f\\"Started fetching {url}\\") await asyncio.sleep(random.uniform(1, 5)) print(f\\"Finished fetching {url}\\") async def process_data(url: str): print(f\\"Started processing {url}\\") await asyncio.sleep(random.uniform(1, 3)) print(f\\"Finished processing {url}\\") async def fetch_and_process(url: str): try: await asyncio.wait_for(fetch_data(url), timeout=6) await asyncio.wait_for(process_data(url), timeout=6) except asyncio.TimeoutError: print(f\\"{url} operation timed out\\") async def process_data_concurrently(urls: List[str]) -> None: tasks = [fetch_and_process(url) for url in urls] await asyncio.gather(*tasks)"},{"question":"# Python C API Reference Counting Simulation In this assessment, you will simulate the reference counting mechanisms provided by the Python C API. Implement the following functions in Python: 1. **`inc_ref(obj_dict, obj_id)`**: Simulate the `Py_INCREF()` behavior by incrementing the reference count of an object. - **Input**: - `obj_dict (dict)`: A dictionary where keys are object IDs and values are their reference counts. - `obj_id (str)`: The ID of the object whose reference count should be incremented. - **Output**: - The updated dictionary after the reference count has been incremented. - **Constraints**: - If `obj_id` does not exist in `obj_dict`, raise a `KeyError`. 2. **`dec_ref(obj_dict, obj_id)`**: Simulate the `Py_DECREF()` behavior by decrementing the reference count of an object. - **Input**: - `obj_dict (dict)`: A dictionary where keys are object IDs and values are their reference counts. - `obj_id (str)`: The ID of the object whose reference count should be decremented. - **Output**: - The updated dictionary after the reference count has been decremented. - **Constraints**: - If `obj_id` does not exist in `obj_dict`, raise a `KeyError`. - If the reference count reaches 0, the object should be considered deallocated and removed from the dictionary. 3. **`xinc_ref(obj_dict, obj_id)`**: Simulate the `Py_XINCREF()` behavior by incrementing the reference count of an object if it exists. - **Input**: - `obj_dict (dict)`: A dictionary where keys are object IDs and values are their reference counts. - `obj_id (str)`: The ID of the object whose reference count should be incremented. - **Output**: - The updated dictionary after the reference count has been incremented. - **Constraints**: - If `obj_id` does not exist in `obj_dict`, this function does nothing. 4. **`xdec_ref(obj_dict, obj_id)`**: Simulate the `Py_XDECREF()` behavior by decrementing the reference count of an object if it exists. - **Input**: - `obj_dict (dict)`: A dictionary where keys are object IDs and values are their reference counts. - `obj_id (str)`: The ID of the object whose reference count should be decremented. - **Output**: - The updated dictionary after the reference count has been decremented. - **Constraints**: - If `obj_id` does not exist in `obj_dict`, this function does nothing. - If the reference count reaches 0, the object should be considered deallocated and removed from the dictionary. 5. **`clear_ref(obj_dict, obj_id)`**: Simulate the `Py_CLEAR()` behavior by releasing a strong reference to an object and setting the object pointer to `NULL`. - **Input**: - `obj_dict (dict)`: A dictionary where keys are object IDs and values are their reference counts. - `obj_id (str)`: The ID of the object whose reference count should be managed. - **Output**: - The updated dictionary after the reference count has been managed. - **Constraints**: - If `obj_id` does not exist in `obj_dict`, this function does nothing. **Example**: ```python # Initial object dictionary objects = {\'obj1\': 2, \'obj2\': 1, \'obj3\': 3} # Increment reference count objects = inc_ref(objects, \'obj1\') # {\'obj1\': 3, \'obj2\': 1, \'obj3\': 3} # Decrement reference count objects = dec_ref(objects, \'obj2\') # {\'obj1\': 3, \'obj3\': 3} # Increment reference count, if exists objects = xinc_ref(objects, \'obj3\') # {\'obj1\': 3, \'obj3\': 4} # Decrement reference count, if exists objects = xdec_ref(objects, \'obj4\') # {\'obj1\': 3, \'obj3\': 4} # Clear reference objects = clear_ref(objects, \'obj3\') # {\'obj1\': 3} ``` Your task is to implement these functions in Python to simulate the reference counting mechanism.","solution":"def inc_ref(obj_dict, obj_id): Simulate the \'Py_INCREF()\' behavior by incrementing the reference count of an object. if obj_id not in obj_dict: raise KeyError(f\\"Object ID {obj_id} not found\\") obj_dict[obj_id] += 1 return obj_dict def dec_ref(obj_dict, obj_id): Simulate the \'Py_DECREF()\' behavior by decrementing the reference count of an object. if obj_id not in obj_dict: raise KeyError(f\\"Object ID {obj_id} not found\\") obj_dict[obj_id] -= 1 if obj_dict[obj_id] == 0: del obj_dict[obj_id] return obj_dict def xinc_ref(obj_dict, obj_id): Simulate the \'Py_XINCREF()\' behavior by incrementing the reference count of an object if it exists. if obj_id in obj_dict: obj_dict[obj_id] += 1 return obj_dict def xdec_ref(obj_dict, obj_id): Simulate the \'Py_XDECREF()\' behavior by decrementing the reference count of an object if it exists. if obj_id in obj_dict: obj_dict[obj_id] -= 1 if obj_dict[obj_id] == 0: del obj_dict[obj_id] return obj_dict def clear_ref(obj_dict, obj_id): Simulate the \'Py_CLEAR()\' behavior by releasing a strong reference to an object and setting the object pointer to NULL. if obj_id in obj_dict: del obj_dict[obj_id] return obj_dict"},{"question":"# Complex Scope and Exception Handling in Functions **Problem Statement:** You are required to write a function called `complex_scope_handler` that demonstrates a mastery of Python\'s name resolution, scope, and exception handling mechanisms. Your task is to create a function that: 1. Accepts one list `inputs` of arbitrary length containing integers. 2. Within the function: - Define an inner function `process_input` that attempts to multiply each integer by a multiplier, which is a variable set in the outer function\'s scope. - Properly handle the case where an integer is zero by raising a custom exception `ZeroValueError` and catching it within `process_input`. - Ensure that the function can dynamically change the multiplier and correctly reflect this change in `process_input`. 3. Return a list of processed results. If an exception occurs during processing any input, the result for that input should be `\\"Error\\"`. **Requirements:** - Use nested functions to demonstrate name resolution and scope. - Raise and handle a custom exception. - Dynamically change the state of the multiplier between processing different inputs. **Function signature:** ```python def complex_scope_handler(inputs: list) -> list: # Your code here ``` **Constraints:** - The `inputs` list will contain at least one integer. - The integers in the `inputs` list will range between -1000 and 1000. - You must define and use the custom `ZeroValueError` exception class. **Example:** ```python >> def complex_scope_handler(inputs): class ZeroValueError(Exception): pass results = [] multiplier = 2 def process_input(val): nonlocal multiplier try: if val == 0: raise ZeroValueError(\\"Input cannot be zero!\\") return val * multiplier except ZeroValueError: return \\"Error\\" for idx, input_val in enumerate(inputs): if idx % 2 == 0: multiplier = 2 # Even index, use multiplier 2 else: multiplier = 3 # Odd index, use multiplier 3 result = process_input(input_val) results.append(result) return results >> complex_scope_handler([1, 2, 0, 4]) [2, 6, \'Error\', 12] >> complex_scope_handler([0, -3, 5, 0]) [\'Error\', -9, 10, \'Error\'] ``` In this example: - For input `[1, 2, 0, 4]`: - 1 * 2 = 2 - 2 * 3 = 6 - 0 raises ZeroValueError, resulting in \\"Error\\" - 4 * 3 = 12 - For input `[0, -3, 5, 0]`: - 0 raises ZeroValueError, resulting in \\"Error\\" - (-3) * 3 = -9 - 5 * 2 = 10 - 0 raises ZeroValueError, resulting in \\"Error\\" Ensure your function adheres to these requirements and correctly processes input lists accordingly.","solution":"def complex_scope_handler(inputs: list) -> list: class ZeroValueError(Exception): pass results = [] multiplier = 2 def process_input(val): nonlocal multiplier try: if val == 0: raise ZeroValueError(\\"Input cannot be zero!\\") return val * multiplier except ZeroValueError: return \\"Error\\" for idx, input_val in enumerate(inputs): if idx % 2 == 0: multiplier = 2 # Even index, use multiplier 2 else: multiplier = 3 # Odd index, use multiplier 3 result = process_input(input_val) results.append(result) return results"},{"question":"PyTorch Mobile Optimization Problem Statement You are provided with a pre-trained Convolutional Neural Network (CNN) model that has been exported as a TorchScript Module. Your task is to optimize this model for mobile deployment using PyTorch\'s `torch.utils.mobile_optimizer.optimize_for_mobile` utility. The optimizations you need to apply are: 1. Conv2D and BatchNorm fusion. 2. ReLU/Hardtanh fusion. 3. Dropout removal. 4. Insert and fold prepacked ops. Additionally, assume we are targeting a CPU backend (XNNPACK). Instructions 1. Load the given TorchScript model. 2. Apply the required optimizations using the `optimize_for_mobile` utility. 3. Save the optimized model to a specified path. Input 1. The file path to the pre-trained TorchScript model (string). 2. The file path where the optimized model should be saved (string). Output - The optimized model should be saved to the specified output path. Examples ```python def optimize_model_for_mobile(input_model_path: str, output_model_path: str) -> None: Optimizes a TorchScript model for mobile deployment using specified optimization techniques. :param input_model_path: str - Path to the input pre-trained TorchScript model. :param output_model_path: str - Path where the optimized model should be saved. # Your implementation here ``` Here is an example usage of your function: ```python input_model_path = \\"path/to/pretrained_model.pt\\" output_model_path = \\"path/to/optimized_model.pt\\" optimize_model_for_mobile(input_model_path, output_model_path) ``` Upon running, this should perform the specified optimizations and save the optimized model. Constraints - You must use PyTorch version >= 1.9.0 and the `torch.utils.mobile_optimizer` module. - The model should be in evaluation mode during the optimization process. - Add error handling to ensure that the input paths are valid and the optimization passes successfully. Notes - You may need to use other PyTorch utilities such as `torch.jit.load` and `torch.jit.save`. - Ensure you verify that the optimized model retains the correct behavior and performance.","solution":"import torch from torch.utils.mobile_optimizer import optimize_for_mobile def optimize_model_for_mobile(input_model_path: str, output_model_path: str) -> None: Optimizes a TorchScript model for mobile deployment using specified optimization techniques. :param input_model_path: str - Path to the input pre-trained TorchScript model. :param output_model_path: str - Path where the optimized model should be saved. try: # Load the TorchScript model model = torch.jit.load(input_model_path) # Ensure the model is in evaluation mode model.eval() # Optimize the model for mobile deployment optimized_model = optimize_for_mobile(model) # Save the optimized model optimized_model._save_for_lite_interpreter(output_model_path) except Exception as e: print(f\\"An error occurred during model optimization: {e}\\")"},{"question":"Implementing Gaussian Naive Bayes Classifier Objective Demonstrate your understanding of the `GaussianNB` class in the `scikit-learn` package by applying it to a real dataset. Problem Statement You are given the Iris dataset, which is a classic dataset for classification problems. Your task is to implement a Gaussian Naive Bayes classifier to classify the iris species. Dataset The Iris dataset consists of 150 samples with 4 features each: sepal length, sepal width, petal length, and petal width. The target variable is the species of the iris, which can be one of three classes: Iris-setosa, Iris-versicolor, or Iris-virginica. Instructions 1. **Load the Dataset**: Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Split the Data**: Split the dataset into a training set (70%) and a testing set (30%) using `train_test_split`. 3. **Train the Model**: Train a Gaussian Naive Bayes classifier (`GaussianNB`) on the training data. 4. **Predict**: Use the trained model to predict the species for the test data. 5. **Evaluate**: Evaluate the model\'s performance by calculating the accuracy and printing the number of mislabeled points. Expected Function Signature ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB def gaussian_naive_bayes_classifier(): # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into a training set (70%) and a testing set (30%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a Gaussian Naive Bayes classifier on the training data gnb = GaussianNB() gnb.fit(X_train, y_train) # Predict the species for the test data y_pred = gnb.predict(X_test) # Evaluate the model\'s performance by calculating the accuracy accuracy = np.mean(y_pred == y_test) # Print the number of mislabeled points num_mislabeled = (y_test != y_pred).sum() print(f\'Number of mislabeled points out of a total {X_test.shape[0]} points: {num_mislabeled}\') print(f\'Accuracy: {accuracy:.2f}\') ``` Constraints - You must use the `GaussianNB` class from `sklearn.naive_bayes`. - Use the `train_test_split` function from `sklearn.model_selection` with a fixed `random_state` for reproducibility. Example Output ``` Number of mislabeled points out of a total 45 points: 1 Accuracy: 0.98 ``` Notes - Ensure you handle the data appropriately and understand the split between training and testing for model validation. - This implementation should handle any exceptions and edge cases where applicable. Good luck!","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB def gaussian_naive_bayes_classifier(): # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into a training set (70%) and a testing set (30%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a Gaussian Naive Bayes classifier on the training data gnb = GaussianNB() gnb.fit(X_train, y_train) # Predict the species for the test data y_pred = gnb.predict(X_test) # Evaluate the model\'s performance by calculating the accuracy accuracy = np.mean(y_pred == y_test) # Print the number of mislabeled points num_mislabeled = (y_test != y_pred).sum() print(f\'Number of mislabeled points out of a total {X_test.shape[0]} points: {num_mislabeled}\') print(f\'Accuracy: {accuracy:.2f}\') # Returning values for unit testing return accuracy, num_mislabeled"},{"question":"# File and Directory Management Task **Objective:** Write a Python program that demonstrates file and directory manipulation skills using the `os`, `shutil`, `filecmp`, `tempfile`, and `glob` modules. **Task:** You need to create a script that performs the following steps: 1. **Directory Comparison:** - Compare two directories and identify all the files that differ between them. Use the `filecmp` module\'s `dircmp` class. - Input: Two directory paths (`dir1` and `dir2`). - Output: List of filenames that are different between the two directories. 2. **Copy Specific Files:** - Copy files that match a specific pattern (e.g., all `.txt` files) from a source directory to a destination directory. Use the `glob` module for pattern matching and `shutil` for copying. - Input: Source directory path, destination directory path, and a filename pattern. - Output: List of filenames that were copied. 3. **Creating Temporary Backup:** - Create a temporary backup of the destination directory before copying the files to it. Use the `tempfile` module to create a temporary directory. - Input: Destination directory path. - Output: Path to the temporary backup directory. **Function Signatures:** ```python def compare_directories(dir1: str, dir2: str) -> list: ... def copy_pattern_files(src_dir: str, dest_dir: str, pattern: str) -> list: ... def create_temp_backup(directory: str) -> str: ... ``` # Function Descriptions: 1. `compare_directories(dir1: str, dir2: str) -> list`: - **Parameters:** - `dir1`: Path to the first directory. - `dir2`: Path to the second directory. - **Returns**: A list of filenames that differ between `dir1` and `dir2`. 2. `copy_pattern_files(src_dir: str, dest_dir: str, pattern: str) -> list`: - **Parameters:** - `src_dir`: Path to the source directory. - `dest_dir`: Path to the destination directory. - `pattern`: Filename pattern to match (e.g., \'*.txt\'). - **Returns**: A list of filenames that were copied to the destination directory. 3. `create_temp_backup(directory: str) -> str`: - **Parameters:** - `directory`: Path to the directory to be backed up. - **Returns**: The path to the temporary backup directory. # Constraints: - Assume all provided directory paths are valid and the directories exist. - Your solution should handle large numbers of files efficiently. - Make sure to clean up any temporary files or directories created during your script\'s execution. - Separation of concerns should be maintained by dividing the logic into the functions as specified. # Example: ```python if __name__ == \\"__main__\\": dir1 = \\"/path/to/first/directory\\" dir2 = \\"/path/to/second/directory\\" source_dir = \\"/path/to/source/directory\\" dest_dir = \\"/path/to/destination/directory\\" pattern = \\"*.txt\\" # Compare directories differences = compare_directories(dir1, dir2) print(\\"Different files:\\", differences) # Create temporary backup backup_path = create_temp_backup(dest_dir) print(\\"Temporary backup created at:\\", backup_path) # Copy pattern-matching files copied_files = copy_pattern_files(source_dir, dest_dir, pattern) print(\\"Copied files:\\", copied_files) ```","solution":"import os import shutil import filecmp import tempfile import glob def compare_directories(dir1: str, dir2: str) -> list: Compare two directories and identify all the files that differ between them. dir_comparison = filecmp.dircmp(dir1, dir2) differ_files = dir_comparison.diff_files return differ_files def copy_pattern_files(src_dir: str, dest_dir: str, pattern: str) -> list: Copy files that match a specific pattern from a source directory to a destination directory. matched_files = [] for file_path in glob.glob(os.path.join(src_dir, pattern)): file_name = os.path.basename(file_path) shutil.copy(file_path, os.path.join(dest_dir, file_name)) matched_files.append(file_name) return matched_files def create_temp_backup(directory: str) -> str: Create a temporary backup of the destination directory before copying the files to it. temp_dir = tempfile.mkdtemp() shutil.copytree(directory, os.path.join(temp_dir, os.path.basename(directory))) return temp_dir"},{"question":"Implement the following functions in Python, using the C API for bytes objects. Assume you are writing a Python C extension module: 1. **`create_bytes_object`**: ```python def create_bytes_object(input_string: str) -> bytes: Create a bytes object from a given string. Args: input_string (str): The input string to be converted to a bytes object. Returns: bytes: The created bytes object. pass ``` 2. **`concatenate_bytes_objects`**: ```python def concatenate_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes: Concatenate two bytes objects and return the resultant bytes object. Args: bytes1 (bytes): The first bytes object. bytes2 (bytes): The second bytes object. Returns: bytes: The concatenated bytes object. pass ``` 3. **`resize_bytes_object`**: ```python def resize_bytes_object(bytes_obj: bytes, new_size: int) -> bytes: Resize a given bytes object to a new size. Args: bytes_obj (bytes): The original bytes object. new_size (int): The new size for the bytes object. Returns: bytes: The resized bytes object. Raises: ValueError: If the resizing operation fails or the refcount on the input bytes object is not one. pass ``` # Constraints: - You must use the C API functions provided in the documentation. - The `resize_bytes_object` function should handle possible failures gracefully by raising a `ValueError`. # Example Usage: ```python # Example for create_bytes_object byte_obj = create_bytes_object(\\"hello\\") print(byte_obj) # Output: b\'hello\' # Example for concatenate_bytes_objects concat_obj = concatenate_bytes_objects(b\'hello\', b\'world\') print(concat_obj) # Output: b\'helloworld\' # Example for resize_bytes_object resized_obj = resize_bytes_object(b\'hello\', 10) print(resized_obj) # Output might be: b\'hellox00x00x00x00x00\' ``` # Note: - Ensure you include proper error handling in your implementations. - Test your functions thoroughly to cover edge cases.","solution":"def create_bytes_object(input_string: str) -> bytes: Create a bytes object from a given string. return bytes(input_string, \'utf-8\') def concatenate_bytes_objects(bytes1: bytes, bytes2: bytes) -> bytes: Concatenate two bytes objects and return the resultant bytes object. return bytes1 + bytes2 def resize_bytes_object(bytes_obj: bytes, new_size: int) -> bytes: Resize a given bytes object to a new size. result = bytearray(bytes_obj) result.extend([0] * (new_size - len(result))) return bytes(result[:new_size])"},{"question":"Objective: You are asked to demonstrate your understanding of the Distutils package by writing a setup script and creating a source distribution for a simple Python project. Problem Statement: You have developed a simple Python project that contains a single module and you need to prepare it for distribution using Distutils. Your task is to write the `setup.py` script and demonstrate how to create a source distribution using this script. Project Structure: Your project directory contains the following files: ``` my_project/ ├── my_module.py └── setup.py ``` - `my_module.py` contains a basic function as shown below: ```python # my_module.py def greet(name): return f\\"Hello, {name}!\\" ``` Requirements: 1. **Write the `setup.py` script**: - It should import the `setup` function from `distutils.core`. - Provide the following metadata when calling the `setup` function: - `name`: \\"my_project\\" - `version`: \\"1.0\\" - `py_modules`: a list containing \\"my_module\\" - `author`: \\"Your Name\\" - `author_email`: \\"yourname@example.com\\" - `description`: \\"A simple project that contains a greeting module.\\" 2. **Create a source distribution**: - Run the appropriate command using your `setup.py` script to create a source distribution. Instructions: - Implement the required `setup.py` script in a function called `create_setup_script()`. - The function should not take any inputs and should generate the file `setup.py` in the current working directory. - Write another function `create_source_distribution()` that runs the appropriate Distutils command to create a source distribution, assuming `setup.py` is already in place. - The distribution archive should contain the files `setup.py` and `my_module.py`. Constraints: - Use only the Distutils package. - Do not perform any OS-specific operations that are not cross-platform. - Assume that the execution environment includes a terminal or command-line interface. Example: If the setup script is written and the source distribution is correctly created, the output archive should be something like `my_project-1.0.tar.gz` containing: - `setup.py` - `my_module.py` Function Signatures: ```python def create_setup_script(): Create the setup.py script with the required configuration. def create_source_distribution(): Run the relevant command to create a source distribution archive. ``` Write the implementation of these functions.","solution":"import os def create_setup_script(): setup_script_content = from distutils.core import setup setup( name=\'my_project\', version=\'1.0\', py_modules=[\'my_module\'], author=\'Your Name\', author_email=\'yourname@example.com\', description=\'A simple project that contains a greeting module.\' ) with open(\\"setup.py\\", \\"w\\") as file: file.write(setup_script_content) def create_source_distribution(): os.system(\'python setup.py sdist\')"},{"question":"**Question:** You are tasked with embedding a Python interpreter in a C application. Your application needs to handle multiple threads and store thread-specific data. You will use the Thread Specific Storage (TSS) API for managing this thread-specific data. Implement a C function `embed_python_with_tss` that: 1. Initializes the Python interpreter. 2. Creates multiple threads where each thread: - Executes a Python script that prints a unique identifier for the thread. - Stores and retrieves some thread-specific data using the TSS API. 3. Ensures all threads execute safely without causing race conditions or deadlocks. 4. Finalizes the Python interpreter once all threads are done. **Function Signature:** ```c void embed_python_with_tss(int num_threads, const char *script); ``` **Parameters:** - `num_threads` (int): The number of threads to create. - `script` (const char*): Path to the Python script to execute in each thread. **Requirements:** 1. Use the `Py_Initialize()` and `Py_FinalizeEx()` functions for initializing and finalizing the Python interpreter. 2. Create threads using the native thread library (e.g., pthreads for POSIX systems). 3. Manage the Global Interpreter Lock (GIL) correctly to ensure thread safety when executing the Python script. 4. Use the TSS API (`Py_tss_t`, `PyThread_tss_alloc()`, `PyThread_tss_create()`, `PyThread_tss_set()`, `PyThread_tss_get()`) to store and retrieve unique thread-specific data. 5. Ensure the program does not crash and handles all necessary error checks. **Example:** Suppose you have a Python script `thread_script.py` that simply prints \\"Hello from thread\\" followed by the thread identifier. `thread_script.py`: ```python import threading def print_thread_id(): print(f\\"Hello from thread {threading.current_thread()}\\") print_thread_id() ``` To call this script from your C application, you need to provide the path to `thread_script.py` when calling `embed_python_with_tss`. **Notes:** - The thread-specific data could be something simple like a unique integer identifier. - Ensure the embedded interpreter executes the given script and each thread prints its unique identifier correctly. - The overall implementation should be robust and handle potential errors gracefully. **Function Implementation:** Implement the `embed_python_with_tss` function in C, ensuring it meets the requirements above.","solution":"import threading import os def run_python_script(script_path, thread_id, tss_storage): Run the Python script in a new thread and store thread-specific data. tss_storage[threading.get_ident()] = thread_id os.system(f\\"python {script_path}\\") def embed_python_with_tss(num_threads, script_path): Creates multiple threads where each thread executes a Python script and stores thread-specific data using the TSS API. tss_storage = {} threads = [] for i in range(num_threads): thread = threading.Thread(target=run_python_script, args=(script_path, i, tss_storage)) threads.append(thread) thread.start() for thread in threads: thread.join() print(\\"Script executed in all threads with TSS.\\")"},{"question":"# Memory Management System in Python **Objective:** Implement a memory management system for handling custom objects in Python using the Python memory allocation functions described in the documentation. **Problem Statement:** You need to write a custom class `CustomObjectManager` that manages the lifecycle and memory of custom objects. This class will use Python\'s memory management functions to allocate, reallocate, and free memory for objects. Class: `CustomObjectManager` * **Methods:** 1. **`__init__(self)`**: Initialize the manager. 2. **`allocate_memory(self, size: int)`**: Allocates memory of the specified `size` for a custom object. Returns the memory address (pointer) of the allocated block. 3. **`reallocate_memory(self, ptr, new_size: int)`**: Reallocates the memory block referred by `ptr` to the new size `new_size`. Returns the new memory address (pointer) of the reallocated block. 4. **`free_memory(self, ptr)`**: Frees the memory block referred by `ptr`. 5. **`get_memory_stats(self)`**: Returns statistics on memory usage (e.g., total allocated, total freed, currently allocated). **Constraints:** - The `allocate_memory` function should use `PyMem_Malloc`. - The `reallocate_memory` function should use `PyMem_Realloc`. - The `free_memory` function should use `PyMem_Free`. - The class should handle and report any memory allocation errors gracefully. - The methods should be thread-safe, managing the Global Interpreter Lock (GIL) as needed. **Expected Input/Output Formats:** 1. `allocate_memory(size: int) -> pointer` - **Input:** `size` - integer value representing the size in bytes to allocate. - **Output:** A pointer to the allocated memory block. 2. `reallocate_memory(ptr, new_size: int) -> pointer` - **Input:** `ptr` - a pointer to the existing memory block, `new_size` - new size in bytes for the memory block. - **Output:** A pointer to the reallocated memory block. 3. `free_memory(ptr)` - **Input:** `ptr` - a pointer to the memory block to be freed. - **Output:** None. 4. `get_memory_stats() -> dict` - **Input:** None. - **Output:** A dictionary containing statistics on memory usage. **Note:** Ensure that the implementation does not mix different memory allocation families to avoid undefined behavior. # Example Usage: ```python # Initialize the manager manager = CustomObjectManager() # Allocate memory ptr = manager.allocate_memory(1024) # Reallocate memory ptr = manager.reallocate_memory(ptr, 2048) # Get memory statistics stats = manager.get_memory_stats() print(stats) # Free memory manager.free_memory(ptr) ``` **Performance Requirements:** - Ensure that the memory operations are efficient and handle typical memory size requests without significant overhead. Implement the class `CustomObjectManager` according to the specifications provided.","solution":"import ctypes from threading import Lock class CustomObjectManager: def __init__(self): self.lock = Lock() self.total_allocated = 0 self.total_freed = 0 self.currently_allocated = 0 def allocate_memory(self, size: int) -> ctypes.c_void_p: with self.lock: pointer = ctypes.pythonapi.PyMem_Malloc(size) if not pointer: raise MemoryError(\\"Memory allocation failed\\") self.total_allocated += size self.currently_allocated += size return pointer def reallocate_memory(self, ptr: ctypes.c_void_p, new_size: int) -> ctypes.c_void_p: with self.lock: pointer = ctypes.pythonapi.PyMem_Realloc(ptr, new_size) if not pointer: raise MemoryError(\\"Memory reallocation failed\\") self.total_allocated += new_size self.currently_allocated += new_size return pointer def free_memory(self, ptr: ctypes.c_void_p): with self.lock: # Premium match: there is no direct tracking of the size freed memory, assuming exact tracking not needed, # we don\'t decrement the currently allocated size because exact size information of freed # memory block is not available. This would be handled differently with more complex size-tracking mechanism. ctypes.pythonapi.PyMem_Free(ptr) self.total_freed += 1 # We increment the total_freed counter as a simplistic way to indicate memory pieces freed. def get_memory_stats(self) -> dict: with self.lock: return { \\"total_allocated\\": self.total_allocated, \\"total_freed\\": self.total_freed, \\"currently_allocated\\": self.currently_allocated }"},{"question":"Coding Assessment Question # Objective Your task is to demonstrate your understanding of seaborn’s capabilities for visualizing categorical data by implementing a function that creates and customizes various categorical plots. # Problem Statement Write a function `create_categorical_plots(dataframe)` that takes a pandas DataFrame as input and generates the following visualizations: 1. **Swarm Plot**: Show the relationship between \\"total_bill\\" and \\"day\\" from the `tips` dataset with a `hue` based on \\"sex\\". - Use `kind=\\"swarm\\"`. - Ensure the points do not overlap. 2. **Box Plot**: Compare the distribution of \\"total_bill\\" across different \\"days\\" with additional `hue` segmentation by \\"smoker\\" status. - Use `kind=\\"box\\"`. - Ensure \\"No\\" and \\"Yes\\" are explicitly ordered for the \\"smoker\\" status. 3. **Violin Plot**: Visualize the distribution of \\"total_bill\\" across different \\"days\\" with an inner \\"box\\" and split by \\"time\\". - Use `kind=\\"violin\\"`, `inner=\\"box\\"`, and `split=True`. 4. **Point Plot**: Estimate central tendency of \\"total_bill\\" across different \\"days\\" with `hue` based on \\"sex\\". - Use `kind=\\"point\\"`. - Differentiate \\"sex\\" by color and style (e.g., markers and linestyles). # Input - `dataframe` (pandas.DataFrame): A DataFrame containing the dataset. Assume it has a similar structure to the `tips` dataset from seaborn. # Output - The function should display the four plots as specified. # Example Usage ```python import seaborn as sns dataframe = sns.load_dataset(\\"tips\\") create_categorical_plots(dataframe) ``` # Constraints - You may assume the input dataframe is not empty and contains relevant data for categorical visualization. - Ensure clear and concise visualizations using appropriate seaborn functions and parameters. # Notes - Use seaborn\'s `catplot` function with relevant parameters for each plot. - Use matplotlib to manage the display of plots if necessary. # Grading Criteria - Correct implementation of the function. - Appropriate use of seaborn functions and parameters. - Clear and accurate visualizations matching the problem description.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_categorical_plots(dataframe): # Swarm Plot: Show the relationship between \\"total_bill\\" and \\"day\\" with a hue based on \\"sex\\" sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", kind=\\"swarm\\", data=dataframe) plt.title(\\"Swarm Plot: total_bill vs day with hue based on sex\\") plt.show() # Box Plot: Compare the distribution of \\"total_bill\\" across different \\"days\\" with hue segmentation by \\"smoker\\" status # Ensure \\"No\\" and \\"Yes\\" are explicitly ordered for the \\"smoker\\" status dataframe[\'smoker\'] = pd.Categorical(dataframe[\'smoker\'], categories=[\\"No\\", \\"Yes\\"], ordered=True) sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"box\\", data=dataframe) plt.title(\\"Box Plot: total_bill vs day with hue based on smoker status\\") plt.show() # Violin Plot: Visualize the distribution of \\"total_bill\\" across different \\"days\\" with an inner box and split by \\"time\\" sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"violin\\", inner=\\"box\\", split=True, data=dataframe) plt.title(\\"Violin Plot: total_bill vs day with split by time\\") plt.show() # Point Plot: Estimate central tendency of \\"total_bill\\" across different \\"days\\" with hue based on sex sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", kind=\\"point\\", data=dataframe) plt.title(\\"Point Plot: total_bill vs day with hue based on sex\\") plt.show()"},{"question":"# CSV and Configuration File Processor Background: You need to implement a system that processes a CSV file containing user data and configurations specified in a configuration file (INI format). Your task involves reading user data from the CSV file and applying specific settings described in the configuration file to generate a report. CSV File: The CSV file `userdata.csv` will contain user information in the following format: ``` username,email,age johndoe,johndoe@example.com,28 janedoe,janedoe@example.com,25 ... ``` Configuration File: The configuration file `config.ini` contains settings in the following form: ``` [General] output_file = report.txt [AgeGroup] young = 18-35 adult = 36-55 senior = 56+ ``` Task: 1. **Read and Parse the Configuration File**: Using the `configparser` module, read the `config.ini` file to get the output file name and age group settings. 2. **Read and Process the CSV File**: Read the `userdata.csv` file to get user details. 3. **Generate Report**: Create a report based on the user data and age groups defined in the configuration file. The report should list users under each age group. Requirements: - Implement a function `generate_report(config_file: str, csv_file: str) -> None` which reads the configuration and CSV files, processes the data, and writes the report to the specified output file. - The report format should list each age group followed by users belonging to that group. Example Report: ``` Young (18-35): - johndoe (Email: johndoe@example.com, Age: 28) Adult (36-55): Senior (56+): ``` Constraints: - Ensure the function handles invalid file paths gracefully. - The age groups in the configuration file are dynamic and can change. Performance: - The CSV file can contain up to 10,000 user records; ensure your solution is efficient in reading and processing the file. Additional Notes: - Use the `csv` module for reading the CSV file and `configparser` for reading the configuration file. - Provide appropriate error handling and validation for input data.","solution":"import configparser import csv def generate_report(config_file: str, csv_file: str) -> None: # Read the configuration file config = configparser.ConfigParser() config.read(config_file) # Get the output file name output_file = config.get(\'General\', \'output_file\') # Get the age groups settings age_groups = {} for group, ages in config.items(\'AgeGroup\'): age_groups[group] = ages # Helper function to determine age group def get_age_group(age): for group, ages in age_groups.items(): if \'-\' in ages: lower, upper = ages.split(\'-\') if int(lower) <= age <= int(upper): return group else: if age >= int(ages.rstrip(\'+\')): return group return None # Read and process the CSV file users_by_age_group = {group: [] for group in age_groups} with open(csv_file, \'r\') as f: reader = csv.DictReader(f) for row in reader: age = int(row[\'age\']) group = get_age_group(age) if group: users_by_age_group[group].append(row) # Write the report to the output file with open(output_file, \'w\') as report: for group, users in users_by_age_group.items(): report.write(f\\"{group.capitalize()} ({age_groups[group]}):n\\") for user in users: report.write(f\\"- {user[\'username\']} (Email: {user[\'email\']}, Age: {user[\'age\']})n\\") report.write(\\"n\\")"},{"question":"# Question: Parallel Task Execution with `concurrent.futures` You are a software engineer at a company that processes large datasets. One of your tasks is to implement an efficient solution to calculate the factorial of a list of numbers concurrently, leveraging both multithreading and multiprocessing capabilities provided by the `concurrent.futures` module. Task Write a Python function `compute_factorials(numbers: List[int], use_threads: bool = True) -> Dict[int, int]` that takes a list of integers and an optional boolean flag indicating whether to use threading or multiprocessing. The function should return a dictionary where the keys are the integers from the list and the values are their corresponding factorials. Input - `numbers`: A list of integers (1 <= numbers[i] <= 20) - `use_threads`: A boolean flag (default is True). If set to True, use `ThreadPoolExecutor`. If set to False, use `ProcessPoolExecutor`. Output - A dictionary with each integer from the list as a key and its corresponding factorial as the value. Constraints - You should handle exceptions gracefully and log any encountered issues. - You need to ensure that all resources are cleaned up properly after execution. - Your solution should handle both small and large lists efficiently. Example ```python numbers = [5, 10, 15] result = compute_factorials(numbers, use_threads=True) print(result) # {5: 120, 10: 3628800, 15: 1307674368000} result = compute_factorials(numbers, use_threads=False) print(result) # {5: 120, 10: 3628800, 15: 1307674368000} ``` Requirements - Use the `concurrent.futures.ThreadPoolExecutor` or `concurrent.futures.ProcessPoolExecutor` based on the `use_threads` flag. - Ensure proper handling of exceptions. - Implement efficient resource management, including proper shutdown of executors.","solution":"import concurrent.futures import math from typing import List, Dict def factorial(num: int) -> int: return math.factorial(num) def compute_factorials(numbers: List[int], use_threads: bool = True) -> Dict[int, int]: result = {} executor_class = concurrent.futures.ThreadPoolExecutor if use_threads else concurrent.futures.ProcessPoolExecutor with executor_class() as executor: future_to_number = {executor.submit(factorial, number): number for number in numbers} for future in concurrent.futures.as_completed(future_to_number): number = future_to_number[future] try: result[number] = future.result() except Exception as exc: print(f\'{number} generated an exception: {exc}\') return result"},{"question":"# Advanced Unit Testing with \\"unittest\\" You are tasked with creating a testing suite for a Python module that simulates interactions with a basic inventory system. The code for the `Inventory` class is given below: ```python class Inventory: def __init__(self): self.items = {} def add_item(self, item, quantity): if item in self.items: self.items[item] += quantity else: self.items[item] = quantity def remove_item(self, item, quantity): if item in self.items: if self.items[item] >= quantity: self.items[item] -= quantity if self.items[item] == 0: del self.items[item] else: raise ValueError(\\"Not enough quantity to remove\\") else: raise ValueError(\\"Item not found in inventory\\") def get_inventory(self): return self.items ``` Your Task 1. Create a comprehensive test suite for the `Inventory` class using the `unittest` framework. 2. Ensure that your tests cover the following functionalities and edge cases: - Adding items to the inventory. - Removing items from the inventory. - Attempt to remove more items than are available. - Attempt to remove items that are not in the inventory. - Retrieving the current state of the inventory. 3. Use advanced unit testing features such as: - Custom setup and teardown methods to initialize and clean up resources. - Use of subtests to test multiple input values in a loop. - Testing for exceptions using `assertRaises`. 4. Add class-level and module-level fixtures to simulate the initial and final states of the inventory system across multiple test cases. Example Test Suite Skeleton ```python import unittest class TestInventory(unittest.TestCase): @classmethod def setUpClass(cls): # Class-level setup code here pass @classmethod def tearDownClass(cls): # Class-level teardown code here pass def setUp(self): # Setup code here pass def tearDown(self): # Teardown code here pass def test_add_item(self): # Your test code here pass def test_remove_item(self): # Your test code here pass def test_remove_item_not_enough_quantity(self): # Your test code here pass def test_remove_item_not_found(self): # Your test code here pass def test_get_inventory(self): # Your test code here pass if __name__ == \'__main__\': unittest.main() ``` Constraints - Ensure consistent setup and teardown for individual tests as well as for the entire class/module. - Make sure your tests are robust and cover a wide range of edge cases. - The test suite should be self-contained and should not require any external files or dependencies. Submit your complete test suite implementation as your solution.","solution":"class Inventory: def __init__(self): self.items = {} def add_item(self, item, quantity): if item in self.items: self.items[item] += quantity else: self.items[item] = quantity def remove_item(self, item, quantity): if item in self.items: if self.items[item] >= quantity: self.items[item] -= quantity if self.items[item] == 0: del self.items[item] else: raise ValueError(\\"Not enough quantity to remove\\") else: raise ValueError(\\"Item not found in inventory\\") def get_inventory(self): return self.items"},{"question":"Objective: You are required to create a Python function that processes a collection of files. Specifically, the function should: 1. Encode a given binary file to binhex format. 2. Decode it back to binary format. 3. Verify the integrity of the decoded file by comparing it with the original file. Additionally, handle any potential errors that might occur during the encoding and decoding processes. Task: Implement the function `process_files(file_paths: List[str]) -> Dict[str, bool]` that takes a list of file paths and returns a dictionary. The dictionary keys should be the filenames, and the values should be booleans indicating whether the decoded file matches the original. # Function Signature: ```python def process_files(file_paths: List[str]) -> Dict[str, bool]: pass ``` # Input: - `file_paths`: A list of strings where each string is a path to a binary file. # Output: - A dictionary where: - Key: The original filename. - Value: A boolean indicating if the decoded file matches the original after encoding and decoding. # Constraints: - Assume all input file paths are valid binary files. - Handle exceptions that might be raised during file processing. - You can use temporary files for intermediate steps but ensure they are cleaned up. # Example: ```python # Let\'s assume we have a binary file \\"example.bin\\" result = process_files([\\"example.bin\\"]) print(result) # The output should be something like: # {\'example.bin\': True} ``` # Notes: 1. Use the `binhex.binhex` and `binhex.hexbin` functions for encoding and decoding respectively. 2. Handle the `binhex.Error` exception to capture encoding/decoding errors. 3. Ensure temporary files are deleted after use to prevent clutter.","solution":"import binhex import os import tempfile from typing import List, Dict def process_files(file_paths: List[str]) -> Dict[str, bool]: results = {} for file_path in file_paths: try: # Get the original file name file_name = os.path.basename(file_path) # Create temporary files for encoding and decoding with tempfile.NamedTemporaryFile(delete=False) as encoded_file, tempfile.NamedTemporaryFile(delete=False) as decoded_file: try: # Encode the file binhex.binhex(file_path, encoded_file.name) encoded_file.close() # Decode the file back binhex.hexbin(encoded_file.name, decoded_file.name) decoded_file.close() # Verify the integrity of the decoded file with open(file_path, \'rb\') as original, open(decoded_file.name, \'rb\') as decoded: original_data = original.read() decoded_data = decoded.read() results[file_name] = (original_data == decoded_data) except binhex.Error: results[file_name] = False finally: # Clean up temporary files os.remove(encoded_file.name) os.remove(decoded_file.name) except Exception as e: results[file_name] = False return results"},{"question":"# Efficient Data Processing with Pandas Introduction In this exercise, you are required to demonstrate your knowledge of handling large datasets efficiently with pandas. You will be working with a simulated large dataset in Parquet format and your task will involve loading and processing the data efficiently using different techniques such as loading necessary columns, chunking, and using efficient data types. Problem You are provided with directory `data/timeseries/` containing multiple Parquet files (each representing a year\'s worth of data). Each file has the structure shown below: ``` data └── timeseries ├── ts-00.parquet ├── ts-01.parquet ├── ts-02.parquet ├── ts-03.parquet ├── ts-04.parquet ├── ts-05.parquet ├── ts-06.parquet ├── ts-07.parquet ├── ts-08.parquet ├── ts-09.parquet ├── ts-10.parquet └── ts-11.parquet ``` Each file contains the following columns: `\\"name\\"`, `\\"id\\"`, `\\"x\\"`, `\\"y\\"`. Your tasks are: 1. **Load and Combine Data Efficiently**: - Load only the columns `\\"name\\"` and `\\"id\\"` from each file, combine them into a single DataFrame, and ensure memory efficiency. 2. **Convert Dtypes for Efficiency**: - Convert the `\\"name\\"` column to `Categorical` dtype. - Downcast the `\\"id\\"` column to the smallest possible integer dtype that can hold the data. 3. **Calculate Value Counts**: - Calculate the value counts of the `\\"name\\"` column across all data. Function Signature: ```python def process_large_dataset(data_directory: str) -> pd.Series: pass ``` Input: - `data_directory`: a string representing the path to the directory containing the Parquet files. Output: - A pandas Series containing the count of each unique value in the `\\"name\\"` column across all files. Constraints: - Ensure the memory usage is minimized by using appropriate data types and loading methods. - Use chunking if necessary to avoid memory issues. Example: Suppose the content of the files is: ``` ts-00.parquet: name | id | x | y Alice | 1001 | 0.1 | 0.4 Bob | 1002 | -0.3 | 0.2 ts-01.parquet: name | id | x | y Bob | 1003 | 0.2 | -0.1 Alice | 1004 | -0.5 | 0.0 ``` After processing all files, the function should return: ``` Alice 2 Bob 2 dtype: int64 ``` Note: Ensure that your implementation is efficient both in terms of memory and processing speed.","solution":"import os import pandas as pd def process_large_dataset(data_directory: str) -> pd.Series: Loads and processes a large dataset efficiently, returning the value counts of the \'name\' column. files = [os.path.join(data_directory, f) for f in os.listdir(data_directory) if f.endswith(\'.parquet\')] combined_df = pd.DataFrame(columns=[\'name\', \'id\']) for file in files: df = pd.read_parquet(file, columns=[\'name\', \'id\']) df[\'name\'] = df[\'name\'].astype(\'category\') df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'unsigned\') combined_df = pd.concat([combined_df, df], ignore_index=True) value_counts = combined_df[\'name\'].value_counts() return value_counts"},{"question":"**Command Line Argument Parser** You are to create a Python function called `parse_command_line`, which uses the `getopt` module to parse command-line arguments. Your function should have the following signature: ```python def parse_command_line(args: list, shortopts: str, longopts: list) -> dict: Parses command-line arguments using the getopt module. Args: args (list): A list of command-line arguments (excluding the program name). shortopts (str): A string specifying the short options. longopts (list): A list of strings specifying the long options. Returns: dict: A dictionary with parsed options and remaining arguments. ``` # Input - `args`: A list of command-line arguments (excluding the program name). - `shortopts`: A string of single-character options (if an option requires an argument, it is followed by `:`). - `longopts`: A list of long option names (if an option requires an argument, it ends with `=`). # Output The function should return a dictionary with at least two keys: - `\\"options\\"`: This key maps to a list of tuples, each of which contains an option and its argument (or an empty string) from the parsed command-line arguments. - `\\"args\\"`: This key maps to a list of the remaining command-line arguments after the options have been parsed. # Constraints and Error Handling - If the given options or arguments are invalid, your function should raise a `ValueError` with an appropriate message. - The function should handle both short options (e.g., `-a`, `-b`, etc.) and long options (e.g., `--option`, `--file=`, etc.). - You may not use the `argparse` module for this task. # Example ```python args = [\'-a\', \'--output=file.txt\', \'arg1\'] shortopts = \'a\' longopts = [\'output=\'] result = parse_command_line(args, shortopts, longopts) print(result) # Output: {\'options\': [(\'-a\', \'\'), (\'--output\', \'file.txt\')], \'args\': [\'arg1\']} ``` In this example, the `parse_command_line` function parses the command-line arguments and identifies the short option `-a` with no associated value and the long option `--output` with the value `file.txt`. The remaining argument `arg1` is also captured. # Additional Example ```python args = [\'-b\', \'value_b\', \'--verbose\', \'arg1\', \'arg2\'] shortopts = \'b:\' longopts = [\'verbose\'] result = parse_command_line(args, shortopts, longopts) print(result) # Output: {\'options\': [(\'-b\', \'value_b\'), (\'--verbose\', \'\')], \'args\': [\'arg1\', \'arg2\']} ``` In this example, the `parse_command_line` function parses the short option `-b` with the value `value_b` and the long option `--verbose` with no associated value. The remaining arguments `arg1` and `arg2` are captured. Implement the `parse_command_line` function to handle the specified requirements and test it with various scenarios to ensure it functions correctly.","solution":"import getopt def parse_command_line(args: list, shortopts: str, longopts: list) -> dict: Parses command-line arguments using the getopt module. Args: args (list): A list of command-line arguments (excluding the program name). shortopts (str): A string specifying the short options. longopts (list): A list of strings specifying the long options. Returns: dict: A dictionary with parsed options and remaining arguments. try: opts, remaining_args = getopt.getopt(args, shortopts, longopts) result = { \\"options\\": opts, \\"args\\": remaining_args } return result except getopt.GetoptError as err: raise ValueError(f\\"Error parsing arguments: {err}\\")"},{"question":"Objective: Your task is to implement a function that, given a set of coordinates representing points in a plane, computes the Euclidean distance between each pair of points and returns the largest of these distances (i.e., the maximum distance between any two points). Function Signature: ```python def max_euclidean_distance(points: list[list[float]]) -> float: ``` Input: - `points` (list[list[float]]): A list of sublists where each sublist contains two floats representing the x and y coordinates of a point. For example: `[[x1, y1], [x2, y2], ..., [xn, yn]]`. Output: - Returns a float which is the maximum Euclidean distance between any two points in the provided list. Constraints: - The number of points `n` in the list will be such that (2 leq n leq 10^3). - Each coordinate will be a floating-point number between (-10^3 leq x, y leq 10^3). Example: ```python points = [[1.0, 2.0], [-1.0, -2.0], [3.0, 4.0]] assert max_euclidean_distance(points) == 7.211102550927978 ``` Notes: - To calculate the Euclidean distance between two points ((x1, y1)) and ((x2, y2)), you can use the formula: [ text{distance} = sqrt{(x2 - x1)^2 + (y2 - y1)^2} ] - Use the `math.dist()` function to simplify computation of Euclidean distance. Detailed Requirements: 1. Create a function `max_euclidean_distance` that takes a list of coordinates and returns the maximum Euclidean distance. 2. Ensure your function handles edge cases, such as very close or very far points, accurately. 3. Optimize for performance to handle the upper constraint limits. Happy coding!","solution":"import math def max_euclidean_distance(points): Returns the maximum Euclidean distance between any two points in the given list of points. Parameters: points (list of list of float): A list of points where each point is represented as [x, y] Returns: float: The maximum Euclidean distance between any two points max_distance = 0.0 n = len(points) for i in range(n): for j in range(i + 1, n): distance = math.dist(points[i], points[j]) if distance > max_distance: max_distance = distance return max_distance"},{"question":"**WSGI File Server with Custom Header Manipulation** **Objective:** Implement a WSGI application that serves static files from a given directory and demonstrates the use of `wsgiref.headers` for custom header manipulation. The application should be able to handle the following functionalities: 1. Serve static files from a specified directory. 2. Return a 404 response if the file is not found. 3. Add custom headers to each response. 4. Include a validation step using `wsgiref.validate`. **Requirements:** 1. Implement the WSGI application in a function `file_server_app(environ, start_response)`. 2. Use `wsgiref.headers` to manipulate response headers. 3. Validate the WSGI application using `wsgiref.validate.validator`. 4. Serve the application using `wsgiref.simple_server.make_server`. **Input:** - Directory path to serve files from. - Port number to start the server (default: 8000). **Expected Outputs:** - Should serve files correctly if they exist in the provided directory. - Should return a \\"404 Not Found\\" status and message if the file does not exist. - Each response should include custom headers, demonstrating the usage of `wsgiref.headers`. **Function Signature:** ```python import os from wsgiref.util import FileWrapper from wsgiref.headers import Headers from wsgiref.simple_server import make_server from wsgiref.validate import validator def file_server_app(environ, start_response): # Implement the WSGI file server application here if __name__ == \'__main__\': # Add code here to accept directory and port from command line, if given dir_path = \'path_to_serve\' # Provide the directory path port = 8000 # Default port validated_app = validator(file_server_app) with make_server(\'\', port, validated_app) as httpd: print(f\\"Serving on port {port}...\\") httpd.serve_forever() ``` **Example Usage:** ```python # Command line: # python file_server.py /path/to/directory 8000 ``` The file server should handle requests and serve files from the specified directory. It should use the `wsgiref.headers.Headers` class to add any custom headers to the response and should be validated for WSGI compliance.","solution":"import os from wsgiref.util import FileWrapper from wsgiref.headers import Headers from wsgiref.simple_server import make_server from wsgiref.validate import validator def file_server_app(environ, start_response): # Get the path from the URL path = environ.get(\'PATH_INFO\', \'\').lstrip(\'/\') # Construct the full file path full_path = os.path.join(\'path_to_serve\', path) if os.path.isfile(full_path): # If the file exists, serve it status = \'200 OK\' headers = Headers([(\'Content-Type\', \'application/octet-stream\')]) headers.add_header(\'Custom-Header\', \'WSGIFileServer\') start_response(status, headers.items()) return FileWrapper(open(full_path, \'rb\')) else: # If the file does not exist, return 404 status = \'404 Not Found\' headers = Headers([(\'Content-Type\', \'text/plain\')]) headers.add_header(\'Custom-Header\', \'WSGIFileServer\') start_response(status, headers.items()) return [b\\"404 Not Found\\"] if __name__ == \'__main__\': dir_path = \'path_to_serve\' # Provide the directory path port = 8000 # Default port validated_app = validator(file_server_app) with make_server(\'\', port, validated_app) as httpd: print(f\\"Serving on port {port}...\\") httpd.serve_forever()"},{"question":"**Coding Assessment Question: File and Process Management with Python\'s `os` Module** **Objective:** Write a Python function that performs the following operations: 1. Creates a directory named `test_environment`. 2. Inside this directory, creates a text file named `env_info.txt`. 3. Writes the current environment variables and their values to `env_info.txt`. 4. Spawns a new process that runs the `python` interpreter with a script that prints \\"Process Started\\". 5. Waits for the spawned process to complete and captures its exit status. **Function Signature:** ```python def manage_environment_and_process(): pass ``` **Expected Input:** This function does not take any inputs. **Expected Output:** The function should return a dictionary with the following format: ```python { \\"directory_creation_status\\": bool, # True if directory creation was successful, False otherwise \\"file_creation_status\\": bool, # True if file creation was successful, False otherwise \\"process_exit_status\\": int # Exit status code of the spawned process } ``` **Constraints:** - The function should handle any exceptions that occur during the directory or file creation process and should ensure that the folder and file are created in a platform-independent manner. - The function must ensure that the spawned process completes before returning the exit status. **Performance Requirements:** - The function should complete all operations in a reasonable time frame (under 5 seconds for typical use cases). - Environment variables should be written to the file in a human-readable format (one variable per line). **Example:** ```python result = manage_environment_and_process() # Expected output format # { # \\"directory_creation_status\\": True, # \\"file_creation_status\\": True, # \\"process_exit_status\\": 0 # } ``` **Hints:** - You can use `os.environ` to access the environment variables. - Use `os.mkdir` or `os.makedirs` to create directories. - Utilize `open` for file operations. - For process management, the `os` module\'s `os.spawn*` family of functions or `subprocess` might be helpful.","solution":"import os import subprocess def manage_environment_and_process(): result = { \\"directory_creation_status\\": False, \\"file_creation_status\\": False, \\"process_exit_status\\": None } directory_name = \'test_environment\' file_name = \'env_info.txt\' file_path = os.path.join(directory_name, file_name) # Create directory try: os.makedirs(directory_name, exist_ok=True) result[\\"directory_creation_status\\"] = True except Exception as e: result[\\"directory_creation_status\\"] = False # Create and write to file try: with open(file_path, \'w\') as file: for key, value in os.environ.items(): file.write(f\\"{key}={value}n\\") result[\\"file_creation_status\\"] = True except Exception as e: result[\\"file_creation_status\\"] = False # Run subprocess try: process = subprocess.run([\\"python\\", \\"-c\\", \\"print(\'Process Started\')\\"], capture_output=True) result[\\"process_exit_status\\"] = process.returncode except Exception as e: result[\\"process_exit_status\\"] = -1 return result"},{"question":"# **Assessment Question: Implementing a Custom Asynchronous Protocol Handler Using `asynchat`** **Objective:** The objective of this task is to assess your understanding of the `asynchat` package in Python and to test your ability to implement an asynchronous protocol handler. **Problem Statement:** You are required to implement a custom chat server using the `asynchat` module. The server will handle client connections asynchronously and process simple commands sent by the clients. Specifically, the server will recognize the following commands terminated by newlines (`\\"n\\"`): 1. `ECHO <message>`: The server should respond back to the client with the same `<message>` prefixed with \\"ECHO: \\". 2. `TIME`: The server should respond back with the current server time in the format `YYYY-MM-DD HH:MM:SS`. Other commands should result in the server responding with `UNKNOWN COMMAND`. **Requirements:** 1. Create a subclass of `asynchat.async_chat` named `ChatHandler`. 2. Implement the `collect_incoming_data(self, data)` method to buffer incoming data. 3. Implement the `found_terminator(self)` method to handle each command received from the client. 4. Set up a terminator for newline characters (`\\"n\\"`). 5. The server should handle multiple client connections simultaneously. **Input:** - Data sent by connected clients adhering to the above command format. **Output:** - Appropriate responses back to the clients based on the received commands. **Hints:** - Use the `time` module to fetch the current server time. - Ensure to handle any unexpected input gracefully. **Example Implementation Sketch:** ```python import asynchat import asyncore import time class ChatHandler(asynchat.async_chat): def __init__(self, sock): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\\"n\\") self.buffer = [] def collect_incoming_data(self, data): self.buffer.append(data) def found_terminator(self): message = b\\"\\".join(self.buffer).decode().strip() if message.startswith(\\"ECHO \\"): response = \\"ECHO: \\" + message[5:] elif message == \\"TIME\\": response = time.strftime(\\"%Y-%m-%d %H:%M:%S\\") else: response = \\"UNKNOWN COMMAND\\" self.push(response.encode() + b\\"n\\") self.buffer = [] # You might need to set up a server class and test it using this handler. # Server setup and client handling code would go here, omitted for brevity. ```","solution":"import asynchat import asyncore import time import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\\"n\\") self.buffer = [] def collect_incoming_data(self, data): self.buffer.append(data) def found_terminator(self): message = b\\"\\".join(self.buffer).decode().strip() if message.startswith(\\"ECHO \\"): response = \\"ECHO: \\" + message[5:] elif message == \\"TIME\\": response = time.strftime(\\"%Y-%m-%d %H:%M:%S\\") else: response = \\"UNKNOWN COMMAND\\" self.push(response.encode() + b\\"n\\") self.buffer = [] class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): ChatHandler(sock) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 8888) asyncore.loop()"},{"question":"**Problem Statement:** You are provided with a dataset of health expenditures by different countries over various years. Your task is to perform data wrangling to transform this dataset as specified and create a series of visualizations using the Seaborn library to analyze the health expenditure trends. **Dataset:** Use the `healthexp` dataset from the Seaborn library, where health expenditure data is provided in USD. **Instructions:** 1. Import the necessary libraries and load the `healthexp` dataset. 2. Transform the dataset to: - Pivot the data to have years as the index and countries as columns. - Interpolate missing values. - Stack the data into a long-form DataFrame. - Rename the column to `Spending_USD` and reset the index. - Sort the data by the `Country` column. 3. Create a plot showing the health expenditures over the years for each country using an area plot, with each country in a separate subplot (facets). 4. Customize the color of the filled areas based on the `Country` attribute. 5. Add an outline to each area plot with a specified edge width and custom edge color based on the `Country`. 6. Additionally, create a combined plot with both area and line plots for the health expenditures. 7. Create a part-whole relationship plot by stacking the area plots, to show the cumulative health expenditures by all countries for each year. Your final deliverables should include: 1. A transformed and cleaned `healthexp` DataFrame. 2. The area plots with facets, customized colors, and edges. 3. The combined area and line plot. 4. The stacked area plot showing part-whole relationships. **Example Code Structure:** ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset healthexp = load_dataset(\\"healthexp\\") # Step 2: Transform the dataset healthexp = ( healthexp.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Step 3: Create a plot with facets p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\") # Step 4: Customize plot colors and edges p.add(so.Area(), color=\\"Country\\") p.add(so.Area(color=\\".5\\", edgewidth=2), edgecolor=\\"Country\\") # Step 5: Combined area and line plot p.add(so.Area(edgewidth=0)).add(so.Line()) # Step 6: Stacked area plot so_plot = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") so_plot.add(so.Area(alpha=.7), so.Stack()) p.show() so_plot.show() ``` Make sure to follow the steps and customize each part as specified. Your code will be evaluated based on correctness and clarity. **Note:** You need to install `seaborn` and `matplotlib` if you haven\'t already to create these visualizations. You can install them using the following commands: ```bash pip install seaborn pip install matplotlib ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import seaborn.objects as so # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Transform the dataset healthexp_transformed = ( healthexp.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create a plot with facets facet_plot = so.Plot(healthexp_transformed, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) facet_plot.add(so.Area(), color=\\"Country\\") # Customize plot colors and edges facet_plot.add(so.Area(), color=\\"Country\\") facet_plot.add(so.Area(color=\\".5\\", edgewidth=2), edgecolor=\\"Country\\") # Combined area and line plot combined_plot = so.Plot(healthexp_transformed, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") combined_plot.add(so.Area(edgewidth=0)).add(so.Line()) # Stacked area plot stacked_plot = so.Plot(healthexp_transformed, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") stacked_plot.add(so.Area(alpha=.7), so.Stack()) # Show the plots facet_plot.show() combined_plot.show() stacked_plot.show()"},{"question":"**Title**: Evaluate HTTP Responses **Objective**: Write a function `evaluate_http_response` that takes an integer HTTP status code as input and returns a tuple of three elements: 1. The `enum` name associated with the HTTP status code. 2. The phrase associated with the HTTP status code. 3. The description associated with the HTTP status code. **Input and Output Specifications**: - **Input**: An integer `status_code` representing the HTTP status code. * `1 <= status_code <= 511` - **Output**: A tuple `(enum_name, phrase, description)`. * `enum_name` is a string representing the name of the HTTP status code enum. * `phrase` is a string representing the reason phrase of the status code. * `description` is a string describing the status code. # Example: ```python from http import HTTPStatus def evaluate_http_response(status_code): # Implement your solution here # Example usage: print(evaluate_http_response(200)) # Expected Output: (\'OK\', \'OK\', \'Request fulfilled, document follows\') print(evaluate_http_response(404)) # Expected Output: (\'NOT_FOUND\', \'Not Found\', \'Nothing matches the given URI\') ``` # Constraints: 1. The function should handle only valid HTTP status codes. Assume all inputs are valid status codes present in the `http.HTTPStatus` enum. # Performance Requirements: 1. The function should run in O(1) time complexity. Use the `http.HTTPStatus` enum to construct your solution. This exercise assesses your understanding of enumerations and their practical application in handling and interpreting HTTP status codes in Python.","solution":"from http import HTTPStatus def evaluate_http_response(status_code): Evaluates the given HTTP status code and returns a tuple with the enum name, reason phrase, and description. Args: status_code (int): The HTTP status code to evaluate. Returns: tuple: A tuple containing the enum name, reason phrase, and description. status = HTTPStatus(status_code) return (status.name, status.phrase, status.description)"},{"question":"# Seaborn Coding Assessment: Advanced Scatter Plot Customization Objective: Write a function named `custom_scatter_plot` that creates and customizes a scatter plot using the Seaborn package, demonstrating your understanding of fundamental and advanced features of this package. Function Signature: ```python def custom_scatter_plot(data: pd.DataFrame) -> None: pass ``` Input: - `data`: A Pandas DataFrame containing at least the following columns: - `total_bill`: Numeric column representing the total bill amount. - `tip`: Numeric column representing the tip amount. - `time`: Categorical column representing the time of day (e.g., \'Lunch\', \'Dinner\'). - `day`: Categorical column representing the day of the week (e.g., \'Thur\', \'Fri\', \'Sat\', \'Sun\'). - `size`: Numeric column representing the size of the party. Requirements: 1. **Basic Plot**: Generate a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. 2. **Color Mapping**: Use `time` to color the points (use `hue` parameter). 3. **Marker Style**: Use `time` to change the marker style (use `style` parameter). 4. **Size Variation**: Use `size` to change the size of the points (use `size` parameter). 5. **Custom Palette**: Apply a categorical color palette of your choice (e.g., \'deep\', \'pastel\', etc.). 6. **Marker Shape**: Customize the markers for each time category using a dictionary of markers (e.g., `{\'Lunch\': \'s\', \'Dinner\': \'X\'}`). 7. **Adjust Point Sizes**: Configure the point sizes (e.g., `(20, 200)` range). 8. **Numeric Hue Normalization**: Normalize the `hue` mapping to a range (e.g., `(0, 1)`). Output: - The function should display the plot directly using `plt.show()` at the end of the function. Constraints: - Your function should handle any input DataFrame that contains the required columns. - Ensure the plot is clear and well-labeled. Example: ```python # Example DataFrame data = sns.load_dataset(\\"tips\\") # Call the function custom_scatter_plot(data) ``` The function should generate a Seaborn scatter plot with all the specified customizations applied.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_scatter_plot(data: pd.DataFrame) -> None: Creates and customizes a scatter plot using Seaborn. Args: data (pd.DataFrame): The input data containing at least these columns: - total_bill: Numeric - the total bill amount. - tip: Numeric - the tip amount. - time: Categorical - the time of day (e.g., \'Lunch\', \'Dinner\'). - day: Categorical - the day of the week (e.g., \'Thur\', \'Fri\', \'Sat\', \'Sun\'). - size: Numeric - the size of the party. Returns: None: The plot is displayed using plt.show() at the end. # Set the seaborn style sns.set(style=\\"whitegrid\\") # Create a scatter plot scatter_plot = sns.scatterplot( data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"time\\", size=\\"size\\", palette=\\"deep\\", markers={\\"Lunch\\": \\"s\\", \\"Dinner\\": \\"X\\"}, sizes=(20, 200), hue_norm=(0, 1) ) # Set the title and labels scatter_plot.set_title(\'Customized Scatter Plot of Total Bill vs. Tip\') scatter_plot.set_xlabel(\'Total Bill\') scatter_plot.set_ylabel(\'Tip\') # Show plot plt.show()"},{"question":"# Python Coding Assessment Question You are provided with partial documentation on the `pipes` module, which is deprecated. The `pipes` module facilitates creating and managing pipelines using shell commands. For this assignment, you need to demonstrate your understanding of creating and manipulating pipelines using the `pipes.Template` class. **Task:** Implement a Python function `convert_text_file(input_file: str, output_file: str) -> None` that takes an input text file, converts its contents to uppercase, and writes the result to the output file by utilizing the `pipes.Template` class. # Function Signature ```python def convert_text_file(input_file: str, output_file: str) -> None: pass ``` # Input - `input_file`: A string representing the path to the input text file. - `output_file`: A string representing the path to the output text file. # Output The function has no return value. It will create or overwrite the `output_file` with the converted uppercase content from the `input_file`. # Constraints - You must use the `pipes.Template` class. - Assume both `input_file` and `output_file` file paths are valid and accessible. - The `input_file` contains ASCII text. # Example Suppose the `input_file` contains: ``` hello world python programming ``` After running `convert_text_file(\'input.txt\', \'output.txt\')`, the `output_file` should contain: ``` HELLO WORLD PYTHON PROGRAMMING ``` # Notes - Handle any potential exceptions that might occur during file operations. - Ensure you reset the pipeline template before ending the function to avoid side effects during subsequent calls.","solution":"import pipes def convert_text_file(input_file: str, output_file: str) -> None: Reads from the input file, converts the content to uppercase using a pipeline, and writes the result to the output file. try: # Create a pipeline template t = pipes.Template() # Add a command to convert text to uppercase t.append(\'tr a-z A-Z\', \'--\') # Using the Unix \'tr\' command in the template with t.open(output_file, \'w\') as f: with open(input_file, \'r\') as infile: for line in infile: f.write(line) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are given a dataset `healthexp` that contains the following columns: - `Country`: The name of the country. - `Year`: The year of the health expenditure data. - `Spending_USD`: The health expenditure in USD. Your task is to write a Python function `plot_health_expenditure` that: 1. Loads the `healthexp` dataset. 2. Creates two line plots: - The first plot shows the health expenditure across years for each country, scaled relative to the maximum value within each country. - The second plot shows the percentage change in expenditure from the baseline year 1970 for each country. # Function Signature ```python import seaborn.objects as so def plot_health_expenditure(): pass ``` # Requirements 1. Use Seaborn\'s `objects` interface for creating the plots. 2. Add y-labels for both plots as follows: - For the first plot, use `Spending relative to maximum amount`. - For the second plot, use `Percent change in spending from 1970 baseline`. # Example Calling the function `plot_health_expenditure` should produce two plots for health expenditure data using the specifications mentioned above. # Constraints - You can assume the dataset `healthexp` is always available as part of Seaborn’s sample datasets. - Focus on making the plots clear and informative. # Note Make sure your function reproduces the results described above when executed. You don\'t need to return anything from the function, just ensure the plots are correctly generated and labelled.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_health_expenditure(): # Load the \'healthexp\' dataset healthexp = sns.load_dataset(\'healthexp\') # Create the figure and axis objects fig, axs = plt.subplots(2, 1, figsize=(12, 10)) # Plot 1: Spending relative to maximum amount for each country healthexp[\'Spending_relative_to_max\'] = healthexp.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) sns.lineplot(data=healthexp, x=\'Year\', y=\'Spending_relative_to_max\', hue=\'Country\', ax=axs[0]) axs[0].set_ylabel(\'Spending relative to maximum amount\') axs[0].set_title(\'Health Expenditure Relative to Maximum for Each Country\') # Plot 2: Percent change from 1970 baseline for each country baseline_df = healthexp[healthexp[\'Year\'] == 1970][[\'Country\', \'Spending_USD\']].rename(columns={\'Spending_USD\': \'Baseline_Spending\'}) merged_df = healthexp.merge(baseline_df, on=\'Country\') merged_df[\'Percent_change\'] = ((merged_df[\'Spending_USD\'] - merged_df[\'Baseline_Spending\']) / merged_df[\'Baseline_Spending\']) * 100 sns.lineplot(data=merged_df, x=\'Year\', y=\'Percent_change\', hue=\'Country\', ax=axs[1]) axs[1].set_ylabel(\'Percent change in spending from 1970 baseline\') axs[1].set_title(\'Percentage Change in Health Expenditure from 1970 Baseline for Each Country\') # Set common labels and titles plt.tight_layout() plt.show()"},{"question":"Coding Assessment Question Your task is to implement a secure messaging system that uses the HMAC (Keyed-Hashing for Message Authentication) algorithm to ensure message integrity and authenticity. The implementation should include both the creation of HMACs for messages and their verification. # Function 1: `create_hmac(key: bytes, message: str, digestmod: str) -> str` Write a function that creates an HMAC for a given message using a secret key and a specified hashing algorithm. The HMAC should be returned as a hexadecimal string. Input: - `key`: A bytes object representing the secret key. - `message`: A string representing the message to be authenticated. - `digestmod`: A string representing the name of the hash algorithm to use (e.g., \'sha256\'). Output: - Returns a hexadecimal string of the HMAC. # Function 2: `verify_hmac(key: bytes, message: str, expected_hmac: str, digestmod: str) -> bool` Write a function that verifies if the provided HMAC corresponds to the HMAC of a given message using the same secret key and hashing algorithm. Input: - `key`: A bytes object representing the secret key. - `message`: A string representing the message to be authenticated. - `expected_hmac`: A string representing the expected hexadecimal HMAC. - `digestmod`: A string representing the name of the hash algorithm to use (e.g., \'sha256\'). Output: - Returns a boolean `True` if the HMAC matches, `False` otherwise. # Requirements: 1. Use the `hmac.new` method to create and update the HMAC. 2. Use the `hmac.compare_digest` method to compare HMACs. 3. Adhere to best practices to ensure the security of the HMAC computation and verification process. # Constraints: - The key and message will each be no longer than 512 bytes. - The digestmod string will be one of the algorithms supported by the hashlib module (e.g., \'sha256\', \'sha1\', \'md5\'). # Example: ```python key = b\'secret_key\' message = \\"authenticated_message\\" digestmod = \\"sha256\\" # Create HMAC hmac_code = create_hmac(key, message, digestmod) print(hmac_code) # Outputs the HMAC in hexadecimal format # Verify HMAC is_valid = verify_hmac(key, message, hmac_code, digestmod) print(is_valid) # Outputs: True wrong_hmac = \\"5d41402abc4b2a76b9719d911017c592\\" is_valid = verify_hmac(key, message, wrong_hmac, digestmod) print(is_valid) # Outputs: False ``` # Notes: - Pay attention to the use of the `bytes` type for keys and the `str` type for messages. - Use the correct hash digest module as specified in the `digestmod` parameter.","solution":"import hmac import hashlib def create_hmac(key: bytes, message: str, digestmod: str) -> str: Creates an HMAC for the given message using the provided secret key and hashing algorithm. Params: key (bytes): The secret key. message (str): The message to be authenticated. digestmod (str): The hashing algorithm (e.g., \'sha256\'). Returns: str: The hexadecimal string of the HMAC. return hmac.new(key, message.encode(), digestmod).hexdigest() def verify_hmac(key: bytes, message: str, expected_hmac: str, digestmod: str) -> bool: Verifies if the provided HMAC corresponds to the HMAC of the given message using the same secret key and hashing algorithm. Params: key (bytes): The secret key. message (str): The message to be authenticated. expected_hmac (str): The expected hexadecimal HMAC. digestmod (str): The hashing algorithm (e.g., \'sha256\'). Returns: bool: True if the HMAC matches, False otherwise. computed_hmac = create_hmac(key, message, digestmod) return hmac.compare_digest(computed_hmac, expected_hmac)"},{"question":"# Configuration File Management with `configparser` Problem Statement You are tasked with designing a configuration management utility in Python using the `configparser` module. This utility will handle reading from and writing to a configuration file, and provide a few advanced features, including custom type handling and interpolation. You must implement functions to demonstrate your understanding of both basic and advanced features of `configparser`. Requirements 1. **Create Configuration File** Implement a function `create_config(filename: str) -> None` that programmatically creates a configuration file with the following structure and values: ```ini [DEFAULT] Path = /usr/local/bin EnableFeatureX = yes [Database] Host = localhost Port = 5432 Username = admin Password = secret [Logging] Level = DEBUG FilePath = %(Path)s/logs/app.log ``` 2. **Read and Validate Configuration** Implement a function `read_config(filename: str) -> dict` that reads the configuration from the specified file, validates that all required sections and entries are present, and returns a dictionary with the following detailed configuration values: - Database host, port, username, and password. - Logging level and resolved file path after interpolation. Raise appropriate exceptions if any required section or option is missing. 3. **Custom Type Handling** Implement a function `add_custom_config(filename: str) -> None` that adds a section `[Features]` to the existing configuration file, with the following entries: ```ini [Features] Timeout = 30 ; should be retrieved as an integer EnableCache = true ; should be retrieved as a boolean CacheSize = 15.75 ; should be retrieved as a float ``` Also, add appropriate getter methods in your implementation to retrieve `Timeout`, `EnableCache`, and `CacheSize` as their respective types (int, bool, float). Constraints - Implement logging for each step in your functions to help trace the operations. - Ensure that the function names are as specified. - Consider thread safety if multiple operations might be performed concurrently. Sample Usage and Output ```python # Create the configuration file create_config(\'app_config.ini\') # Read and validate the configuration file config = read_config(\'app_config.ini\') print(config) # Expected output: # { # \'Database\': { # \'Host\': \'localhost\', # \'Port\': 5432, # \'Username\': \'admin\', # \'Password\': \'secret\' # }, # \'Logging\': { # \'Level\': \'DEBUG\', # \'FilePath\': \'/usr/local/bin/logs/app.log\' # } # } # Add custom configuration and retrieve values with correct types add_custom_config(\'app_config.ini\') config = read_config(\'app_config.ini\') print(config[\'Features\'][\'Timeout\']) # Expected output: 30 (int) print(config[\'Features\'][\'EnableCache\']) # Expected output: True (bool) print(config[\'Features\'][\'CacheSize\']) # Expected output: 15.75 (float) ``` Implement the required functions to manage the configuration file as described above.","solution":"import configparser import os def create_config(filename: str) -> None: config = configparser.ConfigParser() config[\'DEFAULT\'] = { \'Path\': \'/usr/local/bin\', \'EnableFeatureX\': \'yes\' } config[\'Database\'] = { \'Host\': \'localhost\', \'Port\': \'5432\', \'Username\': \'admin\', \'Password\': \'secret\' } config[\'Logging\'] = { \'Level\': \'DEBUG\', \'FilePath\': \'%(Path)s/logs/app.log\' } with open(filename, \'w\') as configfile: config.write(configfile) def read_config(filename: str) -> dict: config = configparser.ConfigParser() config.read(filename) required_sections = [\'Database\', \'Logging\'] for section in required_sections: if section not in config: raise ValueError(f\\"Missing required section: {section}\\") db_config = { \'Host\': config[\'Database\'][\'Host\'], \'Port\': config.getint(\'Database\', \'Port\'), \'Username\': config[\'Database\'][\'Username\'], \'Password\': config[\'Database\'][\'Password\'] } log_config = { \'Level\': config[\'Logging\'][\'Level\'], \'FilePath\': config[\'Logging\'][\'FilePath\'] } result = { \'Database\': db_config, \'Logging\': log_config } # Check for custom configurations in \'Features\' section if present if \'Features\' in config: features_config = { \'Timeout\': config.getint(\'Features\', \'Timeout\'), \'EnableCache\': config.getboolean(\'Features\', \'EnableCache\'), \'CacheSize\': config.getfloat(\'Features\', \'CacheSize\') } result[\'Features\'] = features_config return result def add_custom_config(filename: str) -> None: config = configparser.ConfigParser() config.read(filename) if \'Features\' not in config: config[\'Features\'] = { \'Timeout\': \'30\', \'EnableCache\': \'true\', \'CacheSize\': \'15.75\' } with open(filename, \'w\') as configfile: config.write(configfile)"},{"question":"**Title: Advanced Pattern Matching and Text Parsing with Regular Expressions in Python** **Objective:** Your task is to implement a Python function, `extract_product_details(text)`, that processes a given text to extract product details using regular expressions. This function should demonstrate your understanding of various regular expression constructs and their applications. **Function Specification:** - **Function name**: `extract_product_details` - **Input**: A single string `text`, containing the product details in a specific format. - **Output**: A list of dictionaries, where each dictionary contains the extracted details of a product with the following keys: `\'Product ID\'`, `\'Product Name\'`, `\'Category\'`, and `\'Price\'`. **Input Format:** The input string `text` contains multiple lines, each describing a product with the following details in this exact format: ``` Product ID: <ID>, Product Name: <Name>, Category: <Category>, Price: <Price> ``` - `<ID>`: A unique identifier consisting of alphanumeric characters (both uppercase and lowercase). - `<Name>`: The name of the product, which may contain spaces. - `<Category>`: The category of the product, which may contain alphanumeric characters and spaces. - `<Price>`: The price of the product, which is a positive number (it can be an integer or a floating-point number). **Output Format:** Return a list of dictionaries, where each dictionary contains the keys `\'Product ID\'`, `\'Product Name\'`, `\'Category\'`, and `\'Price\'`, and their corresponding extracted values from each line in the input string. **Constraints:** - The input string may contain multiple lines, each properly formatted as described. - Ensure your regular expressions are efficient and capable of handling large input strings. **Example:** ```python def extract_product_details(text: str) -> list: pass # Example Input: text = Product ID: A123, Product Name: Widget, Category: Gadgets, Price: 19.99 Product ID: B456, Product Name: Thingamajig, Category: Tools, Price: 5.49 Product ID: C789, Product Name: Doohickey, Category: Widgets, Price: 13.75 # Example Output: [ { \'Product ID\': \'A123\', \'Product Name\': \'Widget\', \'Category\': \'Gadgets\', \'Price\': \'19.99\' }, { \'Product ID\': \'B456\', \'Product Name\': \'Thingamajig\', \'Category\': \'Tools\', \'Price\': \'5.49\' }, { \'Product ID\': \'C789\', \'Product Name\': \'Doohickey\', \'Category\': \'Widgets\', \'Price\': \'13.75\' } ] ``` # Additional Information: - Use the `re` module to construct and apply your regular expressions. - Ensure proper data extraction and type conversion where necessary. - Thoroughly test your implementation with various scenarios to ensure accuracy. **Note**: Besides the function implementation, include a brief explanation of your regular expression patterns and their purposes.","solution":"import re def extract_product_details(text): Extracts product details from the given text using regular expressions. Args: - text (str): Input text containing product details. Returns: - list: A list of dictionaries with product details. # Regular expression for matching product details pattern = re.compile( r\\"Product ID: (?P<Product_ID>[A-Za-z0-9]+), \\" r\\"Product Name: (?P<Product_Name>[^,]+), \\" r\\"Category: (?P<Category>[^,]+), \\" r\\"Price: (?P<Price>d+.d{2})\\" ) # Find all matches in the input text matches = pattern.finditer(text) # Extract details as dictionaries product_details = [ { \\"Product ID\\": match.group(\\"Product_ID\\"), \\"Product Name\\": match.group(\\"Product_Name\\"), \\"Category\\": match.group(\\"Category\\"), \\"Price\\": match.group(\\"Price\\") } for match in matches ] return product_details"},{"question":"Objective Implement a function to perform a series of operations on a pandas DataFrame while adhering to the Copy-on-Write (CoW) constraints to prevent unintended modifications on other objects. Function Signature ```python def manipulate_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input - `df (pd.DataFrame)`: A pandas DataFrame containing two columns, \\"foo\\" (integers) and \\"bar\\" (integers), with at least 3 rows of data. Output - A pandas DataFrame with the following transformations applied: 1. Create a new DataFrame derived from `df` without modifying `df`. 2. In the new DataFrame, update the first row of the \\"foo\\" column to 100. 3. Rename the column \\"bar\\" to \\"baz\\". Constraints 1. Ensure that the original DataFrame `df` is unchanged after the transformations. 2. Use of inplace operations should be avoided to comply with CoW principles. Example ```python import pandas as pd df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) new_df = manipulate_dataframe(df) # Expected output for `new_df` # foo baz # 0 100 4 # 1 2 5 # 2 3 6 # The original `df` should remain unchanged: # foo bar # 0 1 4 # 1 2 5 # 2 3 6 ``` Notes - Pay close attention to ensure no chained assignments occur and CoW principles are respected. - Verify the original DataFrame remains unchanged before and after calling the function.","solution":"import pandas as pd def manipulate_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Create a copy of the DataFrame to ensure the original is not modified new_df = df.copy() # Update the first row of the \\"foo\\" column to 100 new_df.loc[0, \'foo\'] = 100 # Rename the column \\"bar\\" to \\"baz\\" new_df = new_df.rename(columns={\'bar\': \'baz\'}) return new_df"},{"question":"**HTTP Client with Detailed Request Handling** # Objective: Implement a function that performs an HTTP request and processes the response using detailed functionalities provided by the `http.client` module. # Problem Statement: Write a function `http_request_handler` that takes the following parameters: - `method`: An HTTP method like \\"GET\\", \\"POST\\", \\"PUT\\", \\"HEAD\\". - `url`: The complete URL, including the hostname and optionally a port number (e.g., \\"http://example.com:8080/resource\\"). - `params`: (optional) A dictionary of parameters to send with the request (for \\"POST\\" and \\"PUT\\" methods). - `headers`: (optional) A dictionary of HTTP headers to include in the request. - `timeout`: (optional) Timeout for the request in seconds. - `chunk_size`: (optional) Number of bytes to read at a time from the response. Your function should: 1. Parse the URL to extract the hostname, port, and path. 2. Establish a connection to the server using `HTTPConnection` or `HTTPSConnection` based on the URL scheme. 3. If `params` is provided, encode them appropriately for sending in the request body. 4. Send the HTTP request with the specified `method`, `path`, `body` (if applicable), and `headers`. 5. Read the response status and headers. 6. Read the response body in chunks of size `chunk_size` if provided or read the whole response otherwise. 7. Print the status code, reason, headers, and response body. # Constraints: - URLs will be valid HTTP/HTTPS URLs. - Only basic authentication through headers is required (provide `Authorization` header if necessary). - Handle common exceptions such as connection errors and invalid URLs. # Function Signature: ```python def http_request_handler(method: str, url: str, params: dict = None, headers: dict = None, timeout: int = None, chunk_size: int = None) -> None: pass ``` # Example Usage: ```python # Example 1: Performing a GET request http_request_handler(\\"GET\\", \\"https://www.python.org\\") # Example 2: Performing a POST request with parameters and headers params = {\'key1\': \'value1\', \'key2\': \'value2\'} headers = {\'Content-Type\': \'application/x-www-form-urlencoded\'} http_request_handler(\\"POST\\", \\"https://httpbin.org/post\\", params=params, headers=headers) # Example 3: Performing a custom request with timeout and chunked response reading http_request_handler(\\"GET\\", \\"https://www.python.org\\", timeout=5, chunk_size=1024) ``` # Notes: - Use `urllib.parse.urlparse` for parsing the URL to obtain hostname, port, and path. - Use appropriate methods from the `http.client` module to handle different parts of the request and response lifecycle. - Ensure the function prints the required output in a readable format. # Required Outputs: - Print the status code and reason (e.g., `200 OK`). - Print all response headers. - Print the response body, either in chunks or as a whole depending on `chunk_size`. Create a well-tested implementation that applies these concepts effectively and demonstrates handling HTTP operations in Python.","solution":"import http.client import urllib.parse def http_request_handler(method: str, url: str, params: dict = None, headers: dict = None, timeout: int = None, chunk_size: int = None) -> None: # Parse the url to extract hostname, port, path parsed_url = urllib.parse.urlparse(url) scheme = parsed_url.scheme hostname = parsed_url.hostname port = parsed_url.port path = parsed_url.path or \\"/\\" if parsed_url.query: path += \\"?\\" + parsed_url.query # Determine the connection type (HTTP or HTTPS) if scheme == \\"https\\": connection = http.client.HTTPSConnection(hostname, port, timeout=timeout) else: connection = http.client.HTTPConnection(hostname, port, timeout=timeout) # Encode parameters if provided and applicable body = None if params and method in [\\"POST\\", \\"PUT\\"]: body = urllib.parse.urlencode(params) # Send the HTTP request connection.request(method, path, body, headers or {}) # Process the response response = connection.getresponse() # Print status code and reason print(f\\"{response.status} {response.reason}\\") # Print all response headers for header, value in response.getheaders(): print(f\\"{header}: {value}\\") # Read and print the response body if chunk_size: while True: chunk = response.read(chunk_size) if not chunk: break print(chunk.decode()) else: print(response.read().decode()) connection.close()"},{"question":"**Programming Challenge: Implement a Custom Transfer-Encoding Handler** # Objective: You are required to implement a custom transfer-encoding handler that utilizes the `quopri` module to encode and decode MIME quoted-printable data. # Instructions: 1. Implement two functions: - `custom_encode(input_path: str, output_path: str, quotetabs: bool, header: bool) -> None` - `custom_decode(input_path: str, output_path: str, header: bool) -> None` 2. The `custom_encode` function should: - Read the contents of the file at `input_path` as binary. - Encode the contents using quoted-printable encoding. - Write the encoded contents to the binary file at `output_path`. - The function should take into account `quotetabs` and `header` arguments to control encoding behavior. 3. The `custom_decode` function should: - Read the contents of the file at `input_path` as binary. - Decode the contents using quoted-printable decoding. - Write the decoded contents to the binary file at `output_path`. - The function should take into account the `header` argument to control decoding behavior. # Constraints: - The input and output files will always be valid binary files. - You must use the `quopri` module for encoding and decoding operations. - You need to handle exceptions that may arise from file operations (e.g., file not found, permission errors). # Example Usage: Consider the following example to illustrate the usage: ```python # Encoding a file custom_encode(\'input.txt\', \'encoded_output.txt\', True, False) # Decoding the file back custom_decode(\'encoded_output.txt\', \'decoded_output.txt\', False) ``` **Expected Outcomes**: - The `encoded_output.txt` should contain the quoted-printable encoded version of `input.txt`. - The `decoded_output.txt` should be the same as `input.txt` after the decoding process is complete. # Evaluation: Your implementation will be assessed based on: - Correctness of encoding and decoding operations. - Handling of different edge cases (e.g., files with special characters, files with nonprintable characters). - Proper usage of the `quopri` module functions. - Robust error handling.","solution":"import quopri def custom_encode(input_path: str, output_path: str, quotetabs: bool, header: bool) -> None: try: with open(input_path, \'rb\') as input_file: input_data = input_file.read() encoded_data = quopri.encodestring(input_data, quotetabs=quotetabs, header=header) with open(output_path, \'wb\') as output_file: output_file.write(encoded_data) except Exception as e: print(f\\"Error: {e}\\") def custom_decode(input_path: str, output_path: str, header: bool) -> None: try: with open(input_path, \'rb\') as input_file: input_data = input_file.read() decoded_data = quopri.decodestring(input_data, header=header) with open(output_path, \'wb\') as output_file: output_file.write(decoded_data) except Exception as e: print(f\\"Error: {e}\\")"},{"question":"Exception Management and Resource Cleanup Objective: Create a Python function that reads from a file, processes the content, handles various exceptions, and ensures proper resource management. Problem Statement: Write a function, `process_file(filepath: str) -> None`, that: - Reads a file specified by `filepath` - Parses each line as an integer - Calculates the cumulative sum of the parsed integers - Prints the cumulative sum However, you need to handle the following exceptions elegantly: 1. **FileNotFoundError**: If the file does not exist, print a user-friendly error message. 2. **ValueError**: If a line in the file cannot be parsed as an integer, print a user-friendly error message and skip that line. 3. **KeyboardInterrupt**: If the user interrupts the execution (e.g., presses `Ctrl+C`), print a confirmation message that the process was interrupted. 4. **Any other exception**: Log the exception details for debugging. Additionally, ensure that the file is always closed after processing, regardless of whether an operation was successful or an exception was raised. Input: - A string `filepath` representing the path to the file containing the integers. Output: - Print statements as specified in the exception handling and cumulative sum. Constraints: - Assume the file contains one integer per line. - Assume the max cumulative sum can fit within the range of a regular `int` in Python. Example: Suppose the file at `filepath` contains the following lines: ``` 10 20 thirty 40 ``` The function call `process_file(\\"path_to_file\\")` should: 1. Successfully parse `10` and add to the cumulative sum. 2. Successfully parse `20` and add to the cumulative sum. 3. Encounter a `ValueError` on line \\"thirty\\", print an error message, and skip this line. 4. Successfully parse `40` and add to the cumulative sum. 5. Print the cumulative sum: `70`. Template: ```python def process_file(filepath: str) -> None: try: with open(filepath, \'r\') as file: cumulative_sum = 0 for line in file: try: num = int(line.strip()) cumulative_sum += num except ValueError: print(f\\"Could not parse line: {line.strip()}. Skipping...\\") print(f\\"The cumulative sum is: {cumulative_sum}\\") except FileNotFoundError: print(f\\"File not found: {filepath}\\") except KeyboardInterrupt: print(\\"Process was interrupted by the user.\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") finally: print(\\"Finished processing the file.\\") ```","solution":"def process_file(filepath: str) -> None: try: with open(filepath, \'r\') as file: cumulative_sum = 0 for line in file: try: num = int(line.strip()) cumulative_sum += num except ValueError: print(f\\"Could not parse line: \'{line.strip()}\'. Skipping...\\") print(f\\"The cumulative sum is: {cumulative_sum}\\") except FileNotFoundError: print(f\\"File not found: {filepath}\\") except KeyboardInterrupt: print(\\"Process was interrupted by the user.\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") finally: print(\\"Finished processing the file.\\")"},{"question":"# Advanced Python Import System Manipulation **Objective:** The task is to demonstrate an understanding of Python\'s import system by dynamically creating, adding, and importing modules within a Python environment. You will write a function that dynamically creates a module with a given name and code, executes the code in the context of the module, and then imports the module for use in the Python environment. **Task:** 1. Implement a function `dynamic_import(module_name: str, code: str) -> None`. This function should: - Create a new module with the name `module_name`. - Execute the provided `code` within the context of the new module. - Add the newly created module to the current Python environment so it can be imported and used. 2. Implement another function `use_dynamic_module(module_name: str) -> Any`. This function should: - Import the module by the given name. - Call a function named `calculate` from the imported module. - Return the result of the `calculate` function. **Requirements:** - The `module_name` should be a valid Python module name. - The `code` should define a function named `calculate` which takes no arguments and returns a value. - Handle any potential errors, including but not limited to invalid module names or code execution errors. - Ensure that the newly created module can be imported globally within the current Python interpreter session. **Example Usage:** ```python # Example code string code_str = \'\'\' def calculate(): return 42 \'\'\' # Creating and importing the module dynamic_import(\'mymodule\', code_str) # Using the dynamically imported module result = use_dynamic_module(\'mymodule\') print(result) # Output should be 42 ``` **Note:** - You may use functions from the `importlib` module and standard Python functionalities for module handling. - Avoid the use of actual `PyImport_*` C-API functions since they are for a lower-level language interfacing and not required for a pure Python solution. **Constraints:** - The module creation and import should work seamlessly within the same Python interpreter session. - The solution should not rely on writing any files to disk.","solution":"import sys import types def dynamic_import(module_name: str, code: str) -> None: Creates a new module with the given name, executes the provided code in the context of the module, and adds the module to the current Python environment for importing. if not module_name.isidentifier(): raise ValueError(f\\"Invalid module name: \'{module_name}\'\\") new_module = types.ModuleType(module_name) exec(code, new_module.__dict__) sys.modules[module_name] = new_module def use_dynamic_module(module_name: str) -> any: Imports the module by the given name, calls a function named `calculate` from the imported module, and returns the result of the `calculate` function. if module_name not in sys.modules: raise ImportError(f\\"Module \'{module_name}\' is not available for importing.\\") module = sys.modules[module_name] if not hasattr(module, \'calculate\'): raise AttributeError(f\\"Module \'{module_name}\' does not have a \'calculate\' function.\\") calculate = getattr(module, \'calculate\') return calculate()"},{"question":"Advanced Point Plot Customization with Seaborn Objective: You are required to create complex visualizations using the seaborn package, specifically focusing on mastering the `sns.pointplot` function. This will test your understanding of data grouping, visual differentiation, error bars, and plot customization. Task: Using the Seaborn package and the provided `penguins` dataset, create a point plot that satisfies the following conditions: 1. **Group and Aggregate:** Plot the average body mass (`body_mass_g`) of penguins grouped by species (`species`). 2. **Hue Differentiation and Customization:** Differentiate the data points by the island (`island`), using distinct markers and linestyles for each island. 3. **Error Bars:** Include error bars that represent the standard deviation of each group. 4. **Formatting:** Customize the appearance with the following: - Use different colors for each species. - Set the marker size to 10 and adjust linestyles as needed. - Use the \\"o\\", \\"s\\", and \\"^\\" markers for different islands. - Use solid, dashed, and dotted linestyles. - Title the plot as \\"Average Body Mass by Species and Island\\". - Label the x-axis as \\"Species\\" and the y-axis as \\"Body Mass (g)\\". 5. **Dodge:** Ensure that points for different islands are dodged to prevent overplotting. Input: - You will use the `penguins` dataset from the seaborn package. Output: - A Seaborn point plot satisfying all the specified conditions. Constraints: - Ensure the code is efficient and leverages seaborn\'s functionalities effectively. - The visualization must be clear and accurately represent the dataset. Example Dataset: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the point plot with the specified conditions sns.pointplot(data=penguins, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"island\\", markers=[\\"o\\", \\"s\\", \\"^\\"], linestyles=[\\"-\\", \\"--\\", \\"-.\\"], dodge=True, join=False, palette=\\"deep\\", capsize=.2) # Customize the plot plt.title(\\"Average Body Mass by Species and Island\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Body Mass (g)\\") # Show the plot plt.show() ``` Please complete and test the above code to ensure the plot meets all the conditions specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the point plot with the specified conditions sns.pointplot( data=penguins, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"island\\", markers=[\\"o\\", \\"s\\", \\"^\\"], linestyles=[\\"-\\", \\"--\\", \\"-.\\"], dodge=True, join=False, palette=\\"deep\\", ci=\\"sd\\", # standard deviation scale=1.5 # Adjust marker size, equivalent to marker size = 10 ) # Customize the plot plt.title(\\"Average Body Mass by Species and Island\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Body Mass (g)\\") # Show the plot plt.show()"},{"question":"**Synthetic Dataset Generation and Model Application** # Objective: Create a synthetic dataset using scikit-learn\'s dataset generators for a classification task, and then apply a machine learning model to classify the generated data. Visualize and evaluate the model\'s performance. # Requirements: 1. Generate a synthetic dataset combining elements from `make_classification`, `make_blobs`, and `make_gaussian_quantiles`. 2. Split the dataset into training and testing sets. 3. Train a machine learning model (e.g., SVM, Random Forest, Logistic Regression). 4. Evaluate the model\'s performance using appropriate metrics. 5. Visualize the dataset as well as classification boundaries. # Detailed Instructions: 1. **Dataset Generation**: - Use `make_classification` to create a dataset with the following specifications: 1000 samples, 2 informative features, 2 redundant features, and 2 clusters per class. - Use `make_blobs` to create a dataset with 3 clusters. - Combine these datasets into a single dataset. - Augment the dataset with additional classes using `make_gaussian_quantiles` resulting in 3 classes. 2. **Data Preparation**: - Concatenate the above datasets to form a single dataset with appropriate labels. - Split the combined dataset into a training set (70%) and a testing set (30%). 3. **Model Training and Evaluation**: - Choose a machine learning model to classify the data (e.g., Support Vector Machine, Random Forest, or Logistic Regression). - Train the model on the training set. - Evaluate the model using metrics such as accuracy, precision, recall, and F1-score. 4. **Visualization**: - Visualize the generated dataset in a 2D plot with different colors for different classes. - Plot the decision boundaries of the chosen model. # Constraints: - Use `random_state = 42` for generating datasets to ensure reproducibility. - Execute all codes in a single Jupyter Notebook cell. # Expected Input and Output Formats: **Function signature**: ```python def synthetic_classification_model(): pass ``` **Expected Output**: - Plots showing the synthetic dataset and the classification boundaries. - Printed evaluation metrics: accuracy, precision, recall, and F1-score. **Example Output**: ``` Accuracy: 0.90 Precision: 0.89 Recall: 0.88 F1-Score: 0.88 ``` **Note**: This task evaluates your ability to use multiple dataset generators, prepare data, train a model, evaluate its performance, and visualize results.","solution":"import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import make_classification, make_blobs, make_gaussian_quantiles from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.decomposition import PCA def synthetic_classification_model(): # Generate datasets X1, y1 = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=2, n_clusters_per_class=2, random_state=42) X2, y2 = make_blobs(n_samples=300, centers=3, n_features=4, random_state=42) y2 = y2 + 2 # offset labels to distinguish from classification labels X3, y3 = make_gaussian_quantiles(n_samples=300, n_features=4, n_classes=3, random_state=42) y3 = y3 + 5 # offset labels to distinguish from other datasets # Combine datasets X = np.vstack((X1, X2, X3)) y = np.hstack((y1, y2, y3)) # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train model model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) # Predict on test set y_pred = model.predict(X_test) # Evaluate model accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') print(f\\"Accuracy: {accuracy:.2f}\\") print(f\\"Precision: {precision:.2f}\\") print(f\\"Recall: {recall:.2f}\\") print(f\\"F1-Score: {f1:.2f}\\") # Visualize dataset and decision boundaries (using PCA for 2D projection) pca = PCA(n_components=2) X_vis = pca.fit_transform(X) plt.figure(figsize=(10, 6)) scatter = plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y, cmap=\'viridis\', alpha=0.5) plt.legend(*scatter.legend_elements(), title=\\"Classes\\") plt.title(\\"Synthetic Dataset Visualization\\") plt.show() # Visualize decision boundaries X_train_vis = pca.transform(X_train) X_test_vis = pca.transform(X_test) # Create a mesh to plot the decision boundaries x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1 y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) # Perform classification on the grid using the projected model Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()])) Z_vis = pca.transform(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()])) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.5, cmap=\'viridis\') plt.scatter(X_train_vis[:, 0], X_train_vis[:, 1], c=y_train, edgecolor=\'k\', marker=\'o\', alpha=0.7) plt.scatter(X_test_vis[:, 0], X_test_vis[:, 1], c=y_test, edgecolor=\'k\', marker=\'s\', alpha=1) plt.title(\\"Decision Boundaries and Test/Train Data\\") plt.show() # Call the function synthetic_classification_model()"},{"question":"# Seaborn Scatter Plot with Jitter Customization You are given the Palmer Penguins dataset, which has been previously loaded as `penguins`. Your task is to create a function that generates specific visualizations using Seaborn\'s `objects` interface. This function should demonstrate an understanding of how to add and customize jitter in scatter plots. Function Signature ```python def plot_penguins_with_jitter(penguins): pass ``` Inputs - `penguins`: A pandas DataFrame containing the Palmer Penguins dataset. Requirements 1. **Create a scatter plot** showing the species on the x-axis and body mass (in grams) on the y-axis. Apply a small amount of default jitter. 2. **Create another scatter plot** showing body mass (in grams) on the x-axis and species on the y-axis, again with a small amount of default jitter. 3. **Create a scatter plot** where body mass is rounded to the nearest thousand on the x-axis, flipper length (in mm) is on the y-axis, and apply jitter with width relative to the spacing between marks. 4. **Create a scatter plot** where body mass is rounded to the nearest thousand on the x-axis, flipper length (in mm) is rounded to the nearest ten on the y-axis, and apply jitter with x-axis and y-axis jitters of 200 and 5 data units respectively. Output - Your function does not need to return any value but should generate four scatter plots as described above, displaying them inline. Additional Information - Use Seaborn\'s objects interface (`so.Plot`) and `so.Jitter` to add jitter to the plots. - Ensure your plots are visually distinguishable with appropriate axis labels. Here is a structure to get you started: ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguins_with_jitter(penguins): # Plot 1: Species vs. Body Mass with default jitter ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .show() ) # Plot 2: Body Mass vs. Species with default jitter ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\") .add(so.Dots(), so.Jitter()) .show() ) # Plot 3: Rounded Body Mass vs. Flipper Length with relative jitter ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"]) .add(so.Dots(), so.Jitter()) .show() ) # Plot 4: Rounded Body Mass vs. Rounded Flipper Length with specific x and y jitters ( so.Plot( penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1) ) .add(so.Dots(), so.Jitter(x=200, y=5)) .show() ) ```","solution":"import seaborn.objects as so import matplotlib.pyplot as plt def plot_penguins_with_jitter(penguins): # Plot 1: Species vs Body Mass with default jitter ( so.Plot(penguins, x=\'species\', y=\'body_mass_g\') .add(so.Dots(), so.Jitter()) .show() ) # Plot 2: Body Mass vs Species with default jitter ( so.Plot(penguins, x=\'body_mass_g\', y=\'species\') .add(so.Dots(), so.Jitter()) .show() ) # Plot 3: Rounded Body Mass vs Flipper Length with relative width jitter penguins[\'body_mass_g_rounded\'] = penguins[\'body_mass_g\'].round(-3) ( so.Plot(penguins, x=\'body_mass_g_rounded\', y=\'flipper_length_mm\') .add(so.Dots(), so.Jitter(width=.1)) .show() ) # Plot 4: Rounded Body Mass vs Rounded Flipper Length with specific jitter penguins[\'flipper_length_mm_rounded\'] = penguins[\'flipper_length_mm\'].round(-1) ( so.Plot(penguins, x=\'body_mass_g_rounded\', y=\'flipper_length_mm_rounded\') .add(so.Dots(), so.Jitter(x=200, y=5)) .show() ) plt.show()"},{"question":"**Title:** Tuning Decision Thresholds with `TunedThresholdClassifierCV` **Problem Statement:** You are tasked with building a binary classification model to predict whether patients have a particular disease. Given the critical nature of not missing any positive cases (those with the disease), you need to prioritize high recall for your predictions. You will: 1. Train a logistic regression classifier on a provided dataset. 2. Use the `TunedThresholdClassifierCV` from scikit-learn to optimize the decision threshold for the classifier, aiming to maximize recall. 3. Evaluate the performance of the tuned classifier by comparing it to the default decision threshold (0.5). **Dataset:** You are provided a dataset with features `X` (a 2D numpy array) and labels `y` (a 1D numpy array where `1` indicates the presence of the disease, and `0` indicates absence). **Function Signature:** ```python def tune_decision_threshold(X: np.ndarray, y: np.ndarray) -> dict: Trains a logistic regression model and tunes the decision threshold to maximize recall. Parameters: X (np.ndarray): Features of shape (n_samples, n_features). y (np.ndarray): Labels of shape (n_samples,). Returns: dict: A dictionary containing: - \'tuned_threshold\': The optimal threshold found. - \'default_recall\': Recall of the model using the default threshold. - \'tuned_recall\': Recall of the model using the tuned threshold. - \'default_precision\': Precision of the model using the default threshold. - \'tuned_precision\': Precision of the model using the tuned threshold. pass ``` **Constraints:** - Use a logistic regression model from scikit-learn. - Use 5-fold cross-validation for threshold tuning. - Optimize the threshold to maximize the recall score. **Example Usage:** ```python from sklearn.datasets import make_classification import numpy as np X, y = make_classification( n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42 ) result = tune_decision_threshold(X, y) print(result) # Output: { # \'tuned_threshold\': 0.3, # Example value # \'default_recall\': 0.8, # Example value # \'tuned_recall\': 0.95, # Example value # \'default_precision\': 0.7,# Example value # \'tuned_precision\': 0.6 # Example value # } ``` **Notes:** - Ensure you shuffle the data before splitting it into folds for cross-validation. - You may use `sklearn.metrics` to calculate recall and precision. - Remember that while maximizing recall, the precision might drop, so provide both metrics for comparison.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict, StratifiedKFold from sklearn.metrics import recall_score, precision_score, precision_recall_curve def tune_decision_threshold(X: np.ndarray, y: np.ndarray) -> dict: Trains a logistic regression model and tunes the decision threshold to maximize recall. Parameters: X (np.ndarray): Features of shape (n_samples, n_features). y (np.ndarray): Labels of shape (n_samples,). Returns: dict: A dictionary containing: - \'tuned_threshold\': The optimal threshold found. - \'default_recall\': Recall of the model using the default threshold. - \'tuned_recall\': Recall of the model using the tuned threshold. - \'default_precision\': Precision of the model using the default threshold. - \'tuned_precision\': Precision of the model using the tuned threshold. model = LogisticRegression() # Using StratifiedKFold for cross-validation skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) prediction_probs = cross_val_predict(model, X, y, cv=skf, method=\'predict_proba\')[:, 1] # Get the precision-recall values on cross-validated predictions precisions, recalls, thresholds = precision_recall_curve(y, prediction_probs) # Find the threshold that gives us the best recall tuned_threshold = thresholds[np.argmax(recalls)] # Train final model on the whole dataset model.fit(X, y) # Get predictions using the default threshold default_predictions = model.predict(X) default_recall = recall_score(y, default_predictions) default_precision = precision_score(y, default_predictions) # Get predictions using the tuned threshold tuned_predictions = (model.predict_proba(X)[:, 1] >= tuned_threshold).astype(int) tuned_recall = recall_score(y, tuned_predictions) tuned_precision = precision_score(y, tuned_predictions) return { \'tuned_threshold\': tuned_threshold, \'default_recall\': default_recall, \'tuned_recall\': tuned_recall, \'default_precision\': default_precision, \'tuned_precision\': tuned_precision }"},{"question":"# Custom Cell Object Implementation in Python Objective Implement a custom class in Python that mimics the behavior of the \\"Cell\\" objects described in the documentation. This class should allow creating a new cell, getting the current value stored in the cell, and setting a new value for the cell. Requirements 1. **Class Definition** - Define a class named `CustomCell`. - The class should contain an initializer that takes an initial value (that could be `None`) and stores it in an instance variable. 2. **Methods to Implement** - `__init__(self, value: Any)`: - Initialize the cell with the given `value`. - `get(self) -> Any`: - Return the current value stored in the cell. - `set(self, value: Any) -> None`: - Set a new value in the cell. - `is_cell(self, obj: Any) -> bool`: - Return `True` if the given `obj` is an instance of `CustomCell`, otherwise `False`. Constraints - Do not use any external libraries for creating the custom cell objects. - Ensure that your class handles storing and retrieving values correctly. - Validate if an object is a `CustomCell` type correctly. Input and Output - **Input:** There is no direct input to the class. You will be implementing methods that manipulate the cell object. - **Output:** The class methods should function as described in the requirements. Performance Requirements - The methods should perform the required operations efficiently without unnecessary complexity. Example: ```python # Create a cell object with initial value 10 cell = CustomCell(10) print(cell.get()) # Output: 10 # Set a new value in the cell cell.set(20) print(cell.get()) # Output: 20 # Check if the object is a CustomCell instance print(cell.is_cell(cell)) # Output: True # Check if a regular integer is a CustomCell instance print(cell.is_cell(10)) # Output: False ```","solution":"class CustomCell: def __init__(self, value=None): Initialize the cell with the given value. self._value = value def get(self): Return the current value stored in the cell. return self._value def set(self, value): Set a new value in the cell. self._value = value def is_cell(self, obj): Return True if the given obj is an instance of CustomCell, otherwise False. return isinstance(obj, CustomCell)"},{"question":"# Question You are given a large dataset that needs to be processed and stored efficiently. To achieve this, you will implement a function that reads data from a given text file, compresses it using the bzip2 algorithm, and writes the compressed data to a new file. Additionally, you will implement a function to decompress the data from the compressed file and verify its integrity against the original data. Requirements 1. **Compression Function**: Implement a function `compress_file(input_file: str, compressed_file: str, compresslevel: int = 9) -> None` that: - Reads the content from `input_file`. - Compresses the data using the `bz2` module with the specified `compresslevel` (default to 9). - Writes the compressed data to `compressed_file`. 2. **Decompression Function**: Implement a function `decompress_file(compressed_file: str, output_file: str) -> None` that: - Reads the compressed data from `compressed_file`. - Decompresses the data using the `bz2` module. - Writes the decompressed data to `output_file`. 3. **Verification Function**: Implement a function `verify_compression(input_file: str, output_file: str) -> bool` that: - Compares the contents of `input_file` and `output_file`. - Returns `True` if the contents are identical, `False` otherwise. Constraints - Assume that `input_file` and `output_file` are valid paths to readable and writable files respectively. - You may use built-in file reading/writing methods. Example Usage ```python input_file = \\"data.txt\\" compressed_file = \\"data.bz2\\" output_file = \\"data_decompressed.txt\\" compress_file(input_file, compressed_file) decompress_file(compressed_file, output_file) is_identical = verify_compression(input_file, output_file) print(is_identical) # Output: True if the decompression is successful and data is identical ``` Notes - Ensure your implementation is efficient and handles large files gracefully. - Include appropriate error handling to manage file read/write issues.","solution":"import bz2 def compress_file(input_file: str, compressed_file: str, compresslevel: int = 9) -> None: Compress the content of input_file and write it to compressed_file. :param input_file: Path to the input file. :param compressed_file: Path to the compressed output file. :param compresslevel: Compression level (default is 9). try: with open(input_file, \'rb\') as infile: data = infile.read() compressed_data = bz2.compress(data, compresslevel=compresslevel) with open(compressed_file, \'wb\') as outfile: outfile.write(compressed_data) except Exception as e: raise IOError(f\\"Error during file compression: {e}\\") def decompress_file(compressed_file: str, output_file: str) -> None: Decompress the content of compressed_file and write it to output_file. :param compressed_file: Path to the compressed file. :param output_file: Path to the decompressed output file. try: with open(compressed_file, \'rb\') as infile: compressed_data = infile.read() data = bz2.decompress(compressed_data) with open(output_file, \'wb\') as outfile: outfile.write(data) except Exception as e: raise IOError(f\\"Error during file decompression: {e}\\") def verify_compression(input_file: str, output_file: str) -> bool: Verify the contents of input_file and output_file are identical. :param input_file: Path to the original input file. :param output_file: Path to the decompressed output file. :return: True if files are identical, False otherwise. try: with open(input_file, \'rb\') as infile: original_data = infile.read() with open(output_file, \'rb\') as outfile: decompressed_data = outfile.read() return original_data == decompressed_data except Exception as e: raise IOError(f\\"Error during file verification: {e}\\")"},{"question":"**Title:** Advanced DataFrame Styling with pandas **Objective:** Write a function `style_dataframe` that takes a pandas DataFrame and applies a series of styling transformations and exports the result to an HTML string. **Input:** ```python def style_dataframe(df: pd.DataFrame) -> str: pass ``` - `df`: A pandas DataFrame with at least the following columns: `[\'A\', \'B\', \'C\']` containing numeric data. **Output:** - Returns a string containing the HTML representation of the styled DataFrame. **Task Description:** 1. Apply a background gradient color on column \'A\' using the `Styler.background_gradient` method. 2. Highlight the maximum value in each column using the `Styler.highlight_max` method. 3. Set the column headers to have bold and centered text. 4. Hide the index column. 5. Set a caption for the DataFrame: \\"Styled DataFrame Example\\". 6. Export the styled DataFrame to an HTML string and return it. Constraints: - You are not allowed to use any external styling libraries. - The function should handle any numeric values within the DataFrame. - Assume the DataFrame will always contain numeric values in the specified columns. Example: ```python import pandas as pd data = { \'A\': [1, 2, 3], \'B\': [4, 5, 6], \'C\': [7, 8, 9] } df = pd.DataFrame(data) styled_html = style_dataframe(df) print(styled_html) ``` Expected output (HTML string): ```html <style type=\\"text/css\\"> #T_28b55_row0_col0, #T_28b55_row1_col0, #T_28b55_row2_col0 { background-color: #aae6fe; } #T_28b55_row2_col2 { background-color: yellow; } #T_28b55 thead tr th { text-align: center; font-weight: bold; } </style> <table id=\\"T_28b55_\\"> <caption>Styled DataFrame Example</caption> <thead> <tr> <th class=\\"col_heading level0 col0\\">A</th> <th class=\\"col_heading level0 col1\\">B</th> <th class=\\"col_heading level0 col2\\">C</th> </tr> </thead> <tbody> <tr> <td class=\\"data row0 col0\\">1</td> <td class=\\"data row0 col1\\">4</td> <td class=\\"data row0 col2\\">7</td> </tr> <tr> <td class=\\"data row1 col0\\">2</td> <td class=\\"data row1 col1\\">5</td> <td class=\\"data row1 col2\\">8</td> </tr> <tr> <td class=\\"data row2 col0\\">3</td> <td class=\\"data row2 col1\\">6</td> <td class=\\"data row2 col2\\" style=\\"background-color: yellow;\\">9</td> </tr> </tbody> </table> ``` **Notes:** - Your output might differ slightly in terms of the exact HTML structure and styling details. - Ensure that all styling requirements are met. Hint: - Use chain calls for multiple `Styler` methods. - Refer to the `Styler` documentation for syntax and examples. **Evaluation:** - Correctness of styling applications (gradient, highlight, etc.). - Proper export to HTML format. - Clean and understandable code.","solution":"import pandas as pd def style_dataframe(df: pd.DataFrame) -> str: styled_df = (df.style .background_gradient(subset=[\'A\'], cmap=\'Blues\') .highlight_max() .set_properties(**{\'font-weight\': \'bold\', \'text-align\': \'center\'}, subset=pd.IndexSlice[:, :]) .hide(axis=\'index\') .set_caption(\\"Styled DataFrame Example\\")) return styled_df.to_html()"},{"question":"You are required to create a Python script that manages a log directory. The script should perform the following tasks: 1. **Scan the log directory** for all log files with the extension `.log`. 2. **Compress** each `.log` file using zlib and save the compressed file with the extension `.log.gz`. 3. **Move** the compressed files to a new directory named `compressed_logs`. 4. **Provide statistics** about the size reduction of each compressed file (original size vs. compressed size). # Requirements - The script should accept the log directory path as a command line argument using `argparse`. - Ensure that the `compressed_logs` directory is created if it does not exist. - For each file, print out the original size, compressed size, and percentage reduction in size. - Use the `os` and `shutil` modules for file handling and directory management. # Input Format The script should be run with the following command: ```bash python manage_logs.py --log_dir <path_to_log_directory> ``` # Output Format For each `.log` file: ``` Compressed <logfile.log>: Original size = X bytes Compressed size = Y bytes Reduction = Z% ``` # Constraints - The log files can be of any size, but the script should handle them efficiently. - Assume that the log directory path provided will always be valid. - Use only the standard library modules mentioned in the provided documentation. # Example Suppose you have a log directory `/var/logs` with files `error.log` and `access.log`. Running the script: ```bash python manage_logs.py --log_dir /var/logs ``` Should output something like: ``` Compressed error.log: Original size = 2048 bytes Compressed size = 512 bytes Reduction = 75% Compressed access.log: Original size = 4096 bytes Compressed size = 1024 bytes Reduction = 75% ``` # Implementation Implement the function `manage_logs(log_dir: str) -> None` to perform the described tasks.","solution":"import os import zlib import argparse import shutil def manage_logs(log_dir: str) -> None: if not os.path.isdir(log_dir): raise NotADirectoryError(f\\"The specified path {log_dir} is not a directory.\\") compressed_logs_dir = os.path.join(log_dir, \\"compressed_logs\\") if not os.path.exists(compressed_logs_dir): os.makedirs(compressed_logs_dir) for log_file in os.listdir(log_dir): if log_file.endswith(\'.log\'): log_file_path = os.path.join(log_dir, log_file) compress_file_path = os.path.join(compressed_logs_dir, f\\"{log_file}.gz\\") # Read the log file with open(log_file_path, \'rb\') as file: data = file.read() original_size = len(data) # Compress the log file compressed_data = zlib.compress(data) compressed_size = len(compressed_data) # Write the compressed file with open(compress_file_path, \'wb\') as compressed_file: compressed_file.write(compressed_data) # Print statistics reduction = (1 - compressed_size / original_size) * 100 print(f\\"Compressed {log_file}:\\") print(f\\" Original size = {original_size} bytes\\") print(f\\" Compressed size = {compressed_size} bytes\\") print(f\\" Reduction = {reduction:.2f}%\\") if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\'Manage log directory: compress and move log files.\') parser.add_argument(\'--log_dir\', type=str, required=True, help=\'Path to the log directory to be managed.\') args = parser.parse_args() manage_logs(args.log_dir)"},{"question":"**Question: Implementing and Evaluating a Model Using Different Cross-Validation Techniques** *Objective*: Write a Python function that demonstrates the use of various cross-validation techniques in scikit-learn to evaluate the performance of a machine learning model. *Problem Statement*: You are given the well-known iris dataset—a dataset that includes 150 samples, each with four features and a label indicating the species of iris (three possible species). Your task is to implement a function `evaluate_model` that trains a linear SVM model and evaluates it using different cross-validation techniques. Specifically, you should: 1. **Load the iris dataset** using `datasets.load_iris`. 2. **Define a linear SVM classifier** with a fixed random state for reproducibility. 3. **Split the dataset using different cross-validation techniques** and evaluate the model\'s performance. The cross-validation techniques to be used are: - K-Fold Cross-Validation - Stratified K-Fold Cross-Validation - Leave-One-Out Cross-Validation - ShuffleSplit Cross-Validation - TimeSeriesSplit Cross-Validation *Input and Output Formats*: - *Input*: None - *Output*: A dictionary where each key is a string representing the cross-validation technique used and the value is a list of scores obtained during the cross-validation. *Constraints and Performance Requirements*: - You must use scikit-learn for all machine learning operations. - Use `cv=5` for K-Fold and Stratified K-Fold cross-validation. - Use `n_splits=5` and `test_size=0.3` for ShuffleSplit. - Use `n_splits=3` for TimeSeriesSplit. - Ensure that the model evaluation is reproducible by setting `random_state=42` where applicable. ```python import numpy as np from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import (KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, TimeSeriesSplit, cross_val_score) def evaluate_model(): # Load the iris dataset X, y = datasets.load_iris(return_X_y=True) # Define the SVM classifier clf = SVC(kernel=\'linear\', C=1, random_state=42) # Define cross-validation techniques kf = KFold(n_splits=5) skf = StratifiedKFold(n_splits=5) loo = LeaveOneOut() ss = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42) tscv = TimeSeriesSplit(n_splits=3) # Initialize a dictionary to store the scores scores_dict = {} # K-Fold Cross-Validation scores_dict[\'K-Fold\'] = cross_val_score(clf, X, y, cv=kf).tolist() # Stratified K-Fold Cross-Validation scores_dict[\'Stratified K-Fold\'] = cross_val_score(clf, X, y, cv=skf).tolist() # Leave-One-Out Cross-Validation scores_dict[\'Leave-One-Out\'] = cross_val_score(clf, X, y, cv=loo).tolist() # ShuffleSplit Cross-Validation scores_dict[\'ShuffleSplit\'] = cross_val_score(clf, X, y, cv=ss).tolist() # TimeSeriesSplit Cross-Validation scores_dict[\'TimeSeriesSplit\'] = cross_val_score(clf, X, y, cv=tscv).tolist() return scores_dict # Example usage if __name__ == \\"__main__\\": results = evaluate_model() for cv_method, scores in results.items(): print(f\\"{cv_method} scores: {scores}\\") ``` *Expected Output*: A dictionary with keys representing the cross-validation technique used and values representing the list of scores obtained. For example: ```python { \'K-Fold\': [0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 1.0, 0.9333333333333333], \'Stratified K-Fold\': [0.9666666666666667, 1.0, 0.9666666666666667, 0.9666666666666667, 0.9333333333333333], \'Leave-One-Out\': [1.0, 1.0, 0.9666666666666667, ..., 1.0, 1.0], \'ShuffleSplit\': [0.9555555555555556, 0.9777777777777777, 0.9111111111111111, 0.9555555555555556, 0.9555555555555556], \'TimeSeriesSplit\': [0.6, 0.72, 0.86] } ``` *Note*: The actual values may differ slightly due to the randomness of data splitting but should be close to the given example.","solution":"import numpy as np from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import (KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, TimeSeriesSplit, cross_val_score) def evaluate_model(): # Load the iris dataset X, y = datasets.load_iris(return_X_y=True) # Define the SVM classifier clf = SVC(kernel=\'linear\', C=1, random_state=42) # Define cross-validation techniques kf = KFold(n_splits=5) skf = StratifiedKFold(n_splits=5) loo = LeaveOneOut() ss = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42) tscv = TimeSeriesSplit(n_splits=3) # Initialize a dictionary to store the scores scores_dict = {} # K-Fold Cross-Validation scores_dict[\'K-Fold\'] = cross_val_score(clf, X, y, cv=kf).tolist() # Stratified K-Fold Cross-Validation scores_dict[\'Stratified K-Fold\'] = cross_val_score(clf, X, y, cv=skf).tolist() # Leave-One-Out Cross-Validation scores_dict[\'Leave-One-Out\'] = cross_val_score(clf, X, y, cv=loo).tolist() # ShuffleSplit Cross-Validation scores_dict[\'ShuffleSplit\'] = cross_val_score(clf, X, y, cv=ss).tolist() # TimeSeriesSplit Cross-Validation scores_dict[\'TimeSeriesSplit\'] = cross_val_score(clf, X, y, cv=tscv).tolist() return scores_dict # Example usage if __name__ == \\"__main__\\": results = evaluate_model() for cv_method, scores in results.items(): print(f\\"{cv_method} scores: {scores}\\")"},{"question":"**Question: Advanced Diverging Palettes Creation and Application in Seaborn** **Objective:** Design and implement a function that generates a customized diverging palette using Seaborn and applies it to a dataset to visualize the data meaningfully. The function should demonstrate a deep understanding of Seaborn\'s palette generation and customization capabilities. **Function Signature:** ```python def visualize_custom_palette(df, var1, var2, low_color, high_color, center_color=\\"light\\", sep=10, s=100, l=55, as_cmap=False): Generates a customized diverging palette and applies it to visualize the relationship between two variables. Args: df: Pandas DataFrame - The input dataset. var1: str - The name of the first variable (independent). var2: str - The name of the second variable (dependent). low_color: int - The starting hue value for the low-end color of the palette. high_color: int - The ending hue value for the high-end color of the palette. center_color: str - The center color type (\'light\' or \'dark\'). Default is \\"light\\". sep: int - The amount of separation around the center value. Default is 10. s: int - The saturation of the endpoints. Default is 100. l: int - The lightness of the endpoints. Default is 55. as_cmap: bool - Whether to return a continuous colormap (True) or a discrete palette (False). Default is False. Returns: None - The function should render a plot directly. ``` **Requirements:** 1. Read the dataset and ensure it contains the specified variables `var1` and `var2`. 2. Create a diverging palette using `sns.diverging_palette()` with the provided color parameters. 3. Visualize the relationship between `var1` and `var2` using an appropriate Seaborn plot (e.g., `sns.heatmap`, `sns.scatterplot`, or similar). 4. Apply the generated diverging palette to the plot. If `as_cmap` is True, use it as a colormap; otherwise, use it as a palette. 5. Customize the plot with labels, title, etc., to ensure it is informative and visually appealing. **Constraints:** - You will be provided with a Pandas DataFrame `df` containing at least the columns `var1` and `var2`. - You may assume the DataFrame is clean and does not require preprocessing. - The color values (`low_color`, `high_color`) will be valid hue values between 0 and 360. **Example:** ```python import seaborn as sns import pandas as pd # Sample DataFrame data = { \\"feature_1\\": [1, 2, 3, 4, 5], \\"feature_2\\": [3, 1, 4, 2, 5] } df = pd.DataFrame(data) # Example call to the function visualize_custom_palette(df, var1=\'feature_1\', var2=\'feature_2\', low_color=240, high_color=20, center_color=\\"dark\\", sep=20, s=80, l=45, as_cmap=True) ``` **Expected Output:** A plot visualizing the relationship between `feature_1` and `feature_2` with a customized diverging palette applied, reflecting the specified colors and settings.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_custom_palette(df, var1, var2, low_color, high_color, center_color=\\"light\\", sep=10, s=100, l=55, as_cmap=False): Generates a customized diverging palette and applies it to visualize the relationship between two variables. Args: df: Pandas DataFrame - The input dataset. var1: str - The name of the first variable (independent). var2: str - The name of the second variable (dependent). low_color: int - The starting hue value for the low-end color of the palette. high_color: int - The ending hue value for the high-end color of the palette. center_color: str - The center color type (\'light\' or \'dark\'). Default is \\"light\\". sep: int - The amount of separation around the center value. Default is 10. s: int - The saturation of the endpoints. Default is 100. l: int - The lightness of the endpoints. Default is 55. as_cmap: bool - Whether to return a continuous colormap (True) or a discrete palette (False). Default is False. Returns: None - The function should render a plot directly. # Ensure the DataFrame contains the specified variables if var1 not in df.columns or var2 not in df.columns: raise ValueError(f\\"Variables {var1} and {var2} must be present in the DataFrame.\\") # Create the diverging palette palette = sns.diverging_palette(low_color, high_color, sep=sep, s=s, l=l, center=center_color, as_cmap=as_cmap) # Plot the relationship between var1 and var2 if as_cmap: # Use heatmap with colormap for continuous data sns.heatmap(df.pivot_table(index=var1, columns=var2, aggfunc=\'size\', fill_value=0), cmap=palette) else: # Use scatterplot for discrete palette sns.scatterplot(x=var1, y=var2, data=df, palette=palette) # Customize the plot plt.title(f\'Relationship between {var1} and {var2}\') plt.xlabel(var1) plt.ylabel(var2) plt.show()"},{"question":"# Advanced Coding Assessment: Using `urllib` for Complex HTTP Requests **Objective:** Your task is to write a Python function using the `urllib` module to perform a complex HTTP request. This task will test your ability to handle URL fetching, manage custom headers, handle exceptions correctly, and process the response. **Problem Statement:** You need to implement a function named `fetch_url_details` which accepts a URL and a dictionary of parameters as its input. The function should perform the following actions: 1. If the parameters dictionary is provided, encode the parameters and include them in the URL. 2. Create a custom user-agent for the request. 3. Implement exception handling to manage both `URLError` and `HTTPError`. 4. If the request is successful, return a dictionary containing the following details from the response: - The final URL (`geturl` method) - The HTTP status code (`code`) - A dictionary of all response headers - The first 500 characters of the response content **Function Specification:** ```python def fetch_url_details(url: str, params: dict = None) -> dict: Fetches details of a URL with given parameters and custom headers. Args: - url (str): The target URL. - params (dict): Optional dictionary of query parameters to include in the URL. Returns: - dict: A dictionary with keys \\"final_url\\", \\"status_code\\", \\"headers\\", and \\"content\\". pass ``` **Input:** - `url` (str): The URL to be fetched (e.g., \\"http://www.example.com\\"). - `params` (dict): A dictionary containing URL parameters (default is `None`). **Output:** - A dictionary with the following keys: - `final_url` (str): The URL after any redirects. - `status_code` (int): The HTTP status code from the response. - `headers` (dict): A dictionary of response headers. - `content` (str): The first 500 characters of the response content. **Constraints:** 1. Utilize `urllib.parse.urlencode` to handle parameters. 2. Set a custom user-agent header (e.g., \\"MyUserAgent/1.0\\"). 3. Handle exceptions for network errors and HTTP errors gracefully. 4. Ensure the function works with both GET and POST requests (if parameters are provided, use a POST request; otherwise, use a GET request). **Example:** ```python url = \\"http://www.example.com\\" params = {\'name\': \'John Doe\', \'age\': \'30\'} result = fetch_url_details(url, params) print(result) # Expected output (example): # { # \\"final_url\\": \\"http://www.example.com?name=John+Doe&age=30\\", # \\"status_code\\": 200, # \\"headers\\": {\\"Content-Type\\": \\"text/html\\", \\"Content-Length\\": \\"12345\\", ...}, # \\"content\\": \\"<html>... (first 500 chars of the response) ...\\" # } ``` **Note:** Make sure to include the necessary import statements and handle encoding properly. You can assume that the URL provided is reachable and that parameters are always safely formatted strings.","solution":"import urllib.request import urllib.parse import urllib.error def fetch_url_details(url: str, params: dict = None) -> dict: Fetches details of a URL with given parameters and custom headers. Args: - url (str): The target URL. - params (dict): Optional dictionary of query parameters to include in the URL. Returns: - dict: A dictionary with keys \\"final_url\\", \\"status_code\\", \\"headers\\", and \\"content\\". # Handle URL parameters if params: data = urllib.parse.urlencode(params) data = data.encode(\'utf-8\') req = urllib.request.Request(url, data=data) else: req = urllib.request.Request(url) # Set custom user-agent req.add_header(\'User-Agent\', \'MyUserAgent/1.0\') try: with urllib.request.urlopen(req) as response: final_url = response.geturl() status_code = response.getcode() headers = dict(response.getheaders()) content = response.read(500).decode(\'utf-8\', errors=\'ignore\') # Get first 500 chars of the response return { \\"final_url\\": final_url, \\"status_code\\": status_code, \\"headers\\": headers, \\"content\\": content } except urllib.error.HTTPError as e: return { \\"final_url\\": url, \\"status_code\\": e.code, \\"headers\\": dict(e.headers), \\"content\\": str(e.read()) } except urllib.error.URLError as e: return { \\"final_url\\": url, \\"status_code\\": None, \\"headers\\": {}, \\"content\\": str(e.reason) }"},{"question":"Objective Design and implement a Python program that simulates a small-scale task management system using various compound statements explained in the documentation. Problem Statement You are required to write a function `task_manager(data: list[tuple[str, int]]) -> dict[str, list[int]]` that processes a list of tasks. Each task is represented by a tuple containing a task type (a string) and a duration in minutes (an integer). The function should categorize the tasks based on their type and handle specific scenarios using control flow statements. Input - `data`: A list of tuples where each tuple contains: - `task_type` (str): Type of the task, e.g., \\"work\\", \\"exercise\\", \\"study\\". - `duration` (int): Duration of the task in minutes. Output - Returns a dictionary where: - Keys are task types (str). - Values are lists of integers representing the durations of tasks of that type. Requirements 1. **Categorization**: Use a `for` loop to iterate over the `data` list and categorize tasks based on their type. 2. **Skipping Invalid Tasks**: Tasks with a negative duration should be skipped and not processed. Use `continue` statement for this purpose. 3. **Handling Exceptions**: If a task type is not a string or duration is not an integer, handle these scenarios using `try` and `except` blocks. 4. **Summarizing Tasks**: After processing, if a task type has no valid duration entries, it should have a key in the output dictionary with an empty list. 5. **Use of `with` Statement**: Demonstrate the use of a `with` statement by reading from a dummy context manager that simulates reading task descriptions. 6. **Adding a Default Case**: Use an `if-elif-else` construct to handle specific task types and categorize remaining types under \\"other\\". Example ```python data = [ (\\"work\\", 120), (\\"exercise\\", 45), (\\"study\\", -30), (\\"leisure\\", 60), (123, \\"invalid\\"), (\\"work\\", 90), (\\"leisure\\", 30) ] expected_output = { \\"work\\": [120, 90], \\"exercise\\": [45], \\"study\\": [], \\"leisure\\": [60, 30], \\"other\\": [] } ``` Constraints - The provided list can contain up to 1000 tasks. - Each task type can have up to 200 unique task types. - Task types and durations are validated individually. Solution Template ```python class DummyContextManager: def __enter__(self): return \\"Reading tasks\\" def __exit__(self, exc_type, exc_val, exc_tb): pass def task_manager(data): # Initialize an empty dictionary to hold the categorized tasks tasks = {} # Dummy context manager usage example with DummyContextManager() as description: print(description) for task in data: task_type, duration = None, None try: task_type, duration = task if not isinstance(task_type, str) or not isinstance(duration, int): raise ValueError(\\"Invalid task data\\") except ValueError as e: print(e) continue if duration < 0: continue if task_type not in tasks: tasks[task_type] = [] tasks[task_type].append(duration) # Handle task types with no valid entries for task_type in set(tasks.keys()).union({\\"work\\", \\"exercise\\", \\"study\\", \\"leisure\\"}): if task_type not in tasks: tasks[task_type] = [] if \\"other\\" not in tasks: tasks[\\"other\\"] = [] return tasks ``` # Notes - Ensure to test the function with various input scenarios to validate the correctness of the solution. - Comments and proper formatting will be considered during evaluation.","solution":"class DummyContextManager: def __enter__(self): return \\"Reading tasks\\" def __exit__(self, exc_type, exc_val, exc_tb): pass def task_manager(data): # Initialize an empty dictionary to hold the categorized tasks tasks = {} # Dummy context manager usage example with DummyContextManager() as description: print(description) # Simulating the reading of task descriptions for task in data: task_type, duration = None, None try: task_type, duration = task if not isinstance(task_type, str) or not isinstance(duration, int): raise ValueError(\\"Invalid task data\\") except ValueError as e: print(e) continue if duration < 0: continue if task_type not in tasks: tasks[task_type] = [] tasks[task_type].append(duration) # Ensure every specified task type has an entry, even if it\'s an empty list for task_type in [\\"work\\", \\"exercise\\", \\"study\\", \\"leisure\\"]: if task_type not in tasks: tasks[task_type] = [] return tasks"},{"question":"Custom Priority Queue with Task Management **Objective:** Implement a custom priority queue that supports adding tasks with priorities, removing tasks, changing task priorities, and retrieving the task with the highest priority. **Description:** You are tasked with implementing a `PriorityQueue` class that allows for the following operations: 1. **Adding a Task:** Add a new task with a specified priority. 2. **Removing a Task:** Remove a specified task from the priority queue. 3. **Updating Task Priority:** Update the priority of a specified task. 4. **Getting the Highest Priority Task:** Retrieve and remove the task with the highest priority. To solve this problem comprehensively, you will use the `heapq` module and implement additional logic to handle task removal and priority updates. # Requirements 1. Implement the `PriorityQueue` class with the following methods: - `__init__(self)`: Initializes the priority queue. - `add_task(self, task, priority)`: Adds a task with the given priority. - `remove_task(self, task)`: Removes the specified task. If the task is not found, it should raise a `KeyError`. - `update_task(self, task, priority)`: Updates the priority of the specified task. If the task is not found, it should raise a `KeyError`. - `pop_task(self)`: Removes and returns the highest priority task. If the priority queue is empty, it should raise a `KeyError`. 2. Use a unique sequence count to break ties between tasks with the same priority, ensuring tasks are removed in the order they were added. 3. Implement internal logic to mark tasks as removed without disrupting the heap invariant. Use a placeholder, such as `\'<removed-task>\'`, to represent removed tasks. 4. Provide a mapping from tasks to their heap entries to allow efficient updates and removals. # Expected Input and Output Formats - Input: Methods will accept tasks (represented as strings) and priorities (integers). - Output: The `pop_task` method returns the task string with the highest priority. # Usage Example ```python pq = PriorityQueue() pq.add_task(\'task1\', 5) pq.add_task(\'task2\', 3) pq.add_task(\'task3\', 10) pq.remove_task(\'task2\') pq.update_task(\'task1\', 2) print(pq.pop_task()) # Outputs: \'task3\' print(pq.pop_task()) # Outputs: \'task1\' ``` # Constraints - You may assume task strings are unique. - Ensure all operations are efficient and maintain the heap invariant. **Note:** Make sure your implementation uses the `heapq` module and adheres to the described constraints and requirements.","solution":"import heapq from itertools import count class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = count() # unique sequence count def add_task(self, task, priority=0): Add a new task or update the priority of an existing task. if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task): Mark an existing task as REMOVED. Raise KeyError if not found. entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def update_task(self, task, priority): Update the priority of an existing task. self.remove_task(task) self.add_task(task, priority) def pop_task(self): Remove and return the lowest priority task. Raise KeyError if empty. while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"# Question: Implementing a Custom Error Handler with `errno` **Objective:** You are required to implement a custom error handler function that processes error codes using the `errno` module and returns a detailed error message. **Problem Statement:** Write a function `custom_error_handler(error_code)` that accepts an integer `error_code` as its parameter. This function should: 1. Use the `errno.errorcode` dictionary to map the integer `error_code` to its corresponding string code. 2. Convert this string code to a human-readable error message using `os.strerror()`. 3. Return the detailed error message. If the `error_code` is not found in `errno.errorcode`, the function should return the string \\"Unknown error code\\". **Function Signature:** ```python def custom_error_handler(error_code: int) -> str: pass ``` **Input:** - `error_code` (int): An integer representing the error code. **Output:** - (str): A detailed error message corresponding to the error code or \\"Unknown error code\\" if the error code is not recognized. **Constraints:** - You may assume that the input `error_code` is an integer. - You should handle all possible error codes defined in the `errno` module. - Utilize `os.strerror()` to get the error message for a known error code. **Examples:** ```python assert custom_error_handler(1) == \\"Operation not permitted\\" assert custom_error_handler(2) == \\"No such file or directory\\" assert custom_error_handler(9999) == \\"Unknown error code\\" ``` **Hints:** - The `errno` module and the `os.strerror()` function will be useful for this task. - You can get the list of error codes from `errno.errorcode.keys()`. # Additional Notes: - Make sure your function passes all the provided example test cases. - The function should handle any system-defined error codes appropriately and return a meaningful message.","solution":"import errno import os def custom_error_handler(error_code: int) -> str: Processes the error_code using errno and returns a detailed error message. :param error_code: int - The error code to process. :return: str - The corresponding error message or \'Unknown error code\' if not recognized. if error_code in errno.errorcode: return os.strerror(error_code) else: return \\"Unknown error code\\""},{"question":"**Coding Assessment Question:** # Objective Create a Python function that reads a single character from standard input without echoing it to the terminal. The function should utilize the `termios` module to manipulate terminal settings. # Function Signature ```python def read_single_char(prompt: str) -> str: Reads a single character from standard input without echoing. Parameters: prompt (str): The prompt message to display. Returns: str: The single character read from the user. ``` # Input - `prompt` (str): A string containing the prompt message to display (max length 100 characters). # Output - A single character (str) input by the user, with no newline or additional characters. # Constraints - The function should work on Unix-like operating systems only. - You must use the `termios` module to manipulate terminal settings. - Ensure that terminal settings are restored to their original state after reading the character, even if an error occurs. # Example ```python char = read_single_char(\\"Enter a single character: \\") print(f\\"You entered: {char}\\") ``` # Requirements 1. The terminal should not display the entered character. 2. The original terminal settings must be preserved and restored after execution. 3. The function should handle any potential errors by ensuring the terminal settings are correctly restored. You may find inspiration from the provided example in the documentation, which shows how to handle password input without echoing.","solution":"import termios import sys import tty def read_single_char(prompt: str) -> str: Reads a single character from standard input without echoing. Parameters: prompt (str): The prompt message to display. Returns: str: The single character read from the user. if len(prompt) > 100: raise ValueError(\\"Prompt length should not exceed 100 characters.\\") print(prompt, end=\'\', flush=True) fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: tty.setraw(fd) char = sys.stdin.read(1) finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) print() # For a newline after prompt return char"},{"question":"Filesystem Explorer You are to implement a Python script to create a simple filesystem explorer. The script will take command-line arguments to perform various filesystem operations. Implement this script using the `os` and `argparse` modules. # Problem Statement: Write a Python script called `fs_explorer.py` which performs the following functionalities: 1. **List Directory Content**: List all files and directories in a specified directory. 2. **Create Directory**: Create a new directory at a specified path. 3. **Delete File/Directory**: Delete a specified file or directory. 4. **File/Directory Information**: Display information about a specified file or directory, including its size, creation date, and whether it is a file or directory. # Input: The script should accept the following command-line arguments: - `--list <path>`: Lists all files and directories at the given path. - `--create-dir <path>`: Creates a new directory at the given path. - `--delete <path>`: Deletes the specified file or directory. - `--info <path>`: Displays information about the specified file or directory. # Constraints: - If the specified path for any operation does not exist, the script should output an appropriate error message. - Proper error handling should be implemented for all operations. - You cannot use external libraries other than `os` and `argparse`. # Output: - For the `--list` operation, output the list of files and directories. - For the `--create-dir` operation, output a success message or an error if the directory cannot be created. - For the `--delete` operation, output a success message or an error if the file/directory cannot be deleted. - For the `--info` operation, output the size, creation date, and type (file or directory). # Example: ```bash python fs_explorer.py --list /home/user/documents python fs_explorer.py --create-dir /home/user/new_directory python fs_explorer.py --delete /home/user/old_file.txt python fs_explorer.py --info /home/user/some_file.txt ``` Implementing the `fs_explorer.py` script will test your understanding of interacting with the filesystem, handling command-line arguments, and error handling in Python. # Notes: - Consider using `os.listdir()`, `os.mkdir()`, `os.remove()`, `os.rmdir()`, `os.path.getsize()`, `os.path.getctime()`, and `os.path.isdir()` methods from the `os` module. - Use the `argparse` module to handle command-line arguments effectively.","solution":"import os import argparse import datetime def list_directory(path): try: items = os.listdir(path) return items except FileNotFoundError: return f\\"Error: Directory \'{path}\' does not exist.\\" except NotADirectoryError: return f\\"Error: \'{path}\' is not a directory.\\" except PermissionError: return \\"Error: Permission denied.\\" def create_directory(path): try: os.makedirs(path, exist_ok=True) return f\\"Directory \'{path}\' created successfully.\\" except PermissionError: return \\"Error: Permission denied.\\" except Exception as e: return f\\"Error: {e}\\" def delete_path(path): try: if os.path.isdir(path): os.rmdir(path) else: os.remove(path) return f\\"\'{path}\' deleted successfully.\\" except FileNotFoundError: return f\\"Error: \'{path}\' does not exist.\\" except PermissionError: return \\"Error: Permission denied.\\" except OSError: return \\"Error: Directory is not empty.\\" except Exception as e: return f\\"Error: {e}\\" def get_info(path): try: info = {} info[\'size\'] = os.path.getsize(path) info[\'creation_date\'] = datetime.datetime.fromtimestamp(os.path.getctime(path)).strftime(\'%Y-%m-%d %H:%M:%S\') info[\'type\'] = \'Directory\' if os.path.isdir(path) else \'File\' return info except FileNotFoundError: return f\\"Error: \'{path}\' does not exist.\\" except Exception as e: return f\\"Error: {e}\\" def main(): parser = argparse.ArgumentParser(description=\'Filesystem Explorer\') parser.add_argument(\'--list\', help=\'List all files and directories at the given path\') parser.add_argument(\'--create-dir\', help=\'Create a new directory at the given path\') parser.add_argument(\'--delete\', help=\'Delete the specified file or directory\') parser.add_argument(\'--info\', help=\'Display information about the specified file or directory\') args = parser.parse_args() if args.list: result = list_directory(args.list) if isinstance(result, list): for item in result: print(item) else: print(result) if args.create_dir: result = create_directory(args.create_dir) print(result) if args.delete: result = delete_path(args.delete) print(result) if args.info: result = get_info(args.info) if isinstance(result, dict): for key, value in result.items(): print(f\\"{key}: {value}\\") else: print(result) if __name__ == \\"__main__\\": main()"},{"question":"Design a nested dataclass structure to represent a simplified e-commerce system. This will involve defining several dataclasses that interact with each other and utilize advanced features of the `dataclasses` module. Implement the following specifications: 1. **Product Class**: - Attributes: - `name` (str): The name of the product. - `price` (float): The price of the product. - `quantity_on_hand` (int, default=0): The available quantity of this product. - Generated methods by `@dataclass` (requirements): `__init__`, `__repr__`, `__eq__`. 2. **OrderItem Class**: - Attributes: - `product` (Product): A Product object. - `quantity` (int): The quantity of the product ordered. - Methods: - `total_price` (return float): Returns the total price (product price multiplied by quantity). - Generated methods by `@dataclass`: `__init__`, `__repr__`, `__eq__`. 3. **Customer Class**: - Attributes: - `name` (str): The name of the customer. - `email` (str): The email address of the customer. - Expect methods to be generated: `__init__`, `__repr__`, `__eq__`. 4. **Order Class**: - Attributes: - `order_id` (str): The order ID. - `customer` (Customer): A Customer object. - `items` (list of OrderItem, default_factory=list): A list of OrderItem objects. - `status` (str, default=\'Pending\'): The status of the order. - Methods: - `total_order_value` (return float): Returns the total value of the order (sum of total price for each item). - `add_item` (OrderItem): Method to add an OrderItem to the order. - `finalize_order`: Method to finalize the order, changing status to \'Completed\'. - Generated methods by `@dataclass`: `__init__`, `__repr__`. - `frozen=True` for imitating immutability once the order is finalized. 5. **Orders Management Class**: - Attributes: - `orders` (dict, default_factory=dict): A dictionary to hold orders, with order ID as the key. - Methods: - `create_order` (order_id: str, customer: Customer, items: list[OrderItem]): Method to create a new Order and add it to the dictionary. - `get_pending_orders` (return list of Order): Method to return a list of pending orders. - `complete_order` (order_id: str): Method to mark an order as completed. - Generated methods by `@dataclass`: `__repr__`. # Constraints: - Ensure immutability for order attributes after finalization. - Handle mutable default values correctly. - Use appropriate type annotations and default values. # Performance: - Consider how your methods scale with the number of orders and order items. - Ensure that operations like adding items, calculating total order value, and finalizing orders are efficient. **Example usage:** ```python # Create products p1 = Product(name=\\"Laptop\\", price=1000.0, quantity_on_hand=5) p2 = Product(name=\\"Mouse\\", price=50.0, quantity_on_hand=100) # Create customer customer = Customer(name=\\"John Doe\\", email=\\"john@example.com\\") # Create order items item1 = OrderItem(product=p1, quantity=1) item2 = OrderItem(product=p2, quantity=2) # Create order and add items order = Order(order_id=\\"ORD123\\", customer=customer) order.add_item(item1) order.add_item(item2) print(order.total_order_value()) # Should print 1100.0 # Finalize the order order.finalize_order() # Manage multiple orders orders_manager = OrdersManagement() orders_manager.create_order(order_id=\\"ORD123\\", customer=customer, items=[item1, item2]) pending_orders = orders_manager.get_pending_orders() print(pending_orders) # Should list orders that are still pending orders_manager.complete_order(order_id=\\"ORD123\\") ```","solution":"from dataclasses import dataclass, field from typing import List, Dict @dataclass class Product: name: str price: float quantity_on_hand: int = 0 @dataclass class OrderItem: product: Product quantity: int def total_price(self) -> float: return self.product.price * self.quantity @dataclass class Customer: name: str email: str @dataclass class Order: order_id: str customer: Customer items: List[OrderItem] = field(default_factory=list) status: str = \'Pending\' def total_order_value(self) -> float: return sum(item.total_price() for item in self.items) def add_item(self, item: OrderItem): self.items.append(item) def finalize_order(self): self.status = \'Completed\' @dataclass class OrdersManagement: orders: Dict[str, Order] = field(default_factory=dict) def create_order(self, order_id: str, customer: Customer, items: List[OrderItem]): order = Order(order_id=order_id, customer=customer, items=items) self.orders[order_id] = order def get_pending_orders(self) -> List[Order]: return [order for order in self.orders.values() if order.status == \'Pending\'] def complete_order(self, order_id: str): if order_id in self.orders: self.orders[order_id].finalize_order()"},{"question":"**Question:** Write a Python function named `configure_site_paths` that performs the following operations: 1. Adds a specified directory to Python\'s module search path. 2. Verifies that the directory was successfully added to the search path. 3. Checks for any `.pth` files in the added directory and processes them to include any additional paths specified in these files. 4. Returns the updated `sys.path`. The function should handle cases where the directory does not exist or is not readable. Use the `site` module functions for manipulating the search paths. # Function Signature: ```python def configure_site_paths(directory: str) -> list: pass ``` # Input: - `directory` (str): A string representing the path to be added to the module search path. # Output: - `list`: The updated list of search paths (`sys.path`). # Constraints: - Assume the Python environment does not use the `-S` flag. - The function must handle exceptions gracefully and take appropriate actions (e.g., logging an error, skipping non-existent directories). - The directory path provided may contain `.pth` files. # Example: ```python # Example usage paths = configure_site_paths(\'/path/to/custom_modules\') print(paths) # should include \'/path/to/custom_modules\' and any additional paths specified in \'.pth\' files ``` In your implementation, ensure that: - You use the `site.addsitedir()` function to add the directory and process `.pth` files. - Proper exception handling is implemented to manage errors related to directory access. - The updated `sys.path` is returned. # Description of Task: 1. Add the given directory to the module search path. 2. Ensure the directory is added to the `sys.path`. 3. Automatically process any `.pth` files in the directory to add additional paths. 4. Return the updated list of search paths. Ensure you provide comments and clear error handling processes in your implementation.","solution":"import sys import site import os def configure_site_paths(directory: str) -> list: Adds a specified directory to Python\'s module search path, processes any .pth files in the directory, and returns updated sys.path. Parameters: directory (str): A string representing the path to be added to the module search path. Returns: list: The updated list of search paths (sys.path) after adding the directory and processing .pth files. try: # Check if the directory exists and is readable if not os.path.isdir(directory) or not os.access(directory, os.R_OK): raise FileNotFoundError(f\\"Directory {directory} does not exist or is not readable.\\") # Add the directory to the module search path site.addsitedir(directory) # Check if there are any .pth files in the directory and process them # (This is automatically handled by addsitedir) # Verify that the directory was successfully added if directory not in sys.path: raise RuntimeError(f\\"Directory {directory} could not be added to sys.path.\\") return sys.path except Exception as e: print(f\\"An error occurred: {e}\\") # If exception occurs, return the original sys.path without modifications return sys.path"},{"question":"Objective: To evaluate students\' comprehension and application of advanced pandas functionalities including pivoting, stacking/unstacking, melting, generating dummy variables, and using the `explode` method. Problem Statement: You are provided with a dataset that includes details about various transactions made in an e-commerce store. The data is in a \\"record\\" or \\"long\\" format. Your task is to reshape and manipulate this data to prepare it for further analysis. Specifically, you need to create functions to accomplish the following tasks: 1. **Pivot the data** to show the total sum of transaction values for each product category on each date. 2. **Unstack the pivoted data** to convert the column index into row index format. 3. **Melt the unstacked data** back into a long format. 4. **Generate dummy variables** for the melted data\'s product categories. 5. **Explode a specific column** containing lists of multiple items ordered in a single transaction into separate rows. Input Format: - A CSV file named `transactions.csv` with the following columns: - `transaction_id` (int): Unique ID for each transaction. - `date` (string): Date of the transaction in \'YYYY-MM-DD\' format. - `product_category` (string): Category of the product. - `transaction_value` (float): Value of the transaction. - `items` (list of strings): List of items in the transaction. Output: - A tuple containing: - A pandas DataFrame showing the pivoted data. - A pandas DataFrame showing the unstacked data. - A pandas DataFrame showing the melted data. - A pandas DataFrame with dummy variables for product categories. - A pandas DataFrame with exploded items column. Constraints: - Assume the CSV file is well-formed and contains no missing values. - The operations should be performed efficiently using pandas functionality. - Preserve the original index wherever applicable. Example: Given the following `transactions.csv`: ``` transaction_id,date,product_category,transaction_value,items 1,2023-01-01,Electronics,1000.00,\\"[\'Laptop\', \'Mouse\']\\" 2,2023-01-01,Clothing,150.00,\\"[\'Shirt\', \'Trousers\']\\" 3,2023-01-02,Electronics,500.00,\\"[\'Tablet\']\\" 4,2023-01-02,Furniture,300.00,\\"[\'Chair\', \'Desk\']\\" ``` You need to implement functions as described to produce the desired data transformations. # Function Signature: ```python import pandas as pd def load_data(filepath: str) -> pd.DataFrame: pass # Load the data from the CSV file def pivot_data(df: pd.DataFrame) -> pd.DataFrame: pass # Pivot the data as specified def unstack_data(pivoted_df: pd.DataFrame) -> pd.DataFrame: pass # Unstack the pivoted data def melt_data(unstacked_df: pd.DataFrame) -> pd.DataFrame: pass # Melt the unstacked data def generate_dummies(melted_df: pd.DataFrame) -> pd.DataFrame: pass # Generate dummy variables def explode_items(df: pd.DataFrame) -> pd.DataFrame: pass # Explode the items column into separate rows # Example usage: if __name__ == \\"__main__\\": df = load_data(\'transactions.csv\') pivoted_df = pivot_data(df) unstacked_df = unstack_data(pivoted_df) melted_df = melt_data(unstacked_df) dummies_df = generate_dummies(melted_df) exploded_df = explode_items(df) print(pivoted_df, unstacked_df, melted_df, dummies_df, exploded_df) ```","solution":"import pandas as pd def load_data(filepath: str) -> pd.DataFrame: Load the data from the CSV file. return pd.read_csv(filepath) def pivot_data(df: pd.DataFrame) -> pd.DataFrame: Pivot the data to show the total sum of transaction values for each product category on each date. return df.pivot_table(index=\'date\', columns=\'product_category\', values=\'transaction_value\', aggfunc=\'sum\') def unstack_data(pivoted_df: pd.DataFrame) -> pd.DataFrame: Unstack the pivoted data to convert the column index into row index format. return pivoted_df.unstack().reset_index(name=\'transaction_value\') def melt_data(unstacked_df: pd.DataFrame) -> pd.DataFrame: Melt the unstacked data back into a long format. return unstacked_df.melt(id_vars=[\'date\', \'product_category\'], var_name=\'metric\', value_name=\'value\') def generate_dummies(melted_df: pd.DataFrame) -> pd.DataFrame: Generate dummy variables for the melted data\'s product categories. return pd.get_dummies(melted_df, columns=[\'product_category\']) def explode_items(df: pd.DataFrame) -> pd.DataFrame: Explode the items column into separate rows. df[\'items\'] = df[\'items\'].apply(lambda x: eval(x)) # Convert string representation of list to actual list return df.explode(\'items\') if __name__ == \\"__main__\\": df = load_data(\'transactions.csv\') pivoted_df = pivot_data(df) unstacked_df = unstack_data(pivoted_df) melted_df = melt_data(unstacked_df) dummies_df = generate_dummies(melted_df) exploded_df = explode_items(df) print(pivoted_df, unstacked_df, melted_df, dummies_df, exploded_df)"},{"question":"**Coding Assessment Question: Unicode Handling in Python** **Objective:** Your task is to demonstrate your understanding of handling Unicode data in Python. You will implement a function that takes a string containing mixed English text and Unicode characters and processes it by normalizing the Unicode characters, converting the string to bytes using a specified encoding, and then decoding it back to a string. **Problem Statement:** Implement a function `process_unicode_text(text: str, encoding: str = \\"utf-8\\") -> str` which does the following: 1. Normalizes the input text to NFC (Normalization Form C) format. 2. Encodes the normalized string to bytes using the specified encoding. 3. Decodes the bytes back to a Unicode string. **Function Signature:** ```python def process_unicode_text(text: str, encoding: str = \\"utf-8\\") -> str: pass ``` **Parameters:** - `text`: A string containing mixed English text and Unicode characters. - `encoding`: A string representing the desired encoding format (default is \\"utf-8\\"). **Returns:** - A string that is the result of normalizing, encoding, and decoding the input text. **Example:** ```python input_text = \\"caféN{LATIN SMALL LETTER E WITH ACUTE}\\" result = process_unicode_text(input_text) print(result) # Output: \\"caféé\\" ``` **Constraints:** - The function must correctly handle cases where the input text contains characters that can be decomposed and recomposed into a normalized form. - The function should work with at least \\"utf-8\\" and \\"ascii\\" encodings. **Hints:** - Use the `unicodedata` module for normalization. - Be aware of error handling while encoding and decoding. **Performance Requirements:** - The function should be efficient enough to handle typical use cases of processing strings of moderate length (up to thousands of characters). You are required to write unit tests to validate your solution against various edge cases and typical inputs.","solution":"import unicodedata def process_unicode_text(text: str, encoding: str = \\"utf-8\\") -> str: Normalizes the input text to NFC (Normalization Form C) format, encodes it to bytes using the specified encoding, and decodes it back to a Unicode string. Args: text (str): A string containing mixed English text and Unicode characters. encoding (str): The desired encoding format (default is \\"utf-8\\"). Returns: str: The normalized, encoded, and decoded string. # Normalize the text to NFC format normalized_text = unicodedata.normalize(\'NFC\', text) # Encode the normalized text to bytes using the specified encoding encoded_text = normalized_text.encode(encoding) # Decode the bytes back to a Unicode string decoded_text = encoded_text.decode(encoding) return decoded_text"},{"question":"# Coding Assessment: Advanced Python Code Analyzer Objective Design a Python function that analyzes Python source code provided as input, generates an abstract syntax tree (AST) from it, identifies all function definitions, and returns a summary of each function including its name, number of arguments, and the complexity of the function measured by the number of statements. Function Signature ```python def analyze_python_code(source_code: str) -> List[Dict[str, Union[str, int]]]: ``` Input - **source_code** (`str`): A multi-line string containing Python source code. Output - **List[Dict[str, Union[str, int]]]**: A list of dictionaries where each dictionary holds information about a function. The dictionary should have the following keys: - `\\"name\\"` (`str`): The name of the function. - `\\"arguments\\"` (`int`): The number of arguments the function takes. - `\\"statements\\"` (`int`): The number of statements within the function body (indicating complexity). Constraints - The function should handle typical Python code structures including nested functions, async functions, and comprehensions. - Ignore lambda functions. - Assume the source code will not have syntax errors. Example ```python source_code = \'\'\' def add(a, b): return a + b def multiply(a, b, c=1): result = a * b * c return result async def fetch_data(): data = await get_data() return data \'\'\' print(analyze_python_code(source_code)) ``` Expected Output ```python [ {\\"name\\": \\"add\\", \\"arguments\\": 2, \\"statements\\": 1}, {\\"name\\": \\"multiply\\", \\"arguments\\": 3, \\"statements\\": 2}, {\\"name\\": \\"fetch_data\\", \\"arguments\\": 0, \\"statements\\": 2} ] ``` Guidelines 1. Use the `ast` module to parse the source code into an Abstract Syntax Tree. 2. Traverse the AST to identify all function definitions. 3. Collect and compute the required information (function name, number of arguments, and number of statements). 4. Return the results as specified. Good luck, and may your ASTs be ever balanced!","solution":"import ast from typing import List, Dict, Union def analyze_python_code(source_code: str) -> List[Dict[str, Union[str, int]]]: Analyzes the given Python source code to extract information about functions defined within it. Parameters: source_code (str): A multi-line string containing Python source code. Returns: List[Dict[str, Union[str, int]]]: A list of dictionaries with details about each function. results = [] # Parse the source code into an AST tree = ast.parse(source_code) # Traverse the AST to identify and collect function definitions for node in ast.walk(tree): if isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef): # Extract function name func_name = node.name # Count the number of arguments arg_count = len([arg.arg for arg in node.args.args]) # Count the number of statements in the function body stmt_count = len(node.body) # Build the function\'s summary dictionary func_info = { \\"name\\": func_name, \\"arguments\\": arg_count, \\"statements\\": stmt_count } # Add to the result list results.append(func_info) return results"},{"question":"# Coding Challenge **Objective:** Implement a message authentication system using HMAC. Your task is to write several functions to simulate the process of creating and verifying message digests using secret keys. Functions to Implement: 1. **`create_hmac(key: bytes, message: bytes, digestmod: str) -> bytes`** - **Input:** - `key`: A bytes object representing the secret key. - `message`: A bytes object representing the message. - `digestmod`: A string indicating the digest algorithm to use (e.g., \'sha256\'). - **Output:** A bytes object representing the HMAC digest. 2. **`verify_hmac(key: bytes, message: bytes, digest_to_verify: bytes, digestmod: str) -> bool`** - **Input:** - `key`: A bytes object representing the secret key. - `message`: A bytes object representing the message. - `digest_to_verify`: A bytes object representing the HMAC digest to verify. - `digestmod`: A string indicating the digest algorithm to use. - **Output:** A boolean indicating whether the provided digest matches the computed HMAC digest using the key and message. 3. **`update_hmac(hmac_obj, message: bytes) -> None`** - **Input:** - `hmac_obj`: An HMAC object created using `hmac.new`. - `message`: A bytes object representing the message to add to the HMAC object. - **Output:** None. The function should update the HMAC object with the new message. 4. **`secure_compare(digest1: bytes, digest2: bytes) -> bool`** - **Input:** - `digest1`: A bytes object representing the first digest. - `digest2`: A bytes object representing the second digest. - **Output:** A boolean indicating whether the two digests are equal, using the `hmac.compare_digest` method for secure comparison to prevent timing attacks. Constraints: - The `key` and `message` inputs will always be non-empty byte objects. - The `digestmod` will always be a valid digest algorithm supported by `hashlib`. - You should handle any edge cases, such as different lengths of digests being compared in the `secure_compare`. Example Usage: ```python key = b\'secret_key\' message = b\'This is a message\' digestmod = \'sha256\' # Create HMAC digest = create_hmac(key, message, digestmod) # Verify HMAC is_valid = verify_hmac(key, message, digest, digestmod) print(is_valid) # Expected: True # Update HMAC object with additional message hmac_obj = hmac.new(key, digestmod=digestmod) update_hmac(hmac_obj, b\'This is a message part 2\') updated_digest = hmac_obj.digest() # Secure Comparison digest_match = secure_compare(digest, updated_digest) print(digest_match) # Expected: False initially, it might differ after the update. ``` Make sure to follow the function signatures and thoroughly test your implementations to ensure accuracy and security.","solution":"import hmac import hashlib def create_hmac(key: bytes, message: bytes, digestmod: str) -> bytes: Creates an HMAC digest using the given key, message, and digestmod algorithm. hmac_obj = hmac.new(key, message, digestmod=getattr(hashlib, digestmod)) return hmac_obj.digest() def verify_hmac(key: bytes, message: bytes, digest_to_verify: bytes, digestmod: str) -> bool: Verifies an HMAC digest using the given key, message, and digestmod algorithm. expected_hmac = create_hmac(key, message, digestmod) return hmac.compare_digest(expected_hmac, digest_to_verify) def update_hmac(hmac_obj, message: bytes) -> None: Updates the HMAC object with the given message. hmac_obj.update(message) def secure_compare(digest1: bytes, digest2: bytes) -> bool: Securely compares two digests to see if they are equal. return hmac.compare_digest(digest1, digest2)"},{"question":"on Unix Specific Services **Objective:** Implement a function in Python that leverages POSIX system calls to perform file operations securely and efficiently. **Task:** Write a function `secure_file_operations()` in Python that performs the following: 1. Creates a file named \\"example.txt\\" in the current directory. 2. Writes the string \\"Hello, Unix World!\\" to the file. 3. Reads the content of the file to verify the written string. 4. Updates the file permission to be readable and writable only by the owner. 5. Appends the string \\" Secure operations are fun!\\" to the existing content of the file. 6. Reads the updated content of the file to verify the append operation. 7. Finally, deletes the file. **Function Signature:** ```python def secure_file_operations() -> str: # Your implementation here ``` **Expected Input:** - No input arguments. **Expected Output:** - Returns the final content of \\"example.txt\\" after all operations: `\\"Hello, Unix World! Secure operations are fun!\\"`. **Constraints:** - You must use the `os` module for POSIX system calls. - Only the owner of the file should have read and write permissions after the permission update. - Handle any potential exceptions that may arise during file operations to ensure the function performs securely and reliably under varying conditions. **Hints:** - Use the `os.open()`, `os.write()`, `os.read()`, and `os.close()` system calls for file operations. - Use `os.chmod()` to change the file permissions. - Handle file descriptors and ensure files are properly closed after operations to avoid resource leaks. **Example Workflow:** 1. Create and write to the file: ```python fd = os.open(\\"example.txt\\", os.O_WRONLY | os.O_CREAT) os.write(fd, b\\"Hello, Unix World!\\") os.close(fd) ``` 2. Read the file: ```python fd = os.open(\\"example.txt\\", os.O_RDONLY) content = os.read(fd, 100) os.close(fd) ``` 3. Change file permissions: ```python os.chmod(\\"example.txt\\", 0o600) ``` 4. Append to the file: ```python fd = os.open(\\"example.txt\\", os.O_WRONLY | os.O_APPEND) os.write(fd, b\\" Secure operations are fun!\\") os.close(fd) ``` 5. Verify the final content: ```python fd = os.open(\\"example.txt\\", os.O_RDONLY) final_content = os.read(fd, 100) os.close(fd) ``` 6. Delete the file: ```python os.remove(\\"example.txt\\") ``` **Notes:** - Ensure exception handling around all file operations to catch any potential errors such as permission issues or file not found errors. - Use meaningful variable names and comments to enhance code readability.","solution":"import os def secure_file_operations() -> str: try: # Create and write to the file fd = os.open(\\"example.txt\\", os.O_WRONLY | os.O_CREAT) os.write(fd, b\\"Hello, Unix World!\\") os.close(fd) # Read the content of the file to verify the written string fd = os.open(\\"example.txt\\", os.O_RDONLY) content = os.read(fd, 100) os.close(fd) assert content == b\\"Hello, Unix World!\\", \\"Initial write verification failed\\" # Update the file permissions to be readable and writable only by the owner os.chmod(\\"example.txt\\", 0o600) # Append to the file fd = os.open(\\"example.txt\\", os.O_WRONLY | os.O_APPEND) os.write(fd, b\\" Secure operations are fun!\\") os.close(fd) # Read the updated content of the file to verify the append operation fd = os.open(\\"example.txt\\", os.O_RDONLY) final_content = os.read(fd, 100) os.close(fd) assert final_content == b\\"Hello, Unix World! Secure operations are fun!\\", \\"Append operation verification failed\\" # Delete the file os.remove(\\"example.txt\\") return final_content.decode() except Exception as e: # If any exception occurs, clean up by ensuring the file is deleted if os.path.exists(\\"example.txt\\"): os.remove(\\"example.txt\\") raise e"},{"question":"# Python Coding Assessment: Warning Control Objective Design a function that processes data from a list and, during this process, issues and handles various types of warnings. You are required to demonstrate your understanding of the `warnings` module by customizing warning behaviors for different scenarios. Problem Statement You are provided with a list of dictionaries, where each dictionary contains information about different user operations. Write a function `process_operations(operations)` that processes this data and issues warnings as follows: 1. If an operation uses a deprecated feature, issue a `DeprecationWarning`. 2. If an operation contains a syntax error, issue a `SyntaxWarning`. 3. If an operation has a high runtime (greater than a threshold), issue a `RuntimeWarning`. In addition: - Configure the warnings so that `DeprecationWarning` is shown only once. - `SyntaxWarning` and `RuntimeWarning` should always be shown. - Capture the warnings issued during the function execution and return a list of warning messages. Input - `operations`: A list of dictionaries where each dictionary represents an operation and contains the keys: - `feature`: A string representing the feature used in the operation. - `syntax`: A boolean indicating whether the operation has a syntax error. - `runtime`: An integer representing the runtime of the operation. Output - A list of warning messages issued during the processing of operations. Function Signature ```python def process_operations(operations: list) -> list: ``` Example ```python operations = [ {\\"feature\\": \\"deprecated_feature\\", \\"syntax\\": False, \\"runtime\\": 100}, {\\"feature\\": \\"new_feature\\", \\"syntax\\": True, \\"runtime\\": 50}, {\\"feature\\": \\"deprecated_feature\\", \\"syntax\\": False, \\"runtime\\": 200}, {\\"feature\\": \\"new_feature\\", \\"syntax\\": False, \\"runtime\\": 300} ] result = process_operations(operations) print(result) ``` Expected Output: ``` [ \\"DeprecationWarning: deprecated_feature is deprecated\\", \\"SyntaxWarning: Operation contains a syntax error\\", \\"RuntimeWarning: Operation runtime is too high (100 ms)\\", \\"SyntaxWarning: Operation contains a syntax error\\", \\"RuntimeWarning: Operation runtime is too high (200 ms)\\", \\"RuntimeWarning: Operation runtime is too high (300 ms)\\" ] ``` Constraints - The runtime threshold is 150 ms. Notes - Use the `warnings` module to issue and handle warnings. - Utilize the context manager `warnings.catch_warnings(record=True)` for capturing warnings. - Customize the warnings filter as specified.","solution":"import warnings def process_operations(operations): # We will store the warning messages here warning_messages = [] # Configure warnings warnings.simplefilter(\\"always\\", SyntaxWarning) warnings.simplefilter(\\"always\\", RuntimeWarning) warnings.simplefilter(\\"once\\", DeprecationWarning) # Use a context manager to catch warnings with warnings.catch_warnings(record=True) as w: # Loop through each operation for op in operations: if op[\\"feature\\"] == \\"deprecated_feature\\": warnings.warn(f\\"{op[\'feature\']} is deprecated\\", DeprecationWarning) if op[\\"syntax\\"]: warnings.warn(\\"Operation contains a syntax error\\", SyntaxWarning) if op[\\"runtime\\"] > 150: warnings.warn(f\\"Operation runtime is too high ({op[\'runtime\']} ms)\\", RuntimeWarning) # Collect the warning messages from the issued warnings for warning in w: warning_messages.append(f\\"{warning.category.__name__}: {warning.message}\\") return warning_messages"},{"question":"# Custom Module Importer In this task, you are required to implement a custom module import system using Python\'s `importlib` module and associated mechanisms. Specifically, you will create a `CustomFinder` and `CustomLoader` to handle the import of modules from a specified directory that contains `.txt` files instead of the standard `.py` files. Each `.txt` file will contain valid Python code. # Details 1. **CustomFinder**: This class should be responsible for finding modules that end with the `.txt` extension within the specified directory. 2. **CustomLoader**: This class should be responsible for loading the contents of the `.txt` file, executing the Python code within, and initializing the module object with the executed code. # Implementation Steps 1. Create a `CustomFinder` class that: * Inherits from `importlib.abc.MetaPathFinder`. * Implements the `find_spec` method to identify `.txt` files within a specified directory and return the appropriate `ModuleSpec` if found. * Searches for modules in a given directory (this directory should be passed during the initialization of the finder). 2. Create a `CustomLoader` class that: * Inherits from `importlib.abc.Loader`. * Implements the `create_module` method to create a new module object. * Implements the `exec_module` method to execute the contents of the `.txt` file and populate the new module’s namespace. 3. Integrate the `CustomFinder` into the Python import system by adding it to `sys.meta_path`. 4. Write an example Python script to test the working of the custom importer. # Constraints * The directory to be searched will be provided and will contain Python scripts in `.txt` files. * Your finder and loader should only handle modules in this directory and ignore any other types of imports. * Handle exceptions such as file not found or invalid Python code within the `.txt` files gracefully and raise appropriate import errors. # Example Usage Assume the given directory is `./custom_modules/` and it contains a file `example.txt` with the following content: ```python def hello(): print(\\"Hello from the example module!\\") ``` Your custom importer should allow the following usage in a Python script: ```python import example example.hello() # This should print \\"Hello from the example module!\\" ``` # Testing Ensure that your script correctly handles: * Importing the module and executing its functions. * Raising `ModuleNotFoundError` if the module does not exist. * Handling and raising appropriate errors if the `.txt` file contains invalid Python code. * Ensuring that the imported module\'s namespace is correctly populated. # Submission Submit your implementation of the `CustomFinder` and `CustomLoader` classes along with the sample script demonstrating the functionality.","solution":"import importlib.abc import importlib.util import sys import os class CustomFinder(importlib.abc.MetaPathFinder): def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path, target=None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self.directory, module_name + \'.txt\') if not os.path.isfile(module_path): return None return importlib.util.spec_from_file_location(fullname, module_path, loader=CustomLoader(module_path)) class CustomLoader(importlib.abc.Loader): def __init__(self, module_path): self.module_path = module_path def create_module(self, spec): return None def exec_module(self, module): with open(self.module_path, \'r\') as file: code = file.read() exec(code, module.__dict__) # Adding the custom finder to the system\'s meta path custom_modules_dir = \'./custom_modules\' sys.meta_path.insert(0, CustomFinder(custom_modules_dir))"},{"question":"# Python Object Lifecycle Management Objective Your task is to implement a class in Python that simulates the lifecycle management of an object, mimicking the behavior described in the low-level memory management documentation above. Requirements 1. **Object Creation:** Create a `CustomObject` class which can initialize its instances and maintain a reference count. 2. **Variable-Sized Objects:** Extend the `CustomObject` class to support a variable-sized object (e.g., an object containing a dynamic list). 3. **Reference Management:** Implement methods to increment and decrement references, emulating garbage collection by deleting objects when their reference count drops to zero. Instructions 1. Define a `CustomObject` class with the following attributes: - A class attribute to keep track of all instances. - An `id` attribute to uniquely identify each object. - A `ref_count` attribute to manage the reference count. - Initialize each new object with a unique `id` and a reference count set to 1. - Add the object to the class-level tracking list upon creation. 2. Implement a `CustomVarObject` class that extends `CustomObject`, adding: - A `size` attribute to indicate the size of the variable attribute (e.g., length of a list). 3. Implement the following methods: - `increment_ref()`: Increments the reference count and returns the new count. - `decrement_ref()`: Decrements the reference count. If the count reaches zero, delete the object and return a message. - `__del__()`: Define this method to perform necessary cleanup when an object is deleted. 4. Demonstrate your implementation by: - Creating a few instances of `CustomObject` and `CustomVarObject`. - Modifying their reference counts. - Showing the deletion of objects when the reference count drops to zero. Example Usage ```python class CustomObject: all_objects = [] id_counter = 0 def __init__(self): self.id = CustomObject.id_counter CustomObject.id_counter += 1 self.ref_count = 1 CustomObject.all_objects.append(self) print(f\\"Object {self.id} created. Reference count set to {self.ref_count}.\\") def increment_ref(self): self.ref_count += 1 print(f\\"Object {self.id} reference count incremented to {self.ref_count}.\\") return self.ref_count def decrement_ref(self): self.ref_count -= 1 print(f\\"Object {self.id} reference count decremented to {self.ref_count}.\\") if self.ref_count == 0: self.__del__() def __del__(self): print(f\\"Object {self.id} is being deleted.\\") CustomObject.all_objects.remove(self) class CustomVarObject(CustomObject): def __init__(self, size): super().__init__() self.size = size self.dynamic_list = [None] * size print(f\\"Variable-sized object {self.id} of size {self.size} created.\\") # Example usage obj1 = CustomObject() obj1.increment_ref() obj1.decrement_ref() obj1.decrement_ref() var_obj = CustomVarObject(5) var_obj.increment_ref() var_obj.decrement_ref() var_obj.decrement_ref() ``` This question is designed to test students\' understanding of object-oriented programming, the lifecycle of objects, and reference counting, which are core concepts in Python programming.","solution":"class CustomObject: all_objects = [] id_counter = 0 def __init__(self): self.id = CustomObject.id_counter CustomObject.id_counter += 1 self.ref_count = 1 CustomObject.all_objects.append(self) print(f\\"Object {self.id} created. Reference count set to {self.ref_count}.\\") def increment_ref(self): self.ref_count += 1 print(f\\"Object {self.id} reference count incremented to {self.ref_count}.\\") return self.ref_count def decrement_ref(self): self.ref_count -= 1 print(f\\"Object {self.id} reference count decremented to {self.ref_count}.\\") if self.ref_count == 0: self.__del__() def __del__(self): print(f\\"Object {self.id} is being deleted.\\") CustomObject.all_objects.remove(self) return \\"Object deleted\\" class CustomVarObject(CustomObject): def __init__(self, size): super().__init__() self.size = size self.dynamic_list = [None] * size print(f\\"Variable-sized object {self.id} of size {self.size} created.\\")"},{"question":"# Complex Number Operations and Properties Problem Statement: You are tasked with implementing two functions `validate_complex_operations` and `analyze_complex_behavior` that leverage the `cmath` module\'s functionality to solve specific complex number operations. 1. **Function `validate_complex_operations`**: - Input: Two complex numbers `z1` and `z2`. - Output: A dictionary with the following keys and their corresponding values: - `\'sum\'`: The sum of `z1` and `z2`. - `\'product\'`: The product of `z1` and `z2`. - `\'sum_polar\'`: The polar coordinates of the sum, returned as a tuple `(r, phi)`. - `\'product_polar\'`: The polar coordinates of the product, returned as a tuple `(r, phi)`. 2. **Function `analyze_complex_behavior`**: - Input: A complex number `z`. - Output: A dictionary with information about `z` containing the following keys and values: - `\'is_finite\'`: A boolean indicating if `z` is finite. - `\'is_nan\'`: A boolean indicating if either part of `z` is NaN. - `\'sqrt\'`: The square root of `z`. - `\'log\'`: The natural logarithm of `z`. - `\'cos\'`: The cosine of `z`. - `\'sin\'`: The sine of `z`. Constraints: - You must use the `cmath` module for all complex number calculations. - Any input that can potentially lead to errors (e.g., taking the log of 0) should be handled gracefully, i.e., catch exceptions and handle them accordingly. - Efficiency: The program should efficiently handle a large number of inputs. Example: ```python import cmath def validate_complex_operations(z1, z2): result = {} sum_z = z1 + z2 product_z = z1 * z2 result[\'sum\'] = sum_z result[\'product\'] = product_z result[\'sum_polar\'] = cmath.polar(sum_z) result[\'product_polar\'] = cmath.polar(product_z) return result def analyze_complex_behavior(z): result = {} result[\'is_finite\'] = cmath.isfinite(z) result[\'is_nan\'] = cmath.isnan(z) try: result[\'sqrt\'] = cmath.sqrt(z) except ValueError: result[\'sqrt\'] = float(\'nan\') try: result[\'log\'] = cmath.log(z) except ValueError: result[\'log\'] = float(\'nan\') result[\'cos\'] = cmath.cos(z) result[\'sin\'] = cmath.sin(z) return result # Example Usage z1 = 1 + 2j z2 = 3 + 4j print(validate_complex_operations(z1, z2)) z = 1 + 1j print(analyze_complex_behavior(z)) ``` Implement the two functions as specified. Your implementation should pass the provided example and additional test cases under the specified constraints.","solution":"import cmath def validate_complex_operations(z1, z2): Validate complex number operations. Arguments: z1 -- First complex number z2 -- Second complex number Returns: A dictionary with the sum, product, and their polar coordinates. result = {} sum_z = z1 + z2 product_z = z1 * z2 result[\'sum\'] = sum_z result[\'product\'] = product_z result[\'sum_polar\'] = cmath.polar(sum_z) result[\'product_polar\'] = cmath.polar(product_z) return result def analyze_complex_behavior(z): Analyze complex number behavior. Arguments: z -- Complex number Returns: A dictionary with information about the complex number such as finiteness, NaN status, and various mathematical operations. result = {} result[\'is_finite\'] = cmath.isfinite(z) result[\'is_nan\'] = cmath.isnan(z) try: result[\'sqrt\'] = cmath.sqrt(z) except ValueError: result[\'sqrt\'] = float(\'nan\') try: result[\'log\'] = cmath.log(z) except ValueError: result[\'log\'] = float(\'nan\') result[\'cos\'] = cmath.cos(z) result[\'sin\'] = cmath.sin(z) return result"},{"question":"Objective: You are given a dataset of sales records. Your task is to transform and analyze this data using pandas general functions to uncover meaningful insights. The Dataset: ```python import pandas as pd data = { \'OrderID\': [1, 2, 3, 4, 5], \'Product\': [\'Laptop\', \'Tablet\', \'Laptop\', \'Phone\', \'Tablet\'], \'Category\': [\'Electronics\', \'Electronics\', \'Electronics\', \'Electronics\', \'Electronics\'], \'Quantity\': [2, 5, 3, 1, 7], \'Price\': [1200, 300, 1200, 800, 300], \'OrderDate\': [\'2023-01-01\', \'2023-01-05\', \'2023-02-15\', \'2023-02-17\', \'2023-03-01\'], \'City\': [\'New York\', \'Los Angeles\', \'New York\', \'Chicago\', \'Los Angeles\'] } df = pd.DataFrame(data) ``` Tasks: 1. **Melt the Data:** Transform the DataFrame `df` such that `OrderID`, `Product`, `Category`, and `OrderDate` are identifiers (id_vars) and all other columns are unpivoted (melted). 2. **Pivot the Data:** Pivot the melted DataFrame such that each product’s quantity by city is displayed in a wide format. 3. **Aggregated Pivot Table:** Create a pivot table to show the total revenue (Quantity * Price) from each product for each city. 4. **Extracting Dummy Variables:** Convert the `Product` column into dummy/indicator variables. 5. **Merge DataFrames:** You are given another dataframe `discounts` specifying the discount on each product. ```python discounts = { \'Product\': [\'Laptop\', \'Tablet\', \'Phone\'], \'Discount\': [100, 50, 25] } discounts_df = pd.DataFrame(discounts) ``` Merge this discounts dataframe with the original dataframe `df` to create a new dataframe showing the discount for each order. Expected Output: 1. A melted DataFrame. 2. A pivot table showing quantities by city for each product. 3. A pivot table showing total revenue from each product for each city. 4. A DataFrame with dummy variables for the `Product` column. 5. A merged DataFrame with discount information included. Implementation: Define the following function: ```python def sales_data_analysis(df): Args: df (pd.DataFrame): DataFrame containing the sales record. Returns: tuple: A tuple containing the following DataFrames: - Melted DataFrame - Pivoted DataFrame with quantities by city - Pivot table with total revenue by city - DataFrame with dummy variables for \'Product\' - Merged DataFrame with discount information # Write your implementation here, ensuring it performs the described transformations and merges. ``` **Constraints:** - Ensure that your solution is efficient and handles potential data inconsistencies gracefully. - Utilize the appropriate pandas functions listed in the provided documentation. Submission: Submit your implementation of the `sales_data_analysis` function along with sample outputs given the dataset provided.","solution":"import pandas as pd def sales_data_analysis(df): Args: df (pd.DataFrame): DataFrame containing the sales record. Returns: tuple: A tuple containing the following DataFrames: - Melted DataFrame - Pivoted DataFrame with quantities by city - Pivot table with total revenue by city - DataFrame with dummy variables for \'Product\' - Merged DataFrame with discount information # Melt the DataFrame melted_df = pd.melt(df, id_vars=[\'OrderID\', \'Product\', \'Category\', \'OrderDate\'], value_vars=[\'Quantity\', \'Price\', \'City\'], var_name=\'Variable\', value_name=\'Value\') # Pivot the DataFrame to show quantities by city for each product pivoted_df = df.pivot_table(index=[\'Product\'], columns=[\'City\'], values=\'Quantity\', aggfunc=\'sum\', fill_value=0).reset_index() # Create a pivot table to show total revenue from each product for each city df[\'Revenue\'] = df[\'Quantity\'] * df[\'Price\'] revenue_pivot_df = df.pivot_table(index=\'Product\', columns=\'City\', values=\'Revenue\', aggfunc=\'sum\', fill_value=0).reset_index() # Convert the \'Product\' column into dummy variables dummy_df = pd.get_dummies(df[\'Product\'], prefix=\'Product\') # Define discounts dataframe discounts = { \'Product\': [\'Laptop\', \'Tablet\', \'Phone\'], \'Discount\': [100, 50, 25] } discounts_df = pd.DataFrame(discounts) # Merge the discounts dataframe with the original dataframe merged_df = df.merge(discounts_df, on=\'Product\', how=\'left\') return melted_df, pivoted_df, revenue_pivot_df, dummy_df, merged_df # Example usage: df = pd.DataFrame({ \'OrderID\': [1, 2, 3, 4, 5], \'Product\': [\'Laptop\', \'Tablet\', \'Laptop\', \'Phone\', \'Tablet\'], \'Category\': [\'Electronics\', \'Electronics\', \'Electronics\', \'Electronics\', \'Electronics\'], \'Quantity\': [2, 5, 3, 1, 7], \'Price\': [1200, 300, 1200, 800, 300], \'OrderDate\': [\'2023-01-01\', \'2023-01-05\', \'2023-02-15\', \'2023-02-17\', \'2023-03-01\'], \'City\': [\'New York\', \'Los Angeles\', \'New York\', \'Chicago\', \'Los Angeles\'] }) melted_df, pivoted_df, revenue_pivot_df, dummy_df, merged_df = sales_data_analysis(df) print(melted_df) print(pivoted_df) print(revenue_pivot_df) print(dummy_df) print(merged_df)"},{"question":"You are required to write a function that uses the `pprint` module to pretty-print a complex nested data structure. The structure should be both printed to the console and returned as a formatted string. Additionally, your function should allow customization of several formatting options. # Function Signature: ```python def custom_pretty_print(data: dict, indent: int=1, width: int=80, depth: int=None, compact: bool=False, sort_dicts: bool=True, underscore_numbers: bool=False) -> str: ``` # Parameters: - `data` (dict): A dictionary representing the data structure to be pretty-printed. - `indent` (int): The amount of indentation added for each nesting level (default is 1). - `width` (int): The desired maximum number of characters per line in the output (default is 80). - `depth` (int, optional): Controls the number of nesting levels that may be printed (default is None, meaning no constraint). - `compact` (bool): If `True`, as many items as will fit within the `width` will be formatted on each line (default is False). - `sort_dicts` (bool): If `True`, dictionaries will be formatted with their keys sorted (default is True). - `underscore_numbers` (bool): If `True`, integers will be formatted with \\"_\\" as a thousands separator (default is False). # Returns: - `str`: The pretty-printed representation of the data structure. # Constraints: - You must use the `pprint` module for pretty-printing. - You should use `PrettyPrinter`\'s methods rather than shortcut functions like `pprint.pprint`. # Example Usage: ```python data = {\'a\': [1, 2, {\'b\': 3, \'c\': [4, 5, {\'d\': 6}]}], \'e\': {\'f\': 7, \'g\': 8}} result = custom_pretty_print(data, indent=2, width=50, depth=3, compact=True, sort_dicts=False) print(result) # Example Output: # { # \'a\': [1, 2, {\'b\': 3, \'c\': [4, 5, {...}]}], # \'e\': {\'f\': 7, \'g\': 8} # } ``` In this example, the data structure is customized to fit within a width of 50 characters, with 2 spaces of indentation for each level, limited to 3 levels of nesting. Your implementation should handle all specified parameters and ensure that the output follows the requested formatting options.","solution":"import pprint def custom_pretty_print(data: dict, indent: int=1, width: int=80, depth: int=None, compact: bool=False, sort_dicts: bool=True, underscore_numbers: bool=False) -> str: Pretty-prints and returns the formatted string of a complex nested data structure. Parameters: - data (dict): The data structure to be pretty-printed. - indent (int): Indentation level. - width (int): Maximum number of characters per line. - depth (int, optional): Number of nesting levels to print. - compact (bool): Minimize the number of lines used. - sort_dicts (bool): Sort dictionary keys. - underscore_numbers (bool): Use underscores as thousand separators in integers. Returns: - str: The pretty-printed data as a string. printer = pprint.PrettyPrinter(indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts, underscore_numbers=underscore_numbers) formatted_str = printer.pformat(data) print(formatted_str) return formatted_str"},{"question":"# Task Description You are tasked with implementing a Python function that mimics some of the sequence protocol-like functionality described in the provided documentation. Specifically, you need to implement a function that accepts a sequence and performs various operations specified by commands passed to it. # Function Signature ```python def sequence_operations(sequence, commands): :param sequence: Input sequence (list or tuple) :type sequence: list or tuple :param commands: List of commands to perform on the sequence :type commands: List of tuples, where each tuple is a command. The first element of each tuple is the command string, followed by arguments for the command. Possible commands and their arguments are: - \\"size\\": No arguments, returns the size of the sequence. - \\"concat\\": A single sequence to concatenate with the original sequence. - \\"repeat\\": An integer, the number of times to repeat the sequence. - \\"getitem\\": An integer index to get the item at that position. - \\"getslice\\": Two integer indices to get the slice of the sequence. - \\"count\\": An element to count the number of occurrences in the sequence. - \\"contains\\": An element to check if it is in the sequence. - \\"index\\": An element to find the first index of its occurrence in the sequence. :return: A list of results corresponding to each command in the order they were issued. :rtype: list pass ``` # Example Usage ```python seq = [1, 2, 3, 4, 2] commands = [ (\\"size\\",), (\\"concat\\", [5, 6]), (\\"repeat\\", 2), (\\"getitem\\", 3), (\\"getslice\\", 1, 4), (\\"count\\", 2), (\\"contains\\", 3), (\\"index\\", 2), ] print(sequence_operations(seq, commands)) # Output: [5, [1, 2, 3, 4, 2, 5, 6], [1, 2, 3, 4, 2, 1, 2, 3, 4, 2], 4, [2, 3, 4], 2, True, 1] ``` # Constraints 1. The sequence will always be a list or tuple. 2. The commands list will not be empty. 3. The commands will be well-formed. 4. Index and slice commands will always be within the valid range of the sequence. 5. The performance should be optimized for sequences with a length of up to 10^5. Ensure your implementation correctly handles each command and follows these constraints.","solution":"def sequence_operations(sequence, commands): results = [] for command in commands: operation = command[0] if operation == \\"size\\": results.append(len(sequence)) elif operation == \\"concat\\": results.append(sequence + command[1]) elif operation == \\"repeat\\": results.append(sequence * command[1]) elif operation == \\"getitem\\": results.append(sequence[command[1]]) elif operation == \\"getslice\\": results.append(sequence[command[1]:command[2]]) elif operation == \\"count\\": results.append(sequence.count(command[1])) elif operation == \\"contains\\": results.append(command[1] in sequence) elif operation == \\"index\\": results.append(sequence.index(command[1])) return results"},{"question":"# Device and Stream Management Assessment **Objective:** Write a Python function using the `torch.xpu` module to manage operations on multiple devices, handle random number generation, and monitor memory usage. **Task Description:** 1. **Device Initialization and Availability Check:** - Initialize the XPU environment. - Check if at least two devices are available. 2. **Random Number Generation:** - Set a manual seed for random number generation on each available device. 3. **Memory Management:** - Clear the memory cache on both devices. 4. **Stream and Operations:** - Create different streams for each device. - On each stream, perform a simple operation that utilizes random number generation (e.g., allocating random tensors and performing some operations on them). - Synchronize streams to ensure all operations are completed. 5. **Report Memory Usage:** - Output the memory allocated on each device after performing the operations. **Function Signature:** ```python def manage_xpu_devices_and_streams(seed: int) -> dict: Manage XPU devices, perform operations on streams, and monitor memory usage. Args: - seed (int): The manual seed for random number generation. Returns: - dict: A dictionary with device ids as keys and their corresponding memory allocated (in bytes) as values. ``` **Input:** - `seed`: An integer to set the manual seed for random number generation. **Output:** - A dictionary with device IDs as keys and the memory allocated on the device (in bytes) as values after performing the operations. **Example:** ```python result = manage_xpu_devices_and_streams(42) print(result) # Output might be {0: 12345678, 1: 12345678} ``` **Constraints:** - Ensure synchronizations are properly handled to avoid race conditions. - Handle exceptions for cases where devices or streams are not available. - Perform operations on at least two devices. **Hints:** - Use `torch.xpu.is_available()` to check for device availability. - Use `torch.xpu.set_device()` to operate on a specific device. - Use `torch.xpu.stream()` to create and manage streams. - Use `torch.xpu.memory_allocated()` to get the allocated memory. Good luck!","solution":"import torch def manage_xpu_devices_and_streams(seed: int) -> dict: Manage XPU devices, perform operations on streams, and monitor memory usage. Args: - seed (int): The manual seed for random number generation. Returns: - dict: A dictionary with device ids as keys and their corresponding memory allocated (in bytes) as values. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU is not available.\\") device_count = torch.xpu.device_count() if device_count < 2: raise RuntimeError(\\"At least two XPU devices are required.\\") memory_usage = {} streams = [] for device_id in range(device_count): torch.xpu.set_device(device_id) torch.manual_seed(seed) torch.xpu.empty_cache() stream = torch.xpu.Stream(device_id) streams.append(stream) for device_id in range(2): torch.xpu.set_device(device_id) stream = streams[device_id] with torch.xpu.stream(stream): tensor = torch.rand((1000, 1000), device=torch.xpu.current_device()) tensor = tensor * 2 stream.synchronize() memory_usage[device_id] = torch.xpu.memory_allocated(device_id) return memory_usage"},{"question":"Objective: Implement a function that generates a noisy signal, applies multiple window functions from `torch.signal.windows` for smoothing, and then compares their effects. Task: Write a function `compare_window_functions` that takes the following inputs: 1. `signal`: A 1D tensor representing the noisy signal. 2. `window_length`: An integer specifying the length of the window to be applied. Your function should: 1. Generate windows using the available window functions in `torch.signal.windows`. 2. Apply each generated window to the signal using element-wise multiplication. 3. Return a dictionary where keys are the names of the window functions and values are the corresponding smoothed signals. Function Signature: ```python import torch def compare_window_functions(signal: torch.Tensor, window_length: int) -> dict: # Your code here pass ``` Example: ```python import torch signal = torch.randn(100) # Generate a random noisy signal of length 100 window_length = 20 result = compare_window_functions(signal, window_length) # Result should be a dictionary where each key is the name of a window function # and each value is the smoothed signal obtained by applying that window function. ``` Constraints: 1. You may assume that `signal` length is greater than or equal to `window_length`. 2. The windows generated should have the same length as `window_length`. Notes: 1. This question requires a basic understanding of signal processing concepts and usage of window functions. 2. Ensure that you handle edge cases where the length of the signal is shorter than the `window_length`. 3. Comment your code to explain the logic behind each step. Performance: - The function should be efficient in terms of both time and space complexity, given that the length of the signal can be large.","solution":"import torch def compare_window_functions(signal: torch.Tensor, window_length: int) -> dict: Given a noisy signal, applies multiple window functions for smoothing and compares their effects. Args: signal (torch.Tensor): A 1D tensor representing the noisy signal. window_length (int): An integer specifying the length of the window to be applied. Returns: dict: A dictionary where keys are the names of the window functions and values are the corresponding smoothed signals. if signal.numel() < window_length: raise ValueError(\\"Signal length must be greater than or equal to window_length.\\") # List of available window functions in torch window_functions = { \'hann\': torch.hann_window, \'hamming\': torch.hamming_window, \'blackman\': torch.blackman_window, \'bartlett\': torch.bartlett_window } smoothed_signals = {} for name, window_func in window_functions.items(): window = window_func(window_length, periodic=False) padded_window = torch.cat([window, torch.zeros(signal.size(0) - window_length)]) smoothed_signal = torch.fft.ifft(torch.fft.fft(signal) * torch.fft.fft(padded_window)).real smoothed_signals[name] = smoothed_signal return smoothed_signals"},{"question":"# Introduction Profiling and debugging are essential tasks in software development, ensuring code runs efficiently and correctly. In this task, you will use Python\'s profiling and tracing capabilities to implement a function that profiles the execution time and memory usage of a given target function. # Problem Statement Implement a function `profile_function` that takes a function `target_func` and its arguments `*args` and `**kwargs`, then: 1. Uses the `cProfile` module to profile the execution time of `target_func`. 2. Uses the `tracemalloc` module to trace memory allocations of `target_func`. 3. Returns a dictionary with profiling statistics including total execution time and memory usage statistics. # Detailed Requirements 1. **Function Signature** ```python def profile_function(target_func: Callable, *args, **kwargs) -> dict: ``` 2. **Input** - `target_func`: The function to be profiled. - `*args`: Positional arguments to pass to `target_func`. - `**kwargs`: Keyword arguments to pass to `target_func`. 3. **Output** - A dictionary containing: - `execution_time`: The total execution time in seconds of `target_func`. - `memory_stats`: A list of the top 10 memory allocations. 4. **Constraints** - You may assume `target_func` and its arguments are well-behaved. # Example ```python import time def sample_func(n): time.sleep(n) return n * n # Expected output may vary since the exact time and memory usage can differ results = profile_function(sample_func, 2) print(results) ``` Output: ```python { \'execution_time\': 2.002, \'memory_stats\': [ # Top 10 memory allocations ] } ``` # Notes - You should use the `cProfile` module for profiling the function\'s execution time. - Use the `tracemalloc` module to track memory usage. - Ensure that your implementation is robust and does not leave profiling or memory tracing processes running after function completion. # Guidance Here are some resources to help you: - [cProfile](https://docs.python.org/3/library/profile.html) - [tracemalloc](https://docs.python.org/3/library/tracemalloc.html)","solution":"import cProfile import tracemalloc import pstats from typing import Callable def profile_function(target_func: Callable, *args, **kwargs) -> dict: # Start tracing memory allocations tracemalloc.start() # Start profiling profiler = cProfile.Profile() profiler.enable() # Execute the target function result = target_func(*args, **kwargs) # Stop profiling and memory tracing profiler.disable() current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() # Get the profiling statistics stats = pstats.Stats(profiler).sort_stats(\'cumulative\') total_time = stats.total_tt # Create a dictionary with profiling statistics profile_stats = { \'execution_time\': total_time, \'memory_stats\': { \'current_memory_usage\': current, \'peak_memory_usage\': peak } } return profile_stats"},{"question":"You are provided with a toy dataset from the scikit-learn library. Your task is to implement a function that loads the `wine` dataset, preprocesses it, trains a machine learning model, and evaluates its performance. # Requirements: 1. **Function Name**: `train_wine_classifier` 2. **Input Parameter**: None 3. **Output**: A dictionary containing: - \'accuracy\': Accuracy score of the trained model on the test set. - \'precision\': Precision score of the trained model on the test set. - \'recall\': Recall score of the trained model on the test set. - \'f1_score\': F1 score of the trained model on the test set. # Constraints: - You must use the `load_wine` function from `sklearn.datasets` to load the wine dataset. - Use an 80-20 train-test split for the dataset. - Normalize the features using `StandardScaler`. - Train a `RandomForestClassifier` from `sklearn.ensemble`. - Evaluate the model performance using `accuracy_score`, `precision_score`, `recall_score`, and `f1_score` from `sklearn.metrics`. # Implementation Details: 1. Load the `wine` dataset using `load_wine`. 2. Split the dataset into training and testing sets (80-20 split). 3. Normalize the feature values using `StandardScaler`. 4. Train a `RandomForestClassifier` on the training data. 5. Make predictions on the test data. 6. Calculate and return the accuracy, precision, recall, and F1 score. # Example: ```python def train_wine_classifier(): from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Step 1: Load dataset data = load_wine() X, y = data.data, data.target # Step 2: Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Normalize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train the model model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) # Step 5: Make predictions y_pred = model.predict(X_test) # Step 6: Evaluate model results = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'weighted\') } return results # Example output print(train_wine_classifier()) ```","solution":"def train_wine_classifier(): from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Step 1: Load dataset data = load_wine() X, y = data.data, data.target # Step 2: Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Normalize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train the model model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) # Step 5: Make predictions y_pred = model.predict(X_test) # Step 6: Evaluate model results = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'weighted\') } return results"},{"question":"Objective: To assess the student\'s understanding of merging and concatenating data using pandas, with both fundamental and advanced concepts applied in a practical scenario. Problem Statement: You are given multiple datasets representing different aspects of retail transactions. Your task is to merge and concatenate these datasets to create a unified DataFrame for analysis. Given DataFrames: 1. `transactions`: ```python transactions = pd.DataFrame({ \\"transaction_id\\": [1001, 1002, 1003, 1004, 1005], \\"product_id\\": [1, 2, 3, 2, 1], \\"quantity\\": [5, 2, 1, 3, 4], \\"timestamp\\": pd.to_datetime([ \\"2023-01-01 10:00:00\\", \\"2023-01-01 11:00:00\\", \\"2023-01-01 12:00:00\\", \\"2023-01-01 13:00:00\\", \\"2023-01-01 14:00:00\\" ]) }) ``` 2. `products`: ```python products = pd.DataFrame({ \\"product_id\\": [1, 2, 3, 4], \\"name\\": [\\"Pen\\", \\"Notebook\\", \\"Pencil\\", \\"Eraser\\"], \\"category\\": [\\"Stationery\\", \\"Stationery\\", \\"Stationery\\", \\"Stationery\\"] }) ``` 3. `store_info`: ```python store_info = pd.DataFrame({ \\"store_id\\": [1, 2], \\"location\\": [\\"Downtown\\", \\"Uptown\\"], \\"manager\\": [\\"Alice\\", \\"Bob\\"] }) ``` 4. `price_changes`: ```python price_changes = pd.DataFrame({ \\"timestamp\\": pd.to_datetime([ \\"2023-01-01 09:59:00\\", \\"2023-01-01 10:59:00\\", \\"2023-01-01 12:00:30\\" ]), \\"product_id\\": [1, 2, 1], \\"price\\": [10.0, 12.0, 9.5] }) ``` Task: 1. Merge the `transactions` DataFrame with the `products` DataFrame to get the product details for each transaction. Keep all transactions, even if there is no matching product. 2. Concatenate the `store_info` DataFrame to the result from the previous step along the columns such that each transaction is associated with each store. 3. Use an asof merge to assign the most recent price to each transaction from the `price_changes` DataFrame based on the `timestamp` and `product_id`. 4. Output the final merged DataFrame. Constraints: - The merge between `transactions` and `products` should be a left join. - The concatenation should match each transaction with all store information. - Ensure that the asof merge only considers price changes that occurred before or exactly at the transaction timestamp. Expected Output: The final DataFrame should have the following columns: - `transaction_id` - `product_id` - `name` - `category` - `quantity` - `timestamp` - `store_id` - `location` - `manager` - `price` Each row should represent a transaction, with corresponding product details, store information, and the relevant price. Function Signature: ```python def merge_and_concatenate_data( transactions: pd.DataFrame, products: pd.DataFrame, store_info: pd.DataFrame, price_changes: pd.DataFrame ) -> pd.DataFrame: pass ```","solution":"import pandas as pd def merge_and_concatenate_data( transactions: pd.DataFrame, products: pd.DataFrame, store_info: pd.DataFrame, price_changes: pd.DataFrame ) -> pd.DataFrame: # Step 1: Merge transactions with products using left join merged_df = pd.merge(transactions, products, on=\\"product_id\\", how=\\"left\\") # Step 2: Cross join merged DataFrame with store_info merged_df[\\"key\\"] = 1 store_info[\\"key\\"] = 1 concatenated_df = pd.merge(merged_df, store_info, on=\\"key\\").drop(\\"key\\", axis=1) # Step 3: Sort price_changes by timestamp to prepare for asof merge price_changes_sorted = price_changes.sort_values(by=\\"timestamp\\") # Step 4: Use asof merge to assign the most recent price to each transaction concatenated_df = pd.merge_asof( concatenated_df.sort_values(\\"timestamp\\"), price_changes_sorted, by=\\"product_id\\", on=\\"timestamp\\", direction=\\"backward\\" ) # Reorder columns as required final_columns = [ \\"transaction_id\\", \\"product_id\\", \\"name\\", \\"category\\", \\"quantity\\", \\"timestamp\\", \\"store_id\\", \\"location\\", \\"manager\\", \\"price\\" ] return concatenated_df[final_columns]"},{"question":"Objective: Implement a function that takes a list of HTTP status codes and returns a dictionary where the keys are the status codes, and the values are dictionaries containing the phrase and description for each status code. # Detailed Requirements: 1. **Function Name**: `http_status_details` 2. **Input**: - A list of integers, each representing an HTTP status code. 3. **Output**: - A dictionary where each key is an input status code (as an integer), and the corresponding value is another dictionary with two key-value pairs: * \\"phrase\\": The phrase associated with the HTTP status code. * \\"description\\": The description associated with the HTTP status code. 4. **Constraints**: - The input list will contain valid HTTP status codes as defined in `http.HTTPStatus`. - The function should correctly handle all status codes described in the documentation. 5. **Performance**: - The function should have a time complexity of O(n), where n is the number of status codes in the input list. # Example: ```python from http import HTTPStatus def http_status_details(status_codes): result = {} for code in status_codes: status = HTTPStatus(code) result[code] = {\\"phrase\\": status.phrase, \\"description\\": status.description} return result # Test the function status_codes = [200, 404, 418] print(http_status_details(status_codes)) ``` Expected Output: ```python { 200: {\\"phrase\\": \\"OK\\", \\"description\\": \\"Request fulfilled, document follows\\"}, 404: {\\"phrase\\": \\"Not Found\\", \\"description\\": \\"Nothing matches the given URI\\"}, 418: {\\"phrase\\": \\"I\'m a teapot\\", \\"description\\": \\"I\'m a teapot\\"} } ``` Notes: - You must import the `HTTPStatus` class from the `http` module to access the status codes and their attributes. - Ensure your code handles and outputs the details for each status code in the input list accurately.","solution":"from http import HTTPStatus def http_status_details(status_codes): Returns a dictionary with the HTTP status codes as keys and their corresponding phrase and description as values. Args: status_codes (list): List of HTTP status codes (integers) Returns: dict: Dictionary with status codes as keys and dictionaries of phrase and description as values. result = {} for code in status_codes: status = HTTPStatus(code) result[code] = {\\"phrase\\": status.phrase, \\"description\\": status.description} return result"},{"question":"Objective: To assess the understanding of Python\'s `pkgutil` module and its utilities for managing packages and submodules. Problem Statement: You are tasked with analyzing Python packages and modules available in a given directory. Specifically, you need to create a function that will list all the submodules of a specified package and retrieve a specific resource from a given submodule. Requirements: 1. Implement a function `list_submodules_and_get_resource(package_path: str, package_name: str, submodule_name: str, resource_name: str) -> Tuple[List[str], Optional[bytes]]` that: - Lists all the submodules within a given package. - Retrieves a specific resource (file) from one of the submodules. 2. The function should: - Use the `pkgutil.iter_modules` to list all the submodules of the provided package. - Use the `pkgutil.get_data` to retrieve the content of a specified resource from one of the submodules. Input: - `package_path` (string): The path to the directory containing the package. - `package_name` (string): The name of the package. - `submodule_name` (string): The name of the submodule from which to retrieve the resource. - `resource_name` (string): The name of the resource to retrieve from the submodule. Output: - A tuple containing: 1. A list of strings representing the names of all submodules within the specified package. 2. The content of the specified resource as bytes, or `None` if the resource is not found. Constraints: - If the package or submodule does not exist, the function should return an empty list for submodules and `None` for the resource content. - The function should handle any exceptions gracefully and should not crash. Example Usage: ```python from typing import Tuple, List, Optional def list_submodules_and_get_resource(package_path: str, package_name: str, submodule_name: str, resource_name: str) -> Tuple[List[str], Optional[bytes]]: # Your implementation here # Example call to the function submodules, resource_content = list_submodules_and_get_resource(\'/path/to/package\', \'mypackage\', \'mysubmodule\', \'data.txt\') print(submodules) # Output: [\'mysubmodule1\', \'mysubmodule2\', \'mysubmodule\'] print(resource_content) # Output: b\'Content of data.txt\' (if the resource is found) or None ``` Notes: - Ensure the code is properly tested with valid paths and resource names. - It is recommended to use the `pkgutil.iter_modules` to list submodules and `pkgutil.get_data` to retrieve resource content.","solution":"import os import pkgutil from typing import Tuple, List, Optional def list_submodules_and_get_resource(package_path: str, package_name: str, submodule_name: str, resource_name: str) -> Tuple[List[str], Optional[bytes]]: Lists all submodules within a specified package and retrieves a specified resource from one of the submodules. Parameters: - package_path (str): The path to the directory containing the package. - package_name (str): The name of the package. - submodule_name (str): The name of the submodule from which to retrieve the resource. - resource_name (str): The name of the resource to retrieve from the submodule. Returns: - Tuple[List[str], Optional[bytes]]: A tuple containing a list of submodule names and the content of the resource (as bytes) or None if the resource is not found. submodule_names = [] resource_content = None try: # Add the package path to the system path if package_path not in os.sys.path: os.sys.path.insert(0, package_path) # List submodules package = __import__(package_name) package_path = package.__path__ for module_info in pkgutil.iter_modules(package_path): submodule_names.append(module_info.name) # Retrieve resource content resource_content = pkgutil.get_data(f\\"{package_name}.{submodule_name}\\", resource_name) except Exception as e: print(f\\"An error occurred: {e}\\") return submodule_names, resource_content"},{"question":"# Advanced Coding Assessment: Numeric Operations and Custom Iterable Implementation **Objective:** Demonstrate understanding of Python\'s numeric operations, custom sequence behaviors, and usage of mappings. **Problem Statement:** 1. **Part 1: Numeric Operations** - Implement a function `bytes_to_int_conversion(byte_data: bytes, byteorder: str, signed: bool) -> int` that converts a `bytes` object (`byte_data`) to an integer, using the specified `byteorder` (`\'big\'` or `\'little\'`) and signed/unsigned specification (`signed` being `True` or `False`). - Implement a function `int_to_bytes_conversion(value: int, length: int, byteorder: str, signed: bool) -> bytes` that converts an integer (`value`) to a `bytes` object of specified `length`, using the specified `byteorder` and signed/unsigned specification. ```python def bytes_to_int_conversion(byte_data: bytes, byteorder: str, signed: bool) -> int: Converts a bytes object to an integer. :param byte_data: Bytes data to be converted. :param byteorder: \'big\' or \'little\' indicating the byte order. :param signed: Boolean indicating whether the integer is signed. :return: Converted integer. # Your code here def int_to_bytes_conversion(value: int, length: int, byteorder: str, signed: bool) -> bytes: Converts an integer to a bytes object. :param value: The integer to be converted. :param length: Number of bytes in the resulting bytes object. :param byteorder: \'big\' or \'little\' indicating byte order. :param signed: Boolean indicating if the integer is signed. :return: Bytes object representation of the integer. # Your code here ``` 2. **Part 2: Custom Iterable Implementation** - Create a custom iterable class `CustomRange` that mimics the behavior of Python\'s built-in `range` but with additional functionality. The CustomRange should support operations like slicing, iteration, and the ability to get the reversed version of the range. - Implement methods to support: - iteration (`__iter__`) - slicing (`__getitem__`) - length (`__len__`) - reversed iteration (`__reversed__`) ```python class CustomRange: def __init__(self, start: int, stop: int, step: int = 1): Initialize CustomRange instance. :param start: Starting value of the range. :param stop: Stopping value of the range. :param step: Step increment, defaults to 1. self.start = start self.stop = stop self.step = step def __iter__(self): Returns an iterator for the range. # Your code here def __getitem__(self, index): Supports indexing and slicing for the range. :param index: Integer index or slice object. :return: The item at index or a CustomRange if slicing. # Your code here def __len__(self): Returns the length of the range. # Your code here def __reversed__(self): Returns a reversed iterator for the range. # Your code here def __contains__(self, value): Check if a value is in the range. # Your code here # Example usage custom_range = CustomRange(1, 10, 2) for num in custom_range: print(num) # Should print 1, 3, 5, 7, 9 print(len(custom_range)) # Should print 5 print(custom_range[2]) # Should print 5 print(custom_range[1:4]) # Should print a CustomRange object that generates 3, 5, 7 print(list(reversed(custom_range))) # Should print [9, 7, 5, 3, 1] ``` **Input Format:** - For `bytes_to_int_conversion`: a `bytes` object, a `string` indicating byte order, and a `bool` indicating if the integer is signed. - For `int_to_bytes_conversion`: an `int` value, an `int` length, a `string` indicating byte order, and a `bool` indicating if the integer is signed. - For `CustomRange`: integers `start`, `stop`, and optionally `step`; methods within the class will then handle various input scenarios like iteration, slicing, and reversing. **Output Format:** - Functions will return converted integers or bytes. - The `CustomRange` class should behave as specified, generating appropriate range sequences. **Constraints:** - `byteorder` can only be `\'big\'` or `\'little\'`. - `signed` can only be `True` or `False`. - `start`, `stop`, and `step` in `CustomRange` must be integers. - For `CustomRange`: The step cannot be 0, and direction of range will dictated by step being positive or negative. **Performance Requirements:** - Implementations should efficiently handle large ranges and conversions without excessive memory or time complexity.","solution":"def bytes_to_int_conversion(byte_data: bytes, byteorder: str, signed: bool) -> int: Converts a bytes object to an integer. :param byte_data: Bytes data to be converted. :param byteorder: \'big\' or \'little\' indicating the byte order. :param signed: Boolean indicating whether the integer is signed. :return: Converted integer. return int.from_bytes(byte_data, byteorder, signed=signed) def int_to_bytes_conversion(value: int, length: int, byteorder: str, signed: bool) -> bytes: Converts an integer to a bytes object. :param value: The integer to be converted. :param length: Number of bytes in the resulting bytes object. :param byteorder: \'big\' or \'little\' indicating byte order. :param signed: Boolean indicating if the integer is signed. :return: Bytes object representation of the integer. return value.to_bytes(length, byteorder, signed=signed) class CustomRange: def __init__(self, start: int, stop: int, step: int = 1): Initialize CustomRange instance. :param start: Starting value of the range. :param stop: Stopping value of the range. :param step: Step increment, defaults to 1. if step == 0: raise ValueError(\\"step argument must not be zero\\") self.start = start self.stop = stop self.step = step def __iter__(self): Returns an iterator for the range. current = self.start if self.step > 0: while current < self.stop: yield current current += self.step else: while current > self.stop: yield current current += self.step def __getitem__(self, index): Supports indexing and slicing for the range. :param index: Integer index or slice object. :return: The item at index or a CustomRange if slicing. if isinstance(index, slice): start, stop, step = index.indices(len(self)) return CustomRange( self.start + start * self.step, self.start + stop * self.step, self.step * (step if step else 1) ) else: if index < 0: index += len(self) if index < 0 or index >= len(self): raise IndexError(\\"CustomRange index out of range\\") return self.start + index * self.step def __len__(self): Returns the length of the range. if self.step > 0: return max(0, (self.stop - self.start + self.step - 1) // self.step) else: return max(0, (self.start - self.stop - self.step - 1) // -self.step) def __reversed__(self): Returns a reversed iterator for the range. current = self.start + (len(self) - 1) * self.step if self.step > 0: while current >= self.start: yield current current -= self.step else: while current <= self.start: yield current current -= self.step def __contains__(self, value): Check if a value is in the range. if self.step > 0: return self.start <= value < self.stop and (value - self.start) % self.step == 0 else: return self.start >= value > self.stop and (value - self.start) % self.step == 0"},{"question":"**Question:** You are given a dataset that contains information about different car models, including features such as horsepower, weight, acceleration, fuel efficiency (mpg), origin, and model year. Your task is to create an insightful visualization using the `pandas.plotting` module that compares the fuel efficiency of cars from different origins over model years. The visualization should allow for easy comparison and highlight trends over time. **Dataset Description:** - `car_data.csv` - A CSV file with the following columns: - `Model`: String indicating the car model. - `Horsepower`: Integer indicating the horsepower of the car. - `Weight`: Integer indicating the weight of the car. - `Acceleration`: Float indicating the car\'s acceleration. - `MPG`: Float indicating the miles per gallon (fuel efficiency) of the car. - `Origin`: String indicating the origin of the car (e.g., USA, Europe, Japan). - `ModelYear`: Integer indicating the model year of the car. **Requirements:** 1. Load the dataset using pandas. 2. Clean the dataset (handle missing values, data types, etc.). 3. Create a visualization using the `parallel_coordinates` function from the `pandas.plotting` module that displays the trends of `MPG` across different `ModelYear` for each `Origin`. 4. Customize the plot for better readability: - Use different colors for different origins. - Add appropriate titles and labels. - Enhance the plot with any other pandas plotting functions as necessary. **Expected Input and Output Formats:** *Input:* - `car_data.csv` file with the structure described above. *Output:* - A `.png` file named `car_fuel_efficiency_trends.png` containing the parallel coordinates plot. **Constraints:** - Ensure that your code is efficient and can handle the dataset with up to 100,000 rows. - Handle any possible missing or inconsistent data points in the dataset adequately. **Note:** - You are required to submit your complete code in a `.py` file and the output visualization as a `.png` file. - Provide comments in your code to explain the key steps. *Example Call:* ```python import pandas as pd # Load the dataset data = pd.read_csv(\'car_data.csv\') # Clean the dataset # ... [Data cleaning steps] # Create the visualization # ... [Code to generate parallel coordinates plot] # Save the plot plt.savefig(\'car_fuel_efficiency_trends.png\') ```","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import parallel_coordinates def visualize_car_fuel_efficiency(file_path): Generates a parallel coordinates plot comparing fuel efficiency trends over the years for different car origins. Parameters: - file_path (str): The path to the \'car_data.csv\' file. Returns: - None: The function saves the plot as \'car_fuel_efficiency_trends.png\'. # Load the dataset data = pd.read_csv(file_path) # Data cleaning: drop rows with missing \'MPG\' or \'Origin\' or \'ModelYear\' values data = data.dropna(subset=[\'MPG\', \'Origin\', \'ModelYear\']) # Ensure correct data types data[\'ModelYear\'] = data[\'ModelYear\'].astype(int) data[\'Origin\'] = data[\'Origin\'].astype(str) # Create a parallel coordinates plot plt.figure(figsize=(12, 6)) parallel_coordinates(data[[\'MPG\', \'Origin\', \'ModelYear\']], class_column=\'Origin\', cols=[\'MPG\', \'ModelYear\']) # Customize the plot plt.title(\'Fuel Efficiency Trends Over Model Years by Car Origin\') plt.xlabel(\'Attributes\') plt.ylabel(\'Values\') plt.legend(title=\'Car Origin\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') # Save the plot plt.savefig(\'car_fuel_efficiency_trends.png\') plt.close() # Note: The function visualize_car_fuel_efficiency(file_path) should be called with the path to the dataset as argument."},{"question":"# **Coding Assessment Question** Objective: You are given an AIFF or AIFF-C audio file, and you need to extract key audio parameters and modify the audio data. Your task is to write a Python function using the `aifc` module that performs the following steps: 1. Open an input AIFF or AIFF-C file and read the following parameters: - Number of channels - Sample width (in bytes) - Frame rate - Number of frames 2. Extract all the audio frames from the file. 3. Create a new AIFF or AIFF-C file that contains the original audio data but with the sample width doubled (e.g., 8-bit samples should become 16-bit samples). Requirements: - You must use the `aifc` module for reading and writing audio files. - The new audio file should retain the same number of channels, frame rate, and frame count as the original file. - You may assume that the input file uses no compression (`comptype` is `b\'NONE\'`). Function Signature: ```python def process_audio(input_file: str, output_file: str) -> None: Processes an AIFF or AIFF-C audio file to double its sample width and saves it as a new file. Parameters: - input_file (str): Path to the input AIFF or AIFF-C file. - output_file (str): Path to the output AIFF or AIFF-C file where the modified audio will be saved. Returns: - None # Example usage: # process_audio(\'input.aiff\', \'output.aiff\') ``` Example: Assume `input.aiff` is a stereo audio file with a sample width of 2 bytes, frame rate of 44100 Hz, and 100000 frames. The expected output file `output.aiff` should be a similar audio file but with the sample width doubled to 4 bytes while maintaining the other parameters. Input: `input.aiff` - Number of channels: 2 - Sample width: 2 - Frame rate: 44100 - Number of frames: 100000 Output: `output.aiff` - Number of channels: 2 - Sample width: 4 - Frame rate: 44100 - Number of frames: 100000 Constraints: - You should handle the possibility of file operations failing (e.g., file not found). - Make sure to handle the conversion of audio data correctly when doubling the sample width (e.g., handle byte manipulation appropriately). Performance: - The solution should efficiently handle large audio files (e.g., several minutes of audio at CD quality).","solution":"import aifc import struct def process_audio(input_file: str, output_file: str) -> None: with aifc.open(input_file, \'rb\') as infile: n_channels = infile.getnchannels() sample_width = infile.getsampwidth() frame_rate = infile.getframerate() n_frames = infile.getnframes() if sample_width not in [1, 2]: raise ValueError(\\"Sample width should be 1 or 2 bytes only.\\") audio_frames = infile.readframes(n_frames) if sample_width == 1: # 8-bit samples format_string = f\'{n_frames * n_channels}B\' audio_data = struct.unpack(format_string, audio_frames) new_audio_data = [] for sample in audio_data: new_audio_data.extend([sample, 0]) # Convert to 16-bit (little-endian) new_audio_frames = struct.pack(f\'{len(new_audio_data)}B\', *new_audio_data) elif sample_width == 2: # 16-bit samples format_string = f\'{n_frames * n_channels}h\' audio_data = struct.unpack(format_string, audio_frames) new_audio_data = [] for sample in audio_data: new_audio_data.extend([sample, 0]) # Convert to 32-bit (little-endian) new_audio_frames = struct.pack(f\'{len(new_audio_data)}h\', *new_audio_data) with aifc.open(output_file, \'wb\') as outfile: outfile.setnchannels(n_channels) outfile.setsampwidth(sample_width * 2) outfile.setframerate(frame_rate) outfile.setnframes(n_frames) outfile.writeframes(new_audio_frames)"},{"question":"**Question:** You have been tasked to create a custom representation of binary trees using the `reprlib` module. Your goal is to define a class `BinaryTreeNode` and a function `custom_repr` that provides a controlled representation of a binary tree. Your custom string representation should limit the depth of recursion to display the tree up to a specified depth, and use an ellipsis (\\"...\\") if a node contains further descendants beyond this depth. # Task 1. **Define a class `BinaryTreeNode`**: - It should have three attributes: `value` (the value of the node), and `left` and `right` (pointers to child nodes, which are also `BinaryTreeNode` instances or `None`). - Implement the `__init__` method to initialize these attributes. - Implement the `__repr__` method for the node using `@reprlib.recursive_repr(fillvalue=\\"...\\")` decorator. 2. **Implement `BinaryTreeRepr` class**: - Subclass the `reprlib.Repr` class. - Override the `repr_BinaryTreeNode` method to provide a formatted string for `BinaryTreeNode` instances with a controlled depth (using the `maxlevel` attribute). 3. **Implement `custom_repr(tree)` function**: - This function should take a `BinaryTreeNode` instance as a parameter and return a string representation using an instance of your custom `BinaryTreeRepr` class. # Input Format - The input to `BinaryTreeNode.__init__` will be an integer (value of the node). - Left and right children should be either `BinaryTreeNode` instances or `None`. # Output Format - The `custom_repr` function should return a string representing the binary tree, limited to the specified `maxlevel` depth. # Constraints - Assume the tree does not include any cyclic references. - Implement handling for large binary trees to avoid excessive output. # Example ```python class BinaryTreeNode: @reprlib.recursive_repr(fillvalue=\\"...\\") def __repr__(self): return f\\"BinaryTreeNode({self.value}, {self.left}, {self.right})\\" class BinaryTreeRepr(reprlib.Repr): def repr_BinaryTreeNode(self, node, level): if level <= 0: return \\"...\\" if not node: return \\"None\\" left_repr = self.repr1(node.left, level - 1) right_repr = self.repr1(node.right, level - 1) return f\\"BinaryTreeNode({node.value}, {left_repr}, {right_repr})\\" def custom_repr(tree: BinaryTreeNode, maxlevel: int = 2): repr_instance = BinaryTreeRepr() repr_instance.maxlevel = maxlevel return repr_instance.repr(tree) # Example usage: root = BinaryTreeNode(1) root.left = BinaryTreeNode(2) root.right = BinaryTreeNode(3) root.left.left = BinaryTreeNode(4) root.left.right = BinaryTreeNode(5) root.right.right = BinaryTreeNode(6) print(custom_repr(root, maxlevel=2)) ``` **Expected Output**: ``` BinaryTreeNode(1, BinaryTreeNode(2, ..., ...), BinaryTreeNode(3, None, ...)) ```","solution":"import reprlib class BinaryTreeNode: def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right @reprlib.recursive_repr(fillvalue=\\"...\\") def __repr__(self): return f\\"BinaryTreeNode({self.value}, {self.left}, {self.right})\\" class BinaryTreeRepr(reprlib.Repr): def repr_BinaryTreeNode(self, node, level): if level <= 0: return \\"...\\" if not node: return \\"None\\" left_repr = self.repr1(node.left, level - 1) right_repr = self.repr1(node.right, level - 1) return f\\"BinaryTreeNode({node.value}, {left_repr}, {right_repr})\\" def custom_repr(tree, maxlevel=2): repr_instance = BinaryTreeRepr() repr_instance.maxlevel = maxlevel return repr_instance.repr(tree) # Example usage: root = BinaryTreeNode(1) root.left = BinaryTreeNode(2) root.right = BinaryTreeNode(3) root.left.left = BinaryTreeNode(4) root.left.right = BinaryTreeNode(5) root.right.right = BinaryTreeNode(6) print(custom_repr(root, maxlevel=2))"},{"question":"Create a Python function using pandas that accepts a dataframe with mixed data types and performs the following operations: 1. Convert all suitable columns in the dataframe to categorical data type. 2. Add a new categorical column named \'rating\' that consists of 5 bins labeled as \\"Very Bad\\", \\"Bad\\", \\"OK\\", \\"Good\\", and \\"Very Good\\" based on the quartile ranges of the numeric data of an existing specified column. 3. Rename the categories of another specified categorical column to a given set of new categories. 4. Ensure the specified categorical column is treated as an ordered categorical and sort the dataframe based on this column. 5. Generate a summary of the categorical columns showing their categories and the number of occurrences of each category. 6. Handle any missing values by replacing them with the most frequent category in their respective columns. **Function Signature:** ```python def process_categorical_data( df: pd.DataFrame, numeric_column_for_rating: str, categorical_column_to_rename: str, new_categories: dict ) -> pd.DataFrame: Processes the dataframe by performing various categorical operations. Parameters: df (pd.DataFrame): Input dataframe with mixed datatypes. numeric_column_for_rating (str): The column name for which the \'rating\' column is added. categorical_column_to_rename (str): The column which will have its categories renamed. new_categories (dict): A dictionary mapping old categories to new categories for the specified column. Returns: pd.DataFrame: The modified dataframe with processed categorical data. pass ``` **Constraints:** - The dataframe `df` can have any number of columns and data types. - The `numeric_column_for_rating` should exist and contain numeric data. - The `categorical_column_to_rename` should exist and have categorical or string data. **Example:** Given the input dataframe `df`: | A | B | C | rating (added automatically) | |-------|-----|---|-----------------------------| | high | 23 | x | - | | low | 45 | y | - | | medium| 67 | z | - | and function call: ```python new_categories = {\\"high\\": \\"H\\", \\"low\\": \\"L\\", \\"medium\\": \\"M\\"} result_df = process_categorical_data(df, \'B\', \'A\', new_categories) ``` The output should be a dataframe similar to: | A | B | C | rating | |-------|---|---|--------------| | H | 23| x | Very Bad | | L | 45| y | Bad | | M | 67| z | Good | where: - Column `A` is converted to ordered categorical and renamed. - A new ordered categorical column `rating` is added based on column `B`. **Notes:** - Ensure to replace any missing values in categorical columns. - The original ordering should be preserved where not specified otherwise.","solution":"import pandas as pd def process_categorical_data( df: pd.DataFrame, numeric_column_for_rating: str, categorical_column_to_rename: str, new_categories: dict ) -> pd.DataFrame: Processes the dataframe by performing various categorical operations. Parameters: df (pd.DataFrame): Input dataframe with mixed datatypes. numeric_column_for_rating (str): The column name for which the \'rating\' column is added. categorical_column_to_rename (str): The column which will have its categories renamed. new_categories (dict): A dictionary mapping old categories to new categories for the specified column. Returns: pd.DataFrame: The modified dataframe with processed categorical data. # Convert all suitable columns to categorical data type for col in df.select_dtypes(include=[\'object\']).columns: df[col] = df[col].astype(\'category\') # Add a new categorical column named \'rating\' based on the numeric_column_for_rating labels = [\\"Very Bad\\", \\"Bad\\", \\"OK\\", \\"Good\\", \\"Very Good\\"] df[\'rating\'] = pd.qcut(df[numeric_column_for_rating], q=5, labels=labels, duplicates=\'drop\') # Rename the categories of the specified categorical column df[categorical_column_to_rename] = df[categorical_column_to_rename].cat.rename_categories(new_categories) # Ensure the specified categorical column is an ordered categorical df[categorical_column_to_rename] = df[categorical_column_to_rename].cat.as_ordered() # Sort the dataframe based on the ordered categorical column df = df.sort_values(by=categorical_column_to_rename) # Replace missing values with the most frequent category in their respective columns for col in df.select_dtypes(include=[\'category\']).columns: most_frequent = df[col].mode().iloc[0] df[col].fillna(most_frequent, inplace=True) return df"},{"question":"Advanced Color Palettes with Seaborn Objective You are tasked with creating a custom color palette using the `seaborn` library and applying it to a data visualization. Background Seaborn allows for the creation of custom color palettes using the `seaborn.blend_palette` function. This function can take a list of color specifications and blend them to create either a discrete color palette or a continuous colormap. Task 1. **Color Palette Creation**: - Write a function `create_custom_palette(colors: list[str], as_cmap: bool = False) -> sns.palettes._ColorPalette` that: - Takes a list of colors (`colors`) as input. - Accepts an optional boolean parameter `as_cmap` which defaults to `False`. - Returns a Seaborn color palette or colormap based on the input. 2. **Visualization Application**: - Using the `create_custom_palette` function, create a continuous colormap from at least three color specifications of your choice. - Load a sample dataset (e.g., `sns.load_dataset(\'iris\')`). - Create a Seaborn scatter plot (`sns.scatterplot`) using the custom colormap, where the colors of the points represent one of the continuous variables in the dataset (e.g., petal_length). Constraints - Use at least three colors in your palette. - Ensure that the color palette is visually distinguishable and suitable for the dataset visualization. Example Here\'s an example of how your function might be used: ```python import seaborn as sns import matplotlib.pyplot as plt # Example colors colors = [\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"] # Create a continuous colormap cmap = create_custom_palette(colors, as_cmap=True) # Load sample dataset iris = sns.load_dataset(\'iris\') # Create scatter plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=iris, x=\'sepal_width\', y=\'sepal_length\', hue=\'petal_length\', palette=cmap) plt.title(\\"Iris Sepal Dimensions with Custom Colormap\\") plt.show() ``` Expected Output The function and the subsequent seaborn visualization should be correctly implemented. The scatter plot should use the custom colormap to represent the specified continuous variable. Your submission should include the implementation of the `create_custom_palette` function and the code to generate the described visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette(colors: list[str], as_cmap: bool = False) -> sns.palettes._ColorPalette: Creates a custom Seaborn color palette or colormap based on the input colors. Args: colors (list of str): A list of color specifications. as_cmap (bool): Whether to return a continuous colormap. Defaults to False. Returns: sns.palettes._ColorPalette: The created color palette or colormap. return sns.blend_palette(colors, as_cmap=as_cmap) # Example usage colors = [\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"] # Create a continuous colormap cmap = create_custom_palette(colors, as_cmap=True) # Load sample dataset iris = sns.load_dataset(\'iris\') # Create scatter plot plt.figure(figsize=(8, 6)) scatter = sns.scatterplot(data=iris, x=\'sepal_width\', y=\'sepal_length\', hue=\'petal_length\', palette=cmap) plt.title(\\"Iris Sepal Dimensions with Custom Colormap\\") plt.show()"},{"question":"# Question: You are tasked with managing a Unix-based network system that uses NIS for central administration of host information. Using the deprecated Python `nis` module, write a function that retrieves information from NIS maps and processes it. Requirements: 1. **Function Name**: `retrieve_nis_info` 2. **Input**: - `key` (str): The key to look up in the NIS map. - `mapname` (str): The name of the NIS map. - `domain` (optional, str): The NIS domain to use for the lookup. Defaults to the system default domain if not provided. 3. **Output**: - Return a dictionary with the key-value pairs from the specified NIS map if the key is found. - If the key is not found, return an empty dictionary. 4. **Exceptions**: - Handle any `nis.error` exceptions by returning an empty dictionary. 5. **Constraints**: - The function should handle arbitrary byte arrays for the key and value. 6. **Performance Requirements**: - The function should have a time complexity of O(1) for the lookup operation due to the dictionary\'s expected constant-time complexity. Example: ```python def retrieve_nis_info(key, mapname, domain=None): import nis try: # If domain is None, use the system default domain if domain is None: domain = nis.get_default_domain() # Retrieve the map as a dictionary nis_map = nis.cat(mapname, domain) # Check if the key exists in the dictionary and return the result if key in nis_map: return {key: nis_map[key]} else: return {} except nis.error: return {} # Example Usage: result = retrieve_nis_info(\\"host123\\", \\"hosts.byname\\") print(result) # Expected output: {\'host123\': b\'<host_info>\'} or {} ``` Additional Notes: - Ensure proper handling of Unicode strings and byte arrays. - It is expected that students run this in a suitable Unix environment where NIS is configured.","solution":"import nis def retrieve_nis_info(key, mapname, domain=None): Retrieves information from NIS maps. Parameters: - key (str): The key to look up in the NIS map. - mapname (str): The name of the NIS map. - domain (optional, str): The NIS domain to use for the lookup. Defaults to the system default domain if not provided. Returns: - dict: A dictionary with the key-value pairs from the specified NIS map if the key is found. An empty dictionary if the key is not found or an error occurs. try: # If domain is None, use the system default domain if domain is None: domain = nis.get_default_domain() # Retrieve the map as a dictionary nis_map = nis.cat(mapname, domain) # Check if the key exists in the dictionary and return the result if key in nis_map: return {key: nis_map[key]} else: return {} except nis.error: return {}"},{"question":"<|Analysis Begin|> The provided documentation introduces the `seaborn.plotting_context` function, which allows customization of plot appearance by scaling various parameters. The examples show how to retrieve current styles, switch to predefined styles, and temporarily change context using a context manager. The fundamental ideas communicated are: 1. Retrieving current plotting parameters. 2. Switching to predefined plotting styles. 3. Using context managers for temporary style changes. 4. Practical application by plotting a line plot with different contexts. Based on this understanding, we can create a coding question to assess students\' comprehension of tweaking plotting context styles and applying them to generate plots using seaborn. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate understanding and application of seaborn\'s plotting context customization. **Problem Statement:** You are provided with sales data for three products over three quarters. Your task is to: 1. Retrieve the default plotting parameters. 2. Print the plotting parameters of a predefined style (`\\"notebook\\"`). 3. Use the `\\"talk\\"` context to generate a line plot of the sales data. 4. Use a custom context where you scale the font size, line width, and grid line width by factors of 1.5, 2, and 0.6, respectively, and then generate another line plot of the sales data. **Data:** | Quarter | Product A | Product B | Product C | |---------|-----------|-----------|-----------| | Q1 | 150 | 200 | 250 | | Q2 | 210 | 180 | 260 | | Q3 | 200 | 150 | 270 | **Expected Input and Output:** Input: - No explicit input; you will define the data within your script. Output: - Print statements for default plotting parameters and the `\\"notebook\\"` style parameters. - Display of two seaborn line plots with different contexts applied (`\\"talk\\"` and custom). **Constraints:** - Ensure your plots are well-labeled with appropriate titles, labels, and legends. - Make sure your custom context changes the specified parameters appropriately. **Performance Requirements:** - The script should run efficiently, and rendering the plots should be quick. **Implementation Details:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Step 1: Retrieve and print default plotting parameters default_params = sns.plotting_context() print(\\"Default plotting parameters:n\\", default_params) # Step 2: Print the plotting parameters for \'notebook\' style notebook_params = sns.plotting_context(\\"notebook\\") print(\\"n\'Notebook\' style plotting parameters:n\\", notebook_params) # Data for plotting data = { \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\"], \\"Product A\\": [150, 210, 200], \\"Product B\\": [200, 180, 150], \\"Product C\\": [250, 260, 270] } df = pd.DataFrame(data) # Step 3: Generate a line plot using the \'talk\' context with sns.plotting_context(\\"talk\\"): sns.lineplot(data=df.set_index(\\"Quarter\\")) plt.title(\\"Sales Data with \'talk\' Context\\") plt.xlabel(\\"Quarter\\") plt.ylabel(\\"Sales\\") plt.show() # Step 4: Define and use a custom context to generate another plot custom_context = sns.plotting_context(rc={\\"font.size\\": 1.5 * default_params[\\"font.size\\"], \\"lines.linewidth\\": 2 * default_params[\\"lines.linewidth\\"], \\"grid.linewidth\\": 0.6 * default_params[\\"grid.linewidth\\"]}) with sns.plotting_context(custom_context): sns.lineplot(data=df.set_index(\\"Quarter\\")) plt.title(\\"Sales Data with Custom Context\\") plt.xlabel(\\"Quarter\\") plt.ylabel(\\"Sales\\") plt.show() ``` Please implement the above steps in your Python environment and ensure you understand how the seaborn context affects the appearance of the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_sales_data(): # Step 1: Retrieve and print default plotting parameters default_params = sns.plotting_context() print(\\"Default plotting parameters:n\\", default_params) # Step 2: Print the plotting parameters for \'notebook\' style notebook_params = sns.plotting_context(\\"notebook\\") print(\\"n\'Notebook\' style plotting parameters:n\\", notebook_params) # Data for plotting data = { \\"Quarter\\": [\\"Q1\\", \\"Q2\\", \\"Q3\\"], \\"Product A\\": [150, 210, 200], \\"Product B\\": [200, 180, 150], \\"Product C\\": [250, 260, 270] } df = pd.DataFrame(data) # Step 3: Generate a line plot using the \'talk\' context with sns.plotting_context(\\"talk\\"): sns.lineplot(data=df.set_index(\\"Quarter\\")) plt.title(\\"Sales Data with \'talk\' Context\\") plt.xlabel(\\"Quarter\\") plt.ylabel(\\"Sales\\") plt.show() # Step 4: Define and use a custom context to generate another plot custom_context = sns.plotting_context(rc={\\"font.size\\": 1.5 * default_params[\\"font.size\\"], \\"lines.linewidth\\": 2 * default_params[\\"lines.linewidth\\"], \\"grid.linewidth\\": 0.6 * default_params[\\"grid.linewidth\\"]}) with sns.plotting_context(custom_context): sns.lineplot(data=df.set_index(\\"Quarter\\")) plt.title(\\"Sales Data with Custom Context\\") plt.xlabel(\\"Quarter\\") plt.ylabel(\\"Sales\\") plt.show() return default_params, notebook_params # return for test verification"},{"question":"# PyTorch Coding Assessment: Using the MPS Backend **Objective:** Demonstrate your understanding of using the MPS backend in PyTorch to perform high-performance training on macOS devices with Metal programming framework. # Problem Statement Write a function `train_mps_model` in Python that demonstrates the usage of PyTorch MPS backend. The function should: 1. Check if the MPS device is available. If not available, return an appropriate message. 2. Create a simple neural network model and move it to the MPS device. 3. Create random input data and target labels as Tensors moved to the MPS device. 4. Define a loss function and an optimizer. 5. Perform a single training step by: - Doing a forward pass. - Computing the loss. - Performing a backward pass. - Updating the model parameters. 6. Return the loss value and the updated model parameters. # Function Signature ```python def train_mps_model() -> (str, torch.Tensor): pass ``` # Constraints - Input data and target labels can be randomly generated. - The neural network model should be simple, such as a single linear layer. - Use mean squared error as the loss function. - Use stochastic gradient descent (SGD) as the optimizer. - Ensure that the function works on systems with macOS version 12.3+ and MPS-enabled devices. # Example ```python loss_value, updated_parameters = train_mps_model() print(\\"Loss:\\", loss_value) print(\\"Updated Parameters:\\", updated_parameters) ``` The output should be the loss value after the training step and the updated model parameters. **Note:** Ensure that the function handles the case where the MPS device is unavailable by returning an appropriate message. # Additional Information You can use the following code snippet to check for MPS availability and to set the device: ```python if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): return \\"MPS not available because the current PyTorch install was not built with MPS enabled.\\" else: return \\"MPS not available because the current macOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\" else: mps_device = torch.device(\\"mps\\") ``` Make sure to use proper comments and follow best coding practices while implementing the function.","solution":"import torch import torch.nn as nn import torch.optim as optim def train_mps_model(): # Check if MPS device is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): return \\"MPS not available because the current PyTorch install was not built with MPS enabled.\\" else: return \\"MPS not available because the current macOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\" # Set the device to MPS mps_device = torch.device(\\"mps\\") # Define a simple neural network model model = nn.Linear(10, 1).to(mps_device) # Create random input data and target labels inputs = torch.randn(5, 10, device=mps_device) targets = torch.randn(5, 1, device=mps_device) # Define the loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Perform a single training step optimizer.zero_grad() # Zero out the gradients outputs = model(inputs) # Forward pass loss = criterion(outputs, targets) # Compute loss loss.backward() # Backward pass optimizer.step() # Update model parameters # Return the loss value and updated model parameters return loss.item(), list(model.parameters())"},{"question":"# UUID Toolkit You are tasked with creating a toolkit leveraging the `uuid` module to handle a variety of tasks related to UUID generation, validation, and analysis. Part 1: UUID Generation 1. Write a function `generate_random_uuids(n: int) -> list` that generates `n` random UUIDs (using `uuid4`). The function should return a list of string representations of the UUIDs. **Input**: An integer `n` (1 ≤ n ≤ 1000) **Output**: A list of `n` UUIDs as strings. 2. Write a function `generate_named_uuid(namespace: str, name: str, version=5) -> str` that generates a UUID based on the given namespace and name string. The default version should use SHA-1 hashing (version 5). The namespace should be one of `NAMESPACE_DNS`, `NAMESPACE_URL`, `NAMESPACE_OID`, or `NAMESPACE_X500`. **Input**: A string `namespace`, a string `name`, and an optional integer `version` (either 3 or 5). **Output**: A UUID as a string. Part 2: UUID Validation & Attributes 3. Write a function `is_valid_uuid(uuid_str: str) -> bool` that checks whether the given string is a valid UUID. The validation should consider the correct format and length. **Input**: A string `uuid_str`. **Output**: A boolean indicating whether the string is a valid UUID. 4. Write a function `uuid_attributes(uuid_str: str) -> dict` that returns a dictionary containing detailed attributes of the UUID such as `time_low`, `time_mid`, `time_hi_version`, `clock_seq_hi_variant`, `clock_seq_low`, `node`, and `version`. **Input**: A string `uuid_str`. **Output**: A dictionary containing the UUID attributes. Performance and Constraints - Make sure all UUID strings are generated using the `uuid` module. - Ensure the returned attributes in part 4 are accurate and sourced from the UUID object. - Your solution should be efficient, especially concerning the generation of multiple UUIDs. Example Usage ```python print(generate_random_uuids(3)) # Output example: [\'16fd2706-8baf-433b-82eb-8c7fada847da\', \'30b443ae-c0c9-4790-9bec-ce1380808435\', \'96338bb4-7e2c-492d-b926-4f7f0d0c7b01\'] print(generate_named_uuid(\'NAMESPACE_DNS\', \'example.com\')) # Output example: \'886313e1-3b8a-5372-9b90-0c9aee199e5d\' print(is_valid_uuid(\'886313e1-3b8a-5372-9b90-0c9aee199e5d\')) # Output: True print(uuid_attributes(\'886313e1-3b8a-5372-9b90-0c9aee199e5d\')) # Output example: {\'time_low\': 2289061345, \'time_mid\': 15256, \'time_hi_version\': 21394, \'clock_seq_hi_variant\': 155, \'clock_seq_low\': 144, \'node\': 9100353289725, \'version\': 5} ``` Notes - Be sure to handle cases where invalid data might be input for UUID generation or attributes retrieval. - Descriptive comments and error handling will be beneficial for readability and robustness of your code.","solution":"import uuid def generate_random_uuids(n: int) -> list: Generates n random UUIDs using uuid4 and returns a list of string representations. return [str(uuid.uuid4()) for _ in range(n)] def generate_named_uuid(namespace: str, name: str, version=5) -> str: Generates a UUID based on the given namespace and name string using the specified version. The default version is 5 (SHA-1 hashing). Accepted namespaces are: \'NAMESPACE_DNS\', \'NAMESPACE_URL\', \'NAMESPACE_OID\', \'NAMESPACE_X500\'. namespaces = { \\"NAMESPACE_DNS\\": uuid.NAMESPACE_DNS, \\"NAMESPACE_URL\\": uuid.NAMESPACE_URL, \\"NAMESPACE_OID\\": uuid.NAMESPACE_OID, \\"NAMESPACE_X500\\": uuid.NAMESPACE_X500 } if namespace not in namespaces: raise ValueError(f\\"Invalid namespace \'{namespace}\'. Expected one of {list(namespaces.keys())}.\\") if version == 3: return str(uuid.uuid3(namespaces[namespace], name)) elif version == 5: return str(uuid.uuid5(namespaces[namespace], name)) else: raise ValueError(\\"Invalid version number. Expected 3 or 5.\\") def is_valid_uuid(uuid_str: str) -> bool: Checks if the provided string is a valid UUID. try: val = uuid.UUID(uuid_str) return str(val) == uuid_str except ValueError: return False def uuid_attributes(uuid_str: str) -> dict: Returns a dictionary containing detailed attributes of the UUID. if not is_valid_uuid(uuid_str): raise ValueError(\\"Provided string is not a valid UUID.\\") u = uuid.UUID(uuid_str) return { \\"time_low\\": u.time_low, \\"time_mid\\": u.time_mid, \\"time_hi_version\\": u.time_hi_version, \\"clock_seq_hi_variant\\": u.clock_seq_hi_variant, \\"clock_seq_low\\": u.clock_seq_low, \\"node\\": u.node, \\"version\\": u.version }"},{"question":"# Task You are required to write a Python function that performs several URL-related tasks using the `urllib.request` module. The function should: 1. **Fetch Content**: Fetches the content of a given URL. 2. **Handle Redirects**: Properly handle HTTP redirects. 3. **Use Authentication**: Use Basic HTTP Authentication when specified. 4. **Proxy Management**: Handle requests through a specified HTTP proxy. # Function Signature ```python def fetch_url(url: str, auth: dict = None, proxy: dict = None) -> str: Fetches the content of the given URL with optional basic authentication and proxy support. Parameters: - url (str): The URL to fetch. - auth (dict, optional): A dictionary containing \'username\' and \'password\' for basic authentication. Example: {\'username\': \'user\', \'password\': \'pass\'} - proxy (dict, optional): A dictionary containing proxy settings. Example: {\'http\': \'http://proxy.example.com:8080/\'} Returns: - str: The content of the response in string format. Raises: - URLError: If any URL-related error occurs. - HTTPError: If an HTTP error occurs. pass ``` # Input 1. `url` (str): A URL string that you want to fetch. Example: \\"http://www.example.com/\\" 2. `auth` (dict): A dictionary containing the keys \'username\' and \'password\' for Basic HTTP Authentication. Optional. Example: `{\'username\': \'user\', \'password\': \'pass\'}` 3. `proxy` (dict): A dictionary containing proxies for different protocols. Optional. Example: `{\'http\': \'http://proxy.example.com:8080/\'}` # Output - A string containing the content of the URL. # Notes 1. If the `auth` parameter is provided, use Basic HTTP Authentication with the given credentials. 2. If the `proxy` parameter is provided, configure the urllib to use the specified proxy. 3. Ensure to handle HTTP redirects properly. The function should follow the redirects and return the final content. 4. Any URL errors or HTTP errors should be properly handled and raised as exceptions. 5. If both `auth` and `proxy` parameters are provided, ensure both are handled correctly. # Example Usage ```python try: content = fetch_url(\'http://www.example.com/\', auth={\'username\': \'user\', \'password\': \'pass\'}, proxy={\'http\': \'http://proxy.example.com:8080/\'}) print(content) except URLError as e: print(f\'Failed to fetch URL: {e.reason}\') except HTTPError as e: print(f\'HTTP error occurred: {e.code} - {e.reason}\') ``` In this task, your implementation of `fetch_url` should be well-tested and robust to handle various cases such as authentication, proxy settings, and redirection handling. This requires a good understanding of the `urllib.request` module\'s capabilities and correct usage.","solution":"import urllib.request from urllib.error import URLError, HTTPError from base64 import b64encode def fetch_url(url: str, auth: dict = None, proxy: dict = None) -> str: Fetches the content of the given URL with optional basic authentication and proxy support. Parameters: - url (str): The URL to fetch. - auth (dict, optional): A dictionary containing \'username\' and \'password\' for basic authentication. Example: {\'username\': \'user\', \'password\': \'pass\'} - proxy (dict, optional): A dictionary containing proxy settings. Example: {\'http\': \'http://proxy.example.com:8080/\'} Returns: - str: The content of the response in string format. Raises: - URLError: If any URL-related error occurs. - HTTPError: If an HTTP error occurs. handlers = [] if proxy: proxy_handler = urllib.request.ProxyHandler(proxy) handlers.append(proxy_handler) if auth: username = auth[\'username\'] password = auth[\'password\'] auth_handler = urllib.request.HTTPBasicAuthHandler() credentials = (\'%s:%s\' % (username, password)).encode(\'utf-8\') auth_handler.add_password(None, url, username, password) headers = {\\"Authorization\\": \\"Basic {}\\".format(b64encode(credentials).decode(\\"ascii\\"))} request = urllib.request.Request(url, headers=headers) handlers.append(auth_handler) else: request = urllib.request.Request(url) opener = urllib.request.build_opener(*handlers) urllib.request.install_opener(opener) try: with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except HTTPError as e: raise e except URLError as e: raise e"},{"question":"**Problem Statement:** You are provided with a dataset related to email spam classification. The dataset has features indicating various properties of the emails and a target label indicating whether the email is spam (1) or not spam (0). Your task is to use different Naive Bayes classifiers provided by scikit-learn to fit this data, compare their performance, and determine which classifier performs the best on this dataset. **Dataset:** The dataset `emails.csv` contains the following columns: - `feature_1`, `feature_2`, ..., `feature_n`: Continuous values representing various properties of the emails. - `label`: Binary target label where 1 indicates spam and 0 indicates not spam. **Requirements:** 1. Load the dataset and split it into training and testing sets using a 70-30 split. 2. Standardize the features using `StandardScaler`. 3. Implement the following Naive Bayes classifiers: - Gaussian Naive Bayes (`GaussianNB`) - Multinomial Naive Bayes (`MultinomialNB`) - Complement Naive Bayes (`ComplementNB`) - Bernoulli Naive Bayes (`BernoulliNB`) 4. For each classifier: - Fit the model using the training data. - Predict the labels for the test data. - Calculate and print the accuracy, precision, recall, and F1-score. 5. Determine and print the classifier that performs the best based on the F1-score. **Constraints:** - You must use scikit-learn for model implementation. - You should handle any necessary data preprocessing, such as binarizing features for `BernoulliNB` if required. **Performance Requirements:** - Model evaluation must include accuracy, precision, recall, and F1-score. - The classifier with the highest F1-score should be considered the best performer. **Example Output:** ``` GaussianNB - Accuracy: 0.85, Precision: 0.83, Recall: 0.88, F1-score: 0.85 MultinomialNB - Accuracy: 0.80, Precision: 0.79, Recall: 0.82, F1-score: 0.80 ComplementNB - Accuracy: 0.82, Precision: 0.81, Recall: 0.84, F1-score: 0.82 BernoulliNB - Accuracy: 0.78, Precision: 0.77, Recall: 0.79, F1-score: 0.78 Best Classifier: GaussianNB with F1-score: 0.85 ``` **Notes:** - Remember to appropriately preprocess the data and specify any additional parameters required for each classifier. - Use appropriate scikit-learn methods for splitting data, scaling features, fitting models, and evaluating performance.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, Binarizer from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_and_preprocess_data(file_path): data = pd.read_csv(file_path) X = data.drop(columns=[\'label\']) y = data[\'label\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def evaluate_model(name, model, X_train, X_test, y_train, y_test): model.fit(X_train, y_train) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print(f\\"{name} - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\\") return f1 def main(file_path): X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path) models = [ (\\"GaussianNB\\", GaussianNB()), (\\"MultinomialNB\\", MultinomialNB()), (\\"ComplementNB\\", ComplementNB()), (\\"BernoulliNB\\", BernoulliNB()) ] best_model = None best_f1_score = 0 for name, model in models: f1 = evaluate_model(name, model, X_train, X_test, y_train, y_test) if f1 > best_f1_score: best_f1_score = f1 best_model = name print(f\\"nBest Classifier: {best_model} with F1-score: {best_f1_score:.2f}\\") if __name__ == \\"__main__\\": main(\\"emails.csv\\")"},{"question":"# Question You are a data analyst working on a dataset of various penguin species. Your task is to visualize this dataset to extract meaningful insights. Specifically, you need to create multiple visualizations to uncover patterns and distributions. 1. Load the dataset `penguins` using Seaborn. 2. Plot the distribution of `bill_length_mm` with the following customizations: - Use 25 bins for this histogram. - Include a kernel density estimate. 3. Create a histogram showcasing the distribution of `body_mass_g` across different species using hue. - Stack the histograms for clarity. 4. Plot a bivariate histogram showing the relationship between `bill_length_mm` and `flipper_length_mm`. - Add hue to differentiate between species. 5. Create another bivariate histogram plotting `body_mass_g` against `bill_depth_mm` and: - Set it to log scale on the y-axis. - Add a color bar to annotate the color map. **Input Format:** - You need to write a function `plot_penguin_data()` that takes no input but saves the plots in the current directory. - Ensure that you save the plots with the specific names: - \'bill_length_distribution.png\' - \'body_mass_distribution_by_species.png\' - \'bill_vs_flipper_length.png\' - \'body_mass_vs_bill_depth.png\' **Output Format:** - Save the plots as PNG files in the current directory. Here is the signature of the function: ```python def plot_penguin_data(): pass ``` **Constraints:** - Ensure your code is efficient and runs in reasonable time. - Handle any missing data appropriately before plotting.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_data(): # Load the penguins dataset from seaborn penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing values to handle missing data penguins.dropna(inplace=True) # 1. Plot the distribution of bill_length_mm plt.figure(figsize=(10, 6)) sns.histplot(penguins[\'bill_length_mm\'], bins=25, kde=True) plt.title(\'Distribution of Bill Length (mm)\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Frequency\') plt.savefig(\'bill_length_distribution.png\') plt.close() # 2. Histogram of body_mass_g by species plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\'body_mass_g\', hue=\'species\', multiple=\'stack\') plt.title(\'Distribution of Body Mass (g) by Species\') plt.xlabel(\'Body Mass (g)\') plt.ylabel(\'Frequency\') plt.savefig(\'body_mass_distribution_by_species.png\') plt.close() # 3. Bivariate histogram of bill_length_mm and flipper_length_mm with species as hue plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\'bill_length_mm\', y=\'flipper_length_mm\', hue=\'species\') plt.title(\'Bill Length vs Flipper Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Flipper Length (mm)\') plt.savefig(\'bill_vs_flipper_length.png\') plt.close() # 4. Bivariate histogram of body_mass_g and bill_depth_mm with log scale on y-axis and color bar plt.figure(figsize=(10, 6)) plot = sns.histplot(data=penguins, x=\'body_mass_g\', y=\'bill_depth_mm\', cbar=True) plt.yscale(\'log\') plt.title(\'Body Mass (g) vs Bill Depth (mm)\') plt.xlabel(\'Body Mass (g)\') plt.ylabel(\'Bill Depth (mm)\') plt.savefig(\'body_mass_vs_bill_depth.png\') plt.close()"},{"question":"Asynchronous TCP Chat Server and Client # Objective Implement an asynchronous TCP chat server and client using the asyncio streams functionality in Python. The server should be able to handle multiple clients, broadcasting messages from any client to all other clients connected to the server. # Instructions 1. **Server Implementation** - Create an asynchronous function `start_chat_server` that starts the TCP chat server. - The server should accept connections from multiple clients. - When a message is received from a client, it should broadcast the message to all other connected clients. - If a client disconnects, the server should handle the disconnection gracefully and continue to serve other clients. 2. **Client Implementation** - Create an asynchronous function `chat_client` that connects to the chat server. - The client should be able to send messages to the server. - The client should also print any messages it receives from the server. # Constraints - Use the asyncio library for handling asynchronous operations. - The server should run indefinitely, handling new client connections. - The example below assumes the server is running on localhost (`127.0.0.1`) and port `8888`. # Function Signatures ```python import asyncio async def start_chat_server(host: str, port: int): # Implement the server logic here pass async def chat_client(host: str, port: int, message: str): # Implement the client logic here pass if __name__ == \\"__main__\\": # Example usage server_task = asyncio.run(start_chat_server(\'127.0.0.1\', 8888)) client_task1 = asyncio.run(chat_client(\'127.0.0.1\', 8888, \\"Hello from Client 1\\")) client_task2 = asyncio.run(chat_client(\'127.0.0.1\', 8888, \\"Hello from Client 2\\")) # (Note: Running client and server concurrently in the same script is for illustration purposes only) ``` # Performance Requirements - The server and clients should handle messages in a non-blocking manner. - The server should be able to handle at least 10 concurrent clients. # Example Output When two clients send messages to the server, the output on each client should be something like: ``` Client 1: Send: Hello from Client 1 Received: Hello from Client 2 Client 2: Send: Hello from Client 2 Received: Hello from Client 1 ``` This question assesses your understanding of asynchronous programming with asyncio, handling multiple client connections, and effectively using StreamReader and StreamWriter classes for network communication.","solution":"import asyncio import sys clients = [] async def handle_client(reader, writer): client_addr = writer.get_extra_info(\'peername\') print(f\\"New connection from {client_addr}\\") clients.append(writer) try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {client_addr}\\") broadcast_message = f\\"{client_addr}: {message}\\" await broadcast(broadcast_message) except asyncio.CancelledError: pass finally: print(f\\"Connection closed from {client_addr}\\") writer.close() await writer.wait_closed() clients.remove(writer) async def broadcast(message): for writer in clients: writer.write(message.encode()) await writer.drain() async def start_chat_server(host, port): server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def chat_client(host, port, message): reader, writer = await asyncio.open_connection(host, port) print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() if __name__ == \\"__main__\\": server_task = asyncio.run(start_chat_server(\'127.0.0.1\', 8888)) # Note: You may need separate scripts or terminals for running server and clients concurrently"},{"question":"# Implement a Custom Caesar Cipher Codec Objective Create a custom codec for a Caesar Cipher, a type of substitution cipher where each letter in the text is shifted by a fixed number of positions in the alphabet. Task You are to implement the `CaesarCipherCodec` class which should inherit from `codecs.Codec` and provide custom implementations for the following: - `encode(input, errors=\'strict\')`: Encode the input text by shifting characters forward by a fixed number of positions (e.g., shift of 3: A -> D, B -> E, etc.). - `decode(input, errors=\'strict\')`: Decode the input text by shifting characters backward by the same number of positions. Follow these steps: 1. Define the `CaesarCipherCodec` class inheriting from `codecs.Codec`. 2. Implement the `encode` method. 3. Implement the `decode` method. 4. Register the codec with a custom search function. Constraints - The codec should only transform letters (both upper and lower case) and leave other characters unchanged. - The shift value should be defined as a constant `SHIFT`. Input and Output - **Encode Method**: - Input: `input` (str), `errors` (str, default \'strict\') - Output: Tuple of encoded string and length consumed. - **Decode Method**: - Input: `input` (str), `errors` (str, default \'strict\') - Output: Tuple of decoded string and length consumed. Example ```python codec = CaesarCipherCodec() encoded, length = codec.encode(\\"Hello World!\\") decoded, length = codec.decode(encoded) print(encoded) # Output: \\"Khoor Zruog!\\" print(decoded) # Output: \\"Hello World!\\" ``` Notes - Register your codec with a unique name, e.g., `caesar_cipher`. - Ensure that your codec can handle edge cases such as wrapping from \'Z\' to \'A\' and from \'z\' to \'a\'. Here is a template to help you get started: ```python import codecs SHIFT = 3 class CaesarCipherCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): def shift_char(c, shift): if \'a\' <= c <= \'z\': return chr((ord(c) - ord(\'a\') + shift) % 26 + ord(\'a\')) elif \'A\' <= c <= \'Z\': return chr((ord(c) - ord(\'A\') + shift) % 26 + ord(\'A\')) else: return c encoded = \'\'.join(shift_char(c, SHIFT) for c in input) return (encoded, len(input)) def decode(self, input, errors=\'strict\'): def shift_char(c, shift): if \'a\' <= c <= \'z\': return chr((ord(c) - ord(\'a\') - shift) % 26 + ord(\'a\')) elif \'A\' <= c <= \'Z\': return chr((ord(c) - ord(\'A\') - shift) % 26 + ord(\'A\')) else: return c decoded = \'\'.join(shift_char(c, SHIFT) for c in input) return (decoded, len(input)) def caesar_cipher_search_function(encoding): if encoding == \'caesar_cipher\': return codecs.CodecInfo( name=\'caesar_cipher\', encode=CaesarCipherCodec().encode, decode=CaesarCipherCodec().decode ) return None codecs.register(caesar_cipher_search_function) ``` Test Your Implementation Ensure your custom codec works by encoding and decoding strings with different characters, including handling case sensitivity and non-alphabet characters correctly.","solution":"import codecs SHIFT = 3 class CaesarCipherCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): def shift_char(c, shift): if \'a\' <= c <= \'z\': return chr((ord(c) - ord(\'a\') + shift) % 26 + ord(\'a\')) elif \'A\' <= c <= \'Z\': return chr((ord(c) - ord(\'A\') + shift) % 26 + ord(\'A\')) else: return c encoded = \'\'.join(shift_char(c, SHIFT) for c in input) return (encoded, len(input)) def decode(self, input, errors=\'strict\'): def shift_char(c, shift): if \'a\' <= c <= \'z\': return chr((ord(c) - ord(\'a\') - shift) % 26 + ord(\'a\')) elif \'A\' <= c <= \'Z\': return chr((ord(c) - ord(\'A\') - shift) % 26 + ord(\'A\')) else: return c decoded = \'\'.join(shift_char(c, SHIFT) for c in input) return (decoded, len(input)) def caesar_cipher_search_function(encoding): if encoding == \'caesar_cipher\': return codecs.CodecInfo( name=\'caesar_cipher\', encode=CaesarCipherCodec().encode, decode=CaesarCipherCodec().decode ) return None codecs.register(caesar_cipher_search_function)"},{"question":"# Array Manipulation Challenge You are given several tasks to perform using the Python `array` module. The goal is to create arrays, perform various operations on them, and manipulate their contents. Ensure your solution adheres to the constraints and utilizes the `array` module appropriately. Tasks: 1. **Create an Array:** - Create an array initialized with the first 10 non-negative integers. Use the appropriate type code to represent these integers. 2. **Append and Extend:** - Append the integer `10` to the array. - Extend the array with another array containing integers `11` to `15`. 3. **Reverse the Array and Count Occurrences:** - Reverse the order of the elements in the array. - Count the number of occurrences of the integer `10` in the array. 4. **Memory Address and Bytesize:** - Retrieve and print the memory address and length of the array. - Determine and print the size in bytes of the memory buffer used by the array. 5. **Convert to List and Bytes:** - Convert the array to a list and print it. - Convert the array to bytes and print the byte representation. 6. **Byteswap:** - Perform a byteswap operation on the array and print the new array representation. - Note: Ensure the operation is performed on compatible type codes (those with sizes 1, 2, 4, or 8 bytes). Constraints: - Only use the `array` module for array-related operations. - Do not use any other data structures such as lists, except where specifically instructed to convert or print the array contents. Expected Input and Output: The functions and their expected outputs are as follows: ```python def array_operations(): # Task 1: Create an array arr = ... # create array with first 10 non-negative integers # Task 2: Append and extend the array ... # append 10 and extend with integers 11 to 15 # Task 3: Reverse the array and count occurrences ... # reverse order and count occurrences of 10 # Task 4: Memory address and bytesize ... # print memory address, length, and size in bytes # Task 5: Convert to list and bytes ... # convert to list and bytes, then print them # Task 6: Byteswap ... # perform byteswap and print new array content return arr # Example usage: # Call the function and print its return value array_result = array_operations() print(array_result) ``` Ensure that each task is correctly implemented and the outputs match the requirements above.","solution":"import array def array_operations(): # Task 1: Create an array initialized with the first 10 non-negative integers arr = array.array(\'I\', range(10)) # Task 2: Append the integer `10` to the array and extend with integers 11 to 15 arr.append(10) arr.extend(array.array(\'I\', range(11, 16))) # Task 3: Reverse the order of the elements in the array and count occurrences of 10 arr_reverse = arr[::-1] occurrences_of_10 = arr_reverse.count(10) # Task 4: Retrieve and print the memory address, length of the array, and its size in bytes memory_address = arr.buffer_info()[0] length_of_array = arr.buffer_info()[1] size_in_bytes = arr.itemsize * length_of_array # Task 5: Convert the array to a list and print it, and convert the array to bytes and print the byte representation array_as_list = arr.tolist() array_as_bytes = arr.tobytes() # Task 6: Perform a byteswap operation on the array # Note: \'I\' is 4 bytes, so byteswap is valid arr.byteswap() byteswapped_array_as_list = arr.tolist() return { \\"initial_array\\": array_as_list, \\"reversed_array\\": arr_reverse.tolist(), \\"occurrences_of_10\\": occurrences_of_10, \\"memory_address\\": memory_address, \\"length_of_array\\": length_of_array, \\"size_in_bytes\\": size_in_bytes, \\"array_as_list\\": array_as_list, \\"array_as_bytes\\": array_as_bytes, \\"byteswapped_array\\": byteswapped_array_as_list }"},{"question":"Write a Python program to manage a small book inventory database using the `sqlite3` module. You need to implement a series of functions to interact with the database, ensuring the appropriate use of parameterized queries, custom type adaptation, and conversion. The program should include the following functionalities: 1. **Create Database and Table**: - Function `create_database(db_name: str)`: * This function should create a database with the given name and a table `books` with the following columns: * `id` (INTEGER PRIMARY KEY), * `title` (TEXT), * `author` (TEXT), * `published_date` (DATE in `YYYY-MM-DD` format), * `quantity` (INTEGER). 2. **Add Book**: - Function `add_book(db_name: str, title: str, author: str, published_date: str, quantity: int)`: * This function should insert a new book into the `books` table. Use parameterized queries to prevent SQL injection. 3. **List Books**: - Function `list_books(db_name: str)`: * This function should query and return all the books in the database, displaying them in a readable format. 4. **Find Books by Author**: - Function `find_books_by_author(db_name: str, author: str)`: * This function should query and return all the books by a specific author. 5. **Custom Adapter for Date Handling**: - Register a custom adapter to store dates as TEXT in `YYYY-MM-DD` format. - Register a custom converter to retrieve dates as `datetime.date` objects. # Constraints: - Ensure the database connection is properly closed after each operation. - Handle exceptions that might occur during database operations. - Use the `sqlite3` module\'s functionality for custom type adaptations where necessary. # Example Usage: ```python # Create the database and table create_database(\'book_inventory.db\') # Add some books add_book(\'book_inventory.db\', \'1984\', \'George Orwell\', \'1949-06-08\', 10) add_book(\'book_inventory.db\', \'To Kill a Mockingbird\', \'Harper Lee\', \'1960-07-11\', 7) # List all books list_books(\'book_inventory.db\') # Find books by author find_books_by_author(\'book_inventory.db\', \'George Orwell\') ``` # Expected Output: ``` ID: 1, Title: 1984, Author: George Orwell, Published Date: 1949-06-08, Quantity: 10 ID: 2, Title: To Kill a Mockingbird, Author: Harper Lee, Published Date: 1960-07-11, Quantity: 7 Books by George Orwell: ID: 1, Title: 1984, Author: George Orwell, Published Date: 1949-06-08, Quantity: 10 ``` Implement this program ensuring the proper use of the `sqlite3` module functionalities, especially focusing on security and custom type handling.","solution":"import sqlite3 import datetime # Function to adapt datetime.date to ISO format string. def adapt_date(date_obj): return date_obj.strftime(\'%Y-%m-%d\') # Function to convert ISO format string to datetime.date. def convert_date(date_str): return datetime.datetime.strptime(date_str.decode(\'utf-8\'), \'%Y-%m-%d\').date() # Register the adapter and converter. sqlite3.register_adapter(datetime.date, adapt_date) sqlite3.register_converter(\'DATE\', convert_date) def create_database(db_name: str): Creates a new SQLite database with the given name and a books table. conn = sqlite3.connect(db_name, detect_types=sqlite3.PARSE_DECLTYPES) try: cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY, title TEXT, author TEXT, published_date DATE, quantity INTEGER ) \'\'\') conn.commit() finally: conn.close() def add_book(db_name: str, title: str, author: str, published_date: str, quantity: int): Adds a new book to the books table. conn = sqlite3.connect(db_name, detect_types=sqlite3.PARSE_DECLTYPES) try: cursor = conn.cursor() cursor.execute(\'\'\' INSERT INTO books (title, author, published_date, quantity) VALUES (?, ?, ?, ?) \'\'\', (title, author, published_date, quantity)) conn.commit() finally: conn.close() def list_books(db_name: str): Lists all books in the books table. conn = sqlite3.connect(db_name, detect_types=sqlite3.PARSE_DECLTYPES) try: cursor = conn.cursor() cursor.execute(\'SELECT * FROM books\') books = cursor.fetchall() for book in books: print(f\\"ID: {book[0]}, Title: {book[1]}, Author: {book[2]}, Published Date: {book[3]}, Quantity: {book[4]}\\") finally: conn.close() def find_books_by_author(db_name: str, author: str): Finds all books by the given author. conn = sqlite3.connect(db_name, detect_types=sqlite3.PARSE_DECLTYPES) try: cursor = conn.cursor() cursor.execute(\'SELECT * FROM books WHERE author = ?\', (author,)) books = cursor.fetchall() print(f\\"Books by {author}:\\") for book in books: print(f\\"ID: {book[0]}, Title: {book[1]}, Author: {book[2]}, Published Date: {book[3]}, Quantity: {book[4]}\\") finally: conn.close()"},{"question":"**Objective:** Demonstrate your understanding of Unicode handling in Python by writing functions that manipulate Unicode strings and characters using the APIs discussed in the documentation. **Problem Statement:** You are required to implement a function `unicode_transformer` that takes a list of mixed Unicode character types and performs the following operations: 1. **Lowercase Transformation:** Convert all alphabetic characters to their lowercase equivalent. 2. **Digit Transformation:** Replace any decimal digit with its Unicode string representation. 3. **Whitespace Removal:** Remove all whitespace characters from the input. You should create helper functions that utilize the given APIs for: - Checking if a character is whitespace (`Py_UNICODE_ISSPACE`) - Converting characters to lowercase (`Py_UNICODE_TOLOWER`) - Checking if a character is decimal (`Py_UNICODE_ISDECIMAL`) - Converting decimal characters to their Unicode string representation(`Py_UNICODE_TODECIMAL`) **Function Signature:** ```python def unicode_transformer(input_list): pass ``` **Requirements:** - Write a helper function `is_whitespace(ch)` that checks if a given character `ch` is a whitespace using `Py_UNICODE_ISSPACE`. - Write a helper function `to_lowercase(ch)` that converts a given character `ch` to its lowercase equivalent using `Py_UNICODE_TOLOWER`. - Write a helper function `to_decimal_str(ch)` that converts a given decimal character `ch` to its Unicode string representation using `Py_UNICODE_TODECIMAL`. - Implement the `unicode_transformer` function using above-defined helper functions to perform the required transformations. - Ensure the input list does not have any memory overheads and contains Unicode characters efficiently. **Input:** - `input_list`: A list of Unicode characters (e.g., [\'A\', \'3\', \' \', \'C\', \'u0391\']). Length of the list will be `n` (1 ≤ n ≤ 10^5). **Output:** - Return a new list of transformed Unicode characters. **Constraints:** - Your implementation should handle memory efficiently and should not repeatedly create new Unicode objects unnecessarily. - Ensure that your solution is optimized for large inputs. **Example:** ```python input_list = [\'A\', \'3\', \' \', \'B\', \'5\'] output = unicode_transformer(input_list) print(output) # Output should be [\'a\', \'3\', \'b\', \'5\'] ``` Implement the `unicode_transformer` function and all necessary helpers using the given APIs to achieve the required transformations.","solution":"def is_whitespace(ch): Check if the given character is a whitespace. return ch.isspace() def to_lowercase(ch): Convert the given character to its lowercase equivalent. return ch.lower() def to_decimal_str(ch): Convert the given decimal character to its Unicode string representation. return str(ch) def unicode_transformer(input_list): Transforms a list of mixed Unicode character types and performs the following operations: - Lowercase Transformation - Digit Transformation - Whitespace Removal result = [] for ch in input_list: if is_whitespace(ch): continue if ch.isdecimal(): result.append(to_decimal_str(ch)) else: result.append(to_lowercase(ch)) return result"},{"question":"**Coding Assessment Question: Pandas Data Manipulation and Analysis** **Objective:** Demonstrate your understanding of pandas by performing various data manipulation and analysis tasks using the pandas library. **Problem Statement:** You are provided with two CSV files representing sales data and customer data for an online retail store. **File 1: sales_data.csv** ``` order_id, product_id, customer_id, quantity, price_per_unit, order_date 1, 101, 1001, 2, 50.0, 2021-01-01 2, 102, 1002, 1, 100.0, 2021-01-02 3, 101, 1003, 3, 50.0, 2021-01-01 4, 103, 1001, 1, 75.0, 2021-01-04 5, 104, 1004, 2, 20.0, 2021-01-03 ``` **File 2: customer_data.csv** ``` customer_id, customer_name, customer_email, join_date 1001, John Doe, johndoe@example.com, 2020-12-25 1002, Jane Smith, janesmith@example.com, 2020-12-26 1003, Emily Davis, emilydavis@example.com, 2020-12-27 1004, Michael Brown, michaelbrown@example.com, 2020-12-28 1005, Sarah Wilson, sarahwilson@example.com, 2020-12-29 ``` **Tasks:** 1. **Load the data:** - Read the contents of the two CSV files into two separate pandas DataFrames named `sales_df` and `customers_df`. 2. **Data Cleaning:** - Identify and handle any missing values in the sales data (`sales_df`). Assume any missing `price_per_unit` should be filled with the mean price of that product. 3. **Data Merging:** - Merge `sales_df` with `customers_df` on `customer_id`. The resulting DataFrame should contain customer details for each order. 4. **Data Analysis:** - Calculate the total revenue generated for each product. Create a DataFrame named `product_revenue` with columns `product_id` and `total_revenue`, sorted by `total_revenue` in descending order. - Calculate the total quantity sold for each customer. Create a DataFrame named `customer_sales` with columns `customer_name` and `total_quantity`, sorted by `total_quantity` in descending order. 5. **Data Visualization:** - Create a bar plot using the `product_revenue` DataFrame that shows the total revenue for each product. - Create a bar plot using the `customer_sales` DataFrame that shows the total quantity of products purchased by each customer. **Constraints:** - You must use the pandas library to perform the tasks. - Your solution should handle the cases where the data files are larger than the given samples. **Expected Input/Output:** - **Input:** CSV files `sales_data.csv` and `customer_data.csv`. - **Output:** DataFrames `product_revenue`, `customer_sales`, and bar plots visualizing the total revenue by `product_id` and total quantity sold by `customer_name`. **Performance Requirements:** - Your code should be efficient and should follow best practices for handling and processing data using pandas. ```python # Sample solution structure import pandas as pd import matplotlib.pyplot as plt # Task 1: Load the data sales_df = pd.read_csv(\'sales_data.csv\') customers_df = pd.read_csv(\'customer_data.csv\') # Task 2: Data Cleaning # Identify missing values and fill them appropriately sales_df[\'price_per_unit\'].fillna(sales_df.groupby(\'product_id\')[\'price_per_unit\'].transform(\'mean\'), inplace=True) # Task 3: Data Merging merged_df = pd.merge(sales_df, customers_df, on=\'customer_id\', how=\'left\') # Task 4: Data Analysis product_revenue = merged_df.groupby(\'product_id\').apply(lambda x: (x[\'quantity\']*x[\'price_per_unit\']).sum()).reset_index(name=\'total_revenue\') product_revenue = product_revenue.sort_values(by=\'total_revenue\', ascending=False) customer_sales = merged_df.groupby(\'customer_name\')[\'quantity\'].sum().reset_index(name=\'total_quantity\') customer_sales = customer_sales.sort_values(by=\'total_quantity\', ascending=False) # Task 5: Data Visualization product_revenue.plot(kind=\'bar\', x=\'product_id\', y=\'total_revenue\', title=\'Total Revenue by Product\') plt.show() customer_sales.plot(kind=\'bar\', x=\'customer_name\', y=\'total_quantity\', title=\'Total Quantity Sold by Customer\') plt.show() ``` *Ensure that all the functions and plotting are encapsulated within appropriate functions to ease testing and modularity.*","solution":"import pandas as pd import matplotlib.pyplot as plt def load_data(sales_file, customers_file): sales_df = pd.read_csv(sales_file) customers_df = pd.read_csv(customers_file) return sales_df, customers_df def clean_sales_data(sales_df): sales_df[\'price_per_unit\'].fillna(sales_df.groupby(\'product_id\')[\'price_per_unit\'].transform(\'mean\'), inplace=True) return sales_df def merge_data(sales_df, customers_df): merged_df = pd.merge(sales_df, customers_df, on=\'customer_id\', how=\'left\') return merged_df def calculate_product_revenue(merged_df): product_revenue = merged_df.groupby(\'product_id\').apply(lambda x: (x[\'quantity\'] * x[\'price_per_unit\']).sum()).reset_index(name=\'total_revenue\') product_revenue = product_revenue.sort_values(by=\'total_revenue\', ascending=False) return product_revenue def calculate_customer_sales(merged_df): customer_sales = merged_df.groupby(\'customer_name\')[\'quantity\'].sum().reset_index(name=\'total_quantity\') customer_sales = customer_sales.sort_values(by=\'total_quantity\', ascending=False) return customer_sales def plot_product_revenue(product_revenue): product_revenue.plot(kind=\'bar\', x=\'product_id\', y=\'total_revenue\', title=\'Total Revenue by Product\') plt.show() def plot_customer_sales(customer_sales): customer_sales.plot(kind=\'bar\', x=\'customer_name\', y=\'total_quantity\', title=\'Total Quantity Sold by Customer\') plt.show() # Example usage within a main guard if __name__ == \\"__main__\\": sales_file = \'sales_data.csv\' customers_file = \'customer_data.csv\' # Load data sales_df, customers_df = load_data(sales_file, customers_file) # Clean data sales_df = clean_sales_data(sales_df) # Merge data merged_df = merge_data(sales_df, customers_df) # Calculate product revenue product_revenue = calculate_product_revenue(merged_df) # Calculate customer sales customer_sales = calculate_customer_sales(merged_df) # Plot the results plot_product_revenue(product_revenue) plot_customer_sales(customer_sales)"},{"question":"**Coding Assessment Question:** # Objective: To assess your understanding of the `secrets` module in Python and your ability to implement secure random number and token generation. # Problem Statement: You have been tasked with creating a secure registration system for a new application. The system needs to handle several security-related tasks: generating secure passwords for users, creating unique URL-safe tokens for verification links, and securely comparing the provided tokens during verification. # Task: Implement the following functions using the `secrets` module: 1. **generate_secure_password(length: int) -> str:** - Generates a password of the specified length following these rules: - It must contain at least one lowercase letter, one uppercase letter, and one digit. 2. **create_verification_link(base_url: str, token_length: int) -> str:** - Generates a unique, secure URL-safe token of the specified length and appends it to the given base URL to create a verification link. 3. **secure_compare(token_a: str, token_b: str) -> bool:** - Securely compares two provided tokens to check if they are the same using a method that minimizes the risk of timing attacks. # Input and Output Formats: 1. `generate_secure_password(length: int) -> str` - Input: An integer specifying the desired length of the password. - Output: A string representing the generated password. 2. `create_verification_link(base_url: str, token_length: int) -> str` - Input: A string representing the base URL and an integer specifying the desired length of the token. - Output: A string representing the full verification link. 3. `secure_compare(token_a: str, token_b: str) -> bool` - Input: Two strings representing the tokens to be compared. - Output: A boolean value indicating whether the tokens are the same. # Constraints: - For `generate_secure_password`: - The length must be at least 4 to ensure the inclusion of a lowercase letter, an uppercase letter, and a digit. - For `create_verification_link`: - The base URL must be a valid URL format string. - The token length must be a positive integer. # Example Usage: ```python # Example 1: Generating a secure password password = generate_secure_password(10) print(password) # Output: a 10-character secure password such as \\"G4geT2bS9L\\" # Example 2: Creating a verification link verification_link = create_verification_link(\\"https://example.com/verify\\", 16) print(verification_link) # Output: A URL such as \\"https://example.com/verify?token=Drmhze6EPcv0fN_81Bj-nA\\" # Example 3: Securely comparing tokens result = secure_compare(\\"token123\\", \\"token123\\") print(result) # Output: True result = secure_compare(\\"token123\\", \\"token456\\") print(result) # Output: False ``` # Implementation: You need to fill in the implementation for each of the specified functions using the `secrets` module as described in the provided documentation.","solution":"import secrets import string def generate_secure_password(length: int) -> str: if length < 4: raise ValueError(\\"Password length must be at least 4 characters.\\") alphabet = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(alphabet) for i in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password)): return password def create_verification_link(base_url: str, token_length: int) -> str: if token_length <= 0: raise ValueError(\\"Token length must be a positive integer.\\") token = secrets.token_urlsafe(token_length) return f\\"{base_url}?token={token}\\" def secure_compare(token_a: str, token_b: str) -> bool: return secrets.compare_digest(token_a, token_b)"},{"question":"**Coding Assessment Question** # Objective: You are asked to demonstrate your understanding of the `email.policy` module in the Python3 standard library. Your task is to create a custom policy by extending the `EmailPolicy` class and to implement a function that processes an email message using the customized policy. # Problem Statement: 1. Create a new class `CustomEmailPolicy` that extends the `EmailPolicy` class. 2. Override the `header_fetch_parse` method such that it: - Adds a prefix `[Parsed] ` to the value of the `Subject` header when it is retrieved. - Leaves other headers unchanged. 3. Implement a function `process_email_with_custom_policy(email_file_path)` that: - Accepts the path to an email file (`email_file_path`) as input. - Reads the email message from the given file using the `CustomEmailPolicy`. - Returns a dictionary where the keys are header names and the values are the corresponding header values according to the customized policy. # Constraints: - The email file will be a plain text file in standard RFC format. - You must ensure your function `process_email_with_custom_policy` reads from the file in binary mode. - For simplicity, assume the email file will not contain any malformed headers. - Your implementation should handle any number of headers and be efficient with respect to I/O operations. # Examples: Suppose `email.txt` contains: ``` From: example@example.com To: recipient@example.com Subject: Test Email Date: Thu, 1 Apr 2021 10:00:00 -0400 This is the body of the email ``` Calling `process_email_with_custom_policy(\'email.txt\')` should return: ```python { \\"From\\": \\"example@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"[Parsed] Test Email\\", \\"Date\\": \\"Thu, 1 Apr 2021 10:00:00 -0400\\" } ``` # Submission: Submit the class definition for `CustomEmailPolicy` and the implementation of the function `process_email_with_custom_policy`.","solution":"import email from email.policy import EmailPolicy class CustomEmailPolicy(EmailPolicy): def header_fetch_parse(self, name, value): if name == \\"Subject\\": value = \\"[Parsed] \\" + value return value def process_email_with_custom_policy(email_file_path): with open(email_file_path, \'rb\') as f: msg = email.message_from_binary_file(f, policy=CustomEmailPolicy()) headers = {k: v for k, v in msg.items()} return headers"},{"question":"You are tasked to write functions that utilize the deprecated \\"uu\\" module for encoding and decoding files. This exercise aims to not only test your ability to use external libraries but also to handle files properly and manage exceptions effectively. # Problem Description: 1. **Function Name: `uu_encode`** - **Input**: - `input_file_path` (str): The path to the input file that needs to be uuencoded. - `output_file_path` (str): The path where the uuencoded file will be saved. - `file_name` (str, optional): The name to use in the uuencode header. - `file_mode` (int, optional): The file mode to use in the uuencode header. - `use_backtick` (bool, optional): Whether to use backticks instead of spaces for zeros. - **Output**: None - **Constraints**: - The input file must exist and be readable. - The output file must be writable. - **Error Handling**: - The function should raise appropriate exceptions with meaningful error messages if the input file does not exist or is not readable. - The function should raise an exception if there is an issue writing to the output file. 2. **Function Name: `uu_decode`** - **Input**: - `input_file_path` (str): The path to the uuencoded file that needs to be decoded. - `output_file_path` (str): The path where the decoded file will be saved. - `file_mode` (int, optional): The file mode to use if the output file must be created. - `suppress_warnings` (bool, optional): Whether to suppress warnings during decoding. - **Output**: None - **Constraints**: - The input file must exist and be readable. - The output file must be writable. - **Error Handling**: - The function should raise appropriate exceptions with meaningful error messages if the input file does not exist or is not readable. - The function should raise an exception if there is a decoding error (e.g., badly formatted header). 3. **Exception Handling Enhancement:** - Define a custom exception class `FileEncodingError` to handle all file-related errors in both the `uu_encode` and `uu_decode` functions. # Example Usage: ```python from uu import Error as UuError class FileEncodingError(Exception): pass def uu_encode(input_file_path, output_file_path, file_name=None, file_mode=None, use_backtick=False): try: with open(input_file_path, \'rb\') as in_file, open(output_file_path, \'wb\') as out_file: uu.encode(in_file, out_file, file_name, file_mode, backtick=use_backtick) except FileNotFoundError: raise FileEncodingError(f\\"Input file \'{input_file_path}\' not found.\\") except UuError as e: raise FileEncodingError(f\\"UUError encountered: {str(e)}\\") except Exception as e: raise FileEncodingError(f\\"An error occurred: {str(e)}\\") def uu_decode(input_file_path, output_file_path, file_mode=None, suppress_warnings=False): try: with open(input_file_path, \'rb\') as in_file, open(output_file_path, \'wb\') as out_file: uu.decode(in_file, out_file, mode=file_mode, quiet=suppress_warnings) except FileNotFoundError: raise FileEncodingError(f\\"Input file \'{input_file_path}\' not found.\\") except UuError as e: raise FileEncodingError(f\\"UUError encountered: {str(e)}\\") except Exception as e: raise FileEncodingError(f\\"An error occurred: {str(e)}\\") # Example calls: try: uu_encode(\\"data.txt\\", \\"data.uu\\") uu_decode(\\"data.uu\\", \\"decoded_data.txt\\") except FileEncodingError as e: print(f\\"Error: {e}\\") ``` # Note: Ensure you handle files correctly and manage exceptions to provide meaningful output. Your implementation will be evaluated based on correctness, error handling, and adherence to the specifications provided.","solution":"import uu from uu import Error as UuError class FileEncodingError(Exception): Exception raised for errors in the file encoding/decoding process. pass def uu_encode(input_file_path, output_file_path, file_name=None, file_mode=None, use_backtick=False): Uuencodes a file. Args: - input_file_path (str): The path to the input file. - output_file_path (str): The path where the uuencoded file will be saved. - file_name (str, optional): The name to use in the uuencode header. - file_mode (int, optional): The file mode to use in the uuencode header. - use_backtick (bool, optional): Whether to use backticks instead of spaces for zeros. try: with open(input_file_path, \'rb\') as in_file, open(output_file_path, \'wb\') as out_file: uu.encode(in_file, out_file, name=file_name, mode=file_mode, backtick=use_backtick) except FileNotFoundError: raise FileEncodingError(f\\"Input file \'{input_file_path}\' not found.\\") except UuError as e: raise FileEncodingError(f\\"UUError encountered: {str(e)}\\") except Exception as e: raise FileEncodingError(f\\"An error occurred: {str(e)}\\") def uu_decode(input_file_path, output_file_path, file_mode=None, suppress_warnings=False): Uudecodes a file. Args: - input_file_path (str): The path to the uuencoded file. - output_file_path (str): The path where the decoded file will be saved. - file_mode (int, optional): The file mode to use if the output file must be created. - suppress_warnings (bool, optional): Whether to suppress warnings during decoding. try: with open(input_file_path, \'rb\') as in_file, open(output_file_path, \'wb\') as out_file: uu.decode(in_file, out_file, mode=file_mode, quiet=suppress_warnings) except FileNotFoundError: raise FileEncodingError(f\\"Input file \'{input_file_path}\' not found.\\") except UuError as e: raise FileEncodingError(f\\"UUError encountered: {str(e)}\\") except Exception as e: raise FileEncodingError(f\\"An error occurred: {str(e)}\\")"},{"question":"# Objective: Implement a custom profiling solution using the Python debugging and profiling tools mentioned in the provided documentation. The goal is to profile a given Python function to identify and optimize its performance bottlenecks. # Problem Statement: You are required to implement a `profile_and_optimize` function that takes another function `func` and its arguments as input, profiles the function to identify the slowest executing parts, and suggests optimizations. Specifically, you will use: 1. `cProfile` to profile the function and generate a detailed breakdown of the execution times. 2. `timeit` to measure the execution time of small code snippets. 3. `tracemalloc` to trace memory allocations and identify potential memory bottlenecks. # Function Signature: ```python def profile_and_optimize(func: callable, *args, **kwargs) -> dict: pass ``` # Input: - `func`: A callable Python function to be profiled. - `*args`: Positional arguments to be passed to the function `func`. - `**kwargs`: Keyword arguments to be passed to the function `func`. # Output: - Returns a dictionary with the following keys: * \'profile_stats\': A string reporting the profiling statistics generated by `cProfile`. * \'timeit_results\': A dictionary with the execution time measurements for different parts of the function using `timeit`. * \'memory_stats\': A string summarizing the memory usage tracing using `tracemalloc`. # Constraints: 1. Ensure the `func` being profiled is a valid callable. 2. Assume that the function `func` and its arguments do not perform file I/O operations. 3. Focus your solution on CPU-bound and memory-bound operations. # Example: ```python def my_function(x, y): result = [] for i in range(x): result.append(i * y) return sum(result) profiled_results = profile_and_optimize(my_function, 10000, 20) print(profiled_results[\'profile_stats\']) # Outputs the profiling statistics print(profiled_results[\'timeit_results\']) # Outputs the timeit results for various parts print(profiled_results[\'memory_stats\']) # Outputs the memory allocation statistics ``` # Notes: 1. Carefully read the provided documentation to understand how to use `cProfile`, `timeit`, and `tracemalloc`. 2. Provide meaningful and actionable optimization tips based on the profiling results. # Hints: 1. Look into the `pstats` module to format the `cProfile` output. 2. Use the built-in `timeit` module to measure the execution time of smaller parts of the function. 3. Utilize `tracemalloc` to understand and report memory usage.","solution":"import cProfile import pstats from io import StringIO import timeit import tracemalloc def profile_and_optimize(func: callable, *args, **kwargs) -> dict: # Profile the function using cProfile pr = cProfile.Profile() pr.enable() func(*args, **kwargs) pr.disable() s = StringIO() sortby = \'cumulative\' ps = pstats.Stats(pr, stream=s).sort_stats(sortby) ps.print_stats() profile_stats = s.getvalue() # Benchmark the function using timeit def wrapper(): return func(*args, **kwargs) timeit_results = timeit.timeit(wrapper, number=100) # Running the function 100 times for accurate timing # Trace memory allocations using tracemalloc tracemalloc.start() func(*args, **kwargs) current, peak = tracemalloc.get_traced_memory() tracemalloc.stop() memory_stats = f\\"Current memory usage: {current / 10**6}MB; Peak: {peak / 10**6}MB.\\" return { \'profile_stats\': profile_stats, \'timeit_results\': {\'execution_time\': timeit_results}, \'memory_stats\': memory_stats, }"},{"question":"Problem Statement You are given a hexadecimal string representing binary data. Your task is to write a Python function that first converts this hexadecimal string to its binary representation. Then, it should encode this binary data using base64 encoding but without adding a newline character at the end. Finally, the function should return the base64 encoded string. Function Signature ```python def hex_to_base64(hexstr: str) -> str: pass ``` Input - `hexstr` (str): A string containing an even number of hexadecimal digits (0-9, a-f, A-F). Output - Returns a base64 encoded string (str) without a newline character at the end. Example ```python >>> hex_to_base64(\\"48656c6c6f20576f726c64\\") \'SGVsbG8gV29ybGQ=\' ``` Constraints - The input hexadecimal string will always contain an even number of characters. - You should handle both uppercase and lowercase hexadecimal digits. - The returned base64 string should not include a newline character at the end, even if the encoded form would normally include one. Notes - You can use the `binascii` module to perform the hexadecimal to binary conversion and the base64 encoding. - Make sure to handle any binary data correctly according to the specifications given. Usage of `binascii` Module You may find the following functions useful: - `binascii.unhexlify(hexstr)` to convert a hexadecimal string to binary data. - `binascii.b2a_base64(data, newline=False)` to convert binary data to base64 encoding without a newline character. Performance Requirements The function should be efficient enough to handle large input strings up to 10^6 hexadecimal digits.","solution":"import binascii def hex_to_base64(hexstr: str) -> str: Converts a hexadecimal string to a base64 encoded string without a newline character at the end. :param hexstr: A string containing an even number of hexadecimal digits (0-9, a-f, A-F). :return: A base64 encoded string without a newline character at the end. binary_data = binascii.unhexlify(hexstr) # Use binascii.b2a_base64(binary_data, newline=False) which doesn\'t add a new line character. base64_encoded = binascii.b2a_base64(binary_data, newline=False) return base64_encoded.decode(\'utf-8\')"},{"question":"**Coding Assessment Question** # Objective You are required to measure and compare the execution times of different Python code snippets using the `timeit` module. This will demonstrate your ability to use `timeit` effectively and understand the performance implications of different implementations. # Problem Statement Write a function `compare_execution_times` that accepts three different code snippets and a setup code. The function should return a dictionary with the code snippets as keys and their respective execution times as values. The code snippets will be strings containing valid Python code. # Function Signature ```python def compare_execution_times(code1: str, code2: str, code3: str, setup: str = \'pass\') -> dict: pass ``` # Input - `code1`: A string containing the first piece of Python code to be timed. - `code2`: A string containing the second piece of Python code to be timed. - `code3`: A string containing the third piece of Python code to be timed. - `setup`: A string containing setup code to be executed before each timing. Default is `\'pass\'`. # Output - Return a dictionary with the three code snippets as keys and their corresponding execution times (in seconds) as values. The execution times should represent the best time of 5 runs with each run executing the code snippet 10,000 times. Example Usage ```python def compare_execution_times(code1, code2, code3, setup=\'pass\'): import timeit times = {} for code in [code1, code2, code3]: times[code] = min(timeit.repeat(stmt=code, setup=setup, repeat=5, number=10000)) return times # Test cases code_snippet1 = \'\\"-\\".join(str(n) for n in range(100))\' code_snippet2 = \'\\"-\\".join([str(n) for n in range(100)])\' code_snippet3 = \'\\"-\\".join(map(str, range(100)))\' setup_code = \'\' result = compare_execution_times(code_snippet1, code_snippet2, code_snippet3, setup_code) for k, v in result.items(): print(f\'Code: {k}nTime: {v}n\') ``` # Constraints - Ensure that the timing measurements are performed accurately. - Assume that the code snippets provided are syntactically correct. - The function should handle the timing process for each code snippet independently. # Requirements - Use the `timeit.repeat()` method to measure the execution times. - Perform timing for each code snippet 5 times and return the minimum execution time for each. - Implement the function in a clear and readable manner, ensuring it adheres to Python conventions and standards.","solution":"def compare_execution_times(code1: str, code2: str, code3: str, setup: str = \'pass\') -> dict: import timeit times = {} for code in [code1, code2, code3]: times[code] = min(timeit.repeat(stmt=code, setup=setup, repeat=5, number=10000)) return times"},{"question":"**Objective**: To assess your understanding and ability to use the `operator` module in Python effectively. # Problem Statement You are given a list of student records, where each record is a tuple containing the student\'s name and their score in a course. Your task is to perform various operations on this list using functions from the `operator` module. Write a function `process_student_records(records: List[Tuple[str, int]]) -> Tuple[List[Tuple[str, int]], List[int], Tuple[int, int]]` that takes in a list of tuples `records` where each tuple contains a student\'s name as a string and their score as an integer. The function should perform the following operations: 1. **Sort the records** in descending order based on the students\' scores. 2. **Extract the scores** into a separate list. 3. **Find and return** the highest and lowest scores. # Input - `records`: A list of tuples `[(str, int), ...]` where each tuple consists of a student\'s name and their score. # Output The function should return a tuple containing three elements: 1. A list of tuples `[(str, int), ...]` sorted in descending order by score. 2. A list of integers representing the scores in descending order. 3. A tuple `(int, int)` where the first element is the highest score and the second element is the lowest score. # Constraints - You must use functions from the `operator` module wherever applicable. # Example ```python from typing import List, Tuple import operator def process_student_records(records: List[Tuple[str, int]]) -> Tuple[List[Tuple[str, int]], List[int], Tuple[int, int]]: # your code here # Example usage records = [(\\"Alice\\", 88), (\\"Bob\\", 95), (\\"Charlie\\", 70), (\\"David\\", 85)] result = process_student_records(records) print(result) ``` Expected output: ```python ([ (\\"Bob\\", 95), (\\"Alice\\", 88), (\\"David\\", 85), (\\"Charlie\\", 70) ], [95, 88, 85, 70], (95, 70)) ``` **Note**: Ensure your solution includes import statements and uses functions from the `operator` module as required.","solution":"from typing import List, Tuple import operator def process_student_records(records: List[Tuple[str, int]]) -> Tuple[List[Tuple[str, int]], List[int], Tuple[int, int]]: # Sort the records in descending order based on the students\' scores sorted_records = sorted(records, key=operator.itemgetter(1), reverse=True) # Extract the scores into a separate list scores = list(map(operator.itemgetter(1), sorted_records)) # Find and return the highest and lowest scores highest_score = scores[0] lowest_score = scores[-1] return (sorted_records, scores, (highest_score, lowest_score))"},{"question":"# SVM Classification Task You are provided with a dataset `iris.csv`, which contains the famous Iris dataset. Your task is to implement a classification model using Support Vector Machines (SVM) from the scikit-learn library. Your solution should demonstrate the following: 1. Load and preprocess the data. 2. Implement both linear and RBF kernel-based SVM classifiers. 3. Use the one-vs-one (`decision_function_shape=\'ovo\'`) approach for multi-class classification with the RBF kernel. 4. Perform a parameter grid search to find the optimal SVM parameters (`C` and `gamma` for RBF kernel) using `GridSearchCV`. 5. Handle any class imbalance using the `class_weight` parameter. 6. Evaluate the performance of your models using accuracy score and confusion matrix. Input Format 1. `iris.csv` file containing the following columns: - `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, `species` - `species` is the target variable: `setosa`, `versicolor`, `virginica` Expected Output - Print the best parameters found for the RBF kernel SVM. - Confusion Matrix and accuracy score for both linear and RBF kernel SVMs on the test data. Constraints 1. Use scikit-learn library for SVM implementation. 2. Use 80-20 train-test split for evaluation. Example Usage ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC, LinearSVC from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.preprocessing import StandardScaler def svm_classification(): # Load the dataset df = pd.read_csv(\'iris.csv\') X = df.drop(columns=[\'species\']) y = df[\'species\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler = StandardScaler().fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) # Linear SVM linear_svc = LinearSVC(random_state=42, class_weight=\'balanced\') linear_svc.fit(X_train, y_train) linear_pred = linear_svc.predict(X_test) print(\\"Linear SVM Accuracy:\\", accuracy_score(y_test, linear_pred)) print(\\"Linear SVM Confusion Matrixn\\", confusion_matrix(y_test, linear_pred)) # RBF Kernel SVM with Grid Search rbf_svc = SVC(kernel=\'rbf\', random_state=42, class_weight=\'balanced\') param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} grid_search = GridSearchCV(rbf_svc, param_grid, cv=5) grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ print(\\"Best parameters for RBF Kernel SVM:\\", best_params) # Evaluate RBF Kernel SVM rbf_pred = grid_search.predict(X_test) print(\\"RBF SVM Accuracy:\\", accuracy_score(y_test, rbf_pred)) print(\\"RBF SVM Confusion Matrixn\\", confusion_matrix(y_test, rbf_pred)) # Call the function svm_classification() ``` Note: You can use the provided dataset to test your implementation.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC, LinearSVC from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.preprocessing import StandardScaler def load_and_preprocess_data(file_path): df = pd.read_csv(file_path) X = df.drop(columns=[\'species\']) y = df[\'species\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler().fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def linear_svm_classification(X_train, y_train, X_test, y_test): linear_svc = LinearSVC(random_state=42, class_weight=\'balanced\', max_iter=10000) linear_svc.fit(X_train, y_train) linear_pred = linear_svc.predict(X_test) linear_accuracy = accuracy_score(y_test, linear_pred) linear_conf_matrix = confusion_matrix(y_test, linear_pred) return linear_accuracy, linear_conf_matrix def rbf_svm_classification(X_train, y_train, X_test, y_test): rbf_svc = SVC(kernel=\'rbf\', random_state=42, class_weight=\'balanced\', decision_function_shape=\'ovo\') param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} grid_search = GridSearchCV(rbf_svc, param_grid, cv=5) grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ rbf_pred = grid_search.predict(X_test) rbf_accuracy = accuracy_score(y_test, rbf_pred) rbf_conf_matrix = confusion_matrix(y_test, rbf_pred) return best_params, rbf_accuracy, rbf_conf_matrix def svm_classification(file_path): X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path) linear_accuracy, linear_conf_matrix = linear_svm_classification(X_train, y_train, X_test, y_test) print(\\"Linear SVM Accuracy:\\", linear_accuracy) print(\\"Linear SVM Confusion Matrixn\\", linear_conf_matrix) best_params, rbf_accuracy, rbf_conf_matrix = rbf_svm_classification(X_train, y_train, X_test, y_test) print(\\"Best parameters for RBF Kernel SVM:\\", best_params) print(\\"RBF SVM Accuracy:\\", rbf_accuracy) print(\\"RBF SVM Confusion Matrixn\\", rbf_conf_matrix) # Call the function # Note: Replace \'iris.csv\' with the path to your CSV file if needed # svm_classification(\'iris.csv\')"},{"question":"# Python Coding Assessment: Custom Byte-Compiler Objective: You are tasked with creating a custom Python byte-code compiler that leverages the `compileall` module. The compiler should take a configuration object and compile Python source files based on the provided settings. Your implementation should demonstrate a deep understanding of the `compileall` module and its capabilities. Task: 1. Implement a class `CustomByteCompiler` with the following methods: - `__init__(self, config: dict)`: Initialize the compiler with a configuration dictionary. - `compile_directory(self, directory: str) -> bool`: Use the configuration to compile all `.py` files in the specified directory. - `compile_file(self, file_path: str) -> bool`: Use the configuration to compile a single `.py` file. - `compile_system_path(self) -> bool`: Use the configuration to compile all `.py` files found along the `sys.path`. Configuration Dictionary: The keys of the configuration dictionary may include any of the parameters allowed by the `compile_dir`, `compile_file`, and `compile_path` functions of the `compileall` module, such as: - `maxlevels` - `ddir` - `force` - `rx` - `quiet` - `legacy` - `optimize` - `workers` - `invalidation_mode` - `stripdir` - `prependdir` - `limit_sl_dest` - `hardlink_dupes` Example Configuration: ```python config = { \\"maxlevels\\": 3, \\"force\\": true, \\"quiet\\": 1, \\"optimize\\": 2, \\"workers\\": 4 } ``` Constraints: - Modify the configuration object to include a new key `\'compile_type\'` which specifies whether to compile a directory (`\'directory\'`), a file (`\'file\'`), or the system path (`\'system_path\'`). - Ensure that `compile_directory` and `compile_file` methods return boolean values indicating success (`True`) or failure (`False`). Expected Input and Output: 1. **Input:** - Configuration object - Directory path (for `compile_directory`) - File path (for `compile_file`) 2. **Output:** - A boolean value indicating the success or failure of the compilation process. Example Usage: ```python config = { \\"compile_type\\": \\"directory\\", \\"maxlevels\\": 3, \\"force\\": True, \\"quiet\\": 1, \\"optimize\\": 2, \\"workers\\": 4 } compiler = CustomByteCompiler(config) result = compiler.compile_directory(\\"/path/to/directory\\") print(result) # Should print True or False based on compilation success config[\\"compile_type\\"] = \\"file\\" compiler = CustomByteCompiler(config) result = compiler.compile_file(\\"/path/to/file.py\\") print(result) # Should print True or False based on compilation success config[\\"compile_type\\"] = \\"system_path\\" compiler = CustomByteCompiler(config) result = compiler.compile_system_path() print(result) # Should print True or False based on compilation success ``` Notes: - You must handle any exceptions that may arise during the compilation process and return `False` in case of any errors. - Feel free to add any helper methods as needed to improve code organization and readability.","solution":"import compileall import sys class CustomByteCompiler: def __init__(self, config: dict): self.config = config def compile_directory(self, directory: str) -> bool: try: return compileall.compile_dir(directory, **self.config) except Exception as e: print(f\\"Error compiling directory: {e}\\") return False def compile_file(self, file_path: str) -> bool: try: return compileall.compile_file(file_path, **self.config) except Exception as e: print(f\\"Error compiling file: {e}\\") return False def compile_system_path(self) -> bool: try: return compileall.compile_path(**self.config) except Exception as e: print(f\\"Error compiling system path: {e}\\") return False"},{"question":"Objective: Implement a multithreaded TCP server using Python\'s `socketserver` module that can handle multiple clients simultaneously. Your server should: - Accept connections from multiple clients. - Echo back any data received from a client, but reversed. - Print the client\'s address and the reversed message. Requirements: - You must use `socketserver.ThreadingMixIn` to make the server handle requests asynchronously. - Implement a custom request handler by subclassing `socketserver.BaseRequestHandler`. - Override the `handle()` method to perform the task of reversing and sending data back to the client. - Set up the server to run until interrupted manually (e.g., by a keyboard interrupt). Implementation Details: 1. **Request Handler:** - Subclass `socketserver.BaseRequestHandler`. - Override the `handle()` method. It should: - Read data from the client. - Reverse the data. - Send the reversed data back to the client. - Print the client\'s address and the reversed message. 2. **Server Class:** - Create a class that inherits from `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. 3. **Main Logic:** - Set up the server to listen on `localhost` and port `9999`. - Keep the server running until interrupted manually. Example: The server should exhibit the following behavior: - When a client sends \\"hello\\", the server should respond with \\"olleh\\". - The server should print the client\'s address and the reversed message. Here is an example interaction: _Server prints:_ ``` (\'127.0.0.1\', 54321) sent: olleh ``` _Client receives:_ ``` olleh ``` Constraints: - Assume the data sent and received will not exceed 1024 bytes. - Use standard Python libraries only. Performance Requirements: - The server should be capable of handling at least 10 simultaneous connections without a significant delay or crash. Boilerplate Code: ```python import socketserver class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): # Implement your logic here pass class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), MyTCPHandler) as server: print(\\"Server running on {}:{}\\".format(HOST, PORT)) server.serve_forever() ``` Complete the `handle` method to perform the described tasks and test the server using a simple client that sends and receives messages.","solution":"import socketserver class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): # Read data from the client, up to 1024 bytes self.data = self.request.recv(1024).strip() print(f\\"{self.client_address} wrote:\\") print(self.data) # Reverse the received data reversed_data = self.data[::-1] # Send the reversed data back to the client self.request.sendall(reversed_data) # Print the client\'s address and the reversed message print(f\\"{self.client_address} sent: {reversed_data}\\") class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), MyTCPHandler) as server: print(\\"Server running on {}:{}\\".format(HOST, PORT)) server.serve_forever()"},{"question":"# Advanced Coding Assessment: Custom Statistical Calculator **Objective**: Demonstrate your understanding of the Python `math` module by creating a custom statistical calculator that performs various computations based on given data. **Problem Statement**: You need to implement a custom statistical calculator that offers the following functionalities: 1. **Calculate Mean**: Compute the mean (average) of a given list of numbers. 2. **Calculate Standard Deviation**: Compute the standard deviation of a given list of numbers. 3. **Calculate Combination**: Return the number of ways to choose `k` items from `n` items without repetition and without order. 4. **Compute Hypotenuse**: Given the lengths of the perpendicular sides of a right triangle, compute the length of the hypotenuse. 5. **Angular Conversion**: Convert a given angle in degrees to radians. # Function Specifications: 1. `calculate_mean(numbers: List[float]) -> float` * **Input**: A list of floating-point numbers. * **Output**: A float representing the mean of the input list. 2. `calculate_std_deviation(numbers: List[float]) -> float` * **Input**: A list of floating-point numbers. * **Output**: A float representing the standard deviation of the input list. 3. `calculate_combination(n: int, k: int) -> int` * **Input**: Two integers `n` and `k`. * **Output**: An integer representing the number of ways to choose `k` items from `n` items without repetition and without order. 4. `calculate_hypotenuse(a: float, b: float) -> float` * **Input**: Two floating-point numbers representing the lengths of the perpendicular sides of a right triangle. * **Output**: A float representing the length of the hypotenuse. 5. `degrees_to_radians(degrees: float) -> float` * **Input**: A floating-point number representing an angle in degrees. * **Output**: A float representing the angle in radians. # Constraints: * The list of numbers for mean and standard deviation calculations will have at least one number. * For `calculate_combination`, `0 <= k <= n`. * All inputs will be valid. # Example Usage: ```python # Example inputs numbers = [1.0, 2.0, 3.0, 4.0, 5.0] n = 5 k = 3 a = 3.0 b = 4.0 degrees = 90.0 # Calculations mean = calculate_mean(numbers) std_deviation = calculate_std_deviation(numbers) combination = calculate_combination(n, k) hypotenuse = calculate_hypotenuse(a, b) radians = degrees_to_radians(degrees) # Expected outputs print(mean) # 3.0 print(std_deviation) # 1.5811388300841898 print(combination) # 10 print(hypotenuse) # 5.0 print(radians) # 1.5707963267948966 ``` Implement these methods to build a robust mathematical utility using Python\'s `math` module. # Submission: Submit your solution as a Python script containing the required functions. Make sure to include error handling where appropriate. You are not required to write the main execution block, but you can include test cases to demonstrate the correctness of your implementation.","solution":"import math from typing import List def calculate_mean(numbers: List[float]) -> float: Calculate the mean (average) of a given list of numbers. return sum(numbers) / len(numbers) def calculate_std_deviation(numbers: List[float]) -> float: Calculate the standard deviation of a given list of numbers. mean = calculate_mean(numbers) variance = sum((x - mean) ** 2 for x in numbers) / len(numbers) return math.sqrt(variance) def calculate_combination(n: int, k: int) -> int: Return the number of ways to choose `k` items from `n` items without repetition and without order. return math.comb(n, k) def calculate_hypotenuse(a: float, b: float) -> float: Compute the length of the hypotenuse of a right triangle given the lengths of the perpendicular sides. return math.hypot(a, b) def degrees_to_radians(degrees: float) -> float: Convert a given angle in degrees to radians. return math.radians(degrees)"},{"question":"# Compression and Decompression with bz2 You are working on an application that requires efficient data storage and retrieval. To achieve this, you plan to use the `bz2` module for compressing data before storing it and decompressing it when retrieving it. Task Write a function `compress_and_save_to_file(text, filepath)` that: 1. Takes a string `text` and a string `filepath` representing the path to a file. 2. Compresses the `text` using the highest compression level. 3. Saves the compressed data to the specified file. Next, write another function `load_and_decompress_from_file(filepath)` that: 1. Takes a string `filepath` representing the path to a compressed file. 2. Decompresses the data from the file. 3. Returns the original uncompressed text. Requirements - Use the `bz2` module to handle compression and decompression. - Ensure that the compression level used in `compress_and_save_to_file` is set to the highest level (9). - Handle any potential exceptions that might occur during file operations (e.g., file not found, read/write errors). Function Signatures ```python def compress_and_save_to_file(text: str, filepath: str) -> None: pass def load_and_decompress_from_file(filepath: str) -> str: pass ``` Example Usage ```python # Define sample text and filepath sample_text = \\"This is a sample text for compression and decompression.\\" sample_filepath = \\"sample.bz2\\" # Compress and save to file compress_and_save_to_file(sample_text, sample_filepath) # Load and decompress from file decompressed_text = load_and_decompress_from_file(sample_filepath) # Verify correctness assert decompressed_text == sample_text ``` **Note**: The file `sample.bz2` should contain the compressed data, and decompressing it should return the original `sample_text`. Evaluation Criteria - Correctness: The functions should correctly compress and save the data, then load and decompress it. - Robustness: The functions should handle exceptions properly. - Code Quality: The code should be well-structured and readable.","solution":"import bz2 def compress_and_save_to_file(text: str, filepath: str) -> None: Compresses the given text using bz2 with the highest compression level and saves it to a file. Args: - text (str): The text to be compressed. - filepath (str): The path to the file where compressed data will be saved. try: compressed_data = bz2.compress(text.encode(\'utf-8\'), compresslevel=9) with open(filepath, \'wb\') as file: file.write(compressed_data) except Exception as e: print(f\\"An error occurred during compression and saving: {e}\\") raise def load_and_decompress_from_file(filepath: str) -> str: Loads compressed data from a file, decompresses it, and returns the original text. Args: - filepath (str): The path to the file containing compressed data. Returns: - str: The decompressed text. try: with open(filepath, \'rb\') as file: compressed_data = file.read() decompressed_data = bz2.decompress(compressed_data) return decompressed_data.decode(\'utf-8\') except Exception as e: print(f\\"An error occurred during loading and decompression: {e}\\") raise"},{"question":"# Question: Implementing and Evaluating Stochastic Gradient Descent (SGD) Classifier You are given a dataset containing numerical features and a binary target variable. Your task is to implement an SGD classifier using scikit-learn, apply necessary preprocessing, and evaluate its performance. Requirements: 1. **Data Preprocessing:** - Load the dataset (provided as a CSV file) into a pandas DataFrame. - Split the data into training and test sets (80-20 split). - Standardize the features using `StandardScaler`. 2. **Model Implementation:** - Implement an SGD classifier using `SGDClassifier` with a logistic loss function (logistic regression). - Use L2 penalty for regularization. - Set `max_iter` to 1000 and `tol` to 1e-3. 3. **Model Training:** - Fit the classifier to the training data. - Ensure the training data is shuffled before fitting. 4. **Model Evaluation:** - Predict the target variable for the test set. - Evaluate the model using accuracy score and ROC-AUC score. 5. **Hyperparameter Tuning:** - Use `GridSearchCV` to find the best regularization parameter (`alpha` ranging from `0.0001` to `0.1`) and `max_iter` ranging from 500 to 1500. - Update the classifier with the best parameters and re-evaluate the performance. Input Format: - A CSV file named `data.csv` with features and a binary target variable `target`. Output Format: - Print the initial accuracy score and ROC-AUC score. - Print the best parameters found using `GridSearchCV`. - Print the accuracy score and ROC-AUC score with the best parameters. Constraints: - You should use scikit-learn\'s `SGDClassifier`, `StandardScaler`, and `GridSearchCV`. - Ensure the implementation is efficient and follows best practices. Example: ```python import pandas as pd from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, roc_auc_score # Load data data = pd.read_csv(\'data.csv\') X = data.drop(columns=[\'target\']) y = data[\'target\'] # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement initial SGD classifier clf = SGDClassifier(loss=\'log_loss\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) clf.fit(X_train, y_train) # Evaluate initial model y_pred = clf.predict(X_test) initial_accuracy = accuracy_score(y_test, y_pred) initial_roc_auc = roc_auc_score(y_test, y_pred) print(f\'Initial Accuracy: {initial_accuracy}\') print(f\'Initial ROC-AUC: {initial_roc_auc}\') # Hyperparameter tuning using GridSearchCV param_grid = {\'alpha\': [0.0001, 0.001, 0.01, 0.1], \'max_iter\': [500, 1000, 1500]} grid_search = GridSearchCV(clf, param_grid, scoring=\'roc_auc\', cv=5) grid_search.fit(X_train, y_train) # Best parameters and evaluation best_params = grid_search.best_params_ best_clf = grid_search.best_estimator_ y_pred_best = best_clf.predict(X_test) best_accuracy = accuracy_score(y_test, y_pred_best) best_roc_auc = roc_auc_score(y_test, y_pred_best) print(f\'Best Parameters: {best_params}\') print(f\'Best Accuracy: {best_accuracy}\') print(f\'Best ROC-AUC: {best_roc_auc}\') ```","solution":"import pandas as pd from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import accuracy_score, roc_auc_score def load_data(file_path): return pd.read_csv(file_path) def preprocess_data(df): X = df.drop(columns=[\'target\']) y = df[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_initial_model(X_train, y_train): clf = SGDClassifier(loss=\'log_loss\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) clf.fit(X_train, y_train) return clf def evaluate_model(clf, X_test, y_test): y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) roc_auc = roc_auc_score(y_test, y_pred) return accuracy, roc_auc def perform_grid_search(X_train, y_train): param_grid = {\'alpha\': [0.0001, 0.001, 0.01, 0.1], \'max_iter\': [500, 1000, 1500]} clf = SGDClassifier(loss=\'log_loss\', penalty=\'l2\', random_state=42) grid_search = GridSearchCV(clf, param_grid, scoring=\'roc_auc\', cv=5) grid_search.fit(X_train, y_train) return grid_search.best_params_, grid_search.best_estimator_ def main(file_path): data = load_data(file_path) X_train, X_test, y_train, y_test = preprocess_data(data) initial_clf = train_initial_model(X_train, y_train) initial_accuracy, initial_roc_auc = evaluate_model(initial_clf, X_test, y_test) print(f\'Initial Accuracy: {initial_accuracy}\') print(f\'Initial ROC-AUC: {initial_roc_auc}\') best_params, best_clf = perform_grid_search(X_train, y_train) best_accuracy, best_roc_auc = evaluate_model(best_clf, X_test, y_test) print(f\'Best Parameters: {best_params}\') print(f\'Best Accuracy: {best_accuracy}\') print(f\'Best ROC-AUC: {best_roc_auc}\')"},{"question":"You are required to implement a simulation of an asynchronous task management system using the `asyncio` module and the `Future` class. The system should: 1. Create multiple asynchronous tasks that perform simple computations and set their results into Future objects after a delay. 2. Implement a function to manage these tasks and gather their results. 3. Use callbacks to handle the completion of each task and collect results in an organized manner. Task Description 1. **Create Tasks Function**: - Implement a function `create_tasks(num_tasks: int) -> List[asyncio.Future]` which: - Creates a list of `num_tasks` asyncio `Future` objects. - Each Future completes after a delay and sets its result to `f\'Task {i} completed\'` where `i` is the task number. 2. **Task Manager Function**: - Implement a function `manage_tasks(futures: List[asyncio.Future]) -> Dict[str, str]` which: - Accepts a list of Future objects. - Waits for all Futures to complete. - Collects and returns their results in a dictionary with keys `Task {i}` and values being the results. 3. **Callback Function**: - Implement a callback function `task_callback(fut: asyncio.Future)` which: - Prints the result of the completed Future along with a message indicating the task has finished. - Example output: `Completed: Task {i} completed`. Additional Details - Make sure tasks complete their actions asynchronously and demonstrate the usage of the `set_result`, `add_done_callback`, `await`, and other relevant methods documented in the provided references. - Implement proper error handling, ensuring the system gracefully handles any exceptions that may arise. Constraints - The number of tasks `num_tasks` will be in the range [1, 100]. - Each task will have a random delay between 1 to 3 seconds before setting its result. # Input ```python num_tasks = 5 ``` # Expected Output ```python { \'Task 0\': \'Task 0 completed\', \'Task 1\': \'Task 1 completed\', \'Task 2\': \'Task 2 completed\', \'Task 3\': \'Task 3 completed\', \'Task 4\': \'Task 4 completed\' } ``` Example Usage ```python import asyncio from typing import List, Dict async def main(): num_tasks = 5 futures = await create_tasks(num_tasks) results = await manage_tasks(futures) print(results) if __name__ == \'__main__\': asyncio.run(main()) ``` Implement the functions `create_tasks`, `manage_tasks`, and `task_callback` and integrate them into the example usage provided. Your implementation should be robust and should demonstrate an in-depth understanding of the asyncio `Future` and related concepts.","solution":"import asyncio import random from typing import List, Dict async def task_runner(i: int, future: asyncio.Future): # Simulate a delay await asyncio.sleep(random.randint(1, 3)) # Set the future result future.set_result(f\'Task {i} completed\') def task_callback(fut: asyncio.Future): print(f\'Completed: {fut.result()}\') async def create_tasks(num_tasks: int) -> List[asyncio.Future]: futures = [] for i in range(num_tasks): future = asyncio.get_event_loop().create_future() future.add_done_callback(task_callback) asyncio.create_task(task_runner(i, future)) futures.append(future) return futures async def manage_tasks(futures: List[asyncio.Future]) -> Dict[str, str]: results = await asyncio.gather(*futures) return {f\'Task {i}\': result for i, result in enumerate(results)} # Example usage async def main(): num_tasks = 5 futures = await create_tasks(num_tasks) results = await manage_tasks(futures) print(results) if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# Binary Data Manipulation and Encoding with Python Libraries You are required to implement a function that reads a custom binary file format, decodes some encoded text data from it, and then re-encodes it into another specified format. The custom binary file format consists of a header followed by multiple records. Each record contains some packed binary data and encoded text. Here\'s the detailed structure of the binary file: 1. Header - `4 bytes` - An unsigned integer indicating the number of records. 2. Records (repeat for each record) - `8 bytes` - A signed 64-bit integer. - `10 bytes` - A floating-point number in IEEE 754 binary64 format. - Variable-length text data encoded in UTF-8. The length of this text data is determined by reading the next `2 bytes` (unsigned short) which indicates the length of the text. The function has the following signature: ```python def process_binary_data(input_file: str, output_encoding: str) -> List[str]: ``` Input: - `input_file` (str): The path to the input binary file. - `output_encoding` (str): The encoding format to which the text data should be converted (e.g., \'utf-16\', \'ascii\'). Output: - `List[str]`: A list of strings where each string represents the re-encoded text data from each record. Constraints: - The input binary file is well-formed. - The specified `output_encoding` is a valid codec supported by the `codecs` module. Example: Suppose the input binary file contains the following data: - Header: `2` records - Record 1: - Signed 64-bit integer: `12345` - Floating-point: `3.14159` - UTF-8 text: `\\"Hello!\\"` (length `6` - `2 bytes` prefix `x00x06`) - Record 2: - Signed 64-bit integer: `67890` - Floating-point: `2.71828` - UTF-8 text: `\\"World!\\"` (length `6` - `2 bytes` prefix `x00x06`) If the `output_encoding` is \'ascii\', the function should return: ```python [\'Hello!\', \'World!\'] ``` If the `output_encoding` is \'utf-16\', the function should return: ```python [\'xffxfeHx00ex00lx00lx00ox00!x00\', \'xffxfeWx00ox00rx00lx00dx00!x00\'] ``` Implement the function by appropriately reading the binary file, decoding the text data, and re-encoding it into the specified format.","solution":"import struct from typing import List def process_binary_data(input_file: str, output_encoding: str) -> List[str]: result = [] with open(input_file, \'rb\') as f: # Read the number of records from the header header = f.read(4) num_records = struct.unpack(\'<I\', header)[0] for _ in range(num_records): # Read the record data int_data = f.read(8) float_data = f.read(8) text_length_data = f.read(2) text_length = struct.unpack(\'<H\', text_length_data)[0] text_data = f.read(text_length) # Decode the text data from UTF-8 decoded_text = text_data.decode(\'utf-8\') # Re-encode the text data to the specified encoding re_encoded_text = decoded_text.encode(output_encoding) result.append(re_encoded_text) return result"},{"question":"# Python Coding Assessment Question Objective Create a detailed report concerning the current state of Python\'s interpreter environment. The report should include the following: 1. A list of currently loaded modules, alongside their sizes. 2. The current search path for module imports. 3. A list of all built-in module names. Instructions 1. Define a function named `generate_sys_report()` that generates this report. 2. The function should return a dictionary with the following structure: ```python { \'loaded_modules\': [ {\'module_name\': \'module1\', \'size\': 100}, ... ], \'search_path\': [ \'path1\', \'path2\', ... ], \'builtin_modules\': [ \'builtin_module1\', ... ] } ``` 3. Use appropriate `sys` module functions and attributes to gather the required information. 4. Ensure that the function handles exceptions and edge cases gracefully. Constraints - The function must be compatible with Python 3.7 or later, as it should use features specified in the documentation provided. - Avoid using any external libraries to collect the data. Rely solely on the `sys` module and built-in Python functionalities. Example ```python def generate_sys_report(): import sys report = { \'loaded_modules\': [], \'search_path\': sys.path, \'builtin_modules\': list(sys.builtin_module_names) } for module_name, module in sys.modules.items(): try: module_size = sys.getsizeof(module) except TypeError: module_size = \'unknown\' report[\'loaded_modules\'].append({ \'module_name\': module_name, \'size\': module_size }) return report ``` This function should provide a comprehensive look at the current state of the Python interpreter environment, validating the students\' understanding of the `sys` module and their ability to use its functionalities effectively.","solution":"def generate_sys_report(): import sys report = { \'loaded_modules\': [], \'search_path\': sys.path, \'builtin_modules\': list(sys.builtin_module_names) } for module_name, module in sys.modules.items(): try: module_size = sys.getsizeof(module) except TypeError: module_size = \'unknown\' report[\'loaded_modules\'].append({ \'module_name\': module_name, \'size\': module_size }) return report"},{"question":"**Question:** You are tasked with implementing a function that processes user data stored in a NIS map to perform a specific operation. Using the `nis` module, write a function `retrieve_and_process_userinfo(mapname, domain=None)` that: 1. Retrieves the entire map from the provided `mapname` using the `nis.cat` function. 2. Each key in the map represents a username, and the corresponding value is the user information in the format `\\"<full_name>:<uid>:<gid>:<home_directory>:<shell>\\"`. 3. The function should return a dictionary where the keys are usernames and the values are another dictionary with the structure: ```python { \\"full_name\\": <full_name>, \\"uid\\": int(<uid>), \\"gid\\": int(<gid>), \\"home_directory\\": <home_directory>, \\"shell\\": <shell> } ``` **Function Signature:** ```python def retrieve_and_process_userinfo(mapname: str, domain: Optional[str] = None) -> Dict[str, Dict[str, Any]]: pass ``` **Input:** - `mapname` (str): Name of the NIS map. - `domain` (Optional[str]): NIS domain. If `None`, the default domain should be used. **Output:** - Returns a dictionary with the structure shown above. **Constraints:** - Assume that all usernames and corresponding user information are valid and well-formatted. - The NIS system is available and accessible. **Example Usage:** ```python user_info = retrieve_and_process_userinfo(\\"passwd.byname\\") # user_info might look like: # { # \\"john_doe\\": { # \\"full_name\\": \\"John Doe\\", # \\"uid\\": 1001, # \\"gid\\": 1001, # \\"home_directory\\": \\"/home/john_doe\\", # \\"shell\\": \\"/bin/bash\\" # }, # ... # } ``` **Note:** - You must handle exceptions appropriately; if a `nis.error` is raised during the function execution, an empty dictionary should be returned.","solution":"import nis from typing import Dict, Any, Optional def retrieve_and_process_userinfo(mapname: str, domain: Optional[str] = None) -> Dict[str, Dict[str, Any]]: try: # Retrieve the entire NIS map nis_map = nis.cat(mapname) except nis.error: # In case of any NIS error, return an empty dictionary return {} user_info = {} for username, info in nis_map.items(): parts = info.split(\':\') user_info[username] = { \\"full_name\\": parts[0], \\"uid\\": int(parts[1]), \\"gid\\": int(parts[2]), \\"home_directory\\": parts[3], \\"shell\\": parts[4] } return user_info"},{"question":"# Custom Numeric Type Implementation in Python Objective: Implement a custom numeric type in Python that supports standard arithmetic and bitwise operations using the Python C API equivalents described in the provided documentation. Description: Create a Python class named `CustomNumber` that mimics the behavior of Python\'s built-in numeric types. This class should support the following operations: 1. **Addition (`+`)**: Use `PyNumber_Add`. 2. **Subtraction (`-`)**: Use `PyNumber_Subtract`. 3. **Multiplication (`*`)**: Use `PyNumber_Multiply`. 4. **True Division (`/`)**: Use `PyNumber_TrueDivide`. 5. **Floor Division (`//`)**: Use `PyNumber_FloorDivide`. 6. **Remainder (`%`)**: Use `PyNumber_Remainder`. 7. **Power (`**`)**: Use `PyNumber_Power`. 8. **Bitwise AND (`&`)**: Use `PyNumber_And`. 9. **Bitwise OR (`|`)**: Use `PyNumber_Or`. 10. **Bitwise XOR (`^`)**: Use `PyNumber_Xor`. 11. **Left Shift (`<<`)**: Use `PyNumber_Lshift`. 12. **Right Shift (`>>`)**: Use `PyNumber_Rshift`. 13. **Negation (`-`)**: Use `PyNumber_Negative`. 14. **Absolute Value (`abs()`)**: Use `PyNumber_Absolute`. Constraints: - All operations should handle exceptions and edge cases gracefully. - Type conversions using `PyNumber_Long`, `PyNumber_Float`, and others should be implemented to support typecasting. - Ensure the performance of each operation is optimized. Input: - Two instances of `CustomNumber` for binary operations. - One instance of `CustomNumber` for unary operations. Output: - A new `CustomNumber` instance representing the result of the operation. - Proper handling of errors with appropriate messages. Example: ```python # Example of usage (not the full implementation detail) a = CustomNumber(10) b = CustomNumber(5) result_add = a + b # Should use PyNumber_Add result_sub = a - b # Should use PyNumber_Subtract result_mul = a * b # Should use PyNumber_Multiply result_truediv = a / b # Should use PyNumber_TrueDivide result_floordiv = a // b # Should use PyNumber_FloorDivide result_rem = a % b # Should use PyNumber_Remainder result_pow = a ** b # Should use PyNumber_Power result_and = a & b # Should use PyNumber_And result_or = a | b # Should use PyNumber_Or result_xor = a ^ b # Should use PyNumber_Xor result_lshift = a << b # Should use PyNumber_Lshift result_rshift = a >> b # Should use PyNumber_Rshift result_neg = -a # Should use PyNumber_Negative result_abs = abs(a) # Should use PyNumber_Absolute ``` **Note:** The actual implementation of the `CustomNumber` class is left as the task for the student. The above examples and descriptions provide a guideline on the expected behavior and the functions to use.","solution":"class CustomNumber: def __init__(self, value): self.value = value def __add__(self, other): return CustomNumber(self.value + other.value) def __sub__(self, other): return CustomNumber(self.value - other.value) def __mul__(self, other): return CustomNumber(self.value * other.value) def __truediv__(self, other): return CustomNumber(self.value / other.value) def __floordiv__(self, other): return CustomNumber(self.value // other.value) def __mod__(self, other): return CustomNumber(self.value % other.value) def __pow__(self, other): return CustomNumber(self.value ** other.value) def __and__(self, other): return CustomNumber(self.value & other.value) def __or__(self, other): return CustomNumber(self.value | other.value) def __xor__(self, other): return CustomNumber(self.value ^ other.value) def __lshift__(self, other): return CustomNumber(self.value << other.value) def __rshift__(self, other): return CustomNumber(self.value >> other.value) def __neg__(self): return CustomNumber(-self.value) def __abs__(self): return CustomNumber(abs(self.value)) def __repr__(self): return f\\"CustomNumber({self.value})\\""},{"question":"# Python Coding Assessment Objective: Demonstrate proficiency in intermediate to advanced Python concepts using modules from the standard library. Problem Statement: Imagine you are working on a logging system that organizes log files for a multi-threaded application. The system must perform the following tasks: 1. Format and print log messages concisely. 2. Generate logs using multiple threads. 3. Track memory usage of certain data objects using weak references. 4. Use decimal arithmetic for any financial calculations, ensuring exact representation. 5. Structure and manage a scheduled queue of log messages. Task: Write a Python program that: 1. Creates a log message generator running in a separate thread which generates a log entry every second. 2. Stores log messages in a deque until further processing. 3. Uses weak references to manage the data objects ensuring they are garbage-collected when no longer needed. 4. Formats and prints the log details using the `pprint` module, ensuring readability even for nested data structures. 5. Performs a financial calculation on each log entry (e.g., computing taxes on a synthetic log attribute that represents a monetary amount). Implementation Requirements: - **Input**: No direct user input required, synthetic log messages can be generated for demonstration. - **Output**: Printed log messages, formatted using `pprint`. - **Constraints**: - Each log message should include a timestamp, a message string, and a synthetic monetary amount. - Log generation should run in a separate thread. - Use weak references to manage log message objects. - Perform a sample decimal calculation on the monetary amount in each log. Expected Output: The program should output formatted log messages generated by the thread, including timestamps and computed tax values. Guidelines: - Use the `threading` module to run log generation in a separate thread. - Store and manage log entries with a `collections.deque`. - Ensure all log messages are stored with weak references. - Use the `pprint` module for printing formatted log messages. - Compute and print a tax (e.g., 5%) on the synthetic monetary amount in each log using the `decimal` module. Sample skeleton code for reference: ```python import threading import time import weakref from collections import deque from decimal import Decimal, getcontext import pprint class LogGenerator(threading.Thread): def __init__(self, log_queue): threading.Thread.__init__(self) self.log_queue = log_queue def run(self): # Generate log entries with some interval while True: timestamp = time.strftime(\'%Y-%m-%d %H:%M:%S\') message = f\\"Log message at {timestamp}\\" amount = Decimal(\'0.70\') # Synthetic monetary amount log_entry = {\'timestamp\': timestamp, \'message\': message, \'amount\': amount} print(\\"Generated a log entry:\\", log_entry) self.log_queue.append(log_entry) time.sleep(1) # Sleep for 1 second between log generations def financial_calculation(log_entry): # Calculate a tax of 5% on the synthetic monetary amount tax = Decimal(\'1.05\') amount = log_entry[\'amount\'] * tax return round(amount, 2) def main(): # Initialize a deque for log storage log_queue = deque() # Start log generation in a separate thread log_gen = LogGenerator(log_queue) log_gen.start() # Initialize weak references for log management weak_log_queue = weakref.WeakValueDictionary() # Process logs and format output while True: if log_queue: log_entry = log_queue.popleft() # Perform financial calculation log_entry[\'calculated_amount\'] = financial_calculation(log_entry) # Add to weak reference dictionary key = log_entry[\'timestamp\'] weak_log_queue[key] = log_entry # Print formatted log entry pprint.pprint(log_entry) time.sleep(1) if __name__ == \\"__main__\\": main() ```","solution":"import threading import time import weakref from collections import deque from decimal import Decimal, getcontext import pprint class LogGenerator(threading.Thread): def __init__(self, log_queue): threading.Thread.__init__(self) self.log_queue = log_queue def run(self): # Generate log entries with some interval while True: timestamp = time.strftime(\'%Y-%m-%d %H:%M:%S\') message = f\\"Log message at {timestamp}\\" amount = Decimal(\'100.00\') # Synthetic monetary amount log_entry = {\'timestamp\': timestamp, \'message\': message, \'amount\': amount} print(\\"Generated a log entry:\\", log_entry) self.log_queue.append(log_entry) time.sleep(1) # Sleep for 1 second between log generations def financial_calculation(log_entry): # Calculate a tax of 5% on the synthetic monetary amount tax_rate = Decimal(\'0.05\') tax_amount = log_entry[\'amount\'] * tax_rate total_amount = log_entry[\'amount\'] + tax_amount return round(total_amount, 2) def main(): # Start log generation in a separate thread log_queue = deque() log_gen = LogGenerator(log_queue) log_gen.start() # Initialize weak references for log management weak_log_queue = weakref.WeakValueDictionary() # Process logs and format output while True: if log_queue: log_entry = log_queue.popleft() # Perform financial calculation log_entry[\'calculated_amount\'] = financial_calculation(log_entry) # Add to weak reference dictionary key = log_entry[\'timestamp\'] weak_log_queue[key] = log_entry # Print formatted log entry pprint.pprint(log_entry) time.sleep(1) if __name__ == \\"__main__\\": main()"},{"question":"Objective You are required to demonstrate your understanding of scikit-learn\'s performance optimizations by implementing a function that predicts output for a given model and dataset. Your function should consider different configurations for improved performance, such as bulk prediction handling, input data representation, and model complexity adjustments. Problem Statement Implement a function `optimized_predict` that makes predictions using a provided scikit-learn model and dataset. The function should support both sparse and dense input data representations and handle bulk predictions efficiently. Additionally, allow adjustments to the model\'s complexity to observe changes in prediction latency. # Input Specifications 1. **model**: A scikit-learn estimator that has been already fitted. 2. **X**: A 2D numpy array (or equivalent sparse matrix) representing the input features. 3. **bulk_size**: An integer representing the number of instances to predict simultaneously in bulk mode (default is `None`, which means no bulk processing). 4. **assume_finite**: A boolean indicating whether to disable input validation for finite values (default is `False`). 5. **complexity_param**: A dictionary containing parameters related to the model complexity that can be set before prediction (default is `None`). # Output Specification The function should return a numpy array containing the predicted values. # Constraints - You may assume that the input features `X` can be very large and can either be in dense or sparse format. - The `bulk_size` parameter, if not `None`, will always be a positive integer. - The `complexity_param` dictionary will only contain relevant parameters suitable for the specific type of model provided (e.g., `alpha` and `l1_ratio` for elastic net). # Performance Requirements - The function should optimize for prediction latency and throughput. - Handle large datasets efficiently in terms of memory usage and computational power. # Function Signature ```python from typing import Union, Dict import numpy as np import scipy.sparse as sp from sklearn.base import BaseEstimator def optimized_predict(model: BaseEstimator, X: Union[np.ndarray, sp.spmatrix], bulk_size: int = None, assume_finite: bool = False, complexity_param: Dict = None) -> np.ndarray: pass ``` # Example Given: ```python from sklearn.linear_model import SGDClassifier from sklearn.datasets import make_classification # Generating a sample dataset X, y = make_classification(n_samples=1000, n_features=50, random_state=42) model = SGDClassifier() model.fit(X, y) # Sparse input X_sparse = sp.csr_matrix(X) # Parameters bulk_size = 100 assume_finite = True complexity_param = {\'alpha\': 0.001, \'l1_ratio\': 0.15} ``` When: ```python predictions = optimized_predict(model, X_sparse, bulk_size, assume_finite, complexity_param) ``` Expected Output: The produced output should be a numpy array containing the predicted values for the input dataset `X_sparse`, efficiently computed with the provided configurations.","solution":"from typing import Union, Dict import numpy as np import scipy.sparse as sp from sklearn.base import BaseEstimator def optimized_predict(model: BaseEstimator, X: Union[np.ndarray, sp.spmatrix], bulk_size: int = None, assume_finite: bool = False, complexity_param: Dict = None) -> np.ndarray: Makes predictions using the provided scikit-learn model and dataset. Arguments: model : A scikit-learn estimator that has been already fitted. X : A 2D numpy array (or equivalent sparse matrix) representing the input features. bulk_size : An integer representing the number of instances to predict simultaneously in bulk mode (default is None, which means no bulk processing). assume_finite : A boolean indicating whether to disable input validation for finite values (default is False). complexity_param : A dictionary containing parameters related to the model complexity that can be set before prediction (default is None). Returns: np.ndarray: A numpy array containing the predicted values. # Adjust model complexity before prediction if complexity_param: model.set_params(**complexity_param) # Predictions without finite value check if assume_finite: import warnings with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\", category=UserWarning) if bulk_size: predictions = [] for i in range(0, X.shape[0], bulk_size): batch = X[i:i + bulk_size] predictions.append(model.predict(batch)) return np.concatenate(predictions) else: return model.predict(X) else: if bulk_size: predictions = [] for i in range(0, X.shape[0], bulk_size): batch = X[i:i + bulk_size] predictions.append(model.predict(batch)) return np.concatenate(predictions) else: return model.predict(X)"},{"question":"**Objective:** Write a Python function `analyze_whitespace_issues(file_or_dir: str, verbose: bool = False, filename_only: bool = False) -> None` that mimics the behavior of the `tabnanny.check()` function. Your implementation should: 1. If `file_or_dir` is a directory (but not a symbolic link), recursively check all `.py` files within that directory and its subdirectories for inconsistent tab/space usage. 2. If `file_or_dir` is a single `.py` file, check it directly. 3. Output diagnostic messages to the standard output (`print()` statements). 4. The `verbose` flag should control whether detailed messages are printed. 5. The `filename_only` flag should control whether only filenames of files with issues are printed. 6. Raise an exception `WhitespaceIndentationError` if an ambiguous indent is detected, which should be caught and handled appropriately within your function. **Constraints:** 1. Do not use the `tabnanny` module or its functions. 2. Handle file I/O and directory traversal using the `os` and `os.path` modules. 3. Use Python\'s `tokenize` module to help with reading and processing tokens from the files. 4. Assume that ambiguous indentations are defined as lines with a mixture of tabs and spaces. **Function Signature:** ```python def analyze_whitespace_issues(file_or_dir: str, verbose: bool = False, filename_only: bool = False) -> None: pass # Define the custom exception class WhitespaceIndentationError(Exception): pass ``` **Input:** - `file_or_dir` (str): The path to a Python file or a directory containing Python files. - `verbose` (bool): Flag to control the verbosity of the output. - `filename_only` (bool): Flag to control if only filenames with issues are printed. **Output:** - No return value. Use `print()` for diagnostic messages. **Example Usage:** 1. `analyze_whitespace_issues(\'/path/to/directory\', verbose=True, filename_only=False)` 2. `analyze_whitespace_issues(\'/path/to/file.py\', verbose=False, filename_only=True)` **Example Output:** ``` Checking file: script.py Line 10: ambiguous indentation. Checking file: helper.py Inconsistent use of tabs and spaces in file helper.py. Files with issues: - helper.py ``` In the example above, the actual lines and messages should be based on the detection within the files. **Important:** - Pay special attention to edge cases such as empty directories, non-Python files, and permission issues. - Performance is a secondary concern but ensure your solution handles directory with a large number of files efficiently. **Hints:** - Use the `os` module to handle directory traversal and file handling. - The `tokenize` module can help you read and identify tokens from Python source files. - Implement exception handling to manage ambiguous indent issues effectively.","solution":"import os import tokenize class WhitespaceIndentationError(Exception): pass def analyze_whitespace_issues(file_or_dir: str, verbose: bool = False, filename_only: bool = False) -> None: def check_file(file_path): issues_found = False if verbose: print(f\\"Checking file: {file_path}\\") try: with open(file_path, \'r\') as file: tokens = tokenize.generate_tokens(file.readline) for toknum, tokval, start, end, line in tokens: if toknum == tokenize.INDENT: # Checking for mixed tabs and spaces in indentation if \' \' in tokval and \'t\' in tokval: raise WhitespaceIndentationError( f\\"Line {start[0]}: ambiguous indentation.\\") except WhitespaceIndentationError as e: issues_found = True if filename_only: print(file_path) else: print(e) except Exception as e: if verbose: print(f\\"Error processing {file_path}: {e}\\") return issues_found def recursive_check(path): problems = [] if os.path.isfile(path): if path.endswith(\'.py\') and check_file(path): problems.append(path) elif os.path.isdir(path) and not os.path.islink(path): for root, dirs, files in os.walk(path): for file in files: if file.endswith(\'.py\'): file_path = os.path.join(root, file) if check_file(file_path): problems.append(file_path) return problems # Check the provided file_or_dir problems = recursive_check(file_or_dir) if problems: if not filename_only: print(\\"Files with issues:\\") for problem in problems: print(problem)"},{"question":"**Problem Statement:** You are developing a multilingual application where users can switch between multiple languages at runtime. You need to create a system that supports loading translations from `.mo` files, switching languages dynamically, and using the translations in the application. # Requirements: 1. **Setup Function** - Implement `setup_translation(domain, localedir)` function which binds the domain to the given locale directory and sets the domain for translations. - **Input:** - `domain (str)`: The text domain (e.g., \'myapplication\'). - `localedir (str)`: The directory where `.mo` files are located (e.g., \'/path/to/my/language/directory\'). - **Output:** None 2. **Switch Language Function** - Implement `switch_language(domain, languages)` function which sets up translations using the provided domain and a list of language codes. The function should use the `gettext.translation` function to accomplish this. - **Input:** - `domain (str)`: The text domain (e.g., \'myapplication\'). - `languages (list of str)`: List of language codes (e.g., [\'en\', \'fr\', \'de\']). - **Output:** The `_()` function that translates strings in the language set. 3. **Translation Function** - Implement a function `translate(message, domain, localedir, language)` that takes a message and translates it into the specified language using the specified domain and localedir. Use the above setup functions to achieve this. - **Input:** - `message (str)`: The message string to be translated. - `domain (str)`: The text domain (e.g., \'myapplication\'). - `localedir (str)`: The directory where `.mo` files are located. - `language (str)`: The language code (e.g., \'en\'). - **Output:** The translated message (str). # Constraints: - Assume the `.mo` files are in the standard format and directory structure. - You can assume the necessary `.mo` files for translations exist in the specified `localedir`. # Example Usage: ```python # Setup translations setup_translation(\'myapplication\', \'/path/to/my/language/directory\') # Switch to French language _ = switch_language(\'myapplication\', [\'fr\']) # Translate a message message = translate(\'Hello, world!\', \'myapplication\', \'/path/to/my/language/directory\', \'fr\') print(message) # Should print \\"Bonjour, le monde!\\" based on the translation in .mo file ``` # Implementation: Implement the specified functions according to the requirements. Make sure to handle errors gracefully and provide meaningful error messages for missing translations or incorrect configurations. ```python import gettext def setup_translation(domain, localedir): Bind the domain to the given locale directory and set the domain for translations. :param domain: The text domain (e.g., \'myapplication\'). :param localedir: The directory where `.mo` files are located (e.g., \'/path/to/my/language/directory\'). gettext.bindtextdomain(domain, localedir) gettext.textdomain(domain) def switch_language(domain, languages): Set up translations using the provided domain and a list of language codes. :param domain: The text domain (e.g., \'myapplication\'). :param languages: List of language codes (e.g., [\'en\', \'fr\', \'de\']). :return: The `_()` function that translates strings in the language set. translation = gettext.translation(domain, localedir=None, languages=languages, fallback=True) translation.install() return translation.gettext def translate(message, domain, localedir, language): Translate a message into the specified language using the specified domain and localedir. :param message: The message string to be translated. :param domain: The text domain (e.g., \'myapplication\'). :param localedir: The directory where `.mo` files are located. :param language: The language code (e.g., \'en\'). :return: The translated message. setup_translation(domain, localedir) _ = switch_language(domain, [language]) return _(message) ```","solution":"import gettext def setup_translation(domain, localedir): Bind the domain to the given locale directory and set the domain for translations. :param domain: The text domain (e.g., \'myapplication\'). :param localedir: The directory where `.mo` files are located (e.g., \'/path/to/my/language/directory\'). gettext.bindtextdomain(domain, localedir) gettext.textdomain(domain) def switch_language(domain, languages): Set up translations using the provided domain and a list of language codes. :param domain: The text domain (e.g., \'myapplication\'). :param languages: List of language codes (e.g., [\'en\', \'fr\', \'de\']). :return: The `_()` function that translates strings in the language set. translation = gettext.translation(domain, localedir=None, languages=languages, fallback=True) translation.install() return translation.gettext def translate(message, domain, localedir, language): Translate a message into the specified language using the specified domain and localedir. :param message: The message string to be translated. :param domain: The text domain (e.g., \'myapplication\'). :param localedir: The directory where `.mo` files are located. :param language: The language code (e.g., \'en\'). :return: The translated message. setup_translation(domain, localedir) _ = switch_language(domain, [language]) return _(message)"},{"question":"**Objective**: Demonstrate your understanding of the `grp` module by implementing a function that processes Unix group database entries. # Problem Statement: Implement a function `get_group_users(groups: List[str]) -> Dict[str, List[str]]` that takes a list of group names and returns a dictionary where the keys are the group names, and the values are lists of all the usernames that are members of those groups. If a group does not exist, it should be skipped and omitted from the resulting dictionary. Note that each user should only appear once in the list for each group. # Input: - `groups`: A list of group names (strings) that need to be processed. # Output: - A dictionary with group names as keys and lists of member usernames as values. # Constraints: - The list of group names will contain between 1 and 100 group names. - Group names and usernames consist of alphanumeric characters, underscores, and hyphens. - Group names should be treated case-insensitively. # Performance Requirements: - The function should handle group names efficiently, even if the system has a large number of groups. - Ensure no duplication of usernames within the lists of each group. # Example: ```python import grp def get_group_users(groups): # Write your code here # Example Usage print(get_group_users([\'admin\', \'users\', \'developers\'])) # Example Output: # { # \'admin\': [\'user1\', \'user2\'], # \'users\': [\'user3\', \'user4\', \'user1\'], # \'developers\': [\'user5\', \'user2\'] # } ``` **Note**: - You can use `grp.getgrnam(name)` to retrieve group information by name. - Handle exceptions appropriately to skip non-existent groups. - Group names should be treated case-insensitively. Use the documentation of the `grp` module effectively to complete this task.","solution":"import grp def get_group_users(groups): Takes a list of group names and returns a dictionary where the keys are the group names, and the values are lists of all the usernames that are members of those groups. group_users = {} for group in groups: try: grp_info = grp.getgrnam(group) group_users[group] = list(set(grp_info.gr_mem)) except KeyError: # Skip non-existent groups continue return group_users"},{"question":"**Problem Statement** You are given a dataset representing survey responses. The dataset contains a column that categorizes the satisfaction level of respondents as \'Very Unsatisfied\', \'Unsatisfied\', \'Neutral\', \'Satisfied\', and \'Very Satisfied\'. However, due to a data collection error, these values are currently recorded as integers from 1 to 5, where 1 represents \'Very Unsatisfied\', 2 represents \'Unsatisfied\', 3 represents \'Neutral\', 4 represents \'Satisfied\', 5 represents \'Very Satisfied\'. Your task is to convert these integer values to their corresponding categorical data types, ensure the categories are ordered logically, and then compute and display the following: 1. A summary description of the categorical data. 2. The count of responses for each category. 3. The minimum and maximum satisfaction levels. **Input and Output Requirements:** - The input will be a pandas DataFrame with a single column named \'Satisfaction\' containing integer values from 1 to 5. - The output should include: 1. The summary description of the \'Satisfaction\' column. 2. The count of responses for each category. 3. The minimum and maximum satisfaction levels. **Constraints:** - The input DataFrame will have at least one record. - The input values will only be between 1 and 5, inclusive. **Function Signature:** ```python def process_satisfaction_data(df: pd.DataFrame) -> None: # Your code here ``` **Example:** Input: ```python data = { \'Satisfaction\': [1, 4, 2, 5, 3, 1, 2, 4, 5, 3, 2] } df = pd.DataFrame(data) process_satisfaction_data(df) ``` Expected Output: ``` Summary Description: count 11 unique 5 top Neutral freq 3 Name: Satisfaction, dtype: object Count of Responses: Very Unsatisfied 2 Unsatisfied 3 Neutral 2 Satisfied 2 Very Satisfied 2 Name: Satisfaction, dtype: int64 Minimum Satisfaction Level: Very Unsatisfied Maximum Satisfaction Level: Very Satisfied ``` **Guidelines:** 1. Use `CategoricalDtype` from `pandas.api.types` to specify the categories and ordering. 2. Use appropriate pandas functions and methods to manipulate and describe the categorical data.","solution":"import pandas as pd from pandas.api.types import CategoricalDtype def process_satisfaction_data(df: pd.DataFrame) -> None: # Define the categorical data type with logical order categories = [\'Very Unsatisfied\', \'Unsatisfied\', \'Neutral\', \'Satisfied\', \'Very Satisfied\'] cat_type = CategoricalDtype(categories=categories, ordered=True) # Map integer values to their corresponding categorical values df[\'Satisfaction\'] = df[\'Satisfaction\'].map({ 1: \'Very Unsatisfied\', 2: \'Unsatisfied\', 3: \'Neutral\', 4: \'Satisfied\', 5: \'Very Satisfied\' }).astype(cat_type) # Summary description summary = df[\'Satisfaction\'].describe() # Count of responses for each category count_responses = df[\'Satisfaction\'].value_counts() # Minimum and Maximum satisfaction levels min_satisfaction = df[\'Satisfaction\'].min() max_satisfaction = df[\'Satisfaction\'].max() # Print the results print(\\"Summary Description:\\") print(summary) print(\\"nCount of Responses:\\") print(count_responses) print(f\\"nMinimum Satisfaction Level: {min_satisfaction}\\") print(f\\"Maximum Satisfaction Level: {max_satisfaction}\\")"},{"question":"You are provided with a PyTorch tensor, and you need to implement a function that takes this tensor as input and returns the size of each dimension along with a new reshaped tensor based on specified dimensions. # Function Signature ```python def get_tensor_size_and_reshape(tensor: torch.Tensor, new_shape: tuple) -> (torch.Size, torch.Tensor): Given a tensor and a new shape, return the size of the tensor and a reshaped tensor. Parameters: - tensor: torch.Tensor -- the input tensor - new_shape: tuple -- the desired shape to reshape the input tensor to Returns: - torch.Size -- the size of the original tensor - torch.Tensor -- the reshaped tensor Constraints: - The product of the dimensions of the new shape must match the product of the dimensions of the original tensor. - If the new shape is invalid, the function should raise a ValueError. ``` # Input - The function takes a PyTorch tensor `tensor`. - A tuple `new_shape` specifying the new desired shape. # Output - The function returns a `torch.Size` object representing the size of the original tensor. - A reshaped `torch.Tensor` based on the `new_shape`. # Constraints - The product of the dimensions in `new_shape` must equal the total number of elements in the original tensor. If not, raise a `ValueError`. # Example ```python import torch x = torch.ones(2, 3, 4) size, reshaped_tensor = get_tensor_size_and_reshape(x, (6, 4)) print(size) # Expected output: torch.Size([2, 3, 4]) print(reshaped_tensor.shape) # Expected output: torch.Size([6, 4]) # Example with invalid new shape try: size, reshaped_tensor = get_tensor_size_and_reshape(x, (7, 4)) except ValueError as e: print(e) # Expected output: \\"Invalid new shape\\" ``` # Additional Note - Ensure that your function checks if `new_shape` correctly corresponds to the total number of elements in the tensor. If the shape is invalid, raise a `ValueError` with a message \\"Invalid new shape\\".","solution":"import torch def get_tensor_size_and_reshape(tensor: torch.Tensor, new_shape: tuple) -> (torch.Size, torch.Tensor): Given a tensor and a new shape, return the size of the tensor and a reshaped tensor. Parameters: - tensor: torch.Tensor -- the input tensor - new_shape: tuple -- the desired shape to reshape the input tensor to Returns: - torch.Size -- the size of the original tensor - torch.Tensor -- the reshaped tensor Constraints: - The product of the dimensions of the new shape must match the product of the dimensions of the original tensor. - If the new shape is invalid, the function should raise a ValueError. original_size = tensor.size() original_elements = tensor.numel() new_elements = 1 for dim in new_shape: new_elements *= dim if original_elements != new_elements: raise ValueError(\\"Invalid new shape\\") reshaped_tensor = tensor.view(new_shape) return original_size, reshaped_tensor"},{"question":"# Question: Interactive Command-Line Todo List Manager You need to implement an interactive command-line todo list manager using the `readline` module in Python. The manager should support adding, listing, and removing todo items, with the following features: 1. **Initialization**: - Load an initialization file (if it exists) to set up keybindings and other configurations. 2. **Commands**: - Support for adding a todo item: `add <item>`. - Support for listing all todo items: `list`. - Support for removing a todo item by its index: `remove <index>`. 3. **History Management**: - Load the command history from a file (`.todo_history`) on startup. - Save the command history to the same file on exit, ensuring that concurrent sessions append to the history file. 4. **Tab Completion**: - Implement tab-completion for commands (`add`, `list`, `remove`). # Constraints: 1. You are not allowed to use any external libraries other than the standard Python library. 2. The max length for the history should be 1000 commands. # Input/Output format: - Input: The user interacts with the program using commands in the terminal. - Output: The program will display appropriate messages for add, list, and remove commands. # Example Usage: ``` > add Buy milk Todo added: Buy milk > list 1: Buy milk > add Go for a run Todo added: Go for a run > list 1: Buy milk 2: Go for a run > remove 1 Removed: Buy milk > list 1: Go for a run ``` # Implementation: ```python import atexit import os import readline class TodoManager: def __init__(self, histfile=os.path.expanduser(\\"~/.todo_history\\")): self.todos = [] self.histfile = histfile self.load_history() atexit.register(self.save_history) self.init_readline() def load_history(self): try: readline.read_history_file(self.histfile) self.h_len = readline.get_current_history_length() except FileNotFoundError: open(self.histfile, \'wb\').close() self.h_len = 0 def save_history(self): new_h_len = readline.get_current_history_length() readline.set_history_length(1000) readline.append_history_file(new_h_len - self.h_len, self.histfile) def init_readline(self): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self.completer) def completer(self, text, state): commands = [\'add\', \'list\', \'remove\'] suggestions = [cmd for cmd in commands if cmd.startswith(text)] return suggestions[state] if state < len(suggestions) else None def run(self): while True: try: command = input(\\"> \\").strip().split(maxsplit=1) if not command: continue cmd = command[0] args = command[1] if len(command) > 1 else \\"\\" if cmd == \'add\' and args: self.add(args) elif cmd == \'list\': self.list() elif cmd == \'remove\' and args.isdigit(): self.remove(int(args) - 1) else: print(\\"Invalid command\\") except (EOFError, KeyboardInterrupt): print(\\"nExiting Todo Manager...\\") break def add(self, item): self.todos.append(item) print(f\\"Todo added: {item}\\") def list(self): if not self.todos: print(\\"No todo items.\\") else: for idx, item in enumerate(self.todos, start=1): print(f\\"{idx}: {item}\\") def remove(self, index): if 0 <= index < len(self.todos): removed = self.todos.pop(index) print(f\\"Removed: {removed}\\") else: print(\\"Invalid index\\") if __name__ == \\"__main__\\": manager = TodoManager() manager.run() ``` # Notes: - Ensure to handle edge cases, like empty commands or invalid indices for the `remove` command. - This basic setup can be expanded further, but the provided structure ensures a focus on the usage of the `readline` module and its capabilities.","solution":"import atexit import os import readline class TodoManager: def __init__(self, histfile=os.path.expanduser(\\"~/.todo_history\\")): self.todos = [] self.histfile = histfile self.load_history() atexit.register(self.save_history) self.init_readline() def load_history(self): try: readline.read_history_file(self.histfile) self.h_len = readline.get_current_history_length() except FileNotFoundError: open(self.histfile, \'wb\').close() self.h_len = 0 def save_history(self): new_h_len = readline.get_current_history_length() readline.set_history_length(1000) readline.append_history_file(new_h_len - self.h_len, self.histfile) def init_readline(self): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self.completer) def completer(self, text, state): commands = [\'add\', \'list\', \'remove\'] suggestions = [cmd for cmd in commands if cmd.startswith(text)] return suggestions[state] if state < len(suggestions) else None def run(self): while True: try: command = input(\\"> \\").strip().split(maxsplit=1) if not command: continue cmd = command[0] args = command[1] if len(command) > 1 else \\"\\" if cmd == \'add\' and args: self.add(args) elif cmd == \'list\': self.list() elif cmd == \'remove\' and args.isdigit(): self.remove(int(args) - 1) else: print(\\"Invalid command\\") except (EOFError, KeyboardInterrupt): print(\\"nExiting Todo Manager...\\") break def add(self, item): self.todos.append(item) print(f\\"Todo added: {item}\\") def list(self): if not self.todos: print(\\"No todo items.\\") else: for idx, item in enumerate(self.todos, start=1): print(f\\"{idx}: {item}\\") def remove(self, index): if 0 <= index < len(self.todos): removed = self.todos.pop(index) print(f\\"Removed: {removed}\\") else: print(\\"Invalid index\\") if __name__ == \\"__main__\\": manager = TodoManager() manager.run()"},{"question":"# Question: Analyzing and Formatting Function Signatures You are given a Python script containing multiple function and class definitions. Your task is to implement a function `format_function_signatures(source_code: str) -> str` that analyzes the given source code to retrieve and format the signatures of all functions and methods defined in it. The formatted signatures should be returned as a single string where each signature is on a new line. Requirements: 1. **Input**: A single string `source_code` containing Python code with multiple function and class definitions. 2. **Output**: A single string containing the formatted signatures of all functions and methods found in the input code, with each signature on a new line. The signatures should include both the parameters and their annotations. Constraints: - You may assume no syntax errors in the given source code. - You need to handle functions, methods (including those within classes), but not lambda functions. - Annotations in function signatures should be included in the output if they exist. Example: Given the following source code as input: ```python import math def add(a: int, b: int) -> int: Add two numbers. return a + b class Calculator: def multiply(self, x: float, y: float) -> float: Multiply two numbers. return x * y def divide(self, x: float, y: float) -> float: Divide two numbers. return x / y ``` Your function should produce the following output: ``` add(a: int, b: int) -> int Calculator.multiply(self, x: float, y: float) -> float Calculator.divide(self, x: float, y: float) -> float ``` Implementation Notes: - Utilize the `inspect` module, in particular, the `signature` function to analyze and format the function signatures. - You can use the `ast` module to parse the input string and locate function and method definitions. - Pay attention to namespace and class scopes when formatting method signatures. Implementation ```python import ast import inspect def format_function_signatures(source_code: str) -> str: # Your implementation here return \'\' ``` Your task is to complete the implementation of `format_function_signatures`.","solution":"import ast def format_function_signatures(source_code: str) -> str: class FunctionSignatureVisitor(ast.NodeVisitor): def __init__(self): self.signatures = [] self.current_class = None def visit_FunctionDef(self, node): if self.current_class: func_name = f\\"{self.current_class}.{node.name}\\" else: func_name = node.name # Extract function signature args_list = [] for arg in node.args.args: if arg.annotation: args_list.append(f\\"{arg.arg}: {ast.unparse(arg.annotation)}\\") else: args_list.append(arg.arg) # Generate formatted arguments string args_str = \', \'.join(args_list) # Handle return annotation if it exists if node.returns: ret_annotation = f\\" -> {ast.unparse(node.returns)}\\" else: ret_annotation = \\"\\" signature = f\\"{func_name}({args_str}){ret_annotation}\\" self.signatures.append(signature) def visit_ClassDef(self, node): self.current_class = node.name self.generic_visit(node) self.current_class = None tree = ast.parse(source_code) visitor = FunctionSignatureVisitor() visitor.visit(tree) return \\"n\\".join(visitor.signatures)"},{"question":"# Question: Implement a custom Python class `CustomFile` that mimics some low-level file operations using Python\'s `io` module. Your class should have the following methods: 1. `__init__(self, file_path, mode, buffering=-1, encoding=None, errors=None, newline=None, closefd=True)`: - Initializes the file object with the given parameters. - Uses the `open` function from the `io` module to open the file. 2. `write_string(self, s: str) -> int`: - Writes the given string `s` to the file. - Returns `0` on success or raises an appropriate exception on failure. 3. `write_object(self, obj) -> int`: - Writes the string representation (`str()`) of the object `obj` to the file. - Returns `0` on success or raises an appropriate exception on failure. 4. `read_line(self, n: int = 0) -> str`: - Reads one line from the file. - If `n` is `0`, exactly one line is read. If `n` is greater than `0`, no more than `n` bytes will be read from the file, potentially returning a partial line. An empty string is returned if reaching the end of the file immediately. If `n` is less than `0`, one line is read but raises an `EOFError` if the end of the file is reached immediately. 5. `get_file_descriptor(self) -> int`: - Returns the file descriptor associated with the file object. 6. `close(self)`: - Closes the file object. # Constraints: 1. Ensure proper exception handling in all methods. 2. Do not use the low-level C API directly; use the `io` module for file operations. # Example Usage: ```python # Create the custom file object cf = CustomFile(\'example.txt\', \'w+\') # Write a string to the file cf.write_string(\\"Hello, world!\\") # Write an object to the file cf.write_object([1, 2, 3]) # Read the first line from the file cf.read_line() # Get the file descriptor fd = cf.get_file_descriptor() print(fd) # Close the file cf.close() ``` # Expected Output: The exact output will vary depending on the actual file operations conducted during execution, but the implementation should properly manage file access, read/write operations and handle exceptions correctly.","solution":"import io class CustomFile: def __init__(self, file_path, mode, buffering=-1, encoding=None, errors=None, newline=None, closefd=True): try: self.file = io.open(file_path, mode, buffering, encoding, errors, newline, closefd) except Exception as e: raise e def write_string(self, s: str) -> int: try: self.file.write(s) return 0 except Exception as e: raise e def write_object(self, obj) -> int: try: self.file.write(str(obj)) return 0 except Exception as e: raise e def read_line(self, n: int = 0) -> str: try: if n == 0: return self.file.readline() elif n > 0: return self.file.readline(n) else: raise ValueError(\\"n should be 0 or positive integer\\") except EOFError as e: raise e except Exception as e: raise e def get_file_descriptor(self) -> int: try: return self.file.fileno() except Exception as e: raise e def close(self): try: self.file.close() except Exception as e: raise e"},{"question":"# Question: Bytearray Manipulation You are required to implement a function that takes two inputs: 1. A list of strings, where each string represents a set of bytes. 2. An index `n`. You will perform the following operations: 1. Convert each string into a `bytearray`. 2. Concatenate all the `bytearray` objects into one large `bytearray`. 3. Return the new `bytearray` chopped off at index `n`. Additionally, if the index `n` is greater than the size of the concatenated `bytearray`, the entire `bytearray` should be returned without modifications. Function Signature ```python def manipulate_bytearrays(byte_strings: List[str], n: int) -> bytearray: pass ``` Input - `byte_strings` (List[str]): A list of strings where each string is a sequence of bytes. - `n` (int): An index specifying the length to which the concatenated `bytearray` should be chopped. Output - `bytearray`: A concatenated and possibly resized `bytearray`. Constraints - All input strings will be valid and will only contain byte values (0-255). - The value of `n` will be a non-negative integer. Example ```python # Example 1 byte_strings = [\\"abc\\", \\"def\\", \\"ghi\\"] n = 5 print(manipulate_bytearrays(byte_strings, n)) # Outputs: bytearray(b\'abcde\') # Example 2 byte_strings = [\\"hello\\", \\"world\\"] n = 15 print(manipulate_bytearrays(byte_strings, n)) # Outputs: bytearray(b\'helloworld\') ``` This problem assesses the understanding of bytearray manipulation and fundamentals of handling strings and sequences in Python.","solution":"from typing import List def manipulate_bytearrays(byte_strings: List[str], n: int) -> bytearray: Converts a list of strings into bytearrays, concatenates them, and returns the resulting bytearray truncated at index `n`. :param byte_strings: List of strings to be converted to bytearrays :param n: The index at which to truncate the final bytearray :return: The truncated concatenated bytearray # Convert all strings in the list to bytearrays and concatenate them concatenated_bytearray = bytearray(\'\'.join(byte_strings), \'utf-8\') # Return the bytearray chopped off at index `n` if n >= len(concatenated_bytearray): return concatenated_bytearray else: return concatenated_bytearray[:n]"},{"question":"**Context:** You are given multiple text files containing log entries of different events. Your task is to process these files, extracting specific information and performing some transformations. We will use the `fileinput` module to achieve this. **Objective:** Write a Python function called `process_logs` that takes the following parameters: - `files` (list of str): List of filenames to be processed. Each file contains log entries, one per line. - `keyword` (str): A keyword to search for in the log entries. - `output_file` (str): The name of a file where the results should be saved. The function should: 1. Iterate over all lines from the given files and identify lines that contain the specified `keyword`. 2. For each matching line, append the current filename and line number to the line, separated by a tab character. 3. Save the modified lines to the specified `output_file`. **Constraints:** - Each log file may have different encoding formats (UTF-8, Latin-1, etc.). Use an appropriate method to handle this. - Handle errors gracefully, skipping any files that cannot be opened or read. **Function Signature:** ```python def process_logs(files: list, keyword: str, output_file: str): pass ``` **Input Format:** - The `files` parameter is a list of strings, each representing a file path. - The `keyword` parameter is a string that specifies the keyword to search for. - The `output_file` parameter is a string that specifies the name of the output file. **Example:** Suppose you have the following log files: - `log1.txt`: ``` [INFO] Process started [ERROR] Unhandled exception [DEBUG] Connection established ``` - `log2.txt`: ``` [INFO] User login [WARNING] Low memory [ERROR] Disk full ``` Calling `process_logs([\'log1.txt\', \'log2.txt\'], \'[ERROR]\', \'output.txt\')` should create `output.txt` with the following content: ``` [ERROR] Unhandled exception log1.txt 2 [ERROR] Disk full log2.txt 3 ``` **Notes:** - You may assume that all input files are text files. - You should open files in the most appropriate mode to read lines with proper encoding. **Hints:** - Use `fileinput.input` and related functions to process multiple files. - Handle different encodings by trying a common set of encodings (e.g., \'utf-8\', \'latin-1\') and skip files with unsupported encodings.","solution":"import fileinput def process_logs(files: list, keyword: str, output_file: str): Process log files, extract lines containing the keyword, and save to the output file. Args: - files (list of str): List of filenames to be processed. - keyword (str): A keyword to search for in the log entries. - output_file (str): The name of a file where the results should be saved. try: # Attempting different common encodings encodings = [\'utf-8\', \'latin-1\'] with open(output_file, \'w\', encoding=\'utf-8\') as out_file: for filename in files: for encoding in encodings: try: with fileinput.input(files=[filename], openhook=fileinput.hook_encoded(encoding)) as f: for line in f: if keyword in line: output_line = f\\"{line.rstrip()} {filename}t{fileinput.lineno()}n\\" out_file.write(output_line) break except UnicodeDecodeError: continue # Try the next encoding for this file except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Advanced Sequence Manipulation in Python You are tasked with creating a class in Python that mimics the behavior of some of the sequence manipulation functions from the `python310` package. Your class should be called `AdvancedSequence` and should handle a list of elements internally. The `AdvancedSequence` class should include the following methods: 1. **`__init__(self, elements: list):`** - Initializes the sequence with the provided list of elements. 2. **`size(self) -> int:`** - Returns the number of elements in the sequence. - Equivalent to the `PySequence_Size` function. 3. **`concat(self, other: \'AdvancedSequence\') -> \'AdvancedSequence\':`** - Returns a new `AdvancedSequence` object that is the concatenation of the current sequence and another sequence. - Equivalent to the `PySequence_Concat` function. 4. **`repeat(self, count: int) -> \'AdvancedSequence\':`** - Returns a new `AdvancedSequence` object that repeats the current sequence elements `count` times. - Equivalent to the `PySequence_Repeat` function. 5. **`getitem(self, index: int):`** - Returns the element at the specified index. - Should raise an `IndexError` if the index is out of bounds. - Equivalent to the `PySequence_GetItem` function. 6. **`setslice(self, start: int, end: int, new_elements: list):`** - Sets the elements in the slice from `start` to `end` to `new_elements`. - Equivalent to the `PySequence_SetSlice` function. 7. **`count(self, value) -> int:`** - Returns the number of occurrences of `value` in the sequence. - Equivalent to the `PySequence_Count` function. 8. **`contains(self, value) -> bool:`** - Returns `True` if `value` is present in the sequence, `False` otherwise. - Equivalent to the `PySequence_Contains` function. Constraints: - You must handle edge cases such as empty sequences, negative indices, and invalid input types. - The methods should be implemented using Python\'s built-in list functionalities, without using any additional libraries. Example Usage: ```python seq1 = AdvancedSequence([1, 2, 3]) seq2 = AdvancedSequence([4, 5]) print(seq1.size()) # Output: 3 print(seq1.concat(seq2).elements) # Output: [1, 2, 3, 4, 5] print(seq1.repeat(2).elements) # Output: [1, 2, 3, 1, 2, 3] print(seq1.getitem(1)) # Output: 2 seq1.setslice(1, 2, [8, 9]) print(seq1.elements) # Output: [1, 8, 9, 3] print(seq1.count(1)) # Output: 1 print(seq1.contains(3)) # Output: True ``` Ensure your implementation is efficient and adheres to the constraints provided.","solution":"class AdvancedSequence: def __init__(self, elements: list): self.elements = elements def size(self) -> int: return len(self.elements) def concat(self, other: \'AdvancedSequence\') -> \'AdvancedSequence\': return AdvancedSequence(self.elements + other.elements) def repeat(self, count: int) -> \'AdvancedSequence\': return AdvancedSequence(self.elements * count) def getitem(self, index: int): return self.elements[index] def setslice(self, start: int, end: int, new_elements: list): self.elements[start:end] = new_elements def count(self, value) -> int: return self.elements.count(value) def contains(self, value) -> bool: return value in self.elements"},{"question":"You are given a set of operations that need to be applied to tensors using PyTorch. Your task is to implement a function that performs these operations step-by-step in a sequence. The operations include creating tensors, performing element-wise operations, and utilizing tensor methods. # Instructions Implement the function `perform_tensor_operations` that takes no arguments and performs the following steps: 1. Create a tensor `A` of shape (3, 3) with the following elements: ``` [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]] ``` 2. Create a tensor `B` of the same shape as `A` with all elements initialized to `1.0` and data type `torch.float64`. 3. Add tensors `A` and `B` element-wise and store the result in tensor `C`. 4. Compute the element-wise exponential of tensor `C` using the `torch.exp` method. 5. Find the maximum value in the tensor resulting from the previous step and return the value using the `item` method. **Output Format:** - Return the maximum value found in the final tensor after performing all operations. **Function Signature:** ```python def perform_tensor_operations() -> float: pass ``` # Example: ```python max_val = perform_tensor_operations() print(max_val) # Expected output: A floating-point number representing the maximum value. ``` # Constraints: - You must use PyTorch to create and manipulate the tensors. - Ensure proper usage of tensor data types and methods as specified. Hint: Refer to the PyTorch documentation provided for detailed description and examples of tensor operations and methods.","solution":"import torch def perform_tensor_operations() -> float: # 1. Create tensor A of shape (3, 3) A = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], dtype=torch.float32) # 2. Create tensor B of the same shape with all elements as 1.0 and dtype torch.float64 B = torch.ones_like(A, dtype=torch.float64) # 3. Add tensors A and B element-wise to get tensor C C = A + B # 4. Compute element-wise exponential of tensor C D = torch.exp(C) # 5. Find the maximum value in tensor D and return it max_val = D.max().item() return max_val"},{"question":"# Secure Password Management System # Objective Create a secure password management system using the `crypt` module, demonstrating your understanding of hashing, salt generation, and password verification. # Problem Statement You are required to design a `PasswordManager` class that securely handles user passwords using the `crypt` module. The class should provide the following functionalities: 1. **Hashing Passwords**: Generate a hashed password using a specified hashing method. 2. **Verifying Passwords**: Check if a provided password matches the previously hashed password. 3. **Storing User Information**: Maintain a record of users with their hashed passwords. 4. **Loading/Exporting User Database**: Save the user information to a file and load it from a file. # Class Definition Implement the `PasswordManager` class with the following methods: `__init__(self, method=crypt.METHOD_SHA512)` Initialize the `PasswordManager` with a specific hashing method. The default method should be `crypt.METHOD_SHA512`. `def hash_password(self, password: str) -> str` Hash the given password using the specified method and return the hashed password. `def verify_password(self, stored_hash: str, password: str) -> bool` Verify if the provided password matches the stored hashed password. Return `True` if they match, otherwise `False`. `def add_user(self, username: str, password: str) -> None` Add the user to the system with the provided username and password. The password should be hashed before storing. `def verify_user(self, username: str, password: str) -> bool` Verify if the provided username and password match the stored record. Return `True` if they match, otherwise `False`. `def export_users(self, file_path: str) -> None` Export the user database (username and corresponding hashed password) to a file. `def load_users(self, file_path: str) -> None` Load the user database from a file. # Constraints - Usernames are unique. - Handle potential exceptions gracefully (e.g., file operations, invalid inputs). - Ensure the salt and hashing method are properly managed to maximize security. # Input/Output Formats - **Input:** - `hash_password` method: a plain text password as a string. - `verify_password` method: a stored hashed password and a plain text password as strings. - `add_user` method: a username and a plain text password as strings. - `verify_user` method: a username and a plain text password as strings. - `export_users` method: a file path as a string. - `load_users` method: a file path as a string. - **Output:** - `hash_password`: a hashed password as a string. - `verify_password`: a boolean indicating if the password matches. - `verify_user`: a boolean indicating if the user credentials are correct. # Example ```python # Initialize PasswordManager with default method pm = PasswordManager() # Add a user pm.add_user(\'alice\', \'securepassword123\') # Verify user credentials print(pm.verify_user(\'alice\', \'securepassword123\')) # Output: True print(pm.verify_user(\'alice\', \'wrongpassword\')) # Output: False # Export users to file pm.export_users(\'user_database.txt\') # Load users from file pm.load_users(\'user_database.txt\') ``` Implement the above methods ensuring the overall system\'s security by correctly using the `crypt` module.","solution":"import crypt class PasswordManager: def __init__(self, method=crypt.METHOD_SHA512): self.method = method self.users = {} def hash_password(self, password: str) -> str: Hash the given password using the specified method. return crypt.crypt(password, crypt.mksalt(self.method)) def verify_password(self, stored_hash: str, password: str) -> bool: Verify if the provided password matches the stored hashed password. # Hash the provided password with the same salt and compare with the stored hash return stored_hash == crypt.crypt(password, stored_hash) def add_user(self, username: str, password: str) -> None: Add the user to the system with the provided username and password. The password should be hashed before storing. hashed_password = self.hash_password(password) self.users[username] = hashed_password def verify_user(self, username: str, password: str) -> bool: Verify if the provided username and password match the stored record. if username not in self.users: return False stored_hash = self.users[username] return self.verify_password(stored_hash, password) def export_users(self, file_path: str) -> None: Export the user database (username and corresponding hashed password) to a file. with open(file_path, \'w\') as file: for username, hashed_password in self.users.items(): file.write(f\\"{username},{hashed_password}n\\") def load_users(self, file_path: str) -> None: Load the user database from a file. with open(file_path, \'r\') as file: for line in file: username, hashed_password = line.strip().split(\',\') self.users[username] = hashed_password"},{"question":"Objective Assess the student\'s understanding of seaborn\'s styling functionality, as well as their ability to use context managers and plot customization features. Problem Statement Write a Python function `create_custom_barplot` that generates and saves a bar plot using seaborn with a specific styling applied. The function should fulfill the following criteria: 1. Accept three parameters: - `x_data`: A list of integers representing the x-axis values. - `y_data`: A list of integers representing the y-axis values. - `style`: A string specifying a predefined seaborn style (e.g., \\"darkgrid\\", \\"whitegrid\\", etc.). 2. The function should: - Set the seaborn style to the provided style using `sns.axes_style`. - Create a barplot of the provided x and y data. - Save the plot to a file named `custom_barplot.png`. Input Format - `x_data`: List of integers, e.g., [1, 2, 3, 4] - `y_data`: List of integers, e.g., [10, 20, 25, 30] - `style`: String representing the seaborn style, e.g., \\"darkgrid\\" Output Format - A file named `custom_barplot.png` containing the generated barplot with the specified styling. The function does not return any value. Constraints - The lengths of `x_data` and `y_data` will always be the same. - Assume valid input ranges and types. Example ```python def create_custom_barplot(x_data, y_data, style): import seaborn as sns import matplotlib.pyplot as plt # Applying the specified style with sns.axes_style(style): sns.barplot(x=x_data, y=y_data) # Save the generated barplot plt.savefig(\\"custom_barplot.png\\") plt.close() # Example usage x_data = [1, 2, 3, 4] y_data = [10, 20, 25, 30] style = \\"darkgrid\\" create_custom_barplot(x_data, y_data, style) ``` After running this function, a file named `custom_barplot.png` should be created in the working directory with the specified plot style applied.","solution":"def create_custom_barplot(x_data, y_data, style): import seaborn as sns import matplotlib.pyplot as plt # Applying the specified style with sns.axes_style(style): sns.barplot(x=x_data, y=y_data) # Save the generated barplot plt.savefig(\\"custom_barplot.png\\") plt.close()"},{"question":"# Python Coding Assessment: Shared Memory Management Objective: Create a Python application that utilizes shared memory to allow multiple processes to write and read data efficiently. Task: 1. Implement a function named `create_shared_memory_block` that: - Takes two arguments: - `size` (int): size of the shared memory block to create. - `data_to_write` (bytes): a bytes object representing the initial data to write into the shared memory block. - Creates a new shared memory block of the given size. - Writes the provided data into the shared memory block. - Returns the unique name of the shared memory block. 2. Implement a function named `read_from_shared_memory` that: - Takes one argument: - `name` (str): the unique name of the shared memory block. - Attaches to the existing shared memory block using this name. - Reads and returns the data from the shared memory block as a bytes object. 3. Implement a function named `write_to_shared_memory` that: - Takes two arguments: - `name` (str): the unique name of the shared memory block. - `data_to_write` (bytes): a bytes object representing the data to write into the shared memory block. - Attaches to the existing shared memory block using this name. - Writes the provided data into the shared memory block. 4. Use multiprocessing to create two processes: - Process A: Writes an initial set of data to the shared memory. - Process B: Reads the data from the shared memory, modifies it, and writes it back. Ensure proper cleanup by closing and unlinking the shared memory block once it is no longer needed. Input and Output Formats: - Function `create_shared_memory_block(size, data_to_write)` returns the unique name of the shared memory block. - Function `read_from_shared_memory(name)` returns the data read from the shared memory block as bytes. - Function `write_to_shared_memory(name, data_to_write)` writes the provided data to the shared memory block (does not return a value). Constraints: - You may assume the size of data to write does not exceed the allocated size of the shared memory block. - Handle any exceptions and edge cases appropriately. Example: ```python def create_shared_memory_block(size, data_to_write): # Implementation here pass def read_from_shared_memory(name): # Implementation here pass def write_to_shared_memory(name, data_to_write): # Implementation here pass if __name__ == \\"__main__\\": from multiprocessing import Process import time shm_name = create_shared_memory_block(1024, b\'Initial data\') def process_a(): write_to_shared_memory(shm_name, b\'Process A data\') def process_b(): time.sleep(1) data = read_from_shared_memory(shm_name) print(f\\"Read from shared memory: {data}\\") write_to_shared_memory(shm_name, b\'Process B data\') p1 = Process(target=process_a) p2 = Process(target=process_b) p1.start() p2.start() p1.join() p2.join() final_data = read_from_shared_memory(shm_name) print(f\\"Final data in shared memory: {final_data}\\") # Proper cleanup from multiprocessing import shared_memory existing_shm = shared_memory.SharedMemory(name=shm_name) existing_shm.close() existing_shm.unlink() ``` This assessment will test the student\'s understanding of creating, managing, and utilizing shared memory for inter-process communication using the multiprocessing.shared_memory module.","solution":"from multiprocessing import shared_memory def create_shared_memory_block(size, data_to_write): Creates a shared memory block of the given size and writes the provided data into it. Returns the unique name of the shared memory block. shm = shared_memory.SharedMemory(create=True, size=size) shm.buf[:len(data_to_write)] = data_to_write return shm.name def read_from_shared_memory(name): Attaches to the existing shared memory block with the given name and reads the data from it. Returns the data as a bytes object. shm = shared_memory.SharedMemory(name=name) data = bytes(shm.buf[:]) shm.close() return data def write_to_shared_memory(name, data_to_write): Attaches to the existing shared memory block with the given name and writes the provided data to it. shm = shared_memory.SharedMemory(name=name) shm.buf[:len(data_to_write)] = data_to_write shm.close()"},{"question":"**Objective:** This assessment will test your understanding of advanced type hints using the `typing` module and your ability to create and organize unit tests using the `unittest` and `unittest.mock` frameworks. Problem Statement You are required to implement a function `find_top_customers` which, given a list of `Transaction` objects, returns a sorted list of `Customer` objects based on their total spending in descending order. Each `Transaction` object contains the following information: `customer_id` (int), `amount` (float). Each `Customer` object contains the following information: `id` (int), `name` (str). You will then write unit tests using `unittest` to ensure the correctness of your implementation. Use `unittest.mock` to mock transactions for testing purposes. Definitions ```python from typing import List, Dict from dataclasses import dataclass @dataclass class Transaction: customer_id: int amount: float @dataclass class Customer: id: int name: str def find_top_customers(transactions: List[Transaction], customer_info: Dict[int, str]) -> List[Customer]: # Implementation should go here pass ``` Function Signature ```python def find_top_customers(transactions: List[Transaction], customer_info: Dict[int, str]) -> List[Customer]: ``` Expected Input and Output - **Input:** - `transactions`: A list of `Transaction` objects, each representing a transaction made by a customer. - `customer_info`: A dictionary mapping `customer_id` (key) to `customer_name` (value). - **Output:** - A sorted list of `Customer` objects in descending order of their total amount spent. Constraints - The `transactions` list can have up to 10,000 `Transaction` objects. - Each `customer_id` in transactions is guaranteed to exist in `customer_info`. - If two customers have the same total spending, they should appear in the order of their customer IDs (ascending). Performance Requirements - The solution should be efficient in terms of time complexity, ideally (O(n log n)) where (n) is the number of transactions. Example **Input:** ```python transactions = [Transaction(customer_id=1, amount=50), Transaction(customer_id=2, amount=20), Transaction(customer_id=1, amount=30), Transaction(customer_id=3, amount=40)] customer_info = {1: \\"Alice\\", 2: \\"Bob\\", 3: \\"Charlie\\"} ``` **Output:** ```python [Customer(id=1, name=\\"Alice\\"), Customer(id=3, name=\\"Charlie\\"), Customer(id=2, name=\\"Bob\\")] ``` **Explanation:** Alice has a total spending of 80, Charlie has 40, and Bob has 20. The list is sorted in descending order by spending. Unit Test Requirements - Create a test class `TestFindTopCustomers` using the `unittest` module. - Write tests to cover various scenarios, including: - Basic functionality with different numbers of transactions. - Edge cases such as an empty list of transactions. - Utilizing `unittest.mock` to create mock transactions and customer info for testing purposes. Implementation You need to implement the `find_top_customers` function and the accompanying unit tests. The function must use appropriate type hints as described above.","solution":"from typing import List, Dict from dataclasses import dataclass @dataclass class Transaction: customer_id: int amount: float @dataclass class Customer: id: int name: str def find_top_customers(transactions: List[Transaction], customer_info: Dict[int, str]) -> List[Customer]: # Dictionary to hold total spending by customer_id spending = {} for transaction in transactions: if transaction.customer_id in spending: spending[transaction.customer_id] += transaction.amount else: spending[transaction.customer_id] = transaction.amount # Create a list of customers with their total spending customers = [Customer(id=cid, name=customer_info[cid]) for cid in spending] # Sort the customers based on total spending and customer_id as a tiebreaker customers.sort(key=lambda c: (-spending[c.id], c.id)) return customers"},{"question":"# Custom Iterator Implementation **Objective:** Implement and demonstrate a custom iterator class in Python that behaves similarly to the `PyCallIter_New()` described in the documentation. The iterator should iterate over values generated by a callable until a specified sentinel value is returned. **Question:** 1. Implement a class `CallableIterator` in Python that takes a `callable` and a `sentinel` value as initialization parameters. 2. The `CallableIterator` class should have the following methods: - `__init__(self, callable, sentinel)`: Initializes the iterator with the given `callable` and `sentinel`. - `__iter__(self)`: Returns the iterator object itself. This is typically implemented as `return self`. - `__next__(self)`: Returns the next value from the `callable`. If the `callable` returns a value equal to the `sentinel`, the iteration should stop, and a `StopIteration` exception should be raised. **Constraints:** - The `callable` must be a function that takes no parameters and returns values on each call. - The `sentinel` can be any value that the `callable` can return to indicate the end of the iteration. **Example:** ```python def counter(): n = 0 while True: n += 1 yield n # Example usage def get_next_from_counter(): counter_gen = counter() return next(counter_gen) iterable = CallableIterator(get_next_from_counter, 10) for val in iterable: print(val) ``` **Expected Output:** ``` 1 2 3 4 5 6 7 8 9 ``` **Notes:** - Explain how the `CallableIterator` class works and show an example of its usage. - The `get_next_from_counter` function provides a clean interface to generate values from the `counter` generator. - Use the example provided to test your implementation.","solution":"class CallableIterator: def __init__(self, callable, sentinel): Initializes the iterator with given callable and sentinel. :param callable: A function that takes no parameters and returns values on each call. :param sentinel: A value which, when returned by the callable, stops the iteration. self._callable = callable self._sentinel = sentinel def __iter__(self): Returns the iterator object itself. :return: self return self def __next__(self): Returns the next value from the callable. If the callable returns a value equal to the sentinel, the iteration stops and raises a StopIteration exception. :return: The next value from the callable. :raises StopIteration: When the callable returns the sentinel value. result = self._callable() if result == self._sentinel: raise StopIteration return result # Example usage: def counter(): n = 0 while True: n += 1 yield n # Utility function to get next value from the counter generator def get_next_from_counter(): global counter_gen return next(counter_gen) # Creating counter generator counter_gen = counter()"},{"question":"**Coding Assessment Question** You are tasked with creating a comprehensive system information summary generator using Python\'s `platform` module. The generator should provide detailed information about the platform it is running on. The output should be in a structured JSON format for easy parsing and display. # Requirements 1. Implement a function `generate_system_summary()` that collects information using the following functions from the `platform` module: - `platform.architecture()` - `platform.machine()` - `platform.node()` - `platform.platform()` - `platform.processor()` - `platform.python_build()` - `platform.python_compiler()` - `platform.python_implementation()` - `platform.python_revision()` - `platform.python_version()` - `platform.python_version_tuple()` - `platform.release()` - `platform.system()` - `platform.version()` - `platform.uname()` 2. The function should return a JSON string (a string in JSON format) which includes all the collected information. Structure the JSON with clear keys corresponding to each piece of information. # Input and Output - **Input:** The function `generate_system_summary()` will not take any parameters. - **Output:** The function will return a JSON string that includes all the system information collected. # Constraints 1. The JSON output should be formatted for readability (pretty-printed with an indentation of 4 spaces). 2. Ensure that all pieces of information are included even if some of them return empty values. # Example Here is an example structure of the expected JSON output (the actual values will depend on the platform the script is executed on): ```json { \\"architecture\\": { \\"bits\\": \\"64bit\\", \\"linkage\\": \\"ELF\\" }, \\"machine\\": \\"x86_64\\", \\"node\\": \\"hostname\\", \\"platform\\": \\"Linux-5.4.0-66-generic-x86_64-with-glibc2.29\\", \\"processor\\": \\"x86_64\\", \\"python_build\\": { \\"buildno\\": \\"default\\", \\"builddate\\": \\"Jan 27 2019 15:42:09\\" }, \\"python_compiler\\": \\"GCC 7.5.0\\", \\"python_implementation\\": \\"CPython\\", \\"python_revision\\": \\"3.8.1\\", \\"python_version\\": \\"3.8.1\\", \\"python_version_tuple\\": [ \\"3\\", \\"8\\", \\"1\\" ], \\"release\\": \\"5.4.0-66-generic\\", \\"system\\": \\"Linux\\", \\"version\\": \\"#74-Ubuntu SMP Mon Jan 18 17:21:14 UTC 2021\\", \\"uname\\": { \\"system\\": \\"Linux\\", \\"node\\": \\"hostname\\", \\"release\\": \\"5.4.0-66-generic\\", \\"version\\": \\"#74-Ubuntu SMP Mon Jan 18 17:21:14 UTC 2021\\", \\"machine\\": \\"x86_64\\", \\"processor\\": \\"x86_64\\" } } ``` **Hint:** Utilize Python\'s `json` module to format the output as a JSON string.","solution":"import platform import json def generate_system_summary(): Generates a summary of the system information using platform module. Returns: str: A JSON formatted string with system information. system_summary = { \\"architecture\\": { \\"bits\\": platform.architecture()[0], \\"linkage\\": platform.architecture()[1] }, \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build\\": { \\"buildno\\": platform.python_build()[0], \\"builddate\\": platform.python_build()[1] }, \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_revision\\": platform.python_revision(), \\"python_version\\": platform.python_version(), \\"python_version_tuple\\": platform.python_version_tuple(), \\"release\\": platform.release(), \\"system\\": platform.system(), \\"version\\": platform.version(), \\"uname\\": { \\"system\\": platform.uname().system, \\"node\\": platform.uname().node, \\"release\\": platform.uname().release, \\"version\\": platform.uname().version, \\"machine\\": platform.uname().machine, \\"processor\\": platform.uname().processor } } # Pretty-print the JSON with indentation of 4 spaces return json.dumps(system_summary, indent=4)"},{"question":"**Coding Assessment Question** # Objective To assess your understanding of PyTorch\'s handling of deterministic algorithms and uninitialized memory, you will write a script that demonstrates the use of the `fill_uninitialized_memory` attribute and measures its impact on program determinism and performance. # Problem Statement Write a PyTorch script that: 1. Generates a tensor with uninitialized memory using `torch.empty()`. 2. Implements a function to fill this tensor using two different methods: - Method A: Enable `fill_uninitialized_memory`. - Method B: Disable `fill_uninitialized_memory`. 3. Measures and compares the execution time of both methods. 4. Validates and demonstrates the deterministic behavior by running a simple deterministic algorithm and showing that it produces the same output across multiple runs. # Tasks 1. **Function Implementation** - Implement a function, `generate_tensor(method: str, size: int) -> torch.Tensor`, that: - Takes a string `method` which can be either \\"A\\" or \\"B\\". - Takes an integer `size` which specifies the size of the tensor. - Generates a tensor based on the method specified. - If `method` is \\"A\\", you should enable `fill_uninitialized_memory`. - If `method` is \\"B\\", you should disable `fill_uninitialized_memory`. 2. **Performance Measurement** - Implement a function, `measure_time(method: str, size: int) -> float`, that: - Takes a string `method` which can be either \\"A\\" or \\"B\\". - Takes an integer `size` which specifies the size of the tensor. - Measures and returns the execution time of generating and filling the tensor. 3. **Determinism Demonstration** - Implement a function, `demonstrate_determinism(tensor: torch.Tensor) -> bool`, that: - Takes a tensor as input. - Runs a simple deterministic algorithm (e.g., summing all elements). - Demonstrates that it produces the same result over multiple runs. 4. **Main Script** - Implement the main script that: - Compares the execution time of both methods for generating a tensor of size `1000000`. - Validates and demonstrates the deterministic behavior for both methods. # Expected Input and Output - Input: Methods \\"A\\" and \\"B\\", tensor sizes, and deterministic algorithm functions. - Output: Execution times of both methods and verification of deterministic behavior. # Constraints - Use PyTorch version ≥ 1.8.0. - Ensure determinism by using `torch.use_deterministic_algorithms(True)`. # Performance Requirements - Measure the performance to an accuracy of at least milliseconds. ```python import torch import time def generate_tensor(method: str, size: int) -> torch.Tensor: if method == \\"A\\": torch.utils.deterministic.fill_uninitialized_memory = True elif method == \\"B\\": torch.utils.deterministic.fill_uninitialized_memory = False else: raise ValueError(\\"Invalid method. Choose \'A\' or \'B\'.\\") return torch.empty(size) def measure_time(method: str, size: int) -> float: start_time = time.time() _ = generate_tensor(method, size) end_time = time.time() return end_time - start_time def demonstrate_determinism(tensor: torch.Tensor) -> bool: torch.use_deterministic_algorithms(True) result_1 = torch.sum(tensor) result_2 = torch.sum(tensor) return result_1.item() == result_2.item() # Main script if __name__ == \\"__main__\\": tensor_size = 1000000 time_a = measure_time(\\"A\\", tensor_size) print(f\\"Execution time with fill_uninitialized_memory enabled: {time_a:.6f}s\\") time_b = measure_time(\\"B\\", tensor_size) print(f\\"Execution time with fill_uninitialized_memory disabled: {time_b:.6f}s\\") tensor_a = generate_tensor(\\"A\\", tensor_size) tensor_b = generate_tensor(\\"B\\", tensor_size) print(f\\"Deterministic behavior with method A: {demonstrate_determinism(tensor_a)}\\") print(f\\"Deterministic behavior with method B: {demonstrate_determinism(tensor_b)}\\") ```","solution":"import torch import time def generate_tensor(method: str, size: int) -> torch.Tensor: if method == \\"A\\": torch.utils.deterministic.fill_uninitialized_memory = True elif method == \\"B\\": torch.utils.deterministic.fill_uninitialized_memory = False else: raise ValueError(\\"Invalid method. Choose \'A\' or \'B\'.\\") return torch.empty(size) def measure_time(method: str, size: int) -> float: start_time = time.time() _ = generate_tensor(method, size) end_time = time.time() return end_time - start_time def demonstrate_determinism(tensor: torch.Tensor) -> bool: torch.use_deterministic_algorithms(True) result_1 = torch.sum(tensor) result_2 = torch.sum(tensor) return result_1.item() == result_2.item() # Main script if __name__ == \\"__main__\\": tensor_size = 1000000 time_a = measure_time(\\"A\\", tensor_size) print(f\\"Execution time with fill_uninitialized_memory enabled: {time_a:.6f}s\\") time_b = measure_time(\\"B\\", tensor_size) print(f\\"Execution time with fill_uninitialized_memory disabled: {time_b:.6f}s\\") tensor_a = generate_tensor(\\"A\\", tensor_size) tensor_b = generate_tensor(\\"B\\", tensor_size) print(f\\"Deterministic behavior with method A: {demonstrate_determinism(tensor_a)}\\") print(f\\"Deterministic behavior with method B: {demonstrate_determinism(tensor_b)}\\")"},{"question":"You are a developer tasked with automating the creation of built distributions for a Python project. Your goal is to write a Python script that: 1. Generates built distributions in multiple formats (e.g., gztar, zip) using the `bdist` command. 2. Creates an RPM package with specific metadata (e.g., packager name, release version) using the `bdist_rpm` command. 3. Includes a post-installation script to be run on the target system during installation and uninstallation. Requirements 1. **Function: `create_built_distributions`** - **Input:** - `dist_path` (str) - The path to the distribution directory. - `formats` (List[str]) - A list of formats for the built distributions (e.g., `[\'gztar\', \'zip\']`). - **Output:** None - **Description:** This function should change the current working directory to the provided `dist_path` and execute the appropriate `bdist` command to generate built distributions in the specified formats. 2. **Function: `create_rpm_package`** - **Input:** - `dist_path` (str) - The path to the distribution directory. - `packager` (str) - The packager name and email (e.g., `\\"John Doe <jdoe@example.org>\\"`). - `release` (str) - The release version of the RPM package (e.g., `1`). - **Output:** None - **Description:** This function should change the current working directory to the provided `dist_path` and execute the `bdist_rpm` command with the specified packager and release version. 3. **Function: `add_post_install_script`** - **Input:** - `dist_path` (str) - The path to the distribution directory. - `script_name` (str) - The name of the post-installation script file. - **Output:** None - **Description:** This function should update the `setup.py` file in the provided `dist_path` to include the specified post-installation script using the `--install-script` option. Constraints - You can assume that the `setup.py` file is correctly configured for your project. - The current working directory should be restored after the execution of each function. - Handle any necessary error catching to ensure the process doesn\'t break unexpectedly. # Example Assume the following directory structure: ``` /myproject/ setup.py dist/ post_install.py ``` A sample call to your functions might look like this: ```python dist_path = \'/path/to/myproject\' formats = [\'gztar\', \'zip\'] packager = \'John Doe <jdoe@example.org>\' release = \'1\' script_name = \'post_install.py\' create_built_distributions(dist_path, formats) create_rpm_package(dist_path, packager, release) add_post_install_script(dist_path, script_name) ``` This should: 1. Create built distributions in `gztar` and `zip` formats in the `dist` directory. 2. Create an RPM package with the specified packager and release version in the `dist` directory. 3. Update the setup script to include the `post_install.py` script for post-installation tasks. Ensure your code is well-documented and follows best practices to handle various edge cases.","solution":"import os import subprocess def create_built_distributions(dist_path, formats): Generates built distributions in the specified formats using the bdist command. Parameters: dist_path (str): The path to the distribution directory. formats (List[str]): A list of formats for the built distributions (e.g., [\'gztar\', \'zip\']). original_cwd = os.getcwd() try: os.chdir(dist_path) for fmt in formats: subprocess.run([\\"python\\", \\"setup.py\\", \\"bdist\\", \\"--formats=\\" + fmt], check=True) finally: os.chdir(original_cwd) def create_rpm_package(dist_path, packager, release): Creates an RPM package with specific metadata using the bdist_rpm command. Parameters: dist_path (str): The path to the distribution directory. packager (str): The packager name and email (e.g., \\"John Doe <jdoe@example.org>\\"). release (str): The release version of the RPM package (e.g., 1). original_cwd = os.getcwd() try: os.chdir(dist_path) subprocess.run([ \\"python\\", \\"setup.py\\", \\"bdist_rpm\\", \\"--packager\\", packager, \\"--release\\", release ], check=True) finally: os.chdir(original_cwd) def add_post_install_script(dist_path, script_name): Updates the setup.py file to include the specified post-installation script. Parameters: dist_path (str): The path to the distribution directory. script_name (str): The name of the post-installation script file. original_cwd = os.getcwd() try: os.chdir(dist_path) subprocess.run([ \\"python\\", \\"setup.py\\", \\"bdist_rpm\\", \\"--install-script\\", script_name ], check=True) finally: os.chdir(original_cwd)"},{"question":"# Assessment Question: Working with ZIP Archives in Python Objective: Implement a function to create a ZIP archive from a directory, list its contents in a specific format, and extract specific files based on a given file extension. Problem Description: Write a function `manage_zip_archive(dir_path, zip_output, file_extension, extract_to)` that performs the following tasks: 1. **Create ZIP Archive**: - Traverse the directory specified by `dir_path` and create a ZIP archive `zip_output`. - Only add files to the ZIP archive with the extension specified by `file_extension`. - Use ZIP_DEFLATED compression method for the ZIP archive. - If the directory contains subdirectories, include them in the ZIP archive structure. 2. **List Contents**: - After creating the ZIP archive, list its contents in the following format: ``` \\"filename1 - <file_size> bytes\\" \\"filename2 - <file_size> bytes\\" ``` - Return this list of strings. 3. **Extract Specific Files**: - Extract only the files with the specified `file_extension` from the ZIP archive `zip_output` to the directory `extract_to`. Input: - `dir_path` (string): Path to the directory to be zipped. - `zip_output` (string): Path where the ZIP archive will be created. - `file_extension` (string): File extension of the files to be added and extracted (e.g., \\".txt\\"). - `extract_to` (string): Path to the directory where files will be extracted. Output: - List of strings representing the contents of the ZIP archive in the specified format. Example: ```python manage_zip_archive(\'/path/to/dir\', \'output.zip\', \'.txt\', \'/path/to/extract\') # Assume \'/path/to/dir\' contains: # /path/to/dir/file1.txt # /path/to/dir/file2.txt # /path/to/dir/file3.jpg # /path/to/dir/subdir/file4.txt # /path/to/dir/subdir/file5.jpg # The function returns: [ \'file1.txt - 100 bytes\', \'file2.txt - 200 bytes\', \'subdir/file4.txt - 150 bytes\' ] # And extracts: # file1.txt, file2.txt, and subdir/file4.txt to \'/path/to/extract\' ``` Constraints: - Ensure to handle exceptions, such as when the ZIP file is invalid. - The listing of contents should be accurate and sorted alphabetically. # Function Signature: ```python import os import zipfile def manage_zip_archive(dir_path: str, zip_output: str, file_extension: str, extract_to: str) -> list: # Your implementation here ``` # Notes: - Use the `zipfile` module for creating and managing ZIP files. - Ensure proper usage of context managers (`with` statements) while handling files. - The function should handle paths and file operations carefully to avoid common pitfalls in file handling. - You may assume the `dir_path` always exists and contains at least one file with the specified extension.","solution":"import os import zipfile def manage_zip_archive(dir_path, zip_output, file_extension, extract_to): # 1. Create ZIP Archive with zipfile.ZipFile(zip_output, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, dirs, files in os.walk(dir_path): for file in files: if file.endswith(file_extension): file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, dir_path) zipf.write(file_path, arcname) # 2. List Contents zip_contents = [] with zipfile.ZipFile(zip_output, \'r\') as zipf: for zipinfo in zipf.infolist(): zip_contents.append(f\\"{zipinfo.filename} - {zipinfo.file_size} bytes\\") # Sort the list alphabetically zip_contents.sort() # 3. Extract Specific Files with zipfile.ZipFile(zip_output, \'r\') as zipf: for zipinfo in zipf.infolist(): if zipinfo.filename.endswith(file_extension): zipf.extract(zipinfo, extract_to) return zip_contents"},{"question":"**Title:** Data Cleaning and Transformation with pandas **Objective:** Create a function `clean_and_transform` that processes a given DataFrame by performing various cleaning and transformation steps. The purpose of this function is to assess your ability to manipulate and transform data using the pandas library. **Task:** Given a DataFrame `df` that contains information about employees including their names, departments, salaries, and dates of joining: 1. **Handle Missing Data:** - Fill missing salary values with the mean salary of their respective department. - Drop any rows where the department is missing. 2. **Type Conversion:** - Convert the \'Date of Joining\' column to datetime type. - Convert the \'Salary\' column to float type if it\'s not already in that type. 3. **Feature Engineering:** - Create a new column \'Tenure\' which is the number of days the employee has been with the company. - Standardize the salary data (zero mean and unit variance). 4. **String Operations:** - Convert all department names to lowercase. - Strip any leading or trailing whitespaces from the employee names. 5. **Return Value:** - The function should return the cleaned and transformed DataFrame. **Function Signature:** ```python def clean_and_transform(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** - `df`: A pandas DataFrame with columns [\'Name\', \'Department\', \'Salary\', \'Date of Joining\']. **Output:** - A pandas DataFrame with the described modifications. **Constraint:** - Assume that the DataFrame will contain at least one valid row after dropping rows with missing departments. - Do not use any external libraries other than pandas and numpy. - Ensure that all operations are efficiently performed using pandas functionalities. **Example:** Input DataFrame: ```plaintext Name Department Salary Date of Joining 0 Alice HR 55000.0 2015-03-01 1 Bob Engineering NaN 2016-07-23 2 Charlie NaN 67000.0 2017-08-15 3 Dave Engineering 77000.0 NaN 4 Eve HR NaN 2018-05-21 ``` Output DataFrame: ```plaintext Name Department Salary Date of Joining Tenure Salary_Std 0 Alice hr 55000.0 2015-03-01 ... -1.15 1 Bob engineering 72000.0 2016-07-23 ... 0.25 2 Dave engineering 77000.0 NaN ... 1.15 3 Eve hr 55000.0 2018-08-15 ... -1.50 ``` (Note: The actual values for \'Tenure\' and \'Salary_Std\' will depend on the current date and calculated statistics) By working through this problem, the students will demonstrate their understanding of handling missing data, type conversions, string operations, and basic statistical transformations using pandas.","solution":"import pandas as pd import numpy as np def clean_and_transform(df: pd.DataFrame) -> pd.DataFrame: # Fill missing salary values with mean salary of their respective department df[\'Salary\'] = df.groupby(\'Department\')[\'Salary\'].transform(lambda x: x.fillna(x.mean())) # Drop any rows where the department is missing df = df.dropna(subset=[\'Department\']) # Convert the \'Date of Joining\' column to datetime type df[\'Date of Joining\'] = pd.to_datetime(df[\'Date of Joining\'], errors=\'coerce\') # Convert the \'Salary\' column to float type if it\'s not already in that type df[\'Salary\'] = df[\'Salary\'].astype(float) # Create a new column \'Tenure\' which is the number of days the employee has been with the company df[\'Tenure\'] = (pd.Timestamp.now() - df[\'Date of Joining\']).dt.days # Standardize the salary data (zero mean and unit variance) df[\'Salary_Std\'] = (df[\'Salary\'] - df[\'Salary\'].mean()) / df[\'Salary\'].std() # Convert all department names to lowercase df[\'Department\'] = df[\'Department\'].str.lower() # Strip any leading or trailing whitespaces from the employee names df[\'Name\'] = df[\'Name\'].str.strip() return df"},{"question":"Python Input Modes Handler Background: You are required to implement a Python function that can process and handle three types of inputs: file input, interactive input, and expression input. The function will take these inputs, execute them, and return the results. Task: Implement a function `input_handler(input_type, input_value)` that processes and executes the following types of input: 1. **File Input**: Reads and executes Python code from a file. 2. **Interactive Input**: Reads and executes a single compound statement or a list of statements interactively, and returns the output. 3. **Expression Input**: Evaluates an expression using `eval()` and returns the result. Specifications: 1. `input_type` is a string that can be `\\"file\\"`, `\\"interactive\\"`, or `\\"expression\\"`. 2. `input_value` is a string representing the input code or expression to be executed. - For `\\"file\\"` input type, `input_value` is the file path. - For `\\"interactive\\"`, `input_value` is the code to execute interactively. - For `\\"expression\\"`, `input_value` is the expression to be evaluated. Constraints: - Assume the file for file input exists and is readable. - For interactive input, the code should support the execution of multiple lines where necessary. - Expression input provided will always be a valid Python expression. - The function should handle exceptions gracefully and return appropriate error messages if any operation fails. Examples: ```python def input_handler(input_type, input_value): # Your implementation here # Example usage: # Assuming `script.py` contains valid Python statements print(input_handler(\\"file\\", \\"script.py\\")) # Output will be the result of executing the script print(input_handler(\\"interactive\\", \\"print(\'Hello, World!\')n\\")) # Output: Hello, World! print(input_handler(\\"expression\\", \\"2 + 2\\")) # Output: 4 ``` Notes: - For file input, you are expected to read and execute the file content. - For interactive input, handle it as if it\'s being executed in Python\'s interactive shell. - For expression input, make sure to use `eval()` and ensure security as best as possible by restricting the built-ins available to `eval()`.","solution":"import builtins def input_handler(input_type, input_value): if input_type == \\"file\\": try: with open(input_value, \'r\') as file: exec(file.read()) return \\"File executed successfully\\" except Exception as e: return f\\"Error executing file: {e}\\" elif input_type == \\"interactive\\": try: exec(input_value) return \\"Interactive input executed successfully\\" except Exception as e: return f\\"Error executing interactive input: {e}\\" elif input_type == \\"expression\\": try: safe_globals = {\'__builtins__\': {}} result = eval(input_value, {\\"__builtins__\\": {\'print\': print}}, safe_globals) return result except Exception as e: return f\\"Error evaluating expression: {e}\\" else: return \\"Invalid input type\\""},{"question":"Question: Managing Multiple Subprocesses in Parallel # Objective You need to create a function that runs multiple shell commands asynchronously and captures their outputs concurrently using asyncio. # Task 1. Write an asynchronous function `run_commands(commands: List[str]) -> List[Dict[str, Any]]` that takes a list of shell commands as input and returns a list of dictionaries. 2. Each dictionary in the output list should correspond to a command and must contain: - `\'command\'`: the original command string. - `\'stdout\'`: the standard output of the command as a decoded string. - `\'stderr\'`: the standard error of the command as a decoded string. - `\'returncode\'`: the return code of the command. 3. Ensure all commands are run concurrently and their outputs are collected efficiently. # Input - `commands` (List[str]): A list of shell commands to be executed. # Output - A list of dictionaries, each dictionary containing: - `\'command\'`: `str`, the original command. - `\'stdout\'`: `str`, the standard output. - `\'stderr\'`: `str`, the standard error. - `\'returncode\'`: `int`, the return code of the process. # Constraints - Commands may produce large outputs; handle outputs efficiently to avoid excessive memory consumption. - Ensure the function is robust and handles exceptions appropriately. # Example ```python import asyncio from typing import List, Dict, Any async def run_commands(commands: List[str]) -> List[Dict[str, Any]]: # Implement your solution here pass # Example usage: commands = [ \'echo \\"Hello, World!\\"\', \'ls /nonexistent_directory\', \'sleep 1; echo \\"Done\\"\' ] result = asyncio.run(run_commands(commands)) for command_result in result: print(command_result) ``` # Expected Output ```json [ { \\"command\\": \\"echo \\"Hello, World!\\"\\", \\"stdout\\": \\"Hello, World!n\\", \\"stderr\\": \\"\\", \\"returncode\\": 0 }, { \\"command\\": \\"ls /nonexistent_directory\\", \\"stdout\\": \\"\\", \\"stderr\\": \\"ls: /nonexistent_directory: No such file or directoryn\\", \\"returncode\\": 1 }, { \\"command\\": \\"sleep 1; echo \\"Done\\"\\", \\"stdout\\": \\"Donen\\", \\"stderr\\": \\"\\", \\"returncode\\": 0 } ] ``` # Notes - Use `asyncio.create_subprocess_shell()` to create subprocesses for executing the shell commands. - Use `await gather(*tasks)` to run all commands concurrently. - The solution should be asynchronous and make use of asyncio capabilities effectively.","solution":"import asyncio from typing import List, Dict, Any async def run_command(command: str) -> Dict[str, Any]: Runs a single command and captures its output. process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return { \'command\': command, \'stdout\': stdout.decode(\'utf-8\'), \'stderr\': stderr.decode(\'utf-8\'), \'returncode\': process.returncode } async def run_commands(commands: List[str]) -> List[Dict[str, Any]]: Runs multiple commands concurrently and captures their outputs. tasks = [run_command(command) for command in commands] return await asyncio.gather(*tasks)"},{"question":"**Problem Statement:** You are tasked with developing a Python function that connects to an IMAP4 server, logs in, fetches all emails from the \'INBOX\', and prints the subject and sender of each email. After fetching the emails, the function should log out gracefully. To assist you in implementing this function, the `imaplib` module documentation has been provided. You are required to use the methods from the `imaplib` module for connecting to the IMAP server, logging in, fetching emails, and logging out. # Requirements: 1. The function `fetch_and_print_emails` should accept the following parameters: - `host` (str): The IMAP server\'s hostname. - `username` (str): The user\'s email username. - `password` (str): The user\'s email password. 2. The function should connect to the IMAP server using the provided host, username, and password. 3. The function should select the \'INBOX\' mailbox. 4. The function should search for all emails in the \'INBOX\'. 5. For each email found, the function should fetch its subject and sender information and print them in the format: - `Subject: <subject>` - `From: <sender>` 6. The function should handle exceptions gracefully and print appropriate error messages if operations fail, especially when connecting to the server or fetching emails. 7. Finally, the function should log out from the IMAP server. # Constraints: - Username and password will always be valid email credentials. - The server is guaranteed to be available and reachable. - Network issues, if any, should only be simulated via exception handling. **Example Function Call:** ```python fetch_and_print_emails(\\"imap.example.com\\", \\"user@example.com\\", \\"password\\") ``` **Expected Output:** ``` Subject: Welcome to Our Service! From: newsletter@example.com Subject: Your Invoice From: billing@example.com ... ``` **Hints:** - Use `IMAP4.login` to authenticate to the server. - Use `IMAP4.select` to select the \'INBOX\' mailbox. - Use `IMAP4.search` to find all messages. - Use `IMAP4.fetch` to retrieve message data. - To extract the \'Subject\' and \'From\' headers, look into parsing the email content using the `email` module.","solution":"import imaplib import email from email.header import decode_header def fetch_and_print_emails(host, username, password): try: # Connect to the IMAP server and log in. mail = imaplib.IMAP4_SSL(host) mail.login(username, password) mail.select(\\"inbox\\") # Search for all emails in the \'INBOX\'. result, data = mail.search(None, \\"ALL\\") if result != \\"OK\\": print(\\"Failed to search emails.\\") return # Fetch the IDs of all emails. email_ids = data[0].split() for eid in email_ids: result, msg_data = mail.fetch(eid, \\"(RFC822)\\") if result != \\"OK\\": print(f\\"Failed to fetch email with ID {eid}.\\") continue # Parse the email content. msg = email.message_from_bytes(msg_data[0][1]) # Decode the email subject. subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \\"utf-8\\") # Decode the email sender. from_, encoding = decode_header(msg.get(\\"From\\"))[0] if isinstance(from_, bytes): from_ = from_.decode(encoding if encoding else \\"utf-8\\") # Print the subject and sender. print(f\\"Subject: {subject}\\") print(f\\"From: {from_}\\") print() # Logout from the server. mail.logout() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Question:** You are given the task to generate and analyze various synthetic datasets using scikit-learn\'s random sample generators. Specifically, you need to implement a function `generate_and_analyze_datasets` that will: 1. Generate three different datasets using the specified sample generators: - A dataset with normally-distributed clusters using `make_blobs`. - A binary classification dataset with interleaving half-circles using `make_moons`. - A regression dataset with random features using `make_regression`. 2. Visualize each dataset: - For clustering and classification datasets, create scatter plots of the data points, color-coded by their labels. - For the regression dataset, create a scatter plot of the first feature against the target values. 3. Return the datasets and the corresponding plots. **Function Signature:** ```python def generate_and_analyze_datasets(): # Your implementation here ``` **Output:** - A dictionary containing three entries, one for each generated dataset. Each entry should have: - `data`: The dataset generated. - `plot`: The plot object (matplotlib figure). **Constraints:** - Use `random_state=42` for reproducibility. - Ensure the plots are properly labeled and titled for clarity. # Example ```python result = generate_and_analyze_datasets() # Example output structure (plots rendered appropriately in the context) # result = { # \\"make_blobs\\": {\\"data\\": (X_blobs, y_blobs), \\"plot\\": <matplotlib.figure.Figure>}, # \\"make_moons\\": {\\"data\\": (X_moons, y_moons), \\"plot\\": <matplotlib.figure.Figure>}, # \\"make_regression\\": {\\"data\\": (X_regression, y_regression), \\"plot\\": <matplotlib.figure.Figure>} # } # To visualize a plot # plt.figure(result[\\"make_blobs\\"][\\"plot\\"].number) # plt.show() ``` Your task is to implement the `generate_and_analyze_datasets` function, ensuring the datasets are generated and visualized as described.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_moons, make_regression def generate_and_analyze_datasets(): # Initialize dictionary to store datasets and plots results = {} # Generate normally-distributed clusters using make_blobs X_blobs, y_blobs = make_blobs(n_samples=300, centers=3, random_state=42) fig_blobs, ax_blobs = plt.subplots() ax_blobs.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap=\'viridis\') ax_blobs.set_title(\\"Blobs (Clustering)\\") ax_blobs.set_xlabel(\\"Feature 1\\") ax_blobs.set_ylabel(\\"Feature 2\\") results[\\"make_blobs\\"] = {\\"data\\": (X_blobs, y_blobs), \\"plot\\": fig_blobs} # Generate binary classification dataset with interleaving half-circles using make_moons X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42) fig_moons, ax_moons = plt.subplots() ax_moons.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\'viridis\') ax_moons.set_title(\\"Moons (Classification)\\") ax_moons.set_xlabel(\\"Feature 1\\") ax_moons.set_ylabel(\\"Feature 2\\") results[\\"make_moons\\"] = {\\"data\\": (X_moons, y_moons), \\"plot\\": fig_moons} # Generate regression dataset with random features using make_regression X_regression, y_regression = make_regression(n_samples=300, n_features=1, noise=10, random_state=42) fig_regression, ax_regression = plt.subplots() ax_regression.scatter(X_regression[:, 0], y_regression) ax_regression.set_title(\\"Regression\\") ax_regression.set_xlabel(\\"Feature\\") ax_regression.set_ylabel(\\"Target\\") results[\\"make_regression\\"] = {\\"data\\": (X_regression, y_regression), \\"plot\\": fig_regression} return results"},{"question":"# Problem: Task Scheduler with Priority You are tasked with designing a simple task scheduler that prioritizes tasks based on their urgency. The scheduler should support the following operations: 1. **Add Task**: Add a task to the scheduler with a given priority. 2. **Remove Task**: Remove a specific task from the scheduler. 3. **Pop Task**: Retrieve and remove the task with the highest priority. 4. **Reschedule Task**: Change the priority of an existing task in the scheduler. 5. **Top Tasks**: Retrieve a list of the top `n` most urgent tasks without removing them. Implement the `TaskScheduler` class with the following methods: - `add_task(task: str, priority: int) -> None`: Adds a task with the given priority to the scheduler. - `remove_task(task: str) -> None`: Removes the specified task from the scheduler. Raises a `KeyError` if the task is not found. - `pop_task() -> str`: Retrieves and removes the most urgent task from the scheduler. Raises a `KeyError` if the scheduler is empty. - `reschedule_task(task: str, new_priority: int) -> None`: Changes the priority of the specified task. Raises a `KeyError` if the task is not found. - `top_tasks(n: int) -> List[str]`: Returns a list of the top `n` most urgent tasks without removing them. # Constraints: - A task is represented as a string and must be unique. - Task priorities are integers, with smaller values representing higher urgency. - The scheduler must handle upto 10^5 tasks efficiently. # Example: ```python # Example usage of TaskScheduler scheduler = TaskScheduler() scheduler.add_task(\'Task1\', 1) scheduler.add_task(\'Task2\', 2) scheduler.add_task(\'Task3\', 3) print(scheduler.pop_task()) # Output: \'Task1\' scheduler.remove_task(\'Task2\') scheduler.add_task(\'Task4\', 0) scheduler.reschedule_task(\'Task3\', 0) print(scheduler.top_tasks(2)) # Output: [\'Task3\', \'Task4\'] ``` # Performance Requirements: Make sure that all operations perform efficiently under the given constraints, making good use of the `heapq` module to maintain the heap property. # Notes: - Your implementation should ensure that adding, removing, and popping tasks maintain the heap structure. - Use the priority queue properties to guarantee that the highest priority tasks are efficiently accessed.","solution":"import heapq class TaskScheduler: def __init__(self): self.priority_queue = [] self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = 0 # unique sequence count def add_task(self, task, priority): if task in self.entry_finder: self.remove_task(task) count = self.counter entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.priority_queue, entry) self.counter += 1 def remove_task(self, task): entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self): while self.priority_queue: priority, count, task = heapq.heappop(self.priority_queue) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def reschedule_task(self, task, new_priority): self.remove_task(task) self.add_task(task, new_priority) def top_tasks(self, n): temp_priority_queue = [] temp_heap = list(self.priority_queue) heapq.heapify(temp_heap) top_tasks = [] while temp_heap and len(top_tasks) < n: priority, count, task = heapq.heappop(temp_heap) if task is not self.REMOVED: top_tasks.append(task) temp_priority_queue.append([priority, count, task]) for entry in temp_priority_queue: heapq.heappush(temp_heap, entry) return top_tasks"},{"question":"Question # Objective: Your task is to demonstrate proficiency in using seaborn\'s `plotting_context` function to manage and apply different plotting styles. # Background: Seaborn offers functionalities to manage and apply different plotting contexts using the `sns.plotting_context` function. This function can be used to retrieve default plotting parameters, apply predefined contexts, and temporarily change settings using context managers. # Instructions: 1. **Retrieve Default Plotting Parameters:** - Write a function `get_default_plotting_params` that uses `sns.plotting_context()` with no arguments to retrieve default plotting parameters. - **Input:** None - **Output:** Dictionary containing the default plotting parameters. 2. **Apply a Predefined Context and Plot:** - Write a function `plot_with_context` that accepts a context name, creates a line plot with sample data (X=[\\"A\\", \\"B\\", \\"C\\"], Y=[1, 3, 2]), and returns the plotting parameters used. - **Input:** A string representing the context name from the predefined contexts (e.g., \\"talk\\"). - **Output:** Dictionary containing the plotting parameters used for the provided context. 3. **Temporary Context Change:** - Write a function `plot_temp_context` that temporarily changes the plotting context within a context manager and creates a line plot. - **Input:** A string representing the context name from the predefined contexts (e.g., \\"talk\\"). - **Output:** Dictionary containing the default plotting parameters before the context manager and the plotting parameters within the context manager. # Example: ```python # Example usage of function definitions: default_params = get_default_plotting_params() # Expected output: returns a dictionary with the default plotting parameters talk_params = plot_with_context(\\"talk\\") # Expected output: creates a plot with \'talk\' context and returns the \'talk\' context parameters temp_context_change = plot_temp_context(\\"talk\\") # Expected output: creates a plot within a temporary \'talk\' context and returns: # { # \\"default_params\\": {...}, # default plotting parameters before the context manager # \\"temp_params\\": {...} # plotting parameters within the \'talk\' context # } ``` Notes: - Ensure you have `matplotlib.pyplot` imported and configured to display plots if running the functions in an environment where plots are visualized. - Handle any edge cases, such as non-existing context names, with appropriate error handling or default to sensible values. # Constraints: - Use seaborn 0.11.2 or higher. # What to Submit: - Python file containing the implementation of the three functions: `get_default_plotting_params`, `plot_with_context`, and `plot_temp_context`. - Any additional helper functions used in your implementation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def get_default_plotting_params(): Retrieve default plotting parameters using sns.plotting_context(). Returns a dictionary of default plotting parameters. return sns.plotting_context() def plot_with_context(context): Apply a predefined context and create a sample line plot. Returns the plotting parameters used for the provided context. with sns.plotting_context(context): X = [\\"A\\", \\"B\\", \\"C\\"] Y = [1, 3, 2] plt.figure() sns.lineplot(x=X, y=Y) plt.title(f\\"Line plot with \'{context}\' context\\") plt.show() return sns.plotting_context() def plot_temp_context(context): Temporarily change the plotting context within a context manager and create a sample line plot. Returns a dictionary with the default plotting parameters before and within the context manager. default_params = sns.plotting_context() with sns.plotting_context(context): temp_params = sns.plotting_context() X = [\\"A\\", \\"B\\", \\"C\\"] Y = [1, 3, 2] plt.figure() sns.lineplot(x=X, y=Y) plt.title(f\\"Line plot within temporary \'{context}\' context\\") plt.show() return { \\"default_params\\": default_params, \\"temp_params\\": temp_params }"},{"question":"# Problem Description You are given a batch of images represented as a PyTorch tensor. Each image has multiple channels, height, and width. Your task is to write a function `process_images` that: 1. Takes an input tensor with named dimensions representing a batch of images. 2. Renames dimensions to meaningful names (e.g., from \'H\' to \'height\' and \'W\' to \'width\'). 3. Flattens the image dimensions (height and width) into a single \\"features\\" dimension. 4. Aligns the tensor such that the \\"features\\" dimension is moved to the second position, keeping the batch and channel dimensions intact. # Function Signature ```python import torch from typing import List, Tuple def process_images(images: torch.Tensor) -> torch.Tensor: pass ``` # Input - `images`: A PyTorch tensor with named dimensions. The input tensor has dimensions (\'batch\', \'channels\', \'H\', \'W\') where: - \'batch\': The batch dimension. - \'channels\': The number of channels in each image. - \'H\': The height of the images. - \'W\': The width of the images. # Output - A PyTorch tensor with named dimensions where: - The \'H\' and \'W\' dimensions have been flattened into a single dimension named \'features\'. - The \'features\' dimension is moved to the second position, with dimensions named (\'batch\', \'features\', \'channels\'). # Example ```python images = torch.randn(4, 3, 128, 128, names=(\'batch\', \'channels\', \'H\', \'W\')) processed_images = process_images(images) assert processed_images.names == (\'batch\', \'features\', \'channels\') assert processed_images.size(\'features\') == 128 * 128 ``` # Constraints - The input tensor will always have four dimensions. - Only the height and width dimensions will be renamed and flattened. # Additional Information For this task, you may find the following PyTorch methods useful: - `Tensor.rename` - `Tensor.flatten` - `Tensor.align_to` Ensure that your function is efficient and follows best practices for manipulating PyTorch tensors.","solution":"import torch def process_images(images: torch.Tensor) -> torch.Tensor: Renames and processes a batch of images by flattening the height and width dimensions into a single features dimension and aligning the tensor. Args: images (torch.Tensor): A batch of images with dimensions (\'batch\', \'channels\', \'H\', \'W\'). Returns: torch.Tensor: Processed tensor with dimensions (\'batch\', \'features\', \'channels\'). # Rename the dimensions to meaningful names images = images.rename(batch=\'batch\', channels=\'channels\', H=\'height\', W=\'width\') # Flatten the height and width dimensions into a single features dimension images = images.flatten((\'height\', \'width\'), \'features\') # Align the tensor such that the \'features\' dimension is second processed_images = images.align_to(\'batch\', \'features\', \'channels\') return processed_images"},{"question":"Objective Demonstrate understanding of pandas options and settings by configuring and manipulating pandas behavior using various option-related functions. Question You are given a pandas DataFrame that contains sales information for a company. Your task is to implement a function that performs the following steps: 1. Temporarily set the pandas option to display float numbers in Engineering format. 2. Retrieve and print the current value of this setting. 3. In the same context, change the display precision to show 3 decimal places. 4. After changing the precision, print a description of the setting to verify the update. 5. Finally, reset all options back to their default values and confirm this by printing the default precision setting. **Function Signature:** ```python import pandas as pd def manipulate_pandas_options(df: pd.DataFrame) -> None: Function to manipulate pandas options and demonstrate their effects. Parameters: df (pd.DataFrame): Input DataFrame containing sales data. Returns: None: The function should print the required output at various steps. # Your code here ``` **Input:** - `df` (pandas.DataFrame): A DataFrame containing sales data with at least one float column. **Output:** - The function should print the following at different steps: 1. The current value of the float format setting. 2. A description of the precision setting after modification. 3. The default precision setting after resetting all options. **Constraints:** - You should use the `option_context` to temporarily set options. - Use appropriate functions to get, describe, and reset options. **Example Execution:** ```python import pandas as pd # Sample DataFrame data = { \'sales\': [1500.456, 1002.123, 675.123, 2345.567], \'profit\': [150.12, 100.67, 62.78, 234.45] } df = pd.DataFrame(data) manipulate_pandas_options(df) ``` **Expected Output:** ``` Current float format: Engineering Formatter Describe precision setting after update: display.precision : int Precision to display in engineering format, if set. [\'10\'] (currently: 3) Default precision setting: 6 ``` **Note:** The exact format of the output may vary slightly but should include the key information stated above.","solution":"import pandas as pd def manipulate_pandas_options(df: pd.DataFrame) -> None: Function to manipulate pandas options and demonstrate their effects. Parameters: df (pd.DataFrame): Input DataFrame containing sales data. Returns: None: The function should print the required output at various steps. with pd.option_context(\'display.float_format\', \'{:e}\'.format): # Step 1: Temporarily set the pandas option to display float numbers in Engineering format. # Step 2: Retrieve and print the current value of this setting. current_float_format = pd.get_option(\'display.float_format\') print(f\\"Current float format: {current_float_format.__name__} Formatter\\") # Step 3: Change the display precision to show 3 decimal places. with pd.option_context(\'display.precision\', 3): # Step 4: Print a description of the setting to verify the update. precision_description = pd.describe_option(\'display.precision\') print(\\"Describe precision setting after update:\\") print(precision_description) # Step 5: Reset all options back to their default values and confirm this by printing the default precision setting. pd.reset_option(\'display.precision\') default_precision = pd.get_option(\'display.precision\') print(f\\"Default precision setting: {default_precision}\\")"},{"question":"**Objective:** To assess the student\'s understanding of kernel density estimation (KDE) using the `scikit-learn` library, and their ability to implement and analyze the effects of different kernels and bandwidth parameters. **Problem Statement:** You are given a 1D dataset representing the ages of a group of individuals. Your task is to: 1. Implement KDE using the `KernelDensity` class from the `scikit-learn` library. 2. Experiment with at least three different kernel functions (`gaussian`, `tophat`, `epanechnikov`). 3. Experiment with at least three different bandwidth values (0.1, 0.5, 1.0). 4. Visualize the results of the KDE for each combination of kernel and bandwidth. 5. Analyze and compare the results, discussing how the choice of kernel and bandwidth affects the smoothness and accuracy of the density estimates. **Input Format:** - A 1D list or array, `ages`, containing the ages of a group of individuals. **Output Format:** 1. A set of visualizations (plot) showing the KDE for each combination of kernel and bandwidth. 2. A detailed explanation of your observations and conclusions based on the visualizations. **Constraints:** - The dataset will have at least 30 data points and no more than 1000 data points. - Ages will be integers in the range [1, 100]. **Performance Requirements:** - Ensure that your implementation is efficient and that the visualizations are clear and informative. **Example:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Example dataset ages = [23, 45, 56, 22, 34, 36, 78, 44, 12, 45, 56, 29, 55, 63, 77, 33, 34, 22, 31, 22, 44, 56, 48, 49, 30, 33, 40, 42] # Define the kernels and bandwidths to be used kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1.0] # Reshape data for KDE ages = np.array(ages).reshape(-1, 1) # Visualize KDE for each combination of kernel and bandwidth fig, axes = plt.subplots(len(kernels), len(bandwidths), figsize=(15, 10)) for i, kernel in enumerate(kernels): for j, bandwidth in enumerate(bandwidths): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(ages) x_d = np.linspace(0, 100, 1000).reshape(-1, 1) log_dens = kde.score_samples(x_d) axes[i, j].plot(x_d[:, 0], np.exp(log_dens), \'-\') axes[i, j].set_title(f\'Kernel: {kernel}, Bandwidth: {bandwidth}\') plt.tight_layout() plt.show() # Provide a detailed explanation and comparison of the results # [Your detailed analysis and comparison of the results here] ``` **Note:** - Be sure to provide detailed explanations for your visualizations. - Justify your choice of any additional parameters used in the KDE implementation. - Discuss how different kernels and bandwidths impact the KDE results based on your visualizations.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kde_analysis(ages): This function performs kernel density estimation on the ages dataset using different kernels and bandwidths and visualizes the results. Parameters: ages (list or numpy array): A 1D array or list containing ages of a group of individuals. Returns: None: Displays the plots. # Define the kernels and bandwidths to be used kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1.0] # Reshape the data to be used with KernelDensity ages = np.array(ages).reshape(-1, 1) # Visualize KDE for each combination of kernel and bandwidth fig, axes = plt.subplots(len(kernels), len(bandwidths), figsize=(15, 10)) for i, kernel in enumerate(kernels): for j, bandwidth in enumerate(bandwidths): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(ages) x_d = np.linspace(0, 100, 1000).reshape(-1, 1) log_dens = kde.score_samples(x_d) axes[i, j].plot(x_d[:, 0], np.exp(log_dens), \'-\') axes[i, j].set_title(f\'Kernel: {kernel}, Bandwidth: {bandwidth}\') axes[i, j].set_xlim(0, 100) axes[i, j].set_ylim(0, max(np.exp(log_dens)) + 0.01) axes[i, j].set_xlabel(\'Age\') axes[i, j].set_ylabel(\'Density\') plt.tight_layout() plt.show()"},{"question":"Evaluate Model Performance with Scikit-Learn Metrics Objective Your task is to implement a function that trains a supervised learning model, makes predictions, and evaluates the model\'s performance using various scikit-learn metrics. You will use a provided dataset for this task. Dataset You will be using the Iris dataset, a classic dataset in machine learning. It consists of 150 samples of iris flowers classified into three species (Setosa, Versicolour, and Virginica) with four features: sepal length, sepal width, petal length, and petal width. Task 1. **Load the dataset**: Use `sklearn.datasets.load_iris` to load the Iris dataset. 2. **Split the dataset** into training and testing sets using `train_test_split`. 3. **Train a classifier**: Use `RandomForestClassifier` from `sklearn.ensemble` to train a model on the training set. 4. **Make predictions** on the test set. 5. **Evaluate the model** using: - Accuracy (`accuracy_score`) - Precision, Recall, and F1-score (`precision_score`, `recall_score`, `f1_score`) - Confusion matrix (`confusion_matrix`) Function Signature ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix def evaluate_iris_model(random_state: int) -> dict: Trains a RandomForestClassifier on the Iris dataset and evaluates its performance. Parameters: random_state (int): Random state for reproducibility Returns: dict: A dictionary with accuracy, precision, recall, F1 score, and confusion matrix. pass # Example usage: # metrics = evaluate_iris_model(random_state=42) # print(metrics) ``` Requirements 1. **Input**: - `random_state` (int): Random state for reproducibility. 2. **Output**: - A dictionary with the following keys: - `accuracy`: The accuracy of the model on the test set. - `precision`: The precision of the model on the test set (macro-averaged). - `recall`: The recall of the model on the test set (macro-averaged). - `f1_score`: The F1-score of the model on the test set (macro-averaged). - `confusion_matrix`: The confusion matrix of the model on the test set. 3. **Constraints**: - Use `RandomForestClassifier` with default parameters for the model. - Use `train_test_split` with a test size of 0.3. - Use `macro` averaging for precision, recall, and F1-score. Example ```python metrics = evaluate_iris_model(random_state=42) print(metrics) # Output: # { # \'accuracy\': 0.9777777777777777, # \'precision\': 0.9777777777777777, # \'recall\': 0.9777777777777777, # \'f1_score\': 0.9777777777777777, # \'confusion_matrix\': array([[16, 0, 0], # [ 0, 13, 1], # [ 0, 0, 15]]) # } ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix def evaluate_iris_model(random_state: int) -> dict: Trains a RandomForestClassifier on the Iris dataset and evaluates its performance. Parameters: random_state (int): Random state for reproducibility Returns: dict: A dictionary with accuracy, precision, recall, F1 score, and confusion matrix. # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state) # Train the model clf = RandomForestClassifier(random_state=random_state) clf.fit(X_train, y_train) # Make predictions y_pred = clf.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') conf_matrix = confusion_matrix(y_test, y_pred) # Return the results in a dictionary return { \'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1_score\': f1, \'confusion_matrix\': conf_matrix }"},{"question":"# Floating-Point Arithmetic and Precision Handling in Python Objective: Write a Python function to sum a list of floating-point numbers accurately. This exercise will demonstrate your understanding of floating-point arithmetic issues and Python techniques to manage these issues. Function Signature: ```python def accurate_sum(numbers: List[float]) -> float: Sums a list of floating-point numbers accurately. Parameters: numbers (List[float]): A list of floating-point numbers to be summed. Returns: float: The sum of the list of floating-point numbers. ``` Input: - `numbers`: A list of floating-point numbers (0 ≤ len(numbers) ≤ 10^6). Output: - A single floating-point number representing the sum of the input list. Constraints: - You should avoid simply using the built-in `sum` function as it may not handle floating-point precision issues effectively for large lists. Performance Requirements: - The function should be efficient enough to handle large lists up to 1,000,000 elements. Example: ```python numbers = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] result = accurate_sum(numbers) print(result) # Expected Output: 1.0 ``` Explanation: In the example, summing 0.1 ten times should yield a result of 1.0. However, due to floating-point representation issues, using the built-in `sum` may not produce this exact result. Your task is to implement the `accurate_sum` function to handle such cases accurately. Hints: - You may find the `math.fsum` function useful for this task, as it is designed to provide precise floating-point summation. - Think about how you can minimize the representation error discussed in the documentation while performing the summation.","solution":"from typing import List import math def accurate_sum(numbers: List[float]) -> float: Sums a list of floating-point numbers accurately. Parameters: numbers (List[float]): A list of floating-point numbers to be summed. Returns: float: The sum of the list of floating-point numbers. return math.fsum(numbers)"},{"question":"# Advanced Python Coding Assessment You are tasked with creating a small utility to manage directories within a filesystem. Specifically, your utility should be able to: 1. List all immediate files and subdirectories within a given directory. 2. Create a new directory. 3. Copy a directory (and its contents) to another location. 4. Delete a directory (and its contents). Implement the following functions using the appropriate modules from the documentation provided: Function 1: `list_dir_contents(dir_path: str) -> Dict[str, List[str]]` - **Input**: A string, `dir_path` representing the path to the directory. - **Output**: A dictionary with two keys, `\\"files\\"` and `\\"dirs\\"`. The value for `\\"files\\"` is a list of filenames, and the value for `\\"dirs\\"` is a list of subdirectory names. Function 2: `create_directory(dir_path: str) -> None` - **Input**: A string, `dir_path` representing the path to the directory to be created. - **Output**: None. This function creates the specified directory. Function 3: `copy_directory(src_path: str, dest_path: str) -> None` - **Input**: Two strings, `src_path` indicating the source directory and `dest_path` indicating the destination directory. - **Output**: None. This function copies the contents of `src_path` to `dest_path`. Function 4: `delete_directory(dir_path: str) -> None` - **Input**: A string, `dir_path` representing the path to the directory to be deleted. - **Output**: None. This function deletes the specified directory and all its contents. # Constraints - You must use appropriate modules from the Python standard library. - Handle exceptions that may arise from filesystem operations (such as directories or files not existing). - Ensure the functions perform operations in a cross-platform manner. # Example Usage ```python # List contents of a directory contents = list_dir_contents(\'/path/to/directory\') print(contents) # Output: {\'files\': [\'file1.txt\', \'file2.jpg\'], \'dirs\': [\'subdir1\', \'subdir2\']} # Create a directory create_directory(\'/path/to/new_directory\') # Copy a directory copy_directory(\'/path/to/source_directory\', \'/path/to/destination_directory\') # Delete a directory delete_directory(\'/path/to/delete_directory\') ``` # Performance Requirements - Your functions should handle large directories efficiently. Use appropriate mechanisms to ensure performance is consistent even with a significant number of files and subdirectories.","solution":"import os import shutil from typing import Dict, List def list_dir_contents(dir_path: str) -> Dict[str, List[str]]: Returns a dictionary with lists of \'files\' and \'dirs\' in the provided directory. if not os.path.isdir(dir_path): raise FileNotFoundError(f\\"The directory {dir_path} does not exist\\") files = [] dirs = [] with os.scandir(dir_path) as it: for entry in it: if entry.is_file(): files.append(entry.name) elif entry.is_dir(): dirs.append(entry.name) return {\\"files\\": files, \\"dirs\\": dirs} def create_directory(dir_path: str) -> None: Creates a new directory. os.makedirs(dir_path, exist_ok=True) def copy_directory(src_path: str, dest_path: str) -> None: Copies the contents of src_path to dest_path. if not os.path.isdir(src_path): raise FileNotFoundError(f\\"The source directory {src_path} does not exist\\") shutil.copytree(src_path, dest_path, dirs_exist_ok=True) def delete_directory(dir_path: str) -> None: Deletes the specified directory and all its contents. if not os.path.isdir(dir_path): raise FileNotFoundError(f\\"The directory {dir_path} does not exist\\") shutil.rmtree(dir_path)"},{"question":"**Objective:** You are to write a Python code that demonstrates your understanding of creating and manipulating a WSGI environment and headers using the `wsgiref` module. **Task:** Implement a WSGI application that: 1. Responds with a \\"Hello, [NAME]!\\" message, where `[NAME]` is extracted from the URL path. 2. Sets an appropriate Content-Type header. 3. Validates the WSGI application using `wsgiref.validate.validator`. 4. Uses `wsgiref.simple_server.make_server` to serve the application on port 8080. **Specific Requirements:** 1. Extract the NAME by shifting the `PATH_INFO`: - Use `wsgiref.util.shift_path_info` to modify `PATH_INFO` and append the extracted segment to `SCRIPT_NAME`. 2. Set the Content-Type header to `\'text/plain; charset=utf-8\'`. 3. Use `wsgiref.headers.Headers` to manage your response headers. 4. Validate the WSGI application before serving using `wsgiref.validate.validator`. 5. The application must handle cases where NAME is not provided in the path, responding with a `404 Not Found` status. **Constraints:** - You should use only standard library modules that are part of Python 3.10. - Your implementation should handle edge cases such as empty paths and missing segments gracefully. **Example:** For a request to `http://localhost:8080/John`, the response should be: ``` HTTP/1.1 200 OK Content-Type: text/plain; charset=utf-8 Hello, John! ``` For a request to `http://localhost:8080/`, the response should be: ``` HTTP/1.1 404 Not Found Content-Type: text/plain; charset=utf-8 Name not provided ``` **Solution Template:** ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.util import shift_path_info, setup_testing_defaults from wsgiref.headers import Headers def hello_app(environ, start_response): setup_testing_defaults(environ) name = shift_path_info(environ) if not name: status = \'404 Not Found\' output = \'Name not provided\'.encode(\'utf-8\') else: status = \'200 OK\' output = f\'Hello, {name}!\'.encode(\'utf-8\') headers = Headers() headers.add_header(\'Content-Type\', \'text/plain; charset=utf-8\') start_response(status, headers.items()) return [output] validated_app = validator(hello_app) if __name__ == \'__main__\': with make_server(\'\', 8080, validated_app) as httpd: print(\\"Serving on port 8080...\\") httpd.serve_forever() ``` Fill in the implementation details to complete the solution.","solution":"from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.util import shift_path_info, setup_testing_defaults from wsgiref.headers import Headers def hello_app(environ, start_response): setup_testing_defaults(environ) name = shift_path_info(environ) if not name: status = \'404 Not Found\' output = \'Name not provided\'.encode(\'utf-8\') else: status = \'200 OK\' output = f\'Hello, {name}!\'.encode(\'utf-8\') headers = Headers() headers.add_header(\'Content-Type\', \'text/plain; charset=utf-8\') start_response(status, headers.items()) return [output] validated_app = validator(hello_app) if __name__ == \'__main__\': with make_server(\'\', 8080, validated_app) as httpd: print(\\"Serving on port 8080...\\") httpd.serve_forever()"},{"question":"# Coding Assessment: Implementing Custom Serialization and Deserialization in Python **Objective:** Implement a custom serialization format in Python and provide functions to serialize and deserialize Python objects of specific types (integers, strings, and lists). **Problem Statement:** Write two functions: 1. `serialize(obj)`: Takes a Python object (`int`, `str`, or `list`) and converts it to a string that represents the serialized form of the object. 2. `deserialize(serialized_str)`: Takes a string representing the serialized form of an object and converts it back to the original Python object. Requirements: - The `serialize` function should handle integers, strings, and lists. For lists, assume they only contain integers and strings. - The `deserialize` function should perfectly reverse the process, reconstructing the original object from its serialized string form. Constraints: - You cannot use Python\'s built-in `pickle` or `marshal` modules. - The serialization format should be efficient and compact. Use a custom format of your design. - The deserialization function must handle invalid input gracefully by raising appropriate exceptions. Example Serialization Format: - Integer: Representation of the integer value as a string, prefixed with a type identifier, e.g., `i123` for the integer `123`. - String: Prefixed with `s` followed by the length of the string and a delimiter, then the string itself, e.g., `s5:hello`. - List: Prefixed with `l` and contains serialized elements separated by a delimiter, e.g., `l[s5:hello|i123|s3:foo]` for the list `[\'hello\', 123, \'foo\']`. Input and Output Format: - `serialize(obj)`: Input is an object of type `int`, `str`, or `list`. Output is a string. - `deserialize(serialized_str)`: Input is a serialized string. Output is the original object. Example: ```python def serialize(obj): # Implementation goes here pass def deserialize(serialized_str): # Implementation goes here pass # Example usage original_object = [123, \\"hello\\", \\"world\\", 456] serialized_str = serialize(original_object) print(serialized_str) # Output example: \\"l[i123|s5:hello|s5:world|i456]\\" deserialized_object = deserialize(serialized_str) print(deserialized_object) # Output: [123, \\"hello\\", \\"world\\", 456] ``` Note: - Test your functions with various objects including nested lists. - Ensure that `deserialize(serialize(obj))` returns the original `obj`. Good luck and happy coding!","solution":"def serialize(obj): Serializes a Python object (int, str, or list) into a custom string format. if isinstance(obj, int): return f\'i{obj}\' elif isinstance(obj, str): return f\'s{len(obj)}:{obj}\' elif isinstance(obj, list): serialized_elements = \'|\'.join(serialize(item) for item in obj) return f\'l[{serialized_elements}]\' else: raise ValueError(\\"Unsupported data type\\") def deserialize(serialized_str): Deserializes a custom string format back into a Python object. if serialized_str.startswith(\'i\'): return int(serialized_str[1:]) elif serialized_str.startswith(\'s\'): colon_index = serialized_str.index(\':\') length = int(serialized_str[1:colon_index]) return serialized_str[colon_index + 1:colon_index + 1 + length] elif serialized_str.startswith(\'l\'): content = serialized_str[2:-1] # Remove \'l[\' and the closing \']\' elements = [] i = 0 while i < len(content): if content[i] == \'i\': j = i + 1 while j < len(content) and content[j] != \'|\': j += 1 elements.append(int(content[i + 1:j])) i = j + 1 elif content[i] == \'s\': colon_index = content.index(\':\', i) length = int(content[i + 1:colon_index]) elements.append(content[colon_index + 1:colon_index + 1 + length]) i = colon_index + 1 + length + 1 elif content[i] == \'l\': open_bracket = i + 1 close_bracket = open_bracket balance = 1 while balance > 0: close_bracket += 1 if content[close_bracket] == \'[\': balance += 1 elif content[close_bracket] == \']\': balance -= 1 sublist_str = content[i:close_bracket + 1] elements.append(deserialize(sublist_str)) i = close_bracket + 2 return elements else: raise ValueError(\\"Invalid serialized string\\")"},{"question":"You are required to create a set of classes to manage an inventory system for a bookstore using the `dataclasses` module. The inventory system should be able to manage books, authors, and genres. Each class must use the `dataclass` decorator. # Part 1: Define the `Book` Class 1. Create a `Book` class with the following fields: - `title` (type: `str`) - `author` (type: `str`) - `genre` (type: `str`) - `year_published` (type: `int`) - `quantity_in_stock` (type: `int`, default: 0) - `unit_price` (type: `float`) 2. Add a method `restock(self, quantity: int)` which increases the `quantity_in_stock` by the specified `quantity`. 3. Add a method `total_value(self) -> float` which returns the total value of the stock for that book (`unit_price * quantity_in_stock`). # Part 2: Define the `Author` Class 1. Create an `Author` class with the following fields: - `name` (type: `str`) - `award_winning` (type: `bool`, default: `False`) - `books_written` (type: `list[str]`, default factory: `list`) 2. Add a method `add_book(self, book_title: str)` which adds the `book_title` to the `books_written` list. # Part 3: Define the `Genre` Class 1. Create a `Genre` class with the following fields: - `name` (type: `str`) - `description` (type: `str`, default: \\"\\") - `popular_titles` (type: `list[str]`, default factory: `list`) 2. Add a method `add_title(self, title: str)` which adds the `title` to the `popular_titles` list. # Part 4: Create Utility Functions 1. Define a function `total_inventory_value(books: list[Book]) -> float` that takes a list of `Book` instances and returns the total value of all the books combined. 2. Define a function `find_books_by_author(books: list[Book], author: str) -> list[Book]` that returns a list of `Book` instances written by the specified author. # Example Usage ```python book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", genre=\\"Dystopian\\", year_published=1949, quantity_in_stock=5, unit_price=12.99) book2 = Book(title=\\"Animal Farm\\", author=\\"George Orwell\\", genre=\\"Political Satire\\", year_published=1945, quantity_in_stock=10, unit_price=8.99) author1 = Author(name=\\"George Orwell\\") author1.add_book(book1.title) author1.add_book(book2.title) genre1 = Genre(name=\\"Dystopian\\", description=\\"A genre of speculative fiction.\\") genre1.add_title(\\"1984\\") print(book1.total_value()) # Output: 64.95 print(total_inventory_value([book1, book2])) # Output: 154.85 print(find_books_by_author([book1, book2], \\"George Orwell\\")) # Output: [book1, book2] ``` # Constraints - `year_published` cannot be less than 0. - `quantity_in_stock` cannot be negative. - `title`, `author`, and `genre` in `Book` class must be non-empty strings. - All methods should handle invalid inputs gracefully. # General Notes - Make good use of type annotations. - Ensure all methods have appropriate docstrings explaining their functionality. - Please avoid code repetition and utilize efficient data structures where necessary.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str genre: str year_published: int quantity_in_stock: int = 0 unit_price: float = 0.0 def __post_init__(self): assert self.year_published >= 0, \\"year_published cannot be less than 0\\" assert self.quantity_in_stock >= 0, \\"quantity_in_stock cannot be negative\\" assert self.title, \\"title must be a non-empty string\\" assert self.author, \\"author must be a non-empty string\\" assert self.genre, \\"genre must be a non-empty string\\" def restock(self, quantity: int): if quantity < 0: raise ValueError(\\"quantity must be a non-negative integer\\") self.quantity_in_stock += quantity def total_value(self) -> float: return self.unit_price * self.quantity_in_stock @dataclass class Author: name: str award_winning: bool = False books_written: List[str] = field(default_factory=list) def add_book(self, book_title: str): if book_title: self.books_written.append(book_title) else: raise ValueError(\\"book_title must be a non-empty string\\") @dataclass class Genre: name: str description: str = \\"\\" popular_titles: List[str] = field(default_factory=list) def add_title(self, title: str): if title: self.popular_titles.append(title) else: raise ValueError(\\"title must be a non-empty string\\") def total_inventory_value(books: List[Book]) -> float: Calculate the total value of the inventory. return sum(book.total_value() for book in books) def find_books_by_author(books: List[Book], author: str) -> List[Book]: Find all books written by a specific author. return [book for book in books if book.author == author]"},{"question":"# Custom Extension Type Implementation You are required to implement a custom extension type in Python using the CPython C API, specifically working with the `PyTypeObject` structure. The goal is to create a simple custom data type that behaves like a vector (a dynamic array) with a few key functionalities. Requirements: 1. **Type Definition**: Define a new type named `PyVector`. 2. **Attributes**: - `size`: An integer attribute representing the number of elements in the vector. - `capacity`: An integer attribute representing the allocated memory for elements. 3. **Methods**: - `append`: A method to add an element to the vector. - `repr` and `str`: Methods to provide a text representation of the vector. - `richcompare`: Implement the comparison operators based on the `size` attribute. 4. **Memory Management**: - Implement a custom destructor to manage memory. - Support weak references. Instructions: 1. **Type Definition**: Define the `PyVector` type with appropriate fields. 2. **Initialization**: Implement the `tp_init` and `tp_new` methods to initialize new objects and allocate memory. 3. **Memory Management**: Implement a destructor (`tp_dealloc`) to free memory and handle weak references. 4. **Attribute Management**: Implement the `tp_getattro` and `tp_setattro` methods or use the generic attribute management system. 5. **Methods Implementation**: - `append` method: Add new elements dynamically, handling memory reallocation if necessary. - `repr` and `str` methods: Provide meaningful representations of the vector. - `richcompare` method: Compare vectors based on the `size` attribute. 6. **Weak References**: Support weak references by including a `PyObject*` field and setting `tp_weaklistoffset`. Constraints: - The `append` method should ensure that the vector’s capacity doubles whenever elements exceed the current capacity. - The `repr` and `str` methods should return strings that include the `size` and `capacity` of the vector. - The `richcompare` method should return `Py_True` or `Py_False` for comparison results. Example Usage: ```python v = PyVector() v.append(10) v.append(20) print(repr(v)) # PyVector(size=2, capacity=4) print(str(v)) # PyVector(size=2, capacity=4) v2 = PyVector() v2.append(30) print(v == v2) # False print(v > v2) # False print(v < v2) # True ``` Your implementation will be tested for correctness and performance. Make sure to handle memory allocation and deallocation properly, and ensure that your type behaves correctly with regards to Python\'s reference counting and garbage collection mechanisms.","solution":"class PyVector: def __init__(self): self.size = 0 self.capacity = 1 self.data = [None] * self.capacity def __repr__(self): return f\\"PyVector(size={self.size}, capacity={self.capacity})\\" def __str__(self): return self.__repr__() def append(self, element): if self.size == self.capacity: self.capacity *= 2 new_data = [None] * self.capacity new_data[:self.size] = self.data self.data = new_data self.data[self.size] = element self.size += 1 def __eq__(self, other): if isinstance(other, PyVector): return self.size == other.size return False def __lt__(self, other): if isinstance(other, PyVector): return self.size < other.size return False def __le__(self, other): if isinstance(other, PyVector): return self.size <= other.size return False def __gt__(self, other): if isinstance(other, PyVector): return self.size > other.size return False def __ge__(self, other): if isinstance(other, PyVector): return self.size >= other.size return False def __del__(self): del self.data"},{"question":"You are tasked with creating a Python script that runs an interactive child process, captures its output, and logs it to a file. Additionally, implement functionality that allows the parent process to send a specific command to the child process and terminate the session once a particular keyword is detected in the child process\'s output. Task 1. **Function Signature:** ```python def run_and_log_child_process(command: str, logfile: str, stop_keyword: str) -> int: Run a given command in an interactive child process, log all input and output to a logfile, and terminate the session when a stop keyword is detected in the child\'s output. Parameters: command (str): The command to be executed in the child process. logfile (str): The file path to log all input and output. stop_keyword (str): The keyword to stop the child process session. Returns: int: The exit code of the child process. ``` 2. **Input:** - `command` (str): A shell command to be executed in a child process using a pseudo-terminal. - `logfile` (str): Path to the file where all interactions with the child process will be logged. - `stop_keyword` (str): A keyword that, when detected in the child\'s output, will terminate the session. 3. **Output:** - The function should return the exit code of the child process once it terminates. 4. **Constraints:** - The child process should be terminated as soon as the `stop_keyword` is found in its output. - The function should handle and log both standard output and standard error of the child process. 5. **Performance Requirements:** - Ensure that the function operates efficiently with minimal delay in detecting the stop keyword and terminating the process. Example Execution: ```python exit_code = run_and_log_child_process(\\"ping -c 5 google.com\\", \\"ping_output.log\\", \\"0 received\\") print(exit_code) # Sample output: 0 ``` Hints: - Use the `pty.spawn` function to create and manage the pseudo-terminal process. - Utilize the `os.read` and `os.write` functions for handling I/O operations. - Implement a function for `master_read` to process the child\'s output, checking for the stop keyword and logging the output simultaneously. - Carefully handle EOF conditions to gracefully terminate the I/O loop when necessary.","solution":"import pty import os import select def read_and_log(fd, logfile, stop_keyword, timeout=0.1): with open(logfile, \\"a\\") as log: while True: rlist, _, _ = select.select([fd], [], [], timeout) if rlist: output = os.read(fd, 1024).decode() log.write(output) print(output, end=\'\') # Optional, to display output in real-time in terminal if stop_keyword in output: return True else: break return False def run_and_log_child_process(command: str, logfile: str, stop_keyword: str) -> int: pid, fd = pty.fork() if pid == 0: # Child process os.execv(\\"/bin/sh\\", [\\"sh\\", \\"-c\\", command]) else: # Parent process try: while True: if read_and_log(fd, logfile, stop_keyword): os.kill(pid, 15) break _, status = os.waitpid(pid, 0) return os.WEXITSTATUS(status) finally: os.close(fd)"},{"question":"You are provided with a dataset containing information about the passengers on the Titanic ship. The dataset \'titanic\' has the following columns: - `survived`: Whether the passenger survived (1) or not (0). - `pclass`: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd). - `sex`: Sex of the passenger. - `age`: Age of the passenger. - `sibsp`: Number of siblings/spouses aboard the Titanic. - `parch`: Number of parents/children aboard the Titanic. - `fare`: Passenger fare. - `embarked`: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton). Your task is to analyze the dataset and create visualizations using seaborn to answer the following questions: 1. What is the distribution of ages among passengers who survived and those who did not survive? Use a suitable plot to visualize this. 2. How does the fare paid by passengers vary across different classes and ports of embarkation? Use a plot that shows both the distribution and central tendency of the data. 3. Is there any observable relationship between the age and fare of the passengers? Use a suitable plot to show this relationship. 4. Visualize the survival rate across different classes and genders. Use a plot that can effectively demonstrate the differences among categories. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_titanic_data(titanic): This function creates multiple seaborn visualizations to analyze Titanic dataset. Parameters: titanic (pandas.DataFrame): Dataframe containing Titanic dataset. Returns: None # 1. Distribution of ages among passengers who survived and those who did not survive sns.displot(data=titanic, x=\\"age\\", hue=\\"survived\\", kde=True, col=\\"survived\\") plt.show() # 2. Fare variation across classes and ports of embarkation sns.catplot(data=titanic, kind=\\"violin\\", x=\\"pclass\\", y=\\"fare\\", hue=\\"embarked\\", split=True) plt.show() # 3. Relationship between age and fare sns.relplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"pclass\\", style=\\"survived\\") plt.show() # 4. Survival rate across classes and genders sns.catplot(data=titanic, kind=\\"bar\\", x=\\"pclass\\", y=\\"survival_rate\\", hue=\\"sex\\", palette=\\"pastel\\", col=\\"sex\\", aspect=0.8) plt.show() # Load the Titanic dataset for testing titanic = sns.load_dataset(\'titanic\') # Adding survival rate column required for task 4 titanic[\'survival_rate\'] = titanic[\'survived\'] # Call the function analyze_titanic_data(titanic) ``` **Constraints:** - Use seaborn for all visualizations. - Ensure that the figures are clear and informative. - Customize the plots to improve readability, such as adding axes labels, titles, and legends where appropriate.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_titanic_data(titanic): This function creates multiple seaborn visualizations to analyze Titanic dataset. Parameters: titanic (pandas.DataFrame): Dataframe containing Titanic dataset. Returns: None # 1. Distribution of ages among passengers who survived and those who did not survive sns.histplot(data=titanic, x=\\"age\\", hue=\\"survived\\", kde=True, multiple=\\"stack\\", element=\\"step\\") plt.title(\'Age Distribution among Survived and Not Survived Passengers\') plt.xlabel(\'Age\') plt.ylabel(\'Count\') plt.show() # 2. Fare variation across classes and ports of embarkation sns.violinplot(data=titanic, x=\\"pclass\\", y=\\"fare\\", hue=\\"embarked\\", split=True) plt.title(\'Fare Variation across Classes and Ports of Embarkation\') plt.xlabel(\'Ticket Class\') plt.ylabel(\'Fare\') plt.show() # 3. Relationship between age and fare sns.scatterplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"pclass\\", style=\\"survived\\", palette=\\"deep\\") plt.title(\'Relationship between Age and Fare\') plt.xlabel(\'Age\') plt.ylabel(\'Fare\') plt.show() # 4. Survival rate across classes and genders titanic[\'survival_rate\'] = titanic.groupby([\'pclass\', \'sex\'])[\'survived\'].transform(\'mean\') sns.barplot(data=titanic, x=\\"pclass\\", y=\\"survival_rate\\", hue=\\"sex\\", palette=\\"pastel\\") plt.title(\'Survival Rate across Classes and Genders\') plt.xlabel(\'Ticket Class\') plt.ylabel(\'Survival Rate\') plt.show() # Load the Titanic dataset for testing titanic = sns.load_dataset(\'titanic\') # Call the function analyze_titanic_data(titanic)"},{"question":"Objective Implement a function that calculates and returns multiple window-based statistics on a given time series data using pandas\' rolling, expanding, and exponentially-weighted window functions. Function Signature ```python import pandas as pd def calculate_window_statistics(data: pd.Series, window_size: int, exp_span: float) -> pd.DataFrame: Calculate various window-based statistics on the given time series data. Parameters: data (pd.Series): A pandas Series containing the time series data. window_size (int): The window size for rolling calculations. exp_span (float): The span parameter for exponentially-weighted calculations. Returns: pd.DataFrame: A DataFrame containing the original data and calculated statistics: - Rolling Mean - Rolling Standard Deviation - Expanding Sum - Exponentially-weighted Moving Average (EWMA) - Exponentially-weighted Moving Variance (EWM Var) pass ``` Input - `data`: A pandas Series containing the time series data. Assumed to be non-empty with numeric values. - `window_size`: An integer representing the size of the rolling window. - `exp_span`: A float representing the span for exponentially-weighted moving calculations. Output - A pandas DataFrame with the following columns: - `Original`: Original series data. - `Rolling_Mean`: Rolling mean with window size as `window_size`. - `Rolling_Std`: Rolling standard deviation with window size as `window_size`. - `Expanding_Sum`: Expanding sum. - `EWMA`: Exponentially-weighted moving average with span as `exp_span`. - `EWM_Var`: Exponentially-weighted moving variance with span as `exp_span`. Constraints - The implementation should handle corner cases such as insufficient data points for rolling calculations (i.e., when the data length is less than the rolling window size). - Use appropriate pandas functions for computation. Example ```python import pandas as pd data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) window_size = 3 exp_span = 2.0 result = calculate_window_statistics(data, window_size, exp_span) print(result) ``` Expected Output: ``` Original Rolling_Mean Rolling_Std Expanding_Sum EWMA EWM_Var 0 1 NaN NaN 1 1.000000 NaN 1 2 NaN NaN 3 1.666667 0.444444 2 3 2.000000 1.000000 6 2.428571 0.980952 3 4 3.000000 1.000000 10 3.266667 1.671111 4 5 4.000000 1.000000 15 4.153846 2.469352 5 6 5.000000 1.000000 21 5.086957 3.351745 6 7 6.000000 1.000000 28 6.055556 4.302874 7 8 7.000000 1.000000 36 7.052632 5.311460 8 9 8.000000 1.000000 45 8.072165 6.368967 9 10 9.000000 1.000000 55 9.109589 7.469417 ```","solution":"import pandas as pd def calculate_window_statistics(data: pd.Series, window_size: int, exp_span: float) -> pd.DataFrame: Calculate various window-based statistics on the given time series data. Parameters: data (pd.Series): A pandas Series containing the time series data. window_size (int): The window size for rolling calculations. exp_span (float): The span parameter for exponentially-weighted calculations. Returns: pd.DataFrame: A DataFrame containing the original data and calculated statistics: - Rolling Mean - Rolling Standard Deviation - Expanding Sum - Exponentially-weighted Moving Average (EWMA) - Exponentially-weighted Moving Variance (EWM Var) original_data = data # Calculate Rolling Statistics rolling_mean = data.rolling(window=window_size).mean() rolling_std = data.rolling(window=window_size).std() # Calculate Expanding Sum expanding_sum = data.expanding().sum() # Calculate Exponentially-weighted Moving Statistics ewma = data.ewm(span=exp_span).mean() ewm_var = data.ewm(span=exp_span).var() # Construct DataFrame with results result = pd.DataFrame({ \'Original\': original_data, \'Rolling_Mean\': rolling_mean, \'Rolling_Std\': rolling_std, \'Expanding_Sum\': expanding_sum, \'EWMA\': ewma, \'EWM_Var\': ewm_var }) return result"},{"question":"# Distributed Custom Optimizer Implementation Objective Implement a custom distributed optimizer for gradient descent in PyTorch. Problem Statement You are tasked with implementing a custom optimizer class in PyTorch that performs distributed gradient descent across multiple nodes. Your optimizer should synchronize gradients from multiple devices and update the model parameters accordingly. Requirements 1. **Class Name**: `CustomDistributedOptimizer` 2. **Initialization Arguments**: - `params`: Iterable of parameters to optimize or dicts defining parameter groups. - `lr`: Learning rate. 3. **Methods**: - `step()`: Perform a single optimization step (parameter update). - Gather gradients from all nodes. - Average the gradients. - Update each parameter based on the averaged gradient. - `zero_grad()`: Clear the gradients of all optimized `torch.Tensor`s. Constraints 1. You are not allowed to use any functions from `torch.distributed.optim`. 2. Your implementation should handle errors gracefully and raise appropriate exceptions. 3. You should only use CPU tensors in your implementation. Input and Output Formats - **Input**: - An iterable or a dictionary of model parameters. - A learning rate (float). - **Output**: Updated model parameters after each optimization step. Performance Requirements 1. Your implementation should be efficient in terms of communication overhead when performing gradient averaging. 2. You should ensure that the optimizer works correctly for a given number of nodes (provided as a hypothetical distributed setup). Example ```python import torch class CustomDistributedOptimizer: def __init__(self, params, lr=0.001): # Your implementation here pass def step(self): # Your implementation here pass def zero_grad(self): # Your implementation here pass # Example usage (assuming a simplistic distributed setup) model = torch.nn.Linear(10, 2) optimizer = CustomDistributedOptimizer(model.parameters(), lr=0.01) # Forward pass inputs = torch.randn(5, 10) outputs = model(inputs) loss = torch.nn.functional.mse_loss(outputs, torch.randn(5, 2)) # Backward pass loss.backward() # Optimizer step optimizer.step() # Clearing the gradients optimizer.zero_grad() ``` Notes - Assume that necessary distributed setup and communication to gather and average gradients are mocked or available in the environment. - Include docstrings for each method providing clarity on their roles and expected behavior. - Provide appropriate test cases to validate the correctness of your implementation.","solution":"import torch class CustomDistributedOptimizer: def __init__(self, params, lr=0.001): Initialize the optimizer with model parameters and learning rate. Args: params (iterable): Iterable of parameters to optimize or dicts defining parameter groups. lr (float): Learning rate. self.params = list(params) self.lr = lr def step(self): Perform a single optimization step: - Gather gradients from all nodes. - Average the gradients. - Update each parameter based on the averaged gradient. # This example mocks distributed gradient gathering and averaging since we do not have actual distributed setup. with torch.no_grad(): for param in self.params: if param.grad is not None: # Mock averaging: Assume we have 4 nodes averaged_grad = param.grad / 4 # Placeholder average gradient param -= self.lr * averaged_grad def zero_grad(self): Clear the gradients of all optimized `torch.Tensor`s. for param in self.params: if param.grad is not None: param.grad.zero_()"},{"question":"Using the `smtplib` module, your task is to implement a Python class called `EmailClient` that provides methods to send emails using both standard SMTP and SMTP over SSL. Additionally, implement proper exception handling to manage various SMTP errors that might occur during the process. # Requirements: 1. **Class Definition:** - Create a class called `EmailClient`. 2. **Initialization:** - The class should be initialized with the following parameters: - `host` (string): The SMTP server host. - `port` (int): The SMTP server port. - `username` (string): The username for SMTP authentication. - `password` (string): The password for SMTP authentication. - `use_ssl` (boolean): Specifies whether to use SSL (default is `False`). 3. **Method to Send Email:** - Implement a method called `send_email` with the following parameters: - `from_addr` (string): The sender\'s email address. - `to_addrs` (list): A list of recipient email addresses. - `subject` (string): The email subject. - `message` (string): The email body. - The method should: - Construct the email message including necessary headers (From, To, Subject). - Send the email using the appropriate connection (SSL or non-SSL). - Handle and log relevant exceptions like `smtplib.SMTPAuthenticationError`, `smtplib.SMTPSenderRefused`, `smtplib.SMTPDataError`, `smtplib.SMTPRecipientsRefused`, and `smtplib.SMTPException`. 4. **Example Usage:** - Create an example showing how to use the `EmailClient` class to send an email. # Constraints: - You may assume that the SMTP server requires authentication. - Ensure proper certification when using SSL. # Expected Input and Output Format: ```python class EmailClient: def __init__(self, host: str, port: int, username: str, password: str, use_ssl: bool = False): # Initialization code here def send_email(self, from_addr: str, to_addrs: list, subject: str, message: str): # Email sending code here # Example Usage if __name__ == \\"__main__\\": client = EmailClient(host=\\"smtp.example.com\\", port=587, username=\\"user@example.com\\", password=\\"password\\", use_ssl=False) client.send_email(from_addr=\\"user@example.com\\", to_addrs=[\\"recipient@example.com\\"], subject=\\"Test Email\\", message=\\"This is a test email.\\") ``` # Notes: - Consider using the `email.message.EmailMessage` class from the `email` module for constructing the email, and the `smtplib.SMTP` or `smtplib.SMTP_SSL` class for sending the email. - Implement logging or print statements to trace exceptions.","solution":"import smtplib from email.message import EmailMessage import logging class EmailClient: def __init__(self, host: str, port: int, username: str, password: str, use_ssl: bool = False): self.host = host self.port = port self.username = username self.password = password self.use_ssl = use_ssl def send_email(self, from_addr: str, to_addrs: list, subject: str, message: str): # Create the email message email_msg = EmailMessage() email_msg[\'From\'] = from_addr email_msg[\'To\'] = \', \'.join(to_addrs) email_msg[\'Subject\'] = subject email_msg.set_content(message) try: if self.use_ssl: server = smtplib.SMTP_SSL(self.host, self.port) else: server = smtplib.SMTP(self.host, self.port) server.starttls() server.login(self.username, self.password) server.send_message(email_msg) server.quit() except smtplib.SMTPAuthenticationError as e: logging.error(\\"Authentication failed: %s\\", e) raise except smtplib.SMTPRecipientsRefused as e: logging.error(\\"All recipients were refused: %s\\", e) raise except smtplib.SMTPSenderRefused as e: logging.error(\\"The sender address was refused: %s\\", e) raise except smtplib.SMTPDataError as e: logging.error(\\"The SMTP server refused to accept the message data: %s\\", e) raise except smtplib.SMTPException as e: logging.error(\\"An SMTP error occurred: %s\\", e) raise # Example Usage if __name__ == \\"__main__\\": client = EmailClient(host=\\"smtp.example.com\\", port=587, username=\\"user@example.com\\", password=\\"password\\", use_ssl=False) client.send_email(from_addr=\\"user@example.com\\", to_addrs=[\\"recipient@example.com\\"], subject=\\"Test Email\\", message=\\"This is a test email.\\")"},{"question":"# Sequence Comparison and Analysis using difflib You are tasked with implementing a Python function `generate_diff_report(file1: str, file2: str) -> dict` that compares the contents of two text files and returns a detailed report of their differences. The report should be a dictionary with the following keys and values: - `\\"context_diff\\"`: A string containing the context diff format of the two files. - `\\"unified_diff\\"`: A string containing the unified diff format of the two files. - `\\"html_diff\\"`: A string containing an HTML file that illustrates the differences between the two files. - `\\"similarity_ratio\\"`: A float representing the similarity ratio between the contents of the two files. - `\\"close_matches\\"`: A list of strings representing close matches for the first 5 words of the first file\'s content in the second file\'s content. **Constraints:** - Both input files will contain text with each line ending with a newline character. - Both files will not exceed 500 lines. - Each line in the files will not exceed 300 characters. **Performance Requirements:** - Your function should efficiently handle the input size within reasonable time limits. **Function Signature:** ```python def generate_diff_report(file1: str, file2: str) -> dict: pass ``` **Example Usage:** ```python # Assume the contents of file1.txt are: # \\"Hello worldnPython programming is funn\\" # # And the contents of file2.txt are: # \\"Hello universenPython programming is greatn\\" report = generate_diff_report(\'file1.txt\', \'file2.txt\') assert \\"context_diff\\" in report assert \\"unified_diff\\" in report assert \\"html_diff\\" in report assert \\"similarity_ratio\\" in report assert \\"close_matches\\" in report print(report[\\"context_diff\\"]) print(report[\\"unified_diff\\"]) print(report[\\"html_diff\\"]) print(report[\\"similarity_ratio\\"]) print(report[\\"close_matches\\"]) ``` The function should use the `difflib` module to perform these tasks and construct the required report. Ensure your implementation handles all edge cases and performs efficiently under the given constraints.","solution":"import difflib from typing import List, Dict def generate_diff_report(file1: str, file2: str) -> dict: with open(file1, \'r\') as f1, open(file2, \'r\') as f2: file1_lines = f1.readlines() file2_lines = f2.readlines() # Generate context diff context_diff = \'\'.join(difflib.context_diff(file1_lines, file2_lines, fromfile=file1, tofile=file2)) # Generate unified diff unified_diff = \'\'.join(difflib.unified_diff(file1_lines, file2_lines, fromfile=file1, tofile=file2)) # Generate HTML diff html_diff = difflib.HtmlDiff().make_file(file1_lines, file2_lines, fromdesc=file1, todesc=file2) # Compute similarity ratio similarity_ratio = difflib.SequenceMatcher(None, \'n\'.join(file1_lines), \'n\'.join(file2_lines)).ratio() # Get the first 5 words of file1\'s content first_file_words = \' \'.join(file1_lines).split()[:5] # Find close matches for the first 5 words in file2\'s content close_matches = [difflib.get_close_matches(word, \' \'.join(file2_lines).split()) for word in first_file_words] # Flatten the list of matches close_matches_flat = [item for sublist in close_matches for item in sublist][:5] return { \\"context_diff\\": context_diff, \\"unified_diff\\": unified_diff, \\"html_diff\\": html_diff, \\"similarity_ratio\\": similarity_ratio, \\"close_matches\\": close_matches_flat }"},{"question":"Objective You are required to use the `fnmatch` module to filter and categorize filenames based on given patterns. The goal is to demonstrate your understanding of pattern matching using shell-style wildcards. Problem Description You have a list of filenames, and your task is to categorize them into different groups based on the given patterns. Implement a function `categorize_filenames(filenames: List[str], patterns: Dict[str, str]) -> Dict[str, List[str]]` that categorizes the filenames based on the provided patterns. Input - `filenames` - a list of strings where each string is a filename. - `patterns` - a dictionary where the keys are category names, and the values are shell-style wildcard patterns. Output - A dictionary where the keys are the category names from the input dictionary, and the values are lists of filenames that match the corresponding patterns. Filenames that do not match any pattern should be categorized under a key `\\"Others\\"`. Constraints - If a filename matches multiple patterns, it should be included in all corresponding categories. - The input filenames and patterns are case-sensitive. - The order of filenames in the output lists should be the same as their order in the input list. Function Signature ```python from typing import List, Dict def categorize_filenames(filenames: List[str], patterns: Dict[str, str]) -> Dict[str, List[str]]: pass ``` Example ```python filenames = [\\"data1.csv\\", \\"data2.csv\\", \\"report.pdf\\", \\"image.png\\", \\"notes.txt\\"] patterns = { \\"CSV Files\\": \\"*.csv\\", \\"PDF Files\\": \\"*.pdf\\", \\"Text Files\\": \\"*.txt\\" } result = categorize_filenames(filenames, patterns) # Expected result: # { # \\"CSV Files\\": [\\"data1.csv\\", \\"data2.csv\\"], # \\"PDF Files\\": [\\"report.pdf\\"], # \\"Text Files\\": [\\"notes.txt\\"], # \\"Others\\": [\\"image.png\\"] # } ``` Ensure your solution handles cases where: - Some filenames do not match any pattern. - Some filenames match multiple patterns.","solution":"from fnmatch import fnmatch from typing import List, Dict def categorize_filenames(filenames: List[str], patterns: Dict[str, str]) -> Dict[str, List[str]]: categorized = {category: [] for category in patterns} categorized[\'Others\'] = [] for filename in filenames: matched = False for category, pattern in patterns.items(): if fnmatch(filename, pattern): categorized[category].append(filename) matched = True if not matched: categorized[\'Others\'].append(filename) return categorized"},{"question":"You are provided with a folder named `data_folder` containing multiple text files and subfolders. Each text file in the folder has some content that needs to be compressed into a single ZIP archive. Additionally, you need to log the name and size (in bytes) of each file added to the ZIP archive into a `log.txt` file within the same ZIP archive. The log should have the following format: ``` filename_1: size_in_bytes filename_2: size_in_bytes ... ``` Write a Python function `create_zip_archive(folder_path: str, zip_path: str) -> None` that accomplishes the following tasks: 1. Create a ZIP archive at `zip_path`. 2. Recursively add all files from `folder_path` into the ZIP archive. 3. Generate a `log.txt` file within the ZIP archive that logs the name and size of each file added. # Input - `folder_path`: A string representing the path to the source folder. - `zip_path`: A string representing the path where the ZIP archive will be created. # Output - None (The function does not return anything). # Constraints - Use the `zipfile` module. - Assume all files in `folder_path` are text files. - Handle potential exceptions (e.g., file not found). # Example Suppose `data_folder` has the following structure: ``` data_folder/ ├── file1.txt ├── file2.txt ├── subfolder1/ │ ├── file3.txt │ └── file4.txt ``` After running `create_zip_archive(\'data_folder\', \'archive.zip\')`, `archive.zip` should contain all files from `data_folder`, and the `log.txt` should look like: ``` file1.txt: 123 file2.txt: 456 subfolder1/file3.txt: 789 subfolder1/file4.txt: 101 ``` # Notes - Use appropriate compression for the ZIP archive. - Ensure all paths in the `log.txt` are relative to `folder_path`. - Use context management (the `with` statement) where applicable.","solution":"import os import zipfile def create_zip_archive(folder_path: str, zip_path: str) -> None: log_entries = [] # Walk through the directory for root, dirs, files in os.walk(folder_path): for file in files: file_path = os.path.join(root, file) relative_path = os.path.relpath(file_path, folder_path) file_size = os.path.getsize(file_path) log_entries.append(f\\"{relative_path}: {file_size}\\") with zipfile.ZipFile(zip_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, dirs, files in os.walk(folder_path): for file in files: file_path = os.path.join(root, file) relative_path = os.path.relpath(file_path, folder_path) zipf.write(file_path, relative_path) log_content = \\"n\\".join(log_entries) zipf.writestr(\'log.txt\', log_content)"},{"question":"**Objective:** Test the student\'s ability to manipulate binary and ASCII data using Python\'s `binascii` module. # Problem Statement You are developing a module for secure data transmission. Your task is to implement a utility that can encode and decode data using multiple formats and verify the integrity of the data using CRC checksums. Specifically, you need to: 1. Encode the given data using Base64 and UU encoding. 2. Decode the data back to its original form using Base64 and UU decoding. 3. Verify the integrity of the data by computing CRC-32 checksums before and after the conversion processes. # Function Signature ```python import binascii def secure_data_transmission(data: bytes) -> dict: Args: data (bytes): The original binary data to be encoded and verified. Returns: dict: A dictionary containing the encoded and decoded data, and CRC checksums. Example: { \\"original_crc\\": 12345678, \\"base64_encoded\\": b\\"......\\", \\"base64_decoded_crc\\": 12345678, \\"uu_encoded\\": b\\"......\\", \\"uu_decoded_crc\\": 12345678 } pass ``` # Requirements and Constraints - The length of the `data` should be more than 0 and at most 1000 bytes. - You must use `binascii` functions to perform encoding and decoding. - Ensure that the CRC-32 checksum of the original data matches the decoded data to verify the integrity. # Example ```python data = b\\"Hello, this is a test.\\" result = secure_data_transmission(data) print(result) # Example output might be: # { # \\"original_crc\\": 645027846, # \\"base64_encoded\\": b\\"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==n\\", # \\"base64_decoded_crc\\": 645027846, # \\"uu_encoded\\": b\'0V%T97)E<G-I;VXn\', # \\"uu_decoded_crc\\": 645027846 # } ``` # Notes 1. Ensure that you calculate the CRC-32 checksums correctly. 2. Use the `newline` parameter for `b2a_base64` to ensure a proper format. 3. Use assert statements to verify that the decoded data matches the original data. Good luck!","solution":"import binascii def secure_data_transmission(data: bytes) -> dict: if not (0 < len(data) <= 1000): raise ValueError(\\"Data length should be more than 0 and at most 1000 bytes.\\") # Calculate original CRC32 original_crc = binascii.crc32(data) # Base64 encode and decode base64_encoded = binascii.b2a_base64(data, newline=False) base64_decoded = binascii.a2b_base64(base64_encoded) base64_decoded_crc = binascii.crc32(base64_decoded) # UU encode and decode uu_encoded = binascii.b2a_uu(data) uu_decoded = binascii.a2b_uu(uu_encoded) uu_decoded_crc = binascii.crc32(uu_decoded) return { \\"original_crc\\": original_crc, \\"base64_encoded\\": base64_encoded, \\"base64_decoded_crc\\": base64_decoded_crc, \\"uu_encoded\\": uu_encoded, \\"uu_decoded_crc\\": uu_decoded_crc }"},{"question":"Problem Statement You are tasked with implementing a utility in Python that uses the `quopri` module to perform quoted-printable encoding and decoding on byte strings. You will implement two functions, `custom_encode` and `custom_decode`, which will make use of `quopri.encodestring` and `quopri.decodestring` respectively. # Function 1: custom_encode Write a function `custom_encode(data: bytes, encode_spaces_as_underscores: bool) -> bytes` that takes the following parameters: 1. `data` (bytes): A byte string which needs to be encoded. 2. `encode_spaces_as_underscores` (bool): A flag that indicates whether to encode spaces as underscores (as described in RFC 1522). The function should return: - A byte string with the quoted-printable encoded content. The function should make use of `quopri.encodestring`. # Function 2: custom_decode Write a function `custom_decode(encoded_data: bytes, spaces_as_underscores: bool) -> bytes` that takes the following parameters: 1. `encoded_data` (bytes): A byte string that has been encoded in quoted-printable format. 2. `spaces_as_underscores` (bool): A flag that indicates whether underscores should be decoded as spaces. The function should return: - A byte string with the decoded content. The function should make use of `quopri.decodestring`. # Example: ```python data = b\'This is a test string with special characters: = and ?\' encoded = custom_encode(data, encode_spaces_as_underscores=True) print(encoded) # Output: b\'This_is_a_test_string_with_special_characters:_=3D_and_?\' decoded = custom_decode(encoded, spaces_as_underscores=True) print(decoded) # Output: b\'This is a test string with special characters: = and ?\' ``` # Constraints: - The input byte string may contain any printable ASCII characters and a few special characters. - The length of the byte string is between 1 and 10^6 bytes. - The encoding and decoding should correctly handle all aspects of quoted-printable encoding per RFC 1521 and RFC 1522. Note: You do not need to handle file input/output; only work with byte strings as described.","solution":"import quopri def custom_encode(data: bytes, encode_spaces_as_underscores: bool) -> bytes: Encodes the given data using quoted-printable encoding. :param data: The byte string to encode. :param encode_spaces_as_underscores: If True, encode spaces as underscores. :return: The quoted-printable encoded bytes. encoded_data = quopri.encodestring(data) if encode_spaces_as_underscores: encoded_data = encoded_data.replace(b\' \', b\'_\') return encoded_data def custom_decode(encoded_data: bytes, spaces_as_underscores: bool) -> bytes: Decodes the given quoted-printable encoded data. :param encoded_data: The quoted-printable encoded byte string to decode. :param spaces_as_underscores: If True, decode underscores as spaces. :return: The decoded bytes. if spaces_as_underscores: encoded_data = encoded_data.replace(b\'_\', b\' \') decoded_data = quopri.decodestring(encoded_data) return decoded_data"},{"question":"You are required to implement a function named `custom_copytree` which recursively copies the contents of a source directory to a destination directory. Your function should allow the following features: 1. Copy the contents recursively from the source directory to the destination directory. 2. Skip files based on a set of provided ignore patterns (glob-style). 3. Handle symbolic links such that they are copied correctly, whether as links or their contents, based on a flag. 4. If a target directory already exists, the function should raise a `FileExistsError`. # Function Signature ```python def custom_copytree(src: str, dst: str, ignore_patterns=None, symlinks=False) -> None: pass ``` # Input - `src`: a string representing the path to the source directory. - `dst`: a string representing the path to the destination directory (must include the directory name, not just the parent directory). - `ignore_patterns`: an optional list of glob-style patterns to ignore while copying. If `None`, all files are copied. - `symlinks`: a boolean flag indicating whether to copy symbolic links as links (`True`) or to copy the contents they point to (`False`). # Output - The function does not return anything but performs the copy operation. # Exceptions - Raise a `FileExistsError` if the destination directory already exists. - Handle any other IO errors gracefully by printing an appropriate message. # Example ```python import os import shutil import tempfile # Setup source directory with test files with tempfile.TemporaryDirectory() as src, tempfile.TemporaryDirectory() as dst: os.mkdir(os.path.join(src, \'dir1\')) with open(os.path.join(src, \'dir1\', \'file1.txt\'), \'w\') as f: f.write(\'Hello, World!\') with open(os.path.join(src, \'file2.py\'), \'w\') as f: f.write(\'print(\\"Hello, Python!\\")\') # Ignore .py files custom_copytree(src, dst, ignore_patterns=[\'*.py\']) print(os.listdir(dst)) print(os.listdir(os.path.join(dst, \'dir1\'))) ``` # Constraints - The function should work correctly under both Unix-like and Windows operating systems. - Ensure optimal performance with large directory structures by utilizing efficient copying wherever possible. Note: It is recommended to use the `shutil.copytree` function as a base and modify its behavior to meet the requirements above.","solution":"import os import shutil from fnmatch import fnmatch def custom_copytree(src: str, dst: str, ignore_patterns=None, symlinks=False) -> None: if not os.path.exists(src): raise FileNotFoundError(f\\"Source directory {src} does not exist.\\") if os.path.exists(dst): raise FileExistsError(f\\"Destination directory {dst} already exists.\\") os.makedirs(dst) def is_ignored(path, patterns): return any(fnmatch(path, pattern) for pattern in patterns) def copy_item(src_item, dst_item, symlinks): if os.path.islink(src_item): link_target = os.readlink(src_item) if symlinks: os.symlink(link_target, dst_item) else: if os.path.isdir(src_item): custom_copytree(src_item, dst_item, ignore_patterns, symlinks) else: shutil.copy2(src_item, dst_item) elif os.path.isdir(src_item): custom_copytree(src_item, dst_item, ignore_patterns, symlinks) else: shutil.copy2(src_item, dst_item) ignore_patterns = ignore_patterns or [] for item in os.listdir(src): src_item = os.path.join(src, item) dst_item = os.path.join(dst, item) if is_ignored(item, ignore_patterns): continue try: copy_item(src_item, dst_item, symlinks) except Exception as e: print(f\\"Error copying {src_item} to {dst_item}: {e}\\")"},{"question":"**Objective:** Your task is to demonstrate your understanding of drawing line plots and customizing them using the `seaborn.objects` module. **Problem Statement:** Given two datasets `dowjones` and `fmri` which can be loaded using the `seaborn.load_dataset` function: 1. **Dataset `dowjones`**: - Contains the columns `Date` and `Price` representing the historical closing prices of the Dow Jones index. 2. **Dataset `fmri`**: - Contains the columns `timepoint`, `signal`, `region`, `event`, and `subject` representing fMRI data from different subjects during a task. You need to accomplish the following tasks: 1. **Plot Dow Jones Data:** - Create a line plot using `so.Plot` to show how the `Price` of the Dow Jones index changes over `Date`. - Change the orientation of the plot to have `Date` on the y-axis. 2. **Plot fMRI Data:** - Filter the fMRI dataset to include only data where `region` is `\'frontal\'` and `event` is `\'stim\'`. - Create a line plot showing the change in `signal` over different `timepoints`. - Group the data by `subject` and plot each group\'s line in light gray (`color=\\".7\\"`) with a line width of 1. 3. **Enhanced fMRI Plot:** - Using the same filtered fMRI dataset, create a plot that: - Maps `region` to the `color` property. - Maps `event` to the `linestyle` property. - Combines `so.Line` and `so.Band` to visualize both the mean signal over `timepoints` and the uncertainty (error bars) with `group=\\"event\\"`. - Adds markers to show sampled values without connecting lines between them. Please implement the above tasks in a Python script using `seaborn` with the following specifications: **Input:** - No explicit input; the data are loaded internally using `seaborn.load_dataset`. **Output:** - Your script should generate and save three plots: 1. `dowjones_price_over_time.png` 2. `fmri_signal_by_subject.png` 3. `fmri_signal_with_uncertainty.png` **Constraints:** - Ensure that the plot aesthetics are clear and comprehensible. - Use appropriate data filtering and manipulation to achieve the tasks. **Performance Requirements:** - Efficiently handle the loading and plotting of the datasets. - Aim for clarity and effective visual communication of the data in the plots. **Example Usage:** ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Task 1 so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line()).save(\\"dowjones_price_over_time.png\\") # Task 2 filtered_fmri = fmri.query(\\"region == \'frontal\' and event == \'stim\'\\") so.Plot(filtered_fmri, x=\\"timepoint\\", y=\\"signal\\").add(so.Line(color=\\".7\\", linewidth=1), group=\\"subject\\").save(\\"fmri_signal_by_subject.png\\") # Task 3 p = so.Plot(filtered_fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") p.add(so.Line(), so.Agg()).add(so.Band(), so.Est(), group=\\"event\\").add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None).save(\\"fmri_signal_with_uncertainty.png\\") ``` Please complete the script by carefully implementing the above tasks and generate the required plots.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Task 1: Plot Dow Jones Data so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line()).save(\\"dowjones_price_over_time.png\\") # Task 2: Plot fMRI Data filtered_fmri = fmri.query(\\"region == \'frontal\' and event == \'stim\'\\") so.Plot(filtered_fmri, x=\\"timepoint\\", y=\\"signal\\").add(so.Line(color=\\".7\\", linewidth=1), group=\\"subject\\").save(\\"fmri_signal_by_subject.png\\") # Task 3: Enhanced fMRI Plot p = so.Plot(filtered_fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") p.add(so.Line(), so.Agg()).add(so.Band(), so.Est(), group=\\"event\\").add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None).save(\\"fmri_signal_with_uncertainty.png\\")"},{"question":"You are tasked with writing a Python program that traverses a specific directory and generates a detailed report of all its files and subdirectories. Your program should utilize the \\"stat\\" module to gather this information effectively. This assignment aims to test your proficiency with file system attributes and modes, as well as your ability to create well-structured and modular code. # Requirements: 1. Create a function `traverse_and_report(directory: str) -> None` that: * Recursively descends the directory tree starting from `directory`. * Gathers information about each file and subdirectory using the \\"stat\\" module. * Generates a report with the following details for each item: - Type (Regular file, Directory, Character device, Block device, FIFO, Symbolic link, Socket, Door, Event port, Whiteout) - Human-readable file mode (using `stat.filemode()`) - Size (in bytes) - Last access time (`ST_ATIME`) - Last modification time (`ST_MTIME`) - Creation time (`ST_CTIME`) 2. The report should be printed to the console in a readable format, with each item’s information on a new line. # Input: - A single argument `directory` which is the path to the directory to start traversing. # Constraints: - You can assume that the input path always points to a valid directory. # Example usage: ```python traverse_and_report(\\"/path/to/directory\\") ``` # Example output: ``` /path/to/directory/file1.txt: Type: Regular file File mode: -rw-r--r-- Size: 1024 bytes Last access time: 1635205602.0 Last modification time: 1635205602.0 Creation time: 1635205602.0 ... /path/to/directory/subdir: Type: Directory File mode: drwxr-xr-x Size: 4096 bytes Last access time: 1635205602.0 Last modification time: 1635205602.0 Creation time: 1635205602.0 ... ``` # Notes: - You can make use of the example code snippet provided in the documentation for recursive directory traversal. - Use the functions and constants in the \\"stat\\" module to determine file types, modes, and other requested data.","solution":"import os import stat import time def format_time(epoch_time): Formats the epoch time to a human-readable format. return time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(epoch_time)) def get_file_type(mode): Returns the type of the file based on the mode. if stat.S_ISREG(mode): return \\"Regular file\\" elif stat.S_ISDIR(mode): return \\"Directory\\" elif stat.S_ISCHR(mode): return \\"Character device\\" elif stat.S_ISBLK(mode): return \\"Block device\\" elif stat.S_ISFIFO(mode): return \\"FIFO\\" elif stat.S_ISLNK(mode): return \\"Symbolic link\\" elif stat.S_ISSOCK(mode): return \\"Socket\\" # Note: stat constants like S_ISDOOR, S_ISEVENTPORT, S_ISWHT might not be available on all systems else: return \\"Unknown\\" def traverse_and_report(directory): Recursively descends the directory tree starting from \'directory\'. Gathers information about each file and subdirectory using the \'stat\' module. Generates a report in human-readable format. for root, dirs, files in os.walk(directory): for name in files + dirs: path = os.path.join(root, name) try: stats = os.lstat(path) except FileNotFoundError: continue file_type = get_file_type(stats.st_mode) file_mode = stat.filemode(stats.st_mode) size = stats.st_size atime = format_time(stats.st_atime) mtime = format_time(stats.st_mtime) ctime = format_time(stats.st_ctime) print(f\\"{path}:\\") print(f\\"tType: {file_type}\\") print(f\\"tFile mode: {file_mode}\\") print(f\\"tSize: {size} bytes\\") print(f\\"tLast access time: {atime}\\") print(f\\"tLast modification time: {mtime}\\") print(f\\"tCreation time: {ctime}\\") print() # Example usage: # traverse_and_report(\\"/path/to/directory\\")"},{"question":"# Coding Assessment: Complex Path Manipulations Objective Write a function in Python that performs several pathname manipulations to fulfill the requirements described below. You will need to utilize multiple functions from the `os.path` module to achieve the desired results. Function Signature ```python def normalize_paths(paths, base_path): Takes a list of paths and a base path, then performs the following manipulations: 1. Normalize each path. 2. Join each normalized path with the base path. 3. Determine if the resulting path is absolute. 4. Get the base name and directory name of the resulting path. 5. Calculate the relative path from the base path to the resulting path. 6. If the resulting path refers to an existing file or directory, return True, otherwise False. Return a list of dictionaries where each dictionary contains the results of the manipulations for the corresponding path. :param paths: List of string paths. :param base_path: String base path. :return: List of dictionaries with results of path manipulations. pass ``` Input and Output **Input:** - `paths`: A list of strings, each representing a path. - `base_path`: A string representing the base path. **Output:** - A list of dictionaries. Each dictionary should contain the following keys: - `normalized_path`: The normalized path. - `joined_path`: The path obtained by joining the normalized path with the base path. - `is_absolute`: Boolean indicating if the joined path is absolute. - `basename`: Base name of the joined path. - `dirname`: Directory name of the joined path. - `relative_path`: Relative path from the base path to the joined path. - `exists`: Boolean indicating if the joined path refers to an existing file or directory. Example ```python paths = [\\"./foo/..//bar\\", \\"/baz/..//qux/\\", \\"example.txt\\"] base_path = \\"/home/user\\" expected_output = [ { \\"normalized_path\\": \\"bar\\", \\"joined_path\\": \\"/home/user/bar\\", \\"is_absolute\\": True, \\"basename\\": \\"bar\\", \\"dirname\\": \\"/home/user\\", \\"relative_path\\": \\"bar\\", \\"exists\\": False # Assuming no existing file }, { \\"normalized_path\\": \\"/qux\\", \\"joined_path\\": \\"/qux\\", \\"is_absolute\\": True, \\"basename\\": \\"qux\\", \\"dirname\\": \\"/\\", \\"relative_path\\": \\"../../qux\\", \\"exists\\": False # Assuming no existing file }, { \\"normalized_path\\": \\"example.txt\\", \\"joined_path\\": \\"/home/user/example.txt\\", \\"is_absolute\\": True, \\"basename\\": \\"example.txt\\", \\"dirname\\": \\"/home/user\\", \\"relative_path\\": \\"example.txt\\", \\"exists\\": False # Assuming no existing file } ] ``` Constraints: - All paths are in POSIX style. - The function should handle edge cases like empty paths and paths with redundant separators correctly. Notes: - You can use the `os.path` module functions such as `normpath`, `join`, `isabs`, `basename`, `dirname`, `relpath`, and `exists` to implement the required manipulations. - Consider the performance implications of calling file system operations like `exists`.","solution":"import os def normalize_paths(paths, base_path): results = [] for path in paths: normalized_path = os.path.normpath(path) joined_path = os.path.join(base_path, normalized_path) if not os.path.isabs(normalized_path) else normalized_path is_absolute = os.path.isabs(joined_path) basename = os.path.basename(joined_path) dirname = os.path.dirname(joined_path) relative_path = os.path.relpath(joined_path, base_path) exists = os.path.exists(joined_path) results.append({ \\"normalized_path\\": normalized_path, \\"joined_path\\": joined_path, \\"is_absolute\\": is_absolute, \\"basename\\": basename, \\"dirname\\": dirname, \\"relative_path\\": relative_path, \\"exists\\": exists }) return results"},{"question":"**Question:** You are given a Python function that executes some operations. Your task is to profile this function, analyze the profiling data, and provide a report that highlights key performance metrics. # Function to Profile ```python def example_function(): import time for i in range(5): time.sleep(0.1) for i in range(3): for j in range(2): time.sleep(0.05) ``` # Instructions 1. **Profile the Given Function**: - Use the `cProfile` module to profile the `example_function`. - Save the profiling data to a file named `example_profile.prof`. 2. **Analyze the Profiling Data**: - Load the profiling data using the `pstats.Stats` class. - Strip directories from the file paths in the report. - Sort the data first by cumulative time (`cumtime`) and then by total time (`tottime`). - Print out the top 5 entries in the profiling report. 3. **Report Insights**: - Write a function `print_profile_insights(profile_file: str)` that profiles `example_function`, analyzes the profiling data, and prints the insights as per the instructions. # Constraints - Use only the standard libraries `cProfile` and `pstats`. - The function `example_function` must remain unmodified. - Ensure that your solution is efficient and clearly commented. # Expected Input and Output - Your function `print_profile_insights(profile_file: str)` should take the path to the profiling data file as input and print the required insights to standard output. - Example usage: ```python def example_function(): import time for i in range(5): time.sleep(0.1) for i in range(3): for j in range(2): time.sleep(0.05) # Ensure your example_function is defined before this line print_profile_insights(\'example_profile.prof\') ``` This will output a formatted profile analysis focusing on the top time-consuming functions.","solution":"import cProfile import pstats def profile_example_function(): # Profile the example_function and save the profiling data to a file with cProfile.Profile() as pr: example_function() pr.dump_stats(\'example_profile.prof\') def print_profile_insights(profile_file: str): # Load the profiling data stats = pstats.Stats(profile_file) # Strip directories from the file paths in the report stats.strip_dirs() # Sort the data first by cumulative time and then by total time stats.sort_stats(\'cumtime\', \'tottime\') # Print out the top 5 entries in the profiling report stats.print_stats(5) def example_function(): import time for i in range(5): time.sleep(0.1) for i in range(3): for j in range(2): time.sleep(0.05)"},{"question":"Implement a class `MemoryBuffer` in Python that simulates the behavior of the buffer protocol\'s read-only buffer exposure. # Requirements: 1. The class should initialize with a given byte sequence. 2. Implement methods to: - Expose the underlying memory in a read-only manner. - Allow slice operations on the internal buffer that return new `MemoryBuffer` instances without copying the underlying data. # Class and Method Specifications: 1. `MemoryBuffer.__init__(self, data: bytes)`: Initializes the object, storing the given byte sequence. 2. `MemoryBuffer.expose(self) -> memoryview`: Returns a read-only memory view of the buffer. 3. `MemoryBuffer.slice(self, start: int, end: int) -> MemoryBuffer`: Returns a new `MemoryBuffer` instance that contains a slice of the original buffer without copying the data. If `start` or `end` are out of bounds, raise a `ValueError`. # Constraints: 1. The `data` input to the `MemoryBuffer` class\'s constructor will always be a non-empty bytes object. 2. The `start` and `end` for the `slice` method will always be valid integers within the range of the data length. 3. Performance is important: avoid copying data whenever possible. # Example Usage: ```python buffer = MemoryBuffer(b\\"Hello, World!\\") view = buffer.expose() # Returns a memoryview object print(view.tobytes()) # Output: b\\"Hello, World!\\" sliced_buffer = buffer.slice(0, 5) # Another MemoryBuffer object print(sliced_buffer.expose().tobytes()) # Output: b\\"Hello\\" ``` # Notes: - Ensure that the `MemoryBuffer` exposes the data in a read-only fashion. - The slicing method should not copy the data but should reference the original buffer\'s memory. # Hint: - Consider utilizing the `memoryview` class, which provides a way to access the buffer\'s memory without copying.","solution":"class MemoryBuffer: def __init__(self, data: bytes): self.data = data def expose(self) -> memoryview: Returns a read-only memory view of the buffer. return memoryview(self.data).toreadonly() def slice(self, start: int, end: int) -> \'MemoryBuffer\': Returns a new MemoryBuffer instance that contains a slice of the original buffer without copying the data. if start < 0 or end > len(self.data) or start >= end: raise ValueError(\\"Invalid start or end range.\\") return MemoryBuffer(self.data[start:end])"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of parallel task execution using the `concurrent.futures` module in Python. # Problem Statement You are provided with a list of URLs. Your task is to write a Python function that downloads the contents of these URLs in parallel and returns a dictionary where the keys are the URLs and the values are the lengths of the content retrieved from each URL. # Function Signature ```python def fetch_url_contents(url_list: list) -> dict: pass ``` # Input - `url_list` (list): A list of URLs (strings) to fetch. The list contains between 1 and 100 URLs. # Output - A dictionary where: - Keys are URLs (strings) from the input list, and - Values are integers representing the length of the content fetched from each URL. # Constraints - You must use the `concurrent.futures` module for parallel execution. - Your function should handle any exceptions that may occur during fetching (e.g., network errors) by storing `None` as the value for the URLs that fail to be fetched. - Use a maximum of 10 worker threads to fetch the URLs in parallel. # Example ```python # Example list of URLs url_list = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] result = fetch_url_contents(url_list) print(result) # Possible output: {\'http://example.com\': 1270, \'http://example.org\': 1345, \'http://example.net\': None} ``` # Performance Requirements - The function must complete execution in a reasonable time frame, even for the upper constraint of 100 URLs, assuming network conditions are typical and consistent. # Additional Information - You may use any suitable HTTP library for fetching the URL contents (e.g., `requests`). - Ensure your implementation includes proper error handling to manage failed URL fetches gracefully. # Hints - You may find it useful to manage futures and their results using the `concurrent.futures.as_completed` iterator. - Consider encapsulating the fetching logic in a helper function that can be used with the executor. Good luck, and happy coding!","solution":"import concurrent.futures import requests def fetch_content(url): try: response = requests.get(url) response.raise_for_status() return url, len(response.content) except requests.RequestException: return url, None def fetch_url_contents(url_list: list) -> dict: result = {} with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: futures = {executor.submit(fetch_content, url): url for url in url_list} for future in concurrent.futures.as_completed(futures): url, length = future.result() result[url] = length return result"},{"question":"The following question assesses your ability to work with chunked data in files using the `chunk` module in Python. Problem Statement You are given an audio file that follows the EA IFF 85 chunk format. Your task is to write a function `extract_chunk_data` that reads this file and extracts data from a specified chunk. Specifically, you need to: 1. Identify the chunk with a given ID. 2. Extract its data. 3. Return the extracted data as a bytes object. Function Signature ```python def extract_chunk_data(file_path: str, chunk_id: str) -> bytes: pass ``` Input - `file_path` (str): The path to the chunked data file. - `chunk_id` (str): The 4-byte string ID of the chunk to extract. Output - Returns a `bytes` object containing the data of the specified chunk. Constraints - You may assume that the file and the specified chunk ID exist. - If the specified chunk is not found, return an empty `bytes` object. - The function should handle files that may contain multiple chunks and skip over irrelevant chunks efficiently. - The chunk size is stored in big-endian format by default. Example Suppose you have a file `test.iff` with the following chunks: ``` Chunk ID: ABCD, Size: 10 bytes, Data: b\'1234567890\' Chunk ID: WXYZ, Size: 8 bytes, Data: b\'abcdefgh\' ``` To extract data from the chunk with ID \\"WXYZ\\": ```python extract_chunk_data(\\"test.iff\\", \\"WXYZ\\") ``` The function should return: ``` b\'abcdefgh\' ``` Notes - You should make use of the `chunk.Chunk` class provided in the documentation. - Ensure your implementation efficiently reads and navigates the file to find and extract the specified chunk. Good luck!","solution":"import chunk def extract_chunk_data(file_path: str, chunk_id: str) -> bytes: Extracts and returns the data corresponding to the given chunk_id from a chunked data file. Parameters: - file_path (str): Path to the chunked data file. - chunk_id (str): 4-byte string ID of the chunk to extract. Returns: - bytes: Data of the specified chunk. with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f, bigendian=True) if ch.getname() == chunk_id.encode(): return ch.read() # Skip the irrelevant chunk ch.skip() except EOFError: # End of file reached before finding the chunk break return b\'\'"},{"question":"# Objective: To evaluate your proficiency in using scikit-learn\'s `MLPClassifier` and `MLPRegressor`, along with understanding model training, evaluation, and hyperparameter tuning. # Problem Statement: You are provided with a dataset consisting of features (`X_train`, `X_test`) and target labels (`y_train_class`, `y_test_class`) for classification, and target values (`y_train_reg`, `y_test_reg`) for regression. Implement and evaluate the following models using scikit-learn\'s `MLPClassifier` and `MLPRegressor`: 1. A `MLPClassifier` that predicts the class labels for the given data. 2. A `MLPRegressor` that predicts continuous values for the given data. --- Requirements: 1. **Data Preprocessing**: - Standardize the features using `StandardScaler`. 2. **Model Implementation**: - Implement `MLPClassifier` with at least two hidden layers. - Set the following hyperparameters: - `hidden_layer_sizes=(50, 20)` - `solver=\'adam\'` - `alpha=1e-4` - `max_iter=200` - `random_state=42` - Implement `MLPRegressor` with at least one hidden layer. - Set the following hyperparameters: - `hidden_layer_sizes=(50,)` - `solver=\'lbfgs\'` - `alpha=1e-4` - `max_iter=200` - `random_state=42` 3. **Model Training and Evaluation**: - Train the `MLPClassifier` using the training data and evaluate it using the test data. Report the accuracy score. - Train the `MLPRegressor` using the training data and evaluate it using the test data. Report the mean square error. --- # Input Format: - `X_train`, `X_test`: Feature sets for training and testing (List of lists of floats). - `y_train_class`, `y_test_class`: Target labels for classification (List of integers). - `y_train_reg`, `y_test_reg`: Target values for regression (List of floats). # Output Format: - Print the accuracy score for `MLPClassifier`. - Print the mean square error for `MLPRegressor`. --- # Constraints: 1. Ensure that all data preprocessing steps (e.g., scaling) are applied uniformly to both training and testing data. 2. Use the specified hyperparameters. 3. The `random_state` should be set to ensure reproducibility. # Example: ```python # Given data X_train = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]] X_test = [[4., 4.], [5., 5.]] y_train_class = [0, 1, 1, 0] y_test_class = [1, 0] y_train_reg = [1.0, 2.0, 3.0, 4.0] y_test_reg = [5.0, 6.0] # Your implementation and output should follow ``` --- Implementation: ```python from sklearn.neural_network import MLPClassifier, MLPRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, mean_squared_error # Data X_train = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]] X_test = [[4., 4.], [5., 5.]] y_train_class = [0, 1, 1, 0] y_test_class = [1, 0] y_train_reg = [1.0, 2.0, 3.0, 4.0] y_test_reg = [5.0, 6.0] # Standardize the data scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) # MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(50, 20), solver=\'adam\', alpha=1e-4, max_iter=200, random_state=42) clf.fit(X_train, y_train_class) y_pred_class = clf.predict(X_test) print(\\"MLPClassifier Accuracy:\\", accuracy_score(y_test_class, y_pred_class)) # MLPRegressor reg = MLPRegressor(hidden_layer_sizes=(50,), solver=\'lbfgs\', alpha=1e-4, max_iter=200, random_state=42) reg.fit(X_train, y_train_reg) y_pred_reg = reg.predict(X_test) print(\\"MLPRegressor Mean Squared Error:\\", mean_squared_error(y_test_reg, y_pred_reg)) ```","solution":"from sklearn.neural_network import MLPClassifier, MLPRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, mean_squared_error def train_evaluate_mlp(X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg): # Standardize the data scaler = StandardScaler() scaler.fit(X_train) X_train_standardized = scaler.transform(X_train) X_test_standardized = scaler.transform(X_test) # Train MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(50, 20), solver=\'adam\', alpha=1e-4, max_iter=200, random_state=42) clf.fit(X_train_standardized, y_train_class) y_pred_class = clf.predict(X_test_standardized) clf_accuracy = accuracy_score(y_test_class, y_pred_class) # Train MLPRegressor reg = MLPRegressor(hidden_layer_sizes=(50,), solver=\'lbfgs\', alpha=1e-4, max_iter=200, random_state=42) reg.fit(X_train_standardized, y_train_reg) y_pred_reg = reg.predict(X_test_standardized) reg_mse = mean_squared_error(y_test_reg, y_pred_reg) return clf_accuracy, reg_mse"},{"question":"Rolling and Expanding Window Operations Problem Statement You are given a time series dataset representing daily stock prices for a period of time. As a data analyst, you need to analyze this data to extract meaningful insights such as moving averages, volatility, and trends using pandas\' various windowing functions. Task Write a function `analyze_stock_data` that takes a pandas DataFrame `df` with columns `Date` and `Close`. The `Date` column represents the date of observation and the `Close` column represents the closing price of the stock on that date. Your function should perform the following operations: 1. **Compute the 7-day rolling mean and standard deviation** of the closing prices and add them as new columns `7_day_rolling_mean` and `7_day_rolling_std`. 2. **Compute the expanding mean and standard deviation** of the closing prices and add them as new columns `expanding_mean` and `expanding_std`. 3. **Compute the exponentially weighted 15-day mean and standard deviation** of the closing prices using a span of 15 days and add them as new columns `ewm_15_mean` and `ewm_15_std`. Return the modified DataFrame with the new columns added. Input Format - `df`: A pandas DataFrame with the following columns: - `Date`: `datetime64[ns]` - `Close`: `float64` Output Format - A pandas DataFrame with the original columns and the following additional columns: - `7_day_rolling_mean`: `float64` - `7_day_rolling_std`: `float64` - `expanding_mean`: `float64` - `expanding_std`: `float64` - `ewm_15_mean`: `float64` - `ewm_15_std`: `float64` Constraints - Assume the DataFrame `df` is sorted by the `Date` column. - The input DataFrame will have at least 15 rows. Example ```python import pandas as pd # Sample data data = { \'Date\': pd.date_range(start=\'1/1/2020\', periods=20, freq=\'D\'), \'Close\': [100, 102, 101, 103, 104, 105, 108, 110, 107, 111, 115, 117, 120, 118, 122, 121, 123, 125, 126, 128] } df = pd.DataFrame(data) # Function call result = analyze_stock_data(df) print(result) ``` The `result` DataFrame should include the original columns and the new calculated columns as specified. Implementation Notes - Use the `rolling`, `expanding`, and `ewm` methods from pandas to compute the respective statistics. - Ensure that your function handles edge cases where there might be insufficient data points to compute the statistics. Solution Template ```python import pandas as pd def analyze_stock_data(df): # Your code here return df ```","solution":"import pandas as pd def analyze_stock_data(df): Analyzes stock data to compute various rolling and expanding statistics. Args: df (pd.DataFrame): DataFrame with \'Date\' and \'Close\' columns. Returns: pd.DataFrame: DataFrame with additional columns for statistical measures. # Compute the 7-day rolling mean and standard deviation df[\'7_day_rolling_mean\'] = df[\'Close\'].rolling(window=7).mean() df[\'7_day_rolling_std\'] = df[\'Close\'].rolling(window=7).std() # Compute the expanding mean and standard deviation df[\'expanding_mean\'] = df[\'Close\'].expanding().mean() df[\'expanding_std\'] = df[\'Close\'].expanding().std() # Compute the exponentially weighted 15-day mean and standard deviation df[\'ewm_15_mean\'] = df[\'Close\'].ewm(span=15).mean() df[\'ewm_15_std\'] = df[\'Close\'].ewm(span=15).std() return df"},{"question":"**Coding Assessment Question** # Objective: Implement a custom subclass of the `reprlib.Repr` class to handle a specific object type with customized size limits for its representation. # Background: The `reprlib` module provides a `Repr` class that helps in producing object representations with constraints on their size. This is useful when dealing with large data structures to avoid generating excessively long strings in their representations. # Task: - Create a subclass of the `reprlib.Repr` class named `CustomRepr`. - Implement a method `repr_MyDataStructure` in `CustomRepr` to handle a custom object type `MyDataStructure`. - Ensure the representation of `MyDataStructure` is size-constrained. - The custom class should limit the number of elements represented for the `MyDataStructure` object to a specified maximum (`maxelements`). # Input/Output Formats: - Implement a class `MyDataStructure` that has a list attribute `data`. - Provide a method `__repr__()` in `MyDataStructure` that uses the `CustomRepr` instance to return the string representation. - Instances of `MyDataStructure` should follow the constraint on the number of elements shown in the `repr`. # Constraints: - The `maxelements` limit should be set to 5. # Example: ```python import reprlib class MyDataStructure: def __init__(self, data): self.data = data def __repr__(self): custom_repr = CustomRepr() return custom_repr.repr(self) class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxelements = 5 def repr_MyDataStructure(self, obj, level): if len(obj.data) > self.maxelements: return f\'MyDataStructure([{\\", \\".join(map(repr, obj.data[:self.maxelements]))}, ...])\' return f\'MyDataStructure({repr(obj.data)})\' # Example usage: data = MyDataStructure([1, 2, 3, 4, 5, 6, 7, 8]) print(data) # Output: MyDataStructure([1, 2, 3, 4, 5, ...]) ``` # Requirements: - Implement the `MyDataStructure` class with the necessary attributes and `__repr__()` method. - Implement the `CustomRepr` subclass with the specified size limits on `MyDataStructure`. - Ensure the output follows the described constraints in the example. **Note**: This question tests the understanding of subclassing, custom representation methods, and size constraints in Python.","solution":"import reprlib class MyDataStructure: def __init__(self, data): self.data = data def __repr__(self): custom_repr = CustomRepr() return custom_repr.repr(self) class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxelements = 5 def repr_MyDataStructure(self, obj, level): if len(obj.data) > self.maxelements: return f\'MyDataStructure([{\\", \\".join(map(repr, obj.data[:self.maxelements]))}, ...])\' return f\'MyDataStructure({repr(obj.data)})\'"},{"question":"Objective You are to implement a function `analyze_sound_files(file_list: List[str]) -> Tuple[List[Dict[str, Any]], List[str]]` that analyzes a list of sound file names and determines their type and attributes using the `sndhdr` module. Your function should return a tuple containing two elements: 1. A list of dictionaries. Each dictionary corresponds to a successfully analyzed sound file and should include the file name, file type, frame rate, number of channels, number of frames, and sample width. 2. A list of file names that could not be analyzed successfully. Input - `file_list` (List[str]): A list of file names (strings) representing sound files to be analyzed. Output - A tuple containing two elements: 1. A list of dictionaries. Each dictionary should have the following keys: - `\'filename\'`: The original file name (string). - `\'filetype\'`: The type of sound file (string). - `\'framerate\'`: The frame rate of the sound file (integer). - `\'nchannels\'`: The number of channels in the sound file (integer). - `\'nframes\'`: The number of frames in the sound file (integer). - `\'sampwidth\'`: The sample width of the sound file (integer or string). 2. A list of file names (strings) that could not be analyzed. Constraints - Use the `sndhdr` module\'s functions `what` and `whathdr` to determine sound file attributes. - Handle cases where sound files cannot be analyzed by including their names in the appropriate output list. Example Input: ```python file_list = [\\"sound1.wav\\", \\"sound2.aiff\\", \\"corrupted_file.x\\", \\"unknown_file.unknown\\"] ``` Output: ```python ([ {\'filename\': \'sound1.wav\', \'filetype\': \'wav\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 10000, \'sampwidth\': 16}, {\'filename\': \'sound2.aiff\', \'filetype\': \'aiff\', \'framerate\': 48000, \'nchannels\': 2, \'nframes\': 20000, \'sampwidth\': 16} ], [\\"corrupted_file.x\\", \\"unknown_file.unknown\\"]) ``` Notes - Make sure to capture all relevant details for analyzing sound files from the `sndhdr` module\'s namedtuple and convert them into the specified dictionary format. - Ensure your function is robust and can handle unexpected file types gracefully.","solution":"import sndhdr from typing import List, Dict, Any, Tuple def analyze_sound_files(file_list: List[str]) -> Tuple[List[Dict[str, Any]], List[str]]: analyzed_files = [] failed_files = [] for file in file_list: sound_info = sndhdr.what(file) if sound_info is not None: # sndhdr.what returns a namedtuple with the following fields: # type, framerate, nchannels, (other fields specific to the type) file_type = sound_info.filetype framerate = sound_info.framerate nchannels = sound_info.nchannels nframes = sound_info.nframes sampwidth = sound_info.sampwidth file_dict = { \'filename\': file, \'filetype\': file_type, \'framerate\': framerate, \'nchannels\': nchannels, \'nframes\': nframes, \'sampwidth\': sampwidth, } analyzed_files.append(file_dict) else: failed_files.append(file) return (analyzed_files, failed_files)"},{"question":"# WSGI Application: Dynamic Greeting Server Task Create a WSGI application that serves HTTP requests to dynamically generate greeting messages. The application should: 1. Parse the request URI to extract a name parameter. 2. If the name is provided, return an HTTP response with a greeting message: \\"Hello, {name}!\\". 3. If the name is not provided, return an HTTP response with a default message: \\"Hello, World!\\". 4. Use the `wsgiref.util` and `wsgiref.headers` modules as appropriate to manipulate the WSGI environment and response headers. You need to implement the WSGI application and run it using the `wsgiref.simple_server`. Input - The WSGI application should handle HTTP GET requests. - The request URI may contain a query string with a `name` parameter, e.g., `http://localhost:8000/?name=Alice`. Output - If the `name` parameter is provided, the response should be \\"Hello, {name}!\\". - If the `name` parameter is not provided, the response should be \\"Hello, World!\\". Constraints - Use only standard Python libraries and the `wsgiref` module. - Ensure the application complies with the WSGI specification as defined in **PEP 3333**. Example 1. Request: `GET http://localhost:8000/?name=Alice` - Response: `Hello, Alice!` 2. Request: `GET http://localhost:8000/` - Response: `Hello, World!` Implementation Details 1. Define a WSGI application callable `greeting_app(environ, start_response)`. 2. Parse the `QUERY_STRING` from the `environ` dictionary to extract the `name` parameter. 3. Construct the response message based on the presence of the `name` parameter. 4. Use `start_response` to send the HTTP status and response headers. 5. Make use of the `wsgiref.util` module as needed for environment variable manipulation. 6. Start the WSGI server using `wsgiref.simple_server` to handle incoming requests. ```python from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults from urllib.parse import parse_qs def greeting_app(environ, start_response): setup_testing_defaults(environ) query = parse_qs(environ.get(\'QUERY_STRING\', \'\')) name = query.get(\'name\', [\'World\'])[0] status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) response_body = f\\"Hello, {name}!\\" return [response_body.encode(\'utf-8\')] if __name__ == \'__main__\': httpd = make_server(\'\', 8000, greeting_app) print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Notes - Ensure you handle edge cases such as empty query strings or multiple parameters gracefully. - Verify compliance with the WSGI specification using `wsgiref.validate.validator`.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults from urllib.parse import parse_qs def greeting_app(environ, start_response): setup_testing_defaults(environ) # Query string parsing query = parse_qs(environ.get(\'QUERY_STRING\', \'\')) name = query.get(\'name\', [\'World\'])[0] # Setting response status and headers status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) # Response body response_body = f\\"Hello, {name}!\\" return [response_body.encode(\'utf-8\')] if __name__ == \'__main__\': httpd = make_server(\'\', 8000, greeting_app) print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"**Objective:** Create a detailed and informative plot using Seaborn with the `Dash` mark and other relevant marks and modifiers to visualize the relationship between specific variables from the `penguins` dataset. **Question:** You are provided with the `penguins` dataset from Seaborn. Your task is to create a plot that effectively visualizes the following aspects of the dataset: 1. **Species** (`species`) vs **Body Mass** (`body_mass_g`) categorized by **Sex** (`sex`). 2. **Flipper Length** (`flipper_length_mm`) vs **Body Mass** (`body_mass_g`), specifically for the species \\"Adelie\\". 3. Use appropriate marks and modifiers to enhance the visualization and provide meaningful insights. **Requirements:** 1. Load the `penguins` dataset. 2. Create a plot with the following specifications: - **Panel 1:** - X-axis: `species` - Y-axis: `body_mass_g` - Color points based on `sex` - Use `Dash` mark - **Panel 2:** - X-axis: `body_mass_g` - Y-axis: `flipper_length_mm`, rounded to the nearest ten - Filter data to only include species `\\"Adelie\\"` - Use `Dash` mark with orientation aligned to Y-axis 3. Enhance the plot by setting properties such as transparency (`alpha`), line width, and width of the `Dash` mark. 4. Combine multiple marks and use alignment modifiers where appropriate (e.g., `Dodge`, `Jitter`). **Expected Input:** - The function should take no inputs as it will operate on the `penguins` dataset loaded from Seaborn. **Expected Output:** - Display a plot (`sns.FacetGrid` or similar) with two panels as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_penguins_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Creating the first panel plot p1 = sns.catplot( data=penguins, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", kind=\\"strip\\", dodge=True, marker=\\"|\\", alpha=0.7, linewidth=1.5, height=5, aspect=1.5 ) p1.set_axis_labels(\\"Species\\", \\"Body Mass (g)\\") p1.add_legend(title=\\"Sex\\") # Filter data for Adelie species adelie_penguins = penguins[penguins[\'species\'] == \'Adelie\'] # Round flipper_length_mm to the nearest ten adelie_penguins[\'flipper_length_mm\'] = adelie_penguins[\'flipper_length_mm\'].apply(lambda x: round(x, -1)) # Creating the second panel plot p2 = sns.lmplot( data=adelie_penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", fit_reg=False, scatter_kws={\'alpha\':0.7, \'s\':100}, height=5, aspect=1.5 ) p2.set_axis_labels(\\"Body Mass (g)\\", \\"Flipper Length (mm)\\") # Show the plots plt.show() # Call the function to execute the plot creation plot_penguins_data()"},{"question":"# Question You are tasked with designing a custom neural network using PyTorch. Your neural network will be used to classify a set of 28x28 grayscale images (like the MNIST dataset) into 10 categories. Your implementation should include: - A customized Convolutional Neural Network (CNN) using various layers and functions from the `torch.nn.functional` module. - Implementation of forward propagation. - A function to compute the loss using `cross_entropy`. Specifications 1. **Model Architecture**: - The network should have the following layers: - A convolutional layer with 1 input channel, 32 output channels, and a kernel size of 3. - A ReLU activation function. - A max pooling layer with a kernel size of 2. - A convolutional layer with 32 input channels, 64 output channels, and a kernel size of 3. - A ReLU activation function. - A max pooling layer with a kernel size of 2. - A fully connected (linear) layer that outputs 128 features. - A ReLU activation function. - A dropout layer with a dropout probability of 0.5. - A final fully connected (linear) layer that outputs 10 scores (one for each class). 2. **Forward Function**: - Implement the `forward()` function which defines how the input data passes through the network. 3. **Loss Function**: - Implement a function `compute_loss()` that takes the network\'s output and the target labels and returns the cross-entropy loss. Inputs and Outputs - **Input**: A batch of images of shape `(batch_size, 1, 28, 28)` where `batch_size` is the number of images in the batch. - **Output**: The final layer of the network should output scores of shape `(batch_size, 10)`. Constraints - Use PyTorch\'s `torch.nn.functional` module wherever possible. - Ensure that the network architecture strictly follows the specification. - Your implementation should be efficient and make optimal use of PyTorch\'s capabilities. Example ```python import torch import torch.nn.functional as F class SimpleCNN(torch.nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = torch.nn.Conv2d(1, 32, 3) self.conv2 = torch.nn.Conv2d(32, 64, 3) self.fc1 = torch.nn.Linear(64 * 5 * 5, 128) # 64 channels * 5 height * 5 width after pooling self.fc2 = torch.nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.conv2(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = x.view(-1, 64 * 5 * 5) # Flatten the tensor x = self.fc1(x) x = F.relu(x) x = F.dropout(x, p=0.5, training=self.training) x = self.fc2(x) return x def compute_loss(output, target): return F.cross_entropy(output, target) # Example usage: # model = SimpleCNN() # images = torch.randn(32, 1, 28, 28) # Batch of 32 images # labels = torch.randint(0, 10, (32,)) # Batch of 32 labels # output = model(images) # loss = compute_loss(output, labels) ```","solution":"import torch import torch.nn.functional as F class SimpleCNN(torch.nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = torch.nn.Conv2d(1, 32, 3) self.conv2 = torch.nn.Conv2d(32, 64, 3) # Compute the size of the feature map after two convolutions and two poolings self.fc1 = torch.nn.Linear(64 * 5 * 5, 128) # 64 channels * 5 height * 5 width after pooling self.fc2 = torch.nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.conv2(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = x.view(-1, 64 * 5 * 5) # Flatten the tensor x = self.fc1(x) x = F.relu(x) x = F.dropout(x, p=0.5, training=self.training) x = self.fc2(x) return x def compute_loss(output, target): return F.cross_entropy(output, target) # Example usage: # model = SimpleCNN() # images = torch.randn(32, 1, 28, 28) # Batch of 32 images # labels = torch.randint(0, 10, (32,)) # Batch of 32 labels # output = model(images) # loss = compute_loss(output, labels)"},{"question":"# LZMA Compression with Custom Filters and Integrity Check **Objective**: Use the `lzma` module to compress and decompress data with custom filters and integrity checks. **Task**: Implement a Python function `compress_decompress_lzma` that accepts a string, compresses it using the LZMA algorithm with a custom filter chain and a specific integrity check, then decompresses it back to the original string, and returns the decompressed string. # Function Signature ```python def compress_decompress_lzma(input_string: str) -> str: pass ``` # Inputs and Outputs **Input**: - `input_string` (str): A string to be compressed and then decompressed. **Output**: - (str): The decompressed string which should be identical to the input string. # Constraints: - The function should use the following custom filter chain for compression: ```python custom_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] ``` - The integrity check used for compression should be `lzma.CHECK_CRC64`. # Examples: 1. Example: ```python input_string = \\"Hello, LZMA Compression!\\" result = compress_decompress_lzma(input_string) assert result == input_string ``` # Hints: - Use the `lzma.compress` and `lzma.decompress` methods for compression and decompression. - Ensure to use the custom filter chain and integrity check as specified. - Handle any potential exceptions that may be raised during compression or decompression. # Implementation: ```python def compress_decompress_lzma(input_string: str) -> str: import lzma try: custom_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] # Compress the data compressed_data = lzma.compress( input_string.encode(\'utf-8\'), format=lzma.FORMAT_XZ, check=lzma.CHECK_CRC64, filters=custom_filters ) # Decompress the data decompressed_data = lzma.decompress(compressed_data) return decompressed_data.decode(\'utf-8\') except lzma.LZMAError as e: raise RuntimeError(\\"Compression/Decompression failed.\\") from e ``` **Note**: The provided implementation should work correctly upon understanding and applying the documented features of the `lzma` module, including handling custom filters and integrity checks. Ensure the test cases verify the correctness of your function under different scenarios.","solution":"def compress_decompress_lzma(input_string: str) -> str: import lzma try: custom_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] # Compress the data compressed_data = lzma.compress( input_string.encode(\'utf-8\'), format=lzma.FORMAT_XZ, check=lzma.CHECK_CRC64, filters=custom_filters ) # Decompress the data decompressed_data = lzma.decompress(compressed_data) return decompressed_data.decode(\'utf-8\') except lzma.LZMAError as e: raise RuntimeError(\\"Compression/Decompression failed.\\") from e"},{"question":"**Objective:** Implement a function that compresses data using the LZMA compression algorithm with a custom filter chain, writes it to a file, reads the compressed data back from the file, and finally decompresses it to its original form. **Requirements:** 1. Use the `lzma` module for all compression and decompression operations. 2. Utilize a custom filter chain during the compression process. 3. Write the compressed data to a file. 4. Read the compressed data from the file and decompress it. 5. Verify that the decompressed data matches the original data. **Specifications:** - Function Signature: ```python def compress_and_decompress(data: bytes, filename: str) -> bool: ``` - **Input:** - `data`: A `bytes` object containing the data to be compressed. - `filename`: A `str` representing the name of the file to write the compressed data to. - **Output:** - Returns a `bool` indicating whether the decompressed data matches the original data. - **Constraints:** - Use a custom filter chain with at least one delta filter and one compression filter. - Handle any exceptions that may occur during the compression or decompression processes and ensure the function returns `False` if an exception is raised. **Example Usage:** ```python data = b\\"This is a sample data to test the LZMA compression and decompression.\\" filename = \\"compressed_data.xz\\" result = compress_and_decompress(data, filename) print(result) # Should print: True if the decompressed data matches the original, False otherwise. ``` **Notes:** - Ensure that the file is properly closed after writing and reading operations. - Use appropriate integrity checks while compressing, if necessary. **Hint:** Refer to the `lzma` module documentation for details on creating a custom filter chain and using the `lzma.open` function to handle file operations.","solution":"import lzma def compress_and_decompress(data: bytes, filename: str) -> bool: Compresses the given data using LZMA algorithm with a custom filter chain, writes to a file, reads from the file and decompresses it, ensuring the decompressed data matches the original. :param data: The original data to compress, as a bytes object. :param filename: The name of the file to write/read the compressed data. :return: True if the decompressed data matches the original data, False otherwise. try: # Define custom filter chain filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME} ] # Compress data and write to file with lzma.open(filename, \'wb\', format=lzma.FORMAT_XZ, check=lzma.CHECK_CRC64, filters=filters) as f: f.write(data) # Read compressed data from file and decompress it with lzma.open(filename, \'rb\') as f: decompressed_data = f.read() # Verify the decompressed data matches the original return decompressed_data == data except Exception as e: # In case of any exception, return False return False"},{"question":"# Seaborn Coding Assessment Objective You will demonstrate your understanding of the seaborn library, specifically focusing on the `sns.histplot` function. You will create both univariate and bivariate histograms, customize their appearance, and discuss your interpretations of the plots. Problem Statement You are given the `penguins` dataset from seaborn. This dataset contains various characteristics of penguins, such as flipper length, body mass, species, etc. You are required to perform the following tasks: 1. **Load the Dataset**: Load the `penguins` dataset. 2. **Univariate Histograms**: - Plot a histogram of `flipper_length_mm`. - Customize the histogram by setting the number of bins to 20 and add a kernel density estimate. - Plot another histogram of `flipper_length_mm`, but this time, use `hue` to differentiate between species. - Customize the hue histogram by using `log_scale` if the data is skewed. 3. **Bivariate Histograms**: - Plot a bivariate histogram to show the relationship between `bill_depth_mm` and `body_mass_g`. - Optionally, add a `hue` parameter to the bivariate histogram to represent different species if it enhances the clarity of the plot. 4. **Interpretation**: - Write a brief interpretation of the univariate and bivariate histograms you created. What do they tell you about the distribution and relationships of the features? Requirements 1. **Input Format**: - No direct input required. The dataset is preloaded via seaborn\'s `load_dataset` function. 2. **Output Format**: - Display the histograms as output. - Provide a text-based interpretation of each histogram. 3. **Constraints**: - Use seaborn for plotting histograms. - Properly comment your code to explain your steps. Sample Code Structure ```python # Step 1: Load the dataset import seaborn as sns import matplotlib.pyplot as plt penguins = sns.load_dataset(\\"penguins\\") # Step 2: Plot univariate histograms # 2.1: Simple histogram of flipper_length_mm sns.histplot(data=penguins, x=\\"flipper_length_mm\\") plt.show() # 2.2: Customized histogram with kernel density estimate sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=20, kde=True) plt.show() # 2.3: Histogram with hue by species sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.show() # 2.4: Log-scaled histogram (if applicable) # Check if log scale makes sense here, otherwise skip sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", log_scale=True) plt.show() # Step 3: Plot bivariate histograms # 3.1: Simple bivariate histogram for bill_depth_mm and body_mass_g sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\") plt.show() # 3.2: Bivariate histogram with hue by species sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.show() # Step 4: Interpretation # Provide a detailed interpretation of the created histograms ``` Notes - Ensure your plots are well-labeled for clear interpretation. - Mention any anomalies or interesting observations in your interpretations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_penguins_dataset(): Loads the penguins dataset from seaborn. return sns.load_dataset(\\"penguins\\") def plot_univariate_histograms(penguins): Plots various univariate histograms of the flipper_length_mm column. # Simple histogram of flipper_length_mm sns.histplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Histogram of Flipper Length (mm)\\") plt.show() # Customized histogram with kernel density estimate and 20 bins sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=20, kde=True) plt.title(\\"Histogram with Kernel Density of Flipper Length (mm)\\") plt.show() # Histogram with hue by species sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.title(\\"Histogram of Flipper Length (mm) by Species\\") plt.show() def plot_bivariate_histograms(penguins): Plots bivariate histograms showing relationships between bill_depth_mm and body_mass_g. # Simple bivariate histogram for bill_depth_mm and body_mass_g sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\") plt.title(\\"Bivariate Histogram of Bill Depth (mm) and Body Mass (g)\\") plt.show() # Bivariate histogram with hue by species sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\\"Bivariate Histogram of Bill Depth (mm) and Body Mass (g) by Species\\") plt.show() def main(): # Step 1: Load the dataset penguins = load_penguins_dataset() # Step 2: Plot univariate histograms plot_univariate_histograms(penguins) # Step 3: Plot bivariate histograms plot_bivariate_histograms(penguins) # Interpretation print( Interpretation: 1. Univariate Histograms: - The first histogram shows the distribution of flipper length among all penguins. - The second histogram, with KDE and customized bins, provides a smoother visualization of the data. - The third histogram differentiates flipper lengths by species. It is evident that different species of penguins have different flipper lengths. 2. Bivariate Histograms: - The first bivariate histogram shows the relationship between bill depth and body mass. - The second bivariate histogram, with the hue parameter by species, shows how the relationship between bill depth and body mass varies among different species. ) if __name__ == \\"__main__\\": main()"},{"question":"XML Data Processor You are given a simple XML document containing information about several books. Your task is to write a Python script using the `xml.etree.ElementTree` module to perform the following operations: 1. Parse the provided XML document from a string. 2. Modify the document by: - Adding a new book element with specified details. - Updating the price of a book identified by its title. - Removing all books published before a specified year. 3. Write the modified XML document to a file. # Input Format - A string containing the XML document. - Details of the new book to be added: title, author, year, price. - The title of the book whose price needs to be updated and the new price. - The cutoff year; books published before this year should be removed. # Output Format Write the modified XML document to a file named `modified_books.xml`. # Constraints - Assume the XML document is well-formed. - The book details (title, author, year, price) provided for addition and update will be valid. - The cutoff year will be an integer. # Example Input ```python xml_data = <catalog> <book> <title>Book A</title> <author>Author A</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book B</title> <author>Author B</author> <year>2003</year> <price>39.99</price> </book> </catalog> # Details of the new book to be added new_book_details = { \'title\': \'Book C\', \'author\': \'Author C\', \'year\': \'2005\', \'price\': \'49.99\' } # Book title to update and the new price update_title = \'Book B\' new_price = \'34.99\' # Cutoff year to remove books published before this year cutoff_year = 2002 ``` Output The content of `modified_books.xml` file should be: ```xml <catalog> <book> <title>Book A</title> <author>Author A</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book B</title> <author>Author B</author> <year>2003</year> <price>34.99</price> </book> <book> <title>Book C</title> <author>Author C</author> <year>2005</year> <price>49.99</price> </book> </catalog> ``` # Function Signature ```python def process_books(xml_data: str, new_book_details: dict, update_title: str, new_price: str, cutoff_year: int): pass ``` Implementation Notes 1. Use `ET.fromstring()` to parse the XML from the provided string. 2. For adding the new book, use `ET.SubElement()`. 3. To update the price, find the book element by its title and update the nested price element. 4. Remove the books published before the cutoff year while iterating over the elements. 5. Use `ET.ElementTree()` and `tree.write()` to write the modified document to `modified_books.xml`.","solution":"import xml.etree.ElementTree as ET def process_books(xml_data: str, new_book_details: dict, update_title: str, new_price: str, cutoff_year: int): # Parse the XML from string root = ET.fromstring(xml_data) # Add the new book element with specified details new_book = ET.SubElement(root, \'book\') for key, value in new_book_details.items(): element = ET.SubElement(new_book, key) element.text = value # Update the price of the book identified by its title for book in root.findall(\'book\'): title = book.find(\'title\').text if title == update_title: book.find(\'price\').text = new_price # Remove all books published before the specified year for book in list(root.findall(\'book\')): year = int(book.find(\'year\').text) if year < cutoff_year: root.remove(book) # Write the modified XML document to a file tree = ET.ElementTree(root) tree.write(\'modified_books.xml\')"},{"question":"# Custom Command-Line Calculator Shell You are required to build a custom command-line calculator shell using the `cmd` module in Python. The calculator should be capable of performing basic arithmetic operations and keeping track of a running total. Requirements: 1. **Class Definition**: - Create a class `CalculatorShell` that inherits from `cmd.Cmd`. - Set the `prompt` attribute to `\'(calc) \'`. - Set the `intro` attribute to `\'Welcome to the calculator shell. Type help or ? to list commands.n\'`. 2. **Commands**: - Implement the following commands by defining methods in the form `do_<command>(self, arg)`: - `add`: Adds a number to the running total. Usage: `add <number>` - `subtract`: Subtracts a number from the running total. Usage: `subtract <number>` - `multiply`: Multiplies the running total by a number. Usage: `multiply <number>` - `divide`: Divides the running total by a number. Usage: `divide <number>` - `reset`: Resets the running total to zero. Usage: `reset` - `total`: Prints the current total. Usage: `total` - `bye`: Exits the calculator shell. Usage: `bye` 3. **Running Total**: - Maintain a running total as an instance variable called `total`, initialized to zero. 4. **Method Details**: - Each arithmetic command method (e.g., `do_add`, `do_subtract`, etc.) should parse the argument, update the running total accordingly, and handle potential errors (e.g., division by zero). - The `do_total` method should print the current value of the running total. - The `do_bye` method should print a farewell message and return `True` to exit the shell. 5. **Error Handling**: - Ensure that commands validate and parse arguments properly. Incorrect arguments should trigger a helpful error message without stopping the shell. 6. **Shell Features**: - Allow users to type `help` or `?` to list available commands and their brief descriptions. ```python import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the calculator shell. Type help or ? to list commands.n\' prompt = \'(calc) \' def __init__(self): super().__init__() self.total = 0 def do_add(self, arg): \'Add a number to the running total: add <number>\' try: num = float(arg) self.total += num except ValueError: print(\\"Please enter a valid number.\\") def do_subtract(self, arg): \'Subtract a number from the running total: subtract <number>\' try: num = float(arg) self.total -= num except ValueError: print(\\"Please enter a valid number.\\") def do_multiply(self, arg): \'Multiply the running total by a number: multiply <number>\' try: num = float(arg) self.total *= num except ValueError: print(\\"Please enter a valid number.\\") def do_divide(self, arg): \'Divide the running total by a number: divide <number>\' try: num = float(arg) if num != 0: self.total /= num else: print(\\"Cannot divide by zero.\\") except ValueError: print(\\"Please enter a valid number.\\") def do_reset(self, arg): \'Reset the running total to zero: reset\' self.total = 0 def do_total(self, arg): \'Print the current total: total\' print(f\'Total: {self.total}\') def do_bye(self, arg): \'Exit the calculator shell: bye\' print(\'Thank you for using the calculator shell.\') return True def default(self, line): print(f\'Command \\"{line}\\" not recognized. Type \\"help\\" or \\"?\\" to list commands.\') def emptyline(self): pass if __name__ == \'__main__\': CalculatorShell().cmdloop() ``` Task: 1. Implement the `CalculatorShell` class as described. 2. Ensure that the shell works as expected and handles both valid and invalid input gracefully.","solution":"import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the calculator shell. Type help or ? to list commands.n\' prompt = \'(calc) \' def __init__(self): super().__init__() self.total = 0 def do_add(self, arg): \'Add a number to the running total: add <number>\' try: num = float(arg) self.total += num print(f\'Total after addition: {self.total}\') except ValueError: print(\\"Please enter a valid number.\\") def do_subtract(self, arg): \'Subtract a number from the running total: subtract <number>\' try: num = float(arg) self.total -= num print(f\'Total after subtraction: {self.total}\') except ValueError: print(\\"Please enter a valid number.\\") def do_multiply(self, arg): \'Multiply the running total by a number: multiply <number>\' try: num = float(arg) self.total *= num print(f\'Total after multiplication: {self.total}\') except ValueError: print(\\"Please enter a valid number.\\") def do_divide(self, arg): \'Divide the running total by a number: divide <number>\' try: num = float(arg) if num != 0: self.total /= num print(f\'Total after division: {self.total}\') else: print(\\"Cannot divide by zero.\\") except ValueError: print(\\"Please enter a valid number.\\") def do_reset(self, arg): \'Reset the running total to zero: reset\' self.total = 0 print(\\"Total reset to zero.\\") def do_total(self, arg): \'Print the current total: total\' print(f\'Total: {self.total}\') def do_bye(self, arg): \'Exit the calculator shell: bye\' print(\'Thank you for using the calculator shell.\') return True def default(self, line): print(f\'Command \\"{line}\\" not recognized. Type \\"help\\" or \\"?\\" to list commands.\') def emptyline(self): pass if __name__ == \'__main__\': CalculatorShell().cmdloop()"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},F={class:"card-container"},q={key:0,class:"empty-state"},R=["disabled"],O={key:0},N={key:1};function L(i,e,l,m,n,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",q,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),s("span",N,"Loading...")):(a(),s("span",O,"See more"))],8,R)):d("",!0)])}const j=p(z,[["render",L],["__scopeId","data-v-66b2e28a"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/65.md","filePath":"deepseek/65.md"}'),M={name:"deepseek/65.md"},B=Object.assign(M,{setup(i){return(e,l)=>(a(),s("div",null,[x(j)]))}});export{Y as __pageData,B as default};
