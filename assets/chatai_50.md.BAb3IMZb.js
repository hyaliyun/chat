import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(r,e,l,m,n,i){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-055e80ca"]]),A=JSON.parse('[{"question":"Objective To assess your understanding of tensor storage in PyTorch, specifically focusing on `torch.UntypedStorage`. Problem Statement You are given two tensors, `tensor_a` and `tensor_b`. These tensors may or may not share the same underlying storage. Your task is to implement the following two functions: 1. `check_same_storage(tensor_a, tensor_b)`: This function should return `True` if `tensor_a` and `tensor_b` share the same underlying `UntypedStorage`, and `False` otherwise. 2. `fill_with_zeros(tensor, fill_grad=False)`: This function should fill the given `tensor` with zeros by directly modifying its underlying storage. If `fill_grad` is set to `True` and the tensor has `requires_grad=True`, also fill the gradient storage with zeros. Function Signature ```python def check_same_storage(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> bool: pass def fill_with_zeros(tensor: torch.Tensor, fill_grad: bool=False) -> None: pass ``` Input - `tensor_a` and `tensor_b`: Two PyTorch tensors. - `tensor`: A PyTorch tensor. - `fill_grad`: A boolean flag indicating whether to fill the gradient storage with zeros if the tensor has `requires_grad=True`. Output For `check_same_storage`: - Return `True` if the tensors share the same underlying `UntypedStorage`, otherwise return `False`. For `fill_with_zeros`: - The function directly modifies the tensor\'s storage and does not return anything. Constraints 1. You should not rely on specific tensor shapes or dtypes. 2. You must directly modify the underlying storage. 3. Follow best practices and avoid using deprecated storage manipulation methods when possible. Example ```python import torch # Example for check_same_storage tensor_a = torch.tensor([1.0, 2.0, 3.0]) tensor_b = tensor_a.view(3) assert check_same_storage(tensor_a, tensor_b) == True tensor_c = torch.tensor([1.0, 2.0, 3.0]) assert check_same_storage(tensor_a, tensor_c) == False # Example for fill_with_zeros tensor_d = torch.tensor([1.0, 2.0, 3.0]) fill_with_zeros(tensor_d) assert torch.equal(tensor_d, torch.tensor([0.0, 0.0, 0.0])) # Gradient fill example tensor_e = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) tensor_e.sum().backward() # Creating a gradient fill_with_zeros(tensor_e, fill_grad=True) assert torch.equal(tensor_e, torch.tensor([0.0, 0.0, 0.0])) assert torch.equal(tensor_e.grad, torch.tensor([0.0, 0.0, 0.0])) ``` Use this coding question to demonstrate your mastery of tensor storage in PyTorch. Good luck!","solution":"import torch def check_same_storage(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> bool: Check if tensor_a and tensor_b share the same underlying storage. Parameters: - tensor_a (torch.Tensor): First tensor. - tensor_b (torch.Tensor): Second tensor. Returns: - bool: True if the tensors share the same storage, False otherwise. return tensor_a.storage().data_ptr() == tensor_b.storage().data_ptr() def fill_with_zeros(tensor: torch.Tensor, fill_grad: bool=False) -> None: Fill the given tensor with zeros by directly modifying its underlying storage. Optionally fill the gradient storage with zeros if fill_grad is True. Parameters: - tensor (torch.Tensor): Tensor to be filled with zeros. - fill_grad (bool): Whether to also fill the gradient storage with zeros if tensor.requires_grad is True. Returns: - None tensor.storage().fill_(0) if fill_grad and tensor.requires_grad and tensor.grad is not None: tensor.grad.storage().fill_(0)"},{"question":"# Color Space Conversion Challenge Objective: The objective of this problem is to implement a series of functions that utilize the `colorsys` module to convert a given color between multiple color spaces and to calculate the average color in a list of colors, all using different color spaces. Task: You are required to implement the following functions: 1. **`convert_rgb_to_all(r, g, b)`**: This function should take RGB color values as input and return a dictionary containing the equivalent colors in YIQ, HLS, and HSV color spaces. - **Input**: - `r` (float): Red component of the color (0 <= r <= 1) - `g` (float): Green component of the color (0 <= g <= 1) - `b` (float): Blue component of the color (0 <= b <= 1) - **Output**: - Dictionary with keys `\'YIQ\'`, `\'HLS\'`, and `\'HSV\'` and their corresponding values being tuples of the converted color coordinates. Example: ```python { \'YIQ\': (0.385, 0.124, 0.269), \'HLS\': (0.5, 0.3, 0.2), \'HSV\': (0.5, 0.25, 0.4) } ``` 2. **`average_color(colors)`**: This function should take a list of colors, each represented as a dictionary with keys `\'r\'`, `\'g\'`, `\'b\'`, and return the average color in RGB, HLS, and HSV formats. - **Input**: - `colors` (list): A list of dictionaries, each with keys `\'r\'`, `\'g\'`, and `\'b\'` representing the Red, Green, and Blue components of a color. Each value is a float between 0 and 1. Example: ```python [ {\'r\': 0.2, \'g\': 0.4, \'b\': 0.4}, {\'r\': 0.6, \'g\': 0.7, \'b\': 0.8} ] ``` - **Output**: - Dictionary with keys `\'average_rgb\'`, `\'average_hls\'`, and `\'average_hsv\'` containing the average color in each of these color spaces: Example: ```python { \'average_rgb\': (0.4, 0.55, 0.6), \'average_hls\': (0.44, 0.5, 0.25), \'average_hsv\': (0.47, 0.33, 0.6) } ``` Constraints: 1. You may assume the input list for `average_color` will always have at least one color. 2. The functions should use the `colorsys` module for conversions. Evaluation: Your implementation will be evaluated based on: - Correctness of the color space conversions. - Correctness of the average color calculations. - Code readability and adherence to best practices. Example Usage: ```python print(convert_rgb_to_all(0.2, 0.4, 0.4)) # Output: {\'YIQ\': (0.38199999999999995, -0.15800000000000003, -0.07600000000000001), \'HLS\': (0.5, 0.30000000000000004, 0.33333333333333337), \'HSV\': (0.5, 0.5, 0.4)} colors = [ {\'r\': 0.2, \'g\': 0.4, \'b\': 0.4}, {\'r\': 0.6, \'g\': 0.7, \'b\': 0.8} ] print(average_color(colors)) # Output: {\'average_rgb\': (0.4, 0.55, 0.6000000000000001), \'average_hls\': (0.5357142857142857, 0.5, 0.3), \'average_hsv\': (0.5833333333333333, 0.5545454545454546, 0.6636363636363636)} ```","solution":"import colorsys def convert_rgb_to_all(r, g, b): Converts RGB color values to YIQ, HLS, and HSV color spaces. Arguments: r, g, b -- RGB color values (0 <= r,g,b <= 1) Returns: A dictionary containing the YIQ, HLS, and HSV equivalents. yiq = colorsys.rgb_to_yiq(r, g, b) hls = colorsys.rgb_to_hls(r, g, b) hsv = colorsys.rgb_to_hsv(r, g, b) return { \'YIQ\': yiq, \'HLS\': hls, \'HSV\': hsv } def average_color(colors): Calculates the average color of a list of RGB colors in RGB, HLS, and HSV color spaces. Arguments: colors -- a list of dictionaries with keys \'r\', \'g\', \'b\' representing RGB color values (0 <= r,g,b <= 1) Returns: A dictionary containing the average color in RGB, HLS, and HSV color spaces. # Calculate average RGB avg_r = sum(color[\'r\'] for color in colors) / len(colors) avg_g = sum(color[\'g\'] for color in colors) / len(colors) avg_b = sum(color[\'b\'] for color in colors) / len(colors) avg_rgb = (avg_r, avg_g, avg_b) # Convert average RGB to HLS and HSV avg_hls = colorsys.rgb_to_hls(avg_r, avg_g, avg_b) avg_hsv = colorsys.rgb_to_hsv(avg_r, avg_g, avg_b) return { \'average_rgb\': avg_rgb, \'average_hls\': avg_hls, \'average_hsv\': avg_hsv }"},{"question":"Objective Design a function that creates a Seaborn visualization with a specific style and customized parameters. Problem Statement Create a function called `custom_barplot` that takes the following parameters: - `x`: a list of labels for the x-axis. - `y`: a list of values corresponding to the x-axis labels. - `style`: a string specifying the Seaborn style to apply (e.g., `whitegrid`, `darkgrid`). - `custom_params`: a dictionary of Seaborn style parameters to customize (e.g., `{\\"grid.color\\": \\".5\\", \\"grid.linestyle\\": \\"--\\"}`). The function should: 1. Set the Seaborn style specified by the `style` parameter. 2. Apply any custom parameters provided in `custom_params`. 3. Create and display a bar plot using the `x` and `y` values. Input - `x`: List[str] (e.g., `[\\"A\\", \\"B\\", \\"C\\"]`) - `y`: List[float] (e.g., `[1.0, 3.0, 2.0]`) - `style`: str (e.g., `\\"whitegrid\\"`) - `custom_params`: Dict (e.g., `{\\"grid.color\\": \\".5\\", \\"grid.linestyle\\": \\"--\\"}`) Output - The function should not return anything, but should display the plot. Constraints - The lengths of `x` and `y` will be the same. - Valid styles are `darkgrid`, `whitegrid`, `dark`, `white`, and `ticks`. - `custom_params` keys will be valid Seaborn style parameters. Example ```python def custom_barplot(x, y, style, custom_params): import seaborn as sns import matplotlib.pyplot as plt # Set the specified style sns.set_style(style, custom_params) # Create barplot sns.barplot(x=x, y=y) # Display the plot plt.show() # Example usage x = [\\"A\\", \\"B\\", \\"C\\"] y = [1, 3, 2] style = \\"darkgrid\\" custom_params = {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"} custom_barplot(x, y, style, custom_params) ``` In this example, the function sets the `darkgrid` style with a customized grid color and linestyle, and then displays the bar plot based on the x and y values.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_barplot(x, y, style, custom_params): This function creates a Seaborn bar plot with the specified style and custom parameters. Parameters: x (list): a list of labels for the x-axis. y (list): a list of values corresponding to the x-axis labels. style (str): a string specifying the Seaborn style to apply. custom_params (dict): a dictionary of Seaborn style parameters to customize. Returns: None # Set the specified style sns.set_style(style, custom_params) # Create barplot sns.barplot(x=x, y=y) # Display the plot plt.show()"},{"question":"You are tasked with developing a web crawler that respects the rules set out in a website\'s `robots.txt` file. Your job is to use the `urllib.robotparser` module to determine if a web crawler (identified by its user agent) can access certain URLs on a specified website based on the site\'s `robots.txt` file. Problem Specification 1. **Function Name**: `check_access` 2. **Parameters**: - `robots_url` (str): The URL to the `robots.txt` file of the website. - `user_agent` (str): The user agent string identifying the web crawler. - `urls` (list of str): A list of URLs that you want to check for access permission. 3. **Returns**: - A dictionary where the keys are the URLs from the `urls` list and the values are boolean values (`True` if the user agent is allowed to access the URL, `False` otherwise). Example ```python def check_access(robots_url, user_agent, urls): # Your implementation here # Example usage: robots_url = \\"http://www.example.com/robots.txt\\" user_agent = \\"MyUserAgent\\" urls = [ \\"http://www.example.com/\\", \\"http://www.example.com/private/\\", \\"http://www.example.com/public/\\" ] result = check_access(robots_url, user_agent, urls) print(result) # Output might be: {\\"http://www.example.com/\\": True, \\"http://www.example.com/private/\\": False, \\"http://www.example.com/public/\\": True} ``` # Constraints 1. The `robots.txt` file must be read from the provided URL. 2. Parse the fetched `robots.txt` data to check the permissions. 3. You must handle potential network errors gracefully. # Performance Requirements Your solution should efficiently read and parse the `robots.txt` file and check access for each URL provided. Ensure that network operations are properly handled to avoid unnecessary delays.","solution":"import urllib.robotparser import urllib.error def check_access(robots_url, user_agent, urls): Given the URL of a robots.txt file and a user agent string, determine if the user agent is allowed to access each URL in a list of URLs. Parameters: robots_url (str): The URL to the robots.txt file. user_agent (str): The user agent string identifying the web crawler. urls (list of str): A list of URLs to check for access permission. Returns: dict: A dictionary where the keys are URLs and the values are booleans indicating whether access is allowed. rp = urllib.robotparser.RobotFileParser() try: rp.set_url(robots_url) rp.read() except (urllib.error.URLError, IOError) as e: print(f\\"Error fetching robots.txt: {e}\\") return {url: False for url in urls} access_dict = {} for url in urls: access_dict[url] = rp.can_fetch(user_agent, url) return access_dict"},{"question":"You are required to write a Python extension module using the Python C API that provides functionality for working with floating point numbers, as specified in the provided documentation. # Objective Implement a Python extension module that provides the following functionalities: 1. **Check if an object is a float (`is_float`)**: A function that returns `True` if the given object is a float or a subtype of float. 2. **Convert a string to a float (`float_from_string`)**: A function that takes a string and returns a float. 3. **Convert a float object to a C double (`float_as_double`)**: A function that takes a Python float object and returns its C double representation. 4. **Get the maximum and minimum representable floats**: Two functions that return the respective maximum and minimum float values. # Specification 1. **Function 1: is_float** - **Input**: A Python object. - **Output**: `True` if the object is a float or a subtype of float, otherwise `False`. 2. **Function 2: float_from_string** - **Input**: A string representing a floating-point number. - **Output**: A float object constructed from the string. 3. **Function 3: float_as_double** - **Input**: A Python float object. - **Output**: The C double representation of the float object. 4. **Function 4: get_max_float** - **Input**: None. - **Output**: The maximum representable finite float (`DBL_MAX`). 5. **Function 5: get_min_float** - **Input**: None. - **Output**: The minimum normalized positive float (`DBL_MIN`). # Constraints - You must use the functions defined in the provided documentation. - Proper error checking and handling is required to ensure robustness. - The module should be named `float_tools`. # Deliverables 1. `float_tools.c` file: The C code implementation of the described functionalities. 2. `setup.py` file: The setup script to build the extension module. 3. Demonstration of usage for each function in a Python script `test_float_tools.py`. # Example ```python # test_float_tools.py import float_tools # Check if an object is float print(float_tools.is_float(3.14)) # Expected: True print(float_tools.is_float(\\"3.14\\")) # Expected: False # Convert string to float print(float_tools.float_from_string(\\"3.14\\")) # Expected: 3.14 # Convert float to double print(float_tools.float_as_double(3.14)) # Expected: 3.14 (as a C double) # Get max/min float print(float_tools.get_max_float()) # Expected: Maximum representable float print(float_tools.get_min_float()) # Expected: Minimum positive float ``` Good luck, and make sure to follow the specifications and constraints carefully!","solution":"from ctypes import cdll, c_char_p, c_double, c_int, c_bool import ctypes.util # Load the shared library lib = cdll.LoadLibrary(ctypes.util.find_library(\\"float_tools\\")) def is_float(obj): Check if an object is a float or a subtype of float. # Ensure the input is encoded correctly for C function if isinstance(obj, float): return True return False def float_from_string(string): Converts a string to a float. return float(string) def float_as_double(py_float): Converts a Python float object to a C double representation. if not isinstance(py_float, float): raise TypeError(\\"Argument must be a float\\") return py_float def get_max_float(): Returns the maximum representable finite float. import sys return sys.float_info.max def get_min_float(): Returns the minimum normalized positive float. import sys return sys.float_info.min"},{"question":"**Question: Efficient Web Scraping with Multithreading and Data Aggregation** You are tasked with writing a Python program that efficiently scrapes data from multiple web pages and aggregates the results. The task involves using concurrent execution techniques to speed up the web scraping process. # Requirements 1. **URL Scraping:** - Given a list of URLs, use the `threading` module to fetch the contents of these URLs concurrently. - Implement a function `fetch_url_content(url: str) -> str` that fetches and returns the content of a given URL. 2. **Content Processing:** - Implement a function `process_content(content: str) -> Dict[str, int]` that takes the HTML content of a webpage and returns a dictionary with the counts of each word (case insensitive). 3. **Data Aggregation:** - Implement a function `aggregate_word_counts(counts: List[Dict[str, int]]) -> Dict[str, int]` that aggregates the word counts from multiple pages into a single dictionary. # Input - A list of URLs (`List[str]`). # Output - A single dictionary representing the aggregated word counts from all the provided URLs. # Constraints - You should use the `threading` module for concurrent fetching. - The number of threads should be user-defined (passed as a parameter `num_threads`). - Handle exceptions that may arise during URL fetching and continue processing other URLs. # Example Given the URLs: ```python urls = [ \'http://example.com/page1\', \'http://example.com/page2\', \'http://example.com/page3\', ] ``` and `num_threads = 3`, the output should be an aggregated word count dictionary: ```python { \'word1\': 10, \'word2\': 15, \'word3\': 7, ... } ``` # Performance Requirements - Efficiently handle up to 100 URLs with a significant amount of data. - Minimize the time taken to fetch and process the data by using concurrent execution. # Your Task Implement the following functions: 1. `fetch_url_content(url: str) -> str` 2. `process_content(content: str) -> Dict[str, int]` 3. `aggregate_word_counts(counts: List[Dict[str, int]]) -> Dict[str, int]` 4. A main function `main(urls: List[str], num_threads: int) -> Dict[str, int]` that orchestrates the scraping, processing, and aggregation. Ensure your code is well-structured and handles exceptions gracefully. **Hints:** - Use `ThreadPoolExecutor` from `concurrent.futures` for managing threads. - Use `requests` module for HTTP requests, but manage imports yourself.","solution":"import requests import threading from concurrent.futures import ThreadPoolExecutor from typing import List, Dict from collections import defaultdict import re def fetch_url_content(url: str) -> str: Fetches and returns the content of the given URL. try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException: return \\"\\" def process_content(content: str) -> Dict[str, int]: Processes the HTML content and returns a dictionary with the counts of each word (case insensitive). words = re.findall(r\'bw+b\', content.lower()) word_count = defaultdict(int) for word in words: word_count[word] += 1 return dict(word_count) def aggregate_word_counts(counts: List[Dict[str, int]]) -> Dict[str, int]: Aggregates the word counts from multiple pages into a single dictionary. aggregated_counts = defaultdict(int) for count in counts: for word, num in count.items(): aggregated_counts[word] += num return dict(aggregated_counts) def main(urls: List[str], num_threads: int) -> Dict[str, int]: Orchestrates the scraping, processing, and aggregation of word counts from multiple URLs. with ThreadPoolExecutor(max_workers=num_threads) as executor: contents = list(executor.map(fetch_url_content, urls)) counts = [process_content(content) for content in contents] aggregated_counts = aggregate_word_counts(counts) return aggregated_counts"},{"question":"# Email Header Handling with `email.header` Module Objective: To demonstrate proficiency in handling email headers using the `email.header` module by creating, encoding, and decoding headers with non-ASCII characters. Problem Description: You are required to implement a function `format_email_header(subject, charset)` that takes two arguments: 1. `subject`: A string representing the subject of the email which may contain non-ASCII characters. 2. `charset`: The character set used to encode the subject (e.g., \'utf-8\', \'iso-8859-1\'). The function should: - Create a `Header` instance using the provided subject and charset. - Encode the header into an RFC-compliant format. - Return the encoded header as a string. Additionally, implement another function `parse_email_header(encoded_header)` that takes one argument: 1. `encoded_header`: A string representing the encoded email header. The function should: - Decode the header using `decode_header()`. - Return the list of `(decoded_string, charset)` pairs that `decode_header()` produces. Constraints: - The `subject` string will always be non-empty. - The `charset` will be one of the following: \'utf-8\', \'iso-8859-1\', \'us-ascii\'. Input: - `format_email_header(subject, charset)` - `subject`: A non-empty string with potential non-ASCII characters. - `charset`: A string representing the character set. - `parse_email_header(encoded_header)` - `encoded_header`: A non-empty string representing the encoded email header. Output: - `format_email_header(subject, charset)`: A single string representing the RFC-compliant encoded header. - `parse_email_header(encoded_header)`: A list of tuples, each containing a `(decoded_string, charset)` pair. Example: ```python from email.header import Header, decode_header def format_email_header(subject, charset): header = Header(subject, charset) return header.encode() def parse_email_header(encoded_header): return decode_header(encoded_header) # Example usage subject = \\"pxf6stal\\" charset = \\"iso-8859-1\\" encoded_header = format_email_header(subject, charset) print(encoded_header) # Output: \'=?iso-8859-1?q?p=F6stal?=\' decoded_header = parse_email_header(encoded_header) print(decoded_header) # Output: [(b\'pxf6stal\', \'iso-8859-1\')] ``` Write your implementation below: ```python from email.header import Header, decode_header def format_email_header(subject, charset): # Your code here pass def parse_email_header(encoded_header): # Your code here pass ```","solution":"from email.header import Header, decode_header def format_email_header(subject, charset): Encodes the given email subject using the specified charset into an RFC-compliant format. Args: subject (str): The subject of the email that may contain non-ASCII characters. charset (str): The character set used to encode the subject (e.g., \'utf-8\', \'iso-8859-1\'). Returns: str: The encoded header as an RFC-compliant string. header = Header(subject, charset) return header.encode() def parse_email_header(encoded_header): Decodes the given encoded email header into a list of (decoded_string, charset) pairs. Args: encoded_header (str): The encoded email header. Returns: list: A list of (decoded_string, charset) pairs. return decode_header(encoded_header)"},{"question":"**Objective:** Implement and validate a custom scikit-learn compatible estimator. **Problem Statement:** You are required to implement a custom K-Nearest Neighbors (KNN) classifier from scratch that complies with the scikit-learn estimator API. Your implementation should be compatible with scikit-learn\'s pipeline and model selection tools. The goal is to demonstrate your understanding of scikit-learn estimator development principles and the required methods. **Requirements:** 1. Your custom KNN classifier should be named `CustomKNNClassifier`. 2. It must inherit from `BaseEstimator` and `ClassifierMixin`. 3. It should have the following methods: - `__init__(self, n_neighbors=5)`: - `n_neighbors`: the number of neighbors to use for classification (default is 5). - `fit(self, X, y)`: - `X`: training data of shape (n_samples, n_features). - `y`: target values of shape (n_samples,). - `predict(self, X)`: - `X`: data to predict of shape (n_samples, n_features). - `predict_proba(self, X)`: - `X`: data to predict probabilities of shape (n_samples, n_features). - `score(self, X, y)`: - `X`: test data of shape (n_samples, n_features). - `y`: true labels for the test data of shape (n_samples,). **Constraints:** 1. You must use Euclidean distance to calculate the distance between points. 2. Use `np.unique` to determine the unique classes in the `fit` method. 3. Make sure to handle edge cases, such as ties in KNN voting, by choosing the class with the smaller label value. 4. Your classifier should accept pandas DataFrame inputs and utilize `validate_data` for input validation. **Bonus Requirement:** - Implement a `get_params` and `set_params` method to mimic scikit-learnâ€™s behavior for hyperparameter tuning. # Example Usage ```python import numpy as np from sklearn.utils.validation import validate_data from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.metrics import euclidean_distances class CustomKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5): self.n_neighbors = n_neighbors def fit(self, X, y): X, y = validate_data(self, X, y) self.classes_, y = np.unique(y, return_inverse=True) self.X_ = X self.y_ = y return self def predict(self, X): check_is_fitted(self) X = validate_data(self, X, reset=False) closest = np.argsort(euclidean_distances(X, self.X_), axis=1)[:, :self.n_neighbors] predictions = np.array([np.argmax(np.bincount(self.y_[neighbor])) for neighbor in closest]) return self.classes_[predictions] def predict_proba(self, X): check_is_fitted(self) X = validate_data(self, X, reset=False) closest = np.argsort(euclidean_distances(X, self.X_), axis=1)[:, :self.n_neighbors] probas = np.zeros((X.shape[0], len(self.classes_)), dtype=np.float64) for i, neighbors in enumerate(closest): counts = np.bincount(self.y_[neighbors], minlength=len(self.classes_)) probas[i, :] = counts / self.n_neighbors return probas def score(self, X, y): from sklearn.metrics import accuracy_score return accuracy_score(y, self.predict(X)) # Example usage X_train = np.array([[1, 2], [2, 3], [3, 4], [5, 5]]) y_train = np.array([0, 0, 1, 1]) X_test = np.array([[1, 2], [5, 5]]) knn = CustomKNNClassifier(n_neighbors=2) knn.fit(X_train, y_train) print(knn.predict(X_test)) # Output: [0 1] ``` **Note:** Ensure that your code passes `sklearn.utils.estimator_checks.check_estimator` to confirm that it adheres to the scikit-learn interface and standards.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_array, check_X_y from sklearn.metrics import euclidean_distances from collections import Counter class CustomKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5): self.n_neighbors = n_neighbors def fit(self, X, y): X, y = check_X_y(X, y) self.classes_, y = np.unique(y, return_inverse=True) self.X_ = X self.y_ = y return self def predict(self, X): check_is_fitted(self) X = check_array(X) dist = euclidean_distances(X, self.X_) closest_indices = np.argsort(dist, axis=1)[:, :self.n_neighbors] predictions = [] for indices in closest_indices: closest_labels = self.y_[indices] common = Counter(closest_labels).most_common() most_common_label = common[0][0] predictions.append(self.classes_[most_common_label]) return np.array(predictions) def predict_proba(self, X): check_is_fitted(self) X = check_array(X) dist = euclidean_distances(X, self.X_) closest_indices = np.argsort(dist, axis=1)[:, :self.n_neighbors] probas = np.zeros((X.shape[0], len(self.classes_))) for i, indices in enumerate(closest_indices): closest_labels = self.y_[indices] count = Counter(closest_labels) for c in self.classes_: probas[i, c] = count.get(np.where(self.classes_ == c)[0][0], 0) / self.n_neighbors return probas def score(self, X, y): from sklearn.metrics import accuracy_score return accuracy_score(y, self.predict(X))"},{"question":"Objective Implement a server that can handle multiple client connections using the `select` module in Python 3.10. The server must be able to read messages from clients and broadcast these messages to all connected clients (except the sender). This will test the student\'s ability to manage multiple I/O streams and synchronization among them using `select.select()` effectively. Requirements 1. The server should accept new connections from clients. 2. The server should handle multiple clients concurrently. 3. When a client sends a message, it should be broadcasted to all other connected clients. 4. The server should handle client disconnections gracefully. Input - The server listens to a specified port for new client connections. - Clients connect to the server and send messages (text). Output - The server broadcasts received messages to all other connected clients. Constraints - Use the `select` module\'s `select()` function for I/O multiplexing. - The server must be capable of handling at least 5 concurrent client connections. Implementation Steps 1. Implement a function `setup_server(port)` to initiate the server. 2. Implement a function `accept_connections(server_socket)` to accept new client connections. 3. Implement a function `broadcast_message(from_socket, message, clients)` to send the message to all connected clients except the sender. 4. Use `select.select()` to manage multiple sockets for reading, writing, and exceptional conditions. 5. Implement a main loop to handle incoming connections and messages. Example ```python import socket import select def setup_server(port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'0.0.0.0\', port)) server_socket.listen(5) return server_socket def accept_connections(server_socket, inputs): client_socket, client_address = server_socket.accept() print(f\\"Client {client_address} connected\\") inputs.append(client_socket) def broadcast_message(from_socket, message, clients): for client in clients: if client != from_socket: try: client.send(message) except: client.close() inputs.remove(client) def run_server(port): server_socket = setup_server(port) inputs = [server_socket] while True: readable, writable, exceptional = select.select(inputs, [], inputs) for s in readable: if s is server_socket: accept_connections(server_socket, inputs) else: try: data = s.recv(1024) if data: broadcast_message(s, data, inputs) else: if s in inputs: inputs.remove(s) s.close() except: if s in inputs: inputs.remove(s) s.close() for s in exceptional: inputs.remove(s) s.close() if __name__ == \\"__main__\\": run_server(12345) ``` In this task, the student is expected to implement the functions using the `select` module effectively and ensure proper synchronization of I/O operations.","solution":"import socket import select def setup_server(port): Create and set up the server socket to listen for incoming connections. server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'0.0.0.0\', port)) server_socket.listen(5) return server_socket def accept_connections(server_socket, inputs): Accept new client connection and add it to the list of input sockets. client_socket, client_address = server_socket.accept() print(f\\"Client {client_address} connected\\") inputs.append(client_socket) def broadcast_message(from_socket, message, clients): Broadcast received message to all connected clients except the sender. for client in clients: if client != from_socket and client != server_socket: try: client.send(message) except Exception as e: print(f\\"Error sending message to client: {e}\\") client.close() clients.remove(client) def run_server(port): The main server loop to manage connections and broadcast messages. server_socket = setup_server(port) inputs = [server_socket] while True: readable, writable, exceptional = select.select(inputs, [], inputs) for s in readable: if s is server_socket: accept_connections(server_socket, inputs) else: try: data = s.recv(1024) if data: broadcast_message(s, data, inputs) else: if s in inputs: inputs.remove(s) s.close() except Exception as e: print(f\\"Error reading from socket: {e}\\") if s in inputs: inputs.remove(s) s.close() for s in exceptional: inputs.remove(s) s.close() if __name__ == \\"__main__\\": run_server(12345)"},{"question":"# Question: Implementing and Evaluating a Custom Classification Metric In this assessment, you are required to implement a custom scoring function to evaluate a classification model using scikit-learn. The custom scoring function will be a combination of precision and recall metrics, named as \\"custom_scoring\\". The \\"custom_scoring\\" function will be computed as follows: [ text{Custom Score} = 2 times left( frac{text{Precision} times text{Recall}}{text{Precision} + text{Recall}} right) ] This formula essentially calculates the F1-score. Task: 1. **Custom Scoring Function**: Implement a function `custom_scoring` that computes this score. 2. **Wrap the Custom Scoring Function**: Use `make_scorer` from scikit-learn to create a scorer that can be used with cross-validation tools. 3. **Model Training and Evaluation**: Use `GridSearchCV` to evaluate different hyperparameters of an SVM classifier using the custom scoring function. Instructions: 1. Implement the custom scoring function. 2. Wrap this function using `make_scorer`. 3. Use the `GridSearchCV` method to find the best hyperparameters for an SVM classifier (`SVC`) on the Iris dataset. Use the custom scorer for evaluation. 4. Output the best hyperparameters found and the corresponding score. Requirements: * **Input and Output Formats**: * Function `custom_scoring` accepts two parameters `y_true` and `y_pred`: - `y_true`: True labels. - `y_pred`: Predicted labels from the model. * The function should return the custom score defined above. * **Constraints**: * Only use the Iris dataset, and split it into training and test sets. * Use `GridSearchCV` to search over a predefined grid of parameters for the SVM classifier. * The predefined grid of parameters should include different values for `C` and `kernel`. ```python from sklearn.metrics import make_scorer, precision_score, recall_score from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.model_selection import GridSearchCV, train_test_split # Step 1: Custom Scoring Function def custom_scoring(y_true, y_pred): precision = precision_score(y_true, y_pred, average=\'macro\') recall = recall_score(y_true, y_pred, average=\'macro\') custom_score = 2 * (precision * recall) / (precision + recall) return custom_score # Step 2: Wrap Custom Scoring Function with make_scorer scorer = make_scorer(custom_scoring) # Step 3: Model Training and Evaluation using GridSearchCV def model_training_and_evaluation(): # Load Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the parameter grid for SVM param_grid = { \'C\': [0.1, 1, 10], \'kernel\': [\'linear\', \'poly\', \'rbf\'] } # Initialize the SVC svm = SVC() # Use GridSearchCV with the custom scorer grid_search = GridSearchCV(svm, param_grid, scoring=scorer, cv=5) # Fit the model grid_search.fit(X_train, y_train) # Output the best parameters and the corresponding score best_params = grid_search.best_params_ best_score = grid_search.best_score_ print(\\"Best Parameters:\\", best_params) print(\\"Best Custom Score:\\", best_score) # Run the function model_training_and_evaluation() ``` Evaluation: * The implementation will be evaluated on the correctness and efficiency of the code. * The use of scikit-learn functionalities such as `make_scorer`, `GridSearchCV`, and the custom scoring function will be assessed. * Proper splitting of the dataset and parameter tuning should be implemented as specified.","solution":"from sklearn.metrics import make_scorer, precision_score, recall_score from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.model_selection import GridSearchCV, train_test_split # Step 1: Custom Scoring Function def custom_scoring(y_true, y_pred): precision = precision_score(y_true, y_pred, average=\'macro\') recall = recall_score(y_true, y_pred, average=\'macro\') custom_score = 2 * (precision * recall) / (precision + recall) return custom_score # Step 2: Wrap Custom Scoring Function with make_scorer scorer = make_scorer(custom_scoring) # Step 3: Model Training and Evaluation using GridSearchCV def model_training_and_evaluation(): # Load Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the parameter grid for SVM param_grid = { \'C\': [0.1, 1, 10], \'kernel\': [\'linear\', \'poly\', \'rbf\'] } # Initialize the SVC svm = SVC() # Use GridSearchCV with the custom scorer grid_search = GridSearchCV(svm, param_grid, scoring=scorer, cv=5) # Fit the model grid_search.fit(X_train, y_train) # Output the best parameters and the corresponding score best_params = grid_search.best_params_ best_score = grid_search.best_score_ return best_params, best_score # Run the function best_params, best_score = model_training_and_evaluation() print(\\"Best Parameters:\\", best_params) print(\\"Best Custom Score:\\", best_score)"},{"question":"# Question: Visualizing Sales Data with pandas.plotting You are given a CSV file containing sales data with the following columns: - `Date`: The date of the sales record (Format: yyyy-mm-dd). - `Product`: The name of the product sold. - `Category`: The category of the product sold. - `Quantity`: The number of units sold. - `Price`: The price per unit. Using the `pandas.plotting` module, your task is to generate and save three specific visualizations to understand the sales patterns better. Requirements: 1. **autocorrelation_plot**: Plot the autocorrelation of the total daily sales over time. 2. **boxplot**: Generate a boxplot to show the distribution of the quantity sold for each product category. 3. **scatter_matrix**: Create a scatter matrix plot to explore the relationships between the numerical attributes (`Quantity`, `Price`). Instructions: 1. Load the CSV file into a pandas DataFrame. 2. Implement the following function: ```python import pandas as pd from pandas.plotting import autocorrelation_plot, boxplot, scatter_matrix import matplotlib.pyplot as plt def visualize_sales_data(file_path): Generates and saves visualizations from sales data. Parameters: file_path (str): The path to the CSV file containing the sales data. The function should generate and save the following plots: - Autocorrelation plot named as \'autocorrelation_plot.png\' - Boxplot named as \'boxplot.png\' - Scatter matrix plot named as \'scatter_matrix.png\' # Load data df = pd.read_csv(file_path) # Convert Date column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # 1. Autocorrelation plot daily_sales = df.groupby(\'Date\')[\'Quantity\'].sum() plt.figure() autocorrelation_plot(daily_sales) plt.savefig(\'autocorrelation_plot.png\') # 2. Boxplot plt.figure() df.boxplot(column=\'Quantity\', by=\'Category\') plt.title(\'Boxplot of Quantity by Category\') plt.suptitle(\'\') plt.savefig(\'boxplot.png\') # 3. Scatter matrix plot plt.figure() scatter_matrix(df[[\'Quantity\', \'Price\']], alpha=0.2, figsize=(6, 6), diagonal=\'kde\') plt.savefig(\'scatter_matrix.png\') ``` Expected Input/Output: - **Input**: The function will take a single argument `file_path`, which is a string representing the path to the CSV file. - **Output**: Three image files saved in the working directory: - `autocorrelation_plot.png` - `boxplot.png` - `scatter_matrix.png` Constraints: - The CSV file will have at least 100 rows and cover multiple product categories. Demonstrate the use of `pandas.plotting` functions and ensure the plots are saved correctly. The function should not return or print anything.","solution":"import pandas as pd from pandas.plotting import autocorrelation_plot, scatter_matrix import matplotlib.pyplot as plt def visualize_sales_data(file_path): Generates and saves visualizations from sales data. Parameters: file_path (str): The path to the CSV file containing the sales data. The function should generate and save the following plots: - Autocorrelation plot named as \'autocorrelation_plot.png\' - Boxplot named as \'boxplot.png\' - Scatter matrix plot named as \'scatter_matrix.png\' # Load data df = pd.read_csv(file_path) # Convert Date column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # 1. Autocorrelation plot daily_sales = df.groupby(\'Date\')[\'Quantity\'].sum() plt.figure() autocorrelation_plot(daily_sales) plt.savefig(\'autocorrelation_plot.png\') plt.close() # 2. Boxplot plt.figure() df.boxplot(column=\'Quantity\', by=\'Category\') plt.title(\'Boxplot of Quantity by Category\') plt.suptitle(\'\') plt.savefig(\'boxplot.png\') plt.close() # 3. Scatter matrix plot plt.figure() scatter_matrix(df[[\'Quantity\', \'Price\']], alpha=0.2, figsize=(6, 6), diagonal=\'kde\') plt.savefig(\'scatter_matrix.png\') plt.close()"},{"question":"**Title**: HTML Text Manipulation **Objective**: Demonstrate understanding of text manipulation using the `html` module\'s escape and unescape functionalities. **Problem Statement**: You are working on a text processing tool that deals with user-generated content for a web application. One of the key requirements is to ensure that any text displayed on the web from users is HTML-safe, and any HTML-encoded text retrieved from the database is properly decoded for further processing. You need to implement two functions, `make_html_safe(text: str) -> str` and `decode_html_entities(text: str) -> str`, which respectively use the `html.escape` and `html.unescape` methods to achieve these tasks. 1. **make_html_safe(text: str) -> str**: - This function should take a string `text` as input, and return the string with the characters `&`, `<`, `>`, `\\"`, and `\'` replaced with their corresponding HTML-safe sequences. - Use the `html.escape` function with the `quote=True` parameter. 2. **decode_html_entities(text: str) -> str**: - This function should take a string `text` as input, and return the string with all HTML-named and numeric character references converted back to the corresponding Unicode characters. - Use the `html.unescape` function. **Input and Output Formats**: 1. **make_html_safe**: - **Input**: A string `text` containing characters that may include `&`, `<`, `>`, `\\"`, and `\'`. - **Output**: A string with the specified characters converted to their HTML-safe sequences. 2. **decode_html_entities**: - **Input**: A string `text` containing HTML-encoded characters (e.g., `&gt;`, `&#62;`, `&#x3e;`). - **Output**: A string with the HTML-encoded characters converted back to their Unicode equivalents. **Examples**: ```python # Example usage of make_html_safe text = \\"Hello & welcome to <Python\'s> world!\\" safe_text = make_html_safe(text) print(safe_text) # Output: \\"Hello &amp; welcome to &lt;Python&apos;s&gt; world!\\" # Example usage of decode_html_entities encoded_text = \\"Hello &amp; welcome to &lt;Python&#39;s&gt; world!\\" decoded_text = decode_html_entities(encoded_text) print(decoded_text) # Output: \\"Hello & welcome to <Python\'s> world!\\" ``` **Constraints**: - You can assume the input strings will have a maximum length of 10,000 characters. **Performance Requirements**: - The functions should handle the input strings efficiently within the provided constraints. Good luck and happy coding!","solution":"import html def make_html_safe(text: str) -> str: This function takes a string `text` as input and returns the string with the characters `&`, `<`, `>`, `\\"`, and `\'` replaced with their corresponding HTML-safe sequences. return html.escape(text, quote=True) def decode_html_entities(text: str) -> str: This function takes a string `text` as input and returns the string with all HTML-named and numeric character references converted back to the corresponding Unicode characters. return html.unescape(text)"},{"question":"Using asyncio for Multithreading and Debugging Objective You are asked to demonstrate your understanding of the asyncio library in Python, particularly focusing on multithreading, handling blocking code, and debugging. You will implement a function that performs a blocking operation in a separate thread, schedules asynchronous callbacks, and uses the asyncio debug mode effectively. Problem Statement Write a function `perform_async_operations()` that does the following: 1. Executes a CPU-bound calculation in a separate OS thread to avoid blocking the event loop. 2. Schedules an asynchronous callback. 3. Enables asyncio debug mode to log any potential issues. 4. Detects and handles any coroutines that were never awaited. The CPU-bound calculation is a function `cpu_bound_task()` that simulates a heavy computation by sleeping for 2 seconds. This function should return a string \\"CPU-bound task complete\\". The asynchronous callback is a function `async_callback()` that simply prints \\"Asynchronous callback executed\\". Instructions 1. Create a function `cpu_bound_task()` that sleeps for 2 seconds (simulating a heavy computation) and returns \\"CPU-bound task complete\\". 2. Create an async function `async_callback()` that prints \\"Asynchronous callback executed\\". 3. Implement the `perform_async_operations()` function to: - Enable asyncio debug mode. - Execute `cpu_bound_task` in a separate OS thread using `loop.run_in_executor`. - Schedule the `async_callback` to be run asynchronously using `loop.call_soon`. - Ensure that any coroutines and future results are awaited to prevent any warnings/errors related to never-awaited coroutines and never-retrieved exceptions. - Use appropriate logging and debugging settings as per the asyncio documentation to aid in development and debugging. Constraints - You must use the asyncio library and its relevant APIs. - The function should handle exceptions gracefully and log any errors encountered during the execution of tasks. - Do not use any external libraries other than `asyncio` and `concurrent.futures`. - Functions must be implemented in a way that they can be run on any standard Python 3.10 environment. Example Usage ```python import asyncio def cpu_bound_task(): import time time.sleep(2) return \\"CPU-bound task complete\\" async def async_callback(): print(\\"Asynchronous callback executed\\") async def perform_async_operations(): loop = asyncio.get_running_loop() loop.set_debug(True) import logging logging.basicConfig(level=logging.DEBUG) executor = concurrent.futures.ThreadPoolExecutor() future = loop.run_in_executor(executor, cpu_bound_task) result = await asyncio.gather(future, return_exceptions=True) print(result[0]) loop.call_soon(asyncio.create_task, async_callback()) # Running the example asyncio.run(perform_async_operations()) ``` Expected Output When running the example, the following should be observed: 1. \\"CPU-bound task complete\\" should be printed after 2 seconds. 2. \\"Asynchronous callback executed\\" should be printed immediately after the CPU-bound task completes. 3. Debug logs should provide additional information about the execution flow.","solution":"import asyncio import concurrent.futures import logging def cpu_bound_task(): import time time.sleep(2) return \\"CPU-bound task complete\\" async def async_callback(): print(\\"Asynchronous callback executed\\") async def perform_async_operations(): loop = asyncio.get_running_loop() loop.set_debug(True) logging.basicConfig(level=logging.DEBUG) executor = concurrent.futures.ThreadPoolExecutor() future = loop.run_in_executor(executor, cpu_bound_task) result = await asyncio.gather(future, return_exceptions=True) print(result[0]) loop.call_soon(asyncio.create_task, async_callback()) # This line would normally be called to run the event loop in a real application, # but for testing purposes, it should be called in the unit test # asyncio.run(perform_async_operations())"},{"question":"Coding Assessment Question # Objective The objective of this assessment is to evaluate your understanding of seaborn\'s error bar functionality and your ability to implement various types of error bars in visualization. You are required to demonstrate your comprehension of fundamental and advanced concepts of this package. # Problem Statement Given a dataset `df` with a continuous variable `x` and a continuous or categorical variable `y`, perform the following tasks: 1. **Visualization with Standard Deviation Error Bars**: - Create a Seaborn Pointplot visualizing `x` against `y` with standard deviation (`sd`) error bars. 2. **Visualization with Percentile Interval Error Bars**: - Create a Seaborn Pointplot visualizing `x` against `y` with percentile interval (`pi`) error bars covering 50% of the data. 3. **Visualization with Confidence Interval Error Bars in Regression**: - Create a Seaborn Regression Plot (`regplot`) visualizing the relationship between `x` and `y` with default confidence interval (`ci`) error bars. 4. **Custom Error Bars**: - Create a Seaborn Pointplot visualizing `x` against `y` with custom error bars using a function that sets the error bars to the minimum and maximum values of the data. # Constraints - Use the seaborn version 0.12 or later. - Ensure the plots are easy to interpret with labeled axes and titles. # Input ```python df = pd.DataFrame({ \'x\': np.random.randn(100), \'y\': np.random.choice([\'A\', \'B\', \'C\'], 100) }) ``` # Expected Output - Four subplots displaying the data with different error bar implementations as described in the problem statement. # Example Code ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample DataFrame df = pd.DataFrame({ \'x\': np.random.randn(100), \'y\': np.random.choice([\'A\', \'B\', \'C\'], 100) }) # Set theme sns.set_theme(style=\\"darkgrid\\") # Create subplots fig, axs = plt.subplots(2, 2, figsize=(14, 10)) # Plot with standard deviation error bars sns.pointplot(x=\'y\', y=\'x\', data=df, errorbar=\'sd\', ax=axs[0, 0]) axs[0, 0].set_title(\'Standard Deviation Error Bars\') # Plot with percentile interval error bars sns.pointplot(x=\'y\', y=\'x\', data=df, errorbar=(\'pi\', 50), ax=axs[0, 1]) axs[0, 1].set_title(\'Percentile Interval Error Bars (50%)\') # Plot with confidence interval error bars in regression sns.regplot(x=\'x\', y=\'y\', data=df, ci=95, ax=axs[1, 0]) axs[1, 0].set_title(\'Regression with Confidence Interval Error Bars\') # Plot with custom error bars sns.pointplot(x=\'y\', y=\'x\', data=df, errorbar=lambda x: (x.min(), x.max()), ax=axs[1, 1]) axs[1, 1].set_title(\'Custom Error Bars (Min-Max)\') # Display plots plt.tight_layout() plt.show() ``` # Performance Requirements - Code should execute efficiently for up to 10,000 data points in the DataFrame. # Evaluation Criteria - Correct implementation of the different types of error bars in Seaborn plots. - Correct usage of parametric and nonparametric approaches where applicable. - Proper labeling and titling of the plots for clear interpretation. - Code efficiency and adherence to best practices in data visualization.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_plots(df): Generates four subplots displaying the data with different error bar implementations: - Standard Deviation Error Bars - Percentile Interval Error Bars (50%) - Regression with Confidence Interval Error Bars - Custom Error Bars (Min-Max) # Set theme sns.set_theme(style=\\"darkgrid\\") # Create subplots fig, axs = plt.subplots(2, 2, figsize=(14, 10)) # Plot with standard deviation error bars sns.pointplot(x=\'y\', y=\'x\', data=df, errorbar=\'sd\', ax=axs[0, 0]) axs[0, 0].set_title(\'Standard Deviation Error Bars\') # Plot with percentile interval error bars sns.pointplot(x=\'y\', y=\'x\', data=df, errorbar=(\'pi\', 50), ax=axs[0, 1]) axs[0, 1].set_title(\'Percentile Interval Error Bars (50%)\') # Plot with confidence interval error bars in regression sns.regplot(x=\'x\', y=\'x\', data=df, ci=95, ax=axs[1, 0]) axs[1, 0].set_title(\'Regression with Confidence Interval Error Bars\') # Plot with custom error bars sns.pointplot(x=\'y\', y=\'x\', data=df, errorbar=lambda x: (x.min(), x.max()), ax=axs[1, 1]) axs[1, 1].set_title(\'Custom Error Bars (Min-Max)\') # Display plots plt.tight_layout() plt.show()"},{"question":"**Objective:** To evaluate the student\'s ability to load datasets using scikit-learn, preprocess the data, and apply standard machine learning algorithms. Problem Statement You are provided with the task of building a classification model using the scikit-learn library. The goal is to classify the wine quality dataset (`load_wine`), which consists of various features describing wine samples. 1. Load the wine quality dataset using the `load_wine` function from scikit-learn. 2. Perform basic data exploration to understand the dataset (e.g., check for missing values, dataset shape, and feature names). 3. Preprocess the data by splitting it into training and test sets using an 80-20 split. 4. Standardize the features using scikit-learn\'s `StandardScaler`. 5. Implement two classification models: - A Support Vector Machine (SVM) classifier. - A Random Forest classifier. 6. Train both models on the training set. 7. Evaluate both models on the test set using accuracy, precision, and recall metrics. **Input and Output Formats:** - Function Name: `train_and_evaluate_wine_classification` - Inputs: None (the function should load the dataset internally) - Outputs: Dictionary containing the following keys: - `\'svm\'`: A dictionary with keys `accuracy`, `precision`, and `recall` for the SVM classifier. - `\'random_forest\'`: A dictionary with keys `accuracy`, `precision`, and `recall` for the Random Forest classifier. Example structure of the output dictionary: ```python { \'svm\': {\'accuracy\': 0.85, \'precision\': 0.84, \'recall\': 0.88}, \'random_forest\': {\'accuracy\': 0.87, \'precision\': 0.86, \'recall\': 0.89} } ``` **Constraints:** - Utilize scikit-learn\'s `load_wine` function to load the dataset. - Use an 80-20 split for training and testing. - Use `StandardScaler` for standardizing the features. - Implement the SVM using `sklearn.svm.SVC` and the Random Forest using `sklearn.ensemble.RandomForestClassifier`. - Evaluate the models using `sklearn.metrics` functions for accuracy, precision, and recall. **Performance Requirements:** - The solution should be optimized to run efficiently on the provided dataset. Given the size of the dataset, the runtime should be reasonable. **Sample Code Template:** ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score def train_and_evaluate_wine_classification(): # Load the dataset wine_data = load_wine() # Your code to implement the solution return { \'svm\': {\'accuracy\': svm_accuracy, \'precision\': svm_precision, \'recall\': svm_recall}, \'random_forest\': {\'accuracy\': rf_accuracy, \'precision\': rf_precision, \'recall\': rf_recall} } ``` Students are required to complete the rest of the implementation within the given code template.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score def train_and_evaluate_wine_classification(): # Load the dataset wine_data = load_wine() X = wine_data.data y = wine_data.target # Split the dataset into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # SVM Classifier svm_clf = SVC() svm_clf.fit(X_train_scaled, y_train) y_pred_svm = svm_clf.predict(X_test_scaled) # Evaluate SVM Classifier svm_accuracy = accuracy_score(y_test, y_pred_svm) svm_precision = precision_score(y_test, y_pred_svm, average=\'weighted\') svm_recall = recall_score(y_test, y_pred_svm, average=\'weighted\') # Random Forest Classifier rf_clf = RandomForestClassifier() rf_clf.fit(X_train, y_train) y_pred_rf = rf_clf.predict(X_test) # Evaluate Random Forest Classifier rf_accuracy = accuracy_score(y_test, y_pred_rf) rf_precision = precision_score(y_test, y_pred_rf, average=\'weighted\') rf_recall = recall_score(y_test, y_pred_rf, average=\'weighted\') return { \'svm\': {\'accuracy\': svm_accuracy, \'precision\': svm_precision, \'recall\': svm_recall}, \'random_forest\': {\'accuracy\': rf_accuracy, \'precision\': rf_precision, \'recall\': rf_recall} }"},{"question":"Objective Your task is to write a Python function using the `sunau` module to read an AU audio file, modify certain properties of its audio data, and save it as a new AU audio file. Problem Statement Write a function `modify_au_file(input_file: str, output_file: str, new_channels: int, new_sampwidth: int, new_framerate: int) -> None` that: 1. Reads an AU file specified by `input_file`. 2. Modifies the audio data to: - Change the number of channels to `new_channels`. - Change the sample width to `new_sampwidth`. - Change the frame rate to `new_framerate`. 3. Saves the modified audio data to a new file specified by `output_file`. Constraints - The input AU file will have a valid AU format with supported encoding types. - `new_channels` can either be `1` (mono) or `2` (stereo). - `new_sampwidth` can be `1`, `2`, `3`, or `4` bytes. - `new_framerate` will be a positive integer. - The function should handle errors gracefully and raise appropriate exceptions if the input file is invalid or if any other issues occur during processing. Example Usage ```python def modify_au_file(input_file: str, output_file: str, new_channels: int, new_sampwidth: int, new_framerate: int) -> None: # Your implementation here # Example usage modify_au_file(\\"input.au\\", \\"output.au\\", 2, 2, 22050) ``` # Additional Notes - You are expected to use the `sunau` module for reading and writing the audio files. - Preserve the audio data integrity while transforming the file properties. - Consider memory and performance optimizations for handling large audio files.","solution":"import sunau import os def modify_au_file(input_file: str, output_file: str, new_channels: int, new_sampwidth: int, new_framerate: int) -> None: Reads an AU file, modifies its properties, and saves it as a new AU file. Args: input_file (str): Path to the input AU file. output_file (str): Path to save the output AU file. new_channels (int): The new number of audio channels (1 or 2). new_sampwidth (int): The new sample width in bytes (1, 2, 3, or 4). new_framerate (int): The new frame rate in Hz. try: with sunau.open(input_file, \'rb\') as reader: framerate = reader.getframerate() nframes = reader.getnframes() audio_data = reader.readframes(nframes) with sunau.open(output_file, \'wb\') as writer: writer.setnchannels(new_channels) writer.setsampwidth(new_sampwidth) writer.setframerate(new_framerate) writer.writeframes(audio_data) except Exception as e: raise Exception(\\"An error occurred while processing the AU file\\") from e"},{"question":"Objective: You are required to simulate a task management system using the `queue.PriorityQueue` class in Python to ensure tasks are executed based on their priority. This system needs to handle multiple threads where each thread picks tasks based on priority and marks them as completed. Task: Create a class `TaskScheduler` with the following functionalities: 1. **Initialization**: - Accepts the maximum size of the queue as an argument. If not provided, the queue size is infinite. 2. **Methods to Implement**: - `add_task(priority: int, task: str) -> None`: Adds a task to the priority queue with the given priority. - `execute_task() -> str`: Executes the highest priority task and marks it as completed. Returns the task string. - `all_tasks_done() -> bool`: Checks if all the tasks in the queue have been executed. - `pending_tasks() -> int`: Returns the count of pending tasks in the queue. 3. **Additional Requirements**: - Ensure that no two tasks are executed simultaneously (thread-safe). - Use the proper handling of task addition and execution to deal with `queue.Full` and `queue.Empty` exceptions where relevant. - Support for multi-threading: multiple threads should be able to add and remove tasks safely. Example Usage: ```python import threading from task_scheduler import TaskScheduler # Assuming your class is in a module named task_scheduler def worker(scheduler: TaskScheduler): while not scheduler.all_tasks_done(): try: task = scheduler.execute_task() print(f\\"Task executed: {task}\\") except queue.Empty: continue if __name__ == \\"__main__\\": scheduler = TaskScheduler(maxsize=5) # Example with a bounded size priority queue # Adding tasks to the queue scheduler.add_task(priority=3, task=\\"Task 1\\") scheduler.add_task(priority=1, task=\\"Task 2\\") scheduler.add_task(priority=2, task=\\"Task 3\\") # Create worker threads threads = [] for _ in range(2): # Number of worker threads thread = threading.Thread(target=worker, args=(scheduler,)) thread.start() threads.append(thread) # Wait for all threads to finish for thread in threads: thread.join() print(\\"All tasks executed.\\") ``` Constraints: - Use the `queue.PriorityQueue` provided by Pythonâ€™s standard library. - Handle cases where queue might be empty or full appropriately. - Ensure proper synchronization between threads. Evaluation Criteria: - Correct implementation of the `TaskScheduler` class. - Thread-safety in adding and executing tasks. - Efficient handling of tasks based on priority. - Proper use of exceptions and queue methods as per the module documentation.","solution":"import queue import threading class TaskScheduler: def __init__(self, maxsize=0): self.task_queue = queue.PriorityQueue(maxsize) self.lock = threading.Lock() # To ensure thread safety def add_task(self, priority: int, task: str) -> None: with self.lock: if self.task_queue.full(): raise queue.Full(\\"Task queue is full, cannot add more tasks\\") self.task_queue.put((priority, task)) def execute_task(self) -> str: with self.lock: if self.task_queue.empty(): raise queue.Empty(\\"No tasks to execute from the queue\\") priority, task = self.task_queue.get() return task def all_tasks_done(self) -> bool: with self.lock: return self.task_queue.empty() def pending_tasks(self) -> int: with self.lock: return self.task_queue.qsize()"},{"question":"**Objective:** Implement a secure SSL client using the `ssl` module to connect to a given server, retrieve the server\'s certificates, validate these certificates, and handle possible SSL-related exceptions. **Problem Statement:** You need to implement a function `secure_ssl_client(hostname: str, port: int, cafile: str) -> dict` which connects securely to a server using SSL/TLS, retrieves and validates the server\'s certificate, and handles various SSL-related exceptions. **Function Signature:** ```python def secure_ssl_client(hostname: str, port: int, cafile: str) -> dict: ``` **Parameters:** - `hostname` (str): The hostname of the server to connect to. - `port` (int): The port number of the server. - `cafile` (str): Path to a file of concatenated CA certificates in PEM format. **Returns:** - A dictionary containing: - `version` (str): The SSL/TLS version used for the connection. - `cipher` (tuple): The cipher used for the connection as returned by `SSLSocket.cipher()`. - `peercert` (dict): The server\'s certificate. **Possible SSL-related exceptions to handle:** - `ssl.SSLError` - `ssl.SSLWantReadError` - `ssl.SSLWantWriteError` - `ssl.SSLZeroReturnError` - `ssl.SSLCertVerificationError` **Constraints:** - Use `ssl.SSLContext` for creating the SSL context. - Use `SSLContext.wrap_socket()` to wrap the socket. - Validate the server\'s certificate using the provided CA file. - Handle and log SSL related exceptions appropriately. **Example Usage:** ```python hostname = \'www.python.org\' port = 443 cafile = \'/path/to/ca-certificates.crt\' try: result = secure_ssl_client(hostname, port, cafile) print(result) except Exception as e: print(f\\"An error occurred: {e}\\") ``` # Example Output: ```python { \'version\': \'TLSv1.2\', \'cipher\': (\'ECDHE-RSA-AES128-GCM-SHA256\', \'TLSv1.2\', 128), \'peercert\': {\'subject\': ..., \'issuer\': ..., ...} # Detailed certificate information } ``` # Additional Notes: - Ensure you handle exceptions gracefully and log necessary details for debugging. - Remember to close the connection and clean up resources properly after communication. **Hints:** - Refer to the provided `ssl` module documentation for detailed usage of `SSLContext`, `wrap_socket()`, and handling of SSL-specific exceptions. - Use the `getpeercert()` method to retrieve and inspect the server\'s certificate. - Test your function thoroughly against known secure and insecure hosts.","solution":"import ssl import socket import logging def secure_ssl_client(hostname: str, port: int, cafile: str) -> dict: context = ssl.create_default_context(cafile=cafile) conn = None try: conn = context.wrap_socket(socket.socket(socket.AF_INET), server_hostname=hostname) conn.connect((hostname, port)) result = { \'version\': conn.version(), \'cipher\': conn.cipher(), \'peercert\': conn.getpeercert() } return result except ssl.SSLError as e: logging.error(f\\"SSL error: {e}\\") raise except ssl.SSLWantReadError as e: logging.error(f\\"SSL want read error: {e}\\") raise except ssl.SSLWantWriteError as e: logging.error(f\\"SSL want write error: {e}\\") raise except ssl.SSLZeroReturnError as e: logging.error(f\\"SSL zero return error: {e}\\") raise except ssl.SSLCertVerificationError as e: logging.error(f\\"SSL certificate verification error: {e}\\") raise finally: if conn: conn.close()"},{"question":"# Question: PyTorch HIP Memory Management and Operations Objective: To assess your understanding of PyTorchâ€™s HIP semantics, memory management, and device operations. You must implement a function that demonstrates the use of HIP devices, memory allocation, and basic tensor operations. Task: Implement a function `hip_tensor_operations(device_index: int) -> dict` that performs the following tasks: 1. **Check for HIP Availability**: - Ensure that the system running this function has HIP support enabled. If not, raise an appropriate exception. 2. **Device Creation and Tensor Allocation**: - Create a `device` object for the specified `device_index`. Verify if the index is valid. - Allocate three tensors on the specified HIP device: - `a`: A tensor of shape `(5, 5)` initialized with random numbers. - `b`: A tensor of shape `(5, 5)` initialized with ones. - `c`: A tensor resulting from element-wise multiplication of `a` and `b`. 3. **Memory Management**: - Report the amount of memory utilized after the tensors have been allocated using `torch.cuda.memory_allocated`. - Release unused cached memory using `torch.cuda.empty_cache`. 4. **Result Compilation**: - Collect the following results in a dictionary: - The device type and index of the created tensors. - The values of tensor `c`. - Memory used before and after running `torch.cuda.empty_cache`. 5. **Output**: - Return the dictionary containing the collected results. Example Input: ```python results = hip_tensor_operations(0) ``` Expected Output (example): ```python { \'device_type\': \'cuda\', \'device_index\': 0, \'tensor_c_values\': [[0.50, 0.12, 0.15, 0.85, 0.95], [0.75, 0.05, 0.65, 0.45, 0.34], ...], \'memory_allocated_before_clear\': 20480, \'memory_allocated_after_clear\': 10240 } ``` Constraints: - Assume the system has at least one HIP-compatible GPU. - Keep the operations simple and primarily focused on demonstrating correct usage of HIP with tensors and memory management in PyTorch. Notes: - Utilize `torch.device` with `cuda` for HIP. - Use `torch.cuda` methods for memory management and allocation checks. Implementation: ```python import torch def hip_tensor_operations(device_index: int) -> dict: # Ensure HIP support is available if not torch.cuda.is_available() or not hasattr(torch.version, \'hip\'): raise RuntimeError(\\"HIP support is not enabled on this system\\") # Creating device object device = torch.device(f\'cuda:{device_index}\') # Ensure the device index is valid if device.index >= torch.cuda.device_count(): raise ValueError(f\\"Invalid device index: {device_index}\\") # Allocate tensors a = torch.randn((5, 5), device=device) b = torch.ones((5, 5), device=device) c = a * b # Memory allocation before empty_cache memory_allocated_before = torch.cuda.memory_allocated(device) # Clear unused cached memory torch.cuda.empty_cache() # Memory allocation after empty_cache memory_allocated_after = torch.cuda.memory_allocated(device) # Compile results results = { \'device_type\': device.type, \'device_index\': device.index, \'tensor_c_values\': c.tolist(), \'memory_allocated_before_clear\': memory_allocated_before, \'memory_allocated_after_clear\': memory_allocated_after } return results ```","solution":"import torch def hip_tensor_operations(device_index: int) -> dict: # Ensure HIP support is available if not torch.cuda.is_available() or not hasattr(torch.version, \'hip\'): raise RuntimeError(\\"HIP support is not enabled on this system\\") # Creating device object device = torch.device(f\'cuda:{device_index}\') # Ensure the device index is valid if device.index >= torch.cuda.device_count(): raise ValueError(f\\"Invalid device index: {device_index}\\") # Allocate tensors a = torch.randn((5, 5), device=device) b = torch.ones((5, 5), device=device) c = a * b # Memory allocation before empty_cache memory_allocated_before = torch.cuda.memory_allocated(device) # Clear unused cached memory torch.cuda.empty_cache() # Memory allocation after empty_cache memory_allocated_after = torch.cuda.memory_allocated(device) # Compile results results = { \'device_type\': device.type, \'device_index\': device.index, \'tensor_c_values\': c.tolist(), \'memory_allocated_before_clear\': memory_allocated_before, \'memory_allocated_after_clear\': memory_allocated_after } return results"},{"question":"**Problem Statement: Secure Contact List Token Generator** You are tasked with creating a secure service for managing a contact list where each contact is associated with a secure token. The contact list will include details such as name and email. The token will be used for user authentication and other secure operations. Implement a function that takes a list of contacts, generates a secure token for each contact, and returns a dictionary with the contact\'s email as the key and the token as the value. # Function Signature ```python def generate_secure_contact_tokens(contacts: list) -> dict: pass ``` # Input - `contacts` (list): A list of dictionaries where each dictionary contains the following keys: - `name` (str): The contact\'s name. - `email` (str): The contact\'s email. # Output - (dict): A dictionary where the keys are emails and the values are URL-safe tokens. # Constraints - Each email in the contact list is unique. - The output tokens should be URL-safe, generated using the `secrets.token_urlsafe` function. - The length of the token should be 32 bytes to ensure sufficient security. # Example ```python # Input contacts = [ {\\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\"}, {\\"name\\": \\"Bob\\", \\"email\\": \\"bob@example.com\\"} ] # Expected Output { \\"alice@example.com\\": \\"<32-byte-url-safe-token>\\", \\"bob@example.com\\": \\"<32-byte-url-safe-token>\\", } ``` # Notes - The provided example output is illustrative. The actual token values will differ each time the function is run due to their random nature. - Ensure that the tokens are 32 bytes long to meet security standards. # Additional Requirements - Use the `secrets` module, specifically `secrets.token_urlsafe(32)`, to generate tokens. - Ensure your solution handles edge cases gracefully, such as an empty contact list, by returning an empty dictionary.","solution":"import secrets def generate_secure_contact_tokens(contacts: list) -> dict: Generates a 32-byte URL-safe token for each contact and returns a dictionary with emails as keys and tokens as values. contact_tokens = {} for contact in contacts: email = contact[\'email\'] token = secrets.token_urlsafe(32) contact_tokens[email] = token return contact_tokens"},{"question":"# Warning Control in Python Objective: To assess your understanding of Python\'s `warnings` module, we ask you to implement functions that demonstrate key functionalities of this module, including issuing warnings, filtering warnings, and capturing warnings. Task: 1. Write a function `deprecated_function()` that uses the `warnings.warn` function to issue a `DeprecationWarning`. 2. Write a function `filter_deprecation_warnings(action)` that takes an action (`\'ignore\'`, `\'default\'`, `\'error\'`, etc.) and uses `warnings.simplefilter` to set this action for `DeprecationWarning`. 3. Write a function `capture_warnings(func, *args, **kwargs)` that captures and returns warnings issued by the provided function `func` when called with `args` and `kwargs`, using the `warnings.catch_warnings` context manager. Input and Output Formats: 1. `deprecated_function()`: - **No input parameters.** - **Output:** None. This function should issue a `DeprecationWarning`. 2. `filter_deprecation_warnings(action)`: - **Input:** A string `action` that specifies the filter action. - **Output:** None. This function should set the specified filter action for `DeprecationWarning`. 3. `capture_warnings(func, *args, **kwargs)`: - **Input:** A callable `func`, followed by its arguments `*args` and keyword arguments `**kwargs`. - **Output:** A list of captured warnings. Each warning should be represented by its string message. Constraints: - Use only the `warnings` module for handling warnings. - The filter action in `filter_deprecation_warnings` must be one of the following: `\'ignore\'`, `\'default\'`, `\'error\'`, `\'always\'`, `\'module\'`, or `\'once\'`. Example: ```python def deprecated_function(): import warnings warnings.warn(\\"This function is deprecated\\", DeprecationWarning) def filter_deprecation_warnings(action): import warnings warnings.simplefilter(action, DeprecationWarning) def capture_warnings(func, *args, **kwargs): import warnings with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") func(*args, **kwargs) return [str(warning.message) for warning in w] # Example usage: deprecated_function() filter_deprecation_warnings(\'ignore\') warnings_list = capture_warnings(deprecated_function) print(warnings_list) # Output: [\\"This function is deprecated\\"] ``` **Note:** - Ensure to implement and test each function individually. - Validate the filter settings using various actions and ensure the warnings are handled as specified.","solution":"import warnings def deprecated_function(): Issues a DeprecationWarning. warnings.warn(\\"This function is deprecated\\", DeprecationWarning) def filter_deprecation_warnings(action): Sets the filter action for DeprecationWarning. Parameters: action (str): The action to set for DeprecationWarning. Must be one of: \'ignore\', \'default\', \'error\', \'always\', \'module\', \'once\'. warnings.simplefilter(action, DeprecationWarning) def capture_warnings(func, *args, **kwargs): Captures warnings issued by the provided function. Parameters: func (callable): The function that may issue warnings. *args: Arguments to pass to the function. **kwargs: Keyword arguments to pass to the function. Returns: List of captured warnings as string messages. with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") func(*args, **kwargs) return [str(warning.message) for warning in w]"},{"question":"Objective: Implement a function that imports a module from a specified ZIP archive and returns the list of functions defined in that module. If the module cannot be found or imported, your function should raise a `zipimport.ZipImportError`. Function Signature: ```python def list_module_functions(zip_path: str, module_name: str) -> list: pass ``` Input: - `zip_path` (str): The path to the ZIP archive. - `module_name` (str): The fully qualified name of the module to import from the ZIP archive. Output: - Returns a list of function names defined inside the specified module (list of strings). Constraints: - The ZIP archive may contain a nested directory structure. - All files inside the ZIP archive will be `.py` or `.pyc`. - You should not use the `load_module` function as it is deprecated. Example: Suppose you have a ZIP file `example.zip` with the following structure: ``` example.zip/ mymodule.py ``` And `mymodule.py` contains: ```python def func1(): pass def func2(a, b): return a + b ``` Calling `list_module_functions(\'example.zip\', \'mymodule\')` should return: ```python [\'func1\', \'func2\'] ``` Notes: - You may assume that no filename collisions will occur within the archive. - Consider examining both `zipimport` and Python\'s built-in `importlib` module to implement this function. - The function should handle any necessary cache invalidation to ensure proper functionality upon multiple calls. Implementation: You are required to implement the `list_module_functions` function. Demonstrate your understanding by correctly utilizing the `zipimporter` methods and handling potential exceptions as specified.","solution":"import zipimport import types from typing import List def list_module_functions(zip_path: str, module_name: str) -> List[str]: Imports a module from the specified ZIP archive and returns a list of function names defined in that module. Parameters: zip_path (str): The path to the ZIP archive. module_name (str): The fully qualified name of the module to import from the ZIP archive. Returns: List[str]: A list of function names defined in the specified module. Raises: zipimport.ZipImportError: If the module cannot be found or imported. try: importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) except Exception as e: raise zipimport.ZipImportError(f\\"Cannot import module {module_name} from {zip_path}\\") from e functions = [name for name, obj in vars(module).items() if isinstance(obj, types.FunctionType)] return functions"},{"question":"Advanced Regular Expression Matcher Objective: To assess your understanding and ability to apply regular expressions for pattern matching and substitution in Python. Background: Regular expressions (regex) are a powerful way to search and manipulate strings based on specific patterns. They are widely used in text processing tasks such as validation, parsing, and transformation. Task: Write a Python function `regex_substitute(pattern: str, substitute: str, text: str) -> str` that performs regex-based substitution. The function should replace all substrings in `text` that match the `pattern` with the `substitute` string. Additionally, provide another function `regex_match_count(pattern: str, text: str) -> int` that returns the number of matches of the `pattern` found in `text`. Input: - `pattern` (str): The regex pattern to search for within the text. - `substitute` (str): The string to replace each match of the pattern. - `text` (str): The original text to perform the operations on. Output: - `regex_substitute`: A new string resulting from replacing all matching substrings in `text` with `substitute`. - `regex_match_count`: The count of occurrences of the pattern in the text. Constraints: - Do not use external libraries other than Python\'s built-in `re` module. - Handle cases where the pattern may contain special regex characters. - Consider edge cases like empty strings, no matches, and overlapping patterns. Example: ```python def regex_substitute(pattern: str, substitute: str, text: str) -> str: # Your implementation here def regex_match_count(pattern: str, text: str) -> int: # Your implementation here # Example usage: text = \\"The rain in Spain stays mainly in the plain.\\" pattern = r\\"binb\\" substitute = \\"out\\" print(regex_substitute(pattern, substitute, text)) # Output: \\"The raout in Spain stays mainly in the plain.\\" print(regex_match_count(pattern, text)) # Output: 2 ``` Notes: - Use `re.sub()` for the substitution. - Use `re.findall()` or a similar method to count matches. - Thoroughly test your functions to handle edge cases effectively.","solution":"import re def regex_substitute(pattern: str, substitute: str, text: str) -> str: Replaces all occurrences of the given pattern in the text with the substitute string. return re.sub(pattern, substitute, text) def regex_match_count(pattern: str, text: str) -> int: Returns the count of all occurrences of the given pattern in the text. return len(re.findall(pattern, text))"},{"question":"# Python Coding Assessment **Objective**: The task is to assess your understanding and ability to use the `timeit` module effectively to measure execution times of code snippets. **Problem Statement**: You are given two different approaches to calculate the nth Fibonacci number. The Fibonacci sequence is defined as: - F(0) = 0 - F(1) = 1 - F(n) = F(n-1) + F(n-2) for n > 1 Your task is to measure and compare the execution time of these two approaches (iterative and recursive) for calculating the 30th Fibonacci number using the `timeit` module. **Approach 1**: Iterative ```python def fibonacci_iterative(n): a, b = 0, 1 for _ in range(n): a, b = b, a + b return a ``` **Approach 2**: Recursive ```python def fibonacci_recursive(n): if n <= 1: return n else: return fibonacci_recursive(n-1) + fibonacci_recursive(n-2) ``` **Your Task**: 1. Use the `timeit` module to measure the execution time of `fibonacci_iterative(30)`. 2. Use the `timeit` module to measure the execution time of `fibonacci_recursive(30)`. 3. Compare the execution times and print the results. **Requirements**: 1. Implement the provided functions `fibonacci_iterative` and `fibonacci_recursive`. 2. Use the `timeit.timeit` function to measure the execution time of calling each function with `n=30`. 3. Print the execution times as follows: - `Iterative approach: X.XXXXXX seconds` - `Recursive approach: X.XXXXXX seconds` **Constraints**: - You may assume that n is always a non-negative integer. - Pay attention to the performance differences; the recursive approach might be significantly slower for larger values of n. **Input Format**: You don\'t need to take any input; just measure the execution times within your script. **Output Format**: Print the execution times of both approaches as specified. ```python import timeit # Provided functions def fibonacci_iterative(n): a, b = 0, 1 for _ in range(n): a, b = b, a + b return a def fibonacci_recursive(n): if n <= 1: return n else: return fibonacci_recursive(n-1) + fibonacci_recursive(n-2) # Measure execution time for iterative approach time_iterative = timeit.timeit(\'fibonacci_iterative(30)\', globals=globals(), number=1000) print(f\\"Iterative approach: {time_iterative} seconds\\") # Measure execution time for recursive approach time_recursive = timeit.timeit(\'fibonacci_recursive(30)\', globals=globals(), number=10) print(f\\"Recursive approach: {time_recursive} seconds\\") ``` **Explanation**: - The `timeit.timeit` function is used to measure the time it takes to execute the given statement a certain number of times. - The `globals=globals()` parameter allows the statement to access the defined functions in the current global namespace. - The `number` parameter specifies how many times to execute the statement to get a more accurate measurement.","solution":"import timeit # Provided functions def fibonacci_iterative(n): Iteratively calculates the nth Fibonacci number. a, b = 0, 1 for _ in range(n): a, b = b, a + b return a def fibonacci_recursive(n): Recursively calculates the nth Fibonacci number. if n <= 1: return n else: return fibonacci_recursive(n-1) + fibonacci_recursive(n-2) # Measure execution time for iterative approach time_iterative = timeit.timeit(\'fibonacci_iterative(30)\', globals=globals(), number=1000) print(f\\"Iterative approach: {time_iterative} seconds\\") # Measure execution time for recursive approach time_recursive = timeit.timeit(\'fibonacci_recursive(30)\', globals=globals(), number=1) print(f\\"Recursive approach: {time_recursive} seconds\\")"},{"question":"**Objective**: Implement a Python function that simulates the configuration and handling of a custom list of wide strings. This exercise will test your ability to understand structures and design logic in terms of initialization and handling sequence as outlined in the given system-level documentation. # Problem Statement You need to implement a `ConfigStringList` class in Python to mimic the behavior of `PyWideStringList` mentioned in the provided documentation. This class manages a list of wide strings and includes initialization, appending, and inserting functionalities while keeping track of errors using custom exception handling. Implement the following features step-by-step: 1. **Initialization**: - `__init__(self)`: Initialize an empty list of wide strings. 2. **Appending Strings**: - `append(self, item: str)`: Append a wide string item (standard Python `str`) to the list. Raise an exception if the item is not a valid string. 3. **Inserting Strings**: - `insert(self, index: int, item: str)`: Insert a wide string item at a specific index in the list. If the index is greater than or equal to the list length, append the item at the end. Raise an exception if the item is not valid or if the index is out of range. 4. **Exception Handling**: - Define a custom exception `ConfigListError` with an appropriate error message. Your methods should raise `ConfigListError` on encountering invalid input conditions or errors in list operations. 5. Provide a method to represent the list as a standard Python list: - `to_list(self) -> list`: Return the managed list of wide strings. # Example Usage ```python if __name__ == \\"__main__\\": config_list = ConfigStringList() config_list.append(\\"first string\\") config_list.append(\\"second string\\") config_list.insert(1, \\"inserted string\\") config_list.insert(10, \\"appended at end\\") # since index 10 is out-of-bounds try: config_list.append(123) # invalid operation, should raise a ConfigListError except ConfigListError as e: print(f\\"Error: {e}\\") print(config_list.to_list()) ``` # Requirements: 1. The `item` parameter for `append` and `insert` must be a valid string, otherwise raise `ConfigListError`. 2. If the `index` parameter for `insert` is negative or out of bounds (greater than the list length), raise `ConfigListError`, except when appending at the end. 3. Provide meaningful error messages in `ConfigListError`. # Expected Output: ```text Error: Invalid item. Expected a string but got <class \'int\'> [\'first string\', \'inserted string\', \'second string\', \'appended at end\'] ``` # Constraints: - **Performance**: Assume a maximum of 1000 operations to be performed on the list. - **Operational Environment**: Python 3.10 # Function Signatures: ```python class ConfigListError(Exception): pass class ConfigStringList: def __init__(self): # Your code here def append(self, item: str): # Your code here def insert(self, index: int, item: str): # Your code here def to_list(self) -> list: # Your code here ``` Implement this according to the specifications.","solution":"class ConfigListError(Exception): def __init__(self, message): super().__init__(message) class ConfigStringList: def __init__(self): self._list = [] def append(self, item: str): if not isinstance(item, str): raise ConfigListError(f\\"Invalid item. Expected a string but got {type(item)}\\") self._list.append(item) def insert(self, index: int, item: str): if not isinstance(item, str): raise ConfigListError(f\\"Invalid item. Expected a string but got {type(item)}\\") if index < 0: raise ConfigListError(\\"Index out of range. Negative index is not allowed.\\") if index >= len(self._list): self._list.append(item) else: self._list.insert(index, item) def to_list(self) -> list: return self._list"},{"question":"**Title:** Advanced Enum Usage in Python **Objective:** The goal of this exercise is to test your understanding and ability to utilize the `enum` module in Python. You will create a set of enumerations for a hypothetical role-playing game (RPG) to handle character types, weapon types, and their respective attributes using various functionalities provided by the `enum` module. **Instructions:** 1. **CharacterType Enum:** - Create an enumeration `CharacterType` using the `Enum` class. - The enum should contain the following members with assigned integer values: - `WARRIOR` with a value of 1 - `MAGE` with a value of 2 - `ARCHER` with a value of 3 2. **WeaponType Enum:** - Create another enumeration `WeaponType` using the `Enum` class. - Use the `auto` helper for automatic value assignment for the following members: - `SWORD` - `STAFF` - `BOW` 3. **UniqueWeaponType Enum:** - Create an enumeration `UniqueWeaponType` using the `Enum` class and decorate it with `@unique`. - Add the following members with manually assigned arbitrary values: - `EXCALIBUR` with a value of 100 - `MJOLNIR` with a value of 101 - `LONG_BOW` with a value of 102 4. **CombinedFlags Enum:** - Create an enumeration `RPGFlag` using the `Flag` class to represent combinations of character attributes. - Use the `auto` helper for automatic value assignment for the following flags: - `AGILE` - `STRONG` - `WISE` - `STEALTH` - Combine these flags to create a member `VERSATILE` representing all the attributes combined. 5. **Custom Behavior in Enums:** - Create an enumeration `Character` using the `Enum` class that also defines custom behavior. - Add the following members with tuples containing their health points and mana points: - `WARRIOR` with (150, 30) - `MAGE` with (70, 200) - `ARCHER` with (100, 50) - Implement a method `character_stats(self)` that returns a string describing the character\'s name and their health and mana points, for example: `\\"WARRIOR: Health = 150, Mana = 30\\"`. 6. **Main Function:** - Write a main function to demonstrate the usage of each of the enums created above. - Create instances, access their members, and call methods where applicable. - Print the details of each enum and their members to confirm correctness. **Constraints:** - Use the `Enum`, `IntEnum`, `Flag`, and `auto` functionalities provided by the `enum` module. - Ensure that the enums are defined as specified, with correct values and behavior. **Expected Input and Output:** - There is no input required from the user. - The output should demonstrate the proper creation and usage of enums, including their members, values, and custom methods. **Example Output:** ```plaintext Character Types: CharacterType.WARRIOR = 1 CharacterType.MAGE = 2 CharacterType.ARCHER = 3 Weapon Types: WeaponType.SWORD = 1 WeaponType.STAFF = 2 WeaponType.BOW = 3 Unique Weapon Types: UniqueWeaponType.EXCALIBUR = 100 UniqueWeaponType.MJOLNIR = 101 UniqueWeaponType.LONG_BOW = 102 RPG Flags: RPGFlag.AGILE = 1 RPGFlag.STRONG = 2 RPGFlag.WISE = 4 RPGFlag.STEALTH = 8 RPGFlag.VERSATILE = 15 Character Stats: WARRIOR: Health = 150, Mana = 30 MAGE: Health = 70, Mana = 200 ARCHER: Health = 100, Mana = 50 ```","solution":"from enum import Enum, auto, unique, Flag class CharacterType(Enum): WARRIOR = 1 MAGE = 2 ARCHER = 3 class WeaponType(Enum): SWORD = auto() STAFF = auto() BOW = auto() @unique class UniqueWeaponType(Enum): EXCALIBUR = 100 MJOLNIR = 101 LONG_BOW = 102 class RPGFlag(Flag): AGILE = auto() STRONG = auto() WISE = auto() STEALTH = auto() VERSATILE = AGILE | STRONG | WISE | STEALTH class Character(Enum): WARRIOR = (150, 30) MAGE = (70, 200) ARCHER = (100, 50) def __init__(self, health, mana): self.health = health self.mana = mana def character_stats(self): return f\\"{self.name}: Health = {self.health}, Mana = {self.mana}\\" def main(): print(\\"Character Types:\\") for char_type in CharacterType: print(f\\"{char_type} = {char_type.value}\\") print(\\"nWeapon Types:\\") for weapon_type in WeaponType: print(f\\"{weapon_type} = {weapon_type.value}\\") print(\\"nUnique Weapon Types:\\") for unique_weapon in UniqueWeaponType: print(f\\"{unique_weapon} = {unique_weapon.value}\\") print(\\"nRPG Flags:\\") for flag in RPGFlag: print(f\\"{flag} = {flag.value}\\") print(\\"nCharacter Stats:\\") for character in Character: print(character.character_stats()) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Managing Custom Garbage Collection in Python The goal of this question is to create a utility that takes a snapshot of the current state of the garbage collector and allows debugging of unreachable objects. **Task:** 1. Implement a function `gc_snapshot()` that returns a dictionary containing: - Current garbage collection `thresholds` returned by `gc.get_threshold()`. - Collection `counts` for each generation, returned by `gc.get_count()`. - Detailed statistics for each generation, returned by `gc.get_stats()`. - A list of all objects currently tracked by the garbage collector, returned by `gc.get_objects()`. 2. Implement a function `analyze_unreachable_objects()` that: - Runs a full garbage collection to identify unreachable objects. Use `gc.collect()` to achieve this. - Returns a tuple containing: - The number of unreachable objects found. - The list of these unreachable objects stored in `gc.garbage`. **Function Signatures:** ```python import gc def gc_snapshot() -> dict: pass def analyze_unreachable_objects() -> tuple: pass ``` **Constraints:** - You should not disable garbage collection during any part of the task. - Ensure that your solution efficiently handles any large amount of objects tracked by the garbage collector. **Example Usage:** ```python # Taking a snapshot of the current GC state snapshot = gc_snapshot() print(snapshot[\'thresholds\']) # Example: (700, 10, 10) print(snapshot[\'counts\']) # Example: (1, 0, 0) print(snapshot[\'stats\']) # Example: [{\'collections\': 1, \'collected\': 10, \'uncollectable\': 0}, ...] # Running garbage collection and analyzing unreachable objects unreachable_count, unreachable_objects = analyze_unreachable_objects() print(unreachable_count) # Example: 2 print(unreachable_objects) # Example: [<object object at 0x7f8f1d99da70>, <object object at 0x7f8f1d99da90>] ``` **Note:** - You may use any combination of `gc` functions described in the documentation to complete this task. - Handle cases where no unreachable objects are found appropriately. Have fun implementing and happy debugging!","solution":"import gc def gc_snapshot() -> dict: Takes a snapshot of the current state of the garbage collector and returns a dictionary. snapshot = { \'thresholds\': gc.get_threshold(), \'counts\': gc.get_count(), \'stats\': gc.get_stats(), \'objects\': gc.get_objects(), } return snapshot def analyze_unreachable_objects() -> tuple: Runs a full garbage collection to identify unreachable objects and returns a tuple containing the number of unreachable objects found and the list of these unreachable objects. unreachable_objects_count = gc.collect() unreachable_objects = gc.garbage[:] return unreachable_objects_count, unreachable_objects"},{"question":"# Sparse Tensors in PyTorch You are tasked with implementing a function that demonstrates a typical use case of sparse tensors in PyTorch. Your function should: 1. Create a dense tensor. 2. Convert the dense tensor into different sparse formats (COO, CSR, and CSC). 3. Perform a matrix multiplication operation between a sparse tensor and a dense tensor. 4. Convert the resulting sparse tensor back to a dense tensor. 5. Compare the memory footprint of the dense tensor and the sparse tensors. Function Signature ```python def demonstrate_sparse_tensors(): pass ``` Requirements 1. **Create a dense tensor** `A` of size `(4, 4)` with the following values: ```plaintext [[1., 0., 0., 0.], [0., 0., 2., 0.], [0., 3., 0., 0.], [4., 0., 0., 5.]] ``` 2. **Convert the dense tensor `A` into** three sparse formats: - COO format - CSR format - CSC format 3. **Perform a matrix multiplication** between the CSR sparse tensor and a new dense tensor `B` of size `(4, 2)` with random values. 4. **Convert the resulting sparse tensor** back into a dense tensor. 5. **Calculate and print the memory footprint** of the original dense tensor `A` and the sparse tensors in COO, CSR, and CSC formats. Compare their sizes. Example Output (formatted) ```plaintext Original Dense Tensor A: tensor([[1., 0., 0., 0.], [0., 0., 2., 0.], [0., 3., 0., 0.], [4., 0., 0., 5.]]) Dense Tensor B: tensor([[0.4321, 0.5723], [0.6715, 0.4234], [0.9876, 0.2345], [0.5678, 0.6789]]) Resulting Dense Tensor after Multiplication: tensor([[0.4284, 0.5083], [1.9752, 0.4690], [2.0146, 1.2702], [4.3996, 6.1076]]) Memory Footprint: Dense Tensor A: 128 bytes COO Sparse Tensor: 64 bytes CSR Sparse Tensor: 56 bytes CSC Sparse Tensor: 56 bytes ``` Note - Use `torch.tensor` to create the initial dense tensor `A`. - Use `tensor.to_sparse()`, `tensor.to_sparse_csr()`, and `tensor.to_sparse_csc()` for conversions. - Use `torch.matmul()` for matrix multiplication. - Print the memory size in bytes for each tensor type using `sys.getsizeof()`. Implement the `demonstrate_sparse_tensors` function to meet these requirements.","solution":"import torch import sys def demonstrate_sparse_tensors(): # Create a dense tensor A A = torch.tensor([ [1., 0., 0., 0.], [0., 0., 2., 0.], [0., 3., 0., 0.], [4., 0., 0., 5.] ]) # Print the original dense tensor A print(\\"Original Dense Tensor A:\\") print(A) # Convert A to sparse formats A_coo = A.to_sparse() A_csr = A.to_sparse_csr() A_csc = A.to_sparse_csc() # Create a dense tensor B with random values B = torch.rand((4, 2)) # Print the dense tensor B print(\\"nDense Tensor B:\\") print(B) # Perform a matrix multiplication between CSR sparse tensor and dense tensor B C_sparse = torch.matmul(A_csr, B) C_dense = C_sparse.to_dense() # Print the resulting dense tensor print(\\"nResulting Dense Tensor after Multiplication:\\") print(C_dense) # Calculate and print the memory footprint dense_size = A.element_size() * A.nelement() coo_size = A_coo._indices().element_size() * A_coo._indices().nelement() + A_coo._values().element_size() * A_coo._values().nelement() csr_size = A_csr.crow_indices().element_size() * A_csr.crow_indices().nelement() + A_csr.col_indices().element_size() * A_csr.col_indices().nelement() + A_csr.values().element_size() * A_csr.values().nelement() csc_size = A_csc.ccol_indices().element_size() * A_csc.ccol_indices().nelement() + A_csc.row_indices().element_size() * A_csc.row_indices().nelement() + A_csc.values().element_size() * A_csc.values().nelement() # Print memory footprint print(\\"nMemory Footprint:\\") print(f\\"Dense Tensor A: {dense_size} bytes\\") print(f\\"COO Sparse Tensor: {coo_size} bytes\\") print(f\\"CSR Sparse Tensor: {csr_size} bytes\\") print(f\\"CSC Sparse Tensor: {csc_size} bytes\\")"},{"question":"# **Coding Assessment Question** Objective You are required to demonstrate your understanding of Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) using Scikit-Learn. This task consists of two main parts: classification and dimensionality reduction. Task Description 1. **Classification with LDA and QDA** You are provided with a dataset that includes several features and a target variable (class labels). Your task is to: - Split the dataset into training and testing sets. - Train a Linear Discriminant Analysis (LDA) classifier and a Quadratic Discriminant Analysis (QDA) classifier on the training data. - Evaluate both classifiers on the test data using accuracy and confusion matrix. **Input**: - `X`: A 2D numpy array or pandas dataframe containing the features of the dataset. - `y`: A 1D numpy array or pandas series containing the class labels. - `test_size`: A float value representing the proportion of the dataset to include in the test split (e.g., 0.2 for 20%). **Output**: - `lda_metrics`: A dictionary containing the accuracy and confusion matrix for LDA. - `qda_metrics`: A dictionary containing the accuracy and confusion matrix for QDA. 2. **Dimensionality Reduction using LDA** After performing classification, use LDA for supervised dimensionality reduction. Your task is to: - Project the data onto a lower-dimensional subspace that maximizes class separability. - Visualize the projected data. **Input**: - `X`: A 2D numpy array or pandas dataframe containing the features of the dataset. - `y`: A 1D numpy array or pandas series containing the class labels. - `n_components`: An integer representing the number of desired dimensions to project the data onto. **Output**: - A 2D scatter plot of the data projected onto `n_components` dimensions, with each class in a different color. Constraints and Requirements - Use the `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis` classes from `sklearn.discriminant_analysis`. - Ensure your implementation adheres to best practices in data preprocessing and modeling. - Your solution should handle any edge cases, such as insufficient data size for the requested `n_components`. - Document your code and include any necessary comments. # Example ```python import numpy as np import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score, confusion_matrix import matplotlib.pyplot as plt def lda_qda_classification(X, y, test_size=0.2): # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) # Linear Discriminant Analysis (LDA) classification lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) y_pred_lda = lda.predict(X_test) lda_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_lda), \'confusion_matrix\': confusion_matrix(y_test, y_pred_lda) } # Quadratic Discriminant Analysis (QDA) classification qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred_qda = qda.predict(X_test) qda_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_qda), \'confusion_matrix\': confusion_matrix(y_test, y_pred_qda) } return lda_metrics, qda_metrics def lda_dimensionality_reduction(X, y, n_components): # Linear Discriminant Analysis (LDA) for dimensionality reduction lda = LinearDiscriminantAnalysis(n_components=n_components) X_lda = lda.fit_transform(X, y) # 2D scatter plot plt.figure() for i, target_name in enumerate(np.unique(y)): plt.scatter(X_lda[y == target_name, 0], X_lda[y == target_name, 1], alpha=.8, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of dataset\') plt.xlabel(\'LD1\') plt.ylabel(\'LD2\') plt.show() # Load example dataset data = load_iris() X = pd.DataFrame(data.data, columns=data.feature_names) y = pd.Series(data.target) # Perform classification with LDA and QDA lda_metrics, qda_metrics = lda_qda_classification(X, y, test_size=0.2) print(\\"LDA Metrics:\\", lda_metrics) print(\\"QDA Metrics:\\", qda_metrics) # Perform dimensionality reduction with LDA lda_dimensionality_reduction(X, y, n_components=2) ``` Notes - You may use other datasets to further validate your implementation. - The provided example uses the Iris dataset from Scikit-Learn as a reference.","solution":"import numpy as np import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score, confusion_matrix import matplotlib.pyplot as plt def lda_qda_classification(X, y, test_size=0.2): Train and evaluate LDA and QDA classifiers on the provided dataset. Parameters: X (numpy.ndarray or pandas.DataFrame): Features of the dataset. y (numpy.ndarray or pandas.Series): Class labels of the dataset. test_size (float): Proportion of the dataset to include in the test split. Returns: Tuple[dict, dict]: Metrics for LDA and QDA classifiers. # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) # Linear Discriminant Analysis (LDA) classification lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) y_pred_lda = lda.predict(X_test) lda_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_lda), \'confusion_matrix\': confusion_matrix(y_test, y_pred_lda) } # Quadratic Discriminant Analysis (QDA) classification qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred_qda = qda.predict(X_test) qda_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_qda), \'confusion_matrix\': confusion_matrix(y_test, y_pred_qda) } return lda_metrics, qda_metrics def lda_dimensionality_reduction(X, y, n_components): Perform supervised dimensionality reduction using LDA and visualize the projected data. Parameters: X (numpy.ndarray or pandas.DataFrame): Features of the dataset. y (numpy.ndarray or pandas.Series): Class labels of the dataset. n_components (int): Number of dimensions to project the data onto. Returns: None # Linear Discriminant Analysis (LDA) for dimensionality reduction lda = LinearDiscriminantAnalysis(n_components=n_components) X_lda = lda.fit_transform(X, y) # 2D scatter plot plt.figure() for i, target_name in enumerate(np.unique(y)): plt.scatter(X_lda[y == target_name, 0], X_lda[y == target_name, 1], alpha=.8, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of dataset\') plt.xlabel(\'LD1\') plt.ylabel(\'LD2\') plt.show()"},{"question":"Implementing a Telnet Client Objective: You are to implement a basic Telnet client using the `telnetlib` module in Python. The client will connect to a specified host and port, send a series of commands to the server, and capture the serverâ€™s responses. You must demonstrate an understanding of connection management, input/output operations, and basic exception handling. Task: 1. **Create a function named `telnet_client`**: - **Parameters**: - `host` (string): The hostname or IP address of the Telnet server. - `port` (int): The port number on which the Telnet server is listening. - `commands` (list of strings): A list of commands to be sent to the server in sequence. - `timeout` (float, optional): The timeout duration for each command read operation. Default is `None`. 2. **Functionality**: - Establish a Telnet connection to the specified host and port. - For each command in the `commands` list: - Send the command to the server. - Read and capture the entire response until the command prompt appears again or timeout occurs. - Store each commandâ€™s output in a dictionary, where the keys are the commands and the values are the respective outputs. - Properly handle any exceptions such as timeouts or connection errors, and ensure that the connection is closed in all scenarios. 3. **Expected Output**: - Return a dictionary containing the commands and their respective outputs. Example: ```python def telnet_client(host, port, commands, timeout=None): \'\'\' Connects to a Telnet server, executes commands, and returns their outputs. Parameters: - host (str): The hostname or IP address of the Telnet server. - port (int): The port number on which the Telnet server is listening. - commands (list of str): A list of commands to be sent in sequence. - timeout (float, optional): The timeout duration for each command read operation. Returns: - dict: A dictionary containing the commands and their respective outputs. \'\'\' # Write your implementation here # Example usage: host = \'localhost\' port = 23 commands = [\'ls\', \'pwd\', \'whoami\'] timeout = 5.0 outputs = telnet_client(host, port, commands, timeout) for cmd, output in outputs.items(): print(f\\"Command: {cmd}nOutput:n{output}n\\") ``` Notes: - The function should demonstrate efficient use of the `telnetlib` methods, especially with respect to reading data (`read_until`, `read_all`, etc.). - Consider edge cases, such as handling an unexpected disconnection or ensuring the client does not hang indefinitely on a read operation. - This question assesses your ability to work with network protocols, manage connections, handle I/O operations, and implement error handling in Python.","solution":"import telnetlib def telnet_client(host, port, commands, timeout=None): Connects to a Telnet server, executes commands, and returns their outputs. Parameters: - host (str): The hostname or IP address of the Telnet server. - port (int): The port number on which the Telnet server is listening. - commands (list of str): A list of commands to be sent in sequence. - timeout (float, optional): The timeout duration for each command read operation. Returns: - dict: A dictionary containing the commands and their respective outputs. output_dict = {} try: # Connect to the Telnet server with telnetlib.Telnet(host, port, timeout) as telnet: for command in commands: # Send the command telnet.write(command.encode(\'ascii\') + b\'n\') # Read output until the prompt appears again or timeout output = telnet.read_until(b\'n\', timeout) output_dict[command] = output.decode(\'ascii\') except Exception as e: print(f\\"An error occurred: {e}\\") return output_dict"},{"question":"Using the Python `trace` module, write a function named `trace_execution` that takes two arguments: 1. `code_to_run` (str): A string representing the Python code that you want to trace. 2. `ignore_dirs` (list): A list of directory paths whose modules should be ignored during tracing. The function should: 1. Create an instance of the `trace.Trace` class, setting it to count line executions and ignore the given directories. 2. Use the `run` method of the `Trace` object to execute the provided code. 3. Generate a coverage report that shows how many times each line of the code was executed. 4. Write the results to files in the current directory, marking lines that were not executed. 5. Return the path of the generated coverage result file. You may assume that all necessary imports are available and valid. Input - `code_to_run`: A string representing valid Python code. - `ignore_dirs`: A list of directory paths to be ignored. Output - A string representing the path to the generated coverage result file. Example Usage ```python code = def sample_function(): for i in range(10): print(i) return sample_function() ignore_dirs = [\\"/usr/lib/python3.10\\"] result_file = trace_execution(code, ignore_dirs) print(\\"Coverage results are in:\\", result_file) ``` This example should trace the execution of `sample_function()`, ignoring modules in `/usr/lib/python3.10`, and produce a coverage report in the current directory. Constraints - Ensure that the function handles the execution of the provided code securely. - The function should handle potential errors gracefully, such as invalid code or directories, and provide meaningful error messages.","solution":"import os import sys import traceback import trace def trace_execution(code_to_run, ignore_dirs): Traces the execution of the given code while ignoring specified directories. Parameters: code_to_run (str): A string representing the Python code that you want to trace. ignore_dirs (list): A list of directory paths whose modules should be ignored during tracing. Returns: str: The path to the generated coverage result file. try: # Configure the trace object tracer = trace.Trace(count=True, trace=False, ignoremods=ignore_dirs) # Execute the code using the tracer tracer.run(code_to_run) # Generate a coverage report results = tracer.results() coverage_file = \\"coverage_report.txt\\" with open(coverage_file, \\"w\\") as report_file: results.write_results(summary=True, coverdir=\\".\\") return os.path.abspath(coverage_file) except Exception as e: error_message = f\\"An error occurred during code execution: {str(e)}n{traceback.format_exc()}\\" print(error_message) return \\"error\\""},{"question":"# Email Message Parsing and Analysis **Objective:** Implement a function that parses an email message from a given byte sequence, navigates through its content, and extracts specified information. **Problem Statement:** You are provided with a byte sequence representing an email message. Your task is to write a function `extract_email_info(message_bytes: bytes) -> dict` that performs the following operations: 1. Parse the email message from the given byte sequence using the `email` package. 2. Determine whether the email is a multipart message. If it is, iterate through its parts and extract the following: - Part content type (e.g., `text/plain`, `text/html`, etc.) - Part payload (decoded if necessary) 3. If the email is not multipart, simply extract the main content type and payload. 4. Return the extracted information in a dictionary with the following format: ```python { \'is_multipart\': bool, \'parts\': [ {\'content_type\': str, \'payload\': str}, ... ] } ``` If the email is not multipart, the dictionary should have only one part with the main content type and payload. **Input:** - `message_bytes` (bytes): Byte sequence of the email message. **Output:** - (dict): Dictionary containing the email content details structured as described above. **Constraints:** - Do not assume the email message to be standards-compliant; handle possible defects. - Ensure all parts are decoded properly based on their content transfer encoding. **Performance requirements:** - The solution should efficiently handle emails up to a size of 10 MB. **Example:** ```python message_bytes = b\\"Your raw email message bytes here\\" # Expected output format for a non-multipart email { \'is_multipart\': False, \'parts\': [ {\'content_type\': \'text/plain\', \'payload\': \'This is a simple email.\'} ] } # Expected output format for a multipart email { \'is_multipart\': True, \'parts\': [ {\'content_type\': \'text/plain\', \'payload\': \'This is the plain text part.\'}, {\'content_type\': \'text/html\', \'payload\': \'<html>This is the HTML part.</html>\'} ] } ``` Your implementation should handle and extract such information accurately, ensuring the proper decoding and formatting of the payload. # Additional Notes: - Consider using `email.message_from_bytes()` for initially parsing the `message_bytes`. - Handle different parts and content types, ensuring accurate extraction and decoding of their payloads. - Use the `policy` keyword effectively as per the guidelines in the documentation.","solution":"import email from email import policy def extract_email_info(message_bytes: bytes) -> dict: Parses an email message from a byte sequence to extract information. Args: message_bytes (bytes): Byte sequence of the email message. Returns: dict: Dictionary containing extracted information. msg = email.message_from_bytes(message_bytes, policy=policy.default) is_multipart = msg.is_multipart() parts_list = [] if is_multipart: for part in msg.iter_parts(): content_type = part.get_content_type() payload = part.get_payload(decode=True) # Decode the payload charset = part.get_content_charset() or \'utf-8\' # Default to utf-8 if charset is None payload_str = payload.decode(charset, errors=\'replace\') if payload else \'\' parts_list.append({ \'content_type\': content_type, \'payload\': payload_str }) else: content_type = msg.get_content_type() payload = msg.get_payload(decode=True) charset = msg.get_content_charset() or \'utf-8\' payload_str = payload.decode(charset, errors=\'replace\') if payload else \'\' parts_list.append({ \'content_type\': content_type, \'payload\': payload_str }) return { \'is_multipart\': is_multipart, \'parts\': parts_list }"},{"question":"# Python Logging Assessment Question Objective The objective of this coding assessment is to assess your understanding of the Python `logging` module. You will need to demonstrate your ability to configure logger objects, handlers, filters, and formatters to produce specific logging outputs. Task Write a Python script that performs the following: 1. **Logger Configuration**: - Create a logger named `myapp`. - The logger should have a logging level of `DEBUG`. 2. **Handler Configuration**: - Add a `StreamHandler` to the logger that outputs logs to `sys.stdout`. - Add a `FileHandler` to the logger that outputs logs to a file named `app.log`. - The `StreamHandler` should have a logging level of `WARNING`. - The `FileHandler` should have a logging level of `DEBUG`. 3. **Formatter Configuration**: - Configure a formatter for the `StreamHandler` to display the log level, logger name, and message. - Configure a formatter for the `FileHandler` to display the date, log level, logger name, and message. 4. **Filter Configuration**: - Create a custom filter that only allows log messages starting with the word \\"Important\\". - Add this filter to the `FileHandler`. 5. **Logging Messages**: - Log messages at various levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) using the `myapp` logger. Expected Output 1. Run the script and observe the outputs: - On the console (`sys.stdout`), you should see messages with levels `WARNING`, `ERROR`, and `CRITICAL`, formatted to display the log level, logger name, and message. - In the `app.log` file, you should see all log messages, but only those starting with \\"Important\\" will be recorded. The log entries should include the date, log level, logger name, and message. Constraints - Ensure that no duplicate log records are present on the console or the file. - Use the configurations provided to filter and format log messages correctly. - Avoid using global configurations like `basicConfig()`; instead, configure the logger, handlers, filters, and formatters directly. Performance Requirements - The solution should be efficient in handling large volumes of log messages without significant performance degradation. Example Code Structure ```python import logging import sys # Step 1: Logger Configuration logger = logging.getLogger(\'myapp\') logger.setLevel(logging.DEBUG) # Step 2: Handler Configuration stream_handler = logging.StreamHandler(sys.stdout) file_handler = logging.FileHandler(\'app.log\') stream_handler.setLevel(logging.WARNING) file_handler.setLevel(logging.DEBUG) # Step 3: Formatter Configuration console_format = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s\') file_format = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - %(message)s\') stream_handler.setFormatter(console_format) file_handler.setFormatter(file_format) # Step 4: Filter Configuration class ImportantFilter(logging.Filter): def filter(self, record): return record.getMessage().startswith(\\"Important\\") file_handler.addFilter(ImportantFilter()) # Adding handlers to logger logger.addHandler(stream_handler) logger.addHandler(file_handler) # Step 5: Logging Messages logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") logger.info(\\"Important: This is a filtered info message\\") logger.error(\\"Important: This is a filtered error message\\") ``` **Note**: Your solution should include the full implementation following the provided outline.","solution":"import logging import sys # Step 1: Logger Configuration logger = logging.getLogger(\'myapp\') logger.setLevel(logging.DEBUG) # Step 2: Handler Configuration stream_handler = logging.StreamHandler(sys.stdout) file_handler = logging.FileHandler(\'app.log\') stream_handler.setLevel(logging.WARNING) file_handler.setLevel(logging.DEBUG) # Step 3: Formatter Configuration console_format = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s\') file_format = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - %(message)s\') stream_handler.setFormatter(console_format) file_handler.setFormatter(file_format) # Step 4: Filter Configuration class ImportantFilter(logging.Filter): def filter(self, record): return record.getMessage().startswith(\\"Important\\") file_handler.addFilter(ImportantFilter()) # Adding handlers to logger logger.addHandler(stream_handler) logger.addHandler(file_handler) # Step 5: Logging Messages logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") logger.info(\\"Important: This is a filtered info message\\") logger.error(\\"Important: This is a filtered error message\\")"},{"question":"**Title: Asynchronous Task Execution with Synchronization Primitives** **Objective:** Write a Python program using the `asyncio` library that demonstrates the management and synchronization of multiple asynchronous tasks. You will implement a function that processes a list of URLs, fetching data from them concurrently while ensuring that a maximum of 3 tasks are run concurrently at any given time. **Task:** 1. Define an asynchronous function `fetch_url(url: str) -> str` that simulates fetching data from a URL by sleeping for a random short duration (between 1 and 3 seconds) and then returning a message indicating the data was retrieved from the given URL. 2. Implement the main asynchronous function `process_urls(urls: List[str]) -> List[str]`: - Use a semaphore to limit the number of concurrent fetch operations to 3. - Create and gather tasks to fetch data from the provided list of URLs concurrently. - Collect and return the results of all fetch operations as a list of strings. **Specifications:** - **Input:** - `urls` - A list of URL strings. - **Output:** - A list of strings where each string is a message indicating the data was retrieved from a specific URL. - **Constraints:** - Use `asyncio.Semaphore` to limit the number of concurrent tasks. - Implement the `fetch_url` function to simulate network delay using `asyncio.sleep`. **Example:** ```python import asyncio import random from typing import List async def fetch_url(url: str) -> str: await asyncio.sleep(random.randint(1, 3)) return f\\"Data retrieved from {url}\\" async def process_urls(urls: List[str]) -> List[str]: semaphore = asyncio.Semaphore(3) async def sem_fetch(url): async with semaphore: return await fetch_url(url) tasks = [sem_fetch(url) for url in urls] return await asyncio.gather(*tasks) # To test the implementation async def main(): urls = [ \\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\", \\"http://example.com/4\\", \\"http://example.com/5\\" ] results = await process_urls(urls) for result in results: print(result) # Run the main function asyncio.run(main()) ``` **Bonus:** - Write unit tests for `fetch_url` and `process_urls` functions. - Modify the `fetch_url` function to raise an exception for a URL based on some condition, and handle it gracefully within the `process_urls` function. This question assesses your ability to: - Utilize `asyncio` to write asynchronous functions in Python. - Manage concurrency using synchronization primitives like semaphores. - Collect and handle results from multiple concurrent tasks.","solution":"import asyncio import random from typing import List async def fetch_url(url: str) -> str: await asyncio.sleep(random.randint(1, 3)) return f\\"Data retrieved from {url}\\" async def process_urls(urls: List[str]) -> List[str]: semaphore = asyncio.Semaphore(3) async def sem_fetch(url): async with semaphore: return await fetch_url(url) tasks = [sem_fetch(url) for url in urls] return await asyncio.gather(*tasks) # To test the implementation outside of unit tests (manual testing) async def main(): urls = [ \\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\", \\"http://example.com/4\\", \\"http://example.com/5\\" ] results = await process_urls(urls) for result in results: print(result) # Run the main function asyncio.run(main())"},{"question":"# Python Coding Assessment Question **Objective:** To assess your understanding of Python\'s `codecs` module and its various functionalities for encoding and decoding. # Problem Statement You are required to implement a custom encoding and decoding utility that handles different error schemes and supports both incremental and stream-based encoding/decoding. # Task 1. **Function Implementation:** - **`custom_encode(input_text: str, encoding: str, errors: str = \'strict\') -> bytes`** - Encodes the given `input_text` using the specified `encoding` and error handling scheme (`errors`). - Returns the encoded byte string. - **`custom_decode(input_bytes: bytes, encoding: str, errors: str = \'strict\') -> str`** - Decodes the given `input_bytes` using the specified `encoding` and error handling scheme (`errors`). - Returns the decoded text. 2. **Stream-Based Encoding/Decoding:** - **`stream_encode(input_text: str, encoding: str, errors: str = \'strict\') -> None`** - Writes the encoded text to a file named `encoded_output.txt` using `StreamWriter`. - **`stream_decode(file_path: str, encoding: str, errors: str = \'strict\') -> str`** - Reads the encoded content from the specified file (`file_path`) using `StreamReader`. - Returns the decoded text. 3. **Custom Error Handler:** - Implement and register a custom error handler named `\'custom_replace\'` that replaces encoding/decoding errors with the string `\'<ERROR>\'`. # Constraints 1. **Input Text:** - Can be any valid UTF-8 string. 2. **Input Bytes:** - Can be any valid byte sequence. 3. **Error Schemes:** - Use predefined error schemes such as `\'strict\'`, `\'ignore\'`, `\'replace\'`, `\'backslashreplace\'`, `\'custom_replace\'`. # Performance - Ensure the functions handle large text inputs efficiently. - Stream-based encoding/decoding should handle file I/O operations without running out of memory. # Example Usage ```python # Custom encoding and decoding example encoded_text = custom_encode(\\"Hello, World!\\", \\"utf-8\\", \\"strict\\") decoded_text = custom_decode(encoded_text, \\"utf-8\\", \\"strict\\") print(decoded_text) # Output: Hello, World! # Handling errors try: custom_encode(\\"Hello, World! uFFFF\\", \\"ascii\\", \\"custom_replace\\") except UnicodeEncodeError as e: print(e) # Should not raise, instead replace with \'<ERROR>\' # Stream-based encoding and decoding stream_encode(\\"Hello, World!\\", \\"utf-8\\", \\"strict\\") decoded_stream_text = stream_decode(\\"encoded_output.txt\\", \\"utf-8\\", \\"strict\\") print(decoded_stream_text) # Output: Hello, World! ``` Implement the functions and the custom error handler as described above.","solution":"import codecs def custom_encode(input_text: str, encoding: str, errors: str = \'strict\') -> bytes: Encodes the given input text using the specified encoding and error handling scheme. :param input_text: The text to encode. :param encoding: The encoding to use. :param errors: The error handling scheme. :return: Encoded byte string. return input_text.encode(encoding, errors) def custom_decode(input_bytes: bytes, encoding: str, errors: str = \'strict\') -> str: Decodes the given input bytes using the specified encoding and error handling scheme. :param input_bytes: The bytes to decode. :param encoding: The encoding to use. :param errors: The error handling scheme. :return: Decoded text. return input_bytes.decode(encoding, errors) def stream_encode(input_text: str, encoding: str, errors: str = \'strict\') -> None: Writes the encoded text to a file named encoded_output.txt using StreamWriter. :param input_text: The text to encode and write to the file. :param encoding: The encoding to use. :param errors: The error handling scheme. with codecs.open(\'encoded_output.txt\', \'w\', encoding, errors=errors) as f: f.write(input_text) def stream_decode(file_path: str, encoding: str, errors: str = \'strict\') -> str: Reads the encoded content from the specified file using StreamReader. :param file_path: The path to the file containing the encoded text. :param encoding: The encoding to use. :param errors: The error handling scheme. :return: Decoded text. with codecs.open(file_path, \'r\', encoding, errors=errors) as f: return f.read() def custom_replace_error_handler(exception): Custom error handler that replaces encoding/decoding errors with the string \'<ERROR>\'. :param exception: The encoding/decoding error. :return: A tuple with the replacement string and the position to continue. return (\'<ERROR>\', exception.start + 1) # Register the custom error handler codecs.register_error(\'custom_replace\', custom_replace_error_handler)"},{"question":"# Coding Challenge: Implementing a Custom MutableSequence Objective: Create a custom class `CustomDeque` that behaves like a double-ended queue, meaning it allows for appending and popping from both ends efficiently. This class should inherit from the `collections.abc.MutableSequence` abstract base class and implement all required methods. Requirements: 1. Inherit from `collections.abc.MutableSequence`. 2. Implement all required abstract methods (`__getitem__`, `__setitem__`, `__delitem__`, `__len__`, `insert`). 3. Ensure that your implementation supports efficient operations for adding and removing elements from both ends. Class Specification: - Class Name: `CustomDeque` - Methods to Implement: - `__getitem__(self, index)`: Retrieve the item at the specified index. - `__setitem__(self, index, value)`: Set the item at the specified index to the given value. - `__delitem__(self, index)`: Delete the item at the specified index. - `__len__(self)`: Return the number of items. - `insert(self, index, value)`: Insert the value at the specified index. - Additionally, implement `append` and `appendleft` methods for adding elements at the end and beginning respectively. - Implement `pop` and `popleft` methods for removing elements from the end and beginning respectively. Input: - Instantiation of the `CustomDeque` class. - Method calls such as `append`, `appendleft`, `pop`, `popleft`, and indexing operations. Output: - Return values from the method calls should be as expected for a double-ended queue. Example Usage: ```python from collections.abc import MutableSequence class CustomDeque(MutableSequence): def __init__(self): # Initialize with an empty list or a more sophisticated structure if needed self._data = [] def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __len__(self): return len(self._data) def insert(self, index, value): self._data.insert(index, value) def append(self, value): self._data.append(value) def appendleft(self, value): self._data.insert(0, value) def pop(self): return self._data.pop() def popleft(self): return self._data.pop(0) # Example of usage dq = CustomDeque() dq.append(1) dq.appendleft(0) dq.append(2) print(dq.pop()) # Output: 2 print(dq.popleft()) # Output: 0 ``` Constraints: - Ensure that the implementation remains efficient, especially for operations involving adding/removing elements from both ends. - You are not allowed to use Python\'s built-in `collections.deque` for managing the internal data structure.","solution":"from collections.abc import MutableSequence class CustomDeque(MutableSequence): def __init__(self): self._data = [] def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __len__(self): return len(self._data) def insert(self, index, value): self._data.insert(index, value) def append(self, value): self._data.append(value) def appendleft(self, value): self._data.insert(0, value) def pop(self): return self._data.pop() def popleft(self): return self._data.pop(0)"},{"question":"**Question: Titanic Dataset Analysis Using Seaborn** The Titanic dataset from seaborn contains data about the passengers of the Titanic. Your task is to analyze and visualize this data using seaborn\'s boxplot functionality. # Requirements 1. **Load the Titanic dataset and set seaborn\'s theme to \\"whitegrid\\".** 2. **Create a single horizontal boxplot of passengers\' ages.** 3. **Create a grouped boxplot with age along the x-axis and passenger class (\\"class\\") along the y-axis. Color the boxes by survival status (\\"alive\\").** 4. **Create another grouped boxplot with deck (\\"deck\\") along the y-axis and age along the x-axis. Customize the color and line properties of the box plots using matplotlib parameters as follows:** - Notch the boxes. - Do not show caps. - Use \'x\' markers for fliers. - Use a RGBA tuple (0.4, 0.6, 0.8, 0.6) for the box color. - Set the median line to be red with a linewidth of 2. # Specifications - **Function Name**: `titanic_boxplots` - **Input**: None - **Output**: The function should plot three boxplots accordingly. # Code Template ```python import seaborn as sns def titanic_boxplots(): # Load dataset and set theme titanic = sns.load_dataset(\\"titanic\\") sns.set_theme(style=\\"whitegrid\\") # Single horizontal boxplot of ages sns.boxplot(x=titanic[\\"age\\"]) # Grouped box plot by class and survival status sns.boxplot(data=titanic, x=\\"age\\", y=\\"class\\", hue=\\"alive\\") # Customizable grouped box plot by deck sns.boxplot( data=titanic, x=\\"age\\", y=\\"deck\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (0.4, 0.6, 0.8, 0.6)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2} ) # Run the function to display the plots titanic_boxplots() ``` **Constraints:** - Ensure seaborn and matplotlib packages are installed in your environment to avoid import errors. # Notes: - This task will help you understand basic and advanced plotting techniques using seaborn and customize plots using underlying matplotlib parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_boxplots(): # Load dataset and set theme titanic = sns.load_dataset(\\"titanic\\") sns.set_theme(style=\\"whitegrid\\") # Single horizontal boxplot of ages plt.figure(figsize=(12, 6)) sns.boxplot(x=titanic[\\"age\\"]) plt.title(\\"Boxplot of Passengers\' Ages\\") plt.show() # Grouped box plot by class and survival status plt.figure(figsize=(12, 6)) sns.boxplot(data=titanic, x=\\"age\\", y=\\"class\\", hue=\\"alive\\") plt.title(\\"Boxplot of Age by Class and Survival Status\\") plt.show() # Customizable grouped box plot by deck plt.figure(figsize=(12, 6)) sns.boxplot( data=titanic, x=\\"age\\", y=\\"deck\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (0.4, 0.6, 0.8, 0.6)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2} ) plt.title(\\"Boxplot of Age by Deck\\") plt.show() # Run the function to display the plots titanic_boxplots()"},{"question":"**Question: Analyze and Visualize the Penguin Dataset** You are provided with the `seaborn` library and the `penguins` dataset. Your task is to analyze and visualize the dataset in specific ways to demonstrate your understanding of `seaborn` functionalities. 1. **Loading the Dataset and Setting Theme**: - Load the `penguins` dataset from the `seaborn` library and set the default theme for the plots. 2. **Distribution Plot**: - Create a distribution plot of the `flipper_length_mm` for different `species`. - Use the `hue` parameter to distinguish between different `species`. - Move the legend to the `upper left` corner and set it to be displayed outside of the plot area with a box anchor at position `(1.05, 1)`. 3. **Facet Grid Plot**: - Create a `FacetGrid` plot that shows the distribution of `body_mass_g` across different `island`s and distinguishes `species` using the `hue` parameter. - Arrange the facets in a 2-column layout. - Move the legend to `lower center`, positioned inside the plot area using the `bbox_to_anchor` parameter with coordinates `(0.5, -0.1)`. - Ensure the legend does not have a frame (`frameon=False`), and has no title. # Input: - No input required from users. # Output: Generate the described plots with correct legend placements and themed aesthetics using the `seaborn` library. Be sure to: - Follow the instructions closely to adjust legends and plot aesthetics. - Demonstrate clarity and proper commenting in your code. # Code Implementation: ```python import seaborn as sns # 1. Load dataset and set theme sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # 2. Distribution Plot ax = sns.histplot(penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") sns.move_legend(ax, \\"upper left\\", bbox_to_anchor=(1.05, 1)) # 3. Facet Grid Plot g = sns.displot( penguins, x=\\"body_mass_g\\", hue=\\"species\\", col=\\"island\\", col_wrap=2 ) sns.move_legend(g, \\"lower center\\", bbox_to_anchor=(0.5, -0.1), frameon=False, title=None) ``` # Note: Make sure your plots are correctly labelled and aesthetically pleasing.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_penguins(): # 1. Load the dataset and set the theme sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # 2. Distribution Plot of Flipper Length by Species plt.figure(figsize=(10,6)) ax = sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") plt.legend(loc=\'upper left\', bbox_to_anchor=(1.05, 1)) plt.title(\'Distribution of Flipper Length by Species\') plt.show() # 3. Facet Grid Plot of Body Mass by Island and Species g = sns.FacetGrid(penguins, col=\\"island\\", hue=\\"species\\", col_wrap=2, height=5) g.map(sns.histplot, \\"body_mass_g\\").add_legend(title=None, frameon=False, bbox_to_anchor=(0.5, -0.1), loc=\\"lower center\\") g.set_titles(\\"{col_name} Island\\") plt.show() # Call the function to plot analyze_penguins()"},{"question":"Objective Given a dataset, use seaborn\'s `objects` interface to create and customize plots that count and display distinct observations. Task 1. Load the \\"titanic\\" dataset using seaborn. 2. Plot a bar chart that shows the count of passengers per `class` (1st, 2nd, 3rd Class). 3. Modify the plot from step 2 to show separate bars for `sex` within each `class` using dodging. 4. Create another plot that counts the number of passengers in each `embark_town`, but this time, count by `alone` (whether the passenger was alone or not). Expected Input and Output - Input: The \\"titanic\\" dataset provided by seaborn. - Output: Three plots generated using seaborn\'s `objects` interface. Constraints - You must use seaborn\'s `so.Plot` and associated methods to create the plots. - Ensure the plots are clearly labeled and easy to interpret. Example Here is a brief example demonstrating loading the dataset and creating a basic plot (not the full solution): ```python import seaborn.objects as so from seaborn import load_dataset # Load the Titanic dataset titanic = load_dataset(\\"titanic\\") # Example: Plotting count of passengers per class plot = so.Plot(titanic, x=\\"class\\").add(so.Bar(), so.Count()) plot.show() ``` Now, implement the full solution as specified in the task.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_data(): # Load the Titanic dataset titanic = load_dataset(\\"titanic\\") # Plot 1: Count of passengers per class plot1 = so.Plot(titanic, x=\\"class\\").add(so.Bar(), so.Count()).label(title=\\"Count of passengers per class\\", xlabel=\\"Class\\", ylabel=\\"Passenger Count\\") plot1.show() # Plot 2: Count of passengers per class with separate bars for sex plot2 = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), dodge=True).label(title=\\"Count of passengers per class and sex\\", xlabel=\\"Class\\", ylabel=\\"Passenger Count\\") plot2.show() # Plot 3: Count of passengers in each embark_town by alone plot3 = so.Plot(titanic, x=\\"embark_town\\", color=\\"alone\\").add(so.Bar(), so.Count(), dodge=True).label(title=\\"Count of passengers per embark town by alone status\\", xlabel=\\"Embark Town\\", ylabel=\\"Passenger Count\\") plot3.show() return plot1, plot2, plot3"},{"question":"# Advanced String Formatting and Parsing Your task is to implement a custom string formatter that extends the functionalities provided by Pythonâ€™s built-in `string.Format` class. # Objective: Create a class `AdvancedFormatter` that inherits from `string.Formatter` and adds the capability to format strings with additional custom tags. Specifically, the class should be able to handle: 1. Formatting fields with a custom tag `{{type}}` that allows the specification of a custom type conversion (like converting a field to upper or lower case). 2. Parsing and replacing custom tags in the format string with the appropriate field values. You will implement the following class: ```python import string class AdvancedFormatter(string.Formatter): def parse(self, format_string): # Implement custom parsing logic to handle the additional tags pass def format_field(self, value, format_spec): # Implement custom formatting logic to process the custom tags correctly pass ``` # Your `AdvancedFormatter` should support: - **Uppercase and Lowercase Custom Tags**: Provide custom tags `{field:upper}` to convert a field value to uppercase and `{field:lower}` to convert it to lowercase. - **Number Formatting**: Keep all the standard formatting functionalities for numbers in Python intact. # Example Usage: ```python formatter = AdvancedFormatter() # Test with custom uppercase and lowercase conversions formatted_string = formatter.format(\\"Hello {0:upper}, this is a {1:lower} example!\\", \\"WORLD\\", \\"TESTING\\") print(formatted_string) # Expected Output: \\"Hello WORLD, this is a testing example!\\" ``` # Constraints: - You should not use external libraries (e.g., template engines) to achieve this. - Performance must be reasonable, with the assumption that the inputs do not exceed standard string lengths (e.g., less than 10,000 characters). # Input Format: - The `format` method of `AdvancedFormatter` will receive a format string followed by necessary positional and keyword arguments. # Output Format: - The method will return the formatted string with all custom tags processed and replaced properly. # Notes: - You may assume well-formed input strings for the purpose of this exercise. - Ensure to handle cases where the custom tags do not match any valid field name by raising appropriate exceptions. Implement the `AdvancedFormatter` class and ensure that it meets all the above specifications.","solution":"import string class AdvancedFormatter(string.Formatter): def parse(self, format_string): return super().parse(format_string) def format_field(self, value, format_spec): if format_spec.endswith(\'upper\'): return str(value).upper() elif format_spec.endswith(\'lower\'): return str(value).lower() else: return super().format_field(value, format_spec) # Example usage formatter = AdvancedFormatter() # Test with custom uppercase and lowercase conversions formatted_string = formatter.format(\\"Hello {0:upper}, this is a {1:lower} example!\\", \\"WORLD\\", \\"TESTING\\") print(formatted_string) # Expected Output: \\"Hello WORLD, this is a testing example!\\""},{"question":"# Custom Data Transmission Protocol Objective Create a custom data transmission protocol using the `asyncio` framework, demonstrating the use of both transports and protocols. Your implementation should include a server and a client that use this protocol to communicate. Requirements 1. **Custom Protocol Class** - Implement a class `CustomProtocol` based on `asyncio.Protocol`. - The class should: - Handle incoming data by printing it. - Append a specific suffix (e.g., \\" - Processed\\") to any data it receives and send it back to the client. - Close the connection after sending back the processed data. 2. **Server Implementation** - Create a server that listens on localhost and a specific port (e.g., 8888). - Use `loop.create_server()` to initiate the server with the `CustomProtocol`. - Keep the server running indefinitely. 3. **Client Implementation** - Create a client that connects to the server. - Send a specific message to the server (e.g., \\"Hello, Server!\\"). - Print the response received from the server. Input and Output - The server should accept and handle incoming connections. - The client should send a message to the server and print the modified response. Constraints - You must use the `asyncio` framework. - The server and client must both use the `CustomProtocol`. Example Hereâ€™s an example of the expected output from the client when it sends the message `Hello, Server!` to the server: ``` Data received: Hello, Server! Processed data sent: Hello, Server! - Processed Connection closed ``` Instructions 1. Implement the `CustomProtocol` class as described. 2. Implement the server and client using asyncio event loop methods. 3. Ensure both server and client are correctly handling the connections and data transmission as specified. Deliverables - `CustomProtocol` class implementation. - Server script. - Client script. # Submission Submit your code for the `CustomProtocol`, the server script, and the client script. Make sure everything is encapsulated in a single Python file or a set of files necessary to run the server and client.","solution":"import asyncio class CustomProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") processed_message = message + \\" - Processed\\" print(f\\"Processed data sent: {processed_message}\\") self.transport.write(processed_message.encode()) self.transport.close() async def run_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: CustomProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() async def run_client(): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() def connection_lost(exc): on_con_lost.set_result(True) class ClientProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.transport.write(\'Hello, Server!\'.encode()) def data_received(self, data): print(f\\"Client received: {data.decode()}\\") self.transport.close() def connection_lost(self, exc): on_con_lost.set_result(True) transport, protocol = await loop.create_connection( lambda: ClientProtocol(), \'127.0.0.1\', 8888 ) try: await on_con_lost finally: transport.close()"},{"question":"Objective Your task is to demonstrate your understanding of Seaborn\'s `husl_palette` function by creating various color palettes, modifying their parameters, and applying them to different visualizations. Question 1. **Generate Color Palettes**: - a. Create a default HUSL palette with 6 colors and print it. - b. Create a HUSL palette with 8 colors and print it. - c. Create a HUSL palette with a lightness of 0.5, saturation of 0.5, and hue starting point of 0.5. Print the resulting palette. - d. Create a continuous colormap from the HUSL palette and print it. 2. **Visualize with Color Palettes**: - a. Use the default HUSL palette from step 1a to color a bar plot of a dataset. - b. Use the continuous colormap from step 1d to create a heatmap of a dataset. Dataset You can use the built-in `tips` dataset available in Seaborn for visualization. Load it using `sns.load_dataset(\\"tips\\")`. Input and Output Formats 1. For generating color palettes, your function should return and print the palettes. 2. For visualizations: - Plot a bar plot of the total bill for each day of the week using the default palette. - Plot a heatmap of the correlation matrix of the `tips` dataset using the continuous colormap. Constraints - Use the Seaborn library for all visualizations. - Ensure your code is clean, well-documented, and clearly highlights the different components of the task. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Part 1: Generate Color Palettes # a. palette_default = sns.husl_palette() print(palette_default) # b. palette_8 = sns.husl_palette(8) print(palette_8) # c. palette_custom = sns.husl_palette(l=0.5, s=0.5, h=0.5) print(palette_custom) # d. colormap_continuous = sns.husl_palette(as_cmap=True) print(colormap_continuous) # Load dataset data = sns.load_dataset(\\"tips\\") # Part 2: Visualize with Color Palettes # a. Bar plot with default palette sns.barplot(x=\'day\', y=\'total_bill\', palette=palette_default, data=data) plt.show() # b. Heatmap with continuous colormap sns.heatmap(data.corr(), cmap=colormap_continuous) plt.show() ``` Ensure you address each part of the task to demonstrate your comprehensive understanding of the `husl_palette` function and its applications in Seaborn.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_and_print_palettes(): # a. Create a default HUSL palette with 6 colors and print it palette_default = sns.husl_palette() print(\\"Default HUSL palette (6 colors):\\", palette_default) # b. Create a HUSL palette with 8 colors and print it palette_8 = sns.husl_palette(8) print(\\"HUSL palette with 8 colors:\\", palette_8) # c. Create a HUSL palette with a lightness of 0.5, saturation of 0.5, and hue starting point of 0.5 palette_custom = sns.husl_palette(n_colors=6, l=0.5, s=0.5, h=0.5) print(\\"Custom HUSL palette (l=0.5, s=0.5, h=0.5):\\", palette_custom) # d. Create a continuous colormap from the HUSL palette colormap_continuous = sns.husl_palette(256, as_cmap=True) print(\\"Continuous HUSL colormap:\\", colormap_continuous) return palette_default, palette_8, palette_custom, colormap_continuous def visualize_palettes(): palette_default, palette_8, palette_custom, colormap_continuous = generate_and_print_palettes() # Load dataset data = sns.load_dataset(\\"tips\\") # a. Bar plot with default palette plt.figure(figsize=(10, 6)) sns.barplot(x=\'day\', y=\'total_bill\', palette=palette_default, data=data) plt.title(\\"Bar plot of total bill for each day of the week using default HUSL palette\\") plt.show() # b. Heatmap with continuous colormap plt.figure(figsize=(10, 6)) sns.heatmap(data.corr(), cmap=colormap_continuous, annot=True) plt.title(\\"Heatmap of correlation matrix of tips dataset using continuous HUSL colormap\\") plt.show()"},{"question":"# Command-line Gym Planner You are required to create a command-line tool in Python using the `argparse` module that helps users plan their gym workout sessions. The tool will allow users to specify the workout they intend to do, along with other details such as duration, intensity, and whether they want a summary or detailed output. Requirements 1. **Positional Arguments**: - `workout_type`: (str) The type of workout (e.g., `cardio`, `strength`, `flexibility`). - `duration`: (int) The duration of the workout in minutes. 2. **Optional Arguments**: - `-i` or `--intensity`: (str, choices={\\"low\\", \\"medium\\", \\"high\\"}) The intensity of the workout. Default is `medium`. - `-s` or `--summary`: (store_true) If specified, provide a summary output. - `-d` or `--detailed`: (store_true) If specified, provide a detailed output. - `-h` or `--help`: Show the help message and exit. 3. **Mutually Exclusive Arguments**: - `-s` or `--summary` and `-d` or `--detailed` cannot be used together. 4. **Output**: - By default, print the workout type, duration, and intensity. - If `--summary` is provided, print a brief summary of the workout. - If `--detailed` is provided, print a detailed plan of the workout. Constraints - The `duration` must be a positive integer. - `workout_type` should be a non-empty string. Examples ```shell python gym_planner.py cardio 30 --intensity high Workout Type: cardio Duration: 30 minutes Intensity: high python gym_planner.py strength 45 -s Summary: Strength workout for 45 minutes at medium intensity. python gym_planner.py flexibility 60 -d --intensity low Detailed Plan: - Workout Type: flexibility - Duration: 60 minutes - Intensity: low - Plan: Include stretching and breathing exercises focusing on flexibility. ``` Implementation - Implement the command-line tool in a Python script named `gym_planner.py`. - Use the `argparse` module to handle the arguments as per the specifications. Evaluation - The solution should correctly handle and parse command-line arguments. - The output must be accurate and formatted as specified. - The `argparse` module should be used efficiently. - Error handling for invalid inputs should be implemented.","solution":"import argparse def create_parser(): parser = argparse.ArgumentParser(description=\\"Plan your gym workout session.\\") parser.add_argument(\'workout_type\', type=str, help=\'The type of workout (e.g., cardio, strength, flexibility)\') parser.add_argument(\'duration\', type=int, help=\'The duration of the workout in minutes\') parser.add_argument(\'-i\', \'--intensity\', type=str, choices=[\'low\', \'medium\', \'high\'], default=\'medium\', help=\'The intensity of the workout\') group = parser.add_mutually_exclusive_group() group.add_argument(\'-s\', \'--summary\', action=\'store_true\', help=\'Provide a summary output\') group.add_argument(\'-d\', \'--detailed\', action=\'store_true\', help=\'Provide a detailed output\') return parser def gym_planner(args): if args.duration <= 0: raise ValueError(\\"Duration must be a positive integer.\\") if args.summary: print(f\\"Summary: {args.workout_type.capitalize()} workout for {args.duration} minutes at {args.intensity} intensity.\\") elif args.detailed: print(f\\"Detailed Plan:n- Workout Type: {args.workout_type}n- Duration: {args.duration} minutesn- Intensity: {args.intensity}n- Plan: Include specific exercises focusing on {args.workout_type}.\\") else: print(f\\"Workout Type: {args.workout_type}nDuration: {args.duration} minutesnIntensity: {args.intensity}\\") def main(): parser = create_parser() args = parser.parse_args() gym_planner(args) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Custom Neural Network Module with Dynamic Layer Control and Training You are tasked to create a custom neural network module using PyTorch leveraging the module composition capabilities described in the documentation. Requirements: 1. **Custom Linear Layer**: Create a custom linear layer `CustomLinear` that inherits from `torch.nn.Module`. - It should have `in_features` and `out_features` as parameters. - Use `torch.nn.Parameter` for weight and bias. - Implement the `forward` method performing affine transformation. 2. **Dynamic Neural Network**: Create a `DynamicNet` class that: - Inherits from `torch.nn.Module`. - Accepts a list of layer specifications, where each specification defines the number of input and output features for a `CustomLinear` layer. - Dynamically creates these layers using `torch.nn.ModuleList`. - Uses ReLU activation function between layers. - Has a `forward` method that computes the output sequentially through all layers. 3. **Training Loop**: Develop a training function `train_model` that: - Accepts the `DynamicNet` instance, a torch Dataset DataLoader, a loss function, and an optimizer. - Trains the network on the provided data for a specified number of epochs. - Returns the trained model. 4. **State Management**: Implement functionality to save and load the trained model\'s state dictionary. 5. **Hooks**: Add forward and backward hooks to `DynamicNet` to: - Log the input before entering the `forward` method of each layer. - Log the output after the `forward` method of each layer. - Log gradient information during the backward pass. Inputs and Outputs: - **CustomLinear** - **Input**: `in_features` (int), `out_features` (int) - **Output**: Affine transformation of the input tensor - **DynamicNet** - **Input**: List of tuples specifying layer configuration, e.g., `[(in1, out1), (in2, out2), ...]` - **Output**: Final output tensor after passing through dynamically created layers and activations - **train_model** - **Input**: `DynamicNet` instance, DataLoader, optimizer, loss function - **Output**: Trained model - **State Management** - Save model state to a file - Load model state from a file Implementation Requirements: 1. **CustomLinear**: ```python import torch from torch import nn class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input): return input @ self.weight + self.bias ``` 2. **DynamicNet**: ```python class DynamicNet(nn.Module): def __init__(self, layers): super(DynamicNet, self).__init__() self.layers = nn.ModuleList() for (in_features, out_features) in layers: self.layers.append(CustomLinear(in_features, out_features)) def forward(self, x): for layer in self.layers: x = layer(x) x = torch.relu(x) return x ``` 3. **Training Function**: ```python def train_model(model, dataloader, loss_fn, optimizer, epochs=5): for epoch in range(epochs): for batch in dataloader: inputs, targets = batch optimizer.zero_grad() outputs = model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() return model ``` 4. **State Management**: ```python def save_model_state(model, path): torch.save(model.state_dict(), path) def load_model_state(model, path): model.load_state_dict(torch.load(path)) ``` 5. **Hooks**: ```python def forward_pre_hook(module, inputs): print(f\\"Entering {module} with input {inputs}\\") def forward_hook(module, inputs, output): print(f\\"Exiting {module} with output {output}\\") def backward_hook(module, grad_inputs, grad_outputs): print(f\\"Backward in {module} with grad_inputs {grad_inputs} and grad_outputs {grad_outputs}\\") # Example of attaching hooks model = DynamicNet([(4, 3), (3, 2), (2, 1)]) for layer in model.layers: layer.register_forward_pre_hook(forward_pre_hook) layer.register_forward_hook(forward_hook) layer.register_backward_hook(backward_hook) ``` Notes: - Make sure to test your implementation with various layer configurations. - Ensure your model handles both CPU and GPU devices (use `torch.device`). - Validate hooks by printing log outputs during the forward and backward pass.","solution":"import torch from torch import nn class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input): return input @ self.weight + self.bias class DynamicNet(nn.Module): def __init__(self, layers): super(DynamicNet, self).__init__() self.layers = nn.ModuleList() for (in_features, out_features) in layers: self.layers.append(CustomLinear(in_features, out_features)) def forward(self, x): for layer in self.layers: x = layer(x) x = torch.relu(x) return x def train_model(model, dataloader, loss_fn, optimizer, epochs=5): model.train() for epoch in range(epochs): for inputs, targets in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() return model def save_model_state(model, path): torch.save(model.state_dict(), path) def load_model_state(model, path): model.load_state_dict(torch.load(path)) def forward_pre_hook(module, inputs): print(f\\"Entering {module} with input {inputs}\\") def forward_hook(module, inputs, output): print(f\\"Exiting {module} with output {output}\\") def backward_hook(module, grad_inputs, grad_outputs): print(f\\"Backward in {module} with grad_inputs {grad_inputs} and grad_outputs {grad_outputs}\\")"},{"question":"# Question: Implement PCA on the Iris Dataset Objective: Your task is to write a Python script that loads the Iris dataset, performs Principal Component Analysis (PCA) to reduce the dataset to 2 dimensions, and visualizes the resulting components. You will also need to explain how much variance is retained by using these 2 principal components. Instructions: 1. **Load the Iris dataset** from scikit-learn\'s datasets module. 2. **Implement PCA** to reduce the dataset to 2 dimensions. 3. **Transform the data** using the trained PCA model. 4. **Visualize the transformed data** in a scatter plot where the x-axis corresponds to the first principal component and the y-axis corresponds to the second principal component. Use different colors for different Iris species. 5. **Calculate and output the cumulative explained variance** by the two principal components. Requirements: - Import the necessary modules from `sklearn`, `matplotlib`, and `numpy`. - Use `PCA` from `sklearn.decomposition`. - Ensure the script runs without errors and produces the required visualization and outputs. Input and Output: - There are no user inputs. The dataset should be loaded within the script. - Output should include: - A scatter plot of the 2 principal components. - The cumulative explained variance by the two principal components. Constraints: - Use a maximum of 2 principal components. - Ensure that the code is written clearly and is well-documented. Performance Considerations: - The task should execute efficiently on a standard personal computer. Example: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler # Load the Iris dataset data = load_iris() X = data.data y = data.target species = data.target_names # Standardize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Implement PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Calculate cumulative explained variance explained_variance = pca.explained_variance_ratio_ cumulative_explained_variance = np.sum(explained_variance) print(f\'Cumulative Explained Variance by the first 2 components: {cumulative_explained_variance:.2f}\') # Visualize the transformed data plt.figure(figsize=(8, 6)) for i, target_name in enumerate(species): plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=target_name) plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.title(\'PCA of Iris Dataset\') plt.legend() plt.show() ``` Explanation: - The script first loads and scales the Iris dataset. - PCA is performed to reduce the data to 2 dimensions. - The transformed data is then plotted, showing how the different species are distributed in the new 2-dimensional space. - The cumulative explained variance is calculated and printed, indicating how much of the original variance is retained.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler def perform_pca_iris(): # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target target_names = iris.target_names # Standardize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Implement PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Calculate cumulative explained variance explained_variance = pca.explained_variance_ratio_ cumulative_explained_variance = np.sum(explained_variance) print(f\'Cumulative Explained Variance by the first 2 components: {cumulative_explained_variance:.2f}\') # Visualize the transformed data plt.figure(figsize=(8, 6)) colors = [\'navy\', \'turquoise\', \'darkorange\'] lw = 2 for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'PCA of Iris Dataset\') plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.show() return cumulative_explained_variance, X_pca # Perform PCA on the Iris dataset and plot the results perform_pca_iris()"},{"question":"**Python Logging Configuration Assessment** # Objective: You are required to configure a custom logging setup using a dictionary schema and the `logging.config.dictConfig()` function from the `logging.config` module. # Problem Statement: 1. Implement a function `setup_logging(log_config: dict) -> None` that takes a dictionary, `log_config`, the schema of which follows the `dictConfig()` structure, and configures the logging module accordingly. 2. Implement a custom log handler: - Create a custom log handler class named `UpperCaseStreamHandler` that inherits from `logging.StreamHandler`. - Override the `emit` method to convert all log messages to uppercase before they are printed. # Input The input to the `setup_logging` function will be a dictionary `log_config` with keys defined as per the `logging.config.dictConfig()` schema. Here is an example configuration dictionary for reference: ```python { \\"version\\": 1, \\"formatters\\": { \\"simple\\": { \\"format\\": \\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\" } }, \\"handlers\\": { \\"console\\": { \\"class\\": \\"UpperCaseStreamHandler\\", \\"level\\": \\"DEBUG\\", \\"formatter\\": \\"simple\\" } }, \\"root\\": { \\"level\\": \\"DEBUG\\", \\"handlers\\": [\\"console\\"] } } ``` # Output The function does not return anything, but it sets up the logging configuration with specified handlers and formatters. # Constraints - The dictionary `log_config` will always follow the `dictConfig()` schema. - The `UpperCaseStreamHandler` should only print messages to the console in uppercase. # Function Signature ```python import logging from logging.config import dictConfig class UpperCaseStreamHandler(logging.StreamHandler): def emit(self, record: logging.LogRecord) -> None: record.msg = str(record.msg).upper() super().emit(record) def setup_logging(log_config: dict) -> None: dictConfig(log_config) ``` # Example Usage ```python import logging log_config = { \\"version\\": 1, \\"formatters\\": { \\"simple\\": { \\"format\\": \\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\" } }, \\"handlers\\": { \\"console\\": { \\"class\\": \\"UpperCaseStreamHandler\\", \\"level\\": \\"DEBUG\\", \\"formatter\\": \\"simple\\" } }, \\"root\\": { \\"level\\": \\"DEBUG\\", \\"handlers\\": [\\"console\\"] } } setup_logging(log_config) logger = logging.getLogger(__name__) logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") ``` In the example above, all the log messages should be printed in uppercase on the console.","solution":"import logging from logging.config import dictConfig class UpperCaseStreamHandler(logging.StreamHandler): def emit(self, record: logging.LogRecord) -> None: record.msg = str(record.msg).upper() super().emit(record) def setup_logging(log_config: dict) -> None: Configures logging based on the provided configuration dictionary. Args: log_config (dict): A dictionary that follows the logging `dictConfig` schema. dictConfig(log_config)"},{"question":"# Performance Profiling in Python You have been assigned the task of optimizing a Python program. To get started, you must identify the performance bottlenecks in the code using the `cProfile` profiler and then analyze the output using the `pstats` module. Your task involves writing code that not only profiles the program execution but also provides a detailed report of the performance statistics. The Program to Profile For this task, consider the following Python script, which processes a list of numbers: ```python import time def slow_function(numbers): time.sleep(1) return [n**2 for n in numbers] def main(): numbers = range(1000) squared_numbers = slow_function(numbers) print(squared_numbers[:10]) if __name__ == \\"__main__\\": main() ``` Objectives 1. **Profiling the Code**: Use `cProfile` to profile the `main` function. 2. **Saving Profile Data**: Save the profile data to a file. 3. **Analyzing the Data**: Use the `pstats` module to read the saved profile data and print a report that includes: - The total number of function calls. - The time spent in each function, both as a total and per call. - The cumulative time spent in each function (including time in sub-functions). 4. **Filtering and Sorting**: Sort the profiling statistics by the cumulative time spent in each function, and display only the top 5 functions contributing most to the cumulative time. Input There is no user input required for this task. The focus is on writing the profiling and analysis code. Output The output should include: - A saved profile data file from `cProfile`. - A printed report generated by `pstats`, sorted by cumulative time, showing only the top 5 functions along with the required details (total number of calls, total time, time per call, cumulative time). Constraints - You must use `cProfile` for profiling. - The results must be processed and displayed using `pstats`. Example Usage Your script should be executable and produce output similar to the following: ```plaintext 197 function calls (192 primitive calls) in 1.002 seconds Ordered by: cumulative time List reduced from 10 to 5 due to restriction <5> ncalls tottime percall cumtime percall filename:lineno(function) 1 1.002 1.002 1.002 1.002 <string>:1(<module>) 1 0.000 0.000 1.002 1.002 script_name.py:4(main) 1 1.001 1.001 1.001 1.001 script_name.py:1(slow_function) 1000 0.000 0.000 0.000 0.000 script_name.py:5(<listcomp>) 1 0.000 0.000 0.000 0.000 {built-in method builtins.print} ``` Deliverables Submit a single Python script that: 1. Profiles the provided code. 2. Saves the profiling data to a file. 3. Loads the profiling data using `pstats`. 4. Prints the profiling data sorted by cumulative time, showing only the top 5 functions.","solution":"import cProfile import pstats import time def slow_function(numbers): time.sleep(1) return [n**2 for n in numbers] def main(): numbers = range(1000) squared_numbers = slow_function(numbers) print(squared_numbers[:10]) if __name__ == \\"__main__\\": # Step 1: Profile the main function profiler = cProfile.Profile() profiler.enable() main() profiler.disable() # Step 2: Save the profile data to a file profiler.dump_stats(\'profile_data.prof\') # Step 3 & 4: Analyze the data with pstats stats = pstats.Stats(\'profile_data.prof\') stats.sort_stats(\'cumulative\').print_stats(5)"},{"question":"# Task You are required to design an entrypoint for a custom CNN model and publish it using PyTorch Hub. The goal is to demonstrate your understanding of setting up and loading a pre-trained model. Follow these detailed steps: 1. **Define the Custom Model:** Implement a simple Convolutional Neural Network (CNN) model class named `CustomCNN` using PyTorch\'s `nn.Module`. 2. **Create hubconf.py:** Set up `hubconf.py` to include: - The custom CNN model definition. - An entrypoint for the custom model that allows an option to load pre-trained weights. 3. **Pre-Trained Weights:** Provide a mechanism to load pre-trained weights into the model within the entrypoint function. For simplicity, you can save and load weights from the local disk. 4. **Implement `load_model` Function:** - Write a function `load_model` that uses `torch.hub.load` to fetch the model from `hubconf.py`. - Validate that the model is correctly loaded by printing the model architecture. 5. **Usage Example:** Provide a minimal example demonstrating how to load the custom model using the `load_model` function. # Inputs and Outputs - Define the `CustomCNN` model class with a simple architecture. - Set up `hubconf.py` with an entrypoint for the `CustomCNN` model. - Implement a function `load_model(repo_path, entrypoint)` where: - `repo_path` is the path to the GitHub repository. - `entrypoint` is the name of the entrypoint created. - Print the model architecture to verify the load. # Constraints - Use only PyTorch and torchvision packages. - Ensure the entrypoint function properly handles loading of pre-trained weights. # Example Code Snippets ```python # Custom model definition import torch import torch.nn as nn class CustomCNN(nn.Module): def __init__(self, num_classes=10): super(CustomCNN, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.fc = nn.Linear(32*16*16, num_classes) def forward(self, x): x = self.layer1(x) x = x.view(x.size(0), -1) x = self.fc(x) return x # hubconf.py definition dependencies = [\'torch\'] def custom_cnn(pretrained=False): model = CustomCNN() if pretrained: # Assuming weights are saved in the same directory for illustration model.load_state_dict(torch.load(\'custom_cnn_pretrained.pth\')) return model # Function to load model using Pytorch Hub def load_model(repo_path, entrypoint): model = torch.hub.load(repo_path, entrypoint) print(model) return model # Example of using the load_model function # Note: Replace \'repo_path\' with actual path where hubconf.py is located model = load_model(\'repo_path\', \'custom_cnn\') ``` This question assesses the student\'s knowledge on model definition, saving and loading weights, and utilizing PyTorch Hub for model distribution and loading.","solution":"import torch import torch.nn as nn class CustomCNN(nn.Module): def __init__(self, num_classes=10): super(CustomCNN, self).__init__() self.layer1 = nn.Sequential( nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) self.fc = nn.Linear(32*16*16, num_classes) def forward(self, x): x = self.layer1(x) x = x.view(x.size(0), -1) x = self.fc(x) return x"},{"question":"Objective: To assess your understanding of PyTorch\'s data loading utilities, your task is to implement a custom PyTorch `Dataset` and load it using a `DataLoader` with specific configurations. Your implementation should include custom sampling, automatic batching, and memory pinning. Problem Statement: 1. **Custom Dataset**: - Implement a map-style `CustomDataset` class that inherits from `torch.utils.data.Dataset`. - The dataset should include 1000 samples, where each sample is a 2D tensor of shape `(3, 3)` filled with random floats, and a corresponding label which is the sum of all elements in the tensor. 2. **Custom Sampler**: - Implement a custom `WeightRandomSampler` that gives higher sampling probability to samples with higher label values. 3. **DataLoader Configuration**: - Use the `DataLoader` to load the `CustomDataset` with the following configurations: - Batch size of 16. - 4 worker processes for data loading. - Enable memory pinning for faster transfer to a CUDA-enabled GPU. - Implement a custom `collate_fn` that ensures each batch\'s tensors are padded to the maximum tensor size in the batch. 4. **Memory Pinning**: - Add support for memory pinning in the `CustomDataset` to enable fast data transfer. Constraints: - Ensure reproducibility by setting a random seed. Input and Output: - There are no specific inputs or outputs beyond the class definitions and configurations. ```python import torch from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler import numpy as np # Implement the CustomDataset class class CustomDataset(Dataset): def __init__(self, size=1000): self.size = size self.data = [torch.randn((3, 3)) for _ in range(size)] self.labels = [d.sum().item() for d in self.data] def __getitem__(self, index): return self.data[index], self.labels[index] def __len__(self): return self.size # Custom WeightRandomSampler def create_weighted_sampler(dataset): labels = np.array(dataset.labels) weights = labels / labels.sum() sampler = WeightedRandomSampler(weights, len(weights)) return sampler # Custom collate function def custom_collate_fn(batch): data, labels = zip(*batch) max_size = max([d.size(0) for d in data]) padded_data = [] for d in data: padding = torch.zeros((max_size, max_size)) padding[:d.size(0), :d.size(1)] = d padded_data.append(padding) return torch.stack(padded_data), torch.tensor(labels) def main(): # Set random seed for reproducibility torch.manual_seed(0) # Instantiate the dataset dataset = CustomDataset() # Create weighted sampler sampler = create_weighted_sampler(dataset) # Create DataLoader with required configurations dataloader = DataLoader( dataset, batch_size=16, sampler=sampler, num_workers=4, collate_fn=custom_collate_fn, pin_memory=True ) # Iterate through the DataLoader for batch in dataloader: data, labels = batch print(f\\"Batch data shape: {data.shape}, Batch labels: {labels}\\") if __name__ == \\"__main__\\": main() ``` Note: - You need to ensure that the data loading is done efficiently without blocking the computational process. - Pinned memory is specifically useful for transferring data to CUDA-enabled GPUs. Ensure that the tensors returned by the `DataLoader` are placed in pinned memory.","solution":"import torch from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler import numpy as np class CustomDataset(Dataset): def __init__(self, size=1000): np.random.seed(0) # For reproducibility self.size = size self.data = [torch.tensor(np.random.rand(3, 3), dtype=torch.float32) for _ in range(size)] self.labels = [d.sum().item() for d in self.data] def __getitem__(self, index): return self.data[index], self.labels[index] def __len__(self): return self.size # Custom WeightedRandomSampler def create_weighted_sampler(dataset): labels = np.array(dataset.labels) weights = labels / labels.sum() sampler = WeightedRandomSampler(weights, len(weights)) return sampler # Custom collate function def custom_collate_fn(batch): data, labels = zip(*batch) max_size = max([d.size(0) for d in data]) padded_data = [] for d in data: padding = torch.zeros((max_size, max_size)) padding[:d.size(0), :d.size(1)] = d padded_data.append(padding) return torch.stack(padded_data), torch.tensor(labels, dtype=torch.float32) def main(): # Set random seed for reproducibility torch.manual_seed(0) # Instantiate the dataset dataset = CustomDataset() # Create weighted sampler sampler = create_weighted_sampler(dataset) # Create DataLoader with required configurations dataloader = DataLoader( dataset, batch_size=16, sampler=sampler, num_workers=4, collate_fn=custom_collate_fn, pin_memory=True ) # Iterate through the DataLoader for batch in dataloader: data, labels = batch print(f\\"Batch data shape: {data.shape}, Batch labels: {labels}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Demonstrate understanding of various pandas Index types and their associated methods. You will create and manipulate DataFrames using specialized indices. **Problem Statement**: 1. Create a pandas DataFrame with the following structure: - Columns: \\"Product\\", \\"Category\\", \\"Quantity\\", \\"Price\\" - Data: ``` [ [\\"Laptop\\", \\"Electronics\\", 10, 999.99], [\\"Chair\\", \\"Furniture\\", 5, 149.99], [\\"Book\\", \\"Stationery\\", 50, 9.99], [\\"Phone\\", \\"Electronics\\", 15, 499.99], [\\"Desk\\", \\"Furniture\\", 7, 199.99], [\\"Notebook\\", \\"Stationery\\", 100, 2.99] ] ``` 2. Convert `Category` column into a `CategoricalIndex`. 3. Perform the following operations: - **a.** Find and display all unique categories. - **b.** Create a new DataFrame which has a `MultiIndex` composed of `Category` and `Product`. - **c.** Sort the DataFrame by `Category` and `Product`. - **d.** Calculate and display the total quantity and average price per `Category`. - **e.** Check if the index of this new DataFrame is monotonically increasing. - **f.** Add a new row: `[\\"Tablet\\", \\"Electronics\\", 20, 299.99]` and update the MultiIndex. - **g.** After adding the new row, drop any duplicates based on the indices if present. 4. Finally, convert the DataFrameâ€™s index to `DatetimeIndex` with a weekly frequency starting from \'2022-01-01\', assigning them as pandas periods. **Expected functions and output formats**: 1. **Input DataFrame Structure**: ```python pd.DataFrame({ \\"Product\\": [\\"Laptop\\", \\"Chair\\", \\"Book\\", \\"Phone\\", \\"Desk\\", \\"Notebook\\"], \\"Category\\": [\\"Electronics\\", \\"Furniture\\", \\"Stationery\\", \\"Electronics\\", \\"Furniture\\", \\"Stationery\\"], \\"Quantity\\": [10, 5, 50, 15, 7, 100], \\"Price\\": [999.99, 149.99, 9.99, 499.99, 199.99, 2.99] }) ``` 2. **Example code that performs the above operations and outputs the result**: ```python import pandas as pd # Step 1 data = [ [\\"Laptop\\", \\"Electronics\\", 10, 999.99], [\\"Chair\\", \\"Furniture\\", 5, 149.99], [\\"Book\\", \\"Stationery\\", 50, 9.99], [\\"Phone\\", \\"Electronics\\", 15, 499.99], [\\"Desk\\", \\"Furniture\\", 7, 199.99], [\\"Notebook\\", \\"Stationery\\", 100, 2.99] ] df = pd.DataFrame(data, columns=[\\"Product\\", \\"Category\\", \\"Quantity\\", \\"Price\\"]) # Step 2 df.index = pd.CategoricalIndex(df[\\"Category\\"]) # Step 3a unique_categories = df.index.unique() print(\\"Unique Categories:\\", unique_categories) # Step 3b df_multi = df.set_index([\\"Category\\", \\"Product\\"]) # Step 3c df_multi = df_multi.sort_index() # Step 3d total_quantity = df_multi.groupby(\\"Category\\")[\\"Quantity\\"].sum() avg_price = df_multi.groupby(\\"Category\\")[\\"Price\\"].mean() print(\\"Total Quantity per Category:n\\", total_quantity) print(\\"Average Price per Category:n\\", avg_price) # Step 3e print(\\"Is the index monotonically increasing?:\\", df_multi.index.is_monotonic_increasing) # Step 3f new_row = pd.DataFrame([[\\"Tablet\\", \\"Electronics\\", 20, 299.99]], columns=[\\"Product\\", \\"Category\\", \\"Quantity\\", \\"Price\\"]) df_new = pd.concat([df, new_row]) df_multi = df_new.set_index([\\"Category\\", \\"Product\\"]) # Step 3g df_multi = df_multi[~df_multi.index.duplicated(keep=\'first\')] # Step 4 date_index = pd.date_range(start=\'2022-01-01\', periods=len(df_multi), freq=\'W\') df_multi.set_index(date_index, inplace=True) # Display final DataFrame print(df_multi) ``` **Constraints**: - Implement solutions without using any non-pandas compatible libraries. - Ensure code readability and include necessary comments to explain the logic. Good luck!","solution":"import pandas as pd def create_dataframe(): data = [ [\\"Laptop\\", \\"Electronics\\", 10, 999.99], [\\"Chair\\", \\"Furniture\\", 5, 149.99], [\\"Book\\", \\"Stationery\\", 50, 9.99], [\\"Phone\\", \\"Electronics\\", 15, 499.99], [\\"Desk\\", \\"Furniture\\", 7, 199.99], [\\"Notebook\\", \\"Stationery\\", 100, 2.99] ] df = pd.DataFrame(data, columns=[\\"Product\\", \\"Category\\", \\"Quantity\\", \\"Price\\"]) return df def convert_category_to_categorical(df): df.index = pd.CategoricalIndex(df[\\"Category\\"]) return df def get_unique_categories(df): return df.index.unique() def create_multiindex_dataframe(df): return df.set_index([\\"Category\\", \\"Product\\"]) def sort_dataframe(df): return df.sort_index() def calculate_totals_and_averages(df): total_quantity = df.groupby(\\"Category\\")[\\"Quantity\\"].sum() avg_price = df.groupby(\\"Category\\")[\\"Price\\"].mean() return total_quantity, avg_price def index_is_monotonically_increasing(df): return df.index.is_monotonic_increasing def add_new_row(df, row): df_new = pd.concat([df, pd.DataFrame([row], columns=df.columns)]) return df_new.set_index([\\"Category\\", \\"Product\\"]) def drop_duplicates(df): return df[~df.index.duplicated(keep=\'first\')] def convert_to_datetime_index(df): date_index = pd.date_range(start=\'2022-01-01\', periods=len(df), freq=\'W\') df.set_index(date_index, inplace=True) return df # Main function to perform all operations def main(): df = create_dataframe() df = convert_category_to_categorical(df) unique_categories = get_unique_categories(df) df_multi = create_multiindex_dataframe(df) df_multi = sort_dataframe(df_multi) total_quantity, avg_price = calculate_totals_and_averages(df_multi) is_monotonic = index_is_monotonically_increasing(df_multi) df_multi = add_new_row(df, [\\"Tablet\\", \\"Electronics\\", 20, 299.99]) df_multi = drop_duplicates(df_multi) df_multi = convert_to_datetime_index(df_multi) return df_multi, unique_categories, total_quantity, avg_price, is_monotonic # Executing main function to get the result main()"},{"question":"**Objective:** Demonstrate your understanding of various Python modules for file and directory access by implementing a function that performs a series of file manipulations. **Task:** Implement a Python function named `organize_files` that takes two arguments: `source_dir` and `dest_dir`. This function will perform the following tasks: 1. **Create Directories**: Ensure that `dest_dir` exists. If not, create it. 2. **Move Files by Extension**: Move all files from `source_dir` to subdirectories within `dest_dir`, organizing them by file extensions. For instance, all `.txt` files should go into a directory named `txt` within `dest_dir`, all `.jpg` files into a directory named `jpg`, and so on. 3. **Temporary File Creation**: For each file that is moved, create a temporary file in the corresponding subdirectory in `dest_dir` with the same content as the original file but with an added prefix \\"temp_\\". Ensure that these temporary files are cleaned up (deleted) before the function finishes. 4. **Pattern Matching**: Find all files in `dest_dir` that match the pattern `temp_*` and log their names to a list. Return this list as the output of the function. **Input:** - `source_dir` (str): Path to the source directory containing files to be organized. - `dest_dir` (str): Path to the destination directory where files will be organized by extension. **Output:** - A list of strings representing the names of temporary files that were created and then deleted in `dest_dir`. **Constraints:** - You can assume that `source_dir` contains only files (no subdirectories). - Filenames will not contain special characters outside of standard alphanumeric sets and common punctuation. - You must handle typical filesystem exceptions (e.g., file not found, permissions issues). **Performance Requirements:** - The function should handle directories with up to 10,000 files efficiently. **Example:** ```python # Assuming source_dir contains files: \\"file1.txt\\", \\"file2.jpg\\", \\"file3.txt\\" # and dest_dir initially does not exist. temporary_files = organize_files(\'/path/to/source_dir\', \'/path/to/dest_dir\') # The function will move files as follows: # /path/to/source_dir/file1.txt -> /path/to/dest_dir/txt/file1.txt # /path/to/source_dir/file2.jpg -> /path/to/dest_dir/jpg/file2.jpg # /path/to/source_dir/file3.txt -> /path/to/dest_dir/txt/file3.txt # Temporary files created and deleted: # /path/to/dest_dir/txt/temp_file1.txt # /path/to/dest_dir/txt/temp_file3.txt # /path/to/dest_dir/jpg/temp_file2.jpg # Output will be: # [\'temp_file1.txt\', \'temp_file3.txt\', \'temp_file2.jpg\'] ``` **Note:** You should leverage the relevant modules mentioned in the documentation (`pathlib`, `shutil`, `tempfile`, `glob`) to accomplish the task.","solution":"import os import shutil from pathlib import Path import tempfile import glob def organize_files(source_dir, dest_dir): Organize files by their extensions from source_dir to dest_dir. Create temporary files, log their names, and ensure they are deleted. Args: source_dir (str): Path to the source directory containing files to be organized. dest_dir (str): Path to the destination directory where files will be organized by extension. Returns: list: A list of names of temporary files that were created and then deleted. temp_files = [] # Ensure the destination directory exists Path(dest_dir).mkdir(parents=True, exist_ok=True) # Iterate over all files in the source directory for file_path in Path(source_dir).glob(\'*\'): if file_path.is_file(): # Determine the file extension ext = file_path.suffix.lstrip(\'.\') ext_dir = Path(dest_dir) / ext # Create the subdirectory for this extension if it doesn\'t exist ext_dir.mkdir(parents=True, exist_ok=True) # Move the file to the new subdirectory shutil.move(str(file_path), ext_dir) # Create a temporary file with the same content dest_file_path = ext_dir / file_path.name temp_file_path = ext_dir / f\'temp_{file_path.name}\' temp_files.append(temp_file_path.name) with open(dest_file_path, \'rb\') as original_file: content = original_file.read() with open(temp_file_path, \'wb\') as temp_file: temp_file.write(content) # Cleanup: Delete the temporary file if temp_file_path.exists(): temp_file_path.unlink() return temp_files"},{"question":"Objective: Your task is to analyze the relationship between various attributes in a dataset using seaborn\'s regression capabilities. You will need to demonstrate both simple and complex visualizations, including handling categorical variables and customizing plots. Dataset: For this assessment, you will use the `tips` dataset, which can be loaded directly from seaborn. Instructions: 1. **Dataset Preparation:** - Load the `tips` dataset from seaborn. - Create a new binary column `generous_tip` which indicates if the tip is greater than 20% of the total bill. (True if `tip/total_bill > 0.20`, False otherwise). 2. **Simple Linear Regression:** - Create a scatter plot with a regression line showing the relationship between `total_bill` and `tip`. 3. **Handling Categorical Data:** - Create a scatter plot showing the relationship between `size` and `tip`. Add jitter to the `size` variable. 4. **Comparative Regression Analysis:** - Plot two regression lines on the same axes to show the relationship between `total_bill` and `tip`, conditioned on whether the customer is a smoker or not. Use different colors and markers for smokers and non-smokers. 5. **Facet Grid:** - Create a facet grid of regression plots showing the relationship between `total_bill` and `tip`, with separate plots for different times of day (`time`) and different dining parties (`sex`). Include confidence intervals in your plots. 6. **Advanced Regression Models:** - Plot a logistic regression line to visualize the probability of a tip being classified as `generous_tip` based on `total_bill`. - Perform a nonparametric regression (lowess) to visualize the relationship between `total_bill` and `tip`. 7. **Residual Analysis:** - Create a residual plot of the linear regression between `total_bill` and `tip` to evaluate the adequacy of the linear model. Constraints: - Your code should be efficient and make use of seaborn\'s functionalities for regression and visualization. - Ensure your plots are well-labeled and visually clear. Expected Output: - You should provide a Python script or notebook containing: 1. The necessary imports and dataset preparation steps. 2. Code for each of the visualization tasks described. 3. The resulting plots, demonstrating a clear understanding of seaborn regression capabilities. Evaluation: Your solution will be evaluated based on: - Correctness (the right calculations and correct usage of seaborn functions). - Completeness (addresses all parts of the question). - Code quality (readability, efficiency, use of seaborn features). - Visualization quality (clarity, informative labels, appropriate use of colors and markers). Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\'tips\') # Create binary column \'generous_tip\' tips[\'generous_tip\'] = tips[\'tip\'] / tips[\'total_bill\'] > 0.20 def plot_simple_linear_regression(): plt.figure(figsize=(10, 6)) sns.regplot(x=\'total_bill\', y=\'tip\', data=tips) plt.title(\'Simple Linear Regression: Total Bill vs. Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() def plot_categorical_regression_with_jitter(): plt.figure(figsize=(10, 6)) sns.regplot(x=\'size\', y=\'tip\', data=tips, x_jitter=0.2) plt.title(\'Scatter plot with Jitter: Size vs. Tip\') plt.xlabel(\'Size\') plt.ylabel(\'Tip\') plt.show() def plot_comparative_regression(): plt.figure(figsize=(10, 6)) sns.lmplot(x=\'total_bill\', y=\'tip\', hue=\'smoker\', data=tips, markers=[\'o\', \'x\']) plt.title(\'Comparative Regression: Total Bill vs. Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() def plot_facet_grid(): g = sns.FacetGrid(tips, row=\'time\', col=\'sex\', margin_titles=True) g.map(sns.regplot, \'total_bill\', \'tip\') g.add_legend() g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Facet Grid: Total Bill vs. Tip by Time and Sex\') plt.show() def plot_logistic_regression(): plt.figure(figsize=(10, 6)) sns.regplot(x=\'total_bill\', y=\'generous_tip\', data=tips, logistic=True, ci=None) plt.title(\'Logistic Regression: Probability of Generous Tip by Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Probability of Generous Tip\') plt.show() def plot_lowess(): plt.figure(figsize=(10, 6)) sns.regplot(x=\'total_bill\', y=\'tip\', data=tips, lowess=True) plt.title(\'Lowess Regression: Total Bill vs. Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() def plot_residuals(): plt.figure(figsize=(10, 6)) sns.residplot(x=\'total_bill\', y=\'tip\', data=tips) plt.title(\'Residual Plot: Total Bill vs. Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Residuals\') plt.show()"},{"question":"Objective To assess the student\'s understanding of temporary file and directory management in Python using the `tempfile` module. Problem Statement You are required to implement a function that creates multiple temporary files and directories, writes data to them, and ensures that they are properly cleaned up after usage. This function must also demonstrate the usage of different interfaces from the `tempfile` module. Function Signature ```python def manage_temp_files_and_directories(num_files: int, num_directories: int, data: bytes) -> dict: \'\'\' Creates temporary files and directories, writes data to them, and returns their paths. Parameters: - num_files: int : Number of temporary files to create - num_directories: int : Number of temporary directories to create - data: bytes : Data to be written to each temporary file Returns: - dict : A dictionary with paths to created temporary files and directories { \\"temp_files\\": [list of file paths], \\"temp_dirs\\": [list of directory paths] } \'\'\' ``` Requirements 1. **Temporary Files**: - Create `num_files` temporary files using `NamedTemporaryFile()`. - Write the `data` to each file. - Store the file paths in a list, which will be part of the returned dictionary. 2. **Temporary Directories**: - Create `num_directories` temporary directories using `TemporaryDirectory()`. - Store the directory paths in a list, which will be part of the returned dictionary. 3. **Cleanup**: - Ensure that all temporary files and directories are properly removed once the function completes. 4. **Constraints**: - `num_files` and `num_directories` will be non-negative integers. - `data` will be non-empty bytes. 5. **Example**: ```python result = manage_temp_files_and_directories(3, 2, b\'Hello, World!\') # Expected output format print(result) # { # \\"temp_files\\": [\\"path_to_temp_file1\\", \\"path_to_temp_file2\\", \\"path_to_temp_file3\\"], # \\"temp_dirs\\": [\\"path_to_temp_dir1\\", \\"path_to_temp_dir2\\"] # } ``` Notes - While writing the function, handle exceptions such as file creation errors and ensure that resources are properly cleaned up. - Use context managers where appropriate to automatically handle the cleanup.","solution":"import tempfile import os def manage_temp_files_and_directories(num_files: int, num_directories: int, data: bytes) -> dict: temp_files = [] temp_dirs = [] try: # Create temporary files and write data to them for _ in range(num_files): temp_file = tempfile.NamedTemporaryFile(delete=False) # delete=False so we can see the file paths later temp_file.write(data) temp_file.seek(0) temp_files.append(temp_file.name) temp_file.close() # We must close it to ensure data is written and the file is accessible # Create temporary directories for _ in range(num_directories): temp_dir = tempfile.TemporaryDirectory() temp_dirs.append(temp_dir.name) result = {\\"temp_files\\": temp_files, \\"temp_dirs\\": temp_dirs} finally: # Cleanup temp files for temp_file in temp_files: if os.path.exists(temp_file): os.remove(temp_file) # Cleanup temp directories for temp_dir in temp_dirs: if os.path.exists(temp_dir): os.rmdir(temp_dir) return result"},{"question":"You are required to create a custom module importer using the `importlib` module. This custom importer should be able to import Python source files from any directory, not just those listed in `sys.path`. Your task is to implement a custom `FileSystemImporter` class and use it to dynamically import a module specified by its file path. # Requirements: 1. **Class Definition**: - Define a class `FileSystemImporter` that implements `importlib.abc.PathEntryFinder`. 2. **Method Implementation**: - Implement the method `find_spec(self, fullname, target=None)` to find and return the module specification for the given module name. - Implement the method `invalidate_caches(self)` to clear any internal cache if used (optional). 3. **Dynamic Import Function**: - Create a function `dynamic_import(file_path: str) -> ModuleType` that utilizes your `FileSystemImporter` to import a module given its file path. 4. **Testing the Importer**: - Test your `FileSystemImporter` by creating a simple module (in source form) at a specific file location and importing it using the `dynamic_import` function. # Example: Assume you have a Python file at `/tmp/test_module.py` with the following contents: ```python def hello(): return \\"Hello, world!\\" ``` # Usage: ```python import os # Assuming \'FileSystemImporter\' and \'dynamic_import\' are implemented as described above file_path = \'/tmp/test_module.py\' imported_module = dynamic_import(file_path) print(imported_module.hello()) # Output should be: Hello, world! ``` # Constraints: - The `file_path` specified in `dynamic_import` should point to a valid Python source file (*.py). - Do not modify the global `sys.path`. # Performance: - Ensure that the importer efficiently handles import operations without significantly increasing load time. # Submission: Submit the code for the `FileSystemImporter` class and the `dynamic_import(file_path: str) -> ModuleType` function.","solution":"import importlib.util import os import sys from types import ModuleType import importlib.abc import importlib.machinery class FileSystemImporter(importlib.abc.PathEntryFinder): def __init__(self, path): self.path = path def find_spec(self, fullname, target=None): module_path = os.path.join(self.path, fullname + \\".py\\") if not os.path.isfile(module_path): return None loader = importlib.machinery.SourceFileLoader(fullname, module_path) return importlib.util.spec_from_loader(fullname, loader) def invalidate_caches(self): pass def dynamic_import(file_path: str) -> ModuleType: file_dir, file_name = os.path.split(file_path) module_name = os.path.splitext(file_name)[0] if file_dir not in sys.path: sys.path.insert(0, file_dir) module_spec = FileSystemImporter(file_dir).find_spec(module_name) if module_spec is None: raise FileNotFoundError(f\\"No module named \'{module_name}\' found in \'{file_dir}\'\\") module = importlib.util.module_from_spec(module_spec) module_spec.loader.exec_module(module) if file_dir in sys.path: sys.path.remove(file_dir) return module"},{"question":"**Problem Statement:** You are working as a data scientist at XYZ Corp. where you need to analyze a dataset using unsupervised learning techniques from scikit-learn. Your task is to write a Python function that performs clustering on a given dataset and visualizes the clusters. # Requirements: - Implement a function `perform_clustering` that: - Takes a dataset as input (a 2D array where each row represents a sample and each column represents a feature). - Performs k-means clustering on the dataset. - Returns the cluster labels for each sample. - Implement a function `visualize_clusters` that: - Takes the dataset and the corresponding cluster labels as input. - Uses t-SNE for dimensionality reduction to 2D. - Plots the 2D points using different colors for different clusters. # Constraints: - You are allowed to use only the `scikit-learn` library for implementing the clustering and dimensionality reduction. - Assume the dataset is not larger than 5000 samples and 100 features. - The clustering algorithm should be k-means with a default of 3 clusters. # Input: - A 2D numpy array `data` of shape `(n_samples, n_features)`. - An integer `n_clusters` (default value is 3) for the number of clusters in the k-means algorithm. # Output: - A 1D array of cluster labels (integers) for each sample corresponding to the cluster assignment. # Function Signatures: ```python import numpy as np def perform_clustering(data: np.ndarray, n_clusters: int = 3) -> np.ndarray: pass def visualize_clusters(data: np.ndarray, labels: np.ndarray) -> None: pass ``` # Example: ```python import numpy as np # Sample Data data = np.array([ [1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0], ]) # Perform Clustering labels = perform_clustering(data, n_clusters=2) # Visualize Clusters visualize_clusters(data, labels) ``` The `perform_clustering` function should use k-means clustering to group the samples into clusters and return the cluster labels, while the `visualize_clusters` function should use t-SNE to reduce the dimensionality of the data and plot it using different colors for each cluster. # Notes: - You need to handle the imports and ensure that the plots have appropriate legends, titles, and axis labels. - Remember to handle the random state for reproducibility in k-means and t-SNE.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.manifold import TSNE import matplotlib.pyplot as plt def perform_clustering(data: np.ndarray, n_clusters: int = 3) -> np.ndarray: Perform K-means clustering on the dataset. Parameters: - data: 2D numpy array of shape (n_samples, n_features). - n_clusters: The number of clusters to form. Returns: - A 1D numpy array of cluster labels. kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(data) return kmeans.labels_ def visualize_clusters(data: np.ndarray, labels: np.ndarray) -> None: Visualize clusters by reducing data to 2D using t-SNE and plotting. Parameters: - data: 2D numpy array of shape (n_samples, n_features). - labels: 1D numpy array of cluster labels. tsne = TSNE(n_components=2, random_state=42) data_2d = tsne.fit_transform(data) plt.figure(figsize=(10, 7)) plt.scatter(data_2d[:, 0], data_2d[:, 1], c=labels, cmap=\'viridis\') plt.colorbar() plt.title(\'Cluster visualization with t-SNE\') plt.xlabel(\'t-SNE feature 1\') plt.ylabel(\'t-SNE feature 2\') plt.show()"},{"question":"Objective Write a Python function that leverages scikit-learn to perform a classification task on a given dataset. The task involves training, tuning, and evaluating a machine learning model using a specific supervised learning algorithm provided by scikit-learn. Problem Statement You are given a dataset containing features and labels for a binary classification problem. Your task is to build and evaluate a Support Vector Machine (SVM) model using scikit-learn. The evaluation should include hyperparameter tuning to find the best model. Your function should follow these steps: 1. **Data Preparation** * Split the given dataset into training and testing sets (80-20 split). 2. **Model Training** * Train an SVM classifier on the training set. Use GridSearchCV to find the optimal hyperparameters (C and kernel). 3. **Model Evaluation** * Evaluate the best model on the test set and return the classification report (including precision, recall, and F1-score). Input Your function should accept the following inputs: * `features` (2D list or numpy array): The feature matrix (m x n), where `m` is the number of samples and `n` is the number of features. * `labels` (list or numpy array): The binary labels (1D array of length `m`). Output The function should return a classification report (string) that includes precision, recall, and F1-score for both classes. Constraints * Use an 80-20 train-test split. * Search hyperparameters `C` in [0.1, 1, 10] and `kernel` in [\'linear\', \'poly\', \'rbf\']. * Assume that the input data is clean and preprocessed. Performance Requirements The function should be efficient, leveraging scikit-learn\'s utilities for splitting, training, and evaluating the model. Example ```python from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV from sklearn.metrics import classification_report import numpy as np def svm_classification(features, labels): # Split the dataset X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42) # Define the model and hyperparameters svc = SVC() param_grid = {\'C\': [0.1, 1, 10], \'kernel\': [\'linear\', \'poly\', \'rbf\']} # Perform Grid Search with Cross Validation grid_search = GridSearchCV(svc, param_grid, cv=5) grid_search.fit(X_train, y_train) # Predict on the test set using the best model best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) # Generate and return the classification report report = classification_report(y_test, y_pred) return report # Example usage: iris = datasets.load_iris() X = iris.data[iris.target != 2] # Use only two classes y = iris.target[iris.target != 2] # Binary labels for these classes print(svm_classification(X, y)) ``` Note The above example is illustrative and may need adjustments based on the actual dataset and further testing.","solution":"from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import classification_report import numpy as np def svm_classification(features, labels): Train an SVM classifier using GridSearchCV to find the best hyperparameters and return a classification report on the test set. Parameters: features (2D list or numpy array): The feature matrix (m x n). labels (list or numpy array): The binary labels (1D array of length m). Returns: str: Classification report including precision, recall, and F1-score for both classes. # Split the dataset into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42) # Define the SVM model and hyperparameters svc = SVC() param_grid = {\'C\': [0.1, 1, 10], \'kernel\': [\'linear\', \'poly\', \'rbf\']} # Perform Grid Search with Cross Validation grid_search = GridSearchCV(svc, param_grid, cv=5) grid_search.fit(X_train, y_train) # Predict on the test set using the best model best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) # Generate and return the classification report report = classification_report(y_test, y_pred) return report"},{"question":"# Email Utility Functions Assessment Your task is to write a Python function that processes email addresses from a raw email header and transforms them into a usable format. Specifically, your function should: 1. Extract all email addresses from the given `To`, `Cc`, `Resent-To`, and `Resent-Cc` fields. 2. Parse these addresses to separate real names and email addresses. 3. Format these parsed addresses back into a single string where each formatted address is on a new line. You are required to use the available functions from the `email.utils` module to achieve this. Function Signature ```python def process_email_addresses(header: dict) -> str: pass ``` Input - `header` (dict): A dictionary representing the email header fields. The keys will be \'To\', \'Cc\', \'Resent-To\', and \'Resent-Cc\', and the values will be lists of strings representing the email addresses in those fields. Output - `result` (str): A single string where each formatted address is on a new line. Constraints - Use `email.utils.getaddresses()` to parse multiple addresses. - Use `email.utils.parseaddr()` and `email.utils.formataddr()` to handle address parsing and formatting. - Ensure that the resulting string is properly formatted and each address is on a new line. Example ```python header = { \'To\': [\'John Doe <john.doe@example.com>\', \'Jane Smith <jane.smith@sample.org>\'], \'Cc\': [\'Alice <alice@wonderland.com>\', \'Bob <bob@builder.org>\'], \'Resent-To\': [\'Resent Person <resent.person@resent.com>\'], \'Resent-Cc\': [\'Another Person <another.person@resent.com>\'] } expected_output = John Doe <john.doe@example.com> Jane Smith <jane.smith@sample.org> Alice <alice@wonderland.com> Bob <bob@builder.org> Resent Person <resent.person@resent.com> Another Person <another.person@resent.com> assert process_email_addresses(header) == expected_output ``` You can assume that the input header will always contain valid email addresses. Make sure your solution effectively uses the `email.utils` module to perform the tasks.","solution":"from email.utils import getaddresses, formataddr def process_email_addresses(header: dict) -> str: Extract and format email addresses from the given header fields. Args: header (dict): Email header fields containing lists of email addresses. Returns: str: A single string with each formatted address on a new line. all_addresses = ( header.get(\'To\', []) + header.get(\'Cc\', []) + header.get(\'Resent-To\', []) + header.get(\'Resent-Cc\', []) ) parsed_addresses = getaddresses(all_addresses) formatted_addresses = [formataddr(pair) for pair in parsed_addresses] return \\"n\\".join(formatted_addresses)"},{"question":"Computation of Custom Function using `torch.special` Objective You need to implement a custom function using PyTorch\'s `torch.special` module that computes a specific mathematical result for given input tensors. Question Implement a function `custom_special_computation` that performs the following operations on an input tensor: 1. Applies the Bessel function of the first kind of order 0 (`bessel_j0`) to each element. 2. Applies the error function (`erf`) to each element obtained from step 1. 3. Computes the log of the sum of exponentials (`logsumexp`) of the resultant tensor from step 2. The function should accept a single input tensor and return a tensor with the same shape where each element is the result of the described computation. Function Signature ```python def custom_special_computation(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` Input - `input_tensor`: A 1-dimensional or 2-dimensional tensor of type `torch.Tensor` containing floating point numbers. Output - Returns a tensor of the same shape as `input_tensor`, where each element is the result of applying the aforementioned sequence of operations. Example ```python import torch from torch.special import bessel_j0, erf, logsumexp input_tensor = torch.tensor([1.0, 2.0, 3.0]) output_tensor = custom_special_computation(input_tensor) print(output_tensor) ``` The `output_tensor` should contain the results of the described operations applied to the input tensor. Constraints - You may not use loops to iterate through tensor elements. Utilize PyTorch operations that efficiently handle tensor computations. - Ensure that the function is capable of handling tensors with both positive and negative values. Performance Requirements The implementation must handle large tensors efficiently within reasonable time limits.","solution":"import torch from torch.special import bessel_j0, erf, logsumexp def custom_special_computation(input_tensor: torch.Tensor) -> torch.Tensor: Apply a custom special computation to each element of the input tensor. The computation steps are: 1. Apply the Bessel function of the first kind of order 0 (`bessel_j0`). 2. Apply the error function (`erf`) to each element obtained from step 1. 3. Compute the log of the sum of exponentials (`logsumexp`) of the resultant tensor from step 2. Args: input_tensor (torch.Tensor): A 1-dimensional or 2-dimensional tensor of floating point numbers. Returns: torch.Tensor: A tensor of the same shape as `input_tensor` containing the resultant values. step1 = bessel_j0(input_tensor) step2 = erf(step1) result = logsumexp(step2, dim=-1) return result"},{"question":"**Python Programming Challenge: File Locking and Manipulation** You are tasked with creating a Python function that utilizes the `fcntl` module to manage file locking in a concurrent environment. Your function should: 1. Lock a specified section of a file using an exclusive lock. 2. Write data to the locked section. 3. Release the lock once the writing is complete. 4. Handle any potential errors gracefully. # Requirements: - Implement a function `exclusive_file_write(file_path: str, data: bytes, start: int, length: int) -> bool`. - **Parameters**: - `file_path` (str): Path to the file to manipulate. - `data` (bytes): Data to write to the file. - `start` (int): Starting byte position in the file to lock and write. - `length` (int): Number of bytes to lock and write. - **Returns**: - `bool`: True if the operation was successful, False otherwise. # Constraints: - Use the `fcntl.lockf` method to lock and unlock the file. - Ensure that the provided `start` and `length` parameters are valid given the size of the file. - Handle any exceptions, such as `OSError`, that may arise from file or system call operations. - Assume the file is in binary mode. # Example: ```python def exclusive_file_write(file_path: str, data: bytes, start: int, length: int) -> bool: import fcntl try: with open(file_path, \'r+b\') as f: # Lock the section of the file fcntl.lockf(f, fcntl.LOCK_EX, length, start) # Seek to the start position f.seek(start) # Write data to the file f.write(data[:length]) # Unlock the section of the file fcntl.lockf(f, fcntl.LOCK_UN, length, start) return True except (OSError, IOError) as e: print(f\\"Error: {e}\\") return False # Example usage: # exclusive_file_write(\'/tmp/sample.txt\', b\'Hello\', 0, 5) ``` # Notes: - Ensure that locking and unlocking occur correctly to avoid deadlocks. - The function should manage the file\'s state correctly before and after writing. - This test assesses your understanding of file manipulations, binary data handling, and system-level programming in Python.","solution":"def exclusive_file_write(file_path: str, data: bytes, start: int, length: int) -> bool: import fcntl try: with open(file_path, \'r+b\') as f: if f.seek(start + length) > f.seek(0, 2): print(f\\"Error: File size smaller than required start+length\\") return False # Lock the section of the file fcntl.lockf(f, fcntl.LOCK_EX, length, start) # Seek to the start position f.seek(start) # Write data to the file f.write(data[:length]) # Unlock the section of the file fcntl.lockf(f, fcntl.LOCK_UN, length, start) return True except (OSError, IOError) as e: print(f\\"Error: {e}\\") return False"},{"question":"Objective: To assess your understanding of `doctest` module\'s functionalities and advanced features in Python, create a Python module with functions that include: 1. Regular functional logic. 2. Docstrings containing `doctest` examples. 3. Custom test cases with directives. Instructions: 1. **Function Implementations**: - Write a Python function `is_prime(n)` that checks whether a number `n` is a prime number. - Write a Python function `sum_of_squares(n)` that returns the sum of squares of integers from 1 to `n`. 2. **Docstrings with Examples for `doctest`**: - Ensure your functions have comprehensive docstrings with valid `doctest` examples. - Include edge cases, exceptions, and usage of `doctest` directives for handling complexities (e.g., whitespace normalization). 3. **Custom Testing**: - Use the `doctest` module to create an executable script that automatically runs the tests when the module is executed directly. Function Specifications: - **`is_prime(n)`** - **Input**: Integer `n` where `n >= 0`. - **Output**: Boolean value, `True` if `n` is a prime number, `False` otherwise. - **Constraints**: - Raise `ValueError` if `n` is negative with appropriate message. - **`sum_of_squares(n)`** - **Input**: Integer `n` where `n >= 0`. - **Output**: Integer representing the sum of squares from 1 to `n`. - **Constraints**: - Raise `ValueError` if `n` is negative with appropriate message. Example: ```python def is_prime(n): Check if the number is prime. >>> is_prime(2) True >>> is_prime(4) False >>> is_prime(17) True >>> is_prime(-1) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... ValueError: n must be >= 0 if n < 0: raise ValueError(\\"n must be >= 0\\") if n in (0, 1): return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def sum_of_squares(n): Calculate the sum of squares from 1 to n. >>> sum_of_squares(3) 14 >>> sum_of_squares(5) 55 >>> sum_of_squares(-10) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... ValueError: n must be >= 0 if n < 0: raise ValueError(\\"n must be >= 0\\") return sum(i * i for i in range(1, n + 1)) if __name__ == \\"__main__\\": import doctest doctest.testmod(verbose=True) ``` **Performance Requirements**: - Your functions should handle typical input ranges efficiently and be tested for both average and edge cases. **Note**: - Please ensure your script runs successfully using the `doctest` module and validates all provided test cases correctly upon the module execution from the command line.","solution":"def is_prime(n): Check if the number is prime. >>> is_prime(2) True >>> is_prime(4) False >>> is_prime(17) True >>> is_prime(1) False >>> is_prime(0) False >>> is_prime(-1) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... ValueError: n must be >= 0 if n < 0: raise ValueError(\\"n must be >= 0\\") if n in (0, 1): return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def sum_of_squares(n): Calculate the sum of squares from 1 to n. >>> sum_of_squares(3) 14 >>> sum_of_squares(5) 55 >>> sum_of_squares(0) 0 >>> sum_of_squares(-10) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... ValueError: n must be >= 0 if n < 0: raise ValueError(\\"n must be >= 0\\") return sum(i * i for i in range(1, n + 1)) if __name__ == \\"__main__\\": import doctest doctest.testmod(verbose=True)"},{"question":"# Question: Implement Python Object Reference Manager You are to implement a simple Python object reference manager to simulate the reference counting mechanism described in the documentation provided. This exercise is intended to cement your understanding of how reference counting works in Python for memory management. Problem Statement Create a class `RefManager` that simulates basic functions of reference counting: 1. `inc_ref(obj_id)`: Increments the reference count for the object with the given `obj_id`. 2. `dec_ref(obj_id)`: Decrements the reference count for the object with the given `obj_id`. If the reference count reaches zero, the object should be deleted from the manager. 3. `get_ref_count(obj_id)`: Returns the current reference count for the object with the given `obj_id`. Additionally, ensure that the `RefManager` can handle `None` (similar to handling NULL in C) by doing nothing when `None` is passed to any method. Constraints - You must use a dictionary to keep track of the references, where keys are `obj_id` and values are the current reference counts. - Methods should handle `None` gracefully by performing no operation when `None` is passed. - Do not use Pythonâ€™s builtin `del` keyword for deleting objects; simulate it by simply removing the entry from the dictionary. Example Usage ```python ref_manager = RefManager() ref_manager.inc_ref(\\"obj1\\") print(ref_manager.get_ref_count(\\"obj1\\")) # Output: 1 ref_manager.inc_ref(\\"obj1\\") print(ref_manager.get_ref_count(\\"obj1\\")) # Output: 2 ref_manager.dec_ref(\\"obj1\\") print(ref_manager.get_ref_count(\\"obj1\\")) # Output: 1 ref_manager.dec_ref(\\"obj1\\") print(ref_manager.get_ref_count(\\"obj1\\")) # Output: 0 # After this, obj1 does not exist in manager anymore print(ref_manager.get_ref_count(\\"obj2\\")) # Output: 0 # Non-existent objects should have a count of 0 ref_manager.inc_ref(None) # No effect print(ref_manager.get_ref_count(None)) # Output: 0 # Since None is ignored, its count should remain 0 ref_manager.dec_ref(\\"non_existent\\") # No effect ``` Implementation Implement the `RefManager` class to meet the requirements. ```python class RefManager: def __init__(self): self.references = {} def inc_ref(self, obj_id): if obj_id is None: return if obj_id in self.references: self.references[obj_id] += 1 else: self.references[obj_id] = 1 def dec_ref(self, obj_id): if obj_id is None or obj_id not in self.references: return if self.references[obj_id] > 0: self.references[obj_id] -= 1 if self.references[obj_id] == 0: del self.references[obj_id] def get_ref_count(self, obj_id): if obj_id is None or obj_id not in self.references: return 0 return self.references[obj_id] ``` Test the functionality of your class thoroughly to verify that it handles reference counting correctly.","solution":"class RefManager: def __init__(self): self.references = {} def inc_ref(self, obj_id): if obj_id is None: return if obj_id in self.references: self.references[obj_id] += 1 else: self.references[obj_id] = 1 def dec_ref(self, obj_id): if obj_id is None or obj_id not in self.references: return if self.references[obj_id] > 0: self.references[obj_id] -= 1 if self.references[obj_id] == 0: del self.references[obj_id] def get_ref_count(self, obj_id): if obj_id is None or obj_id not in self.references: return 0 return self.references[obj_id]"},{"question":"# Tuning Decision Threshold with scikit-learn **Objective:** Create a classification pipeline that trains a binary classifier and tunes its decision threshold to optimize a specific metric. **Problem Statement:** Given a binary classification dataset, your task is to: 1. Train a logistic regression classifier. 2. Use `TunedThresholdClassifierCV` to tune the decision threshold for this classifier to optimize the F1-score. 3. Evaluate the performance of the optimized classifier and compare it with the default threshold (0.5 in the case of `predict_proba`). **Requirements:** - You must use `LogisticRegression` from `sklearn.linear_model`. - The dataset should be created using `make_classification` from `sklearn.datasets` with the following parameters: - `n_samples=1000` - `n_features=20` - `n_informative=2` - `n_redundant=10` - `random_state=42` - `weights=[0.1, 0.9]` (to create class imbalance) **Input:** There is no input as the task involves generating data and processing it within the provided instructions. **Output:** - Display the F1-score for both the classifier with the default threshold and the classifier with the tuned threshold. - Print the optimal decision threshold obtained through cross-validation. **Constraints:** - Use a 5-fold cross-validation strategy. - Ensure reproducibility by using the provided random state. ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, f1_score from sklearn.model_selection import train_test_split # Step 1: Generate the dataset X, y = make_classification( n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42, weights=[0.1, 0.9] ) # Step 2: Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Train the logistic regression classifier base_model = LogisticRegression() # Step 4: Create a scorer for the F1 score (use pos_label=0 due to class imbalance) pos_label = 0 scorer = make_scorer(f1_score, pos_label=pos_label) # Step 5: Use TunedThresholdClassifierCV to tune the decision threshold tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Step 6: Evaluate the model with the tuned threshold on the test set y_pred_default = base_model.predict(X_test) y_pred_tuned = tuned_model.predict(X_test) # Step 7: Calculate F1-score for both classifiers f1_default = f1_score(y_test, y_pred_default, pos_label=pos_label) f1_tuned = f1_score(y_test, y_pred_tuned, pos_label=pos_label) # Display the results print(f\\"F1 Score with default threshold: {f1_default:.4f}\\") print(f\\"F1 Score with tuned threshold: {f1_tuned:.4f}\\") print(f\\"Optimal Decision Threshold: {tuned_model.best_threshold_:.4f}\\") ``` This question combines understanding and practical application of scikit-learn\'s threshold tuning functionalities, thus challenging the student to implement a comprehensive solution.","solution":"from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import make_scorer, f1_score from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.base import BaseEstimator, ClassifierMixin import numpy as np class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_estimator, scoring, cv=None): self.base_estimator = base_estimator self.scoring = scoring self.cv = cv or StratifiedKFold(n_splits=5) def fit(self, X, y): self.base_estimator.fit(X, y) probabilities = self.base_estimator.predict_proba(X)[:, 1] thresholds = np.linspace(0, 1, 101) scores = [] for threshold in thresholds: y_pred = (probabilities >= threshold).astype(int) score = self.scoring._score_func(y, y_pred) scores.append(score) self.best_threshold_ = thresholds[np.argmax(scores)] return self def predict(self, X): probabilities = self.base_estimator.predict_proba(X)[:, 1] return (probabilities >= self.best_threshold_).astype(int) # Step 1: Generate the dataset X, y = make_classification( n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42, weights=[0.1, 0.9] ) # Step 2: Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Train the logistic regression classifier base_model = LogisticRegression() # Step 4: Create a scorer for the F1 score (use pos_label=1 because the minority class is of interest due to the imbalance) scorer = make_scorer(f1_score, pos_label=1) # Step 5: Use TunedThresholdClassifierCV to tune the decision threshold tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Step 6: Evaluate the model with the tuned threshold on the test set base_model.fit(X_train, y_train) y_pred_default = base_model.predict(X_test) y_pred_tuned = tuned_model.predict(X_test) # Step 7: Calculate F1-score for both classifiers f1_default = f1_score(y_test, y_pred_default, pos_label=1) f1_tuned = f1_score(y_test, y_pred_tuned, pos_label=1) # Display the results print(f\\"F1 Score with default threshold: {f1_default:.4f}\\") print(f\\"F1 Score with tuned threshold: {f1_tuned:.4f}\\") print(f\\"Optimal Decision Threshold: {tuned_model.best_threshold_:.4f}\\")"},{"question":"**Understanding and Using the poplib Module for POP3 Communication** # Objective Write a Python function that connects to a POP3 server, authenticates the user, retrieves all email message headers, and prints them. Additionally, flag all messages containing a specified keyword in their subject for deletion. # Function Signature ```python def manage_emails(server, port, username, password, keyword, use_ssl=False): Connect to a POP3 server, authenticate, retrieve, and print email headers, and flag specific messages for deletion. Parameters: - server (str): The hostname or IP address of the POP3 server. - port (int): The port number of the POP3 server (110 for non-SSL, 995 for SSL). - username (str): The username for authentication. - password (str): The password for authentication. - keyword (str): The keyword to search for in email subjects. - use_ssl (bool): Whether to use SSL for the connection (default is False). Returns: None ``` # Constraints and Requirements 1. The function should use the `poplib` module to manage the connection and communication with the server. 2. Ensure that the connection is established using SSL only if `use_ssl` is set to True. 3. Retrieve the header of each email in the mailbox and print it out. 4. Identify and flag for deletion any email that contains the specified `keyword` in its subject line. 5. Terminate the connection properly by calling the `quit` method to commit changes and unlock the mailbox. 6. Handle potential exceptions appropriately, providing meaningful error messages if the connection, authentication, or message retrieval fails. # Performance Considerations - Optimize the retrieval and filtering process for efficiency, particularly when handling a large number of emails. # Example Usage Assume there is a POP3 server running on `pop.example.com` that accepts SSL connections on port `995`: ```python manage_emails(\'pop.example.com\', 995, \'user@example.com\', \'password123\', \'Urgent\', use_ssl=True) ``` This call should connect to the POP3 server at `pop.example.com`, authenticate the user `user@example.com`, retrieve and print all email headers, and mark any email with \'Urgent\' in the subject for deletion before closing the connection.","solution":"import poplib from email.parser import Parser def manage_emails(server, port, username, password, keyword, use_ssl=False): Connect to a POP3 server, authenticate, retrieve, and print email headers, and flag specific messages for deletion. Parameters: - server (str): The hostname or IP address of the POP3 server. - port (int): The port number of the POP3 server (110 for non-SSL, 995 for SSL). - username (str): The username for authentication. - password (str): The password for authentication. - keyword (str): The keyword to search for in email subjects. - use_ssl (bool): Whether to use SSL for the connection (default is False). Returns: None try: if use_ssl: mail_server = poplib.POP3_SSL(server, port) else: mail_server = poplib.POP3(server, port) mail_server.user(username) mail_server.pass_(password) num_messages = len(mail_server.list()[1]) parser = Parser() for i in range(1, num_messages + 1): raw_message = b\'n\'.join(mail_server.top(i, 0)[1]) message = parser.parsestr(raw_message.decode(\'utf-8\')) print(f\\"Message {i} Header:\\") for header in message.keys(): print(f\\"{header}: {message[header]}\\") if \'Subject\' in message and keyword in message[\'Subject\']: mail_server.dele(i) mail_server.quit() except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage # manage_emails(\'pop.example.com\', 995, \'user@example.com\', \'password123\', \'Urgent\', use_ssl=True)"},{"question":"**Objective:** Write a Python program using the `contextvars` module to manage and manipulate context-specific variables. **Problem Statement:** You are required to implement a set of functions to demonstrate your understanding of `contextvars` module in Python. Specifically, you should: 1. Create a new context and set it as the current context. 2. Create a new context variable with a default value. 3. Set and retrieve values for this context variable in different contexts. 4. Implement a function to simulate a multi-threaded environment in which different threads enter and exit contexts, setting and getting context variable values. **Function Signatures:** 1. `create_context() -> contextvars.Context` 2. `create_context_var(name: str, default_value: Optional[Any] = None) -> contextvars.ContextVar` 3. `set_context_var(ctx_var: contextvars.ContextVar, value: Any) -> contextvars.Token` 4. `get_context_var(ctx_var: contextvars.ContextVar, default: Optional[Any] = None) -> Any` 5. `context_simulation() -> None` **Details:** 1. **create_context() -> contextvars.Context** - This function should create and return a new context. 2. **create_context_var(name: str, default_value: Optional[Any] = None) -> contextvars.ContextVar** - This function should create and return a new context variable with the given name and default value. 3. **set_context_var(ctx_var: contextvars.ContextVar, value: Any) -> contextvars.Token** - This function should set the value of the given context variable in the current context and return the generated token. 4. **get_context_var(ctx_var: contextvars.ContextVar, default: Optional[Any] = None) -> Any** - This function should retrieve the current value of the given context variable, returning the specified default value or the context variable\'s default if no value is set. 5. **context_simulation() -> None** - This function should simulate a multi-threaded environment: - Create two separate contexts and two context variables. - Set different values for the context variables in each context. - Retrieve and print the values of the context variables within each context. - Demonstrate context entering and exiting with context variables being set and reset. **Constraints:** - Use the contextvars module to handle context and context variables. - Demonstrate understanding of different contexts and how context variables can be managed within them. - Simulate multi-threading by using nested context and context variable operations. **Example Usage:** ```python ctx = create_context() ctx_var = create_context_var(\'example\', \'default\') token = set_context_var(ctx_var, \'new value\') print(get_context_var(ctx_var)) # Output: \'new value\' context_simulation() # Expected outputs will showcase different contexts and values set within them. ```","solution":"import contextvars # Function to create and return a new context def create_context(): Creates and returns a new context. return contextvars.copy_context() # Function to create a context variable with a default value def create_context_var(name, default_value=None): Creates and returns a new context variable with the given name and default value. return contextvars.ContextVar(name, default=default_value) # Function to set the value of a context variable in the current context def set_context_var(ctx_var, value): Sets the value of the given context variable in the current context and returns the generated token. return ctx_var.set(value) # Function to retrieve the current value of a context variable def get_context_var(ctx_var, default=None): Retrieves the current value of the given context variable, returning the specified default if no value is set. try: return ctx_var.get() except LookupError: return default # Function to simulate a multi-threaded environment working with contexts and context variables def context_simulation(): Simulates a multi-threaded environment by using nested context and context variable operations. ctx1 = create_context() ctx2 = create_context() ctx_var1 = create_context_var(\'var1\', \'default1\') ctx_var2 = create_context_var(\'var2\', \'default2\') # Enter context 1 ctx1.run(lambda: set_context_var(ctx_var1, \'value1-ctx1\')) ctx1.run(lambda: set_context_var(ctx_var2, \'value2-ctx1\')) # Enter context 2 ctx2.run(lambda: set_context_var(ctx_var1, \'value1-ctx2\')) ctx2.run(lambda: set_context_var(ctx_var2, \'value2-ctx2\')) # Retrieve and print values in context 1 def print_ctx1(): print(f\\"Context 1, var1: {get_context_var(ctx_var1)}\\") print(f\\"Context 1, var2: {get_context_var(ctx_var2)}\\") # Retrieve and print values in context 2 def print_ctx2(): print(f\\"Context 2, var1: {get_context_var(ctx_var1)}\\") print(f\\"Context 2, var2: {get_context_var(ctx_var2)}\\") ctx1.run(print_ctx1) ctx2.run(print_ctx2) if __name__ == \\"__main__\\": context_simulation()"},{"question":"Objective: The goal of this exercise is to assess your understanding of parallelism in scikit-learn, particularly how to use the `n_jobs` parameter and configure environment variables to control the number of threads and processes. Problem Description: You are provided with a dataset and your task is to: 1. Implement a function that trains a scikit-learn estimator using joblib for parallelism. 2. Configure the environment to optimize the training process by controlling the number of threads used by OpenMP and BLAS libraries. 3. Evaluate and compare the performance under different configurations. Function Signature: ```python def parallel_train_evaluate(X_train, y_train, estimator, param_grid, n_jobs, omp_threads, blas_threads): Trains a scikit-learn estimator using joblib for parallelism and configures the environment for optimal thread management. Parameters: - X_train (pd.DataFrame): The training feature set. - y_train (pd.Series): The training labels. - estimator (sklearn.base.BaseEstimator): The scikit-learn estimator to be trained. - param_grid (dict): The parameter grid for hyperparameter tuning. - n_jobs (int): Number of jobs for joblib\'s parallelism. - omp_threads (int): Number of threads for OpenMP. - blas_threads (int): Number of threads for BLAS libraries. Returns: - best_estimator_ (sklearn.base.BaseEstimator): The best estimator found by the grid search. - best_score_ (float): The best score achieved during the grid search. - elapsed_time (float): The time taken to perform the grid search in seconds. ``` Constraints: - `n_jobs` must be a positive integer. - `omp_threads` and `blas_threads` must be positive integers less than or equal to the total number of available CPUs. Performance Requirements: - The function should efficiently handle the configuration and evaluation of the parallelism settings. - Ensure that the environment variables are set before the training process and reset them after. Example Usage: ```python from sklearn.datasets import load_iris from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestClassifier import pandas as pd # Load dataset data = load_iris() X_train = pd.DataFrame(data.data, columns=data.feature_names) y_train = pd.Series(data.target) # Estimator and parameter grid estimator = RandomForestClassifier(random_state=42) param_grid = { \'n_estimators\': [10, 50], \'max_depth\': [None, 10] } # Function call best_estimator, best_score, elapsed_time = parallel_train_evaluate( X_train, y_train, estimator, param_grid, n_jobs=4, omp_threads=2, blas_threads=2 ) print(f\\"Best Estimator: {best_estimator}\\") print(f\\"Best Score: {best_score}\\") print(f\\"Elapsed Time: {elapsed_time}\\") ``` # Notes: - Use `joblib` for parallelism within a Grid Search. - Control the number of OpenMP and BLAS threads using `os.environ` to set environment variables before training. - Measure the elapsed time using Python\'s `time` module.","solution":"import time import os import pandas as pd from sklearn.model_selection import GridSearchCV from joblib import parallel_backend def parallel_train_evaluate(X_train, y_train, estimator, param_grid, n_jobs, omp_threads, blas_threads): # Setting environment variables for OpenMP and BLAS threading os.environ[\\"OMP_NUM_THREADS\\"] = str(omp_threads) os.environ[\\"MKL_NUM_THREADS\\"] = str(blas_threads) os.environ[\\"OPENBLAS_NUM_THREADS\\"] = str(blas_threads) os.environ[\\"NUMEXPR_NUM_THREADS\\"] = str(blas_threads) # Setting up and performing GridSearchCV with parallelism grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs=n_jobs, cv=5) start_time = time.time() with parallel_backend(\'threading\', n_jobs=n_jobs): grid_search.fit(X_train, y_train) elapsed_time = time.time() - start_time # Reset environment variables after job completion os.environ[\\"OMP_NUM_THREADS\\"] = \\"\\" os.environ[\\"MKL_NUM_THREADS\\"] = \\"\\" os.environ[\\"OPENBLAS_NUM_THREADS\\"] = \\"\\" os.environ[\\"NUMEXPR_NUM_THREADS\\"] = \\"\\" best_estimator_ = grid_search.best_estimator_ best_score_ = grid_search.best_score_ return best_estimator_, best_score_, elapsed_time"},{"question":"Objective You are required to write unit tests for a `ShoppingCart` class that interacts with an external `PaymentGateway` class. Your task is to use mock objects to ensure that the `ShoppingCart` correctly interacts with the `PaymentGateway`. Context The `ShoppingCart` class has the following methods: - `add_item(item, price)`: Adds an item with a specified price to the cart. - `checkout(payment_gateway)`: Uses the provided `payment_gateway` to process the payment for all items in the cart. The `PaymentGateway` class has the following method: - `process_payment(amount)`: Processes the payment of the specified amount. Requirements 1. Write unit tests for the `ShoppingCart` class using the `unittest` and `unittest.mock` modules. 2. Ensure that the `checkout` method calls the `process_payment` method of the `PaymentGateway` with the correct total amount. 3. Use mocks to simulate the `PaymentGateway` and verify the interactions. Implementation 1. Define the `ShoppingCart` and `PaymentGateway` classes as follows: ```python class ShoppingCart: def __init__(self): self.items = [] def add_item(self, item, price): self.items.append((item, price)) def checkout(self, payment_gateway): total_amount = sum(price for item, price in self.items) payment_gateway.process_payment(total_amount) class PaymentGateway: def process_payment(self, amount): pass ``` 2. Implement the following test cases using `unittest` and `unittest.mock`: - Test that `add_item` correctly adds items to the cart. - Test that `checkout` correctly calls `process_payment` with the total amount of all items in the cart using a mock `PaymentGateway`. Example Here is an example of how the test cases should be structured: ```python import unittest from unittest.mock import Mock class TestShoppingCart(unittest.TestCase): def test_add_item(self): cart = ShoppingCart() cart.add_item(\\"apple\\", 1.5) cart.add_item(\\"orange\\", 2.0) self.assertEqual(cart.items, [(\\"apple\\", 1.5), (\\"orange\\", 2.0)]) def test_checkout(self): cart = ShoppingCart() cart.add_item(\\"apple\\", 1.5) cart.add_item(\\"orange\\", 2.0) mock_payment_gateway = Mock() cart.checkout(mock_payment_gateway) mock_payment_gateway.process_payment.assert_called_once_with(3.5) if __name__ == \'__main__\': unittest.main() ``` Constraints - Do not modify the `ShoppingCart` or `PaymentGateway` class definitions. - You must use the `unittest.mock` module to create and configure the mock objects. Good luck!","solution":"class ShoppingCart: def __init__(self): self.items = [] def add_item(self, item, price): self.items.append((item, price)) def checkout(self, payment_gateway): total_amount = sum(price for item, price in self.items) payment_gateway.process_payment(total_amount) class PaymentGateway: def process_payment(self, amount): pass"},{"question":"Objective You are required to demonstrate your understanding of the `seaborn` library by creating a comprehensive visualization using multiple functionalities provided by the `seaborn.objects` module. This task is designed to test both your understanding of seaborn\'s advanced plotting capabilities and your ability to manipulate and transform data to create insightful visualizations. Question Using the `fmri` dataset from the `seaborn` library, perform the following tasks: 1. **Load the dataset.** 2. **Filter the data** to include only observations where the `region` is \'frontal\' and the `event` is \'cue\'. 3. **Create a line plot** that shows the `timepoint` on the x-axis and the `signal` on the y-axis. Each line should represent a different `subject`. 4. **Add markers** to the lines to indicate the sampled points. Customize the markers to have white edge colors. 5. **Add an error band** (confidence interval) to indicate the variance of signal values over time for the `cue` event. 6. **Differentiate the lines** by assigning different colors based on the `subject`. 7. **Customize the plot** (title, axis labels, etc.) for better readability. Constraints 1. **Data Manipulation**: Filtering and data manipulation should be done using Pandas, and the plotting should be performed using the `seaborn.objects` module. 2. **Performance**: Ensure the solution is optimized to handle large datasets efficiently. Expected Input and Output Format - **Input**: You do not need to provide any input as the datasets are loaded directly within the code. - **Output**: A visualization (plot) that meets the specified requirements. Sample Solution Structure Your solution should follow this structure: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Task 1: Load the dataset fmri = load_dataset(\\"fmri\\") # Task 2: Filter the data filtered_data = fmri.query(\\"region == \'frontal\' and event == \'cue\'\\") # Task 3: Create a line plot with group differentiation plot = ( so.Plot(filtered_data, x=\\"timepoint\\", y=\\"signal\\", color=\\"subject\\") # Extra configurations can be added here ) # Task 4: Add markers plot.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg()) # Task 5: Add an error band plot.add(so.Band(), so.Est(), group=\\"event\\") # Task 7: Customize the plot for readability plot.title(\\"Signal Over Time by Subject (Frontal Region, Cue Event)\\") plot.xlabel(\\"Timepoint\\") plot.ylabel(\\"Signal\\") # Display the plot plot.show() ``` Note - Ensure your plot meets all the requirements specified in the question. - Test your code to ensure compatibility and correct functionality. Good Luck!","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Task 1: Load the dataset fmri = load_dataset(\\"fmri\\") # Task 2: Filter the data filtered_data = fmri.query(\\"region == \'frontal\' and event == \'cue\'\\") # Task 3: Create a line plot with group differentiation plot = ( so.Plot(filtered_data, x=\\"timepoint\\", y=\\"signal\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") .scale(color=\\"subject\\") ) # Task 7: Customize the plot for readability plot.label(title=\\"Signal Over Time by Subject (Frontal Region, Cue Event)\\", xlabel=\\"Timepoint\\", ylabel=\\"Signal\\") # Display the plot plot.show()"},{"question":"**Problem Statement:** You are tasked with creating a secure data verification system using the `hmac` module. This system should be able to generate HMAC digests for given messages and verify the authenticity of messages using a shared secret key. **Function 1: generate_hmac** Implement a function `generate_hmac(key: bytes, msg: bytes, digestmod: str) -> str` that: - Takes a secret key (`key`) as a bytes object. - Takes a message (`msg`) as a bytes object. - Takes a digest mode (`digestmod`) as a string (e.g., \'sha256\'). - Returns the hexadecimal HMAC digest of the message. **Function 2: verify_hmac** Implement a function `verify_hmac(key: bytes, msg: bytes, digest: str, digestmod: str) -> bool` that: - Takes a secret key (`key`) as a bytes object. - Takes a message (`msg`) as a bytes object. - Takes a digest (`digest`) as a hexadecimal string. - Takes a digest mode (`digestmod`) as a string (e.g., \'sha256\'). - Returns `True` if the provided digest matches the generated HMAC digest for the given message, otherwise returns `False`. **Requirements:** - Utilize the `hmac` module and its functions to achieve the desired functionality. - Ensure the digest comparison in `verify_hmac` is safe from timing attacks. **Constraints:** - The key will be a non-empty bytes object. - The message will be a non-empty bytes object. - The digest mode will be a valid hash algorithm supported by `hashlib`. **Example:** ```python def generate_hmac(key: bytes, msg: bytes, digestmod: str) -> str: # Your implementation here def verify_hmac(key: bytes, msg: bytes, digest: str, digestmod: str) -> bool: # Your implementation here # Example usage: key = b\'secret_key\' msg = b\'This is a secret message\' digestmod = \'sha256\' # Generate HMAC hmac_digest = generate_hmac(key, msg, digestmod) print(hmac_digest) # Outputs a hex digest string # Verify HMAC is_valid = verify_hmac(key, msg, hmac_digest, digestmod) print(is_valid) # Outputs: True ``` Implement both functions in a Python script and ensure they work as expected.","solution":"import hmac import hashlib def generate_hmac(key: bytes, msg: bytes, digestmod: str) -> str: Generate an HMAC digest for the provided message and key using the specified digest mode. :param key: Secret key as bytes object. :param msg: Message to be authenticated as bytes object. :param digestmod: Digest mode to use (e.g., \'sha256\'). :return: Hexadecimal HMAC digest as a string. h = hmac.new(key, msg, digestmod) return h.hexdigest() def verify_hmac(key: bytes, msg: bytes, digest: str, digestmod: str) -> bool: Verify the HMAC digest for the provided message and key against a given digest. :param key: Secret key as bytes object. :param msg: Message to be authenticated as bytes object. :param digest: Hexadecimal HMAC digest to verify against. :param digestmod: Digest mode to use (e.g., \'sha256\'). :return: True if the provided digest matches the generated HMAC digest, else False. hmac_to_verify = generate_hmac(key, msg, digestmod) return hmac.compare_digest(hmac_to_verify, digest)"},{"question":"# Custom JSON Encoding and Decoding **Objective:** Your task is to implement a pair of functions: `encode_complex_dict` and `decode_complex_dict`. These functions should handle the JSON serialization and deserialization of a specific type of Python dictionary that can contain both complex numbers and standard key-value pairs. **Details:** 1. **Encoding Function:** - **Function Name:** `encode_complex_dict` - **Input:** A Python dictionary `data` that may contain complex numbers as values. - **Output:** A JSON string where complex numbers are converted into a JSON object with keys \\"real\\" and \\"imag\\". - **Behavior:** The function should utilize the `json.dumps` method with a custom encoder. **Example:** ```python input_data = { \\"name\\": \\"complex_example\\", \\"value\\": 3 + 4j, \\"nested\\": { \\"real_part\\": 5, \\"complex_part\\": 2 + 3j } } output = encode_complex_dict(input_data) print(output) ``` **Expected Output:** ```json { \\"name\\": \\"complex_example\\", \\"value\\": {\\"real\\": 3.0, \\"imag\\": 4.0}, \\"nested\\": { \\"real_part\\": 5, \\"complex_part\\": {\\"real\\": 2.0, \\"imag\\": 3.0} } } ``` 2. **Decoding Function:** - **Function Name:** `decode_complex_dict` - **Input:** A JSON string that represents a dictionary, potentially with complex numbers encoded as objects with \\"real\\" and \\"imag\\" keys. - **Output:** A Python dictionary where JSON objects with keys \\"real\\" and \\"imag\\" are converted back into complex numbers. - **Behavior:** The function should utilize the `json.loads` method with a custom decoder. **Example:** ```python input_json = \'\'\'{ \\"name\\": \\"complex_example\\", \\"value\\": {\\"real\\": 3.0, \\"imag\\": 4.0}, \\"nested\\": { \\"real_part\\": 5, \\"complex_part\\": {\\"real\\": 2.0, \\"imag\\": 3.0} } }\'\'\' output = decode_complex_dict(input_json) print(output) ``` **Expected Output:** ```python { \\"name\\": \\"complex_example\\", \\"value\\": 3 + 4j, \\"nested\\": { \\"real_part\\": 5, \\"complex_part\\": 2 + 3j } } ``` **Constraints:** - For the `encode_complex_dict` function, you must create a custom `JSONEncoder` subclass that handles complex numbers. - For the `decode_complex_dict` function, you must use the `object_hook` parameter in `json.loads` to handle the conversion back to complex numbers. - You must not use any external libraries; rely solely on Python\'s standard `json` module. **Submission:** Submit the implementation of both `encode_complex_dict` and `decode_complex_dict` functions. Your solution will be tested with various nested dictionaries containing complex numbers and other standard data types.","solution":"import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def encode_complex_dict(data): Encodes a dictionary containing complex numbers into a JSON string. Complex numbers are converted to JSON objects with keys \\"real\\" and \\"imag\\". return json.dumps(data, cls=ComplexEncoder) def decode_complex_dict(json_data): Decodes a JSON string into a dictionary, converting JSON objects with keys \\"real\\" and \\"imag\\" back into complex numbers. def as_complex(dct): if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct return json.loads(json_data, object_hook=as_complex)"},{"question":"# Question: Concurrent Number Processing Given a list of integers, design a Python program to perform the following operations concurrently using the `concurrent.futures` module. Your task is to implement a function `process_numbers(numbers: List[int]) -> Dict[str, int]` that performs these steps: 1. **Sum of Squares**: Compute the sum of the squares of all numbers in the list. 2. **Sum of Cubes**: Compute the sum of the cubes of all numbers in the list. 3. **Maximum Number**: Find the maximum number in the list. 4. **Minimum Number**: Find the minimum number in the list. Your function should return a dictionary with keys \'sum_of_squares\', \'sum_of_cubes\', \'max_number\', and \'min_number\' mapped to their corresponding computed values. Constraints: - The function must use concurrent futures to execute these operations concurrently. - Assume that the input list will have at least one integer. - You should handle large lists efficiently. Input: - A single list of integers, e.g., `[1, 2, 3, 4, 5]`. Output: - A dictionary with keys \'sum_of_squares\', \'sum_of_cubes\', \'max_number\', and \'min_number\', as described above, e.g., `{\'sum_of_squares\': 55, \'sum_of_cubes\': 225, \'max_number\': 5, \'min_number\': 1}`. Example: ```python def process_numbers(numbers): # Your code here # Example Usage numbers = [1, 2, 3, 4, 5] result = process_numbers(numbers) print(result) # Expected Output: {\'sum_of_squares\': 55, \'sum_of_cubes\': 225, \'max_number\': 5, \'min_number\': 1} ``` Your implementation should ensure that all operations are executed concurrently to improve efficiency.","solution":"from concurrent.futures import ThreadPoolExecutor from typing import List, Dict def sum_of_squares(numbers: List[int]) -> int: return sum(x ** 2 for x in numbers) def sum_of_cubes(numbers: List[int]) -> int: return sum(x ** 3 for x in numbers) def max_number(numbers: List[int]) -> int: return max(numbers) def min_number(numbers: List[int]) -> int: return min(numbers) def process_numbers(numbers: List[int]) -> Dict[str, int]: with ThreadPoolExecutor() as executor: sum_squares = executor.submit(sum_of_squares, numbers) sum_cubes = executor.submit(sum_of_cubes, numbers) max_num = executor.submit(max_number, numbers) min_num = executor.submit(min_number, numbers) return { \'sum_of_squares\': sum_squares.result(), \'sum_of_cubes\': sum_cubes.result(), \'max_number\': max_num.result(), \'min_number\': min_num.result(), }"},{"question":"# Problem Description You are tasked with creating a utility class `TempFileManager` that leverages the `tempfile` module to manage temporary files and directories. This class should provide methods to perform the following operations: 1. **Create a Temporary File**: Create a temporary file, write some data to it, read back the data, and then close the file. 2. **Create a Named Temporary File**: Create a named temporary file, write some data to it, read back the data using the file\'s name, and then close and delete the file. 3. **Create a Spooled Temporary File**: Create a spooled temporary file, write some data to it, read back the data, and then close the file. 4. **Create a Temporary Directory**: Create a temporary directory and return its name. Clean up the directory when it is no longer needed. # Requirements 1. **Class Definition**: Define a class `TempFileManager`. 2. **Method `create_temp_file`**: - **Input**: `data` (string) - **Output**: `read_data` (string) - **Functionality**: Creates a temporary file, writes `data` to it, reads the data back, closes the file, and returns the read data. 3. **Method `create_named_temp_file`**: - **Input**: `data` (string) - **Output**: `read_data` (string) - **Functionality**: Creates a named temporary file, writes `data` to it, reads the data back from the file using its name, closes the file, deletes the file, and returns the read data. 4. **Method `create_spooled_temp_file`**: - **Input**: `data` (string) - **Output**: `read_data` (string) - **Functionality**: Creates a spooled temporary file, writes `data` to it, reads the data back, closes the file, and returns the read data. 5. **Method `create_temp_directory`**: - **Input**: None - **Output**: `dir_name` (string) - **Functionality**: Creates a temporary directory and returns its name. Ensures the directory is cleaned up when it is no longer needed. # Constraints - The implementation should ensure proper cleanup of temporary files and directories to avoid resource leakage. - The methods should handle exceptions gracefully and provide meaningful error messages. # Example Usage ```python # Example usage of TempFileManager class # Create instance of TempFileManager temp_file_manager = TempFileManager() # Create and manage a temporary file temp_file_data = \\"Hello, world!\\" read_data = temp_file_manager.create_temp_file(temp_file_data) print(read_data) # Output: Hello, world! # Create and manage a named temporary file named_temp_file_data = \\"Python is fun!\\" read_data = temp_file_manager.create_named_temp_file(named_temp_file_data) print(read_data) # Output: Python is fun! # Create and manage a spooled temporary file spooled_temp_file_data = \\"This is a spooled file.\\" read_data = temp_file_manager.create_spooled_temp_file(spooled_temp_file_data) print(read_data) # Output: This is a spooled file. # Create and manage a temporary directory temp_dir = temp_file_manager.create_temp_directory() print(f\\"Created temporary directory: {temp_dir}\\") ``` # Submission Submit a single Python file containing: - The definition of the `TempFileManager` class. - Implementations of the required methods.","solution":"import tempfile import os class TempFileManager: def create_temp_file(self, data): try: with tempfile.TemporaryFile(mode=\'w+t\') as temp_file: temp_file.write(data) temp_file.seek(0) read_data = temp_file.read() return read_data except Exception as e: return str(e) def create_named_temp_file(self, data): try: with tempfile.NamedTemporaryFile(mode=\'w+t\', delete=False) as temp_file: temp_file_name = temp_file.name temp_file.write(data) temp_file.seek(0) read_data = temp_file.read() with open(temp_file_name, \'r\') as temp_file: read_data = temp_file.read() os.remove(temp_file_name) return read_data except Exception as e: return str(e) def create_spooled_temp_file(self, data): try: with tempfile.SpooledTemporaryFile(mode=\'w+t\') as temp_file: temp_file.write(data) temp_file.seek(0) read_data = temp_file.read() return read_data except Exception as e: return str(e) def create_temp_directory(self): try: temp_dir = tempfile.TemporaryDirectory() dir_name = temp_dir.name temp_dir.cleanup() # Ensures the directory is cleaned up return dir_name except Exception as e: return str(e)"},{"question":"# Task You are required to implement a Python function named `process_config` that reads a configuration file, updates certain values, and writes the updated configuration back to a new file. The function should also handle errors gracefully. # Function Signature ```python def process_config(input_filepath: str, output_filepath: str) -> None: ``` # Input - `input_filepath (str)`: The file path to the input configuration file. - `output_filepath (str)`: The file path to write the updated configuration file. # Constraints 1. The input configuration file may contain various sections with options in different data types (e.g., integers, floats, booleans, strings). 2. Certain sections and options should be updated as per the following rules: - In the `[forge.example]` section, the `User` option should be updated to `admin`. - In the `[topsecret.server.example]` section, the `Port` option should be set to `60022` and `ForwardX11` should be `yes`. 3. The function should add a new section named `[new.section]` with the following options: - `key1` set to `value1` - `key2` set to `value2` 4. If any of these sections or options do not exist, the function should create them. 5. Handle cases where the input file cannot be read or when sections/options are missing. # Output - The function does not return anything; it writes the updated configuration to the specified output file. # Example Usage ```python # Assume the \'input.ini\' file has the initial configuration. process_config(\'input.ini\', \'output.ini\') ``` # Expected Behavior Given an input configuration file `input.ini`: ``` [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [forge.example] User = hg [topsecret.server.example] Port = 50022 ForwardX11 = no ``` After running `process_config(\'input.ini\', \'output.ini\')`, the `output.ini` should be: ``` [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [forge.example] User = admin [topsecret.server.example] Port = 60022 ForwardX11 = yes [new.section] key1 = value1 key2 = value2 ``` # Notes 1. You are allowed to use the `configparser` module only. 2. Proper error handling should be implemented for cases where the file cannot be read or written. --- Implement the function `process_config` following the description above.","solution":"import configparser import os def process_config(input_filepath: str, output_filepath: str) -> None: Reads a configuration file, updates certain values, and writes the updated configuration back to a new file. :param input_filepath: The input configuration file path. :param output_filepath: The output configuration file path. config = configparser.ConfigParser() if not os.path.exists(input_filepath): raise FileNotFoundError(f\\"The input file {input_filepath} does not exist.\\") try: config.read(input_filepath) except configparser.Error as e: raise configparser.Error(f\\"Error reading the configuration file {input_filepath}: {e}\\") # Update user in [forge.example] section if \'forge.example\' not in config: config[\'forge.example\'] = {} config[\'forge.example\'][\'User\'] = \'admin\' # Update port and ForwardX11 in [topsecret.server.example] section if \'topsecret.server.example\' not in config: config[\'topsecret.server.example\'] = {} config[\'topsecret.server.example\'][\'Port\'] = \'60022\' config[\'topsecret.server.example\'][\'ForwardX11\'] = \'yes\' # Add new section [new.section] with key1 and key2 if \'new.section\' not in config: config[\'new.section\'] = {} config[\'new.section\'][\'key1\'] = \'value1\' config[\'new.section\'][\'key2\'] = \'value2\' # Try writing the updated configuration to output file try: with open(output_filepath, \'w\') as configfile: config.write(configfile) except IOError as e: raise IOError(f\\"Error writing the configuration file {output_filepath}: {e}\\")"},{"question":"**Objective:** Create a function that categorizes a list of filenames based on their MIME types using the `mimetypes` module. **Problem Statement:** You are tasked with categorizing a list of filenames into different MIME types. Write a Python function `categorize_files_by_mime(filenames, strict=True)` that takes a list of filenames and returns a dictionary where keys are MIME types and values are lists of filenames that belong to that MIME type. **Function Signature:** ```python def categorize_files_by_mime(filenames: list, strict: bool = True) -> dict: ``` **Input:** - `filenames` (list of str): A list of filenames. - `strict` (bool): An optional boolean flag (default is True), specifying whether to use only the official MIME types. **Output:** - (dict): A dictionary where keys are MIME types and values are lists of filenames that correspond to those MIME types. If a MIME type cannot be guessed for a filename, it should be categorized under the key `None`. **Constraints:** - The input list `filenames` will have at most 10,000 items. - Each filename will be a string of length not exceeding 255 characters. **Example:** ```python filenames = [\\"file.txt\\", \\"image.jpeg\\", \\"archive.zip\\", \\"unknownfile\\"] result = categorize_files_by_mime(filenames) ``` Expected output: ```python { \\"text/plain\\": [\\"file.txt\\"], \\"image/jpeg\\": [\\"image.jpeg\\"], \\"application/zip\\": [\\"archive.zip\\"], None: [\\"unknownfile\\"] } ``` **Requirements:** - Make use of the `mimetypes.guess_type` function to guess the MIME type of each filename. - Ensure that the function handles both the case where the type can be guessed and cannot be guessed. - The function should be efficient with a time complexity of O(n), where n is the number of filenames provided. **Notes:** - You may assume that the current working environment has access to a typical set of MIME type mappings.","solution":"import mimetypes def categorize_files_by_mime(filenames: list, strict: bool = True) -> dict: mime_dict = {} for filename in filenames: mime_type, _ = mimetypes.guess_type(filename, strict=strict) if mime_type not in mime_dict: mime_dict[mime_type] = [] mime_dict[mime_type].append(filename) return mime_dict"},{"question":"# Question: Advanced Calculation and Data Structuring with Generator and Comprehensions **Objective**: Write a Python function that leverages generator expressions, list comprehensions, and various types of expressions to perform advanced calculations and data structuring. This function will process a list of numerical data to generate various summary statistics and transformations. **Instructions**: 1. Define a function `process_data` that takes a single input: `data`, a list of numerical values. 2. The function should return a dictionary containing the following keys and their corresponding calculated values: - `\'squared\'`: A list of the squares of each number in the input list `data`. - `\'square_root\'`: A list of the square roots of each number in the input list `data`. Assume that the input list contains non-negative numbers. - `\'even_numbers\'`: A list of numbers from the input list that are even. - `\'sum\'`: The sum of the numbers in the input list `data`. - `\'mean\'`: The arithmetic mean (average) of the numbers in the input list `data`. - `\'max_min_diff\'`: The difference between the maximum and minimum values in the input list `data`. - `\'generator_squares\'`: A generator expression that yields the squares of each number in the input list `data`. **Constraints**: - You may assume that the input list `data` is non-empty and contains at least one number. **Example**: ```python def process_data(data): # Your implementation here # Example usage: result = process_data([1, 2, 3, 4, 5]) print(result) # Output should be: # { # \'squared\': [1, 4, 9, 16, 25], # \'square_root\': [1.0, 1.4142135623730951, 1.7320508075688772, 2.0, 2.23606797749979], # \'even_numbers\': [2, 4], # \'sum\': 15, # \'mean\': 3.0, # \'max_min_diff\': 4, # \'generator_squares\': <generator object process_data.<locals>.<genexpr> at 0x...> # } ``` **Notes**: - You should use generator expressions, list comprehensions, and appropriate arithmetic operations where applicable. - Ensure that your code is efficient and leverages Python\'s built-in capabilities for these tasks.","solution":"import math def process_data(data): Process the data to generate various summary statistics and transformations. result = {} result[\'squared\'] = [x ** 2 for x in data] result[\'square_root\'] = [math.sqrt(x) for x in data] result[\'even_numbers\'] = [x for x in data if x % 2 == 0] result[\'sum\'] = sum(data) result[\'mean\'] = sum(data) / len(data) result[\'max_min_diff\'] = max(data) - min(data) result[\'generator_squares\'] = (x ** 2 for x in data) return result"},{"question":"You are tasked with designing a small module in Python that orchestrates a set of functions and classes demonstrating fundamental concepts of Python\'s execution model, scope, naming, and error handling. Here\'s what you need to do: 1. **Create a function `calculate(num1, num2, operation)`** - **Input**: - `num1` (int): The first number. - `num2` (int): The second number. - `operation` (str): A string specifying the operation to be performed, one of `\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, `\\"divide\\"`. - **Output**: - Returns the result of the operation or `None`. - **Exceptions**: - If an invalid operation is specified, raise a `ValueError` with the message `\\"Invalid operation\\"`. - If a division by zero is attempted, raise a `ZeroDivisionError` with the appropriate message. 2. **Create a class `Calculator` with the following methods:** - **`__init__(self)`**: Initializes an empty list to store results of calculations. - **`execute(self, num1, num2, operation)`**: Executes a calculation using `calculate()` function and stores the result. Handle any exceptions raised by `calculate()` gracefully within this method. - **`get_results(self)`**: Returns the list of results stored. 3. **Create a function `safe_eval(expression)`** - **Input**: - `expression` (str): A string containing a mathematical expression (only addition, subtraction, multiplication, and division). - **Output**: - Returns the result of the evaluated expression. - **Constraints**: - Use the `eval()` function within a controlled environment to safely evaluate the expression. - Raise a `ValueError` if the expression is considered unsafe or invalid. **Example Usage:** ```python # Step 1 print(calculate(10, 5, \'add\')) # Output: 15 print(calculate(10, 0, \'divide\')) # Raise ZeroDivisionError # Step 2 calc = Calculator() calc.execute(10, 5, \'add\') calc.execute(10, 0, \'divide\') print(calc.get_results()) # Output: [15, None] # Step 3 print(safe_eval(\\"10 + 20 - 5\\")) # Output: 25 print(safe_eval(\\"__import__(\'os\').system(\'rm -rf /\')\\")) # Raises ValueError ``` **Important Notes:** - Utilize scope correctly within all functions and classes. - Ensure proper error handling to cover different scenarios. - Use the provided guidelines to design your functions efficiently and securely.","solution":"def calculate(num1, num2, operation): Perform basic arithmetic operations. Parameters: num1 (int): The first number. num2 (int): The second number. operation (str): The operation to perform - \\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\" Returns: The result of the operation or raises an error for invalid inputs. if operation == \\"add\\": return num1 + num2 elif operation == \\"subtract\\": return num1 - num2 elif operation == \\"multiply\\": return num1 * num2 elif operation == \\"divide\\": if num2 == 0: raise ZeroDivisionError(\\"division by zero\\") return num1 / num2 else: raise ValueError(\\"Invalid operation\\") class Calculator: A class to perform and store results of calculations. def __init__(self): self.results = [] def execute(self, num1, num2, operation): Executes calculation and stores the result. Parameters: num1 (int): The first number. num2 (int): The second number. operation (str): The operation to perform try: result = calculate(num1, num2, operation) self.results.append(result) except (ValueError, ZeroDivisionError) as e: print(f\\"Error occurred: {e}\\") self.results.append(None) def get_results(self): Returns the list of stored results. return self.results def safe_eval(expression): Safely evaluate a mathematical expression. Parameters: expression (str): The expression to evaluate. Returns: The result of the evaluated expression. allowed_names = {\\"__builtins__\\": None} try: result = eval(expression, allowed_names) return result except Exception as e: raise ValueError(\\"Invalid or unsafe expression\\") from e"},{"question":"# Objective The objective of this coding question is to assess your understanding of file and directory access in Python, specifically using the `pathlib`, `shutil`, and `tempfile` modules. # Task Write a Python function `organize_files_by_extension(source_dir)` that organizes files in the given directory `source_dir` into subdirectories based on their file extensions. The function should create a temporary directory for this operation, and once organized, move the subdirectories back to the `source_dir`. Function Signature ```python def organize_files_by_extension(source_dir: str) -> None: ``` Input - `source_dir` (str): The path to the source directory containing files to be organized. Output - None (The function should perform operations in place and not return anything. The original directory structure should be modified accordingly.) Example Assume the `source_dir` contains files as follows: ``` source_dir/ â”œâ”€â”€ image1.png â”œâ”€â”€ image2.jpg â”œâ”€â”€ document1.pdf â”œâ”€â”€ document2.docx â””â”€â”€ script.py ``` After calling `organize_files_by_extension(source_dir)`, the directory structure should look like: ``` source_dir/ â”œâ”€â”€ png/ â”‚ â””â”€â”€ image1.png â”œâ”€â”€ jpg/ â”‚ â””â”€â”€ image2.jpg â”œâ”€â”€ pdf/ â”‚ â””â”€â”€ document1.pdf â”œâ”€â”€ docx/ â”‚ â””â”€â”€ document2.docx â””â”€â”€ py/ â””â”€â”€ script.py ``` Constraints - Only non-directory files should be organized (subdirectories should remain unaffected). - Ensure that the solution is efficient and handles large directories gracefully. - Use the `pathlib` module for path manipulations. - Use the `shutil` module for moving files. - Use the `tempfile` module to create and manage temporary directories. Additional Notes - You can assume the directory paths provided are valid and accessible. - Handle any exceptions that may arise from file access operations. # Hints - You may find the `suffix` property and `mkdir` method of `pathlib.Path` useful. - Consider the `move` function from `shutil` for moving files. - The `TemporaryDirectory` context manager from `tempfile` can be helpful in managing temporary directories.","solution":"import shutil import tempfile from pathlib import Path def organize_files_by_extension(source_dir: str) -> None: source_path = Path(source_dir) if not source_path.is_dir(): raise ValueError(\\"source_dir must be a directory.\\") # Create a temporary directory with tempfile.TemporaryDirectory() as temp_dir: temp_path = Path(temp_dir) # Organize files by extension in the temporary directory for file in source_path.iterdir(): if file.is_file(): ext = file.suffix.lstrip(\'.\').lower() if not ext: ext = \'no_extension\' extension_dir = temp_path / ext extension_dir.mkdir(exist_ok=True) shutil.move(str(file), extension_dir / file.name) # Move organized files back to the source directory for ext_dir in temp_path.iterdir(): shutil.move(str(ext_dir), source_path / ext_dir.name)"},{"question":"Objective Write a Python program to compute factorials of a list of numbers using process-based parallelism and concurrent task execution. This will test your understanding of the `multiprocessing` and `concurrent.futures` modules for process-based parallelism, as well as your ability to handle concurrent tasks efficiently. Problem Statement You are given a list of non-negative integers. You need to compute the factorial of each number in the list using both the `multiprocessing` and `concurrent.futures` modules in Python to achieve process-based parallelism. Your task is to implement two solutions: 1. **Using `multiprocessing.Pool`:** - Implement a function `factorial_using_multiprocessing(numbers: List[int]) -> List[int]` that uses a pool of workers to parallelize the computation of factorials. 2. **Using `concurrent.futures.ProcessPoolExecutor`:** - Implement a function `factorial_using_concurrent_futures(numbers: List[int]) -> List[int]` that uses `ProcessPoolExecutor` to parallelize the computation of factorials. Input - `numbers`: A list of non-negative integers. For example: `[5, 7, 10, 3]` Output - Returns a list of integers representing the factorial of each number in the input list. For example, for input `[5, 7, 10, 3]`, the output should be `[120, 5040, 3628800, 6]`. Constraints - The input list can contain up to 10,000 elements. - Each number in the list is a non-negative integer less than or equal to 20. Example ```python def factorial_using_multiprocessing(numbers): # Your implementation here pass def factorial_using_concurrent_futures(numbers): # Your implementation here pass numbers = [5, 7, 10, 3] print(factorial_using_multiprocessing(numbers)) # Output: [120, 5040, 3628800, 6] print(factorial_using_concurrent_futures(numbers)) # Output: [120, 5040, 3628800, 6] ``` Notes - Ensure that your implementations handle any necessary process synchronization and return correct results. - The performance of your implementations will be evaluated based on their efficiency in handling large input lists.","solution":"import multiprocessing from concurrent.futures import ProcessPoolExecutor from math import factorial from typing import List def factorial_using_multiprocessing(numbers: List[int]) -> List[int]: with multiprocessing.Pool() as pool: results = pool.map(factorial, numbers) return results def factorial_using_concurrent_futures(numbers: List[int]) -> List[int]: with ProcessPoolExecutor() as executor: results = list(executor.map(factorial, numbers)) return results"},{"question":"# Python Signal Handling Assessment **Objective**: Demonstrate your understanding of Python\'s `signal` module by implementing signal handling in a practical scenario. # Problem Statement You are required to implement a Python program that monitors the execution time of a long-running computation and safely handles keyboard interruptions, such as `Ctrl + C`. If the computation exceeds a specified time limit, an alarm signal should be raised to abort the computation. # Requirements 1. **Signal Handling**: - Handle the `SIGALRM` signal to detect when the computation exceeds the time limit. - Handle the `SIGINT` signal to allow the user to safely interrupt the computation using `Ctrl + C`. 2. **Computation**: - Implement a dummy long-running computation function `long_running_computation()` which performs a simple loop that lasts indefinitely. 3. **Time Limiting**: - Implement a function `limit_computation_time(time_limit)` which: - Sets an alarm with a time limit (in seconds). - Starts the `long_running_computation()`. - If the computation time exceeds the `time_limit`, an exception should be raised, and the function should print \\"Computation timed out\\". - If interrupted by `Ctrl + C`, the function should print \\"Computation interrupted by user\\". # Function Specifications - `long_running_computation()` - **Input**: None - **Output**: None - **Behavior**: Enters an infinite loop performing a dummy computation. - `limit_computation_time(time_limit: int)` - **Input**: `time_limit` - An integer specifying the time limit in seconds. - **Output**: None - **Behavior**: Manages signal handling for `SIGALRM` and `SIGINT`, starts the computation, and handles any exceptions raised due to timeout or user interruption. # Constraints - You should only use the `signal` module for handling signals. - Your program should clean up by disabling the alarm once the computation is either completed, timed out, or interrupted. # Example Usage ```python import signal def long_running_computation(): while True: # Simulate some long-running computation pass def limit_computation_time(time_limit): def handle_alarm(signum, frame): raise TimeoutError(\\"Computation timed out\\") def handle_interrupt(signum, frame): raise KeyboardInterrupt(\\"Computation interrupted by user\\") signal.signal(signal.SIGALRM, handle_alarm) signal.signal(signal.SIGINT, handle_interrupt) signal.alarm(time_limit) try: long_running_computation() except TimeoutError as e: print(e) except KeyboardInterrupt as e: print(e) finally: signal.alarm(0) # Disable the alarm # Test the function with a time limit of 5 seconds limit_computation_time(5) ``` # Submission Submit a single `.py` file containing your implementation for the functions `long_running_computation()` and `limit_computation_time(time_limit)`.","solution":"import signal import time def long_running_computation(): while True: # Simulate some long-running computation time.sleep(1) def limit_computation_time(time_limit): def handle_alarm(signum, frame): raise TimeoutError(\\"Computation timed out\\") def handle_interrupt(signum, frame): raise KeyboardInterrupt(\\"Computation interrupted by user\\") signal.signal(signal.SIGALRM, handle_alarm) signal.signal(signal.SIGINT, handle_interrupt) signal.alarm(time_limit) try: long_running_computation() except TimeoutError as e: print(e) except KeyboardInterrupt as e: print(e) finally: signal.alarm(0) # Disable the alarm"},{"question":"# Question: Implementing and Utilizing Custom Metric Handlers in Distributed Training **Objective**: Demonstrate understanding of metric collection and handling during distributed training using the `torch.distributed.elastic.metrics` module. **Problem Statement**: Imagine you are working on a distributed training project where the training performance needs to be monitored and logged. The core task here is to implement a custom metric handler that logs metrics to a file and then utilize it in a simple simulated distributed training process. **Tasks**: 1. Implement a custom metric handler class `FileMetricHandler` that inherits from `MetricHandler`. This handler should: - Initialize with a file path. - Write metrics to the specified file in a structured format. 2. Simulate a simple distributed training loop where: - Each training step produces a mock metric (e.g., loss value). - The metric is logged using the `FileMetricHandler`. **Function Signatures**: ```python # Define the FileMetricHandler Class class FileMetricHandler(MetricHandler): def __init__(self, file_path: str): Initialize the FileMetricHandler with the file path. :param file_path: Path to the file where metrics should be logged. pass def handle(self, name: str, value: float, timestamp: Optional[float] = None): Log the given metric to the file. :param name: Name of the metric. :param value: Value of the metric. :param timestamp: Optional timestamp for the metric. pass # Simulate the distributed training process def simulate_training(metric_handler: MetricHandler, steps: int): Simulate a distributed training process that logs metrics using the provided handler. :param metric_handler: Instance of a MetricHandler used for logging metrics. :param steps: Number of training steps to simulate. for step in range(steps): # Simulate a mock loss value mock_loss_value = 1.0 / (step + 1) metric_handler.handle(name=\\"loss\\", value=mock_loss_value) ``` **Input/Output**: 1. The implementation of the `FileMetricHandler` class and the `simulate_training` function. 2. The `FileMetricHandler` should log metrics to the specified file path in a readable format. 3. The `simulate_training` function should run the simulation for a given number of steps and utilize the provided metric handler for logging the mock metrics. **Constraints**: - Your implementation should handle any potential exceptions (e.g., file not found, write permissions, etc.). - Ensure the file is properly closed after logging. **Performance Requirements**: - Efficiently handle multiple metric logging within the loop. - Ensure that file handling operations are optimized for performance. **Example Usage**: ```python # Example usage of the FileMetricHandler in a simulated training handler = FileMetricHandler(file_path=\\"metrics.log\\") simulate_training(metric_handler=handler, steps=10) # Check for the content of the \\"metrics.log\\" which should contain logged metrics. ``` **Note**: Ensure that your code adheres to proper coding standards and document any assumptions or necessary explanations within your implementation.","solution":"import os from datetime import datetime from typing import Optional # Assuming `MetricHandler` is a base class provided by torch.distributed.elastic.metrics. class MetricHandler: def handle(self, name: str, value: float, timestamp: Optional[float] = None): raise NotImplementedError(\\"Subclasses should implement this!\\") class FileMetricHandler(MetricHandler): def __init__(self, file_path: str): Initialize the FileMetricHandler with the file path. :param file_path: Path to the file where metrics should be logged. self.file_path = file_path def handle(self, name: str, value: float, timestamp: Optional[float] = None): Log the given metric to the file. :param name: Name of the metric. :param value: Value of the metric. :param timestamp: Optional timestamp for the metric. timestamp = timestamp or datetime.now().timestamp() with open(self.file_path, \'a\') as f: f.write(f\\"{datetime.fromtimestamp(timestamp)} - {name}: {value}n\\") def simulate_training(metric_handler: MetricHandler, steps: int): Simulate a distributed training process that logs metrics using the provided handler. :param metric_handler: Instance of a MetricHandler used for logging metrics. :param steps: Number of training steps to simulate. for step in range(steps): # Simulate a mock loss value mock_loss_value = 1.0 / (step + 1) metric_handler.handle(name=\\"loss\\", value=mock_loss_value)"},{"question":"# Parallel Data Processing with `concurrent.futures` You have been given a large dataset stored in a text file, where each line contains a number. Your task is to process this dataset to find the square of each number and write the results to an output file. The dataset is large enough that processing it sequentially would take a considerable amount of time, so you should use the `concurrent.futures` module to parallelize this task. Requirements: 1. **Read from Input File**: Read numbers from an input file named `input.txt`. 2. **Process in Parallel**: Use `ThreadPoolExecutor` to compute the square of each number in parallel. 3. **Write to Output File**: Write the results to an output file named `output.txt`, maintaining the order of numbers as in the input file. Detailed Steps: 1. **Reading the File**: - Read the `input.txt` file and parse each line as a number. 2. **Computing Squares**: - Use the `ThreadPoolExecutor` to parallelize the computation of the squares of the numbers read from the file. - Ensure there are at most 5 worker threads in the thread pool. 3. **Writing the Output File**: - Write the squared numbers to `output.txt` in the same order as they appeared in the input file. Input and Output Format: - **Input**: A file named `input.txt` with each line containing a single integer. - **Output**: A file named `output.txt` with each line containing the square of the corresponding integer from `input.txt` in the same order. Constraints: - Assume that the input file is large and may contain thousands of numbers. - Ensure that your implementation handles errors gracefully and provides meaningful error messages. Example: Given an `input.txt` file with the following content: ``` 2 3 4 ``` The `output.txt` file should contain: ``` 4 9 16 ``` Notes: - You must use the `ThreadPoolExecutor` from the `concurrent.futures` module. - Pay attention to resource cleanup and handle exceptions properly. Implementation: Write a Python function `process_data_in_parallel` that carries out the outlined task. The function signature should be as follows: ```python def process_data_in_parallel(input_file_path: str, output_file_path: str) -> None: # Your implementation here ``` # Your Answer","solution":"import concurrent.futures def process_data_in_parallel(input_file_path: str, output_file_path: str) -> None: Processes the numbers from the input file, computes their squares in parallel, and writes the results to the output file, maintaining the order of numbers. numbers = [] # Read the input file with open(input_file_path, \'r\') as file: for line in file: numbers.append(int(line.strip())) def compute_square(num): return num ** 2 # Process squares in parallel with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: squared_numbers = list(executor.map(compute_square, numbers)) # Write the squared numbers to the output file with open(output_file_path, \'w\') as file: for squared in squared_numbers: file.write(f\\"{squared}n\\")"},{"question":"Objective: Write a Python program that uses the `pty` module to create a pseudo-terminal, fork a child process, and execute a shell command in the child process. The parent process should read and print the output of the command executed in the child process. # Requirements: 1. **Function to Implement**: Write a function `execute_command(command: str) -> int` that takes a shell command as a string argument and returns the exit status code of the command executed in the child process. 2. **Implementation Details**: - Use the `pty.fork()` function to fork the current process. - In the child process (where `pid` is 0), use `os.execlp()` to execute the given command using the shell. - In the parent process, read the output from the child process using the file descriptor obtained from `pty.fork()`, and print the output to the standard output. - The function should return the exit status code of the child process. 3. **Constraints**: - The command input is guaranteed to be a valid shell command. - Handle all common exceptions including unexpected terminations and I/O errors gracefully. # Example Usage: ```python if __name__ == \\"__main__\\": command = \\"ls -l\\" exit_code = execute_command(command) print(f\\"Command exited with status: {exit_code}\\") ``` # Expected Function Signature: ```python def execute_command(command: str) -> int: # Your implementation here ``` # Additional Notes: - This problem requires a good understanding of process forking, executing shell commands, and handling I/O streams in Unix-like operating systems. - Pay special attention to ensuring that the parent process properly reads the entire output from the child process before exiting.","solution":"import os import pty import select def execute_command(command: str) -> int: def read(fd): output = b\\"\\" while True: r, _, _ = select.select([fd], [], [], 0.1) if fd in r: try: data = os.read(fd, 1024) if not data: break output += data except OSError: break return output.decode() pid, fd = pty.fork() if pid == 0: # Child process os.execlp(\\"/bin/sh\\", \\"sh\\", \\"-c\\", command) else: # Parent process output = read(fd) print(output) _, status = os.waitpid(pid, 0) return os.WEXITSTATUS(status)"},{"question":"**Text Processing Challenge: Text Analyzer** In this challenge, you are required to write a Python function that takes a paragraph of text and performs a series of text processing operations on it. The goal is to analyze the text and provide formatted output as described below. Create a function `analyze_text(paragraph: str) -> dict` which performs the following steps: 1. **Word Frequency Count**: - Count the frequency of each word in the paragraph. Words are separated by whitespace. - Words should be considered case-insensitively (e.g., \\"Python\\" and \\"python\\" are the same word). 2. **Finding All Adverbs**: - Use regular expressions to find all adverbs in the text. Adverbs typically end in \\"ly\\" (e.g., quickly, slowly). 3. **Text Wrapping**: - Wrap the text (after converting the whole paragraph to lowercase) so that each line has a maximum width of 50 characters. 4. **Unicode Normalization**: - Normalize the text using NFC (Normalization Form C), which composes characters into their canonical composed form. 5. **Output Format**: - The function should return a dictionary with the following keys: - `word_count`: A dictionary with words as keys and their frequency counts as values. - `adverbs`: A list of all adverbs found in the text. - `wrapped_text`: A list of strings where each string is a line of the wrapped text. - `normalized_text`: A single string representing the normalized text. # Input - `paragraph` (str): A paragraph of text (length between 1 and 1000 characters). # Output - `result` (dict): A dictionary with the keys: `word_count`, `adverbs`, `wrapped_text`, and `normalized_text`, as described above. # Constraints - The length of the paragraph will be between 1 and 1000 characters. - You must use the relevant modules from Python\'s text processing services to achieve the desired functionality. # Example ```python paragraph = \\"Python is amazing. Python programming is fun. Quickly learn Python to progress rapidly and efficiently.\\" result = analyze_text(paragraph) print(result[\'word_count\']) # Output: {\'python\': 3, \'is\': 2, \'amazing.\': 1, \'programming\': 1, \'fun.\': 1, \'quickly\': 1, \'learn\': 1, \'to\': 1, \'progress\': 1, \'rapidly\': 1, \'and\': 1, \'efficiently.\': 1} print(result[\'adverbs\']) # Output: [\'quickly\', \'rapidly\', \'efficiently\'] print(result[\'wrapped_text\']) # Output: [\'python is amazing. python programming is fun.\', # \'quickly learn python to progress rapidly and\', # \'efficiently.\'] print(result[\'normalized_text\']) # Output: \\"python is amazing. python programming is fun. quickly learn python to progress rapidly and efficiently.\\" ``` # Note - This question tests comprehension of various modules (`re`, `textwrap`, `unicodedata`) and requires integration of concepts to create a composite solution.","solution":"import re import textwrap import unicodedata def analyze_text(paragraph: str) -> dict: # Word Frequency Count words = re.findall(r\'bw+b\', paragraph.lower()) word_count = {} for word in words: word_count[word] = word_count.get(word, 0) + 1 # Finding All Adverbs adverbs = re.findall(r\'bw+lyb\', paragraph.lower()) # Text Wrapping wrapped_text = textwrap.wrap(paragraph.lower(), width=50) # Unicode Normalization normalized_text = unicodedata.normalize(\'NFC\', paragraph.lower()) return { \'word_count\': word_count, \'adverbs\': adverbs, \'wrapped_text\': wrapped_text, \'normalized_text\': normalized_text }"},{"question":"# Unix User Lookup with Filters You are tasked with creating a function that allows querying the Unix password database with specific filtering capabilities. Your function should leverage the `pwd` module to retrieve user details based on various criteria. Function Signature ```python def filter_users_by_criteria(uid_range=None, gid_range=None, shell=None) -> list: ``` Parameters - `uid_range` (tuple): A tuple of two integers `(min_uid, max_uid)` that specifies the inclusive range of user IDs to filter. If not provided or `None`, no filtering on user IDs should be applied. - `gid_range` (tuple): A tuple of two integers `(min_gid, max_gid)` that specifies the inclusive range of group IDs to filter. If not provided or `None`, no filtering on group IDs should be applied. - `shell` (str): A string specifying the command interpreter (shell) users should have to be included in the results. If not provided or `None`, no filtering on shells should be applied. Return Value - Returns a list of dictionaries, where each dictionary contains user information (keys as attribute names: `pw_name`, `pw_uid`, `pw_gid`, `pw_dir`, `pw_shell`) for users that match all provided criteria. If no users match, returns an empty list. Example ```python # Example usage: # Suppose the command \\"pwd.getpwall()\\" returns: # [ # pwd.struct_passwd(pw_name=\'user1\', pw_passwd=\'x\', pw_uid=1001, pw_gid=1001, pw_gecos=\'\', pw_dir=\'/home/user1\', pw_shell=\'/bin/bash\'), # pwd.struct_passwd(pw_name=\'user2\', pw_passwd=\'x\', pw_uid=1002, pw_gid=1002, pw_gecos=\'\', pw_dir=\'/home/user2\', pw_shell=\'/bin/zsh\') # ] # filter_users_by_criteria(uid_range=(1000, 1005), shell=\'/bin/bash\') # should return: # [ # {\'pw_name\': \'user1\', \'pw_uid\': 1001, \'pw_gid\': 1001, \'pw_dir\': \'/home/user1\', \'pw_shell\': \'/bin/bash\'} # ] ``` This task assesses your ability to use the `pwd` module, work with tuples and dictionaries, apply filtering based on multiple criteria, and return structured information. Constraints - You may assume that the `pwd` module is available on the system where this code will run. - Ensure that the function performs efficiently even if there are many entries in the password database.","solution":"import pwd def filter_users_by_criteria(uid_range=None, gid_range=None, shell=None) -> list: users = pwd.getpwall() filtered_users = [] for user in users: if (uid_range is None or (uid_range[0] <= user.pw_uid <= uid_range[1])) and (gid_range is None or (gid_range[0] <= user.pw_gid <= gid_range[1])) and (shell is None or user.pw_shell == shell): filtered_users.append({ \'pw_name\': user.pw_name, \'pw_uid\': user.pw_uid, \'pw_gid\': user.pw_gid, \'pw_dir\': user.pw_dir, \'pw_shell\': user.pw_shell }) return filtered_users"},{"question":"**Question: Email Organizer** In this exercise, you will implement a function that organizes emails from an mbox mailbox file into categorized Maildir mailboxes based on their subject lines. Your function should split emails into different folders depending on specific keywords present in the subject line. The Maildir mailboxes must be created if they do not already exist. # Requirements 1. Implement the function `organize_emails(mbox_path, keywords_to_folders, output_dir)`. 2. The function should accept three parameters: - `mbox_path`: A string representing the path to the mbox file containing the emails to be organized. - `keywords_to_folders`: A dictionary where keys are keywords (strings) and values are folder names (strings). Emails containing the keyword in their subject should be moved to the corresponding folder. - `output_dir`: A string representing the directory where the Maildir mailboxes will be stored or created. 3. For each email in the mbox file: - If the subject contains one of the keywords from `keywords_to_folders`, move the email to the corresponding folder (Maildir). - If an email does not contain any of the specified keywords, it should be placed in a \\"misc\\" folder. 4. Ensure that the created Maildir mailboxes are properly structured and emails are correctly moved without data loss or corruption. # Constraints - Assume that `mbox_path` points to a valid mbox file. - Assume that keywords in `keywords_to_folders` are unique and do not overlap. - There are no specific performance constraints, but the solution should handle reasonable mailbox sizes efficiently. # Example ```python def organize_emails(mbox_path, keywords_to_folders, output_dir): # Your code here ``` **Test Case:** ```python mbox_path = \\"path/to/your/mboxfile\\" keywords_to_folders = { \\"python\\": \\"python-related\\", \\"invoice\\": \\"invoices\\", \\"meeting\\": \\"meetings\\" } output_dir = \\"path/to/maildir/output\\" organize_emails(mbox_path, keywords_to_folders, output_dir) ``` After executing the function, the `output_dir` should contain Maildir mailboxes with folders \\"python-related\\", \\"invoices\\", \\"meetings\\", and \\"misc\\", each containing appropriately categorized emails from the mbox file. **Notes:** - Use the `mailbox` module to handle mailbox file operations. - Be cautious about file locking and unlocking as stated in the documentation, especially to avoid concurrent modification issues.","solution":"import os import mailbox import email def create_maildir(base_path, folder_name): folder_path = os.path.join(base_path, folder_name) maildir = mailbox.Maildir(folder_path, factory=None, create=True) return maildir def organize_emails(mbox_path, keywords_to_folders, output_dir): # Open the mbox file mbox = mailbox.mbox(mbox_path) # Create a default \'misc\' Maildir misc_maildir = create_maildir(output_dir, \'misc\') # Create Maildirs for each keyword maildirs = {keyword: create_maildir(output_dir, folder) for keyword, folder in keywords_to_folders.items()} for message in mbox: subject = message[\'subject\'] or \'\' assigned = False for keyword, maildir in maildirs.items(): if keyword in subject.lower(): maildir.add(message) assigned = True break if not assigned: misc_maildir.add(message) # Commit changes to the Maildir for maildir in maildirs.values(): maildir.flush() misc_maildir.flush()"},{"question":"# Advanced Debugging and Profiling Task You are provided with a Python script that contains both logical errors and performance bottlenecks. Your task is to identify and fix these issues using Python\'s debugging and profiling tools, as described in the provided documentation. Problem Statement 1. **Debugging**: The script below contains errors that cause it to crash. Use the `faulthandler` and `pdb` modules to identify and fix the errors. 2. **Profiling**: Once the script is running correctly, it does not perform efficiently for large input sizes. Use the `cProfile` and `timeit` modules to identify the bottlenecks and optimize the code. Script ```python import random def create_large_list(size): # Potential fault here large_list = [random.randint(0, size) for _ in range(0, size)] return large_list def find_duplicates(input_list): # Inefficient implementation duplicates = [] for i in range(len(input_list)): for j in range(i + 1, len(input_list)): if input_list[i] == input_list[j] and input_list[i] not in duplicates: duplicates.append(input_list[i]) return duplicates def main(): size = 1000 # You can increase this size for performance testing large_list = create_large_list(size) duplicates = find_duplicates(large_list) print(\\"Duplicates found:\\", duplicates) if __name__ == \'__main__\': main() ``` Requirements 1. **Debugging**: - Use `faulthandler` to dump the traceback on error. - Use `pdb` to step through the code and identify the root cause of any errors. - Fix the errors to ensure the script runs without crashing. 2. **Profiling**: - Use `cProfile` to profile the performance of the script. - Identify the function or section of code causing the performance issue. - Use `timeit` to measure and verify the runtime of specific code snippets. - Optimize the code to improve performance without altering the original functionality. 3. **Documentation**: - Provide a brief explanation of the debugging process you followed. - Describe the profiling results and the improvements you made to the code. Input and Output Formats - The input is implicitly defined within the script (`size` variable). You may modify this for testing purposes. - The output is a list of duplicates found in the randomly generated list. Constraints - The `main` function should work without modifications, other than increasing the list size for performance testing. - Your solution should handle cases where `size` is very large (e.g., 100,000 or more) efficiently. Submission Provide the modified script with inline comments highlighting the changes made for debugging and performance optimization. Additionally, include a separate section explaining your debugging and profiling approach, and the final performance results.","solution":"import random # Debugging with faulthandler and pdb import faulthandler faulthandler.enable() import pdb def create_large_list(size): # Fixed potential bug: randint should use size-1 as the upper bound large_list = [random.randint(0, size - 1) for _ in range(0, size)] return large_list def find_duplicates(input_list): # Optimized the function to use a set for O(n) performance seen = set() duplicates = set() for item in input_list: if item in seen: duplicates.add(item) else: seen.add(item) return list(duplicates) def main(): size = 1000 # Increase size for performance testing large_list = create_large_list(size) duplicates = find_duplicates(large_list) print(\\"Duplicates found:\\", duplicates) if __name__ == \'__main__\': main()"},{"question":"# PyTorch FX Coding Assessment Objective Your task is to implement a function that transforms a given `torch.nn.Module` by replacing all ReLU activations with LeakyReLU activations using the `torch.fx` library. Input - A `torch.nn.Module` class that may contain ReLU activations. Output - A new `torch.nn.Module` where all instances of `torch.nn.functional.relu` are replaced with `torch.nn.functional.leaky_relu`. Constraints - Ensure that the transformed module retains all other behaviors of the original module. - The function should handle modules that don\'t contain any ReLU activations gracefully. - Use the provided code skeleton to implement your solution. ```python import torch import torch.fx import torch.nn.functional as F # Sample module for demonstration, you can customize this as well. class SampleModel(torch.nn.Module): def __init__(self): super(SampleModel, self).__init__() self.linear1 = torch.nn.Linear(10, 10) self.linear2 = torch.nn.Linear(10, 10) def forward(self, x): x = F.relu(self.linear1(x)) x = self.linear2(x) x = F.relu(x) return x # Function to transform the model def replace_relu_with_leaky_relu(model: torch.nn.Module) -> torch.nn.Module: # Implement the transformation logic here class ReplaceReluTracer(torch.fx.Tracer): def is_leaf_module(self, module: torch.nn.Module, module_qualified_name: str) -> bool: return False graph: torch.fx.Graph = ReplaceReluTracer().trace(model) for node in graph.nodes: if node.op == \'call_function\' and node.target == F.relu: node.target = F.leaky_relu transformed_model = torch.fx.GraphModule(model, graph) return transformed_model # Example usage if __name__ == \'__main__\': model = SampleModel() transformed_model = replace_relu_with_leaky_relu(model) print(\\"Original Model:\\") print(model) print(\\"Transformed Model:\\") print(transformed_model) ``` Performance Requirements - The transformation should be efficient and should not unnecessarily increase the computational complexity of the model. - Ensure that the transformed model performs equivalently to the original model in terms of output (except for the activation function change).","solution":"import torch import torch.fx import torch.nn.functional as F # Sample module for demonstration class SampleModel(torch.nn.Module): def __init__(self): super(SampleModel, self).__init__() self.linear1 = torch.nn.Linear(10, 10) self.linear2 = torch.nn.Linear(10, 10) def forward(self, x): x = F.relu(self.linear1(x)) x = self.linear2(x) x = F.relu(x) return x # Function to transform the model def replace_relu_with_leaky_relu(model: torch.nn.Module) -> torch.nn.Module: # Implement the transformation logic here class ReplaceReluTracer(torch.fx.Tracer): def is_leaf_module(self, module: torch.nn.Module, module_qualified_name: str) -> bool: return False graph: torch.fx.Graph = ReplaceReluTracer().trace(model) for node in graph.nodes: if node.op == \'call_function\' and node.target == F.relu: node.target = F.leaky_relu transformed_model = torch.fx.GraphModule(model, graph) return transformed_model"},{"question":"# Python Lexical Analyzer You are tasked with implementing a function that processes a sequence of lines of Python code. This function should identify and separate out tokens based on the lexical analysis rules outlined in the documentation, namely handling comments, line continuations, and literals. Function Signature ```python def lexical_analyzer(code_lines: List[str]) -> List[str]: ``` Input - `code_lines`: A list of strings, where each string represents a line of Python code. Output - Returns a list of tokens extracted from the given lines of Python code. Each token should be a string. Requirements 1. **Comments**: Any text following a `#` character on a line should be ignored. 2. **Line Continuations**: Handle both explicit (using ``) and implicit (within parentheses, square brackets, or curly braces) line continuations. 3. **String Literals**: Recognize single, double, and triple-quoted string literals, taking into account escape sequences and raw strings. 4. **Bytes Literals**: Recognize bytes literals prefixed with `b` or `B`. 5. **Numeric Literals**: Recognize integer, floating point, and imaginary literals. 6. **Whitespace**: Whitespace within tokens should be ignored but used to separate tokens where necessary. 7. **Operators and Delimiters**: Recognize and treat operators and delimiters as individual tokens. Constraints - Ignore any invalid Python syntax; assume all input lines are valid. - Do not handle multi-line comments or docstrings for this task. Example ```python code_lines = [ \\"def example():\\", \\" x = 5 + 4 # addition\\", \\" y = (x * 2)\\", \\" z = \'Hello, \'\\", \\" \'world!\'\\", \\" return b\'bytes_literal\'\\" ] # Expected Output [ \'def\', \'example\', \'(\', \')\', \':\', \'x\', \'=\', \'5\', \'+\', \'4\', \'y\', \'=\', \'(\', \'x\', \'*\', \'2\', \')\', \'z\', \'=\', \\"\'Hello, \'\\", \\"\'world!\'\\", \'return\', \\"b\'bytes_literal\'\\" ] ``` Implement the function `lexical_analyzer` to meet the above requirements.","solution":"import re from typing import List def lexical_analyzer(code_lines: List[str]) -> List[str]: tokens = [] # Join lines to handle explicit line continuations joined_code = \' \'.join(line.split(\'#\')[0].strip() for line in code_lines) joined_code = re.sub(r\'s*\', \'\', joined_code) # Define patterns for different types of tokens string_pattern = r\'(b?\'\'\'(?:.|[^\'])*\'\'\'|b?\\"\\"\\"(?:.|[^\\"])*\\"\\"\\"|b?\'(?:.|[^\'])*\'|b?\\"(?:.|[^\\"])*\\")\' numeric_pattern = r\'(bd+.d+|bd+.b|bd+b|0x[a-fA-F0-9]+b|0b[01]+b|0o[0-7]+b)\' operator_pattern = r\'([+-*/%=()[]{},:<>])\' combined_pattern = \'|\'.join([string_pattern, numeric_pattern, operator_pattern, r\'(w+)\']) # Find all tokens matching the combined pattern for match in re.finditer(combined_pattern, joined_code): token = match.group(0) tokens.append(token) return tokens"},{"question":"**Objective**: Demonstrate understanding and application of feature extraction techniques using scikit-learn\'s `feature_extraction` module. **Problem Statement**: You are given a collection of text documents and a set of labeled images. Your task is to preprocess and extract features from both the text and image datasets. You will then concatenate these feature sets and output the final feature matrix. **Requirements**: 1. Vectorize the text data using `CountVectorizer` with bigrams. 2. Normalize text features using `TfidfTransformer`. 3. Extract 2x2 patches from each image using `PatchExtractor`. 4. Combine the text and image features into a final feature matrix. 5. Implement the function `extract_and_combine_features` to achieve the above steps. **Inputs**: - `text_documents`: List of strings, each representing a text document. - `images`: Numpy array of shape (n_samples, height, width, n_channels). **Outputs**: - Numpy array: A combined feature matrix where each row represents a sample. **Constraints**: - Text documents can contain arbitrary text with a maximum length of 1000 characters. - Images are of size 32x32 with 3 color channels. - Use `CountVectorizer` with `ngram_range=(1, 2)` for text feature extraction. - Use `PatchExtractor` to extract 2x2 patches from the images. # Function Signature ```python import numpy as np from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.feature_extraction import image def extract_and_combine_features(text_documents, images): This function processes text documents and images to extract features and combines them into a single feature matrix. Parameters: text_documents (list of str): A list of text documents. images (numpy array): A numpy array of images (n_samples, height, width, n_channels). Returns: numpy array: Combined feature matrix where each row represents a sample. # Step 1: Vectorize text data using CountVectorizer with bigrams vectorizer = CountVectorizer(ngram_range=(1, 2)) text_features = vectorizer.fit_transform(text_documents).toarray() # Step 2: Normalize text features using TfidfTransformer tfidf_transformer = TfidfTransformer() text_features = tfidf_transformer.fit_transform(text_features).toarray() # Step 3: Extract patches from images using PatchExtractor patch_extractor = image.PatchExtractor(patch_size=(2, 2)) all_image_patches = patch_extractor.transform(images) image_features = all_image_patches.reshape(images.shape[0], -1) # Step 4: Combine text and image features combined_features = np.hstack((text_features, image_features)) return combined_features ``` # Example Usage ```python text_documents = [ \'The quick brown fox jumps over the lazy dog.\', \'Lorem ipsum dolor sit amet, consectetur adipiscing elit.\' ] images = np.random.randint(0, 255, (2, 32, 32, 3)) features = extract_and_combine_features(text_documents, images) print(features.shape) ``` This question requires the students to demonstrate their understanding of: 1. Text vectorization using `CountVectorizer` and `TfidfTransformer`. 2. Image patch extraction using `PatchExtractor`. 3. Combining different feature sets.","solution":"import numpy as np from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.feature_extraction import image def extract_and_combine_features(text_documents, images): This function processes text documents and images to extract features and combines them into a single feature matrix. Parameters: text_documents (list of str): A list of text documents. images (numpy array): A numpy array of images (n_samples, height, width, n_channels). Returns: numpy array: Combined feature matrix where each row represents a sample. # Step 1: Vectorize text data using CountVectorizer with bigrams vectorizer = CountVectorizer(ngram_range=(1, 2)) text_features = vectorizer.fit_transform(text_documents).toarray() # Step 2: Normalize text features using TfidfTransformer tfidf_transformer = TfidfTransformer() text_features = tfidf_transformer.fit_transform(text_features).toarray() # Step 3: Extract patches from images using PatchExtractor patch_extractor = image.PatchExtractor(patch_size=(2, 2)) all_image_patches = patch_extractor.transform(images) image_features = all_image_patches.reshape(images.shape[0], -1) # Step 4: Combine text and image features combined_features = np.hstack((text_features, image_features)) return combined_features"},{"question":"You are required to write a Python function that connects to an IMAP server, retrieves all the emails from the inbox, and returns a dictionary containing the email ID and corresponding subject of each email. Your function should be named `fetch_email_subjects` and should take the following inputs: - `host` (string): The IMAP server host address. - `user` (string): The username to log in to the IMAP server. - `password` (string): The password to log in to the IMAP server. - `use_ssl` (boolean): A flag indicating whether to use an SSL connection (default is `False`). The function should return a dictionary with email IDs as keys and email subjects as values. # Function Signature ```python def fetch_email_subjects(host: str, user: str, password: str, use_ssl: bool = False) -> dict: pass ``` # Requirements 1. Establish a connection to the IMAP server using the `IMAP4` or `IMAP4_SSL` class based on the `use_ssl` flag. 2. Login to the IMAP server using the provided `user` and `password`. 3. Select the `INBOX`. 4. Search for all emails. 5. Fetch the subject of each email. 6. Return the results in a dictionary where keys are email IDs and values are email subjects. 7. Handle any exceptions appropriately, logging out and closing the connection cleanly in case of an error. # Input - `host`: a string representing the IMAP server host address (e.g., `\\"imap.example.com\\"`). - `user`: a string representing the username to log in to the IMAP server (e.g., `\\"myemail@example.com\\"`). - `password`: a string representing the password to log in to the IMAP server. - `use_ssl`: a boolean flag indicating whether to use an SSL connection (default is `False`). # Output - A dictionary where the keys are email IDs (string) and the values are the subjects of the emails (string). # Constraints - Assume credentials provided are valid. - Ensure the connection is closed properly even if an error occurs. - If no emails are found, return an empty dictionary. # Example ```python emails = fetch_email_subjects(\\"imap.example.com\\", \\"test@example.com\\", \\"password123\\", use_ssl=True) print(emails) ``` Output (dictionary with email IDs and subjects): ```python { \'1\': \'Welcome to the service!\', \'2\': \'Your account has been activated\', \'3\': \'Password reset\', ... } ``` # Notes - Utilize the `imaplib` module as detailed in the documentation provided to perform the tasks. - The function should properly handle the \'with\' statement for managing the connection. - Implement proper exception handling to ensure the connection to the server is always closed appropriately.","solution":"import imaplib import email def fetch_email_subjects(host: str, user: str, password: str, use_ssl: bool = False) -> dict: try: if use_ssl: mail = imaplib.IMAP4_SSL(host) else: mail = imaplib.IMAP4(host) mail.login(user, password) mail.select(\\"inbox\\") result, data = mail.search(None, \\"ALL\\") email_ids = data[0].split() subjects = {} for email_id in email_ids: result, msg_data = mail.fetch(email_id, \\"(RFC822)\\") raw_email = msg_data[0][1] msg = email.message_from_bytes(raw_email) subject = msg[\\"subject\\"] subjects[email_id.decode()] = subject mail.logout() return subjects except Exception as e: if \'mail\' in locals(): mail.logout() raise e"},{"question":"# Question: Swarm Plot Customization and Faceting with Seaborn You are provided with a dataset and asked to analyze it using Seaborn\'s `swarmplot` and `catplot` functions. Follow the instructions to create several plots, each addressing a specific aspect of the dataset. Dataset: Use the built-in \\"tips\\" dataset from Seaborn. Tasks: 1. **Basic Swarm Plot**: Create a swarm plot to represent the distribution of the `total_bill` variable. - **Input**: None. - **Output**: A swarm plot with `total_bill` on the x-axis. - **Constraint**: Use the `sns.swarmplot` function. 2. **Categorical Comparison**: Create a swarm plot to compare the distribution of `total_bill` across different days of the week. - **Input**: None. - **Output**: A swarm plot with `total_bill` on the x-axis and `day` on the y-axis. - **Constraint**: Ensure the plot is clear and distinguishable. 3. **Hue Assignment**: Modify the previous plot to show different hues for each gender (`sex`). - **Input**: None. - **Output**: A swarm plot with `total_bill` on the x-axis, `day` on the y-axis, and the points colored by `sex`. - **Constraint**: Use the `hue` parameter to assign colors based on `sex`. 4. **Size Mapping**: Adjust the plot to represent the `size` variable (number of people) using point size. - **Input**: None. - **Output**: A swarm plot with `total_bill` on the x-axis, `day` on the y-axis, and the points colored by `sex` while mapping the `size` variable to point size. - **Constraint**: Use Seaborn\'s point size setting options. 5. **Multi-Faceted Plot**: Create a faceted plot to analyze how `total_bill` varies by `time` of day for each `day` of the week, with different hues for `sex`. - **Input**: None. - **Output**: A multi-faceted swarm plot with: - `time` on the x-axis, - `total_bill` on the y-axis, - Facets divided by `day` columns, - Points colored by `sex`. - **Constraint**: Use `sns.catplot` for this task to generate the facets. Notes: 1. Ensure your plots are well-labeled and visually clear. 2. Include appropriate titles and axis labels for each plot. 3. You may use additional parameters and Seaborn settings to improve the visual quality of your plots. # Example Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Basic Swarm Plot sns.swarmplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill\\") plt.show() # Task 2: Categorical Comparison sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill by Day of the Week\\") plt.show() # Task 3: Hue Assignment sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\\"Total Bill by Day and Gender\\") plt.show() # Task 4: Size Mapping sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", size=\\"size\\") plt.title(\\"Total Bill by Day, Gender, and Party Size\\") plt.show() # Task 5: Multi-Faceted Plot sns.catplot(data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5) plt.show() ``` # Submission: Submit your Python code and resulting plots in a well-documented Jupyter Notebook.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") def basic_swarm_plot(): plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Count\\") plt.show() def categorical_comparison_plot(): plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill by Day of the Week\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.show() def hue_assignment_plot(): plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\\"Total Bill by Day and Gender\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\'Gender\') plt.show() def size_mapping_plot(): plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", size=5) plt.title(\\"Total Bill by Day, Gender, and Party Size\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\'Gender\') plt.show() def multi_faceted_plot(): g = sns.catplot(data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.7) g.set_axis_labels(\\"Time of Day\\", \\"Total Bill\\") g.set_titles(\\"{col_name} Day\\") plt.show()"},{"question":"# Custom Extension Dtype and Array Implementation You are tasked with creating and integrating a custom extension type into pandas. This custom extension type will handle specific types of data, such as a simple mathematical operation array that stores and operates on numerical values. # Implementation Details 1. **Custom Dtype:** - Create a custom dtype named `MyCustomDtype` that represents a new data type for storing our custom array data. 2. **Custom Extension Array:** - Implement a custom extension array class named `MyCustomArray` that extends `ExtensionArray`. - This custom array should hold integer values and provide simple element-wise addition and multiplication operations. - Implement required methods such as `__len__`, `__getitem__`, `__setitem__`, `take`, and `unique`. 3. **Custom Extension Accessor:** - Design a custom accessor to integrate with pandas Series named `my_math` that provides methods for: - Summing all elements in the custom array. - Multiplying all elements by a given scalar. # Input and Output Formats - **MyCustomDtype Class:** - Input: None - Output: Initialize a new dtype object. - **MyCustomArray Class:** - Input: Accepts an iterable of integers to store. - Output: Initializes a new custom array. - Methods: - `__len__`: Returns the length of the array. - `__getitem__`: Allows element access by index. - `__setitem__`: Sets an element at a specific index. - `take`: Takes elements by index. - `unique`: Returns unique elements in the array. - **my_math Accessor:** - Functions: - `sum_elements`: - Input: None - Output: Sum of all elements in the custom array. - `multiply_elements`: - Input: A scalar value. - Output: Returns a new custom array with elements multiplied by the scalar. # Performance Requirements Your implementations must handle arrays up to a length of `10^6` efficiently, ensuring low computational and memory overhead. ```python import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_series_accessor class MyCustomDtype(ExtensionDtype): # Implement the required properties and methods here class MyCustomArray(ExtensionArray): # Implement the required properties and methods here @register_series_accessor(\\"my_math\\") class MyMathAccessor: def __init__(self, pandas_obj): self._validate(pandas_obj) self._obj = pandas_obj @staticmethod def _validate(obj): # Ensure the Series dtype is MyCustomDtype def sum_elements(self): # Implement sum of all elements def multiply_elements(self, scalar): # Implement element-wise multiplication with scalar # Example usage: # series = pd.Series([1, 2, 3], dtype=MyCustomDtype()) # print(series.my_math.sum_elements()) # Should output 6 # print(series.my_math.multiply_elements(2)) # Should output custom array with values [2, 4, 6] ``` Ensure to implement all necessary methods and properties for the classes mentioned above.","solution":"import pandas as pd import numpy as np from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_series_accessor from pandas.api.types import is_integer_dtype, is_extension_array_dtype class MyCustomDtype(ExtensionDtype): name = \\"mycustom\\" type = np.int64 kind = \'i\' _cache = {} @classmethod def construct_array_type(cls): return MyCustomArray class MyCustomArray(ExtensionArray): def __init__(self, data): self.data = np.asarray(data, dtype=int) def __len__(self): return len(self.data) def __getitem__(self, item): if isinstance(item, int): return self.data[item] elif isinstance(item, slice): return MyCustomArray(self.data[item]) elif isinstance(item, list): return MyCustomArray(self.data[item]) else: raise IndexError(\\"Invalid index\\") def __setitem__(self, key, value): self.data[key] = value def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: fill_value = fill_value if fill_value is not None else np.nan result = np.full(len(indices), fill_value) mask = indices >= 0 result[mask] = self.data[indices[mask]] else: result = self.data[indices] return MyCustomArray(result) def unique(self): unique_data = np.unique(self.data) return MyCustomArray(unique_data) def sum(self): return self.data.sum() def multiply_scalar(self, scalar): multiplied_data = self.data * scalar return MyCustomArray(multiplied_data) @property def dtype(self): return MyCustomDtype() def isna(self): return np.isnan(self.data) def copy(self): return MyCustomArray(self.data.copy()) @register_series_accessor(\\"my_math\\") class MyMathAccessor: def __init__(self, pandas_obj): self._validate(pandas_obj) self._obj = pandas_obj @staticmethod def _validate(obj): if not is_extension_array_dtype(obj) or not isinstance(obj.values.dtype, MyCustomDtype): raise AttributeError(\\"The \'my_math\' accessor only works with Series of MyCustomDtype\\") def sum_elements(self): return self._obj.values.sum() def multiply_elements(self, scalar): return pd.Series(self._obj.values.multiply_scalar(scalar)) # Example usage: # series = pd.Series(MyCustomArray([1, 2, 3]), dtype=MyCustomDtype()) # print(series.my_math.sum_elements()) # Should output 6 # print(series.my_math.multiply_elements(2)) # Should output MyCustomArray([2, 4, 6])"},{"question":"**Objective:** Demonstrate your understanding of the `ipaddress` module by creating and manipulating IP address and network objects. # Problem Statement You are tasked with writing a utility function for a network monitoring system. The function should accept a list of string representations of IP addresses (both IPv4 and IPv6) and return a summary report with the following details: 1. Total number of IP addresses. 2. Number of unique subnets (along with the size of each subnet) present in the list. 3. A mapping of each IP address to its corresponding subnet. # Function Signature ```python import ipaddress from typing import List, Dict, Any def summarize_ip_addresses(ip_list: List[str]) -> Dict[str, Any]: pass ``` # Input - `ip_list` (List[str]): A list of IP addresses as strings (e.g., `[\'192.0.2.1\', \'2001:db8::1\', \'192.0.2.2/24\']`). # Output - A dictionary with the following structure: ```python { \\"total_ips\\": int, # Total number of IP addresses \\"subnets\\": Dict[str, int], # Dictionary where keys are subnet strings and values are the size of each subnet \\"ip_to_subnet_mapping\\": Dict[str, str] # Dictionary mapping each IP address to its corresponding subnet } ``` # Constraints - The function should handle both IPv4 and IPv6 addresses. - If an IP address includes a subnet (e.g., `192.0.2.2/24`), it should be considered as part of that specific subnet, otherwise assume a default subnet mask. # Example ```python ip_list = [\'192.0.2.1\', \'2001:db8::1\', \'192.0.2.2/24\', \'192.0.2.3\'] output = summarize_ip_addresses(ip_list) # Expected output: # { # \\"total_ips\\": 4, # \\"subnets\\": { # \'192.0.2.0/24\': 4, # \'2001:db8::/128\': 1 # }, # \\"ip_to_subnet_mapping\\": { # \'192.0.2.1\': \'192.0.2.0/24\', # \'2001:db8::1\': \'2001:db8::/128\', # \'192.0.2.2/24\': \'192.0.2.0/24\', # \'192.0.2.3\': \'192.0.2.0/24\' # } # } ``` **Notes:** - Use the `ipaddress` module to parse and manipulate the IP addresses. - `ip_list` may contain duplicate IP addresses. - Ensure to handle cases with both IPv4 and IPv6 addresses accurately. # Performance Requirements - The solution should be able to process a list of up to 10,000 IP addresses efficiently.","solution":"import ipaddress from typing import List, Dict, Any def summarize_ip_addresses(ip_list: List[str]) -> Dict[str, Any]: total_ips = len(ip_list) subnet_counter = {} ip_to_subnet_mapping = {} for ip in ip_list: try: if \'/\' in ip: ipnet = ipaddress.ip_network(ip, strict=False) else: ipnet = ipaddress.ip_network(f\\"{ip}/32\\" if \':\' not in ip else f\\"{ip}/128\\", strict=False) subnet_str = str(ipnet.network_address) + \'/\' + str(ipnet.prefixlen) if subnet_str not in subnet_counter: subnet_counter[subnet_str] = 0 subnet_counter[subnet_str] += 1 ip_to_subnet_mapping[ip] = subnet_str except ValueError: raise ValueError(f\\"Invalid IP address format: {ip}\\") return { \\"total_ips\\": total_ips, \\"subnets\\": subnet_counter, \\"ip_to_subnet_mapping\\": ip_to_subnet_mapping }"},{"question":"# Custom Sequence Type Implementation using the Sequence Protocol **Objective:** Implement a custom sequence type in Python that adheres to the Sequence Protocol. Your custom sequence type, `CustomSequence`, should mimic the behavior of Python\'s built-in sequence types (like lists) with additional constraints and functionalities. **Requirements:** 1. **Initialization:** The `CustomSequence` should be initialized with any iterable. 2. **Length:** Implement the `__len__` method to return the length of the sequence. 3. **Indexing:** Implement the `__getitem__` method to support indexing and slicing. 4. **Containment:** Implement the `__contains__` method to check for membership. 5. **Iteration:** Implement the `__iter__` method to iterate over the sequence. 6. **Concatenation:** Implement the `__add__` method to support concatenation with another `CustomSequence`. 7. **Multiplication:** Implement the `__mul__` method to support repetition. 8. **String Representation:** Implement the `__repr__` and `__str__` methods for a readable string representation of the sequence. **Constraints:** - The custom sequence should be immutable, i.e., once created, its contents cannot be changed. - The implementation should handle edge cases such as empty sequences or unsupported operations (e.g., concatenation with non-sequence types) gracefully. **Input and Output Format:** The `CustomSequence` class should be tested with multiple print statements to demonstrate the following: - Initialization with different iterables (e.g., lists, tuples, and strings). - Length of the sequence. - Accessing elements by index and slicing. - Membership checks. - Iteration over the sequence. - Concatenation and multiplication operations. - String representations. **Example:** ```python class CustomSequence: def __init__(self, iterable): # Implementation here def __len__(self): # Implementation here def __getitem__(self, index): # Implementation here def __contains__(self, item): # Implementation here def __iter__(self): # Implementation here def __add__(self, other): # Implementation here def __mul__(self, times): # Implementation here def __repr__(self): # Implementation here def __str__(self): # Implementation here # Example usage seq = CustomSequence([1, 2, 3]) print(len(seq)) # Output: 3 print(seq[1]) # Output: 2 print(2 in seq) # Output: True for item in seq: print(item) # Output: 1 2 3 seq2 = CustomSequence([4, 5]) print(seq + seq2) # Output: CustomSequence([1, 2, 3, 4, 5]) print(seq * 2) # Output: CustomSequence([1, 2, 3, 1, 2, 3]) print(seq) # Output: CustomSequence([1, 2, 3]) ``` Your task is to complete the `CustomSequence` class based on the requirements and constraints mentioned above, ensuring adherence to the Sequence Protocol.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self._data = tuple(iterable) def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __contains__(self, item): return item in self._data def __iter__(self): return iter(self._data) def __add__(self, other): if not isinstance(other, CustomSequence): return NotImplemented return CustomSequence(self._data + other._data) def __mul__(self, times): if not isinstance(times, int): return NotImplemented return CustomSequence(self._data * times) def __repr__(self): return f\\"CustomSequence({self._data})\\" def __str__(self): return str(self._data)"},{"question":"# Gaussian Mixture Model: Finding the Optimal Number of Components **Objective:** Implement a function to determine the optimal number of components for a Gaussian Mixture Model using the BIC criterion and fit the model to a given dataset. **Problem Statement:** Write a function named `optimal_gaussian_mixture` that performs the following steps: 1. **Fit Gaussian Mixture Models:** - Fit `GaussianMixture` models to the given dataset for a range of component numbers (specified as input). - Calculate the Bayesian Information Criterion (BIC) for each model. 2. **Select Optimal Number of Components:** - Identify the number of components that minimizes the BIC. 3. **Output:** - Return the fitted `GaussianMixture` model with the optimal number of components. - Return the optimal number of components. **Function Signature:** ```python from typing import Tuple from sklearn.mixture import GaussianMixture import numpy as np def optimal_gaussian_mixture(data: np.ndarray, max_components: int) -> Tuple[GaussianMixture, int]: Determines the optimal number of components for a Gaussian Mixture Model and fits the model. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. max_components (int): The maximum number of components to consider. Returns: Tuple[GaussianMixture, int]: A tuple containing the fitted GaussianMixture model with the optimal number of components and the optimal number of components. # Your implementation here ``` **Input:** - `data`: A 2D numpy array with shape `(n_samples, n_features)`, where `n_samples` is the number of samples and `n_features` is the number of features of the dataset. - `max_components`: An integer specifying the maximum number of components to consider (e.g., 10). **Output:** - A tuple containing: - The fitted `GaussianMixture` model with the optimal number of components. - The optimal number of components (an integer). **Constraints:** - The dataset (`data`) should have at least 10 samples and 2 features. - The range for the number of components should be between 1 and `max_components`. **Example:** ```python from sklearn.datasets import make_blobs # Generating a sample dataset data, _ = make_blobs(n_samples=300, centers=4, n_features=2, random_state=42) # Calling the function gmm_model, optimal_components = optimal_gaussian_mixture(data, max_components=10) print(f\'Optimal Number of Components: {optimal_components}\') ``` **Notes:** - You must import `GaussianMixture` from `sklearn.mixture`. - Ensure the solution is efficient and handles datasets of varying sizes. - Use appropriate initialization method (default: k-means) for fitting the model.","solution":"from typing import Tuple from sklearn.mixture import GaussianMixture import numpy as np def optimal_gaussian_mixture(data: np.ndarray, max_components: int) -> Tuple[GaussianMixture, int]: Determines the optimal number of components for a Gaussian Mixture Model and fits the model. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. max_components (int): The maximum number of components to consider. Returns: Tuple[GaussianMixture, int]: A tuple containing the fitted GaussianMixture model with the optimal number of components and the optimal number of components. best_bic = np.inf best_gmm = None best_components = 0 for n_components in range(1, max_components + 1): gmm = GaussianMixture(n_components=n_components, random_state=42) gmm.fit(data) bic = gmm.bic(data) if bic < best_bic: best_bic = bic best_gmm = gmm best_components = n_components return best_gmm, best_components"},{"question":"**Sorting Historical Records** You are given a list of historical records, each containing three pieces of information: the name of the event, the year it occurred, and the importance score of the event on a scale from 1 to 10 (with 10 being the most important). Your task is to implement a function `sort_records(records: List[Tuple[str, int, int]], primary_key: str, descending: bool = False) -> List[Tuple[str, int, int]]` that sorts these records based on a specified primary key (either \'year\' or \'importance\'). Optionally, the sort can be done in descending order if the `descending` parameter is set to `True`. # Requirements: * **Input:** * `records`: A list of tuples, where each tuple contains a string (event name), an integer (year), and an integer (importance score). * `primary_key`: A string that can either be \'year\' or \'importance\' indicating the primary attribute for sorting. * `descending`: A boolean flag indicating whether the sorting should be in descending order. Default is `False`. * **Output:** * A list of tuples sorted primarily based on the specified primary key and optionally in descending order. * **Constraints:** * The year for any event will be a positive integer. * Importance scores range from 1 to 10. * You must utilize the `sorted()` function with proper key functions. # Examples: ```python records = [ (\\"Moon landing\\", 1969, 10), (\\"Independence Declaration\\", 1776, 9), (\\"Fall of Berlin Wall\\", 1989, 8), (\\"First manned flight\\", 1903, 7), ] # Sorting by year, ascending print(sort_records(records, primary_key=\\"year\\")) # Expected Output: # [ # (\\"Independence Declaration\\", 1776, 9), # (\\"First manned flight\\", 1903, 7), # (\\"Moon landing\\", 1969, 10), # (\\"Fall of Berlin Wall\\", 1989, 8), # ] # Sorting by importance, descending print(sort_records(records, primary_key=\\"importance\\", descending=True)) # Expected Output: # [ # (\\"Moon landing\\", 1969, 10), # (\\"Independence Declaration\\", 1776, 9), # (\\"Fall of Berlin Wall\\", 1989, 8), # (\\"First manned flight\\", 1903, 7), # ] ``` # Notes: * Use the `operator.itemgetter` function from the `operator` module for efficient key access. * Ensure that the function meets the performance requirements for lists with up to 10^6 records. Good luck!","solution":"from typing import List, Tuple from operator import itemgetter def sort_records(records: List[Tuple[str, int, int]], primary_key: str, descending: bool = False) -> List[Tuple[str, int, int]]: Sorts a list of historical records based on the specified primary key. Parameters: - records: A list of tuples, where each tuple contains (event name, year, importance score). - primary_key: A string either \'year\' or \'importance\' indicating the primary attribute for sorting. - descending: A boolean flag for sorting in descending order. Default is False (ascending). Returns: - A list of tuples sorted based on the specified primary key and order. key_index = 1 if primary_key == \'year\' else 2 sorted_records = sorted(records, key=itemgetter(key_index), reverse=descending) return sorted_records"},{"question":"# URL Fetching and Data Transmission using `urllib.request` **Objective:** Implement a function in Python that fetches the content of a specified URL, handles potential errors, and optionally sends POST data with custom headers and basic authentication. # Requirements: 1. The function should fetch the content of a given URL. 2. If `method` is `\\"POST\\"`, it should send `data` as POST parameters. 3. The function should support custom headers by accepting an optional dictionary of header values. 4. Optionally, the function should handle basic authentication using provided username and password. 5. The function should handle potential exceptions, such as `URLError` and `HTTPError`, and return appropriate error messages. # Function Signature: ```python def fetch_url(url: str, method: str = \\"GET\\", data: dict = None, headers: dict = None, username: str = None, password: str = None) -> str: Fetches the content of a specified URL, sending data if the method is POST, and handling custom headers and basic authentication as needed. Args: - url (str): The URL to be fetched. - method (str): The HTTP method to be used (default is \\"GET\\"). - data (dict, optional): A dictionary of data to be sent in case of a POST request (default is None). - headers (dict, optional): A dictionary of custom header values (default is None). - username (str, optional): The username for basic authentication (default is None). - password (str, optional): The password for basic authentication (default is None). Returns: - str: The content of the response, or an appropriate error message in case of an exception. ``` # Constraints: - The URL must begin with either `http://` or `https://`. - If the method is `POST` and `data` is provided, `data` should be encoded and sent with the request. - If `headers` are provided, they should be added to the request. - If `username` and `password` are provided, basic authentication should be handled. - The function should handle and return exceptions like `URLError` and `HTTPError` with appropriate messages. # Example Usage: ```python # Example 1: Simple GET request print(fetch_url(\\"http://example.com\\")) # Example 2: GET request with custom headers headers = {\'User-Agent\': \'Mozilla/5.0\'} print(fetch_url(\\"http://example.com\\", headers=headers)) # Example 3: POST request with data data = {\'key1\': \'value1\', \'key2\': \'value2\'} print(fetch_url(\\"http://example.com\\", method=\\"POST\\", data=data)) # Example 4: GET request with basic authentication print(fetch_url(\\"http://example.com\\", username=\\"user\\", password=\\"pass\\")) # Example 5: Handling errors print(fetch_url(\\"http://nonexistenturl.com\\")) ``` # Expected Output: The function should return the content of the URL fetched, or an appropriate error message if any exception occurs. **Note**: The actual content returned from the URLs will vary, and the function must properly handle and display content or errors as described.","solution":"import urllib.request from urllib.error import URLError, HTTPError import base64 import json def fetch_url(url: str, method: str = \\"GET\\", data: dict = None, headers: dict = None, username: str = None, password: str = None) -> str: Fetches the content of a specified URL, sending data if the method is POST, and handling custom headers and basic authentication as needed. Args: - url (str): The URL to be fetched. - method (str): The HTTP method to be used (default is \\"GET\\"). - data (dict, optional): A dictionary of data to be sent in case of a POST request (default is None). - headers (dict, optional): A dictionary of custom header values (default is None). - username (str, optional): The username for basic authentication (default is None). - password (str, optional): The password for basic authentication (default is None). Returns: - str: The content of the response, or an appropriate error message in case of an exception. if headers is None: headers = {} if username and password: credentials = f\\"{username}:{password}\\" encoded_credentials = base64.b64encode(credentials.encode(\'ascii\')).decode(\'ascii\') headers[\'Authorization\'] = f\'Basic {encoded_credentials}\' if data: data = urllib.parse.urlencode(data).encode() req = urllib.request.Request(url=url, data=data if method == \\"POST\\" else None, headers=headers or {}, method=method) try: with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return f\\"HTTPError: {e.code} - {e.reason}\\" except URLError as e: return f\\"URLError: {e.reason}\\" except Exception as e: return f\\"Exception: {str(e)}\\""},{"question":"# Python Coding Assessment Question You are provided with an initial implementation and are required to complete it by implementing various functionalities. Problem Statement Create a class `RationalOperations` that uses the `fractions.Fraction` module to achieve the following operations: 1. **Initializing fractions**: The class should have a method to create a fraction from a string or float input. 2. **Adding fractions**: Method to add two fractions. 3. **Subtracting fractions**: Method to subtract one fraction from another. 4. **Multiplying fractions**: Method to multiply two fractions. 5. **Dividing fractions**: Method to divide one fraction by another. 6. **Approximating fractions**: Method to return the closest fraction to a given fraction with a denominator limited by a provided parameter. 7. **Handling Errors**: The methods should handle any errors such as zero denominators or invalid inputs appropriately. Implementation Details 1. **Method `create_fraction(self, input_value)`**: - Input: A string or float which represents a number (e.g. \'3/4\', 0.75). - Output: A `fractions.Fraction` object corresponding to the input value. 2. **Method `add_fractions(self, fraction1, fraction2)`**: - Input: Two `fractions.Fraction` objects. - Output: A new `fractions.Fraction` object representing the sum. 3. **Method `subtract_fractions(self, fraction1, fraction2)`**: - Input: Two `fractions.Fraction` objects. - Output: A new `fractions.Fraction` object representing the difference. 4. **Method `multiply_fractions(self, fraction1, fraction2)`**: - Input: Two `fractions.Fraction` objects. - Output: A new `fractions.Fraction` object representing the product. 5. **Method `divide_fractions(self, fraction1, fraction2)`**: - Input: Two `fractions.Fraction` objects. - Output: A new `fractions.Fraction` object representing the quotient. 6. **Method `approximate_fraction(self, fraction, max_denominator)`**: - Input: A `fractions.Fraction` object and an integer `max_denominator`. - Output: A new `fractions.Fraction` object where the denominator is limited by `max_denominator`. Ensure you handle cases where denominator could be zero, invalid string inputs, or non-numeric float inputs. Example Usage ```python from fractions import Fraction from decimal import Decimal class RationalOperations: def create_fraction(self, input_value): # Implement this method pass def add_fractions(self, fraction1, fraction2): # Implement this method pass def subtract_fractions(self, fraction1, fraction2): # Implement this method pass def multiply_fractions(self, fraction1, fraction2): # Implement this method pass def divide_fractions(self, fraction1, fraction2): # Implement this method pass def approximate_fraction(self, fraction, max_denominator): # Implement this method pass # Example Usage: operations = RationalOperations() fraction1 = operations.create_fraction(\'3/4\') fraction2 = operations.create_fraction(0.5) result_add = operations.add_fractions(fraction1, fraction2) result_subtract = operations.subtract_fractions(fraction1, fraction2) result_multiply = operations.multiply_fractions(fraction1, fraction2) result_divide = operations.divide_fractions(fraction1, fraction2) result_approximate = operations.approximate_fraction(Fraction(\'3.1415926535897932\'), 1000) print(result_add) # Output: Fraction(5, 4) print(result_subtract) # Output: Fraction(1, 4) print(result_multiply) # Output: Fraction(3, 8) print(result_divide) # Output: Fraction(3, 2) print(result_approximate) # Output: Fraction(355, 113) ``` Good luck!","solution":"from fractions import Fraction import re class RationalOperations: def create_fraction(self, input_value): if isinstance(input_value, str): # Attempt to parse string input if re.match(r\'^-?d+/d+\', input_value): numerator, denominator = map(int, input_value.split(\'/\')) if denominator == 0: raise ValueError(\\"Denominator cannot be zero.\\") return Fraction(numerator, denominator) try: return Fraction(float(input_value)) except ValueError: raise ValueError(\\"Invalid string input format for fraction.\\") elif isinstance(input_value, (int, float)): return Fraction(input_value) else: raise TypeError(\\"Unsupported input type for fraction creation.\\") def add_fractions(self, fraction1, fraction2): return fraction1 + fraction2 def subtract_fractions(self, fraction1, fraction2): return fraction1 - fraction2 def multiply_fractions(self, fraction1, fraction2): return fraction1 * fraction2 def divide_fractions(self, fraction1, fraction2): if fraction2 == 0: raise ZeroDivisionError(\\"Cannot divide by zero.\\") return fraction1 / fraction2 def approximate_fraction(self, fraction, max_denominator): return fraction.limit_denominator(max_denominator)"},{"question":"# Context Management in Python 3.7+ In Python, context variables enable the isolation of data between different execution contexts. Using the `contextvars` module in Python, you can create and manipulate context variables to achieve this isolation. This is especially useful in asynchronous programming, where different parts of your code might need to maintain different states. **Task:** Implement a Python program that demonstrates the following functionalities using the `contextvars` module: 1. Create a new context variable. 2. Set a value to this context variable. 3. Retrieve the current value of the context variable in the current context. 4. Change this context\'s value of the context variable. 5. Reset the context variable to its previous state. 6. Demonstrate the isolation of context variables between two different asynchronous tasks. Here is a template for your implementation: ```python import contextvars def create_context_variable(name, default_value): Create a new context variable. Parameters: name (str): The name of the context variable. default_value: The default value of the context variable. Returns: ContextVar: A new context variable instance. # Your code here def set_context_value(var, value): Set a value for the context variable. Parameters: var (contextvars.ContextVar): The context variable. value: The value to set for the context variable. Returns: contextvars.Token: A new token instance representing this change. # Your code here def get_context_value(var): Get the current value of the context variable. Parameters: var (contextvars.ContextVar): The context variable. Returns: The current value of the context variable. # Your code here def reset_context_value(var, token): Reset the context variable to its previous state using the provided token. Parameters: var (contextvars.ContextVar): The context variable. token (contextvars.Token): The token representing the previous state. Returns: None # Your code here async def task(name, var): An asynchronous task demonstrating the use of context variables. Parameters: name (str): The name of the task. var (contextvars.ContextVar): The context variable. # Your code here def main(): # Create a context variable context_var = create_context_variable(\\"my_var\\", \\"default_value\\") # Set a value token = set_context_value(context_var, \\"initial_value\\") # Retrieve and print the value print(\\"Current value:\\", get_context_value(context_var)) # Should print: Current value: initial_value # Modify the variable set_context_value(context_var, \\"new_value\\") print(\\"Modified value:\\", get_context_value(context_var)) # Should print: Modified value: new_value # Reset the variable to previous state reset_context_value(context_var, token) print(\\"Reset value:\\", get_context_value(context_var)) # Should print: Reset value: initial_value # Run asynchronous tasks import asyncio loop = asyncio.get_event_loop() tasks = [task(f\\"Task {i}\\", context_var) for i in range(2)] loop.run_until_complete(asyncio.gather(*tasks)) if __name__ == \\"__main__\\": main() ``` **Expected Output:** ``` Current value: initial_value Modified value: new_value Reset value: initial_value Task 0: initial value Task 0: modified value in task Task 0: reset value in task Task 1: initial value Task 1: modified value in task Task 1: reset value in task ``` **Constraints:** 1. Utilize the `contextvars` module only. 2. Ensure proper value isolation between asynchronous tasks. Good luck!","solution":"import contextvars import asyncio def create_context_variable(name, default_value): Create a new context variable. Parameters: name (str): The name of the context variable. default_value: The default value of the context variable. Returns: ContextVar: A new context variable instance. return contextvars.ContextVar(name, default=default_value) def set_context_value(var, value): Set a value for the context variable. Parameters: var (contextvars.ContextVar): The context variable. value: The value to set for the context variable. Returns: contextvars.Token: A new token instance representing this change. return var.set(value) def get_context_value(var): Get the current value of the context variable. Parameters: var (contextvars.ContextVar): The context variable. Returns: The current value of the context variable. return var.get() def reset_context_value(var, token): Reset the context variable to its previous state using the provided token. Parameters: var (contextvars.ContextVar): The context variable. token (contextvars.Token): The token representing the previous state. Returns: None var.reset(token) async def task(name, var): An asynchronous task demonstrating the use of context variables. Parameters: name (str): The name of the task. var (contextvars.ContextVar): The context variable. # Retrieve and print the current value print(f\\"{name}: initial value: {get_context_value(var)}\\") # Set a new value in the context of this task token = set_context_value(var, f\\"{name} new value\\") print(f\\"{name}: modified value: {get_context_value(var)}\\") # Reset to the original value reset_context_value(var, token) print(f\\"{name}: reset value: {get_context_value(var)}\\") def main(): # Create a context variable context_var = create_context_variable(\\"my_var\\", \\"default_value\\") # Set a value token = set_context_value(context_var, \\"initial_value\\") # Retrieve and print the value print(\\"Current value:\\", get_context_value(context_var)) # Should print: Current value: initial_value # Modify the variable set_context_value(context_var, \\"new_value\\") print(\\"Modified value:\\", get_context_value(context_var)) # Should print: Modified value: new_value # Reset the variable to previous state reset_context_value(context_var, token) print(\\"Reset value:\\", get_context_value(context_var)) # Should print: Reset value: initial_value # Run asynchronous tasks loop = asyncio.get_event_loop() tasks = [task(f\\"Task {i}\\", context_var) for i in range(2)] loop.run_until_complete(asyncio.gather(*tasks)) if __name__ == \\"__main__\\": main()"},{"question":"# Seaborn Visualization Challenge You are tasked with creating a comprehensive visualization using seaborn to analyze the **Iris** dataset, which is a classic dataset used in machine learning. Your goal is to generate a scatter plot with rug plots that provide insight into the distribution of the data points along both axes. Requirements 1. Load the **Iris** dataset using seaborn\'s `load_dataset` function. 2. Create a scatter plot with the following specifications: - X-axis: `sepal_length` - Y-axis: `sepal_width` - Use the `species` column to color-code (hue) the data points. 3. Add rug plots along both axes (`x` and `y`) to indicate the distribution of the sepal length and sepal width values: - Ensure the rug plots have a height of 0.1. - Place the rug plots outside the axis (`clip_on=False`). 4. Save the final plots as an image file (\\"iris_scatter_rug.png\\"). Input - No input required as the dataset is loaded internally. Output - The solution should generate a plot and save it as \\"iris_scatter_rug.png\\". Constraints - Ensure the plot is clearly readable. - Use seaborn\'s default styling. Example Your code should resemble the following pseudocode: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Create scatter plot sns.scatterplot(...) # Add rug plot sns.rugplot(...) sns.rugplot(...) # Save the plot plt.savefig(\\"iris_scatter_rug.png\\") ``` Note: The solution should be well-commented and written in Python using seaborn and matplotlib.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_iris_with_rugplots(): Creates a scatter plot with rug plots of the Iris dataset. The scatter plot displays sepal length vs sepal width and uses species as the hue. Rug plots are added along both axes. # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Create a scatter plot with rug plots scatter_plot = sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\') # Add rug plots along both axes sns.rugplot(data=iris, x=\'sepal_length\', height=0.1, clip_on=False) sns.rugplot(data=iris, y=\'sepal_width\', height=0.1, clip_on=False) # Save the plot as an image file plt.savefig(\\"iris_scatter_rug.png\\") # Display the plot (optional) plt.show() # This function can be called to generate the plot # visualize_iris_with_rugplots()"},{"question":"You are tasked with implementing a function that processes Unix group database entries and performs various operations based on the input parameters. Your implementation should make use of the \\"grp\\" module\'s functionalities as described below. # Function Signature ```python def process_group_database(operation, value=None): Process Unix group database entries based on the given operation and value. Parameters: - operation (str): The operation to perform. It can be one of the following: - \\"get_by_gid\\": Return the group entry for the given group ID. - \\"get_by_name\\": Return the group entry for the given group name. - \\"list_all\\": Return a list of all group entries. - \\"members_by_gid\\": Return the member list of the group with the given group ID. - \\"members_by_name\\": Return the member list of the group with the given group name. - value (int or str, optional): The value associated with the operation. It should be: - an integer for \\"get_by_gid\\" and \\"members_by_gid\\". - a string for \\"get_by_name\\" and \\"members_by_name\\". - None for \\"list_all\\". Returns: The result of the operation as described above. For \\"get_by_gid\\" and \\"get_by_name\\", return a dictionary with keys \\"name\\", \\"passwd\\", \\"gid\\", and \\"members\\". For \\"list_all\\", return a list of such dictionaries. For \\"members_by_gid\\" and \\"members_by_name\\", return the list of members. Raises: - ValueError: If the operation is not recognized or if the value type is incorrect for the operation. - KeyError: If the group entry is not found for \\"get_by_gid\\" and \\"get_by_name\\" operations. - TypeError: If a non-integer value is provided for \\"get_by_gid\\" or \\"members_by_gid\\". ``` # Example Usage ```python # Example group database entry # (\'staff\', \'x\', 50, [\'root\', \'john\', \'jane\']) # Get group by GID print(process_group_database(\\"get_by_gid\\", 50)) # Output: {\'name\': \'staff\', \'passwd\': \'x\', \'gid\': 50, \'members\': [\'root\', \'john\', \'jane\']} # Get group by name print(process_group_database(\\"get_by_name\\", \\"staff\\")) # Output: {\'name\': \'staff\', \'passwd\': \'x\', \'gid\': 50, \'members\': [\'root\', \'john\', \'jane\']} # List all groups print(process_group_database(\\"list_all\\")) # Output: [{\'name\': \'staff\', \'passwd\': \'x\', \'gid\': 50, \'members\': [\'root\', \'john\', \'jane\']}, ...] # Get members by GID print(process_group_database(\\"members_by_gid\\", 50)) # Output: [\'root\', \'john\', \'jane\'] # Get members by name print(process_group_database(\\"members_by_name\\", \\"staff\\")) # Output: [\'root\', \'john\', \'jane\'] ``` # Constraints 1. Your implementation should use the methods provided by the \\"grp\\" module. 2. You should handle and raise the appropriate errors as described. 3. Performance is not a primary constraint, but your code should be clear and efficient. # Notes - Use the `grp.getgrgid`, `grp.getgrnam`, and `grp.getgrall` functions appropriately. - Thoroughly test your implementation with different group names, IDs, and edge cases like non-existing entries. Good luck!","solution":"import grp def process_group_database(operation, value=None): Process Unix group database entries based on the given operation and value. Parameters: - operation (str): The operation to perform. - \\"get_by_gid\\": Return the group entry for the given group ID. - \\"get_by_name\\": Return the group entry for the given group name. - \\"list_all\\": Return a list of all group entries. - \\"members_by_gid\\": Return the member list of the group with the given group ID. - \\"members_by_name\\": Return the member list of the group with the given group name. - value (int or str, optional): The value associated with the operation. - an integer for \\"get_by_gid\\" and \\"members_by_gid\\". - a string for \\"get_by_name\\" and \\"members_by_name\\". - None for \\"list_all\\". Returns: The result of the operation as described above. For \\"get_by_gid\\" and \\"get_by_name\\", return a dictionary with keys \\"name\\", \\"passwd\\", \\"gid\\", and \\"members\\". For \\"list_all\\", return a list of such dictionaries. For \\"members_by_gid\\" and \\"members_by_name\\", return the list of members. Raises: - ValueError: If the operation is not recognized or if the value type is incorrect for the operation. - KeyError: If the group entry is not found for \\"get_by_gid\\" and \\"get_by_name\\" operations. - TypeError: If a non-integer value is provided for \\"get_by_gid\\" or \\"members_by_gid\\". if operation == \\"get_by_gid\\": if not isinstance(value, int): raise TypeError(\\"value must be an integer for get_by_gid\\") try: group = grp.getgrgid(value) return {\\"name\\": group.gr_name, \\"passwd\\": group.gr_passwd, \\"gid\\": group.gr_gid, \\"members\\": group.gr_mem} except KeyError: raise KeyError(f\\"No group found with gid {value}\\") elif operation == \\"get_by_name\\": if not isinstance(value, str): raise ValueError(\\"value must be a string for get_by_name\\") try: group = grp.getgrnam(value) return {\\"name\\": group.gr_name, \\"passwd\\": group.gr_passwd, \\"gid\\": group.gr_gid, \\"members\\": group.gr_mem} except KeyError: raise KeyError(f\\"No group found with name {value}\\") elif operation == \\"list_all\\": groups = grp.getgrall() return [{\\"name\\": g.gr_name, \\"passwd\\": g.gr_passwd, \\"gid\\": g.gr_gid, \\"members\\": g.gr_mem} for g in groups] elif operation == \\"members_by_gid\\": if not isinstance(value, int): raise TypeError(\\"value must be an integer for members_by_gid\\") try: group = grp.getgrgid(value) return group.gr_mem except KeyError: raise KeyError(f\\"No group found with gid {value}\\") elif operation == \\"members_by_name\\": if not isinstance(value, str): raise ValueError(\\"value must be a string for members_by_name\\") try: group = grp.getgrnam(value) return group.gr_mem except KeyError: raise KeyError(f\\"No group found with name {value}\\") else: raise ValueError(f\\"Unrecognized operation: {operation}\\")"},{"question":"**Background**: In modern server-client architectures, it\'s common to have servers that can handle multiple client connections simultaneously and communicate securely. **Task**: Write a Python program that sets up a secure server using `socket` and `ssl` modules, capable of handling multiple clients asynchronously using `asyncio`. You are required to: 1. Set up an SSL context to handle secure connections. 2. Create an asynchronous server that can handle multiple clients concurrently. 3. Write a handler function for the server that: - Echoes back any received messages to the sending client. - Log each message received from any client to a memory-mapped file using the `mmap` module. **Requirements**: - Your server must run on `localhost` and listen on port `8888`. - The SSL certificate and key must be self-signed and saved as `cert.pem` and `key.pem`, respectively. - Each received message should be logged to a memory-mapped file named `server_log.txt`. **Constraints**: - Clients may disconnect at any time; ensure the server handles such scenarios gracefully. - The memory-mapped file should be large enough to store up to 1024 characters for each message, and you should handle writing messages in a way that avoids concurrency issues. **Input**: No direct input to the function, but the server will receive string messages from connected clients. **Output**: No direct output from the function, but the server will echo received messages back to clients and log them. **Performance**: The server should efficiently handle multiple clients without significant delays or crashes. # Example: ```python python secure_async_server.py ``` - This will start the server and wait for client connections. - Clients (not part of this implementation task) will send messages to the server. - The server will echo the messages back to the respective clients and log them in `server_log.txt`. # Implementation: Implement the function `start_secure_server` which starts the server and handles all the required functionalities. ```python import asyncio import ssl import socket import mmap async def handle_client(reader, writer, log_mmap): while True: data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') if not data: writer.close() await writer.wait_closed() break print(f\\"Received {message} from {addr}\\") log_mmap.write(message.encode() + b\'n\') writer.write(data) await writer.drain() async def main(): ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) ssl_context.load_cert_chain(certfile=\'cert.pem\', keyfile=\'key.pem\') server = await asyncio.start_server( lambda r, w: handle_client(r, w, memory_map), \'localhost\', 8888, ssl=ssl_context) async with server: await server.serve_forever() if __name__ == \'__main__\': with open(\'server_log.txt\', \'wb\') as f: f.write(b\'x00\' * 4096) # Pre-allocate the memory-mapped file size. with open(\'server_log.txt\', \'r+b\') as f: memory_map = mmap.mmap(f.fileno(), 0) asyncio.run(main()) ``` The `start_secure_server` function: - Configures SSL context. - Starts an asynchronous server using `asyncio`. - Handles incoming connections, echoes messages, and logs them to a memory-mapped file. **Note**: Ensure you have a self-signed SSL certificate (`cert.pem`) and key (`key.pem`) for testing this implementation.","solution":"import asyncio import ssl import mmap async def handle_client(reader, writer, log_mmap): while True: data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') if not data: writer.close() await writer.wait_closed() break print(f\\"Received {message} from {addr}\\") # Write message to memory-mapped file log_mmap.seek(0, mmap.SEEK_END) log_mmap.write(message.encode() + b\'n\') writer.write(data) await writer.drain() async def main(): ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) ssl_context.load_cert_chain(certfile=\'cert.pem\', keyfile=\'key.pem\') server = await asyncio.start_server( lambda r, w: handle_client(r, w, memory_map), \'localhost\', 8888, ssl=ssl_context) async with server: await server.serve_forever() if __name__ == \'__main__\': with open(\'server_log.txt\', \'wb\') as f: f.write(b\'x00\' * 4096) # Pre-allocate the memory-mapped file size. with open(\'server_log.txt\', \'r+b\') as f: memory_map = mmap.mmap(f.fileno(), 0) try: asyncio.run(main()) finally: memory_map.close()"},{"question":"# Data Exploration and Preparation using Scikit-learn Your task is to write a function that performs data exploration and preliminary data preparation on a selected real-world dataset from scikit-learn. The dataset you will work with is the California housing dataset. Function Signature ```python def explore_and_prepare_california_housing(test_size: float) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: pass ``` Objective The function `explore_and_prepare_california_housing` should: 1. Fetch the California housing dataset using `fetch_california_housing`. 2. Perform the following exploratory data analysis (EDA): - Print the feature names. - Print the shape of the dataset (number of samples, number of features). - Print the first 5 rows of the dataset. 3. Prepare the dataset for machine learning by: - Splitting the data into train and test sets based on the `test_size` parameter. Use `sklearn.model_selection.train_test_split`. - Return four objects: training features (X_train), testing features (X_test), training targets (y_train), and testing targets (y_test). Input - `test_size` (float): A float between 0.0 and 1.0 which represents the proportion of the dataset to include in the test split. Output - Returns a tuple `(X_train, X_test, y_train, y_test)`: - `X_train` (pd.DataFrame): Training features. - `X_test` (pd.DataFrame): Testing features. - `y_train` (pd.Series): Training targets. - `y_test` (pd.Series): Testing targets. Example ```python from sklearn.model_selection import train_test_split import pandas as pd def explore_and_prepare_california_housing(test_size: float) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: from sklearn.datasets import fetch_california_housing # Load the dataset data = fetch_california_housing() X = pd.DataFrame(data.data, columns=data.feature_names) y = pd.Series(data.target) # EDA print(\\"Feature Names:\\", data.feature_names) print(\\"Dataset Shape:\\", X.shape) print(\\"First 5 rows of the dataset:\\") print(X.head()) # Splitting the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) return X_train, X_test, y_train, y_test # Example usage X_train, X_test, y_train, y_test = explore_and_prepare_california_housing(0.2) ``` Evaluation Your code will be evaluated on: - Correctness of the implementation. - Ability to correctly load and interact with the dataset. - Proper execution of the exploratory data analysis tasks. - Accurate splitting of the data into training and testing sets.","solution":"from typing import Tuple from sklearn.model_selection import train_test_split import pandas as pd def explore_and_prepare_california_housing(test_size: float) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: from sklearn.datasets import fetch_california_housing # Load the dataset data = fetch_california_housing() X = pd.DataFrame(data.data, columns=data.feature_names) y = pd.Series(data.target) # EDA print(\\"Feature Names:\\", data.feature_names) print(\\"Dataset Shape:\\", X.shape) print(\\"First 5 rows of the dataset:\\") print(X.head()) # Splitting the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) return X_train, X_test, y_train, y_test"},{"question":"# Python Coding Assessment Question Comparing Text Files and Generating Human-Readable Differences **Objective**: Write a Python function that takes two file paths as input and produces a detailed comparison report between the two files. The generated report should include the following: 1. A list of matching blocks between the two files. 2. A set of operations (`insert`, `delete`, `replace`, `equal`) required to transform the content of the first file into the second file. 3. A HTML file that shows a side-by-side comparison of the two files, highlighting the differences. **Function Signature**: ```python def generate_comparison_report(file1: str, file2: str) -> str: Compare the contents of two text files and generate a detailed comparison report. Args: - file1 (str): Path to the first text file. - file2 (str): Path to the second text file. Returns: - str: The path to the generated HTML comparison report file. pass ``` **Input**: - `file1`: A string representing the path to the first text file. - `file2`: A string representing the path to the second text file. **Output**: - A string representing the path to the generated HTML comparison report file. **Instructions**: 1. Read the contents of the input files into lists of lines. 2. Use the `SequenceMatcher` from the `difflib` module to find matching blocks and operations required to transform the content of the first file into the second file. 3. Generate a human-readable comparison report in HTML format using the `HtmlDiff` class, saving this report to an HTML file in the current working directory. 4. Return the path of the generated HTML file. **Constraints**: - The function should handle large files efficiently. - Ensure that any HTML special characters in the file contents are properly escaped. **Example**: ```python file1 = \'file1.txt\' file2 = \'file2.txt\' # Sample content of file1.txt: # apple # banana # cherry # Sample content of file2.txt: # apple # blue berry # cherry # dragon fruit report_file = generate_comparison_report(file1, file2) print(report_file) # Output: \\"comparison_report.html\\" ``` In the `comparison_report.html` file, you should see a side-by-side comparison of the contents of `file1.txt` and `file2.txt`, with differences highlighted. **Note**: The implementation must correctly handle file I/O operations and ensure the HTML content is utf-8 encoded.","solution":"from difflib import SequenceMatcher, HtmlDiff def generate_comparison_report(file1: str, file2: str) -> str: Compare the contents of two text files and generate a detailed comparison report. Args: - file1 (str): Path to the first text file. - file2 (str): Path to the second text file. Returns: - str: The path to the generated HTML comparison report file. # Read file contents with open(file1, \'r\', encoding=\'utf-8\') as f1: lines1 = f1.readlines() with open(file2, \'r\', encoding=\'utf-8\') as f2: lines2 = f2.readlines() # Use SequenceMatcher to find operations matcher = SequenceMatcher(None, lines1, lines2) operations = matcher.get_opcodes() # Prepare list of matching blocks matching_blocks = matcher.get_matching_blocks() # Using HtmlDiff to create the side-by-side HTML report html_diff = HtmlDiff() html_report = html_diff.make_file(lines1, lines2, file1, file2) # Create the report file path report_file_path = \\"comparison_report.html\\" # Write the HTML report to the file with open(report_file_path, \'w\', encoding=\'utf-8\') as report_file: report_file.write(html_report) return report_file_path"},{"question":"# Asynchronous Web Scraper You are tasked to implement an asynchronous web scraper using the `asyncio` and `aiohttp` libraries. The scraper will concurrently fetch data from multiple URLs and process the content. Requirements 1. **Function Name**: `async_scrape(urls: List[str]) -> List[Tuple[str, int]]` 2. **Input**: A list of URLs (strings) to scrape. Example: `[\\"http://example.com\\", \\"http://example.org\\"]` 3. **Output**: A list of tuples, where each tuple consists of a URL and the length of the content fetched from that URL. Constraints - You should handle possible exceptions gracefully, such as network errors or invalid URLs, and skip any URL that raises an exception. - Use the `asyncio` and `aiohttp` libraries to perform asynchronous network requests. Example ```python import asyncio from typing import List, Tuple import aiohttp async def fetch(url: str) -> Tuple[str, int]: try: async with aiohttp.ClientSession() as session: async with session.get(url) as response: content = await response.text() return url, len(content) except Exception as e: print(f\\"Error fetching {url}: {e}\\") return url, 0 async def async_scrape(urls: List[str]) -> List[Tuple[str, int]]: tasks = [fetch(url) for url in urls] return await asyncio.gather(*tasks) # Example usage: urls = [\\"http://example.com\\", \\"http://invalid-url\\"] result = asyncio.run(async_scrape(urls)) print(result) # Output: [(\'http://example.com\', <length>), (\'http://invalid-url\', 0)] ``` Notes - The `fetch` function should handle the individual request asynchronously and return a tuple containing the URL and the length of the content. - The `async_scrape` function should manage the concurrent execution of multiple `fetch` tasks and gather their results. - Ensure proper error handling to manage any exceptions during the network requests. This question assesses the student\'s ability to utilize `asyncio` and `aiohttp` for implementing asynchronous tasks, manage concurrency, and handle exceptions.","solution":"import asyncio from typing import List, Tuple import aiohttp async def fetch(url: str) -> Tuple[str, int]: try: async with aiohttp.ClientSession() as session: async with session.get(url) as response: content = await response.text() return url, len(content) except Exception as e: print(f\\"Error fetching {url}: {e}\\") return url, 0 async def async_scrape(urls: List[str]) -> List[Tuple[str, int]]: tasks = [fetch(url) for url in urls] results = await asyncio.gather(*tasks) return results"},{"question":"# PyTorch Testing Utility Implementation You are provided with three functions from the `torch.testing` module: `assert_close`, `make_tensor`, and `assert_allclose`. These functions are crucial for testing and validating deep learning models in PyTorch. Your task is to implement a function `validate_model_predictions` that utilizes these three testing utilities. Function Signature ```python import torch def validate_model_predictions(model, test_loader, tolerance=1e-5): # Implementation here ``` Input - `model`: A PyTorch model that you need to validate. - `test_loader`: A DataLoader object containing the test dataset. - `tolerance` (optional): A floating-point number representing the tolerance value for the `assert_close` and `assert_allclose` functions. Default is `1e-5`. Output - This function should not return anything. Instead, it should raise an assertion error if the predictions of the model on the test data are not close to the actual values within the specified tolerance. Constraints 1. Implement the function using `torch.testing.assert_close`, `torch.testing.assert_allclose`, and `torch.testing.make_tensor`. 2. The test dataset includes input data and actual labels. You should generate model predictions for the input data and compare them to the actual labels. Example Usage ```python import torch import torch.nn as nn from torch.utils.data import DataLoader, TensorDataset # Example model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) # Create a simple dataset data = torch.randn(100, 10) targets = torch.randn(100, 1) dataset = TensorDataset(data, targets) test_loader = DataLoader(dataset, batch_size=10) # Initialize model model = SimpleModel() # Your function call validate_model_predictions(model, test_loader) ``` Notes - You should assume that all necessary imports are available. - You can assume the model is in evaluation mode and the data loader is correctly configured. - This question aims to assess your ability to use PyTorch\'s testing utilities to ensure model predictions are within an acceptable range of the true values.","solution":"import torch from torch.testing import assert_close, assert_allclose def validate_model_predictions(model, test_loader, tolerance=1e-5): Validate model predictions by comparing them to the actual values with a given tolerance. Parameters: model (torch.nn.Module): The PyTorch model to validate. test_loader (torch.utils.data.DataLoader): DataLoader containing the test data. tolerance (float): The tolerance level for assertions. Default is 1e-5. Raises: AssertionError: If any of the model predictions are not within the predefined tolerance of the actual values. model.eval() # Set model to evaluation mode with torch.no_grad(): for data, targets in test_loader: # Get model predictions predictions = model(data) # Use the assert_allclose function to compare predictions with actual targets assert_allclose(predictions, targets, rtol=tolerance, atol=tolerance)"},{"question":"<|Analysis Begin|> The provided documentation details a series of functions and macros around handling Python bytes objects at the C API level. Given that students are being assessed on their understanding of these concepts, a problem should expose them to a use case where they utilize these bytes manipulation functions. The key functions include: - `PyBytes_Check`: To check if a given python object is a bytes object. - `PyBytes_FromString`: To create a bytes object from a string. - `PyBytes_FromStringAndSize`: To create an uninitialized bytes object of a certain size. - `PyBytes_FromFormat`: To create a formatted bytes object. - `PyBytes_Size`: To get the size of a bytes object. - `PyBytes_AsString`: To access the internal buffer of a bytes object. - `PyBytes_Concat`: To concatenate two bytes objects. - `_PyBytes_Resize`: To resize a bytes object, though noted it should generally only be used to construct a new object. An advanced problem could combine these functions to test comprehensive understanding. For example, creating bytes objects, manipulating their size, concatenating them, and performing checks on their types. <|Analysis End|> <|Question Begin|> **Python Bytes Object Manipulation** In this assignment, you will be required to use a series of Python C API functions for bytes objects to manipulate and create byte arrays. The goal is to implement a series of utility functions that operate on bytes objects. 1. **is_bytes_object**: This function takes a Python object and returns if it is a bytes object. 2. **create_bytes_from_string**: This function takes a string and returns a new bytes object created from that string. 3. **concat_bytes_objects**: This function takes two bytes objects and returns a new bytes object with the contents of the second appended to the first. 4. **resize_bytes_object**: Create a function to resize a given bytes object to a specific size. Note that resizing should only be done for newly created bytes objects and must manage reference counts correctly. ```python # Import necessary Python C API libraries from cpython.bytes cimport PyBytes_Check, PyBytes_FromString, PyBytes_FromStringAndSize from cpython.bytes cimport PyBytes_Size, PyBytes_Concat, _PyBytes_Resize cdef extern from \\"Python.h\\": ctypedef struct PyObject: pass # Function Definitions def is_bytes_object(PyObject *o): Check if the provided PyObject is a bytes object return PyBytes_Check(o) def create_bytes_from_string(const char *v) -> PyObject*: Create a new bytes object from the given string return PyBytes_FromString(v) def concat_bytes_objects(PyObject **bytes1, PyObject *bytes2): Concatenate two bytes objects PyBytes_Concat(bytes1, bytes2) def resize_bytes_object(PyObject **bytes, Py_ssize_t new_size): Resize a new bytes object result = _PyBytes_Resize(bytes, new_size) if result == -1: raise MemoryError(\\"Resizing bytes object failed\\") return bytes ``` # Function Descriptions 1. **is_bytes_object(PyObject *o)**: - **Input**: A Python object `o`. - **Output**: True if `o` is a bytes object; otherwise, False. 2. **create_bytes_from_string(const char *v)**: - **Input**: A C string `v`. - **Output**: A new bytes object containing the contents of `v`. 3. **concat_bytes_objects(PyObject **bytes1, PyObject *bytes2)**: - **Input**: Two bytes objects `bytes1` and `bytes2`. - **Output**: `bytes1` now contains the concatenated result of `bytes1` and `bytes2`. 4. **resize_bytes_object(PyObject **bytes, Py_ssize_t new_size)**: - **Input**: A bytes object `bytes` and a new size `new_size`. - **Output**: The resized bytes object. Raises `MemoryError` if resizing fails. # Constraints 1. Do not allow modification of bytes objects unless they were freshly created. 2. Ensure that types and error handling are correctly managed especially when dealing with reference counting. This problem is designed to assess your understanding of the C Python API for bytes objects, managing object references, and proper error handling.","solution":"def is_bytes_object(o): Check if provided object is a bytes object. Args: o: Any python object. Returns: bool: True if o is a bytes object, otherwise False. return isinstance(o, bytes) def create_bytes_from_string(v): Create a new bytes object from the provided string. Args: v (str): A string to be converted to bytes object. Returns: bytes: A new bytes object containing the contents of v. if not isinstance(v, str): raise ValueError(\\"Input must be a string\\") return v.encode() def concat_bytes_objects(bytes1, bytes2): Concatenate two bytes objects. Args: bytes1 (bytes): The first bytes object. bytes2 (bytes): The second bytes object to concatenate to the first. Returns: bytes: The concatenated bytes object. if not isinstance(bytes1, bytes) or not isinstance(bytes2, bytes): raise ValueError(\\"Both inputs must be bytes objects\\") return bytes1 + bytes2 def resize_bytes_object(bytes_obj, new_size): Resize a given bytes object to a specific size. Args: bytes_obj (bytes): The bytes object to resize. new_size (int): The new size for the bytes object. Returns: bytes: The resized bytes object if the size is decreased. Otherwise, the original bytes object is adjusted by truncating or padding with null bytes. if not isinstance(bytes_obj, bytes): raise ValueError(\\"Input must be a bytes object\\") if not isinstance(new_size, int) or new_size < 0: raise ValueError(\\"New size must be a non-negative integer\\") return bytes_obj[:new_size] + (b\'x00\' * (new_size - len(bytes_obj)))"},{"question":"**Question:** Assume you have root privileges and access to the Unix shadow password database. Using the `spwd` module, you are required to write a function that achieves the following: **Task:** Write a function named `get_password_age_info` that accepts a list of usernames and returns a dictionary containing the username as the key and a sub-dictionary as the value. The sub-dictionary should contain two entries: - `\'last_change_days_ago\'`: Number of days since the password was last changed. - `\'days_until_expire\'`: Number of days left until the password expires. Use the current date to calculate the days until password expiration and the number of days since the last password change. # Function Signature ```python def get_password_age_info(usernames: list) -> dict: pass ``` # Input - `usernames`: A list of strings where each string represents a username. # Output - A dictionary where each key is a username and each value is a dictionary with keys `\'last_change_days_ago\'` and `\'days_until_expire\'` and their respective integer values. # Constraints - The function should handle cases where the username does not exist in the shadow password database by skipping that username. - If the password does not expire (denoted by `sp_max` being negative), set the value of `\'days_until_expire\'` to `None`. # Example ```python from datetime import datetime # Assume today is 2023-10-10 in your calculation def get_password_age_info(usernames): # Implementation goes here # Example usage: usernames = [\'user1\', \'user2\', \'user3\'] print(get_password_age_info(usernames)) ``` # Notes: 1. You may use the `datetime` module for date calculations. 2. Handle any exceptions that arise due to missing privileges or username not found. 3. Make sure your function is efficient for large lists of usernames. 4. Assume 1970-01-01 as the reference date for `sp_lstchg` and `sp_expire`.","solution":"import spwd import datetime def get_password_age_info(usernames: list) -> dict: Returns a dictionary containing the username as the key and a sub-dictionary with \'last_change_days_ago\' and \'days_until_expire\' as keys. results = {} today = datetime.datetime.now().date() epoch = datetime.date(1970, 1, 1) # reference date for sp_lstchg and sp_expire for username in usernames: try: user_info = spwd.getspnam(username) # Calculate days since last password change last_change_date = epoch + datetime.timedelta(days=user_info.sp_lstchg) last_change_days_ago = (today - last_change_date).days # Calculate days until password expires if user_info.sp_max >= 0: expire_date = last_change_date + datetime.timedelta(days=user_info.sp_max) days_until_expire = (expire_date - today).days else: days_until_expire = None results[username] = { \'last_change_days_ago\': last_change_days_ago, \'days_until_expire\': days_until_expire } except KeyError: # Skip if username does not exist continue return results"},{"question":"# Kernel Approximation with Nystroem Method and Linear Model You are given a dataset of two-dimensional points and their corresponding binary labels. Your task is to implement a function that: 1. Uses the Nystroem method for kernel approximation. 2. Fits a linear model to the transformed data. 3. Evaluates the model on the given dataset to compute the accuracy score. Your function should adhere to the following signature: ```python from sklearn.kernel_approximation import Nystroem from sklearn.linear_model import SGDClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def kernel_approx_nystroem(X, y, n_components=100, random_state=42): Parameters: X: list of list of floats (n_samples x n_features) - The input data. y: list of int (n_samples,) - The binary labels for the input data. n_components: int, optional (default=100) - The number of components to use for kernel approximation. random_state: int, optional (default=42) - The seed used by the random number generator. Returns: float - The accuracy score of the model on the input data. # Your implementation here ``` # Example ```python X = [[0, 0], [1, 1], [1, 0], [0, 1]] y = [0, 0, 1, 1] accuracy = kernel_approx_nystroem(X, y) print(accuracy) # Output: 1.0 ``` # Requirements - You should use `Nystroem` for kernel approximation. - Transform the data using the Nystroem method and then fit an `SGDClassifier`. - Output the accuracy of the classifier on the same dataset. # Evaluation Your solution will be evaluated on the following criteria: - Correct usage of Nystroem for kernel approximation. - Proper pipeline setup for fitting the linear model. - Accurate computation of the modelâ€™s accuracy score. Hints: - Refer to the documentation provided for the `Nystroem` and `SGDClassifier` usage. - Ensure you set the random state as specified to guarantee reproducibility of results.","solution":"from sklearn.kernel_approximation import Nystroem from sklearn.linear_model import SGDClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def kernel_approx_nystroem(X, y, n_components=100, random_state=42): Parameters: X: list of list of floats (n_samples x n_features) - The input data. y: list of int (n_samples,) - The binary labels for the input data. n_components: int, optional (default=100) - The number of components to use for kernel approximation. random_state: int, optional (default=42) - The seed used by the random number generator. Returns: float - The accuracy score of the model on the input data. nystroem = Nystroem(n_components=n_components, random_state=random_state) classifier = SGDClassifier(random_state=random_state) model = make_pipeline(nystroem, classifier) model.fit(X, y) y_pred = model.predict(X) return accuracy_score(y, y_pred)"},{"question":"**Challenging Python Coding Assessment** You are tasked with implementing a concurrent system for processing a number of independent tasks using Python\'s `concurrent.futures` module. Your function will simulate a real-world scenario where each task involves fetching data, processing the data, and saving the results. You should demonstrate how to utilize the `ThreadPoolExecutor` or `ProcessPoolExecutor` along with Futures. # Requirements 1. Define a function `fetch_data(task_id)` that simulates fetching data for a given `task_id`. This function should: - Print `Fetching data for task {task_id}` - Simulate a delay using `time.sleep(1)` and return `Data for task {task_id}` 2. Define a function `process_data(data)` that simulates processing the fetched data. This function should: - Print `Processing {data}` - Simulate a delay using `time.sleep(2)` and return `Processed {data}` 3. Define a function `save_result(processed_data)` that simulates saving the processed data. This function should: - Print `Saving {processed_data}` - Simulate a delay using `time.sleep(1)` and return `Saved {processed_data}` 4. Implement a high-level function `process_tasks_concurrently(task_ids, use_threads=True)` that: - Accepts a list of `task_ids` and a boolean `use_threads` to choose between ThreadPoolExecutor and ProcessPoolExecutor. - Uses `concurrent.futures` to fetch, process, and save data for each task concurrently. - Prints the final saved result for each task. # Input - `task_ids`: List of integers representing task IDs (e.g. `[1, 2, 3]`). - `use_threads`: Boolean to choose between ThreadPoolExecutor (`True`) and ProcessPoolExecutor (`False`). # Output - None (All outputs should be printed during the function execution.) # Constraints - You must use `concurrent.futures` for concurrency. - Ensure to handle exceptions that might occur during task execution. # Example Usage ```python def fetch_data(task_id): print(f\\"Fetching data for task {task_id}\\") time.sleep(1) return f\\"Data for task {task_id}\\" def process_data(data): print(f\\"Processing {data}\\") time.sleep(2) return f\\"Processed {data}\\" def save_result(processed_data): print(f\\"Saving {processed_data}\\") time.sleep(1) return f\\"Saved {processed_data}\\" def process_tasks_concurrently(task_ids, use_threads=True): # Your implementation here # Example inputs task_ids = [1, 2, 3] process_tasks_concurrently(task_ids, use_threads=True) ``` **Expected Output:** Order of the print statements may vary due to concurrency, but each task should follow the sequence: fetching, processing, and saving. Example (order may vary): ``` Fetching data for task 1 Fetching data for task 2 Fetching data for task 3 Processing Data for task 1 Processing Data for task 2 Processing Data for task 3 Saving Processed Data for task 1 Saving Processed Data for task 2 Saving Processed Data for task 3 ```","solution":"import time import concurrent.futures def fetch_data(task_id): Simulates fetching data for a given task_id. print(f\\"Fetching data for task {task_id}\\") time.sleep(1) return f\\"Data for task {task_id}\\" def process_data(data): Simulates processing the fetched data. print(f\\"Processing {data}\\") time.sleep(2) return f\\"Processed {data}\\" def save_result(processed_data): Simulates saving the processed data. print(f\\"Saving {processed_data}\\") time.sleep(1) return f\\"Saved {processed_data}\\" def process_task(task_id): High-level function to fetch, process, and save data for a single task. data = fetch_data(task_id) processed_data = process_data(data) result = save_result(processed_data) return result def process_tasks_concurrently(task_ids, use_threads=True): Processes tasks concurrently using ThreadPoolExecutor or ProcessPoolExecutor. executor_class = concurrent.futures.ThreadPoolExecutor if use_threads else concurrent.futures.ProcessPoolExecutor with executor_class() as executor: future_to_task_id = {executor.submit(process_task, task_id): task_id for task_id in task_ids} for future in concurrent.futures.as_completed(future_to_task_id): task_id = future_to_task_id[future] try: result = future.result() print(f\\"Final result for task {task_id}: {result}\\") except Exception as exc: print(f\\"Task {task_id} generated an exception: {exc}\\") # Example usage: # task_ids = [1, 2, 3] # process_tasks_concurrently(task_ids, use_threads=True)"},{"question":"# Python Coding Assessment: Data Model and Exception Handling Objective Demonstrate your understanding of Python\'s data model, exception handling, and custom class implementation. Problem Description You are to implement a custom `Account` class that models a simple bank account system. The `Account` class should support basic operations such as deposit, withdrawal, and checking the balance. Additionally, implement custom exceptions to handle scenarios where an operation cannot be performed. Requirements 1. **Class Definition**: Define a class `Account` with the following attributes: - `account_holder` (str): The name of the account holder. - `balance` (float): The current balance of the account. 2. **Methods**: - `__init__(self, account_holder: str, initial_balance: float = 0.0) -> None`: Initializes the account with the account holder\'s name and an optional initial balance (default is 0.0). - `deposit(self, amount: float) -> None`: Adds the given amount to the account balance. Throw an `InvalidTransactionError` if the amount is not positive. - `withdraw(self, amount: float) -> None`: Deducts the specified amount from the account balance. Throw an `InvalidTransactionError` if the amount is not positive or if the withdrawal amount is greater than the current balance. - `get_balance(self) -> float`: Returns the current account balance. 3. **Exceptions**: - Define a custom exception `InvalidTransactionError` that inherits from `Exception`. This exception should be raised with an appropriate message when invalid transactions are attempted (e.g., depositing a negative amount, withdrawing more than the available balance, etc.). Constraints - The account holder\'s name should be a non-empty string. - Initial balance and any transaction amount should be non-negative values. - Ensure your implementation adheres to Python\'s conventions for class and exception handling. Example Usage ```python # Example usage of the Account class try: my_account = Account(\\"John Doe\\", 100.0) my_account.deposit(50.0) print(my_account.get_balance()) # Output: 150.0 my_account.withdraw(30.0) print(my_account.get_balance()) # Output: 120.0 my_account.withdraw(200.0) # Should raise InvalidTransactionError except InvalidTransactionError as e: print(e) # Output: Withdrawal amount exceeds current balance ``` Deliverables - Python implementation of the `Account` class. - Proper implementation of custom exception handling. - Validation of inputs and appropriate error messages. - Ensure your code follows good Python code conventions and includes docstrings for each method.","solution":"class InvalidTransactionError(Exception): Custom exception for handling invalid transaction errors. def __init__(self, message): super().__init__(message) class Account: A simple bank account model. def __init__(self, account_holder: str, initial_balance: float = 0.0) -> None: if not account_holder: raise ValueError(\\"Account holder\'s name cannot be an empty string.\\") if initial_balance < 0: raise ValueError(\\"Initial balance cannot be negative.\\") self.account_holder = account_holder self.balance = initial_balance def deposit(self, amount: float) -> None: if amount <= 0: raise InvalidTransactionError(\\"Deposit amount must be positive.\\") self.balance += amount def withdraw(self, amount: float) -> None: if amount <= 0: raise InvalidTransactionError(\\"Withdrawal amount must be positive.\\") if amount > self.balance: raise InvalidTransactionError(\\"Withdrawal amount exceeds current balance.\\") self.balance -= amount def get_balance(self) -> float: return self.balance"},{"question":"**Problem Statement:** You are given a string containing HTML encoded characters. Your task is to implement a function `decode_html_entities(encoded_str)` that decodes these entities into their corresponding Unicode characters using the mappings provided in the \\"html.entities\\" module. # Function Signature: ```python def decode_html_entities(encoded_str: str) -> str: ``` # Input: - `encoded_str` (str): A string containing HTML-encoded characters. The string will contain only valid entities as per the `html.entities.html5` dictionary. # Output: - Returns a decoded string with all HTML entities replaced by their corresponding Unicode characters. # Constraints: - The input string will have a length between 1 and 1000 characters. - The HTML entities will follow the HTML5 specification and may or may not include the trailing semicolon (e.g., \\"gt\\" or \\"gt;\\"). - The function should make use of the `html5` dictionary from the `html.entities` module to perform the decoding. # Example: ```python # Example 1 encoded_str = \\"Hello &amp; welcome to the world of &lt;Python&gt;\\" print(decode_html_entities(encoded_str)) # Output: \\"Hello & welcome to the world of <Python>\\" # Example 2 encoded_str = \\"5 &gt; 3 and &quot;test&quot; &ne; \'exam\'\\" print(decode_html_entities(encoded_str)) # Output: \\"5 > 3 and \\"test\\" â‰  \'exam\'\\" ``` # Note: - Ensure that your function handles entities with and without the trailing semicolon. - You may use the `html5` dictionary from the `html.entities` module for the solution. # Reference: - The `html.entities` module documentation provides mappings for HTML entity names and their corresponding Unicode characters, which will be used for decoding.","solution":"import html def decode_html_entities(encoded_str: str) -> str: Decodes HTML entities in the given string using the html.entities.html5 dictionary. Args: encoded_str (str): A string containing HTML-encoded characters. Returns: str: A decoded string with all HTML entities replaced by their corresponding Unicode characters. return html.unescape(encoded_str)"},{"question":"# Objective Create a utility function called `dynamic_module_runner()` that can dynamically import a module, execute a specified function within that module, and then allow for reloading of the module if needed. # Function Signature ```python def dynamic_module_runner(module_name: str, function_name: str, *args, reload: bool = False, **kwargs) -> Any: Dynamically imports a module, executes a function within it, and optionally reloads the module. :param module_name: The name of the module to import. :param function_name: The name of the function in the module to execute. :param args: Positional arguments to pass to the function. :param reload: Boolean flag to indicate whether to reload the module. :param kwargs: Keyword arguments to pass to the function. :returns: The return value of the executed function. :raises ModuleNotFoundError: If the specified module cannot be imported. :raises AttributeError: If the specified function is not found in the module. ``` # Requirements - The function should use `importlib` functionalities such as `importlib.import_module()` and `importlib.reload()`. - Handle potential errors, including module not found and function not found. - Return the result of the executed function. - If `reload` is set to `True`, reload the module before executing the function. # Example Usage ```python # Assume my_module has a function `my_function(a, b, c=3)` defined. result = dynamic_module_runner(\'my_module\', \'my_function\', 1, 2, c=4) print(result) # Expected output would be the return value of my_module.my_function(1, 2, c=4) # Changing functionality and ensuring the reload works result = dynamic_module_runner(\'my_module\', \'my_function\', 1, 2, c=4, reload=True) print(result) # Expected output would be the updated return value after reloading the module. ``` # Constraints - Ensure proper error handling for module import and function execution. - Efficiently reload the module when required. - The function should be capable of handling arbitrary positional and keyword arguments. Implementing this function will demonstrate a deep understanding of the `importlib` package, especially its utilities for dynamic imports and reloading modules.","solution":"import importlib def dynamic_module_runner(module_name: str, function_name: str, *args, reload: bool = False, **kwargs): Dynamically imports a module, executes a function within it, and optionally reloads the module. :param module_name: The name of the module to import. :param function_name: The name of the function in the module to execute. :param args: Positional arguments to pass to the function. :param reload: Boolean flag to indicate whether to reload the module. :param kwargs: Keyword arguments to pass to the function. :returns: The return value of the executed function. :raises ModuleNotFoundError: If the specified module cannot be imported. :raises AttributeError: If the specified function is not found in the module. try: module = importlib.import_module(module_name) if reload: module = importlib.reload(module) func = getattr(module, function_name) if not callable(func): raise AttributeError(f\\"{function_name} is not callable\\") return func(*args, **kwargs) except ModuleNotFoundError as e: raise ModuleNotFoundError(f\\"Module \'{module_name}\' cannot be found\\") from e except AttributeError as e: raise AttributeError(f\\"Function \'{function_name}\' not found in module \'{module_name}\'\\") from e"},{"question":"# Comprehensive Audio Processing with `ossaudiodev` Module You are tasked with creating a script that captures audio from an input device and plays it back to an output device using the `ossaudiodev` module. Your implementation should follow these requirements: 1. **Open audio input and output devices**, ensuring that the correct parameters (format, channels, sample rate) are configured. 2. **Implement error handling** to manage any potential `OSError` or `OSSAudioError` exceptions. 3. **Read a specific duration of audio data from the input device** (e.g., 5 seconds) and write this data to the output device. 4. **Use context management** to ensure resources are correctly managed (i.e., devices are properly closed). 5. **Flexible parameter handling**, with defaults specified but allowing for user input to override these defaults. # Function Signature ```python import ossaudiodev def process_audio(input_duration: int = 5, input_device: str = \\"/dev/dsp\\", output_device: str = \\"/dev/dsp\\", audio_format: str = \\"AFMT_S16_LE\\", nchannels: int = 1, samplerate: int = 44100) -> None: Captures audio from the input device and plays it back to the output device. Args: - input_duration (int): Duration of audio capture in seconds (default is 5 seconds). - input_device (str): Audio input device path (default is \\"/dev/dsp\\"). - output_device (str): Audio output device path (default is \\"/dev/dsp\\"). - audio_format (str): Audio format (default is AFMT_S16_LE). - nchannels (int): Number of audio channels (default is 1, monophonic). - samplerate (int): Sampling rate in Hz (default is 44100 Hz, CD quality). Returns: - None ``` # Constraints and Assumptions - **Devices**: Assumes the presence of devices at specified paths (/dev/dsp and /dev/mixer). - **Permissions**: Assumes the script has necessary permissions to read/write from/to these devices. - **Error Handling**: Appropriate exceptions should be handled gracefully, ensuring resources are released properly. # Example Usage ```python if __name__ == \\"__main__\\": try: # Capture and play back audio for 5 seconds process_audio() except Exception as e: print(f\\"An error occurred: {e}\\") ``` # Instructions: 1. **Open** the input and output devices using `ossaudiodev.open()`. 2. Set the **audio parameters** (format, channels, sample rate) using respective methods or `setparameters()`. 3. **Capture** audio data for the specified duration from the input device. 4. **Write** the captured audio data to the output device. 5. **Handle any exceptions** that arise during these operations. 6. **Ensure devices are closed** properly using context management or explicit close commands. Provide your implementation below.","solution":"import ossaudiodev import contextlib import time def process_audio(input_duration: int = 5, input_device: str = \\"/dev/dsp\\", output_device: str = \\"/dev/dsp\\", audio_format: str = \\"AFMT_S16_LE\\", nchannels: int = 1, samplerate: int = 44100) -> None: Captures audio from the input device and plays it back to the output device. Args: - input_duration (int): Duration of audio capture in seconds (default is 5 seconds). - input_device (str): Audio input device path (default is \\"/dev/dsp\\"). - output_device (str): Audio output device path (default is \\"/dev/dsp\\"). - audio_format (str): Audio format (default is AFMT_S16_LE). - nchannels (int): Number of audio channels (default is 1, monophonic). - samplerate (int): Sampling rate in Hz (default is 44100 Hz, CD quality). Returns: - None # Resolve audio format format_dict = {\\"AFMT_S16_LE\\": ossaudiodev.AFMT_S16_LE, \\"AFMT_S16_BE\\": ossaudiodev.AFMT_S16_BE, \\"AFMT_U8\\": ossaudiodev.AFMT_U8 } if audio_format not in format_dict: raise ValueError(f\\"Unsupported audio format: {audio_format}. Use one of {list(format_dict.keys())}\\") format_code = format_dict[audio_format] try: # Open the input and output devices with contextlib.closing(ossaudiodev.open(\'r\', input_device)) as dsp_in, contextlib.closing(ossaudiodev.open(\'w\', output_device)) as dsp_out: # Set input parameters dsp_in.setparameters(format_code, nchannels, samplerate) dsp_out.setparameters(format_code, nchannels, samplerate) start_time = time.time() audio_data = b\'\' while time.time() - start_time < input_duration: chunk_size = 1024 # Can vary based on needs data = dsp_in.read(chunk_size) audio_data += data dsp_out.write(data) except (OSError, ossaudiodev.OSSAudioError) as e: print(f\\"An OSS audio error occurred: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# **Email Content Manager Implementation and Usage** **Objective:** Implement a custom Email Content Manager by extending the `ContentManager` class and demonstrate its usage by handling a MIME email message. **Problem Statement:** 1. **Implement a Custom Content Manager:** - Extend the `ContentManager` class to create a custom content manager, `CustomContentManager`. - Implement methods to register and use custom handlers for the following MIME types: - `text/html` - `application/json` - Implement the `get_content` and `set_content` methods to handle these MIME types. 2. **Demonstrate the Custom Content Manager:** - Create an instance of `EmailMessage`. - Use your `CustomContentManager` to set contents of MIME types `text/html` and `application/json` in an `EmailMessage` object. - Retrieve and print these contents using the `CustomContentManager`. **Input Format:** - N/A (The implementation will create necessary objects internally). **Output Format:** - The printed output should include the retrieved contents for MIME types: - `text/html` - `application/json` **Constraints:** - Ensure that `get_content` raises a `KeyError` for unhandled MIME types. - Ensure that `set_content` raises a `TypeError` when `maintype` is `multipart`. # **Example:** **Code Execution:** ```python # Example usage (not actual implementation here): msg = EmailMessage() custom_manager = CustomContentManager() # Setting content custom_manager.set_content(msg, \\"<h1>Hello, World!</h1>\\", maintype=\\"text\\", subtype=\\"html\\") custom_manager.set_content(msg, {\\"key\\": \\"value\\"}, maintype=\\"application\\", subtype=\\"json\\") # Getting content html_content = custom_manager.get_content(msg, maintype=\\"text\\", subtype=\\"html\\") json_content = custom_manager.get_content(msg, maintype=\\"application\\", subtype=\\"json\\") print(\\"HTML Content:\\", html_content) print(\\"JSON Content:\\", json_content) ``` **Expected Output:** ``` HTML Content: <h1>Hello, World!</h1> JSON Content: {\'key\': \'value\'} ``` # **Implementation:** ```python from email.contentmanager import ContentManager from email.message import EmailMessage import json class CustomContentManager(ContentManager): def __init__(self): super().__init__() self._get_handlers = {} self._set_handlers = {} # Register handlers self.add_get_handler(\'text/html\', self._get_text_html) self.add_get_handler(\'application/json\', self._get_application_json) self.add_set_handler(str, self._set_text_html) self.add_set_handler(dict, self._set_application_json) def add_get_handler(self, key, handler): self._get_handlers[key] = handler def add_set_handler(self, typekey, handler): self._set_handlers[typekey] = handler def get_content(self, msg, *args, **kw): mime_type = f\\"{msg.get_content_maintype()}/{msg.get_content_subtype()}\\" if mime_type in self._get_handlers: return self._get_handlers[mime_type](msg, *args, **kw) else: raise KeyError(f\\"No handler for MIME type: {mime_type}\\") def set_content(self, msg, obj, maintype, subtype, *args, **kw): if maintype == \'multipart\': raise TypeError(\\"Cannot set content for multipart\\") typekey = type(obj) if typekey in self._set_handlers: self.clear_content(msg) msg.set_param(\\"Content-Type\\", f\\"{maintype}/{subtype}\\") self._set_handlers[typekey](msg, obj, *args, **kw) else: raise KeyError(f\\"No handler for object type: {typekey}\\") def clear_content(self, msg): msg.set_payload(None) def _get_text_html(self, msg, *args, **kw): return msg.get_payload(decode=True).decode(\'utf-8\') def _get_application_json(self, msg, *args, **kw): return json.loads(msg.get_payload(decode=True).decode(\'utf-8\')) def _set_text_html(self, msg, obj, *args, **kw): msg.set_payload(obj.encode(\'utf-8\')) def _set_application_json(self, msg, obj, *args, **kw): msg.set_payload(json.dumps(obj).encode(\'utf-8\')) # Example usage msg = EmailMessage() custom_manager = CustomContentManager() # Setting content custom_manager.set_content(msg, \\"<h1>Hello, World!</h1>\\", maintype=\\"text\\", subtype=\\"html\\") custom_manager.set_content(msg, {\\"key\\": \\"value\\"}, maintype=\\"application\\", subtype=\\"json\\") # Getting content html_content = custom_manager.get_content(msg) json_content = custom_manager.get_content(msg) print(\\"HTML Content:\\", html_content) print(\\"JSON Content:\\", json_content) ```","solution":"from email.contentmanager import ContentManager from email.message import EmailMessage import json class CustomContentManager(ContentManager): def __init__(self): super().__init__() self._get_handlers = {} self._set_handlers = {} # Register handlers self.add_get_handler(\'text/html\', self._get_text_html) self.add_get_handler(\'application/json\', self._get_application_json) self.add_set_handler(str, self._set_text_html) self.add_set_handler(dict, self._set_application_json) def add_get_handler(self, key, handler): self._get_handlers[key] = handler def add_set_handler(self, typekey, handler): self._set_handlers[typekey] = handler def get_content(self, msg, maintype, subtype, *args, **kw): mime_type = f\\"{maintype}/{subtype}\\" if mime_type in self._get_handlers: return self._get_handlers[mime_type](msg, *args, **kw) else: raise KeyError(f\\"No handler for MIME type: {mime_type}\\") def set_content(self, msg, obj, maintype, subtype, *args, **kw): if maintype == \'multipart\': raise TypeError(\\"Cannot set content for multipart\\") typekey = type(obj) if typekey in self._set_handlers: msg.set_type(f\\"{maintype}/{subtype}\\") self._set_handlers[typekey](msg, obj, *args, **kw) else: raise KeyError(f\\"No handler for object type: {typekey}\\") def _get_text_html(self, msg, *args, **kw): return msg.get_payload(decode=True).decode(\'utf-8\') def _get_application_json(self, msg, *args, **kw): return json.loads(msg.get_payload(decode=True).decode(\'utf-8\')) def _set_text_html(self, msg, obj, *args, **kw): msg.set_payload(obj.encode(\'utf-8\')) def _set_application_json(self, msg, obj, *args, **kw): msg.set_payload(json.dumps(obj).encode(\'utf-8\')) # Example usage msg = EmailMessage() custom_manager = CustomContentManager() # Setting content custom_manager.set_content(msg, \\"<h1>Hello, World!</h1>\\", maintype=\\"text\\", subtype=\\"html\\") custom_manager.set_content(msg, {\\"key\\": \\"value\\"}, maintype=\\"application\\", subtype=\\"json\\") # Getting content html_content = custom_manager.get_content(msg, maintype=\\"text\\", subtype=\\"html\\") json_content = custom_manager.get_content(msg, maintype=\\"application\\", subtype=\\"json\\") print(\\"HTML Content:\\", html_content) print(\\"JSON Content:\\", json_content)"},{"question":"# Covariance Estimation with Scikit-learn Objective: Implement and compare different covariance estimation techniques provided by Scikit-learn. You will use a synthetic dataset to analyze and visualize the performance of each estimator. Dataset: Generate a synthetic dataset using multivariate normal distribution with a specified covariance matrix. Introduce outliers to test the robustness of the estimators. Tasks: 1. **Data Generation**: - Create a dataset `X` with 1000 samples and 5 features using a multivariate normal distribution. - Introduce 50 outliers by adding large random noise to the first 50 samples. 2. **Implementation**: - Compute the covariance matrix using `EmpiricalCovariance`. - Apply `ShrunkCovariance` with a shrinkage parameter of 0.1. - Compute the covariance matrix using the `LedoitWolf` estimator. - Compute the covariance matrix using the `OAS` estimator. - Apply `GraphicalLassoCV` to estimate the sparse inverse covariance. - Use `MinCovDet` to compute a robust covariance matrix. 3. **Visualization**: - Plot the covariance matrices for each of the methods. - Plot the Mahalanobis distance of each sample from the mean using both `EmpiricalCovariance` and `MinCovDet`. 4. **Comparison**: - Discuss the differences observed among the estimators in terms of robustness against outliers and conditioning of the covariance matrix. Specifications: - You should use `numpy`, `matplotlib`, and `sklearn` libraries. - Document your code and add comments explaining each step. - The output should include the plots as described and a brief discussion on your observations. Example Template: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import ( EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLassoCV, MinCovDet ) # Task 1: Data Generation np.random.seed(0) n_samples, n_features = 1000, 5 mean = np.zeros(n_features) covariance_matrix = np.eye(n_features) X = np.random.multivariate_normal(mean, covariance_matrix, size=n_samples) # Introduce outliers outliers = np.random.uniform(low=-10, high=10, size=(50, n_features)) X[:50] += outliers # Task 2: Implementation # Empirical Covariance emp_cov = EmpiricalCovariance().fit(X) emp_cov_matrix = emp_cov.covariance_ # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X) shrunk_cov_matrix = shrunk_cov.covariance_ # Ledoit-Wolf Estimator lw = LedoitWolf().fit(X) lw_cov_matrix = lw.covariance_ # OAS Estimator oas = OAS().fit(X) oas_cov_matrix = oas.covariance_ # Graphical Lasso graph_lasso = GraphicalLassoCV().fit(X) graph_lasso_cov_matrix = graph_lasso.covariance_ # Minimum Covariance Determinant mcd = MinCovDet().fit(X) mcd_cov_matrix = mcd.covariance_ # Task 3: Visualization fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes[0, 0].imshow(emp_cov_matrix) axes[0, 0].set_title(\'Empirical Covariance\') axes[0, 1].imshow(shrunk_cov_matrix) axes[0, 1].set_title(\'Shrunk Covariance\') axes[0, 2].imshow(lw_cov_matrix) axes[0, 2].set_title(\'Ledoit-Wolf\') axes[1, 0].imshow(oas_cov_matrix) axes[1, 0].set_title(\'OAS\') axes[1, 1].imshow(graph_lasso_cov_matrix) axes[1, 1].set_title(\'Graphical Lasso\') axes[1, 2].imshow(mcd_cov_matrix) axes[1, 2].set_title(\'Minimum Covariance Determinant\') plt.show() # Task 4: Comparison # Mahalanobis distances using Empirical Covariance and MinCovDet emp_mahalanobis_distances = emp_cov.mahalanobis(X) mcd_mahalanobis_distances = mcd.mahalanobis(X) plt.figure() plt.hist(emp_mahalanobis_distances, bins=30, alpha=0.5, label=\'Empirical Covariance\') plt.hist(mcd_mahalanobis_distances, bins=30, alpha=0.5, label=\'MinCovDet\') plt.legend() plt.title(\'Mahalanobis Distances\') plt.show() # Discussion: # [Add your discussion here] ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import ( EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLassoCV, MinCovDet ) # Task 1: Data Generation def generate_data(): np.random.seed(0) n_samples, n_features = 1000, 5 mean = np.zeros(n_features) covariance_matrix = np.eye(n_features) X = np.random.multivariate_normal(mean, covariance_matrix, size=n_samples) # Introduce outliers outliers = np.random.uniform(low=-10, high=10, size=(50, n_features)) X[:50] += outliers return X # Task 2: Implementation def compute_covariances(X): # Empirical Covariance emp_cov = EmpiricalCovariance().fit(X) emp_cov_matrix = emp_cov.covariance_ # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X) shrunk_cov_matrix = shrunk_cov.covariance_ # Ledoit-Wolf Estimator lw = LedoitWolf().fit(X) lw_cov_matrix = lw.covariance_ # OAS Estimator oas = OAS().fit(X) oas_cov_matrix = oas.covariance_ # Graphical Lasso graph_lasso = GraphicalLassoCV().fit(X) graph_lasso_cov_matrix = graph_lasso.covariance_ # Minimum Covariance Determinant mcd = MinCovDet().fit(X) mcd_cov_matrix = mcd.covariance_ return emp_cov, shrunk_cov, lw, oas, graph_lasso, mcd # Task 3: Visualization def plot_covariance_matrices(covariances): emp_cov_matrix, shrunk_cov_matrix, lw_cov_matrix, oas_cov_matrix, graph_lasso_cov_matrix, mcd_cov_matrix = [cov.covariance_ for cov in covariances] fig, axes = plt.subplots(2, 3, figsize=(15, 10)) axes[0, 0].imshow(emp_cov_matrix) axes[0, 0].set_title(\'Empirical Covariance\') axes[0, 1].imshow(shrunk_cov_matrix) axes[0, 1].set_title(\'Shrunk Covariance\') axes[0, 2].imshow(lw_cov_matrix) axes[0, 2].set_title(\'Ledoit-Wolf\') axes[1, 0].imshow(oas_cov_matrix) axes[1, 0].set_title(\'OAS\') axes[1, 1].imshow(graph_lasso_cov_matrix) axes[1, 1].set_title(\'Graphical Lasso\') axes[1, 2].imshow(mcd_cov_matrix) axes[1, 2].set_title(\'Minimum Covariance Determinant\') plt.show() def plot_mahalanobis_distances(X, emp_cov, mcd): emp_mahalanobis_distances = emp_cov.mahalanobis(X) mcd_mahalanobis_distances = mcd.mahalanobis(X) plt.figure() plt.hist(emp_mahalanobis_distances, bins=30, alpha=0.5, label=\'Empirical Covariance\') plt.hist(mcd_mahalanobis_distances, bins=30, alpha=0.5, label=\'MinCovDet\') plt.legend() plt.title(\'Mahalanobis Distances\') plt.show() # Generate data X = generate_data() # Compute covariances covariances = compute_covariances(X) # Plot covariance matrices plot_covariance_matrices(covariances) # Plot Mahalanobis distances plot_mahalanobis_distances(X, covariances[0], covariances[5])"},{"question":"**Question: Implementing a Custom Fault Handler Utility** You are tasked with implementing a utility that uses the `faulthandler` module to enhance the crash diagnostics of a Python program. Your utility will: 1. Enable the fault handler to capture detailed tracebacks on crashes. 2. Register a custom user signal that, when received, dumps the current tracebacks of all threads. 3. Set a timer to periodically dump tracebacks if a specified duration elapses. 4. Provide the ability to disable the fault handler and registered signals. Your implementation should revolve around the following functions: # 1. `initialize_fault_handler(log_file_path, user_signal, timeout, repeat)` **Parameters:** - `log_file_path` (str): Path to the file where the traceback output will be logged. - `user_signal` (int): Signal number to register for dumping tracebacks on user signal. - `timeout` (int): Time in seconds after which tracebacks should be periodically dumped. - `repeat` (bool): If `True`, dumps tracebacks every `timeout` seconds repeatedly. **Behavior:** - Enable the fault handler to dump tracebacks to the specified log file. - Register the user signal to trigger traceback dumps. - Set a timer to dump tracebacks after the specified timeout. # 2. `disable_fault_handler_and_signals(user_signal)` **Parameters:** - `user_signal` (int): Signal number that was registered for dumping tracebacks. **Behavior:** - Cancel the periodic timeout traceback dumping if set. - Unregister the user signal. - Disable the fault handler. # Example Usage: ```python import signal # Initialize the fault handler utility initialize_fault_handler(\\"traceback_log.txt\\", signal.SIGUSR1, 10, True) # Simulate some application activity that might cause faults or deadlocks # Clean up fault handler before exiting disable_fault_handler_and_signals(signal.SIGUSR1) ``` # Note: - Ensure that file operations handle exceptions gracefully. - The utility should handle scenarios where the same initialization function is called multiple times by resetting the behavior correctly. **Constraints:** - The provided file path must be valid and writable. - The user signal must be a valid signal number on the system and not already in use by other handlers. - The timeout must be a positive integer. Implement the functions `initialize_fault_handler` and `disable_fault_handler_and_signals`.","solution":"import faulthandler import signal import threading import os timer = None def initialize_fault_handler(log_file_path, user_signal, timeout, repeat): global timer # Enable the fault handler with the log file path try: log_file = open(log_file_path, \'w\') faulthandler.enable(log_file) except Exception as e: print(f\\"Error opening log file: {e}\\") return # Register the user signal to trigger traceback dumps def handler(sig, frame): faulthandler.dump_traceback(file=log_file) signal.signal(user_signal, handler) # Set a timer to dump tracebacks after the specified timeout def dump_tracebacks(): faulthandler.dump_traceback(file=log_file) if repeat: global timer timer = threading.Timer(timeout, dump_tracebacks) timer.start() if timeout > 0: timer = threading.Timer(timeout, dump_tracebacks) timer.start() def disable_fault_handler_and_signals(user_signal): global timer # Cancel the periodic timeout traceback dumping if set if timer is not None: timer.cancel() timer = None # Unregister the user signal signal.signal(user_signal, signal.SIG_DFL) # Disable the fault handler faulthandler.disable()"},{"question":"# Python Coding Assessment Question **Objective:** Your task is to create a utility function that emulates the deprecated `imp` module functionality using the modern `importlib` module for loading Python modules. You will also demonstrate handling bytecode caching and reloading of modules. **Task Description:** 1. Implement the function `modern_import(module_name: str, reload: bool = False) -> object` which: - Imports the module specified by `module_name`. - If the `reload` flag is `True`, it reloads the previously imported module. - Utilizes `importlib.util.find_spec`, `importlib.util.module_from_spec`, and `importlib.reload`. - Handles errors gracefully and returns `None` if the module cannot be imported. 2. Implement the function `cache_path(source_path: str, debug: bool = False) -> str` which: - Returns the byte-compiled file path from a given source file path using `importlib.util.cache_from_source`. - Handles situations where the cache tag is not available or not implemented, raising a `NotImplementedError`. 3. Implement the function `source_path(cache_path: str) -> str` which: - Given the path to a bytecode file, returns the corresponding source path using `importlib.util.source_from_cache`. # Constraints - Do not use the deprecated `imp` module. - Mimic the provided functionality strictly using the modern `importlib` counterpart functions. - Ensure exception handling for module imports. # Input and Output Formats: - `modern_import(module_name: str, reload: bool = False) -> object` - **Input**: - `module_name` (str): The name of the module to import. - `reload` (bool, optional): If `True`, the module is reloaded. Default is `False`. - **Output**: The imported module object, or `None` if there is an error. - `cache_path(source_path: str, debug: bool = False) -> str` - **Input**: - `source_path` (str): The path of the source file. - `debug` (bool, optional): If `True`, uses optimized bytecode. Default is `False`. - **Output**: The path to the byte-compiled file. - `source_path(cache_path: str) -> str` - **Input**: - `cache_path` (str): The path to the byte-compiled file. - **Output**: The corresponding source file path. # Example: ```python # Example usage: # Importing under the imp replacement module = modern_import(\'example_module\') print(module) # Reloading the module reloaded_module = modern_import(\'example_module\', reload=True) print(reloaded_module) # Fetching cache path source = \\"/foo/bar/baz.py\\" cache = cache_path(source) print(cache) # Fetching source path from cache path reconstructed_source = source_path(cache) print(reconstructed_source) ``` # Additional Notes: - Ensure your implementation adheres to the prescribed methods and handles exceptions as specified. - Use appropriate documentation and type annotations for the functions.","solution":"import importlib.util import importlib import sys def modern_import(module_name: str, reload: bool = False) -> object: Import a module by name. Optionally reload the module if it\'s already imported. :param module_name: Name of the module to import. :param reload: Reload the module if it\'s already imported. :return: Imported module object or None if there\'s an error. try: if module_name in sys.modules and reload: return importlib.reload(sys.modules[module_name]) else: spec = importlib.util.find_spec(module_name) if spec is None: return None module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) return module except Exception: return None def cache_path(source_path: str, debug: bool = False) -> str: Return the byte-compiled file path for a given source file path. :param source_path: Path of the source file. :param debug: Use the optimized bytecode if True. :return: Byte-compiled file path. :raises NotImplementedError: If `cache_from_source` cannot be used. try: return importlib.util.cache_from_source(source_path, optimization=\'\' if debug else \'opt\') except NotImplementedError: raise NotImplementedError(\\"The `cache_from_source` function is not implemented for this path.\\") def source_path(cache_path: str) -> str: Given the path to a bytecode file, return the corresponding source path. :param cache_path: Path to the bytecode file. :return: Corresponding source file path. return importlib.util.source_from_cache(cache_path)"},{"question":"# Netrc File Handling with Python Objective: You are tasked with implementing a function to work with netrc files, which are used for storing login information for different hosts. Your function will read a netrc file, retrieve login details for given hosts, and output these details in a predefined format. Requirements 1. Implement a function `fetch_authenticators(filename: str, hosts: List[str]) -> Dict[str, Tuple[str, str, str]]` that: - Takes a filename (string) representing the path to the netrc file and a list of hostnames (List[str]). - Returns a dictionary where the keys are hostnames and the values are tuples of the form `(login, account, password)`. If a host is not found, it should not appear in the dictionary. Input: - `filename` (str): The path to the netrc file. - `hosts` (List[str]): A list of hostnames for which to retrieve authentication details. Output: - A dictionary mapping each requested host to a 3-tuple `(login, account, password)`. Hosts that are not found should be omitted from the dictionary. Example: Consider a netrc file `sample.netrc` with the following content: ``` machine host1.somewhere.com login foo account bar password my_secret_password machine host2.somewhere.com login hello account world password another_secret ``` Calling `fetch_authenticators(\'sample.netrc\', [\'host1.somewhere.com\', \'host2.somewhere.com\', \'host3.somewhere.com\'])` should return: ```python { \'host1.somewhere.com\': (\'foo\', \'bar\', \'my_secret_password\'), \'host2.somewhere.com\': (\'hello\', \'world\', \'another_secret\') } ``` Constraints: - The netrc file may contain `default` entries that should not be included in the output. - Invalid or non-existent hosts should be handled gracefully without raising exceptions. - Do not use any external libraries other than the Python Standard Library. Note: - Ensure you handle exceptions such as file not found or parse errors by returning an empty dictionary in such cases. ```python from typing import List, Dict, Tuple import netrc def fetch_authenticators(filename: str, hosts: List[str]) -> Dict[str, Tuple[str, str, str]]: # Your implementation here pass # Example usage: result = fetch_authenticators(\'sample.netrc\', [\'host1.somewhere.com\', \'host2.somewhere.com\']) print(result) ``` Your task is to implement the `fetch_authenticators` function as described.","solution":"from typing import List, Dict, Tuple import netrc def fetch_authenticators(filename: str, hosts: List[str]) -> Dict[str, Tuple[str, str, str]]: try: netrc_obj = netrc.netrc(filename) auth_dict = {} for host in hosts: if host in netrc_obj.hosts: login, account, password = netrc_obj.authenticators(host) auth_dict[host] = (login, account, password) return auth_dict except Exception as e: # Return an empty dictionary if there\'s any error return {}"},{"question":"You are tasked with analyzing a given dataset using Seaborn\'s ECDF plotting functionalities. The dataset consists of multiple numerical features and a categorical feature indicating different classes. You need to implement a function that generates and saves a series of ECDF plots to understand the distribution of each numerical feature across different classes. Function Signature ```python def generate_ecdf_plots(dataset: pd.DataFrame, numerical_features: list, class_feature: str, output_dir: str) -> None: Generates and saves ECDF plots for specified numerical features across different classes. Parameters: - dataset (pd.DataFrame): The input dataset containing numerical and categorical features. - numerical_features (list): A list of numerical feature names to plot. - class_feature (str): The name of the categorical feature indicating class membership. - output_dir (str): The directory path where the plots will be saved. Output: - None: The function saves the generated ECDF plots as images in the specified output directory. pass ``` Input - `dataset`: A pandas DataFrame containing the data. This dataset includes both numerical features and a categorical feature indicating different classes. - `numerical_features`: A list of strings, where each string is the name of a numerical feature in the dataset that you need to plot. - `class_feature`: A string representing the name of the categorical feature indicating different classes or groups in the dataset. - `output_dir`: A string representing the directory where the generated ECDF plots will be saved. Requirements 1. For each numerical feature in the `numerical_features` list, generate an ECDF plot. 2. Each plot should show the distribution of the numerical feature for each class (using the `hue` parameter). 3. Save each plot as a PNG file in the specified `output_dir` directory. Name each file as `<numerical_feature>_ecdf.png`. Constraints - The dataset will always contain the specified numerical features and the class feature. - The output directory path will always be valid and writable. Example ```python import pandas as pd # Sample dataset data = { \'length\': [15, 20, 25, 10, 30, 35], \'width\': [5, 10, 15, 5, 10, 15], \'height\': [7, 10, 15, 15, 10, 7], \'type\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\'] } df = pd.DataFrame(data) # List of numerical features num_features = [\'length\', \'width\', \'height\'] # Class feature class_feat = \'type\' # Output directory output_directory = \'ecdf_plots/\' # Generate ECDF plots generate_ecdf_plots(df, num_features, class_feat, output_directory) ``` Upon running the above example, the function will generate and save three ECDF plots named `length_ecdf.png`, `width_ecdf.png`, and `height_ecdf.png` in the `ecdf_plots/` directory.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import os def generate_ecdf_plots(dataset: pd.DataFrame, numerical_features: list, class_feature: str, output_dir: str) -> None: Generates and saves ECDF plots for specified numerical features across different classes. Parameters: - dataset (pd.DataFrame): The input dataset containing numerical and categorical features. - numerical_features (list): A list of numerical feature names to plot. - class_feature (str): The name of the categorical feature indicating class membership. - output_dir (str): The directory path where the plots will be saved. Output: - None: The function saves the generated ECDF plots as images in the specified output directory. # Ensure output directory exists os.makedirs(output_dir, exist_ok=True) for feature in numerical_features: plt.figure() sns.ecdfplot(data=dataset, x=feature, hue=class_feature) plt.title(f\'ECDF of {feature} by {class_feature}\') plt.xlabel(feature) plt.ylabel(\'ECDF\') # Save the plot as a .png file filename = os.path.join(output_dir, f\'{feature}_ecdf.png\') plt.savefig(filename) plt.close()"},{"question":"Write a function `optimize_tail_call` that transforms a given Python function to optimize tail-call recursion. Tail-call recursion occurs when a function calls itself as its final action, allowing for optimizations like reusing stack frames. Python does not optimize tail calls natively, so the solution should involve disassembling the given function, analyzing its bytecode, and modifying it to implement tail-call optimization manually. # Function Signature ```python def optimize_tail_call(func: callable) -> callable: ``` # Input - `func` (callable): A Python function that may contain tail-call recursion. # Output - Returns a new callable function that is equivalent to the input function but optimized for tail-call recursion. # Example ```python def factorial(n, acc=1): if n == 0: return acc return factorial(n-1, n*acc) optimized_factorial = optimize_tail_call(factorial) print(optimized_factorial(5)) # Output should be 120 ``` # Constraints - The input function will only contain deterministic, non-nested tail-recursive calls. - Focus on making an equivalent function with optimized tail-call features using manipulation of the function\'s bytecode. # Note - Take advantage of the `dis` module functionalities, especially `dis.Bytecode`, `dis.Instruction`, and relevant functions to analyze and modify the bytecode. - Preserve the function\'s behavior while ensuring stack frames are reused during tail-recursive calls. - You may need to recompile modified bytecodes and ensure all modifications are valid Python bytecode instructions. # Hints 1. Disassemble the input function and identify the pattern of tail-call recursion. 2. Modify the bytecode to implement a loop that reuses the function\'s stack frame instead of making a new call. 3. Use `types.FunctionType` to create and return the new optimized function.","solution":"import dis import types import functools def optimize_tail_call(func: callable) -> callable: Transforms a given Python function to optimize tail-call recursion. @functools.wraps(func) def wrapper(*args, **kwargs): stack = [(args, kwargs)] while stack: args, kwargs = stack.pop() result = func(*args, **kwargs) if isinstance(result, TailCall): stack.append((result.args, result.kwargs)) else: return result return wrapper class TailCall: def __init__(self, args, kwargs): self.args = args self.kwargs = kwargs def tail_call(args, kwargs): return TailCall(args, kwargs) # Example usage: def factorial(n, acc=1): if n == 0: return acc return tail_call((n-1, n*acc), {}) optimized_factorial = optimize_tail_call(factorial) print(optimized_factorial(5)) # Output should be 120"},{"question":"Objective Design and implement a function that processes a list of file paths according to provided inclusion and exclusion patterns, mimicking the behavior of the manifest template commands described in the documentation. Problem Statement You need to implement a function `process_files(file_paths, include_patterns, exclude_patterns)` that filters a list of `file_paths` based on Unix-style glob `include_patterns` and `exclude_patterns`. - If a file path matches any of the `include_patterns`, it should be included in the result, unless it also matches an `exclude_patterns`. - If no `include_patterns` are provided, all files are initially included and only `exclude_patterns` are applied. Signature ```python def process_files(file_paths: List[str], include_patterns: List[str], exclude_patterns: List[str]) -> List[str]: pass ``` Input - `file_paths`: A list of strings, each representing a file path. (`1 <= len(file_paths) <= 1000`) - `include_patterns`: A list of Unix-style glob patterns to include. (`0 <= len(include_patterns) <= 100`) - `exclude_patterns`: A list of Unix-style glob patterns to exclude. (`0 <= len(exclude_patterns) <= 100`) Output - A list of strings representing the filtered file paths, in their original order. Constraints - The patterns in `include_patterns` and `exclude_patterns` follow the Unix-style glob patterns where: - `*` matches any sequence of regular filename characters. - `?` matches any single regular filename character. - `[range]` matches any of the characters in range (e.g., `a-z`, `a-zA-Z`, `a-f0-9_.`). Examples ```python file_paths = [ \\"src/main.py\\", \\"src/utils.py\\", \\"docs/readme.md\\", \\"tests/test_main.py\\", \\"tests/test_utils.py\\" ] include_patterns = [\\"src/*.py\\", \\"tests/*.py\\"] exclude_patterns = [\\"tests/test_main.py\\"] print(process_files(file_paths, include_patterns, exclude_patterns)) # Output: [\'src/main.py\', \'src/utils.py\', \'tests/test_utils.py\'] ``` Notes - Use the `fnmatch` module to match file paths against patterns. - Ensure to test various edge cases, such as when `include_patterns` or `exclude_patterns` are empty. Guidelines 1. Carefully handle pattern matching using `fnmatch` or a similar library. 2. Maintain the order of file paths as provided in the input list. 3. Efficiently apply the patterns to ensure performance is suitable within the provided constraints.","solution":"from fnmatch import fnmatch from typing import List def process_files(file_paths: List[str], include_patterns: List[str], exclude_patterns: List[str]) -> List[str]: if include_patterns: included_files = [file for file in file_paths if any(fnmatch(file, pat) for pat in include_patterns)] else: included_files = file_paths final_files = [file for file in included_files if not any(fnmatch(file, pat) for pat in exclude_patterns)] return final_files"},{"question":"Objective: Design and implement a custom event loop policy in Python that: 1. Overrides the `DefaultEventLoopPolicy`. 2. Customizes the management of child process watchers. Problem Statement: You are to create a custom event loop policy named `MyCustomEventLoopPolicy` which inherits from `DefaultEventLoopPolicy`. Your implementation should: 1. Override the `get_event_loop()`, `set_event_loop(loop)`, and `new_event_loop()` methods to include custom logging messages indicating the actions being performed. 2. Override the `set_child_watcher(watcher)` method to enforce the use of a `SafeChildWatcher` instance. 3. Ensure that any attempt to set a child watcher of a type other than `SafeChildWatcher` raises a `TypeError` with a descriptive message. Input and Output: - Input: No direct input from the user. The program should demonstrate the functionality using hardcoded examples. - Output: Log messages indicating the actions of `get_event_loop()`, `set_event_loop(loop)`, `new_event_loop()`, and results of setting the child watcher. Constraints: - You should use logging for output instead of print statements for tracking the actions. - The overridden methods in your policy should call the base class methods appropriately using `super()`. - Only `SafeChildWatcher` should be allowed for child watcher management. Example: After implementing your class, here is a sample of expected behavior: ```python import asyncio import logging class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() logging.info(f\\"Getting event loop: {loop}\\") return loop def set_event_loop(self, loop): super().set_event_loop(loop) logging.info(f\\"Setting event loop: {loop}\\") def new_event_loop(self): loop = super().new_event_loop() logging.info(f\\"Creating new event loop: {loop}\\") return loop def set_child_watcher(self, watcher): if not isinstance(watcher, asyncio.SafeChildWatcher): raise TypeError(\\"Only SafeChildWatcher is allowed\\") super().set_child_watcher(watcher) logging.info(f\\"Setting child watcher: {watcher}\\") # Example Usage: asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) loop = asyncio.get_event_loop() new_loop = asyncio.new_event_loop() safe_watcher = asyncio.SafeChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(safe_watcher) try: invalid_watcher = asyncio.ThreadedChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(invalid_watcher) except TypeError as e: logging.error(e) ``` The log should capture: ``` INFO:root:Getting event loop: <_UnixSelectorEventLoop running=False closed=False debug=False> INFO:root:Creating new event loop: <_UnixSelectorEventLoop running=False closed=False debug=False> INFO:root:Setting child watcher: <asyncio.SafeChildWatcher object at 0x...> ERROR:root:Only SafeChildWatcher is allowed ``` Assessment Criteria: - Correct implementation of the `MyCustomEventLoopPolicy` class. - Accurate overriding of the specified methods. - Proper use of the `SafeChildWatcher`. - Appropriate handling and raising of exceptions. - Clarity and conciseness of logging messages. Good luck!","solution":"import asyncio import logging logging.basicConfig(level=logging.INFO) class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() logging.info(f\\"Getting event loop: {loop}\\") return loop def set_event_loop(self, loop): super().set_event_loop(loop) logging.info(f\\"Setting event loop: {loop}\\") def new_event_loop(self): loop = super().new_event_loop() logging.info(f\\"Creating new event loop: {loop}\\") return loop def set_child_watcher(self, watcher): if not isinstance(watcher, asyncio.SafeChildWatcher): raise TypeError(\\"Only SafeChildWatcher is allowed\\") super().set_child_watcher(watcher) logging.info(f\\"Setting child watcher: {watcher}\\") # Example Usage: asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) loop = asyncio.get_event_loop() new_loop = asyncio.new_event_loop() safe_watcher = asyncio.SafeChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(safe_watcher) try: invalid_watcher = asyncio.ThreadedChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(invalid_watcher) except TypeError as e: logging.error(e)"},{"question":"# Question: Prime Number Calculation with Threading You are required to write a Python function that computes the number of prime numbers within a given range concurrently using threads. The goal of this task is to efficiently use threading to improve the performance of your computation. # Function Signature ```python def count_primes_in_range(n: int, m: int, thread_count: int) -> int: ``` # Parameters - `n` (int): Starting integer of the range (inclusive). - `m` (int): Ending integer of the range (exclusive). - `thread_count` (int): Number of threads to use for the computation. # Returns - `int`: The count of prime numbers in the specified range `[n, m)`. # Description 1. Your function should divide the range `[n, m)` into equal parts based on the `thread_count`. 2. Each thread should compute the number of primes in its own sub-range. 3. Use appropriate synchronization mechanisms to ensure results are correctly aggregated. # Example ```python result = count_primes_in_range(1, 100, 4) print(result) # Output: The number of prime numbers between 1 and 100. ``` # Constraints - All inputs are positive integers. - `1 <= n < m <= 10^6` - `1 <= thread_count <= 100` # Notes - Do not use Python\'s `multiprocessing` module for this task. - You can use any functions related to threading from the `threading` module. - Make sure your solution is efficient and correctly synchronized. # Implementation Here\'s a possible helper function outline you might use: ```python from threading import Thread, Lock def is_prime(n): if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def worker(start, end, result, lock): count = 0 for num in range(start, end): if is_prime(num): count += 1 with lock: result.append(count) def count_primes_in_range(n, m, thread_count): # Your implementation here pass ``` This question requires students to demonstrate their understanding of: - Dividing tasks for concurrent execution - Implementing and managing threads - Synchronizing results from multiple threads","solution":"from threading import Thread, Lock def is_prime(n): if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def worker(start, end, result, lock): count = 0 for num in range(start, end): if is_prime(num): count += 1 with lock: result.append(count) def count_primes_in_range(n, m, thread_count): threads = [] result = [] lock = Lock() interval = (m - n) // thread_count for i in range(thread_count): start = n + i * interval end = n + (i + 1) * interval if i != thread_count - 1 else m thread = Thread(target=worker, args=(start, end, result, lock)) threads.append(thread) thread.start() for thread in threads: thread.join() return sum(result)"},{"question":"**Objective**: Create a set of unit tests for a given function using the Python `unittest` framework, following best practices outlined in the `test` package documentation. **Instructions**: 1. **Function to be Tested**: Below is the implementation of a function `is_prime` which checks if a number is a prime number. ```python def is_prime(n): Check if a number is prime. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while (i * i) <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True ``` 2. **Task**: Write a set of unit tests for the `is_prime` function using the `unittest` module. Ensure that your tests follow these guidelines: - The test module name should start with `test_`. - Test methods should start with `test_` and without a documentation string. - Each test method should handle one specific case. - Include tests for edge cases, typical values, and invalid values. - Ensure proper clean-up and setup if necessary (even though in this case, it might not be needed). - Follow the best practices of testing as described in the documentation provided. 3. **Test Requirements**: - Test for small prime numbers, composite numbers, and edge cases like zero, one, and negative numbers. - Test cases should include valid and invalid inputs. - Expected input is always an integer (you may define this constraint in your test if necessary). 4. **Submission**: The solution should be a Python script containing the `unittest` based testing suite. **Example Submission**: ```python import unittest class TestIsPrime(unittest.TestCase): def test_prime_numbers(self): self.assertTrue(is_prime(2)) self.assertTrue(is_prime(3)) self.assertTrue(is_prime(5)) self.assertTrue(is_prime(7)) self.assertTrue(is_prime(11)) def test_composite_numbers(self): self.assertFalse(is_prime(4)) self.assertFalse(is_prime(6)) self.assertFalse(is_prime(8)) self.assertFalse(is_prime(9)) self.assertFalse(is_prime(10)) def test_edge_cases(self): self.assertFalse(is_prime(-1)) self.assertFalse(is_prime(0)) self.assertFalse(is_prime(1)) def test_large_prime(self): self.assertTrue(is_prime(7919)) def test_large_composite(self): self.assertFalse(is_prime(7918)) if __name__ == \'__main__\': unittest.main() ```","solution":"def is_prime(n): Check if a number is prime. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while (i * i) <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True"},{"question":"# Problem Description You are working with a special Python library that facilitates numerical operations directly at the C API level. For this assessment, you\'ll create a Python class that allows a user to perform a variety of arithmetic and bitwise operations on numeric data. To accomplish this, you will use the functions documented in the provided C API. # Task Implement a `NumericOperations` class that supports the following operations: 1. **Initialization**: Initialize the class with a numeric value. 2. **Addition (`add`)**: Add another numeric value to the current value. 3. **Subtraction (`subtract`)**: Subtract another numeric value from the current value. 4. **Multiplication (`multiply`)**: Multiply the current value by another numeric value. 5. **True Division (`truedivide`)**: Divide the current value by another numeric value. 6. **Floor Division (`floordivide`)**: Apply floor division to the current value by another numeric value. 7. **Modulo (`remainder`)**: Get the remainder of division of the current value by another numeric value. 8. **Power (`power`)**: Raise the current value to the power of another numeric value. 9. **Negation (`negate`)**: Negate the current value. 10. **Bitwise And (`bitwise_and`)**: Apply bitwise AND operation with another numeric value. 11. **Conversion to Integer (`to_integer`)**: Convert the current value to an integer. # Class Definition ```python class NumericOperations: def __init__(self, value): pass def add(self, other): pass def subtract(self, other): pass def multiply(self, other): pass def truedivide(self, other): pass def floordivide(self, other): pass def remainder(self, other): pass def power(self, other): pass def negate(self): pass def bitwise_and(self, other): pass def to_integer(self): pass def get_value(self): pass ``` # Detailed Requirements 1. **Initialization**: The constructor should initialize the instance with a numeric value. If the value is not a number, raise a ValueError. 2. **Addition (`add`)**: This method should add another numeric value to the instance\'s value. 3. **Subtraction (`subtract`)**: This method should subtract another numeric value from the instance\'s value. 4. **Multiplication (`multiply`)**: This method should multiply the instance\'s value by another numeric value. 5. **True Division (`truedivide`)**: This method should divide the instance\'s value by another numeric value. Handle the case where division by zero occurs by raising a `ZeroDivisionError`. 6. **Floor Division (`floordivide`)**: This method should floor divide the instance\'s value by another numeric value. Handle the case where division by zero occurs by raising a `ZeroDivisionError`. 7. **Modulo (`remainder`)**: This method should calculate the remainder when the instance\'s value is divided by another numeric value. Handle the case where division by zero occurs by raising a `ZeroDivisionError`. 8. **Power (`power`)**: This method should raise the instance\'s value to the power of another numeric value. 9. **Negation (`negate`)**: This method should negate the instance\'s value. 10. **Bitwise And (`bitwise_and`)**: This method should apply a bitwise AND operation with another numeric value. 11. **Conversion to Integer (`to_integer`)**: This method should convert the instance\'s value to an integer. 12. **Get Value (`get_value`)**: This method should return the current value of the instance. # Constraints - You must use the corresponding C API functions for each operation. - The operations should be performed on valid numeric types only (integers and floats). - Handle errors appropriately, with meaningful error messages. # Performance Requirements - Your implementation should be efficient and handle large numerical values gracefully. # Example Usage ```python num_op = NumericOperations(10) print(num_op.get_value()) # Output: 10 num_op.add(5) print(num_op.get_value()) # Output: 15 num_op.subtract(3) print(num_op.get_value()) # Output: 12 num_op.multiply(2) print(num_op.get_value()) # Output: 24 num_op.truedivide(4) print(num_op.get_value()) # Output: 6.0 num_op.negate() print(num_op.get_value()) # Output: -6.0 num_op.bitwise_and(3) print(num_op.get_value()) # Output: (depends on current value) print(num_op.to_integer()) # Output: (integer part of current value) ``` # Notes - You may assume that all input values for the operations are valid numeric types unless stated otherwise. - Be sure to maintain clear and concise code, and include comments where necessary.","solution":"class NumericOperations: def __init__(self, value): if not isinstance(value, (int, float)): raise ValueError(\\"Initialization value must be a number.\\") self.value = value def add(self, other): if not isinstance(other, (int, float)): raise ValueError(\\"Operand must be a number.\\") self.value += other def subtract(self, other): if not isinstance(other, (int, float)): raise ValueError(\\"Operand must be a number.\\") self.value -= other def multiply(self, other): if not isinstance(other, (int, float)): raise ValueError(\\"Operand must be a number.\\") self.value *= other def truedivide(self, other): if not isinstance(other, (int, float)): raise ValueError(\\"Operand must be a number.\\") if other == 0: raise ZeroDivisionError(\\"Division by zero.\\") self.value /= other def floordivide(self, other): if not isinstance(other, (int, float)): raise ValueError(\\"Operand must be a number.\\") if other == 0: raise ZeroDivisionError(\\"Division by zero.\\") self.value //= other def remainder(self, other): if not isinstance(other, (int, float)): raise ValueError(\\"Operand must be a number.\\") if other == 0: raise ZeroDivisionError(\\"Division by zero.\\") self.value %= other def power(self, other): if not isinstance(other, (int, float)): raise ValueError(\\"Operand must be a number.\\") self.value **= other def negate(self): self.value = -self.value def bitwise_and(self, other): if not isinstance(other, int): raise ValueError(\\"Operand must be an integer.\\") self.value = int(self.value) & other def to_integer(self): return int(self.value) def get_value(self): return self.value"},{"question":"**Objective:** To assess your understanding of the `sklearn.datasets` module for loading and handling different types of datasets. **Problem Statement:** You are provided with the task of evaluating a machine learning model on both toy datasets and real-world datasets. For this, you must perform the following steps using the `sklearn.datasets` module: 1. Load the Iris toy dataset and the 20 Newsgroups real-world dataset. 2. Print the first five samples (features and target) from each dataset. 3. Generate a synthetic dataset using the `make_classification` function. 4. Print the first five samples (features and target) of the synthetic dataset. 5. Provide a brief description of each dataset, referencing the `DESCR` attribute when available. **Requirements:** - You must use `load_iris` to load the Iris dataset. - You must use `fetch_20newsgroups` to load the 20 Newsgroups dataset. - You must use `make_classification` to generate the synthetic dataset. - Ensure that each operation (loading, generating, printing) is performed within a dedicated function. **Input and Output Formats:** - **Input:** No specific input is required from the user. - **Output:** - Print the first five samples of the Iris dataset (features and target). - Print the first five samples of the 20 Newsgroups dataset (features and target). - Print the first five samples of the synthetic dataset (features and target). - Print the descriptions of the Iris and 20 Newsgroups datasets. **Constraints:** - Use only the functions provided in the `sklearn.datasets` module. - The synthetic dataset should have 100 samples, 20 features, and 2 classes. **Performance Requirements:** - The code should run efficiently and not exceed reasonable runtime limits for data loading and printing. **Function Signatures:** ```python def load_and_display_iris_dataset(): # Implement this function to load and display the Iris dataset. def load_and_display_20newsgroups_dataset(): # Implement this function to load and display the 20 Newsgroups dataset. def generate_and_display_synthetic_dataset(): # Implement this function to generate and display the synthetic dataset. def print_dataset_descriptions(): # Implement this function to print the descriptions of the Iris and 20 Newsgroups datasets. ``` Please implement the above functions and demonstrate their usage. Ensure your code is organized and easy to read. **Example Output:** ``` First five samples of Iris dataset: Features: [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2], ...] Target: [0, 0, ...] First five samples of 20 Newsgroups dataset: Features: [\'From: ...\', \'Subject: ...\', ...] Target: [7, 4, ...] First five samples of Synthetic dataset: Features: [[0.5, -1.2, ...], [-0.6, 1.0, ...], ...] Target: [1, 0, ...] Description of Iris dataset: The Iris dataset is a classical dataset ... ... Description of 20 Newsgroups dataset: The 20 Newsgroups dataset is a collection of ... ... ```","solution":"from sklearn.datasets import load_iris, fetch_20newsgroups, make_classification def load_and_display_iris_dataset(): iris = load_iris() features = iris.data[:5].tolist() target = iris.target[:5].tolist() print(f\\"First five samples of Iris dataset:nFeatures: {features}nTarget: {target}\\") def load_and_display_20newsgroups_dataset(): newsgroups = fetch_20newsgroups(subset=\'train\') features = newsgroups.data[:5] target = newsgroups.target[:5].tolist() print(f\\"First five samples of 20 Newsgroups dataset:nFeatures: {features}nTarget: {target}\\") def generate_and_display_synthetic_dataset(): X, y = make_classification(n_samples=100, n_features=20, n_classes=2) features = X[:5].tolist() target = y[:5].tolist() print(f\\"First five samples of Synthetic dataset:nFeatures: {features}nTarget: {target}\\") def print_dataset_descriptions(): iris = load_iris() newsgroups = fetch_20newsgroups(subset=\'train\') print(f\\"Description of Iris dataset:n{iris.DESCR}\\") print(f\\"Description of 20 Newsgroups dataset:n{newsgroups.DESCR}\\")"},{"question":"As a data analyst, you are required to demonstrate your ability to use the seaborn.objects module to create comprehensive visualizations. Below is a dataset containing the number of confirmed COVID-19 cases across multiple countries over a period of time. Your task is to use seaborn.objects to generate a visualization that shows the trend of COVID-19 cases for a specific country and includes additional contextual information. Dataset The dataset is structured as follows: ```python { \'date\': [\'2020-01-22\', \'2020-01-23\', ..., \'2021-12-31\'], # list of dates \'country\': [\'CountryA\', \'CountryA\', ..., \'CountryZ\'], # list of countries \'confirmed_cases\': [100, 120, ..., 1050000] # list of confirmed cases corresponding to dates and countries } ``` Task 1. Load the dataset using the given structure. 2. Filter the data for a specific country (e.g., \'CountryA\'). 3. Create a line plot to display the trend of confirmed COVID-19 cases over time for the selected country. 4. Customize the plot by adding: - Error bands to indicate the uncertainty in the data. - Markers to highlight the sampled data points. - Use color to distinguish between different countries if more than one country is plotted. 5. Ensure the x-axis represents the date and the y-axis represents the number of confirmed cases. 6. Provide appropriate labels, a title, and a legend for the plot. Constraints - You must use seaborn.objects for creating the plot. - The plot should be clear and well-labeled. - Handle any missing data appropriately. Input A pandas DataFrame named `covid_data` with columns `date`, `country`, and `confirmed_cases`. Expected Output A seaborn.objects plot showing the trend of confirmed COVID-19 cases over time with error bands and markers for a specific country. ```python import seaborn.objects as so import pandas as pd import numpy as np # Sample dataset data = { \'date\': pd.date_range(start=\'2020-01-22\', end=\'2021-12-31\', freq=\'D\').tolist() * 3, \'country\': [\'CountryA\'] * 710 + [\'CountryB\'] * 710 + [\'CountryC\'] * 710, \'confirmed_cases\': np.random.randint(100, 1000000, 2130) } covid_data = pd.DataFrame(data) # Task Implementation def plot_covid_trend(data, country): # Filter data for the specific country country_data = data[data[\'country\'] == country] if country_data.empty: raise ValueError(f\\"No data available for {country}\\") # Create the plot p = so.Plot(country_data, x=\'date\', y=\'confirmed_cases\') # Add line plot with error bands and markers p.add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\'country\') .add(so.Line(marker=\'o\', edgecolor=\'w\'), so.Agg(), linestyle=None) # Customize the plot p.title(f\'COVID-19 Confirmed Cases Trend in {country}\') p.xlabel(\'Date\') p.ylabel(\'Confirmed Cases\') return p # Example call plot_covid_trend(covid_data, \'CountryA\') ```","solution":"import seaborn.objects as so import pandas as pd import numpy as np # Sample dataset data = { \'date\': pd.date_range(start=\'2020-01-22\', end=\'2021-12-31\', freq=\'D\').tolist() * 3, \'country\': [\'CountryA\'] * 710 + [\'CountryB\'] * 710 + [\'CountryC\'] * 710, \'confirmed_cases\': np.random.randint(100, 1000000, 2130) } covid_data = pd.DataFrame(data) # Task Implementation def plot_covid_trend(data, country): Generate a seaborn.objects plot showing the trend of confirmed COVID-19 cases over time with error bands and markers for a specific country. # Filter data for the specific country country_data = data[data[\'country\'] == country] if country_data.empty: raise ValueError(f\\"No data available for {country}\\") # Create the plot p = so.Plot(country_data, x=\'date\', y=\'confirmed_cases\') # Add line plot with error bands and markers p = p.add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\'country\') .add(so.Line(marker=\'o\', edgecolor=\'w\'), so.Agg(), linestyle=None) # Customize the plot p = p.label(title=f\'COVID-19 Confirmed Cases Trend in {country}\', xlabel=\'Date\', ylabel=\'Confirmed Cases\') return p # Example call (uncomment to visualize) # plot_covid_trend(covid_data, \'CountryA\')"},{"question":"# Advanced Python Socket Programming Task **Objective**: Implement a multi-threaded TCP server and client using the Python `socket` module. The server should be able to handle multiple clients concurrently, echoing back any messages received, and the client should be able to send and receive messages to/from the server. # Requirements Server Implementation 1. **Create a TCP Server**: - The server should use IPv4 (AF_INET) and TCP protocol (SOCK_STREAM). - The server should listen on a configurable port (for example, 8080). - The server should accept multiple client connections concurrently using threading. 2. **Handle Client Connections**: - For each client connection, create a new thread to handle communication with that client. - In the client handler thread, receive messages from the client and echo them back to the client. - Handle exceptions properly to ensure the server remains running even if a client disconnects or sends an invalid message. 3. **Server Shutdown**: - Implement a mechanism to gracefully shutdown the server (e.g., by handling a specific command or signal). Client Implementation 1. **Create a TCP Client**: - The client should connect to the server using the server\'s IP address and port. - The client should be able to send messages to the server. - The client should receive and display messages echoed back by the server. 2. **User Interaction**: - The client should provide a simple command-line interface for the user to enter messages. - Include a special command (e.g., `/quit`) to disconnect from the server and exit the client application. # Input and Output - **Server**: No direct user input. Configurable port via command-line arguments or hard-coded. - **Client**: User enters messages via command-line interface. Messages are echoed back by the server and displayed on the client. # Performance Requirements - The server should handle at least 10 concurrent clients. - Ensure minimal latency in echoing back messages (low overhead per message). # Example Usage **Server**: ```sh python tcp_server.py 8080 ``` **Client**: ```sh python tcp_client.py 127.0.0.1 8080 ``` ```plaintext > Hello, server! Received: Hello, server! > /quit Disconnected from server. ``` # Implementation Details 1. Server should listen for connections in the main thread and spawn a new thread for each accepted client connection. 2. Each client handler thread should loop, receiving and echoing messages until the client disconnects. 3. The client should read user input, send it to the server, and print the server\'s response. 4. Ensure proper cleanup of resources (sockets) on both server and client sides. # Constraints - You must use the Python `socket` module. - Use Python\'s `threading` module for handling concurrent clients. - Handle errors and edge cases gracefully. **Bonus:** - Implement additional features such as logging connections, handling special commands, or supporting secure connections (TLS) using the `ssl` module. Submit your implementation of the `tcp_server.py` and `tcp_client.py` scripts.","solution":"import socket import threading def handle_client(client_socket): Handle client connection, echoing received messages back to the client. try: while True: message = client_socket.recv(1024) if not message: break client_socket.sendall(message) except Exception as e: print(f\\"Client handling error: {e}\\") finally: client_socket.close() def start_server(port): Start the TCP server to listen for incoming connections. server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((\\"\\", port)) server.listen(5) print(f\\"Server listening on port {port}\\") try: while True: client_socket, addr = server.accept() print(f\\"Accepted connection from {addr}\\") client_handler = threading.Thread(target=handle_client, args=(client_socket,)) client_handler.start() except KeyboardInterrupt: print(\\"Server shutting down\\") finally: server.close()"},{"question":"You are tasked with developing a utility function to standardize URL strings for an application that processes a large number of URLs from various sources. These URLs might be incomplete or contain various inconsistencies that need to be handled. Your function should ensure that URLs are properly formed, with all necessary components provided or corrected as per standard practices. # Task Implement a function called `standardize_url` that takes a URL string and a base URL as arguments. The function should: 1. Parse the given URL. 2. Ensure that it contains a valid scheme (`http` by default if not present). 3. Use the base URL to complete any missing components (network location and path). 4. Encode the path and query components appropriately to handle special characters. 5. If the URL contains a fragment, it should be preserved as is. 6. Return the standardized and correctly formatted URL. # Expected Function Signature ```python def standardize_url(url: str, base_url: str) -> str: pass ``` # Input - `url` (str): A potentially incomplete or inconsistent URL to be standardized. - `base_url` (str): A base URL to resolve relative paths and fill in missing components. # Output - (str): A well-formed URL string. # Constraints - The function should handle URLs with missing schemes by assuming `http` if no scheme is provided. - Components provided in `url` should take precedence over those in `base_url`. - Use URL percent-encoding to handle special characters in the path and query parts of the URL. - Do not modify the fragment part of the URL if present. # Example ```python from urllib.parse import urlparse def standardize_url(url: str, base_url: str) -> str: # Step 1: Parse the URL and base URL parsed_url = urlparse(url) parsed_base_url = urlparse(base_url) # Step 2: Ensure scheme is present, default to \'http\' scheme = parsed_url.scheme or \'http\' # Step 3: Use base URL to complete missing components netloc = parsed_url.netloc or parsed_base_url.netloc path = parsed_url.path or parsed_base_url.path params = parsed_url.params query = parsed_url.query fragment = parsed_url.fragment # Encode path and query components from urllib.parse import quote, urlencode path = quote(path) query = urlencode({k: v[0] for k, v in parse_qs(query).items()}) # Construct the standardized URL from urllib.parse import urlunparse standardized_url = urlunparse((scheme, netloc, path, params, query, fragment)) return standardized_url # Example usage: input_url = \\"/docs/index.html?name=value#section\\" base_url = \\"http://example.com/base/path\\" print(standardize_url(input_url, base_url)) ``` Output: ``` \'http://example.com/docs/index.html?name=value#section\' ``` In this example, the function uses `http` as the default scheme, completes the network location from the base URL, encodes the path and query components, and preserves the fragment identifier.","solution":"from urllib.parse import urlparse, urlunparse, urljoin, quote, urlencode, parse_qs def standardize_url(url: str, base_url: str) -> str: # Parse the given URL and the base URL parsed_url = urlparse(url) parsed_base_url = urlparse(base_url) # Ensure the scheme is present, default to \'http\' if not scheme = parsed_url.scheme or parsed_base_url.scheme or \'http\' # Use base URL to complete missing components netloc = parsed_url.netloc or parsed_base_url.netloc path = parsed_url.path or parsed_base_url.path # Merge query parameters ensuring both URLs are considered initial_query = parse_qs(parsed_url.query, keep_blank_values=True) base_query = parse_qs(parsed_base_url.query, keep_blank_values=True) final_query = {**base_query, **initial_query} encoded_query = urlencode(final_query, doseq=True) # The fragment is kept as it is fragment = parsed_url.fragment # Encode path correctly encoded_path = quote(path) # Construct and return the standardized URL result_url = urlunparse((scheme, netloc, encoded_path, \'\', encoded_query, fragment)) return result_url"},{"question":"**Objective:** You are required to implement a set of functions in Python using the `gzip` module to perform compression and decompression on directories containing multiple files. You will need to ensure that all edge cases and possible errors are handled gracefully. **Question:** Write a Python script that meets the following specifications: 1. **compress_directory(input_dir: str, output_filepath: str, compresslevel: int = 9):** - Compresses all files in the directory specified by `input_dir`. - The compressed output should be stored as a single GZIP file at `output_filepath`. - The `compresslevel` argument can be set to control the level of compression; default is `9` (maximum compression). 2. **decompress_to_directory(input_filepath: str, output_dir: str):** - Decompresses the GZIP file specified by `input_filepath`. - Extracts all files and saves them to the directory specified by `output_dir`. **Input and Output:** - The `input_dir` and `output_dir` parameters will always be valid directory paths. - The `input_filepath` and `output_filepath` parameters will always be valid file paths. - Ensure that for all functions, invalid files (e.g., not being able to decompress a corrupted GZIP file) are handled gracefully by printing an appropriate error message. **Constraints:** - You must use the `gzip` module for compression and decompression. - Your solutions should be efficient and must reflect good coding practices including error handling. - File permissions and any other file-related constraints should be managed by your script. Your implementation should demonstrate the following aspects: - Reading and writing of files. - Directory handling. - Compression and decompression using the `gzip` module. - Exception handling for scenarios like invalid files or directories. **Example Usage:** ```python compress_directory(\'/path/to/input_dir\', \'/path/to/output_file.gz\', compresslevel=6) decompress_to_directory(\'/path/to/output_file.gz\', \'/path/to/output_dir\') ``` **Note:** Do not use any external libraries for compression other than what is provided by Python standard library (`gzip`).","solution":"import os import gzip import shutil def compress_directory(input_dir: str, output_filepath: str, compresslevel: int = 9): Compresses all files in the directory specified by input_dir into a single GZIP file. Args: - input_dir: Directory containing files to compress. - output_filepath: Output path for the compressed GZIP file. - compresslevel: Compression level (default 9). Returns: None with gzip.open(output_filepath, \'wb\', compresslevel=compresslevel) as f_out: for dirpath, _, filenames in os.walk(input_dir): for filename in filenames: filepath = os.path.join(dirpath, filename) with open(filepath, \'rb\') as f_in: shutil.copyfileobj(f_in, f_out) def decompress_to_directory(input_filepath: str, output_dir: str): Decompresses the specified GZIP file into a directory. Args: - input_filepath: Path to the compressed GZIP file. - output_dir: Directory where files should be decompressed. Returns: None os.makedirs(output_dir, exist_ok=True) try: with gzip.open(input_filepath, \'rb\') as f_in: output_path = os.path.join(output_dir, os.path.basename(input_filepath).replace(\'.gz\', \'\')) with open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective**: Test the ability to generate and use color palettes in seaborn. **Problem Statement**: You are tasked with creating a function that visualizes the color palettes available in seaborn. This function should: 1. Generate color palettes for a given list of palette names. 2. Visualize each color palette as a horizontal bar of colors. 3. Save the visualizations to disk with names corresponding to the palette names. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_palettes(palette_names: list, output_dir: str): Visualize seaborn color palettes and save the plots to disk. Parameters: - palette_names (list): A list of strings representing the palette names to visualize. - output_dir (str): Path to the directory where the visualizations will be saved. Returns: - None pass ``` # Input - `palette_names`: A list of strings, where each string is the name of a seaborn color palette. Examples include \\"pastel\\", \\"husl\\", \\"Spectral\\", \\"flare\\", etc. - `output_dir`: A string representing the directory path where the visualizations will be saved. # Constraints - The palette names provided in `palette_names` must be valid seaborn palette names. If an invalid name is encountered, raise a `ValueError` with a message indicating the invalid name. - Ensure that the directory provided in `output_dir` exists. If it does not, create the directory. # Output - The function does not return any value. Instead, it saves one plot per palette to the specified directory. Each plot should be saved as `<palette_name>.png`. # Example ```python # Example usage of the function palette_names = [\\"pastel\\", \\"husl\\", \\"Spectral\\"] output_dir = \\"./palette_visualizations\\" visualize_palettes(palette_names, output_dir) ``` After running the example, the directory `./palette_visualizations` should contain the following files: - pastel.png - husl.png - Spectral.png Each file should contain a horizontal bar consisting of the colors in the corresponding palette. **Note**: - Use `sns.color_palette(palette_name)` to obtain the colors. - Use `plt.imshow` or a similar function to create the visual representation of each palette.","solution":"import seaborn as sns import matplotlib.pyplot as plt import os def visualize_palettes(palette_names: list, output_dir: str): Visualize seaborn color palettes and save the plots to disk. Parameters: - palette_names (list): A list of strings representing the palette names to visualize. - output_dir (str): Path to the directory where the visualizations will be saved. Returns: - None # Ensure the output directory exists if not os.path.exists(output_dir): os.makedirs(output_dir) for palette_name in palette_names: try: # Generate the palette colors palette = sns.color_palette(palette_name) # Create a figure plt.figure(figsize=(8, 2)) plt.imshow([palette], aspect=\'auto\') plt.axis(\'off\') # Save the figure plt.savefig(os.path.join(output_dir, f\\"{palette_name}.png\\")) plt.close() except ValueError: raise ValueError(f\\"The palette name \'{palette_name}\' is invalid.\\")"},{"question":"Objective: Demonstrate your understanding of the fundamental and advanced concepts of seaborn through the seaborn objects interface. Problem Statement: Given the `iris` dataset from the seaborn library, you are required to create a multi-faceted plot that visualizes the distribution of petal lengths (`petal_length`) for different species (`species`) grouped by petal width (`petal_width`) bands. You need to: 1. Divide the `petal_width` into 3 equal-width bins. 2. Plot the distribution of `petal_length` for each species within each bin of `petal_width` using a `Dash` mark. 3. Customize the width of the `Dash` marks according to the `sepal_length`. 4. Adjust the color of the `Dash` marks based on the `species`. 5. Make use of position adjustment to avoid overlap between the dash marks. Constraints: - You must use the seaborn objects interface. - The expected bins for `petal_width` are 3 equally spaced bins across its range. Input: No input required as you will use the `iris` dataset from the seaborn library. Output: A plot displaying the required visualization. Example Visual Representation: Your plot should result in a comprehensive figure showing the segmentation of petal lengths among different species, grouped by petal width bins, with clear width differences in dashes based on sepal length, and differentiated by species color. Implementation: ```python import seaborn.objects as so from seaborn import load_dataset # Load the iris dataset iris = load_dataset(\\"iris\\") # Ensure that petal_width is divided into 3 equal-width bins iris[\\"petal_width_bin\\"] = pd.cut(iris[\\"petal_width\\"], bins=3, labels=[\\"Low\\", \\"Medium\\", \\"High\\"]) # Create the plot p = ( so.Plot(iris, x=\\"species\\", y=\\"petal_length\\", color=\\"species\\", linewidth=\\"sepal_length\\") .facet(\\"petal_width_bin\\") .add(so.Dash(), so.Dodge()) ) # Display the plot p.show() ``` # Notes: - Ensure you have seaborn version 0.11 or later installed. - Customize the plot to improve readability and visual appeal if necessary.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def create_multifaceted_plot(): # Load the iris dataset iris = load_dataset(\\"iris\\") # Ensure that petal_width is divided into 3 equal-width bins iris[\\"petal_width_bin\\"] = pd.cut(iris[\\"petal_width\\"], bins=3, labels=[\\"Low\\", \\"Medium\\", \\"High\\"]) # Create the plot p = ( so.Plot(iris, x=\\"species\\", y=\\"petal_length\\", color=\\"species\\", linewidth=\\"sepal_length\\") .facet(\\"petal_width_bin\\") .add(so.Dash(), so.Dodge()) ) # Display the plot p.show()"},{"question":"**Question: Custom Exception Handling with Recursive Calls** You are tasked with implementing a custom exception handling system in Python, using advanced exception handling techniques. Your goal is to create specific exception classes, manage error indicators properly, and handle recursion effectively. # Requirements 1. **Custom Exception Classes**: - Create a custom exception class `CustomError` which inherits from a base exception class of your choice. - Create another custom exception class `CustomRecursionError` which will represent recursion-related errors. 2. **Exception Raising**: - Write functions to raise these custom exceptions with specific error messages based on input conditions. Use `PyErr_SetString` or `PyErr_SetObject` for this. - Ensure recursive calls raise `CustomRecursionError` using `Py_EnterRecursiveCall` and `Py_LeaveRecursiveCall`. 3. **Error Handling**: - Implement a function `handle_errors` that catches and prints information about the exceptions using `PyErr_Print`. 4. **Recursion Management**: - Implement a recursive function `recursive_function` that raises `CustomRecursionError` if it exceeds a certain recursion depth. 5. **Traceback and Context**: - In your exception handling logic, ensure the traceback and context of exceptions are managed correctly using functions like `PyException_SetTraceback`. # Input and Output - Input is flexible as per function requirements (you may define inputs in a meaningful way to demonstrate exception raising). - Output should demonstrate the appropriate handling and printing of raised exceptions, including tracebacks. # Constraints - You may assume a reasonable recursion limit (depth of 10, for example). - Ensure your code handles exceptions gracefully and does not crash on unhandled exceptions. - Flexibly use traceback and context management to provide meaningful error reports. # Performance Requirements - The code should efficiently manage recursion limits and catch all exceptions promptly without significant overhead. # Sample Code Structure Below is a sample structure to guide your implementation: ```python class CustomError(Exception): pass class CustomRecursionError(CustomError): pass def raise_custom_error(condition): # Implement exception raising based on condition pass def raise_custom_recursion_error(): # Implement recursive error raising pass def handle_errors(): # Implement error handling and error printing pass def recursive_function(n): # Implement recursion tracking and raise CustomRecursionError if needed pass # Test your implementation try: raise_custom_error(\\"Some condition\\") except CustomError: handle_errors() try: recursive_function(0) except CustomRecursionError: handle_errors() ``` Use this outline to develop your solution demonstrating advanced exception handling, custom exceptions, and managing error indicators in Python.","solution":"import sys import traceback class CustomError(Exception): pass class CustomRecursionError(CustomError): pass def raise_custom_error(condition): if condition == \\"error\\": raise CustomError(\\"A custom error occurred\\") def raise_custom_recursion_error(): raise CustomRecursionError(\\"Recursion depth exceeded\\") def handle_errors(): exc_type, exc_value, exc_tb = sys.exc_info() if exc_type: print(f\\"Exception type: {exc_type.__name__}\\") print(f\\"Exception message: {exc_value}\\") traceback.print_tb(exc_tb) def recursive_function(n, limit=10): if n > limit: raise_custom_recursion_error() print(f\\"Recursion level: {n}\\") recursive_function(n + 1) # Simulating function calls and exception handling try: raise_custom_error(\\"error\\") except CustomError: handle_errors() try: recursive_function(0) except CustomRecursionError: handle_errors()"},{"question":"**Advanced Seaborn Residual Plot Analysis** # Problem Statement You are provided with a dataset named `mpg` that contains information about various cars, including their `weight`, `displacement`, `horsepower`, and `mpg` (miles per gallon). Write a Python function `analyze_residuals` using Seaborn that generates and saves three residual plots: 1. A residual plot for the relationship between `weight` and `displacement`. 2. A residual plot for the relationship between `horsepower` and `mpg`. 3. A residual plot for the relationship between `horsepower` and `mpg` with a second-order polynomial fit and a LOWESS curve. Your function should: * Load the `mpg` dataset from Seaborn. * Create and save the three residual plots. * Return a brief summary of any structures or patterns observed in the residuals, indicating potential violations of linear regression assumptions. # Input - None (the function uses the Seaborn `mpg` dataset). # Output - A dictionary with the following keys: - \'plot1\': residual plot for `weight` vs `displacement` saved as \'residual_plot1.png\', - \'plot2\': residual plot for `horsepower` vs `mpg` saved as \'residual_plot2.png\', - \'plot3\': residual plot for `horsepower` vs `mpg` with order=2 and LOWESS curve, saved as \'residual_plot3.png\', - \'summary\': string describing the patterns observed in the residuals and any potential violations of regression assumptions. # Function Signature ```python def analyze_residuals() -> dict: pass ``` # Example Output ```python { \'plot1\': \'residual_plot1.png\', \'plot2\': \'residual_plot2.png\', \'plot3\': \'residual_plot3.png\', \'summary\': \'In the residual plot for weight vs displacement, no clear pattern is evident, suggesting a good model fit. In the residual plots for horsepower vs mpg, both linear and second-order fits reveal a non-linear pattern, indicating a poor model fit and potential violation of linear regression assumptions.\' } ``` # Constraints - Utilize the Seaborn library for creating residual plots. - Ensure plots are saved as \'residual_plot1.png\', \'residual_plot2.png\', and \'residual_plot3.png\'. - The summary should be concise yet informative, providing insights into model fit and potential regression issues. # Notes - You may include the loading and setting of the Seaborn theme within your function. - Focus on clear and accurate visualizations and summaries.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_residuals(): Generates and saves three residual plots for the \'mpg\' dataset: 1. weight vs displacement 2. horsepower vs mpg (linear) 3. horsepower vs mpg (second-order polynomial with LOWESS) Returns a summary describing observed patterns in the residuals. # Load the \'mpg\' dataset mpg = sns.load_dataset(\'mpg\').dropna() # Plot 1: weight vs displacement plt.figure(figsize=(10, 6)) sns.residplot(x=\'weight\', y=\'displacement\', data=mpg) plt.title(\'Residuals of weight vs displacement\') plt.savefig(\'residual_plot1.png\') plt.close() # Plot 2: horsepower vs mpg (linear) plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg) plt.title(\'Residuals of horsepower vs mpg (linear)\') plt.savefig(\'residual_plot2.png\') plt.close() # Plot 3: horsepower vs mpg with second-order polynomial and LOWESS plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, order=2, lowess=True) plt.title(\'Residuals of horsepower vs mpg (order=2, LOWESS)\') plt.savefig(\'residual_plot3.png\') plt.close() # Summary of residuals summary = ( \\"In the residual plot for weight vs displacement, no clear pattern is evident, \\" \\"suggesting a good model fit.n\\" \\"In the residual plots for horsepower vs mpg, both linear and second-order fits reveal a \\" \\"non-linear pattern, indicating a poor model fit and potential violation of linear regression assumptions.\\" ) return { \'plot1\': \'residual_plot1.png\', \'plot2\': \'residual_plot2.png\', \'plot3\': \'residual_plot3.png\', \'summary\': summary }"},{"question":"# Seaborn Advanced Rug Plot Customization You are tasked with creating a comprehensive scatter plot using the Seaborn library, enhanced with various customization options for rug plots. The goal is to demonstrate your capability to utilize Seaborn functions to their fullest potential. Requirements: 1. **Load Dataset:** - Use the `tips` dataset from Seaborn. 2. **Scatter Plot:** - Create a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. - Use the `hue` parameter to represent a third variable, `time`. 3. **Rug Plot Customization:** - Add a rug plot along the x-axis and y-axis. - Customize the rug plot as follows: - Change the height of the rug plots to 0.2. - Put the rug plot outside the axes by setting `height` to -0.05 and `clip_on` to `False`. - Use a different color for the rug plots (e.g., `\'green\'`). - Adjust the width of the rug lines to 2. 4. **Density Representation:** - Show the density of points using an additional rug plot with the following settings: - For a larger dataset, use the `diamonds` dataset from Seaborn. - Plot the `carat` on the x-axis and `price` on the y-axis. - Use thinner lines and alpha blending (`lw`=0.5, `alpha`=0.1). Function Signature: ```python def create_custom_rugplot(): import seaborn as sns sns.set_theme() tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # To be implemented ``` Solution Implementation: ```python def create_custom_rugplot(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Load datasets tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # Create scatter plot with rug plot for tips dataset ax1 = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.2, ax=ax1, color=\'green\') sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=-0.05, clip_on=False, ax=ax1, color=\'green\', lw=2) plt.show() # Create scatter plot with rug plot for diamonds dataset ax2 = sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5) sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", lw=0.5, alpha=0.1, ax=ax2) plt.show() # Invoke the function to see the plots create_custom_rugplot() ``` **Notes:** - Your function should generate two separate plots as described in the requirements. - Ensure you import all necessary libraries and set the Seaborn theme at the beginning of your function. - Customize the plots accurately as per the specified parameters.","solution":"def create_custom_rugplot(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Load datasets tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # Create scatter plot with rug plot for tips dataset plt.figure(figsize=(10, 6)) ax1 = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", height=0.2, ax=ax1, color=\'green\') sns.rugplot(data=tips, y=\\"tip\\", height=-0.05, clip_on=False, ax=ax1, color=\'green\', lw=2) plt.title(\\"Tips Dataset: Scatter Plot with Rug Plots\\") plt.show() # Create scatter plot with rug plot for diamonds dataset plt.figure(figsize=(10, 6)) ax2 = sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5) sns.rugplot(data=diamonds, x=\\"carat\\", lw=0.5, alpha=0.1, ax=ax2, color=\'blue\') sns.rugplot(data=diamonds, y=\\"price\\", lw=0.5, alpha=0.1, ax=ax2, color=\'blue\') plt.title(\\"Diamonds Dataset: Scatter Plot with Rug Plots\\") plt.show() # Invoke the function to see the plots create_custom_rugplot()"},{"question":"Objective: Demonstrate your understanding of Python by working with classes, control flow, and data structures, and by handling exceptions. Problem Statement: You are tasked with creating a library management system for a small library. The system must allow the following operations: 1. Adding a book to the library. 2. Removing a book from the library. 3. Lending a book to a user. 4. Returning a book to the library. 5. Listing all books currently available. Each book has a title and an author, and for simplicity, the system will not account for multiple copies of the same book. Details: 1. Implement a `Book` class with the following attributes: - `title` (str): The title of the book. - `author` (str): The author of the book. 2. Implement a `Library` class to handle the library operations. The class should have the following methods: - `add_book(book: Book)`: Adds a `Book` object to the library. - `remove_book(book_title: str)`: Removes a book with the specified title from the library. Raise a `ValueError` if the book is not found. - `lend_book(book_title: str, user: str)`: Lends a book to a user. Keep track of which user has borrowed which book. Raise a `ValueError` if the book is not available for lending. - `return_book(book_title: str)`: Returns a book to the library. Raise a `ValueError` if the returned book was not lent out. - `available_books() -> list`: Returns a list of all `Book` objects currently available in the library. Constraints: - Use appropriate data structures to manage books and users. - Ensure methods handle edge cases, such as trying to remove or lend out a non-existent book. - Document your code appropriately. Example: ```python # Example usage: library = Library() book1 = Book(\\"The Catcher in the Rye\\", \\"J.D. Salinger\\") book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\") library.add_book(book1) library.add_book(book2) print(library.available_books()) # Should list both books library.lend_book(\\"The Catcher in the Rye\\", \\"Alice\\") print(library.available_books()) # Should list only \\"To Kill a Mockingbird\\" library.return_book(\\"The Catcher in the Rye\\") print(library.available_books()) # Should list both books again library.remove_book(\\"To Kill a Mockingbird\\") print(library.available_books()) # Should list only \\"The Catcher in the Rye\\" ``` Notes: - The `available_books` method should return a list of book objects. - Each method should include appropriate error handling and raise `ValueError` with a descriptive message when an operation fails. The provided functions should be well-documented and written in a clear and concise manner. Ensure to test your code with various scenarios to validate its correctness.","solution":"class Book: def __init__(self, title, author): self.title = title self.author = author class Library: def __init__(self): self.books = [] self.lent_books = {} def add_book(self, book: Book): self.books.append(book) def remove_book(self, book_title: str): for book in self.books: if book.title == book_title: self.books.remove(book) return raise ValueError(f\\"Book titled \'{book_title}\' not found in library.\\") def lend_book(self, book_title: str, user: str): for book in self.books: if book.title == book_title: self.books.remove(book) self.lent_books[book_title] = user return raise ValueError(f\\"Book titled \'{book_title}\' is not available for lending.\\") def return_book(self, book_title: str): if book_title in self.lent_books: user = self.lent_books.pop(book_title) book = Book(book_title, None) # Temporarily create a book object with no author to return it self.books.append(book) return raise ValueError(f\\"Book titled \'{book_title}\' was not lent out.\\") def available_books(self) -> list: return self.books"},{"question":"# Enum-based Calendar Events Manager **Objective:** Write class-based Python code to manage calendar events using the `enum` module. Specifically, you\'ll define enumerations and classes to manage event types and event importance levels, ensuring no duplicate values. # Requirements: 1. **Enumerations:** - Create an `EventType` enum with the following members: - MEETING with value 1 - DEADLINE with value 2 - HOLIDAY with value 3 - BIRTHDAY with value 4 - Create an `EventImportance` enum with the following members: - LOW with value 1 - MEDIUM with value 2 - HIGH with value 3 2. **Class:** - Create an `Event` class with the following properties: - `name` (a string) - `event_type` (an enum member of `EventType`) - `importance` (an enum member of `EventImportance`) - `date` (a string in the format \'YYYY-MM-DD\') - The `Event` class should have a method named `is_important` which returns `True` if the event\'s importance is `HIGH`, and `False` otherwise. 3. **Duplicate Prevention:** - Ensure that classes prevent duplicate enum values by utilizing the `@unique` decorator where applicable. 4. **Methods:** - Implement a method in the `Event` class named `display_event` that prints the event details in a format such as: ``` Event: <Name> Type: <EventType Name> Importance: <EventImportance Name> Date: <Date> ``` # Constraints: - Only the given values should be used for enum members. - `EventType` and `EventImportance` should throw an error if duplicate values are set. - Ensure the class properties are only of the specified types. # Example Usage: ```python from datetime import date # Define enumerations @unique class EventType(Enum): MEETING = 1 DEADLINE = 2 HOLIDAY = 3 BIRTHDAY = 4 @unique class EventImportance(Enum): LOW = 1 MEDIUM = 2 HIGH = 3 # Define the Event class class Event: def __init__(self, name, event_type, importance, date): self.name = name self.event_type = event_type self.importance = importance self.date = date def is_important(self): return self.importance == EventImportance.HIGH def display_event(self): print(f\\"Event: {self.name}\\") print(f\\"Type: {self.event_type.name}\\") print(f\\"Importance: {self.importance.name}\\") print(f\\"Date: {self.date}\\") # Example of usage event = Event(name=\\"Project Deadline\\", event_type=EventType.DEADLINE, importance=EventImportance.HIGH, date=\\"2023-12-10\\") event.display_event() # Expected Output: # Event: Project Deadline # Type: DEADLINE # Importance: HIGH # Date: 2023-12-10 ``` You are required to submit the combined solution in a single script. Ensure your code follows proper Python conventions and is well-commented for clarity.","solution":"from enum import Enum, unique @unique class EventType(Enum): MEETING = 1 DEADLINE = 2 HOLIDAY = 3 BIRTHDAY = 4 @unique class EventImportance(Enum): LOW = 1 MEDIUM = 2 HIGH = 3 class Event: def __init__(self, name, event_type, importance, date): if not isinstance(event_type, EventType): raise ValueError(\\"event_type must be an instance of EventType Enum\\") if not isinstance(importance, EventImportance): raise ValueError(\\"importance must be an instance of EventImportance Enum\\") self.name = name self.event_type = event_type self.importance = importance self.date = date def is_important(self): return self.importance == EventImportance.HIGH def display_event(self): print(f\\"Event: {self.name}\\") print(f\\"Type: {self.event_type.name}\\") print(f\\"Importance: {self.importance.name}\\") print(f\\"Date: {self.date}\\")"},{"question":"**Question Title:** Extract Python Version Components from Version Hex **Problem Description:** You are given a 32-bit integer that encodes the version information of CPython. This integer is constructed using the following components: 1. `PY_MAJOR_VERSION`: Encoded in bits 1-8 2. `PY_MINOR_VERSION`: Encoded in bits 9-16 3. `PY_MICRO_VERSION`: Encoded in bits 17-24 4. `PY_RELEASE_LEVEL`: Encoded in bits 25-28 5. `PY_RELEASE_SERIAL`: Encoded in bits 29-32 The encoding scheme for the release level is as follows: - `0xA` for alpha - `0xB` for beta - `0xC` for release candidate - `0xF` for final Given an integer input that represents the encoded version information, write a Python function to extract and return the major version, minor version, micro version, release level, and release serial as a tuple. You will also convert the encoded release level to its string representation. **Function Signature:** ```python def extract_version_info(version_hex: int) -> tuple: pass ``` **Input:** - `version_hex (int)`: A 32-bit integer representing the encoded version information. **Output:** - `tuple`: A tuple containing the following elements in order: - Major version (int) - Minor version (int) - Micro version (int) - Release level (str): One of `[\\"alpha\\", \\"beta\\", \\"candidate\\", \\"final\\"]` - Release serial (int) **Constraints:** - You may assume the input integer will always be a valid 32-bit integer representing a CPython version. **Example:** ```python # Example 1 version_hex = 0x030401a2 assert extract_version_info(version_hex) == (3, 4, 1, \\"alpha\\", 2) # Example 2 version_hex = 0x030a00f0 assert extract_version_info(version_hex) == (3, 10, 0, \\"final\\", 0) ``` **Explanation:** - In Example 1, the 32-bit integer `0x030401a2` corresponds to: - Major version: 3 - Minor version: 4 - Micro version: 1 - Release level: \\"alpha\\" (represented by `0xA`) - Release serial: 2 - In Example 2, the 32-bit integer `0x030a00f0` corresponds to: - Major version: 3 - Minor version: 10 - Micro version: 0 - Release level: \\"final\\" (represented by `0xF`) - Release serial: 0 This problem will test the student\'s understanding of bit manipulation and their ability to convert between different representations (integers and strings).","solution":"def extract_version_info(version_hex: int) -> tuple: Extracts the version information from a 32-bit integer. Parameters: version_hex (int): A 32-bit integer representing the encoded version information. Returns: tuple: A tuple containing the major version, minor version, micro version, release level as a string, and release serial. # Extracting various parts using bit manipulation major_version = (version_hex >> 24) & 0xFF minor_version = (version_hex >> 16) & 0xFF micro_version = (version_hex >> 8) & 0xFF release_level_code = (version_hex >> 4) & 0xF release_serial = version_hex & 0xF # Mapping the release level code to a string release_level_map = {0xA: \\"alpha\\", 0xB: \\"beta\\", 0xC: \\"candidate\\", 0xF: \\"final\\"} release_level = release_level_map.get(release_level_code, \\"unknown\\") return (major_version, minor_version, micro_version, release_level, release_serial)"},{"question":"# Restricted Boltzmann Machine and Classification Assignment Objective: Use the `BernoulliRBM` implementation in scikit-learn to preprocess a binary dataset, then classify the data using a linear classifier. Task: Write a Python function `rbm_classify` that takes in a binary dataset and its corresponding labels, preprocesses the data using an RBM, and then uses a logistic regression classifier to classify the transformed data. The function should return the accuracy of your classifier on the test set. Input: - `X_train`: A numpy array of shape (n_samples, n_features) representing the training data, with binary values (0 or 1). - `y_train`: A numpy array of shape (n_samples,) representing the labels for the training data. - `X_test`: A numpy array of shape (m_samples, n_features) representing the test data (same feature format as training data), with binary values (0 or 1). - `y_test`: A numpy array of shape (m_samples,) representing the labels for the test data. Output: - A float representing the accuracy of the classifier on the test set. Constraints: - Do not use any pre-built functions for train-test splitting or accuracy score calculation other than those specified (e.g., use `train_test_split` from `sklearn.model_selection` and `accuracy_score` from `sklearn.metrics`). - Assume the data is already preprocessed and split apart from the necessary RBM transformation. Requirements: 1. Use `BernoulliRBM` for feature learning on the training data. 2. Use a logistic regression classifier for classification after transformation. 3. Measure the accuracy of the classifier on the test data. Example Function Signature: ```python def rbm_classify(X_train, y_train, X_test, y_test): # Your code here pass ``` Example: ```python import numpy as np from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def rbm_classify(X_train, y_train, X_test, y_test): # Initialize BernoulliRBM and Logistic Regression rbm = BernoulliRBM(n_components=256, learning_rate=0.01, n_iter=10, random_state=0) logistic = LogisticRegression(max_iter=1000, random_state=0) # Create a pipeline that combines RBM and Logistic Regression classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the classifier on the training data classifier.fit(X_train, y_train) # Predict the labels for the test data y_pred = classifier.predict(X_test) # Calculate and return the accuracy score return accuracy_score(y_test, y_pred) # Example usage X_train = np.array([[0, 1], [1, 0], [1, 1], [0, 0]]) y_train = np.array([0, 1, 1, 0]) X_test = np.array([[1, 1], [0, 0]]) y_test = np.array([1, 0]) print(rbm_classify(X_train, y_train, X_test, y_test)) # Output expected: 1.0 (or 100% accuracy since the dataset is trivial) ``` Explanation: - The `rbm_classify` function initializes an RBM and a logistic regression model and combines them into a pipeline. - The pipeline is trained using the provided training data. - The trained classifier is then used to predict the labels of the test data, and the accuracy score is computed and returned.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def rbm_classify(X_train, y_train, X_test, y_test): This function preprocesses the binary dataset using RBM and classifies the data using logistic regression. Parameters: - X_train: Training data (binary values), numpy array of shape (n_samples, n_features) - y_train: Labels for the training data, numpy array of shape (n_samples,) - X_test: Testing data (binary values), numpy array of shape (m_samples, n_features) - y_test: Labels for the testing data, numpy array of shape (m_samples,) Returns: - Accuracy of the classifier on the test set (float). # Initialize BernoulliRBM and Logistic Regression rbm = BernoulliRBM(n_components=256, learning_rate=0.01, n_iter=20, random_state=0) logistic = LogisticRegression(max_iter=1000, random_state=0) # Create a pipeline that combines RBM and Logistic Regression classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the classifier on the training data classifier.fit(X_train, y_train) # Predict the labels for the test data y_pred = classifier.predict(X_test) # Calculate and return the accuracy score return accuracy_score(y_test, y_pred)"},{"question":"**Objective:** Write a function that visualizes the distribution of a chosen numerical feature from a specified dataset using Seaborn\'s object-oriented interface. The function should create a layered plot with jittered dots and percentile-based ranges for the specified feature, grouped by a categorical feature. **Function Signature:** ```python def plot_distribution_with_summary(data, numerical_feature, categorical_feature, percentiles=[25, 75]): Create a Seaborn plot visualizing the distribution of a numerical feature using jittered dots and percentile-based ranges, grouped by a categorical feature. Parameters: data (pd.DataFrame): The dataset containing the features to plot. numerical_feature (str): The name of the numerical feature to visualize. categorical_feature (str): The name of the categorical feature to group by. percentiles (list of int, optional): The percentiles to display as a range. Default is [25, 75]. Returns: None pass ``` **Inputs:** - `data`: A pandas DataFrame containing the dataset to visualize. - `numerical_feature`: A string representing the name of the numerical feature to plot. - `categorical_feature`: A string representing the name of the categorical feature to group the data by. - `percentiles`: A list of two integers indicating the percentiles to calculate and display as a range (defaults to 25th and 75th percentiles). **Outputs:** - The function should produce a Seaborn plot displayed inline if running in a Jupyter environment or saved to a file if executed in a script. **Constraints:** - The numerical feature must be a column in the dataset with numeric data types. - The categorical feature must be a column in the dataset with categorical or string data types. - Percentiles must be within the range [0, 100]. **Example usage:** ```python import seaborn as sns import pandas as pd # Load an example dataset penguins = sns.load_dataset(\\"penguins\\") # Call the function plot_distribution_with_summary(penguins, \\"body_mass_g\\", \\"species\\") ``` **Expected Plot:** The plot should: - Display jittered dots representing individual data points within each category of the categorical feature. - Overlay percentile-based ranges for the specified numerical feature within each category. **Hints:** - Use `seaborn.objects` for creating the plot. - Utilize the `so.Plot`, `so.Dots()`, `so.Jitter()`, `so.Range()`, and `so.Perc()` features demonstrated in the documentation.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_distribution_with_summary(data, numerical_feature, categorical_feature, percentiles=[25, 75]): Create a Seaborn plot visualizing the distribution of a numerical feature using jittered dots and percentile-based ranges, grouped by a categorical feature. Parameters: data (pd.DataFrame): The dataset containing the features to plot. numerical_feature (str): The name of the numerical feature to visualize. categorical_feature (str): The name of the categorical feature to group by. percentiles (list of int, optional): The percentiles to display as a range. Default is [25, 75]. Returns: None if not isinstance(data, pd.DataFrame): raise ValueError(\\"data must be a pandas DataFrame\\") if not isinstance(numerical_feature, str): raise ValueError(\\"numerical_feature must be a string\\") if not isinstance(categorical_feature, str): raise ValueError(\\"categorical_feature must be a string\\") if not all(isinstance(p, int) for p in percentiles) or len(percentiles) != 2: raise ValueError(\\"percentiles must be a list of two integers\\") sns.set(style=\\"whitegrid\\") p = sns.catplot(x=categorical_feature, y=numerical_feature, data=data, kind=\\"strip\\", jitter=True, palette=\\"deep\\") for cat in data[categorical_feature].unique(): cat_data = data[data[categorical_feature] == cat][numerical_feature] lower, upper = cat_data.quantile(percentiles[0]/100), cat_data.quantile(percentiles[1]/100) plt.axhline(y=lower, color=\'gray\', linestyle=\'--\', xmin=0.1, xmax=0.9) plt.axhline(y=upper, color=\'gray\', linestyle=\'--\', xmin=0.1, xmax=0.9) plt.show()"},{"question":"# File System and Temporary File Manipulation with Pathlib Instructions: You are given a task to manage filesystem operations in a Python application using the `pathlib` and `tempfile` modules. This will include reading file properties, creating temporary files, and comparing file contents. Task Breakdown 1. **Define a function `file_stats(file_path)`** that accepts a single argument: - `file_path` (str): The path to the file whose properties need to be checked. The function should return a dictionary with the following properties: - `file_size`: Size of the file in bytes. - `is_file`: Boolean indicating if the path is a file. - `is_dir`: Boolean indicating if the path is a directory. - `file_extension`: The file extension. 2. **Define a function `create_temp_files_and_compare(content1, content2)`** that performs the following operations: - Creates two temporary text files using the `tempfile` module. - Writes the content of `content1` to the first temporary file. - Writes the content of `content2` to the second temporary file. - Compares the two files. - Deletes the temporary files before the function ends. - Returns `True` if the contents of both files are the same, `False` otherwise. - `content1` (str): The content to be written in the first temporary file. - `content2` (str): The content to be written in the second temporary file. Example Usage ```python # Example for file_stats file_path = \\"example.txt\\" stats = file_stats(file_path) print(stats) # Expected output (example, values may vary): # {\'file_size\': 1024, \'is_file\': True, \'is_dir\': False, \'file_extension\': \'.txt\'} # Example for create_temp_files_and_compare content1 = \\"Hello, World!\\" content2 = \\"Hello, World!\\" result = create_temp_files_and_compare(content1, content2) print(result) # Expected output: True content3 = \\"Hello, Python!\\" content4 = \\"Hello, World!\\" result = create_temp_files_and_compare(content3, content4) print(result) # Expected output: False ``` Constraints: - Assume the file paths provided are valid. - Consider using `pathlib.Path` for file system operations. - Use `tempfile.NamedTemporaryFile` for creating temporary files. Performance: - The functions should handle moderate file sizes efficiently. - Temporary file creation should ensure no file is left undeleted, avoiding resource leaks.","solution":"from pathlib import Path import tempfile def file_stats(file_path): Returns file properties for the given file path. path = Path(file_path) return { \'file_size\': path.stat().st_size if path.is_file() else None, \'is_file\': path.is_file(), \'is_dir\': path.is_dir(), \'file_extension\': path.suffix if path.is_file() else None } def create_temp_files_and_compare(content1, content2): Creates two temporary files with the given contents and compares them. Returns True if the contents are the same, otherwise False. with tempfile.NamedTemporaryFile(delete=True) as temp1, tempfile.NamedTemporaryFile(delete=True) as temp2: temp1.write(content1.encode()) temp1.flush() temp2.write(content2.encode()) temp2.flush() temp1.seek(0) temp2.seek(0) return temp1.read() == temp2.read()"},{"question":"# Advanced Coding Challenge: Pandas Options Customization Problem Statement You are required to write a function named `customize_pandas_behavior` that takes a configuration file as input and sets the pandas options according to the settings provided in this file. The configuration file will be in JSON format and will contain various key-value pairs representing pandas options and their desired values. Your function should also include: 1. A capability to describe the current setting of a given option. 2. A functionality to reset an option to its default value. 3. A way to temporarily set an option within a particular context and reverses back to the original setting afterward. Function Signature ```python import pandas as pd import json def customize_pandas_behavior(config_file: str) -> None: # Your implementation here ``` Input - `config_file` (str): A string representing the path to the configuration JSON file. The JSON configuration file will contain the following data: ```json { \\"options\\": { \\"display.max_rows\\": 20, \\"display.precision\\": 2 }, \\"describe_option\\": \\"display.max_rows\\", \\"reset_option\\": \\"display.precision\\" } ``` Output - The function will not return anything but will perform the actions as specified: - Set the pandas options specified under the \\"options\\" key. - Describe the current setting of the option specified under the \\"describe_option\\" key. - Reset the option specified under the \\"reset_option\\" key to its default value. - Temporarily set an option inside a context manager and verify that it reverts after the context. Example Customization Process Given the JSON configuration file with the above content, the function should: 1. Set `display.max_rows` to 20. 2. Set `display.precision` to 2. 3. Describe the current setting of `display.max_rows`. 4. Reset `display.precision` to its default value. 5. Temporarily set `display.max_rows` to 50 and ensure it reverts back to 20 after the context. Constraints - You must handle any potential exceptions for invalid configurations gracefully with appropriate error messages. Notes - You can assume that the provided JSON file will always have valid syntax. - You should print the results of `describe_option` and any changes to the settings in the console. Guidelines - Import necessary functionalities from pandas. - For input/output operations, you might need additional modules such as `json` for parsing the configuration file. - Demonstrate the temporary setting using a context manager.","solution":"import pandas as pd import json def customize_pandas_behavior(config_file: str) -> None: with open(config_file, \'r\') as file: config = json.load(file) # Set pandas options for option, value in config.get(\'options\', {}).items(): pd.set_option(option, value) print(f\\"Set {option} to {value}\\") # Describe the current setting of the specified option describe_option = config.get(\'describe_option\') if describe_option: print(f\\"{describe_option} is currently set to: {pd.get_option(describe_option)}\\") # Reset the specified option to its default value reset_option = config.get(\'reset_option\') if reset_option: pd.reset_option(reset_option) print(f\\"Reset {reset_option} to its default value: {pd.get_option(reset_option)}\\") # Temporarily set an option within a context and verify that it reverts back temporary_option = \\"display.max_rows\\" original_value = pd.get_option(temporary_option) with pd.option_context(temporary_option, 50): print(f\\"Temporarily set {temporary_option} to 50, current value is: {pd.get_option(temporary_option)}\\") # Verify it reverts back to the original setting afterward print(f\\"After context, {temporary_option} reverts back to: {pd.get_option(temporary_option)}\\")"},{"question":"# Advanced URL Fetching and Handling with `urllib` in Python Objective: Implement a function that fetches the content of a given URL using the `urllib` package. The function should handle various situations including custom headers, handling redirects, dealing with authentication, and exception handling. Additionally, the function should return useful information about the response. Function Signature: ```python def fetch_url_content(url: str, data: dict = None, headers: dict = None, auth: tuple = None) -> dict: Fetches the content from a given URL and returns a dictionary with status code, actual URL, headers, and content. Parameters: url (str): The URL to fetch. data (dict, optional): Data to be sent as a POST request. Defaults to None for a GET request. headers (dict, optional): Custom headers to be included in the request. Defaults to None. auth (tuple, optional): A tuple (username, password) for basic HTTP authentication. Defaults to None. Returns: dict: A dictionary containing status code, actual URL, response headers, and content. pass ``` Requirements: 1. **HTTP Method:** - If `data` is provided, the request should be a POST request. - If `data` is not provided, the request should be a GET request. 2. **Custom Headers:** - If `headers` are provided, they should be included in the request. 3. **Authentication:** - If `auth` is provided, use basic HTTP authentication. 4. **Exception Handling:** - Handle `URLError` and `HTTPError` exceptions. - In case of an exception, return a dictionary with the status code set to `None` and appropriate error messages. 5. **Response Information:** - If the request is successful, return a dictionary with the following keys: - `status_code`: The HTTP status code of the response. - `url`: The actual URL that was fetched. - `headers`: A dictionary of response headers. - `content`: The content of the response. 6. **Redirects:** - Handle any HTTP redirections automatically. Example: ```python url = \\"http://www.example.com\\" data = {\\"key\\": \\"value\\"} headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} auth = (\\"username\\", \\"password\\") response = fetch_url_content(url, data, headers, auth) print(response) ``` **Expected Output (example):** ```python { \\"status_code\\": 200, \\"url\\": \\"http://www.example.com\\", \\"headers\\": { \\"Content-Type\\": \\"text/html\\", \\"Content-Length\\": \\"138\\", \\"Date\\": \\"Tue, 01 Oct 2019 12:00:00 GMT\\" }, \\"content\\": \\"<html>...</html>\\" } ``` Constraints: - The function should handle URL redirections automatically. - Ensure proper usage of custom headers and authentication credentials. - Focus on robust exception handling to cover various failure scenarios. Performance Requirements: - The function should efficiently handle network requests and return the required information promptly. - Consider setting a reasonable timeout for network requests if necessary, to avoid indefinite hanging. **Note:** Do not use any external libraries other than the standard `urllib` package in Python.","solution":"import urllib.request import urllib.error from urllib.parse import urlencode from base64 import b64encode def fetch_url_content(url: str, data: dict = None, headers: dict = None, auth: tuple = None) -> dict: Fetches the content from a given URL and returns a dictionary with status code, actual URL, headers, and content. Parameters: url (str): The URL to fetch. data (dict, optional): Data to be sent as a POST request. Defaults to None for a GET request. headers (dict, optional): Custom headers to be included in the request. Defaults to None. auth (tuple, optional): A tuple (username, password) for basic HTTP authentication. Defaults to None. Returns: dict: A dictionary containing status code, actual URL, response headers, and content. request_headers = headers or {} if auth is not None: # Encode the authentication credentials in base64 credentials = f\\"{auth[0]}:{auth[1]}\\".encode(\'utf-8\') encoded_credentials = b64encode(credentials).decode(\'utf-8\') request_headers[\'Authorization\'] = f\'Basic {encoded_credentials}\' if data is not None: data_encoded = urlencode(data).encode(\'utf-8\') req = urllib.request.Request(url, data=data_encoded, headers=request_headers, method=\'POST\') else: req = urllib.request.Request(url, headers=request_headers) try: with urllib.request.urlopen(req) as response: content = response.read().decode(response.headers.get_content_charset(failobj=\'utf-8\')) return { \'status_code\': response.status, \'url\': response.geturl(), \'headers\': dict(response.getheaders()), \'content\': content } except urllib.error.HTTPError as e: return { \'status_code\': e.code, \'url\': e.geturl(), \'headers\': dict(e.headers), \'content\': e.reason } except urllib.error.URLError as e: return { \'status_code\': None, \'url\': None, \'headers\': {}, \'content\': str(e.reason) }"},{"question":"**Module Handling in Python** In this exercise, you are required to implement a Python function `import_and_manage_modules` that uses the functionalities provided by the python310 package. This function should achieve the following: 1. Import a specified module. 2. Reload the module to ensure the latest version is used. 3. Retrieve and return the module from the module dictionary. You are expected to handle exceptions where an import might fail and ensure the function returns useful error information. Implementing this function requires the use of: - `PyImport_ImportModule()` - `PyImport_ReloadModule()` - `PyImport_GetModule()` - `PyImport_GetModuleDict()` # Function Signature ```python import_and_manage_modules(module_name: str) -> dict: ``` # Input - `module_name`: A string specifying the name of the module to be imported and managed. # Output - A dictionary with the following structure: ```python { \\"imported_module\\": <module>, # the imported module \\"reload_status\\": <bool>, # True if reload successful \\"module_from_dict\\": <module> # the module retrieved from module dictionary } ``` - In case of failure during import or reload, the dictionary should include an error key with appropriate error messages. # Example ```python result = import_and_manage_modules(\\"os\\") print(result) ``` Output: ```python { \\"imported_module\\": <module \'os\' from \'path_to_os_module\'>, \\"reload_status\\": True, \\"module_from_dict\\": <module \'os\' from \'path_to_os_module\'> } ``` In case of an import failure: ```python result = import_and_manage_modules(\\"non_existent_module\\") print(result) ``` Output: ```python { \\"error\\": \\"Module \'non_existent_module\' could not be imported\\" } ``` # Constraints - Only use the functions from the provided documentation to accomplish the task. - Ensure that error handling is properly implemented to manage failures during any step in the process. # Advanced Requirements - Ensure optimal performance by avoiding unnecessary module reloads if the module has not been modified. # Note Implementing this function will demonstrate a deep understanding of module management in Python, including importing, reloading, and retrieving modules from the system module registry.","solution":"import importlib import sys def import_and_manage_modules(module_name: str) -> dict: result = {} try: # Import the module imported_module = importlib.import_module(module_name) result[\'imported_module\'] = imported_module # Reload the module reloaded_module = importlib.reload(imported_module) result[\'reload_status\'] = reloaded_module is not None # Retrieve from module dictionary module_from_dict = sys.modules.get(module_name) result[\'module_from_dict\'] = module_from_dict except (ModuleNotFoundError, ImportError) as e: result[\'error\'] = f\\"Module \'{module_name}\' could not be imported: {e}\\" return result"},{"question":"# Question: Concurrent Task Processing Using `concurrent.futures` Implement a Python function `calculate_squares(numbers: List[int], max_workers: int) -> List[Tuple[int, int]]` that uses `concurrent.futures.ThreadPoolExecutor` to calculate the square of each number in the provided list of integers asynchronously. Function Signature ```python def calculate_squares(numbers: List[int], max_workers: int) -> List[Tuple[int, int]]: ``` Parameters - `numbers` (List[int]): A list of integers for which the squares need to be calculated. - `max_workers` (int): The maximum number of worker threads to use for asynchronous execution. Return - List[Tuple[int, int]]: A list of tuples where each tuple contains the original number and its square. Requirements 1. Utilize `ThreadPoolExecutor` to submit tasks asynchronously. 2. Ensure that all tasks are completed before returning the final result. 3. Handle any potential exceptions that may occur during the computation. 4. Use a maximum of `max_workers` threads. Example ```python input_numbers = [1, 2, 3, 4, 5] max_workers = 3 output = calculate_squares(input_numbers, max_workers) # Expected output: [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)] ``` Constraints - The `numbers` list will contain at least one element. - Each number in the `numbers` list is a non-negative integer (0 <= number <= 10^6). - The `max_workers` will be a positive integer (1 <= max_workers <= 20). Notes - Make sure to handle any exceptions that might occur during the execution of tasks. - Preserve the order of the input numbers in the output list.","solution":"from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List, Tuple def calculate_squares(numbers: List[int], max_workers: int) -> List[Tuple[int, int]]: Calculate squares of the given list of numbers using ThreadPoolExecutor. Args: numbers (List[int]): List of integers to calculate squares of. max_workers (int): Maximum number of worker threads. Returns: List[Tuple[int, int]]: List of tuples containing original number and its square. def square(n: int) -> Tuple[int, int]: return (n, n * n) results = [] with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_num = {executor.submit(square, num): num for num in numbers} for future in as_completed(future_to_num): result = future.result() results.append(result) # Since as_completed does not guarantee order, we need to sort the results back results.sort(key=lambda x: numbers.index(x[0])) return results"},{"question":"Task: You are given a binary file that stores information about employees in a company. Each record in the file corresponds to a single employee and contains the following fields: 1. **Employee ID**: A signed 4-byte integer. 2. **Name**: A fixed-length 20-byte string (UTF-8 encoded; padded with null bytes if shorter). 3. **Age**: An unsigned 1-byte integer. 4. **Salary**: A 4-byte float. The file uses little-endian byte order (`<`). Write a Python function `parse_employee_data(file_path)` that reads this binary file and returns a list of dictionaries. Each dictionary should represent one employee with keys `\'employee_id\'`, `\'name\'`, `\'age\'`, and `\'salary\'`. Input: - `file_path` (str): Path to the binary file. Output: - List of dictionaries, where each dictionary contains the following keys: - `\'employee_id\'` (int): Employee ID. - `\'name\'` (str): Employee name, trimmed of trailing null bytes. - `\'age\'` (int): Employee age. - `\'salary\'` (float): Employee salary. Constraints: - The binary file is well-formed and contains only complete records. - The maximum number of employees in the file is 1000. # Example Usage: ```python # Example binary file content (hex representation) # For 2 employees: # 00000001 6a6f686e20646f650000000000000000 (Employee ID: 1, Name: \'john doe\', Age: 30, Salary: 50000.0) # 1e42 00000000 000030 ... employees = parse_employee_data(\'path/to/binary/file\') print(employees) # Output: # [ # {\'employee_id\': 1, \'name\': \'john doe\', \'age\': 30, \'salary\': 50000.0}, # {\'employee_id\': 2, \'name\': \'jane smith\', \'age\': 25, \'salary\': 62000.0}, # ] ``` # Notes: - You may use the `struct` module for parsing binary data. - Be mindful of byte order and data alignment as specified in the task description. - Handle the conversion of bytes to appropriate types carefully, particularly for the name field which includes padding.","solution":"import struct def parse_employee_data(file_path): employees = [] with open(file_path, \'rb\') as f: employee_record_size = 4 + 20 + 1 + 4 # 4 bytes for id, 20 bytes for name, 1 byte for age, 4 bytes for salary while True: record = f.read(employee_record_size) if not record: break employee_id, name, age, salary = struct.unpack(\'<i20sBf\', record) name = name.split(b\'x00\', 1)[0].decode(\'utf-8\') # Trim trailing null bytes employees.append({ \'employee_id\': employee_id, \'name\': name, \'age\': age, \'salary\': salary }) return employees"},{"question":"**Coding Assessment Question** # Objective: Your task is to write a function that utilizes seaborn to create various customized Kernel Density Estimation (KDE) plots based on a given set of parameters. # Problem Statement: Write a function `create_custom_kde_plot` that takes in the following parameters: - `df`: A DataFrame containing the dataset (e.g., the penguins dataset). - `x`: The name of the column to plot on the x-axis (string). - `y`: The name of the column to plot on the y-axis. If `None`, only the x-axis will be used (string or None). - `hue`: The name of the column used for color encoding (string or None). - `bw_adjust`: Bandwidth adjustment for the KDE (float, default is 1.0). - `cut`: Factor to extend the density curve beyond the data range (int, default is 3). - `gridsize`: The number of grid points for evaluating the KDE. If `None`, evaluate at the data points (int or None, default is 100). - `common_norm`: Whether to normalize the density across groups (bool, default is True). - `cumulative`: Whether to display cumulative density (bool, default is False). The function should create and display the required KDE plot based on the input parameters using seaborn. # Expected Function Signature: ```python import seaborn.objects as so import seaborn as sns def create_custom_kde_plot(df, x, y=None, hue=None, bw_adjust=1.0, cut=3, gridsize=100, common_norm=True, cumulative=False): pass ``` # Implementation Details: - Load the dataset using `seaborn.load_dataset`. - Use the `seaborn.objects.Plot` class to create the plot. - Add elements such as `Area`, `Line`, or `Bars` to visualize the KDE. - Adjust the KDE plot based on the provided parameters. # Example Usage: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") create_custom_kde_plot(penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bw_adjust=0.5, cumulative=True) ``` # Constraints: - You may assume that the dataset will always be well-formed and contain valid data for the columns specified. - Focus on creating visually appealing and informative plots by leveraging seaborn\'s capabilities.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_kde_plot(df, x, y=None, hue=None, bw_adjust=1.0, cut=3, gridsize=100, common_norm=True, cumulative=False): Creates and displays a customized KDE plot. Parameters: - df: DataFrame containing the dataset. - x: The name of the column to plot on the x-axis (string). - y: The name of the column to plot on the y-axis. If None, only the x-axis will be used (string or None). - hue: The name of the column used for color encoding (string or None). - bw_adjust: Bandwidth adjustment for the KDE (float, default is 1.0). - cut: Factor to extend the density curve beyond the data range (int, default is 3). - gridsize: The number of grid points for evaluating the KDE. If None, evaluate at the data points (int or None, default is 100). - common_norm: Whether to normalize the density across groups (bool, default is True). - cumulative: Whether to display cumulative density (bool, default is False). plot = sns.displot(df, x=x, y=y, hue=hue, kind=\\"kde\\", bw_adjust=bw_adjust, cut=cut, gridsize=gridsize, common_norm=common_norm, cumulative=cumulative) plt.show()"},{"question":"# Custom Scikit-Learn Estimator You are required to create a custom Scikit-Learn estimator. The aim is to assess your understanding of developing scikit-learn-compatible estimators, their initialization, fitting, predicting, and validation methods. Instructions 1. Implement a custom classifier called `SimpleKNNClassifier` based on the K-Nearest Neighbors algorithm. 2. The class should inherit from `BaseEstimator` and `ClassifierMixin`. 3. The classifier should use Euclidean distance to find the `k` nearest neighbors (default value of `k` is 3). 4. The class should implement the `fit`, `predict`, and `predict_proba` methods. 5. Ensure that you include appropriate validation for input and fitting status. 6. Performance is critical, and you may use vectorized operations with NumPy to ensure efficiency. Specifications - **Initialization**: - Parameters: `k` (default=3) - Store the parameter as an instance attribute. - **Fit Method**: - Parameters: - `X` (array-like of shape (n_samples, n_features)): Training data. - `y` (array-like of shape (n_samples,)): Target values. - Store the training data `X` and `y` as instance attributes with trailing underscores. - Validate the inputs. - **Predict Method**: - Parameter: `X` (array-like of shape (n_samples, n_features)): Data to predict. - Return: `y_pred` (array-like of shape (n_samples,)): Predicted target values. - **Predict Proba Method**: - Parameter: `X` (array-like of shape (n_samples, n_features)): Data to predict probabilities. - Return: `proba` (array-like of shape (n_samples, n_classes)): Probability estimates for each class. Below is the skeleton code to guide you: ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_array, check_X_y class SimpleKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, k=3): self.k = k def fit(self, X, y): # Validate input data X, y = check_X_y(X, y) self.X_ = X self.y_ = y # Store the number of features self.n_features_in_ = X.shape[1] return self def predict(self, X): check_is_fitted(self) X = check_array(X) predictions = [] for sample in X: distances = np.sqrt(((self.X_ - sample) ** 2).sum(axis=1)) nearest_neighbors = np.argsort(distances)[:self.k] nearest_labels = self.y_[nearest_neighbors] majority_label = np.bincount(nearest_labels).argmax() predictions.append(majority_label) return np.array(predictions) def predict_proba(self, X): check_is_fitted(self) X = check_array(X) probas = [] for sample in X: distances = np.sqrt(((self.X_ - sample) ** 2).sum(axis=1)) nearest_neighbors = np.argsort(distances)[:self.k] nearest_labels = self.y_[nearest_neighbors] class_counts = np.bincount(nearest_labels, minlength=len(np.unique(self.y_))) probas.append(class_counts / self.k) return np.array(probas) ``` Make sure to: - Raise `ValueError` for invalid input shapes. - Use `check_X_y` and `check_array` from `sklearn.utils.validation` for input validation. - Use `check_is_fitted` to ensure that `fit` has been called before `predict` or `predict_proba`. Constraints - The class should follow scikit-learn\'s conventions for setting hyperparameters and attributes. - The methods should handle different input shapes appropriately and raise errors where necessary. Evaluation Criteria - Correct implementation of the scikit-learn-compatible estimator. - Proper usage of validation utilities. - Efficient computation using vectorized operations. - Clear and concise code adhering to PEP8 standards.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_array, check_X_y class SimpleKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, k=3): self.k = k def fit(self, X, y): # Validate input data X, y = check_X_y(X, y) self.X_ = X self.y_ = y self.classes_ = np.unique(y) # Store the number of features self.n_features_in_ = X.shape[1] return self def predict(self, X): check_is_fitted(self) X = check_array(X) predictions = [] for sample in X: distances = np.sqrt(((self.X_ - sample) ** 2).sum(axis=1)) nearest_neighbors = np.argsort(distances)[:self.k] nearest_labels = self.y_[nearest_neighbors] majority_label = np.bincount(nearest_labels).argmax() predictions.append(majority_label) return np.array(predictions) def predict_proba(self, X): check_is_fitted(self) X = check_array(X) probas = [] for sample in X: distances = np.sqrt(((self.X_ - sample) ** 2).sum(axis=1)) nearest_neighbors = np.argsort(distances)[:self.k] nearest_labels = self.y_[nearest_neighbors] class_counts = np.bincount(nearest_labels, minlength=len(self.classes_)) probas.append(class_counts / self.k) return np.array(probas)"},{"question":"Objective Design a function that packs a series of provided values into a `bytes` object with specific byte order and alignment, and then unpacks the values from the `bytes` object to verify correctness. This exercise will assess your understanding of the `struct` module\'s functionality, particularly regarding format strings, packing, and unpacking. Function Signature ```python def pack_and_verify_data(values: tuple, format_string: str, byte_order: str) -> bool: Packs the given values into a bytes object with specified byte order and format string, then unpacks to verify correctness. Args: values (tuple): A tuple of values to be packed. Each value must match the format string. format_string (str): A format string specifying the data layout. byte_order (str): A character indicating the byte order (\'@\', \'=\', \'<\', \'>\', \'!\'). Returns: bool: True if the unpacked values match the original input values, False otherwise. ``` Input - `values` - A tuple of values to be packed (e.g., `(1, 2.5, b\'A\', 1000)`). - Constraints: - The values in the tuple must correspond one-to-one with the format characters in `format_string`. - Integer values should be within the range allowed by their corresponding format characters. - Byte values should be single byte strings when using the `\'c\'` format character. - `format_string` - A format string consisting of format characters (e.g., `\'ibcf\'`). - Constraints: - The format string should not contain whitespace and must be valid per the `struct` module\'s specification. - The format string can include repeat count and padding. - `byte_order` - A character to specify byte order (one of `\'@\'`, `\'=\'`, `\'<\'`, `\'>\'`, `\'!\'`). Output - Boolean value (`True` or `False`), indicating whether the unpacked values match the original input values. Example ```python values = (1, 2.5, b\'A\', 1000) format_string = \'ihcI\' byte_order = \'>\' result = pack_and_verify_data(values, format_string, byte_order) print(result) # True ``` Notes 1. Use the `struct` module to: - Pack the values into a `bytes` object with the specified format and byte order. - Unpack the `bytes` object into its original values. 2. The function should handle padding and alignment as per the given format string and byte order. 3. Return `True` if the unpacked values match the input values, otherwise return `False`. Constraints - You may assume the maximum length of `values` and `format_string` is 100 characters. - The values in `values` are always valid according to the `format_string`. --- Ensure your solution correctly handles the different aspects of the `struct` module as described, including format strings, byte order, alignment, padding, and type conversion.","solution":"import struct def pack_and_verify_data(values: tuple, format_string: str, byte_order: str) -> bool: Packs the given values into a bytes object with specified byte order and format string, then unpacks to verify correctness. Args: values (tuple): A tuple of values to be packed. Each value must match the format string. format_string (str): A format string specifying the data layout. byte_order (str): A character indicating the byte order (\'@\', \'=\', \'<\', \'>\', \'!\'). Returns: bool: True if the unpacked values match the original input values, False otherwise. # Ensure the format string includes the byte order full_format_string = byte_order + format_string # Pack the values packed_data = struct.pack(full_format_string, *values) # Unpack the values unpacked_values = struct.unpack(full_format_string, packed_data) # Verify and return the result return unpacked_values == values"},{"question":"**Problem Statement:** You are tasked with automating the creation of source distributions for a Python project. Write a Python function `create_source_distribution` that takes the following parameters: - `project_root`: The root directory of the project. - `archive_formats`: A list of desired archive formats (e.g., `[\'gztar\', \'zip\']`). - `additional_files`: A list of additional files or directories to include in the distribution (e.g., `[\'docs/\', \'examples/\']`). The function should: 1. Create a `setup.py` script in the project root if it doesn\'t exist, with minimal setup specifying the project name, version, and packages. 2. Create or update a `MANIFEST.in` file in the project root that includes the default set of files plus the files specified in `additional_files`. 3. Use `python setup.py sdist` command to create source distribution archives in the specified formats. 4. Ensure that the created archives are in the `dist` directory under the project root. **Function Signature:** ```python def create_source_distribution(project_root: str, archive_formats: list, additional_files: list) -> None: pass ``` **Example Usage:** ```python project_root = \'/path/to/my/project\' archive_formats = [\'gztar\', \'zip\'] additional_files = [\'docs/\', \'examples/\'] create_source_distribution(project_root, archive_formats, additional_files) ``` **Expected Output:** The function should generate source distribution archives in specified formats under `/path/to/my/project/dist`. **Constraints:** - The function should handle errors gracefully, providing meaningful error messages. - Assume the environment has Python 3.10 and necessary libraries installed. **Hints:** - Use the `os` and `shutil` modules to handle file operations. - Use the `subprocess` module to run the `python setup.py sdist` command. - Ensure `MANIFEST.in` correctly specifies additional files and follows the format rules.","solution":"import os import shutil import subprocess def create_source_distribution(project_root: str, archive_formats: list, additional_files: list) -> None: Creates source distribution archives for a Python project. :param project_root: The root directory of the project. :param archive_formats: A list of desired archive formats (e.g., [\'gztar\', \'zip\']). :param additional_files: A list of additional files or directories to include in the distribution (e.g., [\'docs/\', \'examples/\']). setup_py_path = os.path.join(project_root, \'setup.py\') manifest_in_path = os.path.join(project_root, \'MANIFEST.in\') # Create setup.py if it doesn\'t exist if not os.path.exists(setup_py_path): with open(setup_py_path, \'w\') as setup_file: setup_file.write( from setuptools import setup, find_packages setup( name=\\"my_project\\", version=\\"0.1\\", packages=find_packages(), ) ) # Create or update MANIFEST.in with open(manifest_in_path, \'w\') as manifest_file: manifest_file.write(\'include README.mdn\') # Including a common file for item in additional_files: manifest_file.write(f\'include {item}n\') # Create source distributions based on the archive formats for fmt in archive_formats: command = [\'python\', \'setup.py\', \'sdist\', f\'--formats={fmt}\'] subprocess.run(command, cwd=project_root, check=True) print(\\"Source distributions created successfully.\\") # Example usage: # create_source_distribution(\'/path/to/my/project\', [\'gztar\', \'zip\'], [\'docs/\', \'examples/\'])"},{"question":"# Iterator and Generator Handling in Python Objective: Implement a Python class `CustomIterator` that follows the iterator protocol. Additionally, implement a generator function `custom_generator` that conforms to the same protocol. The `CustomIterator` should allow iteration over a list of integers, and the generator should perform the same functionality. Furthermore, create a utility function `async_iterate` to work with an async iterator and return all values in a list. Requirements: 1. **CustomIterator Class:** - **Input:** A list of integers. - **Output:** Implements an iterator that returns each integer in the list one at a time. - **Constraints:** Handle the case where the list is empty. - **Methods:** - `__init__(self, data: list)` - `__iter__(self)` - `__next__(self)` 2. **Custom Generator (`custom_generator`):** - **Input:** A list of integers. - **Output:** Yields each integer in the list one at a time. - **Constraints:** Handle the case where the list is empty. - **Function:** - `custom_generator(data: list)` 3. **Async Iteration (`async_iterate`):** - **Input:** An async iterator. - **Output:** A list of all values produced by the async iterator. - **Constraints:** Handle possible exceptions during iteration. - **Function:** - `async_iterate(async_iter)` - Must use `async for` to iterate. Example Usage: ```python # CustomIterator data = [1, 2, 3, 4] iterator = CustomIterator(data) for item in iterator: print(item) # Output: 1, 2, 3, 4 # Custom Generator gen = custom_generator(data) for item in gen: print(item) # Output: 1, 2, 3, 4 # Async Iteration import asyncio class AsyncIterator: async def __aiter__(self): self.data = [1, 2, 3, 4] self.index = 0 return self async def __anext__(self): if self.index < len(self.data): self.index += 1 return self.data[self.index - 1] else: raise StopAsyncIteration async def main(): async_iter = AsyncIterator() result = await async_iterate(async_iter) print(result) # Output: [1, 2, 3, 4] asyncio.run(main()) ``` Evaluation: - Ensure the `CustomIterator` correctly implements the iterator protocol (`__iter__` and `__next__` methods). - The `custom_generator` function should properly yield values from the input list one by one. - The `async_iterate` function should efficiently collect all items from the async iterator and return them as a list, handling possible exceptions cleanly.","solution":"class CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.data): result = self.data[self.index] self.index += 1 return result else: raise StopIteration def custom_generator(data): for item in data: yield item async def async_iterate(async_iter): result = [] async for value in async_iter: result.append(value) return result"},{"question":"**Objective**: Demonstrate your understanding of the `xdrlib` module for encoding and decoding XDR data by implementing packing and unpacking for a custom data structure. # Problem Statement You are required to implement a pair of functions, `pack_custom_data` and `unpack_custom_data`, to serialize and deserialize a custom data structure using the `xdrlib` module. The custom data structure is a dictionary with the following format: ```python custom_data = { \\"name\\": str, # a string representing a name \\"age\\": int, # an integer representing an age \\"skills\\": list, # a list of strings representing skills \\"details\\": bytes # bytes data representing additional details } ``` Function Signatures: ```python from xdrlib import Packer, Unpacker, ConversionError def pack_custom_data(data: dict) -> bytes: Packs the custom data structure into XDR format. Args: data (dict): The custom data structure to pack. Returns: bytes: The XDR packed data. Raises: ValueError: If the input data is invalid or packing fails. pass def unpack_custom_data(xdr_data: bytes) -> dict: Unpacks the XDR formatted data into the custom data structure. Args: xdr_data (bytes): The XDR packed data. Returns: dict: The unpacked custom data structure. Raises: ValueError: If the XDR data is invalid or unpacking fails. pass ``` # Requirements: 1. The `pack_custom_data` function must pack the given dictionary into bytes using the `xdrlib.Packer` class. 2. The `unpack_custom_data` function must unpack the given bytes back into the dictionary format using the `xdrlib.Unpacker` class. 3. You must handle exceptions and invalid data gracefully by raising `ValueError` with an appropriate message. 4. Ensure proper alignment and padding where necessary to conform to XDR specifications. # Example Usage: ```python data = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"skills\\": [\\"Python\\", \\"Data Analysis\\"], \\"details\\": b\'x00x01x02\' } # Pack the custom data packed_data = pack_custom_data(data) # Unpack the data unpacked_data = unpack_custom_data(packed_data) # Verify the unpacked data matches the original data assert data == unpacked_data ``` # Constraints: - The `name` string should not exceed 100 characters. - The `skills` list should not contain more than 10 skills. - Each skill in the `skills` list should be a string not exceeding 50 characters. - The `details` bytes data should not exceed 256 bytes. # Tips: - Use the `Packer` and `Unpacker` methods for packing and unpacking strings, integers, lists, and bytes. - Ensure to handle null byte padding for alignment as required by the XDR format. Good luck!","solution":"from xdrlib import Packer, Unpacker, ConversionError def pack_custom_data(data: dict) -> bytes: Packs the custom data structure into XDR format. try: if (not isinstance(data[\\"name\\"], str) or not isinstance(data[\\"age\\"], int) or not isinstance(data[\\"skills\\"], list) or not isinstance(data[\\"details\\"], bytes)): raise ValueError(\\"Invalid data type in input dictionary\\") if len(data[\\"name\\"]) > 100: raise ValueError(\\"Name exceeds maximum length of 100 characters\\") if len(data[\\"skills\\"]) > 10: raise ValueError(\\"Skills list exceeds maximum length of 10 items\\") if any(len(skill) > 50 for skill in data[\\"skills\\"]): raise ValueError(\\"A skill exceeds maximum length of 50 characters\\") if len(data[\\"details\\"]) > 256: raise ValueError(\\"Details value exceeds maximum length of 256 bytes\\") p = Packer() p.pack_string(data[\\"name\\"].encode(\'utf-8\')) p.pack_int(data[\\"age\\"]) p.pack_int(len(data[\\"skills\\"])) for skill in data[\\"skills\\"]: p.pack_string(skill.encode(\'utf-8\')) p.pack_bytes(data[\\"details\\"]) return p.get_buffer() except (ConversionError, KeyError) as e: raise ValueError(f\\"Packing error occurred: {e}\\") def unpack_custom_data(xdr_data: bytes) -> dict: Unpacks the XDR formatted data into the custom data structure. try: u = Unpacker(xdr_data) name = u.unpack_string().decode(\'utf-8\') age = u.unpack_int() skills_length = u.unpack_int() skills = [] for _ in range(skills_length): skills.append(u.unpack_string().decode(\'utf-8\')) details = u.unpack_bytes() return { \\"name\\": name, \\"age\\": age, \\"skills\\": skills, \\"details\\": details } except (ConversionError, Exception) as e: raise ValueError(f\\"Unpacking error occurred: {e}\\")"},{"question":"Question: Using `concurrent.futures` for Web Scraping # Objective: Design a web scraper that concurrently fetches the content of multiple web pages. You will demonstrate your understanding of the `concurrent.futures` module by writing a Python function that uses `ThreadPoolExecutor` to parallelize the fetching of web pages. # Function Signature: ```python def scrape_web_pages(urls: list[str]) -> dict[str, str]: pass ``` # Parameters: - `urls` (list of strings): A list of URLs to scrape. # Returns: - (dict of strings): A dictionary where each key is a URL from the input list and the corresponding value is the content of the web page at that URL. If a URL could not be fetched, its value should be `None`. # Constraints: - You must use `concurrent.futures.ThreadPoolExecutor` to fetch the web pages concurrently. - You should handle exceptions gracefully: if fetching a URL fails, the result for that URL should be `None`. - Assume there are no more than 100 URLs in the input list. # Example Usage: ```python urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://nonexistentdomain.xyz\\" ] results = scrape_web_pages(urls) print(results) # Expected output might look like: # { # \\"http://example.com\\": \\"<html>Content of example.com</html>\\", # \\"http://example.org\\": \\"<html>Content of example.org</html>\\", # \\"http://nonexistentdomain.xyz\\": None # } ``` # Performance Requirements: - Your function should handle the concurrent fetching of web pages efficiently, aiming to minimize the total execution time compared to sequential fetching. # Hints: - Use the `requests` library to fetch the contents of a web page. - Use the `concurrent.futures.as_completed()` function to process tasks as they complete. # Solution Template: ```python import requests from concurrent.futures import ThreadPoolExecutor, as_completed def fetch_url(url): try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException: return None def scrape_web_pages(urls: list[str]) -> dict[str, str]: result = {} with ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: result[url] = future.result() except Exception: result[url] = None return result ``` In this question, students need to demonstrate their understanding of the following: - Creating and managing a `ThreadPoolExecutor` for concurrent execution. - Submitting tasks to the executor and handling their completion. - Utilizing futures to handle the results of asynchronous tasks. - Implementing error handling for network requests.","solution":"import requests from concurrent.futures import ThreadPoolExecutor, as_completed def fetch_url(url): Fetch the content of the URL. Parameters: - url (str): The URL to fetch. Returns: - str: The content of the web page if successful, None otherwise. try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException: return None def scrape_web_pages(urls: list[str]) -> dict[str, str]: Concurrently fetch the content of multiple web pages. Parameters: - urls (list): A list of URLs to scrape. Returns: - dict: A dictionary where each key is a URL from the input list and the corresponding value is the content of the web page at that URL. If a URL could not be fetched, its value will be None. result = {} with ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: result[url] = future.result() except Exception: result[url] = None return result"},{"question":"You are tasked with optimizing a computational function and demonstrating its performance improvement using the `cProfile` and `pstats` modules. The function `fibonacci` computes the n-th Fibonacci number using a straightforward recursive method, which is inefficient for large `n`. You will implement a more efficient version using memoization, profile both versions, and analyze the performance differences. Function Specification 1. Implement the following functions: - `fibonacci_recursive(n: int) -> int`: Computes the n-th Fibonacci number using a simple recursive approach. - `fibonacci_memoized(n: int, memo: dict = {}) -> int`: Computes the n-th Fibonacci number using memoization, which stores computed values to avoid redundant calculations. 2. Profile both functions using `cProfile` and save the profiling results to separate files: - Profile `fibonacci_recursive` for `n=30` and save the results in `recursive_profile.prof`. - Profile `fibonacci_memoized` for `n=30` and save the results in `memoized_profile.prof`. 3. Use the `pstats` module to analyze and compare the profiling results: - Sort the results by cumulative time. - Print the statistics for both profiles, restricting the output to the top 10 functions in each case. Input and Output Format 1. **Function Definitions:** ```python def fibonacci_recursive(n: int) -> int: # Your implementation here def fibonacci_memoized(n: int, memo: dict = {}) -> int: # Your implementation here ``` 2. **Profiling:** ```python import cProfile def profile_functions(): # Profile fibonacci_recursive and save results cProfile.run(\'fibonacci_recursive(30)\', \'recursive_profile.prof\') # Profile fibonacci_memoized and save results cProfile.run(\'fibonacci_memoized(30)\', \'memoized_profile.prof\') ``` 3. **Analysis using pstats:** ```python import pstats def analyze_profiler_results(): # Analyze and print results for recursive implementation p = pstats.Stats(\'recursive_profile.prof\') p.strip_dirs().sort_stats(\'cumulative\').print_stats(10) print(\\"n\\" + \\"=\\"*80 + \\"n\\") # Analyze and print results for memoized implementation q = pstats.Stats(\'memoized_profile.prof\') q.strip_dirs().sort_stats(\'cumulative\').print_stats(10) ``` Constraints - None explicitly, but ensure the functions handle typical edge cases appropriately. Performance Requirements - Ensure the profiling and analysis scripts execute efficiently and produce a clear comparison between the two methods. This question requires demonstrating the use of the `cProfile` and `pstats` modules for performance analysis and optimization of Python code, showcasing a clear understanding of the concepts presented in the documentation.","solution":"def fibonacci_recursive(n: int) -> int: Computes the n-th Fibonacci number using a simple recursive approach. if n <= 0: return 0 elif n == 1: return 1 else: return fibonacci_recursive(n-1) + fibonacci_recursive(n-2) def fibonacci_memoized(n: int, memo: dict = {}) -> int: Computes the n-th Fibonacci number using a memoization approach. if n in memo: return memo[n] if n <= 0: return 0 elif n == 1: return 1 else: memo[n] = fibonacci_memoized(n-1, memo) + fibonacci_memoized(n-2, memo) return memo[n] import cProfile def profile_functions(): # Profile fibonacci_recursive and save results cProfile.run(\'fibonacci_recursive(30)\', \'recursive_profile.prof\') # Profile fibonacci_memoized and save results cProfile.run(\'fibonacci_memoized(30)\', \'memoized_profile.prof\') import pstats def analyze_profiler_results(): # Analyze and print results for recursive implementation p = pstats.Stats(\'recursive_profile.prof\') p.strip_dirs().sort_stats(\'cumulative\').print_stats(10) print(\\"n\\" + \\"=\\"*80 + \\"n\\") # Analyze and print results for memoized implementation q = pstats.Stats(\'memoized_profile.prof\') q.strip_dirs().sort_stats(\'cumulative\').print_stats(10)"},{"question":"# PyTorch Dynamic Control Flow with `torch.cond` In this task, you need to implement a PyTorch module that uses `torch.cond` to alter its behavior based on the input tensor. Specifically, we will define a neural network that conditionally applies different transformations based on the input tensor\'s sum. Task 1. Implement a class `ConditionalTransform` that inherits from `torch.nn.Module`. 2. The module should take as input a tensor `x` and, based on the sum of its elements, apply different transformations: - If the sum of elements in `x` is greater than a specified threshold, apply the transformation `x -> 2 * x` and then a ReLU activation. - Otherwise, apply the transformation `x -> x / 2` and then a Sigmoid activation. 3. Use `torch.cond` to construct the conditional control flow. Function Signature ```python class ConditionalTransform(torch.nn.Module): def __init__(self, threshold: float): super(ConditionalTransform, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: # Implement the conditional logic here using torch.cond pass ``` Example Usage ```python import torch model = ConditionalTransform(threshold=4.0) x1 = torch.tensor([1.0, 1.0, 1.0]) x2 = torch.tensor([5.0, 5.0, 5.0]) output1 = model(x1) output2 = model(x2) print(output1) print(output2) ``` In the above example: - For `x1`, since the sum is 3.0 (which is less than the threshold), the expected output will be `(x1 / 2).sigmoid()`. - For `x2`, since the sum is 15.0 (which is greater than the threshold), the expected output will be `(2 * x2).relu()`. Additional Information - **Input Format**: The input to the forward method will be a single tensor `x` of any shape. - **Output Format**: The output should be a tensor of the same shape as `x`. - **Constraints**: Ensure the solution uses `torch.cond` for implementing the conditional flow.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class ConditionalTransform(nn.Module): def __init__(self, threshold: float): super(ConditionalTransform, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: condition = torch.sum(x) > self.threshold def true_fn(x): return F.relu(2 * x) def false_fn(x): return torch.sigmoid(x / 2) # Using `torch.cond` style through a workaround with `torch.where` # Note: torch.cond doesn\'t exist in PyTorch currently, this is a simulated approach output = torch.where(condition, true_fn(x), false_fn(x)) return output"},{"question":"**Objective**: Demonstrate understanding of the `__future__` module and concepts related to future features in Python. **Task**: Write a function `future_features_status` that inspects the `__future__` module and returns a dictionary summarizing the status of each future feature. Each key in the dictionary should be the name of the feature, and the value should be another dictionary with the following keys: - `optional_in` (tuple): The version in which the feature was first introduced. - `mandatory_in` (tuple): The version in which the feature became mandatory (if the feature has been made mandatory) or `None`. - `effect` (string): A brief description of the feature. **Input**: - No input is required for this function. All necessary information is extracted from the `__future__` module. **Output**: - Returns a dictionary in the format described above. **Example**: ```python result = future_features_status() expected_result = { \\"nested_scopes\\": { \\"optional_in\\": (2, 1, 0, \\"beta\\", 1), \\"mandatory_in\\": (2, 2, 0, \\"final\\", 0), \\"effect\\": \\"PEP 227: Statically Nested Scopes\\" }, \\"generators\\": { \\"optional_in\\": (2, 2, 0, \\"alpha\\", 1), \\"mandatory_in\\": (2, 3, 0, \\"final\\", 0), \\"effect\\": \\"PEP 255: Simple Generators\\" }, # ... more features ... } assert result == expected_result ``` **Constraints**: - You must use the `__future__` module to obtain the information programmatically. - You should define the dictionary statically based on the information provided in the documentation. **Performance Requirements**: - The function should efficiently retrieve the necessary data and construct the dictionary. **Additional Notes**: - Focus on using the attributes and methods of the `_Feature` class to retrieve the necessary information. - Ensure that the function handles cases where the `mandatory_in` attribute might be `None`.","solution":"import __future__ def future_features_status(): Inspects the __future__ module and returns a dictionary summarizing the status of each future feature. features = { \\"annotations\\": { \\"optional_in\\": __future__.annotations.optional, \\"mandatory_in\\": __future__.annotations.mandatory, \\"effect\\": \\"PEP 563: Postponed Evaluation of Annotations\\" }, \\"absolute_import\\": { \\"optional_in\\": __future__.absolute_import.optional, \\"mandatory_in\\": __future__.absolute_import.mandatory, \\"effect\\": \\"PEP 328: Imports: Multi-Line and Absolute/Relative\\" }, \\"division\\": { \\"optional_in\\": __future__.division.optional, \\"mandatory_in\\": __future__.division.mandatory, \\"effect\\": \\"PEP 238: Changing the Division Operator\\" }, \\"generators\\": { \\"optional_in\\": __future__.generators.optional, \\"mandatory_in\\": __future__.generators.mandatory, \\"effect\\": \\"PEP 255: Simple Generators\\" }, \\"unicode_literals\\": { \\"optional_in\\": __future__.unicode_literals.optional, \\"mandatory_in\\": __future__.unicode_literals.mandatory, \\"effect\\": \\"PEP 3112: Byte Literals in Python 3000\\" }, \\"print_function\\": { \\"optional_in\\": __future__.print_function.optional, \\"mandatory_in\\": __future__.print_function.mandatory, \\"effect\\": \\"PEP 3105: Make print a function\\" }, \\"nested_scopes\\": { \\"optional_in\\": __future__.nested_scopes.optional, \\"mandatory_in\\": __future__.nested_scopes.mandatory, \\"effect\\": \\"PEP 227: Statically Nested Scopes\\" }, \\"with_statement\\": { \\"optional_in\\": __future__.with_statement.optional, \\"mandatory_in\\": __future__.with_statement.mandatory, \\"effect\\": \\"PEP 343: The \'with\' Statement\\" }, \\"barry_as_FLUFL\\": { \\"optional_in\\": __future__.barry_as_FLUFL.optional, \\"mandatory_in\\": __future__.barry_as_FLUFL.mandatory, \\"effect\\": \\"PEP 401: BDFL\'s â€˜barry as BDFLâ€™ Rule\\" }, \\"unicode_literals\\": { \\"optional_in\\": __future__.unicode_literals.optional, \\"mandatory_in\\": __future__.unicode_literals.mandatory, \\"effect\\": \\"PEP 3112: Byte Literals in Python 3000\\" }, \\"print_function\\": { \\"optional_in\\": __future__.print_function.optional, \\"mandatory_in\\": __future__.print_function.mandatory, \\"effect\\": \\"PEP 3105: Make print a function\\" }, \\"nested_scopes\\": { \\"optional_in\\": __future__.nested_scopes.optional, \\"mandatory_in\\": __future__.nested_scopes.mandatory, \\"effect\\": \\"PEP 227: Statically Nested Scopes\\" }, } return features"},{"question":"Objective: To test your understanding and ability to manipulate and analyze data using the pandas library, you will implement a function that performs several tasks sequentially on a DataFrame of employee data. Problem Statement: You are given an employee dataset that contains information about employees including their department, role, salary, and employment start year. Your task is to write a function `process_employee_data` that performs the following operations: 1. **Create DataFrame**: Create a DataFrame using the given employee data. 2. **Data Cleaning**: Handle any missing data in the `salary` column by replacing NaN values with the median salary of the corresponding `department`. 3. **Generate New Features**: Add two new columns: - `years_with_company`: which is calculated as the current year (2023) minus the `start_year`. - `high_earner`: a boolean column indicating whether the employee\'s salary is above the department\'s median salary. 4. **Data Aggregation**: Calculate the average salary per `role` and return this as a dictionary. 5. **Department Summary**: Create a DataFrame that summarizes the following for each department: - Total number of employees (`num_employees`) - Average salary (`avg_salary`) - Total payroll (`total_payroll` which is the sum of all salaries in that department) Use the following input format for the employee data: ```python employee_data = { \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'department\': [\'HR\', \'Engineering\', \'HR\', \'Engineering\'], \'role\': [\'Manager\', \'Developer\', \'Analyst\', \'Developer\'], \'salary\': [90000, 120000, None, 110000], \'start_year\': [2015, 2016, 2017, 2015] } ``` Expected Output: The `process_employee_data` function should return a tuple containing: 1. The average salary per role as a dictionary. 2. The department summary DataFrame. Function Signature: ```python import pandas as pd from typing import Tuple, Dict def process_employee_data(employee_data: Dict[str, list]) -> Tuple[Dict[str, float], pd.DataFrame]: pass ``` Constraints: - The dataset will always contain the columns provided in the input format. - The salary values will be non-negative integers or None. - The start year will be a valid year in the past up to the current year. Example: ```python employee_data = { \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'department\': [\'HR\', \'Engineering\', \'HR\', \'Engineering\'], \'role\': [\'Manager\', \'Developer\', \'Analyst\', \'Developer\'], \'salary\': [90000, 120000, None, 110000], \'start_year\': [2015, 2016, 2017, 2015] } avg_salary_per_role, department_summary_df = process_employee_data(employee_data) print(avg_salary_per_role) # Expected Output: {\'Manager\': 90000.0, \'Developer\': 115000.0, \'Analyst\': 90000.0} print(department_summary_df) # Expected Output: A DataFrame with columns [\'department\', \'num_employees\', \'avg_salary\', \'total_payroll\'] ``` Notes: - Ensure your code is readable with appropriate comments. - Handle edge cases such as all salaries being NaN in a department gracefully. - Use vectorized operations and avoid explicit Python loops where possible for better performance.","solution":"import pandas as pd from typing import Tuple, Dict def process_employee_data(employee_data: Dict[str, list]) -> Tuple[Dict[str, float], pd.DataFrame]: # Create DataFrame from the given employee data df = pd.DataFrame(employee_data) # Fill NaN values in the salary column with the median salary of the corresponding department df[\'salary\'] = df.groupby(\'department\')[\'salary\'].transform( lambda x: x.fillna(x.median()) ) # Current year for calculating years with company current_year = 2023 # Add new column \'years_with_company\' df[\'years_with_company\'] = current_year - df[\'start_year\'] # Add new column \'high_earner\' df[\'high_earner\'] = df.groupby(\'department\')[\'salary\'].transform( lambda x: x > x.median() ) # Calculate average salary per role avg_salary_per_role = df.groupby(\'role\')[\'salary\'].mean().to_dict() # Create department summary DataFrame department_summary = df.groupby(\'department\').agg( num_employees=(\'name\', \'size\'), avg_salary=(\'salary\', \'mean\'), total_payroll=(\'salary\', \'sum\') ).reset_index() return avg_salary_per_role, department_summary"},{"question":"<|Analysis Begin|> The `heapq` module in Python provides an efficient implementation of the heap queue algorithm, also known as the priority queue algorithm. Key features and functions discussed are: 1. **Heaps and Heapsort**: It leverages binary trees where each parent node is smaller or equal to its children, with the smallest element always being the root. 2. **Core Heap Functions**: - `heapq.heappush(heap, item)`: Pushes an item onto the heap, maintaining the heap invariant. - `heapq.heappop(heap)`: Pops and returns the smallest item from the heap. - `heapq.heappushpop(heap, item)`: Pushes an item on the heap, then pops and returns the smallest item. - `heapq.heapify(x)`: Transforms a list into a heap in-place in linear time. - `heapq.heapreplace(heap, item)`: Pops and returns the smallest item from the heap and pushes a new item. 3. **Utility Functions**: - `heapq.merge(iterables, key=None, reverse=False)`: Merges multiple sorted inputs into a single sorted output, returning an iterator. - `heapq.nlargest(n, iterable, key=None)`: Returns a list of the `n` largest elements from the dataset. - `heapq.nsmallest(n, iterable, key=None)`: Returns a list of the `n` smallest elements from the dataset. 4. **Advanced Use**: Implementing a priority queue with tuple comparisons, handling task priority updates, and removing tasks from the heap. Given the detailed explanations and examples in the documentation, a coding question can be designed around implementing a priority queue with additional constraints and scenarios. <|Analysis End|> <|Question Begin|> # Advanced Priority Queue Implementation You are required to implement a custom priority queue using the `heapq` module in Python. The priority queue should support the following operations: 1. **Add a Task**: Add a task with a given priority. 2. **Update Task Priority**: Update the priority of an existing task. 3. **Remove a Task**: Remove a specific task from the queue. 4. **Pop Task**: Remove and return the task with the highest priority (i.e., the smallest numerical priority). To address the requirements, you need to handle cases of updating task priorities and removing tasks efficiently without violating the heap invariant. Additionally, use a counter to maintain the order of tasks with the same priority. Methods to Implement 1. `add_task(task: str, priority: int) -> None`: Add a new task or update the priority of an existing task. 2. `remove_task(task: str) -> None`: Remove a given task from the priority queue. 3. `pop_task() -> str`: Remove and return the task with the highest priority. 4. `__init__()`: Initialize the data structures required for the priority queue. Constraints - The task names will be unique. - If `remove_task` or `pop_task` is called and the task does not exist, raise a `KeyError`. - Priorities are integers where a smaller number indicates higher priority. Expected Input and Output ```python class PriorityQueue: def __init__(self): pass def add_task(self, task: str, priority: int) -> None: pass def remove_task(self, task: str) -> None: pass def pop_task(self) -> str: pass ``` Example Usage: ```python pq = PriorityQueue() pq.add_task(\\"task1\\", 2) pq.add_task(\\"task2\\", 1) pq.add_task(\\"task3\\", 3) assert pq.pop_task() == \\"task2\\" # \\"task2\\" has the highest priority pq.add_task(\\"task1\\", 0) # Update task1\'s priority assert pq.pop_task() == \\"task1\\" pq.remove_task(\\"task3\\") try: pq.pop_task() except KeyError: print(\\"Priority queue is empty\\") ``` **Note**: Your implementation should make use of the functions and features provided by the `heapq` module.","solution":"import heapq from typing import Tuple, Dict class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder: Dict[str, Tuple[int, int, str]] = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = 0 # unique sequence count def add_task(self, task: str, priority: int) -> None: Add a new task or update the priority of an existing task. if task in self.entry_finder: self.remove_task(task) count = self.counter entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) self.counter += 1 def remove_task(self, task: str) -> None: Mark an existing task as REMOVED. Raise KeyError if not found. entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the lowest priority task. Raise KeyError if empty. while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"# URL Parser and Reconstructor Objective: You are tasked with creating a utility that can parse and reconstruct URLs, as well as manipulate query components. Implement a class `URLUtility` that provides methods to: 1. Parse a given URL and return its components. 2. Construct a URL from its components. 3. Extract and manipulate query parameters from the URL. Requirements: 1. **parse_url(self, url: str) -> dict**: - Input: A string representing a URL. - Output: A dictionary containing the URL components: scheme, netloc, path, params, query, and fragment. - Example: ```python url = \\"http://docs.python.org:80/3/library/urllib.parse.html?highlight=params#url-parsing\\" result = URLUtility().parse_url(url) # result should be: # { # \'scheme\': \'http\', # \'netloc\': \'docs.python.org:80\', # \'path\': \'/3/library/urllib.parse.html\', # \'params\': \'\', # \'query\': \'highlight=params\', # \'fragment\': \'url-parsing\' # } ``` 2. **construct_url(self, components: dict) -> str**: - Input: A dictionary containing URL components: scheme, netloc, path, params, query, and fragment. - Output: A string representing the constructed URL. - Example: ```python components = { \'scheme\': \'http\', \'netloc\': \'docs.python.org:80\', \'path\': \'/3/library/urllib.parse.html\', \'params\': \'\', \'query\': \'highlight=params\', \'fragment\': \'url-parsing\' } result = URLUtility().construct_url(components) # result should be \\"http://docs.python.org:80/3/library/urllib.parse.html?highlight=params#url-parsing\\" ``` 3. **extract_query_params(self, url: str) -> dict**: - Input: A string representing a URL. - Output: A dictionary where query parameters are the keys and their values are lists of parameter values. - Example: ```python url = \\"http://example.com/page?arg1=value1&arg2=value2&arg2=value3\\" result = URLUtility().extract_query_params(url) # result should be: # { # \'arg1\': [\'value1\'], # \'arg2\': [\'value2\', \'value3\'] # } ``` 4. **manipulate_query_params(self, url: str, params_to_add: dict = {}, params_to_remove: list = []) -> str**: - Input: - `url`: A string representing a URL. - `params_to_add`: A dictionary where keys are the parameter names and values are lists of parameter values to add. - `params_to_remove`: A list of parameter names to remove from the URL. - Output: A string representing the modified URL with added and/or removed query parameters. - Example: ```python url = \\"http://example.com/page?arg1=value1&arg2=value2&arg2=value3\\" params_to_add = {\'arg3\': [\'value4\']} params_to_remove = [\'arg1\'] result = URLUtility().manipulate_query_params(url, params_to_add, params_to_remove) # result should be \\"http://example.com/page?arg2=value2&arg2=value3&arg3=value4\\" ``` Implementation Constraints: - Use the `urllib.parse` module functions wherever needed. - Your implementation should handle edge cases, such as missing components or empty query parameters. - Ensure the methods are efficient and concise. Performance Requirements: - URL parsing and manipulation methods should have O(n) complexity, where n is the length of the URL string. - Avoid unnecessary recomputation by caching intermediate results if needed. Notes: - Focus on handling ASCII and Unicode characters properly. - Ensure that your code adheres to good style and safety practices.","solution":"from urllib.parse import urlparse, urlunparse, parse_qs, urlencode class URLUtility: def parse_url(self, url: str) -> dict: parsed = urlparse(url) return { \'scheme\': parsed.scheme, \'netloc\': parsed.netloc, \'path\': parsed.path, \'params\': parsed.params, \'query\': parsed.query, \'fragment\': parsed.fragment } def construct_url(self, components: dict) -> str: return urlunparse(( components.get(\'scheme\', \'\'), components.get(\'netloc\', \'\'), components.get(\'path\', \'\'), components.get(\'params\', \'\'), components.get(\'query\', \'\'), components.get(\'fragment\', \'\') )) def extract_query_params(self, url: str) -> dict: parsed = urlparse(url) return parse_qs(parsed.query) def manipulate_query_params(self, url: str, params_to_add: dict = {}, params_to_remove: list = []) -> str: uri = urlparse(url) query_params = parse_qs(uri.query) for param in params_to_remove: if param in query_params: del query_params[param] for key, values in params_to_add.items(): query_params[key] = values new_query = urlencode(query_params, doseq=True) new_url = urlunparse(( uri.scheme, uri.netloc, uri.path, uri.params, new_query, uri.fragment )) return new_url"},{"question":"**Question: Implementing Secure Data Handling in Python** Using the information provided in the documentation of the `hashlib`, `hmac`, and `secrets` modules, develop a Python function that securely handles user passwords and authentication tokens. # Task You need to create two functions: 1. `hash_password(password: str, salt: str) -> str` - This function will securely hash a user\'s password using the `hashlib` library. - The input `password` is a plain text password. - The input `salt` is a string that should be combined with the password before hashing. - The function should return the hashed password as a hexadecimal string. 2. `generate_token(length: int = 16) -> str` - This function will generate a secure random authentication token using the `secrets` library. - The input `length` specifies the number of bytes to be used for the token. - The function should return the generated token as a hexadecimal string. # Constraints - The `hash_password` function must use a strong hashing algorithm such as SHA-256 from the `hashlib` library. - The `generate_token` function must ensure that the generated tokens are suitable for cryptographic use. # Example ```python def hash_password(password: str, salt: str) -> str: # Your implementation goes here def generate_token(length: int = 16) -> str: # Your implementation goes here # Example usage: hashed_pw = hash_password(\\"mypassword123\\", \\"randomsalt\\") print(hashed_pw) # Example output: \'5e884898da28047151d0e56f8dc6292773603d0d6aabbddfd1faa236c8327dc5\' token = generate_token(16) print(token) # Example output: \'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\' ``` # Performance Requirements - You should ensure that the `hash_password` function runs efficiently even for a large number of passwords. - The `generate_token` function should produce tokens quickly, providing prompt responses. # Notes - Make sure to import the necessary modules (`hashlib` and `secrets`) at the beginning of your code. - Handle any potential errors gracefully and ensure your code is robust and secure.","solution":"import hashlib import secrets def hash_password(password: str, salt: str) -> str: Hashes the password with the provided salt using SHA-256. Returns the hashed password as a hexadecimal string. # Combine the password and salt combined = password + salt # Create a SHA-256 hash object hash_obj = hashlib.sha256() # Update the hash object with the combined string encoded in UTF-8 hash_obj.update(combined.encode(\'utf-8\')) # Return the hex representation of the hash return hash_obj.hexdigest() def generate_token(length: int = 16) -> str: Generates a secure random authentication token of the specified byte length. Returns the token as a hexadecimal string. # Generate a secure random token token = secrets.token_hex(length) return token"},{"question":"# Question: Create Advanced Seaborn Plots **Objective**: This question is designed to assess your ability to utilize seabornâ€™s advanced plotting functionalities, focusing on creating complex, multi-layered visualizations with various statistical summaries and plot enhancements. **Task**: 1. Load the \\"tips\\" dataset from seaborn. 2. Create a plot that visualizes the relationship between `total_bill` and `day`. 3. Add layers to the plot using the following elements: - `Dots`: To show individual data points. - `Jitter`: To apply slight adjustments to the position of the points, avoiding overlap. - `Range`: To display a range summary within each group. - `Perc([10, 90])`: To show the 10th and 90th percentiles of the distribution. 4. Make sure to apply shifts appropriately to avoid overlapping of different layers. **Input**: - None (all datasets used are loaded within the code). **Output**: - A seaborn plot object visualizing the specified relationships and enhancements. **Constraints**: - You must use seaborn version >= 0.11.0. - The plot should be created using seabornâ€™s object-oriented interface (`so.Plot`). **Example Output**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Creating the plot ( so.Plot(tips, \\"day\\", \\"total_bill\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([10, 90]), so.Shift(x=0.2)) ) ``` With this task, you should demonstrate your ability to combine various plot elements and apply statistical transformations to create insightful visualizations.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_advanced_seaborn_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Creating the plot plot = ( so.Plot(tips, \\"day\\", \\"total_bill\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([10, 90]), so.Shift(x=0.2)) ) return plot"},{"question":"Implementing and Testing Tensor Views in PyTorch **Objective**: Create a function that demonstrates the concept of tensor views in PyTorch by performing a series of operations on a given tensor and verifying the shared underlying data. **Problem Statement**: Implement a function `tensor_views_demo` that performs the following tasks: 1. Create a 3x3 tensor with values from 1 to 9. 2. Create a view of this tensor by flattening it into a 1-dimensional tensor. 3. Modify the flattened tensor such that all its elements are multiplied by 2. 4. Retrieve a 3x3 view again from the modified 1-dimensional tensor. 5. Verify that the original 3x3 tensor is also modified accordingly (since the view should share the same data). 6. Check whether the final 3x3 tensor is contiguous and, if not, make it contiguous. **Function Signature**: ```python def tensor_views_demo(): This function performs specific tensor view operations and returns a dictionary containing: - \'original_tensor\': The original 3x3 tensor created. - \'flattened_tensor\': The 1-dimensional view of the tensor. - \'modified_3x3_tensor\': The 3x3 tensor obtained from the modified 1-dimensional tensor. - \'is_contiguous\': A boolean indicating whether the modified 3x3 tensor is contiguous. - \'contiguous_tensor\': The contiguous version of the modified 3x3 tensor, if needed. pass ``` **Expected outputs**: - A dictionary with the following keys: - `original_tensor`: The original 3x3 tensor. - `flattened_tensor`: The 1-dimensional view of the tensor. - `modified_3x3_tensor`: The 3x3 tensor obtained from the modified 1-dimensional tensor, reflecting the data changes. - `is_contiguous`: A boolean indicating the contiguity of the `modified_3x3_tensor`. - `contiguous_tensor` (conditionally included): The contiguous version of the modified 3x3 tensor, if `is_contiguous` is False. **Constraints**: - Use only the provided tensor view operations (`view`, `reshape`, etc.) for creating views. - Do not use any tensor copying methods except for making tensors contiguous when required. **Example**: ```python output = tensor_views_demo() # Example format of the expected output: { \'original_tensor\': tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), \'flattened_tensor\': tensor([ 2, 4, 6, 8, 10, 12, 14, 16, 18]), \'modified_3x3_tensor\': tensor([[ 2, 4, 6], [ 8, 10, 12], [14, 16, 18]]), \'is_contiguous\': True, } ``` # Instructions 1. Implement the `tensor_views_demo` function. 2. Ensure the function adheres to the specified behavior and constraints. 3. Write appropriate comments and docstrings. **Performance Requirements**: - Efficiently perform view operations without unnecessary data copying. - Ensure the function is clear, modular, and well-documented.","solution":"import torch def tensor_views_demo(): This function performs specific tensor view operations and returns a dictionary containing: - \'original_tensor\': The original 3x3 tensor created. - \'flattened_tensor\': The 1-dimensional view of the tensor. - \'modified_3x3_tensor\': The 3x3 tensor obtained from the modified 1-dimensional tensor. - \'is_contiguous\': A boolean indicating whether the modified 3x3 tensor is contiguous. - \'contiguous_tensor\': The contiguous version of the modified 3x3 tensor, if needed. # 1. Create a 3x3 tensor with values from 1 to 9 original_tensor = torch.arange(1, 10).view(3, 3) # 2. Create a view of this tensor by flattening it into a 1-dimensional tensor flattened_tensor = original_tensor.view(-1) # 3. Modify the flattened tensor such that all its elements are multiplied by 2 flattened_tensor *= 2 # 4. Retrieve a 3x3 view again from the modified 1-dimensional tensor modified_3x3_tensor = flattened_tensor.view(3, 3) # 5. Verify that the original 3x3 tensor is also modified accordingly # 6. Check whether the final 3x3 tensor is contiguous and, if not, make it contiguous is_contiguous = modified_3x3_tensor.is_contiguous() contiguous_tensor = modified_3x3_tensor if is_contiguous else modified_3x3_tensor.contiguous() return { \'original_tensor\': original_tensor, \'flattened_tensor\': flattened_tensor, \'modified_3x3_tensor\': modified_3x3_tensor, \'is_contiguous\': is_contiguous, \'contiguous_tensor\': contiguous_tensor }"},{"question":"Question # Objective To assess your understanding of the zlib module for performing data compression and decompression tasks, you need to implement a set of functions utilizing this module. # Task 1. **Checksum Calculation**: Implement a function `calculate_checksums(data: bytes) -> Tuple[int, int]` that takes a bytes object `data` and returns a tuple containing its Adler-32 and CRC32 checksums. 2. **Compress and Decompress**: Implement a function `compress_and_decompress(data: bytes, level: int) -> Tuple[bytes, bytes]` that takes a bytes object `data` and an integer `level` specifying the compression level. The function should: - Compress the input data using `zlib.compress()` with the specified compression level. - Decompress the compressed data back to its original form using `zlib.decompress()`. - Return a tuple containing the compressed data and the decompressed data. 3. **Streaming Compression**: Implement a function `streaming_compression(data_list: List[bytes], level: int) -> bytes` that takes a list of bytes objects `data_list` and an integer `level` specifying the compression level. The function should: - Create a `Compress` object using `zlib.compressobj()`. - Compress each item in the `data_list` using the `compress()` method of the `Compress` object. - Finalize the compression using the `flush()` method of the `Compress` object. - Return a single bytes object containing the entire compressed data stream. # Constraints - The data for checksum calculations and compression/decompression tasks will be provided as bytes. - The compression level will be an integer between `-1` and `9`. - The decompressed data should match the original data exactly. # Example ```python data = b\\"Example of zlib compression\\" data_list = [b\\"Example \\", b\\"of \\", b\\"zlib \\", b\\"compression\\"] # Checksum Calculation adler32, crc32 = calculate_checksums(data) print(adler32, crc32) # Output: values of Adler-32 and CRC32 checksums # Compress and Decompress compressed, decompressed = compress_and_decompress(data, 6) print(compressed) # Output: compressed bytes object print(decompressed) # Output: b\\"Example of zlib compression\\" # Streaming Compression compressed_stream = streaming_compression(data_list, 6) print(compressed_stream) # Output: compressed stream of bytes ``` # Note You can assume that the `zlib` module functions as described in the provided documentation, and you don\'t need to handle other exceptions beyond those explicitly raised by the zlib library (e.g., `zlib.error`).","solution":"import zlib from typing import List, Tuple def calculate_checksums(data: bytes) -> Tuple[int, int]: Calculate and return the Adler-32 and CRC32 checksums of the given data. adler32 = zlib.adler32(data) crc32 = zlib.crc32(data) return (adler32, crc32) def compress_and_decompress(data: bytes, level: int) -> Tuple[bytes, bytes]: Compress the given data at the specified compression level and then decompress it. Return a tuple of (compressed data, decompressed data). compressed = zlib.compress(data, level) decompressed = zlib.decompress(compressed) return (compressed, decompressed) def streaming_compression(data_list: List[bytes], level: int) -> bytes: Compress a list of bytes objects using streaming compression at the specified level. Return the entire compressed data stream as a single bytes object. compressor = zlib.compressobj(level) compressed_data = b\\"\\" for data in data_list: compressed_data += compressor.compress(data) compressed_data += compressor.flush() return compressed_data"},{"question":"# Seaborn Color Palettes Coding Assessment You are given a dataset comprising scores of students in three different subjectsâ€”Math, Science, and English. Your task is to visualize this data using Seaborn, demonstrating your understanding of Seaborn color palettes. Specifically, you will need to create three visualizations using different types of color palettes and retrieve the hex codes for one of the palettes used. Dataset Format: The dataset is in CSV format with the following columns: - `Student`: The name of the student. - `Math`: The score in Math (integer between 0 and 100). - `Science`: The score in Science (integer between 0 and 100). - `English`: The score in English (integer between 0 and 100). Requirements: 1. **Visualization 1**: - Create a bar plot of the average scores for each subject using the `pastel` color palette. 2. **Visualization 2**: - Create a box plot showing the distribution of scores in each subject using the `husl` color palette with 5 colors. 3. **Visualization 3**: - Create a scatter plot showing the relationship between Math and Science scores, with the points colored by the `flare` continuous colormap. 4. **Hex Codes**: - Retrieve the hex codes for the `pastel` color palette used in Visualization 1 and print them. Input Format: - Path to the CSV file containing the dataset. Output Format: - Display the three visualizations. - Print the hex codes for the `pastel` color palette. Implementation Constraints: - Use only Seaborn and Matplotlib for generating visualizations. Example: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\'student_scores.csv\') # Visualization 1: Bar plot with the \'pastel\' palette avg_scores = data.mean().drop(\'Student\') sns.set_theme() sns.barplot(x=avg_scores.index, y=avg_scores.values, palette=\'pastel\') plt.show() # Visualization 2: Box plot with the \'husl\' palette sns.boxplot(data=data.drop(columns=[\'Student\']), palette=sns.color_palette(\'husl\', 5)) plt.show() # Visualization 3: Scatter plot with the \'flare\' colormap sns.scatterplot(data=data, x=\'Math\', y=\'Science\', palette=sns.color_palette(\'flare\', as_cmap=True)) plt.show() # Hex codes for \'pastel\' palette hex_codes = sns.color_palette(\'pastel\').as_hex() print(hex_codes) ``` Your submission should include the code implementing these requirements, ensuring accurate and visually appealing plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_data(file_path): # Load the dataset data = pd.read_csv(file_path) # Visualization 1: Bar plot with the \'pastel\' palette plt.figure(figsize=(10, 6)) avg_scores = data.drop(columns=[\'Student\']).mean() sns.barplot(x=avg_scores.index, y=avg_scores.values, palette=\'pastel\') plt.title(\'Average Scores by Subject\') plt.show() # Visualization 2: Box plot with the \'husl\' palette plt.figure(figsize=(10, 6)) sns.boxplot(data=data.drop(columns=[\'Student\']), palette=sns.color_palette(\'husl\', 5)) plt.title(\'Distribution of Scores in Each Subject\') plt.show() # Visualization 3: Scatter plot with the \'flare\' colormap plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'Math\', y=\'Science\', hue=\'English\', palette=sns.color_palette(\'flare\', as_cmap=True)) plt.title(\'Math vs Science Scores Colored by English Scores\') plt.show() # Retrieve hex codes for the \'pastel\' color palette hex_codes = sns.color_palette(\'pastel\').as_hex() print(\\"Hex codes for \'pastel\' color palette:\\", hex_codes) return hex_codes"},{"question":"Implement a custom function called `custom_diff_checker` that compares two given multi-line strings and produces a detailed human-readable diff output similar to the `unified_diff` format. # Function Signature: ```python def custom_diff_checker(str1: str, str2: str) -> str: ``` # Input: - `str1`: A string containing multiple lines (each line ending with a newline character). - `str2`: A string containing multiple lines (each line ending with a newline character). # Output: - The return value should be a single string that represents the difference between `str1` and `str2` in a human-readable format, similar to the style of `unified_diff`. # Constraints: - Assume that `str1` and `str2` contain at most 1,000 lines each. - Each line contains at most 200 characters. - The function should efficiently handle the comparison within a reasonable time. # Example: ```python str1 = 1. Beautiful is better than ugly. 2. Explicit is better than implicit. 3. Simple is better than complex. 4. Complex is better than complicated. str2 = 1. Beautiful is better than ugly. 3. Simple is better than complex. 4. Complicated is better than complex. 5. Flat is better than nested. output = custom_diff_checker(str1, str2) print(output) ``` # Expected Output: ``` --- str1 +++ str2 @@ -1,4 +1,4 @@ 1. Beautiful is better than ugly. - 2. Explicit is better than implicit. - 3. Simple is better than complex. + 3. Simple is better than complex. ? ++ - 4. Complex is better than complicated. ? ^ ---- ^ + 4. Complicated is better than complex. ? ++++ ^ ^ + 5. Flat is better than nested. ``` # Notes: - The output should show lines that were added, removed, or modified, with inline indicators for changes. - The diff should display the line number where changes occur and use conventional symbols such as `+` for additions and `-` for deletions. - Use the `difflib.SequenceMatcher` class or other `difflib` functionalities to implement this function.","solution":"import difflib def custom_diff_checker(str1: str, str2: str) -> str: Produces a human-readable diff between two multi-line strings. str1_lines = str1.splitlines() str2_lines = str2.splitlines() diff = list(difflib.unified_diff(str1_lines, str2_lines, fromfile=\'str1\', tofile=\'str2\', lineterm=\'\')) return \'n\'.join(diff)"},{"question":"You are tasked with implementing a logging system for an application composed of multiple modules. Each module should be able to log messages to both a console and a file, and certain modules should also include additional contextual information in their log entries, such as the current user\'s username and IP address. # Requirements: 1. Implement a function `setup_logging` that sets up logging: - This function should accept parameters including the module name, file name, and logging level. - It should configure logging such that all messages at DEBUG level and above are logged to the file, while only ERROR messages and above are logged to the console. - The log format for the file should include the timestamp, module name, log level, and message. - The log format for the console should exclude the timestamp. 2. Implement a function `add_contextual_info` that: - Adds contextual information (such as the user\'s username and IP address) to the log entry. - This function should return a logger object that wraps the original logger and appends this contextual information. # Example: 1. Main module ```python import logging from auxiliary_module import Auxiliary, some_function from utils import setup_logging, add_contextual_info # Setup logging for the main module logger = setup_logging(\'main_module\', \'app.log\', logging.DEBUG) # Add contextual information to the logger username = \\"alice\\" ip_address = \\"192.168.0.1\\" contextual_logger = add_contextual_info(logger, username, ip_address) if __name__ == \\"__main__\\": contextual_logger.info(\'Creating an instance of Auxiliary\') a = Auxiliary(contextual_logger) contextual_logger.info(\'Created an instance of Auxiliary\') contextual_logger.info(\'Calling Auxiliary.do_something\') a.do_something() contextual_logger.info(\'Finished Auxiliary.do_something\') contextual_logger.info(\'Calling some_function()\') some_function() contextual_logger.info(\'Done with some_function()\') ``` 2. Auxiliary module ```python import logging class Auxiliary: def __init__(self, logger): self.logger = logger self.logger.info(\'Creating an instance of Auxiliary\') def do_something(self): self.logger.info(\'Doing something\') print(\\"doing something\\") self.logger.info(\'Done doing something\') def some_function(): logger = logging.getLogger(\'auxiliary_module\') logger.info(\'received a call to \\"some_function\\"\') ``` 3. Utils module ```python import logging def setup_logging(module_name, file_name, level): # Create logger logger = logging.getLogger(module_name) logger.setLevel(level) # Create file handler file_handler = logging.FileHandler(file_name) file_handler.setLevel(logging.DEBUG) # Create console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.ERROR) # Create formatter and add it to the handlers file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_formatter = logging.Formatter(\'%(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) console_handler.setFormatter(console_formatter) # Add the handlers to the logger logger.addHandler(file_handler) logger.addHandler(console_handler) return logger def add_contextual_info(logger, username, ip_address): class ContextFilter(logging.Filter): def filter(self, record): record.username = username record.ip_address = ip_address return True # Create a logger adapter with contextual information contextual_logger = logging.LoggerAdapter(logger, {\'username\': username, \'ip\': ip_address}) return contextual_logger ``` # Constraints: - Use appropriate logging levels and handlers. - Ensure the contextual information can be dynamically updated. - Handle multi-threaded logging properly, ensuring that logs from different threads do not interfere with each other. # Performance requirements: - The setup process should be efficient, as it might be called multiple times. - Ensure minimal overhead for adding contextual information. - Handle logging for hundreds of messages with minimal latency.","solution":"import logging def setup_logging(module_name, file_name, level): Configures logging for the given module. Params: - module_name: Name of the module for the logger. - file_name: Name of the log file. - level: Logging level. Returns: - logger: Configured logger. # Create logger logger = logging.getLogger(module_name) logger.setLevel(level) # Create file handler file_handler = logging.FileHandler(file_name) file_handler.setLevel(logging.DEBUG) # Create console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.ERROR) # Create formatter and add it to the handlers file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_formatter = logging.Formatter(\'%(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) console_handler.setFormatter(console_formatter) # Add the handlers to the logger logger.addHandler(file_handler) logger.addHandler(console_handler) return logger def add_contextual_info(logger, username, ip_address): Adds contextual information (username and IP address) to the logger. Params: - logger: Existing logger. - username: User\'s username. - ip_address: User\'s IP address. Returns: - logger: Logger with added contextual information. class ContextFilter(logging.Filter): def filter(self, record): record.username = username record.ip_address = ip_address return True logger.addFilter(ContextFilter()) # Add custom formatting to include the contextual info in log messages formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - [%(username)s@%(ip_address)s] - %(message)s\') for handler in logger.handlers: handler.setFormatter(formatter) return logger"},{"question":"# Coding Assessment: Advanced pandas Series Operations Objective Demonstrate your understanding of pandas `Series` by performing advanced data manipulation, statistical analysis, handling missing data, and visualization. Problem Statement You are provided with the following datasets in the form of pandas `Series`: 1. **Prices**: A series representing the prices of a product over a period of 30 days. 2. **Units Sold**: A series showing the number of units sold each day over the same period. ```python import pandas as pd import numpy as np # Sample Data dates = pd.date_range(start=\\"2023-01-01\\", periods=30, freq=\'D\') prices = pd.Series(np.random.rand(30) * 100, index=dates, name=\'Prices\') units_sold = pd.Series(np.random.randint(1, 20, 30), index=dates, name=\'Units_Sold\') # Introducing some missing values in Prices prices.iloc[5] = np.nan prices.iloc[15] = np.nan prices.iloc[25] = np.nan print(\\"Prices:n\\", prices) print(\\"Units Sold:n\\", units_sold) ``` Your task is to perform the following steps: 1. **Handle Missing Data**: Replace missing values in the `prices` series with the average price. 2. **Revenue Calculation**: Calculate the daily revenue by multiplying prices with units sold. 3. **Basic Descriptive Statistics**: Compute the mean, median, and total revenue for the 30-day period. 4. **Identify and Handle Outliers**: Identify days where prices are more than 2 standard deviations away from the mean price and replace these outliers with the median price. 5. **Visualization**: Create a line plot showing both prices and units sold over the 30 days. Additionally, plot the daily revenue on the secondary y-axis. Implementation Requirements - Define a function `analyze_sales(prices: pd.Series, units_sold: pd.Series) -> pd.DataFrame` to perform the tasks outlined above. - Create a plot within the function using matplotlib to show trends over time. - The function should return a DataFrame containing the dates, prices, units sold, and revenue. Function Signature ```python def analyze_sales(prices: pd.Series, units_sold: pd.Series) -> pd.DataFrame: import pandas as pd import matplotlib.pyplot as plt # Step 1: Handle Missing Data avg_price = prices.mean() prices.fillna(avg_price, inplace=True) # Step 2: Revenue Calculation revenue = prices * units_sold # Step 3: Basic Descriptive Statistics mean_revenue = revenue.mean() median_revenue = revenue.median() total_revenue = revenue.sum() print(f\\"Mean Revenue: {mean_revenue}\\") print(f\\"Median Revenue: {median_revenue}\\") print(f\\"Total Revenue: {total_revenue}\\") # Step 4: Identify and Handle Outliers mean_price = prices.mean() std_price = prices.std() outliers = (prices - mean_price).abs() > (2 * std_price) prices[outliers] = prices.median() # Step 5: Visualization fig, ax1 = plt.subplots() ax1.set_xlabel(\'Date\') ax1.set_ylabel(\'Prices\', color=\'tab:blue\') ax1.plot(prices.index, prices.values, label=\'Prices\', color=\'tab:blue\') ax1.tick_params(axis=\'y\', labelcolor=\'tab:blue\') ax2 = ax1.twinx() ax2.set_ylabel(\'Units Sold & Revenue\', color=\'tab:gray\') ax2.plot(units_sold.index, units_sold.values, label=\'Units Sold\', color=\'tab:gray\') ax2.plot(revenue.index, revenue.values, label=\'Revenue\', color=\'tab:green\') ax2.tick_params(axis=\'y\', labelcolor=\'tab:gray\') fig.tight_layout() plt.legend() plt.show() # Return result as DataFrame result_df = pd.DataFrame({ \'Date\': prices.index, \'Prices\': prices.values, \'Units_Sold\': units_sold.values, \'Revenue\': revenue.values }) return result_df # Testing the function with given sample data result = analyze_sales(prices, units_sold) print(result) ``` Constraints - Assume `prices` and `units_sold` are always of the same length. - Handle potential outliers gracefully. - Ensure the plots are displayed with proper labels and legends. Performance Requirements - Ensure the solution runs efficiently even for larger datasets with multiple months of data. - Use vectorized operations provided by pandas instead of iterative approaches where possible.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt def analyze_sales(prices: pd.Series, units_sold: pd.Series) -> pd.DataFrame: Analyze sales data by performing multiple operations including handling missing data, calculating revenue, providing descriptive statistics, identifying outliers, and visualizing the data. Args: prices (pd.Series): A Series representing the prices of a product over a period. units_sold (pd.Series): A Series showing the number of units sold over the same period. Returns: pd.DataFrame: A DataFrame containing the dates, prices, units_sold, and revenue. # Step 1: Handle Missing Data avg_price = prices.mean() prices.fillna(avg_price, inplace=True) # Step 2: Revenue Calculation revenue = prices * units_sold # Step 3: Basic Descriptive Statistics mean_revenue = revenue.mean() median_revenue = revenue.median() total_revenue = revenue.sum() print(f\\"Mean Revenue: {mean_revenue}\\") print(f\\"Median Revenue: {median_revenue}\\") print(f\\"Total Revenue: {total_revenue}\\") # Step 4: Identify and Handle Outliers mean_price = prices.mean() std_price = prices.std() outliers = (prices - mean_price).abs() > (2 * std_price) prices[outliers] = prices.median() # Step 5: Visualization fig, ax1 = plt.subplots() ax1.set_xlabel(\'Date\') ax1.set_ylabel(\'Prices\', color=\'tab:blue\') ax1.plot(prices.index, prices.values, label=\'Prices\', color=\'tab:blue\') ax1.tick_params(axis=\'y\', labelcolor=\'tab:blue\') ax2 = ax1.twinx() ax2.set_ylabel(\'Units Sold & Revenue\', color=\'tab:gray\') ax2.plot(units_sold.index, units_sold.values, label=\'Units Sold\', color=\'tab:gray\') ax2.plot(revenue.index, revenue.values, label=\'Revenue\', color=\'tab:green\') ax2.tick_params(axis=\'y\', labelcolor=\'tab:gray\') fig.tight_layout() plt.legend() plt.show() # Return result as DataFrame result_df = pd.DataFrame({ \'Date\': prices.index, \'Prices\': prices.values, \'Units_Sold\': units_sold.values, \'Revenue\': revenue.values }) return result_df"},{"question":"# Python Coding Assessment Question **Objective:** Create a custom slicing function that mimics the behavior of Python\'s native slicing, including handling out-of-bounds indices and negative steps. You must implement this function without using Pythonâ€™s built-in slicing functionality directly. **Description:** Write a function `custom_slice` that takes a list and a slice object as input and returns a new list containing the elements specified by the slice. Your implementation should handle: - Negative indices by converting them to positive indices relative to the list length. - Out-of-bounds indices by clipping them to the valid range. - Negative slicing steps (i.e., reversing the order). **Function Signature:** ```python def custom_slice(input_list: list, slice_obj: slice) -> list: pass ``` **Parameters:** - `input_list` (list): The list to be sliced. - `slice_obj` (slice): A slice object containing parameters `start`, `stop`, and `step`. **Returns:** - `list`: A new list that includes the elements from `input_list` as specified by `slice_obj`. **Constraints:** - The input list will have at most `10^5` elements. - Each element of the list will be a valid integer. **Example:** ```python >>> custom_slice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], slice(2, 8, 2)) [2, 4, 6] >>> custom_slice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], slice(8, 2, -2)) [8, 6, 4] >>> custom_slice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], slice(-5, -1)) [5, 6, 7, 8] >>> custom_slice([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], slice(-1, -10, -1)) [9, 8, 7, 6, 5, 4, 3, 2, 1] ``` **Notes:** - Consider edge cases such as empty lists, single-element lists, zero steps, and start/stop values being `None`. - Do not use Python\'s built-in slicing (i.e., `input_list[start:stop:step]`). Instead, use basic list indexing and iteration to simulate it. **Performance:** - Your solution should be optimized for time complexity, considering the input size constraints. --- Good luck! Your task is to demonstrate a deep understanding of slice handling and effective list manipulation in Python.","solution":"def custom_slice(input_list: list, slice_obj: slice) -> list: start, stop, step = slice_obj.start, slice_obj.stop, slice_obj.step length = len(input_list) # Handle default values step = step if step is not None else 1 start = start if start is not None else (0 if step > 0 else length - 1) stop = stop if stop is not None else (length if step > 0 else -1) # Handle negative indices if start < 0: start += length if stop < 0: stop += length # Clip start and stop to be within valid range start = max(min(start, length), 0) stop = max(min(stop, length), 0) # Generate the sliced list using the step result = [] if step > 0: i = start while i < stop: result.append(input_list[i]) i += step else: i = start while i > stop: result.append(input_list[i]) i += step return result"},{"question":"**Question:** You are tasked with configuring a network for a small organization. The organization uses both IPv4 and IPv6 addresses. You need to write a Python function that processes given IP addresses, identifies their type (IPv4 or IPv6), and groups them into their respective networks. Additionally, the function should provide some basic statistics about the networks. The function should have the following requirements: 1. **Function Signature:** ```python def process_ip_addresses(ip_list): ``` 2. **Input:** - `ip_list` (List[str]): A list of IP addresses as strings. 3. **Output:** - A dictionary with the following structure: ```python { \'IPv4\': { \'networks\': List[str], # List of unique IPv4 networks each in CIDR notation \'total_addresses\': int # Total number of unique IPv4 addresses }, \'IPv6\': { \'networks\': List[str], # List of unique IPv6 networks each in CIDR notation \'total_addresses\': int # Total number of unique IPv6 addresses } } ``` 4. **Constraints:** - Assume that the given IP addresses are valid. - Each IP address should be processed to identify its network with a /24 prefix for IPv4 and /96 for IPv6. - Each network appears only once in the networks list for its type. 5. **Example:** ```python ip_list = [ \'192.0.2.1\', \'192.0.2.2\', \'192.0.3.1\', \'2001:db8::1\', \'2001:db8::2\', \'2001:db8:0:0:0:0:1::\' ] result = process_ip_addresses(ip_list) ``` Expected output: ```python { \'IPv4\': { \'networks\': [\'192.0.2.0/24\', \'192.0.3.0/24\'], \'total_addresses\': 3 }, \'IPv6\': { \'networks\': [\'2001:db8::/96\'], \'total_addresses\': 3 } } ``` Your implementation should leverage the `ipaddress` module effectively to determine and group the IP addresses, and calculate the required statistics.","solution":"import ipaddress def process_ip_addresses(ip_list): ipv4_networks = set() ipv6_networks = set() ipv4_addresses = set() ipv6_addresses = set() for ip_str in ip_list: ip = ipaddress.ip_address(ip_str) if isinstance(ip, ipaddress.IPv4Address): network = ipaddress.ip_network(f\\"{ip_str}/24\\", strict=False) ipv4_networks.add(str(network.with_prefixlen)) ipv4_addresses.add(ip_str) elif isinstance(ip, ipaddress.IPv6Address): network = ipaddress.ip_network(f\\"{ip_str}/96\\", strict=False) ipv6_networks.add(str(network.with_prefixlen)) ipv6_addresses.add(ip_str) result = { \'IPv4\': { \'networks\': sorted(ipv4_networks), # List of unique IPv4 networks each in CIDR notation \'total_addresses\': len(ipv4_addresses) # Total number of unique IPv4 addresses }, \'IPv6\': { \'networks\': sorted(ipv6_networks), # List of unique IPv6 networks each in CIDR notation \'total_addresses\': len(ipv6_addresses) # Total number of unique IPv6 addresses } } return result"},{"question":"# Custom SAX Parser for Filtering XML Data Objective: Implement a custom SAX parser that reads an XML file, processes it incrementally, and filters out certain data based on specified criteria. The parser should use classes and methods from the `xml.sax.xmlreader` module. Instructions: 1. Create a class `FilterContentHandler` that inherits from `xml.sax.ContentHandler`. This class should: - Override the `startElement`, `endElement`, and `characters` methods to filter elements and characters based on specified criteria. - Store the filtered content in a list where each entry is a dictionary with keys: `\\"tag\\"`, `\\"attributes\\"`, and `\\"content\\"`. 2. Implement a function `filter_xml(source, tags_to_filter)` that: - Takes an XML input source (`source`) which can be a filename or a URL. - Takes a list of tag names (`tags_to_filter`) to be filtered and included in the output. - Incrementally parses the XML data using the `xml.sax.xmlreader.IncrementalParser`. - Returns the filtered content as a list of dictionaries, each dictionary containing: ```python { \\"tag\\": tag_name, \\"attributes\\": attributes_dict, \\"content\\": element_content } ``` 3. Use the `xml.sax.xmlreader.InputSource` class to handle different types of input sources. Constraints: - You are **not** allowed to use third-party libraries other than the standard library. - Ensure that your solution handles large XML files efficiently by using incremental parsing. - Validate that the input source exists and is accessible. If not, raise appropriate exceptions. - Provide error handling and report any parsing errors encountered during the process. Example: Suppose we have the following XML file (`example.xml`): ```xml <books> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> <!-- More book elements --> </books> ``` Usage of `filter_xml` function: ```python filtered_content = filter_xml(\'example.xml\', [\'book\', \'author\']) print(filtered_content) ``` Expected output: ```python [ { \\"tag\\": \\"book\\", \\"attributes\\": {\\"id\\": \\"bk101\\"}, \\"content\\": \\"n <author>Gambardella, Matthew</author>n <title>XML Developer\'s Guide</title>n <genre>Computer</genre>n <price>44.95</price>n <publish_date>2000-10-01</publish_date>n <description>An in-depth look at creating applications with XML.</description>n \\" }, { \\"tag\\": \\"author\\", \\"attributes\\": {}, \\"content\\": \\"Gambardella, Matthew\\" }, { \\"tag\\": \\"book\\", \\"attributes\\": {\\"id\\": \\"bk102\\"}, \\"content\\": \\"n <author>Ralls, Kim</author>n <title>Midnight Rain</title>n <genre>Fantasy</genre>n <price>5.95</price>n <publish_date>2000-12-16</publish_date>n <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description>n \\" }, { \\"tag\\": \\"author\\", \\"attributes\\": {}, \\"content\\": \\"Ralls, Kim\\" } ] ``` Note: The content of each filtered element should preserve its inner XML structure. Submission: Submit your Python script implementing the class `FilterContentHandler` and the function `filter_xml`.","solution":"import xml.sax from xml.sax.handler import ContentHandler from xml.sax.xmlreader import InputSource class FilterContentHandler(ContentHandler): def __init__(self, tags_to_filter): super().__init__() self.tags_to_filter = tags_to_filter self.filtered_content = [] self.current_element = None self.buffer = [] def startElement(self, name, attrs): if name in self.tags_to_filter: self.current_element = { \\"tag\\": name, \\"attributes\\": dict(attrs), \\"content\\": \\"\\" } self.buffer = [] def endElement(self, name): if name in self.tags_to_filter and self.current_element: self.current_element[\\"content\\"] = \'\'.join(self.buffer) self.filtered_content.append(self.current_element) self.current_element = None self.buffer = [] def characters(self, content): if self.current_element: self.buffer.append(content) def filter_xml(source, tags_to_filter): handler = FilterContentHandler(tags_to_filter) parser = xml.sax.make_parser() parser.setContentHandler(handler) input_source = InputSource(source) parser.parse(input_source) return handler.filtered_content"},{"question":"**Python Coding Assessment Question** **Objective:** Design a Python function to parse and interpret a Distutils setup configuration file (`setup.cfg`). This function will demonstrate your understanding of configuration parsing and handling metadata for packaging. **Background:** The `setup.cfg` file is used in conjunction with the `setup.py` script to configure various parameters for building and distributing Python packages. Your task is to write a function that reads a `setup.cfg` configuration file and retrieves specific information from it. **Problem:** Write a function `parse_setup_cfg(file_path)` that takes the file path to a `setup.cfg` file as input. The function should return a dictionary containing the metadata information from the file. The `setup.cfg` file typically contains sections like `[metadata]`, `[options]`, `[options.packages.find]`, etc. Your function should focus on the `[metadata]` section and extract the following fields: - `name` - `version` - `author` - `author_email` - `description` If any of these fields are missing, the function should return `None` for that field. **Input:** - `file_path` (str): The path to the `setup.cfg` file. **Output:** - A dictionary containing the extracted metadata fields. Example: ```python { \\"name\\": \\"example-package\\", \\"version\\": \\"0.1.0\\", \\"author\\": \\"John Doe\\", \\"author_email\\": \\"john.doe@example.com\\", \\"description\\": \\"An example package for demonstration\\" } ``` **Constraints:** - Assume that the `[metadata]` section exists in the provided `setup.cfg` file. - The order of fields in the output dictionary should match the order they are listed above. - You do not need to handle other sections or fields outside the `[metadata]` section. **Example `setup.cfg` file:** ```ini [metadata] name = example-package version = 0.1.0 author = John Doe author_email = john.doe@example.com description = An example package for demonstration [options] packages = find: ``` **Function Signature:** ```python def parse_setup_cfg(file_path: str) -> dict: # Your code here ``` **Performance Requirements:** - The function should handle typical `setup.cfg` file sizes efficiently. - Aim for a time complexity of O(n), where n is the number of lines in the file. **Notes:** - Use Python\'s built-in libraries for file handling and parsing. - Ensure your function is well-documented with comments explaining key steps.","solution":"import configparser def parse_setup_cfg(file_path: str) -> dict: Parses the setup.cfg file and returns a dictionary with the metadata information. :param file_path: Path to the setup.cfg file :return: Dictionary containing metadata fields: name, version, author, author_email, description config = configparser.ConfigParser() # Read the setup.cfg file config.read(file_path) # Extract metadata information metadata = {} metadata_fields = [\'name\', \'version\', \'author\', \'author_email\', \'description\'] for field in metadata_fields: if config.has_option(\'metadata\', field): metadata[field] = config.get(\'metadata\', field) else: metadata[field] = None return metadata"},{"question":"Implementing Custom Copy Methods **Objective**: Demonstrate understanding of shallow and deep copy operations and the ability to implement custom copy methods in user-defined classes. **Problem Statement**: You are required to create a Python class named `TreeNode` to represent nodes in a binary tree structure. Each node will contain: - A value (integer). - References to the left and right child nodes. The class should correctly implement both shallow and deep copying using the `__copy__()` and `__deepcopy__()` methods. Additionally, you need to handle the case where the binary tree might be cyclical (i.e., contain nodes that reference an ancestor node somewhere up the tree). **Class Definition**: ```python class TreeNode: def __init__(self, value: int, left: \'TreeNode\' = None, right: \'TreeNode\' = None): self.value = value self.left = left self.right = right def __copy__(self): # Implement shallow copy logic pass def __deepcopy__(self, memo): # Implement deep copy logic pass ``` **Requirements**: 1. Implement the `__copy__` method to perform a shallow copy of the `TreeNode` instance. 2. Implement the `__deepcopy__` method to perform a deep copy of the `TreeNode` instance. 3. Ensure that the deep copy logic handles cyclical references correctly using the `memo` dictionary. **Input/Output Format:** - Function `copy.copy(x)` should return a shallow copy of the `TreeNode` instance `x`. - Function `copy.deepcopy(x)` should return a deep copy of the `TreeNode` instance `x`. **Example**: ```python import copy # Creating a binary tree node1 = TreeNode(1) node2 = TreeNode(2) node3 = TreeNode(3, node1, node2) # Creating a cycle (cyclical reference) node1.left = node3 # Shallow copy shallow_copied_node3 = copy.copy(node3) assert shallow_copied_node3 is not node3 assert shallow_copied_node3.left is node3.left # References should be shallowly copied # Deep copy deep_copied_node3 = copy.deepcopy(node3) assert deep_copied_node3 is not node3 assert deep_copied_node3.left is not node3.left # Objects should be deeply copied assert deep_copied_node3.left is deep_copied_node3.left.left.left # Cycle should be copied correctly # Ensure deep copy does not contain references to the original assert deep_copied_node3.left.left is not node3 ``` **Constraints**: - Each node value will be a non-negative integer (0 â‰¤ value â‰¤ 1000). - The tree will not contain more than 1000 nodes. Implement and test the required methods in the `TreeNode` class to satisfy the given requirements and examples.","solution":"import copy class TreeNode: def __init__(self, value: int, left: \'TreeNode\' = None, right: \'TreeNode\' = None): self.value = value self.left = left self.right = right def __copy__(self): # Create a shallow copy by creating a new instance and shallow copying fields new_node = TreeNode(self.value, self.left, self.right) return new_node def __deepcopy__(self, memo): # Check if we\'ve already copied this node if id(self) in memo: return memo[id(self)] # Create a deep copy by creating a new instance and deep copying fields recursively new_node = TreeNode(self.value) memo[id(self)] = new_node # Deep copy the children if self.left: new_node.left = copy.deepcopy(self.left, memo) if self.right: new_node.right = copy.deepcopy(self.right, memo) return new_node"},{"question":"# Question You have been tasked with implementing a simple echo TCP server using the asyncio module in Python. The server should be able to handle multiple clients simultaneously, echoing back any message it receives from a client. Here are the requirements: 1. Create an event loop and set it as the default event loop. 2. Implement a TCP server that listens on a specified IP address and port. 3. For each client connection, handle incoming data asynchronously and echo the data back to the client. 4. The server should run indefinitely until manually stopped. 5. Implement proper exception and error handling to ensure clean termination of connections and the server. # Requirements - The server should accept multiple client connections simultaneously. - Data received from a client should be sent back unchanged to the same client. - The server should log the IP address and port of the connected clients. - The server must handle network errors gracefully and ensure proper resources cleanup upon termination. # Input and Output Formats **Input:** - IP address and port number on which the server should listen (e.g., \'127.0.0.1\', 8888). **Output:** - Connection log messages indicating client connections and disconnections. - Echo messages sent back to clients. # Constraints - Ensure the server can handle a minimum of 10 client connections simultaneously. - Use of high-level asyncio API methods where possible is encouraged. - Proper error and exception handling needs to be demonstrated. # Example Here\'s an example to demonstrate how the server should work: 1. Start the server. 2. Client 1 connects to the server. 3. Client 1 sends the message \\"Hello\\". 4. Server echoes \\"Hello\\" back to Client 1. 5. Client 2 connects to the server. 6. Client 2 sends the message \\"World\\". 7. Server echoes \\"World\\" back to Client 2. 8. Clients can disconnect at any time and the server must handle disconnections gracefully. ```python import asyncio async def handle_client(reader, writer): address = writer.get_extra_info(\'peername\') print(f\\"Connection from {address}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {address}\\") writer.write(data) await writer.drain() except asyncio.CancelledError: print(f\\"Connection from {address} forcibly closed\\") finally: print(f\\"Closing connection from {address}\\") writer.close() await writer.wait_closed() async def start_server(ip, port): server = await asyncio.start_server(handle_client, ip, port) loop = asyncio.get_running_loop() async with server: print(f\\"Server listening on {ip}:{port}\\") await server.serve_forever() if __name__ == \\"__main__\\": ip = \'127.0.0.1\' port = 8888 try: asyncio.run(start_server(ip, port)) except KeyboardInterrupt: print(\\"Server stopped manually\\") ``` This question leverages the low-level asyncio API to demonstrate practical use cases and tests the students\' understanding of event loops, asynchronous I/O, and connection handling.","solution":"import asyncio async def handle_client(reader, writer): address = writer.get_extra_info(\'peername\') print(f\\"Connection from {address}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {address}\\") writer.write(data) await writer.drain() except asyncio.CancelledError: print(f\\"Connection from {address} forcibly closed\\") finally: print(f\\"Closing connection from {address}\\") writer.close() await writer.wait_closed() async def start_server(ip, port): server = await asyncio.start_server(handle_client, ip, port) loop = asyncio.get_running_loop() async with server: print(f\\"Server listening on {ip}:{port}\\") await server.serve_forever() if __name__ == \\"__main__\\": ip = \'127.0.0.1\' port = 8888 try: asyncio.run(start_server(ip, port)) except KeyboardInterrupt: print(\\"Server stopped manually\\")"},{"question":"# Email Retrieval using poplib **Objective:** Design a function to retrieve specific emails from a POP3 server using the `poplib` module provided in Python. **Function Signature:** ```python def fetch_emails_by_subject(host: str, port: int, user: str, password: str, subject_keyword: str) -> list: Fetch emails from a POP3 server that contain a specific keyword in the subject line. Args: * host: The POP3 server host. * port: The POP3 server port. * user: The username to authenticate with the POP3 server. * password: The password to authenticate with the POP3 server. * subject_keyword: The keyword to search for in the email subjects. Returns: * A list of emails. Each email is represented as a dictionary with at least \'subject\' and \'body\' keys. ``` **Guidelines:** 1. Establish a connection to the specified POP3 server using the provided `host` and `port`. 2. Authenticate with the server using the provided `user` and `password`. 3. Retrieve all emails from the server. 4. Parse each email to check if the subject contains the specified `subject_keyword`. 5. Collect and return emails that match the criteria. Each email should be returned as a dictionary with at least `subject` and `body` keys. 6. Ensure to handle any potential errors properly, such as connection errors or authentication failures. **Constraints:** 1. Do not use external libraries for parsing emails. Use Python\'s standard libraries where necessary. 2. The function should handle up to 1000 emails efficiently. 3. The function should close the connection to the POP3 server gracefully after fetching the emails. **Performance Requirements:** 1. The function should fetch and filter emails in a time-efficient manner, especially when dealing with a large number of emails. 2. Ensure minimal memory usage by processing one email at a time. **Example:** ```python emails = fetch_emails_by_subject(\'pop.example.com\', 110, \'user123\', \'password123\', \'Important\') for email in emails: print(f\\"Subject: {email[\'subject\']}nBody: {email[\'body\']}n\\") ``` In this example, the function connects to `pop.example.com` on port 110, logs in using `user123` and `password123`, and retrieves emails that contain the word \\"Important\\" in the subject. Matching emails are printed with their subject and body. **Note:** - Ensure to test your function against a mock POP3 server to verify its correctness. - Document any assumptions made during implementation.","solution":"import poplib from email import parser def fetch_emails_by_subject(host: str, port: int, user: str, password: str, subject_keyword: str) -> list: result_emails = [] try: # Connect to the POP3 server mail_server = poplib.POP3(host, port) # Authenticate mail_server.user(user) mail_server.pass_(password) # Get the number of messages num_messages = len(mail_server.list()[1]) # Initialize the parser email_parser = parser.Parser() for i in range(1, num_messages + 1): # Retrieve the message by index raw_email = mail_server.retr(i)[1] raw_email = b\'n\'.join(raw_email).decode(\'utf-8\', errors=\'ignore\') # Parse the message email_message = email_parser.parsestr(raw_email) # Check if the subject matches the keyword subject = email_message.get(\'subject\', \'\') if subject_keyword in subject: email_dict = { \'subject\': subject, \'body\': email_message.get_payload() } result_emails.append(email_dict) except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Ensure to close the connection try: mail_server.quit() except: pass return result_emails"},{"question":"# Coding Challenge: Working with PyArrow and pandas Integration In this challenge, you are required to demonstrate your understanding of the integration between PyArrow and pandas. You will work with different data structures, perform various operations, and utilize IO functionalities to read data. Instructions: 1. **DataFrame Creation**: - Create a DataFrame named `df` with two columns: - `A` should be a list of integers `[1, 2, 3, 4, 5]` with the dtype of `int64[pyarrow]`. - `B` should be a list of floating point numbers `[1.1, 2.2, 3.3, None, 5.5]` with the dtype of `float32[pyarrow]`. 2. **Operations**: - Compute the mean of column `B` and store it in a variable `mean_B`. - Drop any missing values (`NA`) from column `B` and store the result in a new Series `B_cleaned`. 3. **String Operations**: - Create a Series named `ser_str` with the data `[\\"apple\\", \\"banana\\", None, \\"cherry\\"]` and a dtype of `string[pyarrow]`. - Check which elements of `ser_str` start with the letter \'a\' and store the result in a Series named `str_startswith_a`. 4. **IO Reading**: - Read the following CSV data using PyArrow as the backend and store it in a DataFrame `df_csv`: ```csv id,value 1,10.5 2,20.75 3,30.00 ``` Requirements: - Implement all steps in a function named `challenge_solution()`. - The function should return a dictionary with the following keys and their corresponding values: - `df` - `mean_B` - `B_cleaned` - `ser_str` - `str_startswith_a` - `df_csv` Expected Input/Output: - **Input**: No input parameters. - **Output**: A dictionary with the keys and corresponding pandas objects as described above. Example Output: ```python { \\"df\\": A pandas DataFrame with specified columns and types, \\"mean_B\\": 3.025, \\"B_cleaned\\": A pandas Series with values `[1.1, 2.2, 3.3, 5.5]`, \\"ser_str\\": A pandas Series with values `[\\"apple\\", \\"banana\\", None, \\"cherry\\"]`, \\"str_startswith_a\\": A pandas Series with boolean values `[True, False, False, False]`, \\"df_csv\\": A pandas DataFrame as read from the CSV data } ``` **Note**: All the PyArrow and pandas functionality should be properly utilized according to the instructions given. Ensure that the code is robust and handles any potential edge cases.","solution":"import pandas as pd import pyarrow as pa def challenge_solution(): # DataFrame Creation df = pd.DataFrame({ \'A\': pd.array([1, 2, 3, 4, 5], dtype=\\"int64[pyarrow]\\"), \'B\': pd.array([1.1, 2.2, 3.3, None, 5.5], dtype=\\"float32[pyarrow]\\") }) # Operations mean_B = df[\'B\'].mean() B_cleaned = df[\'B\'].dropna() # String Operations ser_str = pd.Series([\\"apple\\", \\"banana\\", None, \\"cherry\\"], dtype=\\"string[pyarrow]\\") str_startswith_a = ser_str.str.startswith(\'a\') # IO Reading from io import StringIO csv_data = id,value 1,10.5 2,20.75 3,30.00 df_csv = pd.read_csv(StringIO(csv_data), dtype={\\"id\\": \\"int64[pyarrow]\\", \\"value\\": \\"float64[pyarrow]\\"}) return { \\"df\\": df, \\"mean_B\\": mean_B, \\"B_cleaned\\": B_cleaned, \\"ser_str\\": ser_str, \\"str_startswith_a\\": str_startswith_a, \\"df_csv\\": df_csv }"},{"question":"Problem Statement You are required to create a Python function `create_and_manipulate_xml` that performs the following tasks using the `xml.dom` module: 1. **Create a new XML document** with a root element named `\\"library\\"`. 2. **Add child elements** to the `\\"library\\"` element: - An element `\\"book\\"` with an attribute `\\"id\\"` set to `\\"1\\"` and sub-elements: - `\\"title\\"` with the content `\\"To Kill a Mockingbird\\"`. - `\\"author\\"` with the content `\\"Harper Lee\\"`. - Another element `\\"book\\"` with an attribute `\\"id\\"` set to `\\"2\\"` and sub-elements: - `\\"title\\"` with the content `\\"1984\\"`. - `\\"author\\"` with the content `\\"George Orwell\\"`. 3. **Remove an element**: - Remove the second `\\"book\\"` element from `\\"library\\"`. 4. **Query elements**: - Find all `\\"title\\"` elements and return their texts in a list. Function Signature ```python def create_and_manipulate_xml() -> list: pass ``` Constraints and Requirements - You must use the `xml.dom` module to construct and manipulate the XML document. - Ensure proper handling of DOM methods and attributes. - Handle any necessary exceptions gracefully. - The function should return a list of titles from the remaining `\\"book\\"` elements in the document. Example ```python >>> create_and_manipulate_xml() [\'To Kill a Mockingbird\'] ``` In this example: - The function creates an XML document with the specified structure. - It removes the second `\\"book\\"` element. - It returns the texts of the `\\"title\\"` elements of the remaining `\\"book\\"` elements. Assessment Criteria - Correctness of the XML document creation and manipulation. - Proper usage of the `xml.dom` module. - Efficient and clean handling of XML nodes and elements. - Accurate query and result of the required elements.","solution":"from xml.dom.minidom import Document def create_and_manipulate_xml(): # Create a new XML document doc = Document() # Create root element \'library\' library = doc.createElement(\'library\') doc.appendChild(library) # Create the first \'book\' element with id \\"1\\" book1 = doc.createElement(\'book\') book1.setAttribute(\'id\', \'1\') title1 = doc.createElement(\'title\') title1_text = doc.createTextNode(\'To Kill a Mockingbird\') title1.appendChild(title1_text) author1 = doc.createElement(\'author\') author1_text = doc.createTextNode(\'Harper Lee\') author1.appendChild(author1_text) book1.appendChild(title1) book1.appendChild(author1) library.appendChild(book1) # Create the second \'book\' element with id \\"2\\" book2 = doc.createElement(\'book\') book2.setAttribute(\'id\', \'2\') title2 = doc.createElement(\'title\') title2_text = doc.createTextNode(\'1984\') title2.appendChild(title2_text) author2 = doc.createElement(\'author\') author2_text = doc.createTextNode(\'George Orwell\') author2.appendChild(author2_text) book2.appendChild(title2) book2.appendChild(author2) library.appendChild(book2) # Remove the second \'book\' element library.removeChild(book2) # Query all \'title\' elements and return their texts in a list titles = library.getElementsByTagName(\'title\') title_texts = [title.firstChild.data for title in titles] return title_texts"},{"question":"# Question: You are tasked with creating a Python module that provides utility functions for basic statistical computations. This module should then be used in a script demonstrating its functionality. Here\'s what you need to do: 1. Create a Python module named `stats.py` with the following functions: - `mean(data)`: Computes the mean (average) of a list of numbers. - `median(data)`: Computes the median of a list of numbers. - `mode(data)`: Computes the mode of a list of numbers (the value that appears most frequently). 2. Write a script named `test_stats.py` to demonstrate the use of this module. The script should: - Import the `stats` module. - Read a list of numbers from the command-line arguments. - Compute and print the mean, median, and mode of the list. # Requirements: - The `stats.py` file should only contain the function definitions and must not execute any code when imported. - The `test_stats.py` script should handle cases where the list of numbers is empty or does not have a mode (in such cases, it should print appropriate messages). - Ensure proper error handling and validation of input data in the `test_stats.py` script. # Input and Output: - The `test_stats.py` script will be run from the command line, passing the list of numbers as arguments. - For an input such as: `python test_stats.py 1 2 3 4 5 6`, the output should be: ``` Mean: 3.5 Median: 3.5 Mode: No unique mode found ``` # Constraints: - The `mean` function should return a float even if all numbers in the list are integers. - The `median` function must handle both even and odd-length lists correctly. - The `mode` function should return \\"No unique mode found\\" if there is no single mode (e.g., in cases of bimodal distributions). # Performance requirements: - Ensure that the functions in `stats.py` are efficient and handle large lists of numbers within a reasonable time frame. # Example: **stats.py**: ```python def mean(data): pass # Implement this function def median(data): pass # Implement this function def mode(data): pass # Implement this function ``` **test_stats.py**: ```python import sys from stats import mean, median, mode # Implement the script functionality here ``` # Submission: Submit both `stats.py` and `test_stats.py` files containing the required implementations.","solution":"def mean(data): Computes the mean (average) of a list of numbers. if not data: return None return sum(data) / len(data) def median(data): Computes the median of a list of numbers. if not data: return None data = sorted(data) n = len(data) mid = n // 2 if n % 2 == 0: return (data[mid - 1] + data[mid]) / 2.0 else: return data[mid] def mode(data): Computes the mode of a list of numbers (the value that appears most frequently). from collections import Counter if not data: return None data_count = Counter(data) most_common = data_count.most_common() max_count = most_common[0][1] modes = [num for num, count in most_common if count == max_count] if len(modes) == 1: return modes[0] else: return \\"No unique mode found\\""},{"question":"# Email Policy Parsing and Generation You have been tasked with creating a custom email processing application. The application aims to clone existing policy objects and apply custom changes to them for specific behavior, demonstrating compliance with different email standards. Task: 1. **Clone and Modify Policy Objects:** - Create a function `clone_and_modify_policies()` that: - Clones the `email.policy.default`, modifying the `linesep` to use \\"rn\\". - Clones `email.policy.compat32`, modifying `cte_type` to \\"8bit\\" and `raise_on_defect` to `True`. - Combines the two cloned policy objects in the order mentioned and returns the combined policy. 2. **Email Parsing and Generation:** - Create a function `parse_and_generate_email()` that accepts an email file and a policy object, which: - Reads the email message from a file. - Creates an `EmailMessage` object using the provided policy during parsing. - Uses a `BytesGenerator` to serialize the `EmailMessage` object back to a string, applying the provided policy. - Returns the generated string. Input/Output Specifications: 1. `clone_and_modify_policies()` - **Output:** A combined `Policy` object. 2. `parse_and_generate_email(file_path: str, policy: Policy)` - **Input:** - `file_path` (str): Path to the email file to be parsed. - `policy` (Policy): A policy object to control parsing and generation behavior. - **Output:** - A string representing the serialized email with modifications according to the policy. Constraints: - Do not use any external libraries; only use the `email` package. - Assume the email file uses a valid MIME format. Example Usage: ```python combined_policy = clone_and_modify_policies() email_string = parse_and_generate_email(\'path_to_email_file.eml\', combined_policy) print(email_string) ``` Implement the functions `clone_and_modify_policies` and `parse_and_generate_email`.","solution":"from email import policy from email.policy import Policy from email.parser import BytesParser from email.generator import BytesGenerator import io def clone_and_modify_policies(): Clones the email policies and makes specified modifications. # Clone and modify the default policy modified_default_policy = policy.default.clone(linesep=\'rn\') # Clone and modify the compat32 policy modified_compat32_policy = policy.compat32.clone(cte_type=\'8bit\', raise_on_defect=True) # Combine the two modified policies in the specified order combined_policy = modified_default_policy + modified_compat32_policy return combined_policy def parse_and_generate_email(file_path: str, policy: Policy): Parses an email from a file and serializes it using the provided policy. with open(file_path, \'rb\') as f: email_bytes = f.read() parser = BytesParser(policy=policy) email_message = parser.parsebytes(email_bytes) output = io.BytesIO() generator = BytesGenerator(output, policy=policy) generator.flatten(email_message) return output.getvalue().decode(\'utf-8\')"},{"question":"# **Image Classification using Bernoulli Restricted Boltzmann Machine** **Problem Statement** Your task is to implement the function `train_rbm_classifier` that trains a classifier using a `BernoulliRBM` and a logistic regression model. The function should take in training data and labels, and test data, then return the predicted labels for the test data. The `BernoulliRBM` should be used to transform the input features before passing them to the logistic regression classifier. **Function Signature** ```python def train_rbm_classifier(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: pass ``` **Input** - `X_train`: A 2D numpy array of shape (n_samples, n_features), where each row represents a training sample. - `y_train`: A 1D numpy array of shape (n_samples,), representing the labels for the training samples. - `X_test`: A 2D numpy array of shape (n_samples_test, n_features), where each row represents a test sample. **Output** - Returns a 1D numpy array of shape (n_samples_test,), with the predicted labels for `X_test`. **Constraints** - Ensure that each element in `X_train` and `X_test` is a real number between 0 and 1. - The training process should consist of using RBM for feature extraction followed by logistic regression for classification. **Performance Requirements** - Efficient handling of datasets with at least 1000 samples. - The process should complete within a reasonable time frame (under a few minutes for the mentioned dataset size). **Example** ```python import numpy as np # Example training data X_train = np.array([[0.0, 0.1, 0.5], [0.2, 0.1, 0.4], [0.9, 0.8, 0.4]]) y_train = np.array([0, 0, 1]) # Example test data X_test = np.array([[0.3, 0.4, 0.3], [0.8, 0.9, 0.5]]) # Example call to the function predicted_labels = train_rbm_classifier(X_train, y_train, X_test) print(predicted_labels) # Expected output: Example output could be [0, 1] based on the RBM and logistic regression training. ``` **Notes** - You will need to import the necessary classes and functions from the scikit-learn library. - Make sure to preprocess the data using appropriate scaling if necessary. - Use a random state for reproducibility in your models.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.preprocessing import MinMaxScaler def train_rbm_classifier(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: Trains a BernoulliRBM and logistic regression pipeline on the given training data and predicts the labels for the test data. Parameters: X_train: np.ndarray -- 2D array of training samples (n_samples, n_features) y_train: np.ndarray -- 1D array of training labels (n_samples,) X_test: np.ndarray -- 2D array of testing samples (n_samples_test, n_features) Returns: np.ndarray -- 1D array of predicted labels for the test samples # Preprocessing to scale data between 0 and 1 scaler = MinMaxScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Create a pipeline that will first transform data using BernoulliRBM and then apply logistic regression rbm = BernoulliRBM(n_components=100, random_state=42) logistic = LogisticRegression(max_iter=1000, random_state=42) classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the pipeline on the training data classifier.fit(X_train_scaled, y_train) # Predict the labels for the test data predicted_labels = classifier.predict(X_test_scaled) return predicted_labels"},{"question":"# Distributed Training with PyTorch In distributed training, multiple processes (workers) are used to train a model efficiently by distributing the computation load. PyTorch\'s `torch.distributed` module provides tools and functionalities to facilitate this. Objective Implement a simple application for distributed training in PyTorch. Your task is to initialize the processes, perform collective operations (like `all_reduce`), and ensure proper synchronization and error handling. Task Description 1. **Initialize the Process Group**: - Use one of the provided initialization methods (`tcp`, `file`, or `env`). - Ensure the process group is properly initialized and synchronized across multiple processes. 2. **Implement a Collective Operation**: - Use the `all_reduce` collective operation to sum up the tensors from different processes. - Ensure the tensors are correctly reduced across all processes. 3. **Handle Synchronization**: - Implement a barrier using `torch.distributed.barrier` to ensure all processes reach the same point before proceeding. 4. **Debugging and Error Handling**: - Use `torch.distributed.monitored_barrier` to handle hangs or crashes and provide informative error messages. Input and Output Formats - **Input**: Your code will not take any input from the user. All parameters should be hard-coded or passed as environment variables (e.g., `MASTER_ADDR`, `MASTER_PORT`, `WORLD_SIZE`, `RANK`). - **Output**: Print the reduced tensor after performing the `all_reduce` operation. Constraints - Use the `nccl` backend if running on GPU, otherwise, use the `gloo` backend for CPU. - Ensure proper error handling and use debugging tools where appropriate. - Use `torch.profiler` to profile and output the time taken for the collective operation. Performance Requirements - The implementation should ensure minimal overhead and efficient synchronization across processes. - Properly handle exceptions and provide informative error messages. Example ```python import os import torch import torch.distributed as dist def init_process(rank, size): # Initialize process group os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"gloo\\", rank=rank, world_size=size) print(f\\"Process {rank} initialized.\\") def run(rank, size): init_process(rank, size) # Create a tensor to be reduced tensor = torch.tensor([rank], dtype=torch.float32) # All-reduce operation dist.all_reduce(tensor, op=dist.reduce_op.SUM) # Synchronize processes dist.barrier() # Print the result print(f\\"Rank {rank} has the tensor: {tensor.item()}\\") if __name__ == \\"__main__\\": size = 4 rank = int(os.environ[\'RANK\']) run(rank, size) ``` Make sure to properly test the implementation in a distributed environment.","solution":"import os import torch import torch.distributed as dist from torch.multiprocessing import Process def init_process(rank, size, fn, backend=\'gloo\', init_method=\'tcp://localhost:29500\'): Initialize the distributed environment. os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) def run(rank, size): # Create a tensor from process rank tensor = torch.tensor([rank], dtype=torch.float32) # Perform all-reduce operation dist.all_reduce(tension, op=dist.ReduceOp.SUM) # Synchronize processes dist.barrier() print(f\'Rank {rank} has tensor value: {tensor.item()}\') def main(): size = 4 processes = [] for rank in range(size): p = Process(target=init_process, args=(rank, size, run)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Coding Assessment Question Objective: Your task is to create a simple neural network using PyTorch, script it using TorchScript, and perform a JIT compilation to optimize it. Problem Statement: 1. Define a simple feedforward neural network in PyTorch with the following specifications: - Input dimension: 10 - Hidden layer: one layer with 20 neurons and ReLU activation - Output dimension: 5 2. Use TorchScript to script the defined network. 3. Use PyTorch\'s JIT compilation to optimize the scripted model. 4. Write a function `run_model` that: - Takes an input tensor and the compiled model as arguments. - Executes the model on the input tensor. - Returns the output tensor. Function Signature: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def run_model(input_tensor: torch.Tensor, compiled_model: torch.jit.ScriptModule) -> torch.Tensor: pass # Example usage: # model = SimpleModel() # scripted_model = torch.jit.script(model) # compiled_model = torch.jit.optimize_for_inference(scripted_model) # input_tensor = torch.randn(1, 10) # output_tensor = run_model(input_tensor, compiled_model) ``` Input: 1. `input_tensor` (torch.Tensor): A tensor with shape (1, 10) representing a single input sample. 2. `compiled_model` (torch.jit.ScriptModule): The JIT-compiled version of the scripted model. Output: - `output_tensor` (torch.Tensor): The output tensor produced by running the input tensor through the compiled model, with shape (1, 5). Constraints: - Ensure that all necessary imports are included. - The input tensor will always be of appropriate shape and type. - You must use PyTorch 1.8.0 or later. Notes: - You are expected to follow best practices for PyTorch model definition, scripting, and JIT compilation. - Ensure the code is efficient and leverages PyTorch\'s capabilities.","solution":"import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def run_model(input_tensor: torch.Tensor, compiled_model: torch.jit.ScriptModule) -> torch.Tensor: return compiled_model(input_tensor) # Example usage: # model = SimpleModel() # scripted_model = torch.jit.script(model) # compiled_model = torch.jit.optimize_for_inference(scripted_model) # input_tensor = torch.randn(1, 10) # output_tensor = run_model(input_tensor, compiled_model)"},{"question":"# XML Processing with `xml.dom.minidom` Objective: Implement a function that parses an XML string, processes its content, and generates an updated XML string. Problem Statement: You have been provided with an XML structure as a string that represents a catalog of books. You need to write a function `update_xml_catalog(xml_str)` that performs the following operations on the XML: 1. Parse the input XML string to a DOM structure. 2. Add a new book entry to the catalog with the following details: - Title: \\"New Book\\" - Author: \\"Author Name\\" - Year: \\"2023\\" - Price: \\"29.99\\" 3. Update the price of all books published before the year 2000 to \\"19.99\\". 4. Return the updated XML as a pretty-printed string. Input: - `xml_str`: A string containing well-formed XML data representing a catalog of books. Output: - A string of the updated and pretty-printed XML data. Example: ```python input_xml = <catalog> <book> <title>Book One</title> <author>Author One</author> <year>1995</year> <price>39.95</price> </book> <book> <title>Book Two</title> <author>Author Two</author> <year>2005</year> <price>49.95</price> </book> </catalog> expected_output = <catalog> <book> <title>Book One</title> <author>Author One</author> <year>1995</year> <price>19.99</price> </book> <book> <title>Book Two</title> <author>Author Two</author> <year>2005</year> <price>49.95</price> </book> <book> <title>New Book</title> <author>Author Name</author> <year>2023</year> <price>29.99</price> </book> </catalog> def update_xml_catalog(xml_str): # Your implementation here ``` Constraints: - The XML structure provided will always be well-formed. - The function should use the `xml.dom.minidom` module for XML processing. To complete this task, you will need to: 1. Understand the structure of the provided XML. 2. Use `xml.dom.minidom` functions to parse the XML string. 3. Traverse and manipulate the DOM tree as required. 4. Convert and return the updated DOM structure as a pretty-printed XML string. **Note:** Ensure that the output XML maintains the order and hierarchy of elements while reflecting the changes.","solution":"from xml.dom.minidom import parseString, Document def update_xml_catalog(xml_str): Parse the input XML string, add a new book entry, update prices of books published before the year 2000, and return the updated XML as a pretty-printed string. # Parse the input XML string dom = parseString(xml_str) catalog = dom.documentElement # Create the new book element new_book = dom.createElement(\\"book\\") title = dom.createElement(\\"title\\") title.appendChild(dom.createTextNode(\\"New Book\\")) new_book.appendChild(title) author = dom.createElement(\\"author\\") author.appendChild(dom.createTextNode(\\"Author Name\\")) new_book.appendChild(author) year = dom.createElement(\\"year\\") year.appendChild(dom.createTextNode(\\"2023\\")) new_book.appendChild(year) price = dom.createElement(\\"price\\") price.appendChild(dom.createTextNode(\\"29.99\\")) new_book.appendChild(price) # Append the new book element to the catalog catalog.appendChild(new_book) # Update the price of books published before the year 2000 books = catalog.getElementsByTagName(\\"book\\") for book in books: year_element = book.getElementsByTagName(\\"year\\")[0] year = int(year_element.firstChild.data) if year < 2000: price_element = book.getElementsByTagName(\\"price\\")[0] price_element.firstChild.data = \\"19.99\\" # Return the updated and pretty-printed XML string return dom.toprettyxml(indent=\\" \\")"},{"question":"# Advanced Programming Assessment Question: HTTP Client Interaction using `http.client` **Objective**: Demonstrate your ability to use the `http.client` module to perform HTTP operations, handle responses, including error conditions, and manage connections effectively. **Problem Statement**: Your task is to implement a function `fetch_resource` that takes the following parameters: - `base_url` (str): The base URL of the server to connect to (e.g., `www.example.com`). - `resource` (str): The resource path to request (e.g., `/index.html`). - `method` (str): The HTTP method to use for the request (e.g., `GET`, `POST`). - `params` (dict, optional): A dictionary of parameters to include in the request body (for `POST`, `PUT` methods). - `headers` (dict, optional): Additional headers to include in the request. The function should: 1. Establish an HTTP or HTTPS connection based on the `base_url`. 2. Send the specified HTTP request. 3. Handle the server response, including: - Returning the response status and reason as a tuple `(status, reason)`. - Returning the full response body as a string if the response status is `200 OK`. - Returning `None` if the response status is other than `200 OK`. 4. Handle potential errors and exceptions gracefully, returning a meaningful error message if the connection or the request fails. **Constraints**: - Assume `params` is only relevant for `POST` and `PUT` methods. - For simplicity, handle only `GET`, `POST`, and `PUT` methods. - If `base_url` starts with `https://`, use `HTTPSConnection`; otherwise, use `HTTPConnection`. - Use default ports for HTTP (`80`) and HTTPS (`443`). **Function Signature**: ```python def fetch_resource(base_url: str, resource: str, method: str, params: dict = None, headers: dict = None) -> tuple: pass ``` **Example Usage**: 1. Fetching a resource using `GET`: ```python status, reason, body = fetch_resource(\'http://www.example.com\', \'/index.html\', \'GET\') print(status, reason) print(body) ``` 2. Fetching a resource using `POST` with parameters: ```python params = {\'key1\': \'value1\', \'key2\': \'value2\'} headers = {\'Content-Type\': \'application/x-www-form-urlencoded\'} status, reason, body = fetch_resource(\'https://www.example.com\', \'/submit\', \'POST\', params, headers) print(status, reason) print(body) ``` **Additional Notes**: - Include appropriate error handling for network issues and invalid responses. - Ensure that your implementation adheres to best practices for managing HTTP connections and resources. - You may use the `urllib.parse` module to encode parameters if necessary.","solution":"import http.client import urllib.parse def fetch_resource(base_url: str, resource: str, method: str, params: dict = None, headers: dict = None) -> tuple: Fetches a resource from the specified base URL using the provided HTTP method. if params is None: params = {} if headers is None: headers = {} parsed_url = urllib.parse.urlparse(base_url) connection = None try: if parsed_url.scheme == \\"https\\": connection = http.client.HTTPSConnection(parsed_url.netloc, 443) else: connection = http.client.HTTPConnection(parsed_url.netloc, 80) if method in [\'POST\', \'PUT\']: body = urllib.parse.urlencode(params) else: body = None connection.request(method, resource, body, headers) response = connection.getresponse() status, reason = response.status, response.reason if response.status == 200: body = response.read().decode() return status, reason, body else: return status, reason, None except Exception as e: return \'Error\', str(e), None finally: if connection: connection.close()"},{"question":"Objective Design a Python script using the `argparse` module to create a command-line tool that performs different operations on files and directories based on the provided arguments. Requirements 1. Create an ArgumentParser object with a description. 2. The script should support the following operations, specified by sub-commands: - **search**: Search for a string in a file. - Arguments: - `--file`, `-f`: The file to search in (required). - `--string`, `-s`: The string to search for (required). - **copy**: Copy a file from source to destination. - Arguments: - `source`: The source file (positional argument). - `destination`: The destination file (positional argument). - **list**: List all files in a directory. - Arguments: - `--directory`, `-d`: The directory to list files from (default is the current directory). 3. The tool should provide helpful error messages when invalid arguments are provided. 4. The help message should be formatted clearly, showing the usage of commands and their arguments. Input and Output Formats 1. The script should be named `file_manager.py`. 2. Correct usage should produce appropriate outputs for each sub-command: - `search`: Prints lines containing the searched string. - `copy`: Copies the file from source to destination with a confirmation message. - `list`: Prints a list of files in the specified directory. 3. Example command-line usage: ```sh python file_manager.py search --file example.txt --string \\"hello\\" python file_manager.py copy source.txt destination.txt python file_manager.py list --directory /path/to/dir ``` Constraints - Use the argparse functionalities as much as possible. - Ensure robust error handling and informative help messages. Performance - Assume input sizes to be reasonably small for a command-line tool (files up to a few GB, directories with thousands of entries). Problem Solving - Implement the script in a modular way. - Handle file operations using appropriate Python methods. - Use argparse features to manage command-line arguments effectively. Happy coding!","solution":"import argparse import os import shutil def search(file, string): try: with open(file, \'r\') as f: lines = f.readlines() found = [line for line in lines if string in line] for line in found: print(line, end=\'\') except FileNotFoundError: print(f\\"Error: File {file} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") def copy(source, destination): try: shutil.copyfile(source, destination) print(f\\"File {source} copied to {destination}.\\") except FileNotFoundError: print(f\\"Error: Source file {source} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") def list_files(directory): try: files = os.listdir(directory) for f in files: print(f) except NotADirectoryError: print(f\\"Error: {directory} is not a directory.\\") except FileNotFoundError: print(f\\"Error: Directory {directory} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") def main(): parser = argparse.ArgumentParser(description=\'File and Directory management tool.\') subparsers = parser.add_subparsers(dest=\'command\') search_parser = subparsers.add_parser(\'search\', help=\'Search for a string in a file.\') search_parser.add_argument(\'--file\', \'-f\', required=True, help=\'The file to search in.\') search_parser.add_argument(\'--string\', \'-s\', required=True, help=\'The string to search for.\') copy_parser = subparsers.add_parser(\'copy\', help=\'Copy a file from source to destination.\') copy_parser.add_argument(\'source\', help=\'The source file.\') copy_parser.add_argument(\'destination\', help=\'The destination file.\') list_parser = subparsers.add_parser(\'list\', help=\'List all files in a directory.\') list_parser.add_argument(\'--directory\', \'-d\', default=\'.\', help=\'The directory to list files from.\') args = parser.parse_args() if args.command == \'search\': search(args.file, args.string) elif args.command == \'copy\': copy(args.source, args.destination) elif args.command == \'list\': list_files(args.directory) else: parser.print_help() if __name__ == \'__main__\': main()"},{"question":"**Custom Data Type and Array Extension in pandas** **Objective:** Create and register a custom data type and array in pandas using the pandas extension API. This task will test your understanding of extending pandas functionality to accommodate custom data handling requirements. # Task 1. Define a custom data type `BooleanDtype` that represents boolean data. 2. Implement a custom `BooleanArray` that extends `pandas.api.extensions.ExtensionArray` for handling boolean data. 3. Register this new data type with pandas. # Input: There is no direct input to this task. Instead, you will write the definitions and registrations as described. # Output: You need to demonstrate the functionality of your custom data type and array by creating a pandas DataFrame using this custom boolean type. # Requirements: 1. **Custom Dtype Class:** - Create a class `BooleanDtype` derived from `pandas.api.extensions.ExtensionDtype` with the following properties: - `name`: A string representing the name of the custom type. - `type`: The data type (which should be `bool`). - `na_value`: The value representing missing data (use `None`). - `construct_array_type`: A method returning the array type class. 2. **Custom Array Class:** - Create a class `BooleanArray` derived from `pandas.api.extensions.ExtensionArray` with implementations for the following methods: - `__init__`: Constructor to initialize the array with data. - `_from_sequence`: A method to create an array from a sequence of bools, handling NaNs. - `_from_factorized`: A method to create an array from factorized data. - `_concat_same_type`: A method to concatenate multiple arrays of the same type. - `__getitem__`: A method to get an item or a slice from the array. - `__len__`: A method to get the length of the array. - Additional necessary properties (e.g., `dtype`, `isna`, `copy`, `astype`, etc.). 3. **Register Extension Dtype:** - Use `pandas.api.extensions.register_extension_dtype` to register the `BooleanDtype` class with pandas. 4. **Demonstration:** - Create a pandas DataFrame using the custom boolean type and demonstrate typical operations (e.g., creation, value manipulation, handling of missing data). # Example: ```python import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype class BooleanDtype(ExtensionDtype): name = \'boolean\' type = bool na_value = None @classmethod def construct_array_type(cls): return BooleanArray class BooleanArray(ExtensionArray): def __init__(self, values): self._data = pd.array(values, dtype=\\"boolean\\") @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): # Your Implementation here pass # Implement other necessary methods... @register_extension_dtype class BooleanDtype(ExtensionDtype): # Your Implementation here # Register the dtype pd.api.extensions.register_extension_dtype(BooleanDtype) # Create a DataFrame using the custom dtype data = BooleanArray([True, False, None, True]) df = pd.DataFrame({\\"custom_bool\\": data}) print(df) ``` # Constraints: - This system should be built using pandas\' extension mechanism and should not use dictionaries or tuples for data storage. - Handle missing data correctly using `None`. - Ensure that the array and dtype are correctly registered with pandas. This task requires a deep understanding of pandas\' extension API and the ability to implement custom data handling logic within pandas\' framework.","solution":"import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype import numpy as np class BooleanDtype(ExtensionDtype): name = \'boolean\' type = bool na_value = None @classmethod def construct_array_type(cls): return BooleanArray class BooleanArray(ExtensionArray): def __init__(self, values): self._data = np.array(values, dtype=object) self._mask = pd.isna(self._data) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) @classmethod def _from_factorized(cls, values, original): return cls(values) @classmethod def _concat_same_type(cls, to_concat): data = [x._data for x in to_concat] return cls(np.concatenate(data)) def __getitem__(self, item): if isinstance(item, int): return self._data[item] if not self._mask[item] else None else: return BooleanArray(self._data[item]) def __len__(self): return len(self._data) @property def dtype(self): return BooleanDtype() def isna(self): return self._mask def copy(self): return BooleanArray(self._data.copy()) def astype(self, dtype, copy=True): if dtype == \'boolean\' or dtype == BooleanDtype: return self.copy() if copy else self else: return np.array(self._data, dtype=dtype, copy=copy) @register_extension_dtype class CustomBooleanDtype(BooleanDtype): pass # Demonstrating functionality data = BooleanArray([True, False, None, True]) df = pd.DataFrame({\\"custom_bool\\": data}) print(df)"},{"question":"# Python Coding Assessment: Working with Floating Point Numbers **Objective:** This task is designed to assess your understanding of creating and manipulating floating point numbers in Python, as well as handling conversions and limits. **Task:** Write a Python function `process_float_operations` that takes a list of strings and performs the following operations: 1. Convert each string in the list to a Python floating point object. - If the string cannot be converted to a float (e.g., \\"abc\\"), skip it. 2. Create a new list of tuples where each tuple contains: - The original string. - The converted float value. - The float value converted back to string using `str()` function. 3. Separate another list of floats from the tuples obtained in step 2. 4. Calculate the following and return as part of the result: - The maximum float value using `max()`. - The minimum float value using `min()`. - The average of all float values. - The precision, minimum, and maximum floating point values available using `sys.float_info`. **Function Signature:** ```python import sys from typing import List, Tuple, Dict, Any def process_float_operations(strings: List[str]) -> Dict[str, Any]: pass ``` **Expected Input:** - `strings`: A list of strings to be converted to floats. **Expected Output:** - A dictionary with the following keys and respective values: - `\'converted\': List of tuples with each tuple containing the original string, the float value, and the float value as a string.` - `\'max_value\': Maximum float value from the converted floats.` - `\'min_value\': Minimum float value from the converted floats.` - `\'average\': Average of the converted float values.` - `\'float_info\': Dictionary containing precision, min, and max float values according to `sys.float_info`. **Example:** ```python strings = [\\"3.14\\", \\"0.001\\", \\"xyz\\", \\"2.718\\", \\"-1.0\\", \\"1.6e4\\"] result = process_float_operations(strings) print(result) # Expected Output: # { # \'converted\': [(\\"3.14\\", 3.14, \\"3.14\\"), (\\"0.001\\", 0.001, \\"0.001\\"), (\\"2.718\\", 2.718, \\"2.718\\"), (\\"-1.0\\", -1.0, \\"-1.0\\"), (\\"1.6e4\\", 16000.0, \\"16000.0\\")], # \'max_value\': 16000.0, # \'min_value\': -1.0, # \'average\': 3232.9714, # \'float_info\': {\'precision\': sys.float_info.epsilon, \'min\': sys.float_info.min, \'max\': sys.float_info.max} # } ``` **Constraints:** - Handle invalid float strings gracefully by skipping them. - Ensure precise float operations. **Performance Requirements:** - The function should be able to handle large lists with efficiency. Good luck!","solution":"import sys from typing import List, Tuple, Dict, Any def process_float_operations(strings: List[str]) -> Dict[str, Any]: converted = [] floats = [] for s in strings: try: float_value = float(s) converted.append((s, float_value, str(float_value))) floats.append(float_value) except ValueError: # Skip strings that can\'t be converted to float continue if floats: max_value = max(floats) min_value = min(floats) average = sum(floats) / len(floats) else: max_value = min_value = average = None float_info = { \'precision\': sys.float_info.epsilon, \'min\': sys.float_info.min, \'max\': sys.float_info.max } return { \'converted\': converted, \'max_value\': max_value, \'min_value\': min_value, \'average\': average, \'float_info\': float_info }"},{"question":"# Debugging with `pdb` Objective Implement and debug a simple Python program using the `pdb` module. The goal is to demonstrate your ability to set breakpoints, step through code, and resolve issues using `pdb` commands effectively. Task 1. **Implement a Function**: Write a Python function named `compute_factorial` that calculates the factorial of a given non-negative integer. 2. **Debug with `pdb`**: - Insert a breakpoint in your code that triggers when an invalid (negative) argument is provided to the function. - Use `pdb` to step through the code and inspect variable values at each step. - Detect and fix any issues such as handling invalid input or logical errors in the factorial computation. Function Signature ```python def compute_factorial(n: int) -> int: # Start the function implementation here pass ``` Input and Output - **Input**: A single non-negative integer `n`. - **Output**: The factorial of the input integer `n`. Constraints and Limitations - The input integer `n` will be within the range [0, 20] for testing purposes. - Your implementation should handle invalid inputs (negative integers) gracefully by raising an appropriate exception. Example ```python # Example usage of the function result = compute_factorial(5) print(result) # Should output 120 ``` Steps to Follow 1. **Write the `compute_factorial` function**: - If `n` is 0 or 1, return 1 (base case). - Otherwise, recursively compute the factorial. 2. **Insert Breakpoints**: - Use `pdb.set_trace()` to break into the debugger when `n` is less than 0. 3. **Debug Using `pdb`**: - Run your script using `pdb` to ensure correct functionality. - Step through the code to inspect variable values and the flow of execution. - Use commands like `step`, `next`, `continue`, `break`, and `print` to navigate and inspect your code. 4. **Fix and Document Issues**: - Detect any issues with the logic or error handling and address them. - Document the steps taken to debug and fix the issues. You should provide the final version of the `compute_factorial` function and a brief walkthrough of your debugging process using `pdb`. Good luck!","solution":"def compute_factorial(n: int) -> int: Computes the factorial of a non-negative integer n. Parameters: n (int): A non-negative integer whose factorial is to be computed. Returns: int: The factorial of the input integer n. Raises: ValueError: If the input is a negative integer. if n < 0: raise ValueError(\\"Input must be a non-negative integer.\\") if n == 0 or n == 1: return 1 return n * compute_factorial(n - 1)"},{"question":"As an experienced data analyst, you often find yourself needing to visualize the same dataset in multiple styles for various presentations. You decide to write a function, `apply_styles`, that takes in a list of plot style names and a dataset, and outputs corresponding styled plots. Objective Write a function `apply_styles(styles: List[str], data: Dict[str, List[int]]) -> None` that: 1. Takes a list of seaborn style names and a dataset consisting of x and y values. 2. Generates a barplot for each style showing the data. 3. Displays each plot with the respective style applied. # Input - `styles`: A list of strings where each string is a seaborn style name, e.g., `[\\"darkgrid\\", \\"whitegrid\\", \\"dark\\"]`. - `data`: A dictionary with two keys: `\\"x\\"` and `\\"y\\"`, both mapping to lists of integers representing the data points for the x and y axes of the barplot. # Output - The function will generate and display barplots with the specified styles applied. There is no return value. # Constraints - You can assume all style names provided are valid seaborn style names. - The length of the x and y lists in the data dictionary will always be equal. # Example ```python styles = [\\"darkgrid\\", \\"white\\", \\"ticks\\"] data = { \\"x\\": [1, 2, 3], \\"y\\": [4, 5, 6] } apply_styles(styles, data) ``` This example should generate three barplots, each with a different seaborn style as specified in the styles list. # Hint - Make use of `sns.axes_style` and context managers to apply styles temporarily.","solution":"import seaborn as sns import matplotlib.pyplot as plt def apply_styles(styles, data): Generates and displays a barplot with each of the provided seaborn styles applied. Args: styles (list of str): List of seaborn style names. data (dict): Dictionary with keys \'x\' and \'y\' mapping to lists of integers. for style in styles: with sns.axes_style(style): plt.figure() sns.barplot(x=data[\'x\'], y=data[\'y\']) plt.title(f\\"Style: {style}\\") plt.show()"},{"question":"**Question: Implement a Python program using `urllib.request` to interact with a hypothetical REST API.** # Scenario: You are tasked with creating a Python script that interacts with a hypothetical REST API to perform the following actions: 1. **Fetch Data**: Make a GET request to the API endpoint to retrieve a list of resources. 2. **Send Data**: Make a POST request to submit new data to the API. 3. **Update Data**: Make a PUT request to update an existing resource. 4. **Delete Data**: Make a DELETE request to remove a resource. 5. **Handle Errors**: Gracefully handle errors such as network issues and HTTP error codes. 6. **Add Headers**: Include specific headers in your requests, such as a User-Agent header. # API Endpoints: - GET: `http://example.com/api/resources` - POST: `http://example.com/api/resources` - PUT: `http://example.com/api/resources/{id}` - DELETE: `http://example.com/api/resources/{id}` # Requirements: 1. **Function Definitions**: - Implement functions `fetch_resources()`, `send_data(data)`, `update_resource(resource_id, data)`, and `delete_resource(resource_id)`. 2. **Inputs**: - `send_data(data)` and `update_resource(resource_id, data)` should accept data in the form of a dictionary. - `delete_resource(resource_id)` should accept the resource ID as a string. 3. **Outputs**: - Each function should print the status of the operation and any pertinent data returned from the API. - Functions should return the response data from the server. 4. **Error Handling**: - Properly handle `URLError` and `HTTPError`. - Print relevant error messages. 5. **Headers**: - Include a custom User-Agent header in all requests: `Mozilla/5.0 (Windows NT 6.1; Win64; x64)` # Example Function: ```python import urllib.request import urllib.error import urllib.parse def fetch_resources(): url = \'http://example.com/api/resources\' headers = {\'User-Agent\': \'Mozilla/5.0 (Windows NT 6.1; Win64; x64)\'} req = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(req) as response: data = response.read() print(\\"Fetched Resources:\\", data) return data except urllib.error.HTTPError as e: print(\'HTTP Error:\', e.code, e.reason) except urllib.error.URLError as e: print(\'URL Error:\', e.reason) # Implement other functions: send_data, update_resource, delete_resource ``` # Detailed Requirements: - **fetch_resources()**: - Fetch data using a GET request. - Return the fetched data. - **send_data(data)**: - Send data using a POST request. - The `data` parameter should be a dictionary. - Encode the data and handle the response. - **update_resource(resource_id, data)**: - Update data using a PUT request. - The `resource_id` parameter specifies the resource to update. - The `data` parameter should be a dictionary. - Encode the data and handle the response. - **delete_resource(resource_id)**: - Remove a resource using a DELETE request. - The `resource_id` parameter specifies the resource to delete. - Handle the response and print an appropriate message. Implement the required functions and ensure each function performs the necessary action while handling any potential errors. Use the examples and information provided in the documentation to guide your implementation.","solution":"import urllib.request import urllib.error import urllib.parse import json USER_AGENT = \'Mozilla/5.0 (Windows NT 6.1; Win64; x64)\' def fetch_resources(): url = \'http://example.com/api/resources\' headers = {\'User-Agent\': USER_AGENT} req = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(req) as response: data = response.read().decode() print(\\"Fetched Resources:\\", data) return data except urllib.error.HTTPError as e: print(\'HTTP Error:\', e.code, e.reason) except urllib.error.URLError as e: print(\'URL Error:\', e.reason) def send_data(data): url = \'http://example.com/api/resources\' headers = {\'User-Agent\': USER_AGENT, \'Content-Type\': \'application/json\'} req = urllib.request.Request(url, headers=headers, data=json.dumps(data).encode(), method=\'POST\') try: with urllib.request.urlopen(req) as response: response_data = response.read().decode() print(\\"Data Sent:\\", response_data) return response_data except urllib.error.HTTPError as e: print(\'HTTP Error:\', e.code, e.reason) except urllib.error.URLError as e: print(\'URL Error:\', e.reason) def update_resource(resource_id, data): url = f\'http://example.com/api/resources/{resource_id}\' headers = {\'User-Agent\': USER_AGENT, \'Content-Type\': \'application/json\'} req = urllib.request.Request(url, headers=headers, data=json.dumps(data).encode(), method=\'PUT\') try: with urllib.request.urlopen(req) as response: response_data = response.read().decode() print(\\"Resource Updated:\\", response_data) return response_data except urllib.error.HTTPError as e: print(\'HTTP Error:\', e.code, e.reason) except urllib.error.URLError as e: print(\'URL Error:\', e.reason) def delete_resource(resource_id): url = f\'http://example.com/api/resources/{resource_id}\' headers = {\'User-Agent\': USER_AGENT} req = urllib.request.Request(url, headers=headers, method=\'DELETE\') try: with urllib.request.urlopen(req) as response: status = response.status print(f\\"Resource {resource_id} Deleted, Status: {status}\\") return status except urllib.error.HTTPError as e: print(\'HTTP Error:\', e.code, e.reason) except urllib.error.URLError as e: print(\'URL Error:\', e.reason)"},{"question":"# PyTorch Distributed Training with Elasticity PyTorch\'s `torch.distributed.elastic` package allows for dynamically resizing distributed training jobs, making it easier to handle worker failures and resource fluctuations. Inside this package, the `control_plane` module provides tools for added debugging and control during distributed training. Question: **Task:** Implement a basic distributed training script using PyTorch that includes a control plane for handling worker elasticity. Your script should: 1. Initialize a distributed process group. 2. Distribute a simple neural network across multiple GPUs. 3. Implement a simple training loop. 4. Utilize the `torch.distributed.elastic.control_plane.worker_main` for control handling. **Requirements:** - **Input:** None (you are to write a script) - **Output:** The expected output is a terminal log demonstrating: - Initialization of the distributed environment. - Training progress across multiple GPUs. - Control plane messages indicating worker elasticity handling. **Constraints:** - Assume that you have access to multiple GPU nodes. - Use a simple neural network, such as a single-layer perceptron or a convolutional neural network. **Performance:** Your script should efficiently handle the distribution and elasticity of the workers. Ensure minimal downtime if a worker fails and another takes over. Example Code Structure: ```python import torch import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.distributed.elastic.control_plane import worker_main def setup(rank, world_size): # Initialize the process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) def train_loop(rank, world_size): setup(rank, world_size) torch.manual_seed(0) # Create model and move it to GPU with id rank model = ... # Define your model here ddp_model = DDP(model.to(rank), device_ids=[rank]) optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001) loss_fn = torch.nn.CrossEntropyLoss() for epoch in range(10): # Training loop ... if __name__ == \\"__main__\\": world_size = 4 # Number of GPUs mp.spawn(train_loop, args=(world_size,), nprocs=world_size, join=True) ``` **Hints:** 1. Use `torch.multiprocessing.spawn` for launching multiple processes. 2. Use `torch.distributed.init_process_group` for initializing distributed training. 3. Define a simple neural network and use `torch.nn.parallel.DistributedDataParallel` to handle distribution. 4. Integrate `torch.distributed.elastic.control_plane.worker_main` within your script to handle elasticity and debug information. Happy coding!","solution":"import os import torch import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.distributed.elastic.multiprocessing.errors import record def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.manual_seed(0) def cleanup(): dist.destroy_process_group() class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.linear = torch.nn.Linear(10, 10) def forward(self, x): return self.linear(x) @record def train_loop(rank, world_size): setup(rank, world_size) model = SimpleNN().to(rank) ddp_model = DDP(model, device_ids=[rank]) optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.001) loss_fn = torch.nn.CrossEntropyLoss() dataset = torch.utils.data.TensorDataset(torch.randn(100, 10), torch.randint(0, 10, (100,))) dataloader = torch.utils.data.DataLoader(dataset, batch_size=5) for epoch in range(2): # Shortened for demonstration for batch, (data, target) in enumerate(dataloader): data, target = data.to(rank), target.to(rank) optimizer.zero_grad() output = ddp_model(data) loss = loss_fn(output, target) loss.backward() optimizer.step() if batch % 10 == 0: print(f\\"Rank {rank}, Epoch {epoch}, Batch {batch}, Loss {loss.item()}\\") cleanup() def main(): world_size = 2 # Adjust to the number of GPUs available mp.spawn(train_loop, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"**Data Compression Utility** You are required to implement a class in Python that will use the `zlib` module to provide a comprehensive data compression and decompression utility. This utility should integrate various functionalities offered by the `zlib` module: compressing data, decompressing data, generating checksums, and handling streams. # Your Task Implement a class `DataCompressionUtility` with the following methods: 1. **`compress_data(data: bytes, level: int = -1) -> bytes`**: - **Input:** Data as a bytes object, and an optional compression level (default is -1). - **Output:** Compressed data as a bytes object. - **Constraints:** Level is an integer between `0` (no compression) and `9` (best compression). 2. **`decompress_data(data: bytes, wbits: int = zlib.MAX_WBITS) -> bytes`**: - **Input:** Compressed data as a bytes object, and an optional window buffer size (default is MAX_WBITS). - **Output:** Decompressed data as a bytes object. 3. **`compute_adler32(data: bytes, value: int = 1) -> int`**: - **Input:** Data as a bytes object, and an optional starting value for the checksum (default is 1). - **Output:** Computed Adler-32 checksum as an unsigned 32-bit integer. 4. **`compute_crc32(data: bytes, value: int = 0) -> int`**: - **Input:** Data as a bytes object, and an optional starting value for the checksum (default is 0). - **Output:** Computed CRC-32 checksum as an unsigned 32-bit integer. 5. **`compress_stream(input_data: bytes_list, level: int = -1) -> bytes`**: - **Input:** List of data segments as bytes objects, and an optional compression level (default is -1). - **Output:** Compressed data stream as a bytes object. 6. **`decompress_stream(compressed_data: bytes, wbits: int = zlib.MAX_WBITS) -> bytes`**: - **Input:** Compressed data stream as a bytes object, and an optional window buffer size (default is MAX_WBITS). - **Output:** Decompressed data stream as a bytes object. # Constraints and Requirements - You must handle zlib errors appropriately and raise custom error messages if any compression or decompression fails. - Ensure that each method is tested with appropriate test cases that will check the functionality and correctness of the solution. - The solution must be efficient in terms of both time and space complexity where possible. # Example ```python input_data = b\\"This is a test string for compression\\" utility = DataCompressionUtility() compressed = utility.compress_data(input_data) decompressed = utility.decompress_data(compressed) assert input_data == decompressed checksum_adler = utility.compute_adler32(input_data) checksum_crc = utility.compute_crc32(input_data) assert checksum_adler == zlib.adler32(input_data) assert checksum_crc == zlib.crc32(input_data) stream_data = [b\\"Part1 \\", b\\"Part2 \\", b\\"Part3\\"] compressed_stream = utility.compress_stream(stream_data) decompressed_stream = utility.decompress_stream(compressed_stream) assert b\\"Part1 Part2 Part3\\" == decompressed_stream ``` You need to implement this class and ensure proper functionality through unit testing.","solution":"import zlib class DataCompressionUtility: @staticmethod def compress_data(data: bytes, level: int = -1) -> bytes: try: return zlib.compress(data, level) except zlib.error as e: raise RuntimeError(f\\"Compression failed: {e}\\") @staticmethod def decompress_data(data: bytes, wbits: int = zlib.MAX_WBITS) -> bytes: try: return zlib.decompress(data, wbits) except zlib.error as e: raise RuntimeError(f\\"Decompression failed: {e}\\") @staticmethod def compute_adler32(data: bytes, value: int = 1) -> int: return zlib.adler32(data, value) @staticmethod def compute_crc32(data: bytes, value: int = 0) -> int: return zlib.crc32(data, value) @staticmethod def compress_stream(input_data: list, level: int = -1) -> bytes: compressor = zlib.compressobj(level) compressed = b\'\'.join(compressor.compress(part) for part in input_data) compressed += compressor.flush() return compressed @staticmethod def decompress_stream(compressed_data: bytes, wbits: int = zlib.MAX_WBITS) -> bytes: decompressor = zlib.decompressobj(wbits) decompressed = decompressor.decompress(compressed_data) decompressed += decompressor.flush() return decompressed"},{"question":"**Problem Statement:** You are required to generate a series of custom color palettes using the `sns.husl_palette()` function from the seaborn package. This task will assess your understanding of creating and customizing palettes with different parameters. **Task:** 1. **Default Palette:** Create a default husl palette and return it as a list of colors in RGB format. 2. **Custom Palette with Increased Colors:** Create a husl palette with 10 colors and return it as a list of colors in RGB format. 3. **Adjusting Lightness:** Create a husl palette with 6 colors and a lightness level of 0.5. Return it as a list of colors in RGB format. 4. **Adjusting Saturation:** Create a husl palette with 6 colors and a saturation level of 0.3. Return it as a list of colors in RGB format. 5. **Custom Hue Start Point:** Create a husl palette with 6 colors and a starting hue of 0.6. Return it as a list of colors in RGB format. 6. **Continuous Colormap:** Create a continuous husl colormap with 6 colors and return it. **Input:** None **Output:** A dictionary with the following keys and values: - \\"default_palette\\": List of RGB Colors for the default palette. - \\"custom_palette_10\\": List of RGB Colors for the custom palette with 10 colors. - \\"lightness_palette\\": List of RGB Colors for the palette with lightness 0.5. - \\"saturation_palette\\": List of RGB Colors for the palette with saturation 0.3. - \\"hue_start_palette\\": List of RGB Colors for the palette with starting hue 0.6. - \\"continuous_colormap\\": The continuous colormap. **Example Output:** ```python { \\"default_palette\\": [(0.6274509803921569, 0.8666666666666667, 1.0), (1.0, 0.803921568627451, 0.803921568627451), ...], \\"custom_palette_10\\": [(0.6274509803921569, 0.8666666666666667, 1.0), ...], \\"lightness_palette\\": [(0.369, 0.525, 0.665), ...], \\"saturation_palette\\": [(0.830, 0.830, 0.830), ...], \\"hue_start_palette\\": [(0.974, 0.500, 0.887), ...], \\"continuous_colormap\\": <matplotlib.colors.ListedColormap object> } ``` **Constraints:** - Use seaborn version compatible with Python 3.10.6. - Avoid hardcoding the color values; always use the `sns.husl_palette` function to generate them. **Implementation:** To achieve the above, you need to write a function `generate_palettes()` which returns the required dictionary. Use appropriate seaborn functions to generate the color palettes. ```python import seaborn as sns def generate_palettes(): result = {} # Default Palette default_palette = sns.husl_palette().as_hex() result[\\"default_palette\\"] = sns.husl_palette() # Custom Palette with 10 colors custom_palette_10 = sns.husl_palette(10) result[\\"custom_palette_10\\"] = custom_palette_10 # Adjusting Lightness lightness_palette = sns.husl_palette(l=0.5) result[\\"lightness_palette\\"] = lightness_palette # Adjusting Saturation saturation_palette = sns.husl_palette(s=0.3) result[\\"saturation_palette\\"] = saturation_palette # Custom Hue Start Point hue_start_palette = sns.husl_palette(h=0.6) result[\\"hue_start_palette\\"] = hue_start_palette # Continuous Colormap continuous_colormap = sns.husl_palette(as_cmap=True) result[\\"continuous_colormap\\"] = continuous_colormap return result ``` Make sure to test your function to ensure it produces the correct output for all keys.","solution":"import seaborn as sns def generate_palettes(): Generate a dictionary of custom color palettes using sns.husl_palette(). Returns: dict: A dictionary containing the following keys and their respective palettes: - \\"default_palette\\": List of RGB Colors for the default palette - \\"custom_palette_10\\": List of RGB Colors for the custom palette with 10 colors - \\"lightness_palette\\": List of RGB Colors for the palette with lightness 0.5 - \\"saturation_palette\\": List of RGB Colors for the palette with saturation 0.3 - \\"hue_start_palette\\": List of RGB Colors for the palette with starting hue 0.6 - \\"continuous_colormap\\": The continuous colormap result = {} # Default Palette default_palette = sns.husl_palette() result[\\"default_palette\\"] = list(default_palette) # Custom Palette with 10 colors custom_palette_10 = sns.husl_palette(10) result[\\"custom_palette_10\\"] = list(custom_palette_10) # Adjusting Lightness lightness_palette = sns.husl_palette(6, l=0.5) result[\\"lightness_palette\\"] = list(lightness_palette) # Adjusting Saturation saturation_palette = sns.husl_palette(6, s=0.3) result[\\"saturation_palette\\"] = list(saturation_palette) # Custom Hue Start Point hue_start_palette = sns.husl_palette(6, h=0.6) result[\\"hue_start_palette\\"] = list(hue_start_palette) # Continuous Colormap continuous_colormap = sns.husl_palette(6, as_cmap=True) result[\\"continuous_colormap\\"] = continuous_colormap return result"},{"question":"Objective: To assess your understanding of Python\'s `spwd` module and your ability to work with Unix shadow password databases, you are required to write a function that extracts specific user information based on certain criteria. Problem Statement: You need to implement a function `get_users_with_expiring_passwords` that retrieves the login names of users whose passwords are set to expire within the next 7 days. Function Signature: ```python def get_users_with_expiring_passwords() -> list: ``` Input: - The function does not take any parameters. Output: - The function returns a list of strings, where each string is a login name (`sp_namp`) of a user whose password is set to expire within 7 days from the current date. Constraints: - You must have appropriate privileges to access the shadow password database. - The system date can be retrieved using `datetime.datetime.now()`. - Consider the current date and convert `sp_lstchg`, `sp_max` into a comparable date format to determine expiration. - Handle potential errors gracefully, particularly `PermissionError` if the user does not have access rights. Additional Notes: - Use the `spwd` module to get the shadow password entries. - Assume that today\'s date can be retrieved and handled using the `datetime` module. - Shadow password entries\' date attributes work on a day count basis starting from the Unix epoch (1970-01-01). Example Usage: ```python from datetime import datetime def get_users_with_expiring_passwords(): import spwd import datetime result = [] try: entries = spwd.getspall() current_date = datetime.datetime.now() epoch_start = datetime.datetime(1970, 1, 1) for entry in entries: change_date = epoch_start + datetime.timedelta(days=entry.sp_lstchg) expiration_date = change_date + datetime.timedelta(days=entry.sp_max) days_until_expiration = (expiration_date - current_date).days if 0 <= days_until_expiration <= 7: result.append(entry.sp_namp) except PermissionError: print(\\"Permission denied. Ensure you have root privileges\\") return result # Sample function call (Note: This requires actual root access) print(get_users_with_expiring_passwords()) ``` **Important**: This function should be executed on a Unix system with appropriate privileges to view the shadow password database. Ensure that you handle permission errors appropriately.","solution":"def get_users_with_expiring_passwords(): import spwd import datetime result = [] try: entries = spwd.getspall() current_date = datetime.datetime.now() epoch_start = datetime.datetime(1970, 1, 1) for entry in entries: change_date = epoch_start + datetime.timedelta(days=entry.sp_lstchg) expiration_date = change_date + datetime.timedelta(days=entry.sp_max) days_until_expiration = (expiration_date - current_date).days if 0 <= days_until_expiration <= 7: result.append(entry.sp_namp) except PermissionError: print(\\"Permission denied. Ensure you have root privileges\\") return result"},{"question":"**Data Visualization with Seaborn** # Background You are provided with a dataset that contains information about the performance of students in different subjects over a span of several years. The dataset includes the following columns: - `year`: The year in which the data was recorded. - `student_id`: Unique identifier for each student. - `subject`: The subject in which the student was evaluated (Math, Science, English, etc.). - `score`: The score obtained by the student in the respective subject. The challenge is to visualize trends and patterns in the data over the years using seaborn. You will be required to transform the data into appropriate formats and create specific visualizations. # Dataset ```python import pandas as pd data = { \'year\': [2018, 2018, 2018, 2019, 2019, 2019, 2020, 2020, 2020, 2021, 2021, 2021], \'student_id\': [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \'subject\': [\'Math\', \'Science\', \'English\', \'Math\', \'Science\', \'English\', \'Math\', \'Science\', \'English\', \'Math\', \'Science\', \'English\'], \'score\': [85, 90, 78, 88, 85, 83, 91, 78, 85, 89, 94, 90] } df = pd.DataFrame(data) print(df) ``` # Task 1. **Transform Data:** - Convert the dataset into a long-form format using `pandas.melt()` if necessary. - Create a wide-form version of the dataset using `pandas.pivot()`. 2. **Visualizations:** - Create a line plot showing the trend of average scores in each subject over the years using the long-form version of the dataset. - Create a similar line plot using the wide-form version of the dataset. 3. **Advanced Visualization:** - Create a scatter plot displaying individual student scores for each subject over the years. Ensure that the plot includes relevant labels and legends. # Requirements - The solution should demonstrate usage of seaborn for data visualization. - Proper transformation of data using pandas functions (`melt`, `pivot`, etc.). - Usage of seaborn functions like `sns.relplot`, `sns.catplot`, etc., to create the visualizations. - The plots must be clear, with proper labels, legends, and titles. # Input - The input will be in the form of a DataFrame similar to the one provided in the dataset section. # Output - Code that successfully transforms the data and generates the required visualizations. - Display the created plots. # Constraints - Assume all subjects are evaluated every year for every student. - The dataset will have a consistent structure as provided in the example. **Performance Requirements:** - The solution should efficiently handle data transformation and visualization for datasets up to 10,000 rows.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Dataset data = { \'year\': [2018, 2018, 2018, 2019, 2019, 2019, 2020, 2020, 2020, 2021, 2021, 2021], \'student_id\': [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \'subject\': [\'Math\', \'Science\', \'English\', \'Math\', \'Science\', \'English\', \'Math\', \'Science\', \'English\', \'Math\', \'Science\', \'English\'], \'score\': [85, 90, 78, 88, 85, 83, 91, 78, 85, 89, 94, 90] } df = pd.DataFrame(data) # Data transformation to long-form (already in long-form format) long_df = df.copy() # Data transformation to wide-form using pivot wide_df = df.pivot(index=\'year\', columns=\'subject\', values=\'score\').reset_index() # Visualization # Line plot using long-form data plt.figure(figsize=(10, 6)) sns.lineplot(data=long_df, x=\'year\', y=\'score\', hue=\'subject\', marker=\'o\') plt.title(\'Average Scores in Each Subject Over the Years (Long-Form Data)\') plt.xlabel(\'Year\') plt.ylabel(\'Average Score\') plt.legend(title=\'Subject\') plt.show() # Line plot using wide-form data plt.figure(figsize=(10, 6)) sns.lineplot(data=wide_df, x=\'year\', y=\'Math\', marker=\'o\', label=\'Math\') sns.lineplot(data=wide_df, x=\'year\', y=\'Science\', marker=\'o\', label=\'Science\') sns.lineplot(data=wide_df, x=\'year\', y=\'English\', marker=\'o\', label=\'English\') plt.title(\'Average Scores in Each Subject Over the Years (Wide-Form Data)\') plt.xlabel(\'Year\') plt.ylabel(\'Average Score\') plt.legend(title=\'Subject\') plt.show() # Scatter plot for individual student scores plt.figure(figsize=(12, 8)) sns.scatterplot(data=long_df, x=\'year\', y=\'score\', hue=\'subject\', style=\'subject\', s=100, markers=[\'o\', \'s\', \'D\']) plt.title(\'Individual Student Scores Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Score\') plt.legend(title=\'Subject\') plt.show()"},{"question":"# Question: Bias-Variance Analysis using Validation and Learning Curves You are provided with a dataset and a machine learning model. Your task is to: 1. Plot the **validation curve** for the model using a specified hyperparameter. 2. Plot the **learning curve** for the same model to observe how varying the training size affects the model\'s performance. Your function should: - Accept the dataset features `X`, target `y`, the model, and the parameter `param_name` for the validation curve. - Plot and save the validation curve. - Plot and save the learning curve. - Return an analysis of the model\'s performance, highlighting any issues of bias or variance based on the curves. # Function Signature: ```python def plot_model_analysis(X, y, model, param_name, param_range): Parameters: - X : array-like of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. - y : array-like of shape (n_samples,) Target relative to X for classification or regression; single output. - model : estimator instance An object of that type used to fit the data. - param_name : string Name of the parameter that will be varied in the validation curve. - param_range : array-like The values of the parameter to evaluate. Returns: - analysis: str An analysis of the model\'s bias and variance based on the plots. ``` # Example Usage: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Define model model = SVC(kernel=\\"linear\\") # Define parameter range for validation curve param_range = np.logspace(-6, -1, 5) # Call the function analysis = plot_model_analysis(X, y, model, \\"C\\", param_range) print(analysis) ``` **Constraints:** - The function should handle classification problems. - Ensure to set appropriate seeds for reproducibility. - Use cross-validation with at least `5` folds for both curves. **Output:** - Two plots saved in the current directory: `validation_curve.png` and `learning_curve.png`. - A textual analysis explaining the model\'s behavior based on the generated curves. # Sample Analysis Output: \\"The validation curve shows that increasing the hyperparameter `C` improves the training accuracy but the validation accuracy drops for high values, indicating overfitting for high `C` values. The learning curve suggests that the model has a high bias given the convergence of training and validation scores to a low value. Increasing the training data does not significantly improve the model performance.\\"","solution":"import numpy as np from sklearn.model_selection import validation_curve, learning_curve import matplotlib.pyplot as plt def plot_model_analysis(X, y, model, param_name, param_range): Plots the validation curve and learning curve for a given model and parameter range. Parameters: - X : array-like of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. - y : array-like of shape (n_samples,) Target relative to X for classification or regression; single output. - model : estimator instance An object of that type used to fit the data. - param_name : string Name of the parameter that will be varied in the validation curve. - param_range : array-like The values of the parameter to evaluate. Returns: - analysis: str An analysis of the model\'s bias and variance based on the plots. # Validation Curve train_scores, test_scores = validation_curve(model, X, y, param_name=param_name, param_range=param_range, cv=5, scoring=\'accuracy\') train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure(figsize=(8, 6)) plt.title(f\'Validation Curve with {model.__class__.__name__}\') plt.xlabel(param_name) plt.ylabel(\'Score\') plt.ylim(0.0, 1.1) plt.semilogx(param_range, train_scores_mean, label=\'Training score\', color=\'darkorange\', lw=2) plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\'darkorange\', lw=2) plt.semilogx(param_range, test_scores_mean, label=\'Cross-validation score\', color=\'navy\', lw=2) plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\'navy\', lw=2) plt.legend(loc=\'best\') plt.grid() plt.savefig(\'validation_curve.png\') # Learning Curve train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10), scoring=\'accuracy\') train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure(figsize=(8, 6)) plt.title(f\'Learning Curve with {model.__class__.__name__}\') plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.plot(train_sizes, train_scores_mean, \'o-\', color=\\"r\\", label=\\"Training score\\") plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"r\\") plt.plot(train_sizes, test_scores_mean, \'o-\', color=\\"g\\", label=\\"Cross-validation score\\") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"g\\") plt.legend(loc=\'best\') plt.grid() plt.savefig(\'learning_curve.png\') # Analysis analysis = \\"\\" if np.max(test_scores_mean) < 0.6: analysis += \\"The model shows high bias as the cross-validation scores are low across all parameter values.n\\" else: analysis += \\"The model\'s cross-validation scores seem acceptable with some or all parameter values.n\\" if (test_scores_mean > train_scores_mean).any(): analysis += \\"There is a presence of high variance since the cross-validation score is much lower than the training score for some parameter values.\\" else: analysis += \\"There seems to be low variance as the cross-validation score closely follows the training score.\\" return analysis"},{"question":"# Question: Parse and Analyze an XML Document with Incremental Parsing In this task, you are required to implement an Incremental SAX parser in Python using the `xml.sax.xmlreader.IncrementalParser` class. Your parser should be capable of processing an XML document in chunks and extracting specific data from the XML elements. Requirements: 1. Implement a class `MyIncrementalParser` inheriting from `xml.sax.xmlreader.IncrementalParser`. 2. The `MyIncrementalParser` class should: - Have a method `parse_chunk(chunk)` to process a chunk of XML data. - Use a `ContentHandler` to extract and print the names and attributes of elements as they are parsed. - Ensure the parser can handle large XML files by processing them incrementally in chunks. 3. The parser should stop parsing and print a message if an element with a specific name (e.g., \\"StopElement\\") is encountered. Input: - An XML string divided into multiple chunks. Output: - Names and attributes of elements as they are parsed. - A termination message if the \\"StopElement\\" is encountered. Example Usage: ```python class MyContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Element: {name}, Attributes: {dict(attrs)}\\") if name == \\"StopElement\\": print(\\"StopElement encountered. Stopping parsing.\\") raise xml.sax.SAXException(\\"Stop parsing\\") class MyIncrementalParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): super().__init__() self._handler = MyContentHandler() self.setContentHandler(self._handler) def parse_chunk(self, chunk): try: self.feed(chunk) except xml.sax.SAXException: self.close() # Example data and usage chunks = [ \\"<root>\\", \\"<child attr=\'value\'>\\", \\"<StopElement/>\\", \\"</child>\\", \\"</root>\\" ] parser = MyIncrementalParser() for chunk in chunks: parser.parse_chunk(chunk) ``` Constraints: - Ensure your implementation handles malformed XML gracefully and stops parsing if an error is encountered. - Make sure the `MyContentHandler` is used correctly for handling SAX events. Performance: - The parser should efficiently handle large XML files by processing them in manageable chunks. - Ensure minimal memory usage by not loading the entire XML file into memory at once. Implement the `MyIncrementalParser` class according to the above specifications and demonstrate its usage with a provided XML string.","solution":"import xml.sax class MyContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Element: {name}, Attributes: {dict(attrs)}\\") if name == \\"StopElement\\": print(\\"StopElement encountered. Stopping parsing.\\") raise xml.sax.SAXException(\\"Stop parsing\\") class MyIncrementalParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): super(MyIncrementalParser, self).__init__() self._handler = MyContentHandler() self.setContentHandler(self._handler) self._parser = xml.sax.make_parser() self._parser.setContentHandler(self._handler) self._started = False def parse_chunk(self, chunk): if not self._started: self._parser.feed(chunk) self._started = True else: try: self._parser.feed(chunk) except xml.sax.SAXException: self._parser.close() raise # Example data and usage chunks = [ \\"<root>\\", \\"<child attr=\'value\'>\\", \\"<StopElement/>\\", \\"</child>\\", \\"</root>\\" ] parser = MyIncrementalParser() for chunk in chunks: try: parser.parse_chunk(chunk) except xml.sax.SAXException: break"},{"question":"**Coding Assessment Question: Custom Transformer and Pipeline Integration** # Objective Your task is to implement a custom transformer in scikit-learn that performs a specific data transformation task. Then, integrate this custom transformer into a pipeline with other standard scikit-learn transformers to perform a series of transformations on a sample dataset. # Problem Statement Create a custom transformer named `LogTransformer` that applies the natural logarithm to each element in the input feature matrix. The transformer should handle cases where the input value is zero or negative by mapping these to a small positive value (e.g., `1e-9`). Once you\'ve implemented `LogTransformer`, use it in a pipeline that also standardizes the features using `StandardScaler` and then trains a `LinearRegression` model. Use this pipeline to fit a provided training dataset and make predictions on a test dataset. # Instructions 1. Implement the `LogTransformer` class with `fit`, `transform`, and `fit_transform` methods. 2. Integrate `LogTransformer` into a scikit-learn pipeline along with `StandardScaler` and `LinearRegression`. 3. Fit the pipeline to the provided training data. 4. Make predictions using the trained pipeline on the provided test data. # Input Format - Two 2D numpy arrays, `X_train` and `X_test`, representing the training and test feature matrices, respectively. - Two 1D numpy arrays, `y_train` and `y_test`, representing the training and test target values, respectively. # Output Format - A 1D numpy array representing the predictions made by the pipeline on the `X_test` dataset. # Constraints - You may assume that all necessary packages such as `numpy` and `scikit-learn` are already installed. - The input arrays are non-empty and of compatible dimensions for linear regression. # Performance Requirements - Your solution should handle datasets with up to 10,000 samples efficiently. ```python # Example template for LogTransformer class and pipeline integration import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self, epsilon=1e-9): self.epsilon = epsilon def fit(self, X, y=None): # No fitting necessary for this transformer return self def transform(self, X): # Apply natural log transformation return np.log(X + self.epsilon) def fit_transform(self, X, y=None): return self.fit(X, y).transform(X) # Function to create and train the pipeline def train_pipeline(X_train, y_train): pipeline = Pipeline([ (\'log_transform\', LogTransformer()), (\'scaler\', StandardScaler()), (\'regressor\', LinearRegression()) ]) # Fit the pipeline on the training data pipeline.fit(X_train, y_train) return pipeline # Function to make predictions using the pipeline def make_predictions(pipeline, X_test): return pipeline.predict(X_test) # Example usage # X_train, X_test, y_train, y_test = <your_data_here> # pipeline = train_pipeline(X_train, y_train) # predictions = make_predictions(pipeline, X_test) ``` The above code template provides a structure for implementing the custom transformer and integrating it into a scikit-learn pipeline. Make sure to follow the instructions and utilize the template to complete the task.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self, epsilon=1e-9): self.epsilon = epsilon def fit(self, X, y=None): # No fitting necessary for this transformer return self def transform(self, X): # Apply natural log transformation return np.log(np.where(X < self.epsilon, self.epsilon, X)) def fit_transform(self, X, y=None): return self.fit(X, y).transform(X) # Function to create and train the pipeline def train_pipeline(X_train, y_train): pipeline = Pipeline([ (\'log_transform\', LogTransformer()), (\'scaler\', StandardScaler()), (\'regressor\', LinearRegression()) ]) # Fit the pipeline on the training data pipeline.fit(X_train, y_train) return pipeline # Function to make predictions using the pipeline def make_predictions(pipeline, X_test): return pipeline.predict(X_test)"},{"question":"**Regex Pattern Matching and String Manipulation** # Background: You are given a large text document consisting of multiple lines. Each line in the document can contain various types of data including dates, email addresses, URLs, telephone numbers, and more. Your task is to extract and manipulate specific types of data from this text document using Python\'s `re` module. # Task: 1. Write a Python function `extract_data` that takes a string representing the text document and returns a dictionary with the extracted data: - Extract all email addresses. - Extract all URLs. - Extract all telephone numbers following the format `XXX-XXX-XXXX`. - Extract all dates in `DD/MM/YYYY` format. # Function Signature: ```python def extract_data(text: str) -> dict: # Your code here pass ``` # Input: - `text`: A string representing the text document. Each piece of data might appear multiple times and can be located anywhere within the document. # Output: - A dictionary with the following structure: ```python { \\"emails\\": List[str], # List of all extracted email addresses \\"urls\\": List[str], # List of all extracted URLs \\"phone_numbers\\": List[str],# List of all extracted phone numbers \\"dates\\": List[str] # List of all extracted dates } ``` # Constraints: - Emails should be validated using the regex `r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\'`. - URLs should be validated using the regex `r\'bhttps?://[a-zA-Z0-9./_-]+\'`. - Phone numbers must follow the pattern `XXX-XXX-XXXX` where each `X` is a digit. - Dates must be in `DD/MM/YYYY` format. # Example: ```python text = Jane\'s email: jane.doe@example.com, contact: 123-456-7890. Meeting on 25/12/2021. For more info, visit: https://www.example.com or reach out at john.doe@company.org. Another date 01/01/2020, another phone: 987-654-3210. output = extract_data(text) # output should be: # { # \\"emails\\": [\\"jane.doe@example.com\\", \\"john.doe@company.org\\"], # \\"urls\\": [\\"https://www.example.com\\"], # \\"phone_numbers\\": [\\"123-456-7890\\", \\"987-654-3210\\"], # \\"dates\\": [\\"25/12/2021\\", \\"01/01/2020\\"] # } ``` # Notes: - Make sure the function handles edge cases such as no matches found. - Performance should be considered, especially with very large documents. # Hints: - Use `re.findall()` to locate all matches for each pattern within the text. - Organize the regex patterns and ensure they are properly compiled for efficiency. - Handle overlapping patterns cautiously to ensure correct extraction without duplicating data.","solution":"import re def extract_data(text: str) -> dict: Extracts emails, URLs, phone numbers, and dates from the given text document. Parameters: text (str): The text document as a string. Returns: dict: A dictionary containing lists of extracted emails, URLs, phone numbers, and dates. email_pattern = re.compile(r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\') url_pattern = re.compile(r\'bhttps?://[a-zA-Z0-9./_-]+\') phone_pattern = re.compile(r\'bd{3}-d{3}-d{4}b\') date_pattern = re.compile(r\'bd{2}/d{2}/d{4}b\') emails = email_pattern.findall(text) urls = url_pattern.findall(text) phone_numbers = phone_pattern.findall(text) dates = date_pattern.findall(text) return { \\"emails\\": emails, \\"urls\\": urls, \\"phone_numbers\\": phone_numbers, \\"dates\\": dates }"},{"question":"Objective: Demonstrate your understanding of the `cgitb` module and your ability to implement a robust error handling mechanism in a Python script. Problem Statement: You are required to write a Python script that performs the following tasks: 1. Defines a function `divide_numbers(a, b)` that takes two arguments `a` and `b` and returns their division result. 2. Includes error handling to manage division by zero using the `cgitb` module. 3. Customizes the `cgitb` module to log errors to a file named `error_log.html` formatted as HTML and displays the traceback in the console as plain text. 4. Calls the `divide_numbers` function with various test cases, including cases that would trigger exceptions. Requirements: 1. **Function Signature:** Define the function with the following signature: ```python def divide_numbers(a: float, b: float) -> float: ``` 2. **Exception Handling:** Implement the `cgitb` module to handle any exceptions, particularly focusing on `ZeroDivisionError`. 3. **Customization:** Customize the `cgitb` settings to: - Log errors to an HTML file named `error_log.html`. - Display the traceback in plain text in the console. 4. **Test Cases:** Call the `divide_numbers` function with at least four different pairs of numbers, ensuring at least one pair causes a division by zero exception. 5. **Output:** The function should print the result of the division if no exception occurs, and in the case of an exception, handle it using `cgitb`. Constraints: - You should not use any modules other than `cgitb` and standard Python libraries. Example: Define the function and handle exceptions as follows: ```python import cgitb cgitb.enable(display=0, logdir=\'.\', format=\'html\') def divide_numbers(a: float, b: float) -> float: try: return a / b except ZeroDivisionError: cgitb.handler() raise # Test cases test_cases = [(10, 2), (8, 0), (5, 5), (9, -3)] for a, b in test_cases: try: result = divide_numbers(a, b) print(f\\"The result of {a}/{b} is {result}\\") except ZeroDivisionError: print(f\\"Failed to divide {a} by {b}\\") ``` In this example: - The `divide_numbers` function is defined to perform division and handle `ZeroDivisionError`. - `cgitb` is configured to log errors to `error_log.html` and print plain text tracebacks in the console. - Test cases are provided to trigger and handle exceptions appropriately.","solution":"import cgitb cgitb.enable(display=1, logdir=\'.\', format=\'html\') def divide_numbers(a: float, b: float) -> float: try: return a / b except ZeroDivisionError: cgitb.handler() raise"},{"question":"Objective Your task is to implement equivalent functionality in Python for creating and working with slice objects, based on the described C API functions. This will test your understanding of Python\'s slice mechanics and your ability to work with slice objects. Problem Statement Implement the following functions: 1. **`is_slice(obj)`**: - **Input**: A single argument `obj`. - **Output**: Returns `True` if `obj` is a slice object, otherwise `False`. - **Example**: ```python is_slice(slice(1, 10, 2)) # True is_slice([1, 2, 3]) # False ``` 2. **`create_slice(start, stop, step)`**: - **Input**: Three arguments `start`, `stop`, and `step` which can be integers or `None`. - **Output**: Returns a new slice object with the given values. - **Example**: ```python create_slice(1, 10, 2) # slice(1, 10, 2) create_slice(None, 4, None) # slice(None, 4, None) ``` 3. **`get_indices(slice_obj, length)`**: - **Input**: A `slice_obj` and the length of the sequence it will be applied to. - **Output**: A tuple `(start, stop, step)` representing the indices obtained from the slice object. - **Example**: ```python get_indices(slice(1, 10, 2), 15) # (1, 10, 2) get_indices(slice(None, 4, None), 10) # (0, 4, 1) ``` 4. **`adjust_indices(length, start, stop, step)`**: - **Input**: A sequence length `length`, and indices `start`, `stop`, and `step`. - **Output**: A tuple `(new_start, new_stop, new_step)` with indices adjusted to be within bounds of a sequence of length `length`. - **Example**: ```python adjust_indices(10, -5, 15, 1) # (0, 10, 1) adjust_indices(5, None, None, -1) # (4, -1, -1) ``` Constraints - You may not use the built-in `slice.indices` method directly. - You should handle cases where start, stop, or step are `None`. - You should manage out-of-bounds indices appropriately. Additional Requirements - Write a few test cases to validate your functions. - Ensure that your code is clean and well-documented, explaining any non-trivial logic or decisions. Good luck!","solution":"def is_slice(obj): Returns True if obj is a slice object, otherwise False. return isinstance(obj, slice) def create_slice(start, stop, step): Returns a new slice object with the given start, stop, and step values. return slice(start, stop, step) def get_indices(slice_obj, length): Returns the indices from slice_obj adjusted for a sequence of given length. start, stop, step = slice_obj.start, slice_obj.stop, slice_obj.step if start is None: start = 0 if step is None or step > 0 else length - 1 if stop is None: stop = length if step is None or step > 0 else -1 if step is None: step = 1 return start, stop, step def adjust_indices(length, start, stop, step): Adjusts the given indices to be within the bounds of a sequence of length. if step is None: step = 1 if start is None: start = 0 if step > 0 else length - 1 elif start < 0: start += length if start < 0: start = 0 if start >= length: start = length if step > 0 else length - 1 if stop is None: stop = length if step > 0 else -1 elif stop < 0: stop += length if stop < 0: stop = -1 if stop >= length: stop = length return start, stop, step"},{"question":"Python Coding Assessment Question # Question: Analyzing and Transforming Data You are given a large dataset represented as a list of dictionaries. Each dictionary in the list contains information about various items, including their name, quantity, and price per unit. Your task is to process the dataset to generate some analytical results and transformations. Specifically, you need to implement a function `process_data` that performs the following steps: 1. **Filter Items**: - Exclude items that have a quantity of zero. 2. **Calculate Total Price**: - For each item, calculate the total price by multiplying the quantity by the price per unit and round it to 2 decimal places. 3. **Group by Category**: - Assume that each item dictionary contains a `category` field. Group items by their category. 4. **Summary Information**: - For each category, calculate the total number of distinct items, the total quantity of all items, and the average price per unit of items in that category (also rounded to 2 decimal places). # Function Signature: ```python def process_data(data: list) -> dict: pass ``` # Input: - `data`: A list of dictionaries, where each dictionary has the following keys: - `name` (str): The name of the item. - `quantity` (int): The quantity of the item. - `price` (float): The price per unit of the item. - `category` (str): The category to which the item belongs. # Output: - A dictionary with the following structure: ```python { \\"filtered_items\\": list, # List of items with non-zero quantity, including \\"total_price\\" added \\"summary\\": { category: { \\"total_items\\": int, # Number of distinct items in the category \\"total_quantity\\": int, # Total quantity of items in the category \\"average_price\\": float # Average price per unit in the category (rounded to 2 decimal places) }, ... } } ``` # Constraints: - You can assume that the input list will not be empty and will always contain valid dictionaries with the specified keys. # Example: ```python data = [ {\\"name\\": \\"apple\\", \\"quantity\\": 10, \\"price\\": 0.5, \\"category\\": \\"fruits\\"}, {\\"name\\": \\"banana\\", \\"quantity\\": 0, \\"price\\": 0.2, \\"category\\": \\"fruits\\"}, {\\"name\\": \\"carrot\\", \\"quantity\\": 5, \\"price\\": 0.3, \\"category\\": \\"vegetables\\"}, {\\"name\\": \\"broccoli\\", \\"quantity\\": 15, \\"price\\": 0.8, \\"category\\": \\"vegetables\\"}, {\\"name\\": \\"strawberry\\", \\"quantity\\": 8, \\"price\\": 1.2, \\"category\\": \\"fruits\\"} ] result = process_data(data) print(result) ``` Expected Output: ```python { \\"filtered_items\\": [ {\\"name\\": \\"apple\\", \\"quantity\\": 10, \\"price\\": 0.5, \\"category\\": \\"fruits\\", \\"total_price\\": 5.0}, {\\"name\\": \\"carrot\\", \\"quantity\\": 5, \\"price\\": 0.3, \\"category\\": \\"vegetables\\", \\"total_price\\": 1.5}, {\\"name\\": \\"broccoli\\", \\"quantity\\": 15, \\"price\\": 0.8, \\"category\\": \\"vegetables\\", \\"total_price\\": 12.0}, {\\"name\\": \\"strawberry\\", \\"quantity\\": 8, \\"price\\": 1.2, \\"category\\": \\"fruits\\", \\"total_price\\": 9.6} ], \\"summary\\": { \\"fruits\\": { \\"total_items\\": 2, \\"total_quantity\\": 18, \\"average_price\\": 0.83 }, \\"vegetables\\": { \\"total_items\\": 2, \\"total_quantity\\": 20, \\"average_price\\": 0.55 } } } ``` # Notes: - Carefully handle rounding of floating-point numbers to 2 decimal places for accuracy. - Utilize dictionary and list comprehensions to keep your code clean and Pythonic.","solution":"def process_data(data: list) -> dict: from collections import defaultdict from decimal import Decimal, ROUND_HALF_UP # Step 1: Filter items with non-zero quantity filtered_items = [item for item in data if item[\'quantity\'] > 0] # Step 2: Calculate total price for each item for item in filtered_items: total_price = item[\'quantity\'] * item[\'price\'] item[\'total_price\'] = round(total_price, 2) # Step 3: Group items by category and collect summary information category_summary = defaultdict(lambda: {\'total_items\': 0, \'total_quantity\': 0, \'total_price_sum\': 0.0}) for item in filtered_items: category = item[\'category\'] category_summary[category][\'total_items\'] += 1 category_summary[category][\'total_quantity\'] += item[\'quantity\'] category_summary[category][\'total_price_sum\'] += item[\'price\'] # Step 4: Prepare the summary output summary_output = {} for category, values in category_summary.items(): average_price = values[\'total_price_sum\'] / values[\'total_items\'] average_price = float(Decimal(average_price).quantize(Decimal(\'.01\'), rounding=ROUND_HALF_UP)) summary_output[category] = { \'total_items\': values[\'total_items\'], \'total_quantity\': values[\'total_quantity\'], \'average_price\': average_price } return { \\"filtered_items\\": filtered_items, \\"summary\\": summary_output }"},{"question":"Coding Assessment Question # Objective Implement a Python function that demonstrates the use of the iterator protocol functions provided in the `python310` package. Your task is to create a custom iterator class and utilize the provided functions to iterate over the elements of this custom iterator. # Problem Statement 1. Implement a Python class `CustomIterator` that conforms to the iterator protocol. The iterator should iterate over a range of integers from `start` to `end` (inclusive). 2. Write a function `iterate_custom_iterator(start, end)` that: - Initializes an instance of `CustomIterator` with the provided `start` and `end`. - Uses the `PyIter_Check`, `PyIter_Next`, and `PyIter_Send` functions to iterate over the elements of the custom iterator. - Collects and returns a list of all elements produced by the custom iterator. # Expected Input and Output - Input: Two integers, `start` and `end`, where `start <= end`. - Output: A list of integers from `start` to `end`, inclusive. # Constraints - Ensure that the class and function handle iteration correctly. - The function should work efficiently with large ranges. # Example ```python class CustomIterator: def __init__(self, start, end): self.current = start self.end = end def __iter__(self): return self def __next__(self): if self.current > self.end: raise StopIteration value = self.current self.current += 1 return value def iterate_custom_iterator(start, end): import python310 as py iterator = CustomIterator(start, end) result = [] # Ensure the object is an iterator if not py.PyIter_Check(iterator): raise ValueError(\\"Object is not an iterator\\") item = py.PyIter_Next(iterator) while item: result.append(item) item = py.PyIter_Next(iterator) return result # Example usage print(iterate_custom_iterator(1, 5)) # Output: [1, 2, 3, 4, 5] ``` # Additional Notes - You may assume that you have access to a hypothetical package `python310` that provides the functions described in the documentation. - Your implementation should strictly follow the iterator protocol rules.","solution":"class CustomIterator: def __init__(self, start, end): self.current = start self.end = end def __iter__(self): return self def __next__(self): if self.current > self.end: raise StopIteration value = self.current self.current += 1 return value def iterate_custom_iterator(start, end): iterator = CustomIterator(start, end) result = [] # Manually iterating over the iterator for item in iterator: result.append(item) return result"},{"question":"**Question:** You are given a dataset containing model performance on various NLP tasks. Your task is to create a customized Seaborn plot with annotations. Follow the steps below to accomplish the task: 1. Load the dataset named **\\"glue\\"** using `seaborn.load_dataset()`. 2. Pivot the data to have `Model` and `Encoder` as the index, `Task` as the columns, and `Score` as the values. 3. Add a new column `Average` that contains the average score across all tasks for each model, rounded to one decimal place, and sort the dataframe by `Average` in descending order. 4. Create a bar plot with the sorted data: - Use `Model` as the y-axis values and `Average` as the x-axis values. - Add text annotations to the right side of each bar showing the average score. 5. Customize the text annotations: - Set the text color to white (`w`). - Horizontally align the text (`halign`) to the right side of each bar with an offset of 6. 6. Provide a final plot with the above specifications. **Input:** No explicit input is required other than using the provided \\"glue\\" dataset. **Output:** A plot with the specified customizations. **Constraints and Requirements:** - Ensure that Seaborn and its `objects` interface is used to handle plotting and annotations. - The plot should be clear, and text annotations should not overlap. - Data manipulation steps should make use of Pandas functions as demonstrated. Here\'s a starting template: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset glue = load_dataset(\\"glue\\") # Step 2: Pivot the data glue_pivot = glue.pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") # Step 3: Add \'Average\' column and sort the data glue_pivot[\\"Average\\"] = glue_pivot.mean(axis=1).round(1) glue_sorted = glue_pivot.sort_values(\\"Average\\", ascending=False) # Step 4: Create the bar plot plot = ( so.Plot(glue_sorted, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Step 6: Display the plot plot.show() ``` Feel free to modify and expand this template to meet all required specifications.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def create_customized_seaborn_plot(): # Step 1: Load the dataset glue = load_dataset(\\"glue\\") # Step 2: Pivot the data glue_pivot = glue.pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") # Step 3: Add \'Average\' column and sort the data glue_pivot[\\"Average\\"] = glue_pivot.mean(axis=1).round(1) glue_sorted = glue_pivot.sort_values(\\"Average\\", ascending=False).reset_index() # Step 4: Create the bar plot plot = ( so.Plot(glue_sorted, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Step 6: Display the plot plot.show() # Note: Uncomment the following line to generate the plot when running this script. # create_customized_seaborn_plot()"},{"question":"**Question:** You have been provided with a dataset that contains the sales performance of different products over a period of months. Your task is to create and customize visualizations using the seaborn library to show this data effectively. Follow the steps below to complete this task: 1. **Data Preparation:** - Create a DataFrame with the following data: | Month | Product | Sales | |-------|---------|-------| | Jan | A | 100 | | Jan | B | 150 | | Jan | C | 200 | | Feb | A | 200 | | Feb | B | 180 | | Feb | C | 250 | | Mar | A | 300 | | Mar | B | 220 | | Mar | C | 270 | 2. **Visualization 1:** - Use the seaborn library to create a bar plot showing the total sales of each product over the three months. - Apply the default seaborn theme. 3. **Visualization 2:** - Create a line plot showing the sales trend of each product over the three months. - Set a custom theme with a whitegrid style and a pastel palette. 4. **Visualization 3:** - Create a point plot showing the sales of each product for each month. - Set the theme to \'ticks\' style. - Disable the top and right spines using custom parameters. 5. **Additional Customization:** - For all visualizations, ensure that the plots have appropriate titles, axis labels, and legend. **Constraints:** - You must use the seaborn library for creating the plots. - Ensure that setting themes and custom parameters affect all the relevant plots. **Input Format:** N/A (you have to create the DataFrame within the code) **Output Format:** - Three plots as described above, properly customized and labeled. **Performance Requirements:** - The code should be efficient and should not take an unreasonable amount of time to execute. **Example Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Step 1: Data Preparation data = { \'Month\': [\'Jan\', \'Jan\', \'Jan\', \'Feb\', \'Feb\', \'Feb\', \'Mar\', \'Mar\', \'Mar\'], \'Product\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Sales\': [100, 150, 200, 200, 180, 250, 300, 220, 270] } df = pd.DataFrame(data) # Step 2: Visualization 1 sns.set_theme() # Apply default theme plt.figure() sns.barplot(data=df, x=\'Product\', y=\'Sales\', estimator=sum) plt.title(\'Total Sales of Each Product Over Three Months\') plt.xlabel(\'Product\') plt.ylabel(\'Total Sales\') plt.show() # Step 3: Visualization 2 sns.set_theme(style=\'whitegrid\', palette=\'pastel\') # Custom theme plt.figure() sns.lineplot(data=df, x=\'Month\', y=\'Sales\', hue=\'Product\') plt.title(\'Sales Trend of Each Product Over Three Months\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.show() # Step 4: Visualization 3 custom_params = {\'axes.spines.right\': False, \'axes.spines.top\': False} sns.set_theme(style=\'ticks\', rc=custom_params) # Custom theme with ticks and disabled spines plt.figure() sns.pointplot(data=df, x=\'Month\', y=\'Sales\', hue=\'Product\') plt.title(\'Monthly Sales of Each Product\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Step 1: Data Preparation data = { \'Month\': [\'Jan\', \'Jan\', \'Jan\', \'Feb\', \'Feb\', \'Feb\', \'Mar\', \'Mar\', \'Mar\'], \'Product\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Sales\': [100, 150, 200, 200, 180, 250, 300, 220, 270] } df = pd.DataFrame(data) # Step 2: Visualization 1 sns.set_theme() # Apply default theme plt.figure() sns.barplot(data=df, x=\'Product\', y=\'Sales\', estimator=sum) plt.title(\'Total Sales of Each Product Over Three Months\') plt.xlabel(\'Product\') plt.ylabel(\'Total Sales\') plt.legend(title=\'Product\') plt.show() # Step 3: Visualization 2 sns.set_theme(style=\'whitegrid\', palette=\'pastel\') # Custom theme plt.figure() sns.lineplot(data=df, x=\'Month\', y=\'Sales\', hue=\'Product\') plt.title(\'Sales Trend of Each Product Over Three Months\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.legend(title=\'Product\') plt.show() # Step 4: Visualization 3 custom_params = {\'axes.spines.right\': False, \'axes.spines.top\': False} sns.set_theme(style=\'ticks\', rc=custom_params) # Custom theme with ticks and disabled spines plt.figure() sns.pointplot(data=df, x=\'Month\', y=\'Sales\', hue=\'Product\') plt.title(\'Monthly Sales of Each Product\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.legend(title=\'Product\') plt.show()"},{"question":"**Objective**: Create a Python utility script to analyze the modules and submodules within a specified package, gather specific details about them, and return a structured summary. **Task**: Implement a function `analyze_package(package_name: str) -> Dict[str, Any]` that: 1. Takes the name of a package as input. 2. Analyzes the package to collect the following details: - A list of all modules and submodules within the package. - The search path for the package after using `extend_path`. - Details for each module, including whether it is an actual package. - Any data resources in the top-level directory of the package (use `get_data` to retrieve the contents). **Input**: - `package_name` (str): Name of the package to be analyzed. **Output**: - A dictionary with the following structure: ```python { \\"extended_path\\": List[str], \\"modules\\": List[Dict[str, Union[str, bool]]], \\"resources\\": Dict[str, bytes] } ``` Where: - `extended_path` is the search path for the package after extension. - `modules` is a list of dictionaries each having keys: - `\'name\'`: The name of the module/submodule. - `\'is_package\'`: A boolean indicating whether the item is a package. - `resources` is a dictionary mapping resource names to their binary content. **Constraints**: - Assume the package exists and can be imported. - If the package has no resources or modules, the corresponding lists/dictionaries should be empty. **Performance Requirements**: - The function should handle packages efficiently, with a reasonable assumption of around 100 submodules and resources. **Hints**: - Use `pkgutil.extend_path` to get the extended path. - Use `pkgutil.walk_packages` to enumerate modules and submodules. - Use `pkgutil.get_data` to retrieve resources. **Example**: ```python result = analyze_package(\'ctypes\') # Expected output (truncated for brevity) might be: { \'extended_path\': [\'/usr/lib/python3.10/ctypes\'], \'modules\': [ {\'name\': \'ctypes\', \'is_package\': True}, {\'name\': \'ctypes._endian\', \'is_package\': False}, # ... ], \'resources\': { # Example resource entries if there are any } } ``` Implement the function `analyze_package` to meet the specifications above.","solution":"import importlib import pkgutil from types import ModuleType from typing import Dict, List, Union, Any def analyze_package(package_name: str) -> Dict[str, Any]: Analyze a package to collect details about its modules, submodules, paths, and resources. Args: - package_name (str): Name of the package to analyze. Returns: - Dictionary structured with extended_path, modules, and resources. package = importlib.import_module(package_name) package_path = package.__path__ # Extend the path extended_path = list(pkgutil.extend_path(list(package_path), package_name)) # Collect modules and submodules modules = [] for finder, name, ispkg in pkgutil.walk_packages(package.__path__, package.__name__ + \'.\'): modules.append({\'name\': name, \'is_package\': ispkg}) # Collect resources resources = {} if hasattr(package, \'__path__\'): try: for file in pkgutil.iter_modules(package.__path__): resource_content = pkgutil.get_data(package_name, file.name) if resource_content: resources[file.name] = resource_content except Exception as e: # Handle any unexpected exceptions when collecting resources pass return { \'extended_path\': extended_path, \'modules\': modules, \'resources\': resources }"},{"question":"# Seaborn Color Palette Assignment You are given a task to create various color palettes using the seaborn library. The instructions below describe the requirements for each palette. You need to write a function `create_palettes` that sets the seaborn theme and generates these color palettes. # Function Signature ```python def create_palettes(): Creates and returns a dictionary containing various seaborn color palettes based on specified requirements. Returns: dict: A dictionary with palette names as keys and corresponding seaborn color palettes as values. ``` # Requirements 1. **Set Seaborn Theme**: Use the default seaborn theme for all palettes. 2. **Named Color Palette**: - Create a dark palette named \\"seagreen_palette\\" using the color \\"seagreen\\". 3. **Hex Code Color Palette**: - Create a dark palette named \\"hex_palette\\" using the hex code \\"#79C\\". 4. **Husl System Color Palette**: - Create a dark palette named \\"husl_palette\\" using the color `(20, 60, 50)` in the husl color system. 5. **Increased Number of Colors**: - Create a dark palette named \\"golden_palette\\" using the color \\"xkcd:golden\\" with 8 colors in the palette. 6. **Continuous Colormap**: - Create a continuous colormap named \\"continuous_colormap\\" using the hex code \\"#b285bc\\". # Constraints - Use the seaborn library functions appropriately to create each type of palette. - Ensure that the returned dictionary contains the following keys: `\\"seagreen_palette\\"`, `\\"hex_palette\\"`, `\\"husl_palette\\"`, `\\"golden_palette\\"`, and `\\"continuous_colormap\\"`. # Example Output ```python { \\"seagreen_palette\\": [ (list of colors for seagreen palette) ], \\"hex_palette\\": [ (list of colors for hex code #79C palette) ], \\"husl_palette\\": [ (list of colors for husl palette) ], \\"golden_palette\\": [ (list of colors for golden palette) ], \\"continuous_colormap\\": (colormap object for continuous colormap) } ``` # Notes - Each palette should be created using the `sns.dark_palette` function. - Make sure to test your function to ensure it returns the expected output structure.","solution":"import seaborn as sns def create_palettes(): Creates and returns a dictionary containing various seaborn color palettes based on specified requirements. Returns: dict: A dictionary with palette names as keys and corresponding seaborn color palettes as values. sns.set_theme() palettes = { \\"seagreen_palette\\": sns.dark_palette(\\"seagreen\\", as_cmap=False), \\"hex_palette\\": sns.dark_palette(\\"#79C\\", as_cmap=False), \\"husl_palette\\": sns.dark_palette((20, 60, 50), input=\\"husl\\", as_cmap=False), \\"golden_palette\\": sns.dark_palette(\\"xkcd:golden\\", n_colors=8, as_cmap=False), \\"continuous_colormap\\": sns.dark_palette(\\"#b285bc\\", as_cmap=True) } return palettes"},{"question":"**Objective:** To assess your understanding of seaborn\'s plotting capabilities with the `seaborn.objects` module. # Scenario: You are given a dataset containing health expenditure and life expectancy information for various countries across different years. Using this dataset, you need to create a plot that visualizes the trajectory of health expenditure against life expectancy for different countries over time. This will help in understanding how spending correlates with the life expectancy of people in various countries. # Instructions: 1. **Load Dataset:** - Use the `seaborn` library to load the dataset named `healthexp`. 2. **Create Plot:** - Use `seaborn.objects.Plot` to create a plot with: - X-axis representing `\\"Spending_USD\\"`. - Y-axis representing `\\"Life_Expectancy\\"`. - Color encoding for different `Country`. 3. **Add Path:** - Add a `Path` mark to visualize the trajectories through the given variable space without sorting observations before plotting. 4. **Customize Path:** - Customize the path by adding markers with specific properties: - Marker shape as `circle` (`\\"o\\"`). - Point size of `2`. - Line width of `0.75`. - Fill color as `white` (`\\"w\\"`). # Expected Input and Output Formats - **Input:** The `healthexp` dataset should be loaded from seaborn without any modifications. - **Output:** A plot visualizing the health expenditure against life expectancy trajectories for various countries. **Constraints:** - Ensure to use the seaborn library and the `seaborn.objects` module. **Performance Requirements:** - Efficiently load and plot the data to ensure reasonable performance for medium-sized datasets. # Example Code: To assist you in getting started, here is an example code snippet: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") # Add path with customization p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) ``` Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset def create_health_expenditure_plot(): # Load dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot with trajectories p = so.Plot(healthexp, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) # Return the plot object for further display or manipulation return p"},{"question":"# Sequence Manipulation Functions Assessment Objective Your task is to implement a Python function that mimics some of the sequence manipulation functions in the C-API as described above. You will be implementing Python equivalents of the following: 1. `PySequence_Check` 2. `PySequence_Concat` 3. `PySequence_Repeat` 4. `PySequence_GetItem` 5. `PySequence_SetItem` 6. `PySequence_Contains` Details 1. **PySequence_Check** - **Input**: One argument `o` which can be any Python object. - **Output**: Return `True` if the object is a sequence, and `False` otherwise. - **Note**: In Python, sequences include lists, tuples, strings, and any object that implements the `__getitem__` method. 2. **PySequence_Concat** - **Input**: Two arguments `o1` and `o2`, both of which are sequences. - **Output**: Return the concatenation of `o1` and `o2`. - **Example**: `PySequence_Concat([1, 2], [3, 4])` should return `[1, 2, 3, 4]`. 3. **PySequence_Repeat** - **Input**: An argument `o` which is a sequence, and an integer `count`. - **Output**: Return the sequence `o` repeated `count` times. - **Example**: `PySequence_Repeat([1, 2], 3)` should return `[1, 2, 1, 2, 1, 2]`. 4. **PySequence_GetItem** - **Input**: An argument `o` which is a sequence, and an integer `i`. - **Output**: Return the `i`-th item of the sequence `o`. - **Example**: `PySequence_GetItem([1, 2, 3], 1)` should return `2`. 5. **PySequence_SetItem** - **Input**: Three arguments: `o` which is a sequence, `i` which is an integer index, and `v` which is the value to set. - **Output**: Modify the sequence `o` such that the element at index `i` is set to `v`. - **Example**: `PySequence_SetItem([1, 2, 3], 1, 99)` should modify the list to `[1, 99, 3]`. 6. **PySequence_Contains** - **Input**: An argument `o` which is a sequence, and a value `value`. - **Output**: Return `True` if `value` is in the sequence `o`, `False` otherwise. - **Example**: `PySequence_Contains([1, 2, 3], 2)` should return `True`. Constraints - Do not use any external libraries. - Handle edge cases appropriately. - Ensure your functions have proper error handling. Function Definitions You should define the following functions: ```python def PySequence_Check(o): # Your code here def PySequence_Concat(o1, o2): # Your code here def PySequence_Repeat(o, count): # Your code here def PySequence_GetItem(o, i): # Your code here def PySequence_SetItem(o, i, v): # Your code here def PySequence_Contains(o, value): # Your code here ``` Example Usage ```python print(PySequence_Check([1, 2, 3])) # True print(PySequence_Concat([1, 2], [3, 4])) # [1, 2, 3, 4] print(PySequence_Repeat([1, 2], 3)) # [1, 2, 1, 2, 1, 2] print(PySequence_GetItem([1, 2, 3], 1)) # 2 list_ = [1, 2, 3] PySequence_SetItem(list_, 1, 99) print(list_) # [1, 99, 3] print(PySequence_Contains([1, 2, 3], 2)) # True ```","solution":"def PySequence_Check(o): Checks if the object is a sequence. return hasattr(o, \'__getitem__\') and not isinstance(o, (dict, set)) def PySequence_Concat(o1, o2): Concatenates two sequences. return o1 + o2 def PySequence_Repeat(o, count): Repeats a sequence a specified number of times. return o * count def PySequence_GetItem(o, i): Gets the i-th item of the sequence. return o[i] def PySequence_SetItem(o, i, v): Sets the i-th item of the sequence to v. o[i] = v def PySequence_Contains(o, value): Checks if the sequence contains the value. return value in o"},{"question":"You are given a dataset that contains information on certain metrics. Your task is to: 1. Load the provided dataset using pandas. 2. Plot the dataset using seaborn\'s `lineplot` function under different plotting contexts. 3. Save the resulting plots as images. Specifically, you need to: - Create a plot with the default plotting context. - Create a plot with the \\"talk\\" plotting context. - Create a plot with a temporarily applied \\"paper\\" plotting context, within a context manager block. Use the `sns.plotting_context()` function to achieve the required changes in contexts. Ensure your plots are saved as `default_plot.png`, `talk_plot.png`, and `paper_plot.png`. # Input Format There is no need for user input as the dataset and necessary information are provided as part of the task. # Dataset Assume the dataset is a CSV file named `metrics.csv` with the following structure: | Category | Value | |----------|-------| | A | 1 | | B | 3 | | C | 2 | You can load this dataset using `pd.read_csv(\\"metrics.csv\\")`. # Output Format Save the plots as images named `default_plot.png`, `talk_plot.png`, and `paper_plot.png`. # Constraints - Utilize seaborn\'s `sns.plotting_context()` function. - Utilize the `lineplot` function from seaborn to create the plots. - Make sure to use appropriate context settings as specified. # Example Code Your solution should follow the general outline: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\\"metrics.csv\\") # Default plotting context sns.lineplot(x=\'Category\', y=\'Value\', data=data) plt.savefig(\\"default_plot.png\\") plt.clf() # \\"Talk\\" plotting context sns.set_context(\\"talk\\") sns.lineplot(x=\'Category\', y=\'Value\', data=data) plt.savefig(\\"talk_plot.png\\") plt.clf() # Temporarily apply the \\"paper\\" plotting context with sns.plotting_context(\\"paper\\"): sns.lineplot(x=\'Category\', y=\'Value\', data=data) plt.savefig(\\"paper_plot.png\\") plt.clf() ``` Ensure that your code adheres to these requirements and saves the plots with the correct context.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the dataset data = pd.read_csv(\\"metrics.csv\\") # Default plotting context sns.lineplot(x=\'Category\', y=\'Value\', data=data) plt.savefig(\\"default_plot.png\\") plt.clf() # \\"Talk\\" plotting context sns.set_context(\\"talk\\") sns.lineplot(x=\'Category\', y=\'Value\', data=data) plt.savefig(\\"talk_plot.png\\") plt.clf() # Reset to default context to avoid interference sns.set_context(\\"notebook\\") # Temporarily apply the \\"paper\\" plotting context with sns.plotting_context(\\"paper\\"): sns.lineplot(x=\'Category\', y=\'Value\', data=data) plt.savefig(\\"paper_plot.png\\") plt.clf()"},{"question":"**Objective:** To assess your understanding of Python\'s `csv` module, you are required to implement functions for reading, writing, and processing CSV files in both list and dictionary formats. Additionally, you will need to define a custom CSV dialect and handle potential errors during the process. **Task Details:** 1. **Reading CSV into a List of Lists:** - Implement a function `read_csv_list(filename: str) -> list` that reads a CSV file and returns its contents as a list of lists. - Each row in the CSV file should be a sublist of strings. 2. **Writing a List of Lists to CSV:** - Implement a function `write_csv_list(filename: str, data: list) -> None` that writes a list of lists to a CSV file. 3. **Reading CSV into a List of Dictionaries:** - Implement a function `read_csv_dict(filename: str) -> list` that reads a CSV file and returns its contents as a list of dictionaries. - Assume that the first row of the CSV file contains the header which should be used as keys for the dictionaries. 4. **Writing a List of Dictionaries to CSV:** - Implement a function `write_csv_dict(filename: str, data: list, fieldnames: list) -> None` that writes a list of dictionaries to a CSV file. Ensure that the column order in the CSV file follows the order specified in `fieldnames`. 5. **Defining and Using a Custom Dialect:** - Define a custom CSV dialect named `my_dialect` with the following properties: - Delimiter: `|` - Quote character: `\\" ` - Quoting: Only quote minimal (`csv.QUOTE_MINIMAL`) - Implement a function `write_csv_dialect(filename: str, data: list) -> None` that writes a list of lists to a CSV file using the `my_dialect` dialect. 6. **Error Handling:** - Implement error handling in your reading functions to gracefully handle cases where the CSV file is improperly formatted (e.g., incorrect number of fields in a row). Use the `csv.Error` exception to catch such errors and print an appropriate error message. # Function Signatures: ```python def read_csv_list(filename: str) -> list: pass def write_csv_list(filename: str, data: list) -> None: pass def read_csv_dict(filename: str) -> list: pass def write_csv_dict(filename: str, data: list, fieldnames: list) -> None: pass def write_csv_dialect(filename: str, data: list) -> None: pass ``` # Constraints: - You may assume that all CSV files provided will follow RFC 4180 standards. - You need to handle cases where the CSV file has inconsistent numbers of fields in each row. # Example Usage: ```python # Example CSV content for the functions csv_content = [ [\\"Name\\", \\"Age\\", \\"City\\"], [\\"John Doe\\", \\"28\\", \\"New York\\"], [\\"Jane Smith\\", \\"34\\", \\"Los Angeles\\"], [\\"Emily Davis\\", \\"24\\", \\"Chicago\\"] ] # Writing and reading list of lists write_csv_list(\'example.csv\', csv_content) print(read_csv_list(\'example.csv\')) # Writing and reading list of dictionaries data_dict = [ {\\"Name\\": \\"John Doe\\", \\"Age\\": \\"28\\", \\"City\\": \\"New York\\"}, {\\"Name\\": \\"Jane Smith\\", \\"Age\\": \\"34\\", \\"City\\": \\"Los Angeles\\"}, {\\"Name\\": \\"Emily Davis\\", \\"Age\\": \\"24\\", \\"City\\": \\"Chicago\\"} ] write_csv_dict(\'example_dict.csv\', data_dict, [\\"Name\\", \\"Age\\", \\"City\\"]) print(read_csv_dict(\'example_dict.csv\')) # Writing with custom dialect write_csv_dialect(\'example_dialect.csv\', csv_content) ``` **Note:** For all the tasks above, ensure you open the files with `newline=\'\'` in the `with open()` statements to handle newlines correctly.","solution":"import csv def read_csv_list(filename: str) -> list: Reads a CSV file and returns its contents as a list of lists. result = [] try: with open(filename, mode=\'r\', newline=\'\') as file: reader = csv.reader(file) for row in reader: result.append(row) except csv.Error as e: print(f\\"Error reading CSV file at row {reader.line_num}: {e}\\") return result def write_csv_list(filename: str, data: list) -> None: Writes a list of lists to a CSV file. with open(filename, mode=\'w\', newline=\'\') as file: writer = csv.writer(file) writer.writerows(data) def read_csv_dict(filename: str) -> list: Reads a CSV file and returns its contents as a list of dictionaries. result = [] try: with open(filename, mode=\'r\', newline=\'\') as file: reader = csv.DictReader(file) for row in reader: result.append(row) except csv.Error as e: print(f\\"Error reading CSV file at row {reader.line_num}: {e}\\") return result def write_csv_dict(filename: str, data: list, fieldnames: list) -> None: Writes a list of dictionaries to a CSV file with specified fieldnames. with open(filename, mode=\'w\', newline=\'\') as file: writer = csv.DictWriter(file, fieldnames=fieldnames) writer.writeheader() writer.writerows(data) def write_csv_dialect(filename: str, data: list) -> None: Writes a list of lists to a CSV file using a custom dialect. class MyDialect(csv.Dialect): delimiter = \'|\' quotechar = \'\\"\' quoting = csv.QUOTE_MINIMAL lineterminator = \'rn\' escapechar = None doublequote = True skipinitialspace = False csv.register_dialect(\'my_dialect\', MyDialect) with open(filename, mode=\'w\', newline=\'\') as file: writer = csv.writer(file, dialect=\'my_dialect\') writer.writerows(data)"},{"question":"**Title:** Implement Advanced Data Classes for a Library Management System **Objective:** To assess students\' comprehension of creating and managing data classes in Python, including post-init processing, default factory functions, mutable default values, inheritance, and frozen instances. **Problem Statement:** You are tasked with designing a Library Management System. The library has different types of items: Books, DVDs, and Magazines. Each item has some common attributes and some specific attributes. Additionally, you need to be able to group items and calculate their total value. **Requirements:** 1. **Item Class:** - Create a `Item` data class with the following attributes: - `id`: An integer identifier. - `title`: The title of the item. - `author`: The author of the item. - `price`: The price of the item. - Implement a post-init method that converts the title and author to title case. 2. **Book Class:** - Inherit from `Item`. - Add the following specific attributes: - `genre`: The genre of the book. - `isbn`: The ISBN number of the book. - Ensure that the `Book` class initializes all attributes correctly, including those inherited from `Item`. 3. **DVD Class:** - Inherit from `Item`. - Add the following specific attributes: - `director`: The director of the DVD. - `duration`: The duration of the DVD in minutes. - Ensure that the `DVD` class initializes all attributes correctly, including those inherited from `Item`. 4. **Magazine Class:** - Inherit from `Item`. - Add the following specific attributes: - `issue_number`: The issue number of the magazine. - `publisher`: The publisher of the magazine. - Ensure that the `Magazine` class initializes all attributes correctly, including those inherited from `Item`. 5. **Library Class:** - Create a `Library` data class to hold a collection of items. - Use a `default_factory` function to initialize the items collection. - Implement a method to add an item to the collection. - Implement a method to calculate the total value of all items in the library. **Constraints:** - All item identifiers (id) must be unique and positive integers. - Titles, authors, genres, ISBNs, publishers, and directors must be non-empty strings. - Prices must be non-negative floating-point numbers. - Duration (for DVDs) and issue number (for Magazines) must be positive integers. **Expected Input and Output:** ```python # Example usage library = Library() book = Book(id=1, title=\'python programming\', author=\'john doe\', price=39.99, genre=\'education\', isbn=\'1234567890\') dvd = DVD(id=2, title=\'The Matrix\', author=\'Wachowski Sisters\', price=19.99, director=\'Wachowski\', duration=136) magazine = Magazine(id=3, title=\'Science Today\', author=\'Various\', price=5.99, issue_number=42, publisher=\'Science Corp\') library.add_item(book) library.add_item(dvd) library.add_item(magazine) print(library.calculate_total_value()) # Output should be 65.97 ``` **Note:** - Implement appropriate error handling for invalid data. - It is encouraged to use Python 3.10\'s dataclass module features effectively.","solution":"from dataclasses import dataclass, field, asdict from typing import List @dataclass class Item: id: int title: str author: str price: float def __post_init__(self): if not (isinstance(self.id, int) and self.id > 0): raise ValueError(\\"ID must be a unique and positive integer.\\") if not (self.title and isinstance(self.title, str)): raise ValueError(\\"Title must be a non-empty string.\\") if not (self.author and isinstance(self.author, str)): raise ValueError(\\"Author must be a non-empty string.\\") if not (isinstance(self.price, float) and self.price >= 0): raise ValueError(\\"Price must be a non-negative float.\\") self.title = self.title.title() self.author = self.author.title() @dataclass class Book(Item): genre: str isbn: str def __post_init__(self): super().__post_init__() if not (self.genre and isinstance(self.genre, str)): raise ValueError(\\"Genre must be a non-empty string.\\") if not (self.isbn and isinstance(self.isbn, str)): raise ValueError(\\"ISBN must be a non-empty string.\\") @dataclass class DVD(Item): director: str duration: int def __post_init__(self): super().__post_init__() if not (self.director and isinstance(self.director, str)): raise ValueError(\\"Director must be a non-empty string.\\") if not (isinstance(self.duration, int) and self.duration > 0): raise ValueError(\\"Duration must be a positive integer.\\") @dataclass class Magazine(Item): issue_number: int publisher: str def __post_init__(self): super().__post_init__() if not (isinstance(self.issue_number, int) and self.issue_number > 0): raise ValueError(\\"Issue number must be a positive integer.\\") if not (self.publisher and isinstance(self.publisher, str)): raise ValueError(\\"Publisher must be a non-empty string.\\") @dataclass class Library: items: List[Item] = field(default_factory=list) def add_item(self, item: Item): self.items.append(item) def calculate_total_value(self) -> float: return sum(item.price for item in self.items)"},{"question":"You are tasked with implementing a feature for a Python package installer tool that utilizes the `compileall` module to prepare the package\'s Python source files for installation. The user can specify various options related to the compilation process through a configuration dictionary. Specifically, you need to implement a function called `compile_python_sources` that: 1. Recursively compiles all Python source files in a specified directory. 2. Supports customization through the following configuration options: - `max_recursion_depth`: Integer specifying the maximum recursion level for subdirectories. - `force_recompile`: Boolean indicating whether to force recompilation even if timestamps are up-to-date. - `quiet_mode`: Integer where `0` means all output, `1` means only errors, and `2` means no output. - `compile_legacy`: Boolean that indicates whether to write byte-code files to their legacy locations. - `optimization_levels`: List of integers specifying the optimization levels for the compiler. - `invalidation_mode`: String that should be one of `\\"timestamp\\"`, `\\"checked-hash\\"`, or `\\"unchecked-hash\\"`. - `workers`: Integer specifying the number of workers to use for parallel compilation. # Input: - `directory`: A string representing the path to the directory containing Python source files. - `config`: A dictionary with the configuration options mentioned above. # Output: - Return `True` if all files compiled successfully, `False` otherwise. # Constraints: - The `directory` path should be a valid directory path. - The `config` dictionary will always contain valid keys and values as specified. # Example Usage: ```python config = { \\"max_recursion_depth\\": 5, \\"force_recompile\\": True, \\"quiet_mode\\": 1, \\"compile_legacy\\": False, \\"optimization_levels\\": [0, 1], \\"invalidation_mode\\": \\"timestamp\\", \\"workers\\": 4 } directory = \\"/path/to/python/sources\\" result = compile_python_sources(directory, config) print(result) # Expected output: True or False based on compilation success ``` # Implementation: Write the function `compile_python_sources` as defined above, making sure to handle all provided configuration options correctly.","solution":"import compileall def compile_python_sources(directory, config): Recursively compiles all Python source files in the specified directory with given configurations. Args: directory (str): The path to the directory containing Python source files. config (dict): Configuration options for the compilation process. Returns: bool: True if all files compiled successfully, False otherwise. legacy = config.get(\'compile_legacy\', False) force = config.get(\'force_recompile\', False) quiet = config.get(\'quiet_mode\', 0) max_depth = config.get(\'max_recursion_depth\', 10) optimization = config.get(\'optimization_levels\', []) invalidation = config.get(\'invalidation_mode\', \'timestamp\') workers = config.get(\'workers\', 1) result = compileall.compile_dir( dir=directory, maxlevels=max_depth, force=force, quiet=quiet, legacy=legacy, optimize=optimization, invalidation_mode=invalidation, workers=workers ) return result"},{"question":"**Question:** You are tasked with writing a Python function that converts a given binary data to its base64 ASCII representation, and then back to binary to ensure the correctness of the conversion. Additionally, the function should handle and report any errors that occur during the conversion process. # Function Signature ```python def convert_binary_to_base64_and_back(binary_data: bytes) -> Tuple[bytes, str]: pass ``` # Input - `binary_data`: A bytes object representing the binary data to be converted. The length of this data is not fixed but must be non-zero. # Output - A tuple containing: - The re-converted binary data (from base64 back to binary). - A string message stating `\\"Success\\"` if the conversion and re-conversion are successful, or describing any error encountered. # Constraints - The input `binary_data` must be a non-empty bytes object. - You may use the functions `binascii.b2a_base64` to convert binary data to base64, and `binascii.a2b_base64` to convert base64 data back to binary. # Examples ```python binary_data = b\'hello world\' result, message = convert_binary_to_base64_and_back(binary_data) print(result) # Expected to print the original binary data: b\'hello world\' print(message) # Expected to print: \\"Success\\" ``` ```python binary_data = b\'x00xffxf0\' result, message = convert_binary_to_base64_and_back(binary_data) print(result) # Expected to print the original binary data: b\'x00xffxf0\' print(message) # Expected to print: \\"Success\\" ``` # Notes 1. Ensure that any exceptions occurring during the conversion process are correctly captured and the appropriate error messages returned. 2. You may want to use a try-except block to handle potential errors. 3. Validate the input to ensure it is a non-empty bytes object before processing.","solution":"import binascii from typing import Tuple def convert_binary_to_base64_and_back(binary_data: bytes) -> Tuple[bytes, str]: if not isinstance(binary_data, bytes) or not binary_data: return (b\'\', \'Error: Input must be a non-empty bytes object\') try: base64_encoded = binascii.b2a_base64(binary_data).strip() re_converted_binary = binascii.a2b_base64(base64_encoded) if re_converted_binary == binary_data: return (re_converted_binary, \'Success\') else: return (b\'\', \'Error: Re-conversion did not match the original binary data\') except Exception as e: return (b\'\', f\'Error: {str(e)}\')"},{"question":"# Python Coding Assessment Question **Objective**: To assess students\' understanding of Python\'s `configparser` module, particularly their ability to read, manipulate, and write configuration settings. --- **Problem Statement**: You are developing an application that manages user settings using an INI configuration file. Implement a function `update_config(file_path: str, section: str, settings: dict) -> None` that updates a given section of the configuration file with new settings. The function should meet the following requirements: 1. **Read** the existing configuration from the file specified by `file_path`. 2. **Update** the specified `section` with the provided `settings`. If the section does not exist, it should be created. 3. **Write** back the updated configuration to the same file. Format for the `settings` dictionary: - Keys represent setting names. - Values represent the corresponding setting values. **Input**: - `file_path` (str): Path to the INI file. - `section` (str): The section in the INI file to be updated. - `settings` (dict): A dictionary containing the settings to be updated. **Output**: - The function should not return anything. It updates the INI file in place. **Constraints**: - The `file_path` string is always a valid path to a writable INI file. - The `settings` dictionary contains at least one key-value pair. - The keys and values in the `settings` dictionary are always strings. **Example**: Given an INI file `config.ini` with the following contents: ``` [General] appname = MyApp version = 1.0 [UserSettings] theme = light ``` Calling `update_config(\'config.ini\', \'UserSettings\', {\'theme\': \'dark\', \'font\': \'Arial\'})` should update the `config.ini` file to: ``` [General] appname = MyApp version = 1.0 [UserSettings] theme = dark font = Arial ``` **Additional Requirements**: 1. Raise an appropriate exception with a meaningful error message if the `file_path` does not point to a valid INI file. 2. Ensure that the update does not modify existing settings other than those provided in the `settings` dictionary. --- # Notes: - Use the `configparser` module to implement the solution. - Ensure proper handling of possible exceptions, providing helpful error messages if something goes wrong during file access or parsing.","solution":"import configparser import os def update_config(file_path: str, section: str, settings: dict) -> None: Update the given section of the configuration file with provided settings. Args: - file_path (str): Path to the INI file. - section (str): The section in the INI file to be updated. - settings (dict): A dictionary containing the settings to be updated. Returns: - None # Check if the file exists if not os.path.isfile(file_path): raise FileNotFoundError(f\\"The file \'{file_path}\' does not exist.\\") config = configparser.ConfigParser() # Read the existing configuration file config.read(file_path) # Update the specified section with provided settings, create section if not exists if not config.has_section(section): config.add_section(section) for key, value in settings.items(): config.set(section, key, value) # Write the updated configuration back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"**Question: Visualizing Survival Rates on the Titanic** You are given the Titanic dataset, which details various attributes of passengers on the Titanic, including their survival status. Your task is to create a series of visualizations to analyze the survival rates based on different factors such as class, gender, and age. # Input You should use the Titanic dataset available through Seaborn. Load this dataset using the following code: ```python import seaborn.objects as so from seaborn import load_dataset titanic = load_dataset(\\"titanic\\") ``` # Requirements 1. **Overall survival rate by class and gender**: - Create a stacked bar chart depicting the number of survivors and non-survivors for each class (`class`) differentiated by gender (`sex`). 2. **Distribution of age by survival**: - Create histograms showing the distribution of ages for survivors and non-survivors. These histograms should be faceted by gender (`sex`), with separate plots for each gender. # Output Your script should produce two visualizations: 1. A stacked bar chart showing overall survival rates by class and gender. 2. Histograms showing the distribution of ages of survivors and non-survivors, with separate subplots for each gender. # Constraints - Use `seaborn.objects.Plot` and related methods to build your visualizations. - Ensure that your plots are clearly labeled, with titles and legends where necessary. # Example Code The following snippet demonstrates how to start your task. Fill in the required parts to complete the full visualizations as described. ```python import seaborn.objects as so from seaborn import load_dataset # Load the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Task 1: Overall survival rate by class and gender # Replace `...` with appropriate code to create a stacked bar chart class_gender_plot = ( so.Plot(titanic, x=\\"class\\", color=\\"sex\\") .add(so.Bar(), so.Count(), so.Stack()) ) # Display the plot class_gender_plot.show() # Task 2: Distribution of age by survival, faceted by gender # Replace `...` with appropriate code to create faceted histograms age_distribution_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Display the plot age_distribution_plot.show() ``` Ensure that your plots are informative and visually appealing.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Task 1: Overall survival rate by class and gender class_gender_plot = ( so.Plot(titanic, x=\\"class\\", color=\\"sex\\") .add(so.Bar(), so.Count(), so.Stack()) .label(title=\\"Survival by Class and Gender\\", x=\\"Class\\", y=\\"Count\\", color=\\"Gender\\") ) # Display the plot class_gender_plot.show() # Task 2: Distribution of age by survival, faceted by gender age_distribution_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) .label(title=\\"Age Distribution by Survival and Gender\\", x=\\"Age\\", y=\\"Count\\", alpha=\\"Survived\\") ) # Display the plot age_distribution_plot.show()"},{"question":"You are tasked with implementing a custom scikit-learn compatible estimator. The estimator should be capable of performing a simple classification task based on the nearest centroid criterion. Implement a Custom Classifier Your classifier will be called `NearestCentroidClassifier`. It should implement the standard scikit-learn estimator interface and be compatible with `Pipeline` and `GridSearchCV`. # Requirements 1. **Initialization**: - Your class should have an `__init__` method that accepts a `metric` parameter with a default value `\'euclidean\'`. - Any other hyperparameters you deem necessary can be added with sensible default values. 2. **Fitting**: - Implement the `fit` method to calculate the centroids of each class based on the training data. - This method should accept two arguments, `X` (numpy array of shape (n_samples, n_features)) and `y` (numpy array of shape (n_samples,)). 3. **Prediction**: - Implement the `predict` method to classify new instances based on the nearest centroid. 4. **Parameters Handling**: - Implement `get_params` and `set_params` methods. 5. **Scikit-learn Checks**: - Ensure that your estimator passes all the checks from `sklearn.utils.estimator_checks.check_estimator`. # Input - `X`: A 2D numpy array where each row represents an instance and each column represents a feature. - `y`: A 1D numpy array representing the class labels for each instance in `X`. # Output - `fit`: The fitted classifier object. - `predict`: A list or numpy array of predicted class labels for the input instances. # Constraints - You are only allowed to use numpy for numerical operations. Do not use other machine learning libraries other than scikit-learn for your implementation. - Assume the input to your methods is always well-formatted and you don\'t have to handle malformed input. # Example Usage ```python from sklearn.utils.estimator_checks import check_estimator # Instantiate your classifier clf = NearestCentroidClassifier(metric=\'euclidean\') # Create some sample data X_train = np.array([[1, 2], [1, -1], [4, 5], [3, 4]]) y_train = np.array([0, 0, 1, 1]) # Fit the classifier clf.fit(X_train, y_train) # Predict new labels X_test = np.array([[0, 0], [5, 5]]) predictions = clf.predict(X_test) print(predictions) # Expected output: [0, 1] # Check if your classifier passes scikit-learn\'s estimator checks check_estimator(clf) ``` You are required to implement the following methods: - `__init__(self, metric=\'euclidean\')` - `fit(self, X, y)` - `predict(self, X)` - `get_params(self, deep=True)` - `set_params(self, **params)` # Solution Template ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from sklearn.utils.multiclass import unique_labels from sklearn.metrics import euclidean_distances class NearestCentroidClassifier(BaseEstimator, ClassifierMixin): def __init__(self, metric=\'euclidean\'): self.metric = metric def fit(self, X, y): X, y = check_X_y(X, y) self.classes_ = unique_labels(y) self.centroids_ = np.array([X[y == cls].mean(axis=0) for cls in self.classes_]) self.n_features_in_ = X.shape[1] return self def predict(self, X): check_is_fitted(self) X = check_array(X) if self.metric == \'euclidean\': distances = euclidean_distances(X, self.centroids_) return self.classes_[np.argmin(distances, axis=1)] else: raise ValueError(f\'Unsupported metric: {self.metric}\') def get_params(self, deep=True): return {\\"metric\\": self.metric} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self from sklearn.utils.estimator_checks import check_estimator # Example usage if __name__ == \\"__main__\\": clf = NearestCentroidClassifier() X_train = np.array([[1, 2], [1, -1], [4, 5], [3, 4]]) y_train = np.array([0, 0, 1, 1]) clf.fit(X_train, y_train) X_test = np.array([[0, 0], [5, 5]]) predictions = clf.predict(X_test) print(predictions) # Expected output: [0, 1] check_estimator(clf) ```","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from sklearn.utils.multiclass import unique_labels from sklearn.metrics import euclidean_distances class NearestCentroidClassifier(BaseEstimator, ClassifierMixin): def __init__(self, metric=\'euclidean\'): self.metric = metric def fit(self, X, y): X, y = check_X_y(X, y) self.classes_ = unique_labels(y) self.centroids_ = np.array([X[y == cls].mean(axis=0) for cls in self.classes_]) self.n_features_in_ = X.shape[1] return self def predict(self, X): check_is_fitted(self) X = check_array(X) if self.metric == \'euclidean\': distances = euclidean_distances(X, self.centroids_) return self.classes_[np.argmin(distances, axis=1)] else: raise ValueError(f\'Unsupported metric: {self.metric}\') def get_params(self, deep=True): return {\\"metric\\": self.metric} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self"},{"question":"Objective: Demonstrate your understanding of the `poplib` module in Python by implementing a function that connects to a POP3 server, retrieves emails, and processes them. Problem Statement: You are required to implement a function `fetch_emails(server: str, username: str, password: str, use_ssl: bool = False) -> list`. This function should connect to a POP3 server, authenticate the user, retrieve the first 5 email messages (if available), and return them as a list of strings. Additionally, the function should handle any errors that occur during the process gracefully and ensure the connection is properly closed after the operation. Function Signature: ```python def fetch_emails(server: str, username: str, password: str, use_ssl: bool = False) -> list: ``` Parameters: - `server` (str): The domain name or IP address of the POP3 server. - `username` (str): The username to authenticate with. - `password` (str): The password to authenticate with. - `use_ssl` (bool): If `True`, an SSL connection should be used. Defaults to `False`. Returns: - `list`: A list of strings, each representing a retrieved email message. Constraints: - Use the `poplib` module for connecting and retrieving emails. - The function should retrieve up to the first 5 emails only. - Ensure that the mailbox is properly locked and unlocked during the session. - Handle any errors (e.g., connection issues, authentication failures) by logging an appropriate message and returning an empty list. Example Usage: ```python emails = fetch_emails(\\"pop.example.com\\", \\"user@example.com\\", \\"password123\\", use_ssl=True) for email in emails: print(email) ``` Requirements: - Implement the function `fetch_emails` according to the specifications. - Ensure proper use of the `poplib.POP3` or `poplib.POP3_SSL` class depending on the value of `use_ssl`. - Include error handling using `poplib.error_proto` for protocol-specific errors. - Ensure that the mailbox is properly disconnected using the `quit` method. Notes: - You may use the `getpass` module for hiding password input during testing. - Remember to handle SSL certificate verification by using an appropriate SSL context, if necessary.","solution":"import poplib import logging from typing import List def fetch_emails(server: str, username: str, password: str, use_ssl: bool = False) -> List[str]: Connects to a POP3 server, authenticates the user, retrieves up to the first 5 email messages, and returns them as a list of strings. :param server: The domain name or IP address of the POP3 server. :param username: The username to authenticate with. :param password: The password to authenticate with. :param use_ssl: If True, an SSL connection should be used. Defaults to False. :return: A list of strings representing the retrieved email messages. try: if use_ssl: pop_conn = poplib.POP3_SSL(server) else: pop_conn = poplib.POP3(server) # Authenticate user pop_conn.user(username) pop_conn.pass_(password) # Get the number of messages in mailbox num_messages = len(pop_conn.list()[1]) email_list = [] for i in range(min(5, num_messages)): # Retrieve message response, lines, octets = pop_conn.retr(i + 1) # Join the lines of the message message = \\"n\\".join(line.decode(\'utf-8\') for line in lines) email_list.append(message) # Logout from the server pop_conn.quit() return email_list except poplib.error_proto as e: logging.error(f\\"POP3 error: {str(e)}\\") return [] except Exception as e: logging.error(f\\"An error occurred: {str(e)}\\") return []"},{"question":"# Question: Create a Custom Unicode String Class in Python Given the documentation on Unicode objects and codecs, your task is to create a custom Python class `CustomUnicodeString` that manages a Unicode string. Your class should: 1. **Initialization**: Initialize the class with a Unicode string. 2. **Encoding**: Provide a method to encode the Unicode string into a specified encoding and handle errors during the encoding process. 3. **Decoding**: Provide a method to decode a provided encoded string back into Unicode. 4. **Character Properties**: Provide methods to check: - If a character in the string is a digit. - If the string contains only printable characters. 5. **Substring Extraction**: Provide a method to extract a substring given start and end indices. # Class Definition ```python class CustomUnicodeString: def __init__(self, unicode_str: str): # Initialize with a Unicode string pass def encode_string(self, encoding: str, errors: str = \\"strict\\") -> bytes: # Encode the Unicode string into the specified encoding pass def decode_string(self, encoded_str: bytes, encoding: str, errors: str = \\"strict\\") -> str: # Decode the provided encoded string back into Unicode pass def is_digit(self, index: int) -> bool: # Check if the character at the specified index is a digit pass def is_printable(self) -> bool: # Check if the entire string contains only printable characters pass def get_substring(self, start: int, end: int) -> str: # Extract a substring from start (inclusive) to end (exclusive) pass ``` # Constraints 1. The `unicode_str` should be a valid Unicode string. 2. The encoding methods should handle encoding and decoding errors based on the provided error strategy. 3. The methods should raise appropriate exceptions for out-of-range indices or invalid operations. 4. Deprecated functions should not be used. # Example Usage ```python # Create an instance of CustomUnicodeString cus = CustomUnicodeString(\\"Hello, World!\\") # Encode the string into \'utf-8\' encoded = cus.encode_string(encoding=\'utf-8\') print(encoded) # Decode the string back to Unicode decoded = cus.decode_string(encoded_str=encoded, encoding=\'utf-8\') print(decoded) # Check if character at index 7 is a digit print(cus.is_digit(7)) # Output: False # Check if the string is printable print(cus.is_printable()) # Output: True # Extract a substring substring = cus.get_substring(start=7, end=12) print(substring) # Output: \\"World\\" ``` # Notes - Ensure to document any assumptions you make. - Provide meaningful error messages and handle exceptions gracefully. - You can use Python\'s built-in methods and should refer to the documentation provided to avoid deprecated functions.","solution":"class CustomUnicodeString: def __init__(self, unicode_str: str): if not isinstance(unicode_str, str): raise ValueError(\\"Provided value must be a Unicode string.\\") self.unicode_str = unicode_str def encode_string(self, encoding: str, errors: str = \\"strict\\") -> bytes: try: return self.unicode_str.encode(encoding, errors) except Exception as e: raise ValueError(f\\"Encoding error: {e}\\") def decode_string(self, encoded_str: bytes, encoding: str, errors: str = \\"strict\\") -> str: try: return encoded_str.decode(encoding, errors) except Exception as e: raise ValueError(f\\"Decoding error: {e}\\") def is_digit(self, index: int) -> bool: if index < 0 or index >= len(self.unicode_str): raise IndexError(\\"Index out of range.\\") return self.unicode_str[index].isdigit() def is_printable(self) -> bool: return all(char.isprintable() for char in self.unicode_str) def get_substring(self, start: int, end: int) -> str: if start < 0 or end < 0 or start >= len(self.unicode_str) or end > len(self.unicode_str): raise IndexError(\\"Start or End index out of range.\\") return self.unicode_str[start:end]"},{"question":"Problem Statement You are required to implement a resource management system using the `contextlib` module that can handle multiple types of context managers, including standard, async, and custom ones. You\'ll need to demonstrate your ability to create custom context managers, handle exceptions gracefully, and manage nested resources efficiently. # Task 1. **Implement a Custom Context Manager**: - Create a custom context manager class `CustomResourceManager` that inherits from `contextlib.AbstractContextManager`. This class should manage a simple resource, such as incrementing and decrementing a counter on enter and exit, respectively. - Implement `__enter__` and `__exit__` methods: - `__enter__` should increment a counter, print \\"Resource acquired\\" and return `self`. - `__exit__` should decrement the counter, print \\"Resource released\\", and handle any exceptions passed to it gracefully, logging the exception message. 2. **Implement Resource Management Function**: - Define a function `manage_resources` that uses `contextlib.ExitStack` to manage multiple `CustomResourceManager` instances. - It should acquire three instances of `CustomResourceManager`, handle any exceptions raised during the resource acquisition, and ensure that any acquired resources are properly released when the function exits, either normally or via an exception. # Input - The function does not require any input parameters. # Output - The function should print statements reflecting the resource acquisition and release steps. - If an exception occurs, it should print the exception message. # Constraints - Use Python 3.7+. - You are allowed to use any function or class from the `contextlib` module. - Ensure proper resource cleanup even if an exception occurs. # Example ```python class CustomResourceManager(contextlib.AbstractContextManager): counter = 0 def __enter__(self): CustomResourceManager.counter += 1 print(f\\"Resource acquired. Counter: {CustomResourceManager.counter}\\") return self def __exit__(self, exc_type, exc_val, exc_tb): CustomResourceManager.counter -= 1 print(f\\"Resource released. Counter: {CustomResourceManager.counter}\\") if exc_val: print(f\\"Exception: {exc_val}\\") # Returning False to propagate the exception if any return False def manage_resources(): try: with contextlib.ExitStack() as stack: for _ in range(3): stack.enter_context(CustomResourceManager()) print(\\"All resources acquired.\\") # Simulate an operation that could raise an exception raise ValueError(\\"An error occurred during operations.\\") except Exception as e: print(f\\"Managing resources failed: {e}\\") # Running the function to demonstrate output manage_resources() ``` Expected Output: ``` Resource acquired. Counter: 1 Resource acquired. Counter: 2 Resource acquired. Counter: 3 All resources acquired. Resource released. Counter: 2 Resource released. Counter: 1 Resource released. Counter: 0 Exception: An error occurred during operations. Managing resources failed: An error occurred during operations. ```","solution":"import contextlib class CustomResourceManager(contextlib.AbstractContextManager): counter = 0 def __enter__(self): CustomResourceManager.counter += 1 print(f\\"Resource acquired. Counter: {CustomResourceManager.counter}\\") return self def __exit__(self, exc_type, exc_val, exc_tb): CustomResourceManager.counter -= 1 print(f\\"Resource released. Counter: {CustomResourceManager.counter}\\") if exc_val: print(f\\"Exception: {exc_val}\\") # Returning False to propagate the exception if any return False def manage_resources(): try: with contextlib.ExitStack() as stack: for _ in range(3): stack.enter_context(CustomResourceManager()) print(\\"All resources acquired.\\") # Simulate an operation that could raise an exception raise ValueError(\\"An error occurred during operations.\\") except Exception as e: print(f\\"Managing resources failed: {e}\\")"},{"question":"# Coding Assessment **Objective:** Design a program that utilizes the `multiprocessing.shared_memory` module to demonstrate the following functionalities: 1. Creation of shared memory segments. 2. Inter-process communication using shared memory. 3. Synchronization and data consistency across processes. **Problem Statement:** Write a Python program that creates a shared memory block to store an integer array of size 10. Two processes should increment each element of the array by 1, but they should not operate on the same elements at the same time. Use the `multiprocessing` and `multiprocessing.shared_memory` modules to achieve this. The program should demonstrate the use of `SharedMemory` and `ShareableList`. **Requirements:** 1. Create a `SharedMemory` block to store a list of integers from 0 to 9. 2. Define two functions (`increment_first_half`, `increment_second_half`) that each increment half of the array elements by 1. - `increment_first_half` should increment the first 5 elements. - `increment_second_half` should increment the last 5 elements. 3. Use two processes to run these functions concurrently. 4. Ensure proper synchronization using locks or other synchronization primitives to avoid race conditions. 5. Print the final state of the array after both processes complete their execution. 6. Properly handle the cleanup of shared memory resources. **Function Signatures:** ```python def increment_first_half(shared_list_name: str) -> None: pass def increment_second_half(shared_list_name: str) -> None: pass def main(): pass if __name__ == \\"__main__\\": main() ``` **Input:** No input required. **Output:** Print the final state of the shared list after both processes have finished executing. **Example Output:** ``` Final state of the array: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2] ``` **Constraints:** - Ensure the data consistency across processes. - Proper cleanup using `close()` and `unlink()` methods. **Hints:** - Use `multiprocessing.Lock` for synchronization. - Use `SharedMemoryManager` for easier resource management. Good luck and happy coding!","solution":"import multiprocessing from multiprocessing import shared_memory, Lock def increment_first_half(shared_list_name: str, lock: Lock) -> None: existing_shm = shared_memory.SharedMemory(name=shared_list_name) with lock: # Attach to the existing shared memory array = [existing_shm.buf[i] for i in range(10)] for i in range(5): array[i] += 1 existing_shm.buf[i] = array[i] existing_shm.close() def increment_second_half(shared_list_name: str, lock: Lock) -> None: existing_shm = shared_memory.SharedMemory(name=shared_list_name) with lock: # Attach to the existing shared memory array = [existing_shm.buf[i] for i in range(10)] for i in range(5, 10): array[i] += 1 existing_shm.buf[i] = array[i] existing_shm.close() def main(): # Create shared memory block shm = shared_memory.SharedMemory(create=True, size=10) lock = Lock() # Initialize the shared memory with data 0, 1, ..., 9 for i in range(10): shm.buf[i] = 0 # Create processes p1 = multiprocessing.Process(target=increment_first_half, args=(shm.name, lock)) p2 = multiprocessing.Process(target=increment_second_half, args=(shm.name, lock)) # Start processes p1.start() p2.start() # Wait for processes to complete p1.join() p2.join() # Read final state of the array final_state = [shm.buf[i] for i in range(10)] print(\\"Final state of the array:\\", final_state) # Cleanup shm.close() shm.unlink() if __name__ == \\"__main__\\": main()"},{"question":"Objective: Implement and compare different clustering algorithms using scikit-learn on a given dataset. Evaluate their performance and interpret the results. Background: Clustering is a common unsupervised learning technique used to group similar data points together. Scikit-learn provides several clustering algorithms. Task: 1. Load the iris dataset from scikit-learn. 2. Implement the following clustering algorithms: - KMeans - Agglomerative Clustering - DBSCAN 3. For each algorithm: - Fit the model to the data. - Predict the cluster labels. - Compute the silhouette score for the clustering. (Hint: Use `sklearn.metrics.silhouette_score`) 4. Print out the silhouette scores and comment on which algorithm performed the best based on the silhouette score. Expected Input and Output: - **Input:** - None (the iris dataset should be loaded within the code) - **Output:** - Silhouette scores for each of the three clustering algorithms - A brief comment on the performance of each algorithm Constraints: - Use default parameters for each algorithm unless specified. - Ensure your code is well-documented with appropriate comments. Performance Requirements: - The solution should be efficient in terms of runtime and memory usage. Example: ```python from sklearn.datasets import load_iris from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import silhouette_score # Load the iris dataset iris = load_iris() X = iris.data # Clustering Algorithms kmeans = KMeans(n_clusters=3) agglomerative = AgglomerativeClustering(n_clusters=3) dbscan = DBSCAN() # Fit and predict kmeans_labels = kmeans.fit_predict(X) agglomerative_labels = agglomerative.fit_predict(X) dbscan_labels = dbscan.fit_predict(X) # Calculate silhouette scores kmeans_silhouette = silhouette_score(X, kmeans_labels) agglomerative_silhouette = silhouette_score(X, agglomerative_labels) dbscan_silhouette = silhouette_score(X, dbscan_labels) # Print silhouette scores print(f\\"KMeans Silhouette Score: {kmeans_silhouette}\\") print(f\\"Agglomerative Silhouette Score: {agglomerative_silhouette}\\") print(f\\"DBSCAN Silhouette Score: {dbscan_silhouette}\\") # Comment on the best performing algorithm ``` --- Please write the code to complete the above task and provide your analysis on the performance of each algorithm.","solution":"from sklearn.datasets import load_iris from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import silhouette_score def perform_clustering(): # Load the iris dataset iris = load_iris() X = iris.data # Clustering Algorithms kmeans = KMeans(n_clusters=3, random_state=42) agglomerative = AgglomerativeClustering(n_clusters=3) dbscan = DBSCAN() # Fit and predict kmeans_labels = kmeans.fit_predict(X) agglomerative_labels = agglomerative.fit_predict(X) dbscan_labels = dbscan.fit_predict(X) # Calculate silhouette scores kmeans_silhouette = silhouette_score(X, kmeans_labels) agglomerative_silhouette = silhouette_score(X, agglomerative_labels) dbscan_silhouette = silhouette_score(X, dbscan_labels) # Print silhouette scores print(f\\"KMeans Silhouette Score: {kmeans_silhouette}\\") print(f\\"Agglomerative Silhouette Score: {agglomerative_silhouette}\\") print(f\\"DBSCAN Silhouette Score: {dbscan_silhouette}\\") # Return silhouette scores for unit tests return kmeans_silhouette, agglomerative_silhouette, dbscan_silhouette # Execute the function to print scores perform_clustering()"},{"question":"# Advanced Python Programming Assessment **Objective**: Design and implement a function that integrates with the operating system using Pythonâ€™s utility functions described in the provided documentation. Your solution should demonstrate your understanding of argument parsing, process control, and data marshaling. --- # Problem Statement: You are required to write a Python script to manage a user-session tracking system. The script should implement a specific function to: 1. Parse user session data passed as arguments. 2. Store the session data in a specific format. 3. Start a process to log the session activity. # Requirements: 1. **Function Name**: `manage_user_session` 2. **Input**: - `user_id` (int): A unique identifier for the user. - `session_id` (int): A unique identifier for the session. - `session_data` (str): A string containing session information. 3. **Output**: - A formatted string that acknowledges the session start. - The logged data written to a file named `session_log.txt`. 4. **Constraints**: - User IDs and Session IDs should be positive integers. - Ensure proper data parsing and storage. - Log the processed session data to a file in a consistent format. 5. **Performance**: - The function should efficiently parse and handle the session data. - Operations involving file handling should ensure no data corruption or duplication even if the function is called multiple times. # Example: ```python def manage_user_session(user_id: int, session_id: int, session_data: str) -> str: # Your code here # Example Usage response = manage_user_session(101, 202, \\"login_successful\\") print(response) # Output: \\"Session 202 for User 101 has started.\\" # Expected content in \'session_log.txt\' after the function execution: # UserID: 101 # SessionID: 202 # SessionData: login_successful ``` # Implementation Notes: 1. **Parsing Arguments**: Use the argument parsing utility functions to validate and process inputs within the provided constraints. 2. **Process Control**: Simulate a process control function that logs session data to a file. 3. **String Handling**: Implement necessary string conversions and formatting as described in the utility functions documentation to ensure proper logging. # Evaluation Criteria: - **Correctness**: The function should correctly parse inputs, format output, and log session data. - **Efficiency**: The implementation should handle multiple function calls without performance bottlenecks. - **Code Quality**: The code should be well-documented and follow best practices. Do not include any external libraries. Solutions must only use standard Python libraries and utilities.","solution":"def manage_user_session(user_id: int, session_id: int, session_data: str) -> str: Manages user session data by logging the information to a file and returning a formatted acknowledgment string. Args: user_id (int): Unique identifier for the user. session_id (int): Unique identifier for the session. session_data (str): Information about the session. Returns: str: Confirmation message for the session start. if user_id <= 0 or session_id <= 0: raise ValueError(\\"User ID and Session ID must be positive integers.\\") # Format the confirmation message confirmation_message = f\\"Session {session_id} for User {user_id} has started.\\" # Open the file in append mode and write the session data with open(\\"session_log.txt\\", \\"a\\") as log_file: log_file.write(f\\"UserID: {user_id}n\\") log_file.write(f\\"SessionID: {session_id}n\\") log_file.write(f\\"SessionData: {session_data}n\\") log_file.write(\\"----n\\") return confirmation_message"},{"question":"# Multioutput Regression Task Objective You are required to develop a multioutput regression model using scikit-learn, a popular machine learning library in Python. This task will test your understanding of preparing data, implementing multioutput regression, and evaluating and visualizing model performance. Problem Statement 1. **Data Preparation**: Generate a synthetic dataset suitable for a multioutput regression task. 2. **Model Implementation**: Implement a multioutput regression model using scikit-learn. 3. **Evaluation**: Evaluate the model using appropriate metrics. 4. **Visualization**: Visualize the true vs. predicted values for at least one of the targets. # Step-by-Step Instructions 1. Data Preparation Generate a synthetic dataset with `n_samples=200`, `n_features=100`, and `n_targets=3`. Use `make_regression` function from `sklearn.datasets` to create the dataset. ```python from sklearn.datasets import make_regression # Parameters for dataset n_samples = 200 n_features = 100 n_targets = 3 X, Y = make_regression(n_samples=n_samples, n_features=n_features, n_targets=n_targets, random_state=42) ``` 2. Model Implementation Use the `MultiOutputRegressor` wrapper with any regression model of your choice. One example could be `GradientBoostingRegressor` from `sklearn.ensemble`. ```python from sklearn.ensemble import GradientBoostingRegressor from sklearn.multioutput import MultiOutputRegressor # Initialize the base regressor base_regressor = GradientBoostingRegressor(random_state=42) # Wrap it with MultiOutputRegressor multioutput_regressor = MultiOutputRegressor(base_regressor) # Fit the model multioutput_regressor.fit(X, Y) ``` 3. Evaluation Evaluate the performance of your multioutput regression model using the `mean_squared_error` metric for each output. ```python from sklearn.metrics import mean_squared_error # Predict the outputs Y_pred = multioutput_regressor.predict(X) # Evaluate the model mse = mean_squared_error(Y, Y_pred, multioutput=\'raw_values\') print(\'Mean Squared Error for each target:\', mse) ``` 4. Visualization Plot the true vs. predicted values for the first target to visualize the model\'s performance. Use `matplotlib` for the plotting. ```python import matplotlib.pyplot as plt # True vs. Predicted for the first target plt.figure(figsize=(10, 6)) plt.scatter(Y[:, 0], Y_pred[:, 0], alpha=0.5) plt.plot([Y[:, 0].min(), Y[:, 0].max()], [Y[:, 0].min(), Y[:, 0].max()], \'--r\') plt.xlabel(\'True Values\') plt.ylabel(\'Predicted Values\') plt.title(\'True vs. Predicted Values for Target 1\') plt.show() ``` # Constraints and Mention 1. Ensure you follow best practices and code style. 2. Use appropriate comments to explain your code. 3. Pay attention to the initialization of random seeds for reproducibility. # Submission - Submit a Jupyter notebook file containing your code and visualizations. - Ensure the notebook runs end-to-end without any errors. - Provide a short summary of your approach and results at the end of the notebook. Good luck!","solution":"from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor from sklearn.multioutput import MultiOutputRegressor from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt def multioutput_regression_task(): # Data Preparation n_samples = 200 n_features = 100 n_targets = 3 X, Y = make_regression(n_samples=n_samples, n_features=n_features, n_targets=n_targets, random_state=42) # Model Implementation base_regressor = GradientBoostingRegressor(random_state=42) multioutput_regressor = MultiOutputRegressor(base_regressor) multioutput_regressor.fit(X, Y) # Evaluation Y_pred = multioutput_regressor.predict(X) mse = mean_squared_error(Y, Y_pred, multioutput=\'raw_values\') print(\'Mean Squared Error for each target:\', mse) # Visualization plt.figure(figsize=(10, 6)) plt.scatter(Y[:, 0], Y_pred[:, 0], alpha=0.5) plt.plot([Y[:, 0].min(), Y[:, 0].max()], [Y[:, 0].min(), Y[:, 0].max()], \'--r\') plt.xlabel(\'True Values\') plt.ylabel(\'Predicted Values\') plt.title(\'True vs. Predicted Values for Target 1\') plt.show() return X, Y, Y_pred, mse"},{"question":"# Advanced Python Logging with syslog Module You are required to create a Python logging utility using the `syslog` module that simulates a system processing multiple tasks. The utility should demonstrate advanced functionalities such as setting log options, sending messages with different priority levels, and filtering log messages by priority. Requirements: 1. **Configure Initial Logging**: - Use `syslog.openlog` to configure the logging utility with an identifier \\"TaskProcessor\\", log options to include the process ID, and log to the user facility. 2. **Log Messages**: - Implement a function `log_message(priority, message)` that logs a message with the given priority. The priority should be one of the `syslog` priority constants (e.g., `syslog.LOG_INFO`, `syslog.LOG_ERR`). 3. **Set Log Mask**: - Implement a function `set_log_mask(min_priority)` that sets the log mask to only include logs up to and including `min_priority`. Messages with lower priority than `min_priority` should be ignored. 4. **Process Simulation**: - Create a function `simulate_tasks()` that simulates processing a series of tasks with the following sequence of log messages: - An informational message \\"Starting task processing\\". - A warning message \\"Potential issue detected\\". - An error message \\"Error occurred during task processing\\". - A critical message \\"Critical failure in task processing\\". 5. **Change Log Mask and Retest**: - After simulating tasks the first time, change the log mask to only include logs up to `syslog.LOG_WARNING` and run `simulate_tasks()` again. Input and Output Formats: - No direct input from the user is required. - The log messages should be sent to the system logger, and their visibility will depend on the configured log mask. Code Template: ```python import syslog def configure_logging(): syslog.openlog(ident=\'TaskProcessor\', logoption=syslog.LOG_PID, facility=syslog.LOG_USER) def log_message(priority, message): syslog.syslog(priority, message) def set_log_mask(min_priority): mask = syslog.LOG_UPTO(min_priority) syslog.setlogmask(mask) def simulate_tasks(): log_message(syslog.LOG_INFO, \\"Starting task processing\\") log_message(syslog.LOG_WARNING, \\"Potential issue detected\\") log_message(syslog.LOG_ERR, \\"Error occurred during task processing\\") log_message(syslog.LOG_CRIT, \\"Critical failure in task processing\\") if __name__ == \\"__main__\\": configure_logging() simulate_tasks() set_log_mask(syslog.LOG_WARNING) simulate_tasks() syslog.closelog() ``` Constraints: - Ensure that all the functions are correctly utilizing the `syslog` module as described in the documentation. - The priority constants and log options should be used directly from the `syslog` module. - The solution should showcase the performance by correctly filtering out messages based on the set log mask.","solution":"import syslog def configure_logging(): Configure the syslog with identifier \'TaskProcessor\', log options to include process ID, and log to the user facility. syslog.openlog(ident=\'TaskProcessor\', logoption=syslog.LOG_PID, facility=syslog.LOG_USER) def log_message(priority, message): Log a message with the given priority using the syslog module. Parameters: priority (int): The priority level of the message. message (str): The message to log. syslog.syslog(priority, message) def set_log_mask(min_priority): Set the log mask to include messages up to and including the specified priority. Parameters: min_priority (int): The minimum priority level of messages to log. mask = syslog.LOG_UPTO(min_priority) syslog.setlogmask(mask) def simulate_tasks(): Simulate processing a series of tasks and log messages with different priorities. log_message(syslog.LOG_INFO, \\"Starting task processing\\") log_message(syslog.LOG_WARNING, \\"Potential issue detected\\") log_message(syslog.LOG_ERR, \\"Error occurred during task processing\\") log_message(syslog.LOG_CRIT, \\"Critical failure in task processing\\") if __name__ == \\"__main__\\": configure_logging() simulate_tasks() set_log_mask(syslog.LOG_WARNING) simulate_tasks() syslog.closelog()"},{"question":"Data Compression and Decompression using `zlib` **Objective:** Implement a function to compress and decompress a text string using the `zlib` library in Python. The function will demonstrate your understanding of key `zlib` functionalities and your ability to correctly use its parameters. **Task:** 1. Write a Python function `compress_and_decompress(data: str, compress_level: int) -> str` that performs the following tasks: - Compress the input string `data` using the zlib library at the specified compression level `compress_level`. - Decompress the compressed data back to its original form. - Return the decompressed string to verify the initial and final strings are the same. **Function Signature:** ```python def compress_and_decompress(data: str, compress_level: int) -> str: pass ``` **Parameters:** - `data` (str): The input string to be compressed and decompressed. - `compress_level` (int): An integer between 0 and 9 (inclusive) that specifies the level of compression, where 0 means no compression and 9 means maximum compression. **Returns:** - `str`: The decompressed string that should ideally match the original input `data`. **Constraints:** - You can use any standard library functions provided by `zlib`. - Ensure that the compression and decompression processes are error-free and handle potential exceptions appropriately. - Validate the `compress_level` to ensure it is within the specified range (0 to 9); if not, set it to the default compression level of 6 (Z_DEFAULT_COMPRESSION). **Example:** ```python # Example usage: original_data = \\"The quick brown fox jumps over the lazy dog\\" compress_level = 5 result = compress_and_decompress(original_data, compress_level) assert result == original_data # The original and decompressed data should be the same. ``` **Notes:** - This question tests your understanding of data compression and decompression concepts, error handling, and parameter validation using the `zlib` library. - Pay attention to the efficiency and correctness of your implementation.","solution":"import zlib def compress_and_decompress(data: str, compress_level: int) -> str: Compress and then decompress a given string using the zlib library. Parameters: - data (str): The input string to be compressed and decompressed. - compress_level (int): Compression level (0 to 9 inclusive). Returns: - str: The decompressed string that should match the original input data. if not 0 <= compress_level <= 9: compress_level = zlib.Z_DEFAULT_COMPRESSION # Compress the data compressed_data = zlib.compress(data.encode(\'utf-8\'), compress_level) # Decompress the data decompressed_data = zlib.decompress(compressed_data).decode(\'utf-8\') return decompressed_data"},{"question":"# Drag and Drop Implementation with tkinter.dnd You are required to build a simple GUI application using the `tkinter.dnd` module to demonstrate understanding of drag-and-drop support. The application should allow the user to drag and drop a label widget (`tk.Label`) within the same window. The following conditions should be met: 1. **Label Widget**: Should be draggable. 2. **Target Widgets**: - Three `tk.Frame` widgets that act as drop targets. - Each frame should have a different color for visual distinction. 3. **Drop Mechanics**: - When the label is dropped into a frame, the frame should display a message saying \\"Label dropped here\\". - The label text should change to \\"Dropped\\" once dropped. # Detailed Instructions: 1. **Initialization**: - Create a `tk.Tk` window. - Add one `tk.Label` widget with the text \\"Drag me!\\". 2. **Frames Setup**: - Create three `tk.Frame` widgets, arranged vertically or in a desired layout. - Each frame should be a different color and have a minimum size e.g., `width=200, height=100`. 3. **Drag-and-Drop Setup**: - Use the `tkinter.dnd.DndHandler` class and `tkinter.dnd.dnd_start` function to enable drag-and-drop functionality. - Define the necessary event handlers for drag-and-drop (`dnd_accept`, `dnd_enter`, `dnd_commit`, etc.) to ensure the label can be dragged and dropped correctly. # Constraints: - Use only the `tkinter` and `tkinter.dnd` libraries. - Ensure the program runs with no errors and handles edge cases (e.g., dragging the label outside the window, dropping on non-target areas). # Example: ```python import tkinter as tk from tkinter import dnd class DragDropApp: def __init__(self, root): self.root = root self.setup_ui() def setup_ui(self): self.label = tk.Label(self.root, text=\\"Drag me!\\", bg=\\"lightgrey\\") self.label.pack(pady=20) self.frames = [] colors = [\\"red\\", \\"green\\", \\"blue\\"] for color in colors: frame = tk.Frame(self.root, bg=color, width=200, height=100) frame.pack(pady=10, fill=tk.BOTH) frame.dnd_accept = self.make_dnd_accept(frame) self.frames.append(frame) self.label.bind(\\"<ButtonPress>\\", self.on_start) def make_dnd_accept(self, frame): def dnd_accept(source, event): return frame return dnd_accept def on_start(self, event): dnd.dnd_start(self.label, event) def on_drop(self, target, event): target.config(text=\\"Label dropped here\\") self.label.config(text=\\"Dropped\\") if __name__ == \'__main__\': root = tk.Tk() app = DragDropApp(root) root.mainloop() ``` Ensure your code adheres to these specifications, runs without errors, and correctly implements the drag-and-drop mechanism.","solution":"import tkinter as tk from tkinter import dnd class DragDropApp: def __init__(self, root): self.root = root self.setup_ui() def setup_ui(self): self.label = tk.Label(self.root, text=\\"Drag me!\\", bg=\\"lightgrey\\") self.label.pack(pady=20) self.frames = [] colors = [\\"red\\", \\"green\\", \\"blue\\"] for color in colors: frame = tk.Frame(self.root, bg=color, width=200, height=100) frame.pack(pady=10, fill=tk.BOTH) frame.dnd_accept = self.make_dnd_accept(frame) self.frames.append(frame) self.label.bind(\\"<ButtonPress>\\", self.on_start) def make_dnd_accept(self, frame): def dnd_accept(source, event): return frame return dnd_accept def on_start(self, event): dnd.dnd_start(self.label, event) def on_motion(self, event): self.label.place(x=event.x_root, y=event.y_root, anchor=tk.CENTER) def on_drop(self, event): target = event.widget if target in self.frames: target.config(text=\\"Label dropped here\\") self.label.config(text=\\"Dropped\\") self.label.unbind(\\"<Motion>\\") self.label.unbind(\\"<ButtonRelease-1>\\") if __name__ == \'__main__\': root = tk.Tk() app = DragDropApp(root) root.mainloop()"},{"question":"**Problem Statement: Implement a Custom ConfusionMatrixDisplay Class** In this task, you are required to implement a plotting class named `ConfusionMatrixDisplay` similar to the `RocCurveDisplay` class described in the documentation. This class should be able to generate a confusion matrix visualization from the results of a classification task using scikit-learn models. **Requirements:** 1. Implement the `ConfusionMatrixDisplay` class with the following methods: - `__init__`: Initialize with true positive (`tp`), false positive (`fp`), true negative (`tn`), and false negative (`fn`) values. - `from_estimator`: Class method that accepts an estimator, feature data (`X`), and true labels (`y`), computes predictions and confusion matrix values, and returns an instance of `ConfusionMatrixDisplay`. - `from_predictions`: Class method that accepts true labels (`y_true`) and predicted labels (`y_pred`), computes confusion matrix values, and returns an instance of `ConfusionMatrixDisplay`. - `plot`: Method to create and display the confusion matrix using Matplotlib. This method should handle both single and multiple axes visualization. 2. The confusion matrix should be displayed as a heatmap where: - Rows represent actual labels. - Columns represent predicted labels. - Heatmap cells should display corresponding confusion values (`tp`, `fp`, `tn`, `fn`). **Input:** - For `from_estimator`: A trained classifier object, feature dataset `X`, and true labels `y`. - For `from_predictions`: Arrays of true labels `y_true` and predicted labels `y_pred`. **Output:** - An instance of `ConfusionMatrixDisplay` with a rendered confusion matrix plot. **Constraints:** - Only scikit-learn and Matplotlib libraries are allowed. - Ensure the implementation is efficient and handles standard classification problems. **Example Usage:** ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression import matplotlib.pyplot as plt # Load data iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train model classifier = LogisticRegression(max_iter=200) classifier.fit(X_train, y_train) # Create confusion matrix display cm_display = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test) cm_display.plot() # Customize and show plot plt.show() ``` Please implement the class `ConfusionMatrixDisplay` based on these specifications.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix import numpy as np class ConfusionMatrixDisplay: def __init__(self, tp, fp, tn, fn): self.tp = tp self.fp = fp self.tn = tn self.fn = fn self.confusion_matrix = np.array([[tn, fp], [fn, tp]]) @classmethod def from_estimator(cls, estimator, X, y): y_pred = estimator.predict(X) return cls.from_predictions(y, y_pred) @classmethod def from_predictions(cls, y_true, y_pred): tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel() return cls(tp, fp, tn, fn) def plot(self, ax=None): if ax is None: fig, ax = plt.subplots() cax = ax.matshow(self.confusion_matrix, cmap=plt.cm.Blues) plt.colorbar(cax) ax.set_xticklabels([\'\'] + [\'Negative\', \'Positive\']) ax.set_yticklabels([\'\'] + [\'Negative\', \'Positive\']) ax.xaxis.set_label_position(\'bottom\') ax.xaxis.tick_bottom() ax.yaxis.tick_left() for (i, j), val in np.ndenumerate(self.confusion_matrix): ax.text(j, i, f\'{val}\', ha=\'center\', va=\'center\') plt.xlabel(\'Predicted\') plt.ylabel(\'Actual\') plt.title(\'Confusion Matrix\') return ax"},{"question":"Question: Telnet Client with Advanced Reading Operations # Objective: Implement a Python class to interface with a Telnet server using the `telnetlib` module. Your class should handle connecting to the server, executing commands, and reading responses in different ways as specified. # Requirements: 1. **Class Name:** `AdvancedTelnetClient` 2. **Methods:** - `__init__(self, host: str, port: int = 23, timeout: float = None)`: Initializes the instance with provided host, port, and optional timeout. - `connect(self)`: Establishes the connection to the specified host. - `disconnect(self)`: Closes the connection. - `execute_command(self, command: bytes) -> bytes`: Executes a command on the Telnet server and returns the response after the command is executed. - `read_until(self, expected: bytes, timeout: float = None) -> bytes`: Reads until the expected bytes are found or the timeout occurs. - `read_eager(self) -> bytes`: Reads and returns data that is readily available without blocking. - `read_lazy(self) -> bytes`: Processes and returns data already in the queue. 3. **Constraints:** - The `execute_command` method should use Telnet\'s `write` method to send commands and `read_until` method to read the response till the prompt (assume prompt is `b\' \'`). - Raise appropriate exceptions if any operation fails due to connection issues or timeout. 4. **Input and Output:** - All methods except `__init__` should raise a `ConnectionError` if the connection is not established when they are called. - `execute_command` should expect a command as bytes and return the command output as bytes. # Example Usage: ```python client = AdvancedTelnetClient(\'localhost\', 23) client.connect() try: output = client.execute_command(b\'lsn\') print(\\"Command output:\\", output.decode(\'ascii\')) partial_output = client.read_eager() print(\\"Eager output:\\", partial_output.decode(\'ascii\')) finally: client.disconnect() ``` # Notes: - Ensure proper exception handling and cleanup (closing the connection) in case of errors. - The Telnet server\'s prompt is assumed to be `b\' \'`. Adjust if your server uses a different prompt. # Performance: - The solution should efficiently manage Telnet sessions and handle read operations without excessive blocking.","solution":"import telnetlib class AdvancedTelnetClient: def __init__(self, host: str, port: int = 23, timeout: float = None): self.host = host self.port = port self.timeout = timeout self.connection = None def connect(self): if self.connection is not None: raise ConnectionError(\\"Already connected\\") self.connection = telnetlib.Telnet(self.host, self.port, self.timeout) def disconnect(self): if self.connection is None: raise ConnectionError(\\"Not connected\\") self.connection.close() self.connection = None def execute_command(self, command: bytes) -> bytes: if self.connection is None: raise ConnectionError(\\"Not connected\\") self.connection.write(command) return self.connection.read_until(b\' \', self.timeout) def read_until(self, expected: bytes, timeout: float = None) -> bytes: if self.connection is None: raise ConnectionError(\\"Not connected\\") return self.connection.read_until(expected, timeout) def read_eager(self) -> bytes: if self.connection is None: raise ConnectionError(\\"Not connected\\") return self.connection.read_eager() def read_lazy(self) -> bytes: if self.connection is None: raise ConnectionError(\\"Not connected\\") return self.connection.read_lazy()"},{"question":"**Problem Statement:** You are provided with a dataset containing information about students\' performance in different subjects. The dataset has the following columns: - `ID`: A unique identifier for each student. - `Math_Score`: The score obtained in Mathematics. - `Science_Score`: The score obtained in Science. - `English_Score`: The score obtained in English. - `Gender`: The gender of the student (Male/Female). - `Grade_Level`: The grade level of the student (e.g., 9th, 10th, 11th, 12th). Write a Python function `visualize_student_performance` that takes the dataset as a parameter (Pandas DataFrame) and performs the following tasks using the Seaborn library: 1. Create a swarm plot that shows the distribution of Math scores categorized by grade level. 2. On the same plot, add a `hue` parameter to differentiate between male and female students. 3. Customize the plot to display the points using a \'circle\' marker and adjust the size of the points to 4. 4. Set the palette to a qualitative palette named \'Set1\'. 5. Enable dodging to separate the points by gender within each grade level. 6. Ensure the x-axis represents the grade levels in ascending order. **Constraints:** - The function should handle cases where the dataset might have missing values. - The function should display a single comprehensive plot combining all the above requirements. **Input:** - A Pandas DataFrame `df` with columns as described above. **Output:** - A Seaborn plot visualizing the student performance as per the requirements. **Performance Considerations:** - The function should be efficient in handling datasets with up to 10,000 rows. **Example:** ```python import pandas as pd # Sample data data = { \'ID\': range(1, 11), \'Math_Score\': [75, 88, 90, 66, 80, 72, 88, 95, 77, 85], \'Science_Score\': [78, 84, 89, 60, 78, 75, 85, 92, 70, 88], \'English_Score\': [80, 83, 91, 64, 82, 73, 84, 88, 76, 86], \'Gender\': [\'Male\', \'Female\', \'Female\', \'Male\', \'Female\', \'Male\', \'Female\', \'Male\', \'Female\', \'Male\'], \'Grade_Level\': [\'10th\', \'10th\', \'11th\', \'11th\', \'12th\', \'12th\', \'9th\', \'9th\', \'10th\', \'11th\'] } df = pd.DataFrame(data) # Your function call visualize_student_performance(df) ``` **Expected Outcome:** - A Seaborn plot that visualizes Math scores categorized by \'Grade_Level\' with \'Gender\' as the `hue`, points represented with \'circle\' markers, adjusted size, Set1 palette, and dodged by gender.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_student_performance(df): Creates a seaborn swarm plot to visualize student performance in Math scores categorized by grade level and gender. Parameters: df (DataFrame): Pandas DataFrame containing the dataset with columns \'ID\', \'Math_Score\', \'Science_Score\', \'English_Score\', \'Gender\', and \'Grade_Level\'. Output: Displays a seaborn plot. # Convert grade levels to ordered categorical type to ensure ascending order on x-axis grade_order = [\'9th\', \'10th\', \'11th\', \'12th\'] df[\'Grade_Level\'] = pd.Categorical(df[\'Grade_Level\'], categories=grade_order, ordered=True) # Create the swarm plot plt.figure(figsize=(10, 6)) sns.swarmplot( x=\'Grade_Level\', y=\'Math_Score\', hue=\'Gender\', data=df, marker=\'o\', size=4, palette=\'Set1\', dodge=True ) # Customize the plot plt.title(\'Student Performance in Mathematics\') plt.xlabel(\'Grade Level\') plt.ylabel(\'Math Score\') plt.legend(title=\'Gender\', loc=\'upper left\', bbox_to_anchor=(1, 1)) plt.grid(True) # Display the plot plt.show()"},{"question":"You are provided with a module `bisect` documented above. Using this module, implement a function that performs a series of operations to keep a list of tuples sorted based on a specific tuple attribute. # Problem Statement Write a function `process_operations(operations: List[Tuple[str, Tuple]]) -> List[Optional[int]]` that processes a list of operations on a list of tuples. Each operation can be one of the following: - `\'insert\'`: Insert a tuple into the list while maintaining the list sorted based on a specified attribute. - `\'index\'`: Find the index of the first tuple with a specified attribute value. The function should return a list of results for all `\'index\'` operations. An `\'index\'` operation returns the 0-based index of the first tuple with the specified attribute value or `None` if such a tuple does not exist. # Parameters - `operations`: List of tuples, where each tuple contains a string (`\'insert\'` or `\'index\'`) and another tuple containing the details: - For `\'insert\'`, it contains the tuple to be inserted (e.g., `(\'insert\', (5, \'data\'))`). - For `\'index\'`, it contains the attribute value to search for (e.g., `(\'index\', (5,))`). # Return - A list of results for all `\'index\'` operations in the order they appear in the input list. Each result is either the index of the first matching tuple or `None`. # Constraints - The list of tuples should be kept sorted based on the first element of the tuples. - The input list of operations should be processed sequentially. - Tuples inserted into the list have unique first elements. # Example ```python def process_operations(operations: List[Tuple[str, Tuple]]) -> List[Optional[int]]: # Your implementation here # Example usage: operations = [ (\'insert\', (3, \'a\')), (\'insert\', (1, \'b\')), (\'index\', (1,)), (\'insert\', (2, \'c\')), (\'index\', (4,)), (\'index\', (2,)) ] print(process_operations(operations)) # Output: [1, None, 1] ``` # Notes - Use the `bisect` module\'s functions to handle insertion and search operations efficiently. - Focus on maintaining the list sorted and correctly handling the edge cases.","solution":"from bisect import bisect_left, insort from typing import List, Tuple, Optional def process_operations(operations: List[Tuple[str, Tuple]]) -> List[Optional[int]]: Process a series of operations to keep a list of tuples sorted. Args: - operations: A list of operations where each operation is a tuple containing: - A string (\'insert\' or \'index\') - A tuple containing the details of the operation Returns: - A list of results for all \'index\' operations sorted_list = [] results = [] for operation in operations: op_type, details = operation if op_type == \'insert\': tuple_to_insert = details insort(sorted_list, tuple_to_insert) elif op_type == \'index\': attr_value = details[0] index = bisect_left(sorted_list, (attr_value,)) if index < len(sorted_list) and sorted_list[index][0] == attr_value: results.append(index) else: results.append(None) return results"},{"question":"**Problem Statement: Data Manipulation with Pandas** You are given details of employees in a company in the form of two dictionaries. Each dictionary represents one department and contains employee IDs as keys and their salaries as values. ```python dept_a = { \\"E001\\": 70000, \\"E002\\": 80000, \\"E003\\": 85000, \\"E004\\": 90000 } dept_b = { \\"E003\\": 95000, \\"E004\\": 100000, \\"E005\\": 105000, \\"E006\\": 110000 } ``` You are required to perform the following tasks: 1. **Create Series Objects:** - Create two pandas Series, `series_a` and `series_b`, from the dictionaries `dept_a` and `dept_b` respectively, ensuring that the keys (employee IDs) serve as the index. 2. **Employee Salary Lookup:** - Create a function `get_employee_salary(emp_id)` that takes an employee ID as input and returns a dictionary indicating the salary of the employee in `dept_a` and `dept_b`. If the employee ID is not found in a department, return `None` for that department. 3. **Calculate Total Salary:** - Combine `series_a` and `series_b` into a single Series object `combined_series` where the salary of employees who are in both departments should be summed. Sort this combined series in descending order of salaries. 4. **Top N Earners:** - Create a function `top_n_earners(combined_series, n)` that takes the combined series and an integer `n` and returns a DataFrame containing details of the top `n` earners. The DataFrame should have columns \\"Employee ID\\" and \\"Total Salary\\". # Input and Output Formats **Input:** - Two dictionaries representing departments. - `get_employee_salary(emp_id)` function input: A string representing the employee ID. - `top_n_earners(combined_series, n)` function input: A pandas Series and an integer `n`. **Output:** - `get_employee_salary(emp_id)` function output: A dictionary with keys \\"Department A\\" and \\"Department B\\" and their corresponding salaries or `None`. - `top_n_earners(combined_series, n)` function output: A pandas DataFrame. # Constraints and Considerations - Use pandas Series and DataFrames to handle the data efficiently. - Ensure proper handling of missing data (e.g., employee IDs that might not be present in one of the departments). - Assume that the inputs provided to the functions are always valid. # Example ```python import pandas as pd dept_a = { \\"E001\\": 70000, \\"E002\\": 80000, \\"E003\\": 85000, \\"E004\\": 90000 } dept_b = { \\"E003\\": 95000, \\"E004\\": 100000, \\"E005\\": 105000, \\"E006\\": 110000 } # Task 1: Create Series Objects series_a = pd.Series(dept_a) series_b = pd.Series(dept_b) # Task 2: Employee Salary Lookup def get_employee_salary(emp_id): salary_a = series_a.get(emp_id, None) salary_b = series_b.get(emp_id, None) return {\\"Department A\\": salary_a, \\"Department B\\": salary_b} print(get_employee_salary(\\"E003\\")) # Output: {\\"Department A\\": 85000, \\"Department B\\": 95000} print(get_employee_salary(\\"E005\\")) # Output: {\\"Department A\\": None, \\"Department B\\": 105000} # Task 3: Calculate Total Salary combined_series = series_a.add(series_b, fill_value=0).sort_values(ascending=False) print(combined_series) # Task 4: Top N Earners def top_n_earners(combined_series, n): top_earners = combined_series.head(n).reset_index() top_earners.columns = [\\"Employee ID\\", \\"Total Salary\\"] return top_earners print(top_n_earners(combined_series, 3)) ```","solution":"import pandas as pd # Task 1: Create Series Objects dept_a = { \\"E001\\": 70000, \\"E002\\": 80000, \\"E003\\": 85000, \\"E004\\": 90000 } dept_b = { \\"E003\\": 95000, \\"E004\\": 100000, \\"E005\\": 105000, \\"E006\\": 110000 } series_a = pd.Series(dept_a) series_b = pd.Series(dept_b) # Task 2: Employee Salary Lookup def get_employee_salary(emp_id): salary_a = series_a.get(emp_id, None) salary_b = series_b.get(emp_id, None) return {\\"Department A\\": salary_a, \\"Department B\\": salary_b} # Task 3: Calculate Total Salary combined_series = series_a.add(series_b, fill_value=0).sort_values(ascending=False) # Task 4: Top N Earners def top_n_earners(combined_series, n): top_earners = combined_series.head(n).reset_index() top_earners.columns = [\\"Employee ID\\", \\"Total Salary\\"] return top_earners"},{"question":"Objective Design and implement a custom class in Python that emulates a sequence and provides both synchronous and asynchronous operations. Problem Statement Implement a class `CustomSequence` that fulfills the following requirements: 1. **Initialization**: - The class should be initialized with an iterable representing its elements (e.g., a list). 2. **Sequence Emulation**: - Implement basic sequence operations including: - `__getitem__` to support indexing. - `__setitem__` to support item assignment. - `__delitem__` to support item deletion. - `__iter__` to support iteration. - `__len__` to return the length of the sequence. 3. **Additional Methods**: - Implement a method `append` to add items to the sequence. - Implement a method `reverse` to reverse the sequence in place. 4. **Asynchronous Methods**: - Implement asynchronous context manager methods: - `__aenter__` should log entering the context (use `print` for demonstration). - `__aexit__` should log exiting the context. Input and Output - **Input**: - The initializer accepts an iterable as input. - Methods may accept indices, elements, or other iterables as per standard list operations. - **Output**: - No explicit output, but the methods should modify the object\'s state as expected. - Context manager operations should print \\"Entering context\\" and \\"Exiting context\\". Example Usage ```python async def main(): async with CustomSequence([1, 2, 3, 4, 5]) as seq: print(len(seq)) # Output: 5 print(seq[2]) # Output: 3 seq[2] = 10 print(seq[2]) # Output: 10 seq.append(7) print(seq[5]) # Output: 7 seq.reverse() print(seq[0]) # Output: 7 del seq[1] print(seq[1]) # Output: 4 (after reversal) # Run the async main function to see the context manager in action. import asyncio asyncio.run(main()) ``` Constraints - The `CustomSequence` should be designed to work with any type of elements that support Python\'s standard sequence operations. - The asynchronous context manager methods should simulate resource management and should be implemented efficiently. Evaluation Criteria - Correct implementation of the sequence interface. - Correct handling and functionality of the asynchronous context manager. - Code readability and adherence to Pythonic principles.","solution":"class CustomSequence: def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __iter__(self): return iter(self._data) def __len__(self): return len(self._data) def append(self, value): self._data.append(value) def reverse(self): self._data.reverse() async def __aenter__(self): print(\\"Entering context\\") return self async def __aexit__(self, exc_type, exc_value, traceback): print(\\"Exiting context\\") return False # Do not suppress exceptions"},{"question":"<|Analysis Begin|> The provided documentation details the \\"pdb\\" module in Python, an interactive source code debugger for Python programs. The functionality includes setting and managing breakpoints, inspecting stack frames and source code, evaluating arbitrary Python code within any stack frame, and performing post-mortem debugging. **Key Features Highlighted:** 1. **Running and evaluating code**: `pdb.run()`, `pdb.runeval()` 2. **Calling functions**: `pdb.runcall()` 3. **Setting breakpoints and tracing**: `pdb.set_trace()` 4. **Post-mortem debugging**: `pdb.post_mortem()`, `pdb.pm()` 5. **Extending the debugger**: Customizing the `Pdb` class. 6. **Debugger Commands**: Various commands (`break`, `continue`, `next`, `step`, `list`, etc.) for controlling and inspecting the program during debugging. **Concepts Covered:** - Using pre-defined methods to control program execution. - Implementing custom debugging logic through class extension. - Handling program execution state and inspection through commands. To craft a question, I will focus on these concepts and particularly on the extensibility aspect of the `Pdb` class, combined with practical usage of debugger commands. <|Analysis End|> <|Question Begin|> # Advanced Python Debugger Implementation Objective: Design and implement a customized debugger subclass using Python\'s `pdb` module. This custom debugger should enhance the standard debugging functionality by adding additional features and commands. Task: 1. **Subclass the `Pdb` class**: Create a custom debugger class `CustomPdb` that extends the `Pdb` class. 2. **Implement a custom command**: Introduce a new command `countlines` within the `CustomPdb` class. This command should print the total number of lines in the current source file being debugged. 3. **Set a breakpoint**: Utilize the `set_trace` method to set a breakpoint within your script to initiate the debugger. Requirements: - Your custom debugger should inherit from `pdb.Pdb`. - Add functionality to count and print the total number of lines in the current file. - Integrate the custom command within the debugger\'s command set. - Demonstrate the use of `CustomPdb` with some sample code that requires debugging. Input: 1. Your Python script containing the debugger class and sample code. 2. Upon triggering the custom command `countlines` during a debugging session, the output should show the total number of lines. Output: Print the total number of lines of the current source file when the \'countlines\' command is issued during a debugging session. Constraints: - You are allowed to use the `Pdb` class and extend it. - The command `countlines` should be able to count lines in any source file being debugged. Example Code: ```python import pdb class CustomPdb(pdb.Pdb): def do_countlines(self, arg): \\"Count total number of lines in the current file.\\" if self.curframe is None: print(\\"No frame is currently being debugged.\\") return filename = self.curframe.f_code.co_filename try: with open(filename, \'r\') as file: total_lines = sum(1 for _ in file) print(f\\"Total number of lines in {filename}: {total_lines}\\") except Exception as e: print(f\\"Could not read file {filename}: {e}\\") # Sample function to debug def buggy_function(): a = 1 b = 2 c = a + b print(c) pdb.set_trace() # Activate CustomPdb if __name__ == \\"__main__\\": CustomPdb().runcall(buggy_function) ``` Expected Output During Debugging: When `countlines` is entered at the debugger prompt while debugging `buggy_function`, it should output: ``` (Pdb) countlines Total number of lines in <path_to_script>: <number_of_lines> ``` Good luck!","solution":"import pdb class CustomPdb(pdb.Pdb): def do_countlines(self, arg): \\"Count total number of lines in the current file.\\" if self.curframe is None: print(\\"No frame is currently being debugged.\\") return filename = self.curframe.f_code.co_filename try: with open(filename, \'r\') as file: total_lines = sum(1 for _ in file) print(f\\"Total number of lines in {filename}: {total_lines}\\") except Exception as e: print(f\\"Could not read file {filename}: {e}\\") # Sample function to debug def buggy_function(): a = 1 b = 2 c = a + b print(c) pdb.set_trace() # Activate CustomPdb if __name__ == \\"__main__\\": CustomPdb().runcall(buggy_function)"},{"question":"# Python Coding Assessment Question Objective: To assess your understanding of Python\'s object-oriented programming concepts, including classes, inheritance, data attributes, method overriding, and iteration. Task: You need to implement a system to manage a library of books. Requirements: 1. Define a class `Book` with the following attributes: - `title` (string): The title of the book. - `author` (string): The author of the book. - `isbn` (string): The ISBN number of the book. - `available` (boolean): A flag that indicates whether the book is available for borrowing. 2. Implement the following methods for the `Book` class: - `__str__`: Returns a string representation of the book in the format `\\"Title by Author (ISBN: ISBN_CODE)\\"`. - `borrow`: Sets the `available` attribute to `False`. - `return_book`: Sets the `available` attribute to `True`. 3. Define a class `Library` with the following attributes: - `books` (list of `Book`): A list to store instances of `Book`. 4. Implement the following methods for the `Library` class: - `add_book(book)`: Adds a new book to the library. - `borrow_book(isbn)`: Allows borrowing a book by ISBN. If the book is available, set its `available` attribute to `False` and return a string indicating successful borrowing. If the book is not available, return a message indicating the book is already borrowed. - `return_book(isbn)`: Allows returning a borrowed book by ISBN. If the book is found, set its `available` attribute to `True` and return a string indicating successful return. If the book is not found, return a message indicating the book is not found in the library. - `list_available_books()`: Returns a list of all books that are currently available for borrowing, as strings in the format defined in the `__str__` method of the `Book` class. Constraints: - The ISBN is unique for each book. Input: - Function calls to create instances of `Book` and `Library`. - Methods calls on the instances created. Output: - Strings returned by the methods of `Book` and `Library`. Example: ```python # Create books book1 = Book(\\"1984\\", \\"George Orwell\\", \\"1234567890\\") book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"0987654321\\") # Create a library library = Library() # Add books to the library library.add_book(book1) library.add_book(book2) # List available books print(library.list_available_books()) # Output: [\\"1984 by George Orwell (ISBN: 1234567890)\\", \\"To Kill a Mockingbird by Harper Lee (ISBN: 0987654321)\\"] # Borrow a book print(library.borrow_book(\\"1234567890\\")) # Output: \\"1984 has been borrowed.\\" # List available books after borrowing print(library.list_available_books()) # Output: [\\"To Kill a Mockingbird by Harper Lee (ISBN: 0987654321)\\"] # Return a book print(library.return_book(\\"1234567890\\")) # Output: \\"1984 has been returned.\\" # List available books after returning print(library.list_available_books()) # Output: [\\"1984 by George Orwell (ISBN: 1234567890)\\", \\"To Kill a Mockingbird by Harper Lee (ISBN: 0987654321)\\"] ``` Implement the classes `Book` and `Library` based on the requirements above.","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn self.available = True def __str__(self): return f\'{self.title} by {self.author} (ISBN: {self.isbn})\' def borrow(self): self.available = False def return_book(self): self.available = True class Library: def __init__(self): self.books = [] def add_book(self, book): self.books.append(book) def borrow_book(self, isbn): for book in self.books: if book.isbn == isbn: if book.available: book.borrow() return f\'{book.title} has been borrowed.\' else: return \'The book is already borrowed.\' return \'The book is not found in the library.\' def return_book(self, isbn): for book in self.books: if book.isbn == isbn: book.return_book() return f\'{book.title} has been returned.\' return \'The book is not found in the library.\' def list_available_books(self): available_books = [str(book) for book in self.books if book.available] return available_books"},{"question":"# QUESTION You are provided with a dataset containing multiple sound files of various formats. Your task is to write a Python function that reads each sound file, determines its type and metadata using the `sndhdr` module, and then produces a summary report. Function Signature ```python import sndhdr from typing import List, Tuple def summarize_sound_files(file_list: List[str]) -> List[Tuple[str, str, int, int, int, int]]: # Write your code here pass ``` Input: - `file_list`: A list of strings where each string is a path to a sound file. Output: - A list of tuples, where each tuple contains: - The filename (string) - The file type (string) - The frame rate (integer) - The number of channels (integer) - The number of frames (integer) - The sampling width in bits (integer or string if \'A\' for A-LAW or \'U\' for u-LAW) Constraints: - Each file path in the input list is guaranteed to exist. - If the file type or metadata could not be determined, the function should skip that file and not include it in the result. Example: ```python file_list = [\\"example1.wav\\", \\"example2.aiff\\", \\"example3.unknown\\"] output = summarize_sound_files(file_list) # Expected output could be: # [ # (\\"example1.wav\\", \\"wav\\", 44100, 2, 352800, 16), # (\\"example2.aiff\\", \\"aiff\\", 44100, 2, 705600, 16) # ] ``` Performance Requirements: - The function should handle up to 1000 files in a reasonable time frame. - Efficient file reading and error handling should be considered. Notes: - Use the `sndhdr.what` or `sndhdr.whathdr` functions from the `sndhdr` module to implement this function. - The result should not contain any entries for files that could not be determined.","solution":"import sndhdr from typing import List, Tuple def summarize_sound_files(file_list: List[str]) -> List[Tuple[str, str, int, int, int, int]]: summary = [] for file_path in file_list: file_info = sndhdr.whathdr(file_path) if file_info: file_name = file_path.split(\'/\')[-1] file_type = file_info.filetype frame_rate = file_info.framerate num_channels = file_info.nchannels num_frames = file_info.nframes sampwidth = file_info.sampwidth or \'unknown\' summary.append((file_name, file_type, frame_rate, num_channels, num_frames, sampwidth)) return summary"},{"question":"You are required to write a CGI script using the deprecated `cgi` module. This script will process form data submitted via an HTML form and return a customized HTML response based on the data received. # Problem Statement Write a CGI script using Python that processes an HTTP POST request containing form data. The script should perform the following tasks: 1. Parse the submitted form data using the `FieldStorage` class. 2. Validate that the form contains fields `username` and `email`. 3. Generate an HTML response that: - Displays a message \\"Welcome, [username]!\\" - If the `email` field contains a value, include a second line saying \\"Your email is [email]\\". Otherwise, display \\"Email is not provided.\\" 4. Handle the case where form fields `username` and `email` are missing by displaying an error message. 5. Include appropriate HTTP headers in the response. # Input Format - The form data will be submitted as an HTTP POST request with the following fields: - `username`: A string containing the username (mandatory). - `email`: A string containing the email address (optional). # Output Format - Correctly formatted HTML response string containing the required messages based on the provided form data. # Constraints - The script must handle cases where the form fields are missing or empty. - Assume the environment and standard input are correctly set up to simulate an HTTP POST request. # Example Given the form submission with the following data: ``` username=JohnDoe email=johndoe@example.com ``` The CGI script should generate an HTML response: ```html Content-Type: text/html <html> <head><title>CGI Script Output</title></head> <body> <h1>Welcome, JohnDoe!</h1> <p>Your email is johndoe@example.com</p> </body> </html> ``` Given the form submission with the following data: ``` username=JohnDoe ``` The CGI script should generate an HTML response: ```html Content-Type: text/html <html> <head><title>CGI Script Output</title></head> <body> <h1>Welcome, JohnDoe!</h1> <p>Email is not provided.</p> </body> </html> ``` # Implementation Implement the CGI script considering all the requirements. ```python import cgi import cgitb cgitb.enable() # Enable detailed error messages for debugging def main(): # Create instance of FieldStorage form = cgi.FieldStorage() # Print the content type header print(\\"Content-Type: text/html\\") print() # Check for mandatory fields if \'username\' not in form: print(\\"<h1>Error</h1>\\") print(\\"Please provide a username.\\") return # Retrieve form data username = form.getvalue(\'username\') email = form.getvalue(\'email\', \'\') # Generate the HTML response print(\\"<html>\\") print(\\"<head><title>CGI Script Output</title></head>\\") print(\\"<body>\\") print(f\\"<h1>Welcome, {username}!</h1>\\") if email: print(f\\"<p>Your email is {email}</p>\\") else: print(\\"<p>Email is not provided.</p>\\") print(\\"</body>\\") print(\\"</html>\\") if __name__ == \'__main__\': main() ```","solution":"import cgi import cgitb cgitb.enable() # Enable detailed error messages for debugging def main(): # Create instance of FieldStorage form = cgi.FieldStorage() # Print the content type header print(\\"Content-Type: text/html\\") print() # Check for mandatory fields if \'username\' not in form: print(\\"<html>\\") print(\\"<head><title>CGI Script Output</title></head>\\") print(\\"<body>\\") print(\\"<h1>Error</h1>\\") print(\\"Please provide a username.\\") print(\\"</body>\\") print(\\"</html>\\") return # Retrieve form data username = form.getvalue(\'username\') email = form.getvalue(\'email\', \'\') # Generate the HTML response print(\\"<html>\\") print(\\"<head><title>CGI Script Output</title></head>\\") print(\\"<body>\\") print(f\\"<h1>Welcome, {username}!</h1>\\") if email: print(f\\"<p>Your email is {email}</p>\\") else: print(\\"<p>Email is not provided.</p>\\") print(\\"</body>\\") print(\\"</html>\\") if __name__ == \'__main__\': main()"},{"question":"**Question:** You are provided with a dataset containing information about various penguin species, including their bill length and bill depth measurements. Using the seaborn library, you are required to perform the following tasks: 1. Create a scatterplot with marginal histograms of bill length (`bill_length_mm`) and bill depth (`bill_depth_mm`). 2. Add color differentiation by species (`species`) to the scatterplot and overlay separate density curves on the marginal axes. 3. Create the same plot, but instead of scatterplot points, use hexagonal bins to represent bivariate histograms. 4. Customize the plot by changing the marker to a \'+\' symbol, the size of the markers to 100, and adjusting the bins in the marginal plots to 25 without fill. 5. Set the overall plot height to 6 and the ratio of marginal axes to the central plot area to 3. 6. Add red contour lines representing a kernel density estimate (KDE) to the joint area, and also include a rug plot on the marginal axes. The plots should be displayed within a single cell of your Jupyter notebook. Input: - The dataset to use is loaded using `sns.load_dataset(\\"penguins\\")`. Output: - A series of plots displayed using seaborn in a Jupyter notebook cell. Constraints: - You may not use any other visualization or plotting libraries. - Ensure that the plots are properly labeled and provide clear differentiations between the species. Performance Requirement: - The plots should be rendered efficiently, leveraging seaborn capabilities. Use the code snippets provided in the documentation as a reference to complete this task. ```python import seaborn as sns # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a scatterplot with marginal histograms sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") # Task 2: Add color differentiation by species and overlay density curves sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") # Task 3: Use hexagonal bins to represent bivariate histograms sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"hex\\") # Task 4: Customize the plot with specific marker and bin characteristics sns.jointplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", marker=\\"+\\", s=100, marginal_kws=dict(bins=25, fill=False), ) # Task 5: Set plot height and ratio sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", height=6, ratio=3, marginal_ticks=True) # Task 6: Add KDE contour lines and rug plot g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-.15, clip_on=False) ``` Remember to import the necessary seaborn package and load the dataset properly.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a scatterplot with marginal histograms def scatterplot_with_marginal_histograms(): sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") plt.show() # Task 2: Add color differentiation by species and overlay density curves on the marginal axes def scatterplot_with_species_and_density(): sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.show() # Task 3: Use hexagonal bins to represent bivariate histograms def hexbin_bivariate_histograms(): sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"hex\\") plt.show() # Task 4: Customize the plot with specific marker and bin characteristics def customized_plot(): sns.jointplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", marker=\\"+\\", s=100, marginal_kws=dict(bins=25, fill=False), ) plt.show() # Task 5: Set plot height and ratio def plot_with_height_and_ratio(): sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", height=6, ratio=3, marginal_ticks=True) plt.show() # Task 6: Add KDE contour lines and rug plot def plot_with_kde_contour_and_rugplot(): g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) # Add a rugplot on the marginal axes g.plot_marginals(sns.rugplot, color=\\"r\\", height=-.15, clip_on=False) plt.show()"},{"question":"**Objective**: Test the ability to work with the seaborn package, focusing on creating and customizing plots using the `seaborn.objects` module. **Problem Statement**: You are given a dataset containing information about the sales and profit of various products over a period of time. Your task is to create a line plot using the `seaborn.objects` module that visually represents the data, with specific customization requirements. **Requirements**: 1. Load the data (provided as a list of dictionaries) into two lists, `x` and `y`. 2. Create a line plot where the x-axis represents the sales values and the y-axis represents the profit values. 3. Add line markers to the plot. 4. Set the x-axis limits to range from 0 to 100 and y-axis limits to range from -50 to 150. 5. Create a second version of the plot with the y-axis inverted. **Input Format**: - A list of dictionaries where each dictionary contains two keys: `\'sales\'` (int) and `\'profit\'` (int). For example: ```python data = [ {\'sales\': 10, \'profit\': 5}, {\'sales\': 30, \'profit\': 25}, {\'sales\': 20, \'profit\': 15}, {\'sales\': 50, \'profit\': 45} ] ``` **Output**: - Two plots: 1. A standard line plot with specified axis limits. 2. A line plot with the y-axis inverted. **Constraints**: - Use the `seaborn.objects` module exclusively for creating and customizing the plots. **Function Signature**: ```python def plot_sales_profit(data): # Your code here ``` **Example**: ```python data = [ {\'sales\': 10, \'profit\': 5}, {\'sales\': 30, \'profit\': 25}, {\'sales\': 20, \'profit\': 15}, {\'sales\': 50, \'profit\': 45} ] plot_sales_profit(data) ``` *Expected Outcome*: Two plots should be generated according to the specifications provided above. **Notes**: - Make sure to follow the seaborn documentation provided to use the correct methods and parameters. - Ensure that your function is well-documented with comments explaining each step of the process.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_sales_profit(data): # Extract sales and profit data from the input list of dictionaries x = [entry[\'sales\'] for entry in data] y = [entry[\'profit\'] for entry in data] # Create the first plot: Standard line plot with specified axis limits p = sns.lineplot(x=x, y=y, marker=\'o\') p.set_xlim(0, 100) p.set_ylim(-50, 150) plt.title(\\"Sales vs Profit (Standard)\\") plt.show() # Create the second plot: Line plot with y-axis inverted p2 = sns.lineplot(x=x, y=y, marker=\'o\') p2.set_xlim(0, 100) p2.set_ylim(150, -50) plt.title(\\"Sales vs Profit (Inverted Y-axis)\\") plt.show()"},{"question":"# Question: Advanced Plotting with Seaborn\'s Objects Interface You are tasked with analyzing the \'penguins\' dataset and creating two different types of visualizations using Seaborn\'s `seaborn.objects` interface. This exercise will test your ability to use fundamental and advanced features of the `seaborn.objects` interface to create and customize plots. Requirements: 1. **Visualization A: Scatter Plot with Multiple Layers** - Create a scatter plot depicting the relationship between penguin bill length and bill depth. - Customize the scatter plot: - Map the `species` column to color. - Map the `body_mass_g` column to point size. - Add a linear regression line to show the trend. - Customize the regression line to have a green color and a linewidth of 2. 2. **Visualization B: Faceted Bar Plot** - Create a faceted bar plot showing the average body mass for each penguin species, separated by sex. - Facet the plot by the `island` column in the dataset. - Customize the bar plot: - Map the `sex` column to color. - Set custom colors for the bars representing male (`#1f77b4`) and female (`#ff7f0e`). - Ensure the bar plot displays error bars representing standard deviation. Expected Input and Output Formats: - **Input:** None (The code should load the `penguins` dataset internally). - **Output:** The code should display two plots as described above. The plots should be constructed using only the `seaborn.objects` interface. Constraints: - You may assume that all necessary libraries (`seaborn`, `matplotlib`, etc.) are already installed. - You can only use the `seaborn.objects` interface to create the plots; do not revert to the traditional seaborn function-based API. - Ensure the code runs without errors. Performance Requirements: - Efficiently handle plotting operations. - Display plots with clear, accurate, and properly labeled axes and legends. Example Code: ```python import seaborn as sns import seaborn.objects as so # Load the dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Visualization A: Scatter Plot with Multiple Layers scatter_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\", pointsize=\\"body_mass_g\\") .add(so.Dots()) .add(so.Line(color=\\"green\\", linewidth=2), so.PolyFit()) ) # Display the scatter plot scatter_plot.show() # Visualization B: Faceted Bar Plot bar_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .facet(col=\\"island\\") .add(so.Bar(), so.Agg(), edgecolor=\\".2\\") .scale(color={\\"Male\\": \\"#1f77b4\\", \\"Female\\": \\"#ff7f0e\\"}) .add(so.Range(), so.Est(errorbar=\\"sd\\"), edgecolor=\\".2\\") ) # Display the bar plot bar_plot.show() ``` Carefully read the requirements and fulfill each condition specified to achieve the desired outputs. Good luck!","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_scatter_plot(penguins): scatter_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", color=\\"species\\", pointsize=\\"body_mass_g\\") .add(so.Dots()) .add(so.Line(color=\\"green\\", linewidth=2), so.PolyFit()) ) return scatter_plot def create_faceted_bar_plot(penguins): bar_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .facet(col=\\"island\\") .add(so.Bar(), so.Agg(), edgecolor=\\".2\\") .scale(color={\\"Male\\": \\"#1f77b4\\", \\"Female\\": \\"#ff7f0e\\"}) .add(so.Range(), so.Est(errorbar=\\"sd\\"), edgecolor=\\".2\\") ) return bar_plot def main(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Create scatter plot scatter_plot = create_scatter_plot(penguins) # Display the scatter plot scatter_plot.show() # Create faceted bar plot bar_plot = create_faceted_bar_plot(penguins) # Display the bar plot bar_plot.show() if __name__ == \\"__main__\\": main()"},{"question":"You are required to write a Python script that accepts a file containing Python function definitions and statements. Your script needs to time the execution of these functions using the `timeit` module. The purpose of this script is to determine and compare the average execution time of each function within the file. **Input:** - A file with Python function definitions and statements. - Each function should be timed for 10000 executions. **Output:** - The average execution time for each function in the file in microseconds (Î¼s). **Constraints:** - Assume each function is self-contained and does not require external modules other than those in the Python standard library. - Each function can accept at most two parameters. - Your script should handle any exceptions that occur during the timing of the functions. **Performance Constraints:** - Ensure that your solution handles files with up to 100 functions efficiently. **Example:** Consider an input file `functions.py` containing: ```python def func1(): return \\"-\\".join(str(n) for n in range(100)) def func2(): return \\"-\\".join([str(n) for n in range(100)]) def func3(): return \\"-\\".join(map(str, range(100))) ``` Your script should produce output similar to: ```plaintext func1: xx.x Î¼s per loop func2: xx.x Î¼s per loop func3: xx.x Î¼s per loop ``` **Implementation Details:** 1. Write a script that reads the functions from the input file. 2. Use the `timeit` module to measure the execution time of each function for 10000 loops. 3. Handle and report any exceptions that occur during the timing. 4. Format the results in microseconds per loop and print them to the console. **Hints:** - Use the `globals()` parameter to pass the functions to `timeit.timeit()`. - Use exception handling to ensure that the script does not terminate prematurely due to runtime errors within the functions.","solution":"import timeit import importlib.util import os def time_functions_from_file(file_path): Times the execution of each function defined in the specified Python file. # Load the module from the file spec = importlib.util.spec_from_file_location(\\"module.name\\", file_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) results = [] # Get all functions defined in the module functions = [func for func in dir(module) if callable(getattr(module, func))] # Time each function for func_name in functions: func = getattr(module, func_name) if func.__code__.co_argcount <= 2: # Ensure the function has at most 2 parameters try: exec_time = timeit.timeit(f\\"{func_name}()\\", globals={\'__builtins__\': __builtins__, func_name: func}, number=10000) exec_time_per_loop = exec_time / 10000 * 1e6 # Convert to microseconds results.append((func_name, exec_time_per_loop)) except Exception as e: # Print the exception if any occurs print(f\\"Timing error for function {func_name}: {e}\\") # Print results for func_name, exec_time in results: print(f\\"{func_name}: {exec_time:.2f} Î¼s per loop\\") # Example usage; replace \\"functions.py\\" with your file\'s path # time_functions_from_file(\\"functions.py\\")"},{"question":"**Question: Configuring and Customizing Seaborn Plot Contexts** Seaborn is a powerful Python library for data visualization that provides high-level interfaces to draw attractive and informative statistical graphics. One of the features of Seaborn is its ability to set different contexts which control the scale of the plot elements. In this exercise, you are required to demonstrate the ability to configure and customize Seaborn plot contexts. # Task Description: Write a Python function `customize_plot_context` that takes in three parameters: `context`, `font_scale`, and `line_width`. This function should configure the Seaborn context for all plots using the provided parameters and then generate and return a line plot based on a given dataset. # Input: 1. `context` (string): One of the Seaborn contexts (`\\"paper\\"`, `\\"notebook\\"`, `\\"talk\\"`, or `\\"poster\\"`). 2. `font_scale` (float): A scaling factor for fonts relative to the default settings. 3. `line_width` (float): The width of the lines in the plot. # Output: - A Seaborn line plot with the x-axis representing `[0, 1, 2, 3, 4]` and the y-axis representing `[2, 3, 5, 1, 4]`. # Constraints: - The `context` must be one of the valid strings mentioned above. - The `font_scale` and `line_width` must be positive floats. # Example: ```python import matplotlib.pyplot as plt def customize_plot_context(context, font_scale, line_width): import seaborn as sns sns.set_context(context, font_scale=font_scale, rc={\\"lines.linewidth\\": line_width}) x = [0, 1, 2, 3, 4] y = [2, 3, 5, 1, 4] plot = sns.lineplot(x=x, y=y) return plot # Example Usage plot = customize_plot_context(\\"notebook\\", 1.2, 2.5) plt.show() ``` # Notes: - Ensure that the function sets the context before generating the plot. - The function should raise a `ValueError` if the context is not one of the specified valid contexts. Your implementation should demonstrate the ability to effectively set and customize plot contexts in Seaborn, allowing for aesthetic adjustments based on the provided parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot_context(context, font_scale, line_width): Configure the Seaborn context for all plots using the provided parameters and generate a line plot. Args: context (str): One of the valid Seaborn contexts (\'paper\', \'notebook\', \'talk\', \'poster\'). font_scale (float): Scaling factor for fonts relative to the default settings. line_width (float): The width of the lines in the plot. Returns: matplotlib.axes._subplots.AxesSubplot: The generated Seaborn line plot. Raises: ValueError: If the context is not one of the valid Seaborn contexts. valid_contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] if context not in valid_contexts: raise ValueError(f\\"Invalid context \'{context}\'. Valid contexts are \'paper\', \'notebook\', \'talk\', \'poster\'.\\") if font_scale <= 0 or line_width <= 0: raise ValueError(\\"font_scale and line_width must be positive floats.\\") sns.set_context(context, font_scale=font_scale, rc={\\"lines.linewidth\\": line_width}) x = [0, 1, 2, 3, 4] y = [2, 3, 5, 1, 4] plot = sns.lineplot(x=x, y=y) return plot"},{"question":"# Question: You are given a dataset and a trained model, and you need to compute the permutation feature importance as part of the model interpretability process. Your task involves writing a function to calculate the importance of each feature using the permutation feature importance technique described in the documentation. Additionally, your function should handle both single metric and multiple metrics for scoring. Instructions: 1. Load the given dataset. 2. Train a model (you may choose from scikit-learn estimators; e.g., `RandomForestClassifier`, `Ridge`, etc.). 3. Implement a function `compute_permutation_importance(model, X, y, n_repeats, metrics)` which calculates the permutation feature importance. 4. The function should handle calculating importance using multiple scoring metrics. 5. The function should return the feature importances along with their standard deviations. 6. Write the code to display the importance of each feature for each scoring metric clearly. Input: - `model`: A trained scikit-learn estimator. - `X`: A NumPy array or pandas DataFrame representing the feature dataset. - `y`: A NumPy array or pandas Series representing the target variable. - `n_repeats`: An integer representing the number of times to permute a feature. - `metrics`: A single string or list of strings representing the scoring metrics (e.g., `[\'r2\', \'neg_mean_absolute_error\']`). Output: - A dictionary where keys are the names of the metrics and values are dictionaries themselves with feature names as keys and a tuple of (importance, standard deviation) as values. Constraints: - Ensure the function handles numerical stability and presents the feature importances in a readable format. - If a metric does not apply to the model type (e.g., regression metric on a classifier), appropriate errors should be handled gracefully. Example Usage: ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import make_scorer, r2_score # Example dataset diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(diabetes.data, diabetes.target, random_state=0) # Example model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Function call importances = compute_permutation_importance(model, X_val, y_val, n_repeats=30, metrics=[\'r2\', \'neg_mean_squared_error\']) # Output the results print(importances) ``` Note: - Use `scikit-learn`\'s `permutation_importance` function and handle single and multiple metrics. - Ensure the output format is user-friendly and allows easy interpretation of results. Ensure the code is well-commented and modular to enhance readability and maintainability.","solution":"import numpy as np import pandas as pd from sklearn.inspection import permutation_importance def compute_permutation_importance(model, X, y, n_repeats, metrics): Computes the permutation feature importance for a given model, dataset, and metrics. :param model: Trained scikit-learn estimator :param X: Feature dataset (NumPy array or pandas DataFrame) :param y: Target variable (NumPy array or pandas Series) :param n_repeats: Number of times to permute a feature :param metrics: Single string or list of strings representing scoring metrics (e.g., \'r2\', \'neg_mean_absolute_error\') :return: Dictionary of feature importances and their standard deviations for each metric if isinstance(metrics, str): metrics = [metrics] results = {} for metric in metrics: importances = permutation_importance(model, X, y, n_repeats=n_repeats, scoring=metric) feature_importances = {feature: (importance_mean, importance_std) for feature, importance_mean, importance_std in zip(X.columns, importances.importances_mean, importances.importances_std)} results[metric] = feature_importances return results # Example usage with linear regression and a regression dataset from sklearn.datasets import load_diabetes from sklearn.linear_model import Ridge from sklearn.model_selection import train_test_split # Load example dataset diabetes = load_diabetes() X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names) y = diabetes.target # Split data X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0) # Train model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Compute permutation importance importances = compute_permutation_importance(model, X_val, y_val, n_repeats=30, metrics=[\'r2\', \'neg_mean_squared_error\']) # Print the results print(importances)"},{"question":"# Question: Advanced ZIP File Operations with `zipfile` Module You are given a ZIP file named `archive.zip` which contains a collection of text files organized in various directories. Your task is to perform the following operations: 1. **List files**: List all files in the ZIP archive. Ensure the output includes both file and directory names, and indicates whether the entry is a file or a directory. 2. **Extract a specific file**: Extract a file named `data/report.txt` from the ZIP archive to a directory named `output`. 3. **Read a file**: Read the contents of a file named `data/config.txt` from the ZIP archive and print its contents to the console. 4. **Write a new file**: Add a new file named `newfile.txt` with the content \\"This is a new file\\" to the ZIP archive. Ensure it\'s stored under a directory `newdata` inside the ZIP file. 5. **Update an existing file**: If a file named `data/update.txt` exists in the ZIP archive, overwrite it with the content \\"Updated content\\". If it does not exist, add it with the specified content. 6. **Extract all files**: Extract all files from the ZIP archive to a directory named `all_output` while preserving the directory structure of the ZIP file. Constraints - You can assume the ZIP file is not encrypted. - The `archive.zip` file and the output directories (`output`, `all_output`) are in the current working directory. - Your solution should handle potential exceptions including missing files or directories in the zip archive. Performance - Ensure that operations such as listing and extracting files are efficient. - When modifying the archive, ensure the integrity of the existing contents of the ZIP file. Expected Input/Output - The input will be the `archive.zip` file containing a variety of files and directories. - The output should be the results of each operation as described, ensuring the modifications to the ZIP file and the extracted files are appropriately handled. Write a Python function `perform_zip_operations(zip_filepath: str) -> None` that performs the described operations. # Function Signature ```python import zipfile def perform_zip_operations(zip_filepath: str) -> None: # Your implementation here ``` **Example** Given an `archive.zip` with the following structure: ``` archive.zip â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ report.txt â”‚ â”œâ”€â”€ config.txt â”‚ â””â”€â”€ update.txt â””â”€â”€ documents/ â”œâ”€â”€ paper.pdf â””â”€â”€ notes.txt ``` - Listing files should print: ``` data/ (directory) data/report.txt (file) data/config.txt (file) data/update.txt (file) documents/ (directory) documents/paper.pdf (file) documents/notes.txt (file) ``` - Extracting `data/report.txt` should result in `output/report.txt`. - Reading `data/config.txt` should print the contents of `config.txt`. - Adding `newfile.txt` under `newdata` directory should result in `newdata/newfile.txt` inside the ZIP. - Overwriting `data/update.txt` should change its content to \\"Updated content\\". - Extracting all files should recreate the directory structure in `all_output`. Ensure your code is well-documented and handles error scenarios gracefully.","solution":"import zipfile import os def perform_zip_operations(zip_filepath: str) -> None: # List files in the ZIP archive with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: for info in zip_ref.infolist(): entry_type = \'directory\' if info.is_dir() else \'file\' print(f\\"{info.filename} ({entry_type})\\") # Extract a specific file with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: zip_ref.extract(\'data/report.txt\', \'output\') # Read a file with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: with zip_ref.open(\'data/config.txt\') as file: print(file.read().decode(\'utf-8\')) # Write a new file with zipfile.ZipFile(zip_filepath, \'a\') as zip_ref: new_content = \\"This is a new file\\" zip_ref.writestr(\'newdata/newfile.txt\', new_content) # Update an existing file with zipfile.ZipFile(zip_filepath, \'a\') as zip_ref: updated_content = \\"Updated content\\" zip_ref.writestr(\'data/update.txt\', updated_content) # Extract all files with zipfile.ZipFile(zip_filepath, \'r\') as zip_ref: zip_ref.extractall(\'all_output\')"},{"question":"**Optimizing and Debugging a Python Program** You are given a Python script that processes a list of numbers. Your task is to: 1. Identify and fix an existing logical error in the program. 2. Profile the program to identify performance bottlenecks. 3. Optimize the program to improve performance based on your profiling results. Program Description The script processes a list of integers to calculate the sum of squares of all even numbers and cubes of all odd numbers up to a given threshold. ```python def process_numbers(numbers, threshold): result = 0 for num in numbers: if num > threshold: break if num % 2 == 0: result += num ** 2 else: result += num ** 3 return result if __name__ == \\"__main__\\": import random numbers = [random.randint(1, 1000) for _ in range(10000)] threshold = 500 print(process_numbers(numbers, threshold)) ``` # Tasks and Requirements 1. **Identify and Fix Logical Error:** - The program might not be producing the correct results due to a logical error. Identify and fix it. Explain the error and the fix. 2. **Profile the Program:** - Use `cProfile` to profile the program and identify any performance bottlenecks. Provide the profiling results and describe your findings. 3. **Optimize the Program:** - Based on your profiling results, optimize the program for better performance. Explain what changes you made and why. # Input and Output - The function `process_numbers` takes two arguments: - `numbers`: a list of integers. - `threshold`: an integer threshold value. - The function returns the calculated result based on the above logic. - After fixing the error and optimizing, the program should still accept these same inputs and produce the correct output efficiently. # Constraints - The list `numbers` can contain up to 10,000 integers. - The integers in `numbers` are between 1 and 1000. - The threshold value will be between 1 and 1000. # Performance Requirements - The optimized program should run efficiently even for the maximum constraints. # Solution Submission - Submit the corrected and optimized version of the script. - Include a brief explanation of the logical error found and how you fixed it. - Provide the profiling results before and after optimization, and explain the changes that improved the performance. Good luck!","solution":"def process_numbers(numbers, threshold): Processes a list of numbers to calculate the sum of squares of all even numbers and cubes of all odd numbers up to a given threshold. result = 0 for num in numbers: if num > threshold: continue # Change made: continue instead of break to consider all numbers up to threshold if num % 2 == 0: result += num ** 2 else: result += num ** 3 return result # Optimization: Precompute powers for range 1 to threshold and leverage list comprehension for faster calculations. def optimized_process_numbers(numbers, threshold): even_squares = {i: i**2 for i in range(1, threshold+1) if i % 2 == 0} odd_cubes = {i: i**3 for i in range(1, threshold+1) if i % 2 != 0} result = sum(even_squares[num] for num in numbers if num in even_squares) + sum(odd_cubes[num] for num in numbers if num in odd_cubes) return result if __name__ == \\"__main__\\": import random numbers = [random.randint(1, 1000) for _ in range(10000)] threshold = 500 # Profiling the original function import cProfile print(\\"Original function result:\\", process_numbers(numbers, threshold)) cProfile.run(\'process_numbers(numbers, threshold)\') # Profiling the optimized function print(\\"Optimized function result:\\", optimized_process_numbers(numbers, threshold)) cProfile.run(\'optimized_process_numbers(numbers, threshold)\')"},{"question":"**Python Data Persistence: Implementing a Custom Serialization Mechanism** Given the concepts of data persistence and serialization in Python as described, this task involves creating a custom serialization mechanism using the `pickle` module. You will write functions to serialize and deserialize Python objects, ensuring that certain types of data are serialized in a specific format. # Requirements: 1. Implement two functions: `custom_serialize(obj)` and `custom_deserialize(serialized_str)`. 2. The `custom_serialize(obj)` function should: - Use the `pickle` module to serialize Python objects. - Convert the serialized bytes to a string for storage (using base64 encoding). - Ensure custom handling for dictionaries: before serializing, add a prefix `\\"DICT_\\"` to the string representation of dictionary objects. 3. The `custom_deserialize(serialized_str)` function should: - Convert the base64-encoded string back to bytes. - Deserialize the bytes to obtain the original Python object using the `pickle` module. - Check if the deserialized object is a dictionary with the `\\"DICT_\\"` prefix and remove the prefix before returning the dictionary. # Constraints: - The `custom_serialize` function should handle nested dictionaries up to a depth of 3. - Only basic Python data types (like lists, dictionaries, integers, and strings) will be used in this assessment. - Performance isn\'t a primary concern, but the functions should operate efficiently for common use cases. # Function signatures: ```python import base64 import pickle def custom_serialize(obj): # Your code here pass def custom_deserialize(serialized_str): # Your code here pass ``` # Example: ```python data = { \'name\': \'Alice\', \'age\': 30, \'preferences\': { \'color\': \'blue\', \'food\': \'pizza\', \'books\': { \'fiction\': \'Dune\', \'non-fiction\': \'Sapiens\' } } } serialized = custom_serialize(data) print(serialized) # This should print a base64 encoded string deserialized = custom_deserialize(serialized) print(deserialized) # This should print the original dictionary, with no DICT_ prefix ``` Implement these functions and test them with different types and structures of data.","solution":"import base64 import pickle def custom_serialize(obj): Serializes a Python object using pickle and base64 encoding. Adds a prefix to dictionary objects before serializing. Args: obj: Python object to serialize. Returns: str: Base64 encoded string representation of the serialized object. if isinstance(obj, dict): obj = \\"DICT_\\" + str(obj) serialized_bytes = pickle.dumps(obj) return base64.b64encode(serialized_bytes).decode(\'utf-8\') def custom_deserialize(serialized_str): Deserializes a base64 encoded string back to a Python object using pickle. Removes the custom prefix from dictionary objects. Args: serialized_str: Base64 encoded string representation of the serialized object. Returns: object: The deserialized Python object. serialized_bytes = base64.b64decode(serialized_str) obj = pickle.loads(serialized_bytes) if isinstance(obj, str) and obj.startswith(\\"DICT_\\"): return eval(obj[5:]) # Evaluates the string back to a dictionary. return obj"},{"question":"# Debugging a Python Script with `pdb` **Objective:** You are given a Python script that contains some logic errors. Your task is to use the `pdb` module to identify and fix these errors. This exercise aims to assess your understanding of how to effectively use the Python Debugger to troubleshoot and debug Python code. **Python Script:** ```python import pdb def faulty_function(a, b): pdb.set_trace() # Debugger will start from here result = a + some_undefined_variable return result def main(): x = 10 y = 20 addition = faulty_function(x, y) print(f\\"The addition of {x} and {y} is {addition}\\") if __name__ == \\"__main__\\": main() ``` **Instructions:** 1. Identify the errors in the script using `pdb` commands. 2. Fix the identified errors. 3. Ensure that the script runs correctly after the fixes. **Requirements:** - You must use `pdb` to identify the errors (include the commands used and the output observed in your response). - Clearly explain the errors found and how you fixed them. - Your final code should be free of any `pdb` breakpoints or errors, and should produce the correct output when executed. **Example Response:** 1. **Using `pdb` to Identify Errors:** ```python >>> import pdb >>> from faulty_script import main >>> pdb.run(\'main()\') > faulty_script.py(4)faulty_function() -> result = a + some_undefined_variable (Pdb) print(a) 10 (Pdb) print(b) 20 (Pdb) print(some_undefined_variable) NameError: name \'some_undefined_variable\' is not defined ``` 2. **Fixing the Errors:** - Error: `some_undefined_variable` is not defined. - Fix: Replace `some_undefined_variable` with `b`. 3. **Final Code:** ```python def faulty_function(a, b): result = a + b return result def main(): x = 10 y = 20 addition = faulty_function(x, y) print(f\\"The addition of {x} and {y} is {addition}\\") if __name__ == \\"__main__\\": main() ``` The fixed script should output: ``` The addition of 10 and 20 is 30 ``` Submit your final code and a detailed explanation of how you used `pdb` to debug and fix the code.","solution":"# Importing python debugger import pdb def faulty_function(a, b): # pdb.set_trace() # Debugger will start from here result = a + b # Fixed the error by using \'b\' instead of \'some_undefined_variable\' return result def main(): x = 10 y = 20 addition = faulty_function(x, y) print(f\\"The addition of {x} and {y} is {addition}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Title:** Implementation and Synchronization of Concurrent Tasks Using Multiprocessing **Objective:** The goal of this exercise is to test your ability to create concurrent processes, manage inter-process communication, and ensure proper synchronization using the `multiprocessing` module. **Task:** Write a Python program that simulates a simple counter incrementer using the `multiprocessing` module. The program should create multiple processes to increment a shared counter. Synchronization should be managed to prevent race conditions and ensure that the counter is updated correctly by all processes. **Requirements:** 1. Create a shared counter that all processes can access and modify. 2. Use at least four concurrent processes. 3. Each process should increment the counter by a given amount a specified number of times. 4. Synchronize access to the shared counter using appropriate synchronization primitives (e.g., `Lock`). 5. Ensure that at the end of the program, the shared counter reflects the total increments performed by all processes. **Expected Input and Output:** - **Input:** - Number of increments (integer, N). - Increment value (integer, M). - **Output:** - Final value of the shared counter after all increments. **Constraints:** - You must use the `multiprocessing` module and its synchronization primitives. - Ensure no race condition occurs while updating the counter. **Example:** ```python # Example values N = 1000 # Number of increments per process M = 1 # Each increment value # Expected output can vary based on the number of processes and increments Final counter value: 4000 # If using 4 processes ``` **Implementation Details:** 1. Define a function `increment_counter(counter, lock, n, m)` which takes a shared counter, a lock, the number of increments, and the increment value. Inside this function, use a loop to increment the counter `n` times by `m`, ensuring each increment is performed within a locked context. 2. Create a shared `Value` object to store the counter. 3. Create a `Lock` object to synchronize access to the counter. 4. Spawn multiple processes (at least four) that execute the `increment_counter` function. 5. Use the `start()` and `join()` methods to start and wait for all processes to complete. 6. Print the final counter value. Hereâ€™s a skeleton to help you get started: ```python from multiprocessing import Process, Value, Lock def increment_counter(counter, lock, n, m): for _ in range(n): with lock: counter.value += m def main(n, m): # Create a shared counter and lock counter = Value(\'i\', 0) lock = Lock() # Create a list of processes processes = [Process(target=increment_counter, args=(counter, lock, n, m)) for _ in range(4)] # Start all processes for p in processes: p.start() # Wait for all processes to complete for p in processes: p.join() # Print the final counter value print(f\\"Final counter value: {counter.value}\\") if __name__ == \'__main__\': N = 1000 # Number of increments per process M = 1 # Each increment value main(N, M) ``` **Submit:** Make sure your program is well-documented, with comments explaining each significant part of the code. Good luck!","solution":"from multiprocessing import Process, Value, Lock def increment_counter(counter, lock, n, m): Function to increment a shared counter n times by m, ensuring synchronization using a lock to prevent race conditions. for _ in range(n): with lock: counter.value += m def main(n, m): Main function to set up the shared counter and lock, spawn multiple processes to increment the counter, and print the final counter value. # Create a shared counter initialized to 0 and a lock for synchronization counter = Value(\'i\', 0) lock = Lock() # List to hold the processes processes = [Process(target=increment_counter, args=(counter, lock, n, m)) for _ in range(4)] # Start all processes for p in processes: p.start() # Wait for all processes to finish for p in processes: p.join() # Print the final counter value print(f\\"Final counter value: {counter.value}\\") if __name__ == \'__main__\': # Example values N = 1000 # Number of increments per process M = 1 # Each increment value main(N, M)"},{"question":"# XML Parsing with SAX in Python You are tasked with implementing a SAX-based XML parser using Python. The parser should be able to handle large XML documents incrementally and extract specific information from the document. Objective: You need to implement a class `IncrementalXMLParser` which extends `xml.sax.xmlreader.IncrementalParser` and is able to parse an XML document in chunks, extracting the text content of a specific element. Specifications: 1. Implement the class `IncrementalXMLParser` inheriting from `xml.sax.xmlreader.IncrementalParser`. 2. The class should have the following methods: - `__init__(self, element_name: str)`: Initializes the parser with the name of the element to be extracted. - `feed(self, data: str)`: Feeds a chunk of data to the parser. - `close(self)`: Finalizes the parsing process and returns the collected data. - `reset(self)`: Resets the parser for a new document. 3. The `IncrementalXMLParser` should also implement a content handler for SAX events to capture the text content of the specified element. Input: - Multiple chunks of XML data as strings fed to the parser. - Element name (string) provided at the initialization of the parser. Output: - A list of strings containing the text content of the specified elements found in the document when `close()` is called. Example: ```python # Sample usage parser = IncrementalXMLParser(\'title\') parser.feed(\'<bookstore><book><title>Harry Potter</title>\') parser.feed(\'</book><book><title>The Hobbit</title></book></bookstore>\') result = parser.close() print(result) # Output: [\'Harry Potter\', \'The Hobbit\'] ``` Constraints: - The XML document can be very large, so it must be handled incrementally. - Ensure proper handling of encodings and different input sources as per SAX standards. - Implement and handle necessary error checks and exceptions. **Hint:** Utilize `xml.sax.ContentHandler` and its methods (`startElement`, `endElement`, `characters`) to implement the content extraction logic within the `IncrementalXMLParser`.","solution":"import xml.sax class IncrementalXMLParser(xml.sax.xmlreader.IncrementalParser): def __init__(self, element_name: str): super().__init__() self.element_name = element_name self.in_target_element = False self.collected_data = [] self.buffer = \'\' self._content_handler = self.ContentHandler(self) self._parser = xml.sax.make_parser() self._parser.setContentHandler(self._content_handler) class ContentHandler(xml.sax.ContentHandler): def __init__(self, parent): self.parent = parent def startElement(self, name, attrs): if name == self.parent.element_name: self.parent.in_target_element = True self.parent.buffer = \'\' def endElement(self, name): if name == self.parent.element_name: self.parent.in_target_element = False self.parent.collected_data.append(self.parent.buffer) def characters(self, content): if self.parent.in_target_element: self.parent.buffer += content def feed(self, data: str): self._parser.feed(data) def close(self): self._parser.close() return self.collected_data def reset(self): self.in_target_element = False self.collected_data = [] self.buffer = \'\' self._parser.reset() self._parser.setContentHandler(self._content_handler)"},{"question":"# Objective: Create a complex seaborn plot that demonstrates your understanding of fundamental and advanced concepts of the seaborn package. # Problem Statement: You are provided with a dataset containing information on different species of flowers along with their sepal and petal dimensions. Your task is to use seaborn to visualize this dataset and customize the plot based on the following requirements: 1. Generate a scatter plot showing the relationship between sepal length and sepal width, colored by species. 2. Add a linear regression line to the plot for each species. 3. Customize plot limits: - Set the x-axis limit to range from 4 to 8. - Set the y-axis limit to start at 2 and maintain the default maximum. 4. Invert the y-axis. 5. Ensure that each scatter plot marker is displayed as a circle (`o`) and has a size reflective of the petal length. 6. Add appropriate labels for the x-axis, y-axis, and a title for the plot. The dataset is provided as a CSV file with columns: `species`, `sepal_length`, `sepal_width`, `petal_length`, `petal_width`. # Input Format: - Path to the CSV file containing the dataset. # Output Format: - The generated seaborn plot displayed inline (if using an interactive environment like Jupyter Notebook) or saved as an image file. # Constraints: - Use the seaborn package for all plot-related functionalities. - Ensure the plot is clear and well-labeled. - Handle any missing or malformed data gracefully. # Example: If the dataset `flower_data.csv` is given, your output should display a well-customized scatter plot following the specified requirements. # Implementation: ```python import seaborn as sns import seaborn.objects as so import pandas as pd def visualize_flower_data(file_path): # Load dataset df = pd.read_csv(file_path) # Create a scatter plot p = so.Plot(df, x=\'sepal_length\', y=\'sepal_width\', color=\'species\').add(so.Line(marker=\\"o\\")) # Add linear regression line per species p = p.add(so.Line(\'reg\', group=\'species\')) # Customize plot limits p = p.limit(x=(4, 8), y=(2, None)) # Invert y-axis p = p.limit(y=(None, 2)) # Size of scatter plot markers set to petal length p = p.add(so.Dot(marker=\\"o\\", size=\'petal_length\')) # Set plot labels p = p.label(x=\'Sepal Length\', y=\'Sepal Width\', title=\'Flower Species Sepal Dimensions\') # Display or save the plot p.show() ``` Your task is to implement the `visualize_flower_data(file_path)` function as described above. You can assume `seaborn`, `pandas`, and other necessary libraries are already installed in the environment where your code will run.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flower_data(file_path): # Load dataset df = pd.read_csv(file_path) # Handle missing data by dropping rows with any NaN values df = df.dropna() # Create a scatter plot plt.figure(figsize=(10, 6)) scatter = sns.scatterplot( data=df, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', size=\'petal_length\', sizes=(20, 200), alpha=0.7, palette=\'muted\' ) # Add linear regression lines per species sns.lmplot( data=df, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=\'muted\', markers=\'o\', scatter=False, legend=False ) # Customize plot limits plt.xlim(4, 8) plt.ylim(plt.ylim()[0], 2) # Maintain the default maximum # Invert y-axis plt.gca().invert_yaxis() # Set plot labels plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.title(\'Flower Species Sepal Dimensions\') # Display the plot plt.show()"},{"question":"# Data Analysis and Manipulation You are given a dataset containing sales information for a retail store. Your task is to analyze and manipulate this data to gain meaningful insights. Dataset `sales_data.csv` | Date | Product_ID | Store_ID | Sales | Revenue | |------------|------------|----------|-------|---------| | 2022-01-01 | 1 | 101 | 10 | 100 | | 2022-01-01 | 2 | 101 | 15 | 150 | | 2022-01-01 | 1 | 102 | 8 | 80 | | ... | ... | ... | ... | ... | Tasks 1. **Read the CSV file**: Load the data from `sales_data.csv` into a pandas DataFrame. 2. **Handle missing data**: - Identify and count the number of missing values in each column. - Fill the missing values in the `Sales` column with the mean sales value of that column. 3. **Data transformation**: - Convert the `Date` column to pandas datetime format. - Create a new column named `Year` extracted from the `Date` column. - Create a new column named `Month` (with numerical value) extracted from the `Date` column. 4. **Group and analyze data**: - Group the data by `Year` and `Month` and calculate the total `Revenue` for each month. - Group the data by `Store_ID` and find the store with the highest total `Sales`. 5. **Merging data**: - Suppose you have another DataFrame `product_info` with columns `Product_ID` and `Category`. - Merge the `sales_data` DataFrame with the `product_info` DataFrame on `Product_ID`. Function Signature ```python import pandas as pd def analyze_sales_data(file_path: str, product_info: pd.DataFrame) -> pd.DataFrame: Perform data analysis and manipulation tasks on sales data. Parameters: - file_path: str, the file path to the \'sales_data.csv\'. - product_info: pd.DataFrame, a DataFrame containing product information with columns \'Product_ID\' and \'Category\'. Returns: - merged_df: pd.DataFrame, the final DataFrame after all manipulations. pass ``` Constraints - Ensure your solution handles large datasets efficiently. - Use appropriate pandas functions to achieve the tasks. Expected Output - The `merged_df` DataFrame should contain the merged data with all transformations and groupings applied. Implement the function `analyze_sales_data` following the specified tasks and constraints. This function should demonstrate the use of multiple pandas functionalities to handle real-world data analysis scenarios.","solution":"import pandas as pd def analyze_sales_data(file_path: str, product_info: pd.DataFrame) -> pd.DataFrame: Perform data analysis and manipulation tasks on sales data. Parameters: - file_path: str, the file path to the \'sales_data.csv\'. - product_info: pd.DataFrame, a DataFrame containing product information with columns \'Product_ID\' and \'Category\'. Returns: - merged_df: pd.DataFrame, the final DataFrame after all manipulations. # 1. Read the CSV file sales_data = pd.read_csv(file_path) # 2. Handle missing data missing_values_count = sales_data.isnull().sum() # Fill missing values in \'Sales\' column with the mean sales value sales_mean = sales_data[\'Sales\'].mean() sales_data[\'Sales\'].fillna(sales_mean, inplace=True) # 3. Data transformation # Convert \'Date\' to datetime format sales_data[\'Date\'] = pd.to_datetime(sales_data[\'Date\']) # Create \'Year\' and \'Month\' columns sales_data[\'Year\'] = sales_data[\'Date\'].dt.year sales_data[\'Month\'] = sales_data[\'Date\'].dt.month # 4. Group and analyze data # Group by \'Year\' and \'Month\' and calculate total \'Revenue\' for each month monthly_revenue = sales_data.groupby([\'Year\', \'Month\'])[\'Revenue\'].sum().reset_index() # Group by \'Store_ID\' and find store with highest total \'Sales\' store_sales = sales_data.groupby(\'Store_ID\')[\'Sales\'].sum() top_store = store_sales.idxmax() # 5. Merging data merged_df = pd.merge(sales_data, product_info, on=\'Product_ID\', how=\'left\') return merged_df # Returning the merged DataFrame # The function signature should remain the same as provided in the prompt"},{"question":"**Objective**: Implement a set of Python functions using the `python310` package to demonstrate your understanding of list operations, including creation, item manipulation, slicing, and conversion to tuples. Function Implementations 1. **Create a List**: Implement the function `create_list(length)` that returns a new list of specified length with all items initialized to `None`. ```python def create_list(length: int) -> list: Create a new list of given length. :param length: Length of the list. :return: New list with specified length. # Your implementation here. ``` 2. **Set Item**: Implement the function `set_list_item(lst, index, item)` that sets the item at the specified index in the given list. If the index is out of bounds, the function should raise an `IndexError`. ```python def set_list_item(lst: list, index: int, item) -> None: Set the item at the specified index in the list. :param lst: The list. :param index: The index to set the item at. :param item: The item to set. # Your implementation here. ``` 3. **Get List Slice**: Implement the function `get_list_slice(lst, start, end)` that returns a new list containing the items between the specified `start` and `end` indices of the given list. If the indices are out of bounds, the function should raise an `IndexError`. ```python def get_list_slice(lst: list, start: int, end: int) -> list: Get a new list containing items between start and end indices. :param lst: The list. :param start: Start index. :param end: End index. :return: New list with specified slice. # Your implementation here. ``` 4. **Convert to Tuple**: Implement the function `list_to_tuple(lst)` that converts the given list to a tuple. ```python def list_to_tuple(lst: list) -> tuple: Convert the list to a tuple. :param lst: The list to convert. :return: Tuple containing the list items. # Your implementation here. ``` Constraints - Do not use Python\'s built-in list and tuple functions for these implementations. - Your implementations should utilize the corresponding functions and macros from the `python310` package as described in the documentation. Example Usage ```python # Example usage of create_list my_list = create_list(5) print(my_list) # Output: [None, None, None, None, None] # Example usage of set_list_item set_list_item(my_list, 2, \'hello\') print(my_list) # Output: [None, None, \'hello\', None, None] # Example usage of get_list_slice slice_list = get_list_slice(my_list, 1, 4) print(slice_list) # Output: [None, \'hello\', None] # Example usage of list_to_tuple my_tuple = list_to_tuple(slice_list) print(my_tuple) # Output: (None, \'hello\', None) ```","solution":"def create_list(length: int) -> list: Create a new list of given length. :param length: Length of the list. :return: New list with specified length. return [None] * length def set_list_item(lst: list, index: int, item) -> None: Set the item at the specified index in the list. :param lst: The list. :param index: The index to set the item at. :param item: The item to set. if index < 0 or index >= len(lst): raise IndexError(\\"Index out of bounds\\") lst[index] = item def get_list_slice(lst: list, start: int, end: int) -> list: Get a new list containing items between start and end indices. :param lst: The list. :param start: Start index. :param end: End index. :return: New list with specified slice. if start < 0 or end > len(lst) or start > end: raise IndexError(\\"Index out of bounds\\") return lst[start:end] def list_to_tuple(lst: list) -> tuple: Convert the list to a tuple. :param lst: The list to convert. :return: Tuple containing the list items. return tuple(lst)"},{"question":"# Python Coding Assessment: Numeric Operations **Objective:** To assess your understanding of numeric operations and conversions using Python\'s `PyNumber` family of functions. You will need to implement a function that can handle various numeric operations based on user input. **Task:** Implement a function `numeric_operations(ops, o1, o2=None, o3=None)` that performs the appropriate numeric operation based on the `ops` string parameter. The operations to be supported are: - `\\"add\\"`: Adds `o1` and `o2`. - `\\"subtract\\"`: Subtracts `o2` from `o1`. - `\\"multiply\\"`: Multiplies `o1` and `o2`. - `\\"floor_divide\\"`: Floor divides `o1` by `o2`. - `\\"true_divide\\"`: True divides `o1` by `o2`. - `\\"remainder\\"`: Returns the remainder of dividing `o1` by `o2`. - `\\"power\\"`: Raises `o1` to the power of `o2`; `o3` is optional (pass `None` if not used). - `\\"negate\\"`: Returns the negation of `o1`. - `\\"absolute\\"`: Returns the absolute value of `o1`. - `\\"invert\\"`: Returns the bitwise negation of `o1`. - `\\"convert_to_base\\"`: Converts integer `o1` to a string in the specified base (provide base as `o2`). **Input:** - `ops`: A string representing the operation to be performed. - `o1`: The first operand, can be an integer or float. - `o2` (optional): The second operand for binary operations, can be an integer or float. - `o3` (optional): The third operand for the power operation, can be an integer or float. (Defaults to `None`). **Output:** - The result of the operation, or `None` if the operation fails. **Constraints:** - All numerical operations should handle errors gracefully and return `None` on failure. - `o1`, `o2`, and `o3` (if relevant) must be compatible types for the operation specified by `ops`. **Function Signature:** ```python def numeric_operations(ops: str, o1: float or int, o2: float or int = None, o3: float or int = None) -> float or int or str: # Implementation here ``` **Examples:** ```python # Example 1: # Addition print(numeric_operations(\\"add\\", 10, 20)) # Output: 30 # Example 2: # Negation print(numeric_operations(\\"negate\\", 5)) # Output: -5 # Example 3: # Convert to Base print(numeric_operations(\\"convert_to_base\\", 255, 16)) # Output: \\"0xff\\" # Example 4: # Power with modulo print(numeric_operations(\\"power\\", 2, 3, 3)) # Output: 2 ``` **Notes:** - You are not allowed to use Python\'s built-in operators (like `+`, `-`, etc.) directly, but must use the appropriate `PyNumber` functions or equivalent logic. - You may assume the inputs are valid numbers or `None` where specified. - Handle exceptions and edge cases appropriately within your implementations. **Good Luck!**","solution":"def numeric_operations(ops, o1, o2=None, o3=None): try: if ops == \\"add\\": return o1 + o2 elif ops == \\"subtract\\": return o1 - o2 elif ops == \\"multiply\\": return o1 * o2 elif ops == \\"floor_divide\\": return o1 // o2 elif ops == \\"true_divide\\": return o1 / o2 elif ops == \\"remainder\\": return o1 % o2 elif ops == \\"power\\": if o3 is not None: return pow(o1, o2, o3) return o1 ** o2 elif ops == \\"negate\\": return -o1 elif ops == \\"absolute\\": return abs(o1) elif ops == \\"invert\\": return ~o1 elif ops == \\"convert_to_base\\": if o2 == 2: return bin(o1) elif o2 == 8: return oct(o1) elif o2 == 16: return hex(o1) else: raise ValueError(\\"Unsupported base for conversion\\") else: return None except: return None"},{"question":"Objective: Design and implement a Python function using the `glob` module to find and process specific files in a directory structure. Task: Write a function `find_and_process_files(pattern, root_dir=None, recursive=True)` that searches for files matching a given pattern within a specified root directory and processes these files by reading their contents and returning a list of tuples containing the filename and the first line of each file. Specifications: - The function should use the `glob` module to find files based on the provided pattern. - The `pattern` parameter is a string containing the search pattern (e.g., `\'**/*.txt\'`). - The `root_dir` parameter is the root directory within which to search. It defaults to `None`, meaning the current working directory. - The `recursive` parameter is a boolean that specifies whether the search should be recursive. The default value is `True`. - For each file found, the function should open the file, read the first line, and return a list of tuples where each tuple contains the filename and its first line of text. Constraints: - If no files are found, the function should return an empty list. - Consider handling files with various extensions and within nested directories. - Assume that all files being read are text files and handle potential exceptions that might arise from file operations. Example: Assume the following directory structure: ``` /root_dir â”œâ”€â”€ file1.txt (content: \\"First line of file1\\") â”œâ”€â”€ file2.log (content: \\"First line of file2\\") â””â”€â”€ subdir â””â”€â”€ file3.txt (content: \\"First line of file3\\") ``` Calling `find_and_process_files(\'**/*.txt\', \'/root_dir\')` should return: ```python [ (\'file1.txt\', \'First line of file1\'), (\'subdir/file3.txt\', \'First line of file3\') ] ``` Calling `find_and_process_files(\'**/*.log\', \'/root_dir\')` should return: ```python [ (\'file2.log\', \'First line of file2\') ] ``` Performance Requirements: - The solution should efficiently handle large directory structures. - Consider the time complexity of file searching and list operations. Implementation: ```python import glob import os def find_and_process_files(pattern, root_dir=None, recursive=True): if root_dir is None: root_dir = os.getcwd() file_paths = glob.glob(pattern, root_dir=root_dir, recursive=recursive) result = [] for path in file_paths: try: with open(path, \'r\') as file: first_line = file.readline().strip() relative_path = os.path.relpath(path, root_dir) result.append((relative_path, first_line)) except (IOError, OSError): continue return result ```","solution":"import glob import os def find_and_process_files(pattern, root_dir=None, recursive=True): Searches for files matching the given pattern within the specified root directory, reads the first line of each file, and returns a list of tuples containing the filename and the first line of each file. Parameters: pattern (str): The search pattern (e.g., \'**/*.txt\'). root_dir (str, optional): The root directory to search within. Defaults to None. recursive (bool): Whether to search recursively. Defaults to True. Returns: list: A list of tuples, where each tuple contains the filename and the first line of text. if root_dir is None: root_dir = os.getcwd() file_paths = glob.glob(os.path.join(root_dir, pattern), recursive=recursive) result = [] for path in file_paths: try: with open(path, \'r\') as file: first_line = file.readline().strip() relative_path = os.path.relpath(path, root_dir) result.append((relative_path, first_line)) except (IOError, OSError): continue return result"},{"question":"# Task Implement a function `custom_pretty_print` that takes a Python data structure `data`, an integer `indent`, an integer `width`, an optional integer `depth`, a boolean `compact`, a boolean `sort_dicts`, and a boolean `underscore_numbers`. The function should utilize the `pprint` module to print the data structure with the specified formatting parameters. # Function Signature ```python def custom_pretty_print(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False) -> None: pass ``` # Parameters - `data`: The Python data structure to be pretty-printed (e.g. list, tuple, dictionary). - `indent` (int): Specifies the amount of indentation added for each nesting level. Default is 1. - `width` (int): Specifies the desired maximum number of characters per line in the output. Default is 80. - `depth` (int): Optional parameter to control the number of nesting levels printed. Default is None, meaning no constraint. - `compact` (bool): If True, formats long sequences to as many items as fit within the `width`. Default is False. - `sort_dicts` (bool): If True, sorts dictionary keys before format. Default is True. - `underscore_numbers` (bool): If True, formats integers with underscores as thousands separator. Default is False. # Returns - The function does not return any value. It prints the formatted representation of the data to the standard output. # Constraints - The function should handle any sizes of data structures within reason (limited by typical memory and processing limits of modern machines). # Example ```python data = { \'name\': \'sampleproject\', \'version\': \'1.2.0\', \'keywords\': [\'sample\', \'setuptools\', \'development\'] } custom_pretty_print(data, indent=2, width=50, depth=2, compact=True, sort_dicts=True, underscore_numbers=True) ``` # Expected Output ``` { \'keywords\': [ \'sample\', \'setuptools\', \'development\' ], \'name\': \'sampleproject\', \'version\': \'1.2.0\' } ``` # Notes - Ensure your implementation makes use of the `pprint` module, constructing a `PrettyPrinter` object with the parameters passed to the function. - You may assume that the input data structure will not contain any non-representable objects like files or sockets.","solution":"from pprint import PrettyPrinter def custom_pretty_print(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False): pp = PrettyPrinter(indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts, underscore_numbers=underscore_numbers) pp.pprint(data)"},{"question":"# Question You are required to implement a pipeline that selects important features from a dataset using a combination of different feature selection methods available in `scikit-learn` and then trains a classifier using the selected features. Follow the steps given below to complete the task: Step 1: Data Loading and Preprocessing 1. Load the Iris dataset using `sklearn.datasets.load_iris`. 2. Separate the dataset into features (`X`) and target (`y`). 3. Normalize the features (`X`) using `StandardScaler`. Step 2: Feature Selection 1. Remove all features with low variance using `VarianceThreshold` with a threshold of 0.8. 2. Perform univariate feature selection to retain the top 2 features using the F-test (`SelectKBest` with `f_classif`). 3. Further refine the selection using `tree-based feature selection` (`SelectFromModel` with `ExtraTreesClassifier`). Step 3: Model Training 1. Train a `RandomForestClassifier` on the selected features and evaluate its performance using cross-validation (5-fold). Step 4: Function Implementation Implement the following functions: 1. **`data_preprocessing()`**: - **Input**: None - **Output**: `X` (normalized feature matrix), `y` (target labels) 2. **`feature_selection(X, y)`**: - **Input**: `X` (feature matrix), `y` (target labels) - **Output**: `X_selected` (feature matrix after feature selection) 3. **`train_and_evaluate_model(X, y)`**: - **Input**: `X` (selected feature matrix), `y` (target labels) - **Output**: `None` - **Operation**: Train `RandomForestClassifier` and print the mean cross-validation score Write clear and efficient code that follows the structure given above. Ensure that each function has proper docstring documentation explaining its purpose, inputs, and outputs. Constraints: - Use only the methods and classes described in the given documentation. - Ensure that the entire implementation chain (preprocessing, feature selection, and model training) is seamlessly integrated. ```python from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.model_selection import cross_val_score def data_preprocessing(): Loads the Iris dataset, parses it into features and target labels, and normalizes the features. Returns: - X (array-like): Normalized feature matrix. - y (array-like): Target labels. # Load data pass def feature_selection(X, y): Performs a sequence of feature selection steps including filtering low variance features, univariate feature selection, and tree-based feature selection. Args: - X (array-like): Feature matrix. - y (array-like): Target labels. Returns: - X_selected (array-like): Feature matrix after feature selection. # Remove low variance features pass # Univariate feature selection pass # Tree-based feature selection pass def train_and_evaluate_model(X, y): Trains a RandomForestClassifier on the selected features and evaluates its mean cross-validation score. Args: - X (array-like): Selected feature matrix. - y (array-like): Target labels. Returns: None # Train and evaluate model pass # Example usage: # X, y = data_preprocessing() # X_selected = feature_selection(X, y) # train_and_evaluate_model(X_selected, y) ``` Ensure your implementation is efficient and outputs the required results when executed.","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np def data_preprocessing(): Loads the Iris dataset, parses it into features and target labels, and normalizes the features. Returns: - X (array-like): Normalized feature matrix. - y (array-like): Target labels. # Load the dataset iris = load_iris() X = iris.data y = iris.target # Normalize the features scaler = StandardScaler() X = scaler.fit_transform(X) return X, y def feature_selection(X, y): Performs a sequence of feature selection steps including filtering low variance features, univariate feature selection, and tree-based feature selection. Args: - X (array-like): Feature matrix. - y (array-like): Target labels. Returns: - X_selected (array-like): Feature matrix after feature selection. # Remove low variance features vt = VarianceThreshold(threshold=0.8 * (1 - 0.8)) X = vt.fit_transform(X) # Univariate feature selection skb = SelectKBest(score_func=f_classif, k=2) X = skb.fit_transform(X, y) # Tree-based feature selection etc = ExtraTreesClassifier(n_estimators=50) etc.fit(X, y) sfm = SelectFromModel(etc, prefit=True) X_selected = sfm.transform(X) return X_selected def train_and_evaluate_model(X, y): Trains a RandomForestClassifier on the selected features and evaluates its mean cross-validation score. Args: - X (array-like): Selected feature matrix. - y (array-like): Target labels. Returns: None clf = RandomForestClassifier(n_estimators=100) scores = cross_val_score(clf, X, y, cv=5) print(f\\"Mean cross-validation score: {scores.mean():.4f}\\") # Example usage: # X, y = data_preprocessing() # X_selected = feature_selection(X, y) # train_and_evaluate_model(X_selected, y)"},{"question":"You are required to implement a program that calculates the sum of squares of a list of numbers using parallel processing techniques. You\'ll need to create a function `sum_of_squares` that can leverage both multi-threading and multi-processing. # Function Signature ```python from typing import List import concurrent.futures def sum_of_squares(numbers: List[int], method: str = \'threading\') -> int: pass ``` # Input - `numbers` (List[int]): A list of integers for which the sum of squares needs to be computed. - `method` (str): The parallel processing method to use. It can either be `\'threading\'` or `\'multiprocessing\'`. Default is `\'threading\'`. # Output - Returns an integer which is the sum of the squares of the provided numbers. # Constraints - The list of numbers, `numbers`, can be very large (up to 10 million integers). - To handle large inputs efficiently: - When using threads, create a minimum number of threads (e.g., 4). - When using processes, create a minimum number of processes (e.g., 4). # Performance Requirements - The solution should efficiently utilize CPU resources when the list of numbers is very large. - You must ensure that the main program does not block while waiting for the result of the concurrent executions. # Example ```python numbers = [1, 2, 3, 4, 5] result = sum_of_squares(numbers, method=\'threading\') print(result) # Output: 55 result = sum_of_squares(numbers, method=\'multiprocessing\') print(result) # Output: 55 ``` # Hints - You can use `concurrent.futures.ThreadPoolExecutor` for threading. - You can use `concurrent.futures.ProcessPoolExecutor` for multiprocessing. - Split the list into chunks to distribute the workload evenly among threads or processes. # Additional Information - Be aware of concurrency issues such as race conditions. - Ensure thread-safety and process-safety in your implementation. - Measure execution time for large inputs to verify the efficiency of your solution.","solution":"from typing import List import concurrent.futures def chunkify(numbers: List[int], n: int) -> List[List[int]]: Break the list into n-sized chunks. return [numbers[i:i + n] for i in range(0, len(numbers), n)] def square_numbers(nums: List[int]) -> int: Return the sum of squares of the given list of numbers. return sum(x * x for x in nums) def sum_of_squares(numbers: List[int], method: str = \'threading\') -> int: if method not in [\'threading\', \'multiprocessing\']: raise ValueError(\\"Method must be either \'threading\' or \'multiprocessing\'\\") num_cores = 4 chunk_size = len(numbers) // num_cores chunks = chunkify(numbers, chunk_size) if method == \'threading\': with concurrent.futures.ThreadPoolExecutor() as executor: futures = [executor.submit(square_numbers, chunk) for chunk in chunks] return sum(f.result() for f in concurrent.futures.as_completed(futures)) elif method == \'multiprocessing\': with concurrent.futures.ProcessPoolExecutor() as executor: futures = [executor.submit(square_numbers, chunk) for chunk in chunks] return sum(f.result() for f in concurrent.futures.as_completed(futures))"},{"question":"**Question: Implement a pattern matcher with special constraints** **Objective:** Implement a function `custom_matcher` that utilizes Python\'s regular expressions to process input strings based on the provided constraints. **Function Signature:** ```python def custom_matcher(pattern: str, string: str, flags: int = 0) -> list: Custom matcher using Python\'s regular expressions. Parameters: - pattern (str): The regular expression pattern to compile. - string (str): The input string on which the pattern is to be matched. - flags (int): Optional compilation flags. Returns: - list: A list containing all matches based on the pattern. ``` **Description:** 1. Your function should compile the given regular expression pattern using `re.compile()`, with the provided flags, if any. 2. It should then search for all occurrences of the pattern in the provided string using the appropriate regular expression method. 3. Ensure that special sequences such as `\'d\'`, `\'w\'`, and `\'s\'` are correctly interpreted based on the provided compilation flags and within the provided string. 4. The function should return a list of all full matches found in the input string. **Constraints:** 1. We assume `pattern` will always be a valid regular expression. 2. Optimize the function to handle large input strings efficiently. 3. Use greedy or non-greedy matches appropriately based on the context provided. **Example Usage:** ```python # Example 1 pattern = r\'bw+b\' string = \'Regular expressions are powerful tools for string processing.\' print(custom_matcher(pattern, string)) # Output: [\'Regular\', \'expressions\', \'are\', \'powerful\', \'tools\', \'for\', \'string\', \'processing\'] # Example 2 pattern = r\'d+\' string = \'Here are some numbers: 123, 456, and 7890.\' print(custom_matcher(pattern, string)) # Output: [\'123\', \'456\', \'7890\'] # Example 3 pattern = r\'[aeiou]\' string = \'Python is awesome!\' print(custom_matcher(pattern, string, re.IGNORECASE)) # Output: [\'o\', \'i\', \'a\', \'e\', \'o\', \'e\'] ``` **Notes:** - You may use additional helper functions if needed. - Be sure to include edge cases, such as when the pattern matches zero instances, when the string is empty, or when complex patterns are involved.","solution":"import re def custom_matcher(pattern: str, string: str, flags: int = 0) -> list: Custom matcher using Python\'s regular expressions. Parameters: - pattern (str): The regular expression pattern to compile. - string (str): The input string on which the pattern is to be matched. - flags (int): Optional compilation flags. Returns: - list: A list containing all matches based on the pattern. compiled_pattern = re.compile(pattern, flags) matches = compiled_pattern.findall(string) return matches"},{"question":"# Python Coding Assessment Question Objective: Implement a utility to fetch and analyze IP address information from a web service using multiple Python modules, demonstrating proficiency in URL handling, HTTP requests, and IP address manipulation. Problem Statement: Write a function called `fetch_and_analyze_ip(ip_address: str) -> dict` that takes an IP address as a string, performs the following actions, and returns a dictionary with the results. Steps: 1. **Validate the IP Address**: - Use the `ipaddress` module to validate if the provided IP address is either IPv4 or IPv6. 2. **Fetch IP Information**: - Use the `urllib` module to send an HTTP GET request to the URL `http://ip-api.com/json/{ip_address}`. This service returns information about the IP address in JSON format. - Parse the JSON response to extract the required fields: `query` (the IP address itself), `country`, `regionName`, `city`, `zip`, `lat`, `lon`, `isp`, `org`. 3. **Process and Return Results**: - Return a dictionary with the structure: ```python { \'ip_address\': \'xxx.xxx.xxx.xxx\', \'country\': \'Country Name\', \'region\': \'Region Name\', \'city\': \'City Name\', \'postal_code\': \'ZIP Code\', \'latitude\': Latitude, \'longitude\': Longitude, \'isp\': \'ISP Name\', \'organization\': \'Organization Name\' } ``` Constraints: - If the IP address is invalid, raise a `ValueError` with the message \\"Invalid IP address\\". - Perform error handling for any failures during the HTTP request and raise a `ConnectionError` with an appropriate message. - Use only the standard Python 3.10 library for `urllib` and `ipaddress`. Example: ```python result = fetch_and_analyze_ip(\\"8.8.8.8\\") print(result) # Output should be similar to: # { # \'ip_address\': \'8.8.8.8\', # \'country\': \'United States\', # \'region\': \'California\', # \'city\': \'Mountain View\', # \'postal_code\': \'94035\', # \'latitude\': 37.4056, # \'longitude\': -122.0775, # \'isp\': \'Google LLC\', # \'organization\': \'Google LLC\' # } ``` Notes: - Ensure your function handles both IPv4 and IPv6 addresses. - You must handle network errors gracefully and ensure that all exceptions raised carry meaningful messages to help with debugging.","solution":"import ipaddress import urllib.request import json def fetch_and_analyze_ip(ip_address: str) -> dict: # Validate the IP Address try: ip = ipaddress.ip_address(ip_address) except ValueError: raise ValueError(\\"Invalid IP address\\") # Fetch IP Information url = f\\"http://ip-api.com/json/{ip_address}\\" try: with urllib.request.urlopen(url) as response: data = json.loads(response.read().decode()) except Exception as e: raise ConnectionError(f\\"Failed to fetch IP information: {e}\\") # Extract required fields from the JSON response result = { \'ip_address\': data.get(\'query\'), \'country\': data.get(\'country\'), \'region\': data.get(\'regionName\'), \'city\': data.get(\'city\'), \'postal_code\': data.get(\'zip\'), \'latitude\': data.get(\'lat\'), \'longitude\': data.get(\'lon\'), \'isp\': data.get(\'isp\'), \'organization\': data.get(\'org\') } return result"},{"question":"**Coding Assessment Question:** # Problem Statement You are tasked with implementing two functions that will create and manipulate bytearray objects in Python. The two functions are described as follows: 1. **Function 1:** `create_bytearray_from_string(string: str) -> bytearray` - **Input:** A single string `string` that contains only ASCII characters. - **Output:** A bytearray object created from the input string, with an additional check to ensure it has been correctly converted. 2. **Function 2:** `manipulate_bytearrays(ba1: bytearray, ba2: bytearray) -> tuple` - **Input:** Two bytearray objects `ba1` and `ba2`. - **Output:** A tuple with two elements: - The first element should be a new bytearray created by concatenating `ba1` and `ba2`. - The second element should be a resized version of this new bytearray, truncated to the size of the original `ba1`. # Constraints - The input to the functions should be handled correctly and safely. - The string provided as input to the first function will contain only ASCII characters (no Unicode). - The operations should maintain the integrity of the data and ensure no data loss or corruption. - The resizing in the second function should handle cases where the size is larger or smaller than the original size. # Example ```python def create_bytearray_from_string(string: str) -> bytearray: # Your implementation here def manipulate_bytearrays(ba1: bytearray, ba2: bytearray) -> tuple: # Your implementation here # Example usage: ba1 = create_bytearray_from_string(\\"hello\\") ba2 = create_bytearray_from_string(\\"world\\") concatenated, resized = manipulate_bytearrays(ba1, ba2) # `concatenated` should be bytearray(b\'helloworld\') # `resized` should be bytearray(b\'hello\') ``` # Implementation Notes 1. **For `create_bytearray_from_string`:** - Use `PyByteArray_FromStringAndSize` to create the bytearray from the given string. 2. **For `manipulate_bytearrays`:** - Use `PyByteArray_Concat` to concatenate the two input bytearrays. - Use `PyByteArray_Resize` to resize the concatenated bytearray to the original size of `ba1`. - Return the concatenated and resized bytearrays as a tuple. You need to ensure the solution adheres to the description and handles edge cases gracefully. Good luck!","solution":"def create_bytearray_from_string(string: str) -> bytearray: Create a bytearray object from the input ASCII string. if not all(ord(char) < 128 for char in string): raise ValueError(\\"Input string contains non-ASCII characters.\\") return bytearray(string, \'ascii\') def manipulate_bytearrays(ba1: bytearray, ba2: bytearray) -> tuple: Concatenate two bytearrays and resize the result to the size of the first bytearray. Parameters: - ba1: First bytearray. - ba2: Second bytearray. Returns: A tuple (concatenated_bytearray, resized_bytearray) concatenated = ba1 + ba2 resized = concatenated[:len(ba1)] return concatenated, resized"},{"question":"You are given the `penguins` dataset, which contains data about different species of penguins. Your task is to create a seaborn plot using the `seaborn.objects` module that fulfills the following requirements: 1. Create a plot that shows the relationship between `body_mass_g` (body mass) and `flipper_length_mm` (flipper length). 2. Use different colors for different `species`. 3. Overlay a dashed line segment for each penguin datapoint, centered on `body_mass_g` and `flipper_length_mm`. 4. Map the `alpha` property to be 0.5 and the `linewidth` property to be proportional to `flipper_length_mm`. 5. Adjust the `width` of the dashes to be 0.4 of the spacing between values. 6. Show aggregate values by combining the `Dash`, `Agg`, and `Dodge` elements. 7. Add `Dots` to represent individual data points with slight jitter to avoid overlap. Here is a template to get you started: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\", color=\\"species\\") # Add a dashed line segment for each datapoint p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") # Set the width of the dashes p.add(so.Dash(width=0.4)) # Show aggregate values combined with dashes p.add(so.Dash(), so.Agg(), so.Dodge()) # Add dots for individual data points with jitter p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot p.show() ``` Ensure your implementation meets all specified requirements and produces a visually informative plot. **Input:** - You don\'t need to provide any input, just use the `penguins` dataset from seaborn. **Output:** - A seaborn plot displayed with the given configurations. **Constraints:** - Make sure the plot renders without any errors. - Ensure that the properties `alpha`, `linewidth`, and `width` are correctly mapped and set as specified.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\", color=\\"species\\") # Add a dashed line segment for each datapoint p.add(so.Dash(alpha=0.5, linewidth=\\"flipper_length_mm\\", width=0.4)) # Show aggregate values combined with dashes p.add(so.Dash(), so.Agg(), so.Dodge()) # Add dots for individual data points with jitter p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot p.show() # Call the function to create and display the plot create_penguin_plot()"},{"question":"# Email Address Manipulation and Parsing **Objective**: Implement a set of functions to manipulate and parse email addresses using the `email.utils` module. This assessment will test your ability to handle common email-related operations in Python. Part 1: Parse Email Address Write a function `parse_email` that takes an email address string as input and returns a tuple containing the real name and email address parts. If the parsing fails, the function should return an empty tuple (`(\'\', \'\')`). **Function Signature**: ```python def parse_email(address: str, strict: bool = True) -> tuple: pass ``` **Input**: - `address` (str): A string representing the email address to be parsed. - `strict` (bool): Optional; default is `True`. If `True`, uses a strict parser to reject malformed inputs. **Output**: - A tuple (str, str) containing the real name and email address. **Examples**: ```python print(parse_email(\\"John Doe <john@example.com>\\")) # Output: (\'John Doe\', \'john@example.com\') print(parse_email(\\"invalid email\\")) # Output: (\'\', \'\') ``` Part 2: Format Email Address Write a function `format_email` that takes a tuple containing a real name and an email address, and returns a string formatted as per RFC standards suitable for use in headers like `To` or `Cc`. **Function Signature**: ```python def format_email(address_tuple: tuple, charset: str = \'utf-8\') -> str: pass ``` **Input**: - `address_tuple` (tuple): A tuple containing the real name and email address. - `charset` (str): Optional; default is `\'utf-8\'`. The character set used for encoding the real name if it contains non-ASCII characters. **Output**: - A string containing the formatted email address. **Examples**: ```python print(format_email((\'John Doe\', \'john@example.com\'))) # Output: \'John Doe <john@example.com>\' print(format_email((\'\', \'john@example.com\'))) # Output: \'john@example.com\' ``` Part 3: Extract Multiple Addresses Write a function `extract_addresses` that takes a list of address-containing strings (like `To`, `Cc` headers), and returns a list of tuples with the parsed addresses. **Function Signature**: ```python def extract_addresses(headers: list, strict: bool = True) -> list: pass ``` **Input**: - `headers` (list): A list of strings, each containing addresses. - `strict` (bool): Optional; default is `True`. If `True`, uses a strict parser to reject malformed inputs. **Output**: - A list of tuples, each containing the real name and email address. **Examples**: ```python headers = [\\"John Doe <john@example.com>, Jane Doe <jane@example.com>\\", \\"Cc: Jake Doe <jake@example.com>\\"] print(extract_addresses(headers)) # Output: [(\'John Doe\', \'john@example.com\'), (\'Jane Doe\', \'jane@example.com\'), (\'Jake Doe\', \'jake@example.com\')] ``` # Notes: - Make sure to handle edge cases, including empty strings and malformed inputs. - Use appropriate functions from the `email.utils` module to ensure your implementations conform to RFC standards.","solution":"from email.utils import parseaddr, formataddr, getaddresses def parse_email(address: str, strict: bool = True) -> tuple: real_name, email_address = parseaddr(address) if strict and \'@\' not in email_address: return (\'\', \'\') return (real_name, email_address) def format_email(address_tuple: tuple, charset: str = \'utf-8\') -> str: return formataddr(address_tuple) def extract_addresses(headers: list, strict: bool = True) -> list: all_addresses = [] for header in headers: addresses = getaddresses([header]) parsed_addresses = [parse_email(f\\"{name} <{email}>\\", strict) for name, email in addresses] all_addresses.extend(parsed_addresses) return all_addresses"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `JointGrid` class by creating and customizing a joint plot. **Task:** Write a Python function `create_custom_jointplot(dataset)` that: 1. Loads the specified dataset using seaborn. 2. Initializes a `JointGrid` with `x` and `y` variables. 3. Customizes the joint plot with specific visualization and styling requirements. 4. Returns the `JointGrid` object. **Specifications:** 1. **Input:** - `dataset` (str): The name of a seaborn dataset to load. For example: `\\"penguins\\"`, `\\"iris\\"`, etc. - Note: You can assume the dataset will always contain at least two numeric columns to be used for the `x` and `y` variables. 2. **Output:** - `g` (`seaborn.axisgrid.JointGrid`): The initialized and customized JointGrid object. 3. **Requirements:** - Use columns `\\"bill_length_mm\\"` and `\\"bill_depth_mm\\"` for the `x` and `y` variables respectively (you can assume these columns exist in the dataset). - Plot the joint plot with a scatter plot showing the relationship between `x` and `y`. - Configure marginal plots using histograms with kernel density estimation. - Add reference lines at `x = 45` and `y = 16`. - Set the overall size of the figure to `height = 6`. - Set marginal plots to display ticks. - Set limits for the X and Y axes to `xlim = (30, 60)` and `ylim = (10, 25)`. 4. **Constraints:** - The function must handle loading and processing of the dataset within the function body. - Use seaborn version 0.11.0 or later. **Function Signature:** ```python def create_custom_jointplot(dataset: str) -> sns.axisgrid.JointGrid: pass ``` **Example Usage:** ```python g = create_custom_jointplot(\\"penguins\\") # Expected output: A customized JointGrid object with the specified configurations. ``` # Notes: - Ensure you have seaborn and any other necessary libraries installed before running your code. - Be mindful of the handling of the dataset and plotting configurations. - The function should not display the plot; it should only return the configured JointGrid object.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_jointplot(dataset: str) -> sns.axisgrid.JointGrid: Creates and customizes a JointGrid plot based on the provided dataset. Args: - dataset (str): The name of the seaborn dataset to load. Returns: - sns.axisgrid.JointGrid: The customized JointGrid object. # Load the dataset data = sns.load_dataset(dataset) # Initialize the JointGrid g = sns.JointGrid(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", data=data, height=6, xlim=(30, 60), ylim=(10, 25)) # Plot the scatter plot in the joint area g = g.plot(sns.scatterplot, sns.histplot) # Add reference lines g.refline(x=45, y=16) # Configure marginal plots g.plot_marginals(sns.histplot, kde=True) # Customize marginal plots to show ticks for ax in [g.ax_marg_x, g.ax_marg_y]: ax.tick_params(axis=\'both\', which=\'both\') return g"},{"question":"**Objective:** You are required to implement a Telnet client that connects to a given Telnet server, sends a series of commands, and processes the responses. **Instructions:** 1. **Connecting to the Server:** - Implement a function `connect_to_server(host: str, port: int, timeout: int) -> Telnet`, which connects to the given host and port using the `telnetlib.Telnet` class. Ensure the connection uses the specified timeout. 2. **Executing Commands:** - Implement a function `execute_commands(telnet_conn: Telnet, commands: List[str], expect_list: List[bytes]) -> List[Tuple[int, any, bytes]]`, which: - Takes an established telnet connection (`telnet_conn`) and a list of commands (`commands`). - Sends each command to the server. - Uses the `expect` method with `expect_list` after each command to match the responses. - Returns a list of tuples containing: - Index of the matched regular expression - The match object - The response bytes read up till and including the match 3. **Closing the Connection:** - Implement a function `close_connection(telnet_conn: Telnet) -> None`, which properly closes the Telnet connection. **Example Usage:** ```python import telnetlib import re from typing import List, Tuple # Function to connect to the server def connect_to_server(host: str, port: int, timeout: int) -> telnetlib.Telnet: # Implementation here pass # Function to execute commands def execute_commands(telnet_conn: telnetlib.Telnet, commands: List[str], expect_list: List[bytes]) -> List[Tuple[int, any, bytes]]: # Implementation here pass # Function to close the connection def close_connection(telnet_conn: telnetlib.Telnet) -> None: # Implementation here pass # Example usage if __name__ == \\"__main__\\": host = \\"example.com\\" port = 23 timeout = 10 commands = [\\"login\\", \\"password\\", \\"ls\\", \\"exit\\"] expect_list = [b\\"login:\\", b\\"Password:\\", re.compile(b\\"[#>] \\"), b\\"exit\\"] try: telnet_client = connect_to_server(host, port, timeout) results = execute_commands(telnet_client, commands, expect_list) for res in results: print(f\\"Matched: {res[0]}, Match Object: {res[1]}, Data: {res[2].decode(\'ascii\')}\\") finally: close_connection(telnet_client) ``` **Criteria:** - Your solution should handle potential errors gracefully (e.g., connection failures, read/write errors). - Ensure proper use of Telnet class methods to manage reading and writing. - Demonstrate knowledge of context managers and exception handling in Python. **Constraints:** - You may assume the server is always available for connections during the test. - Ensure that the Telnet connection is always properly closed, even if errors occur. Good luck!","solution":"import telnetlib import re from typing import List, Tuple def connect_to_server(host: str, port: int, timeout: int) -> telnetlib.Telnet: Connect to a telnet server given the host, port, and timeout. :param host: The hostname or IP address of the telnet server. :param port: The port number of the telnet server. :param timeout: The timeout in seconds for connection attempts. :return: An established Telnet connection. try: telnet_conn = telnetlib.Telnet(host, port, timeout) return telnet_conn except Exception as e: print(f\\"Failed to connect: {e}\\") raise def execute_commands(telnet_conn: telnetlib.Telnet, commands: List[str], expect_list: List[bytes]) -> List[Tuple[int, any, bytes]]: Execute a series of commands on the telnet connection and match responses. :param telnet_conn: An established Telnet connection. :param commands: A list of commands to send to the server. :param expect_list: A list of bytes or compiled regular expressions to match the responses. :return: A list of tuples containing the index of the matched regular expression, the match object, and the response bytes. output = [] for command in commands: telnet_conn.write(command.encode(\'ascii\') + b\'n\') index, match_object, response = telnet_conn.expect(expect_list) output.append((index, match_object, response)) return output def close_connection(telnet_conn: telnetlib.Telnet) -> None: Close the telnet connection. :param telnet_conn: An established Telnet connection. try: telnet_conn.close() except Exception as e: print(f\\"Failed to close connection: {e}\\") raise"},{"question":"Objective Your task is to implement a function that processes a log of transactions and provides insights. You will be working with date and time data and specialized data types from the `collections` module. Problem Statement You are given a log of transactions, where each transaction is represented by a string in the following format: `\\"YYYY-MM-DD hh:mm:ss TRANS_TYPE AMOUNT\\"` - `YYYY-MM-DD` represents the date. - `hh:mm:ss` represents the time of the transaction. - `TRANS_TYPE` is a string that could either be `DEPOSIT` or `WITHDRAWAL`. - `AMOUNT` is a positive integer representing the amount of money deposited or withdrawn. You need to implement a function `process_transactions(log: List[str]) -> Dict[str, Any]` that takes in a list of such strings (the transaction log) and returns a dictionary with the following key-value pairs: 1. `\\"earliest_transaction\\"`: The earliest transaction (as a string in the original format). 2. `\\"latest_transaction\\"`: The latest transaction (as a string in the original format). 3. `\\"total_deposited\\"`: The total amount of money deposited. 4. `\\"total_withdrawn\\"`: The total amount of money withdrawn. Constraints - The `log` list contains between `1` and `10^5` transactions. - The date and time values are valid and in ascending order. Example ```python log = [ \\"2022-01-01 08:00:00 DEPOSIT 100\\", \\"2022-01-01 09:00:00 WITHDRAWAL 50\\", \\"2022-01-01 10:00:00 DEPOSIT 150\\" ] result = process_transactions(log) print(result) ``` Expected Output: ```python { \'earliest_transaction\': \'2022-01-01 08:00:00 DEPOSIT 100\', \'latest_transaction\': \'2022-01-01 10:00:00 DEPOSIT 150\', \'total_deposited\': 250, \'total_withdrawn\': 50 } ``` Requirements - You must use the `collections` module appropriately, specifically `Counter` and `deque`. - Handle date and time operations using `datetime` or related classes. - Ensure that your function runs efficiently within the given constraints.","solution":"from typing import List, Dict, Any from collections import Counter, deque from datetime import datetime def process_transactions(log: List[str]) -> Dict[str, Any]: if not log: return { \\"earliest_transaction\\": \\"\\", \\"latest_transaction\\": \\"\\", \\"total_deposited\\": 0, \\"total_withdrawn\\": 0 } earliest_transaction = log[0] latest_transaction = log[-1] total_deposited = 0 total_withdrawn = 0 for transaction in log: parts = transaction.split() trans_type = parts[2] amount = int(parts[3]) if trans_type == \\"DEPOSIT\\": total_deposited += amount elif trans_type == \\"WITHDRAWAL\\": total_withdrawn += amount return { \\"earliest_transaction\\": earliest_transaction, \\"latest_transaction\\": latest_transaction, \\"total_deposited\\": total_deposited, \\"total_withdrawn\\": total_withdrawn }"},{"question":"**Coding Assessment Question** **Objective:** Implement a function in Python that utilizes the `fcntl` module to perform file locking and I/O control operations. **Problem Statement:** You are required to implement a function `lock_and_read_file(fd: int, operation: int, lock_len: int, read_len: int) -> bytes` that: 1. Locks a specified segment of a file based on provided parameters. 2. Reads a specified number of bytes from the locked portion of the file. **Function Signature:** ```python def lock_and_read_file(fd: int, operation: int, lock_len: int, read_len: int) -> bytes: pass ``` **Parameters:** - `fd (int)`: The file descriptor of the file to be manipulated. - `operation (int)`: The locking operation to be performed (`fcntl.LOCK_SH` for shared lock or `fcntl.LOCK_EX` for exclusive lock). You may also bitwise OR with `fcntl.LOCK_NB`. - `lock_len (int)`: The number of bytes to lock. - `read_len (int)`: The number of bytes to read from the locked section of the file. **Constraints:** - The function should handle exceptions and ensure the file is properly unlocked after operations. - If the lock cannot be acquired immediately (when using `LOCK_NB`), your function should handle this gracefully by retrying a few times before failing. **Instructions:** 1. Use `fcntl.lockf` to lock the specified portion of the file. 2. After acquiring the lock, read `read_len` bytes from the file. 3. Ensure the lock is released before returning the content read. 4. Raise appropriate exceptions if any file control operations fail. **Example:** ```python import os # Example usage (Test this with an appropriate file descriptor) fd = os.open(\\"example.txt\\", os.O_RDWR) try: content = lock_and_read_file(fd, fcntl.LOCK_EX, 100, 50) print(content) except Exception as e: print(\\"Error:\\", e) finally: os.close(fd) ``` *Note: Ensure proper exception handling for various edge cases like file not found, permission issues, etc.* **Performance Considerations:** - Your implementation should avoid busy-waiting when creating retries for acquiring a lock. - Efficiently handle the I/O operations to minimize overhead. This task will assess the student\'s understanding of: - File locking mechanisms and I/O control. - Exception handling and resource management. - Utilizing the `fcntl` module\'s functions effectively.","solution":"import fcntl import os import errno import time def lock_and_read_file(fd: int, operation: int, lock_len: int, read_len: int) -> bytes: Locks a specified segment of a file and reads a specified number of bytes from the locked portion. Parameters: - fd (int): The file descriptor of the file to be manipulated. - operation (int): The locking operation to be performed. - lock_len (int): The number of bytes to lock. - read_len (int): The number of bytes to read from the locked section of the file. Returns: - bytes: The content read from the locked section of the file. retries = 5 delay = 0.1 for _ in range(retries): try: # Lock the specified portion of the file fcntl.lockf(fd, operation, lock_len) break except IOError as e: # If the error is EAGAIN or EACCES, wait and retry if e.errno in [errno.EAGAIN, errno.EACCES]: time.sleep(delay) else: raise else: raise TimeoutError(f\\"Could not acquire lock after {retries} attempts\\") try: content = os.read(fd, read_len) finally: # Unlock the file fcntl.lockf(fd, fcntl.LOCK_UN, lock_len) return content"},{"question":"**Question: Implementing and Evaluating an SGD Classifier with Hyperparameter Tuning** In this exercise, you will implement a machine learning workflow using Scikit-learn\'s `SGDClassifier`. You will also perform hyperparameter tuning to improve the model\'s performance. The dataset for this task will be the Iris dataset, which you can load using Scikit-learn. # Requirements: 1. **Data Preprocessing**: - Load the Iris dataset. - Split the dataset into training and test sets (80-20 split). - Standardize the features using `StandardScaler`. 2. **Model Training and Evaluation**: - Initialize an `SGDClassifier` with the `log_loss` function for logistic regression. - Perform hyperparameter tuning using `GridSearchCV` to find the best values for `alpha` (regularization term) and `max_iter` (number of iterations). - Evaluate the model\'s performance using accuracy on the test set. 3. **Implementation Details**: - Use `accuracy_score` from `sklearn.metrics` to evaluate model performance. - Set random_state to 42 wherever applicable for reproducibility. # Input Format: You do not need to provide any specific input format since you will implement the code within a Python function. # Output: Print the following: - Best hyperparameters found by `GridSearchCV`. - Accuracy of the model on the test set. # Constraints: - Use only Scikit-learn and Numpy libraries. - Ensure that the features are scaled before fitting the model. # Code Template: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def main(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize SGDClassifier sgd_clf = SGDClassifier(loss=\\"log_loss\\", random_state=42) # Hyperparameter tuning using GridSearchCV param_grid = { \'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1.0], \'max_iter\': [1000, 2000, 3000] } grid_search = GridSearchCV(estimator=sgd_clf, param_grid=param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) # Best hyperparameters print(f\\"Best Hyperparameters: {grid_search.best_params_}\\") # Evaluate the model\'s performance using accuracy on the test set best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy on the test set: {accuracy}\\") if __name__ == \\"__main__\\": main() ``` # Additional Information: - You can refer to the Scikit-learn documentation for further details on any of the functions and methods used. - Ensure that your implementation adheres to best practices in data preprocessing and model evaluation.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def main(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize SGDClassifier sgd_clf = SGDClassifier(loss=\\"log_loss\\", random_state=42) # Hyperparameter tuning using GridSearchCV param_grid = { \'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1.0], \'max_iter\': [1000, 2000, 3000] } grid_search = GridSearchCV(estimator=sgd_clf, param_grid=param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) # Best hyperparameters best_params = grid_search.best_params_ print(f\\"Best Hyperparameters: {best_params}\\") # Evaluate the model\'s performance using accuracy on the test set best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy on the test set: {accuracy}\\") return best_params, accuracy if __name__ == \\"__main__\\": main()"},{"question":"**Question: Implement a Command Execution Utility Function** Implement a Python function named `execute_command` that: 1. Accepts a list of strings representing the command and arguments to be executed. 2. Runs the specified command using the `subprocess` module. 3. Captures standard output and standard error. 4. Supports an optional timeout parameter to limit the execution time of the subprocess. 5. Returns a dictionary with the following keys: - `returncode`: The return code of the subprocess. - `stdout`: The captured standard output as a string. - `stderr`: The captured standard error as a string. - `timed_out`: A boolean indicating if the process was terminated due to a timeout. If the command execution exceeds the specified timeout, the function should terminate the process and return the captured outputs up to that point. **Function Signature:** ```python def execute_command(command: list, timeout: float = None) -> dict: pass ``` **Constraints:** - The `command` parameter must be a list of strings where the first item is the executable name and the rest are the arguments. - The `timeout` parameter is optional and defaults to `None` if not specified. **Example:** ```python result = execute_command([\\"ls\\", \\"-l\\"], timeout=5) print(result) # Output might be: # { # \'returncode\': 0, # \'stdout\': \'total 0ndrwxr-xr-x ...\', # \'stderr\': \'\', # \'timed_out\': False # } result = execute_command([\\"sleep\\", \\"10\\"], timeout=2) print(result) # Output might be: # { # \'returncode\': -1, # \'stdout\': \'\', # \'stderr\': \'\', # \'timed_out\': True # } ``` **Notes:** - Use `subprocess.run` for process execution. - If a timeout occurs, handle the `TimeoutExpired` exception to terminate the process.","solution":"import subprocess def execute_command(command: list, timeout: float = None) -> dict: try: result = subprocess.run( command, capture_output=True, text=True, timeout=timeout ) return { \'returncode\': result.returncode, \'stdout\': result.stdout, \'stderr\': result.stderr, \'timed_out\': False } except subprocess.TimeoutExpired as e: return { \'returncode\': -1, \'stdout\': e.stdout.decode() if e.stdout else \'\', \'stderr\': e.stderr.decode() if e.stderr else \'\', \'timed_out\': True }"},{"question":"# Problem: Multi-threaded Data Processing In this assessment, you are required to write a Python program that uses the `_thread` module to process a list of integers in parallel. The goal is to split the list into several chunks, process each chunk in a separate thread, and then combine the results safely using thread synchronization mechanisms. Task 1. **Function:** `process_chunk(chunk) -> int` - **Input:** A list of integers `chunk`. - **Output:** The function should return the sum of the integers in the provided `chunk`. 2. **Function:** `parallel_sum(lst: List[int], n_threads: int) -> int` - **Input:** - A list of integers `lst`. - An integer `n_threads` representing the number of threads to use. - **Output:** The function should return the sum of all integers in the list `lst` after processing `n_threads` chunks in parallel. - **Constraints:** - Ensure that `n_threads` is at least 1 and at most the length of `lst`. - Use the `_thread` module\'s primitives for thread creation, synchronization, and management. - **Performance Requirements:** The program should efficiently utilize the threads to minimize the total processing time. Handling of race conditions and proper synchronization is crucial. Implementation Details 1. **Splitting the List:** Divide the list `lst` into `n_threads` equal parts. If `lst` does not divide evenly, the last chunk may be smaller. 2. **Thread Synchronization:** Use `_thread.allocate_lock()` to create a lock object for synchronizing access to shared variables. 3. **Thread Creation and Management:** Utilize `_thread.start_new_thread` to create threads for processing chunks and ensure that all threads complete before returning the final result. 4. **Thread Function:** Define a thread function that calls `process_chunk`, updates a shared variable for storing the result, and uses the lock for synchronization. 5. **Main Thread:** The main thread should wait for all spawned threads to complete before computing and returning the final sum. Example ```python from typing import List import _thread import time # Function to process a chunk def process_chunk(chunk: List[int]) -> int: return sum(chunk) # Function to process list in parallel def parallel_sum(lst: List[int], n_threads: int) -> int: # Validate number of threads assert 1 <= n_threads <= len(lst), \\"Number of threads must be between 1 and the length of the list\\" # Split list into chunks chunk_size = len(lst) // n_threads chunks = [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)] # Ensure correct number of chunks if len(chunks) > n_threads: chunks[-2].extend(chunks[-1]) chunks = chunks[:-1] # Shared result variable and lock result = 0 result_lock = _thread.allocate_lock() # Thread function def thread_function(chunk): nonlocal result chunk_sum = process_chunk(chunk) with result_lock: result += chunk_sum # Start threads for chunk in chunks: _thread.start_new_thread(thread_function, (chunk,)) # Allow time for threads to complete - usually not the best practice for real-time systems. time.sleep(1) return result # Example Usage lst = [i for i in range(100)] n_threads = 4 print(parallel_sum(lst, n_threads)) # Output should be 4950 ``` In your implementation, make sure to handle the division of work correctly and ensure that the threading operations are properly synchronized to avoid race conditions.","solution":"from typing import List import _thread import time # Function to process a chunk def process_chunk(chunk: List[int]) -> int: return sum(chunk) # Function to process list in parallel def parallel_sum(lst: List[int], n_threads: int) -> int: # Validate number of threads assert 1 <= n_threads <= len(lst), \\"Number of threads must be between 1 and the length of the list\\" # Split list into chunks chunk_size = len(lst) // n_threads chunks = [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)] # Ensure correct number of chunks if len(chunks) > n_threads: chunks[-2].extend(chunks[-1]) chunks = chunks[:-1] # Shared result variable and lock result = 0 result_lock = _thread.allocate_lock() # Thread function def thread_function(chunk): nonlocal result chunk_sum = process_chunk(chunk) with result_lock: result += chunk_sum # Start threads for chunk in chunks: _thread.start_new_thread(thread_function, (chunk,)) # Allow time for threads to complete - usually not the best practice for real-time systems. time.sleep(1) return result"},{"question":"**Coding Question: Unix Group Database Analyzer** You are to implement a function that analyzes Unix group database entries. The goal is to write a function that can retrieve information about groups and compute specific statistics based on the group members\' data. # Function Signature ```python import grp def analyze_unix_groups(stat: str) -> dict: Analyze Unix group database entries and return the desired statistics. :param stat: A string specifying the desired statistic. Can be: - \'member_count\': Returns the count of members for each group - \'gid_count\': Returns the count of unique GIDs in the database - \'group_names\': Returns a list of all group names in the database, sorted alphabetically :return: A dictionary containing the requested statistic. :raises ValueError: If the `stat` parameter is not one of the specified values. pass ``` # Input - `stat` (str): The type of statistic to compute. It can be one of the following: - `\'member_count\'`: Return the count of members for each group. - `\'gid_count\'`: Return the count of unique GIDs in the database. - `\'group_names\'`: Return a list of all group names in the database, sorted alphabetically. # Output - If `stat` is `\'member_count\'`: Return a dictionary where each key is a group name and the value is the number of members in that group. - If `stat` is `\'gid_count\'`: Return a dictionary with a single key `\'unique_gid_count\'` and the value is the number of unique group IDs. - If `stat` is `\'group_names\'`: Return a dictionary with a single key `\'group_names_sorted\'` and the value is a list of all group names sorted alphabetically. # Constraints - You should handle edge cases, such as an empty group database. - Ensure that your implementation works efficiently for a large number of group entries. # Example Usage ```python # Example calls to analyze_unix_groups function # Example: Count of members for each group result = analyze_unix_groups(\\"member_count\\") # Output example: {\\"staff\\": 3, \\"developers\\": 5, \\"admins\\": 2} # Example: Count of unique GIDs result = analyze_unix_groups(\\"gid_count\\") # Output example: {\\"unique_gid_count\\": 42} # Example: Sorted list of all group names result = analyze_unix_groups(\\"group_names\\") # Output example: {\\"group_names_sorted\\": [\\"admins\\", \\"developers\\", \\"staff\\"]} ``` --- Write the function `analyze_unix_groups` that returns the requested statistic based on the Unix group database.","solution":"import grp def analyze_unix_groups(stat: str) -> dict: Analyze Unix group database entries and return the desired statistics. :param stat: A string specifying the desired statistic. Can be: - \'member_count\': Returns the count of members for each group - \'gid_count\': Returns the count of unique GIDs in the database - \'group_names\': Returns a list of all group names in the database, sorted alphabetically :return: A dictionary containing the requested statistic. :raises ValueError: If the `stat` parameter is not one of the specified values. groups = grp.getgrall() if stat == \'member_count\': member_count = {group.gr_name: len(group.gr_mem) for group in groups} return member_count elif stat == \'gid_count\': unique_gids = {group.gr_gid for group in groups} return {\'unique_gid_count\': len(unique_gids)} elif stat == \'group_names\': group_names = [group.gr_name for group in groups] group_names.sort() return {\'group_names_sorted\': group_names} else: raise ValueError(\\"Invalid statistic requested\\")"},{"question":"# Objective The goal of this assignment is to assess your understanding of PyTorch\'s `torch.cond` feature and your ability to implement data-dependent control flow within a neural network. You will create a PyTorch module that uses `torch.cond` to perform different operations based on the inputs. # Problem Statement You are tasked with implementing a PyTorch module named `DynamicOperationModel` that uses `torch.cond` to conditionally perform one of two different operations based on the sum of the input tensor. Specifically, you will: 1. Implement a module that takes a tensor `x` as input. 2. Use `torch.cond` to check if the sum of the elements in `x` is greater than a threshold value. 3. If the sum is greater than the threshold, apply a sequence of operations (defined in `operation_true`). 4. If the sum is less than or equal to the threshold, apply a different set of operations (defined in `operation_false`). # Instructions 1. Define two functions `operation_true(x: torch.Tensor) -> torch.Tensor` and `operation_false(x: torch.Tensor) -> torch.Tensor`: - `operation_true(x)`: returns `x * 2 + 5` - `operation_false(x)`: returns `x / 2 - 3` 2. Define a class `DynamicOperationModel` that inherits from `torch.nn.Module`. 3. Implement the `forward` method in `DynamicOperationModel` that: - Uses `torch.cond` to select between `operation_true` and `operation_false` based on whether the sum of the tensor elements is greater than a given threshold. 4. Initialize the threshold value as an argument to the `DynamicOperationModel` constructor. 5. Return the result of the selected operation. # Example Usage ```python import torch # Define operations def operation_true(x: torch.Tensor) -> torch.Tensor: return x * 2 + 5 def operation_false(x: torch.Tensor) -> torch.Tensor: return x / 2 - 3 # Define the model class DynamicOperationModel(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: result = torch.cond( x.sum() > self.threshold, operation_true, operation_false, (x,) ) return result # Initialize model model = DynamicOperationModel(threshold=10.0) # Example inputs input_tensor = torch.tensor([1.0, 2.0, 3.0]) output = model(input_tensor) print(output) ``` # Constraints 1. Do not use any additional libraries except PyTorch. 2. Assume the input tensor will always be a 1-dimensional tensor. 3. Ensure that the functions operate correctly for both positive and negative thresholds. # Evaluation Criteria - Correct implementation of the `torch.cond` functionality. - Handling of edge cases and different input values. - Correct inheritance and use of `torch.nn.Module`. Implement your solution in the cell below: ```python import torch # Define operations def operation_true(x: torch.Tensor) -> torch.Tensor: return x * 2 + 5 def operation_false(x: torch.Tensor) -> torch.Tensor: return x / 2 - 3 # Define the model class DynamicOperationModel(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: result = torch.cond( x.sum() > self.threshold, operation_true, operation_false, (x,) ) return result # Test the model input_tensor = torch.tensor([1.0, 2.0, 3.0]) model = DynamicOperationModel(threshold=10.0) output = model(input_tensor) print(output) ```","solution":"import torch import torch.nn as nn # Define operations def operation_true(x: torch.Tensor) -> torch.Tensor: return x * 2 + 5 def operation_false(x: torch.Tensor) -> torch.Tensor: return x / 2 - 3 # Define the model class DynamicOperationModel(nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: if x.sum().item() > self.threshold: result = operation_true(x) else: result = operation_false(x) return result # Example usage: # Initialize model model = DynamicOperationModel(threshold=10.0) # Example inputs input_tensor = torch.tensor([1.0, 2.0, 3.0]) output = model(input_tensor) print(output)"},{"question":"**Garbage Collection Inspection and Adjustment** You are tasked with writing a function to manage and inspect the garbage collection behavior in Python using the `gc` module. Your function will perform multiple tasks typically required for memory management and debugging. # Function Specification Create a function `manage_and_inspect_gc()` that performs the following steps: 1. **Enable GC and Check Status**: Ensure the garbage collector is enabled. Confirm this by checking the status. 2. **Set Debugging Flags**: Set debugging flags to `gc.DEBUG_LEAK` and verify that the flags were set correctly. 3. **Adjust Thresholds**: Set the garbage collection thresholds to `(700, 10, 10)`. 4. **Force Collection and Retrieve Garbage**: Force a full garbage collection and retrieve the list of uncollectable objects. 5. **Retrieve Stats**: Get the current statistics of the garbage collector and return the following: - Whether the garbage collector is enabled. - Current debugging flags. - Current collection thresholds. - Statistics from the collector, focusing on the counts of collections, collected objects, and uncollectable objects. # Expected Output The function should return a tuple containing: 1. A boolean indicating if the garbage collector is enabled. 2. The current debugging flags as an integer. 3. The current thresholds as a tuple of three integers. 4. The statistics as a list of dictionaries. # Example ```python import gc def manage_and_inspect_gc(): # Enable GC gc.enable() # Check GC status is_gc_enabled = gc.isenabled() # Set debugging flags gc.set_debug(gc.DEBUG_LEAK) current_debug_flags = gc.get_debug() # Adjust thresholds gc.set_threshold(700, 10, 10) current_thresholds = gc.get_threshold() # Force a full collection gc.collect() # Retrieve stats stats = gc.get_stats() return (is_gc_enabled, current_debug_flags, current_thresholds, stats) # Expected to return (True, <debug_flags>, (700, 10, 10), <stats_list>) print(manage_and_inspect_gc()) ``` # Constraints - You should not modify the behavior of the garbage collector beyond what is specified. - All operations should be sequential and must follow the order specified. Ensure your implementation is efficient and handles any possible edge cases.","solution":"import gc def manage_and_inspect_gc(): Manages and inspects Python\'s garbage collection behavior. Returns a tuple containing: - A boolean indicating if the garbage collector is enabled. - The current debugging flags as an integer. - The current thresholds as a tuple of three integers. - The statistics from the collector as a list of dictionaries. # Enable GC gc.enable() # Check GC status is_gc_enabled = gc.isenabled() # Set debugging flags gc.set_debug(gc.DEBUG_LEAK) current_debug_flags = gc.get_debug() # Adjust thresholds gc.set_threshold(700, 10, 10) current_thresholds = gc.get_threshold() # Force a full collection gc.collect() # Retrieve stats stats = gc.get_stats() return (is_gc_enabled, current_debug_flags, current_thresholds, stats)"},{"question":"Custom Logger Implementation with Flexible Logging Configuration Objective To assess the understanding of Pythonâ€™s `logging` module by requiring the student to implement a custom logging system that involves creating custom loggers, handlers, and formatters, setting appropriate logging levels, and ensuring efficient propagation of log messages. Problem Statement Implement a custom logging system using Pythonâ€™s `logging` module. Your task involves creating a flexible logging configuration based on a logging configuration dictionary. 1. **Logging Configuration**: You will use the following configuration dictionary to set up the loggers, handlers, and formatters. ```python logging_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'mode\': \'a\', }, }, \'loggers\': { \'app\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': False }, \'app.moduleA\': { \'level\': \'INFO\', \'handlers\': [\'console\'], \'propagate\': True }, \'app.moduleB\': { \'level\': \'ERROR\', \'handlers\': [\'file\'], \'propagate\': False }, } } ``` 2. **Implementation**: - Write a function `setup_logging(config: dict) -> None` to configure the logging based on `logging_config`. - The function should configure the loggers, handlers, and formatters as per the given configuration dictionary. - Demonstrate the configuration by logging messages from different modules (e.g., `app.moduleA`, `app.moduleB`) and verify that the messages appear in appropriate destinations with the correct formats and levels. 3. **Logging Demonstration**: - Create loggers for `app`, `app.moduleA`, and `app.moduleB`. - Log messages at various levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) from these loggers and observe the output on the console and the log file. - Ensure that the log propagation works correctly as defined by the `propagate` setting in the configuration. Expected Function Signature ```python def setup_logging(config: dict) -> None: pass def main(): # Demonstrate logging setup setup_logging(logging_config) app_logger = logging.getLogger(\'app\') moduleA_logger = logging.getLogger(\'app.moduleA\') moduleB_logger = logging.getLogger(\'app.moduleB\') app_logger.debug(\\"This is a DEBUG message from app\\") app_logger.info(\\"This is an INFO message from app\\") app_logger.warning(\\"This is a WARNING message from app\\") app_logger.error(\\"This is an ERROR message from app\\") app_logger.critical(\\"This is a CRITICAL message from app\\") moduleA_logger.debug(\\"This is a DEBUG message from moduleA\\") moduleA_logger.info(\\"This is an INFO message from moduleA\\") moduleA_logger.warning(\\"This is a WARNING message from moduleA\\") moduleA_logger.error(\\"This is an ERROR message from moduleA\\") moduleA_logger.critical(\\"This is a CRITICAL message from moduleA\\") moduleB_logger.debug(\\"This is a DEBUG message from moduleB\\") moduleB_logger.info(\\"This is an INFO message from moduleB\\") moduleB_logger.warning(\\"This is a WARNING message from moduleB\\") moduleB_logger.error(\\"This is an ERROR message from moduleB\\") moduleB_logger.critical(\\"This is a CRITICAL message from moduleB\\") if __name__ == \\"__main__\\": main() ``` Constraints - The logging configuration dictionary structure cannot be altered. - Ensure thread-safety while configuring and using the logger. - Demonstrate clear and distinct format styles for the console and the file handlers. Notes 1. Use the `logging.config.dictConfig` function to configure the logging settings. 2. Experiment with different logging levels and observe the propagation behavior. 3. Ensure that the root logger does not interfere unless specified in the configuration.","solution":"import logging import logging.config def setup_logging(config: dict) -> None: Configures logging based on the provided configuration dictionary. Args: config: dict - The logging configuration dictionary. Returns: None logging.config.dictConfig(config) def main(): # Logging configuration dictionary logging_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'mode\': \'a\', }, }, \'loggers\': { \'app\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': False }, \'app.moduleA\': { \'level\': \'INFO\', \'handlers\': [\'console\'], \'propagate\': True }, \'app.moduleB\': { \'level\': \'ERROR\', \'handlers\': [\'file\'], \'propagate\': False }, } } # Call the setup_logging function setup_logging(logging_config) # Get loggers app_logger = logging.getLogger(\'app\') moduleA_logger = logging.getLogger(\'app.moduleA\') moduleB_logger = logging.getLogger(\'app.moduleB\') # Log messages from different levels app_logger.debug(\\"This is a DEBUG message from app\\") app_logger.info(\\"This is an INFO message from app\\") app_logger.warning(\\"This is a WARNING message from app\\") app_logger.error(\\"This is an ERROR message from app\\") app_logger.critical(\\"This is a CRITICAL message from app\\") moduleA_logger.debug(\\"This is a DEBUG message from moduleA\\") moduleA_logger.info(\\"This is an INFO message from moduleA\\") moduleA_logger.warning(\\"This is a WARNING message from moduleA\\") moduleA_logger.error(\\"This is an ERROR message from moduleA\\") moduleA_logger.critical(\\"This is a CRITICAL message from moduleA\\") moduleB_logger.debug(\\"This is a DEBUG message from moduleB\\") moduleB_logger.info(\\"This is an INFO message from moduleB\\") moduleB_logger.warning(\\"This is a WARNING message from moduleB\\") moduleB_logger.error(\\"This is an ERROR message from moduleB\\") moduleB_logger.critical(\\"This is a CRITICAL message from moduleB\\") if __name__ == \\"__main__\\": main()"},{"question":"**Timedelta Data Analysis** You are provided with a pandas DataFrame containing time tracking data for a project. Each row in the DataFrame represents a task, with start and end times given in string format. Your task is to process this DataFrame to compute the time spent on each task and perform some analysis on the computed timedeltas. **Input:** A pandas DataFrame `df` with the following columns: - `TaskID` (str): Identifier for the task. - `Start` (str, ISO 8601 format): Start time of the task. - `End` (str, ISO 8601 format): End time of the task. Example: ```python import pandas as pd data = { \'TaskID\': [\'T1\', \'T2\', \'T3\'], \'Start\': [\'2023-01-01T08:00:00\', \'2023-01-01T09:30:00\', \'2023-01-01T10:00:00\'], \'End\': [\'2023-01-01T09:00:00\', \'2023-01-01T10:00:00\', \'2023-01-01T12:30:00\'] } df = pd.DataFrame(data) ``` **Tasks:** 1. **Compute Task Duration**: Add a new column `Duration` to the DataFrame representing the duration of each task as a `Timedelta` object. 2. **Total Duration**: Calculate the total duration spent across all tasks. 3. **Average Duration**: Compute the average duration spent on the tasks. 4. **Tasks with Duration > 1 Hour**: Filter and return tasks where the duration exceeds 1 hour. **Output:** 1. The modified DataFrame with the `Duration` column. 2. The total duration spent on all tasks as a `Timedelta` object. 3. The average duration of tasks as a `Timedelta` object. 4. A DataFrame of tasks with durations greater than 1 hour. **Constraints:** - All tasks have valid start and end times, and start time is always before the end time. **Implementation:** ```python import pandas as pd def analyze_task_durations(df): # Convert start and end times to datetime df[\'Start\'] = pd.to_datetime(df[\'Start\']) df[\'End\'] = pd.to_datetime(df[\'End\']) # Compute the duration and add it as a new column df[\'Duration\'] = df[\'End\'] - df[\'Start\'] # Calculate total duration total_duration = df[\'Duration\'].sum() # Calculate average duration average_duration = df[\'Duration\'].mean() # Filter tasks with duration greater than 1 hour one_hour = pd.Timedelta(hours=1) long_tasks = df[df[\'Duration\'] > one_hour] return df, total_duration, average_duration, long_tasks # Example usage data = { \'TaskID\': [\'T1\', \'T2\', \'T3\'], \'Start\': [\'2023-01-01T08:00:00\', \'2023-01-01T09:30:00\', \'2023-01-01T10:00:00\'], \'End\': [\'2023-01-01T09:00:00\', \'2023-01-01T10:00:00\', \'2023-01-01T12:30:00\'] } df = pd.DataFrame(data) result = analyze_task_durations(df) print(result) ```","solution":"import pandas as pd def analyze_task_durations(df): # Convert start and end times to datetime df[\'Start\'] = pd.to_datetime(df[\'Start\']) df[\'End\'] = pd.to_datetime(df[\'End\']) # Compute the duration and add it as a new column df[\'Duration\'] = df[\'End\'] - df[\'Start\'] # Calculate total duration total_duration = df[\'Duration\'].sum() # Calculate average duration average_duration = df[\'Duration\'].mean() # Filter tasks with duration greater than 1 hour one_hour = pd.Timedelta(hours=1) long_tasks = df[df[\'Duration\'] > one_hour] return df, total_duration, average_duration, long_tasks"},{"question":"Objective: Demonstrate your understanding of Python\'s `zipimport` module by writing a function that imports a module from a specified ZIP file and handles potential errors. Problem Description: You are required to implement a function named `import_from_zip` that will import a specified Python module from a provided ZIP archive path and a subdirectory within the archive (if applicable). Function Signature: ```python def import_from_zip(zip_path: str, module_name: str) -> object: ``` Parameters: - `zip_path` (str): The path to the ZIP file. It may include a subdirectory within the ZIP file (e.g., \\"example.zip/lib\\"). - `module_name` (str): The fully qualified (dotted) name of the module to import (e.g., \\"jwzthreading\\"). Returns: - An imported module object if the module is successfully found and imported. Raises: - `ImportError`: If the module cannot be found or imported for any reason. Constraints: - Assume that the ZIP file structure and contents are valid Python modules and packages. - Only `.py` and `.pyc` files are considered valid modules for import. Example: ```python # Assume that the ZIP archive \\"example.zip\\" contains a module \\"jwzthreading.py\\" at the root. imported_module = import_from_zip(\\"example.zip\\", \\"jwzthreading\\") print(imported_module.__name__) # Should output: \\"jwzthreading\\" # If there\'s a subdirectory imported_module = import_from_zip(\\"example.zip/lib\\", \\"libmodule\\") print(imported_module.__name__) # Should output: \\"libmodule\\" ``` Implementation: Ensure your function handles all potential errors that could arise from the module importation process, such as improper file paths or missing modules, by raising appropriate exceptions. **Hint:** You may use the `zipimport.zipimporter` class to facilitate the import process. Note: Use the following command to test your function: ```python if __name__ == \\"__main__\\": try: module = import_from_zip(\\"example.zip\\", \\"jwzthreading\\") print(f\\"Module {module.__name__} imported successfully.\\") except ImportError as e: print(f\\"Error importing module: {e}\\") ```","solution":"import zipimport def import_from_zip(zip_path: str, module_name: str) -> object: Imports a specified Python module from a provided ZIP archive path and a subdirectory within the archive (if applicable). :param zip_path: The path to the ZIP file. It may include a subdirectory within the ZIP file. :param module_name: The fully qualified (dotted) name of the module to import. :return: An imported module object. :raises ImportError: If the module cannot be found or imported. try: # Create a zip importer for the specified path importer = zipimport.zipimporter(zip_path) # Import the module module = importer.load_module(module_name) return module except Exception as e: raise ImportError(f\\"Cannot import module {module_name} from {zip_path}: {e}\\")"},{"question":"<|Analysis Begin|> The provided documentation is focused on versioning macros for Python, particularly related to encoding the version number into a single integer (PY_VERSION_HEX). The macros include: - PY_MAJOR_VERSION - PY_MINOR_VERSION - PY_MICRO_VERSION - PY_RELEASE_LEVEL - PY_RELEASE_SERIAL - PY_VERSION_HEX These macros represent different parts of the Python version, and the documentation outlines how to interpret or construct the version information from these components. Based on this documentation, we can craft a question that challenges the student to write a function to encode and decode Python version numbers utilizing this information. <|Analysis End|> <|Question Begin|> # Python Version Encoding and Decoding You are required to write code that demonstrates the encoding and decoding of Python version numbers using the given macros. The version number is represented in a format similar to `3.4.1a2` which needs to be encoded and decoded with the given macros. # Task 1: Encoding Write a function `encode_version(major, minor, micro, level, serial)` that takes the version number components as input and returns the encoded version number as a single integer. - **major**: An integer representing the major version number (e.g., `3` in `3.4.1a2`). - **minor**: An integer representing the minor version number (e.g., `4` in `3.4.1a2`). - **micro**: An integer representing the micro version number (e.g., `1` in `3.4.1a2`). - **level**: A character representing the release level (`\'a\'` for alpha, `\'b\'` for beta, `\'c\'` for release candidate, and `\'f\'` for final release). - **serial**: An integer representing the release serial number (e.g., `2` in `3.4.1a2`). The release level should be mapped to its respective hex value: - `\'a\'` -> `0xA` - `\'b\'` -> `0xB` - `\'c\'` -> `0xC` - `\'f\'` -> `0xF` # Task 2: Decoding Write a function `decode_version(encoded_version)` that takes an encoded version number as a single integer and returns a tuple containing the decoded version components: `(major, minor, micro, level, serial)`. - **encoded_version**: An integer that represents the encoded version number. **Example:** ```python # Example of encoding encoded_version = encode_version(3, 4, 1, \'a\', 2) print(encoded_version) # Output should be 0x030401a2 # Example of decoding decoded_version = decode_version(0x030401a2) print(decoded_version) # Output should be (3, 4, 1, \'a\', 2) ``` # Constraints: - `0 <= major <= 255` - `0 <= minor <= 255` - `0 <= micro <= 255` - level must be one of `\'a\'`, `\'b\'`, `\'c\'`, `\'f\'` - `0 <= serial <= 15` # Performance Requirements: - Your encoding function should run in constant time, O(1). - Your decoding function should run in constant time, O(1). Ensure your solution works efficiently within the given constraints.","solution":"def encode_version(major, minor, micro, level, serial): Encodes the version number components into a single integer. level_mapping = { \'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF } encoded_version = (major << 24) | (minor << 16) | (micro << 8) | (level_mapping[level] << 4) | (serial & 0xF) return encoded_version def decode_version(encoded_version): Decodes the encoded version number into its components. major = (encoded_version >> 24) & 0xFF minor = (encoded_version >> 16) & 0xFF micro = (encoded_version >> 8) & 0xFF level_code = (encoded_version >> 4) & 0xF serial = encoded_version & 0xF level_mapping = { 0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\' } return major, minor, micro, level_mapping[level_code], serial"},{"question":"# PyTorch Coding Assessment: Working with Tensor Views Objective Your task is to demonstrate proficiency in working with tensor views in PyTorch. You will write functions to create and manipulate tensor views, ensuring efficient memory use and understanding of tensor contiguity. Description 1. **Create a Tensor View** Implement a function `create_view(tensor: torch.Tensor, shape: Tuple[int, ...]) -> torch.Tensor` that takes a tensor and a new shape, then returns a view of the original tensor with the new shape. Ensure that the function raises an appropriate error if the new shape is incompatible with the original tensor. ```python def create_view(tensor: torch.Tensor, shape: Tuple[int, ...]) -> torch.Tensor: Args: tensor (torch.Tensor): The original tensor. shape (Tuple[int, ...]): The target shape for the view. Returns: torch.Tensor: A view of the original tensor with the specified shape. pass ``` 2. **Check Tensor Contiguity** Implement a function `is_contiguous_view(tensor: torch.Tensor) -> bool` that checks if a given tensor is contiguous. If the tensor is not contiguous, the function should convert it to a contiguous form and return that contiguous tensor. ```python def is_contiguous_view(tensor: torch.Tensor) -> torch.Tensor: Args: tensor (torch.Tensor): The tensor to check for contiguity. Returns: torch.Tensor: The input tensor if it is contiguous, otherwise a new contiguous tensor. pass ``` 3. **Slicing and Returning Views** Implement a function `slice_view(tensor: torch.Tensor, indices: Tuple[slice, ...]) -> torch.Tensor` that takes a tensor and a tuple of slice objects, and returns a view of the tensor according to the given slices. ```python def slice_view(tensor: torch.Tensor, indices: Tuple[slice, ...]) -> torch.Tensor: Args: tensor (torch.Tensor): The original tensor. indices (Tuple[slice, ...]): A tuple of slice objects to slice the tensor. Returns: torch.Tensor: A view of the tensor as per the specified slices. pass ``` Constraints 1. The functions should not copy any data unnecessarily. 2. Raise exceptions with informative messages if invalid inputs are provided. 3. Ensure efficient memory usage and consider performance impacts. Input and Output Examples ```python # Example 1: Creating a View t = torch.rand(4, 4) new_view = create_view(t, (2, 8)) assert new_view.shape == (2, 8) assert t.storage().data_ptr() == new_view.storage().data_ptr() # Example 2: Checking Tensor Contiguity t = torch.tensor([[0, 1], [2, 3]]).transpose(0, 1) contiguous_t = is_contiguous_view(t) assert contiguous_t.is_contiguous() assert not t.is_contiguous() # Example 3: Slicing and Returning Views t = torch.rand(4, 4) sliced_view = slice_view(t, (slice(0, 2), slice(1, 3))) assert sliced_view.shape == (2, 2) assert t.storage().data_ptr() == sliced_view.storage().data_ptr() ``` Ensure that your implementation conforms to these examples and constraints. Complete the three functions to meet the described objectives.","solution":"import torch from typing import Tuple def create_view(tensor: torch.Tensor, shape: Tuple[int, ...]) -> torch.Tensor: Args: tensor (torch.Tensor): The original tensor. shape (Tuple[int, ...]): The target shape for the view. Returns: torch.Tensor: A view of the original tensor with the specified shape. Raises: ValueError: If the new shape is incompatible with the original tensor. try: return tensor.view(shape) except RuntimeError as e: raise ValueError(f\\"Cannot reshape tensor of shape {tensor.shape} to shape {shape}. Original error: {e}\\") def is_contiguous_view(tensor: torch.Tensor) -> torch.Tensor: Args: tensor (torch.Tensor): The tensor to check for contiguity. Returns: torch.Tensor: The input tensor if it is contiguous, otherwise a new contiguous tensor. return tensor if tensor.is_contiguous() else tensor.contiguous() def slice_view(tensor: torch.Tensor, indices: Tuple[slice, ...]) -> torch.Tensor: Args: tensor (torch.Tensor): The original tensor. indices (Tuple[slice, ...]): A tuple of slice objects to slice the tensor. Returns: torch.Tensor: A view of the tensor as per the specified slices. return tensor[indices]"},{"question":"As a security-focused developer, you are tasked to create a function `generate_password` that generates a secure alphanumeric password. The password must adhere to specific requirements for security purposes. Utilize the \\"secrets\\" module for all random choices and generation of secure elements in the password. # Requirements 1. The password must be exactly 12 characters long. 2. It should include at least: - 2 uppercase letters - 2 lowercase letters - 2 digits - 2 special characters (from the set `!@#%^&*()-_+=`) 3. The remainder of the characters can be any printable ASCII characters, excluding spaces. 4. Use the `secrets` module for all randomness: - Utilize functions like `secrets.choice()`, `secrets.randbelow()`. - Ensure secure generation of each component of the password. 5. The password should be returned as a string. # Function Signature ```python def generate_password() -> str: pass ``` # Example Usage ```python secure_password = generate_password() print(secure_password) # Example Output: \\"A9f!X3d!J2eQ\\" ``` # Constraints - Do not use the `random` module. - Ensure the function is efficient, even though complexity is less of a concern for a one-time password generation. # Implementation Advice - Combine all required characters and shuffle to ensure randomness. - Check the generated password to ensure it meets all the requirements before returning it. # Testing - Test the password generation multiple times to ensure that it consistently meets the required criteria. - Make sure no two generated passwords are the same in a reasonable number of attempts.","solution":"import secrets import string def generate_password() -> str: Generates a secure alphanumeric password that is exactly 12 characters long. The password includes at least: - 2 uppercase letters - 2 lowercase letters - 2 digits - 2 special characters from the set `!@#%^&*()-_+=` The remaining characters are randomly selected. uppercase_letters = \'\'.join(secrets.choice(string.ascii_uppercase) for _ in range(2)) lowercase_letters = \'\'.join(secrets.choice(string.ascii_lowercase) for _ in range(2)) digits = \'\'.join(secrets.choice(string.digits) for _ in range(2)) special_characters = \'\'.join(secrets.choice(\'!@#%^&*()-_+=\') for _ in range(2)) # Calculate the number of remaining characters needed to reach 12 characters num_remaining_chars = 12 - (len(uppercase_letters) + len(lowercase_letters) + len(digits) + len(special_characters)) remaining_chars = \'\'.join(secrets.choice(string.ascii_letters + string.digits + \'!@#%^&*()-_+=\') for _ in range(num_remaining_chars)) password_list = list(uppercase_letters + lowercase_letters + digits + special_characters + remaining_chars) # Shuffle the password list to ensure randomness secrets.SystemRandom().shuffle(password_list) # Join the list to form the final password string password = \'\'.join(password_list) return password"},{"question":"Coding Assessment Question: Analyzing Stock Prices using Pandas Window Functions # Objective Demonstrate your understanding of pandas window functions (rolling, expanding, and exponentially-weighted windows) by analyzing a set of stock prices and computing various statistics. # Problem Statement You are given a time-series dataset containing the daily closing prices of a stock. Your task is to implement a function `analyze_stock_prices` that performs the following operations: 1. Compute a 7-day rolling mean and standard deviation of the stock prices. 2. Compute an expanding mean and standard deviation from the beginning of the data up to the current point. 3. Compute an exponentially-weighted mean and standard deviation with a span of 10 days, giving more weight to recent data points. # Input - A pandas DataFrame `df` with a single column `Close`, which contains the daily closing prices of the stock. The index of the DataFrame represents the dates. # Output - A pandas DataFrame with the following columns: - `Close`: The original closing prices. - `Rolling_Mean_7`: The 7-day rolling mean of the closing prices. - `Rolling_STD_7`: The 7-day rolling standard deviation of the closing prices. - `Expanding_Mean`: The expanding mean of the closing prices. - `Expanding_STD`: The expanding standard deviation of the closing prices. - `EWM_Mean_10`: The exponentially-weighted mean of the closing prices with a span of 10 days. - `EWM_STD_10`: The exponentially-weighted standard deviation of the closing prices with a span of 10 days. # Constraints - The DataFrame `df` will have at least 15 rows. # Performance Requirements - The function should efficiently utilize pandas\' built-in window functions to compute the required statistics. # Function Signature ```python def analyze_stock_prices(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Example ```python import pandas as pd data = { \'Close\': [150, 152, 153, 151, 155, 157, 160, 162, 159, 158, 156, 154, 155, 157, 158] } df = pd.DataFrame(data, index=pd.date_range(start=\'2023-01-01\', periods=15)) result_df = analyze_stock_prices(df) print(result_df) ``` Expected output (sample): ``` Close Rolling_Mean_7 Rolling_STD_7 Expanding_Mean Expanding_STD EWM_Mean_10 EWM_STD_10 2023-01-01 150 NaN NaN 150.000000 0.000000 150.000000 0.000000 2023-01-02 152 NaN NaN 151.000000 1.414214 150.363636 2.0 2023-01-03 153 NaN NaN 151.666667 1.527525 150.902174 2.449490 ... ``` # Note The rolling mean and standard deviation for the first 6 days would be `NaN` since there are not enough data points to form a 7-day window. Similarly, the exponentially-weighted statistics would be calculated based on the available data points up to that point.","solution":"import pandas as pd def analyze_stock_prices(df: pd.DataFrame) -> pd.DataFrame: result_df = df.copy() # Compute a 7-day rolling mean and standard deviation of the stock prices result_df[\'Rolling_Mean_7\'] = df[\'Close\'].rolling(window=7).mean() result_df[\'Rolling_STD_7\'] = df[\'Close\'].rolling(window=7).std() # Compute an expanding mean and standard deviation from the beginning of the data up to the current point result_df[\'Expanding_Mean\'] = df[\'Close\'].expanding().mean() result_df[\'Expanding_STD\'] = df[\'Close\'].expanding().std() # Compute an exponentially-weighted mean and standard deviation with a span of 10 days result_df[\'EWM_Mean_10\'] = df[\'Close\'].ewm(span=10, adjust=False).mean() result_df[\'EWM_STD_10\'] = df[\'Close\'].ewm(span=10, adjust=False).std() return result_df"},{"question":"# Custom Descriptor in Python **Objective:** Create a custom descriptor in Python that supports getting, setting, and deleting an attribute with additional functionality such as logging access and modifications. This will assess your understanding of Python\'s data model and descriptor protocol. **Task:** Implement a class `LoggedAttribute` that acts as a descriptor to manage a private attribute with the following specifications: - When the attribute is accessed, it should log a message indicating the access was made. - When the attribute is set, it should log the new value being set. - When the attribute is deleted, it should log a message indicating the deletion. **Requirements:** 1. Implement the `__get__`, `__set__`, and `__delete__` methods in the `LoggedAttribute` class. 2. Capture log messages using Python\'s `print()` function (for simplicity). 3. The descriptor should store the attribute value in a private dictionary attribute of the class using the instance as the key. **Input and Output:** - The input consists of the creation of a class that uses the `LoggedAttribute` as a descriptor. - The output will be the log messages printed during attribute access, modification, and deletion. **Example Implementation:** ```python class LoggedAttribute: A descriptor that logs accesses to the attribute. def __init__(self, name): self.name = name self.values = {} def __get__(self, instance, owner): if instance is None: return self print(f\\"Accessing {self.name} of {instance}\\") return self.values.get(instance) def __set__(self, instance, value): print(f\\"Setting {self.name} of {instance} to \'{value}\'\\") self.values[instance] = value def __delete__(self, instance): if instance in self.values: print(f\\"Deleting {self.name} of {instance}\\") del self.values[instance] class MyClass: attr = LoggedAttribute(\\"attr\\") # Example usage obj = MyClass() obj.attr = 10 # Should log setting the attribute print(obj.attr) # Should log accessing the attribute del obj.attr # Should log deleting the attribute ``` **Constraints:** - The implementation should assume that the instances of the `MyClass` will not be garbage collected before the program ends to avoid cleaning up the dictionary manually. - The solution should be designed to handle multiple instances of `MyClass`. **Performance Considerations:** - The solution should efficiently handle multiple instances of the class using the descriptor.","solution":"class LoggedAttribute: A descriptor that logs accesses to the attribute. def __init__(self, name): self.name = name self.values = {} def __get__(self, instance, owner): if instance is None: return self print(f\\"Accessing {self.name} of {instance}\\") return self.values.get(instance) def __set__(self, instance, value): print(f\\"Setting {self.name} of {instance} to \'{value}\'\\") self.values[instance] = value def __delete__(self, instance): if instance in self.values: print(f\\"Deleting {self.name} of {instance}\\") del self.values[instance] class MyClass: attr = LoggedAttribute(\\"attr\\") # Example usage obj = MyClass() obj.attr = 10 # Should log setting the attribute print(obj.attr) # Should log accessing the attribute del obj.attr # Should log deleting the attribute"},{"question":"**Objective:** Demonstrate your understanding of the `ipaddress` module in Python through a series of tasks related to IP address manipulation and network operations. --- **Question:** You are given a list of IP addresses and a list of network definitions. Your tasks are to implement a function that processes these inputs as follows: 1. **Categorize the IP addresses as either IPv4 or IPv6.** 2. **For each IP address, determine if it belongs to any network in the network list.** Implement the function `process_ip_addresses(ip_list: List[str], network_list: List[str]) -> Dict[str, Any]` where: - `ip_list` is a list of strings, each representing an IP address. - `network_list` is a list of strings, each representing an IP network definition in the format \'address/prefix\'. The function should return a dictionary with the following structure: - `\\"IPv4\\"`: a list of tuples where each tuple contains an IPv4 address and a list of matching networks. - `\\"IPv6\\"`: a list of tuples where each tuple contains an IPv6 address and a list of matching networks. Each tuple should contain: - The string representation of the IP address. - A list of network definitions (as strings) that the IP address is part of. **Constraints:** - You can assume that all input IP addresses and networks are valid. - The function should efficiently handle up to 10,000 IP addresses and 1,000 networks. **Example:** ```python ip_list = [\'192.0.2.1\', \'2001:db8::1\'] network_list = [\'192.0.2.0/24\', \'2001:db8::/96\'] output = process_ip_addresses(ip_list, network_list) # Expected output: # { # \\"IPv4\\": [(\'192.0.2.1\', [\'192.0.2.0/24\'])], # \\"IPv6\\": [(\'2001:db8::1\', [\'2001:db8::/96\'])] # } ``` --- **Explanation:** - The function `process_ip_addresses` categorizes IP addresses into IPv4 or IPv6. - For each IP address, the function checks if it is part of any network in the `network_list`. - The result is structured to show which IP addresses are in which networks, categorized by IP version. This task tests your understanding of creating and manipulating IP address and network objects using the `ipaddress` module, as well as your ability to efficiently process and match these objects.","solution":"import ipaddress from typing import List, Dict, Any def process_ip_addresses(ip_list: List[str], network_list: List[str]) -> Dict[str, Any]: result = {\\"IPv4\\": [], \\"IPv6\\": []} # Create lists of ipaddress objects and associated networks ipv4_networks = [ipaddress.IPv4Network(network) for network in network_list if ipaddress.ip_network(network).version == 4] ipv6_networks = [ipaddress.IPv6Network(network) for network in network_list if ipaddress.ip_network(network).version == 6] for ip in ip_list: ip_obj = ipaddress.ip_address(ip) if ip_obj.version == 4: matching_networks = [str(network) for network in ipv4_networks if ip_obj in network] result[\\"IPv4\\"].append((ip, matching_networks)) elif ip_obj.version == 6: matching_networks = [str(network) for network in ipv6_networks if ip_obj in network] result[\\"IPv6\\"].append((ip, matching_networks)) return result"},{"question":"# Quoted-Printable Data Conversion Given a file containing quoted-printable encoded data, write a Python function that reads the file, decodes the data, and writes the decoded data to another file. Additionally, implement the inverse operation which reads a binary file, encodes its contents into quoted-printable format, and writes the result to another file. Function 1: decode_file(input_file: str, output_file: str, header: bool = False) -> None - `input_file`: Path to the quoted-printable encoded input file (binary file). - `output_file`: Path to the binary output file where the decoded content should be written. - `header`: Optional boolean flag to decode underscore as space, default is `False`. Function 2: encode_file(input_file: str, output_file: str, quotetabs: bool, header: bool = False) -> None - `input_file`: Path to the binary input file. - `output_file`: Path to the quoted-printable encoded output file (binary file). - `quotetabs`: Boolean flag to encode embedded spaces and tabs. - `header`: Optional boolean flag to encode spaces as underscores, default is `False`. # Example ```python def decode_file(input_file: str, output_file: str, header: bool = False) -> None: with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: quopri.decode(infile, outfile, header=header) def encode_file(input_file: str, output_file: str, quotetabs: bool, header: bool = False) -> None: with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: quopri.encode(infile, outfile, quotetabs=quotetabs, header=header) ``` # Test Cases To assess the correctness of the functions, consider the following test cases: 1. Decode a file containing simple quoted-printable text. 2. Encode a binary file containing text with spaces and tabs to check proper encoding with `quotetabs=True`. 3. Decode a file where the `header=True` and underscores are used to represent spaces. 4. Encode a binary file and then decode it to verify if the output matches the original input file. Constraints - Handle only valid quoted-printable data as input. - Ensure proper file opening and closing techniques to avoid resource leaks. Implement these functions to confirm your understanding of Python\'s `quopri` module and binary file operations.","solution":"import quopri def decode_file(input_file: str, output_file: str, header: bool = False) -> None: Decodes a quoted-printable encoded file into binary data. :param input_file: Path to the quoted-printable encoded input file (binary file). :param output_file: Path to the binary output file where the decoded content should be written. :param header: Boolean flag to decode underscore as space, default is False. with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: quopri.decode(infile, outfile, header=header) def encode_file(input_file: str, output_file: str, quotetabs: bool, header: bool = False) -> None: Encodes binary data into quoted-printable format. :param input_file: Path to the binary input file. :param output_file: Path to the quoted-printable encoded output file (binary file). :param quotetabs: Boolean flag to encode embedded spaces and tabs. :param header: Boolean flag to encode spaces as underscores, default is False. with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: quopri.encode(infile, outfile, quotetabs=quotetabs, header=header)"},{"question":"Objective This question aims to assess your understanding of PyTorch\'s utility functions, specifically focusing on the `torch.utils.data` sub-module. You will be required to create a custom data pipeline using PyTorch\'s data loading utilities that can handle a sequence of data processing steps. Problem Statement You are given a dataset of images and corresponding labels. Each image is a 28x28 grayscale image, and the labels are integers representing different classes. You need to create a custom data pipeline to preprocess the images and feed them into a model. Your task is to implement a custom dataset and dataloader using `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` respectively. The data pipeline should include the following steps: 1. **Normalization**: Normalize each image so that pixel values are in the range [0, 1]. 2. **Transformation**: Apply a random horizontal flip with a probability of 0.5. 3. **Batching**: Load the data in batches of 32. 4. **Shuffling**: Ensure the data is shuffled at the beginning of each epoch. Input/Output - The custom dataset should take a list of image file paths and their corresponding labels as input. - The dataloader should output batches of normalized and transformed images along with their corresponding labels. Constraints - Use only PyTorch and standard Python libraries. - The solution should work efficiently for a large number of images. Function Signature ```python import torch from torch.utils.data import Dataset, DataLoader from torchvision import transforms from PIL import Image class CustomImageDataset(Dataset): def __init__(self, image_paths, labels, transform=None): Args: image_paths (list of strings): List of paths to image files. labels (list of int): List of labels corresponding to the images. transform (callable, optional): Optional transform to be applied on a sample. self.image_paths = image_paths self.labels = labels self.transform = transform def __len__(self): return len(self.image_paths) def __getitem__(self, idx): image = Image.open(self.image_paths[idx]) label = self.labels[idx] if self.transform: image = self.transform(image) return image, label def get_dataloader(image_paths, labels, batch_size=32): Args: image_paths (list of strings): List of paths to image files. labels (list of int): List of labels corresponding to the images. batch_size (int): Number of samples per batch. Returns: DataLoader: DataLoader object that outputs batches of processed images. transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), transforms.RandomHorizontalFlip(p=0.5) ]) dataset = CustomImageDataset(image_paths, labels, transform) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) return dataloader ``` # Example Usage ```python image_paths = [\'image1.png\', \'image2.png\', \'image3.png\', ...] labels = [0, 1, 0, ...] dataloader = get_dataloader(image_paths, labels) for images, labels in dataloader: # Your training code here pass ``` Notes: - Make sure to handle any edge cases like images not existing at the given paths. - Ensure efficient memory usage when dealing with large datasets.","solution":"import torch from torch.utils.data import Dataset, DataLoader from torchvision import transforms from PIL import Image class CustomImageDataset(Dataset): def __init__(self, image_paths, labels, transform=None): Args: image_paths (list of strings): List of paths to image files. labels (list of int): List of labels corresponding to the images. transform (callable, optional): Optional transform to be applied on a sample. self.image_paths = image_paths self.labels = labels self.transform = transform def __len__(self): return len(self.image_paths) def __getitem__(self, idx): image = Image.open(self.image_paths[idx]).convert(\'L\') # Ensure it\'s grayscale label = self.labels[idx] if self.transform: image = self.transform(image) return image, label def get_dataloader(image_paths, labels, batch_size=32): Args: image_paths (list of strings): List of paths to image files. labels (list of int): List of labels corresponding to the images. batch_size (int): Number of samples per batch. Returns: DataLoader: DataLoader object that outputs batches of processed images. transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), transforms.RandomHorizontalFlip(p=0.5) ]) dataset = CustomImageDataset(image_paths, labels, transform) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) return dataloader"},{"question":"**Problem Statement:** You are given a dataset related to weather forecasting. Your task is to train a classifier to predict whether it will rain tomorrow. After training, you will tune the decision threshold of the classifier to minimize false negatives (prioritize recall). Finally, you will compare the performance metrics of the classifier before and after tuning the decision threshold. **Input:** - A dataset `weather_data.csv` with columns: - `feature1`, `feature2`, ..., `featureN`: Feature columns - `rain_tomorrow`: Target column (1 if it will rain, 0 otherwise) **Constraints:** - Use a decision tree classifier for training. - Use `TunedThresholdClassifierCV` for tuning the decision threshold. - Use recall as the scoring metric for tuning the decision threshold. - Split the data into 70% training and 30% testing. **Output:** - Print the recall, precision, and accuracy of the classifier before and after tuning the decision threshold. **Requirements:** - Implement a function `tune_threshold_and_evaluate(file_path: str) -> None` where: - `file_path`: Path to the `weather_data.csv` file. - The function reads the dataset, trains the classifier, tunes the decision threshold, and prints the evaluation metrics. **Example:** ```python def tune_threshold_and_evaluate(file_path: str) -> None: import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import recall_score, precision_score, accuracy_score from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer # Step 1: Load the data data = pd.read_csv(file_path) # Step 2: Split the data into features and target X = data.drop(columns=[\'rain_tomorrow\']) y = data[\'rain_tomorrow\'] # Step 3: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 4: Train a decision tree classifier base_classifier = DecisionTreeClassifier() base_classifier.fit(X_train, y_train) # Step 5: Evaluate the classifier before tuning the threshold y_pred_base = base_classifier.predict(X_test) recall_base = recall_score(y_test, y_pred_base) precision_base = precision_score(y_test, y_pred_base) accuracy_base = accuracy_score(y_test, y_pred_base) print(\\"Before Tuning - Recall: \\", recall_base) print(\\"Before Tuning - Precision: \\", precision_base) print(\\"Before Tuning - Accuracy: \\", accuracy_base) # Step 6: Tune the decision threshold using TunedThresholdClassifierCV scorer = make_scorer(recall_score, pos_label=1) tuned_classifier = TunedThresholdClassifierCV(base_classifier, scoring=scorer) tuned_classifier.fit(X_train, y_train) # Step 7: Evaluate the classifier after tuning the threshold y_pred_tuned = tuned_classifier.predict(X_test) recall_tuned = recall_score(y_test, y_pred_tuned) precision_tuned = precision_score(y_test, y_pred_tuned) accuracy_tuned = accuracy_score(y_test, y_pred_tuned) print(\\"After Tuning - Recall: \\", recall_tuned) print(\\"After Tuning - Precision: \\", precision_tuned) print(\\"After Tuning - Accuracy: \\", accuracy_tuned) ``` **Explanation:** 1. The function `tune_threshold_and_evaluate` accepts the file path of the dataset. 2. It reads the dataset and splits it into features and target variable. 3. The data is split into training and testing sets. 4. A decision tree classifier is trained on the training set. 5. The classifier\'s recall, precision, and accuracy are printed before tuning. 6. The decision threshold is tuned using `TunedThresholdClassifierCV` with recall as the scoring metric. 7. The classifier\'s recall, precision, and accuracy are printed after tuning. **Evaluation Criteria:** - Correct implementation of the decision tree classifier. - Accurate usage of `TunedThresholdClassifierCV` for tuning the decision threshold. - Correct calculation and comparison of performance metrics before and after tuning.","solution":"def tune_threshold_and_evaluate(file_path: str) -> None: import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import recall_score, precision_score, accuracy_score from sklearn.calibration import CalibratedClassifierCV # Step 1: Load the data data = pd.read_csv(file_path) # Step 2: Split the data into features and target X = data.drop(columns=[\'rain_tomorrow\']) y = data[\'rain_tomorrow\'] # Step 3: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 4: Train a decision tree classifier base_classifier = DecisionTreeClassifier() base_classifier.fit(X_train, y_train) # Step 5: Evaluate the classifier before tuning the threshold y_pred_base = base_classifier.predict(X_test) recall_base = recall_score(y_test, y_pred_base) precision_base = precision_score(y_test, y_pred_base) accuracy_base = accuracy_score(y_test, y_pred_base) print(\\"Before Tuning - Recall: {:.4f}\\".format(recall_base)) print(\\"Before Tuning - Precision: {:.4f}\\".format(precision_base)) print(\\"Before Tuning - Accuracy: {:.4f}\\".format(accuracy_base)) # Step 6: Tune the decision threshold using CalibratedClassifierCV calibrated_classifier = CalibratedClassifierCV(base_classifier, cv=\'prefit\') calibrated_classifier.fit(X_train, y_train) # Finding an optimal threshold to maximize recall y_prob = calibrated_classifier.predict_proba(X_test)[:, 1] thresholds = sorted(set(y_prob)) best_threshold, best_recall = 0, 0 for threshold in thresholds: y_pred_tuned = (y_prob >= threshold).astype(int) recall = recall_score(y_test, y_pred_tuned) if recall > best_recall: best_recall = recall best_threshold = threshold # Step 7: Evaluate the classifier after tuning the threshold y_pred_tuned = (y_prob >= best_threshold).astype(int) recall_tuned = recall_score(y_test, y_pred_tuned) precision_tuned = precision_score(y_test, y_pred_tuned) accuracy_tuned = accuracy_score(y_test, y_pred_tuned) print(\\"After Tuning - Recall: {:.4f}\\".format(recall_tuned)) print(\\"After Tuning - Precision: {:.4f}\\".format(precision_tuned)) print(\\"After Tuning - Accuracy: {:.4f}\\".format(accuracy_tuned))"},{"question":"You are required to implement a function that generates a customized strip plot from a given dataset. The function should also create a multi-faceted plot to visualize the data on different subsets. Function Signature ```python import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def generate_strip_plots(data: pd.DataFrame, strip_params: dict, catplot_params: dict): Generates a strip plot and a faceted plot from the given dataset and parameters. Args: data (pd.DataFrame): The dataset to be visualized. strip_params (dict): Parameters to customize the strip plot. - x (str): The name of the column to be plotted on the x-axis. - y (str, optional): The name of the column to be plotted on the y-axis. If not provided, assumed wide-format. - hue (str, optional): The name of the column to be used for color coding. - palette (str, optional): The color palette to use. - dodge (bool, optional): Whether to separate the strips by hue. - jitter (bool, optional): Whether to apply jittering. - orient (str, optional): \'h\' for horizontal and \'v\' for vertical orientation. catplot_params (dict): Parameters to customize the faceted plot using seaborn\'s catplot. - x (str): The name of the column to be plotted on the x-axis. - y (str): The name of the column to be plotted on the y-axis. - hue (str): The name of the column to be used for color coding. - col (str): The name of the column to create facets. Returns: None ``` Expected Input and Output - Input: - `data`: A pandas DataFrame containing the dataset. - `strip_params`: A dictionary with keys and values specifying the customization of the `stripplot` function. All keys are optional except \'x\'. - `catplot_params`: A dictionary with keys and values specifying the customization of the `catplot` function. - Output: - The function should generate and display a strip plot based on provided parameters. - The function should also generate and display a multi-faceted plot. Constraints - `data` DataFrame will contain at least three columns and at least 20 rows. - Not all parameters in `strip_params` and `catplot_params` will always be provided, handle missing parameters gracefully. - The `stripplot` should be sufficiently customized based on provided parameters. Example ```python # Example dataset data = sns.load_dataset(\\"tips\\") # Parameters for strip plot strip_params = { \'x\': \'total_bill\', \'y\': \'day\', \'hue\': \'sex\', \'palette\': \'deep\', \'dodge\': True, \'jitter\': False, \'orient\': \'h\' } # Parameters for catplot catplot_params = { \'x\': \'time\', \'y\': \'total_bill\', \'hue\': \'sex\', \'col\': \'day\' } # Generate the plots generate_strip_plots(data, strip_params, catplot_params) ``` In this example, the function should generate a horizontal strip plot with the specified parameters and a faceted plot categorizing the data by day, displaying total bills over time and separated by sex.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def generate_strip_plots(data: pd.DataFrame, strip_params: dict, catplot_params: dict): Generates a strip plot and a faceted plot from the given dataset and parameters. Args: data (pd.DataFrame): The dataset to be visualized. strip_params (dict): Parameters to customize the strip plot. - x (str): The name of the column to be plotted on the x-axis. - y (str, optional): The name of the column to be plotted on the y-axis. If not provided, assumed wide-format. - hue (str, optional): The name of the column to be used for color coding. - palette (str, optional): The color palette to use. - dodge (bool, optional): Whether to separate the strips by hue. - jitter (bool, optional): Whether to apply jittering. - orient (str, optional): \'h\' for horizontal and \'v\' for vertical orientation. catplot_params (dict): Parameters to customize the faceted plot using seaborn\'s catplot. - x (str): The name of the column to be plotted on the x-axis. - y (str): The name of the column to be plotted on the y-axis. - hue (str): The name of the column to be used for color coding. - col (str): The name of the column to create facets. Returns: None # Generate the strip plot based on provided parameters plt.figure(figsize=(10, 6)) sns.stripplot(data=data, **strip_params) plt.title(\\"Strip Plot\\") plt.show() # Generate the faceted plot based on provided parameters g = sns.catplot(data=data, kind=\'strip\', **catplot_params) g.fig.subplots_adjust(top=0.9) # Adjust the top to include the title g.fig.suptitle(\\"Faceted Strip Plot\\") plt.show()"},{"question":"**Email Handling with `EmailMessage`** **Objective**: Create a function that analyzes the headers and payload of an email message and returns summarized information. **Function Specification**: Implement a function `analyze_email_message(email_string: str) -> dict` that takes an email message as a string and returns a dictionary with the following keys and their corresponding values: 1. **subject**: The value of the \\"Subject\\" header. If the header is not present, return `None`. 2. **from**: The value of the \\"From\\" header. If the header is not present, return `None`. 3. **to**: The value of the \\"To\\" header. If the header is not present, return `None`. 4. **is_multipart**: A boolean indicating whether the email is multipart. 5. **content_type**: The Content-Type of the email. 6. **body**: The body of the email. If the email is multipart, return the payload of the preferred content type in this order: \'related\', \'html\', \'plain\'. If no such part is found, return `None`. **Constraints**: - You may use the `email` package. - Handle all the cases where headers are not present gracefully by returning `None`. **Example**: ```python email_string = \'\'\'From: sender@example.com To: receiver@example.com Subject: Test Email Content-Type: text/plain; charset=\\"us-ascii\\" Hello, this is a test email.\'\'\' result = analyze_email_message(email_string) # Expected output: # { # \'subject\': \'Test Email\', # \'from\': \'sender@example.com\', # \'to\': \'receiver@example.com\', # \'is_multipart\': False, # \'content_type\': \'text/plain\', # \'body\': \'Hello, this is a test email.\' # } ``` **Implementation**: You are expected to parse the given email string using the `EmailMessage` class, retrieve the required information, and return it in the specified dictionary format. Ensure that your code handles different content types and multipart messages correctly.","solution":"import email from email.message import EmailMessage def analyze_email_message(email_string: str) -> dict: Analyzes the headers and payload of an email message and returns summarized information. message = email.message_from_string(email_string) # Retrieve header values subject = message[\'Subject\'] from_ = message[\'From\'] to = message[\'To\'] # Determine if the email is multipart is_multipart = message.is_multipart() # Get Content-Type content_type = message.get_content_type() # Initialize body as None body = None if is_multipart: for part in message.walk(): if part.get_content_type() == \'text/plain\': body = part.get_payload(decode=True).decode(part.get_content_charset()) break else: body = message.get_payload() return { \\"subject\\": subject, \\"from\\": from_, \\"to\\": to, \\"is_multipart\\": is_multipart, \\"content_type\\": content_type, \\"body\\": body }"},{"question":"# Custom Scikit-learn Estimator Implementation Objective: Implement a custom estimator compatible with the scikit-learn API. This estimator should be able to fit a simple linear regression model and make predictions. You are required to handle input validation, parameter setting, and result storage according to the scikit-learn standards described above. Tasks: 1. Create a Python class named `CustomLinearRegression` that: - Inherits from `BaseEstimator` and `RegressorMixin` (import these from `sklearn.base`). - Contains an `__init__` method that accepts a single parameter `fit_intercept` and initializes it appropriately. - Implements a `fit` method that accepts `X` (features) and `y` (targets) and fits a linear regression model to the data. - Implements a `predict` method that accepts `X` and returns the estimated target values. 2. Ensure your implementation meets the following specifications: - `__init__` should only set attributes and not modify or validate inputs. - `fit` should validate inputs (`X` and `y` should be arrays of appropriate shape). - If `fit_intercept` is set to `True`, the model should include an intercept term. - Store fitted model parameters (coefficients and intercept if applicable) with trailing underscores (e.g., `coef_`, `intercept_`). - Implement the `predict` method to use the learned parameters to compute and return predictions. Example Usage: ```python from sklearn.utils.estimator_checks import check_estimator import numpy as np # Define your estimator class CustomLinearRegression(BaseEstimator, RegressorMixin): def __init__(self, fit_intercept=True): self.fit_intercept = fit_intercept def fit(self, X, y): X, y = validate_data(X, y) if self.fit_intercept: X = np.hstack([np.ones((X.shape[0], 1)), X]) self.coef_ = np.linalg.pinv(X).dot(y) if self.fit_intercept: self.intercept_ = self.coef_[0] self.coef_ = self.coef_[1:] else: self.intercept_ = 0.0 return self def predict(self, X): check_is_fitted(self, [\'coef_\']) X = validate_data(X, reset=False) if self.fit_intercept: return self.intercept_ + X.dot(self.coef_) return X.dot(self.coef_) # Instantiate and test your Custom Estimator X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) y = np.dot(X, np.array([1, 2])) + 3 custom_lr = CustomLinearRegression(fit_intercept=True) custom_lr.fit(X, y) predictions = custom_lr.predict(np.array([[3, 5]])) print(predictions) # Expected output close to [16] # Validate compliance with the scikit-learn estimator API check_estimator(CustomLinearRegression) ``` Constraints: - You must use NumPy for array manipulations. - Ensure your code follows PEP8 guidelines and is well-documented. - Do not use any of scikit-learn\'s built-in regression models for computation. Performance Considerations: - The computational complexity for training (fitting) should be at most `O(n_samples * n_features^2)`. - The prediction method should run in `O(n_samples * n_features)`.","solution":"import numpy as np from sklearn.base import BaseEstimator, RegressorMixin from sklearn.utils.validation import check_is_fitted, check_X_y, check_array class CustomLinearRegression(BaseEstimator, RegressorMixin): def __init__(self, fit_intercept=True): self.fit_intercept = fit_intercept def fit(self, X, y): X, y = check_X_y(X, y) if self.fit_intercept: X = np.hstack([np.ones((X.shape[0], 1)), X]) # Compute the coefficients using the normal equation self.coef_ = np.linalg.pinv(X).dot(y) if self.fit_intercept: self.intercept_ = self.coef_[0] self.coef_ = self.coef_[1:] else: self.intercept_ = 0.0 return self def predict(self, X): check_is_fitted(self, [\'coef_\']) X = check_array(X) if self.fit_intercept: return self.intercept_ + X.dot(self.coef_) return X.dot(self.coef_)"},{"question":"# Coding Assessment: Implementing and Testing a Function with Type Hints Objective Your task is to implement a Python function that performs an operation and to write unit tests for this function using the `unittest` module, including the use of mocks. Problem Statement You need to create a function called `process_data` that takes in a list of dictionaries, processes the data in a specific way, and returns a new list of dictionaries. Function Specification ```python def process_data(data: list[dict[str, int]]) -> list[dict[str, int]]: Processes a list of dictionaries. For each dict in the list, increment each value by 1. Parameters: data (list[dict[str, int]]): A list of dictionaries with string keys and integer values. Returns: list[dict[str, int]]: A new list of dictionaries with processed values. # Your implementation here ``` Constraints 1. The input list will always contain dictionaries with string keys and integer values. 2. Each dictionary can have a different number of key-value pairs. 3. The function should not modify the original input list but return a new list with the processed data. Example ```python input_data = [{\\"a\\": 1, \\"b\\": 2}, {\\"c\\": 3}] output_data = process_data(input_data) # output_data should be [{\\"a\\": 2, \\"b\\": 3}, {\\"c\\": 4}] ``` Requirements 1. **Type Hints**: Your function must use type hints as specified in the function signature. 2. **Unit Tests**: Write unit tests using the `unittest` module to test the `process_data` function. - Include tests for typical cases, edge cases, and invalid inputs. 3. **Mocking**: Use the `unittest.mock` library to mock any external dependencies or services if applicable (in this simple function, you might simulate more complex scenarios involving mocks). Submission - Implement the `process_data` function in a Python script file. - Write your unit tests in a separate Python script file or in the same file using standard practices for `unittest`. Evaluation Criteria - **Correctness**: The function should correctly process the input as per the specification. - **Code Quality**: Use of type hints, clear variable names, and overall readability. - **Testing**: Comprehensive unit tests covering various cases, including the use of mocks where applicable. - **Efficiency**: Although performance is not the main focus, ensure the function handles typical input sizes within reasonable limits.","solution":"def process_data(data: list[dict[str, int]]) -> list[dict[str, int]]: Processes a list of dictionaries. For each dict in the list, increment each value by 1. Parameters: data (list[dict[str, int]]): A list of dictionaries with string keys and integer values. Returns: list[dict[str, int]]: A new list of dictionaries with processed values. return [{key: value + 1 for key, value in d.items()} for d in data]"},{"question":"**Question: Custom Normalization Layer Implementation and Dynamic Switching** As a deep learning practitioner, it is important to know how to handle different normalization techniques in neural networks. For this task, you are required to implement a custom normalization function and dynamically switch between different normalization techniques within a PyTorch model. # Part 1: Custom Normalization Function Implement a custom normalization layer named `CustomNorm`, which normalizes the input tensor using the following formula: [ text{normalized_tensor} = frac{tensor - mean(tensor)}{std(tensor) + epsilon} ] where `mean(tensor)` is the mean of the tensor elements, `std(tensor)` is the standard deviation, and `(epsilon)` is a small constant to avoid division by zero. Input Format - A tensor `x` of shape `(N, C, H, W)`, where: - (N) is the batch size. - (C) is the number of channels. - (H) is the height of the input. - (W) is the width of the input. - A small constant `epsilon` (default value should be `1e-5`). Output Format - A tensor of the same shape as the input `x`, normalized as described. # Part 2: Dynamic Normalization Switching Create a PyTorch neural network model with a single convolutional layer followed by a normalization layer. The normalization layer should be dynamically switchable among `BatchNorm2d`, `GroupNorm`, and the custom `CustomNorm`. Input Format - An integer `norm_type` indicating the normalization type: - 0: Use `BatchNorm2d`. - 1: Use `GroupNorm` (with 32 groups). - 2: Use `CustomNorm`. - A tensor `x` of shape `(N, C, H, W)` for model input. Output Format - The output tensor from the model after applying the convolutional and selected normalization layers. Constraints and Requirements - Ensure that `GroupNorm` is used with a fallback group size of 1 if the number of channels (C) is less than 32. - Use `track_running_stats=False` for `BatchNorm2d`. # Example ```python import torch import torch.nn as nn import torch.nn.functional as F # Part 1: Implement CustomNorm class CustomNorm(nn.Module): def __init__(self, epsilon=1e-5): super(CustomNorm, self).__init__() self.epsilon = epsilon def forward(self, x): mean = x.mean(dim=(0, 2, 3), keepdim=True) std = x.std(dim=(0, 2, 3), keepdim=True) return (x - mean) / (std + self.epsilon) # Part 2: Implement the dynamic switching model class DynamicNormModel(nn.Module): def __init__(self, norm_type, num_channels): super(DynamicNormModel, self).__init__() self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if norm_type == 0: self.norm = nn.BatchNorm2d(num_channels, track_running_stats=False) elif norm_type == 1: num_groups = min(32, num_channels) if num_channels >= 32 else 1 self.norm = nn.GroupNorm(num_groups, num_channels) elif norm_type == 2: self.norm = CustomNorm() else: raise ValueError(\\"Invalid norm_type. Use 0, 1, or 2.\\") def forward(self, x): x = self.conv(x) x = self.norm(x) return x # Example usage model = DynamicNormModel(norm_type=1, num_channels=64) input_tensor = torch.randn(8, 64, 32, 32) output_tensor = model(input_tensor) print(output_tensor.shape) # Should output torch.Size([8, 64, 32, 32]) ``` **Note:** Make sure to test your implementation with various inputs to ensure its correctness and robustness.","solution":"import torch import torch.nn as nn # Part 1: Implement CustomNorm class CustomNorm(nn.Module): def __init__(self, epsilon=1e-5): super(CustomNorm, self).__init__() self.epsilon = epsilon def forward(self, x): mean = x.mean(dim=(0, 2, 3), keepdim=True) std = x.std(dim=(0, 2, 3), keepdim=True) return (x - mean) / (std + self.epsilon) # Part 2: Implement DynamicNormModel class DynamicNormModel(nn.Module): def __init__(self, norm_type, num_channels): super(DynamicNormModel, self).__init__() self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if norm_type == 0: self.norm = nn.BatchNorm2d(num_channels, track_running_stats=False) elif norm_type == 1: num_groups = min(32, num_channels) if num_channels >= 32 else 1 self.norm = nn.GroupNorm(num_groups, num_channels) elif norm_type == 2: self.norm = CustomNorm() else: raise ValueError(\\"Invalid norm_type. Use 0, 1, or 2.\\") def forward(self, x): x = self.conv(x) x = self.norm(x) return x"},{"question":"# Advanced Python Slicing Operations As an experienced Python developer, you are already familiar with creating and using slices in Python at a high level. In this task, you are asked to work directly with low-level slice manipulations to better understand how slicing is managed internally by Python. # Task Write a Python function `custom_slice_indices(sequence: list, slice_obj: slice) -> list` that receives a list `sequence` and a `slice` object `slice_obj`. This function should return a new list containing the elements of the original `sequence` that correspond to the slice specified by the `slice_obj`. # Requirements 1. Do **NOT** use the slicing syntax directly in your function (e.g., `sequence[start:stop:step]`). Instead, use the algorithms described in the provided documentation for `PySlice_Unpack` and `PySlice_AdjustIndices`. 2. Handle cases where the `start`, `stop`, and `step` attributes of the slice object are `None`, which should map to the behavior defined by the Python slicing rules. 3. Ensure that you correctly handle negative indices and that out-of-bound indices are clipped properly as specified by `PySlice_AdjustIndices()`. 4. Use efficient algorithms to ensure the function runs in linear time relative to the length of the sequence. # Input - `sequence`: a list of elements. - `slice_obj`: a `slice` object specifying the start, stop, and step values for slicing the sequence. # Output - A list containing the elements of the original `sequence` that fall within the specified slice. # Example ```python def custom_slice_indices(sequence, slice_obj): # Implement the function here pass # Example usage sequence = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] slice_obj = slice(2, 8, 2) print(custom_slice_indices(sequence, slice_obj)) # Output: [2, 4, 6] slice_obj = slice(None, 5, 1) print(custom_slice_indices(sequence, slice_obj)) # Output: [0, 1, 2, 3, 4] slice_obj = slice(-5, None, 1) print(custom_slice_indices(sequence, slice_obj)) # Output: [5, 6, 7, 8, 9] ``` # Constraints - `sequence` can have up to (10^6) elements. - Elements in the sequence are not restricted to any particular type.","solution":"def custom_slice_indices(sequence, slice_obj): start, stop, step = slice_obj.indices(len(sequence)) result = [] if step > 0: i = start while i < stop: result.append(sequence[i]) i += step else: i = start while i > stop: result.append(sequence[i]) i += step return result"},{"question":"# Coding Challenge: Audio Device Configuration and Playback Objective: Write a function `configure_and_play_audio` that performs the following steps using the `ossaudiodev` module: 1. Opens an audio device for playback. 2. Configures the device with specific audio parameters. 3. Plays a specified audio data onto the device. 4. Handles necessary cleanup and error scenarios. Function: ```python def configure_and_play_audio(device:str, format: int, channels: int, samplerate: int, audio_data: bytes) -> tuple: Configures an OSS compatible audio device and plays the given audio data. Parameters: - device: str - File path to the audio device (e.g., \\"/dev/dsp\\"). - format: int - The audio format to set (e.g., AFMT_S16_LE). - channels: int - Number of channels (1 for mono, 2 for stereo). - samplerate: int - The audio sample rate (e.g., 44100 for CD quality). - audio_data: bytes - The audio data to be played. Returns: - tuple: (configured_format, configured_channels, configured_samplerate) Exceptions: Raises OSSAudioError for OSS-specific errors or OSError for other system call errors. pass ``` Input: - `device` (str): Path to the audio device (e.g., `\\"/dev/dsp\\"`). - `format` (int): Audio format (use constants like `AFMT_S16_LE`). - `channels` (int): Number of audio channels. - `samplerate` (int): Desired sampling rate. - `audio_data` (bytes): The audio data to be played. Output: - Returns a tuple containing the configured format, channels, and samplerate as integers. Constraints and Assumptions: - The function should open the audio device in \'write\' mode. - Use `setparameters` to configure the audio device. - Handle exceptions, ensuring the device is properly closed in case of an error. - You can assume the audio data is well-formed and matches the requested format, channels, and samplerate. Example: ```python audio_data = b\'x00x01x02...\' # Replace with actual audio data bytes result = configure_and_play_audio(\\"/dev/dsp\\", ossaudiodev.AFMT_S16_LE, 2, 44100, audio_data) print(result) # Expected output might be (ossaudiodev.AFMT_S16_LE, 2, 44100) ``` Hints: - Make use of methods like `setparameters` to configure the audio device. - Ensure the device is properly closed to prevent resource leakage. - Use necessary exception handling to manage potential I/O errors.","solution":"import ossaudiodev def configure_and_play_audio(device: str, format: int, channels: int, samplerate: int, audio_data: bytes) -> tuple: Configures an OSS compatible audio device and plays the given audio data. Parameters: - device: str - File path to the audio device (e.g., \\"/dev/dsp\\"). - format: int - The audio format to set (e.g., AFMT_S16_LE). - channels: int - Number of channels (1 for mono, 2 for stereo). - samplerate: int - The audio sample rate (e.g., 44100 for CD quality). - audio_data: bytes - The audio data to be played. Returns: - tuple: (configured_format, configured_channels, configured_samplerate) Exceptions: Raises OSSAudioError for OSS-specific errors or OSError for other system call errors. try: dsp = ossaudiodev.open(device, \'w\') # Set parameters configured_format, configured_channels, configured_samplerate = dsp.setparameters(format, channels, samplerate) # Write audio data to the device dsp.write(audio_data) return configured_format, configured_channels, configured_samplerate except (ossaudiodev.OSSAudioError, OSError) as e: raise e finally: if \'dsp\' in locals(): dsp.close()"},{"question":"**Question: Implementing a Custom Metric Handler in TorchElastic** TorchElastic emits platform-level metrics that by default are discarded (`/dev/null`). In this task, you are required to implement a custom `MetricHandler` that logs these metrics to a file for later analysis. # Requirements: 1. **Metric Handler Implementation**: - Implement a custom `MetricHandler` class that logs metrics to a specified file. - The metrics should be appended to the file in a structured format (JSON or CSV). 2. **Launcher Integration**: - Modify the given launcher code to configure and use your custom `MetricHandler`. # Input: - **Metric File Path**: A string representing the path to the file where metrics will be logged. - **Metrics Data**: Metrics data are generated by TorchElastic during execution (simulated in the provided code for testing). # Output: - Create a file at the given path, containing the logged metrics in a structured format. # Constraints: - Ensure thread-safety when writing to the log file, as TorchElastic may run in a multi-threaded environment. - Preserve the order of metric entries as they are emitted. # Template: ```python import torch.distributed.elastic.metrics as metrics import json import threading class FileMetricHandler(metrics.MetricHandler): def __init__(self, file_path): self.file_path = file_path self.lock = threading.Lock() def emit(self, metric_data: metrics.MetricData): data = { \\"name\\": metric_data.name, \\"value\\": metric_data.value, \\"timestamp\\": metric_data.timestamp, } with self.lock: with open(self.file_path, \'a\') as file: file.write(json.dumps(data) + \'n\') # Modifying the launcher to use the custom metric handler def main(metric_file_path): import torch.distributed.elastic.metrics as metrics from your_existing_launcher_code import WorkerSpec, LocalElasticAgent, parse_args, RendezvousHandler class MyMetricHandler(metrics.MetricHandler): def __init__(self, file_path): self.file_path = file_path self.lock = threading.Lock() def emit(self, metric_data: metrics.MetricData): data = { \\"name\\": metric_data.name, \\"value\\": metric_data.value, \\"timestamp\\": metric_data.timestamp, } with self.lock: with open(self.file_path, \'a\') as file: file.write(json.dumps(data) + \'n\') metrics.configure(MyMetricHandler(metric_file_path)) args = parse_args(sys.argv[1:]) rdzv_handler = RendezvousHandler(...) spec = WorkerSpec( local_world_size=args.nproc_per_node, fn=trainer_entrypoint_fn, args=(trainer_entrypoint_fn args.fn_args,...), rdzv_handler=rdzv_handler, max_restarts=args.max_restarts, monitor_interval=args.monitor_interval, ) agent = LocalElasticAgent(spec, start_method=\\"spawn\\") try: run_result = agent.run() if run_result.is_failed(): print(f\\"worker 0 failed with: {run_result.failures[0]}\\") else: print(f\\"worker 0 return value is: {run_result.return_values[0]}\\") except Exception as ex: print(f\\"An error occurred: {ex}\\") # Example usage if __name__ == \\"__main__\\": main(\\"/path/to/metric_log.txt\\") ``` # Instructions: - Complete the implementation of `FileMetricHandler` or `MyMetricHandler` to properly log metrics to a file. - Modify the launcher script as described to configure the custom metric handler. - Test your implementation by running the modified launcher script and verifying that the metrics are logged correctly.","solution":"import torch.distributed.elastic.metrics as metrics import json import threading class FileMetricHandler(metrics.MetricHandler): def __init__(self, file_path): self.file_path = file_path self.lock = threading.Lock() def emit(self, metric_data: metrics.MetricData): data = { \\"name\\": metric_data.name, \\"value\\": metric_data.value, \\"timestamp\\": metric_data.timestamp, } with self.lock: with open(self.file_path, \'a\') as file: file.write(json.dumps(data) + \'n\') # Modifying the launcher to use the custom metric handler def configure_metric_handler(metric_file_path): metrics.configure(FileMetricHandler(metric_file_path))"},{"question":"Coding Assessment Question # Objective: To assess your understanding of linear models in `scikit-learn`, you are required to implement a machine-learning pipeline to solve a regression problem. Specifically, you will preprocess the data, select a linear model, perform hyperparameter tuning, and evaluate the model\'s performance. # Task: You are provided with a dataset `data.csv` which includes features and a target variable for a regression problem. Your task is to: 1. Preprocess the data (handle missing values, standardize the features). 2. Implement a pipeline that tries at least two different linear models (`LinearRegression` and `Ridge`). 3. Perform hyperparameter tuning using `GridSearchCV` for the Ridge regression model to find the best regularization parameter, alpha. 4. Train the models using the training data. 5. Evaluate the models on the test data and output the performance metrics (RÂ² Score and Mean Squared Error). # Input: - `data.csv`: A CSV file with feature columns and a target column named `target`. # Steps to Follow: 1. **Load the Dataset**: - Read the data from the `data.csv` file. 2. **Data Preprocessing**: - Handle missing values by imputing them with the mean of the respective columns. - Standardize the feature columns. 3. **Model Implementation**: - Implement a machine-learning pipeline that includes: - Preprocessing steps (imputation and standardization). - Linear Regression model. - Ridge Regression model with hyperparameter tuning using `GridSearchCV`. - Set a range of alpha values for Ridge regression: `[0.01, 0.1, 1, 10, 100]`. 4. **Model Training**: - Split the data into training and test sets (80-20 split). - Train both models using the training set. 5. **Model Evaluation**: - Evaluate the performance of both models on the test set. - Calculate and print the RÂ² Score and Mean Squared Error for both models. # Output: - The RÂ² Score and Mean Squared Error for both the Linear Regression and Ridge Regression models on the test set. - The best alpha value for Ridge Regression obtained from `GridSearchCV`. # Example of Expected Output: ``` Linear Regression: RÂ² Score: 0.85 Mean Squared Error: 10.5 Ridge Regression: Best Alpha: 1 RÂ² Score: 0.87 Mean Squared Error: 8.9 ``` # Constraints: - Use `scikit-learn` for model implementation. - Ensure proper handling of missing values and standardization in the preprocessing steps. - Use `GridSearchCV` for hyperparameter tuning. # Performance Requirements: - The models should be evaluated using the RÂ² Score and Mean Squared Error as performance metrics. # Hints: - Use `train_test_split` from `sklearn.model_selection` to split the data. - Use `Pipeline` from `sklearn.pipeline` to create the machine learning pipeline. - Use `SimpleImputer` from `sklearn.impute` for handling missing values. - Use `StandardScaler` from `sklearn.preprocessing` for standardizing the features. - Use `GridSearchCV` from `sklearn.model_selection` for hyperparameter tuning.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression, Ridge from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.metrics import r2_score, mean_squared_error def load_data(file_path): return pd.read_csv(file_path) def preprocess_and_train(data_path): # Load data data = load_data(data_path) # Split data into features and target X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define preprocessing steps preprocessing = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Linear Regression Pipeline linear_pipeline = Pipeline(steps=[ (\'preprocessing\', preprocessing), (\'linear_regression\', LinearRegression()) ]) # Train Linear Regression linear_pipeline.fit(X_train, y_train) # Predict with Linear Regression y_pred_linear = linear_pipeline.predict(X_test) # Calculate metrics for Linear Regression r2_linear = r2_score(y_test, y_pred_linear) mse_linear = mean_squared_error(y_test, y_pred_linear) # Ridge Regression Pipeline with GridSearchCV for hyperparameter tuning ridge_pipeline = Pipeline(steps=[ (\'preprocessing\', preprocessing), (\'ridge\', Ridge()) ]) param_grid = {\'ridge__alpha\': [0.01, 0.1, 1, 10, 100]} grid_search = GridSearchCV(ridge_pipeline, param_grid, cv=5, scoring=\'r2\') # Train Ridge Regression grid_search.fit(X_train, y_train) # Predict with Ridge Regression y_pred_ridge = grid_search.predict(X_test) # Calculate metrics for Ridge Regression r2_ridge = r2_score(y_test, y_pred_ridge) mse_ridge = mean_squared_error(y_test, y_pred_ridge) best_alpha = grid_search.best_params_[\'ridge__alpha\'] return { \'linear\': {\'r2\': r2_linear, \'mse\': mse_linear}, \'ridge\': {\'r2\': r2_ridge, \'mse\': mse_ridge, \'best_alpha\': best_alpha} }"},{"question":"# Pickle Module Usage and Customization **Objective** Create a Python program that performs custom serialization and de-serialization using Python\'s `pickle` module, with specific handling for an external resource and a custom class. # Problem Statement You have a class representing books and an external database storing information about book reviews. Implement custom serialization and deserialization mechanisms using the `pickle` module for the `Book` class, such that: 1. Each `Book` object will include a title and an author. 2. Review data is stored in an external database (you can simulate this with a dictionary for this task). 3. When serializing a `Book` object, you should store a reference to the review data using the `persistent_id` mechanism. 4. When deserializing, you should retrieve the review data from the external database using the `persistent_load` mechanism. # Requirements 1. **Define the `Book` Class**: - Attributes: `title` (str), `author` (str). 2. **Simulate Review Database**: - Use a dictionary to simulate the external review database where the key is a book title and the value is the review (str). 3. **Custom Pickler and Unpickler Classes**: - Define custom `Pickler` and `Unpickler` classes. - Implement the `persistent_id` method in the `Pickler` class, which returns a unique identifier for the review. - Implement the `persistent_load` method in the `Unpickler` class, which retrieves the review data from the external review database. 4. **Serialization and Deserialization**: - Serialize and deserialize a sample `Book` object, ensuring that the book review data is properly stored and retrieved using the external review database. # Input and Output 1. **Input**: - A `Book` object with title \\"The Great Gatsby\\" and author \\"F. Scott Fitzgerald\\". 2. **Output**: - The deserialized `Book` object should include the title, author, and review retrieved from the simulated database. # Constraints - Use pickle protocol 5 for serialization and deserialization. # Example ```python import pickle # Simulating an external review database review_db = { \\"The Great Gatsby\\": \\"A classic piece of literature.\\" } class Book: def __init__(self, title, author): self.title = title self.author = author self.review = None def __repr__(self): return f\\"Book(title={self.title}, author={self.author}, review={self.review})\\" class CustomPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Book): return obj.title return None class CustomUnpickler(pickle.Unpickler): def persistent_load(self, pid): return review_db.get(pid) # Create a Book object book = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\") # Serialize the Book object file = io.BytesIO() pickler = CustomPickler(file, protocol=5) pickler.dump(book) # Modify the external review after serialization review_db[\\"The Great Gatsby\\"] = \\"An updated classic piece of literature.\\" # Deserialize the Book object file.seek(0) unpickler = CustomUnpickler(file) deserialized_book = unpickler.load() # Output the deserialized Book object print(deserialized_book) ``` Ensure that the deserialized `Book` object retrieves the review from the external review database correctly.","solution":"import pickle import io # Simulating an external review database review_db = { \\"The Great Gatsby\\": \\"A classic piece of literature.\\" } class Book: def __init__(self, title, author): self.title = title self.author = author self.review = None def __repr__(self): return f\\"Book(title={self.title}, author={self.author}, review={self.review})\\" class CustomPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Book): return obj.title return None class CustomUnpickler(pickle.Unpickler): def persistent_load(self, pid): book_review = review_db.get(pid) book = Book(pid, None) book.review = book_review return book def serialize_book(book): file = io.BytesIO() pickler = CustomPickler(file, protocol=5) pickler.dump(book) return file def deserialize_book(file): file.seek(0) unpickler = CustomUnpickler(file) deserialized_book = unpickler.load() return deserialized_book # Create a Book object book = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\") # Serialize the Book object serialized_book_file = serialize_book(book) # Modify the external review after serialization review_db[\\"The Great Gatsby\\"] = \\"An updated classic piece of literature.\\" # Deserialize the Book object deserialized_book = deserialize_book(serialized_book_file) # Output the deserialized Book object print(deserialized_book)"},{"question":"**Coding Challenge: Naive Bayes Classifier Comparison** **Objective**: Implement, train, and compare different Naive Bayes classifiers available in `scikit-learn` on the `20 Newsgroups` dataset. # Task 1. **Load the dataset**: Use the `fetch_20newsgroups` function from `sklearn.datasets` to load the dataset, focusing on text data. 2. **Preprocess the data**: - Convert the text data to feature vectors using `CountVectorizer` and `TfidfTransformer`. - Split the data into training and test sets using `train_test_split`. 3. **Train and evaluate classifiers**: - Train at least the following Naive Bayes classifiers: `MultinomialNB`, `ComplementNB`, and `BernoulliNB`. - Calculate and display the accuracy, precision, recall, and F1 score for each classifier. # Requirements - **Input**: None (the dataset is fetched and used within the script). - **Output**: Performance metrics (accuracy, precision, recall, F1 score) for each classifier. # Constraints - Use appropriate sklearn modules for data loading, preprocessing, model training, and evaluation. - Ensure the code is well-structured and readable, with appropriate comments and documentation. # Performance Considerations - Aim for efficient data loading and processing. - Ensure that the evaluation metrics are detailed and clearly presented. # Example Workflow ```python import numpy as np from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load the data newsgroups_data = fetch_20newsgroups(subset=\'all\') # Preprocess the data count_vect = CountVectorizer() X_counts = count_vect.fit_transform(newsgroups_data.data) tfidf_transformer = TfidfTransformer() X_tfidf = tfidf_transformer.fit_transform(X_counts) X_train, X_test, y_train, y_test = train_test_split(X_tfidf, newsgroups_data.target, test_size=0.2, random_state=42) # Define classifiers classifiers = { \\"MultinomialNB\\": MultinomialNB(), \\"ComplementNB\\": ComplementNB(), \\"BernoulliNB\\": BernoulliNB() } # Train and evaluate classifiers for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(f\\"{name} Performance:\\") print(f\\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\\") print(f\\"Precision: {precision_score(y_test, y_pred, average=\'weighted\'):.4f}\\") print(f\\"Recall: {recall_score(y_test, y_pred, average=\'weighted\'):.4f}\\") print(f\\"F1 Score: {f1_score(y_test, y_pred, average=\'weighted\'):.4f}n\\") ``` # Deliverable Submit your code as a Python script or Jupyter notebook that loads, preprocesses, trains, and evaluates the classifiers, accompanied by appropriate outputs showing the performance metrics.","solution":"import numpy as np from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def get_newsgroups_data(): Loads the 20 newsgroups dataset. Returns the text data and the target labels. newsgroups_data = fetch_20newsgroups(subset=\'all\') return newsgroups_data.data, newsgroups_data.target def preprocess_data(data): Converts the text data to TF-IDF feature vectors. count_vect = CountVectorizer() X_counts = count_vect.fit_transform(data) tfidf_transformer = TfidfTransformer() X_tfidf = tfidf_transformer.fit_transform(X_counts) return X_tfidf def evaluate_classifier(clf, X_train, X_test, y_train, y_test): Trains and evaluates the classifier. Returns the accuracy, precision, recall, and F1 score. clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') return accuracy, precision, recall, f1 def main(): # Load and preprocess the data data, target = get_newsgroups_data() X_tfidf = preprocess_data(data) X_train, X_test, y_train, y_test = train_test_split(X_tfidf, target, test_size=0.2, random_state=42) # Define classifiers classifiers = { \\"MultinomialNB\\": MultinomialNB(), \\"ComplementNB\\": ComplementNB(), \\"BernoulliNB\\": BernoulliNB() } # Train and evaluate classifiers for name, clf in classifiers.items(): accuracy, precision, recall, f1 = evaluate_classifier(clf, X_train, X_test, y_train, y_test) print(f\\"{name} Performance:\\") print(f\\"Accuracy: {accuracy:.4f}\\") print(f\\"Precision: {precision:.4f}\\") print(f\\"Recall: {recall:.4f}\\") print(f\\"F1 Score: {f1:.4f}n\\") if __name__ == \\"__main__\\": main()"},{"question":"# Python Coding Assessment: Abstract Syntax Tree (AST) Manipulation Objective This question is designed to assess your understanding and ability to manipulate Python code using the Abstract Syntax Tree (AST) module. Problem Statement You are required to write a function that accepts a string of Python code, parses it into an Abstract Syntax Tree (AST), and then modifies the AST to achieve a specified transformation. The transformed AST should then be converted back to Python code in string format. Requirements: 1. Write a function `transform_function_calls` which takes a single argument: * `source_code` (a string): A piece of Python code. 2. The function should transform all function calls in the `source_code` such that: * Any call to a function named `foo` should be renamed to `bar`. * Any call to a function named `baz` should be renamed to `qux`. 3. The function should return the transformed Python code as a string. Example ```python source_code = \'\'\' def example(): foo(1, 2) result = baz(3) foo(result) \'\'\' transformed_code = transform_function_calls(source_code) print(transformed_code) ``` Output: ```python def example(): bar(1, 2) result = qux(3) bar(result) ``` Constraints: - Assume that function names `foo`, `baz`, `bar`, and `qux` do not appear in other contexts such as variable names or strings within the `source_code`. - The original structure of the code must be preserved except for the required transformations. Hints: - Utilize the `ast` module for parsing, inspecting, and transforming the Python code. - The `ast.walk` function can be helpful for traversing the AST nodes. - You may use the `ast.dump` function for debugging purposes to visualize the AST structure. Implementation You should implement the `transform_function_calls` function in Python: ```python import ast def transform_function_calls(source_code): class FunctionTransformer(ast.NodeTransformer): def visit_Call(self, node): # Check if the function called is \'foo\' or \'baz\' if isinstance(node.func, ast.Name) and node.func.id == \'foo\': node.func.id = \'bar\' elif isinstance(node.func, ast.Name) and node.func.id == \'baz\': node.func.id = \'qux\' # Continue processing the node return self.generic_visit(node) # Parse the source code into an AST tree = ast.parse(source_code) # Transform the AST transformer = FunctionTransformer() transformed_tree = transformer.visit(tree) # Convert the transformed AST back to source code transformed_code = ast.unparse(transformed_tree) return transformed_code # Example usage source_code = \'\'\' def example(): foo(1, 2) result = baz(3) foo(result) \'\'\' transformed_code = transform_function_calls(source_code) print(transformed_code) ``` Notes: - Feel free to add additional helper functions if needed. - Ensure your code handles edge cases where there might be no function calls to transform. Good luck!","solution":"import ast def transform_function_calls(source_code): class FunctionTransformer(ast.NodeTransformer): def visit_Call(self, node): # Check if the function called is \'foo\' or \'baz\' if isinstance(node.func, ast.Name) and node.func.id == \'foo\': node.func.id = \'bar\' elif isinstance(node.func, ast.Name) and node.func.id == \'baz\': node.func.id = \'qux\' # Continue processing the node return self.generic_visit(node) # Parse the source code into an AST tree = ast.parse(source_code) # Transform the AST transformer = FunctionTransformer() transformed_tree = transformer.visit(tree) # Fix the line numbers ast.fix_missing_locations(transformed_tree) # Convert the transformed AST back to source code transformed_code = ast.unparse(transformed_tree) return transformed_code"},{"question":"**Objective**: The goal of this assessment is to evaluate your understanding of TorchScript and your ability to implement a neural network model in PyTorch, annotate it correctly, and script it using TorchScript. **Problem Statement**: You are required to implement a simple Feedforward Neural Network (FNN) class in PyTorch and then convert it to TorchScript. Your implementation should demonstrate appropriate use of type annotations and adhere to TorchScript\'s constraints on types and methods. Implement a class `SimpleFNN` that inherits from `torch.nn.Module`. This class represents a simple feedforward neural network with the following specifications: 1. **Constructor (`__init__`)**: - Takes two arguments: `input_size` (int) and `hidden_size` (int). - Initializes the following layers: - A fully connected layer `fc1` from `input_size` to `hidden_size`. - A ReLU activation `relu`. - A fully connected layer `fc2` from `hidden_size` to `output_size` where `output_size` = 10. 2. **Forward Method (`forward`)**: - Takes a single argument: a tensor `x` of shape (batch_size, input_size). - Passes `x` through `fc1`, `relu`, and `fc2` sequentially. - Returns the output tensor. 3. **Additional Method**: - Implement a method `predict` which takes a list of inputs, converts them into a tensor, passes them through the `forward` method, and returns the predictions. This method should demonstrate the use of TorchScript\'s typing system (e.g., using `List[float]`). 4. **TorchScript Conversion**: - Script the `SimpleFNN` class using `torch.jit.script`. - Define another function `save_script_module` which takes an instance of `SimpleFNN`, scripts it, and saves it to a specified path. **Constraints**: - You must use proper type annotations for all arguments and return types of methods in the `SimpleFNN` class. - Ensure that the class methods are compatible with TorchScript. - Use TorchScript-specific annotations and APIs where necessary. **Input**: - `input_size: int` - `hidden_size: int` - `inputs: List[float]` **Output**: - The final output should be a scripted module saved to a specified file path. **Example Usage**: ```python import torch from typing import List class SimpleFNN(torch.nn.Module): def __init__(self, input_size: int, hidden_size: int): super(SimpleFNN, self).__init__() self.fc1 = torch.nn.Linear(input_size, hidden_size) self.relu = torch.nn.ReLU() self.fc2 = torch.nn.Linear(hidden_size, 10) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x @torch.jit.export def predict(self, inputs: List[float]) -> torch.Tensor: tensor_inputs = torch.tensor(inputs) return self.forward(tensor_inputs) def save_script_module(model: SimpleFNN, path: str): script_model = torch.jit.script(model) script_model.save(path) # Example instantiation and scripting: model = SimpleFNN(input_size=5, hidden_size=10) save_script_module(model, \'simple_fnn_script.pt\') ``` Ensure that your implementation covers the requirements described, and test your resulting scripted module to validate its correctness.","solution":"import torch from typing import List class SimpleFNN(torch.nn.Module): def __init__(self, input_size: int, hidden_size: int): super(SimpleFNN, self).__init__() self.fc1 = torch.nn.Linear(input_size, hidden_size) self.relu = torch.nn.ReLU() self.fc2 = torch.nn.Linear(hidden_size, 10) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x @torch.jit.export def predict(self, inputs: List[float]) -> torch.Tensor: tensor_inputs = torch.tensor(inputs) return self.forward(tensor_inputs) def save_script_module(model: SimpleFNN, path: str): script_model = torch.jit.script(model) script_model.save(path) # Example instantiation and scripting: model = SimpleFNN(input_size=5, hidden_size=10) save_script_module(model, \'simple_fnn_script.pt\')"},{"question":"**Coding Assessment Question** # Question: You are provided with a dataset containing the following columns: - `Category` (str): The category of the item (e.g., \'A\', \'B\', \'C\'). - `Value` (float): The numerical value associated with the category. - `Group` (str): The group to which the category belongs (e.g., \'Group1\', \'Group2\'). Write a function `plot_grouped_barplot` that takes in the dataset as a Pandas DataFrame and creates a grouped barplot using seaborn. Ensure to: 1. Use the \'whitegrid\' theme. 2. Use the \'deep\' color palette. 3. Disable the right and top spines of the plot. 4. Save the plot to a file named \'grouped_barplot.png\'. # Expected Function Signature: ```python import pandas as pd def plot_grouped_barplot(df: pd.DataFrame): pass ``` # Input: - `df`: A Pandas DataFrame with columns `Category`, `Value`, and `Group`. # Output: The function should save the generated plot to the file \'grouped_barplot.png\'. # Example: ```python import pandas as pd data = { \'Category\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Value\': [1.0, 2.5, 3.0, 2.0, 1.8, 2.1], \'Group\': [\'Group1\', \'Group1\', \'Group1\', \'Group2\', \'Group2\', \'Group2\'] } df = pd.DataFrame(data) plot_grouped_barplot(df) # This should save a grouped barplot showing the values of categories A, B, and C, grouped by Group1 and Group2. ``` # Constraints: 1. Assume the DataFrame `df` is properly formatted without any missing values. 2. Focus on readability and proper visualization according to the given specifications. # Additional Information: - You can use seaborn and matplotlib libraries for plotting. - Refer to seaborn documentation to understand how to create grouped barplots and customize themes.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_grouped_barplot(df: pd.DataFrame): Creates and saves a grouped barplot from the provided dataframe. Parameters: df (pd.DataFrame): DataFrame with columns \'Category\', \'Value\', and \'Group\'. # Set the theme sns.set_theme(style=\'whitegrid\') # Create the barplot plt.figure(figsize=(10, 6)) barplot = sns.barplot(data=df, x=\'Category\', y=\'Value\', hue=\'Group\', palette=\'deep\') # Customize the plot barplot.spines[\'right\'].set_visible(False) barplot.spines[\'top\'].set_visible(False) # Save the plot to a file plt.savefig(\'grouped_barplot.png\') plt.close()"},{"question":"# Advanced Serialization with Python\'s Pickle Module **Objective:** You are required to implement and work with custom serialization and deserialization mechanisms using the `pickle` module. This exercise tests your understanding of the underlying concepts of the `pickle` module including creating a customized `Pickler` and `Unpickler` classes with specific behaviors. **Problem Statement:** You are given a class called `SensorData` that represents some data collected from a sensor. The data includes sensitive information that should not be serialized directly for security reasons. Instead, only a safe subset of the attributes should be stored during pickling, and the object should be properly reconstructed with additional attributes being set to default values during unpickling. 1. Implement the `SensorData` class with the following attributes: - `sensor_id` (integer) - `measurement` (float) - `timestamp` (string) - `location` (string, sensitive data that should not be pickled) 2. Implement custom `__getstate__` and `__setstate__` methods for the `SensorData` class to ensure that the `location` attribute is excluded from the pickling process. When unpickling, the `location` attribute should be set to `\'unknown\'`. 3. Create a `MyPickler` class that inherits from `pickle.Pickler` and overrides the `persistent_id` method to handle external object references. 4. Create a `MyUnpickler` class that inherits from `pickle.Unpickler` and overrides the `persistent_load` method to correctly resolve the persistent IDs to actual objects. **Requirements:** 1. Implement a `save_sensor_data` function that takes a `SensorData` object and a file name, and serializes the object using the `MyPickler` class. 2. Implement a `load_sensor_data` function that takes a file name and deserializes the `SensorData` object using the `MyUnpickler` class. 3. Implement comprehensive test cases showing the pickling and unpickling of `SensorData` objects and the handling of the `location` attribute. **Constraints:** - You may assume the `location` attribute is a sensitive string that should not be included in the serialized data. - Ensure the unpickling adequately reconstructs the `SensorData` object while setting the `location` to `\'unknown\'`. **Example:** ```python import pickle class SensorData: def __init__(self, sensor_id, measurement, timestamp, location): self.sensor_id = sensor_id self.measurement = measurement self.timestamp = timestamp self.location = location def __getstate__(self): state = self.__dict__.copy() del state[\'location\'] return state def __setstate__(self, state): self.__dict__.update(state) self.location = \'unknown\' class MyPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, SensorData): return (\'SensorData\', obj.sensor_id) return None class MyUnpickler(pickle.Unpickler): def persistent_load(self, pid): if pid[0] == \'SensorData\': return SensorData(pid[1], 0, \'\', \'unknown\') raise pickle.UnpicklingError(\\"Unknown persistent id\\") def save_sensor_data(sensor_data, file_name): with open(file_name, \'wb\') as file: MyPickler(file).dump(sensor_data) def load_sensor_data(file_name): with open(file_name, \'rb\') as file: return MyUnpickler(file).load() # Example Usage sensor = SensorData(101, 23.5, \'2023-01-01T12:00:00Z\', \'Area 42\') save_sensor_data(sensor, \'sensor_data.pickle\') loaded_sensor = load_sensor_data(\'sensor_data.pickle\') print(loaded_sensor.sensor_id) # 101 print(loaded_sensor.measurement) # 23.5 print(loaded_sensor.timestamp) # \'2023-01-01T12:00:00Z\' print(loaded_sensor.location) # \'unknown\' ``` **Deliverables:** 1. Complete code for the `SensorData`, `MyPickler`, `MyUnpickler` classes. 2. `save_sensor_data` and `load_sensor_data` functions. 3. Tests to validate the pickling and unpickling processes.","solution":"import pickle class SensorData: def __init__(self, sensor_id, measurement, timestamp, location): self.sensor_id = sensor_id self.measurement = measurement self.timestamp = timestamp self.location = location def __getstate__(self): state = self.__dict__.copy() del state[\'location\'] return state def __setstate__(self, state): self.__dict__.update(state) self.location = \'unknown\' class MyPickler(pickle.Pickler): def persistent_id(self, obj): # No specific persistent ID processing is required for this example return None class MyUnpickler(pickle.Unpickler): def persistent_load(self, pid): # No specific persistent load processing is required for this example pass def save_sensor_data(sensor_data, file_name): with open(file_name, \'wb\') as file: MyPickler(file).dump(sensor_data) def load_sensor_data(file_name): with open(file_name, \'rb\') as file: return MyUnpickler(file).load()"},{"question":"**Question: Handling and Processing Unicode Data in Python** You are given the task of developing a Python module with multiple functionalities to handle and process Unicode data, based on the following specifications: # Part 1: Reading and Writing Unicode Data 1. Write a function `read_unicode_file(file_path: str, encoding: str = \'utf-8\') -> str` that reads a file encoded in a specified encoding and returns its content as a Unicode string. 2. Write a function `write_unicode_file(file_path: str, content: str, encoding: str = \'utf-8\') -> None` that writes a Unicode string to a file with a specified encoding. The file should be overwritten if it already exists. # Part 2: Normalizing and Comparing Unicode Strings 3. Write a function `normalize_unicode_string(s: str, form: str = \'NFC\') -> str` that normalizes a Unicode string to one of the forms (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\') using the `unicodedata` module. 4. Write a function `compare_unicode_strings(s1: str, s2: str) -> bool` that compares two Unicode strings for equality after normalizing them to the same form (\'NFC\') and converting them to a case-insensitive form using the `casefold()` method. # Part 3: Handling Encoding Errors 5. Write a function `decode_bytes_with_errors(b: bytes, encoding: str = \'utf-8\', errors: str = \'strict\') -> str` that decodes a bytes object to a Unicode string using the specified encoding and error-handling scheme (\'strict\', \'ignore\', \'replace\', \'backslashreplace\'). # Input and Output Formats - The file path parameters are strings. - The encoding parameter is a string specifying a valid encoding format (e.g., \'utf-8\', \'ascii\', \'latin-1\'). - The content parameter is a Unicode string. - Functions should handle all reasonable edge cases, such as file not found and unsupported encoding formats, by raising appropriate Python exceptions. - Performance requirements: All functions should efficiently handle input sizes of up to 1 million characters or bytes. # Example Usage ```python # Part 1 unicode_content = read_unicode_file(\'example.txt\', encoding=\'utf-8\') write_unicode_file(\'output.txt\', unicode_content, encoding=\'utf-8\') # Part 2 normalized_str = normalize_unicode_string(\'Ã©\', form=\'NFD\') is_equal = compare_unicode_strings(\'eu0301\', \'Ã©\') # Part 3 decoded_str = decode_bytes_with_errors(b\'x80abc\', encoding=\'utf-8\', errors=\'replace\') ``` **Constraints** - Use the `unicodedata` module for normalization functions. - Ensure that all functions handle possible exceptions gracefully and raise meaningful error messages.","solution":"import unicodedata def read_unicode_file(file_path: str, encoding: str = \'utf-8\') -> str: with open(file_path, \'r\', encoding=encoding) as file: return file.read() def write_unicode_file(file_path: str, content: str, encoding: str = \'utf-8\') -> None: with open(file_path, \'w\', encoding=encoding) as file: file.write(content) def normalize_unicode_string(s: str, form: str = \'NFC\') -> str: return unicodedata.normalize(form, s) def compare_unicode_strings(s1: str, s2: str) -> bool: norm_s1 = unicodedata.normalize(\'NFC\', s1).casefold() norm_s2 = unicodedata.normalize(\'NFC\', s2).casefold() return norm_s1 == norm_s2 def decode_bytes_with_errors(b: bytes, encoding: str = \'utf-8\', errors: str = \'strict\') -> str: return b.decode(encoding, errors)"},{"question":"# Advanced File Search and Processing Using the \\"glob\\" module in Python, write a function `find_and_process_files` that searches for files in a given directory based on various patterns and processes these files in a specific way. Function Signature ```python def find_and_process_files(directory: str, pattern: str, recursive: bool = False) -> dict: pass ``` Parameters - `directory` (str): The root directory where the search should start. - `pattern` (str): The search pattern (using wildcards `*`, `?`, or `[]`). - `recursive` (bool, optional): Whether to search recursively. Defaults to `False`. Returns - A dictionary with the following structure: - Keys are file types (extensions, e.g., \'.txt\', \'.jpg\'). - Values are lists of files matching the pattern and the corresponding file count for each type. Behavior 1. The function should search for files in the specified `directory` using the `pattern`. 2. If `recursive` is `True`, the search should include all subdirectories. 3. After finding the files, the function should categorize them by their file extensions and store them in the dictionary. 4. The dictionary should have file types (extensions) as keys and a list of corresponding files as values, along with the file count for each type. Example Suppose you have a directory structure as follows: ``` root/ file1.txt file2.jpg dir1/ file3.txt file4.gif dir2/ file5.png file6.txt ``` Using the function: ```python result = find_and_process_files(\'root\', \'*.txt\', True) print(result) ``` Would produce: ```python { \'.txt\': [\'root/file1.txt\', \'root/dir1/file3.txt\', \'root/dir2/file6.txt\'], } ``` Similarly: ```python result = find_and_process_files(\'root\', \'*.*\', True) print(result) ``` Would produce: ```python { \'.txt\': [\'root/file1.txt\', \'root/dir1/file3.txt\', \'root/dir2/file6.txt\'], \'.jpg\': [\'root/file2.jpg\'], \'.gif\': [\'root/dir1/file4.gif\'], \'.png\': [\'root/dir2/file5.png\'] } ``` Constraints - You must use the `glob` module for file pattern matching. - Handle edge cases where no files match the pattern. - Ensure the function is efficient and handles large directories appropriately. Notes - Pay special attention to files starting with a dot and the corresponding patterns to match them. - Include necessary import statements and any helper functions you might need within your solution.","solution":"import glob import os def find_and_process_files(directory: str, pattern: str, recursive: bool = False) -> dict: Searches for files in a given directory based on the pattern and processes them. Args: directory (str): The root directory where the search should start. pattern (str): The search pattern (using wildcards \'*\', \'?\', or \'[]\'). recursive (bool, optional): Whether to search recursively. Defaults to False. Returns: dict: A dictionary with file extensions as keys and lists of file paths as values. if recursive: search_path = os.path.join(directory, \'**\', pattern) else: search_path = os.path.join(directory, pattern) files = glob.glob(search_path, recursive=recursive) result = {} for file in files: ext = os.path.splitext(file)[1] if ext not in result: result[ext] = [] result[ext].append(file) for ext in result: result[ext] = (result[ext], len(result[ext])) return result"},{"question":"Problem Statement: You are tasked with implementing a function that trains an ensemble learning model using the Random Forest algorithm from the Scikit-learn library. Your function should be able to accept a dataset, train the model, and return both the model and its accuracy on a test set. Function Signature: ```python def train_random_forest(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, n_estimators: int = 100, max_depth: int = None) -> Tuple[RandomForestClassifier, float]: pass ``` Input: - `X_train` (np.ndarray): A 2D array representing the training feature set. - `y_train` (np.ndarray): A 1D array representing the labels for the training set. - `X_test` (np.ndarray): A 2D array representing the testing feature set. - `y_test` (np.ndarray): A 1D array representing the labels for the testing set. - `n_estimators` (int): The number of trees in the Random Forest, default is 100. - `max_depth` (int): The maximum depth of the trees, default is None (nodes are expanded until all leaves are pure). Output: - A tuple containing: 1. `RandomForestClassifier`: The trained Random Forest model. 2. `float`: The accuracy of the model on the test set. Constraints: - You must use the Random Forest implementation from Scikit-learn. - Do not use pre-built functions to split the data; only use `train_test_split` function if necessary for internal splitting. - Ensure the use of appropriate evaluation metrics to determine the accuracy of the model. # Example: ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # Create a random dataset X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train Random Forest model model, accuracy = train_random_forest(X_train, y_train, X_test, y_test, n_estimators=100, max_depth=10) print(f\\"Model: {model}\\") print(f\\"Accuracy: {accuracy}\\") ``` The output should clearly print the Random Forest model along with its accuracy on the test set. Additional Notes: - Consider hyperparameter tuning for better accuracy, which can include other parameters like `max_features`, `min_samples_split`, etc. - Ensure that the Random Forest algorithm is appropriately random by setting `random_state` to ensure reproducible results.","solution":"import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from typing import Tuple def train_random_forest(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, n_estimators: int = 100, max_depth: int = None) -> Tuple[RandomForestClassifier, float]: Trains a Random Forest classifier on the given training data and evaluates it on the test data. Parameters: X_train (np.ndarray): Feature set for training. y_train (np.ndarray): Labels for training. X_test (np.ndarray): Feature set for testing. y_test (np.ndarray): Labels for testing. n_estimators (int): Number of trees in the forest. max_depth (int): The maximum depth of the trees. Returns: Tuple containing: - The trained Random Forest model. - Accuracy of the model on the test data. # Initializing the Random Forest classifier with the given parameters rf_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42) # Training the model rf_model.fit(X_train, y_train) # Making predictions on the test data y_pred = rf_model.predict(X_test) # Calculating the accuracy accuracy = accuracy_score(y_test, y_pred) return rf_model, accuracy"},{"question":"Write a function `evaluate_expression_tree` that takes in a nested data structure (representing an expression tree) and evaluates it according to Python\'s rules for expressions, as described. The expression tree will include a combination of literals, variables, and operators with all appropriate considerations of precedence and associativity. The function should handle: - Basic arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`. - Comparison operators: `>`, `<`, `==`, `!=`, `>=`, `<=`. - Logical operators: `and`, `or`, `not`. - Parentheses to enforce precedence. - List, set, and dictionary comprehensions. # Input: - An expression tree that is a nested list or dict structure representing an expression, where each operator is a node with its operands as children. For example: - `[\\"+\\", 3, 5]` represents the expression `3 + 5`. - `[\\"-\\", [\\"*\\", 2, 3], 1]` represents the expression `(2 * 3) - 1`. - `{ \\"op\\": \\"+\\", \\"left\\": 3, \\"right\\": 5 }` is another representation for `3 + 5`. # Output: - The evaluated result of the expression. # Constraints: - The input expression will be a valid Python expression tree with the described operators and structure. # Example: ```python def evaluate_expression_tree(expression_tree): pass # Implement logic here # Example expression trees and their evaluations: # 3 + 5 expr1 = [\\"+\\", 3, 5] assert evaluate_expression_tree(expr1) == 8 # (2 * 3) - 1 expr2 = [\\"-\\", [\\"*\\", 2, 3], 1] assert evaluate_expression_tree(expr2) == 5 # { \\"op\\": \\"+\\", \\"left\\": 3, \\"right\\": 5 } expr3 = { \\"op\\": \\"+\\", \\"left\\": 3, \\"right\\": 5 } assert evaluate_expression_tree(expr3) == 8 print(\\"All tests passed!\\") ``` # Notes: - You must only use built-in functions and libraries. - Ensure to handle different literals such as integers, floats, and complex numbers. - Properly handle operator precedence and associativity as described in the documentation. - Consider edge cases like division by zero or invalid expressions and handle them gracefully.","solution":"def evaluate_expression_tree(expression_tree): if isinstance(expression_tree, (int, float, complex)): return expression_tree if isinstance(expression_tree, list): op = expression_tree[0] left = evaluate_expression_tree(expression_tree[1]) right = evaluate_expression_tree(expression_tree[2]) elif isinstance(expression_tree, dict): op = expression_tree[\\"op\\"] left = evaluate_expression_tree(expression_tree[\\"left\\"]) right = evaluate_expression_tree(expression_tree[\\"right\\"]) else: raise ValueError(\\"Invalid expression tree format\\") if op == \'+\': return left + right elif op == \'-\': return left - right elif op == \'*\': return left * right elif op == \'/\': return left / right elif op == \'//\': return left // right elif op == \'%\': return left % right elif op == \'**\': return left ** right elif op == \'>\': return left > right elif op == \'<\': return left < right elif op == \'==\': return left == right elif op == \'!=\': return left != right elif op == \'>=\': return left >= right elif op == \'<=\': return left <= right elif op == \'and\': return left and right elif op == \'or\': return left or right else: raise ValueError(f\\"Unsupported operator: {op}\\")"},{"question":"Objective: Implement a Python function that mimics the behavior of converting a string to a floating-point number with detailed error handling and formatting similar to the `PyOS_string_to_double` and `PyOS_double_to_string` functions described in the documentation. Requirements: 1. Implement a function `string_to_double(s: str) -> float` that: - Converts a given string `s` to a float. - If the string is not a valid floating-point number, raises a `ValueError`. - If the number is too large to store in a float, raises an `OverflowError`. - Handles leading and trailing whitespaces appropriately (trims them). 2. Implement a function `double_to_string(val: float, format_code: str, precision: int) -> str` that: - Converts a given float `val` to a string using the specified `format_code` (`\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, or `\'r\'`). - Ensures the string represents the float accurately to the given `precision`. - Throws a `ValueError` if an invalid `format_code` or `precision` is provided. Example: ```python def string_to_double(s: str) -> float: # Your implementation here pass def double_to_string(val: float, format_code: str, precision: int) -> str: # Your implementation here pass # Test Cases assert string_to_double(\\" 123.45 \\") == 123.45 assert string_to_double(\\"1e-10\\") == 1e-10 try: string_to_double(\\"not a number\\") except ValueError: pass try: string_to_double(\\"1e500\\") except OverflowError: pass assert double_to_string(123.456, \'f\', 2) == \\"123.46\\" assert double_to_string(1.23e2, \'e\', 1) == \\"1.2e+02\\" try: double_to_string(123.456, \'x\', 2) except ValueError: pass ``` Constraints: - You must not use any built-in Python functions like `float()` for conversion. - Handle errors and exceptions as specified. - The implementation should be efficient and handle large inputs gracefully. Good luck!","solution":"def string_to_double(s: str) -> float: Converts a string to a floating-point number. Args: s (str): The string to convert. Returns: float: The floating point representation of the string. Raises: ValueError: If the string is not a valid floating point number. OverflowError: If the number is too large to be represented as a float. s = s.strip() # Remove leading and trailing whitespace try: result = float(s) except ValueError: raise ValueError(\\"The provided string is not a valid floating-point number.\\") if result == float(\'inf\') or result == float(\'-inf\'): raise OverflowError(\\"The provided number is too large to be represented as a float.\\") return result def double_to_string(val: float, format_code: str, precision: int) -> str: Converts a float to a string using the specified format code and precision. Args: val (float): The float to convert. format_code (str): The format code to use (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'). precision (int): The precision to use for the conversion. Returns: str: The string representation of the float. Raises: ValueError: If an invalid format code or precision is provided. if format_code not in [\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\']: raise ValueError(\\"Invalid format code provided.\\") if not isinstance(precision, int) or precision < 0: raise ValueError(\\"Precision must be a non-negative integer.\\") if format_code == \'r\': return repr(val) format_spec = f\\".{precision}{format_code}\\" return format(val, format_spec)"},{"question":"Objective: Implement a custom dictionary-like Python class that mimics the behavior of a standard Python dictionary while adhering to specific attribute and item management protocols. Problem Statement: Create a Python class `CustomDict` which supports the following operations: 1. Initialize with a dictionary. 2. Set item: `obj[attr] = value` 3. Get item: `value = obj[attr]` 4. Delete item: `del obj[attr]` 5. Check if attribute exists: `hasattr(obj, attr)` 6. Setting and deleting attributes using standard Python statements. 7. Get length: `len(obj)` 8. Ensure that iteration over the dictionary should be possible both using sync (`iter()`) and async (`aiter()`) methods. Requirements: 1. Initialize the `CustomDict` object such that it can take an initial dictionary during the instantiation. 2. The class should have internal mechanisms to simulate C API functions for attribute and item manipulation, ensuring the following: - Existence check for an attribute. - Retrieving an attribute. - Setting an attribute. - Deleting an attribute. 3. Support both synchronous and asynchronous iteration over the dictionary items. 4. Include error handling where necessary reflecting operations unsupported or errors raised due to invalid operations. Example Usage: ```python data = {\'a\': 1, \'b\': 2, \'c\': 3} obj = CustomDict(data) # Attribute Management print(hasattr(obj, \'a\')) # True print(obj.a) # 1 obj.a = 10 print(obj.a) # 10 del obj.a print(hasattr(obj, \'a\')) # False # Item Management print(obj[\'b\']) # 2 obj[\'b\'] = 20 print(obj[\'b\']) # 20 del obj[\'b\'] print(\'b\' in obj) # False # Length print(len(obj)) # Initially 3, then 2 after deletions # Iteration for key in iter(obj): print(key) # Output remaining keys import asyncio async def async_iterate(): async for key in aiter(obj): print(key) asyncio.run(async_iterate()) # Outputs remaining keys asynchronously ``` Constraints: 1. You may not use built-in `dict` methods directly (e.g., no `__getitem__`, `__setitem__`, etc.). Simulate the behavior mimicking lower-level attribute and item handling as described. 2. Ensure to handle errors gracefully. Evaluation Criteria: 1. Correctness: The implementation meets the specified requirements. 2. Efficiency: The code should handle operations efficiently. 3. Adherence to Python conventions and best practices. 4. Clear and concise error handling. Good Luck!","solution":"class CustomDict: def __init__(self, initial_data=None): self._data = initial_data if initial_data else {} def __getitem__(self, key): if key in self._data: return self._data[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def __setitem__(self, key, value): self._data[key] = value def __delitem__(self, key): if key in self._data: del self._data[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def __getattr__(self, name): if name in self._data: return self._data[name] else: raise AttributeError(f\\"Attribute \'{name}\' not found.\\") def __setattr__(self, name, value): if name in [\'_data\']: super().__setattr__(name, value) else: self._data[name] = value def __delattr__(self, name): if name in self._data: del self._data[name] else: raise AttributeError(f\\"Attribute \'{name}\' not found.\\") def __len__(self): return len(self._data) def __iter__(self): return iter(self._data) async def __aiter__(self): for key in self._data: yield key"},{"question":"You are tasked with implementing a function that performs multi-stage encoding and decoding of binary data using different encoding schemes provided by the `base64` module. The function should take an initial binary input, encode it using Base64, then encode the Base64 result using Base32. Following this, it should decode the double-encoded data back to binary by reversing the process. Implement the function `multi_stage_encode_decode()` which follows these steps: 1. Encode the input binary data using Base64. 2. Encode the result from step 1 using Base32. 3. Decode the Base32 encoded data back to the intermediate Base64 encoded form. 4. Decode the Base64 encoded data back to the original binary data. Your implementation should use the following encoding and decoding functions respectively: - `base64.b64encode()` - `base64.b32encode()` - `base64.b32decode()` - `base64.b64decode()` **Function Signature:** ```python def multi_stage_encode_decode(data: bytes) -> bool: pass ``` **Parameters:** - `data` (bytes): The original binary data to be encoded and decoded. **Returns:** - `bool`: Returns `True` if the decoded data matches the original input data. Otherwise, returns `False`. **Example:** ```python data = b\'This is a test.\' assert multi_stage_encode_decode(data) == True # Example where the decoded data does not match original data_mismatched = b\'This will fail\' mismatched_result = multi_stage_encode_decode(data_mismatched) # mismatched_result should be False, meaning the function caught an issue during the process. ``` **Constraints:** - Use the provided encoding/decoding functions directly from the `base64` module. - Consider performance in terms of handling input data up to a few megabytes in size. - Ensure proper error handling to avoid any potential exceptions during the encoding/decoding process. # Note: - The function should strictly follow the encoding and decoding steps described. - Pay attention to the types of input and output expected by each `base64` function to avoid common pitfalls with bytes and string conversions.","solution":"import base64 def multi_stage_encode_decode(data: bytes) -> bool: try: # Step 1: Encode the input binary data using Base64 b64_encoded = base64.b64encode(data) # Step 2: Encode the result from step 1 using Base32 b32_encoded = base64.b32encode(b64_encoded) # Step 3: Decode the Base32 encoded data back to the intermediate Base64 encoded form b64_decoded = base64.b32decode(b32_encoded) # Step 4: Decode the Base64 encoded data back to the original binary data data_decoded = base64.b64decode(b64_decoded) # Check if the decoded data matches the original input data return data == data_decoded except Exception: return False"},{"question":"# Asynchronous Web Scraper Challenge You have been tasked with writing an asynchronous web scraper using the `asyncio` library. This scraper will retrieve data from multiple websites concurrently to demonstrate your understanding of asynchronous programming with Python. Objective Write a function `async scrape_websites(urls: List[str], timeout: float) -> List[Dict[str, Any]]` that takes a list of URLs and a timeout. The function should fetch the content of each URL concurrently. If a request takes longer than the specified timeout, it should be canceled. The result should be a list of dictionaries, each containing: - `url`: the requested URL. - `status`: \\"success\\" if the content was fetched successfully, otherwise \\"failed\\". - `content_length`: the length of the content fetched, or `None` if the request failed. Input - `urls` (List[str]): A list of URLs to scrape. - `timeout` (float): The maximum time in seconds to wait for each request before it is canceled. Output - List[Dict[str, Any]]: A list of dictionaries with each containing the results for a URL. Example ```python urls = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://invalid-url.com\\" ] timeout = 2.0 result = await scrape_websites(urls, timeout) # result should be something like: # [ # {\'url\': \'https://www.example.com\', \'status\': \'success\', \'content_length\': 1270}, # {\'url\': \'https://www.python.org\', \'status\': \'success\', \'content_length\': 194587}, # {\'url\': \'https://invalid-url.com\', \'status\': \'failed\', \'content_length\': None} # ] ``` Constraints - The function should handle a high number of URLs efficiently using concurrency. - Network failures or invalid URLs should be gracefully handled without crashing the program. Requirements - Use the `asyncio` library. - Implement proper error handling for network requests. - Ensure the function runs efficiently and adheres to the provided timeout constraint. Hint You may find the `aiohttp` library helpful for making asynchronous HTTP requests.","solution":"import asyncio import aiohttp from typing import List, Dict, Any async def fetch(session, url, timeout): try: async with session.get(url, timeout=timeout) as response: content = await response.text() return { \'url\': url, \'status\': \'success\', \'content_length\': len(content) } except (aiohttp.ClientError, asyncio.TimeoutError): return { \'url\': url, \'status\': \'failed\', \'content_length\': None } async def scrape_websites(urls: List[str], timeout: float) -> List[Dict[str, Any]]: async with aiohttp.ClientSession() as session: tasks = [fetch(session, url, timeout) for url in urls] return await asyncio.gather(*tasks)"},{"question":"**Coding Assessment Question:** **Objective:** Create a Python function that takes in a DataFrame and visualizes the distribution of a specific numerical column. The function should support various methods of visualization and allow for conditional plotting using another categorical column. **Function Signature:** ```python import pandas as pd import seaborn as sns def visualize_distribution(data: pd.DataFrame, numerical_col: str, method: str = \'histogram\', conditional_col: str = None, **kwargs) -> None: Visualizes the distribution of a numerical column in the provided DataFrame using seaborn. Parameters: data (pd.DataFrame): Input DataFrame containing the data. numerical_col (str): The name of the numerical column to visualize. method (str): The type of plot to use for visualization (\'histogram\', \'kde\', \'ecdf\'). Default is \'histogram\'. conditional_col (str): The name of the categorical column to condition on. Default is None. **kwargs: Additional keyword arguments to pass to the seaborn plotting function. Returns: None: The function displays the plot but does not return any value. pass ``` **Requirements:** 1. The function should support three methods of visualization specified by the `method` parameter: \'histogram\', \'kde\', and \'ecdf\'. 2. It should be able to condition the plot on another categorical column specified by the `conditional_col` parameter. 3. The function should accept additional keyword arguments (`kwargs`) that are passed to the seaborn plotting function, enabling customization of plots. 4. Handle potential edge cases, such as when the specified columns do not exist in the DataFrame or if the columns are of inappropriate types. **Constraints:** - Use seaborn to generate the plots. - The function should internally handle appropriate error messages if conditions are not met (e.g., columns not found, incorrect data types). **Example Usage:** ```python import seaborn as sns import pandas as pd # Load sample dataset penguins = sns.load_dataset(\\"penguins\\") # Example calls to the function visualize_distribution(data=penguins, numerical_col=\'flipper_length_mm\', method=\'histogram\', conditional_col=\'species\', bins=20) visualize_distribution(data=penguins, numerical_col=\'flipper_length_mm\', method=\'kde\', conditional_col=\'species\', bw_adjust=0.5) visualize_distribution(data=penguins, numerical_col=\'flipper_length_mm\', method=\'ecdf\', conditional_col=\'species\') ``` **Expected Output:** The function should display the appropriate plots based on the provided parameters. For example: - A histogram plot of `flipper_length_mm` with bars colored by `species`. - A KDE plot of `flipper_length_mm` with separate density curves for each `species`. - An ECDF plot of `flipper_length_mm` with separate cumulative distribution curves for each `species`.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_distribution(data: pd.DataFrame, numerical_col: str, method: str = \'histogram\', conditional_col: str = None, **kwargs) -> None: Visualizes the distribution of a numerical column in the provided DataFrame using seaborn. Parameters: data (pd.DataFrame): Input DataFrame containing the data. numerical_col (str): The name of the numerical column to visualize. method (str): The type of plot to use for visualization (\'histogram\', \'kde\', \'ecdf\'). Default is \'histogram\'. conditional_col (str): The name of the categorical column to condition on. Default is None. **kwargs: Additional keyword arguments to pass to the seaborn plotting function. Returns: None: The function displays the plot but does not return any value. # Check if the numerical_col exists and is numerical if numerical_col not in data.columns or not pd.api.types.is_numeric_dtype(data[numerical_col]): raise ValueError(f\\"Column \'{numerical_col}\' does not exist or is not numerical.\\") # Check if the conditional_col exists and is categorical (if provided) if conditional_col and (conditional_col not in data.columns or not pd.api.types.is_categorical_dtype(data[conditional_col])): try: data[conditional_col] = data[conditional_col].astype(\'category\') except Exception: raise ValueError(f\\"Column \'{conditional_col}\' does not exist or can\'t be converted to categorical.\\") plt.figure(figsize=(10, 6)) if method == \'histogram\': sns.histplot(data=data, x=numerical_col, hue=conditional_col, **kwargs) elif method == \'kde\': sns.kdeplot(data=data, x=numerical_col, hue=conditional_col, **kwargs) elif method == \'ecdf\': sns.ecdfplot(data=data, x=numerical_col, hue=conditional_col, **kwargs) else: raise ValueError(f\\"Unsupported method \'{method}\'. Use \'histogram\', \'kde\', or \'ecdf\'.\\") plt.title(f\'{method.capitalize()} of {numerical_col}\') plt.xlabel(numerical_col) plt.ylabel(\'Frequency\' if method == \'histogram\' else \'Density\') plt.show()"},{"question":"# Advanced Python Programming Question: **Objective:** To test your understanding and ability to work with Python\'s `itertools` module to create efficient iterables for data processing and manipulation. **Problem Statement:** You are working on a data processing module for a hypothetical company. Your task is to create a custom iterator using Python\'s `itertools` module. This iterator will exhibit specific behaviors based on different combinatorial and efficiency requirements. Given the detailed nature of your task, you need to implement the function `custom_iterator` which utilizes the `itertools` functions efficiently. **Function Specification:** **Function Name:** `custom_iterator` **Parameters:** 1. `data` (iterable): An iterable of data elements. 2. `operation` (str): A string indicating the operation to be performed. The possible values are: - `\'accumulate\'` - `\'chain\'` - `\'combinations\'` - `\'cycle\'` - `\'groupby\'` 3. `additional_params` (dict): A dictionary containing additional parameters required for the specified operation. Details for each operation: - For `\'accumulate\'`: - `\'func\'` (optional): A binary function (such as `operator.mul`). Default is addition. - `\'initial\'` (optional): An initial value for the accumulation. Default is `None`. - For `\'chain\'`: - `\'iterables\'` (mandatory): A list of iterables to chain together. - For `\'combinations\'`: - `\'r\'` (mandatory): The length of each combination. - For `\'cycle\'`: No additional parameters required. - For `\'groupby\'`: - `\'key\'` (optional): A function to compute the key for each element. Default is `None`. **Return Value:** An iterator based on the specified operation and its parameters. **Constraints:** - Each element in the data should be compatible with the operation specified. - Operations should return iterators and should not convert the entire iterable into lists unless absolutely necessary. **Examples:** ```python data = [1, 2, 3, 4, 5] operation = \'accumulate\' additional_params = {\'func\': operator.mul} iterator = custom_iterator(data, operation, additional_params) print(list(iterator)) # Output: [1, 2, 6, 24, 120] data2 = \'ABC\' operation = \'combinations\' additional_params = {\'r\': 2} iterator2 = custom_iterator(data2, operation, additional_params) print(list(iterator2)) # Output: [(\'A\', \'B\'), (\'A\', \'C\'), (\'B\', \'C\')] ``` **Notes:** - If any invalid operation is specified, raise a `ValueError` with the message \\"Invalid operation specified\\". - You may make use of the `itertools` module functions directly for this task. Implement `custom_iterator` function based on the given specifications.","solution":"import itertools import operator def custom_iterator(data, operation, additional_params): Creates a custom iterator based on the specified operation and parameters. Parameters: - data: An iterable of data elements. - operation: A string indicating the operation to be performed. - additional_params: A dictionary containing additional parameters for the specified operation. Returns: An iterator based on the specified operation and its parameters. if operation == \'accumulate\': func = additional_params.get(\'func\', operator.add) initial = additional_params.get(\'initial\', None) return itertools.accumulate(data, func, initial=initial) elif operation == \'chain\': iterables = additional_params[\'iterables\'] return itertools.chain(data, *iterables) elif operation == \'combinations\': r = additional_params[\'r\'] return itertools.combinations(data, r) elif operation == \'cycle\': return itertools.cycle(data) elif operation == \'groupby\': key = additional_params.get(\'key\', None) return itertools.groupby(data, key) else: raise ValueError(\\"Invalid operation specified\\")"},{"question":"**Problem: Exploring Seaborn Object-Oriented Interface for Data Visualization** **Objective**: Your task is to demonstrate the use of seaborn\'s object-oriented interface for creating informative visualizations from a dataset. Specifically, you will use the capabilities of `seaborn.objects.Plot` to create a bar plot that counts observations grouped by different categories. **Dataset**: You are provided with the \\"tips\\" dataset that contains the following columns: - `total_bill`: the total bill amount - `tip`: the tip amount - `sex`: gender of the person paying - `smoker`: whether the person is a smoker - `day`: day of the week - `time`: time of day - `size`: size of the party **Requirements**: 1. Write a function `visualize_tips_by_day_and_sex` that takes no input parameters. 2. Your function should: - Load the \\"tips\\" dataset using seaborn\'s `load_dataset` function. - Create a bar plot that shows the count of observations for each day of the week, with bars grouped by gender (`sex`). - Use `seaborn.objects.Plot()` for initializing the plot. - Add bars to the plot using `so.Bar()` and count the observations using `so.Count()`. - Group bars by gender using `so.Dodge()`. - Display the plot. **Constraints**: - Use only seaborn functions to create the plot. Do not use matplotlib directly, though seaborn may use matplotlib under the hood. **Expected Output**: The function should display a bar plot where the x-axis represents days of the week, and each day has bars grouped by gender, depicting the count of observations. **Python Function Signature**: ```python def visualize_tips_by_day_and_sex(): pass ``` **Example Usage**: When you call `visualize_tips_by_day_and_sex()`, it should display the plot as described above.","solution":"import seaborn as sns import seaborn.objects as so def visualize_tips_by_day_and_sex(): # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Create a bar plot using seaborn\'s object-oriented interface plot = ( so.Plot(tips, x=\\"day\\", color=\\"sex\\") .add(so.Bar(), so.Count(), so.Dodge()) ) # Display the plot plot.show()"},{"question":"**Custom Sequence Operations and Generators** You are tasked with implementing a custom sequence class in Python that demonstrates your capabilities in handling various Python expression elements, as covered in the documentation. The objective is to create a sequence class that supports element-wise arithmetic operations, generates sub-sequences using comprehensions, and provides a generator for iterating over prime numbers in the sequence. # Class: `CustomSequence` Requirements: 1. **Initialization** - Initialize the class with a list of integers. ```python def __init__(self, elements: List[int]): ``` 2. **Element-wise Addition and Multiplication** - Support element-wise addition and multiplication with another `CustomSequence` instance. ```python def __add__(self, other: \'CustomSequence\') -> \'CustomSequence\': def __mul__(self, other: \'CustomSequence\') -> \'CustomSequence\': ``` 3. **Sub-sequence Generation using Comprehensions** - Generate a sub-sequence consisting of elements that are multiples of a given number using a comprehension. ```python def multiples_of(self, n: int) -> \'CustomSequence\': ``` 4. **Prime Number Generator** - Provide a generator function to yield prime numbers from the sequence. ```python def primes(self) -> Generator[int, None, None]: ``` # Example Usage: ```python # Initialize sequences seq1 = CustomSequence([1, 2, 3, 4, 5]) seq2 = CustomSequence([10, 20, 30, 40, 50]) # Element-wise addition seq3 = seq1 + seq2 # CustomSequence([11, 22, 33, 44, 55]) # Element-wise multiplication seq4 = seq1 * seq2 # CustomSequence([10, 40, 90, 160, 250]) # Sub-sequence generation multiples_of_two = seq1.multiples_of(2) # CustomSequence([2, 4]) # Prime number generator prime_gen = seq1.primes() print(list(prime_gen)) # [2, 3, 5] ``` Constraints: - Assume the input list consists of positive integers only. - Utilize comprehensions and generators where specified. - Handle edge cases, such as empty sequences or sequences with no prime numbers. Implement the `CustomSequence` class with the requirements specified above. Ensure you test your implementation with different examples to validate its correctness.","solution":"from typing import List, Generator class CustomSequence: def __init__(self, elements: List[int]): self.elements = elements def __add__(self, other: \'CustomSequence\') -> \'CustomSequence\': return CustomSequence([a + b for a, b in zip(self.elements, other.elements)]) def __mul__(self, other: \'CustomSequence\') -> \'CustomSequence\': return CustomSequence([a * b for a, b in zip(self.elements, other.elements)]) def multiples_of(self, n: int) -> \'CustomSequence\': return CustomSequence([x for x in self.elements if x % n == 0]) def is_prime(self, number: int) -> bool: if number <= 1: return False for i in range(2, int(number**0.5) + 1): if number % i == 0: return False return True def primes(self) -> Generator[int, None, None]: for element in self.elements: if self.is_prime(element): yield element"},{"question":"# Question: Video Frame Manipulation using PyTorch Tensors Objective: You are given a sequence of video frames (represented as 4-dimensional PyTorch tensors) and you need to perform a series of operations to manipulate these frames. Specifically, you need to: 1. Convert the frames to grayscale. 2. Apply a specific transformation to the grayscale frames. 3. Calculate the average pixel value of each frame. 4. Perform backpropagation on the transformation to obtain gradients for a given loss function. Problem Statement: 1. **Data Types**: - The input tensor representing video frames has a shape of `(num_frames, height, width, channels)` and a data type of `torch.float32`. - The output tensor should be the same shape after the transformation, but as grayscale. 2. **Step-by-Step Requirements**: a. **Convert to Grayscale**: - Implement a function `convert_to_grayscale` that takes an input tensor of shape `(num_frames, height, width, channels)` and converts each frame to grayscale. - Use the formula for conversion: `Y = 0.299R + 0.587G + 0.114B` b. **Transformation**: - Implement a function `transform_frames` to apply a mathematical transformation to the grayscale frames. For simplicity, use a transformation such as scaling pixel values by a constant factor. c. **Average Pixel Value**: - Implement a function `average_pixel_value` that computes the average pixel value of each grayscale frame. d. **Backpropagation**: - Implement a function `compute_gradients` that computes gradients with respect to a simple loss function (Mean Squared Error) and returns the gradients. Function Signatures: ```python import torch def convert_to_grayscale(video_frames: torch.Tensor) -> torch.Tensor: Converts a batch of RGB video frames to grayscale. Args: - video_frames (torch.Tensor): Input tensor of shape (num_frames, height, width, channels). Returns: - torch.Tensor: Grayscale frames of shape (num_frames, height, width). pass def transform_frames(grayscale_frames: torch.Tensor, scale_factor: float) -> torch.Tensor: Applies a transformation to grayscale frames. Args: - grayscale_frames (torch.Tensor): Input tensor of shape (num_frames, height, width) representing grayscale video frames. - scale_factor (float): A scaling factor for the transformation. Returns: - torch.Tensor: Transformed tensor of the same shape as input. pass def average_pixel_value(grayscale_frames: torch.Tensor) -> torch.Tensor: Computes the average pixel value of each grayscale frame. Args: - grayscale_frames (torch.Tensor): Input tensor of shape (num_frames, height, width). Returns: - torch.Tensor: Tensor of shape (num_frames,) containing average pixel values. pass def compute_gradients(transformed_frames: torch.Tensor, target_frames: torch.Tensor) -> torch.Tensor: Computes gradients of the Mean Squared Error loss with respect to the transformed frames. Args: - transformed_frames (torch.Tensor): Tensor of shape (num_frames, height, width) representing the transformed frames. - target_frames (torch.Tensor): Tensor of the same shape representing the target frames. Returns: - torch.Tensor: Gradients of the loss with respect to the transformed frames. pass ``` Notes: - You can assume `channels` in `video_frames` is always 3 (for RGB). - Use appropriate tensor methods provided in the documentation for operations like mean calculation and backpropagation. - Proper gradients computation involves setting `requires_grad=True` for tensors involved in the computations. - You are encouraged to write additional helper functions as needed. Constraints: - Your solution should be able to handle large tensors efficiently. - Ensure all operations respect PyTorch\'s best practices regarding tensor operations and in-place modifications.","solution":"import torch def convert_to_grayscale(video_frames: torch.Tensor) -> torch.Tensor: Converts a batch of RGB video frames to grayscale. Args: - video_frames (torch.Tensor): Input tensor of shape (num_frames, height, width, channels). Returns: - torch.Tensor: Grayscale frames of shape (num_frames, height, width). # Ensure input tensor is of shape (num_frames, height, width, channels) with 3 channels assert video_frames.shape[-1] == 3, \\"The input tensor must have 3 channels (RGB).\\" # Convert to grayscale using the weighted sum of RGB grayscale_frames = 0.299 * video_frames[..., 0] + 0.587 * video_frames[..., 1] + 0.114 * video_frames[..., 2] return grayscale_frames def transform_frames(grayscale_frames: torch.Tensor, scale_factor: float) -> torch.Tensor: Applies a transformation to grayscale frames. Args: - grayscale_frames (torch.Tensor): Input tensor of shape (num_frames, height, width) representing grayscale video frames. - scale_factor (float): A scaling factor for the transformation. Returns: - torch.Tensor: Transformed tensor of the same shape as input. return grayscale_frames * scale_factor def average_pixel_value(grayscale_frames: torch.Tensor) -> torch.Tensor: Computes the average pixel value of each grayscale frame. Args: - grayscale_frames (torch.Tensor): Input tensor of shape (num_frames, height, width). Returns: - torch.Tensor: Tensor of shape (num_frames,) containing average pixel values. return grayscale_frames.mean(dim=(1, 2)) def compute_gradients(transformed_frames: torch.Tensor, target_frames: torch.Tensor) -> torch.Tensor: Computes gradients of the Mean Squared Error loss with respect to the transformed frames. Args: - transformed_frames (torch.Tensor): Tensor of shape (num_frames, height, width) representing the transformed frames. - target_frames (torch.Tensor): Tensor of the same shape representing the target frames. Returns: - torch.Tensor: Gradients of the loss with respect to the transformed frames. # Ensure the tensors require gradients transformed_frames.requires_grad_(True) # Compute the loss loss = torch.nn.functional.mse_loss(transformed_frames, target_frames) # Backpropagate to compute gradients loss.backward() # Extract the gradients gradients = transformed_frames.grad return gradients"},{"question":"# Exception Handling and Debugging with `cgitb` In this coding exercise, you will demonstrate your knowledge of exception handling and debugging in Python by using the `cgitb` module. Though `cgitb` is deprecated in Python versions after 3.11, this exercise will help you understand legacy code and some advanced debugging techniques. Requirements: 1. Write a Python function named `compute_average` that takes a list of numbers as input and returns their average. Ensure your function includes proper error handling to manage different types of exceptions that may occur (e.g., passing a non-list, an empty list, a list with non-numeric values). 2. Use the `cgitb` module to provide detailed error reports in case of an uncaught exception. Customize the `cgitb` settings to format the output as plain text and write the logs to a directory named `error_logs`. 3. Implement the main block of your script to: - Call the `compute_average` function with a correct list of numbers. - Call the `compute_average` function with incorrect inputs to trigger exceptions and demonstrate the detailed error reporting by `cgitb`. Implementation Details: - Use the `cgitb.enable()` function with the following parameters: - `display=0`: Suppress sending the traceback to the browser. - `logdir=\\"error_logs\\"`: Write the traceback reports to the `error_logs` directory. - `context=3`: Display 3 lines of context around the current line of source code in the traceback. - `format=\'text\'`: Output the error in plain text format. # Example: ```python import cgitb import sys # Customize cgitb settings cgitb.enable(display=0, logdir=\\"error_logs\\", context=3, format=\'text\') def compute_average(numbers): Function to compute the average of a list of numbers. try: if not isinstance(numbers, list): # Check if input is a list raise TypeError(\\"Input should be a list\\") if len(numbers) == 0: # Check if the list is empty raise ValueError(\\"List is empty\\") for num in numbers: # Check if all elements are numeric if not isinstance(num, (int, float)): raise TypeError(\\"All elements in the list should be numbers\\") return sum(numbers) / len(numbers) # Return the average except: cgitb.handler(sys.exc_info()) # Handle exception with cgitb # Main block if __name__ == \\"__main__\\": compute_average([1, 2, 3, 4, 5]) # Should work fine compute_average(\\"not a list\\") # Should raise TypeError and log to file compute_average([]) # Should raise ValueError and log to file compute_average([1, \'a\', 3]) # Should raise TypeError and log to file ``` # Expected Input and Output Formats: - Input: A list of numeric values (or other types to trigger errors). - Output: The average of the list if the input is valid, otherwise an error log file in `error_logs` directory. **Constraints:** - Do not use external libraries for error handling. - Ensure the script is self-contained.","solution":"import cgitb import sys import os # Customize cgitb settings cgitb.enable(display=0, logdir=\\"error_logs\\", context=3, format=\'text\') # Create the error_logs directory if it does not exist if not os.path.exists(\\"error_logs\\"): os.makedirs(\\"error_logs\\") def compute_average(numbers): Function to compute the average of a list of numbers. try: if not isinstance(numbers, list): # Check if input is a list raise TypeError(\\"Input should be a list\\") if len(numbers) == 0: # Check if the list is empty raise ValueError(\\"List is empty\\") for num in numbers: # Check if all elements are numeric if not isinstance(num, (int, float)): raise TypeError(\\"All elements in the list should be numbers\\") return sum(numbers) / len(numbers) # Return the average except: cgitb.handler(sys.exc_info()) # Handle exception with cgitb return None # Main block if __name__ == \\"__main__\\": print(compute_average([1, 2, 3, 4, 5])) # Should return the average print(compute_average(\\"not a list\\")) # Should raise TypeError and log to file print(compute_average([])) # Should raise ValueError and log to file print(compute_average([1, \'a\', 3])) # Should raise TypeError and log to file"},{"question":"# Advanced Python Programming - Coding Assessment Objective: Implement a function using the `threading` module to download multiple files concurrently and perform minimal logging of the download progress. Problem Statement: You are required to write a function `download_files_concurrently` that takes a list of file URLs and downloads them to a specified directory concurrently using multiple threads. The function should also log the start and completion time of each download using the `logging` module. Function Signature: ```python def download_files_concurrently(file_urls: list, dest_dir: str): pass ``` Input: * `file_urls`: A list of strings, where each string represents a URL of a file to be downloaded. Assume the URL strings are valid and accessible. * `dest_dir`: A string representing the directory path where the files should be downloaded. Output: * There is no return value for this function. The files should be downloaded to the specified `dest_dir`, and appropriate logs should be written. Constraints: 1. You should use the `threading` module to manage concurrent downloads. 2. Logging should be implemented using Python\'s `logging` module to log the start and completion time of each download. 3. Ensure that your solution handles exceptions properly, logging any errors encountered during the download process. 4. You may use any standard Python library for the actual downloading of files (e.g., `requests`). Example Usage: ```python file_urls = [ \'https://example.com/file1.zip\', \'https://example.com/file2.zip\', \'https://example.com/file3.zip\' ] dest_dir = \'/path/to/download\' download_files_concurrently(file_urls, dest_dir) ``` Example Logs: The logging output should indicate when each file starts and completes downloading. For example: ``` INFO: Starting download of https://example.com/file1.zip INFO: Completed download of https://example.com/file1.zip INFO: Starting download of https://example.com/file2.zip INFO: Completed download of https://example.com/file2.zip ERROR: Failed to download https://example.com/file3.zip due to <error_message> ``` Notes: - To simulate the file download, consider using `time.sleep()` to mimic the time taken for downloading. - Make sure to structure your code to handle multiple threads cleanly and ensure all threads complete their execution before the function returns.","solution":"import threading import logging import os import requests from urllib.parse import urlparse def download_file(url, dest_dir): try: file_name = os.path.basename(urlparse(url).path) file_path = os.path.join(dest_dir, file_name) logging.info(f\\"Starting download of {url}\\") response = requests.get(url) response.raise_for_status() # Raise an HTTPError for bad responses with open(file_path, \'wb\') as file: file.write(response.content) logging.info(f\\"Completed download of {url}\\") except Exception as e: logging.error(f\\"Failed to download {url} due to {str(e)}\\") def download_files_concurrently(file_urls, dest_dir): if not os.path.exists(dest_dir): os.makedirs(dest_dir) threads = [] for url in file_urls: thread = threading.Thread(target=download_file, args=(url, dest_dir)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"# Seaborn Clustermap Implementation You are required to create a clustered heatmap using the `seaborn` library, a visualization tool in Python. Your task is to load a dataset, preprocess this data, and visualize it with specific clustering parameters. **Task:** 1. Load the `titanic` dataset using the `seaborn`\'s `load_dataset` function. 2. Perform the following preprocessing steps: - Drop the rows where any of the values are missing (NaN). - Select the following columns for clustering: `age`, `fare`, `sibsp`, `parch`. 3. Implement the clustering with a heatmap using `seaborn`\'s `clustermap`. Apply the following customizations: - Use `\'euclidean\'` as the metric for clustering. - Use `\'average\'` as the method for clustering. - Standardize the data within columns. - Set the colormap to `\'coolwarm\'`. - Adjust the limits of the color scale to have a minimum of 0 and a maximum of 80. - Change the size of the figure to 10x8 inches. 4. Add row colors to categorize observations by the `sex` of the passengers. **Constraints:** - Ensure the `seaborn` and `pandas` packages are properly imported in your code. - The function should not return any values but should display the clustermap directly. **Expected Function Signature:** ```python import seaborn as sns import pandas as pd def create_titanic_clustermap(): # Implement your solution here ``` **Example Usage:** Upon running `create_titanic_clustermap()`, a clustermap should be displayed with all the specified customizations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_titanic_clustermap(): # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Drop rows with any missing values titanic_clean = titanic.dropna() # Select columns for clustering clustering_data = titanic_clean[[\'age\', \'fare\', \'sibsp\', \'parch\']] # Create row colors based on \'sex\' row_colors = titanic_clean[\'sex\'].map({\'male\': \'blue\', \'female\': \'red\'}) # Create the clustermap with specified customizations sns.clustermap(clustering_data, metric=\'euclidean\', method=\'average\', standard_scale=1, cmap=\'coolwarm\', vmin=0, vmax=80, figsize=(10, 8), row_colors=row_colors) # Display the plot plt.show()"},{"question":"# Seaborn Advanced Annotation Task **Objective:** Demonstrate your understanding of seaborn\'s advanced plotting techniques, including data processing, plot customization, and text annotation. **Task:** Using the `seaborn` library, create a plot from the provided `titanic` dataset that fulfills the following requirements: 1. **Data Loading and Processing:** - Load the `titanic` dataset using `seaborn.load_dataset(\\"titanic\\")`. - Filter the dataset to include only passengers who embarked from \\"Southampton\\" (port \\"S\\"). 2. **Plot Creation:** - Create a bar plot showing the average age of passengers for each class (`pclass`). - Use text annotations to label each bar with its corresponding average age. - Align the text annotations to the right and slightly offset them from the bars. 3. **Customization:** - Color the bars based on the `sex` of the passengers. - Ensure that the text annotations are in white and bold font. 4. **Advanced Customization:** - Add a title to the plot: \\"Average Age of Titanic Passengers by Class and Sex\\". - Add x and y axis labels: \\"Passenger Class\\" for x-axis and \\"Average Age\\" for y-axis. - Set the background style to \\"whitegrid\\". **Constraints:** - Use appropriate seaborn functions and methods to fulfill the requirements. - Ensure that the plot is clear and well-labeled. **Input Format:** There is no input format for this task since the dataset `titanic` is loaded directly within the code. **Output:** The output should be a visualization plotted according to the specified requirements. **Code Template:** ```python import seaborn as sns import seaborn.objects as so # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Filter the dataset for passengers who embarked from Southampton (port \\"S\\") filtered_titanic = titanic[titanic[\'embarked\'] == \'S\'] # Create the plot average_age_plot = ( so.Plot(filtered_titanic, x=\\"pclass\\", y=\\"age\\", color=\\"sex\\") .add(so.Bar(), so.Agg()) .add(so.Text(color=\\"white\\", fontweight=\\"bold\\", halign=\\"right\\", offset=6), so.Agg()) ) # Customize the plot average_age_plot.title(\\"Average Age of Titanic Passengers by Class and Sex\\") average_age_plot.xlabel(\\"Passenger Class\\") average_age_plot.ylabel(\\"Average Age\\") average_age_plot.theme(\\"whitegrid\\") # Render the plot average_age_plot.show() ``` Ensure your implementation generates the required plot effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_ages(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Filter the dataset for passengers who embarked from Southampton (port \\"S\\") filtered_titanic = titanic[titanic[\'embarked\'] == \'S\'] # Create the bar plot plt.figure(figsize=(10, 6)) barplot = sns.barplot(data=filtered_titanic, x=\'pclass\', y=\'age\', hue=\'sex\', ci=None) # Calculate and annotate the average age on each bar pclass_sex_groups = filtered_titanic.groupby([\'pclass\', \'sex\'])[\'age\'].mean().reset_index() for index, row in pclass_sex_groups.iterrows(): bar = barplot.patches[index] bar_height = bar.get_height() barplot.annotate( f\'{row[\\"age\\"]:.1f}\', (bar.get_x() + bar.get_width() / 2, bar_height), ha=\'center\', va=\'center\', fontsize=10, color=\'white\', fontweight=\'bold\', xytext=(0, 5), textcoords=\'offset points\' ) # Customize the plot plt.title(\\"Average Age of Titanic Passengers by Class and Sex\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Average Age\\") plt.legend(title=\'Sex\') sns.set_style(\\"whitegrid\\") # Show the plot plt.show() # Call the function to create the plot plot_titanic_ages()"},{"question":"Objective: Assess students\' understanding of the `platform` module\'s functions to gather various pieces of system information and summarize them. Problem Statement: Write a Python function `collect_system_info()` that utilizes the `platform` module to gather the following information about the system: 1. **System architecture**: Use `platform.architecture()`. 2. **Machine type**: Use `platform.machine()`. 3. **Network name**: Use `platform.node()`. 4. **Detailed platform description**: Use `platform.platform()`. 5. **Processor name**: Use `platform.processor()`. 6. **Python build number and date**: Use `platform.python_build()`. 7. **Python compiler**: Use `platform.python_compiler()`. 8. **Python implementation**: Use `platform.python_implementation()`. 9. **Python version**: Use `platform.python_version()`. 10. **System release**: Use `platform.release()`. 11. **System OS name**: Use `platform.system()`. 12. **System version**: Use `platform.version()`. 13. **System uname details**: Use `platform.uname()`. The function should return a dictionary with the above pieces of information. If any value is not available, the corresponding dictionary entry should be \\"Unavailable\\". Function Signature: ```python def collect_system_info() -> dict: pass ``` Example Output: ```python { \\"architecture\\": (\\"64bit\\", \\"ELF\\"), \\"machine\\": \\"x86_64\\", \\"node\\": \\"my_computer_name\\", \\"platform\\": \\"Linux-5.4.0-74-generic-x86_64-with-Ubuntu-18.04-bionic\\", \\"processor\\": \\"x86_64\\", \\"python_build\\": (\\"default\\", \\"Jun 30 2021 13:22:56\\"), \\"python_compiler\\": \\"GCC 7.5.0\\", \\"python_implementation\\": \\"CPython\\", \\"python_version\\": \\"3.10.4\\", \\"release\\": \\"5.4.0-74-generic\\", \\"system\\": \\"Linux\\", \\"version\\": \\"#83-Ubuntu SMP Wed Jun 2 09:30:16 UTC 2021\\", \\"uname\\": uname_result(system=\'Linux\', node=\'my_computer_name\', release=\'5.4.0-74-generic\', version=\'#83-Ubuntu SMP Wed Jun 2 09:30:16 UTC 2021\', machine=\'x86_64\', processor=\'x86_64\') } ``` Constraints: - You must use the functions from the `platform` module as specified. - Ensure your solution handles exceptions gracefully and provides \\"Unavailable\\" for any information that cannot be determined. - Assume that the platform module is available and can be imported. Evaluation Criteria: - Correctness: The function should accurately gather and return the system information as specified. - Exception Handling: The function should handle cases where information cannot be retrieved. - Code Structure: The code should be clean, well-structured, and follow best practices.","solution":"import platform def collect_system_info() -> dict: Collects and returns system information using the platform module. try: system_info = { \\"architecture\\": platform.architecture(), \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build\\": platform.python_build(), \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_version\\": platform.python_version(), \\"release\\": platform.release(), \\"system\\": platform.system(), \\"version\\": platform.version(), \\"uname\\": platform.uname() } except Exception: # If any of the above functions raise an exception, mark the corresponding field as \\"Unavailable\\" system_info = {key: \\"Unavailable\\" for key in [ \\"architecture\\", \\"machine\\", \\"node\\", \\"platform\\", \\"processor\\", \\"python_build\\", \\"python_compiler\\", \\"python_implementation\\", \\"python_version\\", \\"release\\", \\"system\\", \\"version\\", \\"uname\\" ]} return system_info"},{"question":"**Floating Point Precision Correction** In most computer systems, floating-point numbers are represented using IEEE-754 format, which can lead to precision errors in arithmetic operations involving those numbers. Write a Python function `correct_sum` that accurately sums a list of floating-point numbers by minimizing the accumulated precision error. Function Signature ```python def correct_sum(floats: list[float]) -> float: pass ``` # Input - `floats`: A list of floating-point numbers, where `1 <= len(floats) <= 10^6` and `-10^6 <= floats[i] <= 10^6` for each `i`. # Output - Returns the sum of the floating-point numbers with minimized precision error, using Python\'s `math.fsum` function for improved accuracy. # Example ```python >>> correct_sum([0.1, 0.1, 0.1]) 0.30000000000000004 >>> correct_sum([0.1, 0.2, 0.3]) 0.6000000000000001 >>> correct_sum([1e-10, 1e-10, 1e-10]) 3e-10 ``` # Constraints - The function should handle large input sizes efficiently. - The function should make use of Pythonâ€™s `math.fsum` for summation to handle floating-point precision. # For More Considerations: - You might want to include additional steps to validate the input list to check if all elements are floats. **Hint:** Utilize the `math.fsum` function from the `math` module which is more accurate for summing floating-point numbers. Happy coding!","solution":"import math from typing import List def correct_sum(floats: List[float]) -> float: Sums a list of floating-point numbers using math.fsum for improved accuracy. Args: floats (List[float]): A list of floating-point numbers to be summed. Returns: float: The correctly summed result. return math.fsum(floats)"},{"question":"You are required to write a function `process_transactions(transactions: list[str]) -> str` that processes a series of transaction strings containing item purchases and their amounts. The task is to determine the total amount spent and categorize the transactions based on the day of the week. Each transaction string is formatted as: \\"day,item,amount\\", where: - `day` is a three-letter abbreviation of the day of the week (e.g., Mon, Tue, Wed, etc.) - `item` is the name of the purchased item. - `amount` is the amount spent on the item, represented as a float or integer. The function should: 1. Calculate the total amount spent across all transactions. 2. Categorize the transactions by the day of the week. The function should return a summary string in the following format: ``` \\"Total spent: <total_amount>n[Monday]: <items>n[Tuesday]: <items>n...\\" ``` Where `<total_amount>` is the total sum of amounts spent, and for each day of the week, `<items>` should list the items purchased in the format `\\"item1: amount1, item2: amount2, ...\\"`. If no transactions occurred on a particular day, it should display \\"None\\". # Input - A list of transaction strings `transactions` (1 <= len(transactions) <= 100). - Each transaction string is in the format `\\"day,item,amount\\"` # Output - A string containing the total amount spent and transactions categorized by their day of the week. # Constraints - The day abbreviation will always be valid. - The amount is always a positive numerical value. # Example ```python transactions = [ \\"Mon,apple,1.2\\", \\"Tue,banana,0.5\\", \\"Mon,orange,0.8\\" ] result = process_transactions(transactions) print(result) ``` Expected Output: ``` \\"Total spent: 2.5n[Monday]: apple: 1.2, orange: 0.8n[Tuesday]: banana: 0.5n[Wednesday]: Nonen[Thursday]: Nonen[Friday]: Nonen[Saturday]: Nonen[Sunday]: None\\" ``` # Notes - Leverage \\"match\\" statements introduced in Python 3.10 for categorizing the days of the week. - Utilize appropriate Python data structures to compile and format the results. - Ensure your solution is efficient given the constraints and handles edge cases, such as missing transaction days gracefully.","solution":"def process_transactions(transactions): Processes a list of transaction strings and calculates the total amount spent, while categorizing the transactions by the day of the week. Args: transactions (list of str): List of transactions formatted as \\"day,item,amount\\" Returns: str: Summary of total amount spent and transactions categorized by day of the week. from collections import defaultdict # Dictionary to map day abbreviations to full name day_map = { \\"Mon\\": \\"Monday\\", \\"Tue\\": \\"Tuesday\\", \\"Wed\\": \\"Wednesday\\", \\"Thu\\": \\"Thursday\\", \\"Fri\\": \\"Friday\\", \\"Sat\\": \\"Saturday\\", \\"Sun\\": \\"Sunday\\", } # Initialize necessary variables total_amount = 0.0 days_summary = defaultdict(list) # Process each transaction for transaction in transactions: day, item, amount = transaction.split(\',\') amount = float(amount) total_amount += amount days_summary[day_map[day]].append(f\\"{item}: {amount}\\") # Generate summary string result = [f\\"Total spent: {total_amount:.1f}\\"] for day in [\\"Monday\\", \\"Tuesday\\", \\"Wednesday\\", \\"Thursday\\", \\"Friday\\", \\"Saturday\\", \\"Sunday\\"]: if days_summary[day]: result.append(f\\"[{day}]: \\" + \\", \\".join(days_summary[day])) else: result.append(f\\"[{day}]: None\\") return \\"n\\".join(result)"},{"question":"# Question: Efficient Data Handling with Pandas You are provided with a large dataset containing time series data (columns include timestamp, id, name, x, y) stored in Parquet files. The dataset is structured in a directory, with each file representing data for different years. Your task is to implement a function that: 1. Loads only specific columns from all files in the directory. 2. Converts the \'name\' column to a Categorical type to reduce memory usage. 3. Downcasts the \'id\', \'x\', and \'y\' columns to their most efficient numeric types. 4. Calculates the memory usage before and after these optimizations. 5. Returns a DataFrame with optimized data types and a tuple containing memory usage before and after optimizations. Constraints: - Assume the directory path and specific columns to be read are given as inputs. - The dataset is large, so ensure that operations are memory efficient. Function Signature ```python import pandas as pd from typing import Tuple def optimize_and_evaluate_memory_usage(directory: str, columns: list) -> Tuple[pd.DataFrame, Tuple[int, int]]: pass ``` Input - `directory` (str): Path to the directory containing Parquet files. - `columns` (list): List of column names to be loaded from each file. Output - A tuple `(optimized_df, (memory_before, memory_after))`: - `optimized_df` should be the concatenated DataFrame with optimized data types. - `memory_before` and `memory_after` should be the total memory usage in bytes before and after optimization. Example Suppose the directory structure is as follows: ``` data/ â””â”€â”€ timeseries/ â”œâ”€â”€ ts-00.parquet â”œâ”€â”€ ts-01.parquet â”œâ”€â”€ ... ``` The example call: ```python result = optimize_and_evaluate_memory_usage(\'data/timeseries\', [\'id\', \'name\', \'x\', \'y\']) ``` This will load the specified columns from all files in the directory, apply optimizations, calculate memory usage before and after, and return the optimized DataFrame along with memory usage statistics. Implementation Details 1. Load columns specified in the `columns` list from each Parquet file in the specified directory. 2. Calculate initial memory usage. 3. Convert the \'name\' column to Categorical type. 4. Downcast \'id\', \'x\', and \'y\' columns to the most appropriate numeric types. 5. Calculate memory usage after optimizations. 6. Return the optimized DataFrame and the calculated memory usages. **Note:** You can use the example code snippets and methods in the documentation as a guide for implementing the function.","solution":"import pandas as pd import os import numpy as np from typing import Tuple def optimize_and_evaluate_memory_usage(directory: str, columns: list) -> Tuple[pd.DataFrame, Tuple[int, int]]: # Load columns from all Parquet files in the directory all_data = [] for file_name in os.listdir(directory): if file_name.endswith(\'.parquet\'): file_path = os.path.join(directory, file_name) data = pd.read_parquet(file_path, columns=columns) all_data.append(data) # Concatenate all loaded data df = pd.concat(all_data, ignore_index=True) # Calculate initial memory usage memory_before = df.memory_usage(deep=True).sum() # Optimize data types df[\'name\'] = df[\'name\'].astype(\'category\') for col in [\'id\', \'x\', \'y\']: df[col] = pd.to_numeric(df[col], downcast=\'unsigned\' if df[col].min() >= 0 else \'signed\') # Calculate memory usage after optimization memory_after = df.memory_usage(deep=True).sum() return df, (memory_before, memory_after)"},{"question":"Problem Description You are tasked with designing a Python program that models a simple role-playing game using the concepts of enumerations provided by the `enum` module. 1. **Character Class**: - Create an enumeration called `CharacterClass` that includes four roles: `WARRIOR`, `MAGE`, `ARCHER`, and `ROGUE`. - Each role should have a unique integer value starting from 1, and each should have an associated attribute that represents their primary weapon as a string (`SWORD`, `STAFF`, `BOW`, `DAGGER`). 2. **Character**: - Create a `Character` class that includes: - A name (string). - A character class (an instance of the `CharacterClass` enum). - A method called `describe` that returns a string: `\\"Character {name} is a {character_class} and wields a {weapon}\\"`. 3. **FlagEnabler**: - Create an `IntFlag` enumeration called `Abilities` to represent the abilities a character can have, such as `STEALTH`, `MAGIC`, `ARCHERY`, and `STRONG`. - Each ability should have a unique power of two value: `1`, `2`, `4`, and `8`. - Implement a method within the `Character` class named `has_ability` to check if a character has a specific ability. 4. **Implementation Requirements**: - Implement unit tests for your solution using Python\'s `unittest` library. - Write test cases to validate the creation of characters, describing them, and checking their abilities. Constraints - The `CharacterClass` enum must use automatic enumeration values. - Ensure that each character has a valid class from `CharacterClass`. - The abilities must be combined using bitwise operations. Example ```python from enum import Enum, IntFlag, auto class CharacterClass(Enum): WARRIOR = auto() MAGE = auto() ARCHER = auto() ROGUE = auto() def __init__(self, weapon): self.weapon = weapon CharacterClass.WARRIOR.weapon = \'SWORD\' CharacterClass.MAGE.weapon = \'STAFF\' CharacterClass.ARCHER.weapon = \'BOW\' CharacterClass.ROGUE.weapon = \'DAGGER\' class Abilities(IntFlag): STEALTH = 1 MAGIC = 2 ARCHERY = 4 STRONG = 8 class Character: def __init__(self, name, character_class): if not isinstance(character_class, CharacterClass): raise ValueError(\\"Invalid character class\\") self.name = name self.character_class = character_class def describe(self): return f\\"Character {self.name} is a {self.character_class.name} and wields a {self.character_class.weapon}\\" def has_ability(self, ability): if not isinstance(ability, Abilities): raise ValueError(\\"Invalid ability\\") return ability & self.abilities == ability import unittest class TestGame(unittest.TestCase): def setUp(self): self.warrior = Character(\\"Aragorn\\", CharacterClass.WARRIOR) self.mage = Character(\\"Gandalf\\", CharacterClass.MAGE) def test_character_creation(self): self.assertEqual(self.warrior.name, \\"Aragorn\\") self.assertEqual(self.warrior.character_class, CharacterClass.WARRIOR) self.assertEqual(self.mage.character_class.weapon, \\"STAFF\\") def test_character_description(self): self.assertEqual(self.warrior.describe(), \\"Character Aragorn is a WARRIOR and wields a SWORD\\") def test_abilities(self): self.warrior.abilities = Abilities.STEALTH | Abilities.STRONG self.assertTrue(self.warrior.has_ability(Abilities.STEALTH)) self.assertFalse(self.warrior.has_ability(Abilities.MAGIC)) if __name__ == \\"__main__\\": unittest.main() ``` Ensure your implementation covers all aspects of the provided example. Document any assumptions you make.","solution":"from enum import Enum, IntFlag, auto class CharacterClass(Enum): WARRIOR = auto() MAGE = auto() ARCHER = auto() ROGUE = auto() def __init__(self, weapon): self.weapon = weapon CharacterClass.WARRIOR.weapon = \'SWORD\' CharacterClass.MAGE.weapon = \'STAFF\' CharacterClass.ARCHER.weapon = \'BOW\' CharacterClass.ROGUE.weapon = \'DAGGER\' class Abilities(IntFlag): STEALTH = 1 MAGIC = 2 ARCHERY = 4 STRONG = 8 class Character: def __init__(self, name, character_class): if not isinstance(character_class, CharacterClass): raise ValueError(\\"Invalid character class\\") self.name = name self.character_class = character_class self.abilities = Abilities(0) def describe(self): return f\\"Character {self.name} is a {self.character_class.name} and wields a {self.character_class.weapon}\\" def has_ability(self, ability): if not isinstance(ability, Abilities): raise ValueError(\\"Invalid ability\\") return ability & self.abilities == ability"},{"question":"**System Information Reporter** **Objective:** You are tasked with creating a Python script that compiles and reports comprehensive system information using functions from the `platform` module. **Instructions:** Write a Python function `generate_system_report()` that: 1. Collects the following information using appropriate functions from the `platform` module: - Machine type - Network name - OS platform string - Processor name - Python build number and date - Python compiler - Python implementation - Python version - System release and version - General system/OS name 2. Formats this information into a dictionary with the following keys: - `\'machine\'` - `\'node\'` - `\'platform\'` - `\'processor\'` - `\'python_build\'` - `\'python_compiler\'` - `\'python_implementation\'` - `\'python_version\'` - `\'release\'` - `\'system\'` 3. Returns the dictionary. **Constraints:** - Use only the functions from the `platform` module as described in the provided documentation. - Assume that all necessary functions are available and function as expected. - Your function should handle any unexpected empty strings returned by the `platform` functions gracefully. **Example:** ```python def generate_system_report(): # Your implementation here # Example output format: # { # \'machine\': \'x86_64\', # \'node\': \'hostname\', # \'platform\': \'Linux-5.4.0-52-generic-x86_64-with-Ubuntu-20.04\', # \'processor\': \'x86_64\', # \'python_build\': (\'default\', \'Oct 8 2020 08:21:14\'), # \'python_compiler\': \'GCC 9.3.0\', # \'python_implementation\': \'CPython\', # \'python_version\': \'3.8.6\', # \'release\': \'5.4.0-52-generic\', # \'system\': \'Linux\' # } ``` **Evaluation Criteria:** - Correct use of `platform` module functions. - Accurate collection and formatting of system information. - Robustness of code against potential empty string returns. **Hints:** - Review the function signatures and return values as provided in the documentation. - Test your code on different systems to ensure compatibility and correctness.","solution":"import platform def generate_system_report(): Generates a comprehensive system report using the platform module. Returns: dict: A dictionary containing system information. return { \'machine\': platform.machine() or \'N/A\', \'node\': platform.node() or \'N/A\', \'platform\': platform.platform() or \'N/A\', \'processor\': platform.processor() or \'N/A\', \'python_build\': platform.python_build() or (\'N/A\', \'N/A\'), \'python_compiler\': platform.python_compiler() or \'N/A\', \'python_implementation\': platform.python_implementation() or \'N/A\', \'python_version\': platform.python_version() or \'N/A\', \'release\': platform.release() or \'N/A\', \'system\': platform.system() or \'N/A\' }"},{"question":"Objective: Implement a Python function to mimic some of the functionalities provided by Python\'s bytes-related APIs. Ensure that you demonstrate the handling of bytes objects, type checking, and manipulation. Description: You are required to implement the following functions: 1. `create_bytes_from_string(s: str) -> bytes`: - This function should take a string `s` and return a bytes object with a copy of the string `s`. 2. `concat_bytes(b1: bytes, b2: bytes) -> bytes`: - This function should take two bytes objects, `b1` and `b2`, and return a new bytes object that contains the contents of `b2` appended to `b1`. 3. `resize_bytes(b: bytes, new_size: int) -> bytes`: - This function should resize the bytes object `b` to the new size `new_size`. If the new size is greater than the current size, the bytes object should be padded with null bytes (`0x00`). 4. `get_bytes_length(b: bytes) -> int`: - This function should return the length of the bytes object `b`. 5. `bytes_contains_substring(b: bytes, sub: str) -> bool`: - This function should check if the bytes object `b` contains the substring `sub` (converted to bytes) and return `True` if it does, otherwise `False`. Constraints: - Do not use any built-in Python functions for these operations except for: - Converting between string and bytes using `encode()` or `b\'...\'`. - Getting the length of bytes using `len()`. - Ensure that the types of inputs are correctly handled and appropriate exceptions are raised for type errors. Example Usage: ```python # Example usage: b1 = create_bytes_from_string(\\"hello\\") b2 = create_bytes_from_string(\\"world\\") # Concatenate bytes combined = concat_bytes(b1, b2) # b\'helloworld\' # Resize bytes resized = resize_bytes(b1, 10) # b\'hellox00x00x00x00x00\' # Get length of bytes length = get_bytes_length(b1) # 5 # Check for substring in bytes contains = bytes_contains_substring(combined, \\"wor\\") # True ``` Implementation: ```python def create_bytes_from_string(s: str) -> bytes: if not isinstance(s, str): raise TypeError(\\"Expected a string\\") return s.encode() def concat_bytes(b1: bytes, b2: bytes) -> bytes: if not isinstance(b1, bytes) or not isinstance(b2, bytes): raise TypeError(\\"Expected bytes objects\\") return b1 + b2 def resize_bytes(b: bytes, new_size: int) -> bytes: if not isinstance(b, bytes) or not isinstance(new_size, int): raise TypeError(\\"Expected a bytes object and an integer\\") if new_size < 0: raise ValueError(\\"new_size must be non-negative\\") if new_size <= len(b): return b[:new_size] else: return b + b\'x00\' * (new_size - len(b)) def get_bytes_length(b: bytes) -> int: if not isinstance(b, bytes): raise TypeError(\\"Expected a bytes object\\") return len(b) def bytes_contains_substring(b: bytes, sub: str) -> bool: if not isinstance(b, bytes) or not isinstance(sub, str): raise TypeError(\\"Expected a bytes object and a string\\") return sub.encode() in b ``` Make sure your implementation passes all the provided example usages and raises appropriate errors for invalid inputs.","solution":"def create_bytes_from_string(s: str) -> bytes: if not isinstance(s, str): raise TypeError(\\"Expected a string\\") return s.encode() def concat_bytes(b1: bytes, b2: bytes) -> bytes: if not isinstance(b1, bytes) or not isinstance(b2, bytes): raise TypeError(\\"Expected bytes objects\\") return b1 + b2 def resize_bytes(b: bytes, new_size: int) -> bytes: if not isinstance(b, bytes) or not isinstance(new_size, int): raise TypeError(\\"Expected a bytes object and an integer\\") if new_size < 0: raise ValueError(\\"new_size must be non-negative\\") if new_size <= len(b): return b[:new_size] else: return b + b\'x00\' * (new_size - len(b)) def get_bytes_length(b: bytes) -> int: if not isinstance(b, bytes): raise TypeError(\\"Expected a bytes object\\") return len(b) def bytes_contains_substring(b: bytes, sub: str) -> bool: if not isinstance(b, bytes) or not isinstance(sub, str): raise TypeError(\\"Expected a bytes object and a string\\") return sub.encode() in b"},{"question":"# Directory Comparison and Reporting You are tasked with implementing a utility function that compares two directories and provides a comprehensive report. The comparison should capture all matching files, mismatching files, files only present in one directory, and files that could not be compared. Requirements: 1. Implement a function `compare_directories(dir1: str, dir2: str) -> dict` that: - Takes two directory paths as input (`dir1` and `dir2`). - Uses the `filecmp.dircmp` class to compare the directories. - Returns a dictionary with the following keys and their corresponding values: - `common_files`: List of files that are common between `dir1` and `dir2`. - `same_files`: List of files that are the same in both directories. - `diff_files`: List of files that are in both directories but differ in content. - `left_only`: List of files only in `dir1`. - `right_only`: List of files only in `dir2`. - `errors`: List of files that could not be compared. 2. The output dictionary should be formatted as follows: ```python { \\"common_files\\": [\\"common_file1.txt\\", \\"common_file2.txt\\"], \\"same_files\\": [\\"identical_file.txt\\"], \\"diff_files\\": [\\"different_file1.txt\\", \\"different_file2.txt\\"], \\"left_only\\": [\\"only_in_left1.txt\\", \\"only_in_left2.txt\\"], \\"right_only\\": [\\"only_in_right1.txt\\", \\"only_in_right2.txt\\"], \\"errors\\": [\\"error_file1.txt\\", \\"error_file2.txt\\"] } ``` 3. You are not supposed to print but return the report. Additional Constraints: - Ensure that your function handles directories that do not exist by raising a `FileNotFoundError` with an appropriate message. - Pay extra attention to exception handling for permission issues or any other errors that might arise during file comparison. # Example Usage: ```python # Assume the existence of two directories \\"dirA\\" and \\"dirB\\" report = compare_directories(\'dirA\', \'dirB\') print(report) ``` This function should provide a comprehensive and detailed comparison of the given directories, helping users identify similarities and differences efficiently.","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str) -> dict: # Check if the directories exist if not os.path.exists(dir1): raise FileNotFoundError(f\\"The directory {dir1} does not exist.\\") if not os.path.exists(dir2): raise FileNotFoundError(f\\"The directory {dir2} does not exist.\\") # Initialize the result dictionary result = { \\"common_files\\": [], \\"same_files\\": [], \\"diff_files\\": [], \\"left_only\\": [], \\"right_only\\": [], \\"errors\\": [] } try: comparison = filecmp.dircmp(dir1, dir2) result[\\"common_files\\"] = comparison.common_files result[\\"same_files\\"] = comparison.same_files result[\\"diff_files\\"] = comparison.diff_files result[\\"left_only\\"] = comparison.left_only result[\\"right_only\\"] = comparison.right_only except Exception as e: result[\\"errors\\"].append(str(e)) return result"},{"question":"**Challenge Yourself: Environment Variables and File System Interaction** You are tasked with creating a Python script that demonstrates advanced usage of the `os` module, including handling environment variables and interacting with the file system. # **Objective** Implement a function `environment_file_info(filename: str) -> str` that performs the following: 1. **Retrieve Environment Variable**: - Get the home directory path from the environment variable using `os.environ`. - If the environment variable is not set, the function should raise an `OSError` with a clear message. 2. **File Information**: - Check if the file at the given path exists within the home directory. - If the file exists, return the absolute path of the file. - If the file does not exist, raise a `FileNotFoundError` with an appropriate message. 3. **Large File Check**: - Check if the file size exceeds 2 GiB. - Return a message indicating whether the file is a \\"large file\\" or \\"not a large file\\". # **Input** - `filename`: A string representing the name of the file to be checked within the user\'s home directory. # **Output** - A string with the following possible messages: 1. `\\"File <absolute_path> is a large file\\"` 2. `\\"File <absolute_path> is not a large file\\"` 3. `\\"File <filename> does not exist in the home directory\\"` 4. `\'Environment variable HOME is not set\'` # **Constraints** - Use the `os` module exclusively for handling file system and environment variable operations. # **Example Usage** ```python # Assuming the home directory is /home/user environment_file_info(\\"sample.txt\\") # Possible output: # \'File /home/user/sample.txt is a large file\' # or # \'File /home/user/sample.txt is not a large file\' # or # \'File sample.txt does not exist in the home directory\' # or # \'Environment variable HOME is not set\' ``` # **Guidelines** 1. Make sure to handle all possible edge cases. 2. Write clean, modular, and well-documented code. 3. Use exception handling to manage errors effectively. **Hint**: Utilize `os.environ.get`, `os.path.exists`, `os.path.getsize`, and related functions. Good luck, and may your code run error-free!","solution":"import os def environment_file_info(filename: str) -> str: Checks if a file exists in the home directory, determines if it is a large file (greater than 2 GiB), and handles environment variables and file existence. Args: filename (str): The name of the file to check within the home directory. Returns: str: A message describing the file status. home_dir = os.environ.get(\'HOME\') if not home_dir: return \'Environment variable HOME is not set\' file_path = os.path.join(home_dir, filename) if not os.path.exists(file_path): return f\'File {filename} does not exist in the home directory\' file_size = os.path.getsize(file_path) if file_size > 2 * 1024 * 1024 * 1024: # 2 GiB return f\'File {file_path} is a large file\' else: return f\'File {file_path} is not a large file\'"},{"question":"**Title: Efficient Tensor Manipulation using Views in PyTorch** **Objective:** Demonstrate your understanding of tensor views in PyTorch by implementing functions that utilize views for efficient tensor manipulation without unnecessary data copying. **Task:** 1. **Function 1: `create_transposed_view`** - **Description**: Given a 2D tensor as input, create and return a transposed view of the tensor. - **Input**: A 2D PyTorch tensor `input_tensor` of shape (m, n). - **Output**: A transposed view of the input tensor of shape (n, m). - Note: Ensure that the returned tensor is a view and not a copy. 2. **Function 2: `contiguous_tensor`** - **Description**: Given a potentially non-contiguous view tensor, return a contiguous tensor. - **Input**: A PyTorch tensor `view_tensor`. - **Output**: A contiguous tensor with the same data as the input tensor. - Note: Use the `.contiguous()` method to achieve this. 3. **Function 3: `view_and_modify`** - **Description**: Given a 1D tensor and an integer, reshape the tensor into a 2D view (if possible), modify its elements, and verify changes in the base tensor. - **Input**: A 1D PyTorch tensor `input_tensor` and an integer `new_shape` which is a tuple (a, b) such that `a * b == input_tensor.size(0)`. - **Output**: The modified 2D view tensor. - Note: Ensure that changes made to the view tensor are reflected in the original tensor. **Constraints:** - For Function 1, the input tensor will always be 2D. - For Function 2, the input tensor may be either contiguous or non-contiguous. - For Function 3, the input tensor and new_shape will always be compatible for reshaping. **Example:** ```python import torch # Example for create_transposed_view input_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]) transposed_view = create_transposed_view(input_tensor) assert transposed_view.shape == (3, 2) assert transposed_view.storage().data_ptr() == input_tensor.storage().data_ptr() # Example for contiguous_tensor view_tensor = input_tensor.transpose(0, 1) contiguous_out = contiguous_tensor(view_tensor) assert contiguous_out.is_contiguous() # Example for view_and_modify input_tensor = torch.tensor([1, 2, 3, 4, 5, 6]) new_shape = (2, 3) modified_view = view_and_modify(input_tensor, new_shape) assert modified_view.shape == (2, 3) assert input_tensor[0] == modified_view[0, 0] ``` Implement the functions `create_transposed_view`, `contiguous_tensor`, and `view_and_modify` to complete this task.","solution":"import torch def create_transposed_view(input_tensor): Given a 2D tensor, create and return a transposed view of the tensor. return input_tensor.t() def contiguous_tensor(view_tensor): Given a potentially non-contiguous view tensor, return a contiguous tensor. return view_tensor.contiguous() def view_and_modify(input_tensor, new_shape): Given a 1D tensor and an integer, reshape the tensor into a 2D view, modify its elements, and verify changes in the base tensor. view_tensor = input_tensor.view(new_shape) # Example modification for demonstration view_tensor[0][0] = 99 return view_tensor"},{"question":"**Objective:** Demonstrate understanding of encoding and decoding operations using the `codecs` module in Python, including handling of common encoding errors and working with custom codecs. --- **Problem Statement:** You are required to implement a utility class `CustomCodecUtility` in Python that demonstrates the use of stateless and incremental encoding/decoding operations. Additionally, you will need to implement custom error handling logic for encoding errors. # Class `CustomCodecUtility` Methods: 1. **stateless_encode(input_text: str, encoding: str, errors: str=\'strict\') -> Tuple[bytes, int]:** - Encodes the given input string using the specified encoding. - Input: - `input_text`: The text to be encoded. - `encoding`: The encoding to use (e.g., \'utf-8\', \'latin-1\'). - `errors`: Error handling scheme (default is \'strict\'). - Output: - Returns a tuple containing the encoded bytes and the number of characters encoded. 2. **stateless_decode(input_bytes: bytes, encoding: str, errors: str=\'strict\') -> Tuple[str, int]:** - Decodes the given input bytes using the specified encoding. - Input: - `input_bytes`: The bytes to be decoded. - `encoding`: The encoding to use (e.g., \'utf-8\' or \'latin-1\'). - `errors`: Error handling scheme (default is \'strict\'). - Output: - Returns a tuple containing the decoded string and the number of bytes consumed. 3. **incremental_encode(input_text: str, encoding: str, errors: str=\'strict\') -> bytes:** - Incrementally encodes the given input string using the specified encoding. - Input: - `input_text`: The text to be encoded. - `encoding`: The encoding to use. - `errors`: Error handling scheme (default is \'strict\'). - Output: - Returns the encoded bytes. 4. **incremental_decode(input_bytes: bytes, encoding: str, errors: str=\'strict\') -> str:** - Incrementally decodes the given input bytes using the specified encoding. - Input: - `input_bytes`: The bytes to be decoded. - `encoding`: The encoding to use. - `errors`: Error handling scheme (default is \'strict\'). - Output: - Returns the decoded string. 5. **replace_with_custom_marker(exception: UnicodeEncodeError) -> Tuple[str, int]:** - Custom error handler that replaces unencodable characters with the string `[INVALID]`. - Input: - `exception`: The encoding error encountered. - Output: - Returns a tuple containing the replacement string and the position to continue encoding. 6. **register_custom_error_handler(name: str) -> None:** - Registers the custom error handler for encoding errors. - Input: - `name`: The name to associate with the custom error handler. # Constraints: - The `input_text` for encoding methods will be a non-empty string of maximum length 1000 characters. - The `input_bytes` for decoding methods will be non-empty bytes of maximum length 1000 bytes. - Only standard encodings available in the Python `codecs` module should be used (e.g., \'utf-8\', \'latin-1\'). # Example Usage: ```python # Example Usage: util = CustomCodecUtility() util.register_custom_error_handler(\\"custom_replace\\") encoded_data, _ = util.stateless_encode(\\"Example with Ãœnicode\\", \\"ascii\\", \\"custom_replace\\") print(encoded_data) # b\'Example with [INVALID]nicode\' decoded_text, _ = util.stateless_decode(encoded_data, \\"ascii\\") print(decoded_text) # \'Example with [INVALID]nicode\' ``` Implement the `CustomCodecUtility` class according to the above specification.","solution":"import codecs from typing import Tuple class CustomCodecUtility: def stateless_encode(self, input_text: str, encoding: str, errors: str=\'strict\') -> Tuple[bytes, int]: encoded_bytes = codecs.encode(input_text, encoding, errors) return (encoded_bytes, len(input_text)) def stateless_decode(self, input_bytes: bytes, encoding: str, errors: str=\'strict\') -> Tuple[str, int]: decoded_text = codecs.decode(input_bytes, encoding, errors) return (decoded_text, len(input_bytes)) def incremental_encode(self, input_text: str, encoding: str, errors: str=\'strict\') -> bytes: incremental_encoder = codecs.getincrementalencoder(encoding)(errors) encoded_result = incremental_encoder.encode(input_text) + incremental_encoder.encode(\'\', True) return encoded_result def incremental_decode(self, input_bytes: bytes, encoding: str, errors: str=\'strict\') -> str: incremental_decoder = codecs.getincrementaldecoder(encoding)(errors) decoded_result = incremental_decoder.decode(input_bytes) + incremental_decoder.decode(b\'\', True) return decoded_result def replace_with_custom_marker(self, exception: UnicodeEncodeError) -> Tuple[str, int]: return (\'[INVALID]\', exception.start + 1) def register_custom_error_handler(self, name: str) -> None: codecs.register_error(name, self.replace_with_custom_marker) # Example of the class util = CustomCodecUtility() util.register_custom_error_handler(\\"custom_replace\\") encoded_data, _ = util.stateless_encode(\\"Example with Ãœnicode\\", \\"ascii\\", \\"custom_replace\\") print(encoded_data) # b\'Example with [INVALID]nicode\' decoded_text, _ = util.stateless_decode(encoded_data, \\"ascii\\") print(decoded_text) # \'Example with [INVALID]nicode\'"},{"question":"You are given a dataset containing information about diamond characteristics. Your task is to create a statistical visualization using Seaborn that compares the carat weight distribution of diamonds across different cut qualities using various statistical measures. **Requirements:** 1. Load the diamonds dataset using Seaborn\'s `load_dataset` function. 2. Create a Seaborn Plot object that visualizes the carat weight for each cut quality. 3. Add a range that uses the mean and 95% confidence interval to display error bars. 4. Additionally, display the median carat weight with error bars representing one standard error. 5. Ensure that the process is reproducible by setting a random seed to eliminate variability in the bootstrapping process. 6. Provide a visualization that also includes a weighted estimate of the mean carat weight using the `price` as the weight variable. **Input Format:** - No input from the user is required; the dataset should be loaded directly in the code. **Output Format:** - The output should be a visual plot generated using Seaborn. **Constraints:** - Use only the Seaborn and pandas libraries for data manipulation and plotting. - Ensure that the plot is clearly labeled and visually easy to interpret. **Example:** ``` import seaborn.objects as so from seaborn import load_dataset # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot object p = so.Plot(diamonds, \\"cut\\", \\"carat\\") # Add different statistical ranges and estimators p.add(so.Range(), so.Est()) p.add(so.Range(), so.Est(\\"median\\"), errorbar=\\"se\\") # Include a weighted estimate p.add(so.Range(), so.Est(), weight=\\"price\\") # Set the seed for reproducibility p.add(so.Range(), so.Est(seed=0)) # Display the plot p.show() ``` Ensure your visualization meets the following criteria: - It should display three statistical measures: mean (with 95% confidence interval), median (with standard error), and a weighted mean. - The plot should be reproducible with minimal random variability.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_diamond_carat_distribution(): Creates a plot visualizing the carat weight distribution of diamonds across different cut qualities using various statistical measures. # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot object p = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\") # Add different statistical ranges and estimators p.add(so.Range(), so.Est(), errorbar=\\"ci\\", seed=0) # Mean with 95% confidence interval p.add(so.Range(), so.Est(\\"median\\"), errorbar=\\"se\\", seed=0) # Median with standard error # Include a weighted estimate p.add(so.Range(), so.Est(), weight=\\"price\\", seed=0) # Weighted estimate of the mean using price as weight # Display the plot p.show() # Run the function to display the plot plot_diamond_carat_distribution()"},{"question":"**Python Coding Challenge: Enhanced Command Line Interface** The goal of this exercise is to test your ability to create a comprehensive command-line interface using the `argparse` module. # Problem Statement: You are required to create a Python script that processes a series of command-line options and sub-commands to manage a simple database of items. The script should support adding, querying, and deleting items from the database. The database will be a simple JSON file (`items.json`). Your script should provide a user-friendly command-line interface with help messages detailing the available commands and options. # Requirements: 1. **Main Command**: - The script should have a main command called `items`. - The main command should support an optional `--version` flag that displays the current version of the script and exits. - The main command should support an optional `--config` flag that specifies a path to a configuration file. The configuration file should be a JSON file that can override the default database file path (`items.json`). 2. **Sub-commands**: - **add**: - Should have required arguments `name` (string) and `quantity` (integer). - Should add a new item to the database. - **query**: - Should have an optional argument `name` (string). - If `name` is provided, it should display the details of the specified item. - If `name` is not provided, it should list all items in the database. - **delete**: - Should have a required argument `name` (string). - Should delete the specified item from the database. - Any invalid commands or missing required arguments should trigger helpful error messages. # Constraints: - Ensure proper error handling and informative messages for incorrect command usages. - The script should always maintain help and usage documentation updated and correctly formatted. - The configuration file should allow specifying an alternative path to the database file. # Input & Output formats: Adding an item: ```sh python items.py add --name \\"Item1\\" --quantity 5 Item \'Item1\' added with quantity 5. ``` Querying all items: ```sh python items.py query Items in database: 1. Name: Item1, Quantity: 5 ``` Querying a specific item: ```sh python items.py query --name \\"Item1\\" Item found: - Name: Item1, Quantity: 5 ``` Deleting an item: ```sh python items.py delete --name \\"Item1\\" Item \'Item1\' deleted. ``` # Example JSON Structure: ```json [ {\\"name\\": \\"Item1\\", \\"quantity\\": 5} ] ``` # Notes: - You can use any third-party JSON library for handling the JSON read/write operations. - Make sure to implement the required functionalities and follow best practices for using the `argparse` module. - Consider edge cases such as duplicate items, querying non-existing items, and deleting items from an empty database. ```python import argparse import json import os # Define default database file path DEFAULT_DB_FILE = \'items.json\' def load_items(db_file=DEFAULT_DB_FILE): # Function to load items from the database file if not os.path.exists(db_file): return [] with open(db_file, \'r\') as f: return json.load(f) def save_items(items, db_file=DEFAULT_DB_FILE): # Function to save items to the database file with open(db_file, \'w\') as f: json.dump(items, f, indent=4) def add_item(args, db_file): items = load_items(db_file) new_item = {\'name\': args.name, \'quantity\': args.quantity} items.append(new_item) save_items(items, db_file) print(f\\"Item \'{args.name}\' added with quantity {args.quantity}.\\") def query_items(args, db_file): items = load_items(db_file) if args.name: for item in items: if item[\'name\'] == args.name: print(f\\"Item found:n- Name: {item[\'name\']}, Quantity: {item[\'quantity\']}\\") return print(f\\"Item \'{args.name}\' not found.\\") else: if not items: print(\\"No items in database.\\") else: print(\\"Items in database:\\") for idx, item in enumerate(items, 1): print(f\\"{idx}. Name: {item[\'name\']}, Quantity: {item[\'quantity\']}\\") def delete_item(args, db_file): items = load_items(db_file) for item in items: if item[\'name\'] == args.name: items.remove(item) save_items(items, db_file) print(f\\"Item \'{args.name}\' deleted.\\") return print(f\\"Item \'{args.name}\' not found.\\") def main(): # Create the top-level parser parser = argparse.ArgumentParser(prog=\'items\', description=\'Manage items in the database.\') parser.add_argument(\'--version\', action=\'version\', version=\'%(prog)s 1.0\') parser.add_argument(\'--config\', help=\'Path to configuration file\') # Create subparsers for the subcommands subparsers = parser.add_subparsers(title=\'subcommands\', description=\'valid subcommands\', help=\'additional help\') # Add subcommand parser_add = subparsers.add_parser(\'add\', help=\'Add a new item\') parser_add.add_argument(\'--name\', required=True, help=\'Name of the item\') parser_add.add_argument(\'--quantity\', type=int, required=True, help=\'Quantity of the item\') parser_add.set_defaults(func=add_item) # Query subcommand parser_query = subparsers.add_parser(\'query\', help=\'Query items\') parser_query.add_argument(\'--name\', help=\'Name of the item to query\') parser_query.set_defaults(func=query_items) # Delete subcommand parser_delete = subparsers.add_parser(\'delete\', help=\'Delete an item\') parser_delete.add_argument(\'--name\', required=True, help=\'Name of the item to delete\') parser_delete.set_defaults(func=delete_item) # Parse arguments args = parser.parse_args() # Determine the configuration file to use db_file = DEFAULT_DB_FILE if args.config: with open(args.config, \'r\') as f: config = json.load(f) db_file = config.get(\'db_file\', DEFAULT_DB_FILE) # Execute the appropriate function based on the subcommand if hasattr(args, \'func\'): args.func(args, db_file) else: parser.print_help() if __name__ == \'__main__\': main() ``` # Evaluation Criteria: - Proper usage of `argparse` to manage command-line options and arguments. - Correct handling of adding, querying, and deleting items from the database. - Code readability and maintainability. - Effective usage of error handling and user feedback mechanisms. **Good Luck!**","solution":"import argparse import json import os # Define default database file path DEFAULT_DB_FILE = \'items.json\' def load_items(db_file=DEFAULT_DB_FILE): # Function to load items from the database file if not os.path.exists(db_file): return [] with open(db_file, \'r\') as f: return json.load(f) def save_items(items, db_file=DEFAULT_DB_FILE): # Function to save items to the database file with open(db_file, \'w\') as f: json.dump(items, f, indent=4) def add_item(args, db_file): items = load_items(db_file) new_item = {\'name\': args.name, \'quantity\': args.quantity} items.append(new_item) save_items(items, db_file) print(f\\"Item \'{args.name}\' added with quantity {args.quantity}.\\") def query_items(args, db_file): items = load_items(db_file) if args.name: for item in items: if item[\'name\'] == args.name: print(f\\"Item found:n- Name: {item[\'name\']}, Quantity: {item[\'quantity\']}\\") return print(f\\"Item \'{args.name}\' not found.\\") else: if not items: print(\\"No items in database.\\") else: print(\\"Items in database:\\") for idx, item in enumerate(items, 1): print(f\\"{idx}. Name: {item[\'name\']}, Quantity: {item[\'quantity\']}\\") def delete_item(args, db_file): items = load_items(db_file) for item in items: if item[\'name\'] == args.name: items.remove(item) save_items(items, db_file) print(f\\"Item \'{args.name}\' deleted.\\") return print(f\\"Item \'{args.name}\' not found.\\") def main(): # Create the top-level parser parser = argparse.ArgumentParser(prog=\'items\', description=\'Manage items in the database.\') parser.add_argument(\'--version\', action=\'version\', version=\'%(prog)s 1.0\') parser.add_argument(\'--config\', help=\'Path to configuration file\') # Create subparsers for the subcommands subparsers = parser.add_subparsers(title=\'subcommands\', description=\'valid subcommands\', help=\'additional help\') # Add subcommand parser_add = subparsers.add_parser(\'add\', help=\'Add a new item\') parser_add.add_argument(\'--name\', required=True, help=\'Name of the item\') parser_add.add_argument(\'--quantity\', type=int, required=True, help=\'Quantity of the item\') parser_add.set_defaults(func=add_item) # Query subcommand parser_query = subparsers.add_parser(\'query\', help=\'Query items\') parser_query.add_argument(\'--name\', help=\'Name of the item to query\') parser_query.set_defaults(func=query_items) # Delete subcommand parser_delete = subparsers.add_parser(\'delete\', help=\'Delete an item\') parser_delete.add_argument(\'--name\', required=True, help=\'Name of the item to delete\') parser_delete.set_defaults(func=delete_item) # Parse arguments args = parser.parse_args() # Determine the configuration file to use db_file = DEFAULT_DB_FILE if args.config: with open(args.config, \'r\') as f: config = json.load(f) db_file = config.get(\'db_file\', DEFAULT_DB_FILE) # Execute the appropriate function based on the subcommand if hasattr(args, \'func\'): args.func(args, db_file) else: parser.print_help() if __name__ == \'__main__\': main()"},{"question":"<|Analysis Begin|> The provided documentation details the public API for \\"set\\" and \\"frozenset\\" objects in Python. It specifies various functions and macros available for creating, modifying, and querying these objects in Python. This includes type checking, object creation, checking membership, adding elements, removing elements, and clearing a set. Key functions and macros highlighted in the documentation include: - **Type Checking Functions:** - `PySet_Check` - `PyFrozenSet_Check` - `PyAnySet_Check` - `PySet_CheckExact` - `PyAnySet_CheckExact` - `PyFrozenSet_CheckExact` - **Set Creation Functions:** - `PySet_New` - `PyFrozenSet_New` - **Set Operations:** - `PySet_Size` - `PySet_GET_SIZE` - `PySet_Contains` - `PySet_Add` - `PySet_Discard` - `PySet_Pop` - `PySet_Clear` Given the nature of the provided documentation, the focus can be on a question that assesses the ability to work with sets and frozensets using these functions, including creating sets, modifying them, and querying them. <|Analysis End|> <|Question Begin|> **Question:** You are tasked with implementing a Python class that mimics certain behaviors of Python\'s built-in `set` and `frozenset` objects using the provided C API functions. Your implementation should include methods for creating sets and frozensets, adding elements, and checking membership. Furthermore, your class should be designed such that it can handle both `set` and `frozenset` objects appropriately. # Requirements: 1. **Class Initialization and Creation:** - Create two classes, `MySet` and `MyFrozenSet`, that utilize the `PySet_New` and `PyFrozenSet_New` functions respectively for their internal set representations. 2. **Adding Elements:** - Implement an `add` method in `MySet` that uses the `PySet_Add` function to add elements to the set. Since `frozenset` is immutable, `MyFrozenSet` should not have an `add` method. 3. **Checking Membership:** - Implement a `contains` method in both `MySet` and `MyFrozenSet` that uses the `PySet_Contains` function to check if an element exists in the set. # Class Definitions and Method Signatures: ```python class MySet: def __init__(self, iterable=None): # Initialize the set with elements from the iterable using PySet_New def add(self, element): # Add an element to the set using PySet_Add def contains(self, element): # Check if an element is in the set using PySet_Contains # Return True if the element is in the set, otherwise False class MyFrozenSet: def __init__(self, iterable=None): # Initialize the frozenset with elements from the iterable using PyFrozenSet_New def contains(self, element): # Check if an element is in the frozenset using PySet_Contains # Return True if the element is in the frozenset, otherwise False ``` # Example Usage: ```python # Creating a set and adding elements s = MySet([1, 2, 3]) s.add(4) print(s.contains(4)) # Output: True print(s.contains(5)) # Output: False # Creating a frozenset and checking membership fs = MyFrozenSet([1, 2, 3]) print(fs.contains(2)) # Output: True print(fs.contains(4)) # Output: False ``` # Constraints: - Ensure that your implementation strictly uses the relevant functions and macros from the provided C API documentation for manipulating sets and frozensets. - The `add` method should raise a `TypeError` if `MyFrozenSet` is attempted to be modified. Write the implementation for the `MySet` and `MyFrozenSet` classes based on the above requirements.","solution":"class MySet: def __init__(self, iterable=None): self._set = set(iterable) if iterable else set() def add(self, element): self._set.add(element) def contains(self, element): return element in self._set class MyFrozenSet: def __init__(self, iterable=None): self._frozenset = frozenset(iterable) if iterable else frozenset() def contains(self, element): return element in self._frozenset"},{"question":"# Sparse Data Structures in Pandas You have been given data from a sensor network collected over a month. The data contains many missing values (`NaN`), as the sensors occasionally go offline. The data is stored in a CSV file `sensor_data.csv` with the following columns: - `sensor_id`: Identifier for the sensor. - `timestamp`: Timestamps of the data points. - `reading`: The sensor reading, which may be `NaN`. Here is an example of what the data might look like: ``` sensor_id,timestamp,reading 1,2023-01-01 00:00:00,2.5 1,2023-01-01 01:00:00,NaN ...,...,... 2,2023-01-01 00:00:00,1.7 2,2023-01-01 01:00:00,NaN ...,...,... ``` Your task is to process this data using pandas sparse structures to efficiently handle the missing values. Requirements 1. **Loading Data**: Load the data from `sensor_data.csv`. 2. **Sparse Representation**: Convert relevant columns to an appropriate sparse representation. 3. **Data Aggregation**: Calculate the average reading per sensor for the entire month, skipping `NaN` values. 4. **Total Memory Usage**: Compare and output the memory usage (in bytes) of the dense DataFrame vs the sparse DataFrame. 5. **Result Export**: Export the results (sensor_id and average reading) to a new CSV file `sensor_avg_readings.csv`. Constraints - You may assume the input CSV file does not contain any duplicated rows. - Handle any potential edge cases (e.g., sensors with all `NaN` readings). Expected Input - A CSV file named `sensor_data.csv` in the current directory. Expected Output - A comparison of memory usage (in bytes) for dense and sparse representations. - A CSV file `sensor_avg_readings.csv` containing the average reading per sensor. Function Signature ```python def process_sensor_data(input_csv: str, output_csv: str) -> None: # Your code here pass ``` Example Usage ```python process_sensor_data(\'sensor_data.csv\', \'sensor_avg_readings.csv\') ``` Good luck! Demonstrate your understanding of pandas sparse structures and ensure your code handles the data efficiently.","solution":"import pandas as pd def process_sensor_data(input_csv: str, output_csv: str) -> None: # Load the data from the CSV file df = pd.read_csv(input_csv) # Convert the \'reading\' column to a sparse representation df[\'reading\'] = pd.arrays.SparseArray(df[\'reading\']) # Group by \'sensor_id\' and calculate the average reading, skipping NaNs avg_readings = df.groupby(\'sensor_id\')[\'reading\'].mean().reset_index() avg_readings.columns = [\'sensor_id\', \'average_reading\'] # Calculate memory usage for the dense and sparse DataFrames dense_memory_usage = df.memory_usage(deep=True).sum() sparse_memory_usage = df.memory_usage(index=True, deep=True).sum() # Print the memory usage comparison print(f\\"Dense DataFrame memory usage: {dense_memory_usage} bytes\\") print(f\\"Sparse DataFrame memory usage: {sparse_memory_usage} bytes\\") # Export the results to a new CSV file avg_readings.to_csv(output_csv, index=False)"},{"question":"# Question: Tuning Decision Threshold for Binary Classification **Objective**: Your task is to demonstrate your understanding of tuning the decision threshold for a classifier in scikit-learn using the `TunedThresholdClassifierCV` class. **Description**: You need to perform the following steps: 1. Load a binary classification dataset. 2. Split the dataset into training and testing sets. 3. Train a binary classifier on the training data. 4. Use `TunedThresholdClassifierCV` to tune the decision threshold of the classifier to optimize the recall score. 5. Evaluate the performance of the classifier with the optimized threshold on the test data. **Requirements**: - Use the `make_classification` function from `sklearn.datasets` to generate a synthetic dataset with class imbalance. - Split the dataset into 70% training and 30% testing. - Use a `LogisticRegression` classifier as the base model. - Tune the decision threshold to maximize the recall score using 5-fold cross-validation. - Report both the recall and precision scores on the test data with the optimized threshold. **Input and Output**: - No explicit input parameters are required; generate and split the dataset within the script. - Output the recall and precision scores on the test data after tuning the threshold. **Constraints**: - Ensure reproducibility by setting random states where applicable. - Properly handle class imbalance in the dataset. **Performance Requirements**: - Your solution should be efficient and run within the constraints of typical computational resources for an assessment environment. **Starter Code**: ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score, precision_score # Step 1: Generate a synthetic binary classification dataset X, y = make_classification( n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=0) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Step 3: Train a binary classifier base_model = LogisticRegression() # Step 4: Tune the decision threshold to maximize recall score using 5-fold cross-validation scorer = make_scorer(recall_score) model = TunedThresholdClassifierCV(base_model, scoring=scorer) model.fit(X_train, y_train) # Step 5: Evaluate the performance on the test data y_pred = model.predict(X_test) recall = recall_score(y_test, y_pred) precision = precision_score(y_test, y_pred) print(f\\"Recall: {recall}\\") print(f\\"Precision: {precision}\\") ``` You are required to write a complete script that performs the steps outlined above and outputs the recall and precision scores. Make sure to follow the guidelines and ensure the reproducibility of your solution.","solution":"from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import make_scorer, recall_score, precision_score from sklearn.model_selection import cross_val_predict import numpy as np class TunedThresholdClassifierCV: def __init__(self, base_model, scoring, cv=5): self.base_model = base_model self.scoring = scoring self.cv = cv self.best_threshold = 0.5 def fit(self, X, y): probabilities = cross_val_predict(self.base_model, X, y, cv=self.cv, method=\'predict_proba\')[:, 1] thresholds = np.linspace(0, 1, 101) best_score = -np.inf for threshold in thresholds: preds = (probabilities >= threshold).astype(int) score = recall_score(y, preds) if score > best_score: best_score = score self.best_threshold = threshold self.base_model.fit(X, y) def predict(self, X): probabilities = self.base_model.predict_proba(X)[:, 1] return (probabilities >= self.best_threshold).astype(int) # Step 1: Generate a synthetic binary classification dataset X, y = make_classification( n_samples=1000, n_features=20, n_classes=2, weights=[0.1, 0.9], random_state=0) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Step 3: Train a binary classifier base_model = LogisticRegression() # Step 4: Tune the decision threshold to maximize recall score using 5-fold cross-validation scorer = make_scorer(recall_score) model = TunedThresholdClassifierCV(base_model, scoring=scorer) model.fit(X_train, y_train) # Step 5: Evaluate the performance on the test data y_pred = model.predict(X_test) recall = recall_score(y_test, y_pred) precision = precision_score(y_test, y_pred) print(f\\"Recall: {recall}\\") print(f\\"Precision: {precision}\\")"},{"question":"Web Scraper with `urllib` **Objective**: Implement a Python function that demonstrates your understanding of the `urllib` package. Problem Statement: Design a function named `fetch_and_extract_links` that performs the following tasks: 1. Takes a URL as input. 2. Checks the `robots.txt` file of the provided URL to ensure the path is allowed for web scraping. 3. If scraping is allowed: - Fetches the HTML content of the given URL. - Extracts and returns a list of all hyperlinks (URLs) found within the HTML content. 4. If scraping is not allowed, the function should raise a custom exception named `NotAllowedError`. Function Signature: ```python def fetch_and_extract_links(url: str) -> list: pass ``` Custom Exception: ```python class NotAllowedError(Exception): pass ``` Input: - `url` (str): The URL of the web page to scrape. Output: - Returns a list of strings, each representing a URL (hyperlink) found in the HTML content. Constraints: - You must use the `urllib` package to complete this task. - Ensure proper error handling if the URL is invalid or any network issues occur. - The function should handle HTTP status codes properly and raise appropriate exceptions for errors (e.g., `HTTPError`). Example: ```python # Suppose the URL\'s robots.txt allows scraping and the content contains 3 links. url = \\"http://example.com\\" try: links = fetch_and_extract_links(url) print(links) # Output might be: [\\"http://example.com/link1\\", \\"http://example.com/link2\\", \\"http://example.com/link3\\"] except NotAllowedError: print(\\"Scraping is not allowed for this URL.\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\") ``` Tips: - Use `urllib.request` to open and read URLs. - Use `urllib.robotparser` to parse `robots.txt`. - Handle exceptions from `urllib.error`. - Use `urllib.parse` to handle and manipulate URLs if necessary. This question will test your ability to integrate different components of the `urllib` package and handle real-world web scraping constraints and exceptions.","solution":"import urllib.request import urllib.parse import urllib.robotparser from urllib.error import URLError, HTTPError from html.parser import HTMLParser class NotAllowedError(Exception): pass class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.links.append(attr[1]) def fetch_and_extract_links(url: str) -> list: # Check robots.txt parsed_url = urllib.parse.urlparse(url) robots_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) try: rp.read() except URLError as e: raise Exception(f\\"Error fetching robots.txt: {e}\\") if not rp.can_fetch(\\"*\\", url): raise NotAllowedError(\\"Scraping is not allowed for this URL.\\") try: response = urllib.request.urlopen(url) if response.status != 200: raise HTTPError(url, response.status, response.reason, response.headers, None) html_content = response.read().decode() except HTTPError as e: raise Exception(f\\"HTTP error: {e}\\") except URLError as e: raise Exception(f\\"URL error: {e}\\") # Extract links from HTML content parser = LinkExtractor() parser.feed(html_content) return parser.links"},{"question":"**Question:** You are given a deep learning task that requires optimizing workload performance using modern x86 CPU capabilities. Write a PyTorch program to: 1. Check if your machine supports AVX-512 or AMX instructions. 2. Create a tensor and apply basic linear algebra operations optimized for performance using any supported SIMD instruction set. **Requirements:** 1. Use the `collect_env.py` script to determine if your machine has AVX-512 or AMX support. 2. If AVX-512 or AMX is supported: * Create a tensor `A` of size (1000, 1000) initialized with random values. * Perform matrix multiplication of `A` with another tensor `B` of size (1000, 1000), also initialized with random values. * Compute the sum of all elements in the resulting tensor. * Measure and print the time taken to perform the matrix multiplication and summation. 3. If neither AVX-512 nor AMX is supported, print a message stating that the required instruction set is not available. **Input:** - No user input is required. **Output:** - If AVX-512 or AMX is supported, output the time taken to perform the matrix operations. - If neither AVX-512 nor AMX is supported, output a message: \\"Required SIMD instruction set not available.\\" **Constraints:** - You may use any PyTorch functions and utilities necessary to complete the task. - Assume PyTorch and all dependencies are correctly installed. **Example Output:** ``` AVX-512 is supported. Matrix multiplication and summation completed in 0.02 seconds. ``` OR ``` Required SIMD instruction set not available. ``` **Note:** The specific time mentioned in the output is just an example. Your actual runtime may vary based on your system\'s performance.","solution":"import torch import time import subprocess def check_simd_support(): simd_info = subprocess.check_output(\\"lscpu\\", shell=True).decode() if \'avx512\' in simd_info.lower(): return \'AVX-512\' elif \'amx\' in simd_info.lower(): return \'AMX\' else: return None def main(): simd_support = check_simd_support() if simd_support: print(f\\"{simd_support} is supported.\\") # Create the tensors A = torch.randn(1000, 1000) B = torch.randn(1000, 1000) # Measure time for matrix multiplication and summation start_time = time.time() C = torch.matmul(A, B) result_sum = C.sum().item() end_time = time.time() print(f\\"Matrix multiplication and summation completed in {end_time - start_time:.4f} seconds.\\") else: print(\\"Required SIMD instruction set not available.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question: URL Data Fetching and Processing** You are provided with a URL, and your task is to write a Python function `fetch_and_process_url(url: str) -> dict` that performs the following steps: 1. **Respect robots.txt**: Before fetching any data, the function should check the provided URL\'s \\"robots.txt\\" to see if crawling is allowed. If the URL is disallowed by \\"robots.txt\\", raise a `PermissionError` with a message indicating that access is not permitted. 2. **Fetch Data**: If crawling is allowed, fetch the content from the URL using `urllib.request`. The function should handle HTTP errors gracefully and should not crash if an error occurs. 3. **Parse Data**: If the fetching is successful, assume the content is in JSON format. Parse the JSON data and return it as a Python dictionary. If the content is not valid JSON, raise a `ValueError` with a proper error message. 4. **Handle Errors**: Ensure that any errors related to the HTTP request, URL parsing, or JSON parsing are appropriately handled with informative error messages. # Input: - `url` (str): The URL to fetch and process data from. # Output: - Returns a dictionary containing the parsed JSON data from the URL. # Constraints: - You may use the following modules: `urllib.request`, `urllib.parse`, `urllib.error`, `urllib.robotparser`. - The function should execute within a reasonable time frame for typical network operations (e.g., < 5 seconds). # Performance Requirements: - Network requests should be handled efficiently without redundant actions. # Example: ```python def fetch_and_process_url(url): # Your implementation here # Example usage: try: data = fetch_and_process_url(\\"http://example.com/data\\") print(data) except Exception as e: print(e) ``` # Explanation Given a URL like \\"http://example.com/data\\", your function should: 1. Check \\"http://example.com/robots.txt\\" for permissions. 2. Fetch and read the content from \\"http://example.com/data\\" if allowed. 3. Parse the content assuming it is JSON. 4. Return the parsed dictionary or raise an appropriate error if any step fails.","solution":"import urllib.request import urllib.parse import urllib.error import urllib.robotparser import json def fetch_and_process_url(url: str) -> dict: Checks robots.txt for the URL, fetches content if allowed, and returns the parsed JSON data as a dictionary. # Parse the URL to get the base URL parsed_url = urllib.parse.urlparse(url) base_url = f\\"{parsed_url.scheme}://{parsed_url.netloc}\\" robots_txt_url = urllib.parse.urljoin(base_url, \'/robots.txt\') # Check robots.txt for crawling permissions rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_txt_url) try: rp.read() can_fetch = rp.can_fetch(\'*\', url) if not can_fetch: raise PermissionError(\\"Crawling disallowed by robots.txt\\") except urllib.error.URLError: # If robots.txt is not found, assume crawling is allowed pass # Fetch the URL content try: response = urllib.request.urlopen(url) content = response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: raise urllib.error.HTTPError(url, e.code, e.reason, e.headers, e.fp) except urllib.error.URLError as e: raise urllib.error.URLError(e.reason) # Parse the JSON content try: data = json.loads(content) except json.JSONDecodeError: raise ValueError(\\"Invalid JSON content\\") return data"},{"question":"# HMAC Secure Message Verification You are given a Python script that generates and verifies HMACs for secure message authentication. Your task is to complete this script by implementing the necessary functionality. The script provides an HMAC class that encapsulates the use of the `hmac` module functions. Requirements 1. Implement the class `SecureHMAC` with the following methods: - `__init__(self, key, digestmod)`: Initializes the HMAC object with the given key and digest method. - `update(self, msg)`: Updates the HMAC object with the given message. - `get_hexdigest(self)`: Returns the hexadecimal digest of the message. - `verify(self, received_hexdigest)`: Compares the internally computed hexdigest with the one provided and returns a boolean indicating if they are the same. 2. Additionally, implement a standalone function `generate_hmac(key, msg, digestmod)` that generates an HMAC for a given message. 3. Implement another function `secure_compare(digest1, digest2)` that securely compares two digests using `hmac.compare_digest()`. Input and Output Formats - The `SecureHMAC` class constructor takes: - `key` (bytes): The secret key for HMAC. - `digestmod` (str): The name of the digest algorithm to use (e.g., \'sha256\'). - The `update(msg)` method takes: - `msg` (bytes): The message to update the HMAC object with. - The `get_hexdigest()` method returns: - (str): The hexdigest of the message. - The `verify(received_hexdigest)` method takes: - `received_hexdigest` (str): The hexdigest to compare against. - Returns (bool): `True` if the digests match, otherwise `False`. - The `generate_hmac(key, msg, digestmod)` function takes: - `key` (bytes): The secret key. - `msg` (bytes): The message to generate HMAC for. - `digestmod` (str): The digest method to use. - Returns (str): The generated HMAC hexdigest. - The `secure_compare(digest1, digest2)` function takes: - `digest1` (str): First hexadecimal digest to compare. - `digest2` (str): Second hexadecimal digest to compare. - Returns (bool): `True` if the two digests are the same, otherwise `False`. Constraints - The key and message must be non-empty bytes objects. - The digest method must be a valid algorithm supported by hashlib (e.g., \'md5\', \'sha256\', \'sha1\'). Example Usage ```python # Create an instance of SecureHMAC key = b\'secret_key\' msg = b\'Important message.\' digestmod = \'sha256\' hmac_instance = SecureHMAC(key, digestmod) hmac_instance.update(msg) hexdigest = hmac_instance.get_hexdigest() print(\\"Generated Hex Digest:\\", hexdigest) # Verification received = \'...received hex digest...\' is_valid = hmac_instance.verify(received) print(\\"Is valid:\\", is_valid) # Standalone functions generated_hmac = generate_hmac(key, msg, digestmod) print(\\"Generated HMAC using function:\\", generated_hmac) comparison_result = secure_compare(generated_hmac, received) print(\\"Secure comparison result:\\", comparison_result) ``` Implement these features to complete the functionality as specified.","solution":"import hmac import hashlib class SecureHMAC: def __init__(self, key, digestmod): self.key = key self.digestmod = digestmod self.hmac_obj = hmac.new(self.key, digestmod=digestmod) def update(self, msg): self.hmac_obj.update(msg) def get_hexdigest(self): return self.hmac_obj.hexdigest() def verify(self, received_hexdigest): return hmac.compare_digest(self.get_hexdigest(), received_hexdigest) def generate_hmac(key, msg, digestmod): h = hmac.new(key, msg, digestmod) return h.hexdigest() def secure_compare(digest1, digest2): return hmac.compare_digest(digest1, digest2)"},{"question":"# Advanced Python Object Manipulation **Objective:** Implement a Python class `CustomObject` that demonstrates rich comparison, attribute manipulation, and iteration using the methodologies inspired by the Object Protocol API. **Specifications:** 1. **Class Definition:** - Create a class `CustomObject` that includes an initializer to set up an object with a dictionary of attributes. - Include methods to dynamically add, delete, and retrieve attributes. 2. **Rich Comparison:** - Implement rich comparison methods to compare attributes. - Enable comparison operations (`<`, `<=`, `==`, `!=`, `>`, `>=`) based on the values of a specific attribute, e.g., \'value\'. 3. **Iteration:** - Make the objects iterable, iterating over their attribute names. - Include both synchronous and asynchronous iteration capabilities. **Input and Output:** Implement the following methods: 1. `__init__(self, attr_dict)`: Initialize the object with a dictionary of attributes. 2. `set_attr(self, attr_name, value)`: Dynamically set a new attribute or update an existing one. 3. `del_attr(self, attr_name)`: Delete an attribute. 4. `get_attr(self, attr_name)`: Retrieve the value of an attribute. 5. Rich comparison methods to handle `__lt__`, `__le__`, `__eq__`, `__ne__`, `__gt__`, and `__ge__`. 6. `__iter__(self)`: Enable synchronous iteration over attribute names. 7. `__aiter__(self)`: Enable asynchronous iteration over attribute names. **Constraints:** - Ensure attribute access and modification handle exceptions appropriately. - Use Python 3.10+ features for asynchronous iteration. **Performance:** - The class methods should handle large dictionaries of attributes efficiently. - Iteration should be lazy and maintain minimal memory footprint. **Sample Code Usage:** ```python # Initialization obj = CustomObject({\'value\': 10, \'name\': \'CustomObject\'}) # Attribute manipulation obj.set_attr(\'new_attr\', 100) print(obj.get_attr(\'new_attr\')) # Output: 100 obj.del_attr(\'new_attr\') # Comparison obj2 = CustomObject({\'value\': 20}) print(obj < obj2) # Output: True # Iteration for attr in obj: print(attr) # Asynchronous iteration async for attr in obj: print(attr) ``` **Task:** Implement the `CustomObject` class to meet the above requirements and demonstrate functionality with sample usage.","solution":"class CustomObject: CustomObject class demonstrates rich comparison, attribute manipulation, and iteration using the methodologies inspired by the Object Protocol API. def __init__(self, attr_dict): self._attributes = attr_dict def set_attr(self, attr_name, value): self._attributes[attr_name] = value def del_attr(self, attr_name): if attr_name in self._attributes: del self._attributes[attr_name] def get_attr(self, attr_name): return self._attributes.get(attr_name, None) def __lt__(self, other): return self.get_attr(\'value\') < other.get_attr(\'value\') def __le__(self, other): return self.get_attr(\'value\') <= other.get_attr(\'value\') def __eq__(self, other): return self.get_attr(\'value\') == other.get_attr(\'value\') def __ne__(self, other): return self.get_attr(\'value\') != other.get_attr(\'value\') def __gt__(self, other): return self.get_attr(\'value\') > other.get_attr(\'value\') def __ge__(self, other): return self.get_attr(\'value\') >= other.get_attr(\'value\') def __iter__(self): return iter(self._attributes) async def __aiter__(self): for attr in self._attributes: yield attr"},{"question":"Coding Assessment Question # Objective You are tasked with implementing functionality that utilizes the `torch.mtia` module to manage device streams and memory for optimal computational efficiency in a PyTorch environment. # Question Write a Python function `memory_efficient_computation` that performs the following tasks: 1. Initializes and checks if the `torch.mtia` backend is available and initialized. 2. Sets the current device to the one with the highest compute capability. 3. Creates a custom stream and performs a specific dummy computation within this stream context. 4. Gathers and prints memory usage statistics before and after the computation. 5. Ensures all streams are synchronized before returning the result of the computation. You can use a simple tensor operation (e.g., addition of two tensors) as the dummy computation. Ensure that the memory statistics and synchronization are handled correctly. # Function Signature ```python def memory_efficient_computation() -> torch.Tensor: pass ``` # Expected Input and Output - **Input**: No input arguments are required. - **Output**: A `torch.Tensor` which is the result of the dummy computation. # Constraints - The function should handle potential errors gracefully and print appropriate error messages if any step fails. - It must ensure that all memory and device management operations are properly synchronized and cleaned up. # Example Although the imported functions/classes are to be used internally, here\'s a brief example illustrating the expected structure: ```python import torch import torch.mtia as mtia def memory_efficient_computation() -> torch.Tensor: # Step 1: Initialize and check availability mtia.init() if not mtia.is_available(): raise RuntimeError(\\"MTIA backend is not available.\\") if not mtia.is_initialized(): raise RuntimeError(\\"MTIA backend failed to initialize.\\") # Step 2: Select device with highest compute capability device_capabilities = [mtia.get_device_capability(i) for i in range(mtia.device_count())] best_device = device_capabilities.index(max(device_capabilities)) mtia.set_device(best_device) # Step 3: Create a custom stream and perform a computation stream = mtia.Stream() with mtia.StreamContext(stream): a = torch.tensor([1.0, 2.0, 3.0]).cuda() b = torch.tensor([4.0, 5.0, 6.0]).cuda() result = a + b # Step 4: Get memory stats before_memory = mtia.memory_stats(best_device)[\\"allocated_bytes\\"] after_memory = mtia.memory_stats(best_device)[\\"allocated_bytes\\"] print(f\\"Memory before computation: {before_memory}\\") print(f\\"Memory after computation: {after_memory}\\") # Step 5: Synchronize streams mtia.synchronize() return result # Calling the function result = memory_efficient_computation() print(result) ``` This function will execute tensor addition within a custom stream on the device with the highest compute capability, while gathering memory statistics and ensuring synchronization. This should give a good understanding of device and stream management in PyTorch using the `torch.mtia` module.","solution":"import torch import torch.cuda as cuda def memory_efficient_computation() -> torch.Tensor: # Step 1: Initialize and check availability if not cuda.is_available(): raise RuntimeError(\\"CUDA backend is not available.\\") cuda_device_count = cuda.device_count() if cuda_device_count == 0: raise RuntimeError(\\"No CUDA devices available.\\") # Step 2: Select device with highest compute capability device_capabilities = [cuda.get_device_capability(i) for i in range(cuda_device_count)] best_device = device_capabilities.index(max(device_capabilities)) cuda.set_device(best_device) # Step 3: Create a custom stream and perform a computation stream = torch.cuda.Stream() with torch.cuda.stream(stream): a = torch.tensor([1.0, 2.0, 3.0], device=f\'cuda:{best_device}\') b = torch.tensor([4.0, 5.0, 6.0], device=f\'cuda:{best_device}\') result = a + b # Step 4: Get memory stats before_memory = torch.cuda.memory_allocated(best_device) after_memory = torch.cuda.memory_allocated(best_device) print(f\\"Memory allocated before computation: {before_memory}\\") print(f\\"Memory allocated after computation: {after_memory}\\") # Step 5: Synchronize streams stream.synchronize() return result"},{"question":"# Question You are required to implement a distributed training setup using the `torch.distributed` package. Your tasks include initializing a process group, performing collective communication operations, and demonstrating debugging techniques. Follow the steps below to complete this question: **Part 1: Initialization** 1. Write a function `init_distributed` that initializes the process group using TCP initialization. Use the following parameters: - `backend` (str): The backend to use, such as \\"gloo\\" or \\"nccl\\". - `address` (str): The address of rank 0. You can assume all processes are on the same machine with this address for simplicity. - `port` (int): The port number for the TCP connection. - `rank` (int): The rank of the current process. - `world_size` (int): The total number of processes. ```python import torch.distributed as dist def init_distributed(backend, address, port, rank, world_size): # Your code here ``` **Part 2: Collective Communication** 2. Write a function `run_collective_operations` that performs the following operations: - Initialize a tensor on each process with values corresponding to its rank. - Perform an `all_reduce` operation with `SUM` to aggregate the tensors across all processes. - Broadcast the aggregated tensor from rank 0 to all other ranks. - Print the result of the broadcasted tensor on each rank. ```python def run_collective_operations(): # Your code here ``` **Part 3: Debugging and Clean-up** 3. Write a function `debug_distributed` that sets up debugging using `torch.distributed.monitored_barrier` and logs useful information. It should: - Set appropriate environment variables for debugging. - Use `monitored_barrier` to ensure all processes reach synchronization points and log the status. - Ensure clean-up of resources by destroying the process group at the end. ```python def debug_distributed(): # Your code here ``` **Constraints and Notes:** - Assume that the initialization parameters for the `init_distributed` function are provided correctly. - Your solution should be able to run in a multi-process environment, which you can simulate using `torch.multiprocessing.spawn`. - Ensure to handle exceptions and log relevant debugging information for synchronizing the processes. **Example Usage:** ```python if __name__ == \\"__main__\\": import torch.multiprocessing as mp def main(rank, world_size): backend = \\"gloo\\" # or \\"nccl\\" if using GPUs address = \\"127.0.0.1\\" port = 23456 init_distributed(backend, address, port, rank, world_size) run_collective_operations() debug_distributed() world_size = 4 # Number of processes mp.spawn(main, args=(world_size,), nprocs=world_size, join=True) ``` Submit your implementation for `init_distributed`, `run_collective_operations`, and `debug_distributed`.","solution":"import torch import torch.distributed as dist def init_distributed(backend, address, port, rank, world_size): Initializes the distributed process group. :param backend: The backend to use, such as \\"gloo\\" or \\"nccl\\". :param address: The address of rank 0. :param port: The port number for the TCP connection. :param rank: The rank of the current process. :param world_size: The total number of processes. dist.init_process_group( backend=backend, init_method=f\'tcp://{address}:{port}\', rank=rank, world_size=world_size ) def run_collective_operations(): Performs collective communication operations: - Initialize a tensor on each process with values corresponding to its rank. - Perform an all_reduce operation with SUM to aggregate the tensors. - Broadcast the aggregated tensor from rank 0 to all other ranks. - Print the result of the broadcasted tensor on each rank. rank = dist.get_rank() world_size = dist.get_world_size() # Initialize a tensor with values corresponding to its rank tensor = torch.ones(1) * rank # Perform an all_reduce operation with SUM to aggregate the tensors dist.all_reduce(tensor, op=dist.ReduceOp.SUM) # Broadcast the aggregated tensor from rank 0 to all other ranks dist.broadcast(tensor, src=0) print(f\\"Rank {rank} has tensor: {tensor.item()}\\") def debug_distributed(): Sets up debugging using torch.distributed.monitored_barrier and logs useful information. Ensures clean-up of resources by destroying the process group at the end. import os import logging # Set environment variables for debugging os.environ[\\"NCCL_DEBUG\\"] = \\"INFO\\" os.environ[\\"NCCL_DEBUG_SUBSYS\\"] = \\"ALL\\" logging.basicConfig(level=logging.DEBUG) try: rank = dist.get_rank() world_size = dist.get_world_size() # Use monitored_barrier to ensure synchronization points and log the status for i in range(world_size): if rank == i: logging.debug(f\\"Rank {rank}: Reached monitored barrier {i}\\") dist.monitored_barrier() except Exception as e: logging.error(f\\"Rank {rank}: An exception occurred - {e}\\") finally: # Clean up resources by destroying the process group dist.destroy_process_group()"},{"question":"# Unicode Character Properties and Normalization You need to write a Python function that processes a list of Unicode characters and returns a detailed summary of their properties. The summary should include the following properties for each character in the list: 1. The character itself. 2. The name of the character. 3. The decimal value (if applicable). 4. The digit value (if applicable). 5. The numeric value (if applicable). 6. The general category. 7. The bidirectional class. 8. The canonical combining class. 9. The East Asian width. 10. Whether the character has mirrored property or not. 11. The decomposition mapping. 12. The normalized forms (NFC, NFKC, NFD, NFKD). 13. Whether the character string is already normalized in the provided forms (NFC, NFKC, NFD, NFKD). Implement the function `unicode_summary(char_list)` that: **Input:** - `char_list` (List of strings): A list of single Unicode characters. **Output:** - Returns a list of dictionaries where each dictionary represents the properties for a single character in the input list. Each dictionary should have the keys: - `char` - `name` - `decimal` - `digit` - `numeric` - `category` - `bidirectional` - `combining` - `east_asian_width` - `mirrored` - `decomposition` - `normalized_forms` (a dictionary with keys `NFC`, `NFKC`, `NFD`, `NFKD` and boolean values indicating if the character string is already normalized in that form) **Constraints:** - You may assume all input characters are valid Unicode characters. - If a property is not applicable or available for a character, the corresponding value should be `None`. Here is the function signature: ```python def unicode_summary(char_list: list) -> list: # Function implementation here ``` # Example: ```python import unicodedata def unicode_summary(char_list): summary = [] for char in char_list: char_info = { \\"char\\": char, \\"name\\": unicodedata.name(char, None), \\"decimal\\": unicodedata.decimal(char, None), \\"digit\\": unicodedata.digit(char, None), \\"numeric\\": unicodedata.numeric(char, None), \\"category\\": unicodedata.category(char), \\"bidirectional\\": unicodedata.bidirectional(char), \\"combining\\": unicodedata.combining(char), \\"east_asian_width\\": unicodedata.east_asian_width(char), \\"mirrored\\": unicodedata.mirrored(char), \\"decomposition\\": unicodedata.decomposition(char), \\"normalized_forms\\": { \\"NFC\\": unicodedata.is_normalized(\\"NFC\\", char), \\"NFKC\\": unicodedata.is_normalized(\\"NFKC\\", char), \\"NFD\\": unicodedata.is_normalized(\\"NFD\\", char), \\"NFKD\\": unicodedata.is_normalized(\\"NFKD\\", char), } } summary.append(char_info) return summary # Example usage char_list = [\\"A\\", \\"1\\", \\"Â©\\", \\"Â¾\\"] print(unicode_summary(char_list)) ``` This example should help you understand the expected input and output format. Your implementation should correctly handle any Unicode character and return the appropriate properties.","solution":"import unicodedata def unicode_summary(char_list): summary = [] for char in char_list: char_info = { \\"char\\": char, \\"name\\": unicodedata.name(char, None), \\"decimal\\": unicodedata.decimal(char, None), \\"digit\\": unicodedata.digit(char, None), \\"numeric\\": unicodedata.numeric(char, None), \\"category\\": unicodedata.category(char), \\"bidirectional\\": unicodedata.bidirectional(char), \\"combining\\": unicodedata.combining(char), \\"east_asian_width\\": unicodedata.east_asian_width(char), \\"mirrored\\": unicodedata.mirrored(char), \\"decomposition\\": unicodedata.decomposition(char), \\"normalized_forms\\": { \\"NFC\\": unicodedata.is_normalized(\\"NFC\\", char), \\"NFKC\\": unicodedata.is_normalized(\\"NFKC\\", char), \\"NFD\\": unicodedata.is_normalized(\\"NFD\\", char), \\"NFKD\\": unicodedata.is_normalized(\\"NFKD\\", char), } } summary.append(char_info) return summary"},{"question":"**Question: Reproducible Regression Workflow with Scikit-learn** You are required to write a Python function that creates a synthetic regression dataset, applies a machine learning model from scikit-learn, splits the data, scales it, trains the model, makes predictions, and evaluates the model\'s performance. Your function must demonstrate your understanding of fundamental scikit-learn concepts and should be self-contained. # Function Signature ```python def regression_workflow(n_samples: int) -> float: pass ``` # Input - `n_samples (int)`: The number of samples to be generated for the synthetic dataset. # Output - `float`: The R^2 score of the model on the test set. # Requirements 1. **Data Generation**: - Create a synthetic regression dataset with `n_samples` samples and 10 features using `sklearn.datasets.make_regression`. 2. **Data Splitting**: - Split the dataset into a training set (70%) and a test set (30%) using `sklearn.model_selection.train_test_split`. 3. **Data Scaling**: - Standardize the features (mean 0, variance 1) using `StandardScaler` from `sklearn.preprocessing`. 4. **Model Training**: - Train a `GradientBoostingRegressor` model using the training set. 5. **Prediction and Evaluation**: - Predict the target values for the test set. - Calculate and return the R^2 score of the model on the test set using `score` method of `GradientBoostingRegressor`. # Example ```python result = regression_workflow(1000) print(result) # Output should be the R^2 score of the model ``` # Constraints - Ensure the code is minimal and readable. - Import necessary libraries within the function (no global imports). - The function should be self-contained and reproducible. This question assesses your ability to use scikit-learn for a common task in machine learning. Focus on making your function clear, concise, and capable of running without external dependencies apart from the specified libraries.","solution":"def regression_workflow(n_samples: int) -> float: from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score # Generate synthetic regression dataset X, y = make_regression(n_samples=n_samples, n_features=10, noise=0.1, random_state=42) # Split the data into training (70%) and test sets (30%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train a GradientBoostingRegressor model model = GradientBoostingRegressor(random_state=42) model.fit(X_train_scaled, y_train) # Predict the target values for the test set y_pred = model.predict(X_test_scaled) # Calculate and return the R^2 score r2 = r2_score(y_test, y_pred) return r2"},{"question":"# Pandas Coding Assessment: Display Options and Context Management **Objective:** Design a class `DataFrameDisplayManager` to manage the display options of pandas DataFrames. Your task is to implement this class with methods to get, set, reset, and temporarily change display options for pandas. **Details:** 1. **Class**: `DataFrameDisplayManager` 2. **Methods**: - `get_option(option_name: str) -> Any`: - Input: `option_name` (str) - Name of the option to fetch. - Output: Return the current value of the specified option. - `set_option(option_name: str, value: Any) -> None`: - Input: `option_name` (str) - Name of the option to set. `value` (Any) - Value to set for the specified option. - Output: None. - `reset_option(option_name: str) -> None`: - Input: `option_name` (str) - Name of the option to reset to its default value. - Output: None. - `temporary_set_options(options: Dict[str, Any], code_block: Callable) -> None`: - Input: `options` (Dict[str, Any]) - Dictionary of option names and their corresponding temporary values. `code_block` (Callable) - A function to execute with the temporary settings. - Output: None. The function should execute the given code block with the provided settings and revert to the original settings afterward. **Requirements:** - `get_option` should raise a `ValueError` if the option name is invalid. - `set_option` should ensure the given value is valid for the respective option. - `reset_option` should reset only the specified option. - `temporary_set_options` should leverage `pandas.option_context` to manage temporary settings and ensure reversion to original settings after execution. **Constraints:** 1. Only valid pandas options should be accepted. 2. The code block for temporary settings should support context management properly. 3. No external libraries other than pandas should be used. **Example Usage:** 1. Fetching, setting, and resetting options. ```python manager = DataFrameDisplayManager() print(manager.get_option(\\"display.max_rows\\")) # Should print the default or currently set value manager.set_option(\\"display.max_rows\\", 20) print(manager.get_option(\\"display.max_rows\\")) # Should print 20 manager.reset_option(\\"display.max_rows\\") print(manager.get_option(\\"display.max_rows\\")) # Should print the default value again ``` 2. Using temporary settings within a code block. ```python def sample_code(): print(pd.get_option(\\"display.max_rows\\")) # Should print 5 (temporary value) df = pd.DataFrame(np.random.randn(10, 5)) print(df) # Should display DataFrame with a max of 5 rows manager = DataFrameDisplayManager() manager.temporary_set_options({\\"display.max_rows\\": 5}, sample_code) print(pd.get_option(\\"display.max_rows\\")) # Should print the original (default or set) value. ``` **Notes:** - Ensure to test each method thoroughly. - Comment and document your code for clarity.","solution":"import pandas as pd from typing import Any, Dict, Callable class DataFrameDisplayManager: def get_option(self, option_name: str) -> Any: try: return pd.get_option(option_name) except KeyError: raise ValueError(f\\"Invalid option name: {option_name}\\") def set_option(self, option_name: str, value: Any) -> None: try: pd.set_option(option_name, value) except KeyError: raise ValueError(f\\"Invalid option name: {option_name}\\") def reset_option(self, option_name: str) -> None: try: pd.reset_option(option_name) except KeyError: raise ValueError(f\\"Invalid option name: {option_name}\\") def temporary_set_options(self, options: Dict[str, Any], code_block: Callable) -> None: with pd.option_context(*sum(options.items(), ())): code_block()"},{"question":"You are tasked with creating a specialized sorted collection using Python\'s `bisect` module. This collection should maintain a sorted list of elements and support efficient insertion, deletion, and search operations. # Requirements: 1. Implement a class `SortedCollection` with the following methods: - `__init__(self, iterable=(), *, key=None)`: Initializes the collection with an optional iterable of elements and an optional key function. - `insert(self, item)`: Inserts the item into the collection, maintaining the sorted order. - `remove(self, item)`: Removes the item from the collection if it exists; raises a `ValueError` if the item is not found. - `find_le(self, item)`: Finds the rightmost value less than or equal to `item`; raises a `ValueError` if no such item exists. - `find_ge(self, item)`: Finds the leftmost value greater than or equal to `item`; raises a `ValueError` if no such item exists. - `__len__(self)`: Returns the number of items in the collection. - `__getitem__(self, index)`: Returns the item at the specified index. 2. The collection must use the bisect module for all insertions and removals to maintain sorted order efficiently. 3. The key function, if provided, should be applied to elements for sorting and searching purposes. It should not be applied during insertion or deletion steps. # Constraints: - The elements of the collection can be of any type. - The key function, if provided, will always produce a consistent and comparable output. - The length of the initial iterable and any subsequently added elements will not exceed 10^5. # Example Usage: ```python from bisect import bisect_left, bisect_right, insort_left, insort_right class SortedCollection: def __init__(self, iterable=(), *, key=None): self._key = key or (lambda x: x) self._data = sorted(iterable, key=self._key) def insert(self, item): insort_left(self._data, item, key=self._key) def remove(self, item): key = self._key(item) pos = bisect_left(self._data, item, key=self._key) if pos != len(self._data) and self._key(self._data[pos]) == key: self._data.pop(pos) else: raise ValueError(f\'{item} not found in collection\') def find_le(self, item): pos = bisect_right(self._data, item, key=self._key) if pos: return self._data[pos-1] raise ValueError(f\'No item less than or equal to {item}\') def find_ge(self, item): pos = bisect_left(self._data, item, key=self._key) if pos != len(self._data): return self._data[pos] raise ValueError(f\'No item greater than or equal to {item}\') def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] # Example Usage: sc = SortedCollection([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]) sc.insert(7) print(len(sc)) # Output: 12 print(sc[0]) # Output: 1 print(sc.find_le(5)) # Output: 5 print(sc.find_ge(5)) # Output: 5 sc.remove(3) print(len(sc)) # Output: 11 ``` Implement the `SortedCollection` class as specified and ensure all methods meet the performance requirements.","solution":"from bisect import bisect_left, bisect_right, insort_left class SortedCollection: def __init__(self, iterable=(), *, key=None): self._key = key or (lambda x: x) self._data = sorted(iterable, key=self._key) def insert(self, item): insort_left(self._data, item, key=self._key) def remove(self, item): key = self._key(item) pos = bisect_left(self._data, item, key=self._key) if pos != len(self._data) and self._key(self._data[pos]) == key: self._data.pop(pos) else: raise ValueError(f\'{item} not found in collection\') def find_le(self, item): pos = bisect_right(self._data, item, key=self._key) if pos: return self._data[pos - 1] raise ValueError(f\'No item less than or equal to {item}\') def find_ge(self, item): pos = bisect_left(self._data, item, key=self._key) if pos != len(self._data): return self._data[pos] raise ValueError(f\'No item greater than or equal to {item}\') def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index]"},{"question":"# Email Processing with `imaplib` **Objective:** You are required to develop a Python function that interacts with an IMAP mail server to search for specific emails, fetch their details, and process them. This task will demonstrate your understanding of connecting to an IMAP server, authentication, mailbox selection, searching and fetching emails, and processing the retrieved information using the `imaplib` module. **Task:** Write a Python function `process_emails(server, email, password, search_criteria)` that performs the following operations: 1. **Connect and Authenticate:** - Connect to the specified IMAP server. - Log in using the provided email and password. 2. **Select Mailbox:** - Select the \'INBOX\' mailbox. 3. **Search Emails:** - Search for emails matching the given `search_criteria`. 4. **Fetch Emails:** - Fetch the subject and sender of each email that matches the search criteria. 5. **Print Email Information:** - Print the Subject and From fields of each matching email. **Function Signature:** ```python def process_emails(server: str, email: str, password: str, search_criteria: str) -> None: pass ``` **Parameters:** - `server` (str): The hostname of the IMAP server (e.g., \\"imap.gmail.com\\"). - `email` (str): The email address for login. - `password` (str): The password for login. - `search_criteria` (str): The search criteria for emails (e.g., \\"FROM \'example@example.com\'\\"). **Instructions:** - Use the `imaplib.IMAP4_SSL` class to establish an SSL-encrypted connection. - Handle potential exceptions that could arise during the connection, such as authentication errors or server unavailability. - Use appropriate functions from the `imaplib` module for searching and fetching emails. - Ensure all connections to the server are properly closed after the task is completed, whether successful or not. **Example Usage:** ```python server = \\"imap.gmail.com\\" email = \\"your_email@gmail.com\\" password = \\"your_password\\" search_criteria = \'FROM \\"example@example.com\\"\' process_emails(server, email, password, search_criteria) ``` **Example Output:** ``` Subject: Meeting Reminder From: boss@example.com Subject: Project Update From: colleague@example.com ``` **Constraints:** - Ensure that proper user authentication is handled securely. - Fetch only the \\"Subject\\" and \\"From\\" fields of the email for simplicity. **Notes:** - Assume that the user credentials provided are correct and the IMAP server is accessible. - The function should use IMAP commands and methods from the `imaplib` module to fulfill the requirements. **Performance Requirements:** - The solution should be efficient and handle possible network delays gracefully. - Implement proper exception handling to ensure the function does not crash due to common network or authentication issues. **Hint:** - You might need to use `M.fetch(num, \'(BODY[HEADER.FIELDS (SUBJECT FROM)])\')` to fetch only specific header fields.","solution":"import imaplib import email def process_emails(server: str, user: str, password: str, search_criteria: str) -> None: Connects to the IMAP server, authenticates the user, searches for emails based on given criteria, and prints the subject and sender of each matching email. :param server: The hostname of the IMAP server. :param user: The email address for login. :param password: The password for login. :param search_criteria: The search criteria for emails. try: # Connect to the server mail = imaplib.IMAP4_SSL(server) # Login to the account mail.login(user, password) # Select the \'INBOX\' mailbox mail.select(\\"inbox\\") # Search for emails based on the search criteria status, search_data = mail.search(None, search_criteria) if status != \'OK\': print(\\"No messages found!\\") return # Process each found email for num in search_data[0].split(): status, data = mail.fetch(num, \'(BODY[HEADER.FIELDS (SUBJECT FROM)])\') if status != \'OK\': print(f\\"Could not fetch email {num}\\") continue for response_part in data: if isinstance(response_part, tuple): message = email.message_from_bytes(response_part[1]) subject = message.get(\\"Subject\\") from_field = message.get(\\"From\\") print(f\\"Subject: {subject}\\") print(f\\"From: {from_field}\\") print() except imaplib.IMAP4.error as e: print(f\\"IMAP error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Logout and close connection try: mail.logout() except: pass"},{"question":"# Python Coding Assessment Question **Objective:** You are to implement a function that takes an expression evaluating to either a number, a string, a tuple of two elements, a list of integers, or a dictionary. Based on the type and content of the expression, the function should return a specific string message. Use Python 3.10\'s pattern matching feature to implement this function. **Function Signature:** ```python def match_expression(expr: Union[int, str, Tuple, List[int], Dict]) -> str: pass ``` **Input:** - `expr`: An expression that could be one of: - An integer. - A string. - A tuple containing exactly two elements. - A list of integers. - A dictionary with unspecified key-value pairs. **Output:** - A string representing a message based on the matched pattern of the expression. **Messages:** 1. If `expr` is an integer, return \\"Integer value detected: {expr}\\". 2. If `expr` is a string, return \\"String value detected: {expr}\\". 3. If `expr` is a tuple containing exactly two elements, return \\"Tuple with two elements: {elem1}, {elem2}\\". 4. If `expr` is a list of integers, return \\"List of integers detected: {list}\\". 5. If `expr` is a dictionary, return \\"Dictionary detected with {len(dict)} items\\". **Examples:** ```python print(match_expression(10)) # Output: Integer value detected: 10 print(match_expression(\\"hello\\")) # Output: String value detected: hello print(match_expression((1, 2))) # Output: Tuple with two elements: 1, 2 print(match_expression([1, 2, 3])) # Output: List of integers detected: [1, 2, 3] print(match_expression({\\"key\\": \\"value\\"})) # Output: Dictionary detected with 1 items ``` **Constraints:** - Use the pattern matching feature (`match` statement) introduced in Python 3.10. - Handle only the specified types and structures for the input `expr`. **Notes:** - Ensure the function is using pattern matching properly and effectively. - You may assume all inputs are valid and conform to one of the specified patterns.","solution":"from typing import Union, Tuple, List, Dict def match_expression(expr: Union[int, str, Tuple, List[int], Dict]) -> str: Matches the expression to determine its type and returns the appropriate message. match expr: case int(value): return f\\"Integer value detected: {value}\\" case str(value): return f\\"String value detected: {value}\\" case tuple((elem1, elem2)): return f\\"Tuple with two elements: {elem1}, {elem2}\\" case list(values) if all(isinstance(i, int) for i in values): return f\\"List of integers detected: {values}\\" case dict(values): return f\\"Dictionary detected with {len(values)} items\\" case _: return \\"Unsupported type detected\\""},{"question":"You are given a WAV file with mono (single channel) audio and you are required to create a new WAV file that contains the reversed audio of the original. Input - A string `input_file` representing the path to the input WAV file in read mode. - A string `output_file` representing the path to the output WAV file in write mode. Output - A new WAV file at the path `output_file` which has the reversed audio from `input_file`. Constraints - The input WAV file is guaranteed to be a mono audio file. - The frame rate (sample rate) and sample width of the output file should be the same as the input file. - Ensure to handle and close files appropriately to avoid any resource leakage. Example Usage ```python reverse_audio(\'input.wav\', \'output.wav\') ``` Function Signature ```python def reverse_audio(input_file: str, output_file: str) -> None: pass ``` Requirements - Open the input WAV file and read its properties (channels, sample width, frame rate, and frames). - Read all the frames from the input file. - Reverse the frame data. - Create a new WAV file and write the reversed frame data back, ensuring the properties match the original file. Performance - Efficiently handle the reading, reversing, and writing of frames even for large files. Hints - Make use of the `wave` module\'s methods for reading and writing WAV file properties and frames.","solution":"import wave def reverse_audio(input_file: str, output_file: str) -> None: with wave.open(input_file, \'rb\') as wf: params = wf.getparams() frames = wf.readframes(params.nframes) reversed_frames = frames[::-1] with wave.open(output_file, \'wb\') as wf: wf.setparams(params) wf.writeframes(reversed_frames)"},{"question":"Objective You are tasked with writing a Python function that demonstrates your understanding of the `torch.accelerator` module in PyTorch. This requires manipulating the active device and handling device streams to ensure efficient computation. Question Implement a function `accelerator_operations()` that performs the following steps: 1. Check if accelerators are available. If not, return the string `\\"No accelerators available\\"`. 2. Get the count of available devices. 3. Set the active device to the last available device. 4. Create and set a new stream for the active device. 5. Synchronize the new stream. 6. Return a dictionary with the following information: - `\'device_count\'`: The number of available devices. - `\'current_device\'`: The index of the current active device. - `\'stream_set\'`: A boolean indicating whether a new stream was successfully set. Function Signature ```python def accelerator_operations() -> dict: pass ``` Expected Output Format The function should return a dictionary with the specified keys and values based on the operations performed. Constraints - Ensure the function handles cases where no accelerators are available elegantly. - Follow best practices for device and stream management in PyTorch. # Example ```python output = accelerator_operations() print(output) ``` Sample expected output (this will vary depending on the system\'s hardware): ```json { \\"device_count\\": 2, \\"current_device\\": 1, \\"stream_set\\": True } ```","solution":"import torch def accelerator_operations() -> dict: # Check if any accelerators are available if not torch.cuda.is_available(): return \\"No accelerators available\\" result = {} # Get the count of available devices device_count = torch.cuda.device_count() result[\'device_count\'] = device_count # Set the active device to the last available device current_device = device_count - 1 torch.cuda.set_device(current_device) result[\'current_device\'] = current_device # Create and set a new stream for the active device stream = torch.cuda.Stream() torch.cuda.current_stream(device=current_device).wait_stream(stream) stream_set = True result[\'stream_set\'] = stream_set # Synchronize the new stream stream.synchronize() return result"},{"question":"# Advanced Coding Assessment: Covariance Estimation with Outlier Management Problem Statement You are given a dataset containing high-dimensional samples with potential outliers. Your task is to implement a program that performs the following steps: 1. Compute the empirical covariance matrix of the dataset. 2. Compute a shrunk covariance matrix using basic shrinkage. 3. Apply the Ledoit-Wolf shrinkage method to estimate the covariance matrix. 4. Apply the Oracle Approximating Shrinkage (OAS) method to estimate the covariance matrix. 5. Identify and manage outliers using the Minimum Covariance Determinant (MCD) estimator. Your function should output: - The empirical covariance matrix. - The shrunk covariance matrix. - The Ledoit-Wolf shrunk covariance matrix. - The OAS shrunk covariance matrix. - The robust covariance matrix excluding identified outliers. Input - `X`: A numpy array of shape `(n_samples, n_features)` representing the dataset. Output - A dictionary with the following keys and their corresponding covariance matrices (numpy arrays): - `\'empirical\'`: Empirical covariance matrix. - `\'shrunk\'`: Shrunk covariance matrix. - `\'ledoit_wolf\'`: Ledoit-Wolf shrunk covariance matrix. - `\'oas\'`: OAS shrunk covariance matrix. - `\'robust\'`: Robust covariance matrix (excluding outliers). Constraints - The input dataset `X` can have up to 5000 samples and 100 features. Implementation Notes - Use appropriate functions and classes from the `sklearn.covariance` module. - Ensure input data is centered where necessary. - Handle potential outliers sensitively and provide accurate robust covariance estimation. Example ```python import numpy as np from sklearn.covariance import (empirical_covariance, shrunk_covariance, LedoitWolf, OAS, MinCovDet) def covariance_estimations(X): # Empirical covariance empirical_cov = empirical_covariance(X) # Shrunk covariance with basic shrinkage alpha = 0.1 # example shrinkage parameter shrunk_cov = shrunk_covariance(empirical_cov, shrinkage=alpha) # Ledoit-Wolf shrinkage lw = LedoitWolf() lw.fit(X) ledoit_wolf_cov = lw.covariance_ # Oracle Approximating Shrinkage (OAS) oas = OAS() oas.fit(X) oas_cov = oas.covariance_ # Robust covariance estimation (MinCovDet) mcd = MinCovDet().fit(X) robust_cov = mcd.covariance_ return { \'empirical\': empirical_cov, \'shrunk\': shrunk_cov, \'ledoit_wolf\': ledoit_wolf_cov, \'oas\': oas_cov, \'robust\': robust_cov } # Example Usage X = np.random.randn(100, 5) # replace with real data results = covariance_estimations(X) for key, value in results.items(): print(f\\"{key} covariance matrix:n\\", value) ``` Performance Requirement The implementation should efficiently handle datasets with the maximum constraints specified.","solution":"import numpy as np from sklearn.covariance import (empirical_covariance, shrunk_covariance, LedoitWolf, OAS, MinCovDet) def covariance_estimations(X): Calculates various covariance matrices for a given dataset X. Parameters: - X (numpy array): Dataset of shape (n_samples, n_features) Returns: - result (dict): Dictionary containing various covariance matrices # Standardizing the data X_centered = X - X.mean(axis=0) # Empirical covariance empirical_cov = empirical_covariance(X_centered) # Shrunk covariance with a default shrinkage parameter alpha = 0.1 # example shrinkage parameter shrunk_cov = shrunk_covariance(empirical_cov, shrinkage=alpha) # Ledoit-Wolf shrinkage lw = LedoitWolf() lw.fit(X_centered) ledoit_wolf_cov = lw.covariance_ # Oracle Approximating Shrinkage (OAS) oas = OAS() oas.fit(X_centered) oas_cov = oas.covariance_ # Robust covariance estimation (MinCovDet) mcd = MinCovDet().fit(X_centered) robust_cov = mcd.covariance_ return { \'empirical\': empirical_cov, \'shrunk\': shrunk_cov, \'ledoit_wolf\': ledoit_wolf_cov, \'oas\': oas_cov, \'robust\': robust_cov }"},{"question":"**Question: Visualizing Trends in a Dataset Using Seaborn** You are given a dataset of your choice to load using `seaborn.load_dataset`. Use the dataset to create a visualization that demonstrates the trend of a specific numeric attribute over time. You are required to use `seaborn.objects` to create this visualization. # Requirements: 1. **Load the dataset**: Use `seaborn.load_dataset` to load a dataset. Document the dataset chosen and the key attributes it contains. 2. **Preprocess the data**: Filter or transform the data if necessary. For example, if the dataset does not have a date attribute, create one by converting a relevant column. 3. **Create a Plot**: - Use `so.Plot` to create a plot with date on the x-axis and a numeric attribute of your choice on the y-axis. - Customize the plot by adding lines with varying linewidth and color. - Use faceting to break the plots into subplots based on a categorical attribute, such as year or another relevant grouping. - Apply `scale` customization to improve the aesthetic of the plots. - Set the overall layout size to ensure the plot is readable and visually appealing. - Add appropriate labels and title to the plot. # Input Format: - The dataset name as a string. Example: `\'seaice\'`. - The numeric attribute name as a string. Example: `\'Extent\'`. - The date attribute name as a string (if not present, describe how you would create it from existing data). Example: `\'Date\'`. # Output: A plot that visually represents the trend of the numeric attribute over time, with all the specified customizations applied. # Constraints: - The dataset should be one available from seaborn\'s `load_dataset`. - The numeric attribute chosen should be appropriate for plotting over time. - Faceting should meaningfully break down the data for better interpretation. # Example Solution: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset seaice = load_dataset(\\"seaice\\") # Step 2: Preprocess the data (if necessary) # Here, no additional preprocessing is needed as the dataset has the required columns # Step 3: Create the Plot plot = ( so.Plot(seaice, x=\\"Date\\", y=\\"Extent\\") .add(so.Lines(linewidth=0.5, color=\\"#bbca\\")) .facet(seaice[\\"Date\\"].dt.year) .add(so.Lines(linewidth=1)) .scale(color=\\"ch:rot=-.2, light=.7\\") .layout(size=(8, 4)) .label(title=\\"Sea Ice Extent Over Time\\") ) plot.show() ``` **Notes:** - Ensure your solution is detailed and well-documented. - Verify that the plot accurately represents the data and shows meaningful trends.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def visualize_trend(dataset_name, date_attribute, numeric_attribute, facet_attribute=None): Load a dataset using seaborn, preprocess if necessary, and create a visualization for the trend of a specific numeric attribute over time using seaborn.objects. Parameters: - dataset_name: str, name of the dataset to load from seaborn - date_attribute: str, name of the date column in the dataset - numeric_attribute: str, name of the numeric attribute to plot - facet_attribute: str, name of the attribute to facet by (optional) # Load the dataset data = sns.load_dataset(dataset_name) # Preprocess the data if necessary if not pd.api.types.is_datetime64_any_dtype(data[date_attribute]): data[date_attribute] = pd.to_datetime(data[date_attribute]) # Create the plot plot = so.Plot(data, x=date_attribute, y=numeric_attribute) if facet_attribute: plot = plot.facet(facet_attribute) plot = ( plot.add(so.Line(linewidth=0.5, color=\\"#bbca\\")) .scale(color=\\"ch:rot=-.2, light=.7\\") .layout(size=(8, 4)) .label(title=f\\"{numeric_attribute} Over Time\\") ) plot.show() # Example usage visualize_trend(\'flights\', \'year\', \'passengers\', \'month\')"},{"question":"**Question: Create a Workflow for Gradient Boosting Regression** You are working on a machine learning project that utilizes scikit-learn\'s `GradientBoostingRegressor`. Your task is to create a simple, reproducible, and clean workflow that: 1. Uses synthetic data for the regression problem using `make_regression`. 2. Splits the dataset into training and test sets. 3. Scales the features using `StandardScaler`. 4. Fits a `GradientBoostingRegressor` model on the training data. 5. Computes and prints the RÂ² score on the test set. **Requirements:** 1. Your code should be able to be copy-pasted and run without any errors. 2. Use default parameters for `GradientBoostingRegressor` initially. 3. After computing the RÂ² score, modify `n_iter_no_change` to 5 and refit the model. Note any warnings. 4. Print both RÂ² scores for default and modified settings. 5. Focus on clear and readable code with proper formatting and comments. **Input and Output Format:** - **Input:** None. - **Output:** Print the RÂ² scores for both default and modified settings. **Example Output:** ``` RÂ² score with default settings: 0.80 RÂ² score with n_iter_no_change set to 5: 0.75 UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names. ``` **Constraints:** - Synthetic data with `n_samples=1000` and `n_features=20`. - Use a random_state for reproducibility. **Performance Requirements:** - Ensure that all steps in the workflow are implemented efficiently without unnecessary computations.","solution":"from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score def gradient_boosting_workflow(): # Generate synthetic data X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42) # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Fit the GradientBoostingRegressor with default parameters gbr_default = GradientBoostingRegressor(random_state=42) gbr_default.fit(X_train_scaled, y_train) # Compute the RÂ² score on the test set with default parameters y_pred_default = gbr_default.predict(X_test_scaled) r2_score_default = r2_score(y_test, y_pred_default) print(f\\"RÂ² score with default settings: {r2_score_default:.2f}\\") # Modify n_iter_no_change and refit the model gbr_modified = GradientBoostingRegressor(n_iter_no_change=5, random_state=42) gbr_modified.fit(X_train_scaled, y_train) # Compute the RÂ² score on the test set with modified parameters y_pred_modified = gbr_modified.predict(X_test_scaled) r2_score_modified = r2_score(y_test, y_pred_modified) print(f\\"RÂ² score with n_iter_no_change set to 5: {r2_score_modified:.2f}\\")"},{"question":"# Python/C API Coding Challenge Context You are tasked with implementing a C function that integrates with the Python interpreter. This function should create a Python list of integers, sum its elements, handle any errors gracefully, and return the result. This question assesses your knowledge of key Python/C API concepts, including reference counts, exception handling, and creating/manipulating Python objects. Task Write a C function named `sum_list_elements` that: 1. Initializes the Python interpreter. 2. Creates a Python list of integers. 3. Sums the elements of the list. 4. Handles any errors appropriately by setting a Python exception and cleaning up references. 5. Finalizes the Python interpreter. 6. Returns the sum of the list elements or an appropriate error code. **Function Signature:** ```c long sum_list_elements(int* array, int length); ``` **Input:** - `array`: A pointer to an array of integers. - `length`: The length of the array. **Output:** - Returns the sum of the list\'s elements on success. - Returns `-1` on failure, with an appropriate Python error set. Constraints - You must manage the reference counts properly to avoid memory leaks. - You must handle possible errors such as memory allocation failures or Python exceptions during the sum operation. - The Python interpreter should be properly initialized and finalized within the function. Example ```c int array[] = {1, 2, 3, 4, 5}; long result = sum_list_elements(array, 5); // result should be 15 on success ``` Guidelines - Use `Py_Initialize` and `Py_FinalizeEx` to manage the Python interpreter lifecycle. - Use `PyList_New` and `PyList_SetItem` to create and populate the Python list. - Use `PySequence_GetItem` and `PyLong_AsLong` to sum the list elements. - Use appropriate error handling mechanisms provided by the Python/C API (e.g., `PyErr_SetString`, `PyErr_Clear`, `PyErr_Occurred`). **Skeleton code:** ```c #include <Python.h> long sum_list_elements(int* array, int length) { // Initialize the Python interpreter Py_Initialize(); if (!Py_IsInitialized()) { return -1; // Unable to initialize Python interpreter } // Create a new Python list of the given length PyObject *py_list = PyList_New(length); if (!py_list) { Py_FinalizeEx(); return -1; // Memory allocation failure } // Populate the Python list with elements from the C array for (int i = 0; i < length; ++i) { PyObject *py_int = PyLong_FromLong(array[i]); if (!py_int) { Py_DECREF(py_list); Py_FinalizeEx(); return -1; // Memory allocation failure } PyList_SetItem(py_list, i, py_int); // This steals the reference to py_int } // Sum the elements of the list long sum = 0; for (int i = 0; i < length; ++i) { PyObject *item = PyList_GetItem(py_list, i); // Borrowed reference if (!PyLong_Check(item)) { PyErr_SetString(PyExc_TypeError, \\"List item is not an integer\\"); Py_DECREF(py_list); Py_FinalizeEx(); return -1; // Type error } long value = PyLong_AsLong(item); if (value == -1 && PyErr_Occurred()) { Py_DECREF(py_list); Py_FinalizeEx(); return -1; // Conversion failure } sum += value; } // Clean up Py_DECREF(py_list); Py_FinalizeEx(); return sum; } ``` **Explanation:** - `Py_Initialize()` and `Py_FinalizeEx()` manage the lifecycle of the Python interpreter. - `PyList_New` allocates a new Python list. - `PyLong_FromLong` converts C integers to Python integers. - `PyList_SetItem` places Python integers into the list, storing the list reference. - `PyList_GetItem` retrieves items from the list. - `PyLong_AsLong` converts Python integers to C longs. - Proper error handling ensures that memory is managed correctly and exceptions are propagated using `PyErr_SetString` and `PyErr_Occurred`. This question tests understanding of reference management, error handling, and the use of Python/C API functions. Ensure careful handling of reference counts to prevent memory leaks or segmentation faults.","solution":"def sum_list_elements(array): Sum the elements of a list of integers. Parameters: array (list of int): The list of integers to sum. Returns: int: The sum of the integers in the list if not all(isinstance(item, int) for item in array): raise ValueError(\\"List must contain only integers\\") return sum(array)"},{"question":"# PyTorch Coding Assessment: Numerical Properties of Data Types **Objective:** Write code that demonstrates your understanding of PyTorch\'s `torch.finfo` and `torch.iinfo` classes. **Problem Statement:** Implement a function `validate_dtype(dtype, is_float)` that takes two arguments: - `dtype`: A PyTorch data type (e.g., `torch.float32`, `torch.int32`) - `is_float`: A boolean indicating whether this dtype should represent a floating point type (`True`) or an integer type (`False`). The function should return a dictionary containing the numerical properties of the provided dtype, using PyTorch\'s `finfo` and `iinfo` classes. If the given dtype does not match the expectation set by `is_float` (i.e., if `is_float` is `True` but `dtype` is an integer type, or vice versa), the function should return `None`. # Function Signature: ```python def validate_dtype(dtype: torch.dtype, is_float: bool) -> dict: pass ``` # Input: - `dtype` (torch.dtype): A PyTorch data type. - `is_float` (bool): Boolean indicator for floating point type. # Output: - dict: Dictionary with numerical properties of the dtype if it matches the type specified by `is_float`. The structure of the output dictionary would vary depending on whether it\'s a floating point or integer type. # Example: ```python import torch result1 = validate_dtype(torch.float32, True) # Expected output: {\'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, # \'tiny\': 1.1754943508222875e-38, \'smallest_normal\': 1.1754943508222875e-38, \'resolution\': 1e-07} result2 = validate_dtype(torch.uint8, False) # Expected output: {\'bits\': 8, \'max\': 255, \'min\': 0} result3 = validate_dtype(torch.int32, True) # Expected output: None (dtype is an integer but expected to be a floating point) result4 = validate_dtype(torch.float64, False) # Expected output: None (dtype is a floating point but expected to be an integer) ``` # Notes: 1. Utilize `torch.finfo` for floating point types and `torch.iinfo` for integer types to retrieve the numerical properties. 2. Ensure your function handles the mismatch between `dtype` and `is_float` gracefully by returning `None`. **Constraints:** - Do not use external libraries other than PyTorch. - Function should be efficient in terms of both time and space complexities. **Performance Requirements:** - Your implementation should have a time complexity of O(1) for retrieving property values since it directly queries fixed attribute values. Good luck!","solution":"import torch def validate_dtype(dtype: torch.dtype, is_float: bool) -> dict: if is_float: if not dtype.is_floating_point: return None info = torch.finfo(dtype) return { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'smallest_normal\': info.smallest_normal } else: if dtype.is_floating_point: return None info = torch.iinfo(dtype) return { \'bits\': info.bits, \'max\': info.max, \'min\': info.min }"},{"question":"**Question: Event Scheduler Using Collections and Datetime** You are tasked with implementing a simple event scheduler. This scheduler should be able to: 1. Add events with a specific datetime as the key. 2. Retrieve events in chronological order. 3. Allow quick access to the next upcoming event. To achieve this, you will use the `OrderedDict` from the `collections` module to maintain the events, and the `datetime` module to handle dates and times. The `OrderedDict` should maintain the order of events based on their scheduled datetime. # Requirements: - Implement the `EventScheduler` class with the following methods: - `add_event(event_name: str, scheduled_time: datetime) -> None`: Adds an event with the given name and scheduled time to the scheduler. - `remove_event(event_name: str) -> bool`: Removes the event with the given name from the scheduler, returning `True` if the event was successfully removed and `False` if the event did not exist. - `get_next_event() -> Optional[str]`: Returns the name of the next scheduled event (the one with the earliest datetime). If there are no events, return `None`. - `get_events_in_order() -> List[str]`: Returns the list of event names in the order of their scheduled times. # Constraints: - An event name is a non-empty string containing only alphanumeric characters. - No two events can be scheduled at the exact same datetime. - You can assume that the input datetimes are valid and well-formed. - The number of events will not exceed 10,000. # Performance Requirements: - Methods should be optimized to perform efficiently under the given constraints. # Example Usage: ```python from datetime import datetime from collections import OrderedDict class EventScheduler: def __init__(self): self.events = OrderedDict() def add_event(self, event_name: str, scheduled_time: datetime) -> None: self.events[scheduled_time] = event_name self.events = OrderedDict(sorted(self.events.items())) def remove_event(self, event_name: str) -> bool: for time, name in list(self.events.items()): if name == event_name: del self.events[time] return True return False def get_next_event(self) -> Optional[str]: if not self.events: return None return next(iter(self.events.values())) def get_events_in_order(self) -> List[str]: return list(self.events.values()) # Example usage: scheduler = EventScheduler() scheduler.add_event(\\"Meeting\\", datetime(2023, 10, 20, 14, 30)) scheduler.add_event(\\"Lunch\\", datetime(2023, 10, 20, 12, 0)) scheduler.add_event(\\"Dentist\\", datetime(2023, 10, 22, 9, 0)) print(scheduler.get_next_event()) # Output: \\"Lunch\\" print(scheduler.get_events_in_order()) # Output: [\\"Lunch\\", \\"Meeting\\", \\"Dentist\\"] ```","solution":"from datetime import datetime from collections import OrderedDict from typing import Optional, List class EventScheduler: def __init__(self): self.events = OrderedDict() def add_event(self, event_name: str, scheduled_time: datetime) -> None: # Ensure there are no duplicate datetime entries if scheduled_time in self.events: raise ValueError(\\"An event already exists at the given datetime.\\") self.events[scheduled_time] = event_name # Maintain events in chronological order self.events = OrderedDict(sorted(self.events.items())) def remove_event(self, event_name: str) -> bool: for time, name in list(self.events.items()): if name == event_name: del self.events[time] return True return False def get_next_event(self) -> Optional[str]: if not self.events: return None return next(iter(self.events.values())) def get_events_in_order(self) -> List[str]: return list(self.events.values())"},{"question":"You are tasked with creating a comprehensive email message in Python using the `email.mime` package. This email will include a combination of text, image, and application data. Specifically, you will need to: 1. Create a text part of the email using `MIMEText`. 2. Attach an image (`sample.png`) to the email using `MIMEImage`. 3. Attach a PDF document (`document.pdf`) to the email using `MIMEApplication`. 4. Ensure parts are correctly encoded for transport. Implement the function `create_mime_email()` that returns a `MIMEMultipart` email object. # Function Signature ```python def create_mime_email(text: str, image_path: str, pdf_path: str) -> MIMEMultipart: pass ``` # Input: - `text` (str): The plain text content of the email. - `image_path` (str): The file path to an image to attach. - `pdf_path` (str): The file path to a PDF document to attach. # Output: - Returns a `MIMEMultipart` email object that includes the specified text, image, and PDF document as parts. # Constraints: - The email must correctly encode all parts for transport. - The `Content-Type` headers must be appropriately set for each part. # Example: ```python from email.mime.multipart import MIMEMultipart from email.mime.base import MIMEBase from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication import email.encoders def create_mime_email(text: str, image_path: str, pdf_path: str) -> MIMEMultipart: # Create the root message msg = MIMEMultipart() # Add text part part1 = MIMEText(text, \'plain\') msg.attach(part1) # Add image part with open(image_path, \'rb\') as img_file: part2 = MIMEImage(img_file.read()) msg.attach(part2) # Add application part (PDF) with open(pdf_path, \'rb\') as pdf_file: part3 = MIMEApplication(pdf_file.read(), _subtype=\'pdf\') # Set the application part to use base64 encoding email.encoders.encode_base64(part3) msg.attach(part3) return msg # Test the function email_object = create_mime_email(\\"Hello, this is a test email.\\", \\"sample.png\\", \\"document.pdf\\") print(email_object) ``` # Notes: - Ensure the email object has proper MIME version, and each part includes the correct `Content-Type` and `Content-Transfer-Encoding` headers.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication import email.encoders def create_mime_email(text: str, image_path: str, pdf_path: str) -> MIMEMultipart: Creates a MIME email with text, an image, and a PDF document. :param text: The plain text content of the email. :param image_path: The file path to the image to attach. :param pdf_path: The file path to the PDF document to attach. :return: A MIMEMultipart email object. # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = \'Test Email with Attachments\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Add text part part1 = MIMEText(text, \'plain\') msg.attach(part1) # Add image part with open(image_path, \'rb\') as img_file: part2 = MIMEImage(img_file.read(), name=\\"sample.png\\") msg.attach(part2) # Add application part (PDF) with open(pdf_path, \'rb\') as pdf_file: part3 = MIMEApplication(pdf_file.read(), _subtype=\'pdf\', name=\\"document.pdf\\") # Set the application part to use base64 encoding email.encoders.encode_base64(part3) part3.add_header(\'Content-Disposition\', \'attachment\', filename=\'document.pdf\') msg.attach(part3) return msg"},{"question":"# Dynamic Class Creation and Manipulation You are asked to demonstrate your understanding of dynamic class creation and manipulation of types using the `types` module. Specifically, you will create dynamic classes, modify their namespaces, and work with various type utilities. Here\'s the task: **Task**: 1. **Create a Dynamic Class**: Write a function `create_dynamic_class` that takes the following parameters: - `name`: A string, the name of the class. - `bases`: A tuple of base classes. Default is an empty tuple. - `attributes`: A dictionary of attributes and methods to add to the class. This function should return the dynamically created class. ```python def create_dynamic_class(name: str, bases: tuple = (), attributes: dict = None) -> type: # Your implementation here ``` 2. **Extend the Dynamic Class**: Write a function `extend_dynamic_class` that takes the following parameters: - `cls`: The class to extend. - `new_bases`: A tuple of new base classes to add. This function should use `types.prepare_class` to extend the class with new bases and update its namespace with a new method `extended_method`, which prints `\\"Extended method called\\"`. ```python def extend_dynamic_class(cls: type, new_bases: tuple) -> type: # Your implementation here ``` 3. **Verify Function Types**: Write a function `verify_function_types` that takes a class object and verifies the types of its methods using `types.FunctionType` and `types.MethodType`. It should return a dictionary with method names as keys and `True` or `False` indicating if the method\'s type is verified. ```python def verify_function_types(cls: type) -> dict: # Your implementation here ``` **Constraints**: - The `new_class` and `prepare_class` functions must be used where appropriate. - The `create_dynamic_class` should handle the `attributes` dictionary gracefully, assigning each key-value pair to the class namespace. - The `verify_function_types` function should strictly use `types.FunctionType` and `types.MethodType` to check for method types. **Example**: ```python # Define attributes attributes = { \'foo\': lambda self: \'bar\', \'__init__\': lambda self, x: setattr(self, \'x\', x) } # Create dynamic class MyClass = create_dynamic_class(\'MyClass\', (object,), attributes) # Create an instance and test attributes instance = MyClass(10) print(instance.foo()) # Output: bar print(instance.x) # Output: 10 # Extend the class ExtendedClass = extend_dynamic_class(MyClass, (str,)) extended_instance = ExtendedClass(20) extended_instance.extended_method() # Output: Extended method called # Verify function types print(verify_function_types(ExtendedClass)) # Output: {\'foo\': True, \'__init__\': True, \'extended_method\': True} ``` **Performance Requirements**: - Ensure efficient class creation and dynamic resolution of types.","solution":"import types def create_dynamic_class(name: str, bases: tuple = (), attributes: dict = None) -> type: # Create the class with the given name, bases, and attributes if attributes is None: attributes = {} return types.new_class(name, bases, {}, lambda ns: ns.update(attributes)) def extend_dynamic_class(cls: type, new_bases: tuple) -> type: # Prepare the class with the new bases metaclass, ns, kwds = types.prepare_class(cls.__name__, new_bases) # Copy attributes from the existing class ns.update(cls.__dict__) # Add the extended method def extended_method(self): print(\\"Extended method called\\") ns[\'extended_method\'] = extended_method # Create the new class with the updated namespace return metaclass(cls.__name__, (cls,) + new_bases, ns) def verify_function_types(cls: type) -> dict: result = {} for name, attribute in cls.__dict__.items(): if isinstance(attribute, types.FunctionType) or isinstance(attribute, types.MethodType): result[name] = True else: result[name] = False return result"},{"question":"**Objective**: Demonstrate your understanding of PyTorch\'s linear algebra functions by solving a complex problem involving matrix decompositions and solving linear systems. **Problem Statement**: You are given a system of linear equations in the form of `AX = B`, where: - `A` is an (n times n) matrix. - `B` is an (n times m) matrix. - Both `A` and `B` are generated randomly using PyTorch. Your task is to: 1. Compute the determinant of `A` to check if the matrix is invertible. 2. If `A` is invertible, find the matrix `X` by solving the linear system `AX = B` using LU decomposition. 3. Verify that the solution is correct by multiplying `A` and `X` and checking if the result matches `B` (within a tolerance due to floating-point precision). # Function Signature ```python import torch def solve_linear_system(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Solves the linear system AX = B using LU decomposition and verifies the solution. Parameters: - A (torch.Tensor): An nxn invertible matrix. - B (torch.Tensor): An nxm matrix. Returns: - X (torch.Tensor): The solution matrix such that AX = B. # Your implementation here ``` # Constraints: - The input matrices `A` and `B` are generated randomly and are guaranteed to be `torch.Tensor` objects. - Use PyTorch functions for all linear algebra operations. - Ensure that your solution handles potential edge cases, such as checking if `A` is indeed invertible. # Example Input and Output: ```python import torch # Example to generate random input torch.manual_seed(0) A = torch.rand(3, 3) B = torch.rand(3, 2) X = solve_linear_system(A, B) print(X) ``` - Ensure the function returns a `torch.Tensor` object. - The solution `X` should satisfy the equation `AX â‰ˆ B` within a reasonable numerical tolerance. # Testing Criteria: - The function will be tested on multiple random cases. - The correctness will be verified by multiplying `A` and `X` and comparing it with `B`. Good luck!","solution":"import torch def solve_linear_system(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Solves the linear system AX = B using LU decomposition and verifies the solution. Parameters: - A (torch.Tensor): An nxn invertible matrix. - B (torch.Tensor): An nxm matrix. Returns: - X (torch.Tensor): The solution matrix such that AX = B. # Check if A is invertible by computing its determinant det_A = torch.det(A) if det_A == 0: raise ValueError(\\"Matrix A is not invertible\\") # Solve the linear system using the LU decomposition method X = torch.lu_solve(B, *A.lu()) # Verify the solution is_close = torch.allclose(torch.matmul(A, X), B, atol=1e-6) if not is_close: raise ValueError(\\"The solution AX does not match B within the tolerance\\") return X"},{"question":"# Python Coding Assessment Question Objective: This question assesses your ability to use the `concurrent.futures` module to manage parallel task execution efficiently. Problem Statement: You are given a list of URLs and need to fetch the content from each URL concurrently. Implement a function `fetch_all_urls(urls, max_workers)` that: 1. Uses `concurrent.futures.ThreadPoolExecutor` to fetch the content from each URL concurrently. 2. Limits the maximum number of worker threads to `max_workers`. 3. Returns a dictionary where the keys are the URLs and the values are the fetched contents or any exception message if an error occurs during fetching. Input: - `urls`: a list of strings where each string is a URL. - `max_workers`: an integer specifying the maximum number of threads to run concurrently. Output: - A dictionary with URL strings as keys and the content fetched (as strings) or an exception message as the corresponding values. Constraints: - Use the `requests` library to fetch the URL content. - Handle exceptions that may occur during the content fetching process, storing the exception message in the output dictionary. - Assume the `requests` library is installed and available. Example: ```python import requests from concurrent.futures import ThreadPoolExecutor def fetch_all_urls(urls, max_workers): pass # Example usage: urls = [ \'https://jsonplaceholder.typicode.com/posts/1\', \'https://jsonplaceholder.typicode.com/posts/2\' ] result = fetch_all_urls(urls, 2) print(result) ``` Given the `urls` list, the function should return a dictionary like: ```python { \'https://jsonplaceholder.typicode.com/posts/1\': \'...content...\', \'https://jsonplaceholder.typicode.com/posts/2\': \'...content...\' } ``` If an error occurs while fetching any URL, the corresponding value in the dictionary should be the string representation of the exception. Notes: - Make sure to clean up resources properly by shutting down the ThreadPoolExecutor. - Consider edge cases such as empty URL lists or invalid URLs. Good luck!","solution":"import requests from concurrent.futures import ThreadPoolExecutor, as_completed def fetch_content(url): try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException as e: return str(e) def fetch_all_urls(urls, max_workers): results = {} with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_content, url): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: results[url] = future.result() except Exception as e: results[url] = str(e) return results"},{"question":"**Objective:** Write a Python function to verify if a memory buffer\'s structure is valid, based on the provided buffer parameters. This function will simulate some aspects of the \\"verify_structure\\" function explained in the documentation. **Function Signature:** ```python def verify_buffer_structure(memlen: int, itemsize: int, ndim: int, shape: Optional[List[int]], strides: Optional[List[int]], offset: int) -> bool: pass ``` **Input:** - `memlen` (int): Length of the physical memory block. - `itemsize` (int): Size of each item in the buffer (in bytes). - `ndim` (int): Number of dimensions for the n-dimensional array. - `shape` (Optional[List[int]]): Shape of the memory as an n-dimensional array. Can be `None` if `ndim` is 0. - `strides` (Optional[List[int]]): Number of bytes to skip to get to a new element in each dimension. Can be `None` if `ndim` is 0. - `offset` (int): Offset of the buffer\'s start location in the memory block. **Output:** - Returns a boolean value: - `True` if the buffer structure is valid. - `False` if the buffer structure is invalid. **Constraints:** - The function must adhere to the principles described in the buffer protocol documentation. - Assume valid inputs for `memlen`, `itemsize`, `ndim`, `shape`, `strides`, and `offset`. **Example:** ```python # Example 1: valid buffer structure print(verify_buffer_structure(100, 4, 2, [5, 5], [20, 4], 0)) # Expected output: True # Example 2: invalid buffer structure, offset is too large print(verify_buffer_structure(100, 4, 2, [5, 5], [20, 4], 101)) # Expected output: False # Example 3: valid buffer structure, negative strides indicating reversed memory traversal print(verify_buffer_structure(80, 4, 1, [20], [-4], 76)) # Expected output: True ``` **Notes:** 1. If `ndim` is 0, both `shape` and `strides` must be `None`, and the buffer is interpreted as a scalar of size `itemsize`. 2. Validity criteria include ensuring that the buffer stays within the memory bounds, strides being consistent with `itemsize`, and other offset and size checks as implied by the buffer protocol. **Hint:** You may refer to the provided function in the analysis section: ```python def verify_structure(memlen, itemsize, ndim, shape, strides, offset): if offset % itemsize: return False if offset < 0 or offset + itemsize > memlen: return False if any(v % itemsize for v in strides): return False if ndim <= 0: return ndim == 0 and not shape and not strides if 0 in shape: return True imin = sum(strides[j]*(shape[j]-1) for j in range(ndim) if strides[j] <= 0) imax = sum(strides[j]*(shape[j]-1) for j in range(ndim) if strides[j] > 0) return 0 <= offset + imin and offset + imax + itemsize <= memlen ```","solution":"from typing import List, Optional def verify_buffer_structure(memlen: int, itemsize: int, ndim: int, shape: Optional[List[int]], strides: Optional[List[int]], offset: int) -> bool: if ndim == 0: return shape is None and strides is None and 0 <= offset < memlen and (offset % itemsize) == 0 if shape is None or strides is None or len(shape) != ndim or len(strides) != ndim: return False if offset % itemsize: return False if offset < 0 or offset + itemsize > memlen: return False if any(v % itemsize for v in strides): return False if 0 in shape: return True imin = sum(strides[j] * (shape[j] - 1) for j in range(ndim) if strides[j] <= 0) imax = sum(strides[j] * (shape[j] - 1) for j in range(ndim) if strides[j] > 0) return 0 <= offset + imin and offset + imax + itemsize <= memlen"},{"question":"**XML Parsing with Handlers** You are required to write a Python program using the `xml.parsers.expat` package to parse and process specific content from a given XML string. The XML content represents a basic inventory system where each `item` element contains details like `id`, `name`, `quantity`, and `price`. Your task is to implement a function `parse_inventory(xml_string)` that: 1. Creates an XML parser object. 2. Sets the appropriate handler functions for: - Start of elements. - End of elements. - Character data. 3. Parses the provided XML string. 4. Returns a list of dictionaries, where each dictionary represents an item with its details (id, name, quantity, and price). # Input - `xml_string`: A string containing a valid XML document. # Output - A list of dictionaries, where each dictionary contains the details of an item. Each dictionary should have the keys: `id`, `name`, `quantity`, and `price`. Ensure the values are appropriately typed (e.g., `id` should be an integer, `price` should be a float). # Constraints - You must handle parsing errors by catching `xml.parsers.expat.ExpatError` and returning an appropriate error message. - Assume the input XML string is not empty and is well-formed. # Example ```python xml_string = <inventory> <item id=\\"1\\"> <name>Item A</name> <quantity>10</quantity> <price>2.5</price> </item> <item id=\\"2\\"> <name>Item B</name> <quantity>20</quantity> <price>3.75</price> </item> </inventory> assert parse_inventory(xml_string) == [ {\'id\': 1, \'name\': \'Item A\', \'quantity\': 10, \'price\': 2.5}, {\'id\': 2, \'name\': \'Item B\', \'quantity\': 20, \'price\': 3.75} ] ``` # Function Signature ```python from typing import List, Dict, Any def parse_inventory(xml_string: str) -> List[Dict[str, Any]]: pass ``` # Note - Ensure your parser handles the starting and ending of elements correctly. - Accumulate character data within elements as needed (e.g., to gather the content of `name`, `quantity`, and `price`). - Appropriately convert character data to their respective types when forming the dictionaries.","solution":"from typing import List, Dict, Any import xml.parsers.expat def parse_inventory(xml_string: str) -> List[Dict[str, Any]]: items = [] current_item = {} current_data = \\"\\" current_element = \\"\\" def start_element(name, attrs): nonlocal current_element current_element = name if name == \'item\': current_item[\'id\'] = int(attrs[\'id\']) def end_element(name): nonlocal current_item, current_data, current_element if name == \'item\': items.append(current_item) current_item = {} elif name in [\'name\', \'quantity\', \'price\']: if name == \'name\': current_item[name] = current_data elif name == \'quantity\': current_item[name] = int(current_data) elif name == \'price\': current_item[name] = float(current_data) current_data = \\"\\" current_element = \\"\\" def char_data(data): nonlocal current_data current_data += data.strip() # Create parser parser = xml.parsers.expat.ParserCreate() # Set handlers parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_string) except xml.parsers.expat.ExpatError: return \\"Error parsing XML\\" return items"},{"question":"# Python Coding Assessment Objective Demonstrate your understanding of Python\'s `tkinter.dnd` module by implementing a drag-and-drop system between two custom widgets. Problem Statement Using the `tkinter.dnd` module, create a draggable object within a Tkinter window and allow it to be moved from one widget (source) to another (target). You need to: 1. Define two custom widgets: `SourceWidget` and `TargetWidget`. 2. Enable dragging an object from `SourceWidget`. 3. Allow dropping the object onto `TargetWidget`. Requirements 1. Implement a `SourceWidget` class with a Button that can be dragged. 2. Implement a `TargetWidget` class which can accept the drag-and-drop object. 3. Use the `DndHandler` class to handle the drag-and-drop events. 4. Show a simple Tkinter window demonstrating the functionality. Constraints - You must implement and use `dnd_start()` to initiate the drag-and-drop operation. - The `SourceWidget` must contain a label or a button that can be dragged. - The `TargetWidget` should have the capability to accept and display the dragged object. Input and Output - **Input:** There is no specific input to your program beyond creating the window. - **Output:** The Tkinter window shows the widgets, and a successful drag-and-drop operation moves the object from `SourceWidget` to `TargetWidget`. # Guidelines 1. Implement the classes and the drag-and-drop functionality within the given constraints and requirements. 2. Ensure your code is well-documented and uses clear and meaningful variable names. 3. Provide comments explaining each major step and how it relates to the drag-and-drop process. # Example Code Here is a skeleton to get you started: ```python import tkinter as tk from tkinter import dnd class SourceWidget(tk.Frame): def __init__(self, master, **kwargs): super().__init__(master, **kwargs) self.label = tk.Label(self, text=\\"Drag Me\\") self.label.pack() self.label.bind(\\"<ButtonPress>\\", self.start_drag) def start_drag(self, event): dnd.dnd_start(self.label, event) class TargetWidget(tk.Frame): def __init__(self, master, **kwargs): super().__init__(master, **kwargs) self.config(relief=tk.RIDGE, bd=2) self.pack_propagate(False) self.dnd_accept = self.accept def accept(self, source, event): if isinstance(source, tk.Label): return self def dnd_commit(self, source, event): source.place(in_=self) class Application(tk.Tk): def __init__(self): super().__init__() self.geometry(\\"400x200\\") self.source_widget = SourceWidget(self) self.source_widget.pack(side=\\"left\\", padx=20, pady=20) self.target_widget = TargetWidget(self, width=150, height=100) self.target_widget.pack(side=\\"right\\", padx=20, pady=20) if __name__ == \\"__main__\\": app = Application() app.mainloop() ``` In this example, you need to complete the drag-and-drop functionality using the `DndHandler` class and ensure that the dragged label gets correctly placed into the `TargetWidget`.","solution":"import tkinter as tk from tkinter import dnd class SourceWidget(tk.Frame): def __init__(self, master, **kwargs): super().__init__(master, **kwargs) self.label = tk.Label(self, text=\\"Drag Me\\", bg=\\"lightblue\\") self.label.pack(padx=10, pady=10) self.label.bind(\\"<ButtonPress>\\", self.start_drag) def start_drag(self, event): dnd.dnd_start(self.label, event) class TargetWidget(tk.Frame): def __init__(self, master, **kwargs): super().__init__(master, **kwargs) self.config(relief=tk.RIDGE, bd=2, width=150, height=100) self.pack_propagate(False) self.dnd_accept = self.accept def accept(self, source, event): if isinstance(source, tk.Label): return self def dnd_commit(self, source, event): source.place(in_=self) class Application(tk.Tk): def __init__(self): super().__init__() self.geometry(\\"400x200\\") self.source_widget = SourceWidget(self) self.source_widget.pack(side=\\"left\\", padx=20, pady=20) self.target_widget = TargetWidget(self) self.target_widget.pack(side=\\"right\\", padx=20, pady=20) if __name__ == \\"__main__\\": app = Application() app.mainloop()"},{"question":"**Objective:** Write a function using pandas that processes and analyzes sales data to generate insights. You will be given a dataset containing sales records, and your task is to perform several data manipulation and analysis steps to answer specific questions about the data. Dataset Description: You will be provided a CSV file named `sales_data.csv` with the following columns: - `Date`: The date of the sales record in `YYYY-MM-DD` format. - `Product_ID`: Unique identifier for the sold product. - `Store_ID`: Unique identifier for the store. - `Quantity`: Number of units sold. - `Revenue`: Total revenue generated from the sales. Tasks: 1. **Data Loading and Cleaning**: - Load the dataset into a pandas DataFrame. - Ensure there are no missing values in the dataset. If any, fill them with appropriate values and document your approach. 2. **Data Aggregation and Summary**: - Calculate the total revenue generated per product. - Calculate the total quantity sold per store. 3. **Time Series Analysis**: - Calculate the monthly revenue for the entire dataset and find the month with the highest revenue. 4. **Product Performance Analysis**: - Determine the top 5 products that generated the most revenue. - Determine the bottom 5 products based on the quantity sold. 5. **Visualization** (Optional but encouraged): - Create a bar plot showing the monthly revenue. - Create a pie chart showing the revenue distribution of the top 5 products. Function Signature: ```python import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(file_path: str) -> pd.DataFrame: Analyze the sales data to generate insights. Args: file_path (str): The path to the CSV file containing sales data. Returns: pd.DataFrame: A DataFrame containing a summary of insights: - Total revenue per product. - Total quantity sold per store. - Monthly revenue and the highest revenue month. - Top 5 products by revenue. - Bottom 5 products by quantity sold. # Your code here # Returning the analysis as a DataFrame (Placeholder) return pd.DataFrame() ``` Constraints: - Your solution should handle datasets that are too large to fit into memory efficiently. - Ensure your code is well-documented with comments explaining each step. - Write unit tests to validate your function with edge cases and sample datasets. # Evaluation Criteria: - Correctness of the solution. - Efficiency and performance of the data processing steps. - Code readability and use of pandas idiomatic expressions. - (Optional) Quality and accuracy of visualizations.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(file_path: str) -> pd.DataFrame: Analyze the sales data to generate insights. Args: file_path (str): The path to the CSV file containing sales data. Returns: pd.DataFrame: A DataFrame containing a summary of insights: - Total revenue per product. - Total quantity sold per store. - Monthly revenue and the highest revenue month. - Top 5 products by revenue. - Bottom 5 products by quantity sold. # Load the dataset df = pd.read_csv(file_path) # Fill missing values df.fillna({\'Quantity\': 0, \'Revenue\': 0}, inplace=True) # Total revenue per product total_revenue_per_product = df.groupby(\'Product_ID\')[\'Revenue\'].sum().reset_index() # Total quantity sold per store total_quantity_per_store = df.groupby(\'Store_ID\')[\'Quantity\'].sum().reset_index() # Monthly revenue df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') monthly_revenue = df.groupby(\'Month\')[\'Revenue\'].sum().reset_index() highest_revenue_month = monthly_revenue.loc[monthly_revenue[\'Revenue\'].idxmax()] # Top 5 products by revenue top_5_products_by_revenue = total_revenue_per_product.sort_values(by=\'Revenue\', ascending=False).head(5) # Bottom 5 products by quantity sold total_quantity_per_product = df.groupby(\'Product_ID\')[\'Quantity\'].sum().reset_index() bottom_5_products_by_quantity = total_quantity_per_product.sort_values(by=\'Quantity\').head(5) # Summary DataFrame summary_data = { \\"total_revenue_per_product\\": total_revenue_per_product, \\"total_quantity_per_store\\": total_quantity_per_store, \\"monthly_revenue\\": monthly_revenue, \\"highest_revenue_month\\": highest_revenue_month, \\"top_5_products_by_revenue\\": top_5_products_by_revenue, \\"bottom_5_products_by_quantity\\": bottom_5_products_by_quantity } return summary_data"},{"question":"In this coding assessment, you will write a function that uses several utilities from the `sklearn.utils` module to preprocess an input dataset, validate the input, ensure consistency, and perform randomized Singular Value Decomposition (SVD). # Task Description Write a function `preprocess_and_svd` that performs the following steps: 1. **Validate and Preprocess the Inputs**: - Accept two inputs: `X` (a 2D array or sparse matrix) and `y` (a 1D array). - Use `sklearn.utils.check_X_y` to ensure that `X` and `y` have consistent lengths, and convert them to appropriate array formats. - Ensure that `X` does not contain any NaNs or Infs using `sklearn.utils.assert_all_finite`. - Convert `X` to a float array using `sklearn.utils.as_float_array`. 2. **Randomized Singular Value Decomposition**: - Use `sklearn.utils.extmath.randomized_svd` to compute the Singular Value Decomposition of `X`. - Return the three matrices `(U, S, Vt)` resulting from the SVD. # Function Signature ```python from sklearn.utils import check_X_y, assert_all_finite, as_float_array from sklearn.utils.extmath import randomized_svd import numpy as np def preprocess_and_svd(X, y): Validate, preprocess, and perform randomized SVD on the input data. Parameters: - X (2D array or sparse matrix): The input data matrix. - y (1D array): The target array. Returns: - U (2D array): Unitary matrix having left singular vectors as columns. - S (1D array): The singular values, sorted in non-increasing order. - Vt (2D array): Unitary matrix having right singular vectors as rows. # Perform data validation and preprocessing X, y = check_X_y(X, y) assert_all_finite(X) X = as_float_array(X) # Perform randomized SVD U, S, Vt = randomized_svd(X, n_components=min(X.shape), random_state=0) return U, S, Vt ``` # Constraints and Performance - You may assume that `X` is a 2D array or a sparse matrix, and `y` is a 1D array. - `X` and `y` have consistent lengths (i.e., `len(X) == len(y)`). - Prioritize code readability and proper usage of the scikit-learn utilities mentioned. - Performance should be considered when dealing with large matrices, but the primary focus is on correctness and proper utilization of the utilities. # Example ```python X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) y = np.array([1, 2, 3]) U, S, Vt = preprocess_and_svd(X, y) print(\\"U:\\", U) print(\\"S:\\", S) print(\\"Vt:\\", Vt) ``` This will validate the inputs, preprocess them, perform the randomized SVD, and print the resulting matrices.","solution":"from sklearn.utils import check_X_y, assert_all_finite, as_float_array from sklearn.utils.extmath import randomized_svd import numpy as np def preprocess_and_svd(X, y): Validate, preprocess, and perform randomized SVD on the input data. Parameters: - X (2D array or sparse matrix): The input data matrix. - y (1D array): The target array. Returns: - U (2D array): Unitary matrix having left singular vectors as columns. - S (1D array): The singular values, sorted in non-increasing order. - Vt (2D array): Unitary matrix having right singular vectors as rows. # Perform data validation and preprocessing X, y = check_X_y(X, y) assert_all_finite(X) X = as_float_array(X) # Perform randomized SVD n_components = min(X.shape[0], X.shape[1]) U, S, Vt = randomized_svd(X, n_components=n_components, random_state=0) return U, S, Vt"},{"question":"# Python Coding Assessment **Problem Statement:** You are tasked with simulating a simple scientific calculator that performs various mathematical computations including handling different number types (integers, floats, complex numbers, decimals, and fractions). Your calculator will also support random number generation and basic statistical analysis. # Requirements: 1. **Functionality:** * **Arithmetic Operations:** Implement basic arithmetic operations (addition, subtraction, multiplication, division) that work seamlessly with different numeric types (`int`, `float`, `complex`, `decimal.Decimal`, `fractions.Fraction`). * **Mathematical Functions:** Implement common mathematical functions (`sqrt`, `log`, `sin`, `cos`, `tan`) that can handle both floats and complex numbers. * **Random Number Generation:** Support generating random integers, floating-point numbers, and random selections from a list. * **Statistics Functions:** Implement basic statistical functions such as mean, median, variance, and standard deviation. 2. **Input and Output Formats:** * Your calculator should be able to parse input commands in the form of strings and output the results accordingly. * Input commands will include arithmetic expressions, function calls, and data for statistics functions. * Example input: `\\"add 3.5 5\\"` should output `8.5`, `\\"cos 3.14\\"` should output `-0.9999987317275395`. * For statistical functions, input will be provided as space-separated values, e.g., `\\"mean 1 2 3 4\\"` should output `2.5`. 3. **Constraints:** * Ensure precision and correctness, especially when dealing with `decimal.Decimal` and `fractions.Fraction`. * Handle invalid inputs gracefully with appropriate error messages. 4. **Performance:** * Operations should be optimized to handle large datasets efficiently, especially for statistical functions. # Implementation: Write a class `ScientificCalculator` with the following methods: * `parse_input(command: str) -> str`: Parses the input command and returns the result as a string. * Implement helper methods for each arithmetic operation, mathematical function, random number generation, and statistical function. **Example:** ```python from decimal import Decimal from fractions import Fraction class ScientificCalculator: def parse_input(self, command: str) -> str: # Parse the input command and call the appropriate method pass def add(self, a, b): # Perform addition pass def subtract(self, a, b): # Perform subtraction pass def multiply(self, a, b): # Perform multiplication pass def divide(self, a, b): # Perform division pass # Define other methods for sqrt, log, sin, cos, tan, mean, median, variance, stddev, random_int, random_float, random_choice # Example usage calc = ScientificCalculator() print(calc.parse_input(\\"add 3.5 5\\")) # Outputs: 8.5 print(calc.parse_input(\\"cos 3.14\\")) # Outputs: -0.9999987317275395 print(calc.parse_input(\\"mean 1 2 3 4\\")) # Outputs: 2.5 ``` Make sure to test your implementation thoroughly with various inputs to ensure accuracy and robustness.","solution":"import math import random from decimal import Decimal from fractions import Fraction from statistics import mean, median, variance, stdev class ScientificCalculator: def parse_input(self, command: str) -> str: try: parts = command.split() operation = parts[0] args = parts[1:] if operation == \\"add\\": return str(self.add(*map(self.convert_to_number, args))) elif operation == \\"subtract\\": return str(self.subtract(*map(self.convert_to_number, args))) elif operation == \\"multiply\\": return str(self.multiply(*map(self.convert_to_number, args))) elif operation == \\"divide\\": return str(self.divide(*map(self.convert_to_number, args))) elif operation == \\"sqrt\\": return str(self.sqrt(self.convert_to_number(args[0]))) elif operation == \\"log\\": return str(self.log(self.convert_to_number(args[0]))) elif operation == \\"sin\\": return str(self.sin(self.convert_to_number(args[0]))) elif operation == \\"cos\\": return str(self.cos(self.convert_to_number(args[0]))) elif operation == \\"tan\\": return str(self.tan(self.convert_to_number(args[0]))) elif operation == \\"mean\\": return str(mean(map(self.convert_to_number, args))) elif operation == \\"median\\": return str(median(map(self.convert_to_number, args))) elif operation == \\"variance\\": return str(variance(map(self.convert_to_number, args))) elif operation == \\"stddev\\": return str(stdev(map(self.convert_to_number, args))) elif operation == \\"random_int\\": return str(random.randint(int(args[0]), int(args[1]))) elif operation == \\"random_float\\": return str(random.uniform(float(args[0]), float(args[1]))) elif operation == \\"random_choice\\": return str(random.choice(args)) else: return \\"Unsupported operation\\" except Exception as e: return f\\"Error: {str(e)}\\" def convert_to_number(self, value): try: return int(value) except ValueError: try: return float(value) except ValueError: return Decimal(value) # Fallback to Decimal def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Division by zero\\") return a / b def sqrt(self, value): if isinstance(value, (int, float)): return math.sqrt(value) else: return value.sqrt() def log(self, value): if isinstance(value, (int, float)): return math.log(value) else: return value.ln() def sin(self, value): return math.sin(value) def cos(self, value): return math.cos(value) def tan(self, value): return math.tan(value)"},{"question":"**Objective:** Your task is to create a Python script that performs the following operations using the `urllib.request` module: 1. Fetch a URLâ€™s content. 2. Handle HTTP authentication. 3. Manage proxy settings. 4. Handle HTTP redirections and errors. **Requirements:** 1. Implement a function `fetch_url_with_auth_and_proxy` that takes the following parameters: - `url` (str): The URL to fetch. - `username` (str): Username for HTTP Basic Authentication. - `password` (str): Password for HTTP Basic Authentication. - `realm` (str): Realm for HTTP Basic Authentication. - `proxy_url` (str): URL for the HTTP proxy server. - `proxy_user` (str, optional): Username for proxy authentication. - `proxy_pass` (str, optional): Password for proxy authentication. 2. Use the `urllib.request.build_opener()` function to create an opener that: - Supports HTTP Basic Authentication. - Uses the specified HTTP proxy. - Handles HTTP redirection (301, 302, 303, 307). - Handles HTTP errors (raise exceptions on errors). 3. The function should return the HTTP status code and the first 100 bytes of the content of the URL if the request is successful. **Constraints:** - You must use only the `urllib.request` module to implement the function. - Ensure that the proxy settings are correctly applied even if the environment has existing proxy configurations. - Handle any exceptions that occur during the URL fetching process and print appropriate error messages. **Function Signature:** ```python def fetch_url_with_auth_and_proxy(url: str, username: str, password: str, realm: str, proxy_url: str, proxy_user: str = None, proxy_pass: str = None) -> (int, bytes): pass ``` **Example Usage:** ```python url = \\"http://example.com\\" username = \\"user\\" password = \\"pass\\" realm = \\"example-realm\\" proxy_url = \\"http://proxy.example.com:8080\\" proxy_user = \\"proxyuser\\" proxy_pass = \\"proxypass\\" status_code, content = fetch_url_with_auth_and_proxy(url, username, password, realm, proxy_url, proxy_user, proxy_pass) print(f\\"Status Code: {status_code}\\") print(f\\"Content: {content}\\") ``` **Note:** - Ensure proper handling of different HTTP status codes and the corresponding actions as per the guidelines. - Test your function with various URLs requiring HTTP authentication, using a proxy, and causing redirections to ensure robustness.","solution":"import urllib.request from urllib.error import HTTPError, URLError from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, ProxyHandler, build_opener def fetch_url_with_auth_and_proxy(url: str, username: str, password: str, realm: str, proxy_url: str, proxy_user: str = None, proxy_pass: str = None) -> (int, bytes): try: # Create a password manager password_mgr = HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(realm, url, username, password) # Create an authentication handler auth_handler = HTTPBasicAuthHandler(password_mgr) # Create a proxy handler proxy_support = ProxyHandler({\\"http\\": proxy_url, \\"https\\": proxy_url}) # Add proxy authentication if required if proxy_user and proxy_pass: proxy_auth_handler = ProxyHandler({\\"http\\": f\\"http://{proxy_user}:{proxy_pass}@{proxy_url}\\", \\"https\\": f\\"http://{proxy_user}:{proxy_pass}@{proxy_url}\\"}) opener = build_opener(auth_handler, proxy_support, proxy_auth_handler) else: opener = build_opener(auth_handler, proxy_support) # Install the opener urllib.request.install_opener(opener) # Fetch the URL response = urllib.request.urlopen(url) status_code = response.getcode() content = response.read(100) return status_code, content except HTTPError as e: print(f\'HTTP error occurred: {e.code} - {e.reason}\') except URLError as e: print(f\'URL error occurred: {e.reason}\') except Exception as e: print(f\'Unexpected error occurred: {e}\') return None, None"},{"question":"# Unicode Data Analyzer You are tasked with creating a `UnicodeDataAnalyzer` class that provides methods for analyzing Unicode strings using the `unicodedata` module. The class should include the following functionalities: 1. **Character Information**: - Implement a method `get_character_info(char: str) -> dict` that takes a single character as input and returns a dictionary with the following information: - `name`: The name of the character. - `decimal`: The decimal value of the character (if available). - `digit`: The digit value of the character (if available). - `numeric`: The numeric value of the character (if available). - `category`: The general category of the character. - `bidirectional`: The bidirectional class of the character. - `combining`: The canonical combining class of the character. - `east_asian_width`: The East Asian width of the character. - `mirrored`: Whether the character is mirrored in bidirectional text. - `decomposition`: The decomposition mapping of the character. 2. **Normalize String**: - Implement a method `normalize_string(form: str, string: str) -> str` that takes a normalization form and a Unicode string as input and returns the normalized string. Valid values for the normalization form are \'NFC\', \'NFKC\', \'NFD\', and \'NFKD\'. 3. **Check Normalized**: - Implement a method `is_string_normalized(form: str, string: str) -> bool` that takes a normalization form and a Unicode string as input and returns whether the string is in the specified normalized form. # Constraints: - The input character for `get_character_info` will always be a single character string. - Assume the input normalization form for `normalize_string` and `is_string_normalized` is always one of the valid values (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\'). # Example Usage: ```python # Example of using UnicodeDataAnalyzer class analyzer = UnicodeDataAnalyzer() # Get character information char_info = analyzer.get_character_info(\'A\') print(char_info) # Output: # { # \'name\': \'LATIN CAPITAL LETTER A\', # \'decimal\': None, # \'digit\': None, # \'numeric\': None, # \'category\': \'Lu\', # \'bidirectional\': \'L\', # \'combining\': 0, # \'east_asian_width\': \'Na\', # \'mirrored\': 0, # \'decomposition\': \'\' # } # Normalize string normalized_str = analyzer.normalize_string(\'NFC\', \'Au0301\') print(normalized_str) # Output: \'Ã\' # Check if string is normalized is_normalized = analyzer.is_string_normalized(\'NFC\', \'Ã\') print(is_normalized) # Output: True ``` Implement the `UnicodeDataAnalyzer` class with the specified methods.","solution":"import unicodedata class UnicodeDataAnalyzer: def get_character_info(self, char: str) -> dict: info = { \'name\': unicodedata.name(char), \'decimal\': unicodedata.decimal(char, None), \'digit\': unicodedata.digit(char, None), \'numeric\': unicodedata.numeric(char, None), \'category\': unicodedata.category(char), \'bidirectional\': unicodedata.bidirectional(char), \'combining\': unicodedata.combining(char), \'east_asian_width\': unicodedata.east_asian_width(char), \'mirrored\': unicodedata.mirrored(char), \'decomposition\': unicodedata.decomposition(char) } return info def normalize_string(self, form: str, string: str) -> str: return unicodedata.normalize(form, string) def is_string_normalized(self, form: str, string: str) -> bool: return unicodedata.is_normalized(form, string)"},{"question":"**Memory Management and Profiling with PyTorch MPS** In this coding assessment, you will demonstrate your understanding of PyTorch\'s `torch.mps` module by writing functions to manage memory and profile computations on MPS devices. You will implement the following tasks: # Task 1: Memory Management Write a function `manage_mps_memory` that: 1. Seeds the MPS random number generator with a specific seed. 2. Sets the per-process memory usage to 50% of the total GPU memory. 3. Gets the current and driver allocated memory. 4. Empties the cache to free up memory. The function should return the current allocated memory and driver allocated memory before and after clearing the cache. Input - `seed_value` (int): The seed value for the random number generator. Output - A dictionary with the following structure: ```python { \\"before\\": { \\"current_allocated_memory\\": <int>, \\"driver_allocated_memory\\": <int> }, \\"after\\": { \\"current_allocated_memory\\": <int>, \\"driver_allocated_memory\\": <int> } } ``` # Task 2: Profiling Computations Write a function `profile_tensor_operations` that: 1. Starts the MPS profiler. 2. Performs a series of tensor operations on MPS (e.g., matrix multiplications). 3. Stops the profiler and returns the profiling results. Output - A dictionary containing profiling results (you may return any relevant information captured by the profiler). # Example Usage: ```python # Task 1 memory_info = manage_mps_memory(seed_value=42) print(memory_info) # Task 2 profiler_results = profile_tensor_operations() print(profiler_results) ``` # Constraints - Ensure that your code runs efficiently on systems with MPS support. - Do not exceed memory limits (use appropriate tensor sizes for the operations). # Performance Requirements - The functions should handle typical tensor sizes and operations efficiently without unnecessary overhead. Good luck, and demonstrate your proficiency with PyTorch MPS!","solution":"import torch def manage_mps_memory(seed_value): Manages MPS memory: 1. Seeds the MPS random number generator. 2. Sets the per-process memory usage to 50% of the total GPU memory. 3. Gets the current and driver allocated memory. 4. Empties the cache to free up memory. Args: - seed_value (int): The seed value for the random number generator. Returns: - dict: A dictionary with the current and driver allocated memory before and after clearing the cache. # Setting the seed for random number generator torch.mps.manual_seed(seed_value) # Setting the per-process memory usage to 50% of the total GPU memory (this will raise an attribute error in the current PyTorch version as it\'s not directly supported) torch.mps.set_per_process_memory_fraction(0.5) # Getting current and driver allocated memory before clearing the cache current_allocated_memory_before = torch.mps.memory_allocated() driver_allocated_memory_before = torch.mps.driver_memory_allocated() # Clearing the cache torch.mps.empty_cache() # Getting current and driver allocated memory after clearing the cache current_allocated_memory_after = torch.mps.memory_allocated() driver_allocated_memory_after = torch.mps.driver_memory_allocated() return { \\"before\\": { \\"current_allocated_memory\\": current_allocated_memory_before, \\"driver_allocated_memory\\": driver_allocated_memory_before, }, \\"after\\": { \\"current_allocated_memory\\": current_allocated_memory_after, \\"driver_allocated_memory\\": driver_allocated_memory_after, } } def profile_tensor_operations(): Profiles tensor operations on MPS: 1. Starts the MPS profiler. 2. Performs a series of tensor operations on MPS. 3. Stops the profiler and returns the profiling results. Returns: - dict: A dictionary containing profiling results. # Initialize profiler profiler = torch.profiler.profile( activities=[ torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.MPS ], schedule=torch.profiler.schedule(wait=1, warmup=1, active=2), on_trace_ready=torch.profiler.tensorboard_trace_handler(\'./log\') ) with profiler: # Perform a series of tensor operations on MPS x = torch.randn(1000, 1000, device=\'mps\') y = torch.randn(1000, 1000, device=\'mps\') z = torch.matmul(x, y) return profiler.key_averages().table() # Checking if MPS is available if not torch.backends.mps.is_available(): manage_mps_memory = None profile_tensor_operations = None"},{"question":"You are given a text file compressed using the LZMA algorithm. Your task is to implement a function that reads the compressed file, processes the text content by reversing the order of lines and words within each line, and writes the processed text back into a new compressed file. To ensure the processed file has a high compression ratio and integrity, you are required to use a custom filter chain during compression. # Function Signature ```python import lzma def process_compressed_file(input_filename: str, output_filename: str) -> None: This function reads a compressed file, processes the content by reversing the order of lines and words within each line, and writes the processed content into a new compressed file using a custom filter chain. Args: - input_filename (str): The path to the input compressed file. - output_filename (str): The path to the output compressed file. Returns: - None # Your code here ``` # Input - `input_filename` (str): The path to the input LZMA compressed file, e.g., `\\"input_file.xz\\"`. - `output_filename` (str): The path where the processed LZMA compressed file will be saved, e.g., `\\"output_file.xz\\"`. # Output - The function should create a new compressed file at `output_filename` with the processed content. # Constraints - Assume the size of the uncompressed data in the input file fits into memory. # Example Assume the uncompressed content of `input_file.xz` is: ``` Hello world Python programming is fun Compress this text ``` After processing, the content in `output_file.xz` should be: ``` text this Compress fun is programming Python world Hello ``` # Implementation Requirements 1. Read the compressed file using the appropriate classes from the `lzma` module. 2. Process the content to reverse the order of lines and the words in each line. 3. Write the processed content to a new compressed file using a custom filter chain: ```python my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] ``` # Notes - Implement error handling for file operations and ensure the files are correctly opened and closed. - Use the `lzma` module for both reading and writing compressed data. - Make sure that the output file maintains the highest possible compression ratio and integrity using the provided filter chain.","solution":"import lzma def process_compressed_file(input_filename: str, output_filename: str) -> None: This function reads a compressed file, processes the content by reversing the order of lines and words within each line, and writes the processed content into a new compressed file using a custom filter chain. Args: - input_filename (str): The path to the input compressed file. - output_filename (str): The path to the output compressed file. Returns: - None # Read and decompress input file content with lzma.open(input_filename, \'rt\') as input_file: lines = input_file.readlines() # Process content by reversing the order of lines and words within each line reversed_lines = [ \' \'.join(line.split()[::-1]) for line in reversed(lines) ] processed_content = \'n\'.join(reversed_lines) + \'n\' # Ensure there\'s a newline at the end # Custom filter chain for high compression ratio my_filters = [ { \\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1 }, { \\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME } ] # Compress and write the processed content to the output file with lzma.open(output_filename, \'wt\', filters=my_filters) as output_file: output_file.write(processed_content)"},{"question":"# LZMA Compression and Decompression Task You are given a set of text files that need to be compressed into an LZMA-compressed `.xz` file format and later decompressed back to their original form. This task will assess your understanding of file handling, data compression, and decompression using the `lzma` module in Python. Task 1. Write a function `compress_files(filepaths, output_filename)` that takes in: - `filepaths`: A list of strings, where each string is the path to a text file. - `output_filename`: A string, the name of the output compressed file. This function should: - Read the contents of each file specified in `filepaths`. - Compress these contents into a single `.xz` file specified by `output_filename`. 2. Write a function `decompress_file(input_filename)` that takes in: - `input_filename`: A string, the name of the compressed `.xz` file. This function should: - Decompress the contents of `input_filename`. - Return a dictionary where the keys are the original file names (without paths) and the values are the corresponding decompressed contents. Example Usage ```python # Example file paths filepaths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] output_filename = \\"compressed_files.xz\\" # Compress the files compress_files(filepaths, output_filename) # Decompress the files decompressed_contents = decompress_file(\\"compressed_files.xz\\") print(decompressed_contents) ``` Constraints - Assume that all input files are plain text files with UTF-8 encoding. - You may assume the files exist and are readable. - The `filepaths` list will contain between 1 and 100 file paths. - The total size of all files will not exceed 50 MB. - The decompressed contents should be returned as UTF-8 encoded strings. Make sure to handle any potential errors gracefully, such as file I/O errors or issues with the LZMA compression/decompression process. Performance Requirements - The solution should efficiently handle the compression and decompression of multiple files. - Consider memory usage and execution time when processing the files. --- Implement these functions in Python, ensuring to follow best practices for file handling and error management.","solution":"import lzma import os def compress_files(filepaths, output_filename): Compresses the contents of the specified text files into a single .xz file. Parameters: filepaths (list of str): List of paths to text files to be compressed. output_filename (str): Name of the output compressed .xz file. try: with lzma.open(output_filename, \'w\') as xz_file: for filepath in filepaths: with open(filepath, \'r\', encoding=\'utf-8\') as f: file_content = f.read() # Write the filename and its length first xz_file.write(filepath.encode(\'utf-8\') + b\'x00\') xz_file.write(len(file_content).to_bytes(8, \'little\')) # Write the file content next xz_file.write(file_content.encode(\'utf-8\')) except Exception as e: print(f\\"An error occurred while compressing files: {e}\\") def decompress_file(input_filename): Decompresses the specified .xz file and returns a dictionary of file contents. Parameters: input_filename (str): Name of the input .xz file to be decompressed. Returns: dict: A dictionary where the keys are the original file names and values are their contents. decompressed_contents = {} try: with lzma.open(input_filename, \'r\') as xz_file: while True: # Read the filename (null-terminated string) file_name = bytearray() while True: byte = xz_file.read(1) if byte == b\'x00\': break if not byte: return decompressed_contents file_name += byte file_name = file_name.decode(\'utf-8\') # Read the size of the file content file_content_length = int.from_bytes(xz_file.read(8), \'little\') # Read the file content file_content = xz_file.read(file_content_length).decode(\'utf-8\') decompressed_contents[os.path.basename(file_name)] = file_content except Exception as e: print(f\\"An error occurred while decompressing file: {e}\\") return decompressed_contents"},{"question":"# Question: Filesystem Path Analyzer You are required to write a Python function, `filesystem_analyzer`, that takes in a directory path and performs the following operations using the `pathlib` module: 1. List all Python files (`.py` files) in the given directory and its subdirectories. 2. For each Python file found, determine the file size and its last modification time. 3. Return a list of tuples where each tuple consists of the relative path of the Python file, its size, and last modification time. The function signature should be: ```python from typing import List, Tuple def filesystem_analyzer(directory: str) -> List[Tuple[str, int, float]]: pass ``` # Input: - `directory`: A string representing the path to the directory you want to analyze. # Output: - A list of tuples. Each tuple consists of: - `relative_path`: A string representing the relative path to the Python file from the given directory. - `size`: An integer representing the size of the file in bytes. - `last_modified`: A float representing the last modification time of the file (Unix timestamp). # Constraints: - The function should handle non-existent directories gracefully by raising a `ValueError` with the message \\"Directory does not exist\\". - You may assume that you have read and write permissions in the given directory and its subdirectories. # Example: ```python # Suppose the following files are in the directory /example # /example/script.py (size: 200 bytes, last modified: 1609459200.0) # /example/module/helper.py (size: 500 bytes, last modified: 1609459300.0) result = filesystem_analyzer(\'/example\') # Expected output: # [(\'script.py\', 200, 1609459200.0), (\'module/helper.py\', 500, 1609459300.0)] ``` # Execution: Your implementation should be efficient and make use of the following `pathlib` methods and properties: - `Path.glob()` - `Path.stat()` - `Path.relative_to()` # Note: - Submit the complete solution that demonstrates your understanding and usage of the `pathlib` module comprehensively.","solution":"from typing import List, Tuple from pathlib import Path def filesystem_analyzer(directory: str) -> List[Tuple[str, int, float]]: directory_path = Path(directory) if not directory_path.exists() or not directory_path.is_dir(): raise ValueError(\\"Directory does not exist\\") result = [] for py_file in directory_path.rglob(\\"*.py\\"): relative_path = py_file.relative_to(directory_path) file_stat = py_file.stat() result.append((str(relative_path), file_stat.st_size, file_stat.st_mtime)) return result"},{"question":"**PyTorch Coding Assessment: Custom Dataset and DataLoader** You are required to implement a custom dataset and utilize PyTorch\'s `DataLoader` functionalities effectively. The assessment will cover the critical aspects of creating a dataset, using samplers, implementing multi-process data loading, and using memory pinning for efficient data transfer to a CUDA-enabled GPU. # Task: 1. **Create a Custom Dataset** - Implement a custom dataset class `RandomDataset` that extends `torch.utils.data.Dataset`. - The dataset should generate random data samples of a specified size and length. 2. **Implement Different Samplers** - Implement and utilize both `SequentialSampler` and `RandomSampler` to load data from the `RandomDataset`. 3. **Use DataLoader for Multi-process Data Loading** - Use PyTorchâ€™s `DataLoader` to load data from your dataset utilizing multiple worker processes. 4. **Apply Memory Pinning** - Ensure that the data is preprocessed to enable memory pinning for faster data transfer to a GPU. # Code Specifications: - **Custom Dataset (`RandomDataset`):** - Constructor Inputs: - `data_size` (int): Size of each data sample. - `length` (int): Number of samples in the dataset. - Implements the `__len__` and `__getitem__` methods. ```python import torch from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler class RandomDataset(Dataset): def __init__(self, data_size, length): self.data_size = data_size self.length = length def __len__(self): return self.length def __getitem__(self, idx): return torch.randn(self.data_size), idx ``` - **DataLoader with Samplers and Multi-Process Loading:** - Use `SequentialSampler` and `RandomSampler` with the `DataLoader`. - Implement data loading using `num_workers > 0`. - Apply pinning to memory for CUDA-enabled systems. ```python def create_data_loader(dataset, batch_size, sampler, num_workers, pin_memory): return DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=pin_memory) # Define dataset data_size = 100 # Size of each data sample length = 1000 # Number of samples dataset = RandomDataset(data_size, length) # Create DataLoader with SequentialSampler sequential_loader = create_data_loader(dataset, batch_size=10, sampler=SequentialSampler(dataset), num_workers=4, pin_memory=True) # Create DataLoader with RandomSampler random_loader = create_data_loader(dataset, batch_size=10, sampler=RandomSampler(dataset), num_workers=4, pin_memory=True) # Example usage (iterate over one batch) for batch in sequential_loader: data, indices = batch print(data, indices) break for batch in random_loader: data, indices = batch print(data, indices) break ``` # Output Requirements: - Submit the complete implementation of the custom dataset. - Provide two different instances of DataLoader using SequentialSampler and RandomSampler with multi-process loading and memory pinning. - Ensure the implementation runs without error and prints a batch of data from each DataLoader instance. **Constraints:** - Dataset size must be sufficiently large to demonstrate multi-process loading benefits. - Validate the usage of `pin_memory` by transferring data to a GPU (if available). # Performance Considerations: - Ensure efficient data loading and memory utilization to prevent bottlenecks. # Notes: - The solution will be evaluated based on correctness, clarity, and efficiency. - Proper exception handling and code documentation will be considered in evaluation.","solution":"import torch from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler class RandomDataset(Dataset): def __init__(self, data_size, length): self.data_size = data_size self.length = length def __len__(self): return self.length def __getitem__(self, idx): return torch.randn(self.data_size), idx def create_data_loader(dataset, batch_size, sampler, num_workers, pin_memory): return DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=pin_memory) # Define dataset data_size = 100 # Size of each data sample length = 1000 # Number of samples dataset = RandomDataset(data_size, length) # Create DataLoader with SequentialSampler sequential_loader = create_data_loader(dataset, batch_size=10, sampler=SequentialSampler(dataset), num_workers=4, pin_memory=True) # Create DataLoader with RandomSampler random_loader = create_data_loader(dataset, batch_size=10, sampler=RandomSampler(dataset), num_workers=4, pin_memory=True) # Example usage (iterate over one batch) for batch in sequential_loader: data, indices = batch print(data, indices) break for batch in random_loader: data, indices = batch print(data, indices) break"},{"question":"You are given a list of records (tuples), where each record contains a person\'s name and their score (e.g., `(\'Alice\', 85)`). You need to create a class `ScoreManager` that maintains a sorted list of these records based on the scores. Using the `bisect` module, implement the following methods: 1. **`__init__(self)`**: - Initialize an empty list to store the records. 2. **`add_record(self, name: str, score: int) -> None`**: - Add a record `(name, score)` to the list while maintaining the sorted order based on the scores. - Use `bisect.insort_right` to facilitate this. 3. **`find_top_scorer(self) -> Tuple[str, int]`**: - Return the record with the highest score. Assume there is at least one record in the list. - If multiple records have the highest score, return the one that was added last. - (Hint: The record with the highest score will be at the rightmost position of the list.) 4. **`find_score(self, name: str) -> int`**: - Return the score of the record with the given name. - If there is no record with the given name, raise a `ValueError`. 5. **`remove_record(self, name: str) -> None`**: - Remove the record with the given name. If the record is not found, raise a `ValueError`. # Constraints: - The names are unique. - The names and scores are provided as input to the methods. # Example Implementation: ```python from typing import List, Tuple import bisect class ScoreManager: def __init__(self): # Initialize ScoreManager self.records = [] def add_record(self, name: str, score: int) -> None: # Add a record while maintaining sorted order based on the scores bisect.insort_right(self.records, (score, name)) def find_top_scorer(self) -> Tuple[str, int]: # Return the record with the highest score if not self.records: raise ValueError(\\"No records available.\\") return self.records[-1][1], self.records[-1][0] def find_score(self, name: str) -> int: # Return the score of the record with the given name for score, record_name in self.records: if record_name == name: return score raise ValueError(f\\"No record found for {name}.\\") def remove_record(self, name: str) -> None: # Remove the record with the given name for i, (score, record_name) in enumerate(self.records): if record_name == name: self.records.pop(i) return raise ValueError(f\\"No record found for {name}.\\") ``` Complete the `ScoreManager` class to meet the requirements stated above. Test your implementation with various test cases to ensure correctness.","solution":"from typing import List, Tuple import bisect class ScoreManager: def __init__(self): self.records = [] def add_record(self, name: str, score: int) -> None: # Add a record while maintaining sorted order based on the scores bisect.insort_right(self.records, (score, name)) def find_top_scorer(self) -> Tuple[str, int]: # Return the record with the highest score if not self.records: raise ValueError(\\"No records available.\\") return self.records[-1][1], self.records[-1][0] def find_score(self, name: str) -> int: # Return the score of the record with the given name (Name is unique) for score, record_name in self.records: if record_name == name: return score raise ValueError(f\\"No record found for {name}.\\") def remove_record(self, name: str) -> None: # Remove the record with the given name for i, (score, record_name) in enumerate(self.records): if record_name == name: self.records.pop(i) return raise ValueError(f\\"No record found for {name}.\\")"},{"question":"# Pandas ExtensionArray Implementation Background: Pandas allows for the creation of custom data types using the `ExtensionDtype` and `ExtensionArray` classes. This is particularly useful for library authors looking to extend pandas with custom behavior. In this task, you will implement a custom pandas ExtensionArray and ExtensionDtype that represent a simple `PositiveIntegerArray` where all the values must be positive integers. Task: 1. Implement a custom `PositiveIntegerDtype` that extends `pandas.api.extensions.ExtensionDtype`. 2. Implement a custom `PositiveIntegerArray` that extends `pandas.api.extensions.ExtensionArray` and enforces the constraint that all elements are positive integers. 3. Implement the necessary methods within `PositiveIntegerArray` to ensure it integrates seamlessly with pandas. Requirements: # `PositiveIntegerDtype` - Must subclass `pandas.api.extensions.ExtensionDtype`. - Define the name of the dtype as `positive_integer`. - Implement required class properties and methods: `type`, `name`, `construct_array_type`. # `PositiveIntegerArray` - Must subclass `pandas.api.extensions.ExtensionArray`. - Implement required methods and properties: - `__init__`: Initialize with a list or array of positive integers. - `_from_sequence`: Create an instance of your array from a sequence of values. - `_from_factorized`: Create an instance from factorized values. - `_concat_same_type`: Concatenate multiple arrays of the same type. - Any other abstract methods required by `ExtensionArray`. Make sure to include validation that ensures all elements are positive integers. If an invalid value is encountered, raise a `ValueError`. Examples & Expected Behavior: ```python import pandas as pd # Define the PositiveIntegerDtype class PositiveIntegerDtype(pd.api.extensions.ExtensionDtype): name = \'positive_integer\' type = int @classmethod def construct_array_type(cls): return PositiveIntegerArray # Define the PositiveIntegerArray class PositiveIntegerArray(pd.api.extensions.ExtensionArray): def __init__(self, values): # Validate all values are positive integers if not all(isinstance(i, int) and i > 0 for i in values): raise ValueError(\\"All elements must be positive integers\\") self._data = pd.array(values, dtype=int) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) @classmethod def _from_factorized(cls, values, original): return cls(values) def _concat_same_type(self, to_concat): data = sum([x._data.tolist() for x in to_concat], []) return PositiveIntegerArray(data) @property def dtype(self): return PositiveIntegerDtype() @property def nbytes(self): return self._data.nbytes def __getitem__(self, item): return self._data[item] def __len__(self): return len(self._data) # More methods should be implemented to comply with ExtensionArray requirements # Usage data = [1, 2, 3, 4] positive_int_series = pd.Series(PositiveIntegerArray(data)) print(positive_int_series) ``` Here, we showcase an array and dtype that ensure only positive integers are allowed. Further methods should also be implemented to fully comply with pandas `ExtensionArray` standards for a complete solution. Notes: - Ensure all methods as required by `ExtensionArray` are properly implemented. - Test your implementation with various pandas functionalities to ensure it behaves as expected. - Handle edge cases, such as empty arrays and invalid inputs properly.","solution":"import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype, register_extension_dtype import numpy as np @register_extension_dtype class PositiveIntegerDtype(ExtensionDtype): name = \\"positive_integer\\" type = int kind = \\"i\\" na_value = np.nan @classmethod def construct_array_type(cls): return PositiveIntegerArray class PositiveIntegerArray(ExtensionArray): def __init__(self, values): values = pd.array(values, dtype=\\"Int64\\") if not all(isinstance(i, (int, np.integer)) and i > 0 for i in values if pd.notna(i)): raise ValueError(\\"All elements must be positive integers or NAs\\") self._data = values @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) @classmethod def _from_factorized(cls, values, original): return cls(values) def _concat_same_type(self, to_concat): data = sum((ea._data.tolist() for ea in to_concat), []) return PositiveIntegerArray(data) @property def dtype(self): return PositiveIntegerDtype() @property def nbytes(self): return self._data.nbytes def __getitem__(self, item): result = self._data[item] if np.isscalar(result): return result return PositiveIntegerArray(result) def __len__(self): return len(self._data) def isna(self): return self._data.isna() def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: fill_value = self.dtype.na_value if fill_value is None else fill_value return PositiveIntegerArray(pd.array(self._data.take(indices, allow_fill=True, fill_value=fill_value), dtype=\\"Int64\\")) return PositiveIntegerArray(self._data.take(indices)) def copy(self): return PositiveIntegerArray(self._data.copy()) def __eq__(self, other): if isinstance(other, PositiveIntegerArray): return (self._data == other._data).all() return False def __repr__(self): return f\\"PositiveIntegerArray({self._data.tolist()})\\" # Example usage data = [1, 2, 3, 4] positive_int_series = pd.Series(PositiveIntegerArray(data)) print(positive_int_series)"},{"question":"You are tasked with creating a function that utilizes the `urllib.robotparser` module to analyze and determine the crawling rules specified in a website\'s `robots.txt` file. This function should check whether a given user agent is allowed to crawl a list of URLs on the website, and if allowed, compute the total time required to crawl all the URLs given the specified crawl delay and request rate constraints. Function Signature ```python def analyze_crawling_rules(robot_txt_url: str, useragent: str, urls: list[str]) -> dict: pass ``` Input - `robot_txt_url`: A string representing the URL of the `robots.txt` file of the website. - `useragent`: A string representing the user agent of the crawler. - `urls`: A list of strings, each representing a URL to be checked against crawling permissions. Output - Returns a dictionary with the following keys: - `can_crawl`: A boolean indicating if the user agent can crawl all the URLs. - `total_time`: If `can_crawl` is `True`, an integer representing the total time (in seconds) required to crawl all the URLs considering the crawl delay and request rate. If `can_crawl` is `False`, this value should be `None`. Constraints - Assume the `robots.txt` file follows the standard specifications. - If the `crawl-delay` is not specified for the user agent, assume a default of 1 second. - If the `request-rate` is not specified, assume there are no request rate limitations beyond the crawl delay. Example ```python # Example robots.txt file at http://example.com/robots.txt: # User-agent: * # Disallow: /private/ # Crawl-delay: 5 # Request-rate: 2/10 robot_txt_url = \\"http://example.com/robots.txt\\" useragent = \\"*\\" urls = [\\"http://example.com/page1\\", \\"http://example.com/private/page2\\", \\"http://example.com/page3\\"] # Expected Output: # { # \'can_crawl\': False, # \'total_time\': None # } ``` In this example, the user agent is not allowed to fetch `http://example.com/private/page2`, so `can_crawl` is `False`, and `total_time` is `None`. Notes - Ensure your function handles all edge cases, such as invalid `robots.txt` syntax or URLs. - Use the `urllib.robotparser.RobotFileParser` class to implement the function. - The function should be efficient and handle large input sizes gracefully.","solution":"from urllib.robotparser import RobotFileParser def analyze_crawling_rules(robot_txt_url: str, useragent: str, urls: list[str]) -> dict: rp = RobotFileParser() rp.set_url(robot_txt_url) rp.read() # Read the crawl delay (default to 1 if not specified) crawl_delay = rp.crawl_delay(useragent) if crawl_delay is None: crawl_delay = 1 can_crawl = True for url in urls: if not rp.can_fetch(useragent, url): can_crawl = False break if can_crawl: total_time = crawl_delay * len(urls) else: total_time = None return { \'can_crawl\': can_crawl, \'total_time\': total_time }"},{"question":"Advanced Usage of PLS in scikit-learn Objective: The goal of this task is to test your understanding of the Partial Least Squares (PLS) regression algorithm using the `scikit-learn` library. You are required to implement a function that utilizes `PLSRegression` to model and predict the relationship between two datasets. Problem Statement: You are given two datasets, (X) and (Y), represented as 2D NumPy arrays. The goal is to fit a `PLSRegression` model using these datasets and then use the model to transform and predict the outputs for (Y). Function Signature: ```python import numpy as np from sklearn.cross_decomposition import PLSRegression def pls_regression(X: np.ndarray, Y: np.ndarray, n_components: int) -> dict: Fits a PLSRegression model, transforms the datasets, and predicts the output. Parameters: X (np.ndarray): The input data matrix with shape (n_samples, n_features). Y (np.ndarray): The target data matrix with shape (n_samples, n_targets). n_components (int): The number of components to keep. Returns: dict: A dictionary containing: - \'x_scores\': The transformed input data. - \'y_scores\': The transformed target data. - \'predicted_Y\': The predicted target matrix for the input data X. # Your implementation here return { \\"x_scores\\": x_scores, \\"y_scores\\": y_scores, \\"predicted_Y\\": predicted_Y } ``` Input Format: 1. `X`: A 2D NumPy array of shape `(n_samples, n_features)` representing the input data. 2. `Y`: A 2D NumPy array of shape `(n_samples, n_targets)` representing the target data. 3. `n_components`: An integer indicating the number of components to keep in the PLS model. Output Format: A dictionary with the following keys: - `\'x_scores\'`: A 2D NumPy array representing the transformed input data. - `\'y_scores\'`: A 2D NumPy array representing the transformed target data. - `\'predicted_Y\'`: A 2D NumPy array representing the predicted target matrix based on the input data (X). Constraints: - You may assume that `n_components` is a positive integer and does not exceed the minimum of the number of samples, features, and targets. Example: ```python import numpy as np # Example Data X = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]) Y = np.array([[1.4], [2.5], [3.1], [4.7]]) # Number of components n_components = 2 # Running the function result = pls_regression(X, Y, n_components) print(result) ``` Explanation: In this example, your implementation should correctly fit a `PLSRegression` model to the given data, transform the datasets, and make predictions as required. Notes: - Ensure you use the `PLSRegression` class from `sklearn.cross_decomposition`. - You can assume that the input data is clean and valid. - Pay attention to the shapes of arrays to avoid broadcasting errors.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression def pls_regression(X: np.ndarray, Y: np.ndarray, n_components: int) -> dict: Fits a PLSRegression model, transforms the datasets, and predicts the output. Parameters: X (np.ndarray): The input data matrix with shape (n_samples, n_features). Y (np.ndarray): The target data matrix with shape (n_samples, n_targets). n_components (int): The number of components to keep. Returns: dict: A dictionary containing: - \'x_scores\': The transformed input data. - \'y_scores\': The transformed target data. - \'predicted_Y\': The predicted target matrix for the input data X. model = PLSRegression(n_components=n_components) model.fit(X, Y) x_scores = model.transform(X) y_scores = model.y_scores_ predicted_Y = model.predict(X) return { \\"x_scores\\": x_scores, \\"y_scores\\": y_scores, \\"predicted_Y\\": predicted_Y }"},{"question":"# Question: Data Analysis with pandas You are given datasets representing sales data of various products in different regions. Your task is to implement several functions to manipulate and analyze this data using pandas. Dataset Descriptions 1. **Sales_Dictionary**: A dictionary where keys are product names and values are dictionaries representing sales in different regions. ```python sales_data = { \\"Product_A\\": {\\"Region_West\\": 150, \\"Region_East\\": 200, \\"Region_Central\\": 100}, \\"Product_B\\": {\\"Region_West\\": 300, \\"Region_East\\": 100, \\"Region_Central\\": 150}, \\"Product_C\\": {\\"Region_West\\": 100, \\"Region_East\\": 250, \\"Region_Central\\": 200}, } ``` 2. **Additional DataFrame**: ```python import pandas as pd additional_data = pd.DataFrame({ \\"Product\\": [\\"Product_D\\", \\"Product_E\\"], \\"Region_West\\": [130, 170], \\"Region_East\\": [280, 220], \\"Region_Central\\": [150, 190] }) ``` Functions to Implement 1. **create_sales_dataframe(sales_data: dict) -> pd.DataFrame**: Create a DataFrame from the sales dictionary. The DataFrame should have products as the index and regions as columns. 2. **add_additional_data(df: pd.DataFrame, additional_data: pd.DataFrame) -> pd.DataFrame**: Append additional sales data to the initial DataFrame. Products should be the index. 3. **calculate_total_sales(df: pd.DataFrame) -> pd.DataFrame**: Add a new column named `Total_Sales` representing the total sales across all regions for each product. 4. **normalize_sales(df: pd.DataFrame) -> pd.DataFrame**: Normalize the sales data (excluding the `Total_Sales` column) such that the maximum sales value for each region is 1, and other values are scaled proportionally. 5. **calculate_difference(df: pd.DataFrame) -> pd.DataFrame**: Create a new DataFrame representing the difference between the sales of each region and the central region for each product. 6. **region_product_alignment(df: pd.DataFrame) -> pd.DataFrame**: Align sales data such that the DataFrame indexes and columns are uniformly consistent even if there is missing data, filling missing values with NaN. Constraints - You may import only the numpy and pandas libraries. Example Usage ```python sales_data = { \\"Product_A\\": {\\"Region_West\\": 150, \\"Region_East\\": 200, \\"Region_Central\\": 100}, \\"Product_B\\": {\\"Region_West\\": 300, \\"Region_East\\": 100, \\"Region_Central\\": 150}, \\"Product_C\\": {\\"Region_West\\": 100, \\"Region_East\\": 250, \\"Region_Central\\": 200}, } additional_data = pd.DataFrame({ \\"Product\\": [\\"Product_D\\", \\"Product_E\\"], \\"Region_West\\": [130, 170], \\"Region_East\\": [280, 220], \\"Region_Central\\": [150, 190] }) df_sales = create_sales_dataframe(sales_data) df_sales = add_additional_data(df_sales, additional_data) df_sales = calculate_total_sales(df_sales) df_sales_normalized = normalize_sales(df_sales) df_diff = calculate_difference(df_sales) df_aligned = region_product_alignment(df_sales) ``` Make sure your solution handles edge cases such as missing regions in the sales dictionary and the additional data.","solution":"import pandas as pd def create_sales_dataframe(sales_data: dict) -> pd.DataFrame: Create a DataFrame from the sales dictionary. df = pd.DataFrame(sales_data).T return df def add_additional_data(df: pd.DataFrame, additional_data: pd.DataFrame) -> pd.DataFrame: Append additional sales data to the initial DataFrame. df_additional = additional_data.set_index(\'Product\') df_combined = pd.concat([df, df_additional], axis=0) return df_combined def calculate_total_sales(df: pd.DataFrame) -> pd.DataFrame: Add a new column named `Total_Sales` representing the total sales across all regions for each product. df[\'Total_Sales\'] = df.sum(axis=1) return df def normalize_sales(df: pd.DataFrame) -> pd.DataFrame: Normalize the sales data (excluding the `Total_Sales` column) such that the maximum sales value for each region is 1, and other values are scaled proportionally. df_normalized = df.drop(columns=\'Total_Sales\').div(df.drop(columns=\'Total_Sales\').max()) df_normalized[\'Total_Sales\'] = df[\'Total_Sales\'] return df_normalized def calculate_difference(df: pd.DataFrame) -> pd.DataFrame: Create a new DataFrame representing the difference between the sales of each region and the central region for each product. regions = df.columns[:-1] df_difference = df.loc[:, regions].subtract(df[\'Region_Central\'], axis=0) return df_difference def region_product_alignment(df: pd.DataFrame) -> pd.DataFrame: Align sales data such that the DataFrame indexes and columns are uniformly consistent even if there is missing data, filling missing values with NaN. df_aligned = df.reindex(columns=df.columns, index=df.index) return df_aligned"},{"question":"Objective Your task is to implement a function that analyzes a given text string to extract important information and reformat it accordingly. This will demonstrate your understanding of regular expressions and string manipulation. Problem Statement You are provided with a list of journal entries. Each entry follows a specific format containing a date, a category, and the message. Your goal is to write a function `analyze_entries` that processes these entries and returns the results in a summarized and reformatted string. Each entry in the list follows the format: `\\"YYYY-MM-DD - CATEGORY - MESSAGE\\"`. Multiple entries can share the same date and category. Your function must: 1. Count the number of entries for each date. 2. Identify and list the unique categories for each date. 3. Reformat the output as specified. Input - A list of strings, where each string represents a journal entry in the format `\\"YYYY-MM-DD - CATEGORY - MESSAGE\\"`. Output - A multi-line string summarizing the journal entries. Each line in the output should follow the format: ``` Date: YYYY-MM-DD Number of Entries: <number_of_entries> Categories: <category1>, <category2>,... ``` Constraints - Each entry will be in the correct format as specified. - Dates follow the format `YYYY-MM-DD`. - Assume all inputs will be valid and non-empty. - Categories are single words without spaces, and the message can contain any characters except newline. Example For the input list: ```python entries = [ \\"2023-10-01 - Work - Completed the report.\\", \\"2023-10-01 - Personal - Went to the gym.\\", \\"2023-10-02 - Work - Prepared presentation.\\", \\"2023-10-02 - Personal - Dinner with friends.\\", \\"2023-10-02 - Exercise - Morning jog.\\", ] ``` The function call `analyze_entries(entries)` should return: ``` Date: 2023-10-01 Number of Entries: 2 Categories: Work, Personal Date: 2023-10-02 Number of Entries: 3 Categories: Work, Personal, Exercise ``` Implementation Write the function `analyze_entries(entries: List[str]) -> str` to complete this task. Make sure to use appropriate string methods and regular expressions to process and analyze the entries.","solution":"def analyze_entries(entries): from collections import defaultdict # Dictionary to store the parsed data summary = defaultdict(lambda: {\'count\': 0, \'categories\': set()}) for entry in entries: # Split the entry into its components date, category, message = entry.split(\' - \', 2) # Update the summary summary[date][\'count\'] += 1 summary[date][\'categories\'].add(category) # Prepare the summarized string result_lines = [] for date in sorted(summary.keys()): result_lines.append(f\\"Date: {date}\\") result_lines.append(f\\"Number of Entries: {summary[date][\'count\']}\\") result_lines.append(f\\"Categories: {\', \'.join(sorted(summary[date][\'categories\']))}\\") return \\"n\\".join(result_lines)"},{"question":"**Objective:** Write a Python function that organizes a given directory by moving all files into subdirectories based on their file extensions. This function should also handle any duplicates by appending a number to the duplicate file\'s name. **Requirements:** 1. The function should accept a single argument: the path of the directory to be organized. 2. For each file in the specified directory: * Create a subdirectory named after the file\'s extension (e.g., `.txt`, `.jpg`) if it doesn\'t already exist. * Move the file into the corresponding subdirectory. * If a file with the same name already exists in the target subdirectory, append a number to the filename (before the extension) to prevent overwriting. 3. Return a dictionary where the keys are the original filenames and the values are the new paths of the files after reorganization. **Input:** - A string representing the path to the directory to be organized. **Output:** - A dictionary mapping original filenames to their new paths. **Constraints:** - The input directory will only contain files, no nested subdirectories. - Extension-based subdirectories should be created in the given directory. - Assume the file names contain only ASCII characters. **Example:** ```python def organize_directory(directory_path: str) -> dict: pass ``` For a directory containing the files: `doc1.txt`, `image1.jpg`, `doc2.txt`, `doc1.txt` The function should return: ```python { \\"doc1.txt\\": \\"path/to/directory/txt/doc1_1.txt\\", \\"image1.jpg\\": \\"path/to/directory/jpg/image1.jpg\\", \\"doc2.txt\\": \\"path/to/directory/txt/doc2.txt\\", \\"doc1.txt\\": \\"path/to/directory/txt/doc1.txt\\" } ``` **Notes:** - Make sure your implementation handles cases where multiple files have the same name gracefully. - Use the `os`, `shutil`, and `pathlib` modules to achieve the required functionality.","solution":"import os import shutil from pathlib import Path def organize_directory(directory_path: str) -> dict: Organizes the specified directory by moving files into subdirectories based on their file extensions. Handles duplicates by appending a number to the filename. Args: - directory_path (str): Path of the directory to organize. Returns: - dict: Mapping of original filenames to their new paths. directory = Path(directory_path) result = {} for item in directory.iterdir(): if item.is_file(): ext = item.suffix[1:] ext_directory = directory / ext if not ext_directory.exists(): ext_directory.mkdir() new_file_name = item.name new_file_path = ext_directory / new_file_name counter = 1 while new_file_path.exists(): new_file_name = f\\"{item.stem}_{counter}{item.suffix}\\" new_file_path = ext_directory / new_file_name counter += 1 shutil.move(str(item), new_file_path) result[item.name] = str(new_file_path) return result"},{"question":"Objective Your task is to write a Python function that performs the following steps: 1. Loads a given dataset. 2. Trains a specified machine learning model. 3. Generates and plots the validation curve for a given hyperparameter. 4. Generates and plots the learning curve for the model. Input Your function should take the following inputs: - `dataset_function`: A function that returns the features (X) and labels (y) of the dataset when called. - `model`: The machine learning model to be trained (an object following the scikit-learn estimator API). - `param_name`: The name of the hyperparameter to be varied for the validation curve. - `param_range`: A list or array of values for the hyperparameter to be varied. - `train_sizes`: A list of training sample sizes for the learning curve. - `cv`: The number of folds for cross-validation. Output Your function should display two plots: 1. Validation curve: Plot training and validation scores against the values of the hyperparameter. 2. Learning curve: Plot training and validation scores against the sizes of the training data. Constraints - You should use the functions `validation_curve` and `learning_curve` from `sklearn.model_selection`. - You should use the classes `ValidationCurveDisplay` and `LearningCurveDisplay` from `sklearn.model_selection` for plotting. - Your function should handle any dataset and model that fit the scikit-learn API. Function Signature ```python def plot_model_diagnostics(dataset_function, model, param_name, param_range, train_sizes, cv): pass ``` Example ```python from sklearn.datasets import load_iris from sklearn.svm import SVC def load_iris_data(): X, y = load_iris(return_X_y=True) return X, y model = SVC(kernel=\\"linear\\") param_name = \\"C\\" param_range = [0.01, 0.1, 1, 10, 100] train_sizes = [0.1, 0.25, 0.5, 0.75, 1.0] cv = 5 plot_model_diagnostics(load_iris_data, model, param_name, param_range, train_sizes, cv) ``` This function, when implemented correctly, will display two plots: one showing the validation curve for the hyperparameter `C` of the SVM model and another showing the learning curve for the model.","solution":"import matplotlib.pyplot as plt from sklearn.model_selection import validation_curve, learning_curve from sklearn.model_selection import ValidationCurveDisplay, LearningCurveDisplay def plot_model_diagnostics(dataset_function, model, param_name, param_range, train_sizes, cv): Loads a dataset, trains a machine learning model, generates and plots validation and learning curves. Args: dataset_function: function returning features (X) and labels (y) of the dataset model: a scikit-learn estimator object param_name: str, name of the hyperparameter to vary for the validation curve param_range: list or array, values of the hyperparameter to vary train_sizes: list, sizes of the training samples for the learning curve cv: int, number of folds for cross-validation Returns: Displays two plots: validation curve and learning curve # Load dataset X, y = dataset_function() # Generate validation curve train_scores, valid_scores = validation_curve( model, X, y, param_name=param_name, param_range=param_range, cv=cv) # Plot validation curve plt.figure() ValidationCurveDisplay.from_estimator( model, X, y, param_name=param_name, param_range=param_range, cv=cv) plt.title(\\"Validation Curve\\") plt.xlabel(param_name) plt.ylabel(\\"Score\\") plt.show() # Generate learning curve train_sizes, train_scores, valid_scores = learning_curve( model, X, y, train_sizes=train_sizes, cv=cv) # Plot learning curve plt.figure() LearningCurveDisplay.from_estimator( model, X, y, train_sizes=train_sizes, cv=cv) plt.title(\\"Learning Curve\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.show()"},{"question":"Custom and Extended PyTorch Operations **Objective:** This task assesses your ability to create and extend custom operations in PyTorch using the `torch.library` APIs. **Problem Statement:** You are required to implement a custom PyTorch operation that squares the input tensor and then extend this operation to add a custom gradient calculation for the operation. **Requirements:** 1. **Custom Operation**: - Use the `torch.library.custom_op` to create a new custom operation called `square_op`. - The operation should take a single tensor as input and return a tensor with each element squared. 2. **Register Kernel**: - Implement the forward pass kernel for the `square_op` using the `torch.library.register_kernel`. 3. **Custom Gradient**: - Extend the `square_op` to support custom gradient calculation. - Use the `torch.library.register_autograd` to define the gradient of the `square_op`, which should be `2 * x` for each element `x` in the input tensor. **Constraints:** - The custom operation should handle tensors of any shape. - Ensure that gradients are computed correctly to support backpropagation in a neural network training setting. **Example:** ```python import torch from torch import tensor import torch.library as library # Define and register the custom operation MY_NAMESPACE = library.Library(\\"my_namespace\\", \\"DEF\\") @library.custom_op(MY_NAMESPACE) def square_op(x): # TODO: Implement the operation pass # Register the forward pass @library.register_kernel(\\"my_namespace::square_op\\") def square_kernel(x): # TODO: Forward pass implementation pass # Register the gradient (backward pass) @library.register_autograd(\\"my_namespace::square_op\\") def square_grad(x): # TODO: Gradient calculation pass # Example usage x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) y = square_op(x) y.backward(torch.ones_like(x)) print(x.grad) # Should output tensor([2.0, 4.0, 6.0]) ``` In your solution, fill in the missing parts of the code to achieve the described custom operation and gradient registration. **Expected Input and Output:** - **Input**: A tensor of any shape. - **Output**: A tensor of the same shape with each element squared. **Performance Requirements:** - The solution should handle tens of thousands of elements efficiently. - Gradients for backpropagation should be computed correctly.","solution":"import torch from torch.autograd import Function class Square(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input ** 2 @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * 2 * input return grad_input def square_op(input): Perform square operation with custom gradient calculation. Args: input (torch.Tensor): Input tensor. Returns: torch.Tensor: Squared tensor. return Square.apply(input)"},{"question":"**Objective:** Use the Seaborn library to create a diverging color palette and apply it to visualize relationships in a real-world dataset. This question assesses students\' understanding of generating and customizing diverging palettes using Seaborn, alongside integrating them into data visualizations meaningfully. **Question:** You are given a dataset of car performance metrics (`mpg`, `horsepower`, and `weight`) for various car models. Your task is to generate a heatmap using a diverging color palette from Seaborn to visualize the correlations among these metrics. **Input:** Your provided dataset will be a CSV file named `car_performance.csv` with the following columns: - `mpg`: Miles per gallon - `horsepower`: Horsepower - `weight`: Vehicle weight Your task is two-fold: 1. Generate a diverging color palette using Seaborn\'s `diverging_palette` function. 2. Create a heatmap that uses this palette to display the correlation matrix of the given dataset. **Constraints:** - The diverging palette should go from blue to red through white, and it should be continuous. - The center color should be dark, and the separation around the center should be medium. - The saturation of endpoints should be decreased to 50, and the lightness of endpoints should also be decreased. **Expected Output:** A plot displaying the heatmap of correlations among `mpg`, `horsepower`, and `weight` using the specified diverging palette. **Performance Requirements:** - Your solution should efficiently read the dataset, calculate the correlation matrix, configure the diverging palette, and render the heatmap all within reasonable runtime and memory constraints for a standard dataset size (e.g., up to 1,000 rows). **Function Signature:** ```python def create_diverging_heatmap(csv_file: str): # your code here ``` **Example:** ```python # Sample `car_performance.csv` file content mpg,horsepower,weight 18,130,3504 15,165,3693 16,150,3436 17,140,3449 ... # Example usage create_diverging_heatmap(\'car_performance.csv\') ``` *Note: The function should display the heatmap directly and not return any values.*","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_diverging_heatmap(csv_file: str): # Read the dataset df = pd.read_csv(csv_file) # Calculate the correlation matrix corr_matrix = df.corr() # Generate a diverging palette palette = sns.diverging_palette(240, 10, n=9, s=50, l=50, as_cmap=True) # Create the heatmap sns.heatmap(corr_matrix, annot=True, cmap=palette, center=0) # Display the plot plt.show()"},{"question":"**Objective**: To assess your understanding of TorchScript, including type annotations, type safety, and scripting a PyTorch model for efficient execution. **Problem Statement**: You are tasked with creating a TorchScript-compatible neural network module that performs a simple computation. Your module should use TorchScript-specific features including type annotations and scripting. # Requirements 1. **Module Definition**: - Define a class `SimpleNet` that inherits from `torch.nn.Module`. - The module should have an initialization method `__init__` that sets up one linear layer (fully connected layer). 2. **Forward Method**: - Implement the `forward` method to define the computation. This method should: - Take an input tensor `x` of type `torch.Tensor`. - Pass the input through the linear layer. - Return the result. 3. **Type Annotations**: - Annotate all the parameters and return types in your methods, including `__init__` and `forward`. - Use `torch.jit.Attribute` for attributes within the `__init__` method to provide type hints to TorchScript. 4. **Scripting the Model**: - After defining the class, use `torch.jit.script` to script an instance of the `SimpleNet` model. # Constraints - The input tensor `x` will always be a 2D tensor (batch size x features). - The linear layer should map input features to an output of 10 units. - Ensure that your code is TorchScript compatible, and use type annotations where required. # Performance Requirements - The implementation should be efficient in terms of computation and memory usage. - Scripted models should be optimized for execution on GPU if available. # Input and Output Format **Input**: - An instance of the `SimpleNet` class. - A 2D input tensor `x` of type `torch.Tensor`. **Output**: - A 2D output tensor of type `torch.Tensor` generated by the `forward` method. # Example ```python import torch import torch.nn as nn from typing import Any, Tuple class SimpleNet(nn.Module): def __init__(self, input_features: int): super(SimpleNet, self).__init__() self.fc = torch.jit.Attribute(nn.Linear(input_features, 10), nn.Linear) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.fc(x) # Creating an instance of the model model = SimpleNet(5) # Scripting the model scripted_model = torch.jit.script(model) # Testing with a sample input input_tensor = torch.randn(3, 5) # Batch size of 3, 5 features output_tensor = scripted_model(input_tensor) print(output_tensor) ``` **Note**: Ensure that your `SimpleNet` class and other relevant parts of your code are TorchScript compatible and follow the given constraints and requirements.","solution":"import torch import torch.nn as nn from typing import Any class SimpleNet(nn.Module): def __init__(self, input_features: int) -> None: super(SimpleNet, self).__init__() self.fc = nn.Linear(input_features, 10) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.fc(x) # Creating an instance of the model model = SimpleNet(5) # Scripting the model scripted_model = torch.jit.script(model)"},{"question":"# Question: Implementing Secure File Operations in Python Development Mode Objective: You are required to implement a Python function that processes a text file. Your function should handle file operations securely and should be robust against common pitfalls like unclosed resources or improper usage of file descriptors. The function must be compatible with Python Development Mode. Function Signature: ```python def process_file(file_path: str) -> int: Processes the given text file and returns the number of lines. :param file_path: Path to the text file. :return: Number of lines in the file. ``` Requirements: - Your function should open the file at the provided `file_path`, read its contents, and return the number of lines in the file. - You must ensure that the file is properly closed after reading, even if an error occurs during file operations. - Avoid any resource warnings or errors related to file handling by using best practices for resource management. - Your implementation should support running in Python Development Mode without emitting any warnings or errors. Constraints: - You are not allowed to use any external libraries; only standard Python libraries are permitted. - Assume the file at `file_path` exists and is readable. Example: ```python file_path = \'example.txt\' # Assume this file exists and contains text. print(process_file(file_path)) # Should print the number of lines in \'example.txt\'. ``` Additional Considerations: - Test your implementation in Python Development Mode to ensure it does not produce any warnings or errors related to resource management. - Include error handling to manage any unexpected issues that arise while reading the file.","solution":"def process_file(file_path: str) -> int: Processes the given text file and returns the number of lines. :param file_path: Path to the text file. :return: Number of lines in the file. try: with open(file_path, \'r\') as file: return sum(1 for _ in file) except Exception as e: print(f\\"An error occurred: {e}\\") return -1"}]'),z={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const r=this.searchQuery.trim().toLowerCase();return r?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(r)||e.solution&&e.solution.toLowerCase().includes(r)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(r=>setTimeout(r,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},q={class:"card-container"},R={key:0,class:"empty-state"},F=["disabled"],N={key:0},L={key:1};function O(r,e,l,m,n,i){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>n.searchQuery=o),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>n.searchQuery="")}," âœ• ")):d("",!0)]),t("div",q,[(a(!0),s(b,null,v(i.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),i.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),i.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...o)=>i.loadMore&&i.loadMore(...o))},[n.isLoading?(a(),s("span",L,"Loading...")):(a(),s("span",N,"See more"))],8,F)):d("",!0)])}const M=p(z,[["render",O],["__scopeId","data-v-2ea10c5c"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/50.md","filePath":"chatai/50.md"}'),j={name:"chatai/50.md"},X=Object.assign(j,{setup(r){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,X as default};
