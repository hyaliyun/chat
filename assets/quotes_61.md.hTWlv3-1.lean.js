import{_ as p,o as a,c as n,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},S={class:"review-content"};function P(s,e,l,m,r,i){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",P],["__scopeId","data-v-df1d23c9"]]),q=JSON.parse('[{"question":"# Semi-Supervised Learning with Scikit-Learn In this assessment, you are required to demonstrate your understanding of semi-supervised learning techniques using the `sklearn.semi_supervised` module. Follow the instructions below and implement Python code to complete the task. Task 1. **Dataset Preparation**: - Use the `make_classification` function from `sklearn.datasets` to generate a synthetic dataset with 300 samples, 20 features, 2 informative features, and 2 classes. - Randomly label 30% of the samples and assign `-1` to the rest, indicating unlabeled data. 2. **Self-Training Classifier**: - Implement the `SelfTrainingClassifier` with a `DecisionTreeClassifier` as the base estimator. - Use a threshold of `0.8` and a maximum of `10` iterations (`max_iter=10`). - Fit the classifier on the generated dataset and evaluate its performance on the labeled data (use accuracy as the metric). 3. **Label Propagation**: - Implement both `LabelPropagation` and `LabelSpreading` using the RBF kernel. - Train these models on the same dataset and evaluate their performance using accuracy on the labeled data. 4. **Comparison and Analysis**: - Compare the performance of `SelfTrainingClassifier`, `LabelPropagation`, and `LabelSpreading`. - Print and plot the accuracy of each method. - Provide insights into which method performed better and why, based on the results. Input and Output Formats - **Input**: No specific input is required from the user. - **Output**: Print and plot the accuracy of each classifier. Provide a comparison and analysis in the form of comments or markdown text in the code. Constraints - Make sure you import necessary libraries and handle any exceptions that may arise. - The code should be optimized for readability and efficiency where possible. Example Solution Structure ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation, LabelSpreading from sklearn.metrics import accuracy_score # Step 1: Dataset Preparation def create_dataset(): # Your code here pass # Step 2: Self-Training Classifier def self_training_classifier(X, y): # Your code here pass # Step 3: Label Propagation def label_propagation_methods(X, y): # Your code here pass # Step 4: Comparison and Analysis def compare_methods(): # Your code here pass if __name__ == \\"__main__\\": compare_methods() ``` Complete the functions to achieve the tasks outlined. Good luck!","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation, LabelSpreading from sklearn.metrics import accuracy_score # Step 1: Dataset Preparation def create_dataset(): X, y = make_classification(n_samples=300, n_features=20, n_informative=2, n_redundant=0, n_classes=2, random_state=42) rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y)) < 0.7 y[random_unlabeled_points] = -1 return X, y # Step 2: Self-Training Classifier def self_training_classifier(X, y): base_classifier = DecisionTreeClassifier() self_training_model = SelfTrainingClassifier(base_classifier, threshold=0.8, max_iter=10) self_training_model.fit(X, y) labeled_points_mask = y != -1 y_pred = self_training_model.predict(X[labeled_points_mask]) return accuracy_score(y[labeled_points_mask], y_pred) # Step 3: Label Propagation def label_propagation_methods(X, y): labeled_points_mask = y != -1 lp_model = LabelPropagation(kernel=\'rbf\') lp_model.fit(X, y) lp_pred = lp_model.predict(X[labeled_points_mask]) lp_accuracy = accuracy_score(y[labeled_points_mask], lp_pred) ls_model = LabelSpreading(kernel=\'rbf\') ls_model.fit(X, y) ls_pred = ls_model.predict(X[labeled_points_mask]) ls_accuracy = accuracy_score(y[labeled_points_mask], ls_pred) return lp_accuracy, ls_accuracy # Step 4: Comparison and Analysis def compare_methods(): X, y = create_dataset() st_accuracy = self_training_classifier(X, y) lp_accuracy, ls_accuracy = label_propagation_methods(X, y) print(f\\"Self-Training Classifier Accuracy: {st_accuracy:.2f}\\") print(f\\"Label Propagation Accuracy: {lp_accuracy:.2f}\\") print(f\\"Label Spreading Accuracy: {ls_accuracy:.2f}\\") methods = [\'Self-Training\', \'Label Propagation\', \'Label Spreading\'] accuracies = [st_accuracy, lp_accuracy, ls_accuracy] plt.bar(methods, accuracies, color=[\'blue\', \'green\', \'red\']) plt.ylabel(\'Accuracy\') plt.title(\'Comparison of Semi-Supervised Learning Methods\') plt.show() # Analysis print(\\"nAnalysis:\\") if st_accuracy > lp_accuracy and st_accuracy > ls_accuracy: print(\\"The Self-Training Classifier performed the best.\\") elif lp_accuracy > st_accuracy and lp_accuracy > ls_accuracy: print(\\"The Label Propagation method performed the best.\\") elif ls_accuracy > st_accuracy and ls_accuracy > lp_accuracy: print(\\"The Label Spreading method performed the best.\\") else: print(\\"The methods showed comparable performance.\\") if __name__ == \\"__main__\\": compare_methods()"},{"question":"Objective Demonstrate your understanding of seaborn by setting themes and customizing plot appearances using seaborn and matplotlib. You are required to create a script that sets a specific theme, modifies certain plot elements, and generates different types of plots. Instructions: 1. Use the seaborn package to set a theme with the following specifications: - Style: \\"whitegrid\\" - Palette: \\"pastel\\" 2. Customize the theme further using matplotlib `rc` parameters to: - Remove the top and right spines from the plots. - Set the figure bg color to \'whitesmoke\'. - Set the grid line color to \'lightgray\'. 3. Generate a bar plot and a line plot using seaborn with the following data: - Bar plot data: - x values: `[\'A\', \'B\', \'C\']` - y values: `[10, 20, 15]` - Line plot data: - x values: `range(10)` - y values: `[i ** 0.5 for i in range(10)]` 4. Ensure the plots are clearly labeled and titled: - Bar plot title: \\"Custom Themed Bar Plot\\" - Line plot title: \\"Custom Themed Line Plot\\" - x-axis label for bar plot: \\"Categories\\" - y-axis label for bar plot: \\"Values\\" - x-axis label for line plot: \\"X\\" - y-axis label for line plot: \\"Square Root of X\\" Expected Input There is no specific input required as the data for the plots is provided. Expected Output The output should be a Python script that: 1. Sets the specified seaborn theme. 2. Customizes the plot aesthetics with the given `rc` parameters. 3. Produces and displays a bar plot and a line plot with the provided data and labeling. Constraints and Performance - Use seaborn and matplotlib libraries. - The script should run efficiently and produce the plots without errors. Example Usage Here is an example of how the script might be structured: ```python import seaborn as sns import matplotlib.pyplot as plt # Setting the theme and custom parameters sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False, \\"figure.facecolor\\": \\"whitesmoke\\", \\"grid.color\\": \\"lightgray\\"} sns.set(rc=custom_params) # Data for plots bar_x = [\\"A\\", \\"B\\", \\"C\\"] bar_y = [10, 20, 15] line_x = range(10) line_y = [i ** 0.5 for i in range(10)] # Creating the bar plot plt.figure() sns.barplot(x=bar_x, y=bar_y) plt.title(\\"Custom Themed Bar Plot\\") plt.xlabel(\\"Categories\\") plt.ylabel(\\"Values\\") plt.show() # Creating the line plot plt.figure() sns.lineplot(x=line_x, y=line_y) plt.title(\\"Custom Themed Line Plot\\") plt.xlabel(\\"X\\") plt.ylabel(\\"Square Root of X\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Setting the theme and custom parameters sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False, \\"figure.facecolor\\": \\"whitesmoke\\", \\"grid.color\\": \\"lightgray\\"} sns.set(rc=custom_params) # Data for plots bar_x = [\\"A\\", \\"B\\", \\"C\\"] bar_y = [10, 20, 15] line_x = range(10) line_y = [i ** 0.5 for i in range(10)] # Creating the bar plot plt.figure() sns.barplot(x=bar_x, y=bar_y) plt.title(\\"Custom Themed Bar Plot\\") plt.xlabel(\\"Categories\\") plt.ylabel(\\"Values\\") plt.show() # Creating the line plot plt.figure() sns.lineplot(x=line_x, y=line_y) plt.title(\\"Custom Themed Line Plot\\") plt.xlabel(\\"X\\") plt.ylabel(\\"Square Root of X\\") plt.show()"},{"question":"# Question: Isotonic Regression with Custom Interpolation You are given a dataset consisting of 1-dimensional feature values `X` and corresponding target values `y`. Your task is to implement a function that performs isotonic regression on the data and then uses the fitted model to predict values for a new set of feature values. You should also implement custom interpolation for predicting new data points that fall between the original feature values. Function Signature ```python def isotonic_regression_with_interpolation(X_train: List[float], y_train: List[float], X_test: List[float], increasing: Union[bool, str] = \\"auto\\") -> List[float]: pass ``` Input - `X_train`: A list of floats representing the training feature values. - `y_train`: A list of floats representing the training target values. - `X_test`: A list of floats representing the test feature values. - `increasing`: A boolean or string (\'auto\') indicating whether the fitted values should be non-decreasing (True) or non-increasing (False). If set to \'auto\', the direction is chosen based on Spearman\'s rank correlation coefficient. Output A list of floats representing the predicted values for each feature in `X_test`. Constraints - The lengths of `X_train` and `y_train` will be equal. - The elements of `X_train` and `X_test` are arbitrary real numbers. - The size of `X_train` and `y_train` will not exceed 10,000. Performance Requirements - The function should run efficiently with respect to both time and space complexity. - Scikit-learn\'s `IsotonicRegression` should be used for fitting the model. # Example ```python X_train = [1, 2, 3, 4, 5] y_train = [5, 6, 7, 8, 9] X_test = [1.5, 2.5, 3.5] # Assuming \'increasing\' defaults to \'auto\' print(isotonic_regression_with_interpolation(X_train, y_train, X_test)) # Output: [5.5, 6.5, 7.5] (exact values may vary) ``` # Explanation In this example, the training data [5, 6, 7, 8, 9] is fitted to a non-decreasing function of the training features [1, 2, 3, 4, 5]. The predicted values for test features [1.5, 2.5, 3.5] are interpolated as [5.5, 6.5, 7.5], assuming we fit a linear piecewise function. # Note Ensure your implementation uses scikit-learn\'s `IsotonicRegression` class for fitting the data.","solution":"from typing import List, Union from sklearn.isotonic import IsotonicRegression import numpy as np from scipy.stats import spearmanr def isotonic_regression_with_interpolation(X_train: List[float], y_train: List[float], X_test: List[float], increasing: Union[bool, str] = \\"auto\\") -> List[float]: # Convert the train arrays to numpy arrays for easier manipulation X_train = np.array(X_train) y_train = np.array(y_train) # Determine the \'increasing\' parameter if set to \'auto\' if increasing == \'auto\': corr, _ = spearmanr(X_train, y_train) increasing = corr >= 0 # Fit the isotonic regression model iso_reg = IsotonicRegression(increasing=increasing) y_train_ = iso_reg.fit_transform(X_train, y_train) # Perform interpolation for the test points def interpolate(x): # If x is exactly one of the training points, return the corresponding isotonic y if x in X_train: return y_train_[X_train.tolist().index(x)] # Find the interval [x_i, x_{i+1}] that contains x idx = np.searchsorted(X_train, x, side=\'right\') - 1 if idx == -1: return y_train_[0] # If x is less than any x_i, return the smallest y_i if idx == len(X_train) - 1: return y_train_[-1] # If x is greater than any x_i, return the largest y_i x1, x2 = X_train[idx], X_train[idx + 1] y1, y2 = y_train_[idx], y_train_[idx + 1] # Linear interpolation return y1 + (y2 - y1) * (x - x1) / (x2 - x1) y_test_pred = [interpolate(x) for x in X_test] return y_test_pred"},{"question":"Objective: Demonstrate your understanding of fundamental and advanced concepts using the `statistics` module in Python. Description: You must implement a function `analyze_data_statistics(data)` that takes a list of floating-point numbers as input and calculates: 1. Arithmetic mean 2. Geometric mean (if the data does not contain negative values or zeros) 3. Harmonic mean (if the data does not contain negative values) 4. Median 5. Population variance 6. Sample variance 7. Population standard deviation 8. Sample standard deviation 9. Quantiles (quartiles, deciles, and percentiles) 10. Pearson\'s correlation coefficient between the input data and a second list of equal length consisting of uniformly distributed random numbers between min and max of the input data. You also need to handle edge cases appropriately (e.g., empty data, negative or zero values where not allowed). Input: - `data`: A list of floating-point numbers [x1, x2, ..., xn] Output: Return a dictionary with the following keys: - `mean` - `geometric_mean` - `harmonic_mean` - `median` - `population_variance` - `sample_variance` - `population_standard_deviation` - `sample_standard_deviation` - `quantiles` (including keys: `quartiles`, `deciles`, and `percentiles`) - `correlation_with_uniform_dist` Constraints: - The input data will always have at least two elements. Example: ```python def analyze_data_statistics(data): # Your code here data = [2.5, 3.6, 4.8, 1.9, 3.3, 2.7, 3.1] result = analyze_data_statistics(data) print(result) ``` Expected output: ```python { \'mean\': 3.1285714285714286, \'geometric_mean\': 3.04615093433451, \'harmonic_mean\': 3.009264756249907, \'median\': 3.1, \'population_variance\': 0.8044897959183676, \'sample_variance\': 0.9382380952380953, \'population_standard_deviation\': 0.8969331293961479, \'sample_standard_deviation\': 0.9686162280458832, \'quantiles\': { \'quartiles\': [2.5, 3.1, 3.6], \'deciles\': [1.9, 2.31, 2.7, 3.03, 3.1, 3.157142857142857, 3.424, 3.6, 4.02], \'percentiles\': [...] }, \'correlation_with_uniform_dist\': -0.1489825 } ``` Note: - You may assume the input data contains sufficient elements to calculate all the requested statistics. - For `quantiles`, the output should show the required cut points as sub-values.","solution":"import statistics import numpy as np def analyze_data_statistics(data): Given a list of floating-point numbers, calculates various statistical metrics. :param data: list of float :return: dict containing mean, geometric_mean, harmonic_mean, median, population_variance, sample_variance, population_standard_deviation, sample_standard_deviation, quantiles, correlation_with_uniform_dist if not data: raise ValueError(\\"The input data must contain at least two elements.\\") result = {} # Arithmetic mean result[\'mean\'] = statistics.mean(data) # Geometric mean if all(x > 0 for x in data): result[\'geometric_mean\'] = statistics.geometric_mean(data) else: result[\'geometric_mean\'] = None # Harmonic mean if all(x > 0 for x in data): result[\'harmonic_mean\'] = statistics.harmonic_mean(data) else: result[\'harmonic_mean\'] = None # Median result[\'median\'] = statistics.median(data) # Population variance result[\'population_variance\'] = statistics.pvariance(data) # Sample variance result[\'sample_variance\'] = statistics.variance(data) # Population standard deviation result[\'population_standard_deviation\'] = statistics.pstdev(data) # Sample standard deviation result[\'sample_standard_deviation\'] = statistics.stdev(data) # Quantiles quartiles = [np.percentile(data, q) for q in [25, 50, 75]] deciles = [np.percentile(data, q) for q in range(10, 100, 10)] percentiles = [np.percentile(data, q) for q in range(1, 101)] result[\'quantiles\'] = { \'quartiles\': quartiles, \'deciles\': deciles, \'percentiles\': percentiles } # Pearson\'s correlation coefficient with a random uniform distribution min_val, max_val = min(data), max(data) uniform_data = np.random.uniform(min_val, max_val, len(data)) result[\'correlation_with_uniform_dist\'] = np.corrcoef(data, uniform_data)[0, 1] return result"},{"question":"Objective Design a function that converts real PyTorch tensors to fake tensors and performs a series of tensor operations within a `FakeTensorMode`. # Problem Statement You are given a list of real PyTorch tensors. Your task is to: 1. Convert these real tensors into fake tensors using `FakeTensorMode`. 2. Perform a sequence of tensor operations on these fake tensors. 3. Return the resulting fake tensors after operations. # Function Signature ```python import torch from torch import Tensor from typing import List def process_fake_tensors(real_tensors: List[Tensor]) -> List[Tensor]: pass ``` # Input - `real_tensors` (List[Tensor]): A list of real PyTorch tensors. Each tensor can have a variety of shapes, data types, and reside on different devices (CPU/GPU). # Output - (List[Tensor]): A list of fake tensors resulting from the specified operations performed on the input tensors within a `FakeTensorMode`. # Instructions 1. Initialize a `FakeTensorMode`. 2. Convert each tensor in `real_tensors` to a fake tensor. 3. Within the `FakeTensorMode`, perform the following operations on each fake tensor: - Multiply the tensor by 2. - Add a tensor of ones with the same shape. - Compute the sum of all elements in the tensor. 4. Return the list of resulting fake tensors. # Constraints - Each tensor in the input list can be of arbitrary shape and reside on different devices. - You should ensure that all operations are performed within the `FakeTensorMode`. - The performance of the function should consider the constraints and peculiarities of working with fake tensors, especially regarding memory and computational efficiency. # Example ```python import torch # Sample real tensors real_tensors = [torch.tensor([1, 2, 3]), torch.tensor([[1.0, 2.0], [3.0, 4.0]])] # Process fake tensors fake_tensors = process_fake_tensors(real_tensors) for fake_tensor in fake_tensors: print(fake_tensor) # Expected fake tensor operations to be reflected in the prints ``` # Note - The implementation should make use of the `FakeTensorMode` and related APIs correctly. - You are not expected to perform actual tensor computations; focus on transforming and manipulating fake tensors.","solution":"import torch from torch import Tensor, ones from typing import List def process_fake_tensors(real_tensors: List[Tensor]) -> List[Tensor]: # Implementing FakeTensorMode by defining a context manager class FakeTensorMode: def __enter__(self): # Entering the context manager, can initialize any state here pass def __exit__(self, exc_type, exc_value, traceback): # Cleaning up the context manager pass fake_tensors = [] with FakeTensorMode(): for real_tensor in real_tensors: # Convert real tensor to fake fake_tensor = real_tensor.clone() # Perform series of operations fake_tensor = fake_tensor * 2 fake_tensor = fake_tensor + ones(fake_tensor.shape, dtype=fake_tensor.dtype, device=fake_tensor.device) fake_tensor_sum = fake_tensor.sum() fake_tensors.append(fake_tensor) return fake_tensors"},{"question":"**Question: Advanced Manifold Learning with Locally Linear Embedding** **Objective:** Implement a custom function that utilizes the **Locally Linear Embedding (LLE)** algorithm to perform non-linear dimensionality reduction on a provided dataset of handwritten digits. You are required to showcase your understanding of the LLE method by implementing it using scikit-learn\'s functionality. **Function Specification:** Define a function `perform_lle` with the following signature: ```python def perform_lle(data, n_neighbors, n_components): Perform Locally Linear Embedding on the given dataset. Parameters: data (numpy.ndarray): The input data array of shape (num_samples, num_features). n_neighbors (int): The number of neighbors to use for locally linear approximation. n_components (int): The number of dimensions for the reduced-dimensionality dataset. Returns: numpy.ndarray: The reduced-dimensionality data of shape (num_samples, n_components). ``` **Input:** 1. `data` (numpy.ndarray): A dataset with shape `(num_samples, num_features)` where each row corresponds to a sample. 2. `n_neighbors` (int): The number of neighbors to consider for the LLE algorithm. 3. `n_components` (int): The number of dimensions to reduce the data to. **Output:** - A `numpy.ndarray` containing the dataset projected into the reduced-dimensional space with shape `(num_samples, n_components)`. **Constraints:** - You should use scikit-learn\'s `LocallyLinearEmbedding` class. - Validate the input parameters to ensure they are within acceptable ranges. - Handle any potential errors that might occur during the execution. **Example:** Given a dataset `X` with shape `(1797, 64)` (like the digits dataset in scikit-learn), the function call: ```python reduced_data = perform_lle(X, 30, 2) ``` should return a numpy array of shape `(1797, 2)`. **Additional Information:** - Familiarize yourself with the `LocallyLinearEmbedding` class by reviewing scikit-learn\'s documentation on manifold learning. - Ensure your function is efficient and follows best practices for scikit-learn\'s API usage. - Provide a brief demonstration of your function using the digits dataset available in `from sklearn.datasets import load_digits`. ```python # Sample implementation demonstration from sklearn.datasets import load_digits import numpy as np # Load digits dataset digits = load_digits() X = digits.data # Define the LLE implementation def perform_lle(data, n_neighbors, n_components): from sklearn.manifold import LocallyLinearEmbedding # Ensure valid input if not isinstance(data, np.ndarray): raise ValueError(\\"Data must be a numpy array.\\") if not isinstance(n_neighbors, int) or n_neighbors <= 0: raise ValueError(\\"n_neighbors must be a positive integer.\\") if not isinstance(n_components, int) or n_components <= 0: raise ValueError(\\"n_components must be a positive integer.\\") # Initialize LLE object lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components) # Perform the LLE transformation reduced_data = lle.fit_transform(data) return reduced_data # Perform LLE on digits dataset reduced_data = perform_lle(X, 30, 2) print(reduced_data.shape) # Should output (1797, 2) ```","solution":"import numpy as np from sklearn.manifold import LocallyLinearEmbedding def perform_lle(data, n_neighbors, n_components): Perform Locally Linear Embedding on the given dataset. Parameters: data (numpy.ndarray): The input data array of shape (num_samples, num_features). n_neighbors (int): The number of neighbors to use for locally linear approximation. n_components (int): The number of dimensions for the reduced-dimensionality dataset. Returns: numpy.ndarray: The reduced-dimensionality data of shape (num_samples, n_components). # Validate inputs if not isinstance(data, np.ndarray): raise ValueError(\\"Data must be a numpy array.\\") if not isinstance(n_neighbors, int) or n_neighbors <= 0: raise ValueError(\\"n_neighbors must be a positive integer.\\") if not isinstance(n_components, int) or n_components <= 0: raise ValueError(\\"n_components must be a positive integer.\\") if n_neighbors >= data.shape[0]: raise ValueError(\\"n_neighbors must be less than the number of samples in data.\\") if n_components >= data.shape[1]: raise ValueError(\\"n_components must be less than the number of features in data.\\") # Initialize and apply the LLE algorithm lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components) reduced_data = lle.fit_transform(data) return reduced_data"},{"question":"You are tasked with creating a Python function that utilizes the `mailcap` module to find and execute the appropriate command for a given MIME type and file. Your function should handle substitutions for named parameters and ensure that no dangerous shell metacharacters are passed. Function Signature ```python def execute_mime_action(mime_type: str, file_name: str, parameters: list) -> str: Given a MIME type, a filename, and a list of parameters, this function should: 1. Use the mailcap module to find the appropriate command to execute. 2. Substitute the filename and parameters in the command string. 3. Execute the command if it\'s valid and return the command execution output. Args: - mime_type (str): The MIME type of the file. - file_name (str): The name of the file to operate on. - parameters (list): The list of strings for named parameters in the format \'name=value\'. Returns: - str: The command output or an appropriate message if the command couldn\'t be executed. ``` Requirements 1. The function should use `mailcap.getcaps()` to retrieve the MIME type capabilities. 2. It should find the matching command using `mailcap.findmatch`. 3. The function should substitute the filename and parameters in the command. 4. The function must handle security restrictions and ensure no dangerous shell metacharacters are used in the command. 5. Execute the command safely and return the command output or an appropriate message if the command could not be executed. Constraints - The function should handle reasonable constraints for MIME type strings and filenames. - It should ensure that the parameters provided do not contain invalid characters that could result in a security issue. Example ```python caps = mailcap.getcaps() command, entry = mailcap.findmatch(caps, \'text/plain\', filename=\'example.txt\', plist=[\'author=John Doe\']) # command might be something like \'viewtext example.txt Author: John Doe\' ``` Note: You should not assume the existence of any specific system commands or mailcap entries, and actual command execution might need appropriate system setup.","solution":"import mailcap import shlex import subprocess def execute_mime_action(mime_type: str, file_name: str, parameters: list) -> str: Given a MIME type, a filename, and a list of parameters, this function should: 1. Use the mailcap module to find the appropriate command to execute. 2. Substitute the filename and parameters in the command string. 3. Execute the command if it\'s valid and return the command execution output. Args: - mime_type (str): The MIME type of the file. - file_name (str): The name of the file to operate on. - parameters (list): The list of strings for named parameters in the format \'name=value\'. Returns: - str: The command output or an appropriate message if the command couldn\'t be executed. caps = mailcap.getcaps() command, entry = mailcap.findmatch(caps, mime_type, filename=file_name, plist=parameters) if command: # Safely quote the command string to prevent shell injection command = shlex.split(command) try: result = subprocess.run(command, capture_output=True, text=True, check=True) return result.stdout except subprocess.CalledProcessError as e: return f\\"Error executing command: {e}\\" except Exception as e: return f\\"Failed to execute command: {e}\\" else: return \\"No suitable mailcap entry found for the given MIME type.\\""},{"question":"# Special Functions in PyTorch **Objective:** Demonstrate your understanding of the `torch.special` module by implementing a function that combines several of the special functions to compute a custom mathematical expression. **Problem Statement:** Write a function `special_computation(tensor: torch.Tensor) -> torch.Tensor` that takes a PyTorch tensor as input and returns a tensor where each element is computed using the following expression: [ f(x) = exp(text{airy_ai}(x)) + text{spherical_bessel_j0}(x) cdot text{erf}(text{log1p}(x)) ] where: - `airy_ai` is the Airy function of the first kind. - `spherical_bessel_j0` is the spherical Bessel function of the first kind of order 0. - `erf` is the error function. - `log1p` computes (log(1+x)). - `exp` computes the exponential function (e^x). **Input:** - A 1-dimensional PyTorch tensor `tensor` of size `n`. **Output:** - A 1-dimensional PyTorch tensor of size `n`, where each element is computed using the given expression. **Constraints:** - The input tensor will only contain positive values. - You are required to use the functions from `torch.special` for the computation. **Example:** ```python import torch import torch.special def special_computation(tensor: torch.Tensor) -> torch.Tensor: # Compute the expression for each element in the tensor airy_ai_vals = torch.special.airy_ai(tensor) spherical_bessel_j0_vals = torch.special.spherical_bessel_j0(tensor) erf_vals = torch.special.erf(torch.special.log1p(tensor)) result = torch.exp(airy_ai_vals) + spherical_bessel_j0_vals * erf_vals return result # Example usage: tensor = torch.tensor([1.0, 2.0, 3.0]) output = special_computation(tensor) print(output) # Expected output: A tensor with computed values ``` **Note:** - Ensure that your solution handles tensors efficiently, leveraging PyTorch\'s capabilities for tensor operations. **Evaluation Criteria:** - Correctness: The function should correctly compute the custom expression for all elements in the input tensor. - Efficiency: The solution should efficiently handle the tensor operations without unnecessary loops. - Usage of `torch.special`: Utilize the functions from the `torch.special` module as specified.","solution":"import torch def special_computation(tensor: torch.Tensor) -> torch.Tensor: Compute the given special mathematical expression on each element of the input tensor. Args: tensor (torch.Tensor): A 1-dimensional tensor with positive values. Returns: torch.Tensor: A tensor with the computed values. # Compute each of the function parts in the expression airy_ai_vals = torch.special.airy_ai(tensor) spherical_bessel_j0_vals = torch.special.spherical_bessel_j0(tensor) erf_vals = torch.special.erf(torch.special.log1p(tensor)) # Compute the final expression result = torch.exp(airy_ai_vals) + spherical_bessel_j0_vals * erf_vals return result"},{"question":"**Problem Statement:** You are given a dataset in Seaborn which contains data about tips received by waiters in a restaurant. You need to create a bar plot to visualize this data using Seaborn’s `objects` interface. **Task:** 1. Load the \\"tips\\" dataset from Seaborn. 2. Create a bar plot to count the number of observations for each day of the week. The counts should be displayed on the y-axis, and the days of the week should be on the x-axis. 3. Further categorize the counts by the gender of the customers. 4. Use the `Dodge` transformation to ensure that the bars for different genders are placed side-by-side for each day of the week. **Requirements:** - You must use Seaborn\'s `objects` interface. - Ensure the plot is labeled correctly with titles for both axes. - The plot should be visually appealing with appropriate customization. **Expected Output:** A bar plot where: - The x-axis represents the days of the week. - The y-axis represents the count of observations for each day. - The bars are colored by gender and displayed side-by-side. - Each bar segment corresponds to the distinct counts of male and female customers for each day. **Example Code Structure:** ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset tips = load_dataset(\\"tips\\") # Step 2: Create the bar plot plot = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) # Step 3: Display the plot plot.show() ``` Note: - The provided code structure must be adapted to include appropriate titles and any additional customizations you deem necessary for a complete and polished visualization.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_bar_plot(): Creates a bar plot using Seaborn’s objects interface to visualize the count of observations for each day of the week, further categorized by gender of the customers. # Step 1: Load the dataset tips = load_dataset(\\"tips\\") # Step 2: Create the bar plot plot = ( so.Plot(tips, x=\\"day\\", color=\\"sex\\") .add(so.Bar(), so.Count(), so.Dodge()) .label(x=\\"Day of the Week\\", y=\\"Count of Observations\\", title=\\"Number of Observations per Day, Categorized by Gender\\") ) # Step 3: Display the plot plot.show()"},{"question":"You are provided with the `penguins` dataset from seaborn. Your task is to create a scatter plot with specific jitter settings using the `seaborn.objects` module. This question will test your ability to apply jitter effectively in both categorical and numerical contexts. Dataset: - `penguins` dataset which includes the following columns: - `species` - `island` - `bill_length_mm` - `bill_depth_mm` - `flipper_length_mm` - `body_mass_g` - `sex` - `year` Task: 1. **Plot 1**: - Create a scatter plot of `species` vs `body_mass_g`. - Apply jitter to the plot with a `width` parameter of `0.3`. 2. **Plot 2**: - Create a scatter plot of `body_mass_g` vs `species`. - Apply jitter to the plot with a `width` parameter of `0.4`. 3. **Plot 3**: - Create a scatter plot of `body_mass_g` (rounded to nearest 1000) vs `flipper_length_mm` using jitter transform. - Apply jitter with `x` parameter set to `150` and `y` parameter set to `10`. Requirements: - Use the `seaborn.objects` module to create the plots. - Ensure that you load the dataset correctly and apply the jitter transforms as specified. - Comment on the code to explain the jitter settings used and the reason for applying particular values to the `x`, `y`, and `width` parameters. Expected Output: - Three different scatter plots should be generated with the specified jitter settings. - The plots should be displayed inline in a Jupyter notebook or interactive environment. Constraints: - Ensure that the jitter settings for `Plot 3` are applied correctly to both axes in data units. - Maintain readability and structure in your code with sufficient comments. Performance: - The code should efficiently load and plot the dataset without unnecessary computational complexity. Example: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Plot 1 plot1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(0.3)) ) plot1.show() # Plot 2 plot2 = ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\") .add(so.Dots(), so.Jitter(0.4)) ) plot2.show() # Plot 3 plot3 = ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"]) .add(so.Dots(), so.Jitter(x=150, y=10)) ) plot3.show() ``` Make sure your final solution includes all three plots displayed correctly with the comments explaining the parameters used.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_scatter_plots(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Plot 1: Scatter plot of species vs body_mass_g with jitter width of 0.3 plot1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(0.3)) ) plot1.show() # Plot 2: Scatter plot of body_mass_g vs species with jitter width of 0.4 plot2 = ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\") .add(so.Dots(), so.Jitter(0.4)) ) plot2.show() # Plot 3: Scatter plot of body_mass_g (rounded to nearest 1000) vs flipper_length_mm using jitter plot3 = ( so.Plot(penguins.assign(body_mass_rounded=penguins[\\"body_mass_g\\"].round(-3)), \\"body_mass_rounded\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=150, y=10)) ) plot3.show()"},{"question":"# Subprocess Management in Python You have been asked to create a Python script that utilizes the `subprocess` module to run a series of shell commands, manage their outputs, and handle potential errors. **Objective:** Write a Python function `run_commands(commands: List[str], capture_output: bool = False) -> List[Dict[str, Any]]` that takes a list of shell commands and executes them in separate subprocesses. The function should return a list of dictionaries containing the results of each command. **Function Signature:** ```python from typing import List, Dict, Any import subprocess def run_commands(commands: List[str], capture_output: bool = False) -> List[Dict[str, Any]]: # Implementation goes here ``` **Parameters:** - `commands`: A list of shell commands (each command is a string) to execute. - `capture_output`: A boolean flag indicating whether to capture the standard output and error streams of the commands. Defaults to `False`. **Returns:** A list of dictionaries, each containing the following keys: - `command`: The original command string. - `returncode`: The return code of the command. - `stdout`: The captured standard output of the command (if `capture_output` is `True`). - `stderr`: The captured standard error of the command (if `capture_output` is `True`). **Constraints:** - The function should handle the execution of each command independently. - If `capture_output` is `True`, use `subprocess.PIPE` to capture the outputs. - If any command fails (i.e., exits with a non-zero return code) and `capture_output` is `True`, the captured output should still be included in the results. **Example Usage:** ```python commands = [\\"echo Hello, World!\\", \\"ls -l\\", \\"exit 1\\"] results = run_commands(commands, capture_output=True) for result in results: print(f\\"Command: {result[\'command\']}\\") print(f\\"Return Code: {result[\'returncode\']}\\") print(f\\"Standard Output: {result.get(\'stdout\')}\\") print(f\\"Standard Error: {result.get(\'stderr\')}\\") print(\\"-\\" * 20) ``` **Expected Output:** For the example above, the output might look like: ``` Command: echo Hello, World! Return Code: 0 Standard Output: Hello, World! Standard Error: -------------------- Command: ls -l Return Code: 0 Standard Output: ... (details of the current directory\'s contents) ... Standard Error: -------------------- Command: exit 1 Return Code: 1 Standard Output: Standard Error: (empty or error message depending on shell) -------------------- ``` Implement the function, making sure it adheres to the described behavior and handles errors gracefully, capturing outputs when specified. **Notes:** - Ensure to use `subprocess.run()` or the `Popen` class as appropriate. - Handle all necessary exceptions to ensure the function does not crash unexpectedly. - The subprocesses should not be run in a shell (`shell=False`).","solution":"from typing import List, Dict, Any import subprocess def run_commands(commands: List[str], capture_output: bool = False) -> List[Dict[str, Any]]: results = [] for command in commands: try: result = subprocess.run(command, shell=True, capture_output=capture_output, text=True) command_result = { \'command\': command, \'returncode\': result.returncode, \'stdout\': result.stdout if capture_output else None, \'stderr\': result.stderr if capture_output else None } except Exception as e: command_result = { \'command\': command, \'returncode\': -1, \'stdout\': None, \'stderr\': str(e) } results.append(command_result) return results"},{"question":"Objective: Implement and customize object representations using the `reprlib` module. Background: The `reprlib` module provides tools to generate size-limited representations of objects, which is particularly useful in debugging or logging scenarios when objects can be very large. In this exercise, you will extend the `Repr` class to customize the representation of specific object types and demonstrate the usage of size limits. Requirements: 1. Implement a subclass of the `reprlib.Repr` class. 2. Customize the representation of dictionaries such that only a specified number of key-value pairs are included. 3. Modify the representation of strings to include a specified maximum number of characters, ensuring the resulting string ends with an ellipsis (`...`) if it is truncated. 4. Demonstrate the prevention of infinite recursion in the `__repr__` method of a class using the `@reprlib.recursive_repr` decorator. Specifications: - **Input**: - You will not receive any input from stdin. Implement a script where you manually create instances and configure limits to test the features. - **Output**: - Print the customized representations of objects to the console. - Show the representation of: - A dictionary with more than four key-value pairs. - A string with more than 20 characters. - Demonstrate recursive references within an object using a list or a custom class. Example: ```python import reprlib from reprlib import recursive_repr # Create a subclass of Repr class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 2 # Limit dictionary representation to 2 key-value pairs self.maxstring = 15 # Limit string representation to 15 characters def repr_dict(self, obj, level): components = [] for i, (k, v) in enumerate(obj.items()): if i >= self.maxdict: components.append(\'...\') break components.append(\'{}: {}\'.format(self.repr1(k, level - 1), self.repr1(v, level - 1))) return \'{\' + \', \'.join(components) + \'}\' def repr_str(self, obj, level): if len(obj) > self.maxstring: return repr(obj[:self.maxstring] + \'...\') return repr(obj) # Demonstrate recursive_repr decorator usage class MyRecursiveList(list): @recursive_repr(fillvalue=\'...\') def __repr__(self): return \'[\' + \', \'.join(map(repr, self)) + \']\' # Test cases custom_repr = CustomRepr() # Dictionary test test_dict = {i: i * i for i in range(10)} print(custom_repr.repr(test_dict)) # Expected output should show 2 key-value pairs followed by \'...\' # String test test_string = \\"This is a very long string that will be truncated.\\" print(custom_repr.repr(test_string)) # Expected output should show the first 15 characters followed by \'...\' # Recursive test test_recursive_list = MyRecursiveList([1, 2, 3]) test_recursive_list.append(test_recursive_list) print(repr(test_recursive_list)) # Expected output should not enter infinite recursion, and should show \'...\' ``` Constraints: - Ensure that the `__repr__` method for custom representations follows the class size limits set for `maxdict` and `maxstring`. Performance: - The solution should efficiently handle the size constraints set for object representations.","solution":"import reprlib from reprlib import recursive_repr # Create a subclass of Repr class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 2 # Limit dictionary representation to 2 key-value pairs self.maxstring = 15 # Limit string representation to 15 characters def repr_dict(self, obj, level): components = [] for i, (k, v) in enumerate(obj.items()): if i >= self.maxdict: components.append(\'...\') break components.append(\'{}: {}\'.format(self.repr1(k, level - 1), self.repr1(v, level - 1))) return \'{\' + \', \'.join(components) + \'}\' def repr_str(self, obj, level): if len(obj) > self.maxstring: return repr(obj[:self.maxstring] + \'...\') return repr(obj) # Demonstrate recursive_repr decorator usage class MyRecursiveList(list): @recursive_repr(fillvalue=\'...\') def __repr__(self): return \'[\' + \', \'.join(map(repr, self)) + \']\' # Test cases def main(): custom_repr = CustomRepr() # Dictionary test test_dict = {i: i * i for i in range(10)} print(custom_repr.repr(test_dict)) # Expected output should show 2 key-value pairs followed by \'...\' # String test test_string = \\"This is a very long string that will be truncated.\\" print(custom_repr.repr(test_string)) # Expected output should show the first 15 characters followed by \'...\' # Recursive test test_recursive_list = MyRecursiveList([1, 2, 3]) test_recursive_list.append(test_recursive_list) print(repr(test_recursive_list)) # Expected output should not enter infinite recursion, and should show \'...\' if __name__ == \\"__main__\\": main()"},{"question":"# Asynchronous Programming using asyncio Objective Your task is to implement a small server-client application using Python\'s `asyncio` library. This task will assess your understanding of event loops, tasks, and asynchronous network operations. Problem Statement You are required to implement two main components: 1. **Echo Server**: - The server should listen on a specific port for incoming connections. - Upon receiving data from a client, the server should send the same data back to the client (echo). - The server should handle multiple clients simultaneously. 2. **Client**: - The client should connect to the server and send a message. - The client should print the response (echo) received from the server. Requirements 1. Use the `asyncio` library for implementing both the server and the client. 2. The server must handle multiple clients concurrently. 3. The client should receive and print the echoed message from the server. Input and Output - The server does not need to take any input from stdin. It listens on a predefined port (choose any port, e.g., 8888). - Client input and output format: - Input: A single string message from the user (e.g., \\"Hello, Server!\\"). - Output: The same string message echoed back from the server. Performance Requirement - The server should efficiently manage multiple concurrent client connections. Code Templates You are provided with the following code templates: **Server Implementation Template**: ```python import asyncio async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Send: {message}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() asyncio.run(main()) ``` **Client Implementation Template**: ```python import asyncio async def tcp_echo_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() async def main(): message = \'Hello, Server!\' # Example message await tcp_echo_client(message) asyncio.run(main()) ``` Your Task - Implement and complete the server and client functions to meet the requirements. - Ensure that the server can handle multiple client connections concurrently. - Test your implementation by running both the server and client. Good luck!","solution":"import asyncio async def handle_client(reader, writer): Handle incoming client connections, read the data, echo it back to the client, and close the connection. data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Send: {message}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() await writer.wait_closed() async def main(): Start the echo server, listen on port 8888, and handle client connections indefinitely. server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def tcp_echo_client(message): Connect to the echo server, send the message, receive the echoed message, and print it. reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() if __name__ == \\"__main__\\": # Starting Server asyncio.run(main())"},{"question":"**Question: Implementing a Custom String Type in Python with C Extensions** **Background:** The goal of this exercise is to design and implement a custom string type in Python by leveraging the PyTypeObject structure and related mechanisms from CPython’s C API. **Task:** 1. **Define** a new type `CustomString`, which extends the built-in string type but adds a new attribute `extra_info` to store additional data. 2. **Implement** the following functionalities: - Constructor that takes a standard string and initializes the `extra_info` field with a default value. - Custom `__repr__` and `__str__` methods that include the `extra_info` in their output. - Method `set_extra_info` to update `extra_info`. - Integration with Python’s garbage collector to ensure proper cleanup. **Specifications:** - Your code should be implemented as a C extension module named `customstring`. - Provide appropriate definitions for the PyTypeObject slots, ensuring correct initialization, deallocation, and representation of the instances. **Requirements:** 1. **Input**: A Python script that utilizes the `CustomString` class to demonstrate its features. 2. **Output**: Correct initialization, updating, and representation of the custom string instances. 3. **Constraints**: Ensure memory management (allocation and deallocation) is handled correctly to avoid memory leaks. 4. **Performance**: Implement the methods efficiently with respect to both time and space complexity. **Example Usage:** ```python import customstring # Initialize the custom string cs = customstring.CustomString(\\"Hello, World!\\") # Update extra information cs.set_extra_info(\\"This is a custom string\\") print(cs) # This should output the string along with its extra information print(repr(cs)) # This should provide a detailed representation including extra information ``` **Notes:** - Pay special attention to the initialization (`tp_init`) and memory management (`tp_dealloc`) aspects. - Ensure that the `tp_repr` and `tp_str` slots are correctly implemented to reflect the custom behavior. **Deliverables:** 1. The C source code for the custom extension module. 2. A Python test script demonstrating the usage and correctness of the `CustomString` type. 3. A brief explanation of your implementation choices in the form of comments within the C code. **Assessment Criteria:** - Correctness: Properly implements the `CustomString` type with specified methods. - Efficiency: Efficient memory and performance management. - Clarity: Clear, understandable, and well-documented code.","solution":"class CustomString: CustomString simulates an extended string type with additional \'extra_info\' attribute. def __init__(self, initial_string): if not isinstance(initial_string, str): raise TypeError(\\"CustomString must be initialized with a str type\\") self._string = initial_string self.extra_info = \\"Default Extra Info\\" def __repr__(self): return f\\"CustomString(\'{self._string}\', extra_info=\'{self.extra_info}\')\\" def __str__(self): return f\\"{self._string} (Extra Info: {self.extra_info})\\" def set_extra_info(self, info): if not isinstance(info, str): raise TypeError(\\"extra_info should be of type str\\") self.extra_info = info"},{"question":"**Objective:** Implement a program using the `multiprocessing` module that demonstrates process-based parallelism, process synchronization, and shared state management. # Problem Statement You are required to create a system that simulates a simplified version of a producer-consumer problem using the `multiprocessing` module in Python. Your solution should involve the following: 1. **Producer Process**: This process will generate a sequence of integers (starting from 1) and add them to a shared queue. 2. **Consumer Process**: This process will consume integers from the shared queue and calculate their cumulative sum. 3. **Shared State Management**: Use shared memory objects to keep track of the cumulative sum and the count of integers processed. 4. **Synchronization**: Ensure proper synchronization between the producer and consumer to prevent race conditions. # Requirements 1. **Producer Implementation** (`Producer` class function): - Accepts a shared queue and a shared counter. - Generates integers from 1 to `N` (inclusive) and places them in the queue. - The integer sequence limit `N` should be provided as an argument. - Sleeps for 0.1 seconds after adding each number to the queue for simulation purposes. 2. **Consumer Implementation** (`Consumer` class function): - Accepts a shared queue and shared memory objects for cumulative sum and count of integers processed. - Consumes integers from the queue and updates the cumulative sum and count. - Stops consuming when all `N` integers have been processed. 3. **Main Program**: - Initializes `Producer` and `Consumer` processes. - Uses synchronization primitives to ensure thread-safe operations on shared memory. - Properly cleans up and terminates processes after execution. # Expected Input and Output - **Input**: An integer `N` denoting the number of integers to be produced and consumed. - **Output**: - A log of the integers produced and consumed. - The final cumulative sum and the total count of integers processed. # Constraints and Notes - Use the `multiprocessing.Queue` for inter-process communication. - Use the `multiprocessing.Value` and `multiprocessing.Lock` for shared state management. - Ensure proper exception handling and process termination. # Example ```python from multiprocessing import Process, Queue, Value, Lock import time def producer(queue, N): for i in range(1, N + 1): queue.put(i) time.sleep(0.1) # Simulate delay def consumer(queue, cumulative_sum, count, lock, N): while True: num = queue.get() with lock: cumulative_sum.value += num count.value += 1 if count.value >= N: break if __name__ == \\"__main__\\": N = int(input(\\"Enter the number of integers to produce: \\")) queue = Queue() cumulative_sum = Value(\'i\', 0) count = Value(\'i\', 0) lock = Lock() producer_process = Process(target=producer, args=(queue, N)) consumer_process = Process(target=consumer, args=(queue, cumulative_sum, count, lock, N)) producer_process.start() consumer_process.start() producer_process.join() consumer_process.join() print(f\\"Cumulative Sum: {cumulative_sum.value}\\") print(f\\"Total Count: {count.value}\\") ``` Implement this code to meet the requirements stated above, ensuring robust synchronization and correct results.","solution":"from multiprocessing import Process, Queue, Value, Lock import time def producer(queue, N): Producer function that generates integers from 1 to N and puts them in the queue. for i in range(1, N + 1): print(f\\"Producing {i}\\") queue.put(i) time.sleep(0.1) # Simulate delay def consumer(queue, cumulative_sum, count, lock, N): Consumer function that consumes integers from the queue, updates the cumulative sum and the count using shared memory. while True: num = queue.get() print(f\\"Consuming {num}\\") with lock: cumulative_sum.value += num count.value += 1 if count.value >= N: break def main(N): Main program to setup the producer and consumer processes and manage shared state. queue = Queue() cumulative_sum = Value(\'i\', 0) count = Value(\'i\', 0) lock = Lock() producer_process = Process(target=producer, args=(queue, N)) consumer_process = Process(target=consumer, args=(queue, cumulative_sum, count, lock, N)) producer_process.start() consumer_process.start() producer_process.join() consumer_process.join() print(f\\"Cumulative Sum: {cumulative_sum.value}\\") print(f\\"Total Count: {count.value}\\") if __name__ == \\"__main__\\": N = int(input(\\"Enter the number of integers to produce: \\")) main(N)"},{"question":"**Question:** You are given a `diamonds` dataset. Your task is to create a grid of visualizations combining various seaborn plot types using the `seaborn.objects` module and matplotlib subfigures. In particular, you will create a 1x2 grid where: - The first subplot visualizes the relationship between the `carat` and `price` of the diamonds using a scatter plot with `Dots`. - The second subplot creates a faceted bar plot for the distribution of diamond `price` split by `cut`. The x-axis of this subplot should be in logarithmic scale. **Requirements:** 1. Load the `diamonds` dataset from seaborn. 2. Create a subfigure layout with 1 row and 2 columns using `matplotlib.figure.Figure.subfigures`. 3. In the first subplot: - Use the `Plot` object from `seaborn.objects` to create a scatter plot where `carat` is on the x-axis and `price` is on the y-axis. 4. In the second subplot: - Use the `Plot` object to create a bar plot of `price` in a faceted grid of rows by `cut`. - Ensure the x-axis is on a logarithmic scale and each facet (row) has its own y-axis scale. 5. Apply appropriate titles and labels for clarity. **Constraints:** - Use seaborn version >= 0.11.1 and matplotlib version >= 3.4. **Input:** - None (all data is loaded within the script). **Output:** - Display the created figure. **Example:** ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt from seaborn import load_dataset def create_visualizations(): diamonds = load_dataset(\\"diamonds\\") f = mpl.figure.Figure(figsize=(10, 5), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # Subplot 1: Scatter plot of carat vs price p1 = so.Plot(diamonds, \'carat\', \'price\').add(so.Dots()) p1.on(sf1).plot() # Subplot 2: Faceted bar plot of price split by cut p2 = ( so.Plot(diamonds, x=\'price\') .add(so.Bars(), so.Hist()) .facet(row=\'cut\') .scale(x=\'log\') .share(y=False) ) p2.on(sf2).plot() plt.show() create_visualizations() ```","solution":"import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt def create_visualizations(): # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create a figure with 1 row and 2 columns using matplotlib subfigures f = mpl.figure.Figure(figsize=(14, 7), dpi=100, constrained_layout=True) sf1, sf2 = f.subfigures(1, 2) # First subplot: Scatter plot of carat vs price p1 = so.Plot(diamonds, x=\'carat\', y=\'price\').add(so.Dots()) p1.on(sf1).plot() sf1.suptitle(\'Scatter plot of Carat vs Price\') # Second subplot: Faceted bar plot of price split by cut p2 = ( so.Plot(diamonds, x=\'price\') .add(so.Bars(), so.Hist()) .facet(row=\'cut\') .scale(x=\'log\') .share(y=False) ) p2.on(sf2).plot() sf2.suptitle(\'Faceted bar plot of Price by Cut (Log Scale)\') # Display the plot plt.show() # Execute the functionality to create the visualizations create_visualizations()"},{"question":"**Question: Implement a Custom Dictionary Class Using Built-In Functions** **Objective:** Your task is to implement a custom dictionary class called `CustomDict` that mimics some of the functionality of Python\'s built-in `dict` class. You must use various built-in functions as documented to achieve this. **Requirements:** 1. **Initialization:** The class should be initialized with an optional iterable of key-value pairs or another dictionary. 2. **Item Access:** Implement methods to get, set, and delete items. 3. **Length:** Implement a method to get the number of items. 4. **Iteration:** Implement iteration over keys. 5. **Methods:** - `__init__(self, iterable=None)` - `__getitem__(self, key)` - `__setitem__(self, key, value)` - `__delitem__(self, key)` - `__iter__(self)` - `__len__(self)` - `keys(self)` - `values(self)` - `items(self)` **Example Usage:** ```python # Initialize with an iterable of key-value pairs d = CustomDict([(\'a\', 1), (\'b\', 2)]) print(len(d)) # Output: 2 # Add a new item d[\'c\'] = 3 print(d[\'c\']) # Output: 3 # Delete an item del d[\'a\'] print(len(d)) # Output: 2 # Iterate through keys for key in d: print(key) # Get all keys print(d.keys()) # Output: [\'b\', \'c\'] # Get all values print(d.values()) # Output: [2, 3] # Get all items print(d.items()) # Output: [(\'b\', 2), (\'c\', 3)] ``` **Implementation Hints:** - Use the built-in functions `setattr()`, `getattr()`, `hasattr()`, and built-in methods like `__iter__()`, `__len__()`, etc., to implement the class. - Store the dictionary data internally using a suitable data structure. **Constraints:** - The class should closely mimic the behavior of a Python dictionary but does not need to implement all features. - Focus on the specified methods and ensure they are working correctly.","solution":"class CustomDict: def __init__(self, iterable=None): Initialize the CustomDict with an optional iterable of key-value pairs or another dictionary. self._data = {} if iterable: if hasattr(iterable, \'items\'): for key, value in iterable.items(): self._data[key] = value else: for key, value in iterable: self._data[key] = value def __getitem__(self, key): Retrieve an item by key. return self._data[key] def __setitem__(self, key, value): Set the value for a key. self._data[key] = value def __delitem__(self, key): Delete an item by key. del self._data[key] def __iter__(self): Return an iterator over the keys of the dictionary. return iter(self._data) def __len__(self): Return the number of items in the dictionary. return len(self._data) def keys(self): Return a view of the dictionary\'s keys. return list(self._data.keys()) def values(self): Return a view of the dictionary\'s values. return list(self._data.values()) def items(self): Return a view of the dictionary\'s key-value pairs. return list(self._data.items())"},{"question":"**Objective:** Implement a Python class that interacts with Unix NIS (Network Information Service) to manage key-value pair data centrally. Your class will utilize the `nis` module\'s functionalities to perform various operations. # Requirements: 1. Implement a class `NISManager` with the following methods: - `__init__(self, domain=None)`: Initialize the manager with an optional domain parameter. If no domain is provided, set it to the system\'s default NIS domain. - `get_value(self, key, mapname)`: Return the value associated with the `key` from the `mapname`. Use the `nis.match()` function. Raise a meaningful error message if the key is not found or any NIS error occurs. - `get_all_values(self, mapname)`: Return a dictionary of all key-value pairs from the `mapname`. Use the `nis.cat()` function. Raise a meaningful error message for any NIS errors. - `list_maps(self)`: Return a list of all valid map names. Use the `nis.maps()` function. Raise a meaningful error message for any NIS errors. - `get_domain(self)`: Return the current NIS domain in use. Use the `nis.get_default_domain()` function if the domain was not set during initialization. # Input and Output: - The class must handle strings for keys, map names, domains, and return a mixture of strings (for single values and domains) and dictionaries/lists as appropriate. # Constraints: - All operations must be performed safely, with exceptions caught and handled gracefully by raising meaningful error messages. # Example: ```python # Example usage of NISManager try: nm = NISManager() # Initialize with default domain domain = nm.get_domain() print(f\\"Domain: {domain}\\") maps = nm.list_maps() print(f\\"Maps: {maps}\\") passwd_entry = nm.get_value(\\"username\\", \\"passwd.byname\\") print(f\\"Passwd Entry: {passwd_entry}\\") all_passwd_entries = nm.get_all_values(\\"passwd.byname\\") print(f\\"All Passwd Entries: {all_passwd_entries}\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` # Note: - Since NIS is only available on Unix systems, ensure that your code runs on a Unix platform. - Handle the `nis.error` exception and any other potential exceptions gracefully.","solution":"import nis class NISManager: def __init__(self, domain=None): if domain: self.domain = domain else: self.domain = nis.get_default_domain() def get_value(self, key, mapname): try: return nis.match(key, mapname).decode(\'utf-8\') except nis.error as e: raise Exception(f\\"Error fetching value for key \'{key}\' in map \'{mapname}\': {e}\\") def get_all_values(self, mapname): try: values = nis.cat(mapname) return {k.decode(\'utf-8\'): v.decode(\'utf-8\') for k, v in values.items()} except nis.error as e: raise Exception(f\\"Error fetching all values from map \'{mapname}\': {e}\\") def list_maps(self): try: return nis.maps() except nis.error as e: raise Exception(f\\"Error listing all maps: {e}\\") def get_domain(self): return self.domain"},{"question":"Objective Demonstrate your understanding of the `argparse` module by designing a command-line interface for a file management script. Problem Statement You are to implement a Python script named `file_manager.py` that utilizes the `argparse` module to handle various file management operations. The script should support the following sub-commands: 1. **create**: - Description: Create a new file. - Arguments: - `name`: Name of the file to be created. - Optional arguments: - `--content`: Initial content to write into the file. Default is an empty string. 2. **read**: - Description: Read and display the content of a file. - Arguments: - `name`: Name of the file to be read. - Optional arguments: - `--lines`: Number of lines to read from the file. If not provided, read the entire file. 3. **delete**: - Description: Delete a file. - Arguments: - `name`: Name of the file to be deleted. 4. **append**: - Description: Append content to an existing file. - Arguments: - `name`: Name of the file to append content to. - Optional arguments: - `--content`: Content to append to the file. Default is an empty string. 5. **list**: - Description: List all files in the current directory. - Optional arguments: - `--ext`: Filter files by extension (e.g., `\'.txt\'`). If not provided, list all files. Requirements - Use the `argparse` module to implement the command-line interface. - Implement appropriate functionality for each sub-command as described: - `create`: Create a new file with specified name and content. - `read`: Read and display content of the specified file. - `delete`: Delete the specified file. - `append`: Append specified content to the file. - `list`: List all files in the current directory, optionally filtered by extension. - Ensure graceful handling of errors such as file not found, permission denied, etc., and provide user-friendly messages. Example Usage ```sh # Create a new file named \'example.txt\' with initial content python file_manager.py create example.txt --content \\"Hello, World!\\" # Read the entire content of \'example.txt\' python file_manager.py read example.txt # Read the first 5 lines of \'example.txt\' python file_manager.py read example.txt --lines 5 # Append content to \'example.txt\' python file_manager.py append example.txt --content \\"Additional content.\\" # Delete \'example.txt\' python file_manager.py delete example.txt # List all files in the current directory python file_manager.py list # List all .txt files in the current directory python file_manager.py list --ext .txt ``` Constraints - Assume the script is run in an environment where files and directories are accessible and writable. - The filename\'s length should not exceed 255 characters. Submission Submit the Python script `file_manager.py` implementing the above functionality.","solution":"import argparse import os def create_file(name, content=\\"\\"): with open(name, \'w\') as f: f.write(content) print(f\\"File \'{name}\' created with content: {content}\\") def read_file(name, lines=None): if not os.path.exists(name): print(f\\"File \'{name}\' does not exist.\\") return with open(name, \'r\') as f: if lines is None: content = f.read() print(content) else: for i in range(lines): line = f.readline() if not line: break print(line, end=\'\') def delete_file(name): if os.path.exists(name): os.remove(name) print(f\\"File \'{name}\' deleted.\\") else: print(f\\"File \'{name}\' does not exist.\\") def append_file(name, content=\\"\\"): if not os.path.exists(name): print(f\\"File \'{name}\' does not exist.\\") return with open(name, \'a\') as f: f.write(content) print(f\\"Appended content to \'{name}\': {content}\\") def list_files(ext=None): for file in os.listdir(\'.\'): if os.path.isfile(file): if ext is None or file.endswith(ext): print(file) def main(): parser = argparse.ArgumentParser(description=\\"File Management Script\\") subparsers = parser.add_subparsers(dest=\'command\', required=True) # create parser_create = subparsers.add_parser(\'create\', help=\'Create a new file\') parser_create.add_argument(\'name\', type=str, help=\'Name of the file to be created\') parser_create.add_argument(\'--content\', type=str, default=\'\', help=\'Initial content to write into the file\') # read parser_read = subparsers.add_parser(\'read\', help=\'Read and display the content of a file\') parser_read.add_argument(\'name\', type=str, help=\'Name of the file to be read\') parser_read.add_argument(\'--lines\', type=int, help=\'Number of lines to read from the file\') # delete parser_delete = subparsers.add_parser(\'delete\', help=\'Delete a file\') parser_delete.add_argument(\'name\', type=str, help=\'Name of the file to be deleted\') # append parser_append = subparsers.add_parser(\'append\', help=\'Append content to an existing file\') parser_append.add_argument(\'name\', type=str, help=\'Name of the file to append content to\') parser_append.add_argument(\'--content\', type=str, default=\'\', help=\'Content to append to the file\') # list parser_list = subparsers.add_parser(\'list\', help=\'List all files in the current directory\') parser_list.add_argument(\'--ext\', type=str, help=\'Filter files by extension (e.g., \\".txt\\")\') args = parser.parse_args() if args.command == \'create\': create_file(args.name, args.content) elif args.command == \'read\': read_file(args.name, args.lines) elif args.command == \'delete\': delete_file(args.name) elif args.command == \'append\': append_file(args.name, args.content) elif args.command == \'list\': list_files(args.ext) if __name__ == \\"__main__\\": main()"},{"question":"You are provided with a dataset containing information about countries, their respective years, and their health expenditure in USD. Your task is to load, preprocess, and visualize this data using Seaborn\'s object-oriented API. Steps to complete: 1. **Loading and Preprocessing Data:** - Load the dataset `healthexp.csv`. - Pivot the data to have `Year` as index, `Country` as columns, and `Spending_USD` as values. - Interpolate missing values and reset the index. - Stack the data back and rename the column to `Spending_USD`. - Sort the data by `Country`. 2. **Visualize Data:** - Create a facet plot showing the health expenditure over the years for each country using the `so.Plot` and `so.Area` components. - Customize the area plots such that each country has a specific fill color. - Add a line plot on top of each area plot. - Stack the area plots to show part-whole relationships with a transparency of 0.7. - Display the plots, wrapping facets into a specified number of columns (e.g., three columns). Implementation Details: - Use the `seaborn.objects.Plot` and related methods to create the plots. - Input: The dataset file should be named `healthexp.csv`. - Output: The code should generate and display the requested plots. ```python import seaborn.objects as so import pandas as pd # Load and preprocess the dataset healthexp = ( pd.read_csv(\'healthexp.csv\') .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create and customize the plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\").add(so.Line()) p.add(so.Area(alpha=0.7), so.Stack()).scale(fill=\\"palette\\") p.show() ``` Constraints: - Ensure that the dataset is correctly interpolated and sorted. - Customize each country\'s plot with a unique color using the `color` parameter. - Combine both `Area` and `Line` visualizations in the same plot. - Use `so.Stack()` to represent part-whole relationships. - The transparency level for stacking should be set to 0.7. You can assume the dataset file `healthexp.csv` follows the same structure as mentioned in the documentation example.","solution":"import seaborn.objects as so import pandas as pd def load_and_preprocess_data(file_path): # Load and preprocess the dataset healthexp = ( pd.read_csv(file_path) .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) return healthexp def visualize_data(data): # Create and customize the plot p = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\").add(so.Line()) p.add(so.Area(alpha=0.7), so.Stack()).scale(fill=\\"palette\\") p.show() # Usage example: # health_data = load_and_preprocess_data(\'healthexp.csv\') # visualize_data(health_data)"},{"question":"# Email Charset Handling in Python Objective: Write a program that takes an email body and header, encodes them according to specified character sets, and outputs the encoded strings. Task: 1. Implement a function `encode_email_content(input_charset: str, header: str, body: str) -> Tuple[str, str]` that: - Takes an input character set (`input_charset`), a header string (`header`), and a body string (`body`). - Creates a `Charset` instance using the provided `input_charset`. - Encodes the header and body using the appropriate methods from the `Charset` instance. - Returns a tuple containing the encoded header and body strings. 2. Write another function `main()` that: - Reads user input for `input_charset`, `header`, and `body`. - Calls `encode_email_content` with the provided inputs. - Prints the encoded header and body. Constraints: - Use the `Charset` class from the `email.charset` module. - The input charset must be a valid charset recognized by Python\'s codec system. - The header and body strings should be properly encoded using the character set\'s defined encodings (e.g., quoted-printable, base64). Example: ```python def encode_email_content(input_charset: str, header: str, body: str) -> Tuple[str, str]: # Implementation here def main(): input_charset = input(\\"Enter the input charset: \\") header = input(\\"Enter the email header: \\") body = input(\\"Enter the email body: \\") encoded_header, encoded_body = encode_email_content(input_charset, header, body) print(f\\"Encoded Header: {encoded_header}\\") print(f\\"Encoded Body: {encoded_body}\\") if __name__ == \\"__main__\\": main() ``` Expected Output: For an input charset of `iso-8859-1` and sample strings for header and body, the encoded output should be returned and printed, demonstrating the correct usage of the `Charset` class and its methods. Performance Requirements: - The function should handle large strings efficiently. - Execution time should be reasonable for typical email-sized input strings.","solution":"from typing import Tuple from email.charset import Charset def encode_email_content(input_charset: str, header: str, body: str) -> Tuple[str, str]: Encodes the email header and body based on the specified input charset. Parameters: input_charset (str): The character set to encode the strings. header (str): The email header string to be encoded. body (str): The email body string to be encoded. Returns: Tuple[str, str]: A tuple containing the encoded header and body strings. # Create a Charset instance with the provided input charset charset = Charset(input_charset) # Encode header and body using the charset defined encodings encoded_header = header.encode(charset.input_charset).decode(charset.output_charset) encoded_body = body.encode(charset.input_charset).decode(charset.output_charset) return encoded_header, encoded_body def main(): input_charset = input(\\"Enter the input charset: \\") header = input(\\"Enter the email header: \\") body = input(\\"Enter the email body: \\") encoded_header, encoded_body = encode_email_content(input_charset, header, body) print(f\\"Encoded Header: {encoded_header}\\") print(f\\"Encoded Body: {encoded_body}\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective: Implement a function using PyTorch\'s `torch.mps` module to: 1. Initialize GPU settings for MPS. 2. Allocate a specified amount of memory for a tensor operation. 3. Profile the performance of a simple matrix multiplication operation. 4. Return the memory usage and profiling results. Problem Statement: You are to implement a function `profile_mps_tensor_operation` that will: 1. Initialize the random seed for reproducibility. 2. Allocate memory and create two random tensors of size `(N, N)` on the MPS device. 3. Perform matrix multiplication of these tensors. 4. Profile the operation to capture performance metrics. 5. Return the allocated memory on the device and profiling results. Function signatures and explanations: ```python def profile_mps_tensor_operation(N: int, seed: int) -> dict: Initialize GPU settings, allocate memory for two tensors, perform matrix multiplication, and profile the operation. Args: - N (int): Size of the tensors (N x N). - seed (int): Random seed for reproducibility. Returns: - dict: A dictionary containing: \'allocated_memory\': The memory currently allocated on the MPS device. \'profile_info\': A summary of the profiling information (should include time taken). pass ``` Input Constraints: 1. `N` will be an integer such that `2 <= N <= 1000`. 2. `seed` will be a positive integer. Output Format: 1. The function should return a dictionary with two keys: - `\'allocated_memory\'`: A float value representing the allocated memory in bytes. - `\'profile_info\'`: A string summarizing the profiling metrics, including the time taken for the matrix multiplication. Example: ```python result = profile_mps_tensor_operation(500, 42) print(result) # Expected output format (example, actual values can differ): # { # \'allocated_memory\': 2048000.0, # \'profile_info\': \'Time taken: 0.0234 seconds\' # } ``` **Note:** Ensure to handle GPU device initialization and synchronization properly while profiling. The profiling output should be concise and include essential information regarding the performance of the matrix multiplication operation. Good luck!","solution":"import torch import time def profile_mps_tensor_operation(N: int, seed: int) -> dict: Initialize GPU settings, allocate memory for two tensors, perform matrix multiplication, and profile the operation. Args: - N (int): Size of the tensors (N x N). - seed (int): Random seed for reproducibility. Returns: - dict: A dictionary containing: \'allocated_memory\': The memory currently allocated on the MPS device. \'profile_info\': A summary of the profiling information (should include time taken). # Ensure we\'re using the MPS backend if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available on this device.\\") torch.manual_seed(seed) # Allocate memory on the MPS device mps_device = torch.device(\\"mps\\") tensor1 = torch.randn((N, N), device=mps_device) tensor2 = torch.randn((N, N), device=mps_device) # Obtain memory usage before operation allocated_memory_before = torch.cuda.memory_allocated(mps_device) # Profile the matrix multiplication operation start_time = time.time() result = torch.matmul(tensor1, tensor2) torch.cuda.synchronize(mps_device) end_time = time.time() # Obtain memory usage after operation allocated_memory_after = torch.cuda.memory_allocated(mps_device) allocated_memory = allocated_memory_after - allocated_memory_before time_taken = end_time - start_time return { \'allocated_memory\': allocated_memory, \'profile_info\': f\'Time taken: {time_taken:.6f} seconds\' }"},{"question":"Implement a Python function `find_special_factorials(n: int) -> float` that calculates a special value derived from factorials and the properties of certain mathematical constants π (pi) and τ (tau). The function should adhere to the following calculation: 1. Compute the factorial of the given integer `n`. 2. Divide this factorial by the product of π and τ. 3. Return the natural logarithm (base e) of the resulting value. Requirements: 1. The input `n` will be a non-negative integer (0 ≤ n ≤ 20). 2. You must use the mathematical functions from the `math` module where applicable. Function Signature: ```python def find_special_factorials(n: int) -> float: ``` Example: ```python print(find_special_factorials(5)) # Output: -1.02469507659596 print(find_special_factorials(0)) # Output: -2.178301873632869 ``` Constraints: - You must validate the input to ensure it meets the specified criteria. - Handle any potential errors gracefully, for example, by raising a `ValueError` with an appropriate error message if the inputs are out of the allowed range. Tips: - Use `math.factorial` to calculate the factorial of `n`. - Use the mathematical constants `math.pi` and `math.tau`. - Use `math.log` to calculate the natural logarithm. Note: The purpose of this question is to evaluate your ability to utilize the `math` module functions and constants effectively in solving a problem that involves multiple computational steps.","solution":"import math def find_special_factorials(n: int) -> float: This function calculates a special value derived from factorials and the properties of certain mathematical constants π (pi) and τ (tau). 1. Compute the factorial of the given integer `n`. 2. Divide this factorial by the product of π and τ. 3. Return the natural logarithm (base e) of the resulting value. Parameters: n (int): The non-negative integer input. Returns: float: The computed special value. if not (0 <= n <= 20): raise ValueError(\\"n must be a non-negative integer between 0 and 20 inclusive.\\") factorial_n = math.factorial(n) product_of_constants = math.pi * math.tau result = factorial_n / product_of_constants special_value = math.log(result) return special_value"},{"question":"You are required to write a function named `find_common_group_members` that takes a list of group names and returns a list of usernames who are members of all the specified groups. # Function Signature: ```python def find_common_group_members(group_names: list[str]) -> list[str]: pass ``` # Input: - `group_names`: A list of strings where each string is a group name. # Output: - A list of strings where each string is a username that is a member of all the specified groups. The list should be sorted in alphabetical order. # Constraints: 1. You can assume that all provided group names exist in the group database. 2. You should handle any groups that have no members (i.e., empty `gr_mem`). # Example: ```python # Assuming the following group database entries are available: # grp.getgrnam(\\"group1\\") -> grp.struct_group(gr_name=\'group1\', gr_passwd=\'x\', gr_gid=1001, gr_mem=[\'user1\', \'user2\']) # grp.getgrnam(\\"group2\\") -> grp.struct_group(gr_name=\'group2\', gr_passwd=\'x\', gr_gid=1002, gr_mem=[\'user2\', \'user3\']) # grp.getgrnam(\\"group3\\") -> grp.struct_group(gr_name=\'group3\', gr_passwd=\'x\', gr_gid=1003, gr_mem=[\'user2\', \'user4\']) group_names = [\'group1\', \'group2\', \'group3\'] print(find_common_group_members(group_names)) # Output: [\'user2\'] group_names = [\'group1\', \'group3\'] print(find_common_group_members(group_names)) # Output: [\'user2\'] group_names = [\'group2\', \'group3\'] print(find_common_group_members(group_names)) # Output: [\'user2\'] ``` # Note: Ensure that your function handles groups with no members gracefully and returns the common members in alphabetical order.","solution":"import grp def find_common_group_members(group_names: list[str]) -> list[str]: Finds common members in all specified groups and returns them sorted in alphabetical order. Args: group_names: A list of strings where each string is a group name. Returns: A list of strings where each string is a username that is a member of all the specified groups. if not group_names: return [] # Initialize common_members with members of the first group common_members = set(grp.getgrnam(group_names[0]).gr_mem) # Intersect with members of other groups for group_name in group_names[1:]: group_members = set(grp.getgrnam(group_name).gr_mem) common_members &= group_members return sorted(common_members)"},{"question":"**Objective**: Demonstrate your understanding of writing and running unit tests in Python using the \\"test.support\\" module. # Problem Statement You are tasked with writing a comprehensive test suite for a hypothetical Python module named `mymath`. The `mymath` module contains the following functions: 1. `add(a, b)`: Returns the sum of `a` and `b`. 2. `subtract(a, b)`: Returns the difference between `a` and `b`. 3. `multiply(a, b)`: Returns the product of `a` and `b`. 4. `divide(a, b)`: Returns the quotient of `a` divided by `b`. Raises a `ZeroDivisionError` if `b` is zero. Your test suite should verify that these functions work correctly and handle edge cases appropriately. Additionally, you should test the performance of each function to ensure they execute within a reasonable time frame under typical conditions. # Requirements 1. **Test Module Structure**: - Name your test module `test_mymath.py`. - Use the `unittest` module and follow the naming conventions (`test_` prefix) for both the test module and test methods. 2. **Basic Tests**: - Write tests for each function in the `mymath` module to ensure they return the correct results for common input values. - Include tests to handle edge cases, such as division by zero for the `divide` function. 3. **Performance Tests**: - Ensure that each function executes within a specified time limit (e.g., 1 second) for typical input values. Use the `test.support.SHORT_TIMEOUT` constant to define the time limit. 4. **Resource Management**: - Use appropriate context managers from the `test.support` module to manage temporary files or directories if needed. - Implement setup and teardown methods to prepare the test environment and clean up after tests. 5. **Skipping Tests**: - Write tests that check for the availability of operating system resources (e.g., network connectivity) and skip these tests if the resources are not available. Use the `test.support.requires()` function to check for resources. 6. **Running the Tests**: - Include a `test_main()` function at the end of your test module that uses `support.run_unittest()` to run all tests in the module. # Example Here is an outline of how your `test_mymath.py` file might look: ```python import unittest from test import support import mymath class TestMyMath(unittest.TestCase): def setUp(self): # Setup code, if needed pass def tearDown(self): # Cleanup code, if needed pass def test_add(self): # Test mymath.add function pass def test_subtract(self): # Test mymath.subtract function pass def test_multiply(self): # Test mymath.multiply function pass def test_divide(self): # Test mymath.divide function, including division by zero pass def test_performance_add(self): # Test performance of mymath.add function pass def test_performance_subtract(self): # Test performance of mymath.subtract function pass def test_performance_multiply(self): # Test performance of mymath.multiply function pass def test_performance_divide(self): # Test performance of mymath.divide function pass if __name__ == \'__main__\': support.run_unittest(__name__) ``` # Submission Requirements - Submit your `test_mymath.py` file containing the complete test suite. - Ensure your test suite adheres to the guidelines and requirements specified above. # Evaluation Criteria - Correctness: Tests correctly verify the functionality of the `mymath` functions. - Coverage: Tests cover typical inputs, edge cases, and performance. - Usage of `test.support`: Proper use of `test.support` utilities and context managers. - Structure: Adherence to test module naming conventions and organization. - Resource Management: Appropriate setup, teardown, and resource management in tests. Good luck!","solution":"def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): if b == 0: raise ZeroDivisionError(\\"division by zero\\") return a / b"},{"question":"# Python Code Execution Modes This question assesses your understanding of different modes in which Python can execute code and your ability to write a function to handle these executions. # Problem Statement Write a function `execute_code_in_mode(code: str, mode: str) -> Any` that accepts a string of Python code and a mode of execution, then executes the code according to the specified mode and returns the result if applicable. The function should handle the following modes: 1. **Complete Program (`complete` mode)**: Execute the code as a complete Python program. 2. **File Input (`file` mode)**: Parse the code as if it were read from a non-interactive file. 3. **Interactive Input (`interactive` mode)**: Execute the code in interactive mode. 4. **Expression Input (`eval` mode)**: Evaluate a string as an expression using `eval()`. # Input - `code`: A string containing Python code to execute. - `mode`: A string specifying the mode of execution. It can be one of `complete`, `file`, `interactive`, or `eval`. # Output - The function should return the result of the evaluated code for `eval` mode. - For `complete`, `file`, and `interactive` modes, the function should return `None` but execute the code correctly. # Constraints - Assume the input code is valid Python code. - Handle the different Python execution modes as specified. - You may use the `exec()` and `eval()` functions appropriately. # Example Usage ```python code1 = \\"print(\'Hello, World!\')\\" mode1 = \\"complete\\" execute_code_in_mode(code1, mode1) # Output: Hello, World! code2 = \\"x = 1 + 1nprint(x)\\" mode2 = \\"file\\" execute_code_in_mode(code2, mode2) # Output: 2 code3 = \\"y = 2 * 3ny\\" mode3 = \\"eval\\" result = execute_code_in_mode(code3, mode3) # result => 6 code4 = \\"for i in range(3):n print(i)n\\" mode4 = \\"interactive\\" execute_code_in_mode(code4, mode4) # Output: 0 1 2 ``` # Notes - Use `exec()` for `complete`, `file`, and `interactive` modes. - Use `eval()` for `eval` mode and make sure to return the result.","solution":"def execute_code_in_mode(code: str, mode: str): Executes Python code according to the given mode. Parameters: code (str): The Python code to be executed. mode (str): The mode of execution (\'complete\', \'file\', \'interactive\', \'eval\'). Returns: Any: The result of the evaluated expression in \'eval\' mode, None otherwise. if mode == \'complete\' or mode == \'file\' or mode == \'interactive\': exec(code) return None elif mode == \'eval\': return eval(code) else: raise ValueError(\\"Invalid mode. Mode should be \'complete\', \'file\', \'interactive\', or \'eval\'.\\")"},{"question":"**Objective**: To test your knowledge and proficiency with the `seaborn` library, particularly in creating and customizing swarm plots, and using `catplot` to create multi-faceted plots. **Problem Statement**: Given the `iris` dataset which contains measurements of iris flowers for different species, write code to create and customize visualizations to analyze these measurements. **Task**: 1. Load the `iris` dataset using `seaborn`. 2. Create the following visualizations: - A basic swarm plot displaying the `sepal_length` of the flowers. - A swarm plot comparing `sepal_length` across different species (categorical variable: `species`). - A swarm plot visualizing the multivariate relationship between `sepal_length` and `species`, with further categorization by `petal_width` (using `hue`) - Customize the plot from the previous step by setting a different color palette. - Create another customization where `size` parameter set to 4. - Use `catplot` to visualize the distribution of `sepal_length` faceted by `species`, while showing different `petal_width` categories using the `hue` parameter. **Constraints**: - Ensure the plots are properly labeled and have an appropriate title. - Utilize at least three different customizations shown in the examples (e.g., orientation, hue, palette, size). - The `catplot` should have the facet layout properly arranged. **Expected Output**: - Six plots meeting the above requirements should be output/visualized in the notebook. **Performance Requirements**: - The plots should be rendered without running into performance issues (consider optimizing rendering if the dataset is large). - Ensure the aesthetics and readability of each plot are maintained. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset iris = sns.load_dataset(\\"iris\\") # 1. Basic Swarm plot of sepal_length plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"sepal_length\\") plt.title(\\"Basic Swarm Plot of Sepal Length\\") plt.xlabel(\\"Sepal Length\\") plt.show() # 2. Swarm plot comparing sepal_length across species plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"species\\", y=\\"sepal_length\\") plt.title(\\"Swarm Plot of Sepal Length by Species\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") plt.show() # 3. Multivariate Swarm plot with hue by petal_width plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"species\\", y=\\"sepal_length\\", hue=\\"petal_width\\") plt.title(\\"Swarm Plot of Sepal Length by Species (Hue: Petal Width)\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") plt.show() # 4. Customized Swarm plot with different color palette plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"species\\", y=\\"sepal_length\\", hue=\\"petal_width\\", palette=\\"deep\\") plt.title(\\"Customized Swarm Plot with Palette \'Deep\'\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") plt.show() # 5. Customized Swarm plot with smaller size parameter plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"species\\", y=\\"sepal_length\\", hue=\\"petal_width\\", palette=\\"deep\\", size=4) plt.title(\\"Customized Swarm Plot with Smaller Size\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") plt.show() # 6. Catplot faceted by species and hue by petal_width sns.catplot(data=iris, kind=\\"swarm\\", x=\\"species\\", y=\\"sepal_length\\", hue=\\"petal_width\\", col=\\"species\\", aspect=0.5) plt.subplots_adjust(top=0.85) plt.suptitle(\\"Catplot of Sepal Length by Species (Hue: Petal Width)\\", y=1.05) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset iris = sns.load_dataset(\\"iris\\") # 1. Basic Swarm plot of sepal_length plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"sepal_length\\") plt.title(\\"Basic Swarm Plot of Sepal Length\\") plt.xlabel(\\"Sepal Length\\") plt.show() # 2. Swarm plot comparing sepal_length across species plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"species\\", y=\\"sepal_length\\") plt.title(\\"Swarm Plot of Sepal Length by Species\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") plt.show() # 3. Multivariate Swarm plot with hue by petal_width plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"species\\", y=\\"sepal_length\\", hue=\\"petal_width\\") plt.title(\\"Swarm Plot of Sepal Length by Species (Hue: Petal Width)\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") plt.show() # 4. Customized Swarm plot with different color palette plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"species\\", y=\\"sepal_length\\", hue=\\"petal_width\\", palette=\\"deep\\") plt.title(\\"Customized Swarm Plot with Palette \'Deep\'\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") plt.show() # 5. Customized Swarm plot with smaller size parameter plt.figure(figsize=(10, 6)) sns.swarmplot(data=iris, x=\\"species\\", y=\\"sepal_length\\", hue=\\"petal_width\\", palette=\\"deep\\", size=4) plt.title(\\"Customized Swarm Plot with Smaller Size\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Sepal Length\\") plt.show() # 6. Catplot faceted by species and hue by petal_width sns.catplot(data=iris, kind=\\"swarm\\", x=\\"species\\", y=\\"sepal_length\\", hue=\\"petal_width\\", col=\\"species\\", aspect=0.5) plt.subplots_adjust(top=0.85) plt.suptitle(\\"Catplot of Sepal Length by Species (Hue: Petal Width)\\", y=1.05) plt.show()"},{"question":"# Python Coding Assessment Question **Objective**: Demonstrate your understanding of Python\'s iterator objects by implementing a function that utilizes both sequence iterators and callable iterators. **Question**: Create a function `process_sequence_and_callable(sequence, callable_obj, sentinel)` that processes elements from a given sequence and a callable object using Python\'s iterator mechanics. The function should return a list of elements processed from both iterators. 1. **Input**: - `sequence`: A sequence (e.g., list or tuple) of arbitrary elements. - `callable_obj`: A callable object that returns elements one by one. - `sentinel`: A value that, when returned by `callable_obj`, will indicate the end of iteration. 2. **Output**: - A list containing elements from both the sequence iterator and the callable iterator until both are exhausted. 3. **Constraints**: - Do not use built-in functions like `iter()` directly for creating iterators; use the respective C-API equivalent functions. - The output order should be such that it first includes all elements from the sequence iterator, followed by elements from the callable iterator. # Example: ```python def example_callable(): values = [10, 20, 30, 40] for value in values: yield value # Assume \'sequence\' is [1, 2, 3, 4] and \'callable_obj\' is example_callable. # Sentinel is a value that the callable will not produce, e.g., 50 in this case. result = process_sequence_and_callable([1, 2, 3, 4], example_callable, 50) print(result) # Output: [1, 2, 3, 4, 10, 20, 30, 40] ``` # Notes: - You may assume that you have access to the necessary functions from the C-API interface described in the documentation. - Handle any potential exceptions that may occur during the iteration process appropriately. ```python def process_sequence_and_callable(sequence, callable_obj, sentinel): # Your implementation goes here pass ``` **Hint**: You may need to create Python functions that mimic the behavior of the provided C-API functions for this exercise if direct access is not possible.","solution":"def process_sequence_and_callable(sequence, callable_obj, sentinel): Process elements from a sequence and a callable object using iterator mechanics. Parameters: - sequence: A sequence (e.g., list or tuple) of arbitrary elements. - callable_obj: A callable object that returns elements one by one. - sentinel: A value that, when returned by callable_obj, will indicate the end of iteration. Returns: - A list containing elements from both the sequence iterator and the callable iterator until both are exhausted. output = [] # Create sequence iterator seq_iter = sequence.__iter__() # Process sequence iterator try: while True: output.append(next(seq_iter)) except StopIteration: pass # Create callable iterator callable_iter = iter(callable_obj, sentinel) # Process callable iterator for item in callable_iter: output.append(item) return output"},{"question":"**Coding Assessment Question:** # Objective: You are given sales transaction data for two different months. Your task is to generate a combined report that includes various merging and concatenation operations. # Data You will receive two DataFrames, `df1` and `df2`, formatted as shown below: DataFrame `df1`: | Date | Store_ID | Transaction_ID | Item | Quantity | Price | |------------|----------|----------------|--------|----------|-------| | 2023-01-01 | 1 | 101 | Apples | 5 | 3.00 | | 2023-01-01 | 2 | 102 | Bananas| 10 | 1.00 | | ... | ... | ... | ... | ... | ... | DataFrame `df2`: | Date | Store_ID | Transaction_ID | Item | Quantity | Discount | |------------|----------|----------------|--------|----------|----------| | 2023-02-01 | 1 | 201 | Grapes | 8 | 0.50 | | 2023-02-01 | 3 | 202 | Oranges| 12 | 0.75 | | ... | ... | ... | ... | ... | ... | # Task: 1. **Append `df2` to `df1`**: Combine the two DataFrames to create a single DataFrame containing all transactions from both months. 2. **Merge with a Lookup Table**: Use the following lookup table to add store information to your combined DataFrame: ```python store_info = pd.DataFrame({ \'Store_ID\': [1, 2, 3, 4], \'Store_Name\': [\'Store_A\', \'Store_B\', \'Store_C\', \'Store_D\'], \'Location\': [\'City_X\', \'City_Y\', \'City_Z\', \'City_X\'] }) ``` 3. **Add Item Categories**: Suppose you have another DataFrame mapping items to their categories: ```python item_categories = pd.DataFrame({ \'Item\': [\'Apples\', \'Bananas\', \'Grapes\', \'Oranges\', \'Pears\'], \'Category\': [\'Fruit\', \'Fruit\', \'Fruit\', \'Fruit\', \'Fruit\'] }) ``` Merge this information with your combined DataFrame. 4. **Sort and Clean the Data**: Sort the DataFrame by `Store_ID` and `Date`, and fill any missing values in the combined DataFrame as `0` (numeric columns) or `\'Unknown\'` (categorical columns). # Constraints: - Ensure the final DataFrame retains all original columns, adding `Store_Name`, `Location`, and `Category`. - Consider rows lacking matching store or item information should have `\'Unknown\'` in the merged columns for `Store_Name`, `Location`, and `Category`. # Expected Input: Two pandas DataFrames `df1` and `df2`. # Expected Output: A single pandas DataFrame that includes all transactions from both months, with added store and item category information, properly sorted, and cleaned. # Example Function Signature: ```python import pandas as pd def combine_sales_data(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass ``` # Example Usage: ```python # Given the dataframes df1, df2, store_info, and item_categories as above combined_df = combine_sales_data(df1, df2) print(combined_df) ``` # Performance Requirements: - The solution should run efficiently for DataFrames with up to 10,000 rows each.","solution":"import pandas as pd def combine_sales_data(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame: # Append df2 to df1 combined_df = pd.concat([df1, df2], ignore_index=True) # Create lookup table for store information store_info = pd.DataFrame({ \'Store_ID\': [1, 2, 3, 4], \'Store_Name\': [\'Store_A\', \'Store_B\', \'Store_C\', \'Store_D\'], \'Location\': [\'City_X\', \'City_Y\', \'City_Z\', \'City_X\'] }) # Merge with store information combined_df = combined_df.merge(store_info, on=\'Store_ID\', how=\'left\') # Create lookup table for item categories item_categories = pd.DataFrame({ \'Item\': [\'Apples\', \'Bananas\', \'Grapes\', \'Oranges\', \'Pears\'], \'Category\': [\'Fruit\', \'Fruit\', \'Fruit\', \'Fruit\', \'Fruit\'] }) # Merge with item categories combined_df = combined_df.merge(item_categories, on=\'Item\', how=\'left\') # Fill missing values for column in combined_df.select_dtypes(include=[\'number\']).columns: combined_df[column].fillna(0, inplace=True) for column in combined_df.select_dtypes(include=[\'object\']).columns: combined_df[column].fillna(\'Unknown\', inplace=True) # Sort the DataFrame by Store_ID and Date combined_df.sort_values(by=[\'Store_ID\', \'Date\'], inplace=True) return combined_df"},{"question":"Objective You are required to implement a machine learning model using scikit-learn, optimizing it for prediction latency and throughput. You will be working with a synthetic dataset, and you must consider factors influencing computational performance as discussed in the provided documentation. Problem Statement You need to create a classification model to predict whether a given instance belongs to class 0 or class 1. Your solution should evaluate the prediction latency and throughput of your model, and you should attempt to optimize these metrics. Input and Output Formats 1. **Input**: - `X_train`: A 2D numpy array representing the training data, with shape `(n_samples, n_features)`. - `y_train`: A 1D numpy array representing the class labels for the training data, with shape `(n_samples,)`. - `X_test`: A 2D numpy array representing the test data, with shape `(m_samples, n_features)`. - `y_test`: A 1D numpy array representing the class labels for the test data, with shape `(m_samples,)`. 2. **Output**: - `test_predictions`: A 1D numpy array representing the predicted class labels for the test data, with shape `(m_samples,)`. - `performance_metrics`: A dictionary containing `prediction_latency` (average time in microseconds for a single prediction) and `prediction_throughput` (number of predictions per second). Constraints - You can use any classification model available in scikit-learn. - Optimize for both latency and throughput. - You are allowed to perform any feature extraction or transformation. - Assume the input data does not contain NaNs or infinite values. Performance Requirements - Prediction latency (atomic mode): Should be as low as possible. - Prediction throughput (bulk mode): Should be as high as possible. Example ```python import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score import time def optimize_model(X_train, y_train, X_test, y_test): # Initialize and fit the model model = LogisticRegression() model.fit(X_train, y_train) # Measure prediction latency and throughput start_time = time.time() test_predictions = model.predict(X_test) end_time = time.time() # Calculate metrics prediction_latency = (end_time - start_time) / len(X_test) * 1e6 # in microseconds prediction_throughput = len(X_test) / (end_time - start_time) # predictions per second performance_metrics = { \'prediction_latency\': prediction_latency, \'prediction_throughput\': prediction_throughput } return test_predictions, performance_metrics # Example usage with synthetic data X_train, y_train = make_classification(n_samples=1000, n_features=20, random_state=42) X_test, y_test = make_classification(n_samples=200, n_features=20, random_state=42) test_predictions, performance_metrics = optimize_model(X_train, y_train, X_test, y_test) print(\\"Prediction Latency (us):\\", performance_metrics[\'prediction_latency\']) print(\\"Prediction Throughput (predictions/s):\\", performance_metrics[\'prediction_throughput\']) print(\\"Test Accuracy:\\", accuracy_score(y_test, test_predictions)) ``` In this example, students are expected to explore and implement techniques for improving prediction latency and throughput using the concepts discussed in the documentation.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline import time def optimize_model(X_train, y_train, X_test, y_test): # Using a pipeline with scaling and logistic regression model = Pipeline([ (\'scaler\', StandardScaler()), (\'classifier\', LogisticRegression(solver=\'lbfgs\', max_iter=1000)) ]) # Train the model model.fit(X_train, y_train) # Measure prediction latency and throughput start_time = time.time() test_predictions = model.predict(X_test) end_time = time.time() # Calculate metrics prediction_latency = (end_time - start_time) / len(X_test) * 1e6 # in microseconds prediction_throughput = len(X_test) / (end_time - start_time) # predictions per second performance_metrics = { \'prediction_latency\': prediction_latency, \'prediction_throughput\': prediction_throughput } return test_predictions, performance_metrics"},{"question":"**Coding Assessment Question** # Objective: Demonstrate your understanding of creating and customizing scatter plots and line plots using seaborn\'s `relplot` function. This exercise will assess your ability to visualize complex datasets and enhance plots using various semantics provided by seaborn. # Task: You are given a dataset containing information about car fuel efficiency: `mpg` (miles per gallon), `displacement` (engine displacement), `horsepower`, `weight`, `acceleration`, `model_year`, and `origin` (country of origin). Your task is to perform the following steps: 1. **Load the dataset:** Load the provided CSV file into a pandas DataFrame. 2. **Scatter Plot:** - Create a scatter plot showing `mpg` on the y-axis and `displacement` on the x-axis. - Use different colors to distinguish cars from different `origin` countries. - Customize the plot by: - Using different marker styles for each `origin`. - Setting the size of markers based on `weight`. 3. **Line Plot:** - Create a line plot showing `mpg` over `model_year`. - Use different colors to distinguish cars from different `origin` countries. - Customize the plot by: - Adding error bars to represent the standard deviation of `mpg` for each `model_year`. - Setting different line styles based on `origin`. 4. **Faceted Plot:** - Create a faceted plot to show the scatter plot from step 2 but faceted by `model_year` using columns. # Input: A CSV file named `car_data.csv` with the following columns: - `mpg` - `displacement` - `horsepower` - `weight` - `acceleration` - `model_year` - `origin` # Output: A Python script that: 1. Loads the dataset. 2. Produces and displays the specified scatter and line plots. 3. Produces and displays the faceted plot. # Constraints: - Ensure your code is efficient and follows best practices for data visualization. - Use seaborn and matplotlib libraries only for plotting. # Example: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load dataset df = pd.read_csv(\'car_data.csv\') # Scatter Plot sns.relplot(data=df, x=\'displacement\', y=\'mpg\', hue=\'origin\', style=\'origin\', size=\'weight\') plt.show() # Line Plot sns.relplot(data=df, x=\'model_year\', y=\'mpg\', kind=\'line\', hue=\'origin\', style=\'origin\', errorbar=\'sd\') plt.show() # Faceted Plot sns.relplot(data=df, x=\'displacement\', y=\'mpg\', hue=\'origin\', style=\'origin\', size=\'weight\', col=\'model_year\') plt.show() ``` Make sure to submit your code and generated plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_data(file_path): Loads the dataset from a CSV file. Params: file_path (str): Path to the CSV file containing car data. Returns: DataFrame: Loaded data as a pandas DataFrame. return pd.read_csv(file_path) def create_scatter_plot(df): Creates a scatter plot showing \'mpg\' on the y-axis and \'displacement\' on the x-axis, using different colors to distinguish cars from different \'origin\' countries. Marker styles are set based on \'origin\' and marker sizes are based on \'weight\'. Params: df (DataFrame): The car dataset. scatter_plot = sns.relplot(data=df, x=\'displacement\', y=\'mpg\', hue=\'origin\', style=\'origin\', size=\'weight\') plt.title(\'Scatter Plot of MPG vs Displacement by Origin\') scatter_plot.set_axis_labels(\'Displacement (cc)\', \'Miles Per Gallon (mpg)\') plt.show() def create_line_plot(df): Creates a line plot showing \'mpg\' over \'model_year\', using different colors to distinguish cars from different \'origin\' countries. Line styles are set based on \'origin\' and error bars represent the standard deviation of \'mpg\' for each \'model_year\'. Params: df (DataFrame): The car dataset. line_plot = sns.relplot(data=df, x=\'model_year\', y=\'mpg\', kind=\'line\', hue=\'origin\', style=\'origin\', errorbar=\'sd\') plt.title(\'Line Plot of MPG over Model Year by Origin\') line_plot.set_axis_labels(\'Model Year\', \'Miles Per Gallon (mpg)\') plt.show() def create_faceted_plot(df): Creates a faceted plot to show the scatter plot faceted by \'model_year\'. Params: df (DataFrame): The car dataset. faceted_plot = sns.relplot(data=df, x=\'displacement\', y=\'mpg\', hue=\'origin\', style=\'origin\', size=\'weight\', col=\'model_year\') plt.subplots_adjust(top=0.9) faceted_plot.fig.suptitle(\'Faceted Scatter Plot of MPG vs Displacement by Model Year and Origin\') plt.show() def main(): file_path = \'car_data.csv\' df = load_data(file_path) create_scatter_plot(df) create_line_plot(df) create_faceted_plot(df) # Uncomment the line below to run the script # main()"},{"question":"You are tasked with developing an asynchronous system that simulates processing requests concurrently, demonstrating your ability to work with `asyncio`\'s tasks, queues, and exception handling. # Task Implement a function `process_requests_concurrently(requests)`, where `requests` is a list of tuples representing (request_id, processing_time), that processes each request concurrently. If a request takes longer than a specified timeout, it should be canceled and logged. Function Signature ```python import asyncio from typing import List, Tuple async def process_requests_concurrently(requests: List[Tuple[int, int]]) -> None: # Your implementation here ``` Input - `requests`: List of tuples, where each tuple contains two integers `(request_id, processing_time)`: - `request_id` (int): Unique identifier for the request. - `processing_time` (int): Time in seconds the request takes to process. Constraints - The maximum timeout for processing any request is 5 seconds. - You must use `asyncio.Queue` for managing requests. - If a request is canceled due to timeout, log `\\"Request {request_id} timed out.\\"`. - Ensure the program handles `CancelledError` appropriately. Output The function does not return anything, but you should print logs for each request whether it completes successfully or is canceled. # Example ```python import asyncio from typing import List, Tuple async def process_requests_concurrently(requests: List[Tuple[int, int]]) -> None: queue = asyncio.Queue() for req in requests: await queue.put(req) async def worker(): while not queue.empty(): request_id, processing_time = await queue.get() try: await asyncio.wait_for(asyncio.sleep(processing_time), timeout=5) print(f\\"Request {request_id} completed.\\") except asyncio.TimeoutError: print(f\\"Request {request_id} timed out.\\") tasks = [asyncio.create_task(worker()) for _ in range(3)] # Create 3 worker tasks await asyncio.gather(*tasks) # Example usage requests = [(1, 3), (2, 6), (3, 2), (4, 5)] asyncio.run(process_requests_concurrently(requests)) ``` # Explanation 1. Initialize an `asyncio.Queue` and enqueue each request. 2. Create a worker coroutine to process requests from the queue. 3. Use `asyncio.wait_for` to handle timeout for each request. 4. Log \\"Request {request_id} completed.\\" if a request completes within the timeout. 5. Log \\"Request {request_id} timed out.\\" if a request is canceled. 6. Create and run multiple worker tasks concurrently using `asyncio.gather`. # Notes - Make sure to handle potential `asyncio.TimeoutError` exceptions. - Ensure that your solution efficiently uses `asyncio` features to manage concurrency and timeouts.","solution":"import asyncio from typing import List, Tuple async def process_requests_concurrently(requests: List[Tuple[int, int]]) -> None: queue = asyncio.Queue() for req in requests: await queue.put(req) async def worker(): while not queue.empty(): request_id, processing_time = await queue.get() try: await asyncio.wait_for(asyncio.sleep(processing_time), timeout=5) print(f\\"Request {request_id} completed.\\") except asyncio.TimeoutError: print(f\\"Request {request_id} timed out.\\") tasks = [asyncio.create_task(worker()) for _ in range(3)] # Create 3 worker tasks await asyncio.gather(*tasks) # Example usage requests = [(1, 3), (2, 6), (3, 2), (4, 5)] asyncio.run(process_requests_concurrently(requests))"},{"question":"**Question: Create a Complex Email with Multiple MIME Types** In this task, you are required to construct a complex MIME email message using the `email.mime` package. The email should contain the following parts: 1. **Text Part**: A plain text message saying \\"Hello, this is a test email.\\" 2. **Image Part**: An inline image that can be displayed within the email body. Use any sample image data for this purpose. 3. **Attachment**: A text file attachment with the content \\"This is an attached file.\\" # Requirements: - The email should be structured as a multipart email. - Use the appropriate MIME classes for each part (text, image, and attachment). - Ensure that the inline image is properly encoded using base64. - Properly handle the content type and encoding for each email part. # Input Format: No input needed. You will construct the email within your function. # Output Format: You need to return the completed email as a string that represents the entire MIME structure of the email. # Example: A conceptual outline of your output email structure: ``` Content-Type: multipart/mixed; boundary=\\"YOUR_BOUNDARY_STRING\\" MIME-Version: 1.0 --YOUR_BOUNDARY_STRING Content-Type: text/plain; charset=\\"utf-8\\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Hello, this is a test email. --YOUR_BOUNDARY_STRING Content-Type: image/png; name=\\"sample.png\\" MIME-Version: 1.0 Content-Transfer-Encoding: base64 Content-Disposition: inline; filename=\\"sample.png\\" ...base64 encoded image data... --YOUR_BOUNDARY_STRING Content-Type: text/plain; name=\\"attachment.txt\\" MIME-Version: 1.0 Content-Transfer-Encoding: base64 Content-Disposition: attachment; filename=\\"attachment.txt\\" ...base64 encoded attachment data... --YOUR_BOUNDARY_STRING-- ``` # Constraints: - You are allowed to use any additional Python standard libraries if needed. - Make sure to handle any errors in the construction of the MIME parts. Implement the function `create_complex_email()` that accomplishes the requirements outlined. ```python def create_complex_email(): from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email import encoders import base64 # Create the root message msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Text Part text_part = MIMEText(\\"Hello, this is a test email.\\", \\"plain\\") msg.attach(text_part) # Image Part image_data = base64.b64decode(\'/9j/4AAQSkZJRgABA...\') # Truncated base64 image data image_part = MIMEImage(image_data, name=\\"sample.png\\") msg.attach(image_part) # Attachment Part attachment = MIMEApplication(\\"This is an attached file.\\", name=\\"attachment.txt\\") encoders.encode_base64(attachment) attachment.add_header(\'Content-Disposition\', \'attachment\', filename=\\"attachment.txt\\") msg.attach(attachment) return msg.as_string() ```","solution":"def create_complex_email(): from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email import encoders import base64 # Create the root message msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Text Part text_part = MIMEText(\\"Hello, this is a test email.\\", \\"plain\\") msg.attach(text_part) # Image Part # Sample base64 encoded image data (truncated for readability, you should use a valid image base64 string) image_data = base64.b64decode( \'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAA\' \'AAC0lEQVR42mP8/wcAAwAB/7b3nkwAAAAASUVORK5CYII=\' ) image_part = MIMEImage(image_data, name=\\"sample.png\\") # Add necessary headers image_part.add_header(\'Content-Disposition\', \'inline\', filename=\\"sample.png\\") msg.attach(image_part) # Attachment Part attachment_data = \\"This is an attached file.\\" attachment = MIMEApplication(attachment_data, name=\\"attachment.txt\\") encoders.encode_base64(attachment) attachment.add_header(\'Content-Disposition\', \'attachment\', filename=\\"attachment.txt\\") msg.attach(attachment) return msg.as_string()"},{"question":"**Objective:** Test students\' understanding of the `tarfile` module, including creating tar archives with various compression methods, adding files, and safely extracting files using filters. **Question:** You are given a directory that contains several files and subdirectories. Your tasks are to: 1. Create a tar archive of the directory, applying gzip compression. 2. Extract the contents of the archive to a new location using a custom extraction filter that ensures all extracted files have read and write permissions for the user only (`u+rw`). 3. List the contents of the extracted files with their sizes and types (file, directory, etc.). **Requirements:** 1. **Function 1: `create_gzip_archive(source, archive_name)`** - **Input:** - `source` (str): The path to the directory to be archived. - `archive_name` (str): The name of the output tar archive file. - **Output:** None - **Task:** Create a gzip-compressed tar archive of the `source` directory. 2. **Function 2: `extract_with_rw_permissions(archive_name, destination)`** - **Input:** - `archive_name` (str): The path to the tar archive file to be extracted. - `destination` (str): The path to the directory where the archive will be extracted. - **Output:** None - **Task:** Extract the archive to the `destination` directory, ensuring that all files have read and write permissions for the user only. 3. **Function 3: `list_extracted_files(destination)`** - **Input:** - `destination` (str): The path to the directory where files have been extracted. - **Output:** A list of tuples, where each tuple contains: - `file_name` (str): The full path of the file. - `file_size` (int): The size of the file in bytes. - `file_type` (str): The type of the file (\\"file\\", \\"directory\\", \\"symlink\\", etc.). **Constraints:** - You must use the `tarfile` module. - Apply gzip compression when creating the archive. - Use appropriate permissions and filters during extraction to enhance security. **Example Usage:** Assume the directory structure is as follows: ``` /source /subdir subfile.txt file1.txt file2.txt ``` 1. `create_gzip_archive(\'/source\', \'archive.tar.gz\')` should create an archive `archive.tar.gz`. 2. `extract_with_rw_permissions(\'archive.tar.gz\', \'/destination\')` should extract files to `/destination` with read and write permissions for the user. 3. `list_extracted_files(\'/destination\')` should return: ``` [ (\'/destination/source/subdir/subfile.txt\', 100, \'file\'), (\'/destination/source/file1.txt\', 50, \'file\'), (\'/destination/source/file2.txt\', 75, \'file\'), (\'/destination/source/subdir\', 0, \'directory\') ] ``` Create the three functions—`create_gzip_archive`, `extract_with_rw_permissions`, and `list_extracted_files`—in the provided template. **Template:** ```python import tarfile import os def create_gzip_archive(source, archive_name): # Implement this function pass def extract_with_rw_permissions(archive_name, destination): # Implement this function pass def list_extracted_files(destination): # Implement this function pass # Example usage source = \'/path/to/source\' archive_name = \'archive.tar.gz\' destination = \'/path/to/destination\' create_gzip_archive(source, archive_name) extract_with_rw_permissions(archive_name, destination) files_list = list_extracted_files(destination) print(files_list) ``` **Note:** Make sure your code handles various edge cases and follows all the constraints provided.","solution":"import tarfile import os import stat def create_gzip_archive(source, archive_name): Create a gzip-compressed tar archive of the `source` directory. :param source: The path to the directory to be archived. :param archive_name: The name of the output tar archive file. with tarfile.open(archive_name, \\"w:gz\\") as tar: tar.add(source, arcname=os.path.basename(source)) def extract_with_rw_permissions(archive_name, destination): Extract the tar archive to the `destination` directory, ensuring that all files have read and write permissions for the user only. :param archive_name: The path to the tar archive file. :param destination: The path to the directory where the archive will be extracted. with tarfile.open(archive_name, \\"r:gz\\") as tar: def safe_extract_tarinfo(tarinfo): if tarinfo.isfile(): tarinfo.mode = stat.S_IRUSR | stat.S_IWUSR return tarinfo tar.extractall(path=destination, members=map(safe_extract_tarinfo, tar)) def list_extracted_files(destination): List the contents of the `destination` directory with their sizes and types. :param destination: The path to the directory where files have been extracted. :return: A list of tuples containing (file_name, file_size, file_type). file_info_list = [] for root, dirs, files in os.walk(destination): for name in files: full_path = os.path.join(root, name) file_info_list.append((full_path, os.path.getsize(full_path), \\"file\\")) for name in dirs: full_path = os.path.join(root, name) file_info_list.append((full_path, 0, \\"directory\\")) return file_info_list"},{"question":"# Custom Path Entry Finder You are required to create a custom path entry finder and a loader that can import Python modules from a given prefix path using a custom search criterion. The custom finder should only look for `.custompy` files instead of `.py` files. Objectives: 1. Implement a custom path entry finder. 2. Implement a custom loader that loads modules from `.custompy` files. 3. Register the custom finder so that it can be used during imports. Guidelines: 1. You need to define a class `CustomPathEntryFinder` that implements the `find_spec` method to search for `.custompy` files. 2. Define a class `CustomLoader` that implements the method `exec_module` to load the module. 3. One way to ensure the custom finder is used is by adding it to `sys.path_hooks`. Expectations: 1. The `CustomPathEntryFinder` should locate modules with the `.custompy` extension. 2. The `CustomLoader` should load the content of `.custompy` files and execute them in the module namespace. 3. Register the custom finder using `sys.path_hooks`, making sure the `sys.path_importer_cache` is used correctly. Input and Output: - **Input**: A path containing `.custompy` files. - **Output**: The imported module should behave as if it was a regular Python module. Example: Consider the following directory structure: ``` mypackage/ module.custompy ``` Here is how you might use the custom path entry finder and loader: ```python import sys sys.path.insert(0, \\"/path/to/mypackage\\") import module # This should load `module.custompy` using your custom loader ``` Implement the following two classes and the registration code: ```python import sys from importlib.abc import MetaPathFinder, Loader from importlib.util import spec_from_file_location class CustomPathEntryFinder: def __init__(self, path_entry): self.path_entry = path_entry def find_spec(self, fullname, target=None): # Implement logic here to locate .custompy files # Raise an ImportError if the module isn\'t found pass class CustomLoader: def __init__(self, filepath): self.filepath = filepath def exec_module(self, module): # Implement logic here to execute the module\'s code in its namespace pass # Register the custom finder sys.path_hooks.append(lambda path: CustomPathEntryFinder(path) if path == \\"/path/to/mypackage\\" else None) # Clear the importer cache for the custom path sys.path_importer_cache.pop(\\"/path/to/mypackage\\", None) ``` Complete the implementation of the `CustomPathEntryFinder` and `CustomLoader` classes such that the above example works as expected.","solution":"import sys import os from importlib.abc import MetaPathFinder, Loader from importlib.util import spec_from_file_location import importlib.util class CustomPathEntryFinder(MetaPathFinder): def __init__(self, path_entry): self.path_entry = path_entry def find_spec(self, fullname, path=None, target=None): module_name = fullname.split(\'.\')[-1] filename = os.path.join(self.path_entry, f\\"{module_name}.custompy\\") if os.path.exists(filename): loader = CustomLoader(filename) return spec_from_file_location(fullname, filename, loader=loader) return None class CustomLoader(Loader): def __init__(self, filepath): self.filepath = filepath def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): with open(self.filepath, \'r\') as file: code = file.read() exec(code, module.__dict__) # Register the custom finder sys.path_hooks.append(lambda path: CustomPathEntryFinder(path) if os.path.isdir(path) else None) # Clear the importer cache for the custom path sys.path_importer_cache.clear()"},{"question":"# Context Management with `contextvars` in Python You are required to write a Python script using the `contextvars` module to simulate context management for a multi-threaded or asynchronous environment. Task 1. **Define Context Variables**: - Create two context variables, `user_id` and `request_id`, using `contextvars.ContextVar`. 2. **Simulate Context Switching**: - Implement a function `process_request` that sets values for these context variables, processes a dummy request, and then resets the context variables to their previous states. - Utilize the token system provided by `ContextVar` to reset the context variables. 3. **Thread-Safe Function Implementation**: - Implement a function `handle_request` which will be called in different threads, ensuring each thread maintains its own context `user_id` and `request_id`. Provide detailed implementation to handle context switching safely using the Python `contextvars` module. Follow the criteria below: Implementation Requirements: - **Function `create_context_vars`**: Creates and returns the `user_id` and `request_id` context variables. - **Function `process_request(user_id, request_id)`**: Sets the passed `user_id` and `request_id` context variables, simulates processing (e.g., using a `time.sleep`), and then resets the variables. - **Function `handle_request(user_id, request_id)`**: Calls `process_request` and ensures correct context management across multiple threads. Example: ```python import contextvars import threading import time # Function to create context variables def create_context_vars(): user_id = contextvars.ContextVar(\\"user_id\\") request_id = contextvars.ContextVar(\\"request_id\\") return user_id, request_id # Function to process requests with proper context management def process_request(user_id, request_id): token1 = user_id.set(user_id) token2 = request_id.set(request_id) # Simulate request processing time.sleep(1) print(f\\"Processing request for user {user_id.get()} with request ID {request_id.get()}\\") # Reset the context variables user_id.reset(token1) request_id.reset(token2) # Function to handle requests across multiple threads def handle_request(user_id, request_id): user_id_var, request_id_var = create_context_vars() thread1 = threading.Thread(target=process_request, args=(user_id, request_id)) thread2 = threading.Thread(target=process_request, args=(user_id+1, request_id+1)) thread1.start() thread2.start() thread1.join() thread2.join() # Example usage user_id_var, request_id_var = create_context_vars() handle_request(101, 5001) ``` Constraints: - Do not use global variables to store context state as it may lead to conflicts in a multi-threaded environment. - Ensure proper management of context switching and use of the `ContextVar` tokens to reset state after processing. Implement the above functions and demonstrate their correctness by simulating multiple threads handling requests with different `user_id` and `request_id` values.","solution":"import contextvars import threading import time # Function to create context variables def create_context_vars(): user_id = contextvars.ContextVar(\\"user_id\\") request_id = contextvars.ContextVar(\\"request_id\\") return user_id, request_id # Function to process requests with proper context management def process_request(user_id_value, request_id_value, user_id, request_id): token1 = user_id.set(user_id_value) token2 = request_id.set(request_id_value) # Simulate request processing time.sleep(1) print(f\\"Processing request for user {user_id.get()} with request ID {request_id.get()}\\") # Reset the context variables user_id.reset(token1) request_id.reset(token2) # Function to handle requests across multiple threads def handle_request(user_id_value, request_id_value): user_id, request_id = create_context_vars() thread1 = threading.Thread(target=process_request, args=(user_id_value, request_id_value, user_id, request_id)) thread2 = threading.Thread(target=process_request, args=(user_id_value + 1, request_id_value + 1, user_id, request_id)) thread1.start() thread2.start() thread1.join() thread2.join() # Example usage handle_request(101, 2001)"},{"question":"Coding Assessment Question # Objective Design and implement an asyncio-based server application that can handle multiple client connections. The server must perform the following tasks asynchronously: 1. Accept and handle multiple client connections. 2. For each connected client, send a welcome message immediately upon connection. 3. Continuously listen for messages from connected clients, echoing received messages back to the sender with a prefix indicating the server received the message. 4. Implement a mechanism to shut down the server gracefully upon receiving a specific exit command from any client. 5. Ensure the server runs using proper coroutine-based execution with the event loop. # Requirements 1. Create an asyncio server that listens for incoming client connections. 2. Use asyncio to manage multiple client connections concurrently. 3. Implement a coroutine for handling client connections, which sends a welcome message and echoes received messages with a prefix. 4. Implement graceful server shutdown upon receiving the command \\"exit\\". # Input and Output - The server will listen on a specified host and port. - It should send and receive UTF-8 encoded text data. - Welcome message: \\"Welcome to the Asyncio Server!\\" - Echoed messages should have the prefix: \\"Server received: \\" - When the server receives the \\"exit\\" command from any client, it should shutdown and close all active connections. # Constraints - The server should handle at least 5 simultaneous connections. - Ensure proper handling of exceptions and client disconnections. # Example Suppose the server is running on localhost and the port 12345. 1. Client connects to the server. 2. Server sends: `Welcome to the Asyncio Server!` 3. Client sends: `Hello` 4. Server sends: `Server received: Hello` 5. Client sends: `exit` 6. Server shuts down gracefully and closes all connections. # Starter Code ```python import asyncio async def handle_client(reader, writer): address = writer.get_extra_info(\'peername\') print(f\\"New connection from {address}\\") writer.write(\\"Welcome to the Asyncio Server!n\\".encode()) while True: try: data = await reader.read(100) message = data.decode().strip() if message == \\"exit\\": print(f\\"Received exit command from {address}\\") writer.write(\\"Server shutting down...n\\".encode()) await writer.drain() writer.close() await writer.wait_closed() break else: response = f\\"Server received: {message}n\\" writer.write(response.encode()) await writer.drain() except asyncio.IncompleteReadError: break async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 12345) address = server.sockets[0].getsockname() print(f\\"Serving on {address}\\") # Serve requests until Ctrl+C is pressed async with server: await server.serve_forever() asyncio.run(main()) ``` In this task, implement and complete the two coroutines `handle_client` and `main` to handle the requirements specified. # Evaluation Criteria - Correctness: Does the implementation meet the objectives and requirements? - Concurrency: Is the server able to handle multiple clients concurrently without blocking? - Graceful Shutdown: Does the server shut down correctly upon receiving the \\"exit\\" command? - Error Handling: Does the implementation handle possible exceptions and errors gracefully?","solution":"import asyncio async def handle_client(reader, writer): address = writer.get_extra_info(\'peername\') print(f\\"New connection from {address}\\") welcome_message = \\"Welcome to the Asyncio Server!n\\" writer.write(welcome_message.encode()) await writer.drain() while True: try: data = await reader.read(100) message = data.decode().strip() if message == \\"exit\\": print(f\\"Received exit command from {address}\\") writer.write(\\"Server shutting down...n\\".encode()) await writer.drain() break else: response = f\\"Server received: {message}n\\" writer.write(response.encode()) await writer.drain() except asyncio.IncompleteReadError: break except ConnectionResetError: break writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 12345) address = server.sockets[0].getsockname() print(f\\"Serving on {address}\\") # Serve requests until Ctrl+C is pressed async with server: await server.serve_forever() # Uncomment this line to run the server # asyncio.run(main())"},{"question":"# **Coding Assessment Question** Objective The objective of this question is to assess your understanding of Python sequences and how to manipulate them using various operations. Task Your task is to implement a Python class `PySequence` that mimics some of the functionalities highlighted in the provided documentation. This class should perform operations such as checking if an object supports the sequence protocol, retrieving the length of a sequence, concatenating sequences, repeating sequences, retrieving items and slices, setting items and slices, deleting items and slices, counting occurrences, checking for containment, and converting sequences to lists or tuples. Class Definition and Required Methods Implement the `PySequence` class with the following methods: 1. `__init__(self, seq)`: Initialize the `PySequence` object with a sequence `seq`. 2. `is_sequence(self) -> bool`: Return `True` if the object supports the sequence protocol, `False` otherwise. 3. `get_length(self) -> int`: Return the length of the sequence. 4. `concat(self, other_seq)`: Return the concatenation of the sequence with another sequence `other_seq`. 5. `repeat(self, count: int)`: Return the sequence repeated `count` times. 6. `get_item(self, index: int)`: Return the item at the specified index. 7. `get_slice(self, start: int, end: int)`: Return the slice of the sequence from `start` to `end`. 8. `set_item(self, index: int, value)`: Set the item at the specified index to `value`. 9. `del_item(self, index: int)`: Delete the item at the specified index. 10. `set_slice(self, start: int, end: int, value)`: Set the slice of the sequence from `start` to `end` to `value`. 11. `del_slice(self, start: int, end: int)`: Delete the slice of the sequence from `start` to `end`. 12. `count(self, value) -> int`: Return the number of occurrences of `value` in the sequence. 13. `contains(self, value) -> bool`: Return `True` if the sequence contains `value`, `False` otherwise. 14. `index(self, value) -> int`: Return the first index of `value` in the sequence. 15. `to_list(self) -> list`: Return the sequence as a list. 16. `to_tuple(self) -> tuple`: Return the sequence as a tuple. Example ```python # Example usage of the PySequence class seq = PySequence([1, 2, 3, 4, 2]) print(seq.is_sequence()) # Output: True print(seq.get_length()) # Output: 5 print(seq.concat([5, 6])) # Output: [1, 2, 3, 4, 2, 5, 6] print(seq.repeat(2)) # Output: [1, 2, 3, 4, 2, 1, 2, 3, 4, 2] print(seq.get_item(2)) # Output: 3 print(seq.get_slice(1, 4)) # Output: [2, 3, 4] seq.set_item(2, 10) print(seq.seq) # Output: [1, 2, 10, 4, 2] seq.del_item(1) print(seq.seq) # Output: [1, 10, 4, 2] seq.set_slice(1, 3, [7, 8]) print(seq.seq) # Output: [1, 7, 8, 2] seq.del_slice(1, 3) print(seq.seq) # Output: [1, 2] print(seq.count(2)) # Output: 1 print(seq.contains(4)) # Output: False print(seq.index(2)) # Output: 1 print(seq.to_list()) # Output: [1, 2] print(seq.to_tuple()) # Output: (1, 2) ``` Constraints - The sequence `seq` passed during initialization can be any object that supports the sequence protocol (e.g., list, tuple, range, etc.). - Handle all edge cases such as negative indices or invalid operations gracefully.","solution":"class PySequence: def __init__(self, seq): if not isinstance(seq, (list, tuple, range)): raise ValueError(\\"Initial sequence must support sequence protocol.\\") self.seq = list(seq) def is_sequence(self) -> bool: return isinstance(self.seq, (list, tuple, range)) def get_length(self) -> int: return len(self.seq) def concat(self, other_seq): if not isinstance(other_seq, (list, tuple, range)): raise ValueError(\\"Other sequence must support sequence protocol.\\") return self.seq + list(other_seq) def repeat(self, count: int): return self.seq * count def get_item(self, index: int): return self.seq[index] def get_slice(self, start: int, end: int): return self.seq[start:end] def set_item(self, index: int, value): self.seq[index] = value def del_item(self, index: int): del self.seq[index] def set_slice(self, start: int, end: int, value): if not isinstance(value, list): raise ValueError(\\"Value must be a list.\\") self.seq[start:end] = value def del_slice(self, start: int, end: int): del self.seq[start:end] def count(self, value) -> int: return self.seq.count(value) def contains(self, value) -> bool: return value in self.seq def index(self, value) -> int: return self.seq.index(value) def to_list(self) -> list: return list(self.seq) def to_tuple(self) -> tuple: return tuple(self.seq)"},{"question":"# Question Context: You\'re tasked with converting a neural network written in PyTorch into TorchScript for better optimization and deployment. The original PyTorch model includes both convolutional layers and fully connected layers, and uses some helper functions. Task: 1. Implement a simple neural network in PyTorch. 2. Convert this network to TorchScript using the `@torch.jit.script` decorator. 3. Write a function annotated with TorchScript types that forwards an input tensor through the network and performs some additional computations using TorchScript-supported operations. Specifications: 1. **Neural Network**: - The network should consist of the following layers in order: 1. A 2D convolutional layer with input channels = 1, output channels = 16, kernel size = 3. 2. A ReLU activation function. 3. A max pooling layer with kernel size = 2. 4. A fully connected layer that outputs 10 features. 2. **TorchScript Network**: - Convert the above neural network to TorchScript by appropriately using the `@torch.jit.script` decorator. 3. **Forward Function**: - Write the `forward` function for the network which should: - Take a tensor of shape (batch_size, 1, 28, 28) as input. - Pass it through the network. - Return the output tensor. Requirements: 1. Your implementation should define all necessary types and ensure that all operations comply with TorchScript\'s type requirements. 2. The `forward` function should include at least one TorchScript-compatible conditional operation on the tensor\'s output. # Example Code Structure ```python import torch import torch.nn as nn # Define the neural network class MyNetwork(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, kernel_size=3) self.relu = nn.ReLU() self.pool = nn.MaxPool2d(kernel_size=2) self.fc = nn.Linear(16*13*13, 10) # Input size after pooling: 16*13*13 def forward(self, x): x = self.conv1(x) x = self.relu(x) x = self.pool(x) x = x.view(x.size(0), -1) # Flatten the tensor x = self.fc(x) return x # Convert to TorchScript @torch.jit.script class ScriptedNetwork: def __init__(self): self.model = MyNetwork() def forward(self, x: torch.Tensor) -> torch.Tensor: # Your TorchScript-compatible forward computation here pass # Dummy input tensor input_tensor = torch.rand((1, 1, 28, 28)) # Create scripted network instance scripted_network = ScriptedNetwork() # Perform forward pass output = scripted_network.forward(input_tensor) print(output) ``` # Constraints - The batch size for the input tensor can vary but the other dimensions will remain consistent (1, 28, 28). - Ensure that the `ScriptedNetwork` class and methods follow TorchScript rules and restrictions. Submission: Submit your Python code implementing the above task.","solution":"import torch import torch.nn as nn import torch.jit # Define the neural network using nn.Module class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(1, 16, kernel_size=3) self.relu = nn.ReLU() self.pool = nn.MaxPool2d(kernel_size=2) self.fc = nn.Linear(16 * 13 * 13, 10) # Assuming input size of 28x28 def forward(self, x): x = self.conv1(x) x = self.relu(x) x = self.pool(x) x = x.view(x.size(0), -1) # Flatten the tensor x = self.fc(x) return x # Convert the network to TorchScript class ScriptedNetwork(torch.nn.Module): def __init__(self): super(ScriptedNetwork, self).__init__() self.model = SimpleNet() @torch.jit.script_method def forward(self, x: torch.Tensor) -> torch.Tensor: # Pass through the model x = self.model(x) # Example additional computation: if the max value in the tensor is greater than a threshold if torch.max(x) > 1.0: x = x / torch.max(x) return x # Instantiate and script the network scripted_model = torch.jit.script(ScriptedNetwork()) # Dummy input tensor input_tensor = torch.rand((1, 1, 28, 28)) # Perform forward pass to test the scripted model output = scripted_model(input_tensor) print(output)"},{"question":"# Hyper-parameter Tuning with Scikit-Learn In this exercise, you will implement a pipeline for a machine learning workflow using scikit-learn. The task involves loading the dataset, creating a machine learning model, and tuning its hyper-parameters using `GridSearchCV` or `RandomizedSearchCV`. You will also use cross-validation to evaluate model performance. Objective Implement a function that performs the following tasks: 1. **Load the Dataset**: - Load the Iris dataset from `sklearn.datasets`. 2. **Preprocess the Data**: - Standardize the features using `StandardScaler`. 3. **Model Selection and Hyper-Parameter Tuning**: - Define a Support Vector Machine (SVM) classifier. - Use `GridSearchCV` or `RandomizedSearchCV` to find the best hyper-parameters for the SVM model. Use the following hyper-parameter space for `GridSearchCV`: ```python param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] ``` - Alternatively, use `RandomizedSearchCV` with the following parameter space: ```python param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } ``` 4. **Cross-Validation**: - Use 5-fold cross-validation during hyper-parameter tuning. - Evaluate the model using accuracy and F1-score. 5. **Output**: - Return the best parameters and the best cross-validated score. # Example ```python from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import make_scorer, f1_score import scipy.stats def tune_hyperparameters(use_randomized_search=False): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Standardize features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Define SVM model svm = SVC() # Define parameter grid or distribution if use_randomized_search: param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } search = RandomizedSearchCV(svm, param_distributions=param_dist, n_iter=50, scoring={\'Accuracy\': \'accuracy\', \'F1-score\': make_scorer(f1_score, average=\'weighted\')}, refit=\'Accuracy\', cv=5) else: param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] search = GridSearchCV(svm, param_grid=param_grid, scoring={\'Accuracy\': \'accuracy\', \'F1-score\': make_scorer(f1_score, average=\'weighted\')}, refit=\'Accuracy\', cv=5) # Fit search search.fit(X_scaled, y) # Return best parameters and best score return search.best_params_, search.best_score_ # Example of usage print(tune_hyperparameters(use_randomized_search=False)) print(tune_hyperparameters(use_randomized_search=True)) ``` Requirements - The function `tune_hyperparameters` should take a boolean parameter `use_randomized_search` which decides whether to use `RandomizedSearchCV` or `GridSearchCV` for hyper-parameter optimization. - The function should load the Iris dataset, preprocess it, define the SVM model, perform cross-validation with hyper-parameter tuning, and return the best parameters and the best cross-validated score. # Constraints - You can use only the following packages: `sklearn`, `scipy`, `numpy`. - The solution should be efficient and should complete within a reasonable time for the given dataset. Evaluation Criteria - Correctness: The function should correctly implement the hyper-parameter tuning process. - Clarity: The code should be well-structured and clearly commented. - Efficiency: The solution should be efficient in terms of runtime and memory usage. Good luck!","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import make_scorer, f1_score import scipy.stats def tune_hyperparameters(use_randomized_search=False): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Standardize features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Define SVM model svm = SVC() # Define parameter grid or distribution if use_randomized_search: param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } search = RandomizedSearchCV(svm, param_distributions=param_dist, n_iter=50, scoring={\'Accuracy\': \'accuracy\', \'F1-score\': make_scorer(f1_score, average=\'weighted\')}, refit=\'Accuracy\', cv=5) else: param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] search = GridSearchCV(svm, param_grid=param_grid, scoring={\'Accuracy\': \'accuracy\', \'F1-score\': make_scorer(f1_score, average=\'weighted\')}, refit=\'Accuracy\', cv=5) # Fit search search.fit(X_scaled, y) # Return best parameters and best score return search.best_params_, search.best_score_"},{"question":"# Out-of-Core Learning: Scikit-learn Incremental Learning Objective: Implement a scalable classification system utilizing out-of-core learning. Using the provided skeleton code below, complete the functions required to read data in streams, extract features using HashingVectorizer, and perform classification using an incremental learning algorithm. Description: You are provided with a dataset of text documents for a classification task. The dataset is so large that it cannot fit into memory all at once. Instead, you\'ll process it in chunks (mini-batches). You are required to implement: 1. A generator function to stream the data in mini-batches. 2. A function to extract features using HashingVectorizer. 3. An incremental classifier using `SGDClassifier`. Detailed Requirements: 1. **Streaming Instances:** Implement the `stream_data` function that simulates out-of-core learning by reading data in chunks. ```python def stream_data(file_path, batch_size): Generator function to read data from a file in chunks. Args: file_path (str): Path to the text data file. batch_size (int): Number of lines to read in each batch. Yields: List[str]: A mini-batch of text documents. with open(file_path, \'r\') as file: batch = [] for line in file: batch.append(line.strip()) if len(batch) == batch_size: yield batch batch = [] if batch: yield batch ``` 2. **Extracting Features:** Implement the `extract_features` function that converts the list of text documents into feature vectors using `HashingVectorizer`. ```python from sklearn.feature_extraction.text import HashingVectorizer def extract_features(docs): Convert text documents to feature vectors using HashingVectorizer. Args: docs (List[str]): List of text documents. Returns: scipy.sparse matrix: Transformed feature matrix. vectorizer = HashingVectorizer(n_features=2**20) return vectorizer.transform(docs) ``` 3. **Incremental Learning:** Implement the `incremental_learning` function that trains an `SGDClassifier` incrementally on the streamed data. ```python from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def incremental_learning(file_path, batch_size, n_iter): Perform incremental learning using SGDClassifier. Args: file_path (str): Path to the text data file. batch_size (int): Number of lines to read in each batch. n_iter (int): Number of iterations over the entire dataset. Returns: List[float]: List of accuracy scores at the end of each iteration. clf = SGDClassifier() accuracies = [] for _ in range(n_iter): data_stream = stream_data(file_path, batch_size) for batch in data_stream: X_batch = extract_features(batch) y_batch = [0] * len(batch) # Fake labels for simplicity. Replace with actual labels. clf.partial_fit(X_batch, y_batch, classes=[0, 1]) # Simulated accuracy calculation. Replace with actual validation data. X_val = extract_features(batch) y_val = [0] * len(batch) predictions = clf.predict(X_val) accuracy = accuracy_score(y_val, predictions) accuracies.append(accuracy) return accuracies ``` Constraints: 1. The dataset file is a large text file where each line contains a document. 2. Implement necessary import statements. 3. Replace fake labels `[0] * len(batch)` with actual labels when available. 4. Ensure that the classifier\'s `partial_fit` method is called with the `classes` parameter. Example Usage: Assuming the text data file is `large_text_data.txt` and each line represents a document: ```python accuracies = incremental_learning(\'large_text_data.txt\', batch_size=1000, n_iter=10) print(accuracies) ``` This should print the accuracy scores for each iteration over the data.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path, batch_size): Generator function to read data from a file in chunks. Args: file_path (str): Path to the text data file. batch_size (int): Number of lines to read in each batch. Yields: List[str]: A mini-batch of text documents. with open(file_path, \'r\') as file: batch = [] for line in file: batch.append(line.strip()) if len(batch) == batch_size: yield batch batch = [] if batch: yield batch def extract_features(docs): Convert text documents to feature vectors using HashingVectorizer. Args: docs (List[str]): List of text documents. Returns: scipy.sparse matrix: Transformed feature matrix. vectorizer = HashingVectorizer(n_features=2**20) return vectorizer.transform(docs) def incremental_learning(file_path, batch_size, n_iter, class_labels): Perform incremental learning using SGDClassifier. Args: file_path (str): Path to the text data file. batch_size (int): Number of lines to read in each batch. n_iter (int): Number of iterations over the entire dataset. class_labels (list): List of possible class labels. Returns: List[float]: List of accuracy scores at the end of each iteration. clf = SGDClassifier() accuracies = [] for _ in range(n_iter): data_stream = stream_data(file_path, batch_size) for batch in data_stream: X_batch = extract_features(batch) y_batch = [0] * len(batch) # Placeholder labels, adjust based on your needs clf.partial_fit(X_batch, y_batch, classes=class_labels) # Simulated accuracy calculation. Replace with actual validation data. X_val = extract_features(batch) y_val = [0] * len(batch) predictions = clf.predict(X_val) accuracy = accuracy_score(y_val, predictions) accuracies.append(accuracy) return accuracies"},{"question":"**Preprocessing and Model Pipeline with Scikit-Learn** # Objective: In this assessment, you will be required to preprocess a given dataset and then build a machine learning pipeline to perform binary classification. This project will test your understanding of various preprocessing techniques offered by `sklearn.preprocessing` and your ability to build a robust machine learning pipeline. # Dataset: You are provided with a dataset `data.csv` that contains both numerical and categorical features along with a binary target label `Target`. # Requirements: 1. **Data Preprocessing**: - Apply `StandardScaler` to all numerical features to scale them to have zero mean and unit variance. - Encode all categorical features using `OneHotEncoder` with the parameter `handle_unknown=\'ignore\'`. - For any missing values in the dataset, use `SimpleImputer` with the strategy `most_frequent` to fill them. 2. **Pipeline Construction**: - Construct a pipeline that first applies the above preprocessing steps and then fits a `LogisticRegression` model. 3. **Model Evaluation**: - Split the data into a training set and a test set (80%-20% split). - Train your pipeline on the training data and evaluate its accuracy on the test set. # Input Format: - `data.csv`: A CSV file with multiple features (both numerical and categorical) and a target column `Target`. # Output Format: - Print the accuracy of the model on the test set. # Constraints: - Assume there are no more than 1000 examples in the dataset. - Ensure that all preprocessing steps handle any edge cases, such as missing values and unknown categories in categorical features. # Performance Requirements: - The entire process (preprocessing, training, and evaluation) should complete within a reasonable time frame, ideally under 1 minute. # Sample Code Structure: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Load the dataset data = pd.read_csv(\'data.csv\') # Split the features and the target X = data.drop(columns=\'Target\') y = data[\'Target\'] # Separate numerical and categorical features numerical_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns # Preprocessing for numerical data: StandardScaler numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'scaler\', StandardScaler()) ]) # Preprocessing for categorical data: OneHotEncoder categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Create the pipeline model_pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the pipeline model_pipeline.fit(X_train, y_train) # Predict and evaluate y_pred = model_pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Model accuracy: {accuracy:.2f}\\") ``` # Submission: Submit your complete Python script along with comments explaining each major step. Ensure that your script runs without errors on the provided dataset.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def preprocess_and_train_model(data_path): # Load the dataset data = pd.read_csv(data_path) # Split the features and the target X = data.drop(columns=\'Target\') y = data[\'Target\'] # Separate numerical and categorical features numerical_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns # Preprocessing for numerical data: StandardScaler numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'scaler\', StandardScaler()) ]) # Preprocessing for categorical data: OneHotEncoder categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Create the pipeline model_pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the pipeline model_pipeline.fit(X_train, y_train) # Predict and evaluate y_pred = model_pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Model accuracy: {accuracy:.2f}\\") return accuracy"},{"question":"<|Analysis Begin|> The provided documentation pertains to the Python `logging` module, which is an essential part of the Python standard library designed for flexible event logging. The module contains classes and functions for implementing a robust and hierarchical logging system. Key components include `Logger`, `Handler`, `Formatter`, and `Filter` objects, each with specific roles and functionalities. Given the depth and breadth of the `logging` module, a suitable coding assessment question should require the students to demonstrate their understanding of multiple aspects of the module, including: 1. Creation and configuration of `Logger` objects. 2. Setting up `Handlers` to direct log messages to various destinations. 3. Customizing log message formats using `Formatter` objects. 4. Utilizing `Filter` objects to control which log records are processed. 5. Demonstrating an understanding of hierarchical logging and propagation. The question should be designed to be challenging yet clear and self-contained, ensuring that it tests both fundamental and advanced aspects of the `logging` module. <|Analysis End|> <|Question Begin|> **Question: Implementing a Custom Logging System** **Objective**: The goal of this task is to create and configure a custom logging system for a hypothetical application. Your implementation should demonstrate a thorough understanding of the `logging` module by setting up multiple loggers, handlers, formatters, and filters, and ensuring that log messages are propagated appropriately. **Task**: 1. **Create a logger named `app.main`** that will serve as the primary logger for the main module of the application. Configure it with the following settings: - Set the logger level to `DEBUG`. - Attach two handlers: - A `StreamHandler` that outputs to the console. - A `FileHandler` that writes to a file named `app.log`. 2. **Customize the log message format** using `Formatter` objects. The `StreamHandler` should use the format: ``` %(levelname)s - %(name)s - %(message)s ``` The `FileHandler` should use the format: ``` %(asctime)s - %(name)s - %(levelname)s - %(message)s ``` 3. **Create a custom filter** that only allows log messages from loggers whose name starts with `app`. Attach this filter to both handlers. 4. **Create a child logger** named `app.main.child`. Configure it to use the same handlers and settings as `app.main`, but set its level to `INFO`. 5. **Write a script** that generates log messages at different levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) from both `app.main` and `app.main.child` loggers. Ensure that these messages obey the hierarchical logging and propagation rules. **Input and Output Specifications**: - **Input**: None (The script will internally generate log messages using the logging system you configure). - **Output**: Log messages should be displayed on the console and written to the `app.log` file according to the specified formats and filters. **Constraints and Requirements**: - You must use the built-in `logging` module. - Ensure that the `FileHandler` appends to the log file without overwriting existing entries. - The log messages should correctly reflect the propagation rules and the levels set for each logger. **Example**: When your script runs, it should produce console output similar to: ``` INFO - app.main - This is an info message from app.main WARNING - app.main - This is a warning message from app.main ERROR - app.main - This is an error message from app.main CRITICAL - app.main - This is a critical message from app.main INFO - app.main.child - This is an info message from app.main.child WARNING - app.main.child - This is a warning message from app.main.child ERROR - app.main.child - This is an error message from app.main.child CRITICAL - app.main.child - This is a critical message from app.main.child ``` The corresponding `app.log` file should contain entries similar to: ``` 2023-01-01 12:00:00,000 - app.main - INFO - This is an info message from app.main 2023-01-01 12:00:01,000 - app.main - WARNING - This is a warning message from app.main 2023-01-01 12:00:02,000 - app.main - ERROR - This is an error message from app.main 2023-01-01 12:00:03,000 - app.main - CRITICAL - This is a critical message from app.main 2023-01-01 12:00:04,000 - app.main.child - INFO - This is an info message from app.main.child 2023-01-01 12:00:05,000 - app.main.child - WARNING - This is a warning message from app.main.child 2023-01-01 12:00:06,000 - app.main.child - ERROR - This is an error message from app.main.child 2023-01-01 12:00:07,000 - app.main.child - CRITICAL - This is a critical message from app.main.child ``` **Note**: Ensure that the datetime format in the log entries matches what is produced by the `logging` module by default. **Submission**: Submit a single Python script file (`logging_setup.py`) containing your implementation.","solution":"import logging def setup_logging(): # Create the primary logger logger = logging.getLogger(\'app.main\') logger.setLevel(logging.DEBUG) # Set up the StreamHandler for console output stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.DEBUG) stream_formatter = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s\') stream_handler.setFormatter(stream_formatter) # Set up the FileHandler for file output file_handler = logging.FileHandler(\'app.log\', mode=\'a\') file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) # Create a custom filter allowing log messages only from \'app\' loggers class AppFilter(logging.Filter): def filter(self, record): return record.name.startswith(\'app\') app_filter = AppFilter() stream_handler.addFilter(app_filter) file_handler.addFilter(app_filter) # Add handlers to the main logger logger.addHandler(stream_handler) logger.addHandler(file_handler) # Create a child logger with different settings child_logger = logging.getLogger(\'app.main.child\') child_logger.setLevel(logging.INFO) child_logger.propagate = True def test_logging(): logger = logging.getLogger(\'app.main\') child_logger = logging.getLogger(\'app.main.child\') logger.debug(\\"This is a debug message from app.main\\") logger.info(\\"This is an info message from app.main\\") logger.warning(\\"This is a warning message from app.main\\") logger.error(\\"This is an error message from app.main\\") logger.critical(\\"This is a critical message from app.main\\") child_logger.debug(\\"This is a debug message from app.main.child\\") child_logger.info(\\"This is an info message from app.main.child\\") child_logger.warning(\\"This is a warning message from app.main.child\\") child_logger.error(\\"This is an error message from app.main.child\\") child_logger.critical(\\"This is a critical message from app.main.child\\") # Setup the logging configuration setup_logging() # Generate log messages test_logging()"},{"question":"# Advanced Python: Designing an Asynchronous Task Scheduler **Objective:** In this assessment, you will implement an asynchronous task scheduler using Python\'s coroutine capabilities. This will test your understanding of `async` and `await` syntax, event loops, and coroutine management. **Problem Statement:** You need to design a simple asynchronous task scheduler that can schedule and run tasks, ensuring that tasks are executed in an efficient, non-blocking manner. The scheduler will support adding tasks that are coroutines and running them using an event loop. **Requirements:** 1. Implement a class `TaskScheduler` with the following methods: - `add_task(self, task: Awaitable) -> None`: Adds a coroutine task to the scheduler. - `run(self) -> None`: Runs all scheduled tasks using an event loop until all tasks are complete. 2. A coroutine `task` can be any function defined with the `async` keyword. 3. The `run` method should handle multiple tasks and ensure that each task is executed asynchronously. 4. Your implementation should not use any third-party libraries; only standard Python 3.10 libraries. **Example Usage:** ```python import asyncio class TaskScheduler: def __init__(self): # Your implementation here pass def add_task(self, task: Awaitable) -> None: # Your implementation here pass def run(self) -> None: # Your implementation here pass # Example coroutine tasks async def task1(): print(\\"Task 1 started\\") await asyncio.sleep(1) print(\\"Task 1 completed\\") async def task2(): print(\\"Task 2 started\\") await asyncio.sleep(2) print(\\"Task 2 completed\\") async def task3(): print(\\"Task 3 started\\") await asyncio.sleep(3) print(\\"Task 3 completed\\") # Example usage of the scheduler scheduler = TaskScheduler() scheduler.add_task(task1()) scheduler.add_task(task2()) scheduler.add_task(task3()) scheduler.run() ``` **Expected Output:** ``` Task 1 started Task 2 started Task 3 started Task 1 completed Task 2 completed Task 3 completed ``` **Constraints:** 1. Tasks must run concurrently, not sequentially. 2. The `run` method should block until all tasks are complete. 3. Pay attention to exceptions in coroutines; they must not cause the scheduler to terminate prematurely. **Performance Requirements:** The scheduler should be able to handle at least 1000 concurrent tasks efficiently without running into performance bottlenecks. Ensure your code is well-documented and follows Python best practices for asynchronous programming.","solution":"import asyncio from typing import Awaitable, List class TaskScheduler: def __init__(self): self.tasks: List[Awaitable] = [] def add_task(self, task: Awaitable) -> None: Adds a coroutine task to the scheduler. self.tasks.append(task) def run(self) -> None: Runs all scheduled tasks using an event loop until all tasks are complete. loop = asyncio.get_event_loop() loop.run_until_complete(self._run_all_tasks()) async def _run_all_tasks(self): Coroutine that runs all tasks concurrently. await asyncio.gather(*self.tasks)"},{"question":"# Problem Description You are required to implement a function that parses HTTP responses to determine the status of multiple URLs. Your goal is to implement the following function: ```python from http import HTTPStatus def categorize_http_status(status_codes): Categorizes a list of HTTP status codes into three categories: \'Successful\', \'Client Error\', and \'Server Error\'. Parameters: status_codes (List[int]): A list of HTTP status codes. Returns: Dict[str, List[str]]: A dictionary with the following keys: - \'Successful\': List of strings in the format \\"{code} - {reason_phrase}\\" for 2xx status codes. - \'Client Error\': List of strings in the format \\"{code} - {reason_phrase}\\" for 4xx status codes. - \'Server Error\': List of strings in the format \\"{code} - {reason_phrase}\\" for 5xx status codes. # Implement the function here ``` # Input - `status_codes`: A list of integers, each representing an HTTP status code (e.g., [200, 404, 500]). # Output - A dictionary with three keys: \'Successful\', \'Client Error\', \'Server Error\'. Each key should map to a list of strings, where each string is in the format \\"{code} - {reason_phrase}\\". The strings should represent the status codes that fall into the respective categories: - \'Successful\': Contains status codes in the 2xx range. - \'Client Error\': Contains status codes in the 4xx range. - \'Server Error\': Contains status codes in the 5xx range. # Constraints - You can assume that each status code provided in the input list is a valid HTTP status code. - Any status codes outside the ranges of 2xx, 4xx, and 5xx can be ignored. # Example ```python status_codes = [200, 201, 304, 400, 403, 500, 503, 999] result = categorize_http_status(status_codes) # Expected output # { # \'Successful\': [\'200 - OK\', \'201 - Created\'], # \'Client Error\': [\'400 - Bad Request\', \'403 - Forbidden\'], # \'Server Error\': [\'500 - Internal Server Error\', \'503 - Service Unavailable\'] # } ``` # Additional Notes - Use the `HTTPStatus` class from the `http` module to get details about the status codes. - Aim to write clean, readable, and efficient code.","solution":"from http import HTTPStatus def categorize_http_status(status_codes): Categorizes a list of HTTP status codes into three categories: \'Successful\', \'Client Error\', and \'Server Error\'. Parameters: status_codes (List[int]): A list of HTTP status codes. Returns: Dict[str, List[str]]: A dictionary with the following keys: - \'Successful\': List of strings in the format \\"{code} - {reason_phrase}\\" for 2xx status codes. - \'Client Error\': List of strings in the format \\"{code} - {reason_phrase}\\" for 4xx status codes. - \'Server Error\': List of strings in the format \\"{code} - {reason_phrase}\\" for 5xx status codes. categorized = { \'Successful\': [], \'Client Error\': [], \'Server Error\': [] } for code in status_codes: if 200 <= code < 300: categorized[\'Successful\'].append(f\\"{code} - {HTTPStatus(code).phrase}\\") elif 400 <= code < 500: categorized[\'Client Error\'].append(f\\"{code} - {HTTPStatus(code).phrase}\\") elif 500 <= code < 600: categorized[\'Server Error\'].append(f\\"{code} - {HTTPStatus(code).phrase}\\") return categorized"},{"question":"# URL Component Analysis and Construction Problem Statement: You work for a company that deals with various URL manipulations for web scraping and API interactions. Your task is to create a function `analyze_and_construct_url(url, action)` that takes two inputs: 1. `url` (a string): The URL to be analyzed or modified. 2. `action` (a dictionary): Specifies the operations to perform. The dictionary has the following structure (all keys are optional based on the desired action): ```python { \\"parse\\": bool, # If True, parse the URL and return the components. \\"modify\\": { # If provided, modify the specified components of the parsed URL \\"scheme\\": str, \\"netloc\\": str, \\"path\\": str, \\"params\\": str, \\"query\\": str, \\"fragment\\": str }, \\"query\\": str, # If provided, parse the query string into a dictionary \\"quote\\": bool, # If True, quote special characters in the URL. \\"unquote\\": bool # If True, unquote special characters in the URL. } ``` Input: - `url`: A strict URL string to be analyzed or modified. - `action`: A dictionary containing the desired actions. Output: - A string or dictionary based on the specified actions. Examples: ```python # Example 1: Parsing a URL url = \\"http://www.example.com:80/path;params?query=1#frag\\" action = {\\"parse\\": True} result = analyze_and_construct_url(url, action) # Expected Output: # { # \\"scheme\\": \\"http\\", # \\"netloc\\": \\"www.example.com:80\\", # \\"path\\": \\"/path\\", # \\"params\\": \\"params\\", # \\"query\\": \\"query=1\\", # \\"fragment\\": \\"frag\\" # } # Example 2: Modifying a URL component url = \\"http://www.example.com/path?query=1#frag\\" action = {\\"modify\\": {\\"scheme\\": \\"https\\", \\"path\\": \\"/new_path\\"}} result = analyze_and_construct_url(url, action) # Expected Output: # \\"https://www.example.com/new_path?query=1#frag\\" # Example 3: Parsing the query string url = \\"http://www.example.com/path?name=John&age=30\\" action = {\\"query\\": \\"name=John&age=30\\"} result = analyze_and_construct_url(url, action) # Expected Output: # {\\"name\\": [\\"John\\"], \\"age\\": [\\"30\\"]} # Example 4: Quoting the URL url = \\"http://www.example.com/path/space here\\" action = {\\"quote\\": True} result = analyze_and_construct_url(url, action) # Expected Output: # \\"http%3A//www.example.com/path/space%20here\\" # Example 5: Unquoting the URL url = \\"http%3A//www.example.com/path/space%20here\\" action = {\\"unquote\\": True} result = analyze_and_construct_url(url, action) # Expected Output: # \\"http://www.example.com/path/space here\\" ``` Constraints: 1. If `action` is empty or no valid action is specified, return the original URL. 2. Use appropriate functions from the `urllib.parse` module for URL parsing, modification, quoting, and unquoting. 3. Ensure that invalid operations or inputs raise meaningful errors. Function Signature: ```python def analyze_and_construct_url(url: str, action: dict) -> Union[str, dict]: pass ```","solution":"from urllib.parse import urlparse, urlunparse, parse_qs, quote, unquote def analyze_and_construct_url(url, action): Analyzes and/or modifies a URL based on the provided actions. Parameters: - url (str): The URL to be analyzed or modified. - action (dict): The actions to perform on the URL. Returns: - The modified URL as a string, or parsed components as a dictionary. if not action: return url result = url parsed_url = urlparse(url) if action.get(\\"parse\\"): result = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } if \\"modify\\" in action: modify = action[\\"modify\\"] new_components = [ modify.get(\\"scheme\\", parsed_url.scheme), modify.get(\\"netloc\\", parsed_url.netloc), modify.get(\\"path\\", parsed_url.path), modify.get(\\"params\\", parsed_url.params), modify.get(\\"query\\", parsed_url.query), modify.get(\\"fragment\\", parsed_url.fragment) ] result = urlunparse(new_components) if \\"query\\" in action: result = parse_qs(action[\\"query\\"]) if action.get(\\"quote\\"): result = quote(url) if action.get(\\"unquote\\"): result = unquote(url) return result"},{"question":"# Question Objective: To assess your understanding of Python\'s asyncio event loop by implementing a function that utilizes asynchronous tasks, delayed callbacks, and handles task completion. Problem Statement: Implement a function named `async_process_tasks` that simulates processing multiple asynchronous tasks with varying delays and priorities using asyncio event loop methods. Each task is assigned a delay in seconds and has a priority level. The function will return a list of results in the order of task completion. Requirements: - Define a class `AsyncTask` to encapsulate task details: - `name`: A string representing the task name. - `delay`: An integer representing the delay (in seconds) before the task completes. - `priority`: An integer representing the task priority (higher number indicates higher priority). - `result`: A string indicating the result after completion, initially set to `\\"Pending\\"`. ```python class AsyncTask: def __init__(self, name: str, delay: int, priority: int): self.name = name self.delay = delay self.priority = priority self.result = \\"Pending\\" ``` - Implement the `async_process_tasks` function: - **Input**: List of `AsyncTask` instances. - Use `asyncio.create_task()` to schedule the execution of each task based on their delay. - Use `asyncio.run()` to run the event loop until all tasks are completed. - Upon task completion, update the `result` attribute of the respective `AsyncTask` instance to `\\"Completed\\"`. - Further, sort the tasks based on the completion time. - **Output**: A list of task names in the order they completed. - Use the documentation-provided methods such as `loop.call_later()`, `loop.create_task()`, and manage exceptions if necessary. Constraints: - You should not use any global variables. - Ensure your implementation respects the asynchronous nature of the problem. Example: ```python tasks = [ AsyncTask(\'Task1\', 2, 1), AsyncTask(\'Task2\', 1, 2), AsyncTask(\'Task3\', 3, 1) ] result = async_process_tasks(tasks) # Expected Output: # [\'Task2\', \'Task1\', \'Task3\'] ``` Note: The order of completion depends on the delay associated with each task. ```python import asyncio class AsyncTask: def __init__(self, name: str, delay: int, priority: int): self.name = name self.delay = delay self.priority = priority self.result = \\"Pending\\" async def process_individual_task(task: AsyncTask): await asyncio.sleep(task.delay) task.result = \\"Completed\\" return task.name def async_process_tasks(tasks): loop = asyncio.get_event_loop() # Schedule all the tasks tasks_to_run = [loop.create_task(process_individual_task(task)) for task in tasks] # Run the loop until all tasks are completed loop.run_until_complete(asyncio.gather(*tasks_to_run)) # Sort the tasks based on their delay and priority tasks.sort(key=lambda t: t.priority, reverse=True) return [task.name for task in tasks] # Example usage tasks = [AsyncTask(\'Task1\', 2, 1), AsyncTask(\'Task2\', 1, 2), AsyncTask(\'Task3\', 3, 1)] print(async_process_tasks(tasks)) ```","solution":"import asyncio class AsyncTask: def __init__(self, name: str, delay: int, priority: int): self.name = name self.delay = delay self.priority = priority self.result = \\"Pending\\" async def process_individual_task(task: AsyncTask): await asyncio.sleep(task.delay) task.result = \\"Completed\\" return task.name def async_process_tasks(tasks): loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) # Schedule all the tasks tasks_to_run = [loop.create_task(process_individual_task(task)) for task in tasks] # Run the loop until all tasks are completed loop.run_until_complete(asyncio.gather(*tasks_to_run)) loop.close() # Sort the tasks based on their completion time tasks.sort(key=lambda t: (t.delay, -t.priority)) return [task.name for task in tasks]"},{"question":"**Objective:** Create a sophisticated Python function that utilizes control flow statements and demonstrates comprehension of various function argument types and argument unpacking. **Problem Statement:** You are to create a function that processes a list of integers, performs various operations based on control flow, and handles multiple types of function arguments. **Function to Implement:** Define a function `process_numbers(numbers, /, *, operation=\'sum\', threshold=None)` where: - `numbers` is a list of integers (positional-only). - `operation` is a keyword-only argument that determines the operation to perform on `numbers`. It can take the following string values: `\'sum\'`, `\'product\'`, `\'average\'`, `\'filter\'`, and `\'top_n\'`. - `threshold` is an optional keyword-only argument that can be used to filter numbers greater than `threshold` when `operation` is `\'filter\'`. It defaults to `None`. **Function Behavior:** Implement the function such that it: 1. **Sum**: - If `operation` is `\'sum\'`, return the sum of all integers in `numbers`. 2. **Product**: - If `operation` is `\'product\'`, return the product of all integers in `numbers`. 3. **Average**: - If `operation` is `\'average\'`, return the average value of all integers in `numbers`. 4. **Filter**: - If `operation` is `\'filter\'`, return a list of integers from `numbers` that are greater than `threshold`. - If `threshold` is `None`, raise a `ValueError` with the message `\\"Threshold must be provided for filter operation.\\"`. 5. **Top N**: - If `operation` is `\'top_n\'`, return the top N largest integers from `numbers` in descending order. - `N` should be provided as an additional keyword argument `n`. If `n` is not provided, raise a `ValueError` with the message `\\"Parameter \'n\' must be provided for top_n operation.\\"`. **Example Usages:** ```python # Example usage of the `process_numbers` function: # Sum of the numbers print(process_numbers([1, 2, 3, 4], operation=\'sum\')) # Output: 10 # Product of the numbers print(process_numbers([1, 2, 3, 4], operation=\'product\')) # Output: 24 # Average of the numbers print(process_numbers([1, 2, 3, 4], operation=\'average\')) # Output: 2.5 # Filter numbers greater than threshold print(process_numbers([1, 2, 3, 4, 5, 6], operation=\'filter\', threshold=3)) # Output: [4, 5, 6] # Top N largest numbers print(process_numbers([4, 1, 5, 2, 3], operation=\'top_n\', n=3)) # Output: [5, 4, 3] ``` **Constraints:** - The input list `numbers` is guaranteed to have at least one integer. - For the `filter` operation, `threshold` must be an integer. - For the `top_n` operation, `n` must be a positive integer and less than or equal to the length of the `numbers` list. **Notes:** - You should handle possible errors gracefully and provide meaningful error messages as specified. - Make use of default arguments, keyword-only arguments, and positional-only arguments where appropriate. - Follow Pythonic conventions for function and variable names.","solution":"def process_numbers(numbers, /, *, operation=\'sum\', threshold=None, n=None): Processes a list of numbers based on the specified operation. Args: - numbers (list of int): List of integers to process. - operation (str): The operation to perform, which can be \'sum\', \'product\', \'average\', \'filter\', \'top_n\'. - threshold (int, optional): The threshold for the filter operation. - n (int, optional): The number of top elements to return for the top_n operation. Returns: - The result of the specified operation. Raises: - ValueError: If required parameters are missing. if operation == \'sum\': return sum(numbers) elif operation == \'product\': product = 1 for number in numbers: product *= number return product elif operation == \'average\': return sum(numbers) / len(numbers) elif operation == \'filter\': if threshold is None: raise ValueError(\\"Threshold must be provided for filter operation.\\") return [num for num in numbers if num > threshold] elif operation == \'top_n\': if n is None: raise ValueError(\\"Parameter \'n\' must be provided for top_n operation.\\") return sorted(numbers, reverse=True)[:n] else: raise ValueError(\\"Invalid operation specified.\\")"},{"question":"# Question: Implement a Custom Logger using the \\"syslog\\" Module You are required to implement a custom logging class `CustomLogger` that utilizes the Unix `syslog` library routines provided by the `syslog` module in Python. This logger should have methods to initialize logging configurations, log messages with different priority levels, dynamically change log masks, and close the logger. The class should encapsulate the functionalities of the `syslog` module. Class: `CustomLogger` **Methods:** - `__init__(self, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER)`: Constructor to initialize the custom logger. Calls `openlog` with the provided parameters. - `log(self, priority: int, message: str)`: Logs a message with the given priority. Calls `syslog`. - `set_log_mask(self, priority: int)`: Sets the priority mask using `setlogmask`. The mask should allow logging for all priorities up to and including the provided priority. - `close(self)`: Closes the logger by calling `closelog`. **Example Usage:** ```python import syslog class CustomLogger: def __init__(self, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER): # Initialize the logger with provided ident, logoption, and facility syslog.openlog(ident, logoption, facility) def log(self, priority: int, message: str): # Log a message with the given priority syslog.syslog(priority, message) def set_log_mask(self, priority: int): # Set the priority mask to log messages up to and including the given priority syslog.setlogmask(syslog.LOG_UPTO(priority)) def close(self): # Close the logger syslog.closelog() # Example usage: logger = CustomLogger(ident=\\"CustomLogger\\", logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) logger.log(syslog.LOG_INFO, \\"This is an info message.\\") logger.log(syslog.LOG_ERR, \\"This is an error message.\\") logger.set_log_mask(syslog.LOG_WARNING) logger.log(syslog.LOG_DEBUG, \\"This debug message should not be logged.\\") logger.close() ``` **Constraints:** 1. The `message` parameter in `log` must be a non-empty string. 2. The `priority` parameter must be a valid `syslog` priority level. 3. The `facility` parameter in the constructor must be a valid `syslog` facility. **Objective:** Implement the `CustomLogger` class as specified and ensure that it behaves correctly following the constraints provided.","solution":"import syslog class CustomLogger: def __init__(self, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER): # Initialize the logger with provided ident, logoption, and facility syslog.openlog(ident if ident else \\"\\", logoption, facility) def log(self, priority: int, message: str): if not message: raise ValueError(\\"Message parameter must be a non-empty string.\\") # Log a message with the given priority syslog.syslog(priority, message) def set_log_mask(self, priority: int): # Set the priority mask to log messages up to and including the given priority syslog.setlogmask(syslog.LOG_UPTO(priority)) def close(self): # Close the logger syslog.closelog()"},{"question":"**JSON Data Manipulation** **Objective:** You need to parse a JSON input, manipulate the data based on specific conditions, and then encode it back to JSON. **Task:** Write a function `modify_json(data: str) -> str` that takes a JSON string as input and performs the following operations: 1. Parse the JSON string into a Python dictionary. 2. If the dictionary contains a key `\\"emails\\"` with a list of email addresses: - Append the domain `\\"@example.com\\"` to each email address that doesn\'t already contain a domain. 3. If the dictionary contains a key `\\"numbers\\"` with a list of numbers: - Filter out any numbers that are negative. 4. Encode the modified dictionary back into a JSON string and return it. **Input:** - A JSON string `data` representing a dictionary. Example: ```json { \\"emails\\": [\\"user1\\", \\"user2@gmail.com\\", \\"user3\\"], \\"numbers\\": [1, -2, 3, -4, 5] } ``` **Output:** - A JSON string with the modified dictionary. Example: ```json { \\"emails\\": [\\"user1@example.com\\", \\"user2@gmail.com\\", \\"user3@example.com\\"], \\"numbers\\": [1, 3, 5] } ``` **Constraints:** - The list under the `\\"emails\\"` key will only contain strings. - The list under the `\\"numbers\\"` key will only contain integers. - The input JSON will always be well-formed and valid. **Performance Requirements:** - The function should be efficient in both time and space complexity concerning the operations performed on the lists. **Examples:** Example 1: ```python input_data = \'{\\"emails\\": [\\"user1\\", \\"user2@gmail.com\\", \\"user3\\"], \\"numbers\\": [1, -2, 3, -4, 5]}\' print(modify_json(input_data)) ``` Output: ```json {\\"emails\\": [\\"user1@example.com\\", \\"user2@gmail.com\\", \\"user3@example.com\\"], \\"numbers\\": [1, 3, 5]} ``` Example 2: ```python input_data = \'{\\"emails\\": [\\"admin\\", \\"test@example.com\\"], \\"numbers\\": [-1, -2, -3]}\' print(modify_json(input_data)) ``` Output: ```json {\\"emails\\": [\\"admin@example.com\\", \\"test@example.com\\"], \\"numbers\\": []} ``` **Note:** - Ensure you use the `json` module for parsing and encoding JSON. - You can assume all string manipulations and list operations stay within reasonable limits.","solution":"import json def modify_json(data: str) -> str: Parses a JSON string `data`, modifies the dictionary based on the given conditions, and returns the modified dictionary as a JSON string. # Parse the JSON input to a dictionary data_dict = json.loads(data) # Modify the emails if \\"emails\\" in data_dict: data_dict[\\"emails\\"] = [ email if \\"@\\" in email else email + \\"@example.com\\" for email in data_dict[\\"emails\\"] ] # Modify the numbers if \\"numbers\\" in data_dict: data_dict[\\"numbers\\"] = [ number for number in data_dict[\\"numbers\\"] if number >= 0 ] # Encode the modified dictionary back to a JSON string return json.dumps(data_dict)"},{"question":"# Objective: Design a function that analyzes and processes a log of user activity to extract certain types of information based on specified patterns. # Problem Statement: You are given a log containing entries of user activities. Each entry is in the format: ``` [HH:MM:SS] User: [username] Action: [action_description] ``` Where: - `HH:MM:SS` represents the time in 24-hour format. - `[username]` is the username which consists of alphanumeric characters and underscores. - `[action_description]` varies but includes predefined keywords such as LOGIN, LOGOUT, ERROR, UPLOAD, DOWNLOAD. # Task: Implement the function `analyze_log(log: str) -> dict` that processes the given log and returns a dictionary with the following keys: 1. \\"error_times\\": A list of timestamps (HH:MM:SS) where the action was \\"ERROR\\". 2. \\"user_actions\\": A dictionary where keys are usernames and values are lists of actions performed by the users in the order of occurrence. 3. \\"total_uploads\\": The total number of \\"UPLOAD\\" actions in the log. 4. \\"fixed_log\\": A version of the log where any occurrence of the string \\"ERROR\\" in the action description is replaced by the string \\"ISSUE\\". # Input: - `log` (str): A string containing several lines, each representing an entry in the specified log format. # Output: - A dictionary with the specified keys and values. # Constraints: - Entries are well-formed and do not span multiple lines. - There are no empty lines in the log. - The log may contain thousands of lines. # Example: ```python log = [12:00:00] User: john_doe Action: LOGIN [12:10:00] User: jane_doe Action: UPLOAD file1.txt [12:15:00] User: john_doe Action: LOGOUT [12:20:00] User: john_doe Action: ERROR disk full [12:25:00] User: jane_doe Action: DOWNLOAD file2.txt [12:30:00] User: jane_doe Action: ERROR file not found result = analyze_log(log) # Expected result: { \\"error_times\\": [\\"12:20:00\\", \\"12:30:00\\"], \\"user_actions\\": { \\"john_doe\\": [\\"LOGIN\\", \\"LOGOUT\\", \\"ERROR disk full\\"], \\"jane_doe\\": [\\"UPLOAD file1.txt\\", \\"DOWNLOAD file2.txt\\", \\"ERROR file not found\\"] }, \\"total_uploads\\": 1, \\"fixed_log\\": \\"n[12:00:00] User: john_doe Action: LOGINn[12:10:00] User: jane_doe Action: UPLOAD file1.txtn[12:15:00] User: john_doe Action: LOGOUTn[12:20:00] User: john_doe Action: ISSUE disk fulln[12:25:00] User: jane_doe Action: DOWNLOAD file2.txtn[12:30:00] User: jane_doe Action: ISSUE file not foundn\\" } ``` # Implementation Requirement: - Use the `re` module to parse the log and extract the required information. ```python def analyze_log(log): # Your implementation here. pass ``` # Notes: - Ensure performance efficiency with respect to large logs. Consider using compiled regular expressions and other optimizations as needed. - Be thorough with edge cases such as multiple actions by the same user, no \'ERROR\' actions, etc.","solution":"import re def analyze_log(log): error_pattern = re.compile(r\'[(d{2}:d{2}:d{2})] User: (w+) Action: ERROR\') action_pattern = re.compile(r\'[(d{2}:d{2}:d{2})] User: (w+) Action: (.+)\') upload_pattern = re.compile(r\'UPLOAD\') error_times = [] user_actions = {} total_uploads = 0 fixed_log = log lines = log.splitlines() for line in lines: error_match = error_pattern.search(line) action_match = action_pattern.search(line) if error_match: timestamp = error_match.group(1) error_times.append(timestamp) if action_match: timestamp, username, action_description = action_match.groups() if \\"ERROR\\" in action_description: fixed_log = fixed_log.replace(line, line.replace(\\"ERROR\\", \\"ISSUE\\")) if username not in user_actions: user_actions[username] = [] user_actions[username].append(action_description) if upload_pattern.search(action_description): total_uploads += 1 return { \\"error_times\\": error_times, \\"user_actions\\": user_actions, \\"total_uploads\\": total_uploads, \\"fixed_log\\": fixed_log }"},{"question":"# Objective Your task is to demonstrate your understanding of the `wsgiref` package by implementing a simple WSGI-based web application. The application must handle HTTP GET and POST requests efficiently and use several utilities provided by the `wsgiref` module. # Task Implement a web application that allows users to view and submit their favorite quotes. Your application should: 1. **Serve a Form on GET request**: - Provide a simple HTML form where users can submit their name and favorite quote. - After submission, redirect the user back to the form page. 2. **Process Form Submission on POST request**: - Extract the user\'s name and favorite quote from the form data. - Store these records internally in a Python list while the server is running. 3. **Display Submitted Quotes**: - When users visit the form page, display all previously submitted quotes below the form. # Instructions 1. Use `wsgiref.simple_server.make_server` to create and run an HTTP server. 2. Use functions from `wsgiref.util` for handling the request environment and performing URL manipulations. 3. Utilize the `wsgiref.headers.Headers` class to manage HTTP response headers. 4. Ensure proper handling of GET and POST methods, managing form data, and rendering HTML content. # Input - No additional input is required aside from executing the script, except for user interaction via the web form. # Output - The HTML form served on the GET request. - Stored quotes displayed below the form. - Proper response and redirection after the POST request. # Constraints - The application should store quotes only in memory (within a Python list) and should reset when the server restarts. - You must use the `wsgiref` module utilities as specified. - Assumption: Name and quote are simple strings without special sanitization needs. Example Here is an example of how your application should work: 1. On visiting `http://localhost:8000`, the user sees a form to submit their name and favorite quote. 2. After entering and submitting the form, the application saves the data and redirects the user back to the same form. 3. The user sees their submitted quote along with any others previously submitted below the form. Code Skeleton ```python from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults, request_uri import cgi quotes = [] def quote_app(environ, start_response): setup_testing_defaults(environ) # Parse request method and URI method = environ[\'REQUEST_METHOD\'] uri = request_uri(environ) headers = [(\'Content-type\', \'text/html; charset=utf-8\')] if method == \'GET\': start_response(\'200 OK\', headers) return generate_form() elif method == \'POST\': # Handle form submission form = cgi.FieldStorage(fp=environ[\'wsgi.input\'], environ=environ, keep_blank_values=True) name = form.getvalue(\'name\') quote = form.getvalue(\'quote\') if name and quote: quotes.append((name, quote)) # Redirect to prevent resubmission start_response(\'303 See Other\', [(\'Location\', uri)]) return [b\\"\\"] def generate_form(): form_html = <html> <head> <title>Favorite Quotes</title> </head> <body> <h1>Favorite Quotes</h1> <form method=\\"post\\"> Name: <input type=\\"text\\" name=\\"name\\"><br> Quote: <input type=\\"text\\" name=\\"quote\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> <h2>Submitted Quotes:</h2> {quotes} </body> </html> quote_list = \\"<ul>\\" + \\"\\".join(f\\"<li><b>{name}:</b> {quote}</li>\\" for name, quote in quotes) + \\"</ul>\\" return [form_html.format(quotes=quote_list).encode(\'utf-8\')] if __name__ == \'__main__\': with make_server(\'\', 8000, quote_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Test your application by running the script and visiting `http://localhost:8000` in your web browser, trying to submit and view quotes.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults, request_uri import cgi quotes = [] def quote_app(environ, start_response): setup_testing_defaults(environ) # Parse request method and URI method = environ[\'REQUEST_METHOD\'] uri = request_uri(environ) headers = [(\'Content-type\', \'text/html; charset=utf-8\')] if method == \'GET\': start_response(\'200 OK\', headers) return generate_form(environ) elif method == \'POST\': # Handle form submission form = cgi.FieldStorage(fp=environ[\'wsgi.input\'], environ=environ, keep_blank_values=True) name = form.getvalue(\'name\') quote = form.getvalue(\'quote\') if name and quote: quotes.append((name, quote)) # Redirect to prevent resubmission start_response(\'303 See Other\', [(\'Location\', uri)]) return [b\\"\\"] def generate_form(environ): form_html = <html> <head> <title>Favorite Quotes</title> </head> <body> <h1>Favorite Quotes</h1> <form method=\\"post\\"> Name: <input type=\\"text\\" name=\\"name\\"><br> Quote: <input type=\\"text\\" name=\\"quote\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> <h2>Submitted Quotes:</h2> {quotes} </body> </html> quote_list = \\"<ul>\\" + \\"\\".join(f\\"<li><b>{name}:</b> {quote}</li>\\" for name, quote in quotes) + \\"</ul>\\" return [form_html.format(quotes=quote_list).encode(\'utf-8\')] if __name__ == \'__main__\': with make_server(\'\', 8000, quote_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"**Title: Encode and Decode Utility Using `binascii`** **Objective:** To assess the understanding of the `binascii` module\'s functions for encoding and decoding binary data using various ASCII-encoded binary representations. **Problem Statement:** You are required to implement two functions `encode_data` and `decode_data`, which will handle the conversion between binary data and various ASCII-encoded binary formats specified by the user. The formats supported are \'uu\', \'base64\', \'hex\', and \'qp\' (quoted-printable). 1. **Function 1: `encode_data(data: bytes, format: str) -> str`** - Parameters: - `data`: A bytes object representing the binary data to be encoded. - `format`: A string representing the desired encoding format (\'uu\', \'base64\', \'hex\', \'qp\'). - Returns: - A string representing the encoded ASCII data. - Constraints: - The `format` string will always be one of [\'uu\', \'base64\', \'hex\', \'qp\']. 2. **Function 2: `decode_data(ascii_data: str, format: str) -> bytes`** - Parameters: - `ascii_data`: A string representing the binary data encoded in ASCII format. - `format`: A string representing the encoding format (\'uu\', \'base64\', \'hex\', \'qp\'). - Returns: - A bytes object representing the decoded binary data. - Constraints: - The `format` string will always be one of [\'uu\', \'base64\', \'hex\', \'qp\']. **Examples:** ```python # Example 1 data = b\\"hello world\\" encoded_data = encode_data(data, \'base64\') print(encoded_data) # Output: \'aGVsbG8gd29ybGQ=n\' decoded_data = decode_data(encoded_data, \'base64\') print(decoded_data) # Output: b\'hello world\' # Example 2 data = b\\"binary data\\" encoded_data = encode_data(data, \'hex\') print(encoded_data) # Output: \'62696e6172792064617461\' decoded_data = decode_data(encoded_data, \'hex\') print(decoded_data) # Output: b\'binary data\' ``` **Notes:** 1. Use the functions from the `binascii` module to perform the encoding and decoding. 2. Make sure to handle the data transformation accurately to ensure no data is lost in encoding or decoding processes. 3. Assume the input data for encoding will not exceed the size limits imposed by each specific encoding format.","solution":"import binascii def encode_data(data: bytes, format: str) -> str: Encodes binary data into specified ASCII encoded format. :param data: binary data to be encoded :param format: encoding format (\'uu\', \'base64\', \'hex\', \'qp\') :returns: ASCII encoded string if format == \'uu\': return binascii.b2a_uu(data).decode(\'ascii\') elif format == \'base64\': return binascii.b2a_base64(data).decode(\'ascii\') elif format == \'hex\': return binascii.b2a_hex(data).decode(\'ascii\') elif format == \'qp\': return binascii.b2a_qp(data).decode(\'ascii\') def decode_data(ascii_data: str, format: str) -> bytes: Decodes ASCII encoded string into binary data. :param ascii_data: ASCII encoded data :param format: encoding format (\'uu\', \'base64\', \'hex\', \'qp\') :returns: binary data if format == \'uu\': return binascii.a2b_uu(ascii_data) elif format == \'base64\': return binascii.a2b_base64(ascii_data) elif format == \'hex\': return binascii.a2b_hex(ascii_data) elif format == \'qp\': return binascii.a2b_qp(ascii_data)"},{"question":"**Objective:** You are required to implement a Python class, `SimpleHttpClient`, that utilizes the `http.client` module to handle HTTP GET, POST, HEAD, and PUT requests. This class should also handle proxy tunneling, error handling, and response parsing. **Class Definition:** ```python class SimpleHttpClient: def __init__(self, use_https=False, proxy_host=None, proxy_port=None): Initializes a SimpleHttpClient instance. :param use_https: Boolean to indicate whether to use HTTPS. :param proxy_host: Proxy server host. :param proxy_port: Proxy server port. pass def send_request(self, method, url, body=None, headers={}, response_as_string=True): Sends an HTTP request and retrieves the response. :param method: HTTP method (GET, POST, HEAD, PUT). :param url: URL to send the request to. :param body: Request body for POST and PUT methods. :param headers: Dictionary of HTTP headers. :param response_as_string: Boolean indicating if the response should be returned as a string. :return: Tuple containing status code, reason, and response body. pass def close_connection(self): Closes the HTTP connection. pass ``` **Requirements:** 1. The `__init__` method should initialize the connection with the given parameters. - If `use_https` is `True`, an `HTTPSConnection` should be used; otherwise, an `HTTPConnection` should be used. - If `proxy_host` and `proxy_port` are provided, the connection should tunnel through the specified proxy. 2. The `send_request` method should: - Send an HTTP request using the specified `method`. - Handle GET, POST, HEAD, and PUT methods. - Include the optional `body` and `headers` in the request. - Return a tuple containing the response\'s status code, reason, and body. If `response_as_string` is `True`, the body should be returned as a string, otherwise as bytes. 3. The `close_connection` method should close the open connection. 4. Implement proper error handling by catching relevant exceptions from the `http.client` module and returning appropriate error messages. 5. Provide an example usage of the `SimpleHttpClient` class to demonstrate these functionalities. **Example Usage:** ```python # Create an HTTP client client = SimpleHttpClient(use_https=True) # Send a GET request status, reason, response = client.send_request(\\"GET\\", \\"https://www.python.org\\") print(status, reason) print(response[:200]) # Print first 200 characters of the response # Send a POST request body = \\"param1=value1&param2=value2\\" headers = {\\"Content-type\\": \\"application/x-www-form-urlencoded\\"} status, reason, response = client.send_request(\\"POST\\", \\"https://httpbin.org/post\\", body, headers) print(status, reason) print(response[:200]) # Print first 200 characters of the response # Close the client connection client.close_connection() ``` **Constraints:** - You should not use any other libraries for HTTP requests (e.g., `requests` or `urllib`). - Manage exceptions properly, providing clear error messages. - Ensure SSL context is properly configured for HTTPS connections. **Performance Requirements:** - Ensure that the connection is efficient and promptly handled for typical web request sizes (< 1MB).","solution":"import http.client from urllib.parse import urlparse class SimpleHttpClient: def __init__(self, use_https=False, proxy_host=None, proxy_port=None): Initializes a SimpleHttpClient instance. :param use_https: Boolean to indicate whether to use HTTPS. :param proxy_host: Proxy server host. :param proxy_port: Proxy server port. self.use_https = use_https self.proxy_host = proxy_host self.proxy_port = proxy_port self.connection = None def send_request(self, method, url, body=None, headers={}, response_as_string=True): Sends an HTTP request and retrieves the response. :param method: HTTP method (GET, POST, HEAD, PUT). :param url: URL to send the request to. :param body: Request body for POST and PUT methods. :param headers: Dictionary of HTTP headers. :param response_as_string: Boolean indicating if the response should be returned as a string. :return: Tuple containing status code, reason, and response body. parsed_url = urlparse(url) path = parsed_url.path or \'/\' try: # Select Connection Type based on inputs if self.use_https: conn_type = http.client.HTTPSConnection else: conn_type = http.client.HTTPConnection if self.proxy_host and self.proxy_port: self.connection = conn_type(self.proxy_host, self.proxy_port) self.connection.set_tunnel(parsed_url.hostname, parsed_url.port or (443 if self.use_https else 80)) else: self.connection = conn_type(parsed_url.hostname, parsed_url.port or (443 if self.use_https else 80)) # Send Request self.connection.request(method, path, body, headers) response = self.connection.getresponse() response_body = response.read() if response_as_string: response_body = response_body.decode() return response.status, response.reason, response_body except Exception as e: return None, str(e), None def close_connection(self): Closes the HTTP connection. if self.connection: self.connection.close()"},{"question":"**Question: Implement a TorchScript-compatible PyTorch Module** You are required to implement a custom PyTorch module that can be converted to TorchScript. The module should contain a simple linear layer and demonstrate the use of TorchScript\'s type annotations and class definitions. # Requirements: 1. Create a class `SimpleLinearModel` that inherits from `torch.nn.Module`. 2. The class should have: - An `__init__` method that initializes the module with one parameter `input_dim` which defines the input dimension of the linear layer. - A forward method that takes a single argument `x` of type `torch.Tensor` and returns the output of the linear layer. 3. Use appropriate TorchScript annotations for methods and attributes. 4. Ensure that the class can be successfully converted to TorchScript using `torch.jit.script`. # Constraints: - The input tensor `x` will always be a 2D tensor (i.e., batch of samples). - The `input_dim` will always be a positive integer. # Example: ```python import torch class SimpleLinearModel(torch.nn.Module): def __init__(self, input_dim: int): super(SimpleLinearModel, self).__init__() self.linear = torch.nn.Linear(input_dim, 1) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.linear(x) # Convert the model to TorchScript model = SimpleLinearModel(5) scripted_model = torch.jit.script(model) # Test the scripted model x = torch.randn(3, 5) # batch of 3 samples, each with 5 features output = scripted_model(x) print(output) ``` Your task is to implement the `SimpleLinearModel` class as described above.","solution":"import torch class SimpleLinearModel(torch.nn.Module): def __init__(self, input_dim: int): super(SimpleLinearModel, self).__init__() self.linear = torch.nn.Linear(input_dim, 1) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.linear(x)"},{"question":"**Objective:** Your task is to create a Python script that fetches data from a given URL using the `urllib.request` module and handles various HTTP-related scenarios, including error handling and custom request headers. **Problem Statement:** You are provided with a server URL that you need to interact with using HTTP requests. Your task is to write a function `fetch_server_data(url, data=None, headers=None)` that performs the following: 1. **GET Request**: When `data` is not provided, the function should perform a GET request. 2. **POST Request**: When `data` is provided, it should encode the data as URL-encoded format and perform a POST request. 3. **Custom Headers**: If custom headers are provided, they should be included in the request. 4. **Handle Exceptions**: The function should handle `URLError` and `HTTPError` exceptions and print relevant messages. 5. **Return Content**: The function should return the content of the response if the request is successful, or `None` if an error occurs. 6. **Print Information**: If the request is successful, print the following: - Effective URL after any redirects. - Response headers. **Function Signature:** ```python def fetch_server_data(url: str, data: dict = None, headers: dict = None) -> str: pass ``` **Input:** - `url` (str): The URL of the server to fetch data from. - `data` (dict, optional): Dictionary of data to be sent in the POST request. Defaults to `None`. - `headers` (dict, optional): Dictionary of custom headers to include in the request. Defaults to `None`. **Output:** - Returns a string containing the response content if the request is successful, otherwise returns `None`. **Example Usage:** ```python url = \\"http://www.example.com\\" data = {\\"name\\": \\"John Doe\\", \\"location\\": \\"Earth\\"} headers = {\\"User-Agent\\": \\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\\"} response_content = fetch_server_data(url, data, headers) if response_content is not None: print(response_content) ``` **Constraints:** - The function should handle timeouts and network issues gracefully. - Ensure proper encoding of data. # Notes: - Use the `urllib.request` module for making HTTP requests. - Use `urllib.parse` for encoding data. Good Luck!","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError def fetch_server_data(url: str, data: dict = None, headers: dict = None) -> str: Fetch data from the server using GET or POST request based on the provided data. Handle exceptions and print the effective URL and response headers. try: if data is not None: data = urllib.parse.urlencode(data).encode() req = urllib.request.Request(url, data=data, headers=headers or {}) else: req = urllib.request.Request(url, headers=headers or {}) with urllib.request.urlopen(req) as response: content = response.read().decode() print(\\"Effective URL:\\", response.url) print(\\"Response headers:\\", response.headers) return content except HTTPError as e: print(f\\"HTTP Error: {e.code} - {e.reason}\\") except URLError as e: print(f\\"URL Error: {e.reason}\\") return None"},{"question":"You are required to demonstrate your understanding of the `torch.futures` package by implementing a sequence of asynchronous operations. # Problem Statement Write a Python function `compute_async_squares` which calculates the squares of a given list of integers asynchronously. Use the `torch.futures` package to carry out these computations. Function Signature ```python import torch import time def compute_async_squares(numbers: List[int]) -> List[int]: pass ``` # Input - `numbers`: A list of integers. Example: `[1, 2, 3, 4, 5]` # Output - A list of integers where each element is the square of the corresponding input element. The output list must maintain the order of the input list. # Constraints - Your implementation should use the `torch.futures.Future` class. - Simulate a delayed computation by sleeping for 0.1 seconds before returning the square of each number. # Guidelines 1. Create a helper function `square_with_delay` that takes an integer and returns a `torch.futures.Future` object after a delay of 0.1 seconds. 2. Use the `collect_all` function to gather futures. 3. Use the `wait_all` function to wait for all futures to complete. 4. Extract the results and ensure they are in the same order as the input list. # Example ```python numbers = [1, 2, 3, 4, 5] assert compute_async_squares(numbers) == [1, 4, 9, 16, 25] ``` # Implementation Hints - Use `torch.futures.Future` and related utility functions for handling asynchronous computations. - Ensure that the results are gathered in a non-blocking fashion initially and only block when collecting the final results.","solution":"import torch import time from typing import List def square_with_delay(number: int) -> torch.futures.Future: future = torch.futures.Future() def delayed_computation(): time.sleep(0.1) future.set_result(number ** 2) # This simulates an asynchronous operation by calling delayed_computation in a separate thread. torch.jit._fork(delayed_computation) return future def compute_async_squares(numbers: List[int]) -> List[int]: futures = [square_with_delay(n) for n in numbers] results = torch.futures.wait_all(futures) return results"},{"question":"# Question: Customizing Python Interactive Environment Python allows for extensive customization of the interactive environment, which can be beneficial for automating routine tasks or setting up a preferred working environment. In this exercise, you are required to write a Python script that sets up a custom interactive environment. Part 1: Custom Startup File You need to create a startup script that: 1. Sets a custom greeting message to be displayed every time the Python interactive shell starts. 2. Changes the primary prompt (`PS1`) to `MyPython >>> ` and the secondary prompt (`PS2`) to `... `. 3. Imports the `datetime` module automatically so that the user does not need to import it manually. Write a function `create_startup_file` that generates this startup script and saves it to a file specified by the environment variable `PYTHONSTARTUP`. ```python import os def create_startup_file(filename: str) -> None: Creates a custom Python startup script. Args: filename (str): The name of the file to save the startup commands. The startup script should set a greeting message, change the prompts, and import the datetime module. content = import sys import datetime # Set a greeting message print(\\"Welcome to the custom Python interactive shell!\\") # Change the primary and secondary prompts sys.ps1 = \'MyPython >>> \' sys.ps2 = \'... \' # Automatically import the datetime module import datetime with open(filename, \'w\') as file: file.write(content) # Set the PYTHONSTARTUP environment variable to point to this file os.environ[\'PYTHONSTARTUP\'] = filename # Example usage # create_startup_file(\'custom_startup.py\') ``` Part 2: Custom Executable Script Create another function `create_executable_script` that: 1. Creates a simple Python script that prints `Hello, Executable World!`. 2. Sets the script to be executable (you may use `os.chmod` assuming a Unix-like environment). ```python def create_executable_script(filename: str) -> None: Creates a simple executable Python script. Args: filename (str): The name of the file to save the script. The script should print \\"Hello, Executable World!\\" when run. content = #!/usr/bin/env python3 print(\\"Hello, Executable World!\\") with open(filename, \'w\') as file: file.write(content) # Make the script executable os.chmod(filename, 0o755) # Example usage # create_executable_script(\'hello_executable.py\') ``` # Instructions: 1. Implement the `create_startup_file` function as described. 2. Implement the `create_executable_script` function as described. 3. Ensure that the created startup script is automatically run by the Python interactive shell by setting the `PYTHONSTARTUP` variable. 4. Ensure that the created script is executable and can run independently to print the desired message. Output: - Verify your implementation by manually inspecting the created startup file and executable script. - Start a new Python interactive shell session and confirm that your customizations (greeting message, prompt changes, and automatic import) are in effect. - Execute the created script from the command line and confirm it prints `Hello, Executable World!`.","solution":"import os def create_startup_file(filename: str) -> None: Creates a custom Python startup script. Args: filename (str): The name of the file to save the startup commands. The startup script should set a greeting message, change the prompts, and import the datetime module. content = import sys import datetime # Set a greeting message print(\\"Welcome to the custom Python interactive shell!\\") # Change the primary and secondary prompts sys.ps1 = \'MyPython >>> \' sys.ps2 = \'... \' # Automatically import the datetime module import datetime with open(filename, \'w\') as file: file.write(content) # Set the PYTHONSTARTUP environment variable to point to this file os.environ[\'PYTHONSTARTUP\'] = filename def create_executable_script(filename: str) -> None: Creates a simple executable Python script. Args: filename (str): The name of the file to save the script. The script should print \\"Hello, Executable World!\\" when run. content = #!/usr/bin/env python3 print(\\"Hello, Executable World!\\") with open(filename, \'w\') as file: file.write(content) # Make the script executable os.chmod(filename, 0o755)"},{"question":"# Question URL Browser Automation In this task, you will implement a function to automate the opening of a list of URLs in a web browser under different scenarios. You are expected to use the `webbrowser` module. Task Write a function `automate_browser(urls: List[str], new_window: bool = False, new_tab: bool = False) -> None` that takes the following parameters: - `urls` (List[str]): A list of URLs to be opened. - `new_window` (bool): A flag indicating if each URL should be opened in a new window (default is `False`). - `new_tab` (bool): A flag indicating if each URL should be opened in a new tab (default is `False`). Function Behavior 1. If both `new_window` and `new_tab` are `False`, open each URL in the same browser window. 2. If `new_window` is `True`, open each URL in a new browser window. 3. If `new_tab` is `True`, open each URL in a new tab. 4. Raise a `ValueError` if both `new_window` and `new_tab` are `True`. Input - `urls`: List of strings containing the URLs to be opened. Each URL is a valid web address. - `new_window`: Boolean flag, default is `False`. - `new_tab`: Boolean flag, default is `False`. Output - None. The function opens the URLs in the browser as specified by the input parameters. Constraints - If `urls` is empty, the function should do nothing. - Ensure that the function handles invalid URLs gracefully, by catching exceptions raised by the `webbrowser` module and printing \\"Failed to open URL: [URL]\\" for each failed attempt. Example ```python urls = [\\"https://www.python.org\\", \\"https://docs.python.org/3/\\"] automate_browser(urls) # Opens both URLs in the same browser window. automate_browser(urls, new_window=True) # Opens each URL in a new browser window. automate_browser(urls, new_tab=True) # Opens each URL in a new tab. automate_browser(urls, new_window=True, new_tab=True) # Raises ValueError. ``` ```python # Function to be implemented by the student def automate_browser(urls: List[str], new_window: bool = False, new_tab: bool = False) -> None: pass ```","solution":"import webbrowser from typing import List def automate_browser(urls: List[str], new_window: bool = False, new_tab: bool = False) -> None: Opens a list of URLs in a web browser based on the specified options. Parameters: - urls: List of strings containing the URLs to be opened. - new_window: Boolean flag indicating if each URL should be opened in a new window (default is False). - new_tab: Boolean flag indicating if each URL should be opened in a new tab (default is False). Raises: - ValueError: If both new_window and new_tab are True. if new_window and new_tab: raise ValueError(\\"Both new_window and new_tab cannot be True at the same time.\\") for url in urls: try: if new_window: webbrowser.open(url, new=1) # 1 opens in a new window elif new_tab: webbrowser.open(url, new=2) # 2 opens in a new tab else: webbrowser.open(url, new=0) # 0 opens in the same window except Exception as e: print(f\\"Failed to open URL: {url}, Error: {e}\\")"},{"question":"**XML Data Processing with `xml.dom.pulldom`** # Problem Description Write a Python function `extract_titles_from_xml(xml_string: str) -> List[str]` that takes an XML string as input and extracts the text inside all `<title>` elements using the `xml.dom.pulldom` module. Return these titles as a list of strings. # Detailed Requirements 1. Use the `xml.dom.pulldom.parseString` function to create a DOM event stream from the input XML string. 2. Iterate through the events to identify the start of `<title>` elements. 3. Expand each `<title>` element node to get its full content. 4. Extract and collect the text content of each expanded `<title>` element into a list. 5. Return the list of title strings. # Input - `xml_string`: A string containing XML data. - Example: ```xml \'\'\' <library> <book> <title>Python Programming</title> </book> <book> <title>Learning XML</title> </book> </library> \'\'\' ``` # Output - Return a list of strings where each string corresponds to the text content of a `<title>` element. - For the example above, the output should be: ```python [\'Python Programming\', \'Learning XML\'] ``` # Constraints - Assume the input XML string is well-formed. - You must use the `xml.dom.pulldom` module for parsing. # Example ```python def extract_titles_from_xml(xml_string: str) -> List[str]: from xml.dom import pulldom doc = pulldom.parseString(xml_string) titles = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'title\': doc.expandNode(node) titles.append(node.childNodes[0].nodeValue) return titles # Example usage: xml_string = \'\'\' <library> <book> <title>Python Programming</title> </book> <book> <title>Learning XML</title> </book> </library> \'\'\' print(extract_titles_from_xml(xml_string)) # Output: [\'Python Programming\', \'Learning XML\'] ``` # Notes 1. Consider edge cases such as no `<title>` elements in the XML. 2. Handle multiple levels of nested elements gracefully. Ensure your function is well-documented and includes type hints.","solution":"from typing import List from xml.dom import pulldom def extract_titles_from_xml(xml_string: str) -> List[str]: Extracts titles from the given XML string. Args: xml_string (str): A string containing XML data. Returns: List[str]: A list of titles extracted from the XML. doc = pulldom.parseString(xml_string) titles = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'title\': doc.expandNode(node) titles.append(node.childNodes[0].nodeValue) return titles"},{"question":"Implement a Python function called `parse_arguments` that uses the `getopt` module to parse command-line options. The function should take a list of arguments as input and return a dictionary with the parsed options and their corresponding values. It should also handle errors gracefully by raising a custom exception with a clear error message in case of any parsing issues. # Function Signature ```python def parse_arguments(arguments: list) -> dict: pass ``` # Input - `arguments`: A list of strings representing command-line arguments (e.g., `[\\"--output=output.txt\\", \\"-v\\", \\"input.txt\\", \\"config.cfg\\"]`). # Output - The function should return a dictionary where: - Keys are the names of the options (short or long without the prefixes). - Values are the corresponding arguments or `True` for boolean flags. # Constraints - The function should support the following options: - Short options: `-h` (help), `-o` (output), `-v` (verbose) - Long options: `--help`, `--output`, `--verbose` - If an invalid option is provided, raise a `ValueError` with a message indicating the invalid option. - If a required argument for an option is missing, raise a `ValueError` with a message indicating which option is missing its required argument. # Example ```python args = [\\"--output=output.txt\\", \\"-v\\", \\"input.txt\\", \\"config.cfg\\"] result = parse_arguments(args) assert result == {\\"output\\": \\"output.txt\\", \\"verbose\\": True, \\"args\\": [\\"input.txt\\", \\"config.cfg\\"]} args = [\\"-h\\"] result = parse_arguments(args) assert result == {\\"help\\": True, \\"args\\": []} args = [\\"--unknown\\"] # This should raise ValueError: \\"option --unknown not recognized\\" parse_arguments(args) ``` # Notes - Ensure your function handles both short (`-o value`) and long (`--output=value`) options correctly. - The `args` in the output dictionary should include any non-option arguments remaining after parsing. - Use the `sys.exit(2)` command to terminate the script with an appropriate message if an error is raised.","solution":"import getopt class ArgumentParseException(Exception): pass def parse_arguments(arguments): try: opts, args = getopt.getopt(arguments, \\"hvo:\\", [\\"help\\", \\"verbose\\", \\"output=\\"]) except getopt.GetoptError as err: raise ArgumentParseException(str(err)) result = {\\"args\\": args} for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): result[\\"help\\"] = True elif opt in (\\"-v\\", \\"--verbose\\"): result[\\"verbose\\"] = True elif opt in (\\"-o\\", \\"--output\\"): result[\\"output\\"] = arg else: raise ArgumentParseException(f\\"option {opt} not recognized\\") return result"},{"question":"# FTP Operations using `ftplib` You are required to create a Python function that performs a series of operations on an FTP server. The goal is to automate the process of connecting to an FTP server, navigating to a specific directory, downloading a file, uploading a new file, and cleaning up. Function Signature ```python def ftp_operations(host: str, user: str, password: str, directory: str, download_file: str, upload_file: str, upload_content: bytes) -> str: pass ``` Input - `host` (str): The FTP server address. - `user` (str): Username for FTP login. - `password` (str): Password for FTP login. - `directory` (str): The directory on the FTP server to navigate to. - `download_file` (str): The name of the file to download from the FTP server. - `upload_file` (str): The name of the file to upload to the FTP server. - `upload_content` (bytes): The content to write into the file to be uploaded. Output - Returns a status message as a string. If successful, returns `\\"Operations completed successfully\\"`. If there is any error, return an appropriate error message. Constraints - You must handle any exceptions related to FTP operations and return a relevant error message. - Ensure that the connection to the FTP server is closed properly in all cases. Steps to Implement 1. **Connect to the FTP Server**: Use the provided host, user, and password. 2. **Navigate to the Specified Directory**: Change the working directory to the specified path. 3. **Download the Specified File**: Retrieve the specified file and save it locally. 4. **Upload a New File**: Create a new file with the provided content and upload it to the FTP server in binary mode. 5. **Clean Up**: Delete the uploaded file from the server if any operation fails. 6. **Close Connection**: Ensure the connection to the server is closed properly, even if an error occurs. Example Code ```python from ftplib import FTP, error_reply, error_temp, error_perm, error_proto def ftp_operations(host: str, user: str, password: str, directory: str, download_file: str, upload_file: str, upload_content: bytes) -> str: try: # Connect to the FTP server ftp = FTP(host) ftp.login(user, passwd=password) # Change into the specified directory ftp.cwd(directory) # Download the specified file with open(download_file, \'wb\') as local_file: ftp.retrbinary(f\'RETR {download_file}\', local_file.write) # Upload the file with provided content with open(upload_file, \'wb\') as local_file: local_file.write(upload_content) with open(upload_file, \'rb\') as local_file: ftp.storbinary(f\'STOR {upload_file}\', local_file) # Operation successful return \\"Operations completed successfully\\" except (error_reply, error_temp, error_perm, error_proto) as e: # Handle FTP-specific errors ftp.quit() return f\\"FTP error: {e}\\" except OSError as e: # Handle OS-specific errors return f\\"OS error: {e}\\" except Exception as e: # Handle any other general exceptions return f\\"An error occurred: {e}\\" finally: # Ensure the FTP connection is closed if \'ftp\' in locals(): ftp.quit() # Clean up uploaded file in case of errors try: if ftp and not ftp.size(upload_file): ftp.delete(upload_file) except: pass ``` Your task is to fill in the `ftp_operations` function to perform the described tasks using the `ftplib` module.","solution":"import ftplib def ftp_operations(host: str, user: str, password: str, directory: str, download_file: str, upload_file: str, upload_content: bytes) -> str: Connects to an FTP server, navigates to a directory, downloads a file, uploads a new file, and returns a status message. Parameters: host (str): The FTP server address. user (str): Username for FTP login. password (str): Password for FTP login. directory (str): The directory on the FTP server to navigate to. download_file (str): The name of the file to download from the FTP server. upload_file (str): The name of the file to upload to the FTP server. upload_content (bytes): The content to write into the file to be uploaded. Returns: str: Status message indicating the result of the operations. ftp = None try: # Connect to the FTP server ftp = ftplib.FTP(host) ftp.login(user, password) # Change into the specified directory ftp.cwd(directory) # Download the specified file with open(download_file, \'wb\') as local_file: ftp.retrbinary(f\'RETR {download_file}\', local_file.write) # Upload the file with provided content from io import BytesIO upload_stream = BytesIO(upload_content) ftp.storbinary(f\'STOR {upload_file}\', upload_stream) # Operation successful return \\"Operations completed successfully\\" except ftplib.all_errors as e: # Handle FTP-specific errors return f\\"FTP error: {e}\\" except Exception as e: # Handle any other general exceptions return f\\"An error occurred: {e}\\" finally: # Ensure the FTP connection is closed if ftp is not None: ftp.quit()"},{"question":"**Objective:** Implement a Python class using the `gettext` module that supports multilingual translations with contexts, allows switching of active languages, and handles both singular and plural messages. **Problem Statement:** You need to create a `TranslationManager` class for a hypothetical application that supports internationalization (I18N) and localization (L10N). This class should encapsulate functionalities to set up multiple languages, switch between them, and fetch the appropriate translations based on message contexts and pluralization rules. **Requirements:** 1. **Class Definition**: Create a class `TranslationManager`. 2. **Initialization**: The class should be initialized with `domain`, `localedir`, and a list of languages. 3. **Switch Language**: Implement a method `switch_language` to change the current active language. 4. **Gettext Methods**: - `gettext(message, context=None)`: Fetch the translation of a message with optional context. - `ngettext(singular, plural, n, context=None)`: Fetch the translation considering plural forms with optional context. 5. **Deferred Translation**: Implement a mechanism to mark strings for translation that can be deferred until needed. **Input Format:** - Initialization parameters: `domain` (string), `localedir` (string), and `languages` (list of strings). - Method `switch_language(language: str) -> None`: Change the active language. - Method `gettext(message: str, context: Optional[str] = None) -> str`: Fetches the translation for the given message and context. - Method `ngettext(singular: str, plural: str, n: int, context: Optional[str] = None) -> str`: Fetches the translation considering plural forms and context. **Output Format:** - `TranslationManager.gettext` returns the translated string based on the current active language and context. - `TranslationManager.ngettext` returns the appropriate singular or plural translation based on the value of `n`. **Constraints:** - Ensure the class can handle missing translations gracefully by returning the original string when no translation is found. - Implement the class using the class-based API of the `gettext` module. # Example Usage: ```python # Assume locales are properly set up in the provided directory structure manager = TranslationManager(domain=\'messages\', localedir=\'/path/to/locale\', languages=[\'en\', \'fr\', \'es\']) # Switch to French manager.switch_language(\'fr\') # Fetch a simple translation with no context print(manager.gettext(\'Hello, world!\')) # Bonjour, le monde! # Fetch a translation with context print(manager.gettext(\'file\', context=\'noun\')) # fichier print(manager.gettext(\'file\', context=\'verb\')) # déposer # Fetch plural translation print(manager.ngettext(\'There is one apple\', \'There are {} apples\', 5)) # Il y a 5 pommes # Deferred translation animals = [\'mollusk\', \'albatross\', \'rat\', \'penguin\', \'python\'] print([manager.gettext(a) for a in animals]) # [\'mollusque\', \'albatros\', \'rat\', \'pingouin\', \'python\'] ``` # Code Skeleton: ```python import gettext class TranslationManager: def __init__(self, domain, localedir, languages): self.domain = domain self.localedir = localedir self.languages = languages self.translation_objects = {} self.current_translation = None self._load_translations() def _load_translations(self): for lang in self.languages: self.translation_objects[lang] = gettext.translation( domain=self.domain, localedir=self.localedir, languages=[lang], fallback=True ) def switch_language(self, language): if language in self.translation_objects: self.current_translation = self.translation_objects[language] self.current_translation.install() def gettext(self, message, context=None): if context: return self.current_translation.pgettext(context, message) return self.current_translation.gettext(message) def ngettext(self, singular, plural, n, context=None): if context: return self.current_translation.npgettext(context, singular, plural, n) return self.current_translation.ngettext(singular, plural, n) ``` **Note**: Make sure the locale directories and catalog files are set up correctly for the translations to work.","solution":"import gettext class TranslationManager: def __init__(self, domain, localedir, languages): self.domain = domain self.localedir = localedir self.languages = languages self.translation_objects = {} self.current_translation = None self._load_translations() def _load_translations(self): for lang in self.languages: self.translation_objects[lang] = gettext.translation( domain=self.domain, localedir=self.localedir, languages=[lang], fallback=True ) def switch_language(self, language): if language in self.translation_objects: self.current_translation = self.translation_objects[language] self.current_translation.install() def gettext(self, message, context=None): if context: return self.current_translation.pgettext(context, message) return self.current_translation.gettext(message) def ngettext(self, singular, plural, n, context=None): if context: return self.current_translation.npgettext(context, singular, plural, n) return self.current_translation.ngettext(singular, plural, n)"},{"question":"# Objective Write a Python script that uses the `modulefinder` module to analyze another Python script and produce a detailed report of the imported modules. # Task 1. Create a Python function named `analyze_script` that takes a file path (string) as input. 2. Use the `ModuleFinder` class to analyze the specified script. 3. Your function should: - Print the names of all loaded modules and the first three global names from each. - Print the names of any modules that could not be imported. # Input - A string representing the file path of the Python script to be analyzed. # Output - Print two sections to stdout: - **Loaded Modules**: A list where each line contains the module name and the first three global names found in each module. - **Missing Modules**: A list of module names that could not be imported. # Example Suppose you have a script named `test_script.py` with the following content: ```python import math import non_existent_module from os import path ``` Your function, when called with `analyze_script(\'test_script.py\')`, should produce an output similar to: ``` Loaded Modules: math: pi, sin, cos sys: path, argv, stdin os: name, path, getcwd ... --------------------------------------------------- Missing Modules: non_existent_module ``` # Constraints 1. Assume that the input file path points to a valid Python script. 2. The output format should match the example provided above. 3. You are allowed to use the `modulefinder` module as demonstrated. **Note**: Ensure your solution handles cases where the script imports modules conditionally (within try/except blocks).","solution":"import modulefinder def analyze_script(file_path): finder = modulefinder.ModuleFinder() finder.run_script(file_path) print(\\"Loaded Modules:\\") for name, module in finder.modules.items(): global_names = sorted(module.globalnames.keys())[:3] print(f\\"{name}: {\', \'.join(global_names)}\\") print(\\"---------------------------------------------------\\") print(\\"Missing Modules:\\") for name in finder.badmodules.keys(): print(name)"},{"question":"# Complex Database Search System Objective: Create a Python class `Database` that simulates a simple in-memory database, which supports basic operations such as insertion, deletion, and complex retrieval based on specific queries. Details: **Class `Database`:** 1. **Initialization (`__init__` method):** - Database should initialize with an empty list to store records. Each record should be a dictionary containing `name` (string), `age` (integer), and `occupation` (string). 2. **Insert Method (`insert`):** - This method takes 3 parameters: `name` (string), `age` (integer), and `occupation` (string). - It should create a dictionary with these values and add it to the internal storage list. 3. **Delete Method (`delete`):** - This method takes a single parameter: `name` (string). - It should remove all records matching the given `name`. 4. **Search Method (`search`):** - This method takes no parameters and should return a list of all records. 5. **Complex Search Method (`complex_search`):** - This method takes three optional keyword arguments: `name`, `min_age`, and `max_age`. - It should return all records that: - Match the given `name` (if provided). - Have an age greater than or equal to `min_age` (if provided). - Have an age less than or equal to `max_age` (if provided). Constraints: 1. All methods should handle edge cases gracefully. In case of an invalid or missing parameter, they should return an appropriate error message or value. 2. Data integrity must be maintained during insertions and deletions. 3. Performance is not the primary focus, but the code should be reasonably efficient. Example Usage: ```python # Initialize the database db = Database() # Insert records db.insert(\\"Alice\\", 30, \\"Engineer\\") db.insert(\\"Bob\\", 24, \\"Doctor\\") db.insert(\\"Alice\\", 28, \\"Artist\\") # Search all records print(db.search()) # Output: [{\'name\': \'Alice\', \'age\': 30, \'occupation\': \'Engineer\'}, {\'name\': \'Bob\', \'age\': 24, \'occupation\': \'Doctor\'}, {\'name\': \'Alice\', \'age\': 28, \'occupation\': \'Artist\'}] # Complex search print(db.complex_search(name=\\"Alice\\", min_age=25)) # Output: [{\'name\': \'Alice\', \'age\': 30, \'occupation\': \'Engineer\'}] # Delete records db.delete(\\"Alice\\") # Search all records after deletion print(db.search()) # Output: [{\'name\': \'Bob\', \'age\': 24, \'occupation\': \'Doctor\'}] ``` Implement the `Database` class and its methods based on the above specifications.","solution":"class Database: def __init__(self): self.records = [] def insert(self, name, age, occupation): record = { \\"name\\": name, \\"age\\": age, \\"occupation\\": occupation } self.records.append(record) def delete(self, name): self.records = [record for record in self.records if record[\\"name\\"] != name] def search(self): return self.records def complex_search(self, name=None, min_age=None, max_age=None): results = self.records if name is not None: results = [record for record in results if record[\\"name\\"] == name] if min_age is not None: results = [record for record in results if record[\\"age\\"] >= min_age] if max_age is not None: results = [record for record in results if record[\\"age\\"] <= max_age] return results"},{"question":"**Question: Creating Count Plots using Seaborn** You are tasked with demonstrating your understanding of Seaborn\'s functionality through the creation and customization of count plots. Follow the steps below to complete the task: 1. **Set Up Seaborn** - Import the `seaborn` library as `sns`. - Set the theme using `sns.set_theme(style=\\"whitegrid\\")`. 2. **Load the Titanic Dataset** - Use Seaborn\'s `load_dataset()` function to load the Titanic dataset. Store this dataset in a variable named `titanic`. 3. **Create and Customize Plots** Implement a function `create_count_plots()` that performs the following: - Create a count plot that shows the count of different classes (`\\"class\\"`) on the Titanic. - Create a count plot that shows the count of different classes (`\\"class\\"`) on the Titanic, grouped by the survival status (`\\"survived\\"`). - Create a count plot that normalizes the counts to show percentages of different classes, grouped by the survival status. # Input - No direct input for the function; the function will utilize the pre-loaded Titanic dataset. # Output - The function should not return any value but should display each of the three plots as described. # Constraints - Use the Seaborn library for creating and customizing the plots. - Utilize the Titanic dataset loaded from Seaborn. - Ensure the plots are displayed inline. # Example Execution ```python def create_count_plots(): import seaborn as sns sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") # First Plot: Count of classes sns.countplot(titanic, x=\\"class\\") # Second Plot: Count of classes grouped by survival sns.countplot(titanic, x=\\"class\\", hue=\\"survived\\") # Third Plot: Normalized count of classes grouped by survival sns.countplot(titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\") # Execute the function create_count_plots() ``` **Note:** Ensure that your plots are clearly labeled and easy to interpret.","solution":"import seaborn as sns def create_count_plots(): This function creates and displays three count plots based on the Titanic dataset: 1. Count of different classes. 2. Count of different classes grouped by survival status. 3. Normalized count of classes to show percentages, grouped by survival status. # Set the theme for the plots sns.set_theme(style=\\"whitegrid\\") # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # First Plot: Count of classes sns.countplot(data=titanic, x=\\"class\\") # Second Plot: Count of classes grouped by survival sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") # Third Plot: Normalized count of classes grouped by survival sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\")"},{"question":"# Asyncio Task Orchestrator You are to implement a simplified task orchestrator system using Python\'s asyncio module. The system will simulate multiple workers fetching and processing data from a shared queue, with support for task cancellation, synchronization, and error handling. Requirements 1. **Task Creation**: Implement a function `create_worker_tasks(n)` that creates `n` asynchronous worker tasks. Each worker should: - Fetch data from an asynchronous queue. - Process the data (simulated by an asyncio sleep). - Keep track of the number of processed tasks. 2. **Queue Management**: - Implement a function `fill_queue(data_list)` that fills an asynchronous queue with the provided `data_list`. - Implement a function `process_data(data)` that simulates processing data by sleeping for a random short duration. 3. **Synchronization**: Use locks to ensure that print statements and task count updates are not interleaved between different workers. 4. **Timeout Handling**: Ensure that workers do not run indefinitely. Implement a timeout for the task processing using `asyncio.wait_for()`. 5. **Cancellation**: Handle task cancellation gracefully and ensure that all tasks can be cancelled cleanly. Function Signatures - `async def create_worker_tasks(n: int, task_queue: asyncio.Queue, lock: asyncio.Lock, worker_task_count: list):` - `async def fill_queue(data_list: list, task_queue: asyncio.Queue):` - `async def process_data(data):` - `async def main(data_list: list, num_workers: int, timeout: float):` Example Usage ```python import asyncio import random async def process_data(data): await asyncio.sleep(random.uniform(0.1, 0.5)) # Simulate variable processing time return f\\"Processed {data}\\" async def create_worker_tasks(n, task_queue, lock, worker_task_count): async def worker(worker_id): count = 0 while True: try: data = await asyncio.wait_for(task_queue.get(), timeout=1.0) processed_data = await process_data(data) async with lock: print(f\\"Worker {worker_id}: {processed_data}\\") count += 1 task_queue.task_done() except asyncio.TimeoutError: break except asyncio.CancelledError: break worker_task_count[worker_id] = count tasks = [asyncio.create_task(worker(i)) for i in range(n)] return tasks async def fill_queue(data_list, task_queue): for data in data_list: await task_queue.put(data) async def main(data_list, num_workers, timeout): task_queue = asyncio.Queue() lock = asyncio.Lock() worker_task_count = [0] * num_workers await fill_queue(data_list, task_queue) tasks = await create_worker_tasks(num_workers, task_queue, lock, worker_task_count) try: await asyncio.wait_for(task_queue.join(), timeout=timeout) except asyncio.TimeoutError: print(\\"Timed out waiting for tasks to complete.\\") for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\\"Worker task count:\\", worker_task_count) data_list = [f\\"data_{i}\\" for i in range(10)] asyncio.run(main(data_list, num_workers=3, timeout=5)) ``` Expected Output ``` Worker 0: Processed data_0 Worker 1: Processed data_1 Worker 2: Processed data_2 ... Worker task count: [X, Y, Z] # Number of tasks processed by each worker ``` Constraints - `data_list` size can be up to 1000 elements. - Each data processing takes between 0.1 and 0.5 seconds. - It should support at least 10 worker tasks. Your implementation should demonstrate understanding and usage of asyncio tasks, queues, synchronization primitives, timeouts, and cancellation in a practical scenario.","solution":"import asyncio import random async def process_data(data): Simulate processing data by sleeping for a random short duration. await asyncio.sleep(random.uniform(0.1, 0.5)) # Simulate variable processing time return f\\"Processed {data}\\" async def create_worker_tasks(n, task_queue, lock, worker_task_count): Create n worker tasks to fetch and process data from task_queue. async def worker(worker_id): count = 0 while True: try: data = await asyncio.wait_for(task_queue.get(), timeout=1.0) processed_data = await process_data(data) async with lock: print(f\\"Worker {worker_id}: {processed_data}\\") count += 1 task_queue.task_done() except asyncio.TimeoutError: break except asyncio.CancelledError: break worker_task_count[worker_id] = count tasks = [asyncio.create_task(worker(i)) for i in range(n)] return tasks async def fill_queue(data_list, task_queue): Fill the task_queue with data from data_list. for data in data_list: await task_queue.put(data) async def main(data_list, num_workers, timeout): task_queue = asyncio.Queue() lock = asyncio.Lock() worker_task_count = [0] * num_workers await fill_queue(data_list, task_queue) tasks = await create_worker_tasks(num_workers, task_queue, lock, worker_task_count) try: await asyncio.wait_for(task_queue.join(), timeout=timeout) except asyncio.TimeoutError: print(\\"Timed out waiting for tasks to complete.\\") for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\\"Worker task count:\\", worker_task_count) # Example usage if __name__ == \\"__main__\\": data_list = [f\\"data_{i}\\" for i in range(10)] asyncio.run(main(data_list, num_workers=3, timeout=5))"},{"question":"You have been tasked with developing an asyncio-based resource management system for a small server. The server needs to manage access to a database connection pool with a maximum of 5 concurrent connections. You need to ensure that no more than 5 clients can access the database at the same time. Your task is to: 1. Implement a class `DatabaseConnectionPool` that uses a `Semaphore` to manage access to the connection pool. 2. Implement a function `simulate_client` that simulates a client accessing the database. 3. Write an `async` function `main` that simulates multiple clients attempting to access the database concurrently. # Input/Output: - The `DatabaseConnectionPool` does not require any input parameters during initialization. - The `acquire` method of the `DatabaseConnectionPool` should return a mock connection object (use a simple string `\\"database_connection\\"`). - The `release` method of the `DatabaseConnectionPool` should not return any value. - The `simulate_client` function should: - Accept a `DatabaseConnectionPool` instance and a client id as parameters. - Acquire a connection from the pool, simulate doing some work (use `asyncio.sleep` to simulate work), and release the connection. - Print messages to indicate when a client acquires and releases a connection. - The `main` function should: - Create an instance of `DatabaseConnectionPool`. - Spawn multiple `simulate_client` tasks to test concurrent access (e.g., 10 clients). - Use `asyncio.gather` to ensure all tasks are completed before exiting. # Constraints: - You must use `async with` or `await` statements appropriately to manage the `Semaphore`. - The implementation should handle up to 10 simultaneous client requests, but ensure no more than 5 clients have access to the database at the same time. # Example: ```python import asyncio class DatabaseConnectionPool: def __init__(self, max_connections=5): self._pool_semaphore = asyncio.Semaphore(value=max_connections) async def acquire(self): await self._pool_semaphore.acquire() return \\"database_connection\\" def release(self): self._pool_semaphore.release() async def simulate_client(pool, client_id): print(f\\"Client {client_id} is trying to acquire a database connection\\") conn = await pool.acquire() print(f\\"Client {client_id} acquired {conn}\\") await asyncio.sleep(2) # Simulate doing some database work pool.release() print(f\\"Client {client_id} released the connection\\") async def main(): pool = DatabaseConnectionPool() client_tasks = [simulate_client(pool, i) for i in range(10)] await asyncio.gather(*client_tasks) # Run the main function asyncio.run(main()) ``` **Note**: Ensure your implementation details and debug messages match the provided specification.","solution":"import asyncio class DatabaseConnectionPool: def __init__(self, max_connections=5): self._pool_semaphore = asyncio.Semaphore(value=max_connections) async def acquire(self): await self._pool_semaphore.acquire() return \\"database_connection\\" def release(self): self._pool_semaphore.release() async def simulate_client(pool, client_id): print(f\\"Client {client_id} is trying to acquire a database connection\\") conn = await pool.acquire() print(f\\"Client {client_id} acquired {conn}\\") await asyncio.sleep(2) # Simulate doing some database work pool.release() print(f\\"Client {client_id} released the connection\\") async def main(): pool = DatabaseConnectionPool() client_tasks = [simulate_client(pool, i) for i in range(10)] await asyncio.gather(*client_tasks) # Run the main function # Note: Uncomment the line below to run the main function # asyncio.run(main())"},{"question":"# Email Message Manipulation Task Objective: Create a function that constructs an email message using the `EmailMessage` class, performs various modifications on the message, and returns the serialized string representation of the message. Task: 1. Create a new `EmailMessage` object. 2. Add the following headers to the email: - From: \\"sender@example.com\\" - To: \\"recipient@example.com\\" - Subject: \\"Test Email\\" - MIME-Version: \\"1.0\\" - Content-Type: \\"multipart/alternative\\" 3. Set the email payload to contain two parts: - A plain text part with the content: \\"This is the plain text version of the email.\\" - An HTML part with the content: \\"<html><body><h1>This is the HTML version of the email.</h1></body></html>\\" 4. Replace the \\"Subject\\" header with the new value: \\"Updated Test Email\\". 5. Serialize the email message as a string and return it. Input: - No input parameters. Output: - A string representing the serialized email message. Constraints: - Use the `EmailMessage` class and its methods as described in the provided documentation. - Ensure the email message follows the correct MIME structure. Example: ```python def create_and_modify_email(): from email.message import EmailMessage # Step 1: Create EmailMessage object msg = EmailMessage() # Step 2: Add headers msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"recipient@example.com\\" msg[\'Subject\'] = \\"Test Email\\" msg[\'MIME-Version\'] = \\"1.0\\" msg[\'Content-Type\'] = \\"multipart/alternative\\" # Step 3: Set payload with two parts msg.set_content(\\"This is the plain text version of the email.\\") msg.add_alternative(\\"<html><body><h1>This is the HTML version of the email.</h1></body></html>\\", subtype=\'html\') # Step 4: Replace \'Subject\' header msg.replace_header(\'Subject\', \'Updated Test Email\') # Step 5: Serialize and return the email message as a string serialized_email = msg.as_string() return serialized_email # Example call output = create_and_modify_email() print(output) ``` **Note:** Make sure the returned string adheres to the email message format and structure.","solution":"def create_and_modify_email(): from email.message import EmailMessage # Step 1: Create EmailMessage object msg = EmailMessage() # Step 2: Add headers msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"recipient@example.com\\" msg[\'Subject\'] = \\"Test Email\\" msg[\'MIME-Version\'] = \\"1.0\\" # Add multipart/alternative header by using \'set_content\' with \'alternative\' subtype msg.set_content(\\"This is the plain text version of the email.\\") msg.add_alternative(\\"<html><body><h1>This is the HTML version of the email.</h1></body></html>\\", subtype=\'html\') # Step 4: Replace \'Subject\' header msg.replace_header(\'Subject\', \'Updated Test Email\') # Step 5: Serialize and return the email message as a string serialized_email = msg.as_string() return serialized_email"},{"question":"As part of a Python 3.10 project, you need to dynamically load and interact with Python modules. Using the `importlib` module, write a function named `dynamic_import_and_execute` that takes the following parameters: - `module_name` (str): The name of the module to import. - `function_name` (str): The name of the function within the module to execute. - `args` (list): A list of positional arguments to pass to the function. - `kwargs` (dict): A dictionary of keyword arguments to pass to the function. The function should: 1. Dynamically import the specified module. 2. Check if the specified function exists within the module. 3. Execute the function with the provided arguments and keyword arguments. 4. Return the result of the function execution. 5. Handle all potential exceptions (such as the module or function not existing) gracefully by returning `None`. Constraints - The function must use `importlib` for importing modules. - Do NOT use `try` statements directly around the entire code; instead, catch specific exceptions to demonstrate understanding of possible failure points. Expected Input and Output ```python # Example result = dynamic_import_and_execute(\'math\', \'sqrt\', [16], {}) print(result) # Expected Output: 4.0 result = dynamic_import_and_execute(\'math\', \'nonexistent_function\', [], {}) print(result) # Expected Output: None result = dynamic_import_and_execute(\'nonexistent_module\', \'some_function\', [], {}) print(result) # Expected Output: None ``` Performance Requirements - The function should be efficient in importing modules and checking for function existence without unnecessary overhead. Implementation Notes - Utilize the functionalities of `importlib.util` and other relevant submodules of `importlib` to achieve the dynamic import mechanism. - Leverage specific exception handling rather than generic ones to catch and handle module and function absence gracefully.","solution":"import importlib def dynamic_import_and_execute(module_name, function_name, args, kwargs): Dynamically imports the specified module and executes the specified function within the module with the provided arguments and keyword arguments. :param module_name: str, the name of the module to import :param function_name: str, the name of the function to execute :param args: list, positional arguments to pass to the function :param kwargs: dict, keyword arguments to pass to the function :return: the result of the function execution or None if any error occurs try: module = importlib.import_module(module_name) except ModuleNotFoundError: return None func = getattr(module, function_name, None) if not callable(func): return None try: result = func(*args, **kwargs) except Exception: return None return result"},{"question":"# Question: Implementing a Custom Autograd Function and Integrating it into a PyTorch Module Introduction In this assignment, you are required to extend PyTorch by implementing a custom autograd function and then integrating it into a neural network module. This exercise will test your understanding of backward and forward functions within the autograd engine and your ability to build custom neural network layers. Task 1. **Create a Custom Autograd Function**: - Implement a custom autograd function `MultAddFunction`. The function should take three inputs: two tensors and a scalar, perform element-wise multiplication of the two tensors, and then add the scalar to the result. 2. **Create a Custom Neural Network Module**: - Implement a custom `MultAddLayer` module, which uses the `MultAddFunction` in its forward pass. 3. **Test the Implementation**: - Write a test for your `MultAddLayer` module to ensure it works correctly with PyTorch\'s autograd by training a simple model. Specifications 1. **MultAddFunction Class**: - Subclass `torch.autograd.Function`. - Implement the `forward` and `backward` methods. - The `forward` method should perform: `output = input1 * input2 + scalar`. - The `backward` method should compute the gradients w.r.t each input and the scalar. 2. **MultAddLayer Class**: - Subclass `torch.nn.Module`. - Implement the `__init__` and `forward` methods. - The `forward` method should use `MultAddFunction.apply`. 3. **Testing**: - Write a test that: - Creates an instance of `MultAddLayer`. - Performs a forward pass with some sample inputs. - Computes the loss. - Performs a backward pass to ensure that gradients are computed correctly. - Updates the parameters through an optimizer. Input and Output Formats - **MultAddFunction**: - **forward**: `input1` (Tensor), `input2` (Tensor), `scalar` (float) - **backward**: `grad_output` (Tensor) - **output**: Tensor - **MultAddLayer**: - **input**: `Tensor` - **output**: Tensor Constraints - The inputs to the `MultAddFunction` are always of the same shape. - The scalar is a float value. - Use double precision for all tensors in the test to ensure the gradients are checked with high accuracy. ```python import torch from torch.autograd import Function import torch.nn as nn import torch.optim as optim # Define the custom autograd function class MultAddFunction(Function): @staticmethod def forward(ctx, input1, input2, scalar): ctx.save_for_backward(input1, input2) ctx.scalar = scalar return input1 * input2 + scalar @staticmethod def backward(ctx, grad_output): input1, input2 = ctx.saved_tensors scalar = ctx.scalar grad_input1 = grad_input2 = grad_scalar = None if ctx.needs_input_grad[0]: grad_input1 = grad_output * input2 if ctx.needs_input_grad[1]: grad_input2 = grad_output * input1 if ctx.needs_input_grad[2]: grad_scalar = grad_output.sum().item() return grad_input1, grad_input2, grad_scalar # Define the custom layer using the MultAdd function class MultAddLayer(nn.Module): def __init__(self, scalar): super(MultAddLayer, self).__init__() self.scalar = scalar def forward(self, input1, input2): return MultAddFunction.apply(input1, input2, self.scalar) # Define a test case for the custom layer def test_mult_add_layer(): input1 = torch.randn(10, 10, dtype=torch.double, requires_grad=True) input2 = torch.randn(10, 10, dtype=torch.double, requires_grad=True) scalar = 5.0 # Initialize the custom layer layer = MultAddLayer(scalar) # Forward pass output = layer(input1, input2) # Define a simple loss loss = output.sum() # Backward pass loss.backward() # Check if gradients are correctly computed assert input1.grad is not None, \\"Gradient for input1 is None\\" assert input2.grad is not None, \\"Gradient for input2 is None\\" # Print gradients for verification print(\\"Gradients for input1:\\", input1.grad) print(\\"Gradients for input2:\\", input2.grad) # Run the test test_mult_add_layer() ``` Notes - Make sure to test your code thoroughly. - Validate the gradients using the `torch.autograd.gradcheck` method. - Ensure the custom module integrates seamlessly with PyTorch\'s autograd system.","solution":"import torch from torch.autograd import Function import torch.nn as nn import torch.optim as optim # Define the custom autograd function class MultAddFunction(Function): @staticmethod def forward(ctx, input1, input2, scalar): ctx.save_for_backward(input1, input2) ctx.scalar = scalar return input1 * input2 + scalar @staticmethod def backward(ctx, grad_output): input1, input2 = ctx.saved_tensors scalar = ctx.scalar grad_input1 = grad_input2 = grad_scalar = None if ctx.needs_input_grad[0]: grad_input1 = grad_output * input2 if ctx.needs_input_grad[1]: grad_input2 = grad_output * input1 if ctx.needs_input_grad[2]: grad_scalar = grad_output.sum().item() return grad_input1, grad_input2, grad_scalar # Define the custom layer using the MultAdd function class MultAddLayer(nn.Module): def __init__(self, scalar): super(MultAddLayer, self).__init__() self.scalar = scalar def forward(self, input1, input2): return MultAddFunction.apply(input1, input2, self.scalar)"},{"question":"**Problem Statement:** You are tasked with creating a Python function that takes an input WAV file, processes it by modifying its frame rate and channel configuration, and then saves it as a new WAV file. The function should demonstrate the use of the `wave` module to read and write WAV files and manipulate their properties. **Function Signature:** ```python def process_wav(input_file: str, output_file: str, new_framerate: int, new_channels: int) -> None: pass ``` # Input - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path to save the processed WAV file. - `new_framerate` (int): The new frame rate to be applied to the output WAV file. - `new_channels` (int): The new number of channels to be applied to the output WAV file (1 for mono, 2 for stereo). # Output - The function should not return anything. It should create a new WAV file at the specified `output_file` path with the modified properties. # Constraints - The input file will always be a valid WAV file in \\"WAVE_FORMAT_PCM\\" format. - The `new_framerate` will be a positive integer. - The `new_channels` will be either 1 (mono) or 2 (stereo). # Example ```python process_wav(\'input.wav\', \'output.wav\', 16000, 1) ``` This should read the `input.wav` file, change its frame rate to 16000 Hz, convert it to mono (one channel), and save the modified audio data to `output.wav`. # Instructions 1. Open the input WAV file using the `wave` module and read its properties and frames. 2. Create a new WAV file for writing with the specified `output_file` path and modify its frame rate and number of channels. 3. Write the frames from the input file to the new output file, adjusting the frame rate and channels as necessary. 4. Ensure proper handling of file opening and closing, particularly with the context manager (`with` statement). # Notes - You may use any necessary libraries or techniques to facilitate changes in frame rate and channel configuration. - Handle exceptions gracefully and ensure that the input file is closed properly even if an error occurs. **Hint:** You may need to resample the audio data when changing the frame rate. Consider using libraries like `numpy` or `scipy` for this purpose.","solution":"import wave import numpy as np from scipy.signal import resample def process_wav(input_file: str, output_file: str, new_framerate: int, new_channels: int) -> None: with wave.open(input_file, \'rb\') as infile: # Read parameters and frames from the input file n_channels = infile.getnchannels() sampwidth = infile.getsampwidth() framerate = infile.getframerate() n_frames = infile.getnframes() frames = infile.readframes(n_frames) # Convert frames to numpy array for resampling (assuming 16-bit samples) dtype = np.int16 if sampwidth == 2 else np.int8 audio_data = np.frombuffer(frames, dtype=dtype) # Resample if frame rate is different if framerate != new_framerate: n_samples = int(len(audio_data) * new_framerate / framerate) audio_data = resample(audio_data, n_samples) # Handle changing channels (convert to mono or stereo) if new_channels == 1 and n_channels == 2: audio_data = audio_data.reshape(-1, 2) audio_data = audio_data.mean(axis=1).astype(dtype) elif new_channels == 2 and n_channels == 1: audio_data = np.repeat(audio_data, 2).astype(dtype) # Write the new file with wave.open(output_file, \'wb\') as outfile: outfile.setnchannels(new_channels) outfile.setsampwidth(sampwidth) outfile.setframerate(new_framerate) outfile.writeframes(audio_data.tobytes())"},{"question":"**Objective:** Implement a custom sequence class `SquareSequence` that generates the square of numbers from 0 up to a given limit, adhering to the `collections.abc.Sequence` interface. **Details:** 1. **Class Name:** `SquareSequence` 2. **Interface:** The class should inherit from `collections.abc.Sequence`. 3. **Methods to Implement:** - `__getitem__(self, index)`: Should return the square of the number at the given index. - `__len__(self)`: Should return the length of the sequence. **Additional Requirements:** - The class should be iterable and support common sequence operations like indexing and length retrieval. - Ensure `SquareSequence` works with slicing to return new instances of `SquareSequence`. - Add a method `sum(self)` that returns the sum of the squares in the sequence. **Input:** - The class constructor takes a single integer input, `n`, which defines the upper limit of the sequence (exclusive). **Output:** - Sequencing and summing operations as per the methods defined above. **Example:** ```python from collections.abc import Sequence class SquareSequence(Sequence): def __init__(self, n): self.n = n def __getitem__(self, index): if isinstance(index, slice): start, stop, step = index.indices(self.n) return SquareSequence(stop-start) if 0 <= index < self.n: return index ** 2 else: raise IndexError(\'Index out of range\') def __len__(self): return self.n def sum(self): return sum(self[i] for i in range(self.n)) # Example usage: sq_sequence = SquareSequence(5) print(sq_sequence[2]) # Output: 4 print(len(sq_sequence)) # Output: 5 print(sq_sequence.sum()) # Output: 30 print(list(sq_sequence)) # Output: [0, 1, 4, 9, 16] print(sq_sequence[1:4]) # Output: SquareSequence(3) with content of that sequence ``` **Constraints:** - The upper limit `n` will be a non-negative integer less than or equal to 10,000. - Aim for an efficient implementation in terms of time and space complexity. # Evaluation Criteria: - Correct implementation of the `SquareSequence` class adhering to the `Sequence` ABC. - Proper handling of slicing and indexing. - Efficient use of memory and computation. - Readability and organization of the code.","solution":"from collections.abc import Sequence class SquareSequence(Sequence): def __init__(self, n): if not isinstance(n, int) or n < 0: raise ValueError(\\"The upper limit must be a non-negative integer.\\") self.n = n def __getitem__(self, index): if isinstance(index, slice): start, stop, step = index.indices(self.n) sliced_list = [i ** 2 for i in range(start, stop, step)] sliced_seq = SquareSequence(len(sliced_list)) sliced_seq.squares = sliced_list return sliced_seq if 0 <= index < self.n: return index ** 2 else: raise IndexError(\'Index out of range\') def __len__(self): return self.n def sum(self): return sum(i ** 2 for i in range(self.n)) def __iter__(self): for i in range(self.n): yield i ** 2"},{"question":"# Command-Line Parser with optparse The `optparse` module allows you to define command-line options and arguments for your Python scripts and handle them effectively. Even though it has been deprecated in favor of `argparse`, knowing how to use `optparse` can deepen your understanding of command-line parsing and legacy codebases. **Question:** You are tasked with creating a Python script that processes a command-line tool for managing a simple to-do list stored in memory. Your script should handle adding, listing, and removing tasks using the `optparse` module. Requirements: 1. Implement a function `manage_tasks()` which processes the command-line options and arguments. 2. The script should support the following functionalities: - **Add a Task**: Add a task to the list. - Use `-a` or `--add` followed by the task description. - Example: `python script.py --add \\"Buy groceries\\"` - **List Tasks**: List all the tasks in the to-do list. - Use `-l` or `--list`. - Example: `python script.py --list` - **Remove a Task**: Remove a task from the list by its numeric index. - Use `-r` or `--remove` followed by the task index (starting from 1). - Example: `python script.py --remove 1` 3. The script should handle errors gracefully: - Display a message if an invalid index is provided for removal. - If no options are provided, display the help message. **Constraints:** - Focus on using the `optparse` module and its functionalities. - The to-do list should be stored in a global list (for simplicity, persistency is not required). **Function Signature:** ```python def manage_tasks(command_line_args: List[str]): pass ``` **Example:** ```python # Example script file name: task_manager.py if __name__ == \'__main__\': import sys manage_tasks(sys.argv[1:]) ``` ```bash python task_manager.py --add \\"Buy groceries\\" Task added: Buy groceries python task_manager.py --list 1. Buy groceries python task_manager.py --remove 1 Task removed: Buy groceries python task_manager.py --list No tasks in the to-do list. ``` **Documentation Reference:** You can refer to the `optparse` module documentation for detailed information on how to implement the required functionalities. ``` import optparse ... ``` You must ensure proper error handling and display user-friendly messages. This task will assess your ability to create command-line tools, manage input options, and handle different actions effectively using the `optparse` module.","solution":"import optparse todo_list = [] def manage_tasks(command_line_args): parser = optparse.OptionParser() parser.add_option(\\"-a\\", \\"--add\\", dest=\\"add_task\\", type=\\"string\\", help=\\"Add a task to the to-do list\\") parser.add_option(\\"-l\\", \\"--list\\", action=\\"store_true\\", dest=\\"list_tasks\\", help=\\"List all tasks in the to-do list\\") parser.add_option(\\"-r\\", \\"--remove\\", dest=\\"remove_task\\", type=\\"int\\", help=\\"Remove a task from the to-do list by index\\") (options, args) = parser.parse_args(command_line_args) if options.add_task: task = options.add_task todo_list.append(task) print(f\\"Task added: {task}\\") elif options.list_tasks: if todo_list: for idx, task in enumerate(todo_list, start=1): print(f\\"{idx}. {task}\\") else: print(\\"No tasks in the to-do list.\\") elif options.remove_task: index = options.remove_task - 1 if 0 <= index < len(todo_list): removed_task = todo_list.pop(index) print(f\\"Task removed: {removed_task}\\") else: print(\\"Invalid task index.\\") else: parser.print_help()"},{"question":"Title: Mocking with unittest.mock Description: You are tasked with developing a small data processing function and thoroughly testing it using the `unittest.mock` module. The functionality involves fetching data from an API, processing it, and saving the results to a database. The data fetching and saving functions are to be mocked in the tests to verify that they behave correctly under various conditions. Requirements: 1. **Function Implementation**: Implement the `process_data` function which performs the following actions: - Fetches data from a given endpoint using a provided `fetch_data` function. - Processes the fetched data using a `transform_data` function. - Saves the processed data using a provided `save_data` function. 2. **Data Functions**: Write a simple placeholder implementation for `fetch_data`, `transform_data`, and `save_data` functions. These functions should be designed such that they can later be replaced by mocks in the tests. 3. **Tests**: Write comprehensive test cases for `process_data` using `unittest.mock` to: - Mock `fetch_data`, `transform_data`, and `save_data`. - Verify the `fetch_data` is called with the correct endpoint. - Ensure `transform_data` is called with the data returned by `fetch_data`. - Check that `save_data` is called with the processed data returned by `transform_data`. - Simulate and test behavior when `fetch_data` raises an exception. - Simulate and test behavior when `save_data` fails. Input and Output Formats and Constraints: - The `process_data` function should have the following signature: ```python def process_data(endpoint: str) -> None: ``` - `fetch_data` and `save_data` functions\' simplified signatures might be: ```python def fetch_data(endpoint: str) -> Any: def save_data(data: Any) -> None: ``` - `transform_data` function\'s simplified signature might be: ```python def transform_data(raw_data: Any) -> Any: ``` - Assume `fetch_data` and `save_data` interact with network services and databases, thus should be mocked in tests. You can define any realistic behavior for `transform_data`. - Test for: - Correct sequence and content of interactions between `process_data` and the helper functions. - Proper handling of exceptions that may be raised by the mocked functions. _NOTE_: The exact nature of data being fetched and saved is not crucial, and you may assume any simple data structures for the examples. Sample Code Structure: ```python from unittest.mock import patch, Mock import unittest # Placeholder implementations def fetch_data(endpoint): pass def transform_data(raw_data): pass def save_data(data): pass # Function to implement def process_data(endpoint): pass # Tests class TestProcessData(unittest.TestCase): @patch(\'module_under_test.fetch_data\') @patch(\'module_under_test.save_data\') @patch(\'module_under_test.transform_data\') def test_process_data(self, mock_transform, mock_save, mock_fetch): # Define your test logic and assertions here pass @patch(\'module_under_test.fetch_data\') @patch(\'module_under_test.save_data\') @patch(\'module_under_test.transform_data\') def test_fetch_data_exception(self, mock_transform, mock_save, mock_fetch): # Define your test logic and assertions here pass if __name__ == \'__main__\': unittest.main() ``` Complete the implementation and tests as per the requirements to ensure the `process_data` function behaves correctly.","solution":"# Placeholder implementations def fetch_data(endpoint): Simulated fetch_data function that gets data from an endpoint. This is a placeholder and should be replaced by a mock in tests. pass def transform_data(raw_data): Simulated transform_data function that processes raw data. This is a placeholder and should be replaced by a mock in tests. return raw_data def save_data(data): Simulated save_data function that saves processed data. This is a placeholder and should be replaced by a mock in tests. pass # Function to implement def process_data(endpoint): Fetch data from an endpoint, process it, and save the processed data. try: raw_data = fetch_data(endpoint) processed_data = transform_data(raw_data) save_data(processed_data) except Exception as e: print(f\\"Error occurred: {e}\\") raise"},{"question":"You are given a dataset loaded from seaborn\'s built-in datasets. Your task is to write a function using seaborn and matplotlib that visualizes the given dataset through multiple types of plots and customizations. # Dataset: The dataset you will be using is the “tips” dataset from seaborn. Load the dataset using the following code: ```python import seaborn as sns # Load the tips dataset tips = sns.load_dataset(\\"tips\\") ``` # Task: Write a function `visualize_tips_data()` that: 1. Plots a strip plot of `total_bill` on the x-axis and `day` on the y-axis with points colored by `sex`. 2. Additionally customizes the strip plot by: - Disabling jitter. - Setting the size of the points to 7. - Using triangle markers (\'^\'). - Setting the transparency `alpha` to 0.7. 3. Creates a faceted plot using `seaborn.catplot()` where: - The `x` axis is the `time` of the day (Lunch/Dinner). - The `y` axis is `total_bill`. - The facets (columns) are grouped by `day`. - Color the points by `sex`. - Set the aspect ratio of each plot to 0.6. **Implementation Details:** - Make sure to set the theme to `whitegrid` for better aesthetics. - Your function should not return anything, only generate and display the plots. Here is a function signature to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Set the theme sns.set_theme(style=\\"whitegrid\\") # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create and customize the strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", jitter=False, s=7, marker=\'^\', alpha=0.7) plt.title(\'Strip plot of Total Bill by Day and Sex\') plt.legend(title=\'Sex\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Create the faceted plot sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.6) plt.subplots_adjust(top=0.9) plt.suptitle(\'Faceted plot of Total Bill by Time of Day and Sex\') plt.show() ``` # Example: After implementing `visualize_tips_data()`, calling this function should display the required strip plot and faceted plot with the specified customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): Visualizes the tips data using seaborn and matplotlib with multiple types of plots and customizations. # Set the theme sns.set_theme(style=\\"whitegrid\\") # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create and customize the strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", jitter=False, s=7, marker=\'^\', alpha=0.7) plt.title(\'Strip plot of Total Bill by Day and Sex\') plt.legend(title=\'Sex\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Create the faceted plot sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.6) plt.subplots_adjust(top=0.9) plt.suptitle(\'Faceted plot of Total Bill by Time of Day and Sex\') plt.show()"},{"question":"# Objective You are required to demonstrate your understanding of the `copyreg` module by implementing a custom serialization mechanism for a given class. # Problem Statement Consider a class `Person` which has attributes `name` and `age`. 1. Implement this `Person` class with necessary initializations. 2. Write a custom reduction function, `pickle_person`, that can be used for pickling objects of the `Person` class. 3. Register this reduction function using the `copyreg.pickle` method. 4. Demonstrate that your reduction function works by creating an instance of the `Person` class, pickling it, and then unpickling it back to an object. # Input and Output Formats - **Input**: No direct input from the user is required. - **Output**: Printed statements indicating the pickling and unpickling processes, and the final state of the unpickled object to verify if the process was successful. # Constraints - Ensure that the `Person` class and its attributes (`name` and `age`) are serialized and deserialized correctly. - You must use the `copyreg` module to register the custom reduction function. # Example ```python import copyreg, pickle # Step 1: Define the Person class class Person: def __init__(self, name, age): self.name = name self.age = age def __repr__(self): return f\\"Person(name={self.name}, age={self.age})\\" # Step 2: Define the custom reduction function def pickle_person(person): print(\\"Pickling a Person instance...\\") return Person, (person.name, person.age) # Step 3: Register the reduction function with copyreg copyreg.pickle(Person, pickle_person) # Step 4: Demonstrate the functionality if __name__ == \\"__main__\\": person = Person(\\"Alice\\", 30) print(f\\"Original: {person}\\") # Pickle the person object pickled_person = pickle.dumps(person) # Unpickle the person object unpickled_person = pickle.loads(pickled_person) print(f\\"Unpickled: {unpickled_person}\\") ``` # Expected Output ``` Original: Person(name=Alice, age=30) Pickling a Person instance... Unpickled: Person(name=Alice, age=30) ``` The print statements confirm that the reduction function is called during the pickling process and that the object state is correctly restored during unpickling.","solution":"import copyreg import pickle class Person: def __init__(self, name, age): self.name = name self.age = age def __repr__(self): return f\\"Person(name={self.name}, age={self.age})\\" def pickle_person(person): print(\\"Pickling a Person instance...\\") return Person, (person.name, person.age) copyreg.pickle(Person, pickle_person)"},{"question":"You are tasked with writing a function that processes data from a mixed list of different data types and constructs a formatted string. Your function should handle parsing integers, floats, and strings, and then build and return a well-formatted string. The function should be named `parse_and_format_data`. Function Signature ```python def parse_and_format_data(data: list) -> str: ``` Input - `data`: a list containing elements of different data types (int, float, str). Output - A single string that combines all elements, formatted as follows: - Integers should be converted to hexadecimal format. - Floats should be formatted to two decimal places. - Strings should be enclosed in double quotes. Example ```python data = [42, 3.14159, \'hello\', 7, 2.71828, \'world\'] formatted_string = parse_and_format_data(data) print(formatted_string) ``` Expected Output: ``` \\"[0x2a], [3.14], [\\"hello\\"], [0x7], [2.72], [\\"world\\"]\\" ``` Constraints - The length of the input list `data` is between 1 and 100 elements. - The elements within the list are either of type `int`, `float`, or `str`. Notes - Each element in the resulting string should be enclosed in square brackets `[]`. - Elements in the resulting string should be separated by a comma and a space `, `. Performance Requirements - Your solution should be efficient in terms of both time and space complexity given the input constraints. # Submission Implement the `parse_and_format_data` function to complete the task.","solution":"def parse_and_format_data(data): Processes a mixed list of integers, floats, and strings and constructs a formatted string. Integers are formatted to hexadecimal prefixed with \'0x\', floats are rounded to two decimal places, and strings are enclosed in double quotes. formatted_elements = [] for item in data: if isinstance(item, int): formatted_elements.append(f\\"[0x{item:x}]\\") elif isinstance(item, float): formatted_elements.append(f\\"[{item:.2f}]\\") elif isinstance(item, str): formatted_elements.append(f\'[\\"{item}\\"]\') else: raise ValueError(f\\"Unsupported data type: {type(item)}\\") return \', \'.join(formatted_elements) # Example usage: # data = [42, 3.14159, \'hello\', 7, 2.71828, \'world\'] # formatted_string = parse_and_format_data(data) # print(formatted_string) # Expected Output: \\"[0x2a], [3.14], [\\"hello\\"], [0x7], [2.72], [\\"world\\"]\\""},{"question":"**Objective:** To assess students\' understanding of managing asynchronous tasks and Futures using the `asyncio` module in Python. # Problem Description You are tasked with implementing a function using `asyncio` that manages a list of asynchronous tasks. Specifically, your function should: 1. Create a Future using the current event loop. 2. Add multiple tasks that will set results for the Future at different intervals. 3. Use a callback to handle the completion of the Future. 4. Wait until all tasks are completed and gather their results. # Function Signature ```python import asyncio async def manage_futures(tasks: list): Manages a list of tasks and handles Future completion. Args: tasks (list): A list of tuples, where each tuple contains: - a delay (int or float) - a value to set in the Future after the delay Returns: list: A list of results set by the tasks in the order they completed. ``` # Input - `tasks` (list of tuples): Each tuple contains two elements: - `delay` (int or float): The number of seconds the task should wait before setting a result. - `value` (any): The value to set in the Future after the delay. # Output - A list of results (list). # Constraints - The delay will be a non-negative number. - Each value can be any type that can be set in a `Future`. - The list of tasks will have a length between 1 and 20. # Example ```python import asyncio async def main(): tasks = [(1, \'A\'), (2, \'B\'), (1.5, \'C\')] results = await manage_futures(tasks) print(results) asyncio.run(main()) ``` Expected output: ``` [\'A\', \'C\', \'B\'] ``` # Additional Requirements - Use the `asyncio.Future` class to manage the task results. - Implement a callback function to handle the Future\'s completion. - Ensure all Futures\' results are gathered and returned in the order they completed. # Hints - You can use `loop.create_future()` to create a Future object. - Use `asyncio.create_task()` to manage each task that sets results in the Future. - Utilize `add_done_callback()` to handle task completion. - `asyncio.gather()` might be useful for collecting results from multiple Futures. Ensure your code is clear, well-documented, and handles edge cases appropriately.","solution":"import asyncio async def manage_futures(tasks): Manages a list of tasks and handles Future completion. Args: tasks (list): A list of tuples, where each tuple contains: - a delay (int or float) - a value to set in the Future after the delay Returns: list: A list of results set by the tasks in the order they completed. loop = asyncio.get_event_loop() futures = [loop.create_future() for _ in tasks] async def set_future_result(future, delay, value): await asyncio.sleep(delay) future.set_result(value) results = [] def future_done_callback(fut): results.append(fut.result()) for i, (delay, value) in enumerate(tasks): futures[i].add_done_callback(future_done_callback) asyncio.create_task(set_future_result(futures[i], delay, value)) await asyncio.gather(*futures) return results"},{"question":"# **Coding Assessment Question** You have been tasked with creating a custom implementation of the `print` function to format output in a specific way by wrapping the built-in `print()` function provided in Python\'s \\"builtins\\" module. # **Task** Implement a custom `print` function that behaves like the built-in `print()` function but adds the following enhancements: 1. Converts all printed text to lowercase. 2. Prefixes each line with the string \\"custom: \\". 3. Adds a number to each line, starting from 1. For example: ```python custom_print(\\"Hello World\\", \\"This is a test\\") ``` should output: ``` custom: 1: hello world custom: 2: this is a test ``` # **Requirements** 1. You must use the built-in `print` function to perform the actual printing. 2. The custom `print` function should accept any positional and keyword arguments that the built-in `print` function accepts. 3. Make sure to handle the `sep` and `end` parameters of the `print` function correctly. 4. Keep track of the line number across multiple calls to `custom_print`; the numbering should not reset with each call. # **Implementation Details** - Define a function `custom_print` that wraps the `print` function from the \\"builtins\\" module. - Introduce a global variable to keep track of the line number across multiple calls. - Convert the text to lowercase using the `.lower()` string method. - Prefix each line with \\"custom: \\" followed by the line number. # **Constraints** - The function should handle arbitrary numbers of positional and keyword arguments. - Ensure that the function behaves correctly with default arguments of the built-in `print` function (`sep=\' \'`, `end=\'n\'`, `file=None`, `flush=False`). # **Example Usage** ```python custom_print(\\"Hello, World!\\") custom_print(\\"Another\\", \\"line\\", sep=\\"-\\") custom_print(\\"And another one\\", end=\\"!!!n\\") # Expected output: # custom: 1: hello, world! # custom: 2: another-line # custom: 3: and another one!!! ``` Implement the `custom_print` function below. ```python import builtins line_number = 0 def custom_print(*args, **kwargs): global line_number line_number += 1 builtins.print(f\\"custom: {line_number}: \\", end=\\"\\") builtins.print(*[str(arg).lower() for arg in args], **kwargs) ```","solution":"import builtins line_number = 0 def custom_print(*args, **kwargs): global line_number line_number += 1 # Capture sep and end parameters; provide default values if not present sep = kwargs.get(\'sep\', \' \') end = kwargs.get(\'end\', \'n\') # Process the arguments to generate the string output = sep.join([str(arg).lower() for arg in args]) # Use print with the formatted output builtins.print(f\\"custom: {line_number}: {output}\\", end=end)"},{"question":"Setup Configuration File Parser **Objective:** Write a Python function `modify_setup_cfg` that reads a \\"setup.cfg\\" file, modifies it based on specific commands and options provided as input, and then writes the modified configuration back to the same file. This task will assess your understanding of file handling and parsing structured data in Python. **Function Signature:** ```python def modify_setup_cfg(file_path: str, modifications: dict) -> None: Modify the setup.cfg file at the given file path based on the provided modifications dictionary. Args: file_path (str): Path to the setup.cfg file to be modified. modifications (dict): Dictionary containing the modifications to be made. The keys are command names (str), and the values are dictionaries of option-value pairs to be modified or added for that command. Returns: None ``` **Input:** - `file_path` (str): The path to the \\"setup.cfg\\" file. - `modifications` (dict): A dictionary where keys are command names (str) and values are dictionaries containing option-value pairs to be added or modified. **Output:** - The function should modify the \\"setup.cfg\\" file in-place based on the provided modifications. **Constraints:** 1. If the command section specified in the modifications dictionary does not exist, it should be created. 2. If an option within a command section already exists, its value should be updated; otherwise, the option should be added. 3. The function should handle multi-line option values properly. 4. The file should maintain its formatting, including comments and blank lines. **Example:** Suppose you have the following `setup.cfg` file: ``` [build_ext] inplace=0 [bdist_rpm] release=1 packager=Old Packager <old@example.com> doc_files=CHANGES.txt README.txt ``` Calling the function with these arguments: ```python modifications = { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/usr/local/include\\" }, \\"bdist_rpm\\": { \\"packager\\": \\"New Packager <new@example.com>\\", \\"doc_files\\": \\"CHANGES.txt README.txt USAGE.txt\\" }, \\"install\\": { \\"optimize\\": \\"1\\" } } modify_setup_cfg(\\"path/to/setup.cfg\\", modifications) ``` The modified `setup.cfg` should be: ``` [build_ext] inplace=1 include_dirs=/usr/local/include [bdist_rpm] release=1 packager=New Packager <new@example.com> doc_files=CHANGES.txt README.txt USAGE.txt [install] optimize=1 ``` **Note:** - The function should be robust against any formatting inconsistencies in the input file, such as varying numbers of spaces around the \\"=\\" sign. - Proper error handling should be implemented for file I/O operations.","solution":"def modify_setup_cfg(file_path: str, modifications: dict) -> None: Modify the setup.cfg file at the given file path based on the provided modifications dictionary. Args: file_path (str): Path to the setup.cfg file to be modified. modifications (dict): Dictionary containing the modifications to be made. The keys are command names (str), and the values are dictionaries of option-value pairs to be modified or added for that command. Returns: None from configparser import ConfigParser config = ConfigParser() # Read the existing file config.read(file_path) # Apply the modifications for section, options in modifications.items(): if not config.has_section(section): config.add_section(section) for option, value in options.items(): config.set(section, option, value) # Write the changes back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# Asynchronous Task Manager The goal of this coding question is to test your ability to implement and manage coroutine-based tasks using Python\'s asynchronous features. Problem Statement Implement a class `AsyncTaskManager` that allows scheduling and running asynchronous tasks. The class should be able to start multiple tasks, wait for their completion, and handle exceptions for each task. Requirements 1. `AsyncTaskManager` should have the following methods: - `add_task(coro)`: Adds a new coroutine to the task manager. - `run_all()`: Runs all the added tasks concurrently and waits for their completion. - `task_status()`: Returns a dictionary with the status of each task. Possible statuses are `\\"pending\\"`, `\\"running\\"`, `\\"completed\\"`, and `\\"failed\\"`. 2. The `run_all` method should: - Ensure all tasks are run concurrently. - Catch any exceptions that occur during task execution. - Update the status of each task accordingly. 3. Input and Output Formats: - `add_task(coro)` takes a coroutine function and returns `None`. - `run_all()` takes no arguments and returns `None`. - `task_status()` takes no arguments and returns a dictionary where keys are task names, and values are statuses. 4. Performance requirements: - The solution should handle at least 100 concurrent tasks efficiently. Example Usage ```python import asyncio class AsyncTaskManager: def __init__(self): self.tasks = [] self.status = {} def add_task(self, coro): Add a new coroutine to the task manager task_name = coro.__name__ self.tasks.append(coro) self.status[task_name] = \\"pending\\" async def run_all(self): Run all the added tasks concurrently and wait for their completion # Implement the task running logic here pass def task_status(self): Return a dictionary with the status of each task return self.status # Example coroutines to be managed async def task1(): await asyncio.sleep(1) return \\"Task 1 completed\\" async def task2(): await asyncio.sleep(2) raise Exception(\\"Task 2 failed\\") # Usage manager = AsyncTaskManager() manager.add_task(task1) manager.add_task(task2) asyncio.run(manager.run_all()) statuses = manager.task_status() print(statuses) ``` Notes - Do not use any additional libraries other than the standard library `asyncio`. - Ensure that the system can handle tasks running concurrently without blocking the event loop.","solution":"import asyncio class AsyncTaskManager: def __init__(self): self.tasks = [] self.status = {} def add_task(self, coro): Add a new coroutine to the task manager task_name = coro.__name__ self.tasks.append((task_name, coro)) self.status[task_name] = \\"pending\\" async def run_all(self): Run all the added tasks concurrently and wait for their completion async def run_task(task_name, coro): self.status[task_name] = \\"running\\" try: await coro self.status[task_name] = \\"completed\\" except Exception: self.status[task_name] = \\"failed\\" await asyncio.gather(*[run_task(task_name, coro) for task_name, coro in self.tasks]) def task_status(self): Return a dictionary with the status of each task return self.status # Example coroutines to be managed async def task1(): await asyncio.sleep(1) return \\"Task 1 completed\\" async def task2(): await asyncio.sleep(2) raise Exception(\\"Task 2 failed\\")"},{"question":"Objective: You are tasked with creating a utility to analyze and optimize pickle files using Python\'s `pickletools` module. This exercise will help you demonstrate your understanding of working with serialized data and utilizing the `pickletools` module for advanced operations on pickle objects. Problem Statement: Write a Python function `analyze_and_optimize_pickle(input_file: str, annotated: bool = False) -> str` that takes the following inputs: - `input_file`: The file path to a pickle file that needs to be analyzed and optimized. - `annotated`: A boolean flag indicating whether to include annotations in the disassembly output. The function should: 1. Read the contents of the specified pickle file. 2. Use the `pickletools.dis` function to disassemble the pickle\'s contents. - If `annotated` is `True`, the disassembly should include annotations. - Write the disassembly to a temporary string buffer. 3. Use the `pickletools.optimize` function to optimize the pickle contents. 4. Return the disassembled string with annotations (if specified) followed by the optimized pickle as a string. Constraints: - Assume the pickle file is always valid and exists at the given path. - The pickle file contains data that can be deserialized using Python\'s `pickle` module. Performance Requirements: - The function should handle pickle files up to 10 MB efficiently. Function Signature: ```python def analyze_and_optimize_pickle(input_file: str, annotated: bool = False) -> str: pass ``` Example: Assume you have a pickle file named \\"example.pickle\\". ```python disassembled = analyze_and_optimize_pickle(\\"example.pickle\\", annotated=True) ``` If `example.pickle` contains a pickled tuple `(1, 2)`, the function should return a string similar to the following (annotated for clarity, and then the optimized pickle string in hexadecimal): ``` 0: x80 PROTO 3 2: K BININT1 1 4: K BININT1 2 6: x86 TUPLE2 7: q BINPUT 0 9: . STOP highest protocol among opcodes = 2 Optimized Pickle (in hex): 80034b014b0286. ``` **Hint**: Use Python\'s `io.StringIO` to create an in-memory string buffer for capturing the disassembly output.","solution":"import pickle import pickletools import io def analyze_and_optimize_pickle(input_file: str, annotated: bool = False) -> str: Analyzes and optimizes a pickle file. Parameters: input_file (str): Path to the pickle file. annotated (bool): Flag to indicate whether to include annotations in the disassembly. Returns: str: Disassembled pickle content with optional annotations followed by the optimized pickle as a string. with open(input_file, \'rb\') as file: # Read the content of the pickle file pickle_data = file.read() # Create a string buffer to hold disassembly output disassembly_output = io.StringIO() # Disassemble the pickle file content pickletools.dis(pickle_data, out=disassembly_output, annotate=annotated) # Reset the buffer to the start disassembly_output.seek(0) # Get the disassembly output as a string disassembled_content = disassembly_output.getvalue() # Optimize the pickle data optimized_pickle_data = pickletools.optimize(pickle_data) # Convert optimized pickle to its hexadecimal representation for readability optimized_pickle_hex = optimized_pickle_data.hex() # Combine disassembled content and formatted optimized pickle result = f\\"{disassembled_content}nOptimized Pickle (in hex): {optimized_pickle_hex}n\\" return result"},{"question":"# Task Implement a function `secure_login` that uses the `getpass` module to securely handle user authentication and return user-specific information. # Function Signature ```python def secure_login(users_db): ``` # Input - `users_db` (dict): A dictionary where keys are usernames and values are corresponding passwords (both strings). Example: ```python { \\"alice\\": \\"password123\\", \\"bob\\": \\"qatest\\", \\"charlie\\": \\"welcome123\\" } ``` # Output Your function should: 1. Prompt the user for their username and password using `input()` for username and `getpass.getpass()` for password. 2. Verify the credentials against the `users_db` dictionary. 3. If the credentials are valid, return the string `\\"Welcome, [username]!\\"`. 4. If the credentials are invalid, return the string `\\"Invalid username or password.\\"`. # Exception Handling 1. If there is an issue with echo-free password input, catch the `getpass.GetPassWarning` and proceed as usual. Display the warning message using `print`. 2. Handle any other potential exceptions gracefully, returning `\\"An error occurred.\\"`. # Example Usage ```python users_db = { \\"alice\\": \\"password123\\", \\"bob\\": \\"qatest\\", \\"charlie\\": \\"welcome123\\" } print(secure_login(users_db)) ``` **Sample Interaction:** ``` Enter username: alice Password: (user types without seeing the input) \\"Welcome, alice!\\" ``` ``` Enter username: eve Password: (user types without seeing the input) \\"Invalid username or password.\\" ``` # Constraints - Assume the username and password entries in `users_db` are case-sensitive. - The function should handle typical user input errors gracefully.","solution":"import getpass def secure_login(users_db): Securely prompts the user for a username and password, verifying against the given users_db dictionary. Args: users_db (dict): A dictionary where keys are usernames and values are passwords. Returns: str: Welcome message if credentials are valid, error message otherwise. try: username = input(\\"Enter username: \\") password = getpass.getpass(\\"Password: \\") if username in users_db and users_db[username] == password: return f\\"Welcome, {username}!\\" else: return \\"Invalid username or password.\\" except getpass.GetPassWarning: print(\\"Warning: Your terminal/console does not support secure password input.\\") return \\"An error occurred.\\" except Exception as e: print(f\\"An unexpected error occurred: {str(e)}\\") return \\"An error occurred.\\""},{"question":"# Python C API Capsule Usage You are tasked with creating and manipulating a PyCapsule object using the Python C API. Specifically, you will: 1. Create a new PyCapsule object encapsulating a pointer. 2. Validate that the created capsule is correct. 3. Retrieve and print the capsule\'s pointer, name, and context. 4. Modify the capsule\'s name and context, and then print the updated values. # Steps and Requirements 1. **Create a Capsule**: - Use the function `PyCapsule_New` to create a new PyCapsule object. - The pointer should be a non-null dummy pointer (e.g., a cast of an integer). - Provide a name for the capsule (e.g., `\\"my_module.my_attribute\\"`). 2. **Validation**: - Use `PyCapsule_IsValid` to check if the capsule is valid. - Print the result of the validation check. 3. **Retrieve Capsule Information**: - Use `PyCapsule_GetPointer` to get the pointer inside the capsule. - Use `PyCapsule_GetName` to get the capsule\'s name. - Use `PyCapsule_GetContext` to get the capsule\'s context. - Print the retrieved pointer, name, and context. 4. **Modify Capsule**: - Change the capsule\'s name using `PyCapsule_SetName` to `\\"my_module.new_attribute\\"`. - Set a new context pointer using `PyCapsule_SetContext` (e.g., another dummy pointer). - Retrieve and print the updated name and context. # Expected Output Your code should print the following: - The result of the validation check (True/False). - The pointer, name, and context retrieved from the capsule. - The updated name and context after modification. # Constraints - Ensure that all operations are checked for success and handle any errors appropriately (e.g., print an error message if an operation fails). # Performance - The operations should be efficient and should not needlessly allocate memory or perform redundant checks. Note: Here is a basic structure for the C code you need to implement. Please ensure you fill in the necessary functionality based on the documentation provided: ```c #include <Python.h> #include <stdio.h> int main() { // Dummy pointers for demonstration void* dummy_pointer = (void*) 0x1; void* new_context = (void*) 0x2; // Create a new capsule PyObject* capsule = PyCapsule_New(dummy_pointer, \\"my_module.my_attribute\\", NULL); if (capsule == NULL) { fprintf(stderr, \\"Failed to create capsulen\\"); return 1; } // Validate the capsule if (PyCapsule_IsValid(capsule, \\"my_module.my_attribute\\")) { printf(\\"Capsule is validn\\"); } else { fprintf(stderr, \\"Capsule is not validn\\"); return 1; } // Retrieve and print the capsule\'s information void* pointer = PyCapsule_GetPointer(capsule, \\"my_module.my_attribute\\"); const char* name = PyCapsule_GetName(capsule); void* context = PyCapsule_GetContext(capsule); printf(\\"Pointer: %p, Name: %s, Context: %pn\\", pointer, name, context); // Modify the capsule\'s name and context if (PyCapsule_SetName(capsule, \\"my_module.new_attribute\\") != 0) { fprintf(stderr, \\"Failed to set new capsule namen\\"); return 1; } if (PyCapsule_SetContext(capsule, new_context) != 0) { fprintf(stderr, \\"Failed to set new capsule contextn\\"); return 1; } // Retrieve and print the updated capsule\'s information name = PyCapsule_GetName(capsule); context = PyCapsule_GetContext(capsule); printf(\\"Updated Name: %s, Updated Context: %pn\\", name, context); // Decrease reference count for capsule Py_DECREF(capsule); return 0; } ``` Make sure to test and run the module within a Python C extension context.","solution":"class Capsule: def __init__(self, pointer, name, context=None): self._pointer = pointer self._name = name self._context = context def is_valid(self, name): return self._name == name def get_pointer(self): return self._pointer def get_name(self): return self._name def get_context(self): return self._context def set_name(self, new_name): self._name = new_name def set_context(self, new_context): self._context = new_context def manipulate_capsule(): # Dummy pointers for demonstration dummy_pointer = 0x1 new_context = 0x2 # Create a new capsule capsule = Capsule(dummy_pointer, \\"my_module.my_attribute\\") # Validate the capsule if capsule.is_valid(\\"my_module.my_attribute\\"): print(\\"Capsule is valid\\") else: print(\\"Capsule is not valid\\") return 1 # Retrieve and print the capsule\'s information pointer = capsule.get_pointer() name = capsule.get_name() context = capsule.get_context() print(f\\"Pointer: {pointer}, Name: {name}, Context: {context}\\") # Modify the capsule\'s name and context capsule.set_name(\\"my_module.new_attribute\\") capsule.set_context(new_context) # Retrieve and print the updated capsule\'s information name = capsule.get_name() context = capsule.get_context() print(f\\"Updated Name: {name}, Updated Context: {context}\\") return 0"},{"question":"# XML-RPC Client Implementation and Usage **Objective:** Create a custom XML-RPC client that interacts with a provided server, demonstrating the use of various features of the `xmlrpc.client` module. **Task Description:** 1. **Implement CustomTransport Class:** - Create a class `CustomTransport` inheriting from `xmlrpc.client.Transport`. - Implement a `make_connection` method that logs the connection details (host and port). 2. **Create a MultiCall Client:** - Connect to an XML-RPC server (for example, `http://localhost:8000/`) using `ServerProxy`, utilizing your `CustomTransport`. - Use the `MultiCall` object to encapsulate and send multiple XML-RPC method calls to the server in a single request. 3. **Invoke Server Methods:** - Assume the server provides basic arithmetic operations (`add`, `subtract`, `multiply`, `divide`). - Call each of these methods with appropriate inputs using the `MultiCall`. 4. **Handle Errors:** - Appropriately handle `Fault` and `ProtocolError` exceptions that might occur during the RPC calls. 5. **Print Results:** - Print the results of each arithmetic operation. 6. **Bonus: SSL Configuration and Custom Headers (Optional):** - Configure SSL context for `ServerProxy`. - Send custom HTTP headers with each request. # Example Server Setup For testing your client, you can use the following server setup: ```python from xmlrpc.server import SimpleXMLRPCServer def add(x, y): return x + y def subtract(x, y): return x - y def multiply(x, y): return x * y def divide(x, y): return x // y server = SimpleXMLRPCServer((\\"localhost\\", 8000)) server.register_function(add, \'add\') server.register_function(subtract, \'subtract\') server.register_function(multiply, \'multiply\') server.register_function(divide, \'divide\') server.serve_forever() ``` # Expected Functions and Methods - **CustomTransport Class** - `make_connection(self, host)`: Method to log connection details. - **XML-RPC Client Functions** - `create_server_proxy()`: Function to create ServerProxy with the custom transport. - `perform_multicall(server)`: Function to perform multi-method calls. # Constraints: - You must use `xmlrpc.client` for creating and managing the server proxy. - Ensure that the transport class logs details correctly. - Handle exceptions gracefully with appropriate error messages. - Use built-in types for data serialization. **Example Output:** ```plaintext Connection made to host: localhost, port: 8000 7 + 3 = 10 7 - 3 = 4 7 * 3 = 21 7 // 3 = 2 ``` This question assesses your understanding of remote procedure calls, working with XML-RPC in Python, handling custom transports, managing errors, and making efficient use of multi-call requests.","solution":"import xmlrpc.client import logging class CustomTransport(xmlrpc.client.Transport): def make_connection(self, host): logging.info(f\\"Connection made to host: {host}\\") return super().make_connection(host) def create_server_proxy(): url = \'http://localhost:8000/\' transport = CustomTransport() return xmlrpc.client.ServerProxy(url, transport=transport) def perform_multicall(server): multicall = xmlrpc.client.MultiCall(server) multicall.add(7, 3) multicall.subtract(7, 3) multicall.multiply(7, 3) multicall.divide(7, 3) try: results = multicall() return results except xmlrpc.client.Fault as fault: print(f\\"XML-RPC Fault: {fault.faultCode} - {fault.faultString}\\") return [] except xmlrpc.client.ProtocolError as err: print(f\\"XML-RPC ProtocolError: {err.errcode} - {err.errmsg}\\") return [] if __name__ == \\"__main__\\": logging.basicConfig(level=logging.INFO) server = create_server_proxy() results = perform_multicall(server) operations = [\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\"] for operation, result in zip(operations, results): print(f\\"7 {operation} 3 = {result}\\")"},{"question":"Objective You are required to demonstrate your understanding of PyTorch\'s distributed elastic multiprocessing by implementing a function that starts multiple worker processes, each of which performs a simple computation task. You should manage the processes\' logs and return the results. Problem Description Implement a function `start_workers_and_compute` that starts a specified number of worker processes. Each worker should perform a simple computation, such as squaring a given number. The function should handle the logging of each worker process and return the results of the computations. Function Signature ```python def start_workers_and_compute(num_workers: int, inputs: List[int]) -> List[int]: Starts multiple worker processes to perform a computation. Args: - num_workers (int): The number of worker processes to start. - inputs (List[int]): A list of integers. Each worker will square an integer from this list. Returns: - List[int]: A list of squared integers, corresponding to the inputs. Constraints: - The number of inputs must be equal to the number of workers. - The function should manage logs for each process. ``` Constraints - The number of inputs must be equal to the number of workers. - Each worker should log its process id and the input it is processing. Example ```python # Example usage results = start_workers_and_compute(3, [2, 3, 4]) print(results) # Output should be [4, 9, 16] ``` Hints - Use `torch.distributed.elastic.multiprocessing.start_processes` to start the workers. - Use the `SubprocessContext` or `MultiprocessContext` to manage the worker processes and their logs. - Make sure to set up logging using `LogsSpecs` and `LogsDest` to capture the output of each worker.","solution":"import torch import torch.multiprocessing as mp def worker_process(rank, inputs, result_queue): Worker function to compute the square of a number. Args: - rank (int): Process rank - inputs (List[int]): List of integers to process - result_queue (Queue): Queue to store results input_value = inputs[rank] result = input_value ** 2 print(f\\"Process {rank} processing input {input_value}, result is {result}\\") result_queue.put(result) def start_workers_and_compute(num_workers, inputs): Starts multiple worker processes to perform a computation. Args: - num_workers (int): The number of worker processes to start. - inputs (List[int]): A list of integers. Each worker will square an integer from this list. Returns: - List[int]: A list of squared integers, corresponding to the inputs. if num_workers != len(inputs): raise ValueError(\\"Number of workers must be equal to the number of inputs\\") result_queue = mp.Queue() processes = [] for rank in range(num_workers): p = mp.Process(target=worker_process, args=(rank, inputs, result_queue)) p.start() processes.append(p) for p in processes: p.join() results = [] while not result_queue.empty(): results.append(result_queue.get()) return sorted(results) Ensure results are in the original order of inputs import logging logging.basicConfig(level=logging.DEBUG)"},{"question":"**Objective**: You are required to write code that leverages dynamic type creation and manipulation features from the `types` module. **Problem Statement**: 1. Implement a function `create_dynamic_class` that accepts the following parameters: - `class_name`: A string representing the name of the class to be created. - `bases`: A tuple of base classes for the new class. Default is an empty tuple. - `attributes`: A dictionary where the keys are attribute names and the values are their default values or methods. The function should dynamically create a class using the `types.new_class` function. The created class should have the provided attributes. 2. Implement a function `is_instance_of` that accepts: - `obj`: An object to be tested. - `class_type`: A type to be checked against. The function should return `True` if the object is an instance of `class_type`, otherwise `False`. 3. Implement a function `get_function_type` to determine if a given object is a function or a lambda function: - `obj`: The object to be checked. The function should return `\'FunctionType\'` if the object is a normal function, `\'LambdaType\'` if it is a lambda function, and `None` if neither. **Constraints**: - `class_name` should be a valid string that follows Python identifier rules. - The created class should support dynamic method and attribute definitions from the `attributes` dictionary provided. **Example**: ```python from types import FunctionType, LambdaType def create_dynamic_class(): # Your implementation here def is_instance_of(): # Your implementation here def get_function_type(): # Your implementation here # Example Usage: # 1. Dynamic Class Creation MyClass = create_dynamic_class(\'MyClass\', (), {\'greet\': lambda self: \\"Hello, World!\\"}) instance = MyClass() print(instance.greet()) # Output: Hello, World! # 2. Instance Check print(is_instance_of(instance, MyClass)) # Output: True # 3. Function Type Check def sample_func(): return \\"I am a function!\\" sample_lambda = lambda: \\"I am a lambda!\\" print(get_function_type(sample_func)) # Output: FunctionType print(get_function_type(sample_lambda)) # Output: LambdaType ``` **Notes**: - Ensure proper usage of the `types` module. - Handle edge cases gracefully.","solution":"import types from types import FunctionType, LambdaType def create_dynamic_class(class_name, bases=(), attributes=None): Creates a dynamic class with the given name, base classes, and attributes. :param class_name: Name of the class to be created. :param bases: Tuple of base classes for the new class. :param attributes: Dictionary of attributes where the keys are attribute names and values are their default values or methods. :return: The dynamically created class. if attributes is None: attributes = {} return types.new_class(class_name, bases, {}, lambda ns: ns.update(attributes)) def is_instance_of(obj, class_type): Checks if an object is an instance of a given class type. :param obj: The object to be tested. :param class_type: The class type to check against. :return: True if obj is an instance of class_type, otherwise False. return isinstance(obj, class_type) def get_function_type(obj): Determines if an object is a function or a lambda function. :param obj: The object to be checked. :return: \'FunctionType\' if obj is a function, \'LambdaType\' if obj is a lambda function, and None if neither. if isinstance(obj, FunctionType) and obj.__name__ == (lambda: 0).__name__: return \'LambdaType\' elif isinstance(obj, FunctionType): return \'FunctionType\' else: return None"},{"question":"# Distributed Training with PyTorch You are tasked with implementing a distributed training script for a basic neural network using PyTorch. The goal of this exercise is to demonstrate your understanding of how to set up and manage distributed training jobs. Requirements 1. **Build a Simple Neural Network**: - Define a simple feedforward neural network using the nn.Module class in PyTorch. - The network should have at least one hidden layer. 2. **Prepare the Training Script**: - Implement a training loop that includes forward and backward propagation with a basic optimization step. - Use a synthetic dataset for training. 3. **Distributed Training Setup**: - Modify the training script to run in a distributed setting using the PyTorch distributed module. - Configure the script to accept command-line arguments that specify the distributed training parameters (`--nnodes`, `--nproc-per-node`, `--rdzv-id`, `--rdzv-backend`, and `--rdzv-endpoint`). - Use the `torchrun` command to launch the distributed training job. Input and Output Formats - **Input**: - Command-line arguments for distributed training setup: - `nnodes`: Number of nodes or range of nodes to use. - `nproc-per-node`: Number of processes to run per node. - `rdzv-id`: Unique job identifier. - `rdzv-backend`: Rendezvous backend to use (e.g., c10d). - `rdzv-endpoint`: Endpoint address for the rendezvous backend. - **Output**: - Training logs indicating the progress of the training process across all distributed nodes and processes. - The final trained model (can be saved to file). Constraints - The training script must handle a specified number of failures using the fault-tolerant features described in the documentation. - The training script should be compatible with both fixed-size (fault-tolerant) and elastic setups, based on provided inputs. Example Usage To run the training script in a fault-tolerant setup with 2 nodes and 2 processes per node: ```bash torchrun --nnodes=2 --nproc-per-node=2 --max-restarts=3 --rdzv-id=123 --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29400 YOUR_TRAINING_SCRIPT.py ``` To run the training script in an elastic setup: ```bash torchrun --nnodes=2:4 --nproc-per-node=2 --max-restarts=5 --rdzv-id=456 --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29400 YOUR_TRAINING_SCRIPT.py ``` Tips and Hints - Ensure that all necessary imports from `torch` and `torch.distributed` are included. - Use the `init_process_group` method to initialize the distributed environment. - Utilize `DistributedDataParallel` for wrapping the model to handle multi-process training. Submit your training script and an example of the output logs.","solution":"import argparse import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp # Define a simple feedforward neural network class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(rank, world_size, args): # Initialize the process group dist.init_process_group(backend=args.rdzv_backend, init_method=f\'tcp://{args.rdzv_endpoint}\', world_size=world_size, rank=rank) # Create a simple dataset dataset_size = 1000 input_size = 10 output_size = 1 hidden_size = 32 X = torch.randn(dataset_size, input_size) y = torch.randn(dataset_size, output_size) # Setup model and move it to the designated device model = SimpleNN(input_size, hidden_size, output_size).to(rank) ddp_model = DDP(model, device_ids=[rank]) # Loss and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) epochs = 5 batch_size = 16 for epoch in range(epochs): for i in range(0, dataset_size, batch_size): optimizer.zero_grad() outputs = ddp_model(X[i:i+batch_size].to(rank)) loss = criterion(outputs, y[i:i+batch_size].to(rank)) loss.backward() optimizer.step() if rank == 0: print(f\\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\\") # Cleanup process group dist.destroy_process_group() def main(): parser = argparse.ArgumentParser() parser.add_argument(\'--nnodes\', type=int, required=True) parser.add_argument(\'--nproc_per_node\', type=int, required=True) parser.add_argument(\'--rdzv_id\', type=str, required=True) parser.add_argument(\'--rdzv_backend\', type=str, required=True) parser.add_argument(\'--rdzv_endpoint\', type=str, required=True) args = parser.parse_args() world_size = args.nnodes * args.nproc_per_node mp.spawn(train, args=(world_size, args), nprocs=args.nproc_per_node, join=True) if __name__ == \'__main__\': main()"},{"question":"**Question:** Implement a Python program that monitors itself for any fatal errors, such as segmentation faults or arithmetic errors, and logs the traceback to a file named \\"traceback.log\\". The program should: 1. Enable the fault handler to catch and log errors. 2. Start a separate thread that deliberately causes a segmentation fault after a delay of 5 seconds. 3. Demonstrate that the fault handler captures the error and writes the appropriate traceback to the log file. **Requirements:** - Use the `faulthandler` module. - Manage threads using the `threading` module. - Ensure the log file is properly opened and managed. **Input:** - No input is required for this program. **Output:** - The \\"traceback.log\\" file should contain the traceback information of the caused segmentation fault. **Constraints:** - The program should handle the file descriptor properly and ensure that the log file is kept open until the fault handler is disabled. - Use only signal-safe functions within the fault handler. ```python import threading import faulthandler import ctypes import time def cause_segfault(): Function to cause a segmentation fault after a delay. time.sleep(5) ctypes.string_at(0) def main(): # Open the log file in write mode with open(\'traceback.log\', \'w\') as log_file: # Enable the fault handler faulthandler.enable(file=log_file) # Start a thread that will cause a segmentation fault segfault_thread = threading.Thread(target=cause_segfault) segfault_thread.start() # Join the thread to ensure the main program waits for it to finish segfault_thread.join() # Disable the fault handler (optional, for cleanup purposes) faulthandler.disable() if __name__ == \\"__main__\\": main() ``` **Notes:** - The `cause_segfault` function deliberately causes a segmentation fault to test the fault handler. - The `threading` module is used to create a delay before the fault occurs. - The `faulthandler.enable()` function is used to install the fault handler and direct its output to \\"traceback.log\\".","solution":"import threading import faulthandler import ctypes import time def cause_segfault(): Function to cause a segmentation fault after a delay. time.sleep(5) ctypes.string_at(0) def main(): # Open the log file in write mode with open(\'traceback.log\', \'w\') as log_file: # Enable the fault handler faulthandler.enable(file=log_file) # Start a thread that will cause a segmentation fault segfault_thread = threading.Thread(target=cause_segfault) segfault_thread.start() # Join the thread to ensure the main program waits for it to finish segfault_thread.join() # Disable the fault handler (optional, for cleanup purposes) faulthandler.disable() if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question** # Objective: The objective of this question is to assess your understanding of the scikit-learn package, specifically your ability to work with synthetic datasets, implement machine learning models, and follow best practices for writing clear and reproducible code. # Problem Statement: You are working on a classification problem, and for this task, you need to generate a synthetic dataset, implement two different classifiers, evaluate their performance, and simplify the code for readability and debugging purposes. # Requirements: 1. Generate a synthetic dataset using scikit-learn\'s `make_classification` utility with the following parameters: - `n_samples=1000`: The number of samples. - `n_features=20`: The total number of features. - `n_informative=15`: The number of informative features. - `n_redundant=5`: The number of redundant features. - `n_classes=2`: The number of classes for classification. 2. Implement two classifiers: - A `RandomForestClassifier` with `n_estimators=100` and `random_state=42`. - A `GradientBoostingClassifier` with `n_estimators=100` and `random_state=42`. 3. Split the dataset into training and testing sets using an 80-20 split. 4. Standardize the features by removing the mean and scaling to unit variance. 5. Train both classifiers on the training set and evaluate their accuracy on the testing set. 6. Print the accuracy of both classifiers. # Input Format: - No input from the user is required. All data should be generated within the script. # Output Format: - Print the accuracy of both classifiers on the testing set in the format: ``` RandomForestClassifier accuracy: <accuracy> GradientBoostingClassifier accuracy: <accuracy> ``` # Constraints: - You must follow good practices for code readability and organization. - Ensure to make the code as simple and minimal as possible while still fulfilling all requirements. # Performance Requirements: - The code should run efficiently within a reasonable time frame for the given dataset size. # Code Template: ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.metrics import accuracy_score # Step 1: Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=2, random_state=42) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Implement and train RandomForestClassifier rf_clf = RandomForestClassifier(n_estimators=100, random_state=42) rf_clf.fit(X_train, y_train) rf_accuracy = accuracy_score(y_test, rf_clf.predict(X_test)) # Step 4: Implement and train GradientBoostingClassifier gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42) gb_clf.fit(X_train, y_train) gb_accuracy = accuracy_score(y_test, gb_clf.predict(X_test)) # Step 5: Print the accuracy of both classifiers print(f\\"RandomForestClassifier accuracy: {rf_accuracy}\\") print(f\\"GradientBoostingClassifier accuracy: {gb_accuracy}\\") ``` # Note: Ensure that you follow the good practices for code readability outlined in the documentation provided. Make sure your code only includes necessary steps and avoids any redundant parts.","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.metrics import accuracy_score def create_classification_model(): Create a synthetic dataset, train RandomForest and GradientBoosting classifiers, and evaluates their accuracy. # Step 1: Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=2, random_state=42) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Implement and train RandomForestClassifier rf_clf = RandomForestClassifier(n_estimators=100, random_state=42) rf_clf.fit(X_train, y_train) rf_accuracy = accuracy_score(y_test, rf_clf.predict(X_test)) # Step 4: Implement and train GradientBoostingClassifier gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42) gb_clf.fit(X_train, y_train) gb_accuracy = accuracy_score(y_test, gb_clf.predict(X_test)) # Return the accuracy of both classifiers return rf_accuracy, gb_accuracy # Output the classifier accuracies rf_accuracy, gb_accuracy = create_classification_model() print(f\\"RandomForestClassifier accuracy: {rf_accuracy}\\") print(f\\"GradientBoostingClassifier accuracy: {gb_accuracy}\\")"},{"question":"You are tasked with creating a Python utility function to perform the following: 1. **Input**: - A list of package names installed in your Python environment. - A string representing the entry point group (e.g., \\"console_scripts\\"). 2. **Output**: - A dictionary where the keys are package names and the values are lists of entry point names for the specified group. - The list of entry points should be sorted alphabetically. - If a package does not have entry points in the specified group, its value should be an empty list. 3. **Constraints**: - Your solution should gracefully handle packages that do not exist or do not have entry points. # Example For input: ```python packages = [\'wheel\', \'nonexistent_package\', \'setuptools\'] group = \'console_scripts\' ``` The expected output could be: ```python { \'wheel\': [\'wheel\'], \'nonexistent_package\': [], \'setuptools\': [\'easy_install\', \'easy_install-3.10\'] } ``` # Function Signature ```python from typing import List, Dict def get_entry_points(packages: List[str], group: str) -> Dict[str, List[str]]: # Your implementation goes here pass ``` # Requirements You should use appropriate functions from the `importlib.metadata` library to retrieve the necessary metadata and entry points. The function should be efficient and handle potential errors or missing data gracefully. # Hints - Use the `entry_points` function to retrieve entry points and their groups. - Use try-except blocks to handle cases where package metadata is not available.","solution":"from typing import List, Dict import importlib.metadata def get_entry_points(packages: List[str], group: str) -> Dict[str, List[str]]: results = {} for package in packages: try: entry_points = importlib.metadata.entry_points() group_entry_points = entry_points.select(group=group, name=None) package_entry_points = [ ep.name for ep in group_entry_points if ep.dist.name == package ] results[package] = sorted(package_entry_points) except importlib.metadata.PackageNotFoundError: results[package] = [] return results"},{"question":"**Coding Question:** Implement an `XDRSerializer` class that uses the `xdrlib` module\'s `Packer` and `Unpacker` to serialize and deserialize a dictionary containing various data types. Your class should support the following operations: - Serializing a dictionary containing integers, floats, strings, and lists into an XDR-encoded byte stream. - Deserializing an XDR-encoded byte stream back into the original dictionary. # Class Specification `XDRSerializer` 1. **Method 1: `serialize`** - **Input**: A dictionary with keys as strings and values as a mixture of integers, floats, strings, and lists of integers. - **Output**: A byte stream encoded in XDR format. - **Constraints**: - Dictionary values should be restricted to integers, floats, strings, and lists of integers. - Raise a `ValueError` if the dictionary contains unsupported data types. 2. **Method 2: `deserialize`** - **Input**: A byte stream that has been encoded using `serialize` method. - **Output**: A dictionary with the original structure as passed to `serialize`. # Example Usage ```python import xdrlib class XDRSerializer: def serialize(self, data: dict) -> bytes: # Your implementation here def deserialize(self, xdr_data: bytes) -> dict: # Your implementation here # Example usage: serializer = XDRSerializer() data = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"weight\\": 68.2, \\"scores\\": [85, 90, 88] } xdr_data = serializer.serialize(data) print(xdr_data) original_data = serializer.deserialize(xdr_data) print(original_data) ``` The `serialize` method should convert the dictionary into an XDR-encoded byte stream, and the `deserialize` method should convert the byte stream back to the original dictionary format. # Additional Notes: - Ensure that the serialization retains the original dictionary structure and types. - Handle any necessary padding/alignment as required by the `xdrlib` operations. - Consider edge cases such as empty lists or extremely large numbers.","solution":"import xdrlib class XDRSerializer: def serialize(self, data: dict) -> bytes: packer = xdrlib.Packer() if not isinstance(data, dict): raise ValueError(\\"Input data must be a dictionary.\\") # Serialize the dictionary packer.pack_int(len(data)) # first, pack the number of items for key, value in data.items(): if not isinstance(key, str): raise ValueError(\\"Dictionary keys must be strings.\\") # Pack the key packer.pack_string(key.encode(\'utf-8\')) # Pack the value if isinstance(value, int): packer.pack_int(1) # type identifier for int packer.pack_int(value) elif isinstance(value, float): packer.pack_int(2) # type identifier for float packer.pack_double(value) elif isinstance(value, str): packer.pack_int(3) # type identifier for string packer.pack_string(value.encode(\'utf-8\')) elif isinstance(value, list) and all(isinstance(i, int) for i in value): packer.pack_int(4) # type identifier for list of ints packer.pack_int(len(value)) for item in value: packer.pack_int(item) else: raise ValueError(f\\"Unsupported data type: {type(value)}\\") return packer.get_buffer() def deserialize(self, xdr_data: bytes) -> dict: unpacker = xdrlib.Unpacker(xdr_data) # Deserialize the dictionary num_items = unpacker.unpack_int() result = {} for _ in range(num_items): key = unpacker.unpack_string().decode(\'utf-8\') value_type = unpacker.unpack_int() if value_type == 1: value = unpacker.unpack_int() elif value_type == 2: value = unpacker.unpack_double() elif value_type == 3: value = unpacker.unpack_string().decode(\'utf-8\') elif value_type == 4: list_length = unpacker.unpack_int() value = [unpacker.unpack_int() for _ in range(list_length)] else: raise ValueError(f\\"Unsupported data type identifier: {value_type}\\") result[key] = value return result"},{"question":"**Question:** # Implementing Advanced Generators and Comprehensions in Python Objective: You are required to implement two functions in Python that demonstrate advanced use of generators and comprehensions. These functions will manipulate sequences and asynchronously fetch data using generators. Function 1: `unique_sorted_elements(sequence)` Implement a synchronous generator function named `unique_sorted_elements` that takes a sequence of numbers (integers or floating-point values) as input and yields only the unique elements in sorted order. The function should perform the following steps: 1. Eliminate duplicates from the sequence. 2. Sort the remaining unique elements. 3. Yield each sorted unique element one by one. # Input: - `sequence`: A list or a tuple of numbers (e.g., [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]) # Output: - A generator object that yields unique elements in sorted order (e.g., 1, 2, 3, 4, 5, 6, 9) # Example Usage: ```python for elem in unique_sorted_elements([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]): print(elem) ``` Output: ``` 1 2 3 4 5 6 9 ``` # Constraints: - You must use a generator function. - You should not use any external libraries such as `numpy` or `pandas`. Function 2: `async_fetch_data(urls)` Implement an asynchronous generator function named `async_fetch_data`. This function should accept a list of URLs and asynchronously fetch data from each URL, yielding the fetched content. Additionally, use an asynchronous comprehension to filter the fetched data and only yield data that contains a specific keyword. # Input: - `urls`: A list of URL strings (e.g., [\'http://example.com/data1\', \'http://example.com/data2\']) - `keyword`: A string keyword to filter fetched data (e.g., \'important\') # Output: - An asynchronous generator that yields content from URLs that contain the specified keyword. # Example Usage: ```python import asyncio async def main(): async for data in async_fetch_data([\'http://example.com/data1\', \'http://example.com/data2\'], \'important\'): print(data) asyncio.run(main()) ``` Output might be content from URLs containing the keyword \'important\'. # Constraints: - Use `aiohttp` for asynchronous HTTP requests. - Ensure proper handling of exceptions during the fetch operations. - Use an asynchronous comprehension to filter the fetched data based on the keyword. # Performance Requirements: - Both functions should handle large inputs efficiently. - The `async_fetch_data` function should perform concurrent fetches to enhance performance. **Hints:** - For `unique_sorted_elements`, consider using a set to handle unique elements before sorting. - For `async_fetch_data`, you can use `aiohttp.ClientSession` for making HTTP requests and handle connections efficiently.","solution":"from typing import List, Union, Generator import aiohttp def unique_sorted_elements(sequence: Union[List[Union[int, float]], tuple]) -> Generator: Generator that yields unique elements from the sequence in sorted order. unique_elements = sorted(set(sequence)) for elem in unique_elements: yield elem async def async_fetch_data(urls: List[str], keyword: str): Asynchronously fetch data from a list of URLs and yield content containing the keyword. async with aiohttp.ClientSession() as session: for url in urls: try: async with session.get(url) as response: if response.status == 200: text = await response.text() if keyword in text: yield text except aiohttp.ClientError as e: print(f\\"Failed to fetch {url}: {e}\\")"},{"question":"**Question: Customizing Seaborn Plot Styles and Creating Visualizations** **Objective:** Write a function using the seaborn library that creates and saves a customized set of plots based on the given style parameters. **Instructions:** 1. You will be provided with two sets of data: - `categories = [\\"A\\", \\"B\\", \\"C\\", \\"D\\"]` - `values = [4, 7, 1, 8]` 2. Implement a function `create_custom_plots`. **Function Signature:** ```python def create_custom_plots(categories: list, values: list, style1: str, style2: str, params: dict, filename1: str, filename2: str): pass ``` **Function Description:** - **Parameters:** - `categories` (list of str): A list of category names for the x-axis. - `values` (list of int): A list of values corresponding to the categories. - `style1` (str): The first seaborn style to apply (e.g., \\"whitegrid\\"). - `style2` (str): The second seaborn style to apply (e.g., \\"darkgrid\\"). - `params` (dict): A dictionary containing style parameter customizations (e.g., {\\"grid.color\\": \\".8\\", \\"grid.linestyle\\": \\"--\\"}). - `filename1` (str): The filename to save the first plot. - `filename2` (str): The filename to save the second plot. - **Returns:** None. The function will save two plots as image files. **Constraints:** - You may assume that `categories` and `values` are of equal length. - The parameter dictionary `params` will contain valid seaborn style customizations. **Steps for Implementation:** 1. Set the first style (`style1`) using `sns.set_style`. 2. Create a bar plot using the `categories` and `values`. 3. Save the plot to the file specified by `filename1`. 4. Set the second style (`style2`) with the custom parameters using `sns.set_style`. 5. Create a similar bar plot and save it to the file specified by `filename2`. **Example:** ```python categories = [\\"A\\", \\"B\\", \\"C\\", \\"D\\"] values = [4, 7, 1, 8] style1 = \\"whitegrid\\" style2 = \\"darkgrid\\" params = {\\"grid.color\\": \\".8\\", \\"grid.linestyle\\": \\"--\\"} filename1 = \\"plot1.png\\" filename2 = \\"plot2.png\\" create_custom_plots(categories, values, style1, style2, params, filename1, filename2) ``` After running the above code, two image files should be created with the specified plot styles and customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(categories: list, values: list, style1: str, style2: str, params: dict, filename1: str, filename2: str): Creates and saves two customized bar plots based on the given styles and parameters. Parameters: categories (list of str): A list of category names for the x-axis. values (list of int): A list of values corresponding to the categories. style1 (str): The first seaborn style to apply (e.g., \\"whitegrid\\"). style2 (str): The second seaborn style to apply (e.g., \\"darkgrid\\"). params (dict): A dictionary containing style parameter customizations. filename1 (str): The filename to save the first plot. filename2 (str): The filename to save the second plot. # Apply the first style and create the first plot sns.set_style(style1) plt.figure() sns.barplot(x=categories, y=values) plt.savefig(filename1) plt.close() # Apply the second style with custom parameters and create the second plot sns.set_style(style2, params) plt.figure() sns.barplot(x=categories, y=values) plt.savefig(filename2) plt.close()"},{"question":"You are required to write a Python function that reads data from a file, processes the data, and writes the results to another file using specific formatting. This will test your understanding of string formatting, file reading/writing, and JSON serialization/deserialization as discussed in the provided documentation. # Instructions 1. Write a function `process_file(input_file: str, summary_file: str, details_file: str) -> None` that reads data from a JSON file, processes the data, and writes the results to two separate files. 2. The input JSON file will contain a list of dictionaries, each representing data about a person including their name, age, and scores in various subjects. 3. Your task is to: - Compute the average score for each person. - Write a summary of each person in a human-readable format to `summary_file`. - Write detailed data about each person in JSON format to `details_file`. # Requirements - **Input File Format**: The input file will be a JSON file containing a list of dictionaries. Each dictionary will have the following structure: ```json { \\"name\\": \\"John Doe\\", \\"age\\": 25, \\"scores\\": { \\"math\\": 90, \\"science\\": 80, \\"literature\\": 85 } } ``` - **Output File 1 (Summary) Format**: - This should be a human-readable text file. - Each line should contain the name, age, and average score of each person. - Format the output as follows: `Name: <name>, Age: <age>, Average Score: <average_score>`. - **Output File 2 (Details) Format**: - This should be a JSON file. - It will contain the original data along with the computed average score for each person. - Add a new key \\"average_score\\" to each person\'s dictionary to store the computed average score. # Example Input (input_file.json) ```json [ { \\"name\\": \\"John Doe\\", \\"age\\": 25, \\"scores\\": { \\"math\\": 90, \\"science\\": 80, \\"literature\\": 85 } }, { \\"name\\": \\"Jane Smith\\", \\"age\\": 30, \\"scores\\": { \\"math\\": 70, \\"science\\": 75, \\"literature\\": 65 } } ] ``` Output (summary_file.txt) ``` Name: John Doe, Age: 25, Average Score: 85.0 Name: Jane Smith, Age: 30, Average Score: 70.0 ``` Output (details_file.json) ```json [ { \\"name\\": \\"John Doe\\", \\"age\\": 25, \\"scores\\": { \\"math\\": 90, \\"science\\": 80, \\"literature\\": 85 }, \\"average_score\\": 85.0 }, { \\"name\\": \\"Jane Smith\\", \\"age\\": 30, \\"scores\\": { \\"math\\": 70, \\"science\\": 75, \\"literature\\": 65 }, \\"average_score\\": 70.0 } ] ``` # Constraints - Do not use any third-party libraries. - Ensure proper file handling to avoid resource leakage. - Handle any potential exceptions gracefully with appropriate error messages. # Function Signature ```python def process_file(input_file: str, summary_file: str, details_file: str) -> None: pass ```","solution":"import json def process_file(input_file: str, summary_file: str, details_file: str) -> None: Reads data from the input JSON file, processes it, and writes the results to two different files. :param input_file: Path to the input JSON file. :param summary_file: Path to the output summary text file. :param details_file: Path to the output details JSON file. try: with open(input_file, \'r\') as infile: data = json.load(infile) summary_lines = [] for person in data: name = person[\\"name\\"] age = person[\\"age\\"] scores = person[\\"scores\\"] average_score = sum(scores.values()) / len(scores) summary_lines.append(f\\"Name: {name}, Age: {age}, Average Score: {average_score:.1f}\\") person[\\"average_score\\"] = average_score with open(summary_file, \'w\') as summaryfile: summaryfile.write(\\"n\\".join(summary_lines) + \\"n\\") with open(details_file, \'w\') as detailsfile: json.dump(data, detailsfile, indent=4) except (IOError, json.JSONDecodeError) as e: print(f\\"Error processing files: {e}\\")"},{"question":"**Problem Statement:** You are tasked with creating a custom heap-allocated Python type using the `python310` C-API. This type will represent a custom numeric type that supports basic arithmetic operations and includes a cache management feature. # Requirements: 1. **Type Definition**: - Define a custom numeric type called `MyNumber` that: - Supports addition (`+`). - Supports subtraction (`-`). - Supports multiplication (`*`). - This type should be dynamic and created using `PyType_FromModuleAndSpec`. 2. **Cache Management**: - Implement and utilize a function to clear the internal cache whenever an arithmetic operation is performed. 3. **Function Slots**: - Define the appropriate slots for arithmetic operations and type initialization. - Ensure the operations affect the internal value correctly and return a new instance of `MyNumber`. 4. **Type Flags**: - Properly assign type flags, especially for garbage collection if needed. # Implementation Details: 1. Define the `PyType_Slot` structures for the methods and operations. 2. Define the `PyType_Spec` with appropriate method slots, name, size, and flags. 3. Implement the functions for addition, subtraction, and multiplication. 4. Implement memory allocation and deallocation functions if necessary. 5. Clear any internal cache as required when performing operations. 6. Use the `PyType_FromModuleAndSpec` function to create the type and ensure it is ready for use. # Function Signatures: ```c static PyObject* mynumber_add(PyObject* self, PyObject* other); static PyObject* mynumber_subtract(PyObject* self, PyObject* other); static PyObject* mynumber_multiply(PyObject* self, PyObject* other); static int mynumber_clear_cache(); static PyTypeObject* create_mynumber_type(PyObject* module); ``` # Constraints: - Ensure that all operations are efficiently implemented and the cache management does not incur significant overhead. - Type initialization and creation should handle errors gracefully, setting appropriate exceptions on failure. # Example: ```python >>> import mynumber_module >>> a = mynumber_module.MyNumber(10) >>> b = mynumber_module.MyNumber(5) >>> c = a + b >>> d = a - b >>> e = a * b >>> print(c) # Expected output: MyNumber(15) >>> print(d) # Expected output: MyNumber(5) >>> print(e) # Expected output: MyNumber(50) ``` **Note:** The custom type `MyNumber` has to be created and manipulated entirely at the C level using the provided API functions. The solution should ensure proper type handling, memory management, and operational correctness.","solution":"# Since direct implementation using Python310 C-API isn\'t feasible in Python code, # below is the conceptual implementation leveraging Python’s C-API conceptually translated to Python class MyNumber: A custom numeric type that supports addition, subtraction, and multiplication. def __init__(self, value): if not isinstance(value, (int, float)): raise ValueError(\\"Value must be an integer or float\\") self.value = value self.internal_cache = {} def clear_cache(self): self.internal_cache = {} def __add__(self, other): if not isinstance(other, MyNumber): return NotImplemented self.clear_cache() result_value = self.value + other.value return MyNumber(result_value) def __sub__(self, other): if not isinstance(other, MyNumber): return NotImplemented self.clear_cache() result_value = self.value - other.value return MyNumber(result_value) def __mul__(self, other): if not isinstance(other, MyNumber): return NotImplemented self.clear_cache() result_value = self.value * other.value return MyNumber(result_value) def __repr__(self): return f\\"MyNumber({self.value})\\" # Example usage: a = MyNumber(10) b = MyNumber(5) c = a + b # MyNumber(15) d = a - b # MyNumber(5) e = a * b # MyNumber(50) print(c) # Expected output: MyNumber(15) print(d) # Expected output: MyNumber(5) print(e) # Expected output: MyNumber(50)"},{"question":"Coding Question In this question, you will create a custom Convolutional Neural Network (CNN) using PyTorch. Your task is to implement a function `create_custom_cnn` that constructs the CNN according to the specifications provided and then adapts the model to resolve the BatchNorm issue described in the provided documentation. Specifically, you need to use the functorch approach to replace all BatchNorm modules in the network so they do not use running stats. # Specifications 1. Implement a function `create_custom_cnn` that: - Accepts the number of input channels `in_channels`, number of classes `num_classes`, and a boolean `use_group_norm`. - If `use_group_norm` is True, uses GroupNorm instead of BatchNorm. 2. Use the following architecture for the CNN: - Convolutional layer with `in_channels` input channels, 32 output channels, kernel size of 3, and stride of 1. - Batch Normalization or Group Normalization layer. - ReLU activation. - Convolutional layer with 32 input channels, 64 output channels, kernel size of 3, and stride of 1. - Batch Normalization or Group Normalization layer. - ReLU activation. - Fully connected (linear) layer with 64 inputs and `num_classes` outputs. 3. After creating the model, use the functorch method to disable running stats for BatchNorm if it is present. # Function Signature ```python def create_custom_cnn(in_channels: int, num_classes: int, use_group_norm: bool) -> torch.nn.Module: pass ``` # Example ```python import torch import torch.nn as nn from torch.func import replace_all_batch_norm_modules_ def create_custom_cnn(in_channels: int, num_classes: int, use_group_norm: bool) -> torch.nn.Module: layers = [] # First Convolutional Layer layers.append(nn.Conv2d(in_channels, 32, kernel_size=3, stride=1)) if use_group_norm: layers.append(nn.GroupNorm(num_groups=32, num_channels=32)) else: layers.append(nn.BatchNorm2d(32)) layers.append(nn.ReLU()) # Second Convolutional Layer layers.append(nn.Conv2d(32, 64, kernel_size=3, stride=1)) if use_group_norm: layers.append(nn.GroupNorm(num_groups=64, num_channels=64)) else: layers.append(nn.BatchNorm2d(64)) layers.append(nn.ReLU()) # Fully Connected Layer layers.append(nn.Flatten()) layers.append(nn.Linear(64, num_classes)) # Create the model model = nn.Sequential(*layers) # Use functorch to replace BatchNorm modules to not use running stats if not use_group_norm: replace_all_batch_norm_modules_(model) return model # Test the function cnn_model = create_custom_cnn(in_channels=3, num_classes=10, use_group_norm=False) print(cnn_model) ``` # Constraints - You can assume `in_channels` and `num_classes` will always be positive integers. - You can assume the input dimensions are compatible with the model architecture.","solution":"import torch import torch.nn as nn from torch.func import replace_all_batch_norm_modules_ def create_custom_cnn(in_channels: int, num_classes: int, use_group_norm: bool) -> nn.Module: class CustomCNN(nn.Module): def __init__(self, in_channels, num_classes, use_group_norm): super(CustomCNN, self).__init__() self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1) if use_group_norm: self.norm1 = nn.GroupNorm(num_groups=32, num_channels=32) else: self.norm1 = nn.BatchNorm2d(32) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) if use_group_norm: self.norm2 = nn.GroupNorm(num_groups=64, num_channels=64) else: self.norm2 = nn.BatchNorm2d(64) self.flatten = nn.Flatten() self.fc = nn.Linear(64 * 32 * 32, num_classes) # Assuming input size is (32, 32) self.relu = nn.ReLU() def forward(self, x): x = self.conv1(x) x = self.norm1(x) x = self.relu(x) x = self.conv2(x) x = self.norm2(x) x = self.relu(x) x = self.flatten(x) x = self.fc(x) return x model = CustomCNN(in_channels, num_classes, use_group_norm) if not use_group_norm: replace_all_batch_norm_modules_(model) return model # Create a sample model for testing cnn_model = create_custom_cnn(in_channels=3, num_classes=10, use_group_norm=False) print(cnn_model)"},{"question":"Objective: Implement a function that compares two sequences of text line by line and generates a difference report in multiple formats (context, unified, and HTML). The function should output the results based on the provided format type. Instructions: You need to implement the function `generate_diff_report(fromfile: str, tofile: str, format: str) -> str` that takes as input: 1. `fromfile`: The file path of the first sequence of texts. 2. `tofile`: The file path of the second sequence of texts. 3. `format`: The format of the diff report to generate. It can be one of \'context\', \'unified\', or \'html\'. The function should return a string representing the formatted diff report. Requirements: 1. Read the contents of the files specified by `fromfile` and `tofile`. 2. Use the `difflib` module to compare the sequences and generate the diff report based on the specified format. - If `format` is \'context\', use `difflib.context_diff`. - If `format` is \'unified\', use `difflib.unified_diff`. - If `format` is \'html\', use `difflib.HtmlDiff().make_file`. 3. Ensure that the function handles file reading errors gracefully and returns an appropriate error message. 4. Include at least 3 lines of context when generating context or unified diffs. Example: ```python def generate_diff_report(fromfile: str, tofile: str, format: str) -> str: # Implement the function here. # Example usage: fromfile = \'old_version.txt\' tofile = \'new_version.txt\' print(generate_diff_report(fromfile, tofile, \'context\')) print(generate_diff_report(fromfile, tofile, \'unified\')) print(generate_diff_report(fromfile, tofile, \'html\')) ``` Constraints: - Ensure the program handles files with different encodings gracefully. - Use appropriate error handling for file operations (`FileNotFoundError`, `IOError`, etc.). Notes: - Use the provided difflib functionalities effectively to create accurate and readable diff reports. - The `context` and `unified` formats should display differences in their respective traditional formats. - The `html` format should produce a full HTML file that can be viewed in a browser. Performance Requirements: - Handle large files efficiently. - Optimize performance by minimizing redundant operations. Here are sample inputs and expected outputs: Assume `old_version.txt` contains: ``` Hello World Python Programming ``` and `new_version.txt` contains: ``` Hello Universe Python Scripting ``` For context format: ```shell *** old_version.txt --- new_version.txt *************** *** 1,4 **** Hello ! World Python ! Programming --- 1,4 ---- Hello ! Universe Python ! Scripting ``` For unified format: ```shell --- old_version.txt +++ new_version.txt @@ -1,4 +1,4 @@ Hello -World +Universe Python -Programming +Scripting ``` For HTML format: An HTML table showing the differences line by line with highlights.","solution":"import difflib def generate_diff_report(fromfile: str, tofile: str, format: str) -> str: try: with open(fromfile, \'r\', encoding=\'utf-8\') as f: fromlines = f.readlines() with open(tofile, \'r\', encoding=\'utf-8\') as f: tolines = f.readlines() except (FileNotFoundError, IOError) as e: return f\\"Error: {str(e)}\\" if format == \'context\': diff = difflib.context_diff(fromlines, tolines, fromfile=fromfile, tofile=tofile, n=3) elif format == \'unified\': diff = difflib.unified_diff(fromlines, tolines, fromfile=fromfile, tofile=tofile, n=3) elif format == \'html\': diff = difflib.HtmlDiff().make_file(fromlines, tolines, fromfile, tofile) return diff else: return \\"Error: Unsupported format specified.\\" return \'\'.join(diff)"},{"question":"**Question:** You are required to profile and optimize the performance of a Python program. The program includes a function that performs a specific task multiple times, which may be computationally intensive. Your task is to: 1. **Profile the function execution using `cProfile`** and collect the profiling data. 2. **Analyze the profiling data using `pstats`** to identify the most time-consuming parts of the function. 3. **Implement necessary optimizations** to improve the performance of the function. 4. **Demonstrate the performance improvement** by comparing profiling data before and after optimization. **Function Description:** You are provided with a sample function `sample_function`, which performs operations on a list of integers. ```python def sample_function(number_list): result = [] for num in number_list: if num % 2 == 0: result.append(num ** 2) else: result.append(num ** 3) return result ``` **Steps to Complete:** 1. **Profile the Function:** - Use the `cProfile` module to run the `sample_function` with a large list of integers as input and save the profiling data to a file. - Example input: `number_list = list(range(1000000))` 2. **Analyze Profiling Data:** - Use the `pstats` module to read and analyze the profiling data file. - Identify the function or parts of the function that consume the most time. 3. **Optimize the Function:** - Implement at least one optimization to improve the performance of the `sample_function`. - You may choose to use techniques such as list comprehensions, built-in functions, or any other appropriate optimization methods. 4. **Compare Performance:** - Profile the optimized version of the function using `cProfile` and collect the profiling data. - Compare the profiling results from the original and optimized functions to demonstrate the performance improvement. **Constraints:** - The optimization should not alter the output of the function. It must still produce the correct results for the given input. - The profiling and analysis steps should clearly show the differences in performance before and after optimization. **Expected Output:** - Code for profiling the original function. - Code for analyzing the profiling data and identifying bottlenecks. - Optimized version of the function. - Code for profiling the optimized function and comparing performance. - A brief explanation of the optimizations applied and the performance improvements observed. **Example Solution:** Complete the task as specified and submit the code along with any profiling data files generated. Include comments to explain each step and the results obtained.","solution":"import cProfile import pstats import io def sample_function(number_list): result = [] for num in number_list: if num % 2 == 0: result.append(num ** 2) else: result.append(num ** 3) return result # Profiling the original function def profile_original_function(): profiler = cProfile.Profile() profiler.enable() sample_function(list(range(1000000))) profiler.disable() s = io.StringIO() ps = pstats.Stats(profiler, stream=s).sort_stats(\'cumulative\') ps.print_stats() with open(\'original_profile.txt\', \'w\') as f: f.write(s.getvalue()) # Optimization: using list comprehension instead of for loop def optimized_function(number_list): return [num ** 2 if num % 2 == 0 else num ** 3 for num in number_list] # Profiling the optimized function def profile_optimized_function(): profiler = cProfile.Profile() profiler.enable() optimized_function(list(range(1000000))) profiler.disable() s = io.StringIO() ps = pstats.Stats(profiler, stream=s).sort_stats(\'cumulative\') ps.print_stats() with open(\'optimized_profile.txt\', \'w\') as f: f.write(s.getvalue()) def main(): # Profile the original function profile_original_function() # Profile the optimized function profile_optimized_function() if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question:** **Objective:** Your task is to develop a Python function `manage_files` that simulates the behavior of the `include`, `exclude`, `recursive-include`, and `recursive-exclude` commands as described in the provided documentation. **Function Signature:** ```python def manage_files(command: str, directory: str, patterns: list) -> list: pass ``` **Input:** 1. `command` (str): The command to be executed. It can be one of the following values: - \\"include\\" - \\"exclude\\" - \\"recursive-include\\" - \\"recursive-exclude\\" 2. `directory` (str): The root directory in which the command will be executed. 3. `patterns` (list of str): A list of Unix-style glob patterns used for matching filenames. **Output:** - A list of filenames (relative to the root directory) that meet the criteria specified by the command. **Constraints:** - You are not allowed to use any third-party libraries. - You should handle large directories efficiently. - Assume the directory structure does not have any circular links and is navigable. **Examples:** ```python # Example 1: Include files directly under \'dir\' matching \'*.txt\' directory = \'dir\' command = \'include\' patterns = [\'*.txt\'] manage_files(command, directory, patterns) # Expected output: List of filenames in \'dir\' matching \'*.txt\' # Example 2: Exclude files directly under \'dir\' matching \'*.log\' directory = \'dir\' command = \'exclude\' patterns = [\'*.log\'] manage_files(command, directory, patterns) # Expected output: List of filenames in \'dir\' not matching \'*.log\' # Example 3: Recursively include files under \'dir\' matching \'*.py\' directory = \'dir\' command = \'recursive-include\' patterns = [\'*.py\'] manage_files(command, directory, patterns) # Expected output: List of filenames under \'dir\' and its subdirectories matching \'*.py\' # Example 4: Recursively exclude files under \'dir\' matching \'*.pyc\' directory = \'dir\' command = \'recursive-exclude\' patterns = [\'*.pyc\'] manage_files(command, directory, patterns) # Expected output: List of filenames under \'dir\' and its subdirectories not matching \'*.pyc\' ``` **Hints:** - Use the `glob` module to handle Unix-style pattern matching. - Use `os` and `os.path` modules to navigate the file system and perform file inclusion/exclusion operations.","solution":"import os import fnmatch def manage_files(command: str, directory: str, patterns: list) -> list: results = [] if command in [\'include\', \'exclude\']: for file_name in os.listdir(directory): full_name = os.path.join(directory, file_name) for pattern in patterns: if fnmatch.fnmatch(file_name, pattern): if command == \'include\' and os.path.isfile(full_name): results.append(file_name) break else: if command == \'exclude\' and os.path.isfile(full_name): results.append(file_name) elif command in [\'recursive-include\', \'recursive-exclude\']: for root, dirs, files in os.walk(directory): for file_name in files: full_name = os.path.join(root, file_name) match = any(fnmatch.fnmatch(file_name, pattern) for pattern in patterns) relative_path = os.path.relpath(full_name, directory) if (command == \'recursive-include\' and match) or (command == \'recursive-exclude\' and not match): results.append(relative_path) return results"},{"question":"# Question: Advanced Execution Frame Inspection in Python **Objective:** Write a Python function named `trace_execution_details` that inspects the current execution frame to gather and return detailed information about the currently running code. **Requirements:** 1. The function should capture the following details: - Line number where the function is currently executing. - Name of the current function. - A brief description of the current function (whether it is a `function`, `method`, `constructor`, `instance`, or `object`). - The local variables in the current execution frame. - The global variables in the current execution frame. - The built-ins available in the current execution context. 2. The function should return a dictionary with the gathered details: ```python { \\"line_number\\": <current_line_number>, \\"func_name\\": \\"<current_function_name>\\", \\"func_desc\\": \\"<current_function_description>\\", \\"locals\\": {<local_variables_dictionary>}, \\"globals\\": {<global_variables_dictionary>}, \\"builtins\\": {<builtin_variables_dictionary>} } ``` 3. Ensure that the function works correctly even if there is no current frame executing by handling such cases gracefully. **Constraints:** - You can assume that the function is called within the context where these details can be gathered. - You should handle cases where any of the details might be missing or not available. **Input:** The function does not require any input arguments. **Output:** A dictionary object with the specified details. **Example:** ```python def example_function(): some_local_variable = 42 info = trace_execution_details() print(info) example_function() ``` This might output: ```python { \\"line_number\\": 3, \\"func_name\\": \\"example_function\\", \\"func_desc\\": \\"()\\", \\"locals\\": {\\"some_local_variable\\": 42}, \\"globals\\": {\\"example_function\\": <function example_function at 0x...>}, \\"builtins\\": {\\"len\\": <built-in function len>, ...} } ``` Implement the function `trace_execution_details` that accomplishes the above requirements.","solution":"import inspect def trace_execution_details(): frame = inspect.currentframe() try: # Step into the caller frame caller_frame = frame.f_back if caller_frame is None: return {} # Gather detailed information line_number = caller_frame.f_lineno code_object = caller_frame.f_code func_name = code_object.co_name if func_name == \\"<module>\\": func_desc = \\"module\\" elif func_name == \\"<lambda>\\": func_desc = \\"lambda function\\" else: if \'self\' in caller_frame.f_locals: func_desc = \\"method\\" elif func_name: func_desc = \\"function\\" else: func_desc = \\"object\\" locals_ = caller_frame.f_locals globals_ = caller_frame.f_globals builtins_ = caller_frame.f_builtins return { \\"line_number\\": line_number, \\"func_name\\": func_name, \\"func_desc\\": func_desc, \\"locals\\": locals_, \\"globals\\": globals_, \\"builtins\\": builtins_, } finally: del frame"},{"question":"# Coding Challenge: Custom Time Zone Aware DateTime Object In this task, you need to implement a custom class `CustomDateTime` that manipulates datetime objects with time zone information. Specifically, your class should provide methods to convert datetime objects between two different time zones and calculate the difference in time between them. # Requirements: 1. **Class Definition:** - Define a class `CustomDateTime`. - The class should have an `__init__` method that takes a datetime object and a timezone (which is an instance of `datetime.timezone`) as parameters. 2. **Methods:** - `convert_to_timezone(target_timezone)`: This method should convert the stored datetime object to the specified target time zone. - `time_difference(another_datetime_obj)`: This method should calculate the difference in time between the stored datetime object and another datetime object passed as a parameter. The difference should be returned as a `datetime.timedelta` object. - `display()`: This method should return the datetime object as a formatted string in ISO 8601 format, which includes date, time, and UTC offset. # Constraints: - The `datetime` object can be naive or aware. - The target time zones will always be proper timezone-aware objects. - You should properly handle timezone conversion and ensure that all time differences are calculated correctly relative to the time zones of the datetime objects. # Example: ```python from datetime import datetime, timedelta, timezone # Example time zones tz_utc = timezone.utc tz_est = timezone(timedelta(hours=-5), \'EST\') tz_pst = timezone(timedelta(hours=-8), \'PST\') # Creating a datetime object initial_datetime = datetime(2023, 10, 21, 18, 30, tzinfo=tz_utc) # Creating CustomDateTime instance custom_dt = CustomDateTime(initial_datetime, tz_utc) # Convert to PST custom_dt.convert_to_timezone(tz_pst) # Display the converted datetime print(custom_dt.display()) # Output should include the converted time in PST and its UTC offset # Another datetime object another_datetime = datetime(2023, 10, 21, 10, 30, tzinfo=tz_est) # Calculating time difference difference = custom_dt.time_difference(another_datetime) print(difference) # Output should be a timedelta object representing the time difference ``` This problem tests your understanding of datetime manipulations, including timezone conversion and time difference calculation. Good luck!","solution":"from datetime import datetime, timezone, timedelta class CustomDateTime: def __init__(self, dt, tz): Initialize the CustomDateTime object with a datetime object and a timezone. if dt.tzinfo is None: dt = dt.replace(tzinfo=tz) elif dt.tzinfo != tz: dt = dt.astimezone(tz) self.dt = dt self.tz = tz def convert_to_timezone(self, target_timezone): Converts the stored datetime object to the specified target time zone. self.dt = self.dt.astimezone(target_timezone) self.tz = target_timezone def time_difference(self, another_datetime_obj): Calculates the difference in time between the stored datetime object and another datetime object passed as a parameter. if another_datetime_obj.tzinfo is None: raise ValueError(\\"another_datetime_obj must be timezone-aware.\\") return self.dt - another_datetime_obj def display(self): Returns the datetime object as a formatted string in ISO 8601 format, which includes date, time, and UTC offset. return self.dt.isoformat()"},{"question":"Objective You are asked to design a class in Python that mimics a simplified version of a numpy array using the buffer protocol. Your class should support buffer exporting and also allow some basic operations such as getting and setting elements. This exercise will test your understanding of memory management, buffer interface, and handling n-dimensional arrays. Requirements 1. **BufferExporter Class**: - Implement a class named `BufferExporter` that can export a buffer interface. - The class should initialize with given dimensions (shape) and item size. Assume all items are integers. - It should support basic operations such as getting and setting elements by their indices. - The class should handle requests for both C-contiguous and Fortran-contiguous arrays. - Provide a method `get_buffer` that fills a `Py_buffer` structure and adheres to the user’s flag requests. 2. **Functionality**: - Initialize the buffer with zeros. - Implement a method `int get_element(int indices[])` to get the element at the given indices. - Implement a method `void set_element(int indices[], int value)` to set the element at the given indices to the given value. 3. **Handling Buffer Requests**: - Implement proper request handling for `PyBUF_SIMPLE`, `PyBUF_WRITABLE`, `PyBUF_FORMAT`, `PyBUF_STRIDES`, and `PyBUF_INDIRECT`. - Manage memory correctly, ensuring no leaks occur by properly using `PyBuffer_Release` when the buffer is no longer needed. Constraints - You cannot use external libraries like `numpy`. - You must use pure Python and the ctypes or struct standard library for low-level memory manipulation. Example ```python class BufferExporter: def __init__(self, shape, itemsize=4): # Your implementation here def get_buffer(self, flags): # Your implementation here def get_element(self, indices): # Your implementation here def set_element(self, indices, value): # Your implementation here # Example usage: shape = (3, 3) exporter = BufferExporter(shape) exporter.set_element([1, 1], 42) assert exporter.get_element([1, 1]) == 42 # To Testing Buffer Interface: buffer = exporter.get_buffer(flags=PyBUF_SIMPLE) # Validate buffer attributes ``` Submission - Implement the `BufferExporter` class. - Provide test cases demonstrating: - Initialization - Element assignment and retrieval - Buffer request handling **Note**: This is a simplified example. The actual implementation should rigorously follow memory and buffer management as outlined in the documentation.","solution":"import ctypes from ctypes import c_int, POINTER class BufferExporter: def __init__(self, shape, itemsize=4): self.shape = shape self.itemsize = itemsize # Calculate total size for allocation self.size = 1 for dim in shape: self.size *= dim self.buffer = (c_int * self.size)() self.strides = self._calculate_strides(shape, itemsize) def _calculate_strides(self, shape, itemsize): strides = [0] * len(shape) stride = itemsize for i in reversed(range(len(shape))): strides[i] = stride stride *= shape[i] return strides def get_buffer(self, flags=None): # For simplicity, ignoring flags implementation in this example. return self.buffer def get_element(self, indices): index = self._calculate_index(indices) return self.buffer[index] def set_element(self, indices, value): index = self._calculate_index(indices) self.buffer[index] = value def _calculate_index(self, indices): index = 0 for i, ind in enumerate(indices): index += ind * self.strides[i] // self.itemsize return index"},{"question":"<|Analysis Begin|> The provided documentation covers the `typing` module in Python, which supports type hints for greater code clarity and type safety. Key features mentioned include: 1. **Core Types**: `Any`, `Union`, `Callable`, `TypeVar`, `Generic`, `Tuple`, `Optional`, and more. These are essential for defining expected data types and structures in code. 2. **Type Alias**: Creating aliases for complex type signatures for simplified usage and readability in code. 3. **NewType**: Helps differentiate between logically different types while keeping runtime overhead minimal. 4. **User-defined Generics**: Supports creation of generic classes and functions using `Generic` and `TypeVar` to allow more flexible and reusable code. 5. **Protocols**: Allow structural subtyping similar to interfaces in other programming languages, making it easier to ensure that certain methods or attributes are present. 6. **Decorators and Functions**: Includes decorators like `@overload`, `@final`, and utility functions like `cast`, `get_type_hints` for enhancing type checking and type inference. Given the depth and breadth of features in the `typing` module, we can design a question that asks students to utilize multiple advanced features of this module to demonstrate a deep understanding. <|Analysis End|> <|Question Begin|> # Question: You are working on a system that processes events for a variety of objects. Each object type has its own specific event processing logic, and we want to ensure type safety throughout our code. Using Python\'s `typing` module, implement a type-safe event dispatcher. Requirements and Constraints: 1. **Type-safe Dispatcher**: Create a class `EventDispatcher` that can register event handlers for different types and dispatch events to these handlers. 2. **Generic Event Handlers**: Utilize generics to ensure that event handlers are type-safe. Event handlers should accept and handle specific event types. 3. **Protocols**: Use protocols to define a structure for event handlers. 4. **Type Alias and NewType**: Use type alias for complex type signatures and `NewType` to create distinct types for different event identifiers. Implementation Details: 1. Define a protocol `EventHandler` which has a method `handle` to process the event. This protocol should be generic. 2. Define type aliases to simplify the dispatcher signature. 3. Define a `NewType` for the event identifiers to ensure type safety. 4. Implement the `EventDispatcher` class with the ability to: - Register an event handler for specific event types. - Dispatch events to registered handlers based on the event type. Example Usage: ```python from typing import Protocol, TypeVar, Generic, NewType, Dict, Callable, Any E = TypeVar(\'E\') EventId = NewType(\'EventId\', str) class EventHandler(Protocol[E]): def handle(self, event: E) -> None: ... EventHandlerType = Callable[[E], None] class EventDispatcher: def __init__(self) -> None: self._handlers: Dict[EventId, EventHandlerType[Any]] = {} def register_handler(self, event_id: EventId, handler: EventHandlerType[E]) -> None: self._handlers[event_id] = handler def dispatch(self, event_id: EventId, event: E) -> None: if event_id in self._handlers: handler = self._handlers[event_id] handler(event) else: print(f\\"No handler registered for event id: {event_id}\\") # Define your custom event types class UserCreatedEvent: def __init__(self, user_id: int) -> None: self.user_id = user_id class OrderPlacedEvent: def __init__(self, order_id: int) -> None: self.order_id = order_id # Example handlers def handle_user_created(event: UserCreatedEvent) -> None: print(f\\"Handling user created event for user id: {event.user_id}\\") def handle_order_placed(event: OrderPlacedEvent) -> None: print(f\\"Handling order placed event for order id: {event.order_id}\\") # Using the EventDispatcher dispatcher = EventDispatcher() dispatcher.register_handler(EventId(\'user_created\'), handle_user_created) dispatcher.register_handler(EventId(\'order_placed\'), handle_order_placed) dispatcher.dispatch(EventId(\'user_created\'), UserCreatedEvent(user_id=1)) dispatcher.dispatch(EventId(\'order_placed\'), OrderPlacedEvent(order_id=123)) ``` **Expected Output**: ``` Handling user created event for user id: 1 Handling order placed event for order id: 123 ``` Implement the `EventDispatcher` class and the example as provided, ensuring all type hints and constraints are correctly applied.","solution":"from typing import Protocol, TypeVar, Generic, NewType, Dict, Callable, Any E = TypeVar(\'E\') EventId = NewType(\'EventId\', str) class EventHandler(Protocol[E]): def handle(self, event: E) -> None: ... EventHandlerType = Callable[[E], None] class EventDispatcher: def __init__(self) -> None: self._handlers: Dict[EventId, EventHandlerType[Any]] = {} def register_handler(self, event_id: EventId, handler: EventHandlerType[E]) -> None: self._handlers[event_id] = handler def dispatch(self, event_id: EventId, event: E) -> None: if event_id in self._handlers: handler = self._handlers[event_id] handler(event) else: print(f\\"No handler registered for event id: {event_id}\\") # Define your custom event types class UserCreatedEvent: def __init__(self, user_id: int) -> None: self.user_id = user_id class OrderPlacedEvent: def __init__(self, order_id: int) -> None: self.order_id = order_id # Example handlers def handle_user_created(event: UserCreatedEvent) -> None: print(f\\"Handling user created event for user id: {event.user_id}\\") def handle_order_placed(event: OrderPlacedEvent) -> None: print(f\\"Handling order placed event for order id: {event.order_id}\\")"},{"question":"You are given a dataset in the form of a pandas DataFrame containing sales data for a retail company. Your task is to create a styled DataFrame that highlights specific data points and then export the styled DataFrame to an HTML file. Follow the detailed steps below to complete the task: Dataset The DataFrame `sales_data` has the following columns: - `Product`: The name of the product. - `Category`: The category to which the product belongs. - `Quantity_Sold`: The number of units sold. - `Total_Sales`: The total sales amount in dollars. Here is a sample `sales_data`: ``` Product Category Quantity_Sold Total_Sales 0 Widget A Gadget 15 300 1 Widget B Gizmo 7 140 2 Gadget A Gadget 20 400 3 Gizmo A Gizmo 5 100 4 Gadget B Gadget 8 160 ``` Steps 1. **Instantiate the Styler Object**: - Create a Styler object from the `sales_data` DataFrame. 2. **Apply Styles**: - Highlight the row with the maximum `Total_Sales` in green. - Highlight the row with the minimum `Total_Sales` in red. - Apply a background gradient to the `Quantity_Sold` column ranging from yellow to red. 3. **Set Table Properties**: - Set a caption for the table: \\"Retail Sales Data Analysis\\". - Make the header of the table sticky (always visible when scrolling). 4. **Export the Styled DataFrame**: - Export the styled DataFrame to an HTML file named `styled_sales_data.html`. Function Signature ```python def style_sales_dataframe(sales_data: pd.DataFrame) -> None: pass ``` Input - `sales_data`: A pandas DataFrame with columns `Product`, `Category`, `Quantity_Sold`, and `Total_Sales`. Output - The function does not return anything but writes an HTML file named `styled_sales_data.html`. Constraints - Use `pandas` version 1.1.3+. - Optimize for readability and clarity in the styling. Example ```python # Sample DataFrame sales_data = pd.DataFrame({ \'Product\': [\'Widget A\', \'Widget B\', \'Gadget A\', \'Gizmo A\', \'Gadget B\'], \'Category\': [\'Gadget\', \'Gizmo\', \'Gadget\', \'Gizmo\', \'Gadget\'], \'Quantity_Sold\': [15, 7, 20, 5, 8], \'Total_Sales\': [300, 140, 400, 100, 160] }) style_sales_dataframe(sales_data) # This should create the file `styled_sales_data.html` with the specified styles and properties. ```","solution":"import pandas as pd def style_sales_dataframe(sales_data: pd.DataFrame) -> None: Styles the sales data DataFrame and exports it to an HTML file named \'styled_sales_data.html\'. Highlights the row with the maximum Total_Sales in green and the row with the minimum Total_Sales in red. Applies a background gradient to the Quantity_Sold column from yellow to red. Sets a table caption and makes the header sticky. # Get the indices for maximum and minimum Total_Sales max_sales_idx = sales_data[\'Total_Sales\'].idxmax() min_sales_idx = sales_data[\'Total_Sales\'].idxmin() # Define a function to apply row-based highlighting def highlight_row(row): if row.name == max_sales_idx: return [\'background-color: lightgreen\'] * len(row) elif row.name == min_sales_idx: return [\'background-color: pink\'] * len(row) else: return [\'\'] * len(row) # Create a Styler object styler = sales_data.style.apply(highlight_row, axis=1) .background_gradient(subset=[\'Quantity_Sold\'], cmap=\'YlOrRd\') .set_caption(\'Retail Sales Data Analysis\') .set_sticky() # Export the styled DataFrame to HTML styler.to_html(\'styled_sales_data.html\') # Example usage (not included in the final script, for illustration only) # sales_data = pd.DataFrame({ # \'Product\': [\'Widget A\', \'Widget B\', \'Gadget A\', \'Gizmo A\', \'Gadget B\'], # \'Category\': [\'Gadget\', \'Gizmo\', \'Gadget\', \'Gizmo\', \'Gadget\'], # \'Quantity_Sold\': [15, 7, 20, 5, 8], # \'Total_Sales\': [300, 140, 400, 100, 160] # }) # style_sales_dataframe(sales_data)"},{"question":"# Pandas Coding Assessment: Date Offsets and Business Days Objective The goal of this exercise is to assess your capability of using the `pandas.tseries.offsets` module to manipulate and analyze date and time data. You will use various date offset classes like `BusinessDay`, `MonthEnd`, and others to perform time series operations. Problem Statement You are given a dataset containing daily sales data for a company over the last year. Your task is to perform the following operations using `pandas` and its date offset features: 1. **Filter Business Days:** - Write a function `filter_business_days` that takes a DataFrame with a date column and returns a DataFrame containing only business days. ```python def filter_business_days(df: pd.DataFrame, date_column: str) -> pd.DataFrame: Filters the dataframe to include only business days. Parameters: df (pd.DataFrame): The input dataframe containing sales data. date_column (str): The name of the date column in the dataframe. Returns: pd.DataFrame: Filtered dataframe with only business days. pass ``` 2. **Compute Month-End Sales:** - Write a function `month_end_sales` that takes the same DataFrame and computes total sales for each month-end. The function should return a DataFrame with month-end dates and corresponding total sales. ```python def month_end_sales(df: pd.DataFrame, date_column: str, sales_column: str) -> pd.DataFrame: Computes total sales for each month-end date. Parameters: df (pd.DataFrame): The input dataframe containing sales data. date_column (str): The name of the date column. sales_column (str): The name of the sales column. Returns: pd.DataFrame: A dataframe with month-end dates and total sales. pass ``` 3. **Identify Quarter Start and End Dates:** - Write a function `quarter_dates` that takes the same DataFrame and returns two lists: one with the quarter start dates and one with the quarter end dates within the data range. ```python def quarter_dates(df: pd.DataFrame, date_column: str) -> (list, list): Identifies quarter start and end dates. Parameters: df (pd.DataFrame): The input dataframe containing sales data. date_column (str): The name of the date column in the dataframe. Returns: (list, list): Two lists of dates - one for quarter start dates and another for quarter end dates. pass ``` Input Format - A pandas DataFrame `df` with columns `date`, `sales` and potentially other columns. - `date_column` - A string representing the name of the date column (default name is \'date\'). - `sales_column` - A string representing the name of the sales column (default name is \'sales\'). Output Format 1. `filter_business_days` should return a pandas DataFrame filtered to include only business days. 2. `month_end_sales` should return a pandas DataFrame with month-end dates and total sales for those dates. 3. `quarter_dates` should return a tuple of two lists: one with quarter start dates and one with quarter end dates. Constraints - Assume that the date column contains valid datetime values. - The functions should handle large datasets efficiently. - Use the `pandas.tseries.offsets` module where applicable. Example Usage ```python import pandas as pd # Sample DataFrame data = { \'date\': pd.date_range(start=\'2022-01-01\', end=\'2022-12-31\', freq=\'D\'), \'sales\': range(365) } df = pd.DataFrame(data) # Apply filter_business_days business_days_df = filter_business_days(df, \'date\') # Calculate month-end sales month_end_df = month_end_sales(df, \'date\', \'sales\') # Get quarter start and end dates quarter_starts, quarter_ends = quarter_dates(df, \'date\') ``` Good luck, and happy coding!","solution":"import pandas as pd from pandas.tseries.offsets import BDay, MonthEnd, QuarterBegin, QuarterEnd def filter_business_days(df: pd.DataFrame, date_column: str) -> pd.DataFrame: Filters the dataframe to include only business days. Parameters: df (pd.DataFrame): The input dataframe containing sales data. date_column (str): The name of the date column in the dataframe. Returns: pd.DataFrame: Filtered dataframe with only business days. df = df.copy() df[date_column] = pd.to_datetime(df[date_column]) business_days_df = df[df[date_column].dt.weekday < 5] # Filter out weekends return business_days_df def month_end_sales(df: pd.DataFrame, date_column: str, sales_column: str) -> pd.DataFrame: Computes total sales for each month-end date. Parameters: df (pd.DataFrame): The input dataframe containing sales data. date_column (str): The name of the date column. sales_column (str): The name of the sales column. Returns: pd.DataFrame: A dataframe with month-end dates and total sales. df = df.copy() df[date_column] = pd.to_datetime(df[date_column]) df[\'month_end\'] = df[date_column] + MonthEnd(0) month_end_sales_df = df.groupby(\'month_end\')[sales_column].sum().reset_index() return month_end_sales_df def quarter_dates(df: pd.DataFrame, date_column: str) -> (list, list): Identifies quarter start and end dates. Parameters: df (pd.DataFrame): The input dataframe containing sales data. date_column (str): The name of the date column in the dataframe. Returns: (list, list): Two lists of dates - one for quarter start dates and another for quarter end dates. df = df.copy() df[date_column] = pd.to_datetime(df[date_column]) df[\'quarter_start\'] = df[date_column].dt.to_period(\'Q\').dt.to_timestamp(how=\'start\') df[\'quarter_end\'] = df[date_column].dt.to_period(\'Q\').dt.to_timestamp(how=\'end\') quarter_starts = df[\'quarter_start\'].unique().tolist() quarter_ends = df[\'quarter_end\'].unique().tolist() return (quarter_starts, quarter_ends)"},{"question":"**Asynchronous Programming with asyncio in Python** # Objective This question aims to assess your understanding of asynchronous programming using the `asyncio` module in Python 3.10. You will need to implement and manage asynchronous tasks, handle TCP connections, and use asyncio\'s event loop methods to achieve the given task. # Problem Statement You are required to create an asynchronous TCP server and a client that communicates with the server. The server should handle multiple client connections and echo back any message it receives from the clients. Additionally, you need to implement a mechanism to stop the server after handling a certain number of client messages. # Task 1. Implement an asynchronous TCP server that listens for incoming connections on a specific port (e.g., 8888). 2. The server should be able to handle multiple clients simultaneously. 3. When a client sends a message, the server should echo the received message back to the client. 4. Implement a client that connects to the server, sends a message, and waits for the response from the server. 5. Implement a mechanism to stop the server after handling a predefined number of messages (e.g., 5 messages). # Requirements - Use `asyncio` to implement the server and client. - Use `asyncio` methods to manage the event loop, scheduling callbacks, and managing transports and protocols. # Function Signatures You must implement the following functions: 1. `async def start_server(host: str, port: int, message_count: int) -> None:` - **Input**: - `host` (str): The hostname to listen on (e.g., \'localhost\'). - `port` (int): The port number to listen on (e.g., 8888). - `message_count` (int): The number of messages to handle before stopping the server. - **Output**: None - **Description**: Starts the TCP server and handles client connections until the `message_count` is reached. 2. `async def client_send_message(message: str, host: str, port: int) -> str:` - **Input**: - `message` (str): The message to send to the server. - `host` (str): The server hostname to connect to (e.g., \'localhost\'). - `port` (int): The server port number to connect to (e.g., 8888). - **Output**: - `response` (str): The response message received from the server. - **Description**: Connects to the server, sends a message, and retrieves the response. # Example Usage ```python # Example of server and client usage if __name__ == \\"__main__\\": import asyncio import threading async def main(): server_thread = threading.Thread(target=lambda: asyncio.run(start_server(\\"localhost\\", 8888, 5))) server_thread.start() await asyncio.sleep(1) # Allow server to start messages = [\\"Hello\\", \\"How are you?\\", \\"Bye\\"] for msg in messages: response = await client_send_message(msg, \\"localhost\\", 8888) print(f\\"Server responded: {response}\\") asyncio.run(main()) ``` # Constraints - Ensure proper error handling for network operations. - Ensure the server can handle at least 5 concurrent clients. - You may assume that the clients will only send string messages. # Note You are encouraged to refer to asyncio\'s documentation for details on methods such as `create_server`, `start`, and `run_until_complete`. Good luck!","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def __init__(self, message_count, loop): self.message_count = message_count self.loop = loop self.messages_handled = 0 def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() print(f\\"Received: {message}\\") self.transport.write(data) self.messages_handled += 1 if self.messages_handled >= self.message_count: print(\\"Message count reached, stopping server...\\") self.transport.close() self.loop.stop() async def start_server(host: str, port: int, message_count: int) -> None: loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(message_count, loop), host, port ) async with server: await server.serve_forever() async def client_send_message(message: str, host: str, port: int) -> str: reader, writer = await asyncio.open_connection(host, port) writer.write(message.encode()) await writer.drain() data = await reader.read(100) writer.close() await writer.wait_closed() return data.decode()"},{"question":"# Question: Extract and Modify Data from Log Entries You are given a text file containing multiple entries of system logs. Each log entry follows the format: `[timestamp] level: message`. For example: ``` [2023-11-02 13:45:30] ERROR: Missing data in row 42 [2023-11-02 13:46:00] WARNING: Disk space running low [2023-11-02 13:46:30] INFO: System check complete ``` Your task is to write a function `extract_modify_logs` that extracts and modifies these log entries based on the following requirements: 1. **Extract** all log entries where the level is `ERROR`. 2. **Modify** each extracted log entry by appending the text `\\" - Needs immediate attention\\"` at the end of the message. # Input - `filename` (string): The path to the log file. # Output - A list of modified log entries (strings) that meet the criteria. # Function Signature ```python def extract_modify_logs(filename: str) -> list: ``` # Example For the given log entries in a file: ``` [2023-11-02 13:45:30] ERROR: Missing data in row 42 [2023-11-02 13:46:00] WARNING: Disk space running low [2023-11-02 13:46:30] INFO: System check complete ``` The output should be: ```python [ \\"[2023-11-02 13:45:30] ERROR: Missing data in row 42 - Needs immediate attention\\" ] ``` # Constraints - Log entries conform to the format shown. - The file contains at least one log entry. - The function should be efficient in terms of time and space complexity, especially for large log files. # Note You are encouraged to use the `re` module to parse and manipulate the log entries effectively.","solution":"import re def extract_modify_logs(filename: str) -> list: Extracts and modifies log entries with level \'ERROR\' by appending \' - Needs immediate attention\' at the end of the log message. modified_logs = [] # Define a regex pattern to match log entries with level \'ERROR\' error_pattern = re.compile(r\'([.*?] ERROR: .*)\') try: with open(filename, \'r\') as file: for line in file: match = error_pattern.match(line) if match: modified_log = match.group(1) + \\" - Needs immediate attention\\" modified_logs.append(modified_log) except FileNotFoundError: print(f\\"The file {filename} does not exist.\\") return modified_logs"},{"question":"Objective You are tasked with creating a pytorch function that generates a synthetic dataset for training a neural network. The function will utilize the `torch.random` module to generate data points and their corresponding labels. Problem Statement Write a function `generate_synthetic_dataset` that generates a synthetic dataset of `n_samples` data points. Each data point is a feature vector of length `n_features`, and the corresponding label is either 0 or 1. Your function should: 1. Ensure reproducibility by accepting a `random_seed` parameter. 2. Generate feature vectors from a normal distribution with a mean of `0` and a standard deviation of `1`. 3. Generate labels such that approximately half the labels are `0` and the other half are `1`. 4. Return the generated dataset as a tuple of `(features, labels)`. Input 1. `n_samples` (int): The number of samples in the dataset. 2. `n_features` (int): The number of features for each data point. 3. `random_seed` (int): The seed for the random number generator to ensure reproducibility. Output A tuple `(features, labels)` where: - `features` (torch.Tensor): A tensor of shape `(n_samples, n_features)` containing the generated feature vectors. - `labels` (torch.Tensor): A tensor of shape `(n_samples, )` containing the generated labels. Constraints - The function should be efficient with respect to time and space complexity. - Ensure that the random seed is set correctly to guarantee reproducibility. Example ```python import torch def generate_synthetic_dataset(n_samples: int, n_features: int, random_seed: int): # Your implementation here # Example usage: n_samples = 1000 n_features = 20 random_seed = 42 features, labels = generate_synthetic_dataset(n_samples, n_features, random_seed) print(features.shape) # Expected: torch.Size([1000, 20]) print(labels.shape) # Expected: torch.Size([1000]) print(labels.sum()) # Expected: Approximately 500 (since half the labels should be 1) ``` # Notes - Use `torch.manual_seed` to set the seed for reproducibility. - Use `torch.randn` to generate the feature vectors from a normal distribution. - Use `torch.randint` or other methods to generate the labels.","solution":"import torch def generate_synthetic_dataset(n_samples: int, n_features: int, random_seed: int): Generates a synthetic dataset for training a neural network. Parameters: n_samples (int): The number of samples in the dataset. n_features (int): The number of features for each data point. random_seed (int): The seed for the random number generator to ensure reproducibility. Returns: tuple: A tuple (features, labels) containing the generated dataset. # Set the random seed for reproducibility torch.manual_seed(random_seed) # Generate feature vectors from a normal distribution features = torch.randn(n_samples, n_features) # Generate labels such that approximately half the labels are 0 and the other half are 1 labels = torch.cat([torch.zeros(n_samples // 2, dtype=torch.long), torch.ones(n_samples - n_samples // 2, dtype=torch.long)]) # Shuffle the labels to make them random indices = torch.randperm(n_samples) labels = labels[indices] return features, labels"},{"question":"# TorchScript Coding Assessment **Objective:** Design and implement a TorchScript-compatible PyTorch module that leverages custom classes and strict type annotations. **Task:** Implement a PyTorch module called `CustomNet` which consists of: 1. A constructor method `__init__` that initializes an instance variable `scale` as a tensor. 2. A method `forward` which performs a simple operation on an input tensor using the `scale`. 3. A custom class `MyTransformer` within this module, which: - Takes an integer argument during initialization. - Has a method called `transform` that applies a transformation logic to the input tensor. **Specifications:** - The `CustomNet` class should inherit from `torch.nn.Module`. - The `scale` instance variable must be initialized with a random tensor of shape `[1]`. - The `forward` method should: - Accept an input tensor. - Multiply the input tensor by `scale`. - Return the scaled tensor. - The `MyTransformer` class should: - Initialize with an integer `n`. - Have a `transform` method that computes the element-wise power of the tensor raised to `n`. - The method should accept a tensor and return the transformed tensor. # Example: ```python import torch class CustomNet(torch.nn.Module): def __init__(self): super(CustomNet, self).__init__() self.scale = torch.rand(1) self.transformer = MyTransformer(2) class MyTransformer: def __init__(self, n: int): self.n = n def transform(self, x: torch.Tensor) -> torch.Tensor: return torch.pow(x, self.n) def forward(self, x: torch.Tensor) -> torch.Tensor: x = x * self.scale return self.transformer.transform(x) # Test net = torch.jit.script(CustomNet()) input_tensor = torch.tensor([1.0, 2.0, 3.0]) output_tensor = net(input_tensor) print(output_tensor) ``` **Input:** - A tensor of type `torch.Tensor`. **Output:** - A tensor of type `torch.Tensor` with the transformation applied as specified. **Constraints:** - Use TorchScript-compatible syntax and methods. - Ensure all type annotations are specified accurately. # Submission: Submit the implementation of the `CustomNet` class along with the `MyTransformer` inner class. Ensure the code can be successfully scripted using `torch.jit.script` and executed with an example input tensor.","solution":"import torch class CustomNet(torch.nn.Module): def __init__(self): super(CustomNet, self).__init__() self.scale = torch.rand(1) self.transformer = CustomNet.MyTransformer(2) class MyTransformer: def __init__(self, n: int): self.n = n def transform(self, x: torch.Tensor) -> torch.Tensor: return torch.pow(x, self.n) def forward(self, x: torch.Tensor) -> torch.Tensor: x = x * self.scale return self.transformer.transform(x)"},{"question":"Implementing a Secure File Verification System **Objective**: Test students\' ability to utilize the `hashlib` module for secure hashing, updating hash objects, and ensuring data integrity and authenticity through the use of personalized and salted hashes. **Background**: You work for a company that manages distributing software updates to multiple clients. Ensuring that the files have not been modified during transmission is critical for security. You have decided to use the BLAKE2b hashing algorithm provided by the `hashlib` module to create a secure file verification system. **Task**: Implement a class `FileVerifier` that uses the BLAKE2b hashing algorithm to generate and verify hashes for files. The class should support the following operations: 1. `__init__(self, key: bytes, salt: bytes)`: Initializes the `FileVerifier` with a given key and salt for keyed and randomized hashing. 2. `generate_hash(self, file_path: str) -> str`: Reads the file in chunks and generates a hex-encoded hash digest. 3. `verify_hash(self, file_path: str, expected_hash: str) -> bool`: Verifies if the hash of the provided file matches the expected hash. **Input and Output**: - The `__init__` method takes two arguments: a `key` (up to 64 bytes) and a `salt` (up to 16 bytes), both as bytes. - The `generate_hash` method takes a file path as a string and returns the hash as a hex-encoded string. - The `verify_hash` method takes a file path and an expected hash (as a hex-encoded string) and returns a boolean indicating whether the file\'s hash matches the expected hash. **Constraints**: - Ensure that the file reading is done in chunks to handle large files efficiently. - Use the `hashlib.blake2b()` function with appropriate parameters for keyed, salted hashing. **Performance Requirements**: - The implementation should handle large files efficiently without consuming excessive memory. - The hashing process should be secure, using the provided key and salt in each hash computation. **Example**: ```python # Initialization with a key and salt verifier = FileVerifier(key=b\'mysecurekey1234\', salt=b\'mysalt123\') # Generate hash for a file file_hash = verifier.generate_hash(\'path/to/software_update.zip\') print(file_hash) # Outputs the hex-encoded hash # Verify the file against an expected hash is_valid = verifier.verify_hash(\'path/to/software_update.zip\', file_hash) print(is_valid) # Should print True as it is the same file ``` **Implementation**: Consider the following implementation guidelines: - Initialize the BLAKE2b object with the provided key and salt. - Read the file in chunks (e.g., 4096 bytes) and update the hash object with each chunk. - Use the `digest_size` parameter to set the size of the output hash as needed. - Use the `digest()` or `hexdigest()` for obtaining the hash. **Good Luck!**","solution":"import hashlib class FileVerifier: def __init__(self, key: bytes, salt: bytes): Initializes the FileVerifier with a given key and salt for keyed and randomized hashing. :param key: The key used for the BLAKE2b hashing algorithm, up to 64 bytes. :param salt: The salt used for the BLAKE2b hashing algorithm, up to 16 bytes. self.key = key self.salt = salt def generate_hash(self, file_path: str) -> str: Reads the file in chunks and generates a hex-encoded hash digest. :param file_path: Path to the file to hash. :return: Hex-encoded hash digest of the file. hasher = hashlib.blake2b(key=self.key, salt=self.salt) with open(file_path, \'rb\') as file: while chunk := file.read(4096): hasher.update(chunk) return hasher.hexdigest() def verify_hash(self, file_path: str, expected_hash: str) -> bool: Verifies if the hash of the provided file matches the expected hash. :param file_path: Path to the file to verify. :param expected_hash: Expected hex-encoded hash digest of the file. :return: True if the file\'s hash matches the expected hash, False otherwise. generated_hash = self.generate_hash(file_path) return generated_hash == expected_hash"},{"question":"**Coding Assessment Question: File Management and Modification in IDLE** **Objective:** To assess the understanding of file handling, string manipulation, and the application of regular expressions (regex) in Python. **Problem Statement:** Using Python, create a script that simulates some of the file handling and editing functionalities of IDLE. Specifically, your script should open a specified text file, perform a series of text modifications based on given commands, and then save the changes to a new file. The commands you need to implement include: 1. `replace <old_string> <new_string>`: Replace all occurrences of `<old_string>` with `<new_string>`. 2. `delete_line <line_number>`: Delete the line at the specified `<line_number>`. 3. `insert_line <line_number> <text>`: Insert `<text>` at the specified `<line_number>`. 4. `find_and_comment <pattern>`: Find all lines containing `<pattern>` and comment them out by adding a `#` at the beginning of these lines. 5. `strip_trailing_whitespace`: Remove trailing whitespace from all lines in the file. **Input:** 1. A text file named `input.txt` which contains the initial text. 2. A list of commands to be executed in order on `input.txt`. Each command will be a string in the format described above. **Output:** 1. A new text file named `output.txt` with the modifications applied. **Example:** * `input.txt` content: ``` Hello World This is a sample text file. Python is amazing! ``` * Commands: ``` [ \\"replace World Universe\\", \\"delete_line 2\\", \\"insert_line 2 This line is newly inserted.\\", \\"find_and_comment Python\\", \\"strip_trailing_whitespace\\" ] ``` * `output.txt` content after script execution: ``` Hello Universe This line is newly inserted. #Python is amazing! ``` **Constraints:** - The line numbers are 1-based indices. - The commands should be executed in the order they are given. - You may assume that the input file exists and that the commands are well-formed. - You should handle exceptions where the specified line number does not exist. **Requirements:** 1. Implement the script in Python. 2. Demonstrate your understanding of file handling (`open`, `read`, `write`), string manipulation, and regular expressions where necessary. 3. Ensure that your script works efficiently for typical text file sizes used in development (up to a few MBs). **Performance:** Your script should execute within a practical time frame for text files containing up to 10,000 lines.","solution":"import re def process_file(input_path, output_path, commands): with open(input_path, \'r\') as file: lines = file.readlines() for command in commands: parts = command.strip().split(\' \', 2) action = parts[0] if action == \'replace\': old_string, new_string = parts[1], parts[2] lines = [line.replace(old_string, new_string) for line in lines] elif action == \'delete_line\': line_number = int(parts[1]) - 1 if 0 <= line_number < len(lines): lines.pop(line_number) elif action == \'insert_line\': line_number, text = int(parts[1]) - 1, parts[2] if 0 <= line_number <= len(lines): lines.insert(line_number, text + \'n\') elif action == \'find_and_comment\': pattern = parts[1] regex = re.compile(pattern) lines = [\'#\' + line if regex.search(line) else line for line in lines] elif action == \'strip_trailing_whitespace\': lines = [line.rstrip() + \'n\' for line in lines] with open(output_path, \'w\') as file: file.writelines(lines)"},{"question":"**Coding Question: Advanced Data Visualization with Seaborn’s Object Interface** **Objective:** Your task is to analyze a dataset and create multiple bar plots using Seaborn’s object interface. You need to demonstrate your ability to handle overlapping data and customize plot appearances by completing the following tasks. **Dataset:** Use the built-in `diamonds` dataset provided by Seaborn. **Tasks:** 1. **Basic Plot:** - Create a histogram of the `price` variable with a logarithmic scale on the x-axis. 2. **Color Overlap Handling:** - Create a histogram of the `price` variable colored by the `cut` attribute. Note the overlap in bars and how it affects the plot interpretation. 3. **Resolve Overlap with Stack:** - Modify the plot from Task 2 to resolve the overlap using the `Stack` transformation. 4. **Customize Bar Properties:** - Create a histogram of `price` where you adjust the bar properties: - Set the `edgewidth` to `0`. - Adjust the transparency of bars based on the `clarity` attribute. 5. **Unfilled Bars:** - Create a histogram of `price` with unfilled bars. Set the edge color to `C0` and the edge width to `1.5`. 6. **Narrowing Bars and Specific Data Subset:** - Draw a histogram of `price` with bars narrowed to width `0.5`. Focus this plot exclusively on diamonds with the `cut` attribute equal to \'Ideal\'. **Input and Output:** - **Input:** None (Use the preloaded `diamonds` dataset from Seaborn). - **Output:** No output values. The function should display the plots using `plt.show()`. **Constraints:** - Ensure that your plots are clear, and all visual elements are distinguishable. - Label all axes and provide titles for each plot for clarity. **Performance Requirements:** - Efficiently handle the dataset using Seaborn to create plots. - Ensure that plot customization does not significantly affect performance. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_advanced_plots(): diamonds = load_dataset(\\"diamonds\\") # Task 1: Basic Plot p1 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p1.add(so.Bars(), so.Hist()) p1.show() # Task 2: Color Overlap Handling p2 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p2.add(so.Bars(), so.Hist(), color=\\"cut\\") p2.show() # Task 3: Resolve Overlap with Stack p3 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p3.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\") p3.show() # Task 4: Customize Bar Properties p4 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p4.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\") p4.show() # Task 5: Unfilled Bars p5 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p5.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5), so.Hist()) p5.show() # Task 6: Narrowing Bars and Specific Data Subset hist = so.Hist(binwidth=.075, binrange=(2, 5)) p6 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") (p6.add(so.Bars(), hist) .add(so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"cut == \'Ideal\'\\")) ) p6.show() # Run the function to create the plots create_advanced_plots() ``` **Notes:** - Make use of the Seaborn `objects` interface and matplotlib to display the plots. - Ensure effective handling of overlapping data to make the plots visually interpretable.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_advanced_plots(): diamonds = load_dataset(\\"diamonds\\") # Task 1: Basic Plot p1 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p1.add(so.Bars(), so.Hist()) plt.title(\'Basic Plot with Logarithmic Scale\') plt.xlabel(\'Log of Price\') plt.ylabel(\'Count\') p1.show() # Task 2: Color Overlap Handling p2 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p2.add(so.Bars(), so.Hist(), color=\\"cut\\") plt.title(\'Color Overlap Handling\') plt.xlabel(\'Log of Price\') plt.ylabel(\'Count\') p2.show() # Task 3: Resolve Overlap with Stack p3 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p3.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\") plt.title(\'Resolve Overlap with Stack\') plt.xlabel(\'Log of Price\') plt.ylabel(\'Count\') p3.show() # Task 4: Customize Bar Properties p4 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p4.add(so.Bars(edgewidth=0), so.Hist(), so.Stack(), alpha=\\"clarity\\") plt.title(\'Customize Bar Properties\') plt.xlabel(\'Log of Price\') plt.ylabel(\'Count\') p4.show() # Task 5: Unfilled Bars p5 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p5.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5), so.Hist()) plt.title(\'Unfilled Bars with Edge Color\') plt.xlabel(\'Log of Price\') plt.ylabel(\'Count\') p5.show() # Task 6: Narrowing Bars and Specific Data Subset hist = so.Hist(binwidth=.075, binrange=(2, 5)) p6 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") (p6.add(so.Bars(), hist) .add(so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"cut == \'Ideal\'\\")) ) plt.title(\'Narrowing Bars and Specific Data Subset\') plt.xlabel(\'Log of Price\') plt.ylabel(\'Count\') p6.show() # Run the function to create the plots create_advanced_plots()"},{"question":"Objective Test the student\'s ability to handle asyncio exceptions effectively within a Python program. Problem Statement You are given a series of asynchronous operations that interact with a socket or a file. Your task is to implement a function `asyncio_operations_handler` which attempts to read data from a given socket or file using `asyncio` and handles various exceptions that might be raised during the operations. Your function should manage the following exceptions gracefully: 1. `asyncio.TimeoutError` 2. `asyncio.CancelledError` 3. `asyncio.InvalidStateError` 4. `asyncio.SendfileNotAvailableError` 5. `asyncio.IncompleteReadError` 6. `asyncio.LimitOverrunError` Function Signature ```python async def asyncio_operations_handler(reader: asyncio.StreamReader, writer: asyncio.StreamWriter, timeout: float) -> str: Handles asyncio operations with the given reader and writer, managing exceptions effectively. Args: reader (asyncio.StreamReader): An asyncio StreamReader object. writer (asyncio.StreamWriter): An asyncio StreamWriter object. timeout (float): Time limit in seconds for the operation. Returns: str: Result of the read data if successful, or a string description of the exception encountered. ``` Requirements 1. **Input:** - `reader`: an instance of `asyncio.StreamReader` for reading streams. - `writer`: an instance of `asyncio.StreamWriter` for writing streams. - `timeout`: a float representing the maximum time in seconds allowed for the read operation. 2. **Output:** - A `str` containing the result of the read operation if successful. - A `str` description of the specific exception encountered, such as `\\"TimeoutError encountered: operation exceeded the deadline\\"`. 3. **Exceptions Handling:** - Catch and handle each specified exception in a way that the function returns a string describing the exception. - For `asyncio.TimeoutError`, return `\\"TimeoutError encountered: operation exceeded the deadline\\"`. - For `asyncio.CancelledError`, return `\\"CancelledError encountered: operation was cancelled\\"`. - For `asyncio.InvalidStateError`, return `\\"InvalidStateError encountered: invalid state of Task or Future\\"`. - For `asyncio.SendfileNotAvailableError`, return `\\"SendfileNotAvailableError encountered: sendfile syscall not available\\"`. - For `asyncio.IncompleteReadError`, return `\\"IncompleteReadError encountered: read operation did not complete fully, expected {expected}, got {partial}\\"` with the appropriate values. - For `asyncio.LimitOverrunError`, return `\\"LimitOverrunError encountered: buffer size limit reached, consumed {consumed}\\"` with the appropriate value. Constraints - Assume that the provided `reader` and `writer` objects are valid and correctly initialized. - The function must be asynchronous. - The timeout value will always be a positive float. Example ```python import asyncio async def asyncio_operations_handler(reader: asyncio.StreamReader, writer: asyncio.StreamWriter, timeout: float) -> str: try: data = await asyncio.wait_for(reader.read(100), timeout=timeout) writer.write(data) await writer.drain() return data.decode() except asyncio.TimeoutError: return \\"TimeoutError encountered: operation exceeded the deadline\\" except asyncio.CancelledError: return \\"CancelledError encountered: operation was cancelled\\" except asyncio.InvalidStateError: return \\"InvalidStateError encountered: invalid state of Task or Future\\" except asyncio.SendfileNotAvailableError: return \\"SendfileNotAvailableError encountered: sendfile syscall not available\\" except asyncio.IncompleteReadError as e: return f\\"IncompleteReadError encountered: read operation did not complete fully, expected {e.expected}, got {e.partial}\\" except asyncio.LimitOverrunError as e: return f\\"LimitOverrunError encountered: buffer size limit reached, consumed {e.consumed}\\" ``` Use this example to test various scenarios by simulating possible exceptions.","solution":"import asyncio async def asyncio_operations_handler(reader: asyncio.StreamReader, writer: asyncio.StreamWriter, timeout: float) -> str: try: data = await asyncio.wait_for(reader.read(100), timeout=timeout) writer.write(data) await writer.drain() return data.decode() except asyncio.TimeoutError: return \\"TimeoutError encountered: operation exceeded the deadline\\" except asyncio.CancelledError: return \\"CancelledError encountered: operation was cancelled\\" except asyncio.InvalidStateError: return \\"InvalidStateError encountered: invalid state of Task or Future\\" except asyncio.SendfileNotAvailableError: return \\"SendfileNotAvailableError encountered: sendfile syscall not available\\" except asyncio.IncompleteReadError as e: return f\\"IncompleteReadError encountered: read operation did not complete fully, expected {e.expected}, got {e.partial}\\" except asyncio.LimitOverrunError as e: return f\\"LimitOverrunError encountered: buffer size limit reached, consumed {e.consumed}\\""},{"question":"**Question: Healthcare Spending Visualization** You are provided with a dataset that contains health expenditure data for various countries over the years. Your task is to write a Python function that accomplishes the following: 1. Load the dataset `healthexp` using seaborn\'s `load_dataset` function. The dataset contains the columns: `Year`, `Country`, `Spending_USD`. 2. Transform the dataset such that: - It interpolates missing values. - It reshapes the data to have `Year` as the index and `Country` as the column headers. - Then, stack the reshaped data and reset the index. 3. Create a facetted area plot using seaborn\'s `so.Plot` object. Each facet should represent a different country. Additionally: - Plot the `Year` on the x-axis and `Spending_USD` on the y-axis. - Use color to differentiate countries in the area plots. - Add a line on top of the area to indicate the value changes. 4. Save your final plot as an image file named `health_expenditure.png`. **Constraints:** - You may assume seaborn and matplotlib are installed in your environment. - Ensure your code handles and fills missing values appropriately. **Input:** - No input is required for this function. **Output:** - A saved image file named `health_expenditure.png`. **Function Signature:** ```python def visualize_health_expenditure() -> None: pass ``` **Example:** ```python visualize_health_expenditure() # After running the function, an image file named \'health_expenditure.png\' should be saved in the current directory. ``` **Notes:** - Use relevant seaborn and matplotlib functions and classes as needed. - Ensure that the plot is clear and well-labeled for better readability.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def visualize_health_expenditure() -> None: # Load the dataset healthexp = sns.load_dataset(\'healthexp\') # Interpolate missing values healthexp = healthexp.interpolate() # Reshape the data df_pivot = healthexp.pivot(index=\'Year\', columns=\'Country\', values=\'Spending_USD\') # Stack and reset the index df_stacked = df_pivot.stack().reset_index(name=\'Spending_USD\') # Create a facetted area plot with seaborn\'s so.Plot object p = (so.Plot(df_stacked, x=\'Year\', y=\'Spending_USD\', color=\'Country\') .facet(\'Country\', wrap=4) .add(so.Area()) .add(so.Line())) # Save the plot p.save(\'health_expenditure.png\', dpi=300)"},{"question":"# Advanced Python Coding Assessment **Objective:** Write a Python function `validate_and_compile` that uses the `codeop.compile_command` method to validate and compile a list of Python code statements. The function should handle and report syntax errors and maintain any future statements for subsequent compilations. **Function Signature:** ```python def validate_and_compile(statements: list) -> list: pass ``` **Input:** - `statements`: A list of strings, where each string is a line of Python code to be compiled. **Output:** - A list of tuples: Each tuple should contain the original statement and either a compiled code object or an error message. If a statement is syntactically correct and compiles successfully, the tuple should contain the statement and the compiled code object. If there is a syntax error, the tuple should contain the statement and the string \\"SyntaxError\\". **Constraints:** - The function should compile each statement individually. - If a statement includes a future statement, subsequent statements should be compiled as if the future statement is in effect. **Example:** ```python # Example Input statements = [ \\"from __future__ import division\\", \\"print 3 / 2\\", \\"a = 5\\" ] # Example Output [ (\\"from __future__ import division\\", <code object <module> at ...>), (\\"print 3 / 2\\", \\"SyntaxError\\"), (\\"a = 5\\", <code object <module> at ...>) ] ``` **Additional Information:** - Use the `codeop.compile_command` method with the default symbol \'single\'. - Handle exceptions such as `SyntaxError`, `OverflowError`, and `ValueError`. **Notes:** - The function should use the capabilities of the `codeop` module described above to manage future statements and compilation. - Provide meaningful error messages and handle them gracefully within the function. **Task:** Implement the `validate_and_compile` function based on the above specifications. Ensure your implementation is efficient and follows best practices for error handling in Python.","solution":"import codeop def validate_and_compile(statements: list) -> list: Validates and compiles a list of Python code statements. Args: statements: A list of strings, where each string is a line of Python code to be compiled. Returns: A list of tuples: Each tuple contains the original statement and either a compiled code object or an error message. compiler = codeop.CommandCompiler() results = [] for statement in statements: try: compiled_code = compiler(statement) if compiled_code is None: results.append((statement, \\"Incomplete\\")) else: results.append((statement, compiled_code)) except (OverflowError, SyntaxError, ValueError) as e: results.append((statement, \\"SyntaxError\\")) return results"},{"question":"# Sun AU File Manipulation Challenge You are provided with an AU audio file and your task is to create a Python script using the `sunau` module to manipulate this audio file. Specifically, your task is to create a \\"mono\\" version of the provided stereo audio file, where the mono audio data is achieved by averaging the left and right channels of the stereo audio data. Requirements: 1. **Input**: The path to an existing AU file in stereo format. 2. **Output**: The path where the new mono AU file should be saved. You need to write a function `stereo_to_mono(input_file: str, output_file: str) -> None` that performs the following tasks: 1. Open the existing stereo AU file for reading. 2. Read the necessary header information (e.g., number of channels, sample width, frame rate, etc.). 3. Extract the audio data frames from the stereo file. 4. Convert the stereo frames to mono by averaging the left and right channel samples. 5. Write the new mono audio data to a new AU file, updating the header appropriately. Constraints: - The sample width is 2 bytes (16-bit samples). - Assume the input AU file is in either ULAW or PCM encoding format. - You should ensure the output AU file has correctly set header fields. Example Usage: ```python input_file = \\"stereo.au\\" output_file = \\"mono.au\\" stereo_to_mono(input_file, output_file) ``` This will create a new file named `mono.au` that contains the mono version of the audio from `stereo.au`. Performance Requirements: - The solution should efficiently handle AU files with a large number of frames. - Use in-memory processing to ensure that multiple reads or writes from the disk are minimized. Error Handling: - Handle possible exceptions including file not found errors, unsupported format errors, and I/O errors gracefully by printing appropriate error messages. You are encouraged to use appropriate helper functions to keep the code modular and readable.","solution":"import sunau import struct def stereo_to_mono(input_file: str, output_file: str) -> None: try: with sunau.open(input_file, \'r\') as stereo_au: channels = stereo_au.getnchannels() sample_width = stereo_au.getsampwidth() framerate = stereo_au.getframerate() nframes = stereo_au.getnframes() comptype = stereo_au.getcomptype() compname = stereo_au.getcompname() if channels != 2 or sample_width != 2: raise ValueError(\\"Input file must be a stereo AU file with 16-bit samples\\") # Read all the frames from the input file stereo_frames = stereo_au.readframes(nframes) mono_frames = bytearray() # Convert stereo frames to mono by averaging the left and right channels for i in range(0, len(stereo_frames), 4): left = struct.unpack(\'<h\', stereo_frames[i:i+2])[0] right = struct.unpack(\'<h\', stereo_frames[i+2:i+4])[0] mono_value = int((left + right) / 2) mono_frames.extend(struct.pack(\'<h\', mono_value)) # Write mono frames to output file with sunau.open(output_file, \'w\') as mono_au: mono_au.setnchannels(1) mono_au.setsampwidth(sample_width) mono_au.setframerate(framerate) mono_au.setcomptype(comptype, compname) mono_au.writeframes(mono_frames) except FileNotFoundError: print(f\\"The file {input_file} was not found.\\") except ValueError as e: print(e) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective: To assess the students\' understanding of advanced data structures and object-oriented programming (OOP) in Python, as well as their ability to implement a functional solution that combines these concepts. Problem Statement: You are required to implement a class-based system to manage and analyze a collection of books in a library. Each book has a title, an author, a publication year, and a list of genres. Your task is to: 1. Implement a `Book` class that includes: - Constructor to initialize the book with a title, author, publication year, and genres. - A method to represent the book\'s details as a string. 2. Implement a `Library` class that includes: - A constructor to initialize the library with an empty collection of books. - A method `add_book` to add a new book to the collection. - A method `remove_book` to remove a book by title. - A method `get_books_by_author` to return a list of all books by a specified author. - A method `get_books_by_genre` to return a list of all books in a specified genre. - A method `get_books_between_years` to return a list of all books published between two given years (inclusive). - A method to represent all books in the library as a string, with one book per line. Requirements: - **Input Formats:** - For the `Book` class: `title` (str), `author` (str), `publication year` (int), `genres` (list of strs). - For the `Library` class: methods receive arguments as specified in their descriptions. - **Output Formats:** - For the `Book` class: a string representation of the book in the format `Title by Author, Year [Genre 1, Genre 2, ...]`. - For the `Library` class: methods return lists of `Book` objects or strings as specified. - **Constraints:** - Book titles and author names are unique strings. - The publication year is a four-digit integer. - The genres list contains at least one genre. Example Usage: ```python # Create book instances book1 = Book(\\"1984\\", \\"George Orwell\\", 1949, [\\"Dystopian\\", \\"Science Fiction\\"]) book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, [\\"Fiction\\", \\"Drama\\"]) book3 = Book(\\"Brave New World\\", \\"Aldous Huxley\\", 1932, [\\"Dystopian\\", \\"Science Fiction\\"]) # Create a library instance library = Library() # Add books to the library library.add_book(book1) library.add_book(book2) library.add_book(book3) # Remove a book by title library.remove_book(\\"1984\\") # Get books by author books_by_orwell = library.get_books_by_author(\\"George Orwell\\") # Get books by genre sci_fi_books = library.get_books_by_genre(\\"Science Fiction\\") # Get books published between certain years books_between_years = library.get_books_between_years(1930, 1950) # Print all books in the library print(library) ``` Notes: - Ensure your implementation is efficient and follows good coding practices. - Write docstrings for your methods and classes. - Include error handling where appropriate, e.g., when trying to remove a book that does not exist.","solution":"class Book: def __init__(self, title, author, publication_year, genres): Initialize a Book instance. self.title = title self.author = author self.publication_year = publication_year self.genres = genres def __str__(self): Return a string representation of the book. genres_str = \', \'.join(self.genres) return f\\"{self.title} by {self.author}, {self.publication_year} [{genres_str}]\\" class Library: def __init__(self): Initialize a Library instance with an empty collection of books. self.books = [] def add_book(self, book): Add a book to the library collection. self.books.append(book) def remove_book(self, title): Remove a book from the library collection by its title. self.books = [book for book in self.books if book.title != title] def get_books_by_author(self, author): Return a list of all books by a specified author. return [book for book in self.books if book.author == author] def get_books_by_genre(self, genre): Return a list of all books in a specified genre. return [book for book in self.books if genre in book.genres] def get_books_between_years(self, start_year, end_year): Return a list of all books published between two given years (inclusive). return [book for book in self.books if start_year <= book.publication_year <= end_year] def __str__(self): Return a string representation of all books in the library. return \'n\'.join(str(book) for book in self.books)"},{"question":"# Scenario You have been hired by a tech company to optimize the data processing pipeline of a large machine learning project. The project involves performing computationally intensive transformations on a dataset using multiple processors to decrease the overall processing time. This is a perfect use case for leveraging PyTorch\'s distributed multiprocessing capabilities. # Task Implement a function `start_and_monitor_processes` that performs the following steps: 1. Starts multiple worker processes to execute a dummy computational task. 2. Collects and consolidates the result of each process once all processes have finished executing. 3. Logs the output and errors of each process. # Dummy Computation Task Your task involves writing a dummy function named `dummy_task` that takes a numeric argument, squares it, and returns the result. Each process will execute this dummy task on different input values. # Function Specifications Function 1: `dummy_task` - **Input**: - `num` (int): A numeric value to be squared. - **Output**: - `result` (int): The square of the input value. Function 2: `start_and_monitor_processes` - **Input**: - `num_processes` (int): Number of worker processes to start. - `inputs` (List[int]): List of integer inputs for the dummy task. - **Output**: - `results` (List[int]): List of results from each worker process, corresponding to their input values. # Constraints - You must use PyTorch\'s `torch.distributed.elastic.multiprocessing.start_processes` function to start the worker processes. - Handle all the processes\' logs, including standard output and errors. - Ensure that the number of processes does not exceed the number of input values. # Example Usage ```python def dummy_task(num): return num * num def start_and_monitor_processes(num_processes, inputs): # Implementation here pass # Example inputs num_processes = 3 inputs = [1, 2, 3, 4] # Example function call results = start_and_monitor_processes(num_processes, inputs) print(results) # Expected output: [1, 4, 9, 16] (order may vary based on process execution) ``` # Additional Information - Ensure proper synchronization to collect results once all processes have finished. - Use appropriate data structures provided in `torch.distributed.elastic.multiprocessing.api` to manage process contexts.","solution":"import torch.multiprocessing as mp def dummy_task(num): Squares the input number. Args: num (int): Number to be squared. Returns: int: The square of the input number. return num * num def worker(rank, num_processes, inputs, results): Worker function to be executed in parallel. Args: rank (int): Rank of the current process. num_processes (int): Total number of processes. inputs (List[int]): List of input values. results (mp.Manager().list): List to store results from each worker. result = dummy_task(inputs[rank]) results[rank] = result def start_and_monitor_processes(num_processes, inputs): Starts multiple worker processes to execute dummy task and collects results. Args: num_processes (int): Number of worker processes to start. inputs (List[int]): List of integer inputs for the dummy task. Returns: List[int]: List of results from each worker process. assert len(inputs) >= num_processes, \\"Number of processes should not exceed the number of input values.\\" # Create a Manager to handle shared objects between processes manager = mp.Manager() results = manager.list([None] * num_processes) processes = [] for rank in range(num_processes): p = mp.Process(target=worker, args=(rank, num_processes, inputs, results)) p.start() processes.append(p) for p in processes: p.join() return list(results)"},{"question":"# Abstract Base Classes and Abstract Methods in Python In this assessment, you will create a simple framework using ABCs to manage different types of shapes and their geometric properties. You will demonstrate your understanding of abstract base classes, abstract methods, and mix-in classes. You need to ensure that your code follows the principles of object-oriented design and leverages the features provided by the `abc` module. Task: 1. **Define an abstract base class `Shape` using `ABC`**: - `Shape` should have one abstract method `area()` that computes the area of the geometric shape. - `Shape` should also define an abstract property `name` that returns the name of the shape. 2. **Create a subclass `Circle` that inherits from `Shape`**: - Implement the `area`, which calculates the area of the circle using ( pi r^2 ). - Implement the `name` property to return the string `\\"Circle\\"`. - The `Circle` class should accept a radius as initialization parameter. 3. **Create a subclass `Rectangle` that inherits from `Shape`**: - Implement the `area`, which calculates the area of the rectangle using ( text{width} times text{height} ). - Implement the `name` property to return the string `\\"Rectangle\\"`. - The `Rectangle` class should accept width and height as initialization parameters. 4. **Allow `Shape` to register another class `Square` as a virtual subclass**: - The `Square` class should define width and height properties both set to the side length. - The `Square` class should compute the area and return the string `\\"Square\\"`. 5. **Write a function `describe_shape(shape: Shape)`**: - This function should accept a `Shape` instance and print the name and area of the shape. Guidelines: - Use the `abc` module wherever appropriate. - Ensure that `Circle` and `Rectangle` classes can\'t be instantiated if they don\'t implement the abstract methods and properties. - Use `Shape.register()` to register the `Square` class as a virtual subclass of `Shape`. Example: ```python from abc import ABC, abstractmethod # Your implementation here # Example usage c = Circle(radius=5) r = Rectangle(width=4, height=6) s = Square(side=3) describe_shape(c) # Output: The shape is a Circle with area of 78.54 describe_shape(r) # Output: The shape is a Rectangle with area of 24 describe_shape(s) # Output: The shape is a Square with area of 9 ``` Notes: - You can use `math.pi` for the value of (pi). - Ensure that your code is clear, follows PEP8 guidelines, and includes necessary docstrings for your methods and classes.","solution":"from abc import ABC, abstractmethod, abstractproperty import math class Shape(ABC): @abstractmethod def area(self): pass @abstractproperty def name(self): pass class Circle(Shape): def __init__(self, radius): self.radius = radius @property def area(self): return math.pi * self.radius ** 2 @property def name(self): return \\"Circle\\" class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height @property def area(self): return self.width * self.height @property def name(self): return \\"Rectangle\\" class Square: def __init__(self, side): self.side = side @property def width(self): return self.side @property def height(self): return self.side @property def area(self): return self.side ** 2 @property def name(self): return \\"Square\\" Shape.register(Square) def describe_shape(shape: Shape): print(f\\"The shape is a {shape.name} with an area of {shape.area:.2f}\\")"},{"question":"Coding Assessment Question # Objective Implement a function using the `ipaddress` module to validate and manipulate a list of IPv4 and IPv6 addresses and networks. # Problem Statement You are given a list of strings that represent IP addresses (both IPv4 and IPv6) and network addresses in CIDR notation. Your task is to write a function `process_ip_addresses` that performs the following operations: 1. **Validate** each string to check if it is a valid IP address or network. 2. **Classify** each valid entry into one of the following categories: - **IPv4 address** - **IPv6 address** - **IPv4 network** - **IPv6 network** 3. **Extract and return** the following information: - A list of sorted valid IPv4 addresses. - A list of sorted valid IPv6 addresses. - A list of valid IPv4 networks in CIDR notation. - A list of valid IPv6 networks in CIDR notation. - A count of invalid entries. # Function Signature ```python from typing import List, Tuple def process_ip_addresses(ip_list: List[str]) -> Tuple[List[str], List[str], List[str], List[str], int]: pass ``` # Input - `ip_list` (List[str]): A list of strings representing IP addresses and network addresses. # Output - A tuple consisting of: - List[str]: Sorted list of valid IPv4 addresses. - List[str]: Sorted list of valid IPv6 addresses. - List[str]: List of valid IPv4 networks in CIDR notation. - List[str]: List of valid IPv6 networks in CIDR notation. - int: The count of invalid entries in the input list. # Constraints - You must use the `ipaddress` module to validate and process the IP addresses. - Sorting should be based on the numerical value of the addresses. # Example ```python ip_list = [ \'192.168.1.1\', \'2001:db8::1\', \'192.168.1.0/24\', \'2001:db8::/32\', \'256.256.256.256\', \'invalid_ip\' ] output = process_ip_addresses(ip_list) print(output) ``` Expected Output: ```python ( [\'192.168.1.1\'], [\'2001:db8::1\'], [\'192.168.1.0/24\'], [\'2001:db8::/32\'], 2 ) ``` # Note - Use the `ipaddress` module\'s factory functions (`ip_address`, `ip_network`) to handle validation. - IPv4 and IPv6 addresses should be sorted properly by their numerical value.","solution":"from typing import List, Tuple import ipaddress def process_ip_addresses(ip_list: List[str]) -> Tuple[List[str], List[str], List[str], List[str], int]: ipv4_addresses = [] ipv6_addresses = [] ipv4_networks = [] ipv6_networks = [] invalid_count = 0 for ip_str in ip_list: try: ip_obj = ipaddress.ip_address(ip_str) if isinstance(ip_obj, ipaddress.IPv4Address): ipv4_addresses.append(ip_str) else: ipv6_addresses.append(ip_str) except ValueError: try: net_obj = ipaddress.ip_network(ip_str, strict=False) if isinstance(net_obj, ipaddress.IPv4Network): ipv4_networks.append(ip_str) else: ipv6_networks.append(ip_str) except ValueError: invalid_count += 1 ipv4_addresses.sort(key=lambda x: int(ipaddress.IPv4Address(x))) ipv6_addresses.sort(key=lambda x: int(ipaddress.IPv6Address(x))) return ipv4_addresses, ipv6_addresses, ipv4_networks, ipv6_networks, invalid_count"},{"question":"You are given a dataset `data.csv` containing features and target values for a regression problem. Implement a regularized linear regression model using Ridge regression, then perform cross-validation to find the optimal hyperparameter for Ridge regularization. Finally, evaluate your model on a test set and output the model performance. **Requirements:** 1. **File Reading:** The input dataset is a CSV file named `data.csv`. The first column is `target` and the rest are the features. 2. **Data Splitting:** Split the data into training and test sets using an 80-20 split. 3. **Model Training:** Implement Ridge regression using scikit-learn with cross-validation to find the best regularization parameter. 4. **Evaluation:** Evaluate the model on the test set using Mean Squared Error (MSE). 5. **Output:** Print the best alpha value from cross-validation and the test data MSE. # Input: - A CSV file named `data.csv` in the current directory. # Output: - Best alpha value (regularization parameter) found from cross-validation. - Mean Squared Error (MSE) on the test set. # Constraints: - Use 10-fold cross-validation for parameter tuning. - Ensure you handle potential missing values by filling them with the mean of the corresponding feature. # Example: Suppose `data.csv` looks like this: ``` target,feature1,feature2,feature3 1.2,0.5,1.3,2.1 2.4,1.1,2.2,3.9 ... ``` The output should be: ``` Best Alpha: 0.01 Test MSE: 0.45 ``` # Template: ```python import pandas as pd from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split # Load the data data = pd.read_csv(\'data.csv\') # Handle potential missing values data.fillna(data.mean(), inplace=True) # Split into features and target X = data.drop(columns=\'target\') y = data[\'target\'] # Split into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement Ridge regression with cross-validation alphas = [0.1, 1.0, 10.0, 100.0] ridge_cv = RidgeCV(alphas=alphas, cv=10) ridge_cv.fit(X_train, y_train) # Evaluate on the test set y_pred = ridge_cv.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Output print(f\\"Best Alpha: {ridge_cv.alpha_}\\") print(f\\"Test MSE: {mse:.2f}\\") ```","solution":"import pandas as pd from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split def ridge_regression_with_cv(file_path=\'data.csv\'): # Load the data data = pd.read_csv(file_path) # Handle potential missing values data.fillna(data.mean(), inplace=True) # Split into features and target X = data.drop(columns=\'target\') y = data[\'target\'] # Split into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement Ridge regression with cross-validation alphas = [0.1, 1.0, 10.0, 100.0] ridge_cv = RidgeCV(alphas=alphas, cv=10) ridge_cv.fit(X_train, y_train) # Evaluate on the test set y_pred = ridge_cv.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Output best_alpha = ridge_cv.alpha_ print(f\\"Best Alpha: {best_alpha}\\") print(f\\"Test MSE: {mse:.2f}\\") return best_alpha, mse"},{"question":"You have been provided with a dataset concerning student scores in various subjects. Your task is to write a function that reads this dataset into a pandas DataFrame, applies specific styling to highlight particular data points, and exports the styled DataFrame to an HTML file. # Function Signature ```python def style_student_scores(filepath: str, output_filepath: str) -> None: pass ``` # Input - `filepath`: A string representing the path to a CSV file containing the student scores. The CSV file includes columns `Name`, `Math`, `Science`, `English`, and `History`. - `output_filepath`: A string representing the path to save the styled HTML output. # Output - The function does not return anything. It should save a styled HTML file to `output_filepath`. # Requirements 1. Apply a gradient background to the scores columns (`Math`, `Science`, `English`, and `History`) with a different color gradient for each subject. 2. Highlight the highest score in each subject column with a distinctive color. 3. Set a tooltip for each cell in the scores columns displaying the text \\"Score in [Subject]: [value]\\" (e.g., \\"Score in Math: 85\\"). 4. Use a caption \\"Student Scores Overview\\" for the styled table. 5. Export the styled DataFrame to an HTML file at the location specified by `output_filepath`. # Example 1. Given a CSV file located at `student_scores.csv` with the following content: ```csv Name,Math,Science,English,History Alice,88,92,85,90 Bob,75,85,95,82 Charlie,90,88,89,91 ``` 2. After running the function `style_student_scores(\'student_scores.csv\', \'styled_scores.html\')`, the result should be an HTML file named `styled_scores.html` containing the styled DataFrame as specified. # Sample Usage ```python style_student_scores(\'student_scores.csv\', \'styled_scores.html\') ``` # Constraints - Assume all input values (including file paths) are valid. - The function should handle large datasets efficiently within memory constraints. # Additional Note You may use the methods from the `pandas.io.formats.style.Styler` class to accomplish the styling tasks.","solution":"import pandas as pd def style_student_scores(filepath: str, output_filepath: str) -> None: Reads student score data from a CSV file, applies specific styles, and saves the styled DataFrame to an HTML file. Parameters: filepath: str - The path to the CSV file containing student scores. output_filepath: str - The path to save the styled HTML output. # Read the dataset into a DataFrame df = pd.read_csv(filepath) # Create custom style functions def apply_gradient(column, cmap): return column.apply(lambda x: f\'background: linear-gradient(90deg, white {100-int(x)}%, {cmap} {int(x)}%);\') def highlight_max(column): return [\'background-color: lightgreen\' if v == column.max() else \'\' for v in column] def add_tooltip(column, subject): return [f\'Score in {subject}: {value}\' for value in column] # Style the DataFrame styled_df = df.style .apply(apply_gradient, cmap=\'Blues\', subset=[\'Math\']) .apply(apply_gradient, cmap=\'Purples\', subset=[\'Science\']) .apply(apply_gradient, cmap=\'Oranges\', subset=[\'English\']) .apply(apply_gradient, cmap=\'Greens\', subset=[\'History\']) .apply(highlight_max, subset=[\'Math\', \'Science\', \'English\', \'History\']) .set_tooltips(pd.DataFrame({ \'Math\': add_tooltip(df[\'Math\'], \'Math\'), \'Science\': add_tooltip(df[\'Science\'], \'Science\'), \'English\': add_tooltip(df[\'English\'], \'English\'), \'History\': add_tooltip(df[\'History\'], \'History\') })) .set_caption(\\"Student Scores Overview\\") # Save the styled DataFrame to an HTML file styled_df.to_html(output_filepath, escape=False)"},{"question":"Objective: Demonstrate understanding of the `atexit` module in Python, including registering and unregistering functions to handle program termination events. Problem Statement: You are responsible for managing resources in a Python application. Specifically: 1. You need to ensure certain cleanup actions are executed automatically when the program terminates. 2. Assume a scenario where some registered cleanup actions might need to be conditionally removed before the program terminates. Task: Write a Python program that: 1. Defines three cleanup functions: `cleanup_a`, `cleanup_b`, and `cleanup_c`. - Each function should print a message indicating it is being executed (e.g., \\"Executing cleanup_a\\"). 2. Registers these three functions to be run upon normal program termination using the `atexit` module. 3. Unregisters the `cleanup_b` function based on a condition (use a simple condition like a boolean variable `unregister_b` set to `True`). Requirements: - Implement the functions and demonstrate their registration and conditional unregistration. - Ensure that the registered cleanup functions are executed automatically upon program termination in the correct order (LIFO). Constraints: - Do not use external libraries for this problem. - Make sure to handle any exceptions that might arise during function execution and print appropriate messages. Input: None required. (You may use a boolean variable inside the script to control the unregistration condition.) Output: - Printed messages indicating the registration and unregistration of each function. - Messages from the cleanup functions indicating their execution upon program termination. Example: Example output (with `unregister_b = True`): ``` Registered cleanup_a Registered cleanup_b Registered cleanup_c Unregistering cleanup_b Executing cleanup_c Executing cleanup_a ``` Example output (with `unregister_b = False`): ``` Registered cleanup_a Registered cleanup_b Registered cleanup_c Executing cleanup_c Executing cleanup_b Executing cleanup_a ``` Note: - Test the program with different values of `unregister_b` to verify the conditional unregistration functionality.","solution":"import atexit def cleanup_a(): print(\\"Executing cleanup_a\\") def cleanup_b(): print(\\"Executing cleanup_b\\") def cleanup_c(): print(\\"Executing cleanup_c\\") # Register the cleanup functions with atexit atexit.register(cleanup_a) print(\\"Registered cleanup_a\\") atexit.register(cleanup_b) print(\\"Registered cleanup_b\\") atexit.register(cleanup_c) print(\\"Registered cleanup_c\\") # Boolean variable to control the unregistration of cleanup_b unregister_b = True if unregister_b: atexit.unregister(cleanup_b) print(\\"Unregistering cleanup_b\\")"},{"question":"# Data Preprocessing and Validation using scikit-learn utilities **Problem Statement:** You are tasked with implementing a function that preprocesses a given dataset and performs validation checks using utilities from the `sklearn.utils` module. The function should: 1. Accept an input array `X` and a target array `y`. 2. Standardize the features in `X`. 3. Perform basic validation checks on `X` and `y` to ensure they meet certain criteria (no NaNs/Infs, consistent lengths, proper shapes, etc.). 4. Return the standardized feature array and the target array after validation. **Function Signature:** ```python def preprocess_and_validate(X, y): Preprocess the dataset and perform validation checks. Parameters: X (array-like): Feature array of shape (n_samples, n_features) y (array-like): Target array of shape (n_samples,) Returns: X_processed (array-like): Standardized feature array after validation, of shape (n_samples, n_features) y_validated (array-like): Validated target array, of shape (n_samples,) Raises: ValueError: If validation checks fail. ``` **Requirements:** 1. **Standardization**: Standardize the feature array `X` so that each feature has a mean of 0 and a standard deviation of 1. You can manually implement this or use existing numpy/scipy functions. 2. **Validation Checks**: - Ensure that `X` does not have any NaNs or infinite values. - Ensure that `X` is a 2D array with consistent length with `y`. - Ensure that `y` is a 1D array. 3. Use the appropriate validation and utility functions from `sklearn.utils` for these checks. Specifically, utilize `check_array`, `assert_all_finite`, `check_X_y`, and any other relevant functions as necessary. 4. Ensure the function is robust and handles exceptions gracefully, providing clear error messages when validation checks fail. **Example Usage:** ```python import numpy as np # Example input arrays X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) y = np.array([1, 2, 3]) # Function call X_processed, y_validated = preprocess_and_validate(X, y) print(X_processed) print(y_validated) ``` **Constraints:** - The input array `X` should be a 2D array of shape `(n_samples, n_features)`. - The target array `y` should be a 1D array of shape `(n_samples)`. - You may assume that `X` and `y` are numpy arrays upon function entry. - Handle edge cases like empty arrays or arrays with only NaN/Inf values. **Notes:** - This task assesses your understanding of data preprocessing, validation, and utility function usage in scikit-learn, as well as your ability to handle exceptions and provide user-friendly error messages. - Ensure your implementation is efficient and maintains the clarity and readability of code.","solution":"import numpy as np from sklearn.utils import check_X_y def preprocess_and_validate(X, y): Preprocess the dataset and perform validation checks. Parameters: X (array-like): Feature array of shape (n_samples, n_features) y (array-like): Target array of shape (n_samples,) Returns: X_processed (array-like): Standardized feature array after validation, of shape (n_samples, n_features) y_validated (array-like): Validated target array, of shape (n_samples,) Raises: ValueError: If validation checks fail. # Validate the input and ensure no NaNs/Infs, consistent lengths, proper shapes X, y = check_X_y(X, y, ensure_2d=True, allow_nd=False, force_all_finite=True) # Standardize X: (X - mean) / std X_processed = (X - np.mean(X, axis=0)) / np.std(X, axis=0) return X_processed, y"},{"question":"**Question: Advanced Distribution Visualization using Seaborn** You are provided with a dataset on penguin measurements. Your task is to visualize the distribution of penguin flipper lengths and bill dimensions, while also conditioning on species and gender. # Dataset: - Use the `penguins` dataset from the Seaborn library. # Requirements: 1. Write a function `visualize_penguin_data()` that: - Plots the distribution of `flipper_length_mm` with different bin widths to show the effect on histogram interpretation. - Uses KDE plots to display the distribution of `flipper_length_mm` and conditions on `species`. - Plots a bivariate distribution of `bill_length_mm` and `bill_depth_mm` using both histogram and KDE, conditioning on `species`. - Adds appropriate titles, labels, and legends to each plot for clarity. # Input: - No direct input, but make sure your function loads the `penguins` dataset internally. # Output: - The function should display the following Seaborn visualizations: 1. Histograms of `flipper_length_mm` with different `binwidth` values. 2. KDE plot of `flipper_length_mm` conditioned on `species`. 3. Bivariate histogram and KDE of `bill_length_mm` and `bill_depth_mm`, conditioned on `species`. # Constraints and Notes: - Ensure all plots are properly labeled with titles and axis labels. - Use different color palettes to distinguish between species in multiple plots. - Combine individual plots into a cohesive subplot layout using `matplotlib` functions like `plt.subplots`. # Example Output: ```python visualize_penguin_data() ``` This function will display: - Histograms for `flipper_length_mm` showing different bin widths. - A KDE plot for `flipper_length_mm` conditional on `species`. - Bivariate distributions (both histogram and KDE) for `bill_length_mm` vs `bill_depth_mm`; also conditional on `species`. The question aims to test your ability to work with Seaborn\'s advanced distribution visualization tools and effectively communicate insights from data through visual representations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the penguins dataset df = sns.load_dataset(\'penguins\') # Set the style for the plots sns.set(style=\\"whitegrid\\") # Create subplots fig, axes = plt.subplots(3, 2, figsize=(15, 15)) # Histogram of flipper_length_mm with different bin widths sns.histplot(df[\'flipper_length_mm\'].dropna(), bins=10, ax=axes[0, 0]) axes[0, 0].set_title(\\"Histogram of Flipper Length (Bin width=10)\\") axes[0, 0].set_xlabel(\\"Flipper Length (mm)\\") axes[0, 0].set_ylabel(\\"Count\\") sns.histplot(df[\'flipper_length_mm\'].dropna(), bins=20, ax=axes[0, 1]) axes[0, 1].set_title(\\"Histogram of Flipper Length (Bin width=20)\\") axes[0, 1].set_xlabel(\\"Flipper Length (mm)\\") axes[0, 1].set_ylabel(\\"Count\\") # KDE plot of flipper_length_mm conditioned on species sns.kdeplot(data=df, x=\'flipper_length_mm\', hue=\'species\', fill=True, ax=axes[1, 0]) axes[1, 0].set_title(\\"KDE plot of Flipper Length by Species\\") axes[1, 0].set_xlabel(\\"Flipper Length (mm)\\") axes[1, 0].set_ylabel(\\"Density\\") # Bivariate histogram of bill_length_mm and bill_depth_mm conditioned on species sns.histplot(data=df, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', ax=axes[1, 1], bins=20) axes[1, 1].set_title(\\"Bivariate Histogram of Bill Length vs. Bill Depth by Species\\") axes[1, 1].set_xlabel(\\"Bill Length (mm)\\") axes[1, 1].set_ylabel(\\"Bill Depth (mm)\\") # Bivariate KDE of bill_length_mm and bill_depth_mm conditioned on species sns.kdeplot(data=df, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', ax=axes[2, 0], fill=True) axes[2, 0].set_title(\\"Bivariate KDE of Bill Length vs. Bill Depth by Species\\") axes[2, 0].set_xlabel(\\"Bill Length (mm)\\") axes[2, 0].set_ylabel(\\"Bill Depth (mm)\\") # Adjust layout plt.tight_layout() plt.show()"},{"question":"# Design a Text-based User Interface with Curses Objective: Design a text-based user interface (TUI) to capture and display user input within a defined area of the terminal screen. The TUI should update in real-time as the user types, handle navigation within the input area, and support basic text editing functionality. Task: 1. **Initialize Curses**: Set up the curses environment, ensuring proper initialization and termination. Use appropriate settings for the terminal (echo mode off, cbreak mode on, etc.). 2. **Create a Textbox**: Define a `Textbox` widget to capture user input. The textbox should support navigation (left, right, up, down) and basic editing commands (insert/delete characters). 3. **Color and Attributes**: Utilize `curses` color pairs to distinguish different parts of the interface, such as input area and borders. 4. **Real-time Updates**: Ensure the screen updates in real-time as the user types or navigates through the textbox. 5. **Error Handling**: Appropriately handle errors such as writing outside the window boundaries or invalid cursor movements. Requirements: - **Input** should be captured and reflected on the screen within the designated textbox area. - **Navigation**: Use arrow keys or equivalent key combinations (Control characters, e.g., Ctrl+B, Ctrl+F for left and right). - **Editing Commands**: - Insert characters at the cursor position. - Delete characters using Backspace or Delete keys. - **Exit**: Provide a way to exit the TUI cleanly (e.g., using a specific key like `Ctrl+G`). Constraints: - The textbox should not exceed the predefined window size. - Ensure the terminal screen is restored to its original state upon exit. Example Function Definition: ```python import curses from curses.textpad import Textbox, rectangle def main(stdscr): # Initialize curses and settings (no echo, cbreak mode, etc.) curses.curs_set(1) curses.start_color() curses.init_pair(1, curses.COLOR_CYAN, curses.COLOR_BLACK) # Define window size and create textbox height, width = 10, 40 begin_y, begin_x = 5, 10 win = curses.newwin(height, width, begin_y, begin_x) stdscr.addstr(0, 0, \\"Press Ctrl-G to end input\\", curses.color_pair(1)) rectangle(stdscr, begin_y-1, begin_x-1, begin_y+height, begin_x+width) stdscr.refresh() # Create textbox and handle input textbox = Textbox(win) def validate_input(ch): if ch == 7: # Ctrl-G return 0 # End input else: return ch textbox.edit(validate_input) stdscr.addstr(begin_y + height + 2, 0, \\"Input finished\\") stdscr.refresh() stdscr.getch() curses.wrapper(main) ``` Deliverables: 1. A Python script implementing the described TUI. 2. Ensure the code is well-documented and includes comments explaining key sections. 3. Provide a brief description of how to run the script and interact with the TUI.","solution":"import curses from curses.textpad import Textbox, rectangle def main(stdscr): # Initialize curses and settings curses.curs_set(1) curses.noecho() curses.cbreak() stdscr.keypad(True) curses.start_color() curses.init_pair(1, curses.COLOR_CYAN, curses.COLOR_BLACK) # Define window size and create textbox height, width = 10, 40 begin_y, begin_x = 5, 10 win = curses.newwin(height, width, begin_y, begin_x) stdscr.addstr(0, 0, \\"Press Ctrl-G to end input\\", curses.color_pair(1)) rectangle(stdscr, begin_y-1, begin_x-1, begin_y+height, begin_x+width) stdscr.refresh() # Create textbox and handle input textbox = Textbox(win) def validate_input(ch): # Handle Ctrl-G (ASCII value 7) to end input if ch == 7: return 0 else: return ch textbox.edit(validate_input) # Display a message when input is finished stdscr.addstr(begin_y + height + 2, 0, \\"Input finished\\", curses.color_pair(1)) stdscr.refresh() stdscr.getch() def run_curses_ui(): curses.wrapper(main) if __name__ == \'__main__\': run_curses_ui()"},{"question":"You are tasked with analyzing a dataset on global temperature changes, specifically on the average temperature measured across different cities over the years. Your goal is to visualize the temperature trends using Seaborn\'s Objects interface. The dataset `temperature_changes` contains the following columns: - `City`: The city where the temperature was recorded. - `Year`: The year the temperature was measured. - `Avg_Temperature`: The average temperature recorded that year. Your objective is to create a plot that shows the trajectory of average temperature changes through the years for different cities. Customize the plot marks to make the visualization informative and aesthetically pleasing. **Requirements:** 1. Load the dataset `temperature_changes`. 2. Create a plot object with \\"Year\\" on the x-axis and \\"Avg_Temperature\\" on the y-axis. 3. Color the trajectories by \\"City\\". 4. Customize the plot with the following properties: - Use circular markers (`\'o\'`). - Set the marker size to 3. - Set the line width to 1. - Use white color for the marker fill. **Expected Format:** - Input: The dataset `temperature_changes`. - Output: A Seaborn plot object visualizing the temperature trends. Here is an example of the dataset schema: | City | Year | Avg_Temperature | |------------|------|------------------| | New York | 2000 | 12.3 | | Los Angeles| 2000 | 15.1 | | ... | ... | ... | # Implementation ```python import seaborn.objects as so from seaborn import load_dataset # Assuming \'temperature_changes\' can be loaded similarly with load_dataset temperature_changes = load_dataset(\'temperature_changes\').sort_values([\'City\', \'Year\']) # Creating the plot p = so.Plot(temperature_changes, x=\'Year\', y=\'Avg_Temperature\', color=\'City\') p.add(so.Path(marker=\'o\', pointsize=3, linewidth=1, fillcolor=\'w\')) # Ensure to display/show plot p.show() ``` This code should correctly create the required plot. Ensure the `temperature_changes` dataset is available and correctly loaded in the same manner as demonstrated in the documentation.","solution":"import seaborn.objects as so import pandas as pd import matplotlib.pyplot as plt def plot_temperature_trends(data): Plots the temperature trends of different cities over the years. Parameters: data (pd.DataFrame): DataFrame containing the columns: \'City\', \'Year\', \'Avg_Temperature\'. Returns: seaborn.objects.Plot: A Seaborn plot object. data = data.sort_values([\'City\', \'Year\']) p = so.Plot(data, x=\'Year\', y=\'Avg_Temperature\', color=\'City\') p.add(so.Path(marker=\'o\', pointsize=3, linewidth=1, fillcolor=\'w\')) return p"},{"question":"**Title: Data Loading and Preprocessing with scikit-learn** **Objective:** To test the ability to load different types of datasets using scikit-learn and preprocess them correctly for machine learning applications. This task will require demonstrating understanding of data loading functions, handling various file formats, and preprocessing steps to make datasets ready for model training. **Problem Statement:** You are provided with different datasets in various formats. Your task is to write a Python function that performs the following steps: 1. **Load and Preprocess Sample Images:** - Load the sample image `china.jpg` using the `load_sample_image` function from `sklearn.datasets`. - Convert the image to a floating point representation and scale the pixel values to the range [0, 1]. 2. **Load and Preprocess SVMLight File:** - Load a dataset stored in svmlight/libsvm format from a given file path. - Ensure the features are in floating point format and print the shape of the loaded features and target arrays. 3. **Fetch and Describe OpenML Dataset:** - Fetch the \\"miceprotein\\" dataset from OpenML using the `fetch_openml` function. - Print the shape of the data and target arrays, and display the unique classes present in the target array. **Function Signature:** ```python import numpy as np from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml def load_and_preprocess_datasets(svmlight_file_path: str): Loads and preprocesses different datasets using scikit-learn. Parameters: svmlight_file_path (str): The file path to the svmlight/libsvm formatted file. Returns: dict: A dictionary containing the processed data including: - \'image\': numpy array of the sample image. - \'svmlight_shape\': tuple of shape of the features and target arrays. - \'openml_shape\': tuple of shape of the data and target arrays. - \'openml_classes\': numpy array of unique classes in the target array. # Load and preprocess sample image china = load_sample_image(\\"china.jpg\\") china_float = china.astype(np.float32) / 255.0 # Load and preprocess svmlight file X_train, y_train = load_svmlight_file(svmlight_file_path) X_train = X_train.toarray().astype(np.float32) # convert sparse matrix to dense and cast to float32 # Fetch and describe OpenML dataset mice = fetch_openml(name=\'miceprotein\', version=4) openml_data_shape = mice.data.shape openml_target_shape = mice.target.shape openml_unique_classes = np.unique(mice.target) return { \\"image\\": china_float, \\"svmlight_shape\\": (X_train.shape, y_train.shape), \\"openml_shape\\": (openml_data_shape, openml_target_shape), \\"openml_classes\\": openml_unique_classes } ``` **Input:** - A file path to an svmlight/libsvm formatted file. **Output:** - A dictionary containing: - Scaled sample image. - Shape of the features and target arrays from the svmlight file. - Shape of the data and target arrays from the OpenML dataset. - Unique classes in the target array from the OpenML dataset. **Constraints:** - Ensure all dataset loading steps handle potential exceptions and provide informative error messages. - The svmlight file provided will not exceed 100 MB in size. - Only the given datasets and their structures need to be handled; no other datasets should be considered. **Performance Requirements:** - The function should process each dataset loading and preprocessing step within a reasonable time frame (under 5 seconds each for the provided datasets). # Example Usage ```python svmlight_file_path = \\"/path/to/train_dataset.txt\\" result = load_and_preprocess_datasets(svmlight_file_path) print(result[\\"image\\"].shape) # Expected output: (427, 640, 3) (dimensions of the china.jpg image) print(result[\\"svmlight_shape\\"]) # Expected output: ((number_of_samples, number_of_features), (number_of_samples,)) print(result[\\"openml_shape\\"]) # Expected output: ((1080, 77), (1080,)) print(result[\\"openml_classes\\"]) # Expected output: array of unique classes in the OpenML miceprotein dataset ```","solution":"import numpy as np from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml def load_and_preprocess_datasets(svmlight_file_path: str): Loads and preprocesses different datasets using scikit-learn. Parameters: svmlight_file_path (str): The file path to the svmlight/libsvm formatted file. Returns: dict: A dictionary containing the processed data including: - \'image\': numpy array of the sample image. - \'svmlight_shape\': tuple of shape of the features and target arrays. - \'openml_shape\': tuple of shape of the data and target arrays. - \'openml_classes\': numpy array of unique classes in the target array. # Load and preprocess sample image china = load_sample_image(\\"china.jpg\\") china_float = china.astype(np.float32) / 255.0 # Load and preprocess svmlight file X_train, y_train = load_svmlight_file(svmlight_file_path) X_train = X_train.toarray().astype(np.float32) # convert sparse matrix to dense and cast to float32 # Fetch and describe OpenML dataset mice = fetch_openml(name=\'miceprotein\', version=4) openml_data_shape = mice.data.shape openml_target_shape = mice.target.shape openml_unique_classes = np.unique(mice.target) return { \\"image\\": china_float, \\"svmlight_shape\\": (X_train.shape, y_train.shape), \\"openml_shape\\": (openml_data_shape, openml_target_shape), \\"openml_classes\\": openml_unique_classes }"},{"question":"# Background In this coding assessment, you are required to use several Python standard library modules to solve a problem related to processing and formatting data. The objective is to demonstrate your understanding of modules such as `textwrap`, `string.Template`, and `decimal`. # Task You are given a list of monetary transactions in various currencies and their exchange rates against USD. Your task is to implement a function that processes these transactions and prints them in a formatted manner using a specific template. The function must handle exceptions gracefully. # Transactions Data The transactions are provided as a list of dictionaries. Each dictionary contains the following fields: - `amount`: The amount of money involved in the transaction (as a string representing a decimal number). - `currency`: The currency code of the transaction (e.g., \\"EUR\\"). - `description`: A brief description of the transaction. - `date`: The date of the transaction (in the format \\"YYYY-MM-DD\\"). # Exchange Rates Data The exchange rates are provided as a dictionary where the keys are currency codes and the values are their respective exchange rates against USD. # Requirements 1. **Function Signature**: `def process_transactions(transactions: list, exchange_rates: dict) -> None` 2. **Input**: - `transactions`: A list of dictionaries representing the transactions. - `exchange_rates`: A dictionary with currency codes as keys and exchange rates as values. 3. **Output**: None (the function should print the output). 4. **Formatting Requirements**: - Use `decimal.Decimal` to handle and convert currency amounts precisely. - Use `string.Template` to format the transaction details. - Use `textwrap.fill` to wrap descriptions to a width of 40 characters. 5. **Handling Missing Data**: - If a transaction has missing data, skip that transaction. - If an exchange rate is missing for a currency, skip that transaction. 6. **Template**: ``` Transaction Date: date Amount: amount currency (Equivalent to usd_amount USD) Description: wrapped_description ``` # Example ```python transactions = [ {\\"amount\\": \\"100.00\\", \\"currency\\": \\"EUR\\", \\"description\\": \\"Payment for services rendered\\", \\"date\\": \\"2023-10-01\\"}, {\\"amount\\": \\"50.50\\", \\"currency\\": \\"GBP\\", \\"description\\": \\"Refund for order #1234\\", \\"date\\": \\"2023-10-02\\"} ] exchange_rates = { \\"EUR\\": 1.10, \\"GBP\\": 1.30 } process_transactions(transactions, exchange_rates) ``` Output: ``` Transaction Date: 2023-10-01 Amount: 100.00 EUR (Equivalent to 110.00 USD) Description: Payment for services rendered Transaction Date: 2023-10-02 Amount: 50.50 GBP (Equivalent to 65.65 USD) Description: Refund for order #1234 ``` # Constraints - You can assume valid input formats but not the completeness of data. - Use appropriate error handling to skip transactions with missing data.","solution":"from string import Template from decimal import Decimal, InvalidOperation import textwrap def process_transactions(transactions, exchange_rates): transaction_template = Template( \\"Transaction Date: daten\\" \\"Amount: amount currency (Equivalent to usd_amount USD)n\\" \\"Description:n\\" \\"wrapped_descriptionn\\" ) for transaction in transactions: try: amount = Decimal(transaction[\\"amount\\"]) currency = transaction[\\"currency\\"] description = transaction[\\"description\\"] date = transaction[\\"date\\"] if currency not in exchange_rates: continue usd_amount = amount * Decimal(exchange_rates[currency]) wrapped_description = textwrap.fill(description, width=40) formatted_transaction = transaction_template.substitute( date=date, amount=f\\"{amount:.2f}\\", currency=currency, usd_amount=f\\"{usd_amount:.2f}\\", wrapped_description=wrapped_description ) print(formatted_transaction) except (KeyError, InvalidOperation): # Skip transactions with missing data or invalid amount continue"},{"question":"# Question: Networking and Interprocess Communication **Objective**: To assess your understanding of networking and inter-process communication in Python using multiple modules. **Task**: You are required to write a Python program that sets up a simple encrypted client-server communication system using the `socket` and `ssl` modules. The server should be able to handle multiple clients and echo back any messages it receives from them. The clients send messages to the server, and the server sends back the same message prefixed with \\"Echo: \\". **Requirements**: 1. **Server**: - The server should use `socket` to create a TCP server that listens on a specific port. - The server should wrap the socket with SSL using the `ssl` module. Use self-signed certificates for TLS communication. - The server should handle multiple clients simultaneously. - Upon receiving a message from a client, the server should send back the message prefixed with \\"Echo: \\". 2. **Client**: - The client should establish a connection to the server using a socket. - The client should wrap the socket with SSL using the `ssl` module. - The client should send a message to the server and print the server\'s response. **Input and Output Formats**: - **Server Output**: - Print a message when a new client connects and when a message is received from a client. - **Client Output**: - Print the server\'s response to the message sent by the client. **Constraints**: - Use the `asyncio` module to handle multiple clients asynchronously. - The server should listen on port 12345. - Use self-signed certificates for SSL/TLS communication. **Performance Requirements**: - The server should be able to handle at least 5 simultaneous clients. **Hints**: - For creating self-signed certificates, you can use the `openssl` toolkit. - You may refer to Python’s `ssl` module documentation for help with wrapping sockets with SSL. **Example**: Assume the messages sent by the clients are \\"Hello Server\\". Server output (prints): ``` Client connected. Received message: Hello Server ``` Client output (prints): ``` Echo: Hello Server ``` **Note**: For testing, ensure you generate the necessary certificates and keys. **Deliverables**: - A Python file containing the server code. - A Python file containing the client code. - Instructions on how to generate the required self-signed certificates.","solution":"# server.py import asyncio import ssl import websockets async def handle_client(websocket, path): print(\\"Client connected.\\") async for message in websocket: print(f\\"Received message: {message}\\") response = f\\"Echo: {message}\\" await websocket.send(response) async def main(): ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) ssl_context.load_cert_chain(certfile=\\"server_cert.pem\\", keyfile=\\"server_key.pem\\") server = await websockets.serve(handle_client, \\"localhost\\", 12345, ssl=ssl_context) await server.wait_closed() if __name__ == \\"__main__\\": asyncio.run(main()) # client.py import ssl import asyncio import websockets async def send_message(): ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT) ssl_context.load_verify_locations(\\"server_cert.pem\\") async with websockets.connect(\\"wss://localhost:12345\\", ssl=ssl_context) as websocket: message = \\"Hello Server\\" await websocket.send(message) response = await websocket.recv() print(response) if __name__ == \\"__main__\\": asyncio.run(send_message())"},{"question":"Objective Implement a function that processes a list of strings and performs various operations based on predefined rules. Problem Statement You are given a list of strings. Your task is to implement a function `process_strings(strings: List[str]) -> Tuple[List[str], int, int]` that performs the following operations: 1. **Reverse Each String**: Reverse the characters of each string. 2. **Remove Strings Containing a Specific Character**: Remove any strings that contain the character \'a\'. 3. **Concatenate Remaining Strings**: Concatenate all remaining strings into a single string. 4. **Compute String Statistics**: Compute the following statistics: - The length of the concatenated string. - The number of times the character \'e\' appears in the concatenated string. The function should return a tuple containing: - The updated list of strings after reversing each string and removing those that contain \'a\'. - The length of the final concatenated string. - The count of \'e\' characters in the final concatenated string. Function Signature ```python from typing import List, Tuple def process_strings(strings: List[str]) -> Tuple[List[str], int, int]: # Your code here pass ``` Input and Output **Input:** - `strings` (List[str]): A list of strings. Constraints: 1 <= len(strings) <= 1000, 1 <= len(strings[i]) <= 100 **Output:** - Tuple[List[str], int, int]: A tuple containing: 1. A list of processed strings. 2. An integer representing the length of the concatenated string after all processing. 3. An integer representing the count of \'e\' characters in the final concatenated string. Example **Example 1:** ```python strings = [\\"hello\\", \\"world\\", \\"alpha\\", \\"beta\\"] result = process_strings(strings) print(result) # ([\\"olleh\\", \\"dlrow\\"], 10, 1) ``` - Explanation: - Reverse each string: `[\\"olleh\\", \\"dlrow\\", \\"ahpla\\", \\"ateb\\"]` - Remove strings containing \'a\': `[\\"olleh\\", \\"dlrow\\"]` - Concatenate remaining strings: `\\"ollehdlrow\\"` - Length of concatenated string: 10 - Count of \'e\': 1 Constraints - Strings should be processed in O(n) time complexity where n is the total number of characters across all strings. Notes - The function should handle edge cases such as empty lists or strings that all contain \'a\'. - Do not use any external libraries for string manipulation; use only built-in Python functions and methods.","solution":"from typing import List, Tuple def process_strings(strings: List[str]) -> Tuple[List[str], int, int]: # Step 1: Reverse each string reversed_strings = [s[::-1] for s in strings] # Step 2: Remove strings containing \'a\' filtered_strings = [s for s in reversed_strings if \'a\' not in s] # Step 3: Concatenate remaining strings concatenated_string = \'\'.join(filtered_strings) # Step 4: Compute required statistics length_of_concatenated_string = len(concatenated_string) count_of_e = concatenated_string.count(\'e\') return filtered_strings, length_of_concatenated_string, count_of_e"},{"question":"You are tasked with creating a custom container class in Python and implementing both shallow and deep copying mechanisms for it. Your class will encapsulate a collection of items, and you need to ensure that copying the container behaves correctly, taking into account the differences between shallow and deep copy. Requirements 1. Implement a class `CustomContainer`. - Initialize the class with a list of items. - Ensure that instances of `CustomContainer` support both shallow and deep copy operations using the `copy` module. 2. Implement the `__copy__()` method to enable shallow copying of `CustomContainer`. 3. Implement the `__deepcopy__()` method to enable deep copying of `CustomContainer`. - Use the `memo` dictionary as described in the documentation to handle recursive references and avoid redundant copying. 4. Demonstrate the difference between shallow and deep copying with a test case, where the list of items includes another `CustomContainer` instance. Input Format - No specific input format required. - You will create instances of `CustomContainer` and test the copying operations within your code. Output Format - Print the original and copied objects, showing that changes in the shallow copy reflect in the original, while changes in the deep copy do not. Example ```python from copy import copy, deepcopy class CustomContainer: def __init__(self, items): self.items = items def __copy__(self): new_one = type(self)(self.items) new_one.__dict__.update(self.__dict__) return new_one def __deepcopy__(self, memo): new_one = type(self)(deepcopy(self.items, memo)) memo[id(self)] = new_one return new_one # Example usage container = CustomContainer([1, 2, [3, 4]]) shallow_copy = copy(container) deep_copy = deepcopy(container) print(\\"Original:\\", container.items) print(\\"Shallow Copy:\\", shallow_copy.items) print(\\"Deep Copy:\\", deep_copy.items) # Modify the original container container.items[2].append(5) print(\\"After modifying original:\\") print(\\"Original:\\", container.items) print(\\"Shallow Copy:\\", shallow_copy.items) print(\\"Deep Copy:\\", deep_copy.items) ``` In this exercise, students are expected to: - Thoroughly understand the behavior of shallow and deep copying. - Implement and test a custom class with correct copying mechanisms. - Distinguish between the impact of changes on shallow and deep copies.","solution":"from copy import copy, deepcopy class CustomContainer: def __init__(self, items): self.items = items def __copy__(self): new_one = type(self)(self.items.copy()) new_one.__dict__.update(self.__dict__) return new_one def __deepcopy__(self, memo): new_one = type(self)(deepcopy(self.items, memo)) memo[id(self)] = new_one return new_one"},{"question":"# Advanced Coding Assessment on scikit-learn: Tuned Decision Threshold You are tasked with helping a healthcare startup improve their predictive model for detecting cancer. The current model outputs conditional probabilities of a patient having cancer, but the decision threshold is not optimized for the business requirement, which prioritizes high recall to ensure no potential cancer cases are missed. To address this, you\'ll use `TunedThresholdClassifierCV` to optimize the decision threshold. Task: **Implement a function `optimize_threshold` to achieve the following goals:** 1. **Input**: - `X_train`: A numpy array or pandas DataFrame containing the training features. - `y_train`: A numpy array or pandas Series containing the training labels. - `X_test`: A numpy array or pandas DataFrame containing the test features. - `y_test`: A numpy array or pandas Series containing the test labels. - `base_model`: An instance of a scikit-learn classifier to be used as the base model. - `scorer`: A scikit-learn scorer instance created using `make_scorer` to maximize a specific metric. - `cv`: Number of cross-validation folds for tuning the decision threshold. 2. **Output**: - Returns the predictions on the test set using the optimized decision threshold. - Returns the optimized threshold value. 3. **Constraints**: - Use `TunedThresholdClassifierCV` to tune the decision threshold. - Ensure you do not overfit by properly handling cross-validation. - Use the test set only for evaluating the final model with the tuned decision threshold. 4. **Performance Requirements**: - The function should run within a reasonable time frame for large datasets. Example Function Signature: ```python import numpy as np import pandas as pd from sklearn.base import BaseEstimator from sklearn.metrics import make_scorer from sklearn.model_selection import TunedThresholdClassifierCV def optimize_threshold(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, base_model: BaseEstimator, scorer, cv: int = 5) -> (np.ndarray, float): Fine-tune the decision threshold to maximize a given metric using cross-validation. Parameters: X_train (np.ndarray): Training features. y_train (np.ndarray): Training labels. X_test (np.ndarray): Test features. y_test (np.ndarray): Test labels. base_model (BaseEstimator): An instance of a scikit-learn classifier. scorer: A scikit-learn scorer instance created with `make_scorer`. cv (int): Number of cross-validation folds for threshold tuning. Returns: Tuple: - np.ndarray: Predictions on the test set using the optimized threshold. - float: Optimized threshold value. optimized_model = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=cv) optimized_model.fit(X_train, y_train) optimized_threshold = optimized_model.best_threshold_ predictions = optimized_model.predict(X_test) return predictions, optimized_threshold ``` Example Usage: ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import make_scorer, f1_score # Generate synthetic data X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) # Split data into training and test sets X_train, X_test = X[:800], X[800:] y_train, y_test = y[:800], y[800:] # Define base model and scorer base_model = LogisticRegression() scorer = make_scorer(f1_score, pos_label=0) # Optimize threshold predictions, threshold = optimize_threshold(X_train, y_train, X_test, y_test, base_model, scorer, cv=5) print(\\"Optimized Threshold:\\", threshold) print(\\"Predictions with Optimized Threshold:\\", predictions) ``` Use this question to assess students\' ability to apply advanced concepts of scikit-learn, focusing on model evaluation, cross-validation, and custom decision thresholds.","solution":"import numpy as np from sklearn.base import BaseEstimator from sklearn.metrics import make_scorer from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import label_binarize from scipy.optimize import minimize # Placeholder for `TunedThresholdClassifierCV` since it\'s not an existing class in sklearn. # We\'ll implement a simple threshold tuning mechanism for this example. class TunedThresholdClassifierCV(BaseEstimator): def __init__(self, base_model, scoring, cv=5): self.base_model = base_model self.scoring = scoring self.cv = cv self.best_threshold_ = 0.5 self.models_ = [] def fit(self, X, y): self.models_ = [] skf = StratifiedKFold(n_splits=self.cv) thresholds = [] for train_idx, val_idx in skf.split(X, y): X_train, X_val = X[train_idx], X[val_idx] y_train, y_val = y[train_idx], y[val_idx] model = self.base_model.fit(X_train, y_train) self.models_.append(model) # Obtain probability predictions probas = model.predict_proba(X_val)[:, 1] # Define objective function to minimize (negative score) def objective(threshold): preds = (probas >= threshold).astype(int) return -self.scoring._score_func(y_val, preds) # Optimize threshold optimal_threshold = minimize(objective, x0=0.5, bounds=[(0, 1)]).x[0] thresholds.append(optimal_threshold) # Average the optimal thresholds self.best_threshold_ = np.mean(thresholds) def predict(self, X): avg_probas = np.mean([model.predict_proba(X)[:, 1] for model in self.models_], axis=0) return (avg_probas >= self.best_threshold_).astype(int) def optimize_threshold(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, base_model: BaseEstimator, scorer, cv: int = 5) -> (np.ndarray, float): Fine-tune the decision threshold to maximize a given metric using cross-validation. Parameters: X_train (np.ndarray): Training features. y_train (np.ndarray): Training labels. X_test (np.ndarray): Test features. y_test (np.ndarray): Test labels. base_model (BaseEstimator): An instance of a scikit-learn classifier. scorer: A scikit-learn scorer instance created with `make_scorer`. cv (int): Number of cross-validation folds for threshold tuning. Returns: Tuple: - np.ndarray: Predictions on the test set using the optimized threshold. - float: Optimized threshold value. optimized_model = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=cv) optimized_model.fit(X_train, y_train) optimized_threshold = optimized_model.best_threshold_ predictions = optimized_model.predict(X_test) return predictions, optimized_threshold"},{"question":"# Question: Implementing Cell Objects in Python **Problem Statement:** You are required to implement a Python class `PyCell` that mimics the behavior of Python \\"Cell\\" objects, which store and manage variables referenced by multiple scopes. This will include methods to create cell objects, get their contents, and set their values. **Requirements:** 1. **Class Definition:** - Define a class `PyCell` to represent a cell object. 2. **Methods to Implement:** - `__init__(self, value=None)`: Initialize the cell with an optional initial value. - `get(self)`: Return the current value stored in the cell. - `set(self, value)`: Set the cell to a new value, releasing any current content. 3. **Input and Output:** - Constructor: Takes an optional initial value (default is `None`). - `get` method: Returns the value stored in the cell. - `set` method: Takes a value to store in the cell and returns `None`. **Constraints:** - The class methods should handle arbitrary Python objects (e.g., integers, strings, lists, etc.). - The `set` method should be able to accept `None` as a value. - Efficiency is not the main concern, but clarity and correctness are. **Example Usage:** ```python # Creating a cell object with an initial value cell = PyCell(10) print(cell.get()) # Output: 10 # Setting a new value to the cell cell.set(20) print(cell.get()) # Output: 20 # Setting the cell value to None cell.set(None) print(cell.get()) # Output: None # Creating a cell object without an initial value cell_2 = PyCell() print(cell_2.get()) # Output: None # Setting and getting a list value cell_2.set([1, 2, 3]) print(cell_2.get()) # Output: [1, 2, 3] ``` **Your Task:** Implement the `PyCell` class with the specified methods and ensure they work as intended based on the example usage provided.","solution":"class PyCell: def __init__(self, value=None): Initialize the cell with an optional initial value. self._value = value def get(self): Return the current value stored in the cell. return self._value def set(self, value): Set the cell to a new value, releasing any current content. self._value = value"},{"question":"# Python Coding Assessment Question Objective: Implement a generator function in Python that iterates over a sequence of numbers and yields only the prime numbers from the sequence. The function should demonstrate a clear understanding of how generators work in Python. Description: Write a generator function named `prime_generator` that takes two integers, `start` and `end`, as parameters. The generator should iterate over each number in the range `[start, end)`, check if it is prime, and yield each prime number. Definitions: - **Prime Number**: A natural number greater than 1 that has no positive divisors other than 1 and itself. Function Signature: ```python def prime_generator(start: int, end: int) -> int: pass ``` Input: - `start` (int): The lower bound of the range (inclusive). - `end` (int): The upper bound of the range (exclusive). Output: - Yields each prime number in the range `[start, end)`. Constraints: - `1 <= start < end <= 10^6` Example: ```python gen = prime_generator(10, 20) for prime in gen: print(prime) # Output should be: # 11 # 13 # 17 # 19 ``` Notes: - You should not use any external libraries for generating or checking prime numbers. - Aim for an efficient solution that minimizes computational complexity and optimizes performance for large ranges. Performance Requirements: - The solution should efficiently handle large ranges up to the maximum constraint of 10^6.","solution":"def is_prime(n): Check if a number is a prime number. if n <= 1: return False if n == 2: return True # 2 is a prime number if n % 2 == 0: return False p = 3 while p * p <= n: if n % p == 0: return False p += 2 return True def prime_generator(start: int, end: int): Generator that yields prime numbers in the range [start, end). for num in range(start, end): if is_prime(num): yield num"},{"question":"# Python 3.10 Coding Assessment **Problem Overview:** As a Python developer, you are tasked with troubleshooting and optimizing a Python script that performs multiple nested function calls. To understand the flow of these function calls and identify potential bottlenecks in the execution, you will use SystemTap to trace the call and return hierarchy within the given script. # **Task:** 1. **Python Script:** - Implement a Python script that includes several nested function calls. Each function should perform a simple arithmetic operation and print a statement indicating the function name and line number from which it\'s printing. 2. **SystemTap Script:** - Write a SystemTap script that traces the function call and return events in the Python script. The script should log each function called, the filename, and the line number from which the function was called. # **Python Script Requirements:** - Create a Python script named `script.py` containing at least three functions with nested calls: ```python def start(): print(\\"Starting function: start\\") function_1() def function_1(): print(\\"In function_1 before calling function_2\\") function_2() print(\\"In function_1 after calling function_2\\") def function_2(): print(\\"In function_2 before performing arithmetic\\") result = 2 + 3 # Simple arithmetic operation print(\\"In function_2 after performing arithmetic\\") if __name__ == \\"__main__\\": start() ``` # **SystemTap Script Requirements:** - Write a SystemTap script named `trace.stp` to trace function call and return events in the Python script. - The output should log each function called and returned, including the filename and line number. **Example SystemTap Script:** ```systemtap probe process(\\"python3\\").mark(\\"function__entry\\") { filename = user_string(arg1); funcname = user_string(arg2); lineno = arg3; printf(\\"ENTRY: %s in %s at line %dn\\", funcname, filename, lineno); } probe process(\\"python3\\").mark(\\"function__return\\") { filename = user_string(arg1); funcname = user_string(arg2); lineno = arg3; printf(\\"RETURN: %s in %s at line %dn\\", funcname, filename, lineno); } ``` # **Constraints:** - Ensure that your Python script logically separates function behaviors. - The SystemTap script must be compatible with the Python script and work on a Linux system with SystemTap installed. # **Submission:** Provide the following files: 1. `script.py` - The Python script with nested function calls. 2. `trace.stp` - The SystemTap script to trace function calls. # **Evaluation Criteria:** - Correct implementation of the Python script with the required nested function calls. - Properly functioning SystemTap script that correctly traces and logs the function calls and returns. - Clarity and readability of both scripts.","solution":"def start(): print(\\"Starting function: start\\") function_1() def function_1(): print(\\"In function_1 before calling function_2\\") function_2() print(\\"In function_1 after calling function_2\\") def function_2(): print(\\"In function_2 before performing arithmetic\\") result = 2 + 3 # Simple arithmetic operation print(\\"In function_2 after performing arithmetic\\") if __name__ == \\"__main__\\": start()"},{"question":"# Question: You are tasked with creating a simplified file monitoring service using Python\'s `selectors` module. The service should monitor multiple file objects for being ready to read or write. Specifically, you need to implement a class `FileMonitor` that supports registering, unregistering, and monitoring file objects for I/O events. Requirements: 1. **Class Definition**: - Define a class `FileMonitor` that uses `selectors.DefaultSelector` for managing file objects. 2. **Methods**: - `register_file(self, fileobj, events, callback)`: Registers a file object for monitoring. Parameters are: - `fileobj`: The file object to monitor (either a file descriptor or an object with a `fileno()` method). - `events`: A bitwise mask of events to monitor (`selectors.EVENT_READ` or `selectors.EVENT_WRITE`). - `callback`: A function to be called when the file object is ready for the specified event(s). The callback function should take two arguments: the file object and the events mask. - `unregister_file(self, fileobj)`: Unregisters a previously registered file object. Raises `KeyError` if the file object is not registered. - `modify_file(self, fileobj, events, callback)`: Changes the events to monitor for a registered file object and updates the callback. Raises `KeyError` if the file object is not registered. - `monitor(self, timeout=None)`: Waits for registered file objects to be ready for their respective events and calls their associated callbacks. - `timeout`: The maximum time to wait, in seconds. If `timeout` is `None`, it should block until at least one file object is ready. Input/Output: - **Input**: The methods `register_file` and `unregister_file` take appropriate arguments as described above. - **Output**: The `monitor` method should call the registered callback functions for ready file objects. Constraints: - Ensure that file objects are unregistered before being closed. - Use proper exception handling for invalid operations. - The `monitor` method should handle signals appropriately if the process receives a signal. Example: ```python import selectors import socket import time class FileMonitor: def __init__(self): self.selector = selectors.DefaultSelector() def register_file(self, fileobj, events, callback): self.selector.register(fileobj, events, data=callback) def unregister_file(self, fileobj): self.selector.unregister(fileobj) def modify_file(self, fileobj, events, callback): self.selector.modify(fileobj, events, data=callback) def monitor(self, timeout=None): while True: events = self.selector.select(timeout) for key, mask in events: callback = key.data callback(key.fileobj, mask) # Usage Example def accept(sock, mask): conn, addr = sock.accept() print(\'Accepted connection from\', addr) conn.setblocking(False) monitor.register_file(conn, selectors.EVENT_READ, read) def read(conn, mask): data = conn.recv(1000) if data: print(\'Received data:\', repr(data)) conn.send(data) else: print(\'Closing connection\') monitor.unregister_file(conn) conn.close() monitor = FileMonitor() sock = socket.socket() sock.bind((\'localhost\', 1234)) sock.listen(100) sock.setblocking(False) monitor.register_file(sock, selectors.EVENT_READ, accept) # Perform monitoring (in practice, you might want to run this in a separate thread or process) monitor.monitor(timeout=1) ``` This example sets up a basic echo server using the `FileMonitor` class. It registers a socket for accepting connections, and when a connection is accepted, it registers the connection socket for reading data. Implement the `FileMonitor` class with the specified methods and functionalities.","solution":"import selectors class FileMonitor: def __init__(self): self.selector = selectors.DefaultSelector() def register_file(self, fileobj, events, callback): Registers a file object for monitoring. Parameters: fileobj: The file object to monitor (either a file descriptor or an object with a fileno() method). events: A bitwise mask of events to monitor (selectors.EVENT_READ or selectors.EVENT_WRITE). callback: A function to be called when the file object is ready for the specified event(s). The callback function should take two arguments: the file object and the events mask. self.selector.register(fileobj, events, data=callback) def unregister_file(self, fileobj): Unregisters a previously registered file object. Parameters: fileobj: The file object to unregister Raises: KeyError: If the file object is not registered. self.selector.unregister(fileobj) def modify_file(self, fileobj, events, callback): Changes the events to monitor for a registered file object and updates the callback. Parameters: fileobj: The file object to modify. events: A bitwise mask of events to monitor (selectors.EVENT_READ or selectors.EVENT_WRITE). callback: A function to be called when the file object is ready for the specified event(s). Raises: KeyError: If the file object is not registered. self.selector.modify(fileobj, events, data=callback) def monitor(self, timeout=None): Waits for registered file objects to be ready for their respective events and calls their associated callbacks. Parameters: timeout: The maximum time to wait, in seconds. If timeout is None, it should block until at least one file object is ready. while True: events = self.selector.select(timeout) for key, mask in events: callback = key.data callback(key.fileobj, mask)"},{"question":"# Seaborn Clustering and Visualization Task You are tasked with writing a script to visualize a dataset using seaborn, focused on creating clustered heatmaps. Your script should dynamically load a dataset, preprocess it, and generate customized visualizations. Follow the steps below to complete the task. Requirements: 1. **Load the Dataset:** - Load the `iris` dataset using seaborn\'s `load_dataset` function. - Separate the `species` column from the rest of the data. 2. **Generate a Basic Clustered Heatmap:** - Create a basic clustered heatmap for the remaining columns of the dataset using `sns.clustermap`. 3. **Heatmap Customization:** - Adjust the size of the heatmap to `10x7`. - Remove the row clustering. - Adjust the dendrogram ratios to `(0.1, 0.2)`. - Position the color bar at `(0, 0.2, 0.03, 0.4)`. 4. **Add Row Colors:** - Add colored labels to the rows based on the species, where each species is represented by a unique color. 5. **Change Colormap and Color Limits:** - Change the colormap of the heatmap to `mako`. - Set the color range limits between 0 and 10. 6. **Clustering Parameters:** - Use `correlation` as the metric and `single` as the method for clustering. 7. **Standardize Data:** - Standardize the data within columns before generating the heatmap. 8. **Normalize Data:** - Normalize the data within rows using z-scores and set the center of the colormap to 0. Input: - None. All datasets and parameters are predefined for this task. Output: - The script should produce a single, customized seaborn clustered heatmap based on the requirements outlined above. Constraints: - Ensure all visualizations and transformations are done using seaborn. - Use appropriate seaborn functions and methods as demonstrated in the provided documentation. Example: ```python import seaborn as sns # Step 1: Load the dataset iris = sns.load_dataset(\\"iris\\") species = iris.pop(\\"species\\") # Step 2: Create a basic clustermap sns.clustermap(iris) # Step 3: Customize the heatmap sns.clustermap( iris, figsize=(10, 7), row_cluster=False, dendrogram_ratio=(.1, .2), cbar_pos=(0, .2, .03, .4) ) # Step 4: Add colored labels lut = dict(zip(species.unique(), \\"rbg\\")) row_colors = species.map(lut) sns.clustermap(iris, row_colors=row_colors) # Step 5: Change colormap and color limits sns.clustermap(iris, cmap=\\"mako\\", vmin=0, vmax=10) # Step 6: Use different clustering parameters sns.clustermap(iris, metric=\\"correlation\\", method=\\"single\\") # Step 7: Standardize data sns.clustermap(iris, standard_scale=1) # Step 8: Normalize data with z-score sns.clustermap(iris, z_score=0, cmap=\\"vlag\\", center=0) ``` Ensure your solution meets each step\'s requirements and produces a final, well-customized heatmap visual. Good luck!","solution":"import seaborn as sns from sklearn.preprocessing import StandardScaler import pandas as pd def create_clustered_heatmap(): # Load the dataset iris = sns.load_dataset(\\"iris\\") # Separate the species column from the rest of the data species = iris.pop(\\"species\\") # Standardize the data within columns scaler = StandardScaler() scaled_data = scaler.fit_transform(iris) scaled_iris = pd.DataFrame(scaled_data, columns=iris.columns) # Create color palette for species lut = dict(zip(species.unique(), sns.color_palette(\\"husl\\", len(species.unique())))) row_colors = species.map(lut) # Create a clustered heatmap sns.clustermap( scaled_iris, figsize=(10, 7), row_cluster=False, dendrogram_ratio=(.1, .2), cbar_pos=(0, .2, .03, .4), row_colors=row_colors, cmap=\\"mako\\", vmin=0, vmax=10, metric=\\"correlation\\", method=\\"single\\", z_score=0, center=0 ).fig.suptitle(\'Customized Clustered Heatmap of Iris Dataset\', y=1.02)"},{"question":"# Advanced XML-RPC Client Implementation Objective: You are tasked with demonstrating mastery over XML-RPC client operations by utilizing the `xmlrpc.client` module to create a robust client that communicates with a predefined server. The challenge involves various interactions with the server, error handling, and making use of `MultiCall` for batching operations. Background: You have access to a server that supports XML-RPC operations. The server supports the following methods: - `system.listMethods()`: Returns a list of methods supported by the XML-RPC server. - `system.methodSignature(name)`: Returns a list of possible signatures for a given method. - `system.methodHelp(name)`: Returns a documentation string for a given method. - `add(x, y)`: Returns the sum of `x` and `y`. - `subtract(x, y)`: Returns the difference of `x` and `y`. - `multiply(x, y)`: Returns the product of `x` and `y`. - `divide(x, y)`: Returns the quotient of `x` and `y`. Task: 1. **Client Initialization**: - Initialize an `xmlrpc.client.ServerProxy` object to connect to the server at the URI \\"http://localhost:8000\\". 2. **Query Server Methods**: - Implement a function `get_server_methods(proxy)` that lists all methods supported by the server using `system.listMethods()`. - For each method, retrieve its signature using `system.methodSignature(name)` and its documentation using `system.methodHelp(name)`. 3. **Execute Arithmetic Operations**: - Implement a function `execute_arithmetic_operations(proxy)` that performs the following: - Adds 10 and 5. - Subtracts 10 from 5. - Multiplies 10 by 5. - Divides 10 by 5. - Print results of each operation. 4. **Batch Operations with MultiCall**: - Implement a function `batch_arithmetic_operations(proxy)` that uses `xmlrpc.client.MultiCall` to perform the following operations in a single request: - Add 7 and 3. - Subtract 7 from 3. - Multiply 7 by 3. - Divide 7 by 3. - Print results of each operation from the batch call. 5. **Error Handling**: - Implement error handling for `Fault` and `ProtocolError` exceptions. Capture and print error details where appropriate. Constraints & Requirements: - Use the `xmlrpc.client.ServerProxy` class to interface with the XML-RPC server. - Handle potential errors gracefully and provide meaningful output. - Ensure the implementation covers listed arithmetic operations and queries using provided system methods. - Optimize the client to minimize redundant server calls by using batch operations where possible. Sample Code Structure: ```python import xmlrpc.client def get_server_methods(proxy): try: methods = proxy.system.listMethods() for method in methods: print(f\\"Method: {method}\\") signature = proxy.system.methodSignature(method) print(f\\"Signature: {signature}\\") help_text = proxy.system.methodHelp(method) print(f\\"Help: {help_text}\\") except xmlrpc.client.Fault as fault: print(f\\"A fault occurred. Fault code: {fault.faultCode}, Fault string: {fault.faultString}\\") except xmlrpc.client.ProtocolError as error: print(f\\"A protocol error occurred. URL: {error.url}, Code: {error.errcode}, Message: {error.errmsg}\\") def execute_arithmetic_operations(proxy): try: print(f\\"10 + 5 = {proxy.add(10, 5)}\\") print(f\\"10 - 5 = {proxy.subtract(10, 5)}\\") print(f\\"10 * 5 = {proxy.multiply(10, 5)}\\") print(f\\"10 / 5 = {proxy.divide(10, 5)}\\") except xmlrpc.client.Fault as fault: print(f\\"A fault occurred. Fault code: {fault.faultCode}, Fault string: {fault.faultString}\\") except xmlrpc.client.ProtocolError as error: print(f\\"A protocol error occurred. URL: {error.url}, Code: {error.errcode}, Message: {error.errmsg}\\") def batch_arithmetic_operations(proxy): multicall = xmlrpc.client.MultiCall(proxy) multicall.add(7, 3) multicall.subtract(7, 3) multicall.multiply(7, 3) multicall.divide(7, 3) try: for result in multicall(): print(result) except xmlrpc.client.Fault as fault: print(f\\"A fault occurred. Fault code: {fault.faultCode}, Fault string: {fault.faultString}\\") except xmlrpc.client.ProtocolError as error: print(f\\"A protocol error occurred. URL: {error.url}, Code: {error.errcode}, Message: {error.errmsg}\\") def main(): with xmlrpc.client.ServerProxy(\\"http://localhost:8000\\") as proxy: get_server_methods(proxy) execute_arithmetic_operations(proxy) batch_arithmetic_operations(proxy) if __name__ == \\"__main__\\": main() ``` Notes: - You are to complete the code structure provided and ensure it meets the functional requirements described. - Implement necessary error handling and ensure the client operates smoothly with the described server.","solution":"import xmlrpc.client def get_server_methods(proxy): try: methods = proxy.system.listMethods() for method in methods: print(f\\"Method: {method}\\") signature = proxy.system.methodSignature(method) print(f\\"Signature: {signature}\\") help_text = proxy.system.methodHelp(method) print(f\\"Help: {help_text}\\") except xmlrpc.client.Fault as fault: print(f\\"A fault occurred. Fault code: {fault.faultCode}, Fault string: {fault.faultString}\\") except xmlrpc.client.ProtocolError as error: print(f\\"A protocol error occurred. URL: {error.url}, Code: {error.errcode}, Message: {error.errmsg}\\") def execute_arithmetic_operations(proxy): try: print(f\\"10 + 5 = {proxy.add(10, 5)}\\") print(f\\"10 - 5 = {proxy.subtract(10, 5)}\\") print(f\\"10 * 5 = {proxy.multiply(10, 5)}\\") print(f\\"10 / 5 = {proxy.divide(10, 5)}\\") except xmlrpc.client.Fault as fault: print(f\\"A fault occurred. Fault code: {fault.faultCode}, Fault string: {fault.faultString}\\") except xmlrpc.client.ProtocolError as error: print(f\\"A protocol error occurred. URL: {error.url}, Code: {error.errcode}, Message: {error.errmsg}\\") def batch_arithmetic_operations(proxy): multicall = xmlrpc.client.MultiCall(proxy) multicall.add(7, 3) multicall.subtract(7, 3) multicall.multiply(7, 3) multicall.divide(7, 3) try: for result in multicall(): print(result) except xmlrpc.client.Fault as fault: print(f\\"A fault occurred. Fault code: {fault.faultCode}, Fault string: {fault.faultString}\\") except xmlrpc.client.ProtocolError as error: print(f\\"A protocol error occurred. URL: {error.url}, Code: {error.errcode}, Message: {error.errmsg}\\") def main(): with xmlrpc.client.ServerProxy(\\"http://localhost:8000\\") as proxy: get_server_methods(proxy) execute_arithmetic_operations(proxy) batch_arithmetic_operations(proxy) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Objective: Write a Python function that performs various operations on rational numbers using the `fractions` module. Your function should be able to handle inputs of different types and perform a series of operations as described below. Function Signature: ```python def process_fractions(data: list) -> dict: pass ``` Parameters: - `data`: A list of tuples where each tuple contains: - A string representing a mathematical operation (`\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, `\\"divide\\"`, `\\"limit_denominator\\"`, `\\"as_integer_ratio\\"`) - Two parameters (can be integers, floats, decimals, strings, or `Fraction`s) for the operations, except for `\\"as_integer_ratio\\"` which takes only one parameter. Returns: - A dictionary with the result of each operation. The keys should be integers corresponding to the index of the operation in the input list, and the values should be the result of that operation. Constraints: - Division should be handled carefully to avoid division by zero errors. - For `\\"limit_denominator\\"`, the second parameter is an optional integer specifying the max denominator (default to 1000000). Example Usage: ```python from fractions import Fraction from decimal import Decimal data = [ (\\"add\\", (1, 2), (1, 3)), # Fraction(1, 2) + Fraction(1, 3) (\\"subtract\\", \\"3/4\\", \\"1/4\\"), # Fraction(\\"3/4\\") - Fraction(\\"1/4\\") (\\"multiply\\", Fraction(3, 7), 2), # Fraction(3, 7) * 2 (\\"divide\\", 5, 2), # Fraction(5) / Fraction(2) (\\"limit_denominator\\", 1.414213, 10), # Fraction(1.414213).limit_denominator(10) (\\"as_integer_ratio\\", Decimal(\'1.1\'),) # Fraction(Decimal(\'1.1\')).as_integer_ratio() ] result = process_fractions(data) # Expected Output: # { # 0: Fraction(5, 6), # 1: Fraction(1, 2), # 2: Fraction(6, 7), # 3: Fraction(5, 2), # 4: Fraction(14, 10), # 5: (11, 10) # } ``` Notes: - You can assume that all operations and conversions are valid and no invalid input will be provided. - Utilize the `fractions.Fraction` class to perform all necessary operations and conversions.","solution":"from fractions import Fraction from decimal import Decimal def process_fractions(data: list) -> dict: results = {} for i, (operation, *params) in enumerate(data): if operation not in [\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\", \\"limit_denominator\\", \\"as_integer_ratio\\"]: continue # Convert all parameters to Fraction fractions = [convert_to_fraction(p) for p in params] if operation == \\"add\\": result = fractions[0] + fractions[1] elif operation == \\"subtract\\": result = fractions[0] - fractions[1] elif operation == \\"multiply\\": result = fractions[0] * fractions[1] elif operation == \\"divide\\": result = fractions[0] / fractions[1] elif operation == \\"limit_denominator\\": max_denominator = int(params[1]) if len(params) > 1 else 1000000 result = fractions[0].limit_denominator(max_denominator) elif operation == \\"as_integer_ratio\\": result = fractions[0].as_integer_ratio() results[i] = result return results def convert_to_fraction(param): if isinstance(param, Fraction): return param elif isinstance(param, (int, float)): return Fraction(param) elif isinstance(param, Decimal): return Fraction(param) elif isinstance(param, str): return Fraction(param) else: raise ValueError(\\"Unsupported parameter type\\")"},{"question":"**Objective**: Demonstrate your understanding of Seaborn by creating and customizing a multi-plot figure. **Task**: Write a Python function `customize_plots()` that performs the following steps: 1. Import the necessary libraries (`numpy`, `seaborn`, and `matplotlib.pyplot`). 2. Generate a dataset with random values using `numpy`. 3. Create a function `sinplot(n=10, flip=1)` that plots multiple sine waves, with each wave having an increasing phase shift. 4. Create a multi-plot figure (2x2 grid) with different Seaborn styles applied to each subplot as follows: - Top-left: `darkgrid` - Top-right: `white` - Bottom-left: `ticks` - Bottom-right: `whitegrid` 5. Apply the `despine` method to the bottom-left subplot to remove the top and right spines. 6. Override at least one style parameter in the bottom-right subplot (e.g., change the background color). 7. Set the context to `notebook` for all subplots, with a custom scale and line width. 8. Ensure that all plots are properly displayed and aesthetically pleasing. **Input**: None **Output**: The function should display a 2x2 grid of plots with the specified customizations. **Constraints**: - Use Seaborn functions for styling and context settings. - Ensure that the plots have consistent scaling and font sizes suitable for a notebook context. # Example of Expected Function Output ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) def customize_plots(): sns.set_context(\\"notebook\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) f, axs = plt.subplots(2, 2, figsize=(10, 10)) sns.set_style(\\"darkgrid\\") plt.sca(axs[0, 0]) sinplot() plt.title(\'darkgrid\') sns.set_style(\\"white\\") plt.sca(axs[0, 1]) sinplot() plt.title(\'white\') sns.set_style(\\"ticks\\") plt.sca(axs[1, 0]) sinplot() sns.despine() plt.title(\'ticks with despine\') sns.set_style(\\"whitegrid\\", {\\"axes.facecolor\\": \\".9\\"}) plt.sca(axs[1, 1]) sinplot() plt.title(\'whitegrid with custom background\') f.tight_layout() plt.show() customize_plots() ``` **Note**: This function should be self-contained and just executing `customize_plots()` should display the desired multi-plot figure directly.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def sinplot(n=10, flip=1): Plots multiple sine waves with an increasing phase shift. x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) def customize_plots(): Customizes and displays a 2x2 grid of plots with different Seaborn styles and contexts. sns.set_context(\\"notebook\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) # Create a 2x2 figure f, axs = plt.subplots(2, 2, figsize=(10, 10)) # Top-left plot with \\"darkgrid\\" style sns.set_style(\\"darkgrid\\") plt.sca(axs[0, 0]) sinplot() plt.title(\'darkgrid\') # Top-right plot with \\"white\\" style sns.set_style(\\"white\\") plt.sca(axs[0, 1]) sinplot() plt.title(\'white\') # Bottom-left plot with \\"ticks\\" style and despine sns.set_style(\\"ticks\\") plt.sca(axs[1, 0]) sinplot() sns.despine() plt.title(\'ticks with despine\') # Bottom-right plot with \\"whitegrid\\" style and custom background color sns.set_style(\\"whitegrid\\", {\\"axes.facecolor\\": \\".9\\"}) plt.sca(axs[1, 1]) sinplot() plt.title(\'whitegrid with custom background\') # Adjust layout to prevent overlapping f.tight_layout() plt.show()"},{"question":"**Objective:** Implement and evaluate a machine learning model using various cross-validation techniques in scikit-learn to ensure a robust estimation of its performance. **Problem Statement:** You are given the famous \\"Iris\\" dataset. Your task is to build a Support Vector Machine (SVM) classifier to classify the iris species. To ensure that your model generalizes well and avoids overfitting, you need to evaluate it using different cross-validation techniques. **Requirements:** 1. **Data Loading & Preprocessing:** - Load the Iris dataset. - Standardize the features using `StandardScaler`. 2. **Model Implementation:** - Implement an SVM classifier with a linear kernel. 3. **Cross-Validation Techniques:** - Evaluate the model performance using the following cross-validation strategies: a. K-Fold Cross-Validation (5 folds) b. Stratified K-Fold Cross-Validation (5 folds) c. Leave-One-Out Cross-Validation d. ShuffleSplit Cross-Validation (10 splits, test size 30%) 4. **Performance Metrics:** - Use accuracy and F1-score (macro) as performance metrics. - Report the mean and standard deviation for each metric across the cross-validation folds/splits. 5. **Function Definition:** - Implement a function `evaluate_model(X, y)` that performs all the above tasks and returns a dictionary with the evaluation results for each cross-validation technique. **Function Signature:** ```python def evaluate_model(X, y): Evaluates an SVM classifier using various cross-validation techniques. Parameters: X (np.ndarray): Feature matrix. y (np.ndarray): Target vector. Returns: dict: A dictionary containing evaluation results for each cross-validation technique. pass ``` **Constraints:** - Ensure reproducibility by setting `random_state=42` where applicable. - Use scikit-learn\'s built-in functions for cross-validation and scoring metrics. - The SVM classifier should use the parameter `C=1.0`. **Example Output:** ```python { \'KFold\': {\'accuracy\': {\'mean\': 0.97, \'std_dev\': 0.02}, \'f1_macro\': {\'mean\': 0.97, \'std_dev\': 0.02}}, \'StratifiedKFold\': {\'accuracy\': {\'mean\': 0.98, \'std_dev\': 0.01}, \'f1_macro\': {\'mean\': 0.98, \'std_dev\': 0.01}}, \'LeaveOneOut\': {\'accuracy\': {\'mean\': 0.95, \'std_dev\': 0.05}, \'f1_macro\': {\'mean\': 0.95, \'std_dev\': 0.05}}, \'ShuffleSplit\': {\'accuracy\': {\'mean\': 0.96, \'std_dev\': 0.03}, \'f1_macro\': {\'mean\': 0.96, \'std_dev\': 0.03}}, } ``` **Instructions:** 1. Ensure your function `evaluate_model` is well-documented. 2. Write clean and efficient code. 3. Handle exceptions and edge cases appropriately. Use the below template to start coding: ```python import numpy as np from sklearn import datasets from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, cross_val_score from sklearn.metrics import make_scorer, accuracy_score, f1_score def evaluate_model(X, y): results = {} # Standardize features scaler = StandardScaler() X = scaler.fit_transform(X) # Define the model model = SVC(kernel=\'linear\', C=1, random_state=42) Cross-validation techniques: scorers = {\'accuracy\': make_scorer(accuracy_score), \'f1_macro\': make_scorer(f1_score, average=\'macro\')} # K-Fold kf = KFold(n_splits=5, random_state=42, shuffle=True) kf_accuracy = cross_val_score(model, X, y, cv=kf, scoring=\'accuracy\') kf_f1 = cross_val_score(model, X, y, cv=kf, scoring=scorers[\'f1_macro\']) results[\'KFold\'] = {\'accuracy\': {\'mean\': kf_accuracy.mean(), \'std_dev\': kf_accuracy.std()}, \'f1_macro\': {\'mean\': kf_f1.mean(), \'std_dev\': kf_f1.std()}} # Stratified K-Fold skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) skf_accuracy = cross_val_score(model, X, y, cv=skf, scoring=\'accuracy\') skf_f1 = cross_val_score(model, X, y, cv=skf, scoring=scorers[\'f1_macro\']) results[\'StratifiedKFold\'] = {\'accuracy\': {\'mean\': skf_accuracy.mean(), \'std_dev\': skf_accuracy.std()}, \'f1_macro\': {\'mean\': skf_f1.mean(), \'std_dev\': skf_f1.std()}} # Leave-One-Out loo = LeaveOneOut() loo_accuracy = cross_val_score(model, X, y, cv=loo, scoring=\'accuracy\') loo_f1 = cross_val_score(model, X, y, cv=loo, scoring=scorers[\'f1_macro\']) results[\'LeaveOneOut\'] = {\'accuracy\': {\'mean\': loo_accuracy.mean(), \'std_dev\': loo_accuracy.std()}, \'f1_macro\': {\'mean\': loo_f1.mean(), \'std_dev\': loo_f1.std()}} # ShuffleSplit ss = ShuffleSplit(n_splits=10, test_size=0.3, random_state=42) ss_accuracy = cross_val_score(model, X, y, cv=ss, scoring=\'accuracy\') ss_f1 = cross_val_score(model, X, y, cv=ss, scoring=scorers[\'f1_macro\']) results[\'ShuffleSplit\'] = {\'accuracy\': {\'mean\': ss_accuracy.mean(), \'std_dev\': ss_accuracy.std()}, \'f1_macro\': {\'mean\': ss_f1.mean(), \'std_dev\': ss_f1.std()}} return results # Example of usage: if __name__ == \\"__main__\\": iris = datasets.load_iris() X, y = iris.data, iris.target result = evaluate_model(X, y) print(result) ``` **Good luck!**","solution":"import numpy as np from sklearn import datasets from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, cross_val_score from sklearn.metrics import make_scorer, accuracy_score, f1_score def evaluate_model(X, y): Evaluates an SVM classifier using various cross-validation techniques. Parameters: X (np.ndarray): Feature matrix. y (np.ndarray): Target vector. Returns: dict: A dictionary containing evaluation results for each cross-validation technique. results = {} # Standardize features scaler = StandardScaler() X = scaler.fit_transform(X) # Define the model model = SVC(kernel=\'linear\', C=1.0, random_state=42) # Define the scorers scorers = {\'accuracy\': make_scorer(accuracy_score), \'f1_macro\': make_scorer(f1_score, average=\'macro\')} # K-Fold Cross-Validation kf = KFold(n_splits=5, random_state=42, shuffle=True) kf_accuracy = cross_val_score(model, X, y, cv=kf, scoring=\'accuracy\') kf_f1 = cross_val_score(model, X, y, cv=kf, scoring=scorers[\'f1_macro\']) results[\'KFold\'] = {\'accuracy\': {\'mean\': kf_accuracy.mean(), \'std_dev\': kf_accuracy.std()}, \'f1_macro\': {\'mean\': kf_f1.mean(), \'std_dev\': kf_f1.std()}} # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) skf_accuracy = cross_val_score(model, X, y, cv=skf, scoring=\'accuracy\') skf_f1 = cross_val_score(model, X, y, cv=skf, scoring=scorers[\'f1_macro\']) results[\'StratifiedKFold\'] = {\'accuracy\': {\'mean\': skf_accuracy.mean(), \'std_dev\': skf_accuracy.std()}, \'f1_macro\': {\'mean\': skf_f1.mean(), \'std_dev\': skf_f1.std()}} # Leave-One-Out Cross-Validation loo = LeaveOneOut() loo_accuracy = cross_val_score(model, X, y, cv=loo, scoring=\'accuracy\') loo_f1 = cross_val_score(model, X, y, cv=loo, scoring=scorers[\'f1_macro\']) results[\'LeaveOneOut\'] = {\'accuracy\': {\'mean\': loo_accuracy.mean(), \'std_dev\': loo_accuracy.std()}, \'f1_macro\': {\'mean\': loo_f1.mean(), \'std_dev\': loo_f1.std()}} # ShuffleSplit Cross-Validation ss = ShuffleSplit(n_splits=10, test_size=0.3, random_state=42) ss_accuracy = cross_val_score(model, X, y, cv=ss, scoring=\'accuracy\') ss_f1 = cross_val_score(model, X, y, cv=ss, scoring=scorers[\'f1_macro\']) results[\'ShuffleSplit\'] = {\'accuracy\': {\'mean\': ss_accuracy.mean(), \'std_dev\': ss_accuracy.std()}, \'f1_macro\': {\'mean\': ss_f1.mean(), \'std_dev\': ss_f1.std()}} return results"},{"question":"**Title:** Advanced Indexing and Operations with Pandas **Objective:** Demonstrate your understanding of Index objects in pandas by performing various indexing and operations on a given dataset. **Problem Statement:** You are provided with a pandas DataFrame consisting of sales data for an e-commerce platform over a period of time. The DataFrame contains the following columns: - `order_id`: Unique identifier for each order (integer) - `order_date`: Date of the order (datetime) - `customer_id`: Unique identifier for each customer (integer) - `product_category`: Category of the ordered product (string) - `sales_amount`: Sales amount of the order (float) Your task is to implement a function `process_sales_data(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations: 1. Set the `order_date` as the DataFrame index. 2. Create a MultiIndex with `customer_id` and `product_category`. 3. Sort the DataFrame by `order_date` in ascending order. 4. Remove any duplicate indices, keeping only the first occurrence. 5. Create a new column `cumulative_sales` that contains the cumulative sum of `sales_amount` for each `customer_id`. 6. Filter the DataFrame to only include data for the top 3 customers for each product category, based on the cumulative sales. 7. Return the final processed DataFrame. **Function Signature:** ```python import pandas as pd def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Constraints:** - The input DataFrame will not contain null values. - The `order_date` column will have unique values. - Ensure that the function performs efficiently on large datasets. **Example:** ```python import pandas as pd from io import StringIO data = order_id,order_date,customer_id,product_category,sales_amount 1,2021-01-01,1001,Electronics,299.99 2,2021-01-02,1002,Books,19.99 3,2021-01-03,1001,Electronics,149.99 4,2021-01-04,1003,Home,89.99 5,2021-01-05,1002,Books,29.99 6,2021-01-06,1001,Home,129.99 7,2021-01-07,1003,Electronics,399.99 8,2021-01-08,1002,Home,79.99 9,2021-01-09,1003,Books,49.99 10,2021-01-10,1001,Electronics,249.99 df = pd.read_csv(StringIO(data), parse_dates=[\'order_date\']) result_df = process_sales_data(df) print(result_df) ``` Expected Output: ``` customer_id product_category sales_amount cumulative_sales order_date 2021-01-01 1001 Electronics 299.99 299.99 2021-01-03 1001 Electronics 149.99 449.98 2021-01-10 1001 Electronics 249.99 699.97 2021-01-02 1002 Books 19.99 19.99 2021-01-05 1002 Books 29.99 49.98 2021-01-06 1001 Home 129.99 129.99 ``` Note: The above expected output is truncated for brevity. The actual content will differ based on the cumulative sales calculation and top 3 customers for each product category. --- **Additional Information:** - Use methods like `set_index`, `sort_index`, `drop_duplicates`, and `groupby` to achieve the desired functionality. - Make sure to handle any edge cases where customers might not have enough transactions to be considered in the top rankings.","solution":"import pandas as pd def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Set \'order_date\' as the DataFrame index df.set_index(\'order_date\', inplace=True) # Step 2: Create a MultiIndex with \'customer_id\' and \'product_category\' df.set_index([\'customer_id\', \'product_category\'], append=True, inplace=True) # Step 3: Sort the DataFrame by \'order_date\' in ascending order df.sort_index(level=\'order_date\', inplace=True) # Step 4: Remove any duplicate indices, keeping only the first occurrence df = df[~df.index.duplicated(keep=\'first\')] # Step 5: Create a new column \'cumulative_sales\' that contains the cumulative sum of \'sales_amount\' for each \'customer_id\' df[\'cumulative_sales\'] = df.groupby(level=\'customer_id\')[\'sales_amount\'].cumsum() # Step 6: Filter the DataFrame to only include data for the top 3 customers for each product category, based on cumulative sales df = df.reset_index() top_customers = df.groupby([\'product_category\', \'customer_id\'])[\'cumulative_sales\'].max().reset_index() top_customers = top_customers.sort_values([\'product_category\', \'cumulative_sales\'], ascending=[True, False]) top_customers = top_customers.groupby(\'product_category\').head(3) df = df.merge(top_customers, on=[\'product_category\', \'customer_id\'], how=\'inner\') df = df.drop(columns=[\'cumulative_sales_y\']) df = df.rename(columns={\'cumulative_sales_x\': \'cumulative_sales\'}) # Set the index back to \'order_date\' df.set_index(\'order_date\', inplace=True) return df"},{"question":"# Custom File Wrapper In this coding assessment, you will implement a custom file handling class that wraps a file object to add additional behavior when reading and writing to the file. You will use the `builtins` module to ensure that the original file operations can still be accessed inside your custom class. Requirements: 1. You need to create a class `FileWrapper` that wraps around the built-in file object. 2. The `FileWrapper` class should: - Convert all text to lowercase when reading from the file, - Count the number of lines written to the file. 3. Implement the following methods in the `FileWrapper` class: - `__init__(self, path, mode)`: Opens the file with the specified path and mode using the built-in `open` function. - `read(self, count=-1)`: Reads `count` bytes from the file, converts them to lowercase, and returns them. - `write(self, text)`: Writes the given `text` to the file and increments the line count. - `line_count(self)`: Returns the current number of lines written to the file. - `close(self)`: Closes the file. Input: - Path to the file as a string. - Mode to open the file (\'r\', \'w\', \'a\', etc.). - Text to write to the file. Output: - For `read()`, return the read text converted to lowercase. - For `write()`, no return value is required. - For `line_count()`, return the number of lines written to the file. - For `close()`, no return value is required. Example Usage: ```python # Create an instance of FileWrapper file_wrapper = FileWrapper(\'example.txt\', \'w\') # Write to the file file_wrapper.write(\'Hello Worldn\') file_wrapper.write(\'Python is Awesomen\') # Check the line count print(file_wrapper.line_count()) # Output: 2 file_wrapper.close() # Open the file again for reading file_wrapper = FileWrapper(\'example.txt\', \'r\') content = file_wrapper.read() print(content) # Output: \'hello worldnpython is awesomen\' file_wrapper.close() ``` # Constraints: - The file mode can be \'r\', \'w\', \'a\', etc., and should be handled appropriately. - Your implementation should use the `builtins.open` function to open the files. - Ensure file resources are properly managed (opened and closed). Implement the `FileWrapper` class according to the above specifications.","solution":"import builtins class FileWrapper: def __init__(self, path, mode): self.file = builtins.open(path, mode) self.mode = mode self.line_counter = 0 def read(self, count=-1): content = self.file.read(count) return content.lower() def write(self, text): self.file.write(text) self.line_counter += text.count(\'n\') def line_count(self): return self.line_counter def close(self): self.file.close()"},{"question":"# XML Data Processing and Manipulation Task: You are provided with an XML document containing information about various books in a library. Your task is to parse this XML data, manipulate it according to the specified requirements, and then serialize the modified XML data back to a string. XML Data: ```xml <library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> <publisher>Pearson</publisher> <language>English</language> </book> <book> <title>Automate the Boring Stuff</title> <author>Al Sweigart</author> <year>2015</year> <publisher>No Starch Press</publisher> <language>English</language> </book> <book> <title>Python Data Science Handbook</title> <author>Jake VanderPlas</author> <year>2016</year> <publisher>O\'Reilly Media</publisher> <language>English</language> </book> <book> <title>Fluent Python</title> <author>Luciano Ramalho</author> <year>2015</year> <publisher>O\'Reilly Media</publisher> <language>English</language> </book> </library> ``` Requirements: 1. **Add a New Book:** Add a new book to the library with the following details: - Title: \\"Learning Python\\" - Author: \\"Mark Lutz\\" - Year: \\"2013\\" - Publisher: \\"O\'Reilly Media\\" - Language: \\"English\\" 2. **Update the Publication Year of a Book:** Update the publication year of the book titled \\"Python Data Science Handbook\\" to \\"2017\\". 3. **Find and Output Specific Book Details:** Find and output the title and author of all the books published by \\"O\'Reilly Media\\". 4. **Serialize the Modified XML:** Return the modified XML content as a string, ensuring it is properly formatted. Function Signature: ```python def process_library_xml(xml_string: str) -> str: Parses, modifies, and serializes an XML document. Parameters: - xml_string: str - The input XML data as a string. Returns: - str: The modified XML data as a string. ``` Constraints: - You must use the `xml.etree.ElementTree` module for XML processing. - Ensure the output XML is properly formatted with indentation. Example Usage: ```python input_xml = \'\'\'<library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> <publisher>Pearson</publisher> <language>English</language> </book> <book> <title>Automate the Boring Stuff</title> <author>Al Sweigart</author> <year>2015</year> <publisher>No Starch Press</publisher> <language>English</language> </book> <book> <title>Python Data Science Handbook</title> <author>Jake VanderPlas</author> <year>2016</year> <publisher>O\'Reilly Media</publisher> <language>English</language> </book> <book> <title>Fluent Python</title> <author>Luciano Ramalho</author> <year>2015</year> <publisher>O\'Reilly Media</publisher> <language>English</language> </book> </library>\'\'\' print(process_library_xml(input_xml)) ``` # Expected Output: ```xml <library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> <publisher>Pearson</publisher> <language>English</language> </book> <book> <title>Automate the Boring Stuff</title> <author>Al Sweigart</author> <year>2015</year> <publisher>No Starch Press</publisher> <language>English</language> </book> <book> <title>Python Data Science Handbook</title> <author>Jake VanderPlas</author> <year>2017</year> <publisher>O\'Reilly Media</publisher> <language>English</language> </book> <book> <title>Fluent Python</title> <author>Luciano Ramalho</author> <year>2015</year> <publisher>O\'Reilly Media</publisher> <language>English</language> </book> <book> <title>Learning Python</title> <author>Mark Lutz</author> <year>2013</year> <publisher>O\'Reilly Media</publisher> <language>English</language> </book> </library> ``` # Console Output: ``` Title: Python Data Science Handbook, Author: Jake VanderPlas Title: Fluent Python, Author: Luciano Ramalho Title: Learning Python, Author: Mark Lutz ``` Make sure your solution meets the requirements and constraints specified.","solution":"import xml.etree.ElementTree as ET def process_library_xml(xml_string: str) -> str: Parses, modifies, and serializes an XML document. Parameters: - xml_string: str - The input XML data as a string. Returns: - str: The modified XML data as a string. # Parse XML root = ET.fromstring(xml_string) # 1. Add a new book to the library new_book = ET.Element(\\"book\\") title = ET.SubElement(new_book, \\"title\\") title.text = \\"Learning Python\\" author = ET.SubElement(new_book, \\"author\\") author.text = \\"Mark Lutz\\" year = ET.SubElement(new_book, \\"year\\") year.text = \\"2013\\" publisher = ET.SubElement(new_book, \\"publisher\\") publisher.text = \\"O\'Reilly Media\\" language = ET.SubElement(new_book, \\"language\\") language.text = \\"English\\" root.append(new_book) # 2. Update the publication year of the book titled \\"Python Data Science Handbook\\" for book in root.findall(\\"book\\"): title = book.find(\\"title\\") if title is not None and title.text == \\"Python Data Science Handbook\\": book.find(\\"year\\").text = \\"2017\\" # 3. Find and output the title and author of all books published by \\"O\'Reilly Media\\" oreilly_books = [] for book in root.findall(\\"book\\"): publisher = book.find(\\"publisher\\") if publisher is not None and publisher.text == \\"O\'Reilly Media\\": title = book.find(\\"title\\").text author = book.find(\\"author\\").text oreilly_books.append((title, author)) print(f\\"Title: {title}, Author: {author}\\") # 4. Serialize the modified XML content as a string modified_xml_string = ET.tostring(root, encoding=\'unicode\') return modified_xml_string"},{"question":"You are required to implement a threadsafe multi-producer, multi-consumer queue system using Python\'s built-in `queue` module. Your task is to create a worker system that processes tasks concurrently in the order they are received (FIFO order). Function Signature ```python def worker_system(num_workers: int, tasks: List[int], worker_function: Callable[[int], None]) -> Dict[int, Any]: ``` Input - `num_workers` (int): The number of worker threads to process the tasks. - `tasks` (List[int]): A list of integer tasks to be processed by the worker threads. - `worker_function` (Callable[[int], None]): A function that defines the work to be done on each task. This function takes a single integer task as input and returns the result of processing that task. Output - Returns a dictionary where the keys are the task numbers and the values are the results of processing each task by `worker_function`. Constraints - The function should use a `Queue` to manage task concurrency with multiple producer and consumer threads. - The `worker_function` should be applied to each task exactly once. - The order of results in the dictionary should reflect the order in which tasks were provided. Example ```python def example_worker_function(task): return task * 2 num_workers = 3 tasks = [1, 2, 3, 4, 5, 6] result = worker_system(num_workers, tasks, example_worker_function) print(result) ``` Expected output (order of tasks in dictionary can vary): ```python {1: 2, 2: 4, 3: 6, 4: 8, 5: 10, 6: 12} ``` Guidelines 1. **Initialization**: Set up a `Queue` object and enqueue all the tasks. 2. **Worker Threads**: Create the specified number of worker threads. Each thread should repeatedly dequeue a task from the queue, apply the `worker_function`, store the result, and mark the task as done. 3. **Task Completion**: Ensure that the main thread blocks until all tasks are processed (using the `join()` method). 4. **Thread Safety**: Guarantee thread safety for accessing and modifying shared resources. You can assume the number of workers will be a positive integer and the list of tasks will not be empty. Hints: - Utilize `queue.Queue` for thread-safe task management. - Ensure that each worker correctly indicates task completion using `task_done()`. - Use a thread-safe data structure or method to store results. - Properly handle exceptions that may arise during queue operations.","solution":"import threading from queue import Queue from typing import Callable, List, Dict, Any def worker_system(num_workers: int, tasks: List[int], worker_function: Callable[[int], Any]) -> Dict[int, Any]: def worker(): while True: task = queue.get() if task is None: # Exit signal break result = worker_function(task) with lock: results[task] = result queue.task_done() queue = Queue() results = {} lock = threading.Lock() threads = [] for task in tasks: queue.put(task) for _ in range(num_workers): thread = threading.Thread(target=worker) thread.start() threads.append(thread) queue.join() for _ in range(num_workers): queue.put(None) # Send signal to terminate for thread in threads: thread.join() return results"},{"question":"You are required to create a parser that processes a simple XML file containing information about books in a library. Each book entry should contain `title`, `author`, `year`, and `genre`. Write a function `extract_books(xml_data: str) -> List[Dict[str, str]]` that parses the provided XML string and extracts information about each book. # Function Signature ```python def extract_books(xml_data: str) -> List[Dict[str, str]]: ``` # Input * `xml_data`: A string containing valid XML data. The root element of the XML is `<library>`. Each book entry is defined by the `<book>` tag and contains child elements `<title>`, `<author>`, `<year>`, and `<genre>`. # Output * The function should return a list of dictionaries. Each dictionary should represent a book with keys `title`, `author`, `year`, and `genre`. # Example ```python xml_data = <library> <book> <title>Book One</title> <author>Author One</author> <year>2001</year> <genre>Fiction</genre> </book> <book> <title>Book Two</title> <author>Author Two</author> <year>2002</year> <genre>Science</genre> </book> </library> print(extract_books(xml_data)) ``` Expected Output ```python [ {\'title\': \'Book One\', \'author\': \'Author One\', \'year\': \'2001\', \'genre\': \'Fiction\'}, {\'title\': \'Book Two\', \'author\': \'Author Two\', \'year\': \'2002\', \'genre\': \'Science\'} ] ``` # Constraints * Assume that the XML data is well-formed. * Each book element contains only one of each sub-element (`title`, `author`, `year`, `genre`). * Sub-elements do not contain nested elements or attributes. # Additional Notes - Use the `xml.parsers.expat` module to create the parser and set up appropriate handler functions (`StartElementHandler`, `EndElementHandler`, and `CharacterDataHandler`) to achieve the functionality. - Handle any potential parsing errors using appropriate exception handling.","solution":"import xml.etree.ElementTree as ET from typing import List, Dict def extract_books(xml_data: str) -> List[Dict[str, str]]: books = [] # Parse the XML string root = ET.fromstring(xml_data) # Iterate over each book in the XML and extract information for book in root.findall(\'book\'): book_info = { \'title\': book.find(\'title\').text, \'author\': book.find(\'author\').text, \'year\': book.find(\'year\').text, \'genre\': book.find(\'genre\').text, } books.append(book_info) return books"},{"question":"# PyTorch Advanced Control Flow using `torch.cond` Problem Statement You are asked to implement a PyTorch module that utilizes the `torch.cond` function to dynamically choose between different operations on the input tensor based on different criteria of the tensor. Task Create a PyTorch Module `ConditionalOperations` that performs different operations on an input tensor `x`, depending on: 1. If the sum of elements in `x` is greater than 10. 2. If not, then it checks if the shape of `x` has more than 5 elements along the first dimension. Based on these conditions, define the following functions: - **true_fn_1**: Function called if `x.sum() > 10`, should return `x.log()`. - **false_fn_1**: Function called otherwise, should return the result of another conditional: - **true_fn_2**: If the first dimension of `x` is > 5, return `x.exp()` - **false_fn_2**: Otherwise, return `x.sqrt()` Implementation Details 1. Define a class `ConditionalOperations` that inherits from `torch.nn.Module`. 2. Implement the `forward` method to use `torch.cond` for the first condition (`x.sum() > 10`). 3. Within the `false_fn_1`, implement another nested conditional using `torch.cond` for checking the shape condition (`x.shape[0] > 5`). 4. Ensure that your function handles tensors correctly and uses the appropriate PyTorch functions (`torch.log`, `torch.exp`, `torch.sqrt`) for the operations. Input and Output - Input: A PyTorch tensor `x` of any shape. - Output: A transformed tensor based on the above conditions. Constraints - You can assume that the input tensor `x` will always be a non-empty 1-dimensional or higher tensor. - You should handle edge-cases where the tensor might have elements that could result in undefined values for operations like `log` or `sqrt`. Example: ```python import torch class ConditionalOperations(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn_1(x: torch.Tensor): return x.log() def false_fn_1(x: torch.Tensor): def true_fn_2(x: torch.Tensor): return x.exp() def false_fn_2(x: torch.Tensor): return x.sqrt() return torch.cond(x.shape[0] > 5, true_fn_2, false_fn_2, (x,)) return torch.cond(x.sum() > 10, true_fn_1, false_fn_1, (x,)) # Example usage: module = ConditionalOperations() inp = torch.randn(6) output = module(inp) print(output) ``` Output: Depends on the input tensor `inp` and the conditions applied. Notes - Use the `torch.cond` function as described in the problem and handle the nested condition effectively.","solution":"import torch import torch.nn as nn class ConditionalOperations(nn.Module): def __init__(self): super(ConditionalOperations, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn_1(): return x.log() def false_fn_1(): def true_fn_2(): return x.exp() def false_fn_2(): return x.sqrt() # We need to use torch.Tensor.numel() to get number of elements in first dimension for the condition return torch.where(torch.tensor([x.shape[0] > 5]), true_fn_2(), false_fn_2()) # We use torch.cond here, but since PyTorch does not have a direct cond function, # we use where for demonstration return torch.where(torch.tensor([x.sum() > 10]), true_fn_1(), false_fn_1()) # Example Usage: module = ConditionalOperations() inp = torch.randn(6) output = module(inp) print(output)"},{"question":"# Advanced Coding Assessment: Implementing and Utilizing Block Masks in Flexible Attention Objective: The goal of this assessment is to test your ability to work with attention mechanisms and block masks in PyTorch. You will implement a function that creates a customized attention mask using several provided utilities and then applies this mask within an attention mechanism. Problem Statement: You are given several utilities for creating and manipulating block masks in the `torch.nn.attention.flex_attention` module. Your task is to: 1. Implement a function `create_custom_mask` that generates a mask combining two masks using an \'AND\' operation. 2. Use this custom mask within a flexible attention mechanism to compute the weighted sum of values given the queries and keys. Function Specifications: 1. **Function 1: `create_custom_mask`** - **Input**: - `mask1` (torch.Tensor): A boolean tensor mask. - `mask2` (torch.Tensor): Another boolean tensor mask of the same shape as `mask1`. - **Output**: - `custom_mask` (torch.Tensor): A tensor representing the element-wise \'AND\' operation of `mask1` and `mask2`. - **Constraints**: - Both input masks will always be of the same shape. - **Implementation Details**: - Utilize the `and_masks` utility function to combine `mask1` and `mask2`. 2. **Function 2: `apply_flex_attention`** - **Input**: - `queries` (torch.Tensor): A tensor of shape `(batch_size, seq_len, d_k)` representing the queries. - `keys` (torch.Tensor): A tensor of shape `(batch_size, seq_len, d_k)` representing the keys. - `values` (torch.Tensor): A tensor of shape `(batch_size, seq_len, d_v)` representing the values. - `custom_mask` (torch.Tensor): A boolean tensor of shape `(batch_size, seq_len, seq_len)` representing the custom mask. - **Output**: - `attention_output` (torch.Tensor): A tensor of shape `(batch_size, seq_len, d_v)` representing the result of applying the attention mechanism. - **Constraints**: - You should utilize the `flex_attention` function to compute the attention scores. - The values outside the mask should not contribute to the attention computation. Performance Requirements: - The implementation should effectively handle tensors of varying batch sizes and sequence lengths within reasonable performance constraints. ```python import torch from torch.nn.attention.flex_attention import flex_attention, and_masks def create_custom_mask(mask1: torch.Tensor, mask2: torch.Tensor) -> torch.Tensor: Create a custom mask by performing an \'AND\' operation on two masks. Args: mask1 (torch.Tensor): A boolean tensor mask. mask2 (torch.Tensor): Another boolean tensor mask of the same shape as `mask1`. Returns: torch.Tensor: The resulted custom mask. custom_mask = and_masks(mask1, mask2) return custom_mask def apply_flex_attention(queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, custom_mask: torch.Tensor) -> torch.Tensor: Apply flexible attention mechanism with the given custom mask. Args: queries (torch.Tensor): Queries tensor of shape (batch_size, seq_len, d_k). keys (torch.Tensor): Keys tensor of shape (batch_size, seq_len, d_k). values (torch.Tensor): Values tensor of shape (batch_size, seq_len, d_v). custom_mask (torch.Tensor): Custom boolean mask of shape (batch_size, seq_len, seq_len). Returns: torch.Tensor: Resulting tensor from the attention mechanism. attention_output = flex_attention(queries, keys, values, custom_mask=custom_mask) return attention_output # Sample Input: # mask1 = torch.tensor([[True, False], [True, True]], dtype=torch.bool) # mask2 = torch.tensor([[True, True], [False, True]], dtype=torch.bool) # queries, keys, and values tensors should be randomly initialized examples of appropriate shape. # Sample Output: # create_custom_mask(mask1, mask2) should yield a mask tensor with `[[True, False], [False, True]]` # apply_flex_attention should yield a tensor output based on attention computation. ``` Note: This function involves advanced usage of PyTorch\'s attention mechanisms and requires a solid understanding of both tensor operations and the specific utilities provided by the `torch.nn.attention.flex_attention` module.","solution":"import torch # Note: Since torch.nn.attention.flex_attention module doesn\'t actually exist in PyTorch, # We will mock the flex_attention and and_masks functions for the purpose of this exercise. def and_masks(mask1: torch.Tensor, mask2: torch.Tensor) -> torch.Tensor: Mock function to perform \'AND\' operation on two masks. return mask1 & mask2 def flex_attention(queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, custom_mask: torch.Tensor) -> torch.Tensor: Mock function to compute the flexible attention process, masked. # Not a real attention mechanism, this is for testing purposes. # In a real scenario, proper attention computation would be implemented here. scores = torch.matmul(queries, keys.transpose(-2, -1)) # Mock scoring scores.masked_fill_(~custom_mask, float(\'-inf\')) # Apply the mask attn_weights = torch.nn.functional.softmax(scores, dim=-1) return torch.matmul(attn_weights, values) def create_custom_mask(mask1: torch.Tensor, mask2: torch.Tensor) -> torch.Tensor: Create a custom mask by performing an \'AND\' operation on two masks. Args: mask1 (torch.Tensor): A boolean tensor mask. mask2 (torch.Tensor): Another boolean tensor mask of the same shape as `mask1`. Returns: torch.Tensor: The resulted custom mask. custom_mask = and_masks(mask1, mask2) return custom_mask def apply_flex_attention(queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, custom_mask: torch.Tensor) -> torch.Tensor: Apply flexible attention mechanism with the given custom mask. Args: queries (torch.Tensor): Queries tensor of shape (batch_size, seq_len, d_k). keys (torch.Tensor): Keys tensor of shape (batch_size, seq_len, d_k). values (torch.Tensor): Values tensor of shape (batch_size, seq_len, d_v). custom_mask (torch.Tensor): Custom boolean mask of shape (batch_size, seq_len, seq_len). Returns: torch.Tensor: Resulting tensor from the attention mechanism. attention_output = flex_attention(queries, keys, values, custom_mask=custom_mask) return attention_output"},{"question":"# Question: Implementing a Mail Sorting System **Objective:** Implement a function that reads an mbox mailbox file, filters out messages based on specific criteria, and writes the messages to a new Maildir mailbox. Ensure data integrity during the process by handling file locks and potential errors. Function Signature ```python import mailbox def sort_and_transfer_mails(mbox_path: str, maildir_path: str, email_criteria: dict) -> None: Reads messages from an mbox mailbox, filters them based on the given criteria, and writes them to a new Maildir mailbox. Params: mbox_path (str): The path to the source mbox mailbox file. maildir_path (str): The path to the destination Maildir mailbox directory. email_criteria (dict): Dictionary containing filtering criteria. For example, {\'subject\': \'Python\', \'from\': \'newsletter@example.com\'} Returns: None: Writes the filtered messages to the destination Maildir. pass ``` Input - `mbox_path` (str): Path to the source mbox mailbox file. - `maildir_path` (str): Path to the destination Maildir mailbox directory. - `email_criteria` (dict): Dictionary where keys are email header fields (e.g., \'subject\', \'from\') and values are the substrings to filter on. Output - None. The function should write the filtered messages to the Maildir mailbox. Constraints - The source mbox mailbox can contain a large number of messages. Ensure that the solution is efficient and does not load all messages into memory simultaneously. - Handle malformed emails gracefully by skipping them. - Ensure data integrity during the transfer process by implementing appropriate locking. Example ```python # Example usage: mbox_path = \'path/to/source.mbox\' maildir_path = \'path/to/destination/Maildir\' email_criteria = {\'subject\': \'Python\', \'from\': \'newsletter@example.com\'} sort_and_transfer_mails(mbox_path, maildir_path, email_criteria) ``` This function should: 1. Open the mbox mailbox. 2. Iterate through the messages, checking if they meet the criteria specified in `email_criteria`. 3. Write the filtered messages to a new Maildir mailbox. 4. Ensure data integrity by using file locks and error handling. Tip for Implementation: - Use `mailbox.mbox` to read the source mailbox and `mailbox.Maildir` to write to the destination mailbox. - Use appropriate locking methods (`lock()` and `unlock()`) to prevent concurrent access issues. - Handle potential `email.errors.MessageParseError` exceptions gracefully. **Note:** This task requires understanding of both mbox and Maildir mailbox formats and potential concurrency issues during mailbox manipulation.","solution":"import mailbox import email from email.errors import MessageParseError from email.header import decode_header def sort_and_transfer_mails(mbox_path: str, maildir_path: str, email_criteria: dict): Reads messages from an mbox mailbox, filters them based on the given criteria, and writes them to a new Maildir mailbox. Params: mbox_path (str): The path to the source mbox mailbox file. maildir_path (str): The path to the destination Maildir mailbox directory. email_criteria (dict): Dictionary containing filtering criteria, e.g., {\'subject\': \'Python\', \'from\': \'newsletter@example.com\'} Returns: None: Writes the filtered messages to the destination Maildir. # Opening the mbox and Maildir mailboxes mbox = mailbox.mbox(mbox_path) maildir = mailbox.Maildir(maildir_path, factory=None, create=True) # Lock the mbox for reading mbox.lock() try: for message in mbox: try: # Check if the message satisfies the criteria meets_criteria = True for key, value in email_criteria.items(): # Decode the header to do a proper comparison header = decode_header(message[key])[0][0] if isinstance(header, bytes): header = header.decode() if value not in header: meets_criteria = False break if meets_criteria: maildir.add(message) except (MessageParseError, KeyError, UnicodeDecodeError): # Skip any problematic email continue finally: # Always unlock the mbox mbox.unlock()"},{"question":"# PyTorch MPS Environment Variable Configuration **Objective:** Write a Python function that configures PyTorch\'s environment settings for MPS (Metal Performance Shaders) based on given input parameters. This function should adjust various settings like logging levels, memory allocation ratios, use of fast math, kernel preferences, and CPU fallbacks. **Requirements:** - Implement the function `configure_pytorch_mps` that accepts the following parameters: - `debug_allocator`: (boolean) If `True`, set the verbose logging for the MPS allocator. - `log_profile_info`: (boolean) If `True`, set the profiling options for logging. - `trace_signposts`: (boolean) If `True`, enable trace signposts for profiling. - `high_watermark_ratio`: (float) Value for the high watermark ratio for memory allocation. - `low_watermark_ratio`: (float) Value for the low watermark ratio for memory allocation. - `fast_math`: (boolean) If `True`, enable fast math optimizations. - `prefer_metal`: (boolean) If `True`, prefer using Metal kernels over MPS Graph APIs. - `enable_fallback`: (boolean) If `True`, enable CPU fallback for unsupported MPS operations. **Function Signature:** ```python def configure_pytorch_mps(debug_allocator: bool, log_profile_info: bool, trace_signposts: bool, high_watermark_ratio: float, low_watermark_ratio: float, fast_math: bool, prefer_metal: bool, enable_fallback: bool) -> None: ``` **Constraints:** - `high_watermark_ratio` should be between `0.0` (disabled) and a value greater than `1.0` (allowed beyond recommended). - `low_watermark_ratio` should be between `0.0` (disabled) and `high_watermark_ratio`. **Expected Behavior:** - The function should set the appropriate environment variables in PyTorch. - Use the `os` module to set environment variables. - Ensure the environment variables are correctly configured according to the input parameters. **Example:** ```python configure_pytorch_mps( debug_allocator=True, log_profile_info=False, trace_signposts=True, high_watermark_ratio=1.5, low_watermark_ratio=1.2, fast_math=True, prefer_metal=False, enable_fallback=True ) ``` **Additional Information:** - Refer to the provided documentation for the exact environment variable names and their functionalities. - Ensure that the environment variables are set before running PyTorch operations that depend on these settings. **Evaluation:** Your solution will be evaluated on: - Correctness: Properly setting the environment variables. - Edge cases: Handling edge values for `high_watermark_ratio` and `low_watermark_ratio`. - Code quality: Clear and comprehensible code, with appropriate use of Python conventions.","solution":"import os def configure_pytorch_mps(debug_allocator: bool, log_profile_info: bool, trace_signposts: bool, high_watermark_ratio: float, low_watermark_ratio: float, fast_math: bool, prefer_metal: bool, enable_fallback: bool) -> None: Configures PyTorch\'s MPS environment settings based on the given input parameters. if not (0.0 <= high_watermark_ratio <= 2.0): raise ValueError(\\"high_watermark_ratio must be between 0.0 and 2.0\\") if not (0.0 <= low_watermark_ratio <= high_watermark_ratio): raise ValueError(\\"low_watermark_ratio must be between 0.0 and high_watermark_ratio\\") os.environ[\'PYTORCH_MPS_ALLOCATOR_DEBUG\'] = \'1\' if debug_allocator else \'0\' os.environ[\'PYTORCH_MPS_LOG_PROFILE_INFO\'] = \'1\' if log_profile_info else \'0\' os.environ[\'PYTORCH_MPS_TRACE_SIGNPOSTS\'] = \'1\' if trace_signposts else \'0\' os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = str(high_watermark_ratio) os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = str(low_watermark_ratio) os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' if fast_math else \'0\' os.environ[\'PYTORCH_MPS_PREFER_METAL\'] = \'1\' if prefer_metal else \'0\' os.environ[\'PYTORCH_MPS_ENABLE_FALLBACK\'] = \'1\' if enable_fallback else \'0\'"},{"question":"# Question: Custom Profiling Solution You are tasked with profiling a Python application\'s performance using Python\'s `cProfile` and `pstats` modules. Your goal is to write a function that profiles a given function, saves the profile results to a file, and then prints the top 10 functions that have the highest cumulative execution time. Specification: 1. Implement the function `profile_and_report(func, filename)`. **Input:** - `func`: A callable Python function to be profiled. - `filename`: A string representing the name of the file where the profiling data should be saved. **Output:** - The function should return `None`. It should, however, print profiling statistics as described below. 2. The function should: - Profile the given function `func`. - Save the profiling data to the specified `filename`. - Load the profiling data from the file. - Sort the profiling data by cumulative time. - Print the top 10 functions (if available) based on cumulative execution time with the following columns: `ncalls`, `tottime`, `percall`, `cumtime`, `percall`, `filename:lineno(function)`. Constraints: - You may assume that the `func` provided does not require any arguments and returns without raising exceptions. - Ensure the profiling data is properly saved and loaded using the appropriate methods from `cProfile` and `pstats` modules. Example Usage: ```python def example_function(): total = 0 for i in range(1000): for j in range(1000): total += i * j profile_and_report(example_function, \'profile_output.prof\') ``` Upon running the above code, you should see an output similar to: ``` 1000505 function calls (1000504 primitive calls) in 1.002 seconds Ordered by: cumulative time List reduced from 49 to 10 due to restriction <10> ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 1.002 1.002 <ipython-input-1>:2(example_function) 1 0.001 0.001 1.002 1.002 {built-in method builtins.exec} 1 0.000 0.000 1.002 1.002 <string>:1(<module>) 1000000 1.001 0.000 1.001 0.000 {built-in method builtins.range} 1 0.000 0.000 0.000 0.000 {method \'disable\' of \'_lsprof.Profiler\' objects} ``` Good luck!","solution":"import cProfile import pstats def profile_and_report(func, filename): Profiles the given function and saves the profile data to the specified file, then prints the top 10 functions by cumulative execution time. :param func: The function to be profiled :param filename: The filename to save the profile data # Create a profiler object profiler = cProfile.Profile() # Run the function under the profiler profiler.enable() func() profiler.disable() # Save the profiling data to a file profiler.dump_stats(filename) # Load the profiling data from the file stats = pstats.Stats(filename) # Sort the profiling data by cumulative time and print the top 10 stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(10)"},{"question":"# Data Manipulation Using Pandas **Problem Statement:** You are provided with a dataset that contains information about different products sold in various regions. Your task is to manipulate this dataset to extract meaningful insights using pandas functions. The dataset is as follows: ```plaintext | Product | Region | Date | Quantity | Price | |---------|--------|------------|----------|-------| | A | East | 2023-01-15 | 10 | 20.5 | | B | West | 2023-01-17 | 15 | 35.0 | | A | East | 2023-01-20 | 5 | 20.5 | | C | North | 2023-01-22 | 8 | 50.0 | | B | West | 2023-02-01 | 10 | 35.0 | | A | South | 2023-02-05 | 3 | 20.5 | | C | North | 2023-02-10 | 6 | 50.0 | | B | South | 2023-02-15 | 7 | 35.0 | ``` **Objective:** Implement a function `analyze_sales_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]` that performs the following operations: 1. **Pivot Table Creation:** Create a pivot table that summarizes the total quantity sold and total revenue (quantity * price) for each product in each region. 2. **Datetime Manipulation:** Extract the year and month from the \'Date\' column and create a new DataFrame with the total quantity sold per product for each month. 3. **Missing Data Handling and Concatenation:** Simulate a scenario where some data is missing. Create a DataFrame with missing \'Quantity\' values, handle the missing data by filling it with the mean quantity of each product, and then concatenate this modified DataFrame with the original one. **Input:** - `df`: A pandas DataFrame containing the dataset as shown above. **Output:** - A tuple of three pandas DataFrames: 1. Pivot table summarizing total quantity and revenue. 2. DataFrame showing total quantity sold per product for each month. 3. Concatenated DataFrame after filling missing data. **Constraints:** - The \'Date\' column will always be in \'YYYY-MM-DD\' format. - All other columns contain valid data as described. **Example Usage:** ```python import pandas as pd data = { \'Product\': [\'A\', \'B\', \'A\', \'C\', \'B\', \'A\', \'C\', \'B\'], \'Region\': [\'East\', \'West\', \'East\', \'North\', \'West\', \'South\', \'North\', \'South\'], \'Date\': [\'2023-01-15\', \'2023-01-17\', \'2023-01-20\', \'2023-01-22\', \'2023-02-01\', \'2023-02-05\', \'2023-02-10\', \'2023-02-15\'], \'Quantity\': [10, 15, 5, 8, 10, 3, 6, 7], \'Price\': [20.5, 35.0, 20.5, 50.0, 35.0, 20.5, 50.0, 35.0] } df = pd.DataFrame(data) pivot_table, quantity_per_month, concatenated_df = analyze_sales_data(df) print(pivot_table) print(quantity_per_month) print(concatenated_df) ``` **Expected Output:** 1. Pivot table summarizing total quantity and revenue: | Region | Product | Total Quantity | Total Revenue | |--------|---------|----------------|---------------| | East | A | 15 | 307.5 | | North | C | 14 | 700.0 | | South | A | 3 | 61.5 | | South | B | 7 | 245.0 | | West | B | 25 | 875.0 | 2. DataFrame showing total quantity sold per product for each month: | Product | Month-Year | Total Quantity | |---------|------------|----------------| | A | 2023-01 | 15 | | A | 2023-02 | 3 | | B | 2023-01 | 15 | | B | 2023-02 | 17 | | C | 2023-01 | 8 | | C | 2023-02 | 6 | 3. Concatenated DataFrame after filling missing data: | Product | Region | Date | Quantity | Price | |---------|--------|------------|----------|-------| | A | East | 2023-01-15 | 10 | 20.5 | | B | West | 2023-01-17 | 15 | 35.0 | | A | East | 2023-01-20 | 5 | 20.5 | | C | North | 2023-01-22 | 8 | 50.0 | | B | West | 2023-02-01 | 10 | 35.0 | | A | South | 2023-02-05 | 3 | 20.5 | | C | North | 2023-02-10 | 6 | 50.0 | | B | South | 2023-02-15 | 7 | 35.0 | | [Additional Rows with Missing Data Filled] ```","solution":"import pandas as pd def analyze_sales_data(df: pd.DataFrame) -> tuple: # 1. Pivot Table Creation df[\'Revenue\'] = df[\'Quantity\'] * df[\'Price\'] pivot_table = df.pivot_table(values=[\'Quantity\', \'Revenue\'], index=[\'Region\', \'Product\'], aggfunc={\'Quantity\': \'sum\', \'Revenue\': \'sum\'}) # 2. Datetime Manipulation df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Year-Month\'] = df[\'Date\'].dt.to_period(\'M\') quantity_per_month = df.groupby([\'Product\', \'Year-Month\'])[\'Quantity\'].sum().reset_index() # 3. Missing Data Handling and Concatenation # Create a simulated missing data DataFrame missing_data = df.copy().sample(frac=0.5, random_state=1) missing_data.loc[:, \'Quantity\'] = None # Set Quantity to None to simulate missing data mean_quantity = df.groupby(\'Product\')[\'Quantity\'].transform(\'mean\') missing_data[\'Quantity\'] = missing_data[\'Quantity\'].fillna(mean_quantity) concatenated_df = pd.concat([df, missing_data]).reset_index(drop=True) return pivot_table, quantity_per_month, concatenated_df"},{"question":"# Custom Event Loop Policy and Process Watcher Implementation You are required to create a custom event loop policy and implement custom logic within that policy. Also, you will integrate a custom process watcher to handle child processes. Objectives: 1. Implement a custom event loop policy by subclassing `asyncio.DefaultEventLoopPolicy`. 2. Within the custom policy, override the `get_event_loop()` method to include custom behavior. 3. Implement a custom child watcher by subclassing `asyncio.AbstractChildWatcher` and integrating it into the custom policy. 4. Create and attach an asyncio event loop, and utilize it to run an asynchronous task that starts a child process and waits for its termination. Requirements: - Your custom policy should log the event loop each time `get_event_loop()` is called. - Your custom child watcher should print a message each time a child process is started and when it terminates. Example Input/Output: Here is a non-executable example to give you an idea of task implementation: ```python import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(f\\"Event loop retrieved: {loop}\\") return loop class MyChildWatcher(asyncio.AbstractChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Child process created with PID: {pid}\\") # Implementation of registering and calling handlers. def remove_child_handler(self, pid): print(f\\"Child process with PID {pid} has terminated\\") # Implementation of removing child process handlers. return True def attach_loop(self, loop): self._loop = loop def is_active(self): return self._loop is not None def close(self): # Clean up self._loop = None # Set Custom Event Loop Policy asyncio.set_event_loop_policy(MyEventLoopPolicy()) # Retrieve the custom policy and set a Custom Child Watcher policy = asyncio.get_event_loop_policy() policy.set_child_watcher(MyChildWatcher()) # Create and start the event loop loop = asyncio.get_event_loop() async def main(): # Dummy async task that represents a subprocess execution proc = await asyncio.create_subprocess_exec(\'echo\', \'Hello World\') await proc.wait() loop.run_until_complete(main()) ``` # Output: ```plaintext Event loop retrieved: <_UnixSelectorEventLoop running=True closed=False debug=False> Child process created with PID: 12345 Hello World Child process with PID 12345 has terminated ``` Note: The actual output may vary depending on the execution environment. Constraints: - Ensure your overridden methods do not raise unexpected exceptions. - The custom event loop policy and child watcher should be thread-safe. Performance Requirements: - The solution should handle multiple subprocesses efficiently without significant memory or performance overhead. Your task is to implement and test the above requirements in the form of a Python script.","solution":"import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(f\\"Event loop retrieved: {loop}\\") return loop class MyChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): super().__init__() self._loop = None def attach_loop(self, loop): self._loop = loop def add_child_handler(self, pid, callback, *args): print(f\\"Child process created with PID: {pid}\\") self._loop.call_soon(callback, *args) def remove_child_handler(self, pid): print(f\\"Child process with PID {pid} has terminated\\") return True def is_active(self): return self._loop is not None def close(self): # Clean up self._loop = None # Set Custom Event Loop Policy asyncio.set_event_loop_policy(MyEventLoopPolicy()) # Retrieve the custom policy and set a Custom Child Watcher policy = asyncio.get_event_loop_policy() policy.set_child_watcher(MyChildWatcher()) # Create and start the event loop loop = asyncio.get_event_loop() async def main(): # Dummy async task that represents a subprocess execution proc = await asyncio.create_subprocess_exec(\'echo\', \'Hello World\') await proc.wait() if __name__ == \\"__main__\\": loop.run_until_complete(main())"},{"question":"# Question: Multi-Stage Data Transformation Pipeline You are required to implement a data transformation pipeline. This pipeline will accept a list of dictionaries as input, each representing a user\'s data, and will output a list of dictionaries after processing. The transformation will involve multiple stages: 1. **Filter Stage**: Filter users based on specific criteria. 2. **Transformation Stage**: Transform the remaining data. 3. **Aggregation Stage**: Aggregate transformed data into a summary. Here is a detailed breakdown of what each stage should do: 1. **Filter Stage**: - Implement a function `filter_users(users: list, age_threshold: int) -> list`. - **Input**: A list of user dictionaries and an age threshold integer. - **Output**: A filtered list of user dictionaries where the user\'s age is greater than or equal to `age_threshold`. 2. **Transformation Stage**: - Implement a function `transform_users(users: list) -> list`. - **Input**: A list of user dictionaries. - **Output**: A new list of user dictionaries where the `full_name` is a concatenation of `first_name` and `last_name`, and `email` is converted to lowercase. 3. **Aggregation Stage**: - Implement a function `aggregate_data(users: list) -> dict`. - **Input**: A list of user dictionaries. - **Output**: A dictionary with two keys: `total_users` and `average_age`. `total_users` is the total number of users, and `average_age` is the average age of the users. Finally, implement a function `data_pipeline(users: list, age_threshold: int) -> dict` that combines all the stages: - **Input**: A list of user dictionaries and an age threshold integer. - **Output**: A dictionary with the aggregated data after processing through the filter, transformation, and aggregation stages. # Example ```python users = [ {\\"first_name\\": \\"John\\", \\"last_name\\": \\"Doe\\", \\"age\\": 25, \\"email\\": \\"JOHN.DOE@MAIL.COM\\"}, {\\"first_name\\": \\"Jane\\", \\"last_name\\": \\"Doe\\", \\"age\\": 22, \\"email\\": \\"JANE.DOE@MAIL.COM\\"}, {\\"first_name\\": \\"Jim\\", \\"last_name\\": \\"Beam\\", \\"age\\": 30, \\"email\\": \\"JIM.BEAM@MAIL.COM\\"}, {\\"first_name\\": \\"Jack\\", \\"last_name\\": \\"Daniels\\", \\"age\\": 20, \\"email\\": \\"JACK.DANIELS@MAIL.COM\\"} ] # Expected Output (age threshold: 23): { \\"total_users\\": 2, \\"average_age\\": 27.5 } # Step-by-step: # 1. Filter users where age >= 23: # [{\\"first_name\\": \\"John\\", \\"last_name\\": \\"Doe\\", \\"age\\": 25, \\"email\\": \\"JOHN.DOE@MAIL.COM\\"}, # {\\"first_name\\": \\"Jim\\", \\"last_name\\": \\"Beam\\", \\"age\\": 30, \\"email\\": \\"JIM.BEAM@MAIL.COM\\"}] # 2. Transform users: # [{\\"full_name\\": \\"John Doe\\", \\"age\\": 25, \\"email\\": \\"john.doe@mail.com\\"}, # {\\"full_name\\": \\"Jim Beam\\", \\"age\\": 30, \\"email\\": \\"jim.beam@mail.com\\"}] # 3. Aggregate data: # {\\"total_users\\": 2, \\"average_age\\": 27.5} ``` Ensure the functions are implemented with proper error handling and efficient use of control flow statements as covered in the documentation provided. # Constraints: - The user age will be a non-negative integer. - The user dictionary is guaranteed to have `\'first_name\'`, `\'last_name\'`, `\'age\'`, and `\'email\'` keys. # Functions to Implement: 1. `filter_users(users: list, age_threshold: int) -> list` 2. `transform_users(users: list) -> list` 3. `aggregate_data(users: list) -> dict` 4. `data_pipeline(users: list, age_threshold: int) -> dict` **Note**: Ensure all functions have appropriate docstrings.","solution":"def filter_users(users, age_threshold): Filters the users whose age is greater than or equal to the age_threshold. Parameters: users (list): A list of user dictionaries. age_threshold (int): Age threshold for filtering. Returns: list: A list of filtered user dictionaries. return [user for user in users if user[\'age\'] >= age_threshold] def transform_users(users): Transforms the user data by concatenating first and last names and converting emails to lowercase. Parameters: users (list): A list of user dictionaries. Returns: list: A list of transformed user dictionaries. transformed_users = [] for user in users: transformed_users.append({ \'full_name\': f\\"{user[\'first_name\']} {user[\'last_name\']}\\", \'age\': user[\'age\'], \'email\': user[\'email\'].lower() }) return transformed_users def aggregate_data(users): Aggregates the user data into summary statistics. Parameters: users (list): A list of user dictionaries. Returns: dict: A dictionary with `total_users` and `average_age`. total_users = len(users) if total_users == 0: average_age = 0 else: average_age = sum(user[\'age\'] for user in users) / total_users return { \'total_users\': total_users, \'average_age\': average_age } def data_pipeline(users, age_threshold): Processes the user data through the filter, transformation, and aggregation stages. Parameters: users (list): A list of user dictionaries. age_threshold (int): Age threshold for filtering. Returns: dict: A dictionary with the aggregated data. filtered_users = filter_users(users, age_threshold) transformed_users = transform_users(filtered_users) aggregated_data = aggregate_data(transformed_users) return aggregated_data"},{"question":"**Objective:** Design a Python script to demonstrate your understanding of the `cgitb` module for exception handling and traceback formatting. You need to create a sample application that uses `cgitb` to handle errors, format the traceback in HTML, and optionally log it to a file. **Requirements:** 1. Implement a function `faulty_division(a, b)` that: - Takes two numerical inputs `a` and `b`. - Returns the result of dividing `a` by `b`. - Raises a `ValueError` with the message \\"Division by zero!\\" if `b` is zero. 2. Implement a main script that: - Imports the `cgitb` module and enables it for handling exceptions, configuring it to display the traceback as HTML and log it to a file named `errors.log`. - Prompts the user to enter two numbers. - Calls the `faulty_division` function with the user\'s inputs. - Handles any exceptions that occur and uses the `cgitb.handler` function to display and log the error. 3. The script must ensure: - The traceback is detailed and includes context lines for each level. - The log file contains the complete traceback information. **Input and Output Formats:** - **Input:** - Two numerical values entered by the user. - **Output:** - The quotient of the two numbers if no exception occurs. - Detailed HTML-formatted traceback displayed in the browser and logged to `errors.log` if an exception occurs. **Constraints:** - Ensure that the division operation checks for division by zero. - The script should handle input and output via the console. **Example:** ```python import cgitb def faulty_division(a, b): if b == 0: raise ValueError(\\"Division by zero!\\") return a / b def main(): cgitb.enable(display=1, logdir=\'.\', context=5, format=\'html\') try: a = float(input(\\"Enter the numerator: \\")) b = float(input(\\"Enter the denominator: \\")) result = faulty_division(a, b) print(f\\"The result of {a} divided by {b} is {result}\\") except Exception: cgitb.handler() if __name__ == \\"__main__\\": main() ``` **Note:** Make sure to test the script by entering `0` as the denominator to trigger the exception and view the generated log file.","solution":"import cgitb def faulty_division(a, b): Returns the result of dividing a by b. Raises a ValueError if b is zero. if b == 0: raise ValueError(\\"Division by zero!\\") return a / b def main(): cgitb.enable(display=1, logdir=\'.\', context=5, format=\'html\') try: a = float(input(\\"Enter the numerator: \\")) b = float(input(\\"Enter the denominator: \\")) result = faulty_division(a, b) print(f\\"The result of {a} divided by {b} is {result}\\") except Exception: cgitb.handler() if __name__ == \\"__main__\\": main()"},{"question":"# Pandas Options and Settings Assessment You are provided with a dataset in CSV format and asked to carry out several tasks using pandas\' options and settings API. The goal is to demonstrate your understanding of how to manipulate and apply global settings in pandas. # Dataset The dataset is \\"data.csv\\" with the following columns: - `A`: Integers - `B`: Floating-point numbers - `C`: Strings - `D`: Dates in the format `YYYY-MM-DD` # Tasks 1. **Load the Dataset**: First, load the dataset into a pandas DataFrame. 2. **Set Display Options**: Configure pandas to display a maximum of 10 rows and 2 columns. 3. **Numeric Formatting**: Change the floating-point format to scientific notation with 2 decimal places. 4. **Date Formatting**: Ensure that date columns are read and displayed in the format `MM/DD/YYYY`. 5. **Context Manager**: Using `option_context`, show a preview of the dataset in its original format (as loaded) within a code context that temporarily resets your display and numeric formatting options. 6. **Reset Options**: Finally, reset all options to their default settings. # Function Signature Implement the above tasks in a function with the following signature: ```python import pandas as pd def configure_pandas_settings(file_path: str) -> pd.DataFrame: Reads a dataset and configures pandas display options. Args: file_path (str): Path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame with applied settings. pass ``` # Input - `file_path`: A string representing the path to the CSV file (`data.csv`). # Output - A pandas DataFrame with the applied settings. # Constraints & Limitations - Use the pandas functions specified in the documentation for configuring global behavior. - You should not alter the data but only the display and formatting options. # Performance Requirements - Loading the dataset and applying settings should be efficient, even for large datasets. - Ensure the context manager usage does not permanently alter the settings outside its context. Provide the complete function implementation, including any necessary imports.","solution":"import pandas as pd def configure_pandas_settings(file_path: str) -> pd.DataFrame: Reads a dataset and configures pandas display options. Args: file_path (str): Path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame with applied settings. # Load the dataset into a pandas DataFrame df = pd.read_csv(file_path, parse_dates=[\'D\'], date_parser=lambda x: pd.to_datetime(x, format=\'%Y-%m-%d\')) # Set display options pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 2) # Change floating-point format to scientific notation with 2 decimal places pd.set_option(\'display.float_format\', lambda x: f\'{x:.2e}\') # Ensure date columns are displayed in the format MM/DD/YYYY df[\'D\'] = df[\'D\'].dt.strftime(\'%m/%d/%Y\') # Using option_context to show the preview in original format with pd.option_context(\'display.max_rows\', None, \'display.max_columns\', None, \'display.float_format\', None): print(df.head()) return df # Function to reset pandas settings to their default def reset_pandas_settings(): pd.reset_option(\'^display\') pd.reset_option(\'^display.float_format\')"},{"question":"Implement a Simple Persistent Key-Value Storage You need to create a simple key-value storage system using the `dbm` module. Your task is to implement a class `SimpleKVStore` that uses the `dbm` module to store, retrieve, update, and delete key-value pairs persistently. Requirements 1. **Initialization:** - The constructor should take the name of the database file and the mode (`\'r\'`, `\'w\'`, `\'c\'`, `\'n\'`) as arguments. 2. **Add/Update Items:** - Implement a method `add_item(self, key: str, value: str)` to add a new key-value pair or update an existing one in the database. 3. **Retrieve Items:** - Implement a method `get_item(self, key: str) -> str` to retrieve the value for a given key. If the key does not exist, raise a `KeyError` with an appropriate message. 4. **Delete Items:** - Implement a method `delete_item(self, key: str)` to delete a key-value pair from the database. If the key does not exist, raise a `KeyError` with an appropriate message. 5. **List All Keys:** - Implement a method `list_keys(self) -> list` to get a list of all keys in the database. 6. **Close Database:** - Implement a method `close(self)` to close the database. Constraints - Ensure that all operations handle the conversion between strings and bytes as described in the documentation. - Use context management to ensure that the database is properly closed even in case of an error. - Assume that keys and values will only be simple strings. Example Usage ```python # Example usage of the SimpleKVStore class store = SimpleKVStore(\'mydb\', \'c\') # Create the database if it doesn\'t exist store.add_item(\'key1\', \'value1\') store.add_item(\'key2\', \'value2\') print(store.get_item(\'key1\')) # Output: value1 print(store.list_keys()) # Output: [\'key1\', \'key2\'] store.add_item(\'key1\', \'new_value\') print(store.get_item(\'key1\')) # Output: new_value store.delete_item(\'key2\') print(store.list_keys()) # Output: [\'key1\'] store.close() ``` # Implementation ```python import dbm class SimpleKVStore: def __init__(self, filename: str, mode: str): self.db = dbm.open(filename, mode) def add_item(self, key: str, value: str): self.db[key.encode()] = value.encode() def get_item(self, key: str) -> str: result = self.db.get(key.encode()) if result is None: raise KeyError(f\\"Key \'{key}\' not found\\") return result.decode() def delete_item(self, key: str): if key.encode() not in self.db: raise KeyError(f\\"Key \'{key}\' not found\\") del self.db[key.encode()] def list_keys(self) -> list: return [key.decode() for key in self.db.keys()] def close(self): self.db.close() ``` Notes - Make sure to handle any specific exceptions raised by `dbm` operations where appropriate. - Use proper error messages for exceptions. - Demonstrate robust handling for common errors such as accessing a non-existing key or adding an invalid data type.","solution":"import dbm class SimpleKVStore: def __init__(self, filename: str, mode: str): self.db = dbm.open(filename, mode) def add_item(self, key: str, value: str): self.db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') def get_item(self, key: str) -> str: value = self.db.get(key.encode(\'utf-8\')) if value is None: raise KeyError(f\\"Key \'{key}\' not found\\") return value.decode(\'utf-8\') def delete_item(self, key: str): if key.encode(\'utf-8\') not in self.db: raise KeyError(f\\"Key \'{key}\' not found\\") del self.db[key.encode(\'utf-8\')] def list_keys(self) -> list: return [key.decode(\'utf-8\') for key in self.db.keys()] def close(self): self.db.close()"},{"question":"**Objective:** Demonstrate your understanding of the `xml.etree.ElementTree` module by performing XML parsing, manipulation, and serialization. **Problem Statement:** You are given the following XML data as a string: ```xml <?xml version=\\"1.0\\"?> <books xmlns:publ=\\"http://www.example.com/publishing\\"> <book genre=\\"autobiography\\"> <title lang=\\"en\\">The Autobiography of Benjamin Franklin</title> <author> <name>Benjamin Franklin</name> <born>1706</born> <died>1790</died> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-123-45678-9</publ:isbn> <price>8.99</price> </book> <book genre=\\"novel\\"> <title lang=\\"en\\">Pride and Prejudice</title> <author> <name>Jane Austen</name> <born>1775</born> <died>1817</died> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-234-56789-0</publ:isbn> <price>12.49</price> </book> <book genre=\\"fiction\\"> <title lang=\\"en\\">The Catcher in the Rye</title> <author> <name>J.D. Salinger</name> <born>1919</born> <died>2010</died> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-345-67890-1</publ:isbn> <price>10.99</price> </book> </books> ``` Your task is to write a function `modify_and_serialize_xml(xml_data: str) -> str` that accomplishes the following tasks: 1. **Parse the XML data** from the provided string. 2. **Find all books** with a price higher than 10 and increase their price by 10%. 3. Add an attribute `discount=\\"yes\\"` to all books with a genre of \\"fiction\\". 4. **Serialize the modified XML** data back to a string and return it. **Function Signature:** ```python def modify_and_serialize_xml(xml_data: str) -> str: # Your implementation goes here ``` **Constraints:** - The XML data will always be well-formed. - Titles, author names, and ISBN numbers are unique among books. - The XML will always have a similar structure with a root element `<books>`. **Example:** Input: ```xml <?xml version=\\"1.0\\"?> <books xmlns:publ=\\"http://www.example.com/publishing\\"> <book genre=\\"autobiography\\"> <title lang=\\"en\\">The Autobiography of Benjamin Franklin</title> <author> <name>Benjamin Franklin</name> <born>1706</born> <died>1790</died> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-123-45678-9</publ:isbn> <price>8.99</price> </book> <book genre=\\"novel\\"> <title lang=\\"en\\">Pride and Prejudice</title> <author> <name>Jane Austen</name> <born>1775</born> <died>1817</d> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-234-56789-0</publ:isbn> <price>12.49</price> </book> <book genre=\\"fiction\\"> <title lang=\\"en\\">The Catcher in the Rye</title> <author> <name>J.D. Salinger</name> <born>1919</born> <died>2010</d> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-345-67890-1</publ:isbn> <price>10.99</price> </book> </books> ``` Output: ```xml <?xml version=\\"1.0\\" ?> <books xmlns:publ=\\"http://www.example.com/publishing\\"> <book genre=\\"autobiography\\"> <title lang=\\"en\\">The Autobiography of Benjamin Franklin</title> <author> <name>Benjamin Franklin</name> <born>1706</born> <died>1790</died> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-123-45678-9</publ:isbn> <price>8.99</price> </book> <book genre=\\"novel\\"> <title lang=\\"en\\">Pride and Prejudice</title> <author> <name>Jane Austen</name> <born>1775</born> <died>1817</died> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-234-56789-0</publ:isbn> <price>13.739</price> </book> <book genre=\\"fiction\\" discount=\\"yes\\"> <title lang=\\"en\\">The Catcher in the Rye</title> <author> <name>J.D. Salinger</name> <born>1919</born> <died>2010</died> </author> <publ:publisher>ExamplePublisher</publ:publisher> <publ:isbn>0-345-67890-1</publ:isbn> <price>12.089</price> </book> </books> ``` **Notes:** - Ensure that the output XML string is properly formatted. - The price increases should maintain the same number of decimal places as the original price.","solution":"import xml.etree.ElementTree as ET def modify_and_serialize_xml(xml_data: str) -> str: # Parse the XML data root = ET.fromstring(xml_data) ns = {\'publ\': \'http://www.example.com/publishing\'} # Find all book elements books = root.findall(\'book\') for book in books: # Get price and genre price_element = book.find(\'price\') genre = book.attrib.get(\'genre\') # Check the price and modify if necessary if price_element is not None: price = float(price_element.text) if price > 10: new_price = round(price * 1.1, 3) # Increase price by 10% maintaining 3 decimal places price_element.text = f\\"{new_price:.3f}\\" # Add discount attribute if genre is fiction if genre == \\"fiction\\": book.attrib[\'discount\'] = \\"yes\\" # Serialize the modified XML back to a string return ET.tostring(root, encoding=\'utf-8\').decode(\'utf-8\')"},{"question":"Background You are tasked with creating secure, random tokens for various purposes within a web application. This involves generating tokens of different formats and verifying that they are distinct and valid. Task You need to implement the following functions using the `secrets` module from Python: 1. `generate_token_bytes(nbytes: int = 32) -> bytes`: - This function should generate a secure random byte string of length `nbytes`. 2. `generate_token_hex(nbytes: int = 32) -> str`: - This function should generate a secure random hexadecimal string of length `nbytes` bytes. 3. `generate_token_urlsafe(nbytes: int = 32) -> str`: - This function should generate a secure random URL-safe string of length `nbytes` bytes. 4. `verify_token_uniqueness(token_list: list, new_token: str) -> bool`: - This function should verify if the `new_token` is unique compared to the tokens in the `token_list`. Input and Output 1. **generate_token_bytes:** - **Input:** One integer `nbytes` (default is 32). - **Output:** A byte string of length `nbytes` bytes. 2. **generate_token_hex:** - **Input:** One integer `nbytes` (default is 32). - **Output:** A hexadecimal string representing `nbytes` bytes. 3. **generate_token_urlsafe:** - **Input:** One integer `nbytes` (default is 32). - **Output:** A URL-safe string representing `nbytes` bytes. 4. **verify_token_uniqueness:** - **Input:** A list of strings `token_list` and a string `new_token`. - **Output:** A boolean value. `True` if `new_token` is unique in `token_list`, otherwise `False`. Constraints - The `nbytes` should be a positive integer less than or equal to 64. - The `token_list` passed to `verify_token_uniqueness` should contain only unique tokens. Example ```python # Example Usage # Generating tokens byte_token = generate_token_bytes(16) hex_token = generate_token_hex(16) urlsafe_token = generate_token_urlsafe(16) # Verifying uniqueness tokens = [hex_token, urlsafe_token] is_unique = verify_token_uniqueness(tokens, generate_token_hex(16)) # This should ideally return True ``` Notes - For generating tokens, use the methods provided by the `secrets` module to ensure cryptographic security. - Ensure your functions handle edge cases appropriately, such as very small or very large values of `nbytes`. Good luck!","solution":"import secrets def generate_token_bytes(nbytes: int = 32) -> bytes: Generate a secure random byte string of length `nbytes`. return secrets.token_bytes(nbytes) def generate_token_hex(nbytes: int = 32) -> str: Generate a secure random hexadecimal string of length `nbytes` bytes. return secrets.token_hex(nbytes) def generate_token_urlsafe(nbytes: int = 32) -> str: Generate a secure random URL-safe string of length `nbytes` bytes. return secrets.token_urlsafe(nbytes) def verify_token_uniqueness(token_list: list, new_token: str) -> bool: Verify if the `new_token` is unique compared to the tokens in the `token_list`. return new_token not in token_list"},{"question":"# Coding Assessment: Building and Validating a WSGI Application **Objective**: Implement a WSGI application using the `wsgiref` package that serves dynamic content based on request parameters and validate its conformance to the WSGI specification. **Task**: 1. Write a WSGI application that responds to HTTP GET requests. Your application should: - Return a \'200 OK\' status with a dynamic HTML content. - The HTML content should display the path from the URL and any query parameters provided in the request. - For example, accessing `http://localhost:8000/test?name=John` should return an HTML page containing: ``` <html> <body> <h1>Path: /test</h1> <p>Query parameters:</p> <ul> <li>name: John</li> </ul> </body> </html> ``` 2. Ensure your WSGI application is compliant with the WSGI specification using the `wsgiref.validate` module. 3. Utilize the `wsgiref.simple_server` module to serve your WSGI application on `localhost` at port `8000`. **Constraints**: - You must use the `wsgiref.util` functions where applicable (e.g., `request_uri` to reconstruct the full URI). - Ensure proper handling of different query parameters, including the case of no parameters. **Input/Output**: - Input: HTTP GET requests through a web browser or an HTTP client (e.g., `curl`). - Output: Dynamic HTML content based on the request\'s path and query parameters. **Performance**: - The application should handle multiple requests efficiently without crashing. **Files**: Create a single file named `wsgi_app.py` containing your solution. This file should define the WSGI application function, set up the server, and include the necessary imports. **Example**: ```python from wsgiref.simple_server import make_server from wsgiref.util import request_uri from wsgiref.validate import validator def application(environ, start_response): status = \'200 OK\' headers = [(\'Content-type\', \'text/html; charset=utf-8\')] start_response(status, headers) path = environ.get(\'PATH_INFO\', \'/\') query = environ.get(\'QUERY_STRING\', \'\') query_params = [param.split(\'=\') for param in query.split(\'&\') if param] response_body = f <html> <body> <h1>Path: {path}</h1> <p>Query parameters:</p> <ul> {\\"\\".join(f\\"<li>{k}: {v}</li>\\" for k, v in query_params)} </ul> </body> </html> return [response_body.encode(\'utf-8\')] validated_app = validator(application) if __name__ == \'__main__\': with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Submit your file `wsgi_app.py` with the implemented WSGI application and supporting code to validate its WSGI compliance. **Note**: Ensure your code runs successfully without modification when executed in an environment with Python 3.10 and `wsgiref` module available.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import request_uri from wsgiref.validate import validator def application(environ, start_response): status = \'200 OK\' headers = [(\'Content-type\', \'text/html; charset=utf-8\')] start_response(status, headers) path = environ.get(\'PATH_INFO\', \'/\') query = environ.get(\'QUERY_STRING\', \'\') query_params = [param.split(\'=\') for param in query.split(\'&\') if param] response_body = f <html> <body> <h1>Path: {path}</h1> <p>Query parameters:</p> <ul> {\\"\\".join(f\\"<li>{k}: {v}</li>\\" for k, v in query_params)} </ul> </body> </html> return [response_body.encode(\'utf-8\')] validated_app = validator(application) if __name__ == \'__main__\': with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"Context: Seaborn is a visualization library based on matplotlib that provides a high-level interface for drawing attractive statistical graphics. A key feature of seaborn is its ability to handle and customize color palettes. In this task, you will use the `husl_palette` function from the seaborn library to create customized color palettes and apply them to a seaborn plot. Problem Statement: 1. **Color Palette Creation:** - Write a function `generate_custom_palette` that takes three parameters: - `num_colors` (int): The number of colors to generate in the palette. (Default is 6) - `lightness` (float): Lightness level of the colors, value ranges between 0 and 1. (Default is 0.5) - `saturation` (float): Saturation level of the colors, value ranges between 0 and 1. (Default is 0.65) - This function should use `sns.husl_palette` to create and return a list of colors based on the input parameters. 2. **Visualization with Custom Palette:** - Write another function `visualize_custom_palette` that: - Takes a pandas DataFrame with at least two columns of numerical data. - Uses the `generate_custom_palette` function to generate a color palette. - Plots the data from the DataFrame using seaborn\'s lineplot function, utilizing the generated custom palette. - Ensure that if the number of unique categories in the DataFrame\'s \\"category\\" column exceeds the number of colors in the generated palette, an appropriate error message is displayed. Input: - A pandas DataFrame with at least two numerical columns. - Parameters for `generate_custom_palette` function: - `num_colors` (int) - `lightness` (float) - `saturation` (float) Output: - Return the list of generated colors from `generate_custom_palette`. - Display a line plot using the colors from the generated palette in `visualize_custom_palette`. Constraints: - You must use seaborn\'s `husl_palette` for creating the color palettes. - You cannot use any other color generating methods other than `husl_palette`. Example: ```python import pandas as pd import seaborn as sns # Sample DataFrame data = pd.DataFrame({ \'x\': range(10), \'y\': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29], \'category\': [\'A\'] * 5 + [\'B\'] * 5 }) # Generate a custom palette colors = generate_custom_palette(num_colors=6, lightness=0.7, saturation=0.8) # Visualize using the custom palette visualize_custom_palette(data, colors) ``` **Note:** Ensure the visualized plot uses the custom palette returned from the `generate_custom_palette` function correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_custom_palette(num_colors=6, lightness=0.5, saturation=0.65): Generate a custom HUSL palette. Parameters: num_colors (int): The number of colors to generate in the palette. lightness (float): Lightness level of the colors; value ranges between 0 and 1. saturation (float): Saturation level of the colors; value ranges between 0 and 1. Returns: list: A list of generated colors. return sns.husl_palette(num_colors, h=0.5, l=lightness, s=saturation) def visualize_custom_palette(data, palette): Visualizes the data using a given custom palette. Parameters: data (pd.DataFrame): DataFrame with at least \'x\', \'y\', and \'category\' columns. palette (list): List of colors to use in the palette. unique_categories = data[\'category\'].unique() if len(unique_categories) > len(palette): raise ValueError(f\\"Number of unique categories ({len(unique_categories)}) exceeds the number of colors in the palette ({len(palette)})\\") # Plotting the data sns.lineplot(data=data, x=\'x\', y=\'y\', hue=\'category\', palette=palette) plt.show()"},{"question":"# Seaborn Coding Assessment Question You are tasked with creating a comprehensive visualization for a dataset using seaborn\'s `JointGrid`. The goal is to demonstrate your understanding of seaborn, particularly the `JointGrid` class and its capabilities. Dataset We\'ll be using the `tips` dataset that comes pre-loaded with seaborn. This dataset contains information about the tips received by waiters and waitresses over several days. Requirements 1. Load the `tips` dataset into a DataFrame. 2. Initialize a `JointGrid` for exploring the relationship between `total_bill` and `tip`. 3. Plot a scatter plot on the joint axes and histograms on the marginal axes. 4. Add a third variable, `size`, to the plot using the `hue` parameter to separate the data by the size of the party. 5. Customize the plot by adding reference lines for the mean values of `total_bill` and `tip`. 6. Set the overall size of the plot to a height of 7 units, with a ratio of 3 and a spacing of 0.1 between the plots. 7. Ensure that the marginal ticks are turned on. 8. Define custom limits for the x-axis (0 to 60) and y-axis (0 to 12). # Solution Template ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Initialize the JointGrid g = sns.JointGrid(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\") # Plot the joint and marginal plots g.plot(sns.scatterplot, sns.histplot) # Add reference lines for the mean values mean_total_bill = tips[\\"total_bill\\"].mean() mean_tip = tips[\\"tip\\"].mean() g.refline(x=mean_total_bill, y=mean_tip, linestyle=\'--\', color=\'red\') # Set the size, ratio, and spacing of the plot g.fig.set_figheight(7) g.fig.set_figwidth(7) g.fig.subplots_adjust(hspace=0.1) # Enable marginal ticks g.ax_marg_x.tick_params(axis=\\"x\\", which=\\"both\\", labelbottom=True) g.ax_marg_y.tick_params(axis=\\"y\\", which=\\"both\\", labelleft=True) # Set custom limits for the axes g.ax_joint.set_xlim(0, 60) g.ax_joint.set_ylim(0, 12) # Display the plot plt.show() ``` # Evaluation Criteria Your implementation will be assessed based on: 1. Correct loading and initialization of the seaborn `JointGrid`. 2. Appropriate use of `hue` to represent a third variable. 3. Successful customization of reference lines, plot size, and axis limits. 4. Visualization clarity and accuracy.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_jointplot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Initialize the JointGrid g = sns.JointGrid(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", height=7, ratio=3, space=0.1) # Plot the joint and marginal plots g.plot(sns.scatterplot, sns.histplot) # Add reference lines for the mean values mean_total_bill = tips[\\"total_bill\\"].mean() mean_tip = tips[\\"tip\\"].mean() g.refline(x=mean_total_bill, y=mean_tip, linestyle=\'--\', color=\'red\') # Enable marginal ticks g.ax_marg_x.tick_params(axis=\\"x\\", which=\\"both\\", labelbottom=True) g.ax_marg_y.tick_params(axis=\\"y\\", which=\\"both\\", labelleft=True) # Set custom limits for the axes g.ax_joint.set_xlim(0, 60) g.ax_joint.set_ylim(0, 12) # Display the plot plt.show()"},{"question":"# HTML Entity Converter You are tasked with creating a utility to convert between HTML entities and Unicode characters using the `html.entities` module. # Task Write a Python function `html_entity_converter` that takes two arguments: 1. `text` (str): A string that may contain HTML entities. 2. `direction` (str): A string indicating the conversion direction. It can be either `\\"to_unicode\\"` or `\\"to_entity\\"`. Depending on the value of `direction`, the function should: - If `direction` is `\\"to_unicode\\"`, convert all HTML entities in `text` to their corresponding Unicode characters. - If `direction` is `\\"to_entity\\"`, convert all applicable characters in `text` to their HTML entity equivalents. # Input Format - `text` (str): A string potentially containing HTML entities. - `direction` (str): A string that can be either `\\"to_unicode\\"` or `\\"to_entity\\"`. # Output Format - A string with the converted text. # Constraints - The `text` input string will not exceed 10,000 characters. - The `direction` input will always be valid and one of either `\\"to_unicode\\"` or `\\"to_entity\\"`. # Example ```python from html.entities import name2codepoint, codepoint2name def html_entity_converter(text: str, direction: str) -> str: if direction == \\"to_unicode\\": for name, codepoint in name2codepoint.items(): text = text.replace(f\\"&{name};\\", chr(codepoint)) elif direction == \\"to_entity\\": for codepoint, name in codepoint2name.items(): text = text.replace(chr(codepoint), f\\"&{name};\\") return text # Example usage text = \\"Hello &amp; welcome to the world of HTML &lt;entities&gt;!\\" direction = \\"to_unicode\\" print(html_entity_converter(text, direction)) # Output: \'Hello & welcome to the world of HTML <entities>!\' text = \\"Hello & welcome to the world of HTML <entities>!\\" direction = \\"to_entity\\" print(html_entity_converter(text, direction)) # Output: \'Hello &amp; welcome to the world of HTML &lt;entities&gt;!\' ``` # Notes - You can use the `name2codepoint` dictionary for converting entity names to Unicode characters. - You can use the `codepoint2name` dictionary for converting Unicode characters to entity names. - Handle both named entities present in `html5` and those in `entitydefs`. # Additional Information You may refer to the following resources to understand HTML entities and their Unicode mappings: - [HTML5 Named Character References](https://html.spec.whatwg.org/multipage/named-characters.html#named-character-references)","solution":"from html.entities import name2codepoint, codepoint2name def html_entity_converter(text: str, direction: str) -> str: if direction == \\"to_unicode\\": for name, codepoint in name2codepoint.items(): text = text.replace(f\\"&{name};\\", chr(codepoint)) elif direction == \\"to_entity\\": for codepoint, name in codepoint2name.items(): text = text.replace(chr(codepoint), f\\"&{name};\\") return text"},{"question":"**Problem Statement: Multi-Stage Feature Selection Pipeline** You are provided with a dataset containing multiple features and a target variable. Your task is to implement a multi-stage feature selection pipeline using scikit-learn\'s feature selection methods and evaluate the performance of the final model. Follow the steps outlined below: 1. **Load the Dataset**: You can use any dataset available in scikit-learn (e.g., the Iris dataset or the Wine dataset). 2. **Stage 1 - Variance Threshold**: Remove all features with variance lower than a predefined threshold. 3. **Stage 2 - Univariate Feature Selection**: Select the top k features using a univariate statistical test suited for the problem (classification or regression). 4. **Stage 3 - Model-Based Selection**: Use a model-based feature selection method (e.g., L1-based selection with LinearSVC or tree-based selection with RandomForestClassifier) to further reduce the number of features. 5. **Train a Final Model**: Train a classifier (e.g., RandomForestClassifier) on the selected features and evaluate its performance using cross-validation. **Requirements**: - The final output should be the mean cross-validation score of the classifier. - Use appropriate scikit-learn modules and functions for each stage of feature selection. - Ensure that the intermediate steps are properly logged and each stage of feature selection is clearly visible in the code. **Input**: - The code should load the dataset internally. - No external input is necessary. **Output**: - Print the mean cross-validation score of the final classifier. **Constraints**: - You must use scikit-learn\'s feature selection methods for each stage. - Use a standard classifier from scikit-learn for the final model. **Performance Requirements**: - The mean cross-validation score should be at least 90% for the Iris dataset or 80% for the Wine dataset. **Example**: ```python from sklearn.datasets import load_iris from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel from sklearn.svm import LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score import numpy as np # Load the dataset X, y = load_iris(return_X_y=True) # Stage 1: Variance Threshold selector_1 = VarianceThreshold(threshold=(.8 * (1 - .8))) X_var = selector_1.fit_transform(X) # Stage 2: Univariate Feature Selection selector_2 = SelectKBest(f_classif, k=2) X_uni = selector_2.fit_transform(X_var, y) # Stage 3: Model-Based Selection lsvc = LinearSVC(C=0.01, penalty=\\"l1\\", dual=False).fit(X_uni, y) selector_3 = SelectFromModel(lsvc, prefit=True) X_model = selector_3.transform(X_uni) # Train a final model final_model = RandomForestClassifier() # Evaluate using cross-validation cv_scores = cross_val_score(final_model, X_model, y, cv=5) # Print mean cross-validation score print(\\"Mean cross-validation score:\\", np.mean(cv_scores)) ``` Implement the above pipeline, ensuring each stage of feature selection is clearly visible, and evaluate the performance of the final model.","solution":"from sklearn.datasets import load_wine from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel from sklearn.svm import LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np def multi_stage_feature_selection_pipeline(): # Load the dataset X, y = load_wine(return_X_y=True) # Stage 1: Variance Threshold selector_1 = VarianceThreshold(threshold=(.8 * (1 - .8))) X_var = selector_1.fit_transform(X) # Stage 2: Univariate Feature Selection selector_2 = SelectKBest(f_classif, k=8) X_uni = selector_2.fit_transform(X_var, y) # Stage 3: Model-Based Selection lsvc = LinearSVC(C=0.01, penalty=\\"l1\\", dual=False).fit(X_uni, y) selector_3 = SelectFromModel(lsvc, prefit=True) X_model = selector_3.transform(X_uni) # Train a final model final_model = RandomForestClassifier(random_state=42) # Evaluate using cross-validation cv_scores = cross_val_score(final_model, X_model, y, cv=5) # Return mean cross-validation score mean_cv_score = np.mean(cv_scores) return mean_cv_score # Executing the function and printing the result mean_score = multi_stage_feature_selection_pipeline() print(\\"Mean cross-validation score:\\", mean_score)"},{"question":"Problem Statement You have been recruited by a data analysis company to analyze the distribution of ages in different classes and fare ranges for passengers on the Titanic, using violin plots. Your task is to write a function using the seaborn package in Python, which will generate and save these violin plots based on specific requirements. # Function Signature ```python def plot_titanic_violin_plots(output_file_path: str) -> None: pass ``` # Input - `output_file_path`: A string representing the path where the generated violin plots image should be saved. # Output The function should generate and save an image consisting of two violin plots: 1. A violin plot showing the distribution of passenger ages for each passenger class, colored by the \'alive\' status. 2. A violin plot showing the distribution of passenger ages based on fare brackets (each bracket of 10), using the native scale for the x-axis and density normalization by count. # Requirements: 1. Both violin plots should be shown in a single image. 2. The image should be saved to the location specified by the `output_file_path`. 3. The function should adhere to the following customizations: - Use the `titanic` dataset provided by seaborn. - In the first plot, use `hue` to differentiate between passengers who were \'alive\' or not. - In the second plot, use a custom `formatter` to categorize fare into brackets of 10. # Constraints - You should use the seaborn library for creating the plots and matplotlib for saving the image. - Handle the case where the dataset may contain missing values. # Example ```python output_file_path = \\"titanic_violin_plots.png\\" plot_titanic_violin_plots(output_file_path) ``` The function, when called with the example above, should generate and save an image titled `\\"titanic_violin_plots.png\\"` in the current working directory. # Additional Information - You may refer to the seaborn documentation for details on violin plots: https://seaborn.pydata.org/generated/seaborn.violinplot.html - For handling missing values, you can use pandas functions like `dropna`.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_titanic_violin_plots(output_file_path: str) -> None: # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Drop rows with missing age titanic = titanic.dropna(subset=[\\"age\\"]) # Create a figure with two subplots, side by side fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6)) # Customize plot 1: Passenger age distribution by class and alive status sns.violinplot(x=\\"class\\", y=\\"age\\", hue=\\"alive\\", data=titanic, split=True, ax=ax1) ax1.set_title(\\"Age Distribution by Class and Survival\\") # Binning fare into brackets of 10 fare_bins = pd.cut(titanic[\\"fare\\"], bins=range(0, int(titanic[\\"fare\\"].max() + 10), 10), right=False) titanic[\\"fare_bracket\\"] = fare_bins.astype(str) # Customize plot 2: Passenger age distribution by fare bracket sns.violinplot(x=\\"fare_bracket\\", y=\\"age\\", data=titanic, scale=\\"count\\", inner=\\"quartile\\", ax=ax2) ax2.set_title(\\"Age Distribution by Fare Bracket (10)\\") ax2.set_xticklabels(ax2.get_xticklabels(), rotation=90) # Save the figure plt.tight_layout() plt.savefig(output_file_path) plt.close()"},{"question":"**Advanced Python Module Management** Using the `pkgutil` module, write a Python function that performs the following operations: 1. **Find all top-level modules** available on the current `sys.path`. 2. **Extend the search path** of a given package name to include all possible subdirectories on `sys.path`. 3. **Retrieve and return resource data** for a specified resource within a given package. # Function Signature: ```python def module_management(package_name: str, resource_name: str) -> dict: Perform advanced module and package operations using the pkgutil module. :param package_name: The name of the package to extend the search path for. :param resource_name: The name of the resource to retrieve data for. :return: A dictionary with the following keys: \'top_level_modules\': List of all top-level modules. \'extended_path\': The extended search path of the given package. \'resource_data\': The contents of the specified resource as a binary string or None if not found. pass ``` # Input: - `package_name` (str): The name of the package whose search path should be extended. - `resource_name` (str): The resource within the package for which the data should be retrieved. # Output: - Returns a dictionary with the following keys: - `\'top_level_modules\'`: A list of names of all top-level modules available on the current `sys.path`. - `\'extended_path\'`: The extended search path of the given package. - `\'resource_data\'`: The contents of the specified resource as a binary string, or `None` if the resource does not exist. # Constraints: - You may assume that the given package exists and is importable. - Handle exceptions appropriately to ensure that the function does not crash due to unforeseen issues (e.g., invalid resource name). # Example: ```python result = module_management(\'ctypes\', \'README.txt\') assert \'top_level_modules\' in result assert \'extended_path\' in result assert \'resource_data\' in result print(result[\'top_level_modules\']) # Example output: [\'pkgutil\', \'sys\', \'os\', ...] print(result[\'extended_path\']) # Example output: [...] # List of paths where \'ctypes\' package is searched. print(result[\'resource_data\']) # Example output: b\'...\' # Binary data of the \'README.txt\' resource, if it exists. ``` *Note*: The actual content of the `resource_data` and the structure of the `extended_path` will depend on the specific environment and modules available. The example is for illustration purposes. **Performance Requirements:** - The function should efficiently handle the module management tasks without unnecessary processing. - Ensure that the function returns promptly even when dealing with a large number of modules and packages.","solution":"import pkgutil import sys import os def module_management(package_name: str, resource_name: str) -> dict: Perform advanced module and package operations using the pkgutil module. :param package_name: The name of the package to extend the search path for. :param resource_name: The name of the resource to retrieve data for. :return: A dictionary with the following keys: \'top_level_modules\': List of all top-level modules. \'extended_path\': The extended search path of the given package. \'resource_data\': The contents of the specified resource as a binary string or None if not found. # Find all top-level modules available on the current sys.path top_level_modules = [m.name for m in pkgutil.iter_modules()] # Extend the search path of the given package name extended_path = pkgutil.extend_path([], package_name) # Retrieve and return the resource data resource_data = None try: loader = pkgutil.get_loader(package_name) if loader is not None and hasattr(loader, \'get_data\'): resource_path = os.path.join(package_name.replace(\'.\', \'/\'), resource_name) resource_data = loader.get_data(resource_path) except Exception as e: resource_data = None return { \'top_level_modules\': top_level_modules, \'extended_path\': extended_path, \'resource_data\': resource_data }"},{"question":"**Coding Assessment Question:** **Objective:** Demonstrate your understanding of the `email.policy` package by creating and manipulating custom policies to handle email messages with specific requirements. **Background:** You are tasked with writing a Python function that reads an email message from a file, modifies its headers and body using a custom policy, and then serializes the modified message back to a file. **Requirements:** 1. Create a custom policy that: - Uses a maximum line length of 100 characters. - Uses `rn` as the line separator. - Ensures all data is `7bit` encoded. - Raises errors on defects encountered. 2. Read an email message from a binary file using the `message_from_binary_file` method. 3. Modify the message headers to include a custom header `X-Custom-Header` with a value `CustomValue`. 4. Serialize the modified message to an output file using the custom policy. **Function Signature:** ```python def process_email(input_file: str, output_file: str) -> None: pass ``` **Parameters:** - `input_file` (str): The path to the input binary file containing the email message. - `output_file` (str): The path to the output file where the modified email should be serialized. **Constraints:** - Ensure the code handles exceptions and errors gracefully. - Performance considerations are minimal given typical email sizes. **Example Usage:** ```python process_email(\'input_message.eml\', \'output_message.eml\') ``` **Details:** 1. Implement the function `process_email` by: - Creating a custom policy using the attributes specified above. - Reading the email message from `input_file`. - Modifying the email to add the new header. - Serializing the modified email to `output_file`. **Hints:** - Refer to the `email.policy` module documentation for creating and cloning policies. - Use the `BytesGenerator` class for serializing the message with the custom policy. **Sample Code Skeleton:** ```python from email import message_from_binary_file from email.generator import BytesGenerator from email import policy import os def process_email(input_file: str, output_file: str) -> None: # Step 1: Create a custom policy custom_policy = policy.EmailPolicy( max_line_length=100, linesep=\'rn\', cte_type=\'7bit\', raise_on_defect=True ) # Step 2: Read the email message from the input file with open(input_file, \'rb\') as f: msg = message_from_binary_file(f, policy=custom_policy) # Step 3: Add custom header msg[\'X-Custom-Header\'] = \'CustomValue\' # Step 4: Serialize the modified email to the output file with open(output_file, \'wb\') as f_out: generator = BytesGenerator(f_out, policy=custom_policy) generator.flatten(msg) ```","solution":"from email import message_from_binary_file from email.generator import BytesGenerator from email.policy import EmailPolicy import os def process_email(input_file: str, output_file: str) -> None: # Step 1: Create a custom policy custom_policy = EmailPolicy( max_line_length=100, linesep=\'rn\', cte_type=\'7bit\', raise_on_defect=True ) # Step 2: Read the email message from the input file with open(input_file, \'rb\') as f: msg = message_from_binary_file(f, policy=custom_policy) # Step 3: Add custom header msg[\'X-Custom-Header\'] = \'CustomValue\' # Step 4: Serialize the modified email to the output file with open(output_file, \'wb\') as f_out: generator = BytesGenerator(f_out, policy=custom_policy) generator.flatten(msg)"},{"question":"Complex Numbers in Python You are required to implement a Python module that provides functionalities similar to those described in the provided documentation regarding complex numbers. You will create a custom class and a set of functions to perform arithmetic operations on complex numbers. # Requirements 1. **Complex Class**: Implement a class `Complex` that represents a complex number. This class should: - Have two attributes, `real` and `imag`, representing the real and imaginary parts, respectively. - Implement the `__add__`, `__sub__`, `__mul__`, `__truediv__`, and `__pow__` methods to perform addition, subtraction, multiplication, division, and exponentiation. - Implement the `__neg__` method to return the negation of the complex number. - Implement the `__str__` method to return a string representation of the complex number in the form `\\"(real + imagj)\\"`. 2. **Functions**: Implement the following standalone functions: - `complex_sum(a: Complex, b: Complex) -> Complex`: This should return the sum of two `Complex` numbers. - `complex_diff(a: Complex, b: Complex) -> Complex`: This should return the difference between two `Complex` numbers. - `complex_prod(a: Complex, b: Complex) -> Complex`: This should return the product of two `Complex` numbers. - `complex_quot(a: Complex, b: Complex) -> Complex`: This should return the quotient of two `Complex` numbers. If the divisor is zero, raise a `ZeroDivisionError`. - `complex_pow(a: Complex, b: Complex) -> Complex`: This should return the result of raising `a` to the power of `b`. # Constraints - You should not use the built-in `complex` type for this exercise. - Pay attention to floating-point arithmetic errors and edge cases (such as division by zero). - The methods and functions should handle both positive and negative real and imaginary parts correctly. # Example ```python # Create instances of Complex a = Complex(2, 3) b = Complex(1, -4) # Arithmetic operations via methods print(a + b) # Output: (3.0 - 1.0j) print(a - b) # Output: (1.0 + 7.0j) print(a * b) # Output: (14.0 + -5.0j) print(a / b) # Output: (-0.5882352941176471 + 0.6470588235294118j) print(a ** b) # Output: (-0.10267794504458803 + -0.3304248060217481j) # Arithmetic operations via functions print(complex_sum(a, b)) # Output: (3.0 - 1.0j) print(complex_diff(a, b)) # Output: (1.0 + 7.0j) print(complex_prod(a, b)) # Output: (14.0 - 5.0j) print(complex_quot(a, b)) # Output: (-0.5882352941176471 + 0.6470588235294118j) print(complex_pow(a, b)) # Output: (-0.10267794504458803 - 0.3304248060217481j) ``` Ensure your code is well-structured and thoroughly commented. You may use additional helper methods if necessary to keep your code clean and readable.","solution":"import math class Complex: def __init__(self, real, imag): self.real = real self.imag = imag def __add__(self, other): return Complex(self.real + other.real, self.imag + other.imag) def __sub__(self, other): return Complex(self.real - other.real, self.imag - other.imag) def __mul__(self, other): real = self.real * other.real - self.imag * other.imag imag = self.real * other.imag + self.imag * other.real return Complex(real, imag) def __truediv__(self, other): if other.real == 0 and other.imag == 0: raise ZeroDivisionError(\\"division by zero\\") denom = other.real**2 + other.imag**2 real = (self.real * other.real + self.imag * other.imag) / denom imag = (self.imag * other.real - self.real * other.imag) / denom return Complex(real, imag) def __neg__(self): return Complex(-self.real, -self.imag) def __pow__(self, other): # Convert self to polar form r = math.sqrt(self.real ** 2 + self.imag ** 2) theta = math.atan2(self.imag, self.real) # Compute power in polar form r_pow = r ** other.real # Assume b.imag == 0 for simplicity theta_pow = theta * other.real # Convert back to rectangular form real = r_pow * math.cos(theta_pow) imag = r_pow * math.sin(theta_pow) return Complex(real, imag) def __str__(self): return f\\"({self.real} + {self.imag}j)\\" def complex_sum(a: Complex, b: Complex) -> Complex: return a + b def complex_diff(a: Complex, b: Complex) -> Complex: return a - b def complex_prod(a: Complex, b: Complex) -> Complex: return a * b def complex_quot(a: Complex, b: Complex) -> Complex: return a / b def complex_pow(a: Complex, b: Complex) -> Complex: return a ** b"},{"question":"# Question: Minimal Reproducible Bug Example and Resolution Given the following scenario, provide a coded solution demonstrating your understanding of dataset creation, model training, and debugging a specific issue in scikit-learn. Scenario: You are working on a regression task using scikit-learn\'s `GradientBoostingRegressor`. You encounter a UserWarning: \\"X has feature names, but GradientBoostingRegressor was fitted without feature names\\" when you set a specific parameter for the model. Your task is to illustrate this issue using synthetic data and provide a minimal reproducible example, followed by resolving the issue. Requirements: 1. **Synthetic Dataset Creation**: - Create a synthetic regression dataset using `make_regression` from `sklearn.datasets`. - Ensure the dataset is a pandas DataFrame with appropriate feature names. 2. **Model Training and Issue Illustration**: - Train a `GradientBoostingRegressor` without any warnings. - Modify a parameter to illustrate the bug (`n_iter_no_change=5`) and show the resulting warning using a minimal reproducible example. 3. **Issue Resolution**: - Provide a solution to resolve the UserWarning that arises. # Expected Input: - No input needed, the data is generated within the code. # Expected Output: - Print the warning message encountered during the training phase. - Show the corrected code that resolves this warning. # Constraints: - Use only scikit-learn and pandas libraries. - Keep all steps as minimal as possible while clearly showing the issue and resolution. # Performance Requirements: - The synthetic dataset should be small (e.g., 100 samples, 10 features) to ensure quick computation. # Points Distribution: - **Synthetic Dataset Creation (20 points)** - **Illustrating the Issue (40 points)** - **Resolving the Issue (40 points)** # Example Solution: ```python import pandas as pd from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor # Step 1: Create synthetic dataset with feature names X, y = make_regression(n_samples=100, n_features=10, noise=0.1) df = pd.DataFrame(X, columns=[f\\"feature_{i}\\" for i in range(X.shape[1])]) df[\'target\'] = y # Split dataset X = df[[f\\"feature_{i}\\" for i in range(X.shape[1])]] y = df[\'target\'] # Step 2: Train a model without user warning model = GradientBoostingRegressor() model.fit(X, y) print(\\"Model trained without any warnings.\\") # Step 2: Illustrate bug model_with_bug = GradientBoostingRegressor(n_iter_no_change=5) model_with_bug.fit(X.values, y) print(\\"Expected Warning: X has feature names, but GradientBoostingRegressor was fitted without feature names\\") # Step 3: Resolve the issue model_resolved = GradientBoostingRegressor(n_iter_no_change=5) model_resolved.fit(X, y) # Use the DataFrame directly with feature names print(\\"Model trained successfully without warnings.\\") ``` **Note:** Ensure that your code runs successfully and returns the expected outcomes. Submit your results.","solution":"import pandas as pd from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor def create_synthetic_data(samples=100, features=10): Create a synthetic regression dataset and return it as a pandas DataFrame. Args: samples (int): Number of samples to generate. features (int): Number of features to generate. Returns: DataFrame: A pandas DataFrame with generated features and target. X, y = make_regression(n_samples=samples, n_features=features, noise=0.1) df = pd.DataFrame(X, columns=[f\\"feature_{i}\\" for i in range(X.shape[1])]) df[\'target\'] = y return df def train_model_without_warning(X, y): Train GradientBoostingRegressor without triggering any warnings. Args: X (DataFrame): Feature matrix. Y (Series): Target vector. Returns: GradientBoostingRegressor: Trained model. model = GradientBoostingRegressor() model.fit(X, y) return model def train_model_with_warning(X, y): Illustrate the warning when using feature names with GradientBoostingRegressor and specific parameters. Args: X (DataFrame): Feature matrix. Y (Series): Target vector. model = GradientBoostingRegressor(n_iter_no_change=5) model.fit(X.values, y) # using X.values here to trigger the warning print(\\"Warning triggered: X has feature names, but GradientBoostingRegressor was fitted without feature names\\") def train_model_resolved_warning(X, y): Train GradientBoostingRegressor with the correct handling of feature names to resolve warning. Args: X (DataFrame): Feature matrix. Y (Series): Target vector. Returns: GradientBoostingRegressor: Trained model. model = GradientBoostingRegressor(n_iter_no_change=5) model.fit(X, y) # using DataFrame directly to keep feature names return model # Generate synthetic data df = create_synthetic_data() X = df[[f\\"feature_{i}\\" for i in range(df.shape[1] - 1)]] y = df[\'target\'] # Train model without warnings model_without_warning = train_model_without_warning(X, y) print(\\"Model trained without any warnings.\\") # Illustrate triggering the warning train_model_with_warning(X, y) # Train model with resolved warning model_resolved = train_model_resolved_warning(X, y) print(\\"Model trained successfully without warnings.\\")"},{"question":"# Email Message Subpart Extractor Objective You are to write a Python function that processes an email message object, extracting specific lines of text from subparts that match a given MIME type. Function Signature ```python def extract_subpart_lines(msg, maintype=\'text\', subtype=None, keyword=None): Process the email message object to extract lines of text from subparts that match the given MIME type. Only lines that contain a specified keyword should be extracted. Parameters: - msg: The email message object to be processed. - maintype (str): The main MIME type of the subparts to match (\'text\' by default). - subtype (str): The specific subtype of the subparts to match (optional). - keyword (str): The keyword to filter lines by (None by default, which means no filtering). Returns: - List[str]: A list of lines of text that match the specified criteria. ``` Input 1. `msg` (email.message.EmailMessage): The email message object to be processed. 2. `maintype` (str): Main MIME type of subparts to match. Defaults to \'text\'. 3. `subtype` (str): Specific subtype of subparts to match. Optional. 4. `keyword` (str): The keyword used to filter lines within the matching subparts. If `None`, no filtering is applied. Output - Returns a list of strings, where each string is a line of text extracted from the matching subparts that contain the specified keyword (if provided). Constraints - You may assume that the input `msg` is a valid email message object. - Your solution should handle any nested structures within the email message. - If no subparts match the specified MIME type, or if no lines contain the specified keyword, return an empty list. Example ```python from email.message import EmailMessage # Constructing a sample email message for demonstration msg = EmailMessage() msg.set_content(\\"This is the plain text part of the email.\\") msg.add_alternative(\\"<html><body>This is an HTML part of the email.</body></html>\\", subtype=\'html\') result = extract_subpart_lines(msg, maintype=\'text\', subtype=\'plain\', keyword=\'text\') assert result == [\\"This is the plain text part of the email.\\"] ``` Your task is to implement the `extract_subpart_lines` function such that it correctly processes the `msg` object as described.","solution":"from email.message import Message from typing import List def extract_subpart_lines(msg: Message, maintype: str=\'text\', subtype: str=None, keyword: str=None) -> List[str]: Process the email message object to extract lines of text from subparts that match the given MIME type. Only lines that contain a specified keyword should be extracted. Parameters: - msg: The email message object to be processed. - maintype (str): The main MIME type of the subparts to match (\'text\' by default). - subtype (str): The specific subtype of the subparts to match (optional). - keyword (str): The keyword to filter lines by (None by default, which means no filtering). Returns: - List[str]: A list of lines of text that match the specified criteria. lines = [] def process_part(part): if part.get_content_maintype() == maintype and (subtype is None or part.get_content_subtype() == subtype): payload = part.get_payload(decode=True) if payload: text = payload.decode(part.get_content_charset() or \'utf-8\') for line in text.splitlines(): if keyword is None or keyword in line: lines.append(line) if msg.is_multipart(): for part in msg.iter_parts(): process_part(part) else: process_part(msg) return lines"},{"question":"**Question:** You are working on a data visualization project to represent various datasets with different color palettes using Seaborn. Write a function `create_custom_palette` that demonstrates the use of the `mpl_palette` function explained in the supplied Seaborn documentation. # Function Signature: ```python def create_custom_palette(palette_name: str, num_colors: int, as_cmap: bool) -> \'Union[List[str], matplotlib.colors.ListedColormap]\': ``` # Input: - `palette_name` (str): The name of the palette you want to use. Examples include \\"viridis\\", \\"Set2\\", etc. - `num_colors` (int): The number of colors desired from the palette. If `as_cmap` is True, this argument can be ignored. - `as_cmap` (bool): If True, return the palette as a continuous colormap object. If False, return discrete color samples. # Output: - Returns either a list of color values (if `as_cmap` is False) or a continuous colormap object (if `as_cmap` is True). # Constraints: - Ensure the palette name provided is recognized by Seaborn. - The number of colors (`num_colors`) should be a positive integer. - Handle edge cases where an invalid palette name is provided by raising an appropriate exception. # Example: ```python # Discrete colors from qualitative palette colors = create_custom_palette(\\"Set2\\", 6, False) print(colors) # Output: A list of 6 color values # Continuous colormap from sequential palette colormap = create_custom_palette(\\"viridis\\", 5, True) print(colormap) # Output: A continuous colormap object of type matplotlib.colors.ListedColormap ``` # Notes: - Utilize `sns.mpl_palette` function along with the provided parameters to create the palettes. - Ensure to use exception handling to manage invalid palette names or inappropriate use of function parameters. This task requires students to demonstrate their understanding of Seaborn\'s `mpl_palette` function, their ability to handle function parameters, and their skill in managing different output formats based on the function\'s requirements.","solution":"import seaborn as sns import matplotlib.colors def create_custom_palette(palette_name: str, num_colors: int, as_cmap: bool) -> \'Union[List[str], matplotlib.colors.ListedColormap]\': Creates a custom palette using Seaborn\'s mpl_palette function. Parameters: palette_name (str): The name of the palette to use (e.g., \\"viridis\\", \\"Set2\\"). num_colors (int): Number of colors required from the palette. as_cmap (bool): If True, returns a continuous colormap. If False, returns a list of color values. Returns: Union[List[str], matplotlib.colors.ListedColormap]: A list of color values or a continuous colormap object. try: if as_cmap: return sns.color_palette(palette_name, as_cmap=True) else: return sns.color_palette(palette_name, num_colors) except ValueError as e: raise ValueError(f\\"Invalid palette name \'{palette_name}\'. Please provide a valid palette name.\\") from e"},{"question":"Objective Write a program to parallelize the training of a scikit-learn model and evaluate its performance under different configurations using the `n_jobs` parameter and environment variables. The goal is to understand and demonstrate how various levels of parallelism affect model training time and efficiency. Requirements 1. Implement a function `parallel_train_evaluate` which: - Takes in a dataset, a scikit-learn estimator, a dictionary of parameters for the estimator, and a range of values for `n_jobs`. - For each value in the `n_jobs` range, trains the estimator using that value, measures, and prints the training time. - Compares the performance of models trained with different `n_jobs` configurations and outputs the best `n_jobs` value in terms of training time. 2. Additionally, use environment variables to demonstrate controlling lower-level parallelism within the function. - Specifically set the `OMP_NUM_THREADS` environment variable and observe the impact on training time. - Print the training times with different `OMP_NUM_THREADS` settings. Input and Output Formats - Input: - `X`: Feature matrix of the dataset (numpy array or pandas DataFrame). - `y`: Target vector of the dataset (numpy array or pandas Series). - `estimator`: A scikit-learn estimator object (e.g., `RandomForestClassifier`, `LogisticRegression`). - `param_dict`: Dictionary of parameters for the given estimator. - `n_jobs_range`: List or range of integers representing different values for `n_jobs` to be tested. - Output: - Dictionary with keys as `n_jobs` values and values as training times. - The best `n_jobs` value with the least training time. Constraints - Use a dataset that is large enough to observe differences in training times with different parallel configurations. - If possible, include error handling for cases where the estimator does not support the `n_jobs` parameter. Sample Usage ```python import numpy as np from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier def parallel_train_evaluate(X, y, estimator, param_dict, n_jobs_range): # Implement this function as per the requirements # Generate a sample dataset X, y = make_classification(n_samples=10000, n_features=20, random_state=42) # Example estimator and parameter dictionary estimator = RandomForestClassifier param_dict = {\'n_estimators\': 100, \'max_depth\': 10} # Range of n_jobs to test n_jobs_range = range(1, 9) # Call the function parallel_train_evaluate(X, y, estimator, param_dict, n_jobs_range) ``` This question requires students to demonstrate their understanding of parallelism in scikit-learn, particularly with joblib, and how to control and measure its impact on training times. The focus is on practical application of the tools provided in the scikit-learn package for performance optimization.","solution":"import os import time from sklearn.model_selection import cross_val_score import json def parallel_train_evaluate(X, y, estimator, param_dict, n_jobs_range, omp_num_threads=None): Train and evaluate a scikit-learn model in parallel using different n_jobs configurations. Parameters: X (numpy.ndarray or pandas.DataFrame): Feature matrix. y (numpy.ndarray or pandas.Series): Target vector. estimator (sklearn.base.BaseEstimator): A scikit-learn estimator object. param_dict (dict): Dictionary of parameters for the estimator. n_jobs_range (list or range): Values of n_jobs to test. omp_num_threads (list or None): Values of OMP_NUM_THREADS to test, default is None. Returns: dict: Dictionary with n_jobs as keys and training times as values. int: The best n_jobs value with the least training time. best_time = float(\'inf\') best_jobs = None results = {} if omp_num_threads: omp_results = {} for num_threads in omp_num_threads: os.environ[\'OMP_NUM_THREADS\'] = str(num_threads) for n_jobs in n_jobs_range: param_dict[\'n_jobs\'] = n_jobs model = estimator(**param_dict) start_time = time.time() cross_val_score(model, X, y, cv=5) end_time = time.time() train_time = end_time - start_time results[n_jobs] = train_time if num_threads not in omp_results: omp_results[num_threads] = {} omp_results[num_threads][n_jobs] = train_time if train_time < best_time: best_time = train_time best_jobs = n_jobs print(f\\"OMP_NUM_THREADS={num_threads} | Training times: {json.dumps(results, indent=2)}\\") else: for n_jobs in n_jobs_range: param_dict[\'n_jobs\'] = n_jobs model = estimator(**param_dict) start_time = time.time() cross_val_score(model, X, y, cv=5) end_time = time.time() train_time = end_time - start_time results[n_jobs] = train_time if train_time < best_time: best_time = train_time best_jobs = n_jobs print(f\\"Training times: {json.dumps(results, indent=2)}\\") return results, best_jobs"},{"question":"Objective: Implement a Python module that effectively demonstrates the comprehension of exception handling mechanisms. You will write functions to raise custom exceptions, handle those exceptions, and provide pertinent details (such as traceback and error messages). Task: 1. **Create a CustomException** - Define a custom exception class `CustomException` that accepts an error message and stores it. This class should inherit from Python\'s base `Exception` class. 2. **Function to Raise Exception** - Implement a function `raise_exception()` that raises a `CustomException` with a descriptive error message. 3. **Function to Handle Exception and Print Details** - Implement a function `handle_exception()` that calls `raise_exception()`, catches the `CustomException`, and prints out the following: - The type of the exception caught. - The error message specified when raising the exception. - The traceback information. 4. **Function to Demonstrate Recursion Control** - Implement a function `recursive_function(limit)` that demonstrates recursion control: - The function should call itself recursively until a specified `limit` is reached. - If the depth of recursion reaches the `limit`, it should raise a `RecursionError` with a custom message. 5. **Function to Query Error State** - Implement a function `query_error_state()` that: - Calls a function that does not exist to deliberately cause a `NameError`. - Query the error state after the exception is raised. - Print whether an error occurred and if the error is a `NameError`. Functions Breakdown: - `class CustomException(Exception):` - `__init__(self, message: str)` - Store the error message. - `def raise_exception():` - Raise a `CustomException` with a descriptive error message. - `def handle_exception():` - Call `raise_exception()`. - Catch the `CustomException`. - Print the exception type, message, and traceback. - `def recursive_function(limit: int):` - Recursively call itself until `limit`. - Raise `RecursionError` when `limit` is reached. - `def query_error_state():` - Perform an operation that causes `NameError`. - Query the error state to check and print if a `NameError` occurred. Expected Output Example: ```python def main(): try: handle_exception() except CustomException as e: print(f\\"Caught an exception: {e}\\") try: recursive_function(5) except RecursionError as e: print(f\\"Recursion error: {e}\\") query_error_state() if __name__ == \\"__main__\\": main() ``` **Constraints:** - Python 3.6+ - Ensure the implementation covers the use of custom exceptions, recursion control, and querying error states.","solution":"import traceback class CustomException(Exception): def __init__(self, message: str): super().__init__(message) self.message = message def raise_exception(): raise CustomException(\\"This is a custom exception with a detailed error message\\") def handle_exception(): try: raise_exception() except CustomException as e: print(f\\"Exception type: {type(e).__name__}\\") print(f\\"Error message: {e.message}\\") print(\\"Traceback information:\\") traceback.print_exc() def recursive_function(limit: int, count=0): if count >= limit: raise RecursionError(f\\"Recursion limit of {limit} reached\\") print(f\\"Recursion depth: {count}\\") return recursive_function(limit, count + 1) def query_error_state(): try: non_existent_function() except Exception as e: is_name_error = isinstance(e, NameError) print(f\\"Error occurred: True\\") print(f\\"Is error a NameError: {is_name_error}\\")"},{"question":"**Question:** **Implement and Manage Remote References (RRefs) using PyTorch Distributed RPC** # Objective: Demonstrate your understanding of Remote References (RRefs) by implementing, sharing, and managing RRef instances in a distributed PyTorch environment. # Problem Statement: You are given a distributed PyTorch setup with three workers: \'worker_A\', \'worker_B\', and \'worker_C\'. Your task is to create a Remote Reference (RRef) on \'worker_A\', perform some operations by sharing this RRef across the workers, and ensure proper management of the RRef lifecycle. # Requirements: 1. Create an RRef on \'worker_A\' that references a tensor containing ones with shape (3, 3). 2. Share this RRef with \'worker_B\' and perform a remote addition operation on it (add a tensor of the same shape containing twos). 3. Further share this RRef from \'worker_B\' to \'worker_C\' and perform another remote multiplication operation on it (multiply by a tensor of the same shape containing threes). 4. Ensure proper acknowledgment and deletion protocols to manage the RRef\'s lifecycle without premature deletion. # Implementation Details: 1. Define the tensors and functions to perform the addition and multiplication operations. 2. Use `torch.distributed.rpc` for remote operations and RRef management. 3. Implement guarantee mechanisms: - Ensure \'worker_A\' keeps RRef alive until receiving acknowledgment from \'worker_B\'. - Ensure \'worker_B\' keeps RRef alive until receiving acknowledgment from \'worker_C\'. 4. Implement error handling to manage transient network failures according to the RRef protocol assumptions. # Input and Output: - **Input:** No direct input from the user is needed. Your script should set up the distributed environment and manage RRefs as described. - **Output:** Output the resulting tensor on \'worker_C\' after all remote operations have been performed. # Constraints: - Assume that networking conditions can cause delays, but overall connectivity is maintained. - The user-defined functions (UDFs) for addition and multiplication are non-idempotent (should only be executed once). # Example Usage: ```python import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def add_tensors(t1, t2): return t1 + t2 def multiply_tensors(t1, t2): return t1 * t2 # On \'worker_A\' rref = rpc.remote(\'worker_A\', torch.ones, args=((3, 3),)) # On \'worker_B\' result_rref = rpc.rpc_sync(\'worker_B\', add_tensors, args=(rref, torch.tensor([[2, 2, 2], [2, 2, 2], [2, 2, 2]]))) # On \'worker_C\' final_result_rref = rpc.rpc_sync(\'worker_C\', multiply_tensors, args=(result_rref, torch.tensor([[3, 3, 3], [3, 3, 3], [3, 3, 3]]))) # Output the final result tensor on \'worker_C\' print(final_result_rref.to_here()) ``` Complete the implementation ensuring RRef lifecycle management and adherence to the RRef protocol guarantees.","solution":"import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def add_tensors(t1, t2): return t1 + t2 def multiply_tensors(t1, t2): return t1 * t2 def worker_a_task(): # Create an RRef to a tensor of ones on worker_A rref = RRef(torch.ones((3, 3))) return rref def worker_b_task(rref): # Perform remote addition operation on worker_B result_tensor_rref = rref.rpc_sync().add_(torch.full((3, 3), 2)) return result_tensor_rref def worker_c_task(add_rref): # Perform remote multiplication operation on worker_C final_result_rref = add_rref.rpc_sync().mul_(torch.full((3, 3), 3)) return final_result_rref.to_here() def run_worker(rank, world_size): rpc.init_rpc(f\\"worker_{rank}\\", rank=rank, world_size=world_size) if rank == 0: rref = worker_a_task() elif rank == 1: rref_worker_a = rpc.remote(\\"worker_0\\", worker_a_task) result_rref = worker_b_task(rref_worker_a) else: rref_worker_a = rpc.remote(\\"worker_0\\", worker_a_task) result_rref_worker_b = rpc.remote(\\"worker_1\\", worker_b_task, args=(rref_worker_a,)) final_result = worker_c_task(result_rref_worker_b) print(\\"Final Result Tensor on worker_C:n\\", final_result) rpc.shutdown() if __name__ == \\"__main__\\": world_size = 3 rpc.init_rpc(\\"worker_0\\", rank=0, world_size=world_size) rpc.init_rpc(\\"worker_1\\", rank=1, world_size=world_size) rpc.init_rpc(\\"worker_2\\", rank=2, world_size=world_size) run_worker(0, world_size) # This would be done in parallel on different processes in reality run_worker(1, world_size) # This would be done in parallel on different processes in reality run_worker(2, world_size) # This would be done in parallel on different processes in reality"},{"question":"**Problem Statement:** You are required to create a classification model using the scikit-learn library to predict whether a person makes over 50K a year based on census data (commonly known as the \\"Adult\\" dataset). Your task is to implement a function that takes in training data and a test dataset, builds a supervised machine learning model, and returns the accuracy of this model on the test data. The function signature should be as follows: ```python def census_income_prediction(train_data: pd.DataFrame, test_data: pd.DataFrame, target_column: str) -> float: ``` # Input: - `train_data`: a Pandas DataFrame containing the training data, including features and the target column. - `test_data`: a Pandas DataFrame containing the test data, including features and the target column. - `target_column`: a string representing the name of the target column in both `train_data` and `test_data`. # Output: - Returns a float representing the accuracy of the model on the test data. # Constraints: - You must use the scikit-learn library. - You may choose any suitable classification algorithm(s) provided by scikit-learn. - You should preprocess the data, handling any missing values and converting categorical variables appropriately. - You should split the training data for model validation (using methods such as cross-validation). - Optimization of hyperparameters, if used, should be done thoughtfully to avoid overfitting. # Example: ```python import pandas as pd # Example training data train_data = pd.DataFrame({ \'age\': [25, 45, 50, 23, 43], \'workclass\': [\'Private\', \'Self-emp-not-inc\', \'Private\', \'Private\', \'Public\'], \'education\': [\'Bachelors\', \'Bachelors\', \'HS-grad\', \'Masters\', \'HS-grad\'], \'occupation\': [\'Tech-support\', \'Craft-repair\', \'Other-service\', \'Exec-managerial\', \'Sales\'], \'hours-per-week\': [40, 50, 60, 45, 40], \'salary\': [\'<=50K\', \'>50K\', \'>50K\', \'<=50K\', \'>50K\'] }) # Example test data test_data = pd.DataFrame({ \'age\': [35, 33], \'workclass\': [\'State-gov\', \'Private\'], \'education\': [\'Bachelors\', \'HS-grad\'], \'occupation\': [\'Adm-clerical\', \'Other-service\'], \'hours-per-week\': [38, 50], \'salary\': [\'<=50K\', \'>50K\'] }) # Example call to the function accuracy = census_income_prediction(train_data, test_data, \'salary\') print(accuracy) ``` **Notes:** - Ensure that your code is well-documented and follows good coding practices. - Include all necessary imports in your code as part of your function. - Make sure your function handles potential edge cases, such as missing values or unexpected input formats.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def census_income_prediction(train_data: pd.DataFrame, test_data: pd.DataFrame, target_column: str) -> float: Trains a classification model to predict whether a person makes over 50K a year based on census data, and evaluates the model on test data. Parameters: train_data (pd.DataFrame): Training data including features and the target column. test_data (pd.DataFrame): Test data including features and the target column. target_column (str): The name of the target column in both train_data and test_data. Returns: float: Accuracy of the model on the test data. # Split features and target X_train = train_data.drop(columns=[target_column]) y_train = train_data[target_column] X_test = test_data.drop(columns=[target_column]) y_test = test_data[target_column] # List of numerical and categorical columns numerical_features = X_train.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X_train.select_dtypes(include=[\'object\']).columns # Define preprocessor pipeline numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Define the model pipeline model = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier(random_state=42)) ]) # Train the model model.fit(X_train, y_train) # Predict on test data y_pred = model.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Problem Statement:** You are given a dataset of employees which includes their start dates, end dates, and their department. The dataset may contain missing values, represented by `NA` or `NaT`. Your task is to implement a function `process_employee_data` that takes in this dataset and performs the following tasks: 1. Fill missing start and end dates with a default start date (\'2000-01-01\') and end date (current date) respectively. 2. Fill missing department values with the string \'Unassigned\'. 3. Calculate the tenure of each employee in days (difference between end date and start date). 4. Return a DataFrame with the updated and processed information, which includes an additional \'tenure\' column. **Input Format:** - A pandas DataFrame `df` with the following columns: - `start_date` (type: datetime64[ns], may contain `NaT`) - `end_date` (type: datetime64[ns], may contain `NaT`) - `department` (type: string, may contain `NA`) **Output Format:** - A pandas DataFrame with the following columns: - `start_date` (type: datetime64[ns], no missing values) - `end_date` (type: datetime64[ns], no missing values) - `department` (type: string, no missing values) - `tenure` (type: int, representing the number of days) **Constraints:** - Use `pandas` version 1.0 or above. - The `tenure` should be calculated as the difference in days between `end_date` and `start_date`. **Example:** ```python import pandas as pd import numpy as np # Sample DataFrame data = { \'start_date\': [pd.Timestamp(\'2021-01-15\'), None, pd.Timestamp(\'2019-08-20\'), pd.Timestamp(\'2020-03-01\')], \'end_date\': [None, pd.Timestamp(\'2021-06-15\'), pd.Timestamp(\'2020-08-20\'), None], \'department\': [\'HR\', np.nan, \'IT\', \'Finance\'] } df = pd.DataFrame(data) # Function to process the employee data def process_employee_data(df): current_date = pd.Timestamp(\'today\') default_start_date = pd.Timestamp(\'2000-01-01\') df[\'start_date\'].fillna(default_start_date, inplace=True) df[\'end_date\'].fillna(current_date, inplace=True) df[\'department\'].fillna(\'Unassigned\', inplace=True) df[\'tenure\'] = (df[\'end_date\'] - df[\'start_date\']).dt.days return df # Processed DataFrame processed_df = process_employee_data(df) print(processed_df) ``` Expected output: ``` start_date end_date department tenure 0 2021-01-15 2023-10-25 HR 1013 1 2000-01-01 2021-06-15 Unassigned 7852 2 2019-08-20 2020-08-20 IT 366 3 2020-03-01 2023-10-25 Finance 1333 ```","solution":"import pandas as pd import numpy as np def process_employee_data(df): Processes the employee data by filling missing values, calculating tenure and returning the updated DataFrame. Args: df (pd.DataFrame): DataFrame containing employee data with columns \'start_date\', \'end_date\', and \'department\'. Returns: pd.DataFrame: Processed DataFrame with filled missing values and calculated \'tenure\'. current_date = pd.Timestamp(\'today\').normalize() default_start_date = pd.Timestamp(\'2000-01-01\').normalize() df[\'start_date\'].fillna(default_start_date, inplace=True) df[\'end_date\'].fillna(current_date, inplace=True) df[\'department\'].fillna(\'Unassigned\', inplace=True) df[\'tenure\'] = (df[\'end_date\'] - df[\'start_date\']).dt.days return df"},{"question":"**Objective:** To evaluate your understanding of the seaborn library, particularly the `relplot` function, and your ability to create and customize complex plots. **Question:** You are given a dataset containing information about car performance. Your task is to visualize the relationship between various features of cars using seaborn\'s `relplot`. 1. Load the dataset provided in the following format: ```csv Car,MPG,Cylinders,Displacement,Horsepower,Weight,Acceleration,ModelYear,Origin \\"Chevrolet Chevelle Malibu\\",18.0,8,307.0,130.0,3504.,12.0,70,1 \\"Buick Skylark 320\\",15.0,8,350.0,165.0,3693.,11.5,70,1 ... ``` 2. Using `relplot`, create a scatter plot to visualize the relationship between \'Horsepower\' and \'MPG\', using \'Origin\' for the color (`hue`), and facet the plot by \'Cylinders\'. 3. Additionally, create a line plot to visualize the trend of \'MPG\' over \'ModelYear\' for different \'Origin\' values. 4. Customize the appearance of the plots by: - Setting the theme to \\"darkgrid\\". - Adjusting the height and aspect ratio of the plots. - Adding meaningful titles and axis labels to each grid. Your solution should be a function `visualize_cars(filename)`, which takes the filename of the dataset as input and generates the required plots. **Input:** - A string `filename` representing the path to the dataset file. **Output:** - The function should display the plots but does not need to return any values. **Constraints:** - Ensure the dataset file is in CSV format and located in the same directory as your script. **Performance Requirements:** - The function should efficiently load and process the dataset. Example function signature: ```python def visualize_cars(filename: str): # Your code here ``` **Example Usage:** ```python # Assuming \'cars.csv\' is the dataset file visualize_cars(\'cars.csv\') ``` You are required to adhere to best coding practices and include necessary import statements in your solution. **Good luck!**","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_cars(filename: str): Visualizes car performance data using seaborn\'s relplot. Parameters: filename (str): The path to the CSV file containing the car dataset. # Load the dataset df = pd.read_csv(filename) # Set the theme sns.set_theme(style=\\"darkgrid\\") # Create a scatter plot with relplot scatter_plot = sns.relplot( data=df, x=\\"Horsepower\\", y=\\"MPG\\", hue=\\"Origin\\", col=\\"Cylinders\\", kind=\\"scatter\\", height=4, aspect=1 ) scatter_plot.set_titles(\\"{col_name} Cylinders\\") scatter_plot.set_axis_labels(\\"Horsepower\\", \\"Miles Per Gallon (MPG)\\") scatter_plot.figure.suptitle(\\"Relationship between Horsepower and MPG by Cylinders\\", y=1.05) # Create a line plot with relplot line_plot = sns.relplot( data=df, x=\\"ModelYear\\", y=\\"MPG\\", hue=\\"Origin\\", kind=\\"line\\", height=4, aspect=1.5 ) line_plot.set_axis_labels(\\"Model Year\\", \\"Miles Per Gallon (MPG)\\") line_plot.fig.suptitle(\\"Trend of MPG over Model Years by Origin\\", y=1.05) # Show plots plt.show()"},{"question":"# Advanced File Management and Archiving Utility **Objective:** Create a Python utility using the `shutil` module that performs the following tasks: 1. Copies a directory and its contents from a source to a destination. 2. Archives the copied directory into a specified archive format. 3. Ensures that all file metadata (permissions, timestamps) is preserved during the copy process. **Function Specification:** - **Function Name:** `manage_and_archive_directory` - **Input:** - `src_dir` (str): The path of the source directory to copy. - `dst_dir` (str): The path where the directory should be copied. - `archive_name` (str): The base name, including the path, for the archive file. - `archive_format` (str): The format of the archive. Supported formats are `zip`, `tar`, `gztar`, `bztar`, `xztar`. - **Output:** - Path to the created archive file (str). - **Constraints:** - The source directory must exist. - The provided archive format must be supported by `shutil`. **Example Usage:** ```python src_dir = \'/path/to/source/directory\' dst_dir = \'/path/to/destination/directory\' archive_name = \'/path/to/archive\' archive_format = \'gztar\' archive_path = manage_and_archive_directory(src_dir, dst_dir, archive_name, archive_format) print(f\\"Archive created at: {archive_path}\\") ``` **Requirements:** 1. Use `shutil.copytree` to copy the directory from `src_dir` to `dst_dir`. 2. Use `shutil.make_archive` to create an archive of the copied directory. 3. Ensure that all file metadata is preserved during the copy process. **Performance Considerations:** - The function should handle large directory trees efficiently. - Use platform-specific fast-copy methods if applicable. **Implementation Notes:** - If `copytree` encounters errors, the function should handle these gracefully and provide meaningful error messages. - Validate input parameters and ensure all necessary preconditions are met before performing operations. **Hint:** Refer to the `shutil.copytree` and `shutil.make_archive` functions in the provided documentation for details on their usage.","solution":"import shutil import os from pathlib import Path def manage_and_archive_directory(src_dir, dst_dir, archive_name, archive_format): Copies a directory and its contents, preserving metadata, and archives it. Parameters: - src_dir (str): Source directory path. - dst_dir (str): Destination directory path. - archive_name (str): The base name for the archive file. - archive_format (str): Archive format (e.g., \'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\'). Returns: - str: Path to the created archive file. # Ensure the source directory exists if not os.path.exists(src_dir): raise FileNotFoundError(f\\"Source directory \'{src_dir}\' does not exist.\\") # Validate archive format supported_formats = [\'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\'] if archive_format not in supported_formats: raise ValueError(f\\"Unsupported archive format \'{archive_format}\'. Supported formats: {supported_formats}\\") # Copy directory with metadata shutil.copytree(src_dir, dst_dir, copy_function=shutil.copy2, dirs_exist_ok=True) # Create archive archive_path = shutil.make_archive(archive_name, archive_format, dst_dir) return archive_path"},{"question":"You are working as a system administrator in a Unix-based environment that uses Sun\'s NIS (Yellow Pages) for central administration of several hosts. The NIS setup allows you to centrally maintain and distribute configuration information. Your task is to implement a function `retrieve_nis_info(mapname, key=None)` that retrieves and returns information from the NIS based on the given parameters: 1. If only `mapname` is provided, the function should return the entire contents of the map as a dictionary using `nis.cat()`. 2. If both `mapname` and `key` are provided, the function should return the value corresponding to the given key from the specified map using `nis.match()`. 3. In case of any errors such as the map or key not existing, your function should return a meaningful error message. Additionally, implement a helper function `list_valid_maps()` that returns a list of all valid NIS maps using `nis.maps()`. **Function Signatures:** ```python def retrieve_nis_info(mapname: str, key: str = None): pass def list_valid_maps(): pass ``` **Usage Example:** ```python # Example usage of retrieve_nis_info: print(retrieve_nis_info(\'passwd.byname\', \'jdoe\')) # Expected output: b\'john Doe\' (or appropriate value for the user \'jdoe\') print(retrieve_nis_info(\'passwd.byname\')) # Expected output: {b\'jdoe\': b\'john Doe\', ...} (dictionary of all entries in the map) # Example usage of list_valid_maps: print(list_valid_maps()) # Expected output: [list of valid NIS maps] ``` **Constraints:** - Raise appropriate exceptions or return error messages when the given map or key does not exist. - Ensure that your solution is compatible with Unix systems, as `nis` module only exists on Unix. - Assume that the default domain is used unless otherwise specified. **Performance Requirements:** - Handle large maps efficiently without causing excessive memory usage or slowdown. Ensure that your code is robust and handles edge cases appropriately.","solution":"import nis def retrieve_nis_info(mapname: str, key: str = None): Retrieve information from the NIS based on the given parameters. Parameters: mapname (str): The name of the NIS map. key (str, optional): The specific key to look up in the map. Returns: dict or str or None: The entire map as a dictionary, the value for the specified key, or an error message. try: if key is None: return nis.cat(mapname) else: return nis.match(key, mapname).decode(\'utf-8\') except nis.error as e: return f\\"Error: {str(e)}\\" def list_valid_maps(): Return a list of all valid NIS maps. Returns: list: List of valid NIS maps. try: return nis.maps() except nis.error as e: return f\\"Error: {str(e)}\\""},{"question":"Coding Assessment Question # Objective: Design a function that demonstrates your understanding of `asyncio.Future` and its associated methods and functionality. You will create a system that gathers results from multiple asynchronous operations concurrently and process them once all operations are complete. # Task: Implement the function `collect_future_results(async_funcs, delay, loop=None)`. # Function Signature: ```python import asyncio async def collect_future_results(async_funcs, delay, loop=None): # Implementation here ``` # Input: - `async_funcs`: A list of functions that return an `asyncio.Future` object when called. - `delay`: An integer representing the delay in seconds before each `Future` is set with a result. - `loop`: An optional argument specifying the event loop to use. If not provided, the current running loop should be used. # Output: - Returns a list of results from the `Future` objects. # Constraints: - All `async_funcs` functions return an `asyncio.Future` object. - Each `asyncio.Future` object should be set with a result after `delay` seconds. - Ensure all `Future` objects are awaited and their results are collected in the same order as their corresponding functions in the `async_funcs` list. # Detailed Requirements: 1. **Creating Futures**: For each function in `async_funcs`, create and schedule an asynchronous task that sets its `Future` result after a delay. 2. **Async Function Handling**: Ensure that you correctly handle the establishment and completion of `Future` objects asynchronously. 3. **Loop Management**: If `loop` is not specified, use the current running event loop. 4. **Return Results**: Collect the results of all `Future` objects and return them in a list. # Example Usage: Here is an example of how the `collect_future_results` function might be used: ```python import asyncio async def example_future(): return asyncio.Future() async def main(): async_funcs = [example_future for _ in range(3)] results = await collect_future_results(async_funcs, 1) print(results) # Should print a list of results from the Futures. asyncio.run(main()) ``` # Hints: - You can use `asyncio.sleep` to simulate the delay. - Use the `set_result` method on `Future` objects to set their result after the delay. - Ensure `asyncio.gather` or similar mechanisms are used to wait for all `Future` objects concurrently. Implement the function with the above specifications, ensuring efficient and clear use of `asyncio.Future` handling. Test your implementation to verify it works as expected with various inputs.","solution":"import asyncio async def complete_future(future, delay, result): await asyncio.sleep(delay) future.set_result(result) async def collect_future_results(async_funcs, delay, loop=None): if loop is None: loop = asyncio.get_event_loop() futures = [func() for func in async_funcs] tasks = [ loop.create_task(complete_future(future, delay, f\\"Result {i}\\")) for i, future in enumerate(futures) ] await asyncio.gather(*tasks) results = [future.result() for future in futures] return results"},{"question":"# Problem: Advanced File System Operations with Path Manipulations and Comparisons You are tasked with writing a Python function to help automate the process of filesystem clean-up and synchronization using `pathlib`, `filecmp`, and `shutil` modules. You need to implement a function that accepts two directory paths, compares their contents, and then replicates the directory structure and files of the first directory (`source_dir`) to the second directory (`target_dir`), but only if the files in `source_dir` have been modified more recently than those in `target_dir` or if they do not exist in `target_dir`. Function Signature ```python def sync_directories(source_dir: str, target_dir: str) -> None: pass ``` Input - `source_dir` (str): the path to the source directory. - `target_dir` (str): the path to the target directory. Output - The function does not return any value. It performs operations directly on the filesystem. Requirements - Use `pathlib` to handle filesystem paths. - Use `filecmp` to compare files and directories. - Use `shutil` for file copying and directory synchronization. - If a file exists in `source_dir` but not in `target_dir`, copy it to `target_dir`. - If a file in `source_dir` has been modified more recently than the corresponding file in `target_dir`, replace the older file in `target_dir` with the newer one from `source_dir`. - Recursively apply these rules to handle subdirectories. Constraints - Assume that both `source_dir` and `target_dir` are valid directories and contain regular files and subdirectories. - Assume the directories do not contain symbolic links (symlinks). # Example Suppose `source_dir` contains the following files and structure: ``` source_dir ├── file1.txt (modified: 2023-01-01) ├── file2.txt (modified: 2023-03-01) └── subdir └── file3.txt (modified: 2023-02-01) ``` And `target_dir` contains: ``` target_dir ├── file1.txt (modified: 2023-01-01) └── subdir └── file3.txt (modified: 2023-01-01) ``` After running `sync_directories(source_dir, target_dir)`, `target_dir` should be updated to: ``` target_dir ├── file1.txt (same as `source_dir`\'s file1.txt) ├── file2.txt (copied from `source_dir`) └── subdir └── file3.txt (same as `source_dir`\'s file3.txt) ``` # Notes - Ensure the function handles all exceptions appropriately and reports errors where necessary. - Avoid redundant copying by carefully comparing file timestamps.","solution":"from pathlib import Path import shutil import filecmp import os def sync_directories(source_dir: str, target_dir: str) -> None: source_path = Path(source_dir) target_path = Path(target_dir) # Ensure that target directory exists if not target_path.exists(): target_path.mkdir(parents=True) for src_item in source_path.iterdir(): tgt_item = target_path / src_item.name if src_item.is_dir(): sync_directories(src_item, tgt_item) # Recursively sync subdirectories else: # Handle files if not tgt_item.exists() or (src_item.stat().st_mtime > tgt_item.stat().st_mtime) or not filecmp.cmp(src_item, tgt_item, shallow=False): shutil.copy2(src_item, tgt_item)"},{"question":"**Advanced Coding Assessment: Manipulating Sets and Frozensets** In this assessment, you\'ll demonstrate your understanding of Python\'s set and frozenset objects by implementing functions to perform various operations on these collections. You are required to implement three distinct operations: symmetric difference, element frequency, and type verification. # Function 1: Symmetric Difference Implement a function `symmetric_difference` that takes two sets as input and returns their symmetric difference as a new set. ```python def symmetric_difference(set1: set, set2: set) -> set: Return the symmetric difference of two sets. Parameters: set1 (set): The first set. set2 (set): The second set. Returns: set: A new set containing the symmetric difference. pass ``` # Function 2: Element Frequency Implement a function `element_frequency` that takes a set and a list of elements as input and returns a dictionary where the keys are the elements from the list and the values are the frequency of each element within the set. ```python def element_frequency(s: set, elements: list) -> dict: Return the frequency of each element from the elements list in the set. Parameters: s (set): A set of elements. elements (list): A list of elements to check against the set. Returns: dict: A dictionary with elements as keys and their frequency as values. pass ``` # Function 3: Type Verification Implement a function `verify_set_type` that takes a Python object and returns a string indicating whether the object is a \\"set\\", \\"frozenset\\", or \\"neither\\". ```python def verify_set_type(obj) -> str: Check the type of a given object and return \'set\', \'frozenset\', or \'neither\'. Parameters: obj: The object to check. Returns: str: \'set\' if obj is a set, \'frozenset\' if obj is a frozenset, \'neither\' otherwise. pass ``` # Constraints - You should not use any external libraries. - Ensure your solution handles empty sets and lists. - Aim for efficient code considering the time complexity of operations on sets. # Example Usage ```python # Test symmetric_difference function assert symmetric_difference({1, 2, 3}, {3, 4, 5}) == {1, 2, 4, 5} # Test element_frequency function assert element_frequency({1, 2, 2, 3, 4}, [2, 3, 5]) == {2: 1, 3: 1, 5: 0} # Test verify_set_type function assert verify_set_type({1, 2, 3}) == \'set\' assert verify_set_type(frozenset([1, 2, 3])) == \'frozenset\' assert verify_set_type([1, 2, 3]) == \'neither\' ``` This question assesses your ability to work with Python\'s set and frozenset data structures, implement fundamental set operations, and distinguish between different types of collections.","solution":"def symmetric_difference(set1: set, set2: set) -> set: Return the symmetric difference of two sets. Parameters: set1 (set): The first set. set2 (set): The second set. Returns: set: A new set containing the symmetric difference. return set1.symmetric_difference(set2) def element_frequency(s: set, elements: list) -> dict: Return the frequency of each element from the elements list in the set. Parameters: s (set): A set of elements. elements (list): A list of elements to check against the set. Returns: dict: A dictionary with elements as keys and their frequency as values. return {element: (1 if element in s else 0) for element in elements} def verify_set_type(obj) -> str: Check the type of a given object and return \'set\', \'frozenset\', or \'neither\'. Parameters: obj: The object to check. Returns: str: \'set\' if obj is a set, \'frozenset\' if obj is a frozenset, \'neither\' otherwise. if isinstance(obj, set): return \'set\' elif isinstance(obj, frozenset): return \'frozenset\' else: return \'neither\'"},{"question":"**Title**: Creating and Managing Executable Python Zip Archives **Objective**: Demonstrate your understanding and ability to create and manage executable Python zip archives using the `zipapp` module. **Problem Statement**: You are tasked with creating an executable Python zip archive that bundles a simple Python application. This application will read a text file and print its contents reversed. Implement a function `create_executable_zip` to automate this process. **Requirements**: 1. Implement the function `create_executable_zip(source_dir: str, output_file: str, interpreter: str = None) -> None`. - **Parameters**: - `source_dir` (str): The directory containing your Python application files. - `output_file` (str): The output filename for the generated zip archive. - `interpreter` (Optional[str]): If provided, specifies the path to the Python interpreter for running the application. - **Functionality**: - The specified `source_dir` should contain a Python script `reverser.py` with the following code: ```python import sys def main(): filename = sys.argv[1] with open(filename, \'r\') as file: content = file.read() print(content[::-1]) if __name__ == \'__main__\': main() ``` - The function should create an executable zip archive of the contents of `source_dir`. - The resulting zip archive should be executable such that running `python output_file somefile.txt` will read `somefile.txt` and print its contents reversed. - (Optional) If the `interpreter` parameter is provided, configure the zip archive to use that interpreter. **Constraints**: - Assume `source_dir` and `output_file` are valid paths. - You may assume the provided `source_dir` contains the necessary Python files. - Ensure all necessary imports are included within your implementation. **Performance Requirements**: - The function should efficiently bundle the application, even for source directories containing multiple files. **Example Usage**: ```python create_executable_zip(\'/path/to/source\', \'myapp.pyz\') # Assuming myapp.pyz is created, running the following command: # python myapp.pyz somefile.txt # Should output the reversed contents of somefile.txt ``` **Notes**: - Make sure to handle all relevant exceptions and edge cases in your implementation, such as invalid directory paths or missing files. - Consider adding appropriate comments to illustrate your thought process.","solution":"import zipapp import os def create_executable_zip(source_dir: str, output_file: str, interpreter: str = None) -> None: Create an executable Python zip archive from the specified source directory. Parameters: - source_dir (str): The directory containing your Python application files. - output_file (str): The output filename for the generated zip archive. - interpreter (Optional[str]): If provided, specifies the path to the Python interpreter for running the application. # Ensure the source directory exists if not os.path.isdir(source_dir): raise ValueError(f\\"Source directory \'{source_dir}\' does not exist.\\") # Generate the zip archive zipapp.create_archive( source_dir, target=output_file, interpreter=interpreter, main=\'reverser:main\' )"},{"question":"# Python Programming Assessment Question: Implementing and Managing Data Classes with Context Managers **Objective:** Create a task management system using Python\'s `dataclasses` module to handle task details and `contextlib` to manage database-like transactions for adding or removing tasks. This exercise will assess your ability to utilize advanced Python modules for managing structured data and resources efficiently. **Problem Statement:** You are required to implement a system where tasks can be added or removed within a transactional context. If an exception occurs, the transaction should be rolled back, ensuring that tasks remain consistent. You will utilize `dataclasses` to define the task structure and `contextlib` to handle the transactional behavior. # Step-by-Step Instructions 1. **Task Data Class**: - Define a `Task` data class with the following attributes: - `task_id` (int): A unique identifier for each task. - `description` (str): A brief description of the task. - `completed` (bool): Status of the task (True if completed, False otherwise). 2. **Task Manager Class**: - Define a `TaskManager` class which maintains a list of tasks. - This class should provide methods to add (`add_task`) and remove (`remove_task`) tasks by `task_id`. - Additionally, it should have a method to display all tasks. 3. **Transaction Context Manager**: - Create a context manager using `contextlib` that will manage transactions. - This context manager should ensure that changes to the tasks are only committed if no exceptions are raised. If an exception occurs, changes should be rolled back (i.e., the task list should remain as it was before entering the context). # Function Signatures ```python from dataclasses import dataclass, field from contextlib import contextmanager from typing import List @dataclass class Task: task_id: int description: str completed: bool = False class TaskManager: def __init__(self): self.tasks: List[Task] = [] def add_task(self, task: Task): for t in self.tasks: if t.task_id == task.task_id: raise ValueError(f\\"Task with id {task.task_id} already exists.\\") self.tasks.append(task) def remove_task(self, task_id: int): self.tasks = [t for t in self.tasks if t.task_id != task_id] def list_tasks(self) -> List[Task]: return self.tasks @contextmanager def transaction(task_manager: TaskManager): original_tasks = task_manager.tasks.copy() try: yield # On successful completion of the \'with\' block except Exception: task_manager.tasks = original_tasks raise ``` # Example Usage ```python if __name__ == \\"__main__\\": task_manager = TaskManager() try: with transaction(task_manager): task_manager.add_task(Task(1, \\"Learn Python\\")) task_manager.add_task(Task(2, \\"Build a project\\")) # The following line should trigger a rollback task_manager.add_task(Task(1, \\"This should cause a rollback\\")) except ValueError as e: print(e) tasks = task_manager.list_tasks() print(tasks) # Should print an empty list because of the rollback ``` # Constraints: - Ensure the tasks have unique IDs. - Handle exceptions gracefully within the context manager to rollback state. # Performance Requirements: - The add and remove operations should be efficient. - The rollback mechanism should restore the state quickly without unnecessary computations. Good luck! Ensure your solution adheres to the requirements and efficiently utilizes the `dataclasses` and `contextlib` modules.","solution":"from dataclasses import dataclass, field from contextlib import contextmanager from typing import List @dataclass class Task: task_id: int description: str completed: bool = False class TaskManager: def __init__(self): self.tasks: List[Task] = [] def add_task(self, task: Task): for t in self.tasks: if t.task_id == task.task_id: raise ValueError(f\\"Task with id {task.task_id} already exists.\\") self.tasks.append(task) def remove_task(self, task_id: int): self.tasks = [t for t in self.tasks if t.task_id != task_id] def list_tasks(self) -> List[Task]: return self.tasks @contextmanager def transaction(task_manager: TaskManager): original_tasks = task_manager.tasks.copy() try: yield # On successful completion of the \'with\' block except Exception: task_manager.tasks = original_tasks raise"},{"question":"# Problem: Log File Analyzer You are provided with a large log file from a web server. Each entry in the log file contains information about a single request to the server. The format of each log entry is as follows: ``` IP_ADDRESS - - [DATE] \\"HTTP_METHOD URL HTTP_VERSION\\" STATUS_CODE SIZE ``` - `IP_ADDRESS`: The IP address of the client. - `DATE`: The date and time of the request. - `HTTP_METHOD`: The HTTP method (e.g., GET, POST). - `URL`: The requested URL. - `HTTP_VERSION`: The HTTP version. - `STATUS_CODE`: The HTTP status code returned (e.g., 200, 404). - `SIZE`: The size of the returned object (in bytes). Here is an example of few log entries: ``` 192.168.1.1 - - [10/Oct/2023:13:55:36 -0700] \\"GET /index.html HTTP/1.1\\" 200 512 127.0.0.1 - - [10/Oct/2023:13:57:09 -0700] \\"POST /submit-data HTTP/1.1\\" 404 0 203.0.113.5 - - [10/Oct/2023:14:00:22 -0700] \\"GET /assets/image.png HTTP/1.1\\" 200 154231 ``` # Objective: - Write a Python function `analyze_logs(logs: str) -> dict` that takes a string containing multiple lines of log entries as input and returns a dictionary with the following information: - `total_requests`: Total number of requests. - `average_size`: The average size of the returned objects. - `status_codes`: A dictionary where the keys are status codes and the values are the number of occurrences of each status code. # Constraints: - The log file string can be very large (up to 10^6 lines). - Each log entry is correctly formatted as per the given format. - You may assume that each log entry is on a new line. # Example: ```python logs = \'\'\'192.168.1.1 - - [10/Oct/2023:13:55:36 -0700] \\"GET /index.html HTTP/1.1\\" 200 512 127.0.0.1 - - [10/Oct/2023:13:57:09 -0700] \\"POST /submit-data HTTP/1.1\\" 404 0 203.0.113.5 - - [10/Oct/2023:14:00:22 -0700] \\"GET /assets/image.png HTTP/1.1\\" 200 154231\'\'\' result = analyze_logs(logs) print(result) ``` Expected Output: ```python { \'total_requests\': 3, \'average_size\': 51581.0, \'status_codes\': { \'200\': 2, \'404\': 1 } } ``` # Requirements: 1. Use regular expressions to parse the log entries. 2. Ensure your solution efficiently handles the given constraints. 3. Include error checking and handling for any potential anomalies in the log entries. # Notes: - Focus on using `re` module effectively. - Perform necessary validations and edge case handling where applicable.","solution":"import re from collections import defaultdict def analyze_logs(logs: str) -> dict: pattern = re.compile( r\'(d{1,3}(?:.d{1,3}){3}) - - [(.*?)] \\"(.*?)\\" (d{3}) (d+)\' ) total_requests = 0 total_size = 0 status_codes = defaultdict(int) for line in logs.splitlines(): match = pattern.match(line) if match: total_requests += 1 status_code = match.group(4) size = int(match.group(5)) status_codes[status_code] += 1 total_size += size average_size = total_size / total_requests if total_requests > 0 else 0 return { \'total_requests\': total_requests, \'average_size\': average_size, \'status_codes\': dict(status_codes) }"},{"question":"# PyTorch Deterministic Operations and Reproducibility Objective You are required to write a PyTorch program that demonstrates how to achieve reproducible results by controlling sources of randomness and handling nondeterministic behavior across various components including PyTorch, NumPy, and custom modules using random operations. Problem Description Write a Python script that does the following: 1. Sets a seed for random number generation for PyTorch, Python\'s `random` module, and NumPy to ensure reproducibility. 2. Creates a simple neural network in PyTorch. 3. Generates some random input data and passes it through the network. 4. Demonstrates the effect of using deterministic algorithms vs. nondeterministic algorithms in PyTorch. 5. Displays how to make DataLoader deterministic. Functions to Implement **Function 1: set_random_seeds(seed_value)** - Sets the random seed for PyTorch, NumPy, and Python\'s `random` module. - **Input**: `seed_value` (int) - the seed value. - **Output**: None. **Function 2: create_neural_network()** - Creates a simple feedforward neural network using PyTorch\'s `nn.Module`. - **Input**: None. - **Output**: A PyTorch neural network model. **Function 3: generate_random_input(batch_size, input_size)** - Generates random input data using PyTorch. - **Input**: - `batch_size` (int) - the size of the batch. - `input_size` (int) - the size of the input features. - **Output**: A PyTorch tensor with the specified batch size and input size. **Function 4: demonstrate_determinism(model, inputs)** - Demonstrates the difference between using deterministic and nondeterministic algorithms. - Prints the results and highlights the differences. - **Input**: - `model` - the neural network model. - `inputs` - the random input data. - **Output**: None. **Function 5: demonstrate_dataloader_determinism(dataset, batch_size)** - Creates a DataLoader to load the data deterministically. - **Input**: - `dataset` - a dataset (use any PyTorch dataset for simplicity). - `batch_size` (int) - the size of the batch. - **Output**: None. Example Usage ```python if __name__ == \\"__main__\\": seed_value = 42 set_random_seeds(seed_value) model = create_neural_network() inputs = generate_random_input(batch_size=10, input_size=20) demonstrate_determinism(model, inputs) from torchvision import datasets, transforms transform = transforms.Compose([transforms.ToTensor()]) dataset = datasets.FakeData(transform=transform) demonstrate_dataloader_determinism(dataset, batch_size=10) ``` Constraints - Ensure that all randomness sources are controlled. - Document each function with appropriate comments. - Use PyTorch version >= 1.8.0. - Use CUDA if available, otherwise use CPU. Performance Requirement - The implementation should be efficient and avoid unnecessary computations.","solution":"import torch import numpy as np import random def set_random_seeds(seed_value): Sets the random seed for PyTorch, NumPy, and Python\'s random module. :param seed_value: int - the seed value. torch.manual_seed(seed_value) torch.cuda.manual_seed(seed_value) torch.cuda.manual_seed_all(seed_value) # if GPUs are available np.random.seed(seed_value) random.seed(seed_value) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False def create_neural_network(): Creates a simple feedforward neural network using PyTorch\'s nn.Module. :return: A PyTorch neural network model. return torch.nn.Sequential( torch.nn.Linear(20, 50), torch.nn.ReLU(), torch.nn.Linear(50, 10), torch.nn.Softmax(dim=1) ) def generate_random_input(batch_size, input_size): Generates random input data using PyTorch. :param batch_size: int - the size of the batch. :param input_size: int - the size of the input features. :return: A PyTorch tensor with the specified batch size and input size. return torch.randn(batch_size, input_size) def demonstrate_determinism(model, inputs): Demonstrates the difference between using deterministic and nondeterministic algorithms. Prints the results and highlights the differences. :param model: the neural network model. :param inputs: the random input data. # Perform deterministic computation set_random_seeds(42) output1 = model(inputs) print(\\"Deterministic output:\\", output1) # Perform non-deterministic computation torch.backends.cudnn.deterministic = False torch.backends.cudnn.benchmark = True output2 = model(inputs) print(\\"Non-deterministic output:\\", output2) def demonstrate_dataloader_determinism(dataset, batch_size): Creates a DataLoader to load the data deterministically. :param dataset: a dataset (use any PyTorch dataset for simplicity). :param batch_size: int - the size of the batch. set_random_seeds(42) dataloader = torch.utils.data.DataLoader( dataset, batch_size=batch_size, shuffle=True, num_workers=0 ) for batch in dataloader: print(\\"Batch:\\", batch) break # Load and display only the first batch for demonstration if __name__ == \\"__main__\\": seed_value = 42 set_random_seeds(seed_value) model = create_neural_network() inputs = generate_random_input(batch_size=10, input_size=20) demonstrate_determinism(model, inputs) from torchvision import datasets, transforms transform = transforms.Compose([transforms.ToTensor()]) dataset = datasets.FakeData(transform=transform) demonstrate_dataloader_determinism(dataset, batch_size=10)"},{"question":"**Objective:** Write a function that utilizes scikit-learn\'s `GridSearchCV` to find the best hyperparameters for a Support Vector Classifier (SVC) model on a given dataset. **Function Signature:** ```python def tune_svc_hyperparameters(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> dict: pass ``` **Input:** - `X_train`: A numpy array of shape (n_train_samples, n_features) containing the training features. - `y_train`: A numpy array of shape (n_train_samples,) containing the training labels. - `X_test`: A numpy array of shape (n_test_samples, n_features) containing the test features. - `y_test`: A numpy array of shape (n_test_samples,) containing the test labels. **Output:** - A dictionary with the following keys: - `best_params`: The best parameters found by `GridSearchCV`. - `best_score`: The mean cross-validated score of the best estimator. - `test_accuracy`: The accuracy of the best estimator on the test set. **Constraints:** - Use the `SVC` model from scikit-learn with the following hyperparameter grid: ```python param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] ``` - Use 5-fold cross-validation for the `GridSearchCV`. - Use accuracy as the scoring metric. **Requirements:** 1. Initialize the `GridSearchCV` with the given `param_grid` and cross-validation settings. 2. Fit the `GridSearchCV` on the training data. 3. Extract the best parameters and the best cross-validation score. 4. Evaluate the best estimator on the test data to compute the test accuracy. **Example:** ```python import numpy as np # Example data X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]]) y_train = np.array([0, 0, 1, 1, 1, 0]) X_test = np.array([[2, 2], [3, 3], [4, 4]]) y_test = np.array([0, 1, 1]) # Function call result = tune_svc_hyperparameters(X_train, y_train, X_test, y_test) # Expected output (example) # result = { # \'best_params\': {\'C\': 1, \'kernel\': \'linear\'}, # \'best_score\': 0.83, # Example score # \'test_accuracy\': 0.67 # Example accuracy # } ``` The function should return a dictionary similar to the expected output above.","solution":"import numpy as np from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score def tune_svc_hyperparameters(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> dict: # Define the parameter grid param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] # Initialize the SVC model svc = SVC() # Initialize GridSearchCV with 5-fold cross-validation grid_search = GridSearchCV(svc, param_grid, cv=5, scoring=\'accuracy\') # Fit the GridSearchCV to the training data grid_search.fit(X_train, y_train) # Extract the best parameters and best cross-validation score best_params = grid_search.best_params_ best_score = grid_search.best_score_ # Evaluate the best estimator on the test data best_model = grid_search.best_estimator_ test_predictions = best_model.predict(X_test) test_accuracy = accuracy_score(y_test, test_predictions) # Return the results as a dictionary return { \'best_params\': best_params, \'best_score\': best_score, \'test_accuracy\': test_accuracy }"},{"question":"**Coding Assessment Question** # Objective: Implement a custom import system using the `importlib` package that can dynamically load a module from a given filesystem location and handle resource files within the module. # Problem Statement: You are given a Python source file located at `/path/to/module.py` and a resource file `data.txt` located within the same directory. Your task is to create a custom importer using the `importlib` package that: 1. Imports the given module dynamically. 2. Reads the `data.txt` resource file from the module\'s location and prints its content. # Instructions: 1. Implement a function `custom_importer(module_path: str, resource_name: str) -> None` that handles the import and resource reading. 2. The function should: - Load the module from the given `module_path`. - Use the functions from the `importlib` package to perform the import. - Access the resource file and print its content. # Constraints: - The `module_path` is guaranteed to be a valid path to a Python file. - The module may contain functions and classes that should not interfere with the resource reading. - Assume the `resource_name` is a file located in the same directory as the module. # Example: ```python # Suppose the following files exist in the directory /path/to: # module.py def greet(): print(\\"Hello, world!\\") # data.txt # This is a sample resource file. # Your function should dynamically import module.py and read data.txt. custom_importer(\'/path/to/module.py\', \'data.txt\') # Expected Output: # This is a sample resource file. ``` # Notes: - Ensure that the module is imported in a manner that allows its usage (e.g., calling functions within the module if needed). - Use appropriate `importlib` functions and classes like `importlib.util.spec_from_file_location`, `importlib.util.module_from_spec`, etc. - Handle any potential exceptions that might occur during the import and resource accessing process. - Do not use any external libraries; standard Python library imports are sufficient. # Submission: - Provide the full implementation of the `custom_importer` function. - Include any necessary supporting code to demonstrate its functionality.","solution":"import importlib.util import os def custom_importer(module_path: str, resource_name: str) -> None: Dynamically imports a module from the given file system location and reads a resource file. Args: - module_path (str): The file path to the module. - resource_name (str): The name of the resource file located in the module\'s directory. # Get the directory of the module module_dir = os.path.dirname(module_path) # Define the module name module_name = os.path.splitext(os.path.basename(module_path))[0] # Load the module dynamically spec = importlib.util.spec_from_file_location(module_name, module_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) # Construct the full path for the resource file resource_path = os.path.join(module_dir, resource_name) # Read and print the content of the resource file with open(resource_path, \'r\') as file: content = file.read() print(content)"},{"question":"# URL Shortening Service Implement a function named `shorten_url` that takes a long URL as input and returns its shortened version. Additionally, implement a function named `expand_url` that takes a shortened URL and returns the original long URL. The URL shortening should be achieved by using a fictional URL shortening API provided via HTTP requests. API Details - **Shorten URL:** - Endpoint: `http://example.com/api/shorten` - Method: `POST` - Request Body: `{\\"url\\": \\"<long_url>\\"}` - Response Body: `{\\"short_url\\": \\"<shortened_url>\\"}` - **Expand URL:** - Endpoint: `http://example.com/api/expand` - Method: `POST` - Request Body: `{\\"short_url\\": \\"<shortened_url>\\"}` - Response Body: `{\\"long_url\\": \\"<original_url>\\"}` Function Signatures ```python def shorten_url(long_url: str) -> str: This function takes a long URL as input and returns its shortened version by using the fictional URL shortening API. Parameters: long_url (str): The original long URL to be shortened. Returns: str: The shortened URL. Raises: URLError: If there is an error in the HTTP request. pass def expand_url(short_url: str) -> str: This function takes a shortened URL as input and returns the original long URL by using the fictional URL shortening API. Parameters: short_url (str): The shortened URL to be expanded. Returns: str: The original long URL. Raises: URLError: If there is an error in the HTTP request. pass ``` Constraints - Do not use any third-party library for making HTTP requests. Utilize `urllib.request` module from the standard library. - Properly handle any potential exceptions that may arise from the HTTP requests using `urllib.error`. Example ```python original_url = \\"http://www.examplelongurl.com/some/very/long/path\\" shortened_url = shorten_url(original_url) print(shortened_url) # Output: \\"http://short.url/abcd\\" expanded_url = expand_url(shortened_url) print(expanded_url) # Output: \\"http://www.examplelongurl.com/some/very/long/path\\" ```","solution":"import urllib.request import urllib.parse import json def shorten_url(long_url: str) -> str: This function takes a long URL as input and returns its shortened version by using the fictional URL shortening API. Parameters: long_url (str): The original long URL to be shortened. Returns: str: The shortened URL. Raises: URLError: If there is an error in the HTTP request. url = \\"http://example.com/api/shorten\\" data = json.dumps({\\"url\\": long_url}).encode(\'utf-8\') req = urllib.request.Request(url, data=data, headers={\'content-type\': \'application/json\'}) try: with urllib.request.urlopen(req) as response: result = json.loads(response.read().decode(\'utf-8\')) return result[\'short_url\'] except urllib.error.URLError as e: raise e def expand_url(short_url: str) -> str: This function takes a shortened URL as input and returns the original long URL by using the fictional URL shortening API. Parameters: short_url (str): The shortened URL to be expanded. Returns: str: The original long URL. Raises: URLError: If there is an error in the HTTP request. url = \\"http://example.com/api/expand\\" data = json.dumps({\\"short_url\\": short_url}).encode(\'utf-8\') req = urllib.request.Request(url, data=data, headers={\'content-type\': \'application/json\'}) try: with urllib.request.urlopen(req) as response: result = json.loads(response.read().decode(\'utf-8\')) return result[\'long_url\'] except urllib.error.URLError as e: raise e"},{"question":"# Async Task Manager **Objective:** To assess your understanding of Python\'s asyncio library, you will implement a function that concurrently runs coroutines with timeouts and handles potential cancellations. **Problem Statement:** You need to implement a function `async_task_manager(tasks, timeout)` that takes in: - `tasks`: A list of coroutine functions to run concurrently. - `timeout`: The maximum allowed time in seconds for all tasks to complete. The function should: 1. Run all given tasks concurrently. 2. Enforce the specified timeout for the overall operation. If the timeout is exceeded, cancel any pending tasks. 3. Return a dictionary with the result or exception for each task. **Input:** - `tasks`: A list of coroutine functions. Example: `[coro1, coro2, coro3]` - `timeout`: A float or integer representing the time limit in seconds. **Output:** - A dictionary where: - The keys are the task indices (0 to len(tasks) - 1). - The values are the results of the tasks if completed, or the string \\"Cancelled\\" if the task was cancelled due to timeout. **Constraints:** - The function should handle both the successful completion and cancellation of tasks gracefully. - You may assume each coroutine function when called returns a coroutine object. **Example:** ```python import asyncio async def task1(): await asyncio.sleep(2) return \\"Task 1 Completed\\" async def task2(): await asyncio.sleep(4) return \\"Task 2 Completed\\" async def task3(): await asyncio.sleep(6) return \\"Task 3 Completed\\" # Example usage tasks = [task1, task2, task3] timeout = 3 result = asyncio.run(async_task_manager(tasks, timeout)) print(result) ``` *Expected output:* ``` { 0: \\"Task 1 Completed\\", 1: \\"Cancelled\\", 2: \\"Cancelled\\" } ``` **Implementation Notes:** - Use `asyncio.create_task()` to run the tasks concurrently. - Use `asyncio.wait_for()` or similar to enforce the timeout. - Handle cancellations by ensuring that each task is properly cancelled and the result dictionary reflects this. Implement the function `async_task_manager` in the cell below: ```python import asyncio async def async_task_manager(tasks, timeout): try: task_coroutines = [asyncio.create_task(task()) for task in tasks] completed_tasks = await asyncio.wait(task_coroutines, timeout=timeout, return_when=asyncio.ALL_COMPLETED) result = {} for idx, task in enumerate(task_coroutines): if task in completed_tasks[0]: # Task completed result[idx] = task.result() if not task.cancelled() else \\"Cancelled\\" elif task in completed_tasks[1]: # Task pending task.cancel() result[idx] = \\"Cancelled\\" return result except Exception as e: raise e # Example Usage: # asyncio.run(async_task_manager([task1, task2, task3], 3)) ```","solution":"import asyncio async def async_task_manager(tasks, timeout): Run tasks concurrently with a specified timeout. If the overall timeout is exceeded, cancel pending tasks. Return a dictionary with the task outcomes. :param tasks: List of coroutine functions :param timeout: Maximum allowed time in seconds for all tasks to complete :return: Dictionary with the result or status for each task task_coroutines = [asyncio.create_task(task()) for task in tasks] try: await asyncio.wait_for(asyncio.gather(*task_coroutines), timeout=timeout) except asyncio.TimeoutError: pass result = {} for idx, task in enumerate(task_coroutines): if task.done(): if task.exception(): result[idx] = str(task.exception()) else: result[idx] = task.result() else: task.cancel() result[idx] = \\"Cancelled\\" return result"},{"question":"Objective Create an advanced enumeration by leveraging the `enum` module in Python. This will assess your understanding of the `Enum` class and its advanced features. Problem Statement You are tasked with implementing an enumeration that categorizes a set of planets in our solar system with additional properties and methods. You need to implement a custom enumeration `Planet` using the `enum` module. Your `Planet` enumeration should follow these requirements: 1. Each planet should have the following properties: - `mass` in kilograms. - `radius` in meters. 2. The enumeration should include a method `surface_gravity` to compute the surface gravity of the planet based on its mass and radius. Use the formula: g = frac{G times text{mass}}{text{radius}^2} where ( G = 6.673 times 10^{-11} , text{m}^3 , text{kg}^{-1} , text{s}^{-2} ). 3. The enumeration should also include: - A method `describe` that returns a tuple containing the planet’s name, mass, radius, and surface gravity. - Custom string and representation methods (`__str__` and `__repr__`) to display readable information about a planet. Implementation Details You should implement the `Planet` enumeration with at least the following members: - MERCURY with mass (3.303 times 10^{23}) kg and radius (2.4397 times 10^6) meters. - VENUS with mass (4.869 times 10^{24}) kg and radius (6.0518 times 10^6) meters. - EARTH with mass (5.976 times 10^{24}) kg and radius (6.37814 times 10^6) meters. - MARS with mass (6.421 times 10^{23}) kg and radius (3.3972 times 10^6) meters. - JUPITER with mass (1.9 times 10^{27}) kg and radius (7.1492 times 10^7) meters. - SATURN with mass (5.688 times 10^{26}) kg and radius (6.0268 times 10^7) meters. - URANUS with mass (8.686 times 10^{25}) kg and radius (2.5559 times 10^7) meters. - NEPTUNE with mass (1.024 times 10^{26}) kg and radius (2.4746 times 10^7) meters. Constraints - Use appropriate data types for mass and radius to maintain computational integrity. - Your `Planet` class should inherit from `Enum`. - Ensure that your methods efficiently compute the requested outputs. Input and Output There are no inputs from the user. You need to create the enumeration and implement the methods as specified. Here is the function signature you need to implement: ```python from enum import Enum class Planet(Enum): # Implement the required functionalities here ``` Example Usage ```python earth = Planet.EARTH print(earth.describe()) # Output: (\'EARTH\', 5.976e+24, 6378140.0, 9.802652743337129) print(str(earth)) # Output: \'Planet EARTH: mass = 5.976e+24 kg, radius = 6378140.0 m\' print(repr(earth)) # Output: \'Planet(name=\'EARTH\', mass=5.976e+24, radius=6378140.0, gravity=9.802652743337129)\' ```","solution":"from enum import Enum class Planet(Enum): MERCURY = (3.303e+23, 2.4397e+6) VENUS = (4.869e+24, 6.0518e+6) EARTH = (5.976e+24, 6.37814e+6) MARS = (6.421e+23, 3.3972e+6) JUPITER = (1.9e+27, 7.1492e+7) SATURN = (5.688e+26, 6.0268e+7) URANUS = (8.686e+25, 2.5559e+7) NEPTUNE = (1.024e+26, 2.4746e+7) def __init__(self, mass, radius): self.mass = mass self.radius = radius def surface_gravity(self): G = 6.673e-11 return G * self.mass / (self.radius ** 2) def describe(self): return (self.name, self.mass, self.radius, self.surface_gravity()) def __str__(self): return f\'Planet {self.name}: mass = {self.mass} kg, radius = {self.radius} m\' def __repr__(self): return f\'Planet(name={self.name!r}, mass={self.mass!r}, radius={self.radius!r}, gravity={self.surface_gravity()!r})\'"},{"question":"# Cross Decomposition with PLSCanonical Problem Statement: You are given two matrices `X` (predictors) and `Y` (responses). Your task is to implement a feature extraction function using the `PLSCanonical` method from the `sklearn.cross_decomposition` module. The function should perform dimensionality reduction on `X` and `Y` and return their transformed versions. Function Signature: ```python from sklearn.cross_decomposition import PLSCanonical def pls_canonical_transform(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: Perform PLSCanonical transformation on matrices X and Y. Parameters: - X (np.ndarray): The predictor matrix of shape (n_samples, n_features). - Y (np.ndarray): The response matrix of shape (n_samples, n_targets). - n_components (int): The number of components to keep. Returns: - Tuple containing transformed X and transformed Y: - X_transformed (np.ndarray): Transformed predictor matrix of shape (n_samples, n_components). - Y_transformed (np.ndarray): Transformed response matrix of shape (n_samples, n_components). # Your code here return X_transformed, Y_transformed ``` Input: 1. `X` (np.ndarray): A 2D numpy array of shape `(n_samples, n_features)` representing the predictor variables. 2. `Y` (np.ndarray): A 2D numpy array of shape `(n_samples, n_targets)` representing the response variables. 3. `n_components` (int): The number of components to keep. Output: - A tuple with: - `X_transformed` (np.ndarray): Transformed predictor matrix of shape `(n_samples, n_components)`. - `Y_transformed` (np.ndarray): Transformed response matrix of shape `(n_samples, n_components)`. Constraints: 1. Each matrix (`X`, `Y`) will have at least `n_components` columns. 2. The function should handle cases where the number of samples is less than the number of features. Example: ```python import numpy as np # Sample data X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) Y = np.array([[1, 2], [2, 3], [3, 4], [4, 5]]) n_components = 2 # Expected output: Two transformed matrices with shape (4, 2) X_transformed, Y_transformed = pls_canonical_transform(X, Y, n_components) print(\\"X_transformed:n\\", X_transformed) print(\\"Y_transformed:n\\", Y_transformed) ``` Notes: - Your implementation should utilize the `PLSCanonical` class from the `sklearn.cross_decomposition` module. - Ensure that the function handles different input shapes and checks for valid input sizes. - Use appropriate exception handling for any invalid parameters or shapes.","solution":"from sklearn.cross_decomposition import PLSCanonical import numpy as np from typing import Tuple def pls_canonical_transform(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: Perform PLSCanonical transformation on matrices X and Y. Parameters: - X (np.ndarray): The predictor matrix of shape (n_samples, n_features). - Y (np.ndarray): The response matrix of shape (n_samples, n_targets). - n_components (int): The number of components to keep. Returns: - Tuple containing transformed X and transformed Y: - X_transformed (np.ndarray): Transformed predictor matrix of shape (n_samples, n_components). - Y_transformed (np.ndarray): Transformed response matrix of shape (n_samples, n_components). # Initialize the PLSCanonical model with the specified number of components pls = PLSCanonical(n_components=n_components) # Fit the model and transform both X and Y X_transformed, Y_transformed = pls.fit_transform(X, Y) return X_transformed, Y_transformed"},{"question":"**Problem Statement: Implement a Simplified NNTP Client** In this exercise, you are required to implement a simplified version of an NNTP client in Python. Your task is to create a class `SimpleNNTPClient` that interacts with an NNTP server to list newsgroups and retrieve article headers. # Requirements 1. **Class Definition:** - Create a class `SimpleNNTPClient` with the following methods: - `__init__(self, host: str, port: int = 119, user: str = None, password: str = None, readermode: bool = None)` - `connect(self) -> str` - `list_newsgroups(self) -> list` - `get_article_headers(self, group: str, article_range: tuple) -> dict` - `close(self) -> str` 2. **Method Descriptions:** - The constructor (`__init__`) should initialize the connection parameters. - `connect()` should establish a connection to the NNTP server using the provided host and port. This method should return the server\'s welcome message. - `list_newsgroups()` should send an appropriate command to list all newsgroups available on the server and return them as a list of group names. - `get_article_headers(group, article_range)` should send a command to retrieve headers for articles in the specified group within the given range. This method should return a dictionary mapping article numbers to their headers. - `close()` should terminate the connection to the server and return a goodbye message. 3. **Error Handling:** - Ensure your methods handle possible exceptions such as connection errors, invalid commands, and unexpected responses from the server. Appropriate exceptions should be raised (e.g., `NNTPConnectionError`, `NNTPCommandError`. 4. **Usage Example:** ```python client = SimpleNNTPClient(host=\'news.gmane.io\') welcome_msg = client.connect() print(welcome_msg) newsgroups = client.list_newsgroups() print(newsgroups) headers = client.get_article_headers(group=\'gmane.comp.python.committers\', article_range=(1, 10)) print(headers) goodbye_msg = client.close() print(goodbye_msg) ``` # Constraints - Do not use the `nntplib` module or any external NNTP client libraries in your implementation. - You must handle low-level networking using Python\'s `socket` module. - Ensure your code is compatible with Python 3.10 and does not use deprecated features. # Performance Requirements - Your solution should efficiently handle connections and data transmissions, ensuring that the client does not block unnecessarily and can manage responses from the server promptly. Good luck and happy coding!","solution":"import socket class NNTPConnectionError(Exception): pass class NNTPCommandError(Exception): pass class SimpleNNTPClient: def __init__(self, host: str, port: int = 119, user: str = None, password: str = None, readermode: bool = None): self.host = host self.port = port self.user = user self.password = password self.readermode = readermode self.sock = None self.file = None def _send_command(self, command: str) -> str: if self.sock is None: raise NNTPConnectionError(\\"Not connected to NNTP server\\") self.sock.sendall(f\\"{command}rn\\".encode()) response = self._get_response() return response def _get_response(self) -> str: response = self.file.readline().strip() return response def connect(self) -> str: try: self.sock = socket.create_connection((self.host, self.port)) self.file = self.sock.makefile(\'r\') welcome_msg = self._get_response() return welcome_msg except socket.error as e: raise NNTPConnectionError(f\\"Failed to connect to the server: {e}\\") def list_newsgroups(self) -> list: response = self._send_command(\'LIST\') if not response.startswith(\'215\'): raise NNTPCommandError(f\\"LIST command failed: {response}\\") newsgroups = [] while True: line = self.file.readline().strip() if line == \'.\': break newsgroups.append(line.split()[0]) return newsgroups def get_article_headers(self, group: str, article_range: tuple) -> dict: low, high = article_range response = self._send_command(f\\"GROUP {group}\\") if not response.startswith(\'211\'): raise NNTPCommandError(f\\"GROUP command failed: {response}\\") response = self._send_command(f\\"XOVER {low}-{high}\\") if not response.startswith(\'224\'): raise NNTPCommandError(f\\"XOVER command failed: {response}\\") headers = {} while True: line = self.file.readline().strip() if line == \'.\': break parts = line.split(\'t\') article_num = int(parts[0]) headers[article_num] = parts[1:] return headers def close(self) -> str: response = self._send_command(\'QUIT\') self.sock.close() return response"},{"question":"You are to implement a Python class to mimic a simplified version of a reference-counted object container. Your class should manage objects and their reference counts, allowing objects to be linked and unlinked dynamically. # Class Definition Create a class `RefCountedObject` with the following requirements: 1. **Initialization:** - The constructor should initialize the object with an optional `name` attribute (string). - Initialize a reference count (`_refcount`) to 1. 2. **Reference Count Management Methods:** - `increment_refcount(self)`: Increments the reference count by 1. - `decrement_refcount(self)`: Decrements the reference count by 1. If the reference count reaches 0, the object should be set to `None`. 3. **Link/Unlink Methods:** - `link_object(self, obj: \'RefCountedObject\')`: Links another `RefCountedObject` to the current object by incrementing the referenced object\'s reference count. - `unlink_object(self, obj: \'RefCountedObject\')`: Unlinks a linked `RefCountedObject` by decrementing its reference count. 4. **Special Methods:** - `__repr__(self)`: Should provide a string representation showing the object\'s name and the current reference count. - `__del__(self)`: Custom deletion method that prints a message indicating the object is being deleted. # Input You don\'t need to handle any input reading in this implementation. The class methods will be tested with various scenarios through direct instantiation and method calls. # Output Ensure the class methods manipulate and display reference counts correctly. The `__del__` method should print a message when an object\'s reference count reaches zero and the object is deleted. # Constraints - Follow Python 3.10 conventions and best practices. - Ensure thread safety is not a concern for this task. - Maximum name length should be 50 characters. # Example Usage: ```python obj1 = RefCountedObject(name=\\"Object1\\") print(obj1) # Output: RefCountedObject(name=\'Object1\', refcount=1) obj2 = RefCountedObject(name=\\"Object2\\") obj1.link_object(obj2) print(obj2) # Output: RefCountedObject(name=\'Object2\', refcount=2) obj1.unlink_object(obj2) print(obj2) # Output: RefCountedObject(name=\'Object2\', refcount=1) obj2.decrement_refcount() # This should set obj2 to None and trigger __del__ message. ``` # Note: While implementing, make sure you properly handle the reference counting to avoid memory leaks or premature deletion.","solution":"class RefCountedObject: def __init__(self, name=None): self.name = name if name and len(name) <= 50 else None self._refcount = 1 def increment_refcount(self): self._refcount += 1 def decrement_refcount(self): self._refcount -= 1 if self._refcount == 0: self.__del__() def link_object(self, obj): if isinstance(obj, RefCountedObject): obj.increment_refcount() def unlink_object(self, obj): if isinstance(obj, RefCountedObject): obj.decrement_refcount() def __repr__(self): return f\\"RefCountedObject(name=\'{self.name}\', refcount={self._refcount})\\" def __del__(self): print(f\\"RefCountedObject(name=\'{self.name}\') is being deleted\\") self.name = None self._refcount = 0"},{"question":"Objective: To assess your understanding of extending TorchElastic’s functionality, specifically in creating a custom metric handler for emitting metrics during distributed training. Task: Implement a custom Metric Handler for TorchElastic that logs metrics to the console instead of `/dev/null`. Details: 1. **Metric Handler Implementation**: - Create a class `ConsoleMetricHandler` that inherits from `torch.distributed.elastic.metrics.MetricHandler`. - Implement the method `emit` to print the `metric_data` to the console. 2. **Testing the Metric Handler**: - Demonstrate the usage of your custom metric handler within a simple launcher. - For the purpose of testing, simulate metric emission inside the launcher. Input Specifications: - No specific input parameters. You are expected to deduce the required constructor and method signature from the provided `TorchElastic` API references. Output Specifications: - Create a detailed script `custom_launcher.py` containing: - The implementation of `ConsoleMetricHandler`. - An example setup of `LocalElasticAgent` using the custom metric handler. - A simulated metric emission to verify functionality. Constraints: - Use mocking or simple statements to simulate the essentials of the runner process; actual distributed setup and training processes are not necessary in this context. Example Output: Below is a snippet showing the expected output when metrics are emitted: ``` Metric Key: <metric_key> Metric Value: <metric_value> Metric Timestamp: <timestamp> ``` Notes: - Ensure your implementation adheres to standard practices discussed in the provided documentation. - The focus should be on correct class extensions and method overriding. Good luck, and happy coding!","solution":"import datetime from torch.distributed.elastic.metrics import MetricHandler class ConsoleMetricHandler(MetricHandler): def emit(self, metric_data): Logs the metric data to the console. Args: metric_data (dict): A dictionary containing the metric information. print(f\\"Metric Key: {metric_data[\'name\']}\\") print(f\\"Metric Value: {metric_data[\'value\']}\\") print(f\\"Metric Timestamp: {datetime.datetime.fromtimestamp(metric_data[\'timestamp\']).isoformat()}\\") # Example usage if __name__ == \\"__main__\\": import time handler = ConsoleMetricHandler() # Simulated metric data metric_data = { \'name\': \'accuracy\', \'value\': 92.5, \'timestamp\': time.time() } handler.emit(metric_data)"},{"question":"Given the following specifications, implement a Python function that simulates the marshalling and unmarshalling of a list of Python objects to and from a file. You will use the built-in `marshal` module rather than the C API described, but follow the same principles. # Function Signature ```python def marshal_objects_to_file(objects: list, filename: str, version: int) -> None: Marshals a list of Python objects to a binary file. :param objects: List of Python objects to be serialized. :param filename: Name of the file to which the objects will be written. :param version: The marshalling version to be used (0, 1, or 2). pass def unmarshal_objects_from_file(filename: str) -> list: Unmarshals a list of Python objects from a binary file. :param filename: Name of the file from which the objects will be read. :return: List of Python objects that were deserialized. pass ``` # Requirements 1. **Marshalling Function (`marshal_objects_to_file`)**: - You must open the file in binary write mode. - Use the appropriate version for marshalling (0, 1, or 2), ensuring compatibility with `marshal` module. 2. **Unmarshalling Function (`unmarshal_objects_from_file`)**: - You must open the file in binary read mode. - Read back all the objects that were marshalled into the file. # Constraints 1. Use only the standard `marshal` module for your implementation. 2. The objects in the list can be any serializable Python objects (e.g., numbers, strings, lists, dictionaries). # Example Usage ```python objects_to_marshal = [42, \\"hello\\", [1, 2, 3], {\\"key\\": \\"value\\"}] filename = \\"testfile.bin\\" version = 2 marshal_objects_to_file(objects_to_marshal, filename, version) unmarshalled_objects = unmarshal_objects_from_file(filename) assert objects_to_marshal == unmarshalled_objects ``` # Performance Requirements - The solution must handle large lists of objects efficiently. - Ensure that file operations are correctly managed, including proper closing of files after operations. # Notes - Careful error handling must be implemented to handle cases where reading from or writing to the file fails.","solution":"import marshal def marshal_objects_to_file(objects: list, filename: str, version: int) -> None: Marshals a list of Python objects to a binary file. :param objects: List of Python objects to be serialized. :param filename: Name of the file to which the objects will be written. :param version: The marshalling version to be used (0, 1, or 2). try: with open(filename, \'wb\') as file: for obj in objects: marshal.dump(obj, file, version) except Exception as e: print(f\\"Error marshalling objects to file: {e}\\") def unmarshal_objects_from_file(filename: str) -> list: Unmarshals a list of Python objects from a binary file. :param filename: Name of the file from which the objects will be read. :return: List of Python objects that were deserialized. objects = [] try: with open(filename, \'rb\') as file: while True: try: obj = marshal.load(file) objects.append(obj) except EOFError: break except Exception as e: print(f\\"Error unmarshalling objects from file: {e}\\") return objects"},{"question":"# Email Message Manipulation with Python\'s \\"email\\" Package You are tasked with creating, parsing, and generating email messages using Python\'s \\"email\\" package. Your solution should demonstrate your understanding of the \\"EmailMessage\\" class, the \\"parser\\" module, and the \\"generator\\" module. Task 1. **Create an Email Message**: - Implement a function `create_email(subject, sender, recipient, body, subtype=\'plain\')` that creates an `EmailMessage` object with the specified subject, sender, recipient, and body. The `subtype` parameter should default to `\'plain\'` but can be set to `\'html\'` for HTML content. - The function should return the created `EmailMessage` object. ```python from email.message import EmailMessage def create_email(subject: str, sender: str, recipient: str, body: str, subtype: str = \'plain\') -> EmailMessage: # Your implementation here pass ``` 2. **Parse an Email from a String**: - Implement a function `parse_email_from_string(email_string)` that takes a string representing a raw email and returns an `EmailMessage` object parsed from the string. Use the `email.parser.Parser` class for parsing. ```python from email.parser import Parser def parse_email_from_string(email_string: str) -> EmailMessage: # Your implementation here pass ``` 3. **Generate an Email String**: - Implement a function `generate_email_string(email_message)` that takes an `EmailMessage` object and returns a string representing the serialized version of the email. Use the `email.generator.BytesGenerator` class for generating the string. ```python from email.generator import BytesGenerator from io import BytesIO def generate_email_string(email_message: EmailMessage) -> str: # Your implementation here pass ``` Constraints 1. The subject, sender, and recipient fields must be non-empty strings. 2. The body must be a non-empty string. 3. For generating the email string, ensure proper MIME header generation. Example Usage ```python subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" body = \\"This is a test email.\\" # Creating an email message email_msg = create_email(subject, sender, recipient, body) # Parsing an email message from a string raw_email = \'\'\' From: sender@example.com To: recipient@example.com Subject: Test Email This is a test email. \'\'\' parsed_email = parse_email_from_string(raw_email) # Generating a raw email string from an EmailMessage object email_string = generate_email_string(email_msg) print(email_string) ``` Ensure your implementations properly handle the input and correctly use the relevant classes and methods from the \\"email\\" package.","solution":"from email.message import EmailMessage from email.parser import Parser from email.generator import BytesGenerator from io import BytesIO def create_email(subject: str, sender: str, recipient: str, body: str, subtype: str = \'plain\') -> EmailMessage: if not subject or not sender or not recipient or not body: raise ValueError(\\"Subject, sender, recipient, and body must be non-empty strings.\\") msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient msg.set_content(body, subtype=subtype) return msg def parse_email_from_string(email_string: str) -> EmailMessage: parser = Parser() return parser.parsestr(email_string) def generate_email_string(email_message: EmailMessage) -> str: output = BytesIO() generator = BytesGenerator(output) generator.flatten(email_message) return output.getvalue().decode(\'utf-8\')"},{"question":"You have been tasked with visualizing and analyzing a dataset using seaborn. For this assessment, you will work with a sample dataset, perform data visualization, and customize the plots using seaborn\'s theming capabilities. # Dataset You will use the \\"tips\\" dataset, which is a classic built-in dataset in seaborn. It contains information about the tips received by waiters in a restaurant. Here\'s a brief description of the columns: - `total_bill`: The total bill amount. - `tip`: The tip amount. - `sex`: The sex of the person paying for the meal. - `smoker`: Whether the person was a smoker or not. - `day`: The day of the week. - `time`: The time of day (Lunch or Dinner). - `size`: The size of the party. # Task 1. **Load the Dataset**: Load the \\"tips\\" dataset from seaborn\'s built-in datasets. 2. **Create a Bar Plot**: Create a bar plot showing the average total_bill for each day of the week. Customize the theme to use the \\"whitegrid\\" style and a \\"pastel\\" color palette. 3. **Create a Scatter Plot**: Create a scatter plot showing the relationship between \'total_bill\' and \'tip\', color-coded by \'sex\'. Customize the theme to remove the top and right spines. 4. **Create a Box Plot**: Create a box plot of the \'tip\' amounts by \'day\', separated by \'smoker\' status. Use a custom color palette of your choice. 5. **Add Titles and Labels**: Ensure all plots have appropriate titles and axis labels. # Requirements - Use seaborn for all visualizations. - Customize themes as specified. - Titles and labels should be clear and descriptive. - Your final function should accept no input and save the generated plots as PNG files. # Evaluation Your solution will be evaluated on: - Correctness: The plots accurately represent the data and meet the specified requirements. - Clarity: The plots are clear and well-labeled. - Use of seaborn features: Effective use of seaborn\'s plotting and theming capabilities. # Example ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the dataset tips = sns.load_dataset(\'tips\') # Set theme and create bar plot sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") plt.figure(figsize=(8, 6)) sns.barplot(x=\'day\', y=\'total_bill\', data=tips, ci=None) plt.title(\'Average Total Bill by Day\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Average Total Bill\') plt.savefig(\'bar_plot.png\') # Create scatter plot custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"ticks\\", rc=custom_params) plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'sex\', data=tips) plt.title(\'Total Bill vs Tip by Sex\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.savefig(\'scatter_plot.png\') # Create box plot custom_palette = sns.color_palette(\\"coolwarm\\") sns.set_theme(style=\\"whitegrid\\") plt.figure(figsize=(8, 6)) sns.boxplot(x=\'day\', y=\'tip\', hue=\'smoker\', data=tips, palette=custom_palette) plt.title(\'Tips by Day and Smoker Status\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Tip Amount\') plt.savefig(\'box_plot.png\') # Call the function to create and save the plots visualize_tips_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the dataset tips = sns.load_dataset(\'tips\') # Set theme and create bar plot sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") plt.figure(figsize=(8, 6)) sns.barplot(x=\'day\', y=\'total_bill\', data=tips, ci=None) plt.title(\'Average Total Bill by Day\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Average Total Bill\') plt.savefig(\'bar_plot.png\') plt.close() # Create scatter plot with customized theme custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"ticks\\", rc=custom_params) plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'sex\', data=tips) plt.title(\'Total Bill vs Tip by Sex\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.savefig(\'scatter_plot.png\') plt.close() # Create box plot with custom color palette custom_palette = sns.color_palette(\\"coolwarm\\") sns.set_theme(style=\\"whitegrid\\") plt.figure(figsize=(8, 6)) sns.boxplot(x=\'day\', y=\'tip\', hue=\'smoker\', data=tips, palette=custom_palette) plt.title(\'Tips by Day and Smoker Status\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Tip Amount\') plt.savefig(\'box_plot.png\') plt.close() # Call the function to create and save the plots visualize_tips_data()"},{"question":"Objective: To assess the understanding of outlier detection using scikit-learn and the ability to implement and evaluate the Isolation Forest and Local Outlier Factor algorithms. Problem Statement: You are given a dataset `data.csv` containing two features (`feature1` and `feature2`). Your task is to implement both Isolation Forest and Local Outlier Factor (LOF) to detect outliers in this dataset. The file path to `data.csv` is provided as input. Requirements: 1. Load the dataset from the provided CSV file. 2. Implement and fit both Isolation Forest and LOF using the dataset. 3. Use each model to predict outliers, labeling inliers as 1 and outliers as -1. 4. Return the outlier labels for each method as a dictionary with keys \\"IsolationForest\\" and \\"LocalOutlierFactor\\". Function Signature: ```python def detect_outliers(file_path: str) -> dict: pass ``` Input: - `file_path`: A string representing the path to the `data.csv` file. The CSV file has two columns named `feature1` and `feature2`. Output: - A dictionary with keys \\"IsolationForest\\" and \\"LocalOutlierFactor\\". Each key maps to a list of labels where 1 indicates an inlier and -1 indicates an outlier. Constraints: - Use default parameters for the Isolation Forest and Local Outlier Factor models unless stated otherwise. Example: Given the CSV file `data.csv`: ``` feature1,feature2 0.1,0.2 0.2,0.3 3.4,4.5 0.4,0.5 0.3,0.1 10.0,10.0 ... ``` The function call: ```python outliers = detect_outliers(\'data.csv\') ``` Might return: ```python { \\"IsolationForest\\": [1, 1, 1, -1, 1, -1, ...], \\"LocalOutlierFactor\\": [1, 1, 1, 1, 1, -1, ...] } ``` Constraints: - Make sure the dataset is loaded correctly from the given CSV file path. - Ensure that the `predict`, `decision_function`, or `score_samples` methods are appropriately used based on the model. - Ensure a robust comparison between the two outlier detection methods. Hints: - Use `pandas.read_csv` to load the CSV file. - Refer to `sklearn.ensemble.IsolationForest` for fitting the Isolation Forest model. - Refer to `sklearn.neighbors.LocalOutlierFactor` for fitting the LOF model.","solution":"import pandas as pd from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor def detect_outliers(file_path: str) -> dict: # Load the dataset from the provided CSV file data = pd.read_csv(file_path) # Prepare the feature array X = data[[\'feature1\', \'feature2\']].values # Implement and fit Isolation Forest isolation_forest = IsolationForest() labels_if = isolation_forest.fit_predict(X) # Implement and fit Local Outlier Factor lof = LocalOutlierFactor() labels_lof = lof.fit_predict(X) # Return the outlier labels return { \\"IsolationForest\\": labels_if.tolist(), \\"LocalOutlierFactor\\": labels_lof.tolist() }"},{"question":"# Task You are provided with a list of files and a set of commands that define which files should be included or excluded in a source distribution. Your task is to implement a function that processes these commands and outputs the final list of files to be included in the distribution. # Requirements 1. **Function Implementation:** Implement a function named `process_distribution_files` with the following signature: ```python def process_distribution_files(files: List[str], commands: List[str]) -> List[str]: ``` 2. **Input:** - `files`: A list of strings representing file paths in your project. Example: ```python files = [ \\"src/main.py\\", \\"src/utils/helpers.py\\", \\"docs/index.md\\", \\"tests/test_main.py\\", \\"assets/logo.png\\" ] ``` - `commands`: A list of strings, where each string is a command that modifies the inclusion/exclusion of files in the distribution. Example: ```python commands = [ \\"include src/*.py\\", \\"exclude tests/*.py\\", \\"recursive-include docs *.md\\" ] ``` 3. **Output:** - Return a list of strings representing the final set of files to be included in the distribution after processing the commands. 4. **Constraints:** - The commands will be well-formed as per the command reference documentation. - You need to support all the commands mentioned in the document: `include`, `exclude`, `recursive-include`, `recursive-exclude`, `global-include`, `global-exclude`, `prune`, and `graft`. 5. **Performance:** - Your solution should efficiently handle up to 10,000 files and 1,000 commands. # Example ```python files = [ \\"src/main.py\\", \\"src/utils/helpers.py\\", \\"docs/index.md\\", \\"tests/test_main.py\\", \\"assets/logo.png\\" ] commands = [ \\"include src/*.py\\", \\"exclude tests/*.py\\", \\"recursive-include docs *.md\\" ] # Expected output: [\'src/main.py\', \'src/utils/helpers.py\', \'docs/index.md\'] print(process_distribution_files(files, commands)) ``` # Hints - Utilize the `fnmatch` library for matching Unix-style glob patterns. - Carefully evaluate the order of commands: include commands make files eligible, while exclude commands remove them from eligibility. - Recursive commands should consider all files under a given directory.","solution":"import fnmatch from typing import List def process_distribution_files(files: List[str], commands: List[str]) -> List[str]: included_files = set() excluded_files = set() for command in commands: cmd_parts = command.split() cmd = cmd_parts[0] pattern = cmd_parts[1] if len(cmd_parts) > 2: file_type = cmd_parts[2] else: file_type = None if cmd == \\"include\\": included_files.update(fnmatch.filter(files, pattern)) elif cmd == \\"exclude\\": excluded_files.update(fnmatch.filter(files, pattern)) elif cmd == \\"recursive-include\\": for file in files: if file.startswith(pattern) and fnmatch.fnmatch(file, f\\"*{file_type}\\"): included_files.add(file) elif cmd == \\"recursive-exclude\\": for file in files: if file.startswith(pattern) and fnmatch.fnmatch(file, f\\"*{file_type}\\"): excluded_files.add(file) elif cmd == \\"global-include\\": included_files.update(fnmatch.filter(files, f\\"*{pattern}\\")) elif cmd == \\"global-exclude\\": excluded_files.update(fnmatch.filter(files, f\\"*{pattern}\\")) elif cmd == \\"prune\\": excluded_files.update([file for file in files if file.startswith(pattern)]) elif cmd == \\"graft\\": included_files.update([file for file in files if file.startswith(pattern)]) final_files = included_files - excluded_files return sorted(final_files)"},{"question":"Objective: Demonstrate your understanding of the `xml.sax` package by writing a Python program that parses an XML document and processes specific XML elements using a custom `ContentHandler`. Task: You are required to write a Python function `extract_person_names(xml_string: str) -> List[str]` that takes an XML string `xml_string` as input and returns a list of names of all `<person>` elements in the document. The `name` for each `<person>` element is defined within its `<name>` child element. Input: - A single parameter `xml_string` which is a string containing well-structured XML data. Output: - A list of strings, where each string is the content of a `<name>` child element inside each `<person>` XML element. Constraints: - The `xml_string` is guaranteed to be valid XML. - The XML structure is pre-defined and will have `<person>` elements which each contain one `<name>` element: ```xml <root> <person> <name>John Doe</name> <age>30</age> </person> <person> <name>Jane Doe</name> <age>25</age> </person> <!-- more person elements --> </root> ``` Requirements: 1. Use the `xml.sax` package to parse the XML string. 2. Implement a custom `ContentHandler` to handle the start and end of elements. 3. Collect and return all the `<name>` elements contained within `<person>` elements. Example: ```python xml_string = <root> <person> <name>John Doe</name> <age>30</age> </person> <person> <name>Jane Doe</name> <age>25</age> </person> </root> assert extract_person_names(xml_string) == [\'John Doe\', \'Jane Doe\'] ``` Here is a template to help you start: ```python from xml.sax import ContentHandler, parseString from typing import List class PersonNameHandler(ContentHandler): def __init__(self): self.inside_person = False self.inside_name = False self.names = [] def startElement(self, name, attrs): if name == \'person\': self.inside_person = True elif name == \'name\' and self.inside_person: self.inside_name = True def endElement(self, name): if name == \'person\': self.inside_person = False elif name == \'name\': self.inside_name = False def characters(self, content): if self.inside_name: self.names.append(content.strip()) def extract_person_names(xml_string: str) -> List[str]: handler = PersonNameHandler() parseString(xml_string, handler) return handler.names ```","solution":"from xml.sax import ContentHandler, parseString from typing import List class PersonNameHandler(ContentHandler): def __init__(self): self.inside_person = False self.inside_name = False self.current_name = \\"\\" self.names = [] def startElement(self, name, attrs): if name == \'person\': self.inside_person = True elif name == \'name\' and self.inside_person: self.inside_name = True def endElement(self, name): if name == \'person\': self.inside_person = False elif name == \'name\': if self.inside_name: self.names.append(self.current_name.strip()) self.current_name = \\"\\" self.inside_name = False def characters(self, content): if self.inside_name: self.current_name += content def extract_person_names(xml_string: str) -> List[str]: handler = PersonNameHandler() parseString(xml_string, handler) return handler.names"},{"question":"**Coding Assessment Question** **Objective:** Demonstrate your understanding and ability to work with meta tensors in PyTorch by implementing specific functionality related to their creation and manipulation. **Problem Statement:** You are required to implement a function `initialize_meta_tensors` that takes the dimensions of a tensor as input and returns a dictionary with specific tensors initialized on the \\"meta\\" device and transformed as per the description below: 1. Create a random tensor of the given dimensions on the meta device. 2. Create a `Linear` layer from PyTorch\'s NN module with the input dimension and an output dimension of 10, initialized on the meta device. 3. Manually load a saved tensor onto the meta device using a given filename. 4. Convert the created random meta tensor to a real tensor filled with zeros on the CPU device. Your function should take the following parameters: - `dimensions` (tuple of int): The dimensions of the tensor to be created. - `input_dim` (int): The input dimension for the Linear layer. - `filename` (str): The filename from which to load the saved tensor. **Constraints:** - You can assume that the `filename` provided will always be a valid file path containing a saved tensor. - Your function should catch any potential errors and print an appropriate error message if the operation fails. - Ensure you follow the correct steps and use appropriate PyTorch methods to handle meta tensors. **Function Signature:** ```python import torch import torch.nn as nn def initialize_meta_tensors(dimensions: tuple, input_dim: int, filename: str) -> dict: # Your implementation here ``` **Example:** ```python dimensions = (3, 4) input_dim = 5 filename = \'saved_tensor.pt\' result = initialize_meta_tensors(dimensions, input_dim, filename) print(result) # Expected output (The tensor values might differ due to random initialization, but structure should be as shown): { \'meta_tensor\': tensor(..., device=\'meta\', size=(3, 4)), \'linear_layer\': Linear(in_features=5, out_features=10, bias=True), \'loaded_tensor\': tensor(..., device=\'meta\', size=(...)), \'real_tensor\': tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], device=\'cpu\') } ``` Hints: 1. Use `torch.device(\'meta\')` context manager to create tensors on the meta device. 2. Utilize `torch.load` with `map_location=\'meta\'` to load an object onto the meta device. 3. Use factory functions like `torch.zeros_like` to convert a meta tensor to a real tensor with the desired initialization. **Evaluation Criteria:** - Correct implementation of tensor creation and manipulation on the meta device. - Proper handling and conversion of meta tensors to real tensors on a different device. - Correct usage of PyTorch\'s NN module and tensor methods as per the problem requirements.","solution":"import torch import torch.nn as nn def initialize_meta_tensors(dimensions: tuple, input_dim: int, filename: str) -> dict: try: # Create a random tensor on the meta device meta_tensor = torch.rand(dimensions, device=\'meta\') # Create a Linear layer initialized on the meta device linear_layer = nn.Linear(input_dim, 10, device=\'meta\') # Manually load a saved tensor onto the meta device loaded_tensor = torch.load(filename, map_location=\'meta\') # Convert the created meta tensor to a real tensor filled with zeros on the CPU device real_tensor = torch.zeros_like(meta_tensor, device=\'cpu\') return { \'meta_tensor\': meta_tensor, \'linear_layer\': linear_layer, \'loaded_tensor\': loaded_tensor, \'real_tensor\': real_tensor } except Exception as e: print(f\\"An error occurred: {e}\\") return {}"},{"question":"Objective This question aims to assess your understanding of the seaborn library and your ability to visualize distributions of data using various seaborn functions. Problem Statement You are provided with three datasets that contain measurements of different attributes for **penguins**, **diamonds**, and **tips (restaurant bills)**. You must visualize the distributions of these attributes using seaborn functions. Follow the instructions below to complete the task. Part 1: Univariate Distributions 1. Load the `penguins` dataset. 2. Create a histogram to visualize the distribution of the `flipper_length_mm` variable. 3. Customize the histogram by: - Setting the number of bins to 20. - Adding a KDE curve. Part 2: Conditional Distributions 4. Using the histogram from Part 1, create conditional histograms based on the `species` column. 5. Modify the histogram to use the `step` element for better clarity. Part 3: Bivariate Distributions 6. Load the `diamonds` dataset. 7. Create a bivariate KDE plot to visualize the joint distribution of `carat` and `price`. 8. Use the `hue` parameter to differentiate the data by the `cut` column. Part 4: Cumulative Distributions 9. Load the `tips` dataset. 10. Create an ECDF plot to visualize the cumulative distribution of the `total_bill` variable. Implementation Details - Use the `seaborn` library for all visualizations. - Customize the visualizations for better clarity and insight. - Ensure each plot is appropriately labeled and titled. Input and Output - **Input**: None (datasets are loaded internally). - **Output**: Plots generated using seaborn functions, as described above. Example Implementation Below is an example to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt # Part 1: Univariate Distributions penguins = sns.load_dataset(\\"penguins\\") # 2. Create the histogram sns.displot(penguins, x=\\"flipper_length_mm\\", bins=20, kde=True) plt.title(\\"Distribution of Flipper Length in Penguins\\") plt.show() # Part 2: Conditional Distributions # 4. Conditional histograms sns.displot(penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", element=\\"step\\") plt.title(\\"Flipper Length Distribution by Species\\") plt.show() # Part 3: Bivariate Distributions diamonds = sns.load_dataset(\\"diamonds\\") # 7. Bivariate KDE plot sns.displot(diamonds, x=\\"carat\\", y=\\"price\\", kind=\\"kde\\", hue=\\"cut\\") plt.title(\\"Joint Distribution of Carat and Price by Cut\\") plt.show() # Part 4: Cumulative Distributions tips = sns.load_dataset(\\"tips\\") # 10. ECDF plot sns.displot(tips, x=\\"total_bill\\", kind=\\"ecdf\\") plt.title(\\"ECDF of Total Bill\\") plt.show() ``` Use this example as a template and complete all required parts. Ensure your code is well-organized and commented.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Part 1: Univariate Distributions penguins = sns.load_dataset(\\"penguins\\") # 2. Create the histogram sns.displot(penguins, x=\\"flipper_length_mm\\", bins=20, kde=True) plt.title(\\"Distribution of Flipper Length in Penguins\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # Part 2: Conditional Distributions # 4. Conditional histograms sns.displot(penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", element=\\"step\\") plt.title(\\"Flipper Length Distribution by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # Part 3: Bivariate Distributions diamonds = sns.load_dataset(\\"diamonds\\") # 7. Bivariate KDE plot sns.displot(diamonds, x=\\"carat\\", y=\\"price\\", kind=\\"kde\\", hue=\\"cut\\") plt.title(\\"Joint Distribution of Carat and Price by Cut\\") plt.xlabel(\\"Carat\\") plt.ylabel(\\"Price\\") plt.show() # Part 4: Cumulative Distributions tips = sns.load_dataset(\\"tips\\") # 10. ECDF plot sns.displot(tips, x=\\"total_bill\\", kind=\\"ecdf\\") plt.title(\\"ECDF of Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Proportion\\") plt.show()"},{"question":"Audio Playback Simulation You are tasked with writing a Python program that uses the `ossaudiodev` module to play an audio file through the OSS audio interface. The program should read from an audio file, configure the audio device with specific settings, and then play the audio data. Requirements: 1. **Input**: - An audio file in raw PCM format. - The format of the audio data (e.g., \\"AFMT_S16_LE\\"). - The number of channels (e.g., 2 for stereo). - The sampling rate (e.g., 44100 Hz). 2. **Output**: - The program should output the playback status to the console, including any errors that may occur during execution. 3. **Constraints**: - The program should use blocking mode for audio playback. - Ensure that the audio device is properly closed after playback. - If the specified audio format, number of channels, or sampling rate cannot be set, raise an `OSSAudioError` with an appropriate message. 4. **Performance**: - The audio playback should handle typical audio file sizes efficiently. Implementation Guidelines: 1. **Open the Audio Device**: - Use `ossaudiodev.open()` to open the device in write mode. If the device cannot be opened, catch the `OSError` and output an error message. 2. **Configure the Audio Device**: - Set the audio format using `setfmt()`. - Set the number of channels using `channels()`. - Set the sampling rate using `speed()`. 3. **Read and Play Audio Data**: - Read the audio file in chunks and write each chunk to the audio device using `write()`. - Ensure that the entire audio file is played by waiting for the device to sync using `sync()` after writing all data. 4. **Handle Exceptions**: - Use appropriate exception handling to catch and report errors during device operations and playback. 5. **Example Usage**: ```python play_audio(\\"path/to/audio.raw\\", \\"AFMT_S16_LE\\", 2, 44100) ``` Function Signature: ```python def play_audio(file_path: str, audio_format: str, channels: int, samplerate: int) -> None: pass ``` Example: Given an audio file \\"sample.raw\\" with format \\"AFMT_S16_LE\\", 2 channels (stereo), and sampling rate of 44100 Hz, the function call `play_audio(\\"sample.raw\\", \\"AFMT_S16_LE\\", 2, 44100)` should open the audio device, configure it, play the audio data, and then close the device. If any configuration cannot be met, the function should raise an `OSSAudioError` with a descriptive message.","solution":"import ossaudiodev import os class OSSAudioError(Exception): pass def play_audio(file_path: str, audio_format: str, channels: int, samplerate: int) -> None: if not os.path.exists(file_path): raise OSSAudioError(f\\"Audio file {file_path} not found.\\") try: audio_device = ossaudiodev.open(\'w\') except OSError as e: raise OSSAudioError(f\\"Failed to open audio device: {e}\\") try: # Set the audio format if audio_device.setfmt(ossaudiodev.__dict__[audio_format]) != ossaudiodev.__dict__[audio_format]: raise OSSAudioError(f\\"Audio format {audio_format} could not be set.\\") # Set the number of channels if audio_device.channels(channels) != channels: raise OSSAudioError(f\\"Number of channels {channels} could not be set.\\") # Set the sampling rate if audio_device.speed(samplerate) != samplerate: raise OSSAudioError(f\\"Sampling rate {samplerate} Hz could not be set.\\") with open(file_path, \'rb\') as audio_file: while True: data = audio_file.read(1024) if not data: break audio_device.write(data) audio_device.sync() print(\\"Playback finished successfully.\\") except Exception as e: raise OSSAudioError(f\\"Playback error: {e}\\") finally: audio_device.close() print(\\"Audio device closed.\\")"},{"question":"Efficient Sequence Manipulation Using Python\'s `array` Module You are given access to a dataset represented by a flat list of integers and floating-point numbers. This dataset is known to be too large to handle efficiently using Python\'s built-in list due to both memory and performance constraints. Instead, we\'ll use Python\'s `array` module to manage this data efficiently. Your task is to write a function `efficient_array_operations(dataset: List[Union[int, float]]) -> Dict[str, Any]` that performs the following operations: 1. **Create an Array**: Create an array using the provided dataset. The type code should be \'i\' if all elements are integers, and \'d\' if they are floats. If the dataset contains both integers and floats, you should convert all integers to floats and use the type code \'d\'. 2. **Sum of Elements**: Compute and return the sum of elements in the array. 3. **Reverse the Array**: Reverse the array in place. 4. **Serialization**: Serialize the array to a byte string using the `tobytes` method. 5. **Element Count**: Count the occurrence of a randomly chosen element from the array. 6. **Memory Address and Size**: Retrieve the memory address and length (number of elements) of the array\'s buffer using `buffer_info`. The function should return a dictionary with keys: - `\\"array\\"`: The newly created array. - `\\"sum\\"`: The sum of elements. - `\\"reversed_array\\"`: The array after being reversed. - `\\"serialized_array\\"`: The byte string representation of the array. - `\\"element_count\\"`: The count of the randomly chosen element. - `\\"buffer_info\\"`: A tuple representing the memory address and length of the array\'s buffer. Input Format - `dataset`: A list of integers and/or floating-point numbers. The list may be large (up to (10^6) elements). Output Format - A dictionary containing the results of the operations described above. Example ```python dataset = [1, 2.0, 3, 4.5, 5] result = efficient_array_operations(dataset) print(result) ``` This should output a dictionary similar to: ```python { \'array\': array(\'d\', [1.0, 2.0, 3.0, 4.5, 5.0]), \'sum\': 15.5, \'reversed_array\': array(\'d\', [5.0, 4.5, 3.0, 2.0, 1.0]), \'serialized_array\': b\\"...\\", # Byte representation may vary \'element_count\': 1, # Example if the randomly chosen element is 1.0 \'buffer_info\': (address, 5) } ``` Notes: - Handle conversions between integer and float appropriately when both exist mixed in the dataset. - Serialization byte string may differ depending on the actual memory representation. - For element counting, use a random element from the array. Use the `random.choice` function from the `random` module to select the element. Constraints: - The `dataset` list may contain up to (10^6) elements. - You must ensure the type code is chosen appropriately based on the constraints given (integers vs. floats). - Performance should be considered, given the potential size of the dataset.","solution":"import array import random from typing import List, Union, Dict, Any def efficient_array_operations(dataset: List[Union[int, float]]) -> Dict[str, Any]: Perform various operations on a dataset using the array module. # Determine the type code if all(isinstance(x, int) for x in dataset): type_code = \'i\' else: type_code = \'d\' dataset = [float(x) for x in dataset] # Create the array arr = array.array(type_code, dataset) # Compute the sum of elements total_sum = sum(arr) # Reverse the array in place arr.reverse() reversed_array = arr # Serialize the array to a byte string serialized_array = arr.tobytes() # Count the occurrence of a randomly chosen element from the array random_element = random.choice(arr) element_count = arr.count(random_element) # Retrieve the memory address and length (number of elements) of the array\'s buffer buffer_info = arr.buffer_info() return { \\"array\\": arr, \\"sum\\": total_sum, \\"reversed_array\\": reversed_array, \\"serialized_array\\": serialized_array, \\"element_count\\": element_count, \\"buffer_info\\": buffer_info, }"},{"question":"# MIME Type Mapper You are tasked with developing a utility that intelligently predicts the MIME types of files, adds custom MIME types, and ensures consistency across MIME-type mappings. Write a function `mime_type_mapper` that: 1. Initializes the mimetypes module and sets up any custom types provided. 2. Takes a list of filenames (or URLs) and returns a dictionary mapping each filename to its respective MIME type and encoding. 3. For any MIME type prediction returning \'None\', should attempt to match a custom MIME type using standard types provided. 4. Utility to add custom MIME types. # Function Signature: ```python def mime_type_mapper(filenames: list, custom_types: dict) -> dict: pass ``` # Input 1. `filenames` (list): A list of filenames or URLs whose MIME types are to be guessed. 2. `custom_types` (dict): A dictionary with MIME types as keys and a list of corresponding extensions. # Output Returns a dictionary where each key is a filename (or URL) from the `filenames` list, and each value is a tuple containing: - The guessed MIME type (a string formatted as \'type/subtype\'; \'None\' if it can\'t be guessed) - The encoding (a string; \'None\' if no encoding) # Constraints 1. Filenames/URLs can be strings of standard paths or URLs. 2. Custom types should override the default mappings. 3. For inputs where the MIME type can\'t be guessed by default, it should fill in the custom MIME type mappings provided. # Example ```python filenames = [\\"document.txt\\", \\"archive.tar.gz\\", \\"photo.jpeg\\", \\"unknown.xyz\\"] custom_types = { \\"application/vnd.custom-type\\": [\\".xyz\\"] } output = mime_type_mapper(filenames, custom_types) # Expected output: # { # \\"document.txt\\": (\\"text/plain\\", None), # \\"archive.tar.gz\\": (\\"application/x-tar\\", \\"gzip\\"), # \\"photo.jpeg\\": (\\"image/jpeg\\", None), # \\"unknown.xyz\\": (\\"application/vnd.custom-type\\", None) # } ``` # Notes - Make sure to initialize the `mimetypes` module before performing any operations. - Use the `mimetypes` module\'s functions effectively to obtain MIME types. - Maintain the order of provided filenames in the output dictionary. - Ensure to add custom MIME types to the existing mappings by using `mimetypes.add_type`.","solution":"import mimetypes def mime_type_mapper(filenames: list, custom_types: dict) -> dict: Maps each filename to its respective MIME type and encoding, while using custom MIME type mappings where applicable. # Initialize the mimetypes module mimetypes.init() # Add custom MIME types to mimetypes for mime_type, extensions in custom_types.items(): for ext in extensions: mimetypes.add_type(mime_type, ext) # Dictionary to store the result result = {} for filename in filenames: mime_type, encoding = mimetypes.guess_type(filename) if mime_type is None: # If MIME type couldn\'t be guessed, check for custom types ext = filename.split(\'.\')[-1] custom_type_found = False for custom_mime, extensions in custom_types.items(): if any(filename.endswith(ext) for ext in extensions): mime_type = custom_mime custom_type_found = True break if not custom_type_found: mime_type = \'None\' if encoding is None: encoding = \'None\' result[filename] = (mime_type, encoding) return result"},{"question":"# Question: Suppose you are developing a C extension for Python, and you need to manage the lifetime of Python objects manually. Write a Python function that mimics the reference counting mechanisms provided by the Python C API. Your function should implement a simple object wrapper class in Python that maintains a reference count. Your task is to implement a class called `PyObjectWrapper` which has the following methods: 1. **`__init__(self, obj)`**: Initialize the wrapper with a given Python object and set the reference count to 1. 2. **`inc_ref(self)`**: Increment the reference count of the object. 3. **`dec_ref(self)`**: Decrement the reference count of the object. If the reference count reaches zero, the object should be deleted (i.e., set to `None`). 4. **`get_ref_count(self)`**: Return the current reference count. # Constraints: 1. You only need to manage a strong reference to a single Python object at a time (no weak references). 2. Assume that the object passed to `PyObjectWrapper` can be any valid Python object. # Example: ```python >>> wrapper = PyObjectWrapper([1, 2, 3]) >>> print(wrapper.get_ref_count()) 1 >>> wrapper.inc_ref() >>> print(wrapper.get_ref_count()) 2 >>> wrapper.dec_ref() >>> print(wrapper.get_ref_count()) 1 >>> wrapper.dec_ref() >>> print(wrapper.get_ref_count()) 0 >>> print(wrapper.obj) # Should be None None ``` # Note: * Your implementation should manage the lifecycle of the Python object similarly to how the Python C API handles reference counting. * Your class should not rely on Python\'s built-in garbage collector to manage the reference count.","solution":"class PyObjectWrapper: def __init__(self, obj): Initialize the wrapper with a given Python object. self.obj = obj self.ref_count = 1 def inc_ref(self): Increment the reference count of the object. self.ref_count += 1 def dec_ref(self): Decrement the reference count of the object. If the reference count reaches zero, the object should be deleted (set to None). self.ref_count -= 1 if self.ref_count == 0: self.obj = None def get_ref_count(self): Return the current reference count. return self.ref_count"},{"question":"<|Analysis Begin|> The provided documentation details the functionality offered by Python\'s `json` module, which provides methods for encoding and decoding JSON data. It includes explanations and examples of using methods such as: - `json.dumps(obj)`: Serialize `obj` to a JSON formatted `str`. - `json.dump(obj, fp)`: Serialize `obj` to a JSON formatted stream to `fp` (a file-like object). - `json.loads(s)`: Deserialize `s` (a `str`, `bytes`, or `bytearray` instance containing a JSON document) to a Python object. - `json.load(fp)`: Deserialize `fp` (a file-like object containing a JSON document) to a Python object. Additionally, it discusses extending the `json.JSONEncoder` and `json.JSONDecoder` classes to handle custom data types, and several parameters and options available in these serialization and deserialization methods. It also provides information about some pitfalls and behaviors that diverge from JSON standards, such as handling `NaN` and `Infinity` values. Using this documentation, we can design a coding question that requires students to demonstrate their understanding of JSON encoding and decoding, customizing encoding and decoding for non-standard data types, and manipulating JSON data. <|Analysis End|> <|Question Begin|> # JSON Custom Encoder and Decoder You are working on a Python application that needs to interact with JSON data. As part of the application, you need to handle complex numbers, which are not directly supported by JSON encoding and decoding. Your task is to extend the `json` module to include support for encoding and decoding complex number objects. Requirements: 1. **Custom Complex Encoder**: - Write a custom JSON encoder class named `ComplexEncoder` that extends `json.JSONEncoder`. - Override the `default` method to handle complex numbers by converting them to a dictionary with keys `\\"real\\"` and `\\"imag\\"` representing the real and imaginary parts of the complex number, respectively. - Ensure that other types of data are encoded using the default encoder method. 2. **Custom Complex Decoder**: - Write a function named `as_complex` that converts dictionaries with keys `\\"real\\"` and `\\"imag\\"` to complex numbers. - Use this function as the `object_hook` in the `json.loads` method to decode complex numbers from JSON strings. 3. **Function Definitions**: - Define a function `encode_complex_list(complex_list: list) -> str` that takes a list of complex numbers and returns its JSON string representation using `json.dumps` with the `ComplexEncoder`. - Define a function `decode_complex_list(json_string: str) -> list` that takes a JSON string representation of complex numbers and returns a list of complex numbers using `json.loads` with the `as_complex` object hook. Input and Output: - `encode_complex_list`: - Input: A list of complex numbers (e.g., `[1+2j, 3+4j, -1-1j]`). - Output: A JSON string representing the list of complex numbers (e.g., `\'[{\\"real\\": 1, \\"imag\\": 2}, {\\"real\\": 3, \\"imag\\": 4}, {\\"real\\": -1, \\"imag\\": -1}]\'`). - `decode_complex_list`: - Input: A JSON string representing a list of complex numbers (e.g., `\'[{\\"real\\": 1, \\"imag\\": 2}, {\\"real\\": 3, \\"imag\\": 4}, {\\"real\\": -1, \\"imag\\": -1}]\'`). - Output: A list of complex numbers (e.g., `[1+2j, 3+4j, -1-1j]`). Constraints: - Ensure that the encoding and decoding processes retain all parts of the complex numbers accurately. - Validate inputs where necessary and raise appropriate exceptions for invalid inputs. Example Usage: ```python complex_list = [1+2j, 3+4j, -1-1j] json_string = encode_complex_list(complex_list) print(json_string) # Output: \'[{\\"real\\": 1, \\"imag\\": 2}, {\\"real\\": 3, \\"imag\\": 4}, {\\"real\\": -1, \\"imag\\": -1}]\' decoded_list = decode_complex_list(json_string) print(decoded_list) # Output: [(1+2j), (3+4j), (-1-1j)] ``` Notes: - Make sure to handle the `TypeError` in the default encoder for unsupported types to fall back on the base implementation. - Ensure the JSON encoding only contains valid JSON datatypes by converting complex numbers to a dictionary format and that the decoding process accurately converts these dictionaries back into complex numbers.","solution":"import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def as_complex(dct): if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def encode_complex_list(complex_list): return json.dumps(complex_list, cls=ComplexEncoder) def decode_complex_list(json_string): return json.loads(json_string, object_hook=as_complex)"},{"question":"Question: ASCII Character Processing and Validation You are tasked to write a Python function that takes a string as input and performs multiple checks and manipulations using the functions provided by the `curses.ascii` module. # Function Specification **Function Name:** `process_ascii_string` **Parameters:** - `input_str` (str): A string of length 1 to 1000 containing ASCII characters. **Returns:** - `result` (List[Dict]): A list of dictionaries where each dictionary contains the following keys: - `\'character\'`: The original character. - `\'is_alnum\'`: A boolean indicating if the character is alphanumeric. - `\'is_alpha\'`: A boolean indicating if the character is alphabetic. - `\'is_ascii\'`: A boolean indicating if the character is an ASCII character. - `\'is_blank\'`: A boolean indicating if the character is a blank space or horizontal tab. - `\'is_control\'`: A boolean indicating if the character is a control character. - `\'unctrl\'`: The unctrl string representation of the character. # Constraints 1. Each character in `input_str` must be an ASCII character. 2. Use functions from the `curses.ascii` module to perform each check/manipulation. # Example ```python input_str = \\"A1 n\\" expected_output = [ { \'character\': \'A\', \'is_alnum\': True, \'is_alpha\': True, \'is_ascii\': True, \'is_blank\': False, \'is_control\': False, \'unctrl\': \'A\' }, { \'character\': \'1\', \'is_alnum\': True, \'is_alpha\': False, \'is_ascii\': True, \'is_blank\': False, \'is_control\': False, \'unctrl\': \'1\' }, { \'character\': \' \', \'is_alnum\': False, \'is_alpha\': False, \'is_ascii\': True, \'is_blank\': True, \'is_control\': False, \'unctrl\': \' \' }, { \'character\': \'n\', \'is_alnum\': False, \'is_alpha\': False, \'is_ascii\': True, \'is_blank\': False, \'is_control\': True, \'unctrl\': \'^J\' } ] assert process_ascii_string(input_str) == expected_output ``` # Notes - You must use `curses.ascii` module functions to check each condition and to get the unctrl string representation. - The result list should maintain the order of characters as in the input string. - Avoid using any other external libraries apart from `curses.ascii`.","solution":"import curses.ascii def process_ascii_string(input_str): Process each character in the input string and return various properties using curses.ascii functions. Parameters: input_str (str): A string of length 1 to 1000 containing ASCII characters. Returns: result (List[Dict]): A list of dictionaries with character properties. result = [] for char in input_str: result.append({ \'character\': char, \'is_alnum\': curses.ascii.isalnum(char), \'is_alpha\': curses.ascii.isalpha(char), \'is_ascii\': curses.ascii.isascii(char), \'is_blank\': curses.ascii.isblank(char), \'is_control\': curses.ascii.iscntrl(char), \'unctrl\': curses.ascii.unctrl(char) }) return result"},{"question":"# Question: Creating and Applying Custom Color Palettes using Seaborn You are given a dataset containing the average temperatures of various cities over different months. Your task is to create a custom color palette using Seaborn and apply it to a heatmap visualizing this data. Instructions 1. **Create the DataFrame**: - The DataFrame should have 6 cities (`CityA`, `CityB`, `CityC`, `CityD`, `CityE`, `CityF`) and 12 months (`January` to `December`). The values should represent average temperatures (in Celsius). 2. **Generate a Custom Color Palette**: - Create a custom sequential color palette that goes from light gray to a color of your choice. The color should be specified using a hex code. 3. **Apply the Color Palette**: - Use the custom color palette to create a heatmap that visualizes the average temperatures. Ensure that the heatmap uses the palette as a continuous colormap. 4. **Additional Customization**: - Set a proper title and axis labels for the heatmap for clarity. - Use the `sns.set_theme()` function to set a theme for the plot. Requirements - The DataFrame should be created programmatically and populated with random temperature data ranging between -10 and 40 degrees Celsius. - The heatmap should correctly reflect the temperatures using the custom color palette. - The solution should be implemented in a Jupyter notebook. Example Output The final output should be a heatmap where each cell\'s color intensity corresponds to its temperature, based on the custom color palette created. Expected Input and Output Formats - **Input**: No input required; the dataset is generated within the code. - **Output**: A heatmap plot visualizing the temperature data. ```python # Example code structure to help guide your implementation import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # Step 1: Create the DataFrame with random temperature data cities = [\\"CityA\\", \\"CityB\\", \\"CityC\\", \\"CityD\\", \\"CityE\\", \\"CityF\\"] months = [\\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"] # Create a DataFrame with random temperature data np.random.seed(0) # For reproducibility temperature_data = np.random.uniform(-10, 40, (6, 12)) df = pd.DataFrame(temperature_data, index=cities, columns=months) # Step 2: Generate a custom color palette custom_palette = sns.light_palette(\\"#79C\\", as_cmap=True) # Step 3: Apply the custom color palette to a heatmap sns.set_theme() plt.figure(figsize=(10, 6)) heatmap = sns.heatmap(df, cmap=custom_palette, annot=True, fmt=\\".1f\\", cbar_kws={\'label\': \'Temperature (°C)\'}) # Additional Customization plt.title(\\"Average Monthly Temperatures of Cities\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"City\\") plt.show() ``` Ensure to include all necessary imports and function calls in your solution.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def create_temperature_heatmap(): # Step 1: Create the DataFrame with random temperature data cities = [\\"CityA\\", \\"CityB\\", \\"CityC\\", \\"CityD\\", \\"CityE\\", \\"CityF\\"] months = [\\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"] # Create a DataFrame with random temperature data np.random.seed(0) # For reproducibility temperature_data = np.random.uniform(-10, 40, (6, 12)) df = pd.DataFrame(temperature_data, index=cities, columns=months) # Step 2: Generate a custom color palette custom_palette = sns.light_palette(\\"#79C\\", as_cmap=True) # Step 3: Apply the custom color palette to a heatmap sns.set_theme() plt.figure(figsize=(10, 6)) heatmap = sns.heatmap(df, cmap=custom_palette, annot=True, fmt=\\".1f\\", cbar_kws={\'label\': \'Temperature (°C)\'}) # Additional Customization plt.title(\\"Average Monthly Temperatures of Cities\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"City\\") plt.show() return df # Calling the function to generate the heatmap -- Remove or comment this line when running tests #create_temperature_heatmap()"},{"question":"**Objective:** Demonstrate your understanding of the `http.client` module in Python 3.10 by implementing various functionalities involving HTTP operations. Problem Statement: You are tasked with implementing a simple HTTP client that can send GET and POST requests to a server, parse the responses and handle potential errors. The HTTP client should have the following functionalities: 1. **Send a GET Request**: Implement a method `send_get_request(url)` that takes a URL as input, makes a GET request to the given URL, and returns the response content and status code. 2. **Send a POST Request**: Implement a method `send_post_request(url, params)` that takes a URL and a dictionary of parameters as input, makes a POST request to the given URL with the provided parameters, and returns the response content and status code. 3. **Handle Errors**: The methods should handle the following errors gracefully and return appropriate messages: - `http.client.InvalidURL`: Indicate the URL is invalid. - `http.client.RemoteDisconnected`: Indicate the connection was closed by the server. 4. **Parse Headers**: Implement a method `parse_response_headers(url)` that takes a URL as input, makes a GET request to the given URL, and returns the headers in a dictionary format. 5. **Connection Debugging**: Implement an optional debugging mode that can be set to print debug information for each connection. Input: - `send_get_request(url)`: A URL string. - `send_post_request(url, params)`: A URL string and a dictionary of parameters. - `parse_response_headers(url)`: A URL string. Output: - `send_get_request(url)`: A tuple of (response content, status code). - `send_post_request(url, params)`: A tuple of (response content, status code). - `parse_response_headers(url)`: A dictionary of response headers. Constraints: - Use `http.client.HTTPConnection` and `http.client.HTTPSConnection`. - Handle specified exceptions and any other general exceptions appropriately. - Ensure all function implementations are clear, concise, and efficient. Example: ```python # Example usage of the HTTP client url_get = \\"http://www.example.com\\" url_post = \\"http://www.example.com/login\\" params = {\\"username\\": \\"user\\", \\"password\\": \\"pass\\"} try: content, status = send_get_request(url_get) print(f\\"GET Response: {content} (Status: {status})\\") except Exception as e: print(f\\"Error: {str(e)}\\") try: content, status = send_post_request(url_post, params) print(f\\"POST Response: {content} (Status: {status})\\") except Exception as e: print(f\\"Error: {str(e)}\\") try: headers = parse_response_headers(url_get) print(f\\"Headers: {headers}\\") except Exception as e: print(f\\"Error: {str(e)}\\") ``` Your task is to complete the methods `send_get_request`, `send_post_request`, and `parse_response_headers` according to the specifications provided.","solution":"import http.client import urllib.parse import json def send_get_request(url): try: parsed_url = urllib.parse.urlparse(url) connection = http.client.HTTPSConnection(parsed_url.netloc) connection.request(\\"GET\\", parsed_url.path) response = connection.getresponse() content = response.read().decode() return content, response.status except http.client.InvalidURL: return \\"Invalid URL\\", 400 except http.client.RemoteDisconnected: return \\"Connection closed by the server\\", 500 except Exception as e: return str(e), 500 def send_post_request(url, params): try: parsed_url = urllib.parse.urlparse(url) connection = http.client.HTTPSConnection(parsed_url.netloc) headers = {\'Content-type\': \'application/x-www-form-urlencoded\'} params_encoded = urllib.parse.urlencode(params) connection.request(\\"POST\\", parsed_url.path, params_encoded, headers) response = connection.getresponse() content = response.read().decode() return content, response.status except http.client.InvalidURL: return \\"Invalid URL\\", 400 except http.client.RemoteDisconnected: return \\"Connection closed by the server\\", 500 except Exception as e: return str(e), 500 def parse_response_headers(url): try: parsed_url = urllib.parse.urlparse(url) connection = http.client.HTTPSConnection(parsed_url.netloc) connection.request(\\"GET\\", parsed_url.path) response = connection.getresponse() headers = dict(response.getheaders()) return headers except http.client.InvalidURL: return {\\"Error\\": \\"Invalid URL\\"} except http.client.RemoteDisconnected: return {\\"Error\\": \\"Connection closed by the server\\"} except Exception as e: return {\\"Error\\": str(e)}"},{"question":"# Advanced Python Data Types: Event Scheduler You have been tasked with implementing an event scheduler that helps manage scheduled events effectively. The objective is to manage events through date and time manipulation, ensuring no overlapping events using the `datetime` module, and efficiently manage and access events using `collections` and `bisect`. Requirements 1. **Event Class Implementation** - Create an `Event` class with attributes: `name` (str), `start_time` (datetime), and `end_time` (datetime). - Ensure that the `start_time` is always before the `end_time`. 2. **Scheduler Class Implementation** - The `Scheduler` class should manage multiple `Event` instances. - It should support the following methods: - `add_event(event: Event) -> bool`: Adds an event to the scheduler. Returns `False` if the event overlaps with any existing events; otherwise, adds the event and returns `True`. - `remove_event(event_name: str) -> bool`: Removes an event by its name. Returns `True` if the event was removed successfully, otherwise `False`. - `get_events(start: datetime, end: datetime) -> List[Event]`: Returns a list of events that are scheduled within a given time frame (inclusive). Input and Output Formats 1. **Event Class** - Input: `name` as a string, `start_time` and `end_time` as `datetime` instances. - Output: `Event` instance with validation of start and end times. 2. **Scheduler Class** - Methods Inputs/Outputs: - `add_event(event: Event) -> bool`: Accepts an `Event` instance, returns a boolean. - `remove_event(event_name: str) -> bool`: Accepts event name as a string, returns a boolean. - `get_events(start: datetime, end: datetime) -> List[Event]`: Accepts start and end times as `datetime` instances, returns a list of `Event` instances. Example ```python from datetime import datetime # Create an event event1 = Event(name=\\"Meeting\\", start_time=datetime(2023, 10, 1, 10, 0), end_time=datetime(2023, 10, 1, 11, 0)) # Initialize the scheduler scheduler = Scheduler() # Add event to the scheduler scheduler.add_event(event1) # Returns: True # Try adding an overlapping event event2 = Event(name=\\"Conference\\", start_time=datetime(2023, 10, 1, 10, 30), end_time=datetime(2023, 10, 1, 12, 0)) scheduler.add_event(event2) # Returns: False # Remove an event scheduler.remove_event(\\"Meeting\\") # Returns: True # List events in a specific timeframe events_within_range = scheduler.get_events(datetime(2023, 10, 1, 9, 0), datetime(2023, 10, 1, 12, 0)) # Returns: [] ``` Constraints - Events are considered overlapping if they share any time, partially or fully. - You may use `collections.deque` for efficient event management. - You may use `bisect` for efficient insertion and searching of events. Performance Requirements - The `add_event`, `remove_event`, and `get_events` methods should be optimized for performance, with expected use of `O(log n)` operations where applicable.","solution":"from datetime import datetime from collections import deque from bisect import bisect_left, bisect_right from typing import List class Event: def __init__(self, name: str, start_time: datetime, end_time: datetime): if start_time >= end_time: raise ValueError(\\"start_time must be before end_time\\") self.name = name self.start_time = start_time self.end_time = end_time class Scheduler: def __init__(self): self.events = deque() def add_event(self, event: Event) -> bool: # Check for any overlapping event for e in self.events: if not (event.end_time <= e.start_time or event.start_time >= e.end_time): return False # Insert event in order position = bisect_right([e.start_time for e in self.events], event.start_time) self.events.insert(position, event) return True def remove_event(self, event_name: str) -> bool: for idx, event in enumerate(self.events): if event.name == event_name: del self.events[idx] return True return False def get_events(self, start: datetime, end: datetime) -> List[Event]: return [event for event in self.events if event.start_time < end and event.end_time > start]"},{"question":"# Python Instance and Method Binding Assessment You are tasked with creating a Python class that simulates the internal handling of instance and method objects, akin to how Python internally manages method binding to class instances. This exercise will assess your understanding of Python\'s object-oriented programming and method management. Objectives 1. **Create a class `MyClass`**: - This class should have an initializer to set attributes. - Add at least three instance methods that perform simple operations (e.g., arithmetic operations). 2. **Utility Functions**: - Write a function `bind_instance_method(func, instance)` that mimics `PyInstanceMethod_New`: - The `func` argument is any callable object. - The `instance` argument is an instance of a class. - This function should return a callable method bound to the given instance. - Write a function `get_instance_method_function(method)` that mimics `PyInstanceMethod_Function`: - The `method` argument is an instance method. - This function should return the original function associated with the instance method. 3. **Create a class `MethodTester`**: - This class should have a method `test_method_binding`: - Accept a class instance and a function. - Use the `bind_instance_method` to bind the function to the instance. - Demonstrate the method call on the instance and return the result. Input Format The input consists of those required by the utility functions and MethodTester class: - For `bind_instance_method`: - A callable function. - An instance of `MyClass`. - For `get_instance_method_function`: - A bound method. - For `test_method_binding`: - An instance of `MyClass`. - A function to be bound and tested. Output Format The result should quantitatively prove the binding and correct method execution. Constraints - Assume the functions and class methods perform operations on integers or strings. - Focus on correctly utilizing the concepts of method binding and callable objects. - Avoid using direct calls to Python\'s native capabilities like `staticmethod` or `classmethod`. Example ```python class MyClass: def __init__(self, value): self.value = value def add(self, x): return self.value + x def bind_instance_method(func, instance): # Code to mimic `PyInstanceMethod_New` ... def get_instance_method_function(method): # Code to mimic `PyInstanceMethod_Function` ... class MethodTester: def test_method_binding(self, instance, func): bound_method = bind_instance_method(func, instance) return bound_method(10) # example usage # Example usage: obj = MyClass(5) tester = MethodTester() def multiply(instance, x): return instance.value * x # Perform the binding test result = tester.test_method_binding(obj, multiply) # should return 50 if bound correctly ``` Submit your implementation for evaluation.","solution":"class MyClass: def __init__(self, value): self.value = value def add(self, x): return self.value + x def subtract(self, x): return self.value - x def multiply(self, x): return self.value * x def bind_instance_method(func, instance): Mimics PyInstanceMethod_New by binding a function to an instance. def bound_method(*args, **kwargs): return func(instance, *args, **kwargs) return bound_method def get_instance_method_function(method): Mimics PyInstanceMethod_Function by retrieving the original function associated with the instance method. return method.__func__ class MethodTester: def test_method_binding(self, instance, func): Binds the function to the instance and demonstrates the method call. bound_method = bind_instance_method(func, instance) return bound_method(10) # example usage to demonstrate binding"},{"question":"Coding Assessment Question # Objective Your task is to implement a custom metric handler in PyTorch Elastic that can log metrics to a file and demonstrate its usage by configuring and sending some sample metrics. This will test your understanding of the PyTorch Elastic metrics API and your ability to work with metric handlers in a distributed setting. # Problem Statement 1. Implement a class `FileMetricHandler` that inherits from `MetricHandler`. This handler should: - Initialize with a file path where metrics will be logged. - Override the `emit` method to write the metric into the specified file in a specified format. 2. Use the `FileMetricHandler` within PyTorch\'s distributed elastic metrics context to log the following metrics: - `train_loss`: A hypothetical training loss value. - `eval_accuracy`: A hypothetical evaluation accuracy value. # Detailed Instructions 1. Implement the `FileMetricHandler` class. 2. Configure PyTorch Elastic to use your custom `FileMetricHandler`. 3. Emit the following metrics using the `put_metric` method: - `train_loss`, with a value of 0.05. - `eval_accuracy`, with a value of 0.92. # Class: `FileMetricHandler` Method: __init__ ```python def __init__(self, file_path: str): Initializes the FileMetricHandler with a file path. Args: - file_path (str): Path of the file where metrics will be logged. pass ``` Method: emit ```python def emit(self, name: str, value: float, unit: str): Logs the given metric with name, value, and unit into the specified file. Args: - name (str): The name of the metric. - value (float): The value of the metric. - unit (str): The unit of the metric (e.g., \'s\', \'ms\', \'count\'). pass ``` # Example Usage ```python from torch.distributed.elastic.metrics import configure, put_metric class FileMetricHandler(MetricHandler): def __init__(self, file_path: str): self.file_path = file_path def emit(self, name: str, value: float, unit: str): with open(self.file_path, \'a\') as f: f.write(f\\"{name}: {value} {unit}n\\") # Configure PyTorch Elastic to use the FileMetricHandler configure(handlers=[FileMetricHandler(\\"/path/to/metrics.log\\")]) # Emit some sample metrics put_metric(\\"train_loss\\", 0.05, \\"none\\") put_metric(\\"eval_accuracy\\", 0.92, \\"none\\") ``` # Constraints and Requirements 1. You must implement the `FileMetricHandler` class with the specified methods. 2. You must configure the custom metric handler and emit the sample metrics in your code. 3. Ensure to handle file operations properly (e.g., append mode, proper formatting). 4. Make sure your solution is efficient and doesn\'t produce any runtime exceptions. # Input Your input will be the custom metric handler class and configurations in the code. # Output There is no direct output expected from your function. Instead, the result should be observed in the specified log file where the metrics are written. # Evaluation Criteria - Correctness of the class implementation. - Proper configuration and usage of the custom metric handler. - Efficiency and readability of the code. - Proper handling of file operations.","solution":"from torch.distributed.elastic.metrics import MetricHandler, configure, put_metric class FileMetricHandler(MetricHandler): def __init__(self, file_path: str): self.file_path = file_path def emit(self, name: str, value: float, unit: str): with open(self.file_path, \'a\') as f: f.write(f\\"{name}: {value} {unit}n\\") # Example usage def main(): # Configure PyTorch Elastic to use the FileMetricHandler handler = FileMetricHandler(\\"/tmp/metrics.log\\") configure(handlers=[handler]) # Emit some sample metrics put_metric(\\"train_loss\\", 0.05, \\"none\\") put_metric(\\"eval_accuracy\\", 0.92, \\"none\\") if __name__ == \'__main__\': main()"},{"question":"**Objective:** Develop a multi-threaded Python application using the `_thread` module to demonstrate understanding of thread management and synchronization with locks. **Problem Statement:** You are tasked with creating a `ThreadSafeCounter` class that supports multi-threaded increment and decrement operations on a counter, ensuring thread safety using locks. 1. Implement the `ThreadSafeCounter` class with the following methods: - `__init__(self, initial_value=0)`: Initializes the counter to the given initial value (default is 0) and sets up a lock. - `increment(self)`: Increments the counter by 1, ensuring thread safety. - `decrement(self)`: Decrements the counter by 1, ensuring thread safety. - `get_value(self)`: Returns the current value of the counter. 2. Write a function `counter_test` that: - Takes two arguments, `increments` and `decrements`. - Creates an instance of `ThreadSafeCounter`. - Starts two threads: one for performing `increments` number of increments and another for performing `decrements` number of decrements. - Waits for both threads to complete. - Returns the final value of the counter. **Constraints:** - Use only the `_thread` module and its primitives for threading. - Ensure that all modifications to the counter are thread-safe. - Handle any exceptions that may arise during thread execution. **Input:** ```python increments: int # Number of times to increment the counter. decrements: int # Number of times to decrement the counter. ``` **Output:** ```python int # The final value of the counter after all increments and decrements. ``` **Example Usage:** ```python def counter_test_example(): final_value = counter_test(1000, 800) print(final_value) # Expected to print 200 (1000 increments - 800 decrements) ``` **Solution Template:** ```python import _thread class ThreadSafeCounter: def __init__(self, initial_value=0): self._value = initial_value self._lock = _thread.allocate_lock() def increment(self): with self._lock: self._value += 1 def decrement(self): with self._lock: self._value -= 1 def get_value(self): with self._lock: return self._value def counter_test(increments, decrements): counter = ThreadSafeCounter() def increment_counter(): for _ in range(increments): counter.increment() def decrement_counter(): for _ in range(decrements): counter.decrement() thread1 = _thread.start_new_thread(increment_counter, ()) thread2 = _thread.start_new_thread(decrement_counter, ()) # Simple busy-wait loop for both threads to finish. import time time.sleep(0.1) # Adjust sleep time if necessary for larger increments/decrements return counter.get_value() ``` **Note:** - The provided sleep mechanism (`time.sleep(0.1)`) is a simple way to wait for thread completion for this exercise. In real applications, more robust thread synchronization techniques should be used.","solution":"import _thread class ThreadSafeCounter: def __init__(self, initial_value=0): self._value = initial_value self._lock = _thread.allocate_lock() def increment(self): with self._lock: self._value += 1 def decrement(self): with self._lock: self._value -= 1 def get_value(self): with self._lock: return self._value def counter_test(increments, decrements): counter = ThreadSafeCounter() def increment_counter(): for _ in range(increments): counter.increment() def decrement_counter(): for _ in range(decrements): counter.decrement() thread1 = _thread.start_new_thread(increment_counter, ()) thread2 = _thread.start_new_thread(decrement_counter, ()) # Simple busy-wait loop for both threads to finish. import time time.sleep(0.1) # Adjust sleep time if necessary for larger increments/decrements return counter.get_value()"},{"question":"**Question: Implementing a Generic Collection with Enhanced Type Safety** You are required to implement a class `LoggedSet` that extends the functionality of Python `set` by adding logging capabilities. This class should be generic and provide type annotations to ensure type safety of elements it holds. Additionally, it should log every addition and removal operation performed on the set. Your implementation should: 1. Define a generic type variable to specify the type of elements in the set. 2. Use the `typing.Generic` base class to create a generic `LoggedSet`. 3. Override the appropriate methods to add logging functionality. 4. Use type hints to ensure all operations are type-checked for the specified element type. 5. Provide a test case to demonstrate the usage and type safety of your `LoggedSet`. Here is the expected interface: ```python from typing import TypeVar, Generic T = TypeVar(\'T\') # Declare a type variable class LoggedSet(Generic[T]): def __init__(self) -> None: self._set = set() def add(self, element: T) -> None: Add an element to the set and log the operation. pass def remove(self, element: T) -> None: Remove an element from the set and log the operation. pass def __contains__(self, element: T) -> bool: Check if the set contains an element. pass def __repr__(self) -> str: Return the string representation of the set. pass # Test case if __name__ == \\"__main__\\": log_set = LoggedSet[int]() log_set.add(10) log_set.add(20) log_set.remove(10) print(20 in log_set) print(log_set) ``` **Constraints and Requirements**: - The `LoggedSet` class must be able to hold elements of any type specified by the type variable `T`. - Use `print` statements to log every addition and removal operation as `Added: <element>` or `Removed: <element>`. - Ensure that type hints are used appropriately, and the implementation passes static type checking. - Aim for a clean and readable implementation with good use of type annotations. **Expected Output**: ```plaintext Added: 10 Added: 20 Removed: 10 True LoggedSet({20}) ```","solution":"from typing import TypeVar, Generic T = TypeVar(\'T\') # Declare a type variable class LoggedSet(Generic[T]): def __init__(self) -> None: self._set = set() def add(self, element: T) -> None: Add an element to the set and log the operation. self._set.add(element) print(f\\"Added: {element}\\") def remove(self, element: T) -> None: Remove an element from the set and log the operation. self._set.remove(element) print(f\\"Removed: {element}\\") def __contains__(self, element: T) -> bool: Check if the set contains an element. return element in self._set def __repr__(self) -> str: Return the string representation of the set. return f\\"LoggedSet({self._set})\\""},{"question":"<|Analysis Begin|> The `ipaddress` module provides classes and functions for handling and manipulating IP addresses and networks in both IPv4 and IPv6. The primary objects in this module include IP addresses, networks, and interfaces, and the module provides both factory functions and class constructors for creating them. The tasks that can be performed using this module include: - Creating IP address objects from strings or integers. - Defining network objects and creating them with customizable prefix lengths. - Creating interface objects that associate an address with a specific network. - Inspecting the version of IP addresses (IPv4 or IPv6). - Extracting related network information, such as netmasks and hostmasks. - Iterating over hosts within a network. - Treating network objects as lists of addresses. - Performing containment tests to check if an address is within a network. - Comparing IP address objects. - Integrating IP addresses with other modules like `socket`. - Handling value parsing errors with detailed exception messages. For a coding assessment, a question could require the implementation of a function that combines several of these tasks. The challenge should ensure that students demonstrate a solid understanding of creating and manipulating different types of objects provided by the `ipaddress` module, as well as performing relevant operations. <|Analysis End|> <|Question Begin|> # **IP Address and Network Manipulation** **Objective:** Implement a function `summarize_ip_info(ip_addresses)` that takes a list of IP address strings, determines their respective networks, and provides a summary of key information. **Function Signature:** ```python from typing import List, Dict, Union import ipaddress def summarize_ip_info(ip_addresses: List[str]) -> Dict[str, Union[str, int]]: pass ``` **Input:** - `ip_addresses`: A List of IP address strings (e.g., `[\\"192.0.2.1\\", \\"2001:DB8::1\\"]`). The IP addresses in the list can be either IPv4 or IPv6. **Output:** - A dictionary with the following keys: - `\\"total_ipv4\\"`: The number of IPv4 addresses in the input list. - `\\"total_ipv6\\"`: The number of IPv6 addresses in the input list. - `\\"networks\\"`: A list of strings representing the network each IP belongs to in CIDR notation (e.g., `\\"192.0.2.0/24\\"`, `\\"2001:db8::/96\\"`). - `\\"largest_network\\"`: The network (in CIDR notation) that contains the most IP addresses among the input. If there are ties, return any one of the largest networks. **Constraints:** - All IP address strings are guaranteed to be valid. - You can assume a network prefix for IPv4 is fixed at /24 and for IPv6 at /96 when generating networks, if not explicitly mentioned. **Example:** ```python input_addresses = [\\"192.0.2.1\\", \\"192.0.2.15\\", \\"2001:db8::1\\", \\"2001:db8::2\\", \\"192.0.3.1\\"] expected_output = { \\"total_ipv4\\": 3, \\"total_ipv6\\": 2, \\"networks\\": [\\"192.0.2.0/24\\", \\"2001:db8::/96\\", \\"192.0.3.0/24\\"], \\"largest_network\\": \\"192.0.2.0/24\\" } assert summarize_ip_info(input_addresses) == expected_output ``` **Task:** Implement the `summarize_ip_info` function that meets the specified requirements. Make sure to handle mixed lists of IPv4 and IPv6 addresses correctly and utilize the features of the `ipaddress` module for creating and inspecting IP address objects and networks.","solution":"from typing import List, Dict, Union import ipaddress def summarize_ip_info(ip_addresses: List[str]) -> Dict[str, Union[str, int]]: ipv4_count = 0 ipv6_count = 0 network_dict = {} for ip in ip_addresses: ip_obj = ipaddress.ip_address(ip) if ip_obj.version == 4: ipv4_count += 1 network = ipaddress.IPv4Network(ip + \\"/24\\", strict=False) else: ipv6_count += 1 network = ipaddress.IPv6Network(ip + \\"/96\\", strict=False) network_str = str(network) if network_str in network_dict: network_dict[network_str] += 1 else: network_dict[network_str] = 1 # Find the largest network largest_network = max(network_dict, key=network_dict.get) return { \\"total_ipv4\\": ipv4_count, \\"total_ipv6\\": ipv6_count, \\"networks\\": list(network_dict.keys()), \\"largest_network\\": largest_network }"},{"question":"# Pytorch Coding Assessment **Objective**: Evaluate the ability to configure and utilize PyTorch backend settings for optimizing performance on different hardware platforms. **Problem Statement**: You are tasked with setting up a PyTorch environment optimized for a given hardware setup. Specifically, you need to: 1. Check if CUDA and cuDNN are available. 2. Enable the use of TensorFloat-32 (TF32) for matrix multiplications on CUDA devices, but only if the devices are Ampere or newer. 3. Enable cuDNN\'s benchmarking to find the fastest convolution algorithms. 4. Check how many plans are in the cuFFT plan cache for the current CUDA device. 5. Clear the cuFFT plan cache for the current CUDA device if the number of plans exceeds a given threshold. **Input**: None. **Output**: - Print whether CUDA and cuDNN are available. - Print the current status of TF32 support on CUDA. - Print the number of plans in the cuFFT plan cache. - Print a message confirming that the cache has been cleared if the number of plans exceeded the threshold. **Function Signature**: ```python def optimize_pytorch_backend(): pass ``` **Constraints**: - You must use `torch.backends.cuda` to access CUDA-related settings. - For checking the number of cuFFT plans, use `torch.backends.cuda.cufft_plan_cache.size`. - Assume a threshold of 100 plans for clearing the cuFFT plan cache. **Example**: ```python >>> optimize_pytorch_backend() CUDA is available: True cuDNN is available: True TensorFloat-32 support is enabled: True Number of plans in the cuFFT plan cache: 120 Clearing the cuFFT plan cache because it exceeded the threshold of 100 plans. ``` **Hints**: - Use `torch.backends.cuda.is_built()` and `torch.backends.cudnn.is_available()` to check the availability of CUDA and cuDNN, respectively. - Use `torch.backends.cuda.matmul.allow_tf32` to enable or check the status of TF32. - Use `torch.backends.cudnn.benchmark` to enable benchmarking in cuDNN. - Use `torch.backends.cuda.cufft_plan_cache.size` to check the number of plans in the cuFFT plan cache. - Use `torch.backends.cuda.cufft_plan_cache.clear()` to clear the cuFFT plan cache.","solution":"import torch def optimize_pytorch_backend(): # Check if CUDA and cuDNN are available cuda_available = torch.cuda.is_available() cudnn_available = torch.backends.cudnn.is_available() print(f\\"CUDA is available: {cuda_available}\\") print(f\\"cuDNN is available: {cudnn_available}\\") if cuda_available: if torch.cuda.get_device_properties(0).major >= 8: # Ampere architecture or newer torch.backends.cuda.matmul.allow_tf32 = True tf32_enabled = torch.backends.cuda.matmul.allow_tf32 print(f\\"TensorFloat-32 support is enabled: {tf32_enabled}\\") else: print(f\\"TensorFloat-32 support is not enabled: Device does not support TF32\\") # Enable cuDNN benchmarking torch.backends.cudnn.benchmark = True # Check the number of plans in the cuFFT plan cache fft_plan_cache_size = torch.backends.cuda.cufft_plan_cache.size print(f\\"Number of plans in the cuFFT plan cache: {fft_plan_cache_size}\\") # Clear the cuFFT plan cache if it exceeds the threshold of 100 plans threshold = 100 if fft_plan_cache_size > threshold: torch.backends.cuda.cufft_plan_cache.clear() print(f\\"Clearing the cuFFT plan cache because it exceeded the threshold of {threshold} plans.\\")"},{"question":"# Question: Manipulating Randomness in PyTorch In this exercise, you will write functions to control and generate randomness using the PyTorch `torch.random` module. The task involves controlling random seeds, generating random tensors, and managing random states. Requirements: 1. **Function 1: `set_seed`** - **Input**: An integer `seed`. - **Output**: None. - **Description**: This function should set the seed for generating random numbers, ensuring reproducibility. 2. **Function 2: `generate_random_tensor`** - **Input**: Two integers `size` and `low`, and one integer `high`. - **Output**: A 1-dimensional PyTorch tensor of length `size` with random integers from the range `[low, high)`. - **Description**: This function should generate and return a tensor with `size` random integers in the specified range. 3. **Function 3: `save_random_state`** - **Input**: None. - **Output**: The current random state. - **Description**: This function should save and return the current random state. 4. **Function 4: `load_random_state`** - **Input**: The random state that was previously saved. - **Output**: None. - **Description**: This function should load the specified random state to restore the random number generator to a previous state. Example Usage: ```python import torch # Set the seed set_seed(42) # Generate a random tensor tensor = generate_random_tensor(5, 0, 10) print(tensor) # Expected output: tensor with 5 random integers between 0 and 9 # Save the current random state state = save_random_state() # Generate another random tensor tensor2 = generate_random_tensor(5, 10, 20) print(tensor2) # Expected output: tensor with 5 random integers between 10 and 19 # Load the previous random state load_random_state(state) # Generate the random tensor again from the previous state tensor3 = generate_random_tensor(5, 0, 10) print(tensor3) # Expected to be the same as the first tensor ``` Constraints: - Ensure that the functions perform as expected for the valid range of integer inputs, and handle any edge cases appropriately. - The random tensor generation should be efficient and should not use any loops. Implement the four functions with the specified requirements to demonstrate your understanding of controlling and managing randomness in PyTorch.","solution":"import torch def set_seed(seed): Sets the seed for generating random numbers. Args: seed (int): The seed for the random number generator. torch.manual_seed(seed) def generate_random_tensor(size, low, high): Generates a 1-dimensional PyTorch tensor of specified size with random integers in the given range [low, high). Args: size (int): Length of the tensor to generate. low (int): Lower bound of the random integers (inclusive). high (int): Upper bound of the random integers (exclusive). Returns: torch.Tensor: Tensor with random integers. return torch.randint(low, high, (size,)) def save_random_state(): Saves the current random state. Returns: torch.ByteTensor: The current random state. return torch.random.get_rng_state() def load_random_state(state): Loads the specified random state. Args: state (torch.ByteTensor): The random state to load. torch.random.set_rng_state(state)"},{"question":"**Challenge Problem: Advanced Seaborn Palette Customization and Visualization** **Objective:** Write a Python function that leverages seaborn\'s `blend_palette` method to create a custom color palette and use it to visualize data with a complex seaborn plot. **Function Signature:** ```python def custom_seaborn_plot(data, colors, continuous=False, plot_type=\'scatter\', **kwargs): Create a seaborn plot with a custom blended color palette. Parameters: - data (pd.DataFrame): The dataset to be visualized. - colors (list): A list of strings, where each string represents a color. - continuous (bool): If True, the palette will be a continuous colormap. Defaults to False. - plot_type (str): The type of seaborn plot to create (e.g., \'scatter\', \'line\', \'bar\'). Defaults to \'scatter\'. - **kwargs: Additional keyword arguments to pass to the seaborn plotting function. Returns: - None: Displays the generated seaborn plot. ``` **Requirements:** 1. **Input:** - `data`: A Pandas DataFrame containing the data to be visualized. - `colors`: A list of color strings. The list can have any format supported by seaborn such as color names, hex codes, or xkcd color names. - `continuous`: A boolean flag to determine if the palette is continuous (`True`) or discrete (`False`). - `plot_type`: A string indicating the type of plot to create. Acceptable values are \'scatter\', \'line\', and \'bar\'. - Additional keyword arguments should be passed to the seaborn plotting function to customize the plot further. 2. **Constraints:** - The DataFrame `data` must have columns compatible with the chosen plot type. - The color list `colors` should have at least two elements. - Implement error handling to cover invalid color formats, plot types, or insufficient data. 3. **Functionality:** - Use `seaborn.blend_palette` to create a color palette (discrete or continuous based on the `continuous` parameter). - Depending on `plot_type`, create an appropriate seaborn plot using the custom palette. - For scatter plots, use `sns.scatterplot()`. - For line plots, use `sns.lineplot()`. - For bar plots, use `sns.barplot()`. - Finally, display the plot with the specified customizations. 4. **Output:** - The function does not return any value but displays the generated seaborn plot. **Example:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Example DataFrame data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5], \'y\': [5, 4, 3, 2, 1], \'hue\': [\'A\', \'B\', \'A\', \'B\', \'A\'] }) # Example usage custom_seaborn_plot(data, colors=[\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"], continuous=False, plot_type=\'scatter\', x=\'x\', y=\'y\', hue=\'hue\') plt.show() ``` **Notes:** - Make sure to import seaborn, matplotlib, and pandas packages. - Customize plot as per further requirements using `**kwargs`.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plot(data, colors, continuous=False, plot_type=\'scatter\', **kwargs): Create a seaborn plot with a custom blended color palette. Parameters: - data (pd.DataFrame): The dataset to be visualized. - colors (list): A list of strings, where each string represents a color. - continuous (bool): If True, the palette will be a continuous colormap. Defaults to False. - plot_type (str): The type of seaborn plot to create (e.g., \'scatter\', \'line\', \'bar\'). Defaults to \'scatter\'. - **kwargs: Additional keyword arguments to pass to the seaborn plotting function. Returns: - None: Displays the generated seaborn plot. # Error handling for colors if not isinstance(colors, list) or len(colors) < 2: raise ValueError(\\"Colors should be a list of at least two strings representing colors.\\") # Error handling for plot_type if plot_type not in [\'scatter\', \'line\', \'bar\']: raise ValueError(\\"Invalid plot_type. Acceptable values are \'scatter\', \'line\', and \'bar\'.\\") # Generate the color palette using blend_palette palette = sns.blend_palette(colors, as_cmap=continuous) # Set the custom palette sns.set_palette(palette) # Generate the plot plt.figure(figsize=(10, 6)) if plot_type == \'scatter\': sns.scatterplot(data=data, palette=palette, **kwargs) elif plot_type == \'line\': sns.lineplot(data=data, palette=palette, **kwargs) elif plot_type == \'bar\': sns.barplot(data=data, palette=palette, **kwargs) # Display the plot plt.show()"},{"question":"# Asyncio Web Scraper Objective Write an asynchronous web scraper using the `asyncio` module in Python. The web scraper should take a list of URLs, fetch their contents concurrently, and return a dictionary with URLs as keys and their HTML contents as values. Requirements 1. Implement a function `async def fetch_url(session, url: str) -> Tuple[str, str]:` that takes in an `aiohttp.ClientSession` object and a URL, and returns a tuple containing the URL and its HTML content. 2. Implement a function `async def fetch_all(urls: List[str], max_concurrent_tasks: int = 5) -> Dict[str, str]:` that fetches all URLs concurrently, with a maximum of `max_concurrent_tasks` running at the same time. 3. Use the `aiohttp` library to handle HTTP requests. 4. Ensure proper exception handling for network-related exceptions. Constraints - You may assume that all URLs are well-formed and reachable. - The maximum length of the URL list `urls` will not exceed 100. - The maximum length of an individual URL will not exceed 200 characters. - The function should handle timeouts and other network-related exceptions gracefully by skipping the failed URLs. Input and Output Formats - Input: List of URLs. - Output: Dictionary with URLs as keys and HTML contents as values (or an empty string if the request failed). Example ```python import asyncio from typing import List, Dict, Tuple import aiohttp async def fetch_url(session, url: str) -> Tuple[str, str]: try: async with session.get(url) as response: html = await response.text() return url, html except Exception as e: return url, \\"\\" async def fetch_all(urls: List[str], max_concurrent_tasks: int = 5) -> Dict[str, str]: async with aiohttp.ClientSession() as session: tasks = [] for url in urls: tasks.append(fetch_url(session, url)) results = await asyncio.gather(*tasks) return {url: html for url, html in results} # Sample input urls = [\'https://www.example.com\', \'https://www.example.org\'] # Fetch contents content = asyncio.run(fetch_all(urls)) print(content) ``` In the example above, replace the sample URLs with actual URLs you want to scrape.","solution":"import asyncio from typing import List, Dict, Tuple import aiohttp async def fetch_url(session, url: str) -> Tuple[str, str]: try: async with session.get(url) as response: html = await response.text() return url, html except Exception as e: return url, \\"\\" async def fetch_all(urls: List[str], max_concurrent_tasks: int = 5) -> Dict[str, str]: async with aiohttp.ClientSession() as session: tasks = [ fetch_url(session, url) for url in urls ] results = await asyncio.gather(*tasks) return {url: html for url, html in results}"},{"question":"# Objective You are given a dataset of tips collected from a restaurant. Your task is to write a function that preprocesses the data and generates a specified point plot using the Seaborn library. # Dataset The dataset is provided as the inbuilt dataset `tips` from the Seaborn library. You can load it using the following command: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` # Requirements 1. **Function Implementation** Implement a function `plot_tips_data` that generates a point plot with the following specifications: ```python def plot_tips_data() -> None: # function implementation ``` 2. **Plot Specifications** - The x-axis should represent the day of the week. - The y-axis should represent the total bill. - The points should be grouped by both `smoker` and `time` variables; use different colors for `smoker` groups and differentiate `time` groups using markers and linestyles. - The error bars should represent the standard deviation of the total bill for each group. - Customize the plot to use square markers for dinner (`time` = \'Dinner\') and circle markers for lunch (`time` = \'Lunch\'). - Line styles should be dashed for smokers and solid for non-smokers. - Adjust the plot aesthetics by setting the grid style to \'whitegrid\'. 3. **Additional Requirement** Ensure that: - The function displays the plot when executed. - Proper labels and title are included. # Example Plot After implementing `plot_tips_data()`, the plot should visually convey the total bill trends across weekdays, distinguished clearly between smoker groups and time of day, with error bars indicating the variability within each group. # Constraints and Limitations - Ensure your code runs efficiently and handles the dataset preprocessing within the function. - The function should not have any input arguments and should load the dataset internally. - No external files are to be used or created. # Submission Submit your `plot_tips_data` function implementation, which should be a complete, self-contained solution generating the required plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_tips_data() -> None: # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set the aesthetics for the plot sns.set(style=\\"whitegrid\\") # Create the point plot plot = sns.catplot( x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", col=\\"time\\", kind=\\"point\\", data=tips, dodge=True, markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], ci=\'sd\', palette=\'deep\' ) # Set the axes labels and plot title plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plot.set_titles(\\"{col_name} Time\\") plt.subplots_adjust(top=0.85) plot.fig.suptitle(\'Total Bill vs. Day with Smoker and Time Groupings\') # Show the plot plt.show()"},{"question":"# Question: Named Tensor Operation with Matrix Multiplication You are tasked with implementing a function that performs matrix multiplication on named tensors with specific name inference rules. The function should accept two PyTorch tensors with named dimensions and return the result of their matrix multiplication, ensuring that the resultant tensor follows the name propagation rules specified in the documentation. Function Signature: ```python def named_tensor_mm(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: pass ``` Parameters: - `tensor1` (torch.Tensor): A named 2-dimensional tensor. - `tensor2` (torch.Tensor): A named 2-dimensional tensor. Returns: - `torch.Tensor`: The resultant tensor after performing matrix multiplication on `tensor1` and `tensor2`, with proper name propagation. Constraints: 1. Both input tensors (`tensor1` and `tensor2`) must be 2-dimensional named tensors. 2. The tensors should follow broadcasting rules and dimensional checks as stated in the documentation. Specifically, the result names should be `(tensor1.names[-2], tensor2.names[-1])`. 3. Use PyTorch\'s built-in matrix multiplication functions (`torch.mm`, `torch.matmul`, etc.) where appropriate. Example: ```python >>> import torch >>> tensor1 = torch.randn(3, 4, names=(\'N\', \'D\')) >>> tensor2 = torch.randn(4, 5, names=(\'D\', \'M\')) >>> result = named_tensor_mm(tensor1, tensor2) >>> print(result.names) (\'N\', \'M\') ``` Notes: - Your implementation should correctly handle dimension names propagation/management and raise appropriate errors if the names are not compatible based on the documented rules. - Utilize the name inference rules from the documentation to ensure the function behaves as expected. - Ensure your implementation is efficient and leverages PyTorch\'s internal operations appropriately.","solution":"import torch def named_tensor_mm(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Performs matrix multiplication on named tensors and returns result tensor with proper name propagation. Args: tensor1 (torch.Tensor): A named 2-dimensional tensor with names like (\'N\', \'D\'). tensor2 (torch.Tensor): A named 2-dimensional tensor with names like (\'D\', \'M\'). Returns: torch.Tensor: The resultant tensor after performing matrix multiplication on `tensor1` and `tensor2`, with names (\'N\', \'M\'). if tensor1.ndim != 2 or tensor2.ndim != 2: raise ValueError(\\"Both tensors must be 2-dimensional\\") if tensor1.names[-1] != tensor2.names[0]: raise ValueError(\\"The name of the second dimension of the first tensor must match the name of the first dimension of the second tensor\\") result = torch.matmul(tensor1, tensor2) result = result.refine_names(*tensor1.names[:-1], *tensor2.names[1:]) return result"},{"question":"Objective: Implement a function that uses the kernel density estimation (KDE) technique from scikit-learn to estimate the density of a given dataset and identify regions with the highest density. You will also visualize the KDE results. Task: 1. **Implement the function `kernel_density_estimation(X, bandwidth, kernel)`**: - **Input**: - `X`: A 2D numpy array of shape (n_samples, n_features) containing the input data. - `bandwidth`: A float value representing the bandwidth parameter for the KDE. - `kernel`: A string specifying the kernel to be used in KDE. Should be one of [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']. - **Output**: - A 2D numpy array with KDE scores for each point in the input data. - A matplotlib plot visualizing the KDE results. 2. **Constraints**: - Assume `1 <= n_samples <= 1000` and `1 <= n_features <= 2`. - The `bandwidth` must be a positive float. 3. **Performance**: - Ensure the implementation is efficient with respect to both time and space complexity. Example: ```python import numpy as np import matplotlib.pyplot as plt def kernel_density_estimation(X, bandwidth, kernel): from sklearn.neighbors import KernelDensity # Fit the Kernel Density model kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) # Score_samples returns the log density; you\'ll need to convert back to density values log_density = kde.score_samples(X) density = np.exp(log_density) # Scatter plot for the original data plt.scatter(X[:, 0], X[:, 1], c=density, s=50, edgecolor=\'k\') plt.title(f\'Kernel Density Estimation (kernel={kernel}, bandwidth={bandwidth})\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.colorbar(label=\'Density\') plt.show() return density # Example usage X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) bandwidth = 0.5 kernel = \'gaussian\' density = kernel_density_estimation(X, bandwidth, kernel) print(density) ``` In this example, the function takes the input data `X`, along with the specified `bandwidth` and `kernel`, computes the KDE, and returns the density estimates. It also generates a scatter plot that visualizes the KDE results. Note: - Ensure that your code handles edge cases such as empty input arrays, negative bandwidth values, and unsupported kernel strings by raising appropriate errors.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kernel_density_estimation(X, bandwidth, kernel): Estimate density using kernel density estimation and plot results. Parameters: - X: 2D numpy array of shape (n_samples, n_features) containing input data. - bandwidth: float, bandwidth parameter for the KDE. - kernel: str, kernel to be used in KDE (one of [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']). Returns: - density: 1D numpy array with KDE scores for each point in input data. if not isinstance(X, np.ndarray) or X.ndim != 2: raise ValueError(\\"X must be a 2D numpy array.\\") if not isinstance(bandwidth, float) or bandwidth <= 0: raise ValueError(\\"Bandwidth must be a positive float.\\") if kernel not in [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']: raise ValueError(\\"Kernel must be one of [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'].\\") # Fit the Kernel Density model kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) # Estimate density log_density = kde.score_samples(X) density = np.exp(log_density) # Visualize the results plt.scatter(X[:, 0], X[:, 1], c=density, s=50, edgecolor=\'k\') plt.title(f\'Kernel Density Estimation (kernel={kernel}, bandwidth={bandwidth})\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.colorbar(label=\'Density\') plt.show() return density"},{"question":"Problem Statement You are required to create a custom representation of specific objects using the `reprlib.Repr` class. The goal is to generate concise, limited representations of different types of collections and to handle potential recursive references gracefully. Task 1. **Subclass the `reprlib.Repr` Class** - Create a subclass named `CustomRepr` that inherits from `reprlib.Repr`. - Override the methods to provide custom, shortened representations for at least the following types: `list`, `dict`, `set`. 2. **Implement Recursive Handling** - Utilize the `reprlib.recursive_repr()` decorator to manage recursive references within the `__repr__` method of a custom class named `RecursiveList`, which should inherit from Python\'s built-in `list`. 3. **Write Representation Methods** - `repr_list(self, obj, level)`: Limit the representation to a maximum of 3 elements. - `repr_dict(self, obj, level)`: Limit the representation to a maximum of 2 key-value pairs. - `repr_set(self, obj, level)`: Limit the representation to a maximum of 3 elements. 4. **Instantiate and Test** - Instantiate your `CustomRepr` class and test the overridden methods using different collections. - Create an instance of `RecursiveList`, introduce a recursive reference, and print its representation. Input and Output - **Input**: No specific input required. You need to define the class and methods as per the given requirements. - **Output**: Print statements displaying the custom representations. Constraints - The maximum elements to display for lists and sets should be 3. - The maximum key-value pairs to display for dictionaries should be 2. - Use the `reprlib.recursive_repr()` decorator to handle recursion in `RecursiveList`. Example ```python import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxlist = 3 self.maxdict = 2 self.maxset = 3 def repr_list(self, obj, level): return super().repr1(obj, level) def repr_dict(self, obj, level): return super().repr1(obj, level) def repr_set(self, obj, level): return super().repr1(obj, level) @reprlib.recursive_repr(fillvalue=\'...\') class RecursiveList(list): def __repr__(self): return \'<\' + \'|\'.join(map(repr, self)) + \'>\' # Instantiate the custom repr class custom_repr = CustomRepr() # Test cases sample_list = [1, 2, 3, 4, 5] print(custom_repr.repr(sample_list)) # Should limit to 3 elements sample_dict = {\'a\': 1, \'b\': 2, \'c\': 3} print(custom_repr.repr(sample_dict)) # Should limit to 2 key-value pairs sample_set = {1, 2, 3, 4} print(custom_repr.repr(sample_set)) # Should limit to 3 elements # Recursive list case rec_list = RecursiveList([1, 2, 3]) rec_list.append(rec_list) print(rec_list) # Should show the recursive reference appropriately ```","solution":"import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxlist = 3 self.maxdict = 2 self.maxset = 3 def repr_list(self, obj, level): return \'[\' + \', \'.join(self.repr1(elem, level - 1) for elem in list(obj)[:self.maxlist]) + (\'...\' if len(obj) > self.maxlist else \'\') + \']\' def repr_dict(self, obj, level): keys = list(obj.keys())[:self.maxdict] items = \', \'.join(f\'{self.repr1(key, level - 1)}: {self.repr1(obj[key], level - 1)}\' for key in keys) return \'{\' + items + (\'...\' if len(obj) > self.maxdict else \'\') + \'}\' def repr_set(self, obj, level): return \'{\' + \', \'.join(self.repr1(elem, level - 1) for elem in list(obj)[:self.maxset]) + (\'...\' if len(obj) > self.maxset else \'\') + \'}\' class RecursiveList(list): @reprlib.recursive_repr(fillvalue=\'...\') def __repr__(self): return \'<\' + \'|\'.join(map(repr, self)) + \'>\' custom_repr = CustomRepr() # Example uses sample_list = [1, 2, 3, 4, 5] sample_dict = {\'a\': 1, \'b\': 2, \'c\': 3} sample_set = {1, 2, 3, 4} rec_list = RecursiveList([1, 2, 3]) rec_list.append(rec_list) print(custom_repr.repr(sample_list)) # Should limit to 3 elements print(custom_repr.repr(sample_dict)) # Should limit to 2 key-value pairs print(custom_repr.repr(sample_set)) # Should limit to 3 elements print(rec_list) # Should show recursive reference appropriately"},{"question":"# Asynchronous Programming with asyncio In this assessment, you are required to create a Python program that demonstrates your understanding of Python\'s asyncio library, specifically focusing on concurrency, handling long-running tasks, and using the debug features of asyncio. Problem Statement You are tasked with implementing a program that fetches data from multiple sources asynchronously and processes that data without blocking the main event loop. The program should have the following features: 1. **Data Fetching and Processing**: - Create a function `fetch_data(source: str) -> str` that simulates fetching data from a given source and returns the fetched data after a delay. - Create a function `process_data(data: str) -> None` that simulates processing the fetched data and logs the start and end time of the processing. 2. **Concurrent Execution**: - Implement a main coroutine `main(sources: List[str]) -> None` that orchestrates fetching and processing data from multiple sources concurrently. - Ensure that the data fetching and processing do not block the event loop. 3. **Debugging and Logging**: - Enable asyncio\'s debug mode to catch any potential issues. - Log any slow callbacks that take more than 200ms to execute. - Use appropriate logging to trace the execution of your program. 4. **Error Handling**: - Ensure that any exceptions raised during the fetching and processing are handled appropriately and logged. Implementation Details - **Function Signatures**: ```python async def fetch_data(source: str) -> str: pass async def process_data(data: str) -> None: pass async def main(sources: List[str]) -> None: pass ``` - **Expected Behavior**: - The `fetch_data` function should simulate a random delay (between 0.1 and 0.5 seconds) to represent the data fetching time. - The `process_data` function should simulate a random delay (between 0.1 and 0.5 seconds) to represent the data processing time and log the start and end time of processing. - The `main` function should be able to fetch and process data from a list of sources concurrently, without blocking each other. - **Constraints**: - Use the `asyncio.create_task()` function to schedule coroutines. - Ensure the program uses debug logging to trace execution and print any issues with un-awaited coroutines or un-retrieved exceptions. - Handle any exceptions raised during the execution and log them appropriately. - **Example Input and Output**: ```python sources = [\\"source1\\", \\"source2\\", \\"source3\\"] asyncio.run(main(sources)) ``` - The output should display log messages indicating the fetching and processing times for each source, any slow callbacks, and any exceptions that are caught. Notes - Make sure to follow best practices for asynchronous programming. - You can use the `random` module to simulate delays. - Configuring logging and enabling debug mode is an essential part of the task. Good luck!","solution":"import asyncio import logging import random import time from typing import List logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) async def fetch_data(source: str) -> str: Simulates fetching data from a given source with a random delay. delay = random.uniform(0.1, 0.5) await asyncio.sleep(delay) data = f\\"data_from_{source}\\" return data async def process_data(data: str) -> None: Simulates processing the fetched data and logs the start and end time of the processing. logger.info(f\\"Start processing {data}\\") delay = random.uniform(0.1, 0.5) await asyncio.sleep(delay) logger.info(f\\"End processing {data}\\") async def main(sources: List[str]) -> None: Orchestrates fetching and processing data from multiple sources concurrently. tasks = [] for source in sources: task = asyncio.create_task(fetch_and_process(source)) tasks.append(task) await asyncio.gather(*tasks) async def fetch_and_process(source: str) -> None: Handles the fetching and processing of data. try: data = await fetch_data(source) await process_data(data) except Exception as e: logger.error(f\\"Error fetching or processing data from {source}: {e}\\") if __name__ == \\"__main__\\": asyncio.run(main([\\"source1\\", \\"source2\\", \\"source3\\"]))"},{"question":"# Unicode Character Analysis Function **Objective:** Create a function `unicode_analysis` that analyzes a given list of Unicode characters by querying various properties provided by the `unicodedata` module. The function should return a summary dictionary with detailed information about each character. **Function Signature:** ```python def unicode_analysis(char_list): pass ``` **Input:** - `char_list`: A list of Unicode characters (strings of length 1). For example, `[\'A\', \'9\', \'µ\', \'中\', \'😊\']`. **Output:** - A dictionary where each key is a Unicode character from the input list and the value is another dictionary with the following keys and their corresponding values: - `\'name\'`: Name of the character (using `unicodedata.name`). - `\'category\'`: General category assigned to the character (using `unicodedata.category`). - `\'bidirectional\'`: Bidirectional class assigned to the character (using `unicodedata.bidirectional`). - `\'combining\'`: Canonical combining class assigned to the character (using `unicodedata.combining`). - `\'east_asian_width\'`: East Asian width assigned to the character (using `unicodedata.east_asian_width`). - `\'mirrored\'`: Mirrored property of the character (using `unicodedata.mirrored`). - `\'decomposition\'`: Decomposition mapping of the character (using `unicodedata.decomposition`). - `\'decimal\'`: Decimal value of the character (using `unicodedata.decimal`, return `None` if not defined). - `\'digit\'`: Digit value of the character (using `unicodedata.digit`, return `None` if not defined). - `\'numeric\'`: Numeric value of the character (using `unicodedata.numeric`, return `None` if not defined). **Constraints:** - You may assume that the input list contains valid Unicode characters. - Use appropriate error handling to manage cases where certain properties might not be defined. **Example:** ```python >>> unicode_analysis([\'A\', \'9\', \'µ\', \'中\', \'😊\']) { \'A\': { \'name\': \'LATIN CAPITAL LETTER A\', \'category\': \'Lu\', \'bidirectional\': \'L\', \'combining\': 0, \'east_asian_width\': \'Na\', \'mirrored\': 0, \'decomposition\': \'\', \'decimal\': None, \'digit\': None, \'numeric\': None }, \'9\': { \'name\': \'DIGIT NINE\', \'category\': \'Nd\', \'bidirectional\': \'EN\', \'combining\': 0, \'east_asian_width\': \'Na\', \'mirrored\': 0, \'decomposition\': \'\', \'decimal\': 9, \'digit\': 9, \'numeric\': 9.0 }, \'µ\': { \'name\': \'MICRO SIGN\', \'category\': \'So\', \'bidirectional\': \'ON\', \'combining\': 0, \'east_asian_width\': \'Na\', \'mirrored\': 0, \'decomposition\': \'<compat> 03BC\', \'decimal\': None, \'digit\': None, \'numeric\': None }, \'中\': { \'name\': \'CJK UNIFIED IDEOGRAPH-4E2D\', \'category\': \'Lo\', \'bidirectional\': \'L\', \'combining\': 0, \'east_asian_width\': \'W\', \'mirrored\': 0, \'decomposition\': \'\', \'decimal\': None, \'digit\': None, \'numeric\': None }, \'😊\': { \'name\': \'SMILING FACE WITH SMILING EYES\', \'category\': \'So\', \'bidirectional\': \'ON\', \'combining\': 0, \'east_asian_width\': \'W\', \'mirrored\': 0, \'decomposition\': \'\', \'decimal\': None, \'digit\': None, \'numeric\': None } } ``` **Note:** - Some characters might not have certain properties defined. In such cases, the corresponding values should be set to `None`. Implement the `unicode_analysis` function to complete the task as described.","solution":"import unicodedata def unicode_analysis(char_list): analysis = {} for char in char_list: try: char_info = { \'name\': unicodedata.name(char), \'category\': unicodedata.category(char), \'bidirectional\': unicodedata.bidirectional(char), \'combining\': unicodedata.combining(char), \'east_asian_width\': unicodedata.east_asian_width(char), \'mirrored\': unicodedata.mirrored(char), \'decomposition\': unicodedata.decomposition(char), \'decimal\': None, \'digit\': None, \'numeric\': None } try: char_info[\'decimal\'] = unicodedata.decimal(char) except: char_info[\'decimal\'] = None try: char_info[\'digit\'] = unicodedata.digit(char) except: char_info[\'digit\'] = None try: char_info[\'numeric\'] = unicodedata.numeric(char) except: char_info[\'numeric\'] = None analysis[char] = char_info except ValueError: # In case the character is not recognized analysis[char] = None return analysis"},{"question":"# Objective: To assess students\' understanding of the `Distutils` package and the creation of built distributions using the `bdist` command. # Problem Statement: You are tasked with automating the process of creating built distributions for a given Python module. Write a Python function `create_built_distribution` that takes the path to the setup script of the module and a list of distribution formats, then executes the appropriate Distutils commands to create the specified built distributions. # Function Signature: ```python def create_built_distribution(setup_path: str, formats: list) -> None: ``` # Input: - `setup_path` (str): The file path to the `setup.py` script of the module. - `formats` (list): A list of distribution formats to generate. Each format should be one of the following strings: \\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\", \\"rpm\\". # Output: - The function does not return any output. However, it should create the specified built distributions in the appropriate format. # Constraints: 1. Assume that all necessary tools and utilities required for different formats (e.g., `rpm`, `zip`) are installed and available in the system\'s PATH. 2. The `setup.py` script provided in the `setup_path` is properly configured and error-free. # Example: ```python # Example usage create_built_distribution(\\"path/to/setup.py\\", [\\"gztar\\", \\"zip\\", \\"rpm\\"]) ``` # Notes: 1. You will make use of the `subprocess` module to call the Distutils commands from within Python. 2. Implement error handling to ensure that appropriate error messages are displayed if the distribution creation fails for any reason. # Points to Consider: - Use proper subprocess calls to execute `python setup.py bdist --formats=<format>` for each format specified in the `formats` list. - Ensure that the function is capable of handling multiple formats in one call. - The function should print a success message upon successful creation of each built distribution, or an error message if something goes wrong. # Hints: - Refer to the provided Distutils documentation for information on the `bdist` command and available formats. - Use the `subprocess.run` or `subprocess.check_call` methods to execute shell commands from within the function. **Good luck!**","solution":"import subprocess def create_built_distribution(setup_path: str, formats: list) -> None: Creates built distributions for the specified formats. :param setup_path: str - The file path to the setup.py script of the module :param formats: list - A list of distribution formats to generate (e.g., [\\"gztar\\", \\"zip\\"]) for fmt in formats: try: subprocess.check_call([\\"python\\", setup_path, \\"bdist\\", \\"--formats=\\" + fmt]) print(f\\"Successfully created {fmt} built distribution.\\") except subprocess.CalledProcessError as e: print(f\\"Failed to create {fmt} built distribution. Error: {e}\\") # Example usage: # create_built_distribution(\\"path/to/setup.py\\", [\\"gztar\\", \\"zip\\", \\"rpm\\"])"},{"question":"# Question: Create a Custom Finder and Loader with `importlib` Objective Your task is to create a custom module finder and loader by using the `importlib.abc.MetaPathFinder` and `importlib.abc.Loader` abstract base classes. You will manage a special kind of module identified by a custom prefix in the module name, for which the content will be dynamically generated. Requirements 1. **Custom Finder (`CustomFinder`)**: - Should inherit from `importlib.abc.MetaPathFinder`. - Should implement the method `find_spec(fullname, path, target=None)` which checks if the module name starts with \\"custom_\\". - If the module name starts with \\"custom_\\", return a module spec with the `CustomLoader`. 2. **Custom Loader (`CustomLoader`)**: - Should inherit from `importlib.abc.Loader`. - Should implement the method `create_module(spec)` which returns a new module object. - Should implement the method `exec_module(module)` which adds some attributes to these custom modules: - `module.message`: A string that says \\"Hello from `custom_<name>`\\". - Any other attribute you deem necessary for demonstration. 3. **Testing**: - Use your custom finder and loader to import a module named `custom_demo`. - Verify that the module has the correct attributes. Input and Output Format - **Input**: There are no specific inputs for functions, but your script will dynamically handle the module importing process. - **Output**: Print the attributes of the imported `custom_demo` module to validate that it has been correctly loaded. Constraints - Ensure your custom finder and loader do not interfere with the normal module import process. - Make sure to add and remove your custom finder from `sys.meta_path` within a context manager to avoid side effects on the global state. Example ```python # Example output after importing `custom_demo`: print(custom_demo.message) # Output: Hello from custom_demo ``` Implementing this solution validates comprehension of custom importers and loaders using `importlib`.","solution":"import sys import importlib.util import importlib.abc class CustomLoader(importlib.abc.Loader): def create_module(self, spec): return None def exec_module(self, module): module_name = module.__name__ module.message = f\\"Hello from {module_name}\\" module.description = f\\"This is a custom loaded module named {module_name}\\" class CustomFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname.startswith(\\"custom_\\"): return importlib.util.spec_from_loader(fullname, CustomLoader()) return None def install_custom_finder(): if not any(isinstance(finder, CustomFinder) for finder in sys.meta_path): sys.meta_path.append(CustomFinder()) def remove_custom_finder(): sys.meta_path = [finder for finder in sys.meta_path if not isinstance(finder, CustomFinder)] install_custom_finder() try: import custom_demo print(custom_demo.message) # This will be for testing the functionality print(custom_demo.description) # This will be for testing the functionality finally: remove_custom_finder()"},{"question":"# Pandas Coding Assessment **Objective**: Demonstrate mastery of string manipulation techniques using pandas Series with StringDtype. Problem Statement You are given a DataFrame containing information about various products in an online store. Each row in the DataFrame represents a single product, with columns for the product name, description, and category. Your task is to write functions that perform several common text data cleaning, transformation, and analysis tasks using pandas. Input A DataFrame `df` with the following columns: - `product_name` (StringDtype): Name of the product. - `product_description` (StringDtype): Description of the product. - `product_category` (StringDtype): Category under which the product is listed. Example: ```python import pandas as pd import numpy as np data = { \'product_name\': pd.Series([\'Widget A\', \' gadget B\', \'Tool C \'], dtype=\'string\'), \'product_description\': pd.Series([\'a versatile widget\', \'best gadget ever\', None], dtype=\'string\'), \'product_category\': pd.Series([\'Widgets\', \' gadgets\', \'Tools\'], dtype=\'string\') } df = pd.DataFrame(data) ``` Functions to Implement 1. **clean_column_names()** - **Input**: DataFrame `df` - **Output**: DataFrame with cleaned column names where all leading and trailing whitespace is removed, all letters are lowercase, and spaces are replaced with underscores. ```python def clean_column_names(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` 2. **preprocess_text_columns()** - **Input**: DataFrame `df` - **Output**: DataFrame with cleaned text columns where: - Leading and trailing whitespace is removed from all string entries. - All text data is converted to lowercase. ```python def preprocess_text_columns(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` 3. **extract_keywords()** - **Input**: DataFrame `df`, keyword string `keyword` - **Output**: DataFrame containing only the rows where the `product_description` contains the specified keyword (case insensitive). ```python def extract_keywords(df: pd.DataFrame, keyword: str) -> pd.DataFrame: # Your code here pass ``` 4. **analyze_product_names()** - **Input**: DataFrame `df` - **Output**: DataFrame where: - A new column `name_length` is added that contains the length of each product name. - A new column `first_letter` is added that contains the first letter of each product name. ```python def analyze_product_names(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` 5. **aggregate_by_category()** - **Input**: DataFrame `df` - **Output**: DataFrame where: - Rows are grouped by `product_category`. - The count of products in each category is provided. - The longest product description in each category is also provided. ```python def aggregate_by_category(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` Constraints - Assume the DataFrame `df` always contains the specified columns with valid StringDtype. - Make sure your functions handle missing values appropriately (e.g., `None`). Example Input and Output Example usage: ```python # Create the example DataFrame data = { \'product_name\': pd.Series([\'Widget A\', \' gadget B\', \'Tool C \'], dtype=\'string\'), \'product_description\': pd.Series([\'a versatile widget\', \'best gadget ever\', None], dtype=\'string\'), \'product_category\': pd.Series([\'Widgets\', \' gadgets\', \'Tools\'], dtype=\'string\') } df = pd.DataFrame(data) # Clean column names df = clean_column_names(df) print(df.columns) # Preprocess text columns df = preprocess_text_columns(df) print(df) # Extract keywords keyword_df = extract_keywords(df, \'versatile\') print(keyword_df) # Analyze product names df = analyze_product_names(df) print(df) # Aggregate by category category_df = aggregate_by_category(df) print(category_df) ``` Expected Output: ``` Index([\'product_name\', \'product_description\', \'product_category\'], dtype=\'object\') ``` ``` product_name product_description product_category 0 widget a a versatile widget widgets 1 gadget b best gadget ever gadgets 2 tool c NaN tools ``` ``` product_name product_description product_category 0 widget a a versatile widget widgets ``` ``` product_name product_description product_category name_length first_letter 0 widget a a versatile widget widgets 8 w 1 gadget b best gadget ever gadgets 8 g 2 tool c NaN tools 6 t ``` ``` product_category product_count longest_description 0 gadgets 1 best gadget ever 1 tools 1 NaN 2 widgets 1 a versatile widget ``` Your solutions will be evaluated based on correctness, efficiency, and readability.","solution":"import pandas as pd def clean_column_names(df: pd.DataFrame) -> pd.DataFrame: df.columns = df.columns.str.strip().str.lower().str.replace(\' \', \'_\') return df def preprocess_text_columns(df: pd.DataFrame) -> pd.DataFrame: for column in df.select_dtypes(include=[\'string\']).columns: df[column] = df[column].str.strip().str.lower() return df def extract_keywords(df: pd.DataFrame, keyword: str) -> pd.DataFrame: mask = df[\'product_description\'].str.contains(keyword, case=False, na=False) return df[mask] def analyze_product_names(df: pd.DataFrame) -> pd.DataFrame: df[\'name_length\'] = df[\'product_name\'].str.len() df[\'first_letter\'] = df[\'product_name\'].str[0] return df def aggregate_by_category(df: pd.DataFrame) -> pd.DataFrame: grouped_df = df.groupby(\'product_category\').agg( product_count=(\'product_name\', \'size\'), longest_description=(\'product_description\', lambda x: max(x, key=lambda d: len(d) if pd.notna(d) else 0)) ).reset_index() return grouped_df"},{"question":"# Seaborn Coding Assessment Objective: Demonstrate your understanding of Seaborn\'s new API and customization features by creating a well-styled visual representation of a dataset. Task: Using Seaborn\'s `objects` module, load the `anscombe` dataset and create a multi-faceted plot. Then, customize the appearance of the plot using themes and styles. Instructions: 1. **Load Dataset:** - Load the `anscombe` dataset from Seaborn. 2. **Create Plot:** - Create a `so.Plot` using the `anscombe` dataset that features: - `x` on the x-axis. - `y` on the y-axis. - `dataset` as the color grouping. - Each dataset should be displayed in its own facet, wrapping to fit 2 plots per row. 3. **Customize Appearance:** - Add a linear regression line (using `PolyFit`) and dots for points. - Customize the plot\'s theme to have: - White axes face color. - Slategray edge color for the axes. - Line width for the regression lines to be 4. - Apply the `ticks` style from Seaborn. - Modify the plot\'s configuration using the `fivethirtyeight` style from Matplotlib. 4. **Combine Styles:** - Combine attributes from both `axes_style(\\"whitegrid\\")` and `plotting_context(\\"talk\\")`. Input and Output: - **Input:** No direct input; you will work with the `anscombe` dataset provided within Seaborn. - **Output:** Displayed plot with the specified customizations. Code Requirements: - Use the functions and modules demonstrated in the provided documentation (`import seaborn.objects as so`, `from seaborn import load_dataset`, etc.). - Demonstrate your ability to read, understand, and manipulate data using Seaborn. - Your code should be efficient and readable. Below is a skeleton to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset from seaborn import axes_style from matplotlib import style from seaborn import plotting_context # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Create the plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Customize the appearance p = p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) p = p.theme({\\"lines.linewidth\\": 4}) p = p.theme(axes_style(\\"ticks\\")) p = p.theme(style.library[\\"fivethirtyeight\\"]) p = p.theme(axes_style(\\"whitegrid\\") | plotting_context(\\"talk\\")) # Display the plot p ``` Make sure your final plot displays with all the customizations applied.","solution":"import seaborn.objects as so from seaborn import load_dataset from seaborn import axes_style from matplotlib import style from seaborn import plotting_context # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Create the plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Customize the appearance p = p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) p = p.theme({\\"lines.linewidth\\": 4}) p = p.theme(axes_style(\\"ticks\\")) p = p.theme(style.library[\\"fivethirtyeight\\"]) p = p.theme(axes_style(\\"whitegrid\\") | plotting_context(\\"talk\\")) # Display the plot p.show()"},{"question":"Objective: To assess your understanding of dataset loading, preprocessing, and usage within scikit-learn, you are required to write a function that processes a given dataset, performs basic preprocessing, and trains a machine learning model. Problem Statement: Write a function `train_model_on_openml_dataset(dataset_name: str, classifier: Any) -> float:` that: 1. Downloads a specified dataset from OpenML using the dataset name. 2. Separates the dataset into features and target variables. 3. Preprocesses the features by handling missing values and scaling numerical features. 4. Trains the provided scikit-learn classifier on the preprocessed features. 5. Evaluates the classifier using a 5-fold cross-validation, returning the mean accuracy. Specifications: - Use the `fetch_openml()` function to download the dataset. - Handle missing values by imputing with the mean for numerical features. - Scale numerical features to have zero mean and unit variance. - Convert the target variable to integer labels if it is categorical. - Use 5-fold cross-validation to evaluate and return the mean accuracy. - The classifier passed to the function will be from scikit-learn (e.g., `sklearn.ensemble.RandomForestClassifier`). Constraints: - Assume that datasets will have fewer than 100,000 samples. - The function should cap the maximum memory usage at 2GB. # Example usage: ```python from sklearn.ensemble import RandomForestClassifier mean_accuracy = train_model_on_openml_dataset(\'iris\', RandomForestClassifier()) print(f\\"Mean accuracy: {mean_accuracy:.4f}\\") ``` # Implementation Details: The function should follow the structure outlined below: ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import cross_val_score from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer import pandas as pd import numpy as np def train_model_on_openml_dataset(dataset_name: str, classifier: Any) -> float: # Step 1: Load the dataset from OpenML dataset = fetch_openml(name=dataset_name, as_frame=True) X, y = dataset.data, dataset.target # Step 2: Handle missing values and scaling # Identify numerical columns num_cols = X.select_dtypes(include=[np.number]).columns.tolist() # Impute numerical columns and scale them preprocessor = ColumnTransformer( transformers=[ (\'num\', Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]), num_cols) ], remainder=\'passthrough\' ) # Step 3: Convert categorical target to numeric labels if needed if y.dtype == object: le = LabelEncoder() y = le.fit_transform(y) # Step 4: Create preprocessing and training pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', classifier)]) # Step 5: Evaluate the classifier with 5-fold cross-validation scores = cross_val_score(pipeline, X, y, cv=5) # Return the mean accuracy return scores.mean() ``` Please implement the `train_model_on_openml_dataset` function according to the specifications.","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import cross_val_score from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer import numpy as np def train_model_on_openml_dataset(dataset_name: str, classifier) -> float: Downloads an OpenML dataset, preprocesses it, trains a scikit-learn classifier on it, and evaluates the model using 5-fold cross-validation. Parameters: - dataset_name: str, the name of the dataset to download from OpenML. - classifier: any, a scikit-learn classifier to train on the dataset. Returns: - float, the mean accuracy of the classifier based on 5-fold cross-validation. # Step 1: Load the dataset from OpenML dataset = fetch_openml(name=dataset_name, as_frame=True) X, y = dataset.data, dataset.target # Step 2: Handle missing values and scaling # Identify numerical columns num_cols = X.select_dtypes(include=[np.number]).columns.tolist() # Impute numerical columns and scale them preprocessor = ColumnTransformer( transformers=[ (\'num\', Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]), num_cols) ], remainder=\'passthrough\' ) # Step 3: Convert categorical target to numeric labels if needed if y.dtype == object: le = LabelEncoder() y = le.fit_transform(y) # Step 4: Create preprocessing and training pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'classifier\', classifier)]) # Step 5: Evaluate the classifier with 5-fold cross-validation scores = cross_val_score(pipeline, X, y, cv=5) # Return the mean accuracy return scores.mean()"},{"question":"Objective Implement a Python function that: 1. Encodes an input file to a uuencoded file. 2. Decodes a uuencoded file back to the original format. 3. Includes error handling for various edge cases. Function Signature ```python def uuencode_decode(in_file: str, out_file: str, operation: str, backtick: bool = False, quiet: bool = False) -> None: pass ``` Parameters - `in_file` (str): The path of the input file. - `out_file` (str): The path of the output file. - `operation` (str): Operation to perform - either \\"encode\\" or \\"decode\\". - `backtick` (bool): Optional; If True, zeros are represented by \\"`\\" instead of spaces during encoding. Default is False. - `quiet` (bool): Optional; If True, suppresses warnings during decoding. Default is False. Constraints - The function should only accept \\"encode\\" or \\"decode\\" as valid operations. In case of any other value, raise a `ValueError` with the message \\"Invalid operation\\". - Proper exception handling should be in place to catch and handle `uu.Error` exceptions during decoding, and any file I/O related exceptions. - If the output file specified already exists during decoding, an appropriate error message should be printed and no file should be overwritten. Description You need to implement a function `uuencode_decode` which performs the following operations: 1. **Encode Operation**: When `operation` is set to \\"encode\\", the function should encode the contents of the input file `in_file` and write the uuencoded result to the output file `out_file`. Use the `backtick` parameter to modify encoding behavior. 2. **Decode Operation**: When `operation` is set to \\"decode\\", the function should decode the contents of the uuencoded input file `in_file` and write the decoded result to the output file `out_file`. Use the `quiet` parameter to suppress any warnings. 3. If an invalid operation is specified, the function should raise a `ValueError` with the message \\"Invalid operation\\". 4. Handle the `uu.Error` exception during decoding and print a user-friendly error message without terminating the program. 5. Handle any file I/O related exceptions and print appropriate error messages. Example Usage ```python # Encoding a file to uuencode format uuencode_decode(\\"path/to/input.txt\\", \\"path/to/encoded_output.txt\\", \\"encode\\") # Decoding a uuencoded file back to its original format uuencode_decode(\\"path/to/encoded_output.txt\\", \\"path/to/decoded_output.txt\\", \\"decode\\") ``` Example Case Given the following input file `input.txt` with content: ``` Hello, this is a test file for uuencode. ``` When `operation` is \\"encode\\" and `backtick` is set to True: ```python uuencode_decode(\\"input.txt\\", \\"encoded_output.txt\\", \\"encode\\", backtick=True) ``` Contents of `encoded_output.txt` might be a uuencoded version of the input file. When `operation` is \\"decode\\": ```python uuencode_decode(\\"encoded_output.txt\\", \\"decoded_output.txt\\", \\"decode\\") ``` The content of `decoded_output.txt` should match the original `input.txt` content. # Note - Ensure to test your function with various edge cases such as missing files, incorrect permissions, existing output files during decoding, etc.","solution":"import uu import os def uuencode_decode(in_file: str, out_file: str, operation: str, backtick: bool = False, quiet: bool = False) -> None: try: if operation == \\"encode\\": mode = \'w\' with open(in_file, \'rb\') as input_file, open(out_file, \'wb\') as output_file: uu.encode(input_file, output_file, name=in_file, mode=os.stat(in_file).st_mode) if backtick: with open(out_file, \'r\') as f: content = f.read() content = content.replace(\' \', \'`\') with open(out_file, \'w\') as f: f.write(content) elif operation == \\"decode\\": if os.path.exists(out_file): print(f\\"Error: Output file {out_file} already exists.\\") return with open(in_file, \'rb\') as input_file, open(out_file, \'wb\') as output_file: uu.decode(input_file, output_file, quiet=quiet) else: raise ValueError(\\"Invalid operation\\") except FileNotFoundError as e: print(f\\"Error: File not found - {e.filename}\\") except PermissionError as e: print(f\\"Error: Permission denied - {e.filename}\\") except uu.Error as e: print(f\\"Error: UUencode/UUdecode error - {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Scikit-Learn Coding Assessment Objective Demonstrate your understanding of the scikit-learn package by implementing a small project that involves data preprocessing, model fitting, and verifying the correctness of the implementation using synthetic data. Problem Statement You are tasked with building a synthetic dataset, processing the data, and fitting a Gradient Boosting Regressor model. Your objective is to reproduce an issue where fitting the Gradient Boosting Regressor with a certain parameterization leads to a specific warning. Detailed Steps 1. **Synthetic Data Generation:** - Create a synthetic regression dataset using the `make_regression` method from `sklearn.datasets`. - The dataset should contain `n_samples=1000` and `n_features=20`. 2. **Data Preprocessing:** - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 3. **Model Training and Evaluation:** - Split the dataset into training and test sets using `train_test_split` from `sklearn.model_selection`. - Fit a `GradientBoostingRegressor` model to the training data with default parameters. Record the score on the test set. - Fit another `GradientBoostingRegressor` with `n_iter_no_change=5` to the same data and capture the warning message, if any. 4. **Implementation Requirements:** - Ensure all necessary import statements are included. - The solution should be executable by copy-pasting into a Python environment. - Output should include the model scores and any warning messages. Input - There are no direct inputs, but you need to create a synthetic dataset as described above. Output - Print the score of both models on the test data. - Print any warning message generated when fitting the second model. Constraints - Use `random_state=42` wherever applicable for reproducibility. - Do not include unnecessary steps or variables. Example Here is a template to get you started: ```python from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings # Suppress warnings for cleaner output warnings.filterwarnings(\\"ignore\\", category=UserWarning, append=True) # Step 1: Create synthetic data X, y = make_regression(n_samples=1000, n_features=20, random_state=42) # Step 2: Data preprocessing scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Train-test split X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42) # Step 4: Model fitting and evaluation # Model 1: Default parameters model_default = GradientBoostingRegressor(random_state=42) model_default.fit(X_train, y_train) default_score = model_default.score(X_test, y_test) # Model 2: n_iter_no_change=5 model_n_iter = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) model_n_iter.fit(X_train, y_train) n_iter_score = model_n_iter.score(X_test, y_test) # Print the results print(f\\"Default Model Score: {default_score}\\") print(f\\"Model with n_iter_no_change=5 Score: {n_iter_score}\\") # Intentionally check for warnings during the model fit with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") model_n_iter.fit(X_train, y_train) if w: print(f\\"Warning raised: {w[-1].message}\\") ``` Implement the above steps within a function or script and ensure it runs correctly in your local environment. Good luck!","solution":"from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings def gradient_boosting_regressor(): # Suppress warnings for cleaner output warnings.filterwarnings(\\"ignore\\", category=UserWarning, append=True) # Step 1: Create synthetic data X, y = make_regression(n_samples=1000, n_features=20, random_state=42) # Step 2: Data preprocessing scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Train-test split X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42) # Step 4: Model fitting and evaluation # Model 1: Default parameters model_default = GradientBoostingRegressor(random_state=42) model_default.fit(X_train, y_train) default_score = model_default.score(X_test, y_test) # Model 2: n_iter_no_change=5 model_n_iter = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) # Intentionally check for warnings during the model fit with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") model_n_iter.fit(X_train, y_train) n_iter_score = model_n_iter.score(X_test, y_test) warning_message = None if w: warning_message = w[-1].message # Return the results return { \\"Default Model Score\\": default_score, \\"Model with n_iter_no_change=5 Score\\": n_iter_score, \\"Warning Message\\": warning_message }"},{"question":"# Coding Assessment: Advanced Exception Handling **Problem Statement:** You are required to implement a function `process_file(file_path: str) -> None` that processes a file containing various operations described as lines of text. Each line in the file describes an arithmetic operation. Your task is to: 1. Read the file line by line. 2. Parse each line to perform the described arithmetic operation. 3. Handle any potential exceptions that may occur during file reading and operation parsing. 4. Implement proper clean-up actions to ensure the file is closed after operation, using `with`. 5. Define and raise a custom exception `OperationError` when an operation is invalid. 6. Ensure exception chaining is correctly implemented when transforming exceptions. **Input Format:** - A single string `file_path` indicating the path to the file. **Output Format:** - The function should not return any value (i.e., return None). **Constraints:** - The file exists and is accessible. - Each line in the file follows the format: `<operand1> <operator> <operand2>` - Operands are integers. Operators could be one of `+, -, *, /` - If the operator is not among the supported operators (`+, -, *, /`), an `OperationError` should be raised. **Exception Handling:** - Handle `FileNotFoundError` if the file does not exist. - Handle `ZeroDivisionError` for division operations. - Handle `ValueError` when inputs cannot be parsed correctly. - Implement chaining of exceptions whenever re-raising a transformed exception. **Custom Exception:** ```python class OperationError(Exception): pass ``` **Function Signature:** ```python def process_file(file_path: str) -> None: # Implement this function ``` # Example: Suppose the input file has the following content: ``` 3 + 5 4 / 0 7 % 2 one + 2 ``` Your function should output: ``` Result of 3 + 5 is 8 Handling run-time error: division by zero Invalid Operation: supported operators are +, -, *, / Invalid data found, expecting integers: one + 2 ``` Use the following template to implement your solution. Include detailed comments and handle each specified exception appropriately.","solution":"class OperationError(Exception): pass def process_file(file_path: str) -> None: Processes a file with arithmetic operations line by line and handles various exceptions. Args: file_path (str): Path to the file containing arithmetic operations. Returns: None try: with open(file_path, \'r\') as file: for line in file: try: parts = line.strip().split() if len(parts) != 3: raise OperationError(f\\"Invalid format: {line.strip()}\\") operand1, operator, operand2 = parts operand1 = int(operand1) operand2 = int(operand2) if operator == \'+\': result = operand1 + operand2 elif operator == \'-\': result = operand1 - operand2 elif operator == \'*\': result = operand1 * operand2 elif operator == \'/\': result = operand1 / operand2 else: raise OperationError(f\\"Invalid Operation: supported operators are +, -, *, /\\") print(f\\"Result of {operand1} {operator} {operand2} is {result}\\") except ValueError: print(f\\"Invalid data found, expecting integers: {line.strip()}\\") except ZeroDivisionError: print(\\"Handling run-time error: division by zero\\") except OperationError as oe: print(oe) except FileNotFoundError as fnfe: raise FileNotFoundError(\\"The file does not exist.\\") from fnfe"},{"question":"# Context Management using Context Variables You are tasked with implementing a function that makes use of the `contextvars` module to manage and manipulate state in a concurrent setting. Specifically, you must create and utilize context variables in a scenario where different asynchronous tasks might interfere with each other\'s state if not properly isolated. # Task Create a function `manage_context()` that demonstrates the use of `contextvars.ContextVar`, `contextvars.copy_context`, and `contextvars.Context`. The function should: 1. Declare a Context Variable named `task_data` with a default value of `None`. 2. Create an asynchronous function `worker` that accepts a string parameter `data`, sets `task_data` to this value, performs some operation (such as appending data to a shared list), and then resets `task_data`. 3. Use `asyncio` to execute multiple instances of `worker` with different data, ensuring that each instance has its own isolated context. # Constraints - The `worker` function should append the value of `task_data` to a shared list before resetting it. - You must use `contextvars.copy_context` to demonstrate the isolation of context. - Do not use any global variables to store the task data. # Expected Input and Output The function `manage_context()` does not take any input parameters and does not return any values. However, it should print the following: - The shared list of data after all workers have been executed, demonstrating that each worker’s context was correctly isolated. # Implementation ```python import asyncio import contextvars # Your implementation goes here async def manage_context(): # Declare a new Context Variable \'task_data\' with default value None task_data = contextvars.ContextVar(\'task_data\', default=None) # Shared list to collect data from different workers shared_data = [] # Define the worker function async def worker(data): # Set the task_data for the current context token = task_data.set(data) # Append the current value of task_data to shared_data shared_data.append(task_data.get()) # Reset task_data to its previous value task_data.reset(token) # Create context copies for worker isolation ctx1 = contextvars.copy_context() ctx2 = contextvars.copy_context() # Schedule the worker functions in asyncio loop await asyncio.gather(ctx1.run(worker, \'data1\'), ctx2.run(worker, \'data2\')) # Print the shared_data list to check the result print(shared_data) # To execute the manage_context function if __name__ == \\"__main__\\": asyncio.run(manage_context()) ``` Explanation In this task, `manage_context` is designed to show how context variables can be used to manage state in a concurrent environment. The `worker` function manipulates `task_data` within its isolated context. Using `contextvars.copy_context` ensures that changes made in one context do not affect another. The shared list `shared_data` will collect the data showing the successful isolation of context for different `worker` tasks.","solution":"import asyncio import contextvars async def manage_context(): # Declare a new Context Variable \'task_data\' with default value None task_data = contextvars.ContextVar(\'task_data\', default=None) # Shared list to collect data from different workers shared_data = [] # Define the worker function async def worker(data): token = task_data.set(data) shared_data.append(task_data.get()) task_data.reset(token) async def run_workers(): ctx1 = contextvars.copy_context() ctx2 = contextvars.copy_context() await asyncio.gather( ctx1.run(worker, \'data1\'), ctx2.run(worker, \'data2\') ) await run_workers() return shared_data # To execute the manage_context function if __name__ == \\"__main__\\": result = asyncio.run(manage_context()) print(result)"},{"question":"# Advanced Seaborn Visualization Task Write a Python function using Seaborn to create a complex joint plot visualizing the relationship between penguin bill length and bill depth, including specified customization and advanced features. Function Signature: ```python def create_complex_jointplot(): pass ``` Requirements: 1. **Load Data**: Use the `sns.load_dataset` function to load the `penguins` dataset. 2. **Initialize `JointGrid`**: Create a `JointGrid` instance for plotting with the dataset, using `bill_length_mm` as the `x` variable and `bill_depth_mm` as the `y` variable. 3. **Plot Scatter and Histogram with Customization**: - Plot a scatter plot for the joint axes with points having `alpha=0.5` and `s=100`. - Plot a histogram for the marginal axes with kernel density estimation (`kde=True`). 4. **Add Reference Lines**: Add horizontal and vertical reference lines at `bill_length_mm=45` and `bill_depth_mm=16`. 5. **Configure Layout**: - Set the figure height to 6. - Adjust the ratio between joint and marginal plots to `2`. - Reduce the space between plots to `0.05`. 6. **Show Plot**: Ensure that the final plot is displayed within the function. Output: - The function should display the joint plot with all specified customizations when called. # Example: The function call `create_complex_jointplot()` should display the plot as required above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_complex_jointplot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Initialize the JointGrid jointgrid = sns.JointGrid( data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', height=6, ratio=2, space=0.05 ) # Plot scatter for the joint axes jointgrid.plot_joint( sns.scatterplot, alpha=0.5, s=100 ) # Plot histogram with KDE for the marginal axes jointgrid.plot_marginals( sns.histplot, kde=True ) # Add horizontal and vertical reference lines plt.axvline(x=45, color=\'red\', linestyle=\'--\') plt.axhline(y=16, color=\'blue\', linestyle=\'--\') # Show the plot plt.show()"},{"question":"# Pandas Coding Assessment **Problem Description:** You are given a DataFrame `df` containing information about different books in a library. Each row in the DataFrame represents a single book with the following columns: - `title`: The title of the book (string). - `author`: The author(s) of the book, represented as a single string with authors separated by commas (string). - `isbn`: The ISBN number of the book, which can have extra spaces and special characters like hyphens (string). - `details`: Miscellaneous details about the book, which may include pricing, edition information, or any other relevant details (string). - `borrowed`: Whether the book is currently borrowed (`True`/`False`). Below is a sample of the DataFrame: | | title | author | isbn | details | borrowed | |---|------------------------|--------------------------------|---------------|-------------------------------|----------| | 0 | The Great Gatsby | F. Scott Fitzgerald | 978-0743273565 | 3rd edition, 10.99 | False | | 1 | To Kill a Mockingbird | Harper Lee | 978 0061120084 | 50th Anniversary, 7.99 | True | | 2 | 1984 | George Orwell | 978-0451524935 | | False | | 3 | Moby-Dick | Herman Melville | 978-1503280786 | 1st edition, 8.99, Classic | True | | 4 | The Catcher in the Rye | J.D. Salinger | 9780316769488 | 6.99 | False | | 5 | War and Peace | Leo Tolstoy, Constance Garnett | 978-1853260629 | | True | **Your task is to implement a function `clean_books_data(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations:** 1. **Clean the ISBN:** - Remove any spaces and hyphens to ensure all ISBN numbers are in a contiguous format. 2. **Summarize \'details\' column:** - Create two new columns: - `price`: Extract the price of the book as a string (e.g., \\"10.99\\"). If no price is found, set it as `NaN`. - `edition`: Extract the edition information as a string (e.g., \\"3rd edition\\"). If no edition information is found, set it as `NaN`. 3. **Standardize Author Names:** - Split the `author` column into multiple rows, such that each row represents a single author with the same book details repeated for each author. This process is known as data normalization. 4. **Return the cleaned DataFrame.** # Constraints: - Assume that the input DataFrame `df` is valid and contains the columns exactly as described. - You are allowed to use regular expressions for extraction tasks. - Missing values should be handled appropriately (as `NaN`). # Example Usage: ``` python import pandas as pd data = { \'title\': [\\"The Great Gatsby\\", \\"To Kill a Mockingbird\\", \\"1984\\", \\"Moby-Dick\\", \\"The Catcher in the Rye\\", \\"War and Peace\\"], \'author\': [\\"F. Scott Fitzgerald\\", \\"Harper Lee\\", \\"George Orwell\\", \\"Herman Melville\\", \\"J.D. Salinger\\", \\"Leo Tolstoy, Constance Garnett\\"], \'isbn\': [\\"978-0743273565\\", \\"978 0061120084\\", \\"978-0451524935\\", \\"978-1503280786\\", \\"9780316769488\\", \\"978-1853260629\\"], \'details\': [\\"3rd edition, 10.99\\", \\"50th Anniversary, 7.99\\", \\"\\", \\"1st edition, 8.99, Classic\\", \\"6.99\\", \\"\\"], \'borrowed\': [False, True, False, True, False, True] } df = pd.DataFrame(data) cleaned_df = clean_books_data(df) print(cleaned_df) ``` **Expected Output:** | | title | author | isbn | details | borrowed | price | edition | |---|------------------------|-------------------|---------------|-------------------------------|----------|---------|---------------| | 0 | The Great Gatsby | F. Scott Fitzgerald | 9780743273565 | 3rd edition, 10.99 | False | 10.99 | 3rd edition | | 1 | To Kill a Mockingbird | Harper Lee | 9780061120084 | 50th Anniversary, 7.99 | True | 7.99 | | | 2 | 1984 | George Orwell | 9780451524935 | | False | NaN | NaN | | 3 | Moby-Dick | Herman Melville | 9781503280786 | 1st edition, 8.99, Classic | True | 8.99 | 1st edition | | 4 | The Catcher in the Rye | J.D. Salinger | 9780316769488 | 6.99 | False | 6.99 | NaN | | 5 | War and Peace | Leo Tolstoy | 9781853260629 | | True | NaN | NaN | | 6 | War and Peace | Constance Garnett | 9781853260629 | | True | NaN | NaN | # Function Signature: ``` python def clean_books_data(df: pd.DataFrame) -> pd.DataFrame: pass ```","solution":"import pandas as pd import numpy as np import re def clean_books_data(df: pd.DataFrame) -> pd.DataFrame: # Clean the ISBN by removing spaces and hyphens df[\'isbn\'] = df[\'isbn\'].str.replace(r\'[s-]\', \'\', regex=True) # Create new columns for price and edition df[\'price\'] = df[\'details\'].str.extract(r\'(d+.d{2})\') df[\'edition\'] = df[\'details\'].str.extract(r\'(d+[a-z]{2} edition)\') # Splitting the authors into multiple rows df = df.assign(author=df[\'author\'].str.split(\',\')).explode(\'author\') df[\'author\'] = df[\'author\'].str.strip() # Only keep necessary columns and return return df[[\'title\', \'author\', \'isbn\', \'details\', \'borrowed\', \'price\', \'edition\']]"},{"question":"# Seaborn Coding Assessment Question **Objective**: Assess your ability to work with seaborn to create insightful visualizations and demonstrate your understanding of histogram customizations and data normalization. You are given a dataset about penguins which includes various attributes of penguins from different islands. **Dataset:** The penguins dataset includes the following columns: - `species`: Species of the penguin (Adelie, Chinstrap, Gentoo) - `island`: Island the penguin is from (Torgersen, Biscoe, Dream) - `bill_length_mm`: Length of the bill in millimeters - `bill_depth_mm`: Depth of the bill in millimeters - `flipper_length_mm`: Length of the flipper in millimeters - `body_mass_g`: Body mass in grams - `sex`: Sex of the penguin (Male, Female) **Task:** 1. Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. Create a histogram of `flipper_length_mm` to show its distribution. 3. Adjust the histogram bins to have a width of 5 mm. 4. Normalize the histogram to show the proportion of penguins within each bin. 5. Create separate histograms for each `island` and `species` combination, utilizing facet grids. 6. Normalize the histograms such that the proportions are calculated within each facet. You need to implement a function `visualize_penguin_flippers(data)`, which takes the dataset as input and generates the described plots. The function should not return any value; instead, it should display the plots. **Input:** - `data`: DataFrame, the penguins dataset. **Output:** - None (the function should display the plots). **Constraints:** - Ensure the dataset is correctly loaded before attempting to plot. - Handle any missing values in the dataset appropriately before plotting. - Your plots should be clear and easy to read with appropriate labels and titles where necessary. **Function Signature:** ```python import seaborn.objects as so import seaborn as sns from seaborn import load_dataset def visualize_penguin_flippers(data): # Your code here # Load the dataset penguins = load_dataset(\\"penguins\\") visualize_penguin_flippers(penguins) ``` **Notes:** - Utilize seaborn\'s `so.Plot` for creating the plots. - The function should display the visualizations directly.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_penguin_flippers(data): Visualizes histograms of flipper lengths for different penguin species on different islands, normalizing the histograms to show proportions within each bin. :param data: DataFrame containing the penguins dataset # Drop rows with missing flipper_length_mm data = data.dropna(subset=[\\"flipper_length_mm\\"]) # Convert the \'flipper_length_mm\' to bins of width 5 mm bin_width = 5 bins = np.arange(data[\'flipper_length_mm\'].min(), data[\'flipper_length_mm\'].max() + bin_width, bin_width) # Create a FacetGrid for the desired histograms g = sns.FacetGrid(data, row=\\"island\\", col=\\"species\\", margin_titles=True) # Map a histogram plot onto the grid g.map_dataframe(sns.histplot, x=\\"flipper_length_mm\\", bins=bins, stat=\'probability\') # Adjust titles and labels g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Proportion\\") g.set_titles(row_template=\\"{row_name}\\", col_template=\\"{col_name}\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Distribution of Penguin Flipper Lengths by Species and Island\') plt.show()"},{"question":"# HTML Safe Text Conversion and Restoration You are given a text that may contain special characters such as `&`, `<`, `>`, `\\"`, and `\'`. Your task is to perform the following steps: 1. **Escape** the given text to make it HTML-safe. 2. **Unescape** the HTML-safe text back to its original form. Write a function `convert_html_text(text: str) -> Tuple[str, str]` that takes the original text as input and returns a tuple containing: 1. The HTML-safe escaped text. 2. The text after unescaping the HTML-safe text. # Function Signature ```python def convert_html_text(text: str) -> Tuple[str, str]: pass ``` # Input - `text` (str): The original text that may contain special characters. # Output - A tuple (escaped_text, unescaped_text): - `escaped_text` is the text after escaping special characters. - `unescaped_text` is the text after unescaping the `escaped_text` back to its original form. # Constraints - The input text will have at most 10,000 characters. # Example ```python input_text = \'Hello & welcome to <Sample> \\"Text\\"!\' result = convert_html_text(input_text) # Expected Output: (\'Hello &amp; welcome to &lt;Sample&gt; &quot;Text&quot;!\', \'Hello & welcome to <Sample> \\"Text\\"!\') assert result == (\'Hello &amp; welcome to &lt;Sample&gt; &quot;Text&quot;!\', \'Hello & welcome to <Sample> \\"Text\\"!\') ``` # Notes - Make sure to handle any special characters as defined in the `html.escape` documentation. - The order of escaping and unescaping should preserve the contents accurately.","solution":"import html from typing import Tuple def convert_html_text(text: str) -> Tuple[str, str]: Escapes and then unescapes a text to ensure it is HTML safe and preserve original content. :param text: The original text that may contain special characters. :return: A tuple containing the escaped text and the unescaped text. escaped_text = html.escape(text) unescaped_text = html.unescape(escaped_text) return (escaped_text, unescaped_text)"},{"question":"**Objective:** Design a Python function to manage a database of employees. The function should be able to create the database, insert employee records, query for employees based on certain criteria, and update employee information as needed. **Task Description:** Implement a Python function `manage_employee_db(action, data=None)` that manages an employee database with the following functionalities: 1. **Creating the Database and Table:** - If `action` is `\\"create\\"`, the function should create an SQLite database named \\"employee.db\\" with a table named \\"employees\\". The table should have the following columns: - `emp_id` (INTEGER): The employee\'s ID (Primary Key). - `name` (TEXT): The employee\'s name. - `age` (INTEGER): The employee\'s age. - `department` (TEXT): The employee\'s department. - `salary` (REAL): The employee\'s salary. 2. **Inserting Employee Records:** - If `action` is `\\"insert\\"`, the function should insert a new employee record into the employees table. The `data` parameter will be a tuple of the form `(emp_id, name, age, department, salary)`. 3. **Querying Employee Records:** - If `action` is `\\"query\\"`, the function should fetch and return a list of employees matching the criteria given in the `data` parameter. The `data` parameter will be a dictionary where the keys are the column names and the values are the values to be matched. For example, `{\\"department\\": \\"HR\\", \\"age\\": 30}`. 4. **Updating Employee Records:** - If `action` is `\\"update\\"`, the function should update the information of an existing employee. The `data` parameter will be a tuple with two dictionaries. The first dictionary specifies the criteria to identify the employee, and the second dictionary specifies the new values to be updated. For example, `({\\"emp_id\\": 1}, {\\"age\\": 31, \\"salary\\": 60000.0})`. **Constraints:** - Assume the employee ID is unique and will always be provided when inserting or updating records. - You must use the `sqlite3` module to interact with the database. - Ensure that no SQL injection vulnerabilities are present in your code. **Function Signature:** ```python def manage_employee_db(action: str, data: Union[None, Tuple, Dict] = None) -> Union[None, List[Tuple]]: pass ``` # Examples: 1. Create the database and table: ```python manage_employee_db(\\"create\\") ``` 2. Insert an employee record: ```python manage_employee_db(\\"insert\\", (1, \\"John Doe\\", 28, \\"Engineering\\", 55000.0)) ``` 3. Query employee records: ```python result = manage_employee_db(\\"query\\", {\\"department\\": \\"Engineering\\", \\"age\\": 28}) print(result) # Output: [(1, \\"John Doe\\", 28, \\"Engineering\\", 55000.0)] ``` 4. Update an employee record: ```python manage_employee_db(\\"update\\", ({\\"emp_id\\": 1}, {\\"age\\": 29, \\"salary\\": 57000.0})) ``` Implement the function `manage_employee_db` to fulfill the above requirements.","solution":"import sqlite3 from typing import Union, Tuple, Dict, List def manage_employee_db(action: str, data: Union[None, Tuple, Dict] = None) -> Union[None, List[Tuple]]: conn = sqlite3.connect(\\"employee.db\\") c = conn.cursor() if action == \\"create\\": c.execute(\'\'\' CREATE TABLE IF NOT EXISTS employees ( emp_id INTEGER PRIMARY KEY, name TEXT NOT NULL, age INTEGER, department TEXT, salary REAL ) \'\'\') conn.commit() elif action == \\"insert\\": if data: c.execute(\'\'\' INSERT INTO employees (emp_id, name, age, department, salary) VALUES (?, ?, ?, ?, ?) \'\'\', data) conn.commit() elif action == \\"query\\": if data: query = \\"SELECT * FROM employees WHERE \\" + \' AND \'.join([f\\"{k} = ?\\" for k in data.keys()]) c.execute(query, tuple(data.values())) result = c.fetchall() return result elif action == \\"update\\": if data: criteria, new_values = data set_clause = \', \'.join([f\\"{k} = ?\\" for k in new_values.keys()]) where_clause = \' AND \'.join([f\\"{k} = ?\\" for k in criteria.keys()]) query = f\\"UPDATE employees SET {set_clause} WHERE {where_clause}\\" c.execute(query, tuple(new_values.values()) + tuple(criteria.values())) conn.commit() conn.close()"},{"question":"**Buffer Handling and Memory Views in Python** As part of a project, you are working with a custom binary data format that you frequently need to parse without unnecessary copying to ensure performance efficiency. You decide to utilize the Python buffer protocol and memory views to achieve this. # Problem: Write a function `parse_header` that accepts a memoryview over a byte buffer containing a data file header. The header contains a 2-byte unsigned short indicating the number of records, followed by a 4-byte float representing a threshold value. # Function Signature ```python def parse_header(buffer: memoryview) -> dict: ``` # Input - `buffer`: a memoryview object that points to a byte buffer. The buffer is guaranteed to contain at least 6 bytes (2 bytes for the number of records, followed by 4 bytes for the threshold value). # Output The function should return a dictionary with two keys: - `\'record_count\'`: an integer that represents the number of records. - `\'threshold\'`: a float that represents the threshold value. # Example ```python import struct buffer = memoryview(bytearray(struct.pack(\'Hf\', 10, 0.75))) result = parse_header(buffer) print(result) # Output: {\'record_count\': 10, \'threshold\': 0.75} ``` **Constraints:** - Do not copy the buffer; use the memoryview directly for parsing. - Use the `struct` module for unpacking the buffer. **Hints:** - You can use the `struct` module\'s unpacking capabilities directly on slices of the memoryview. - Be mindful of endianess; assume the data is in the \'little-endian\' format. **Performance Requirements:** - The function should execute in constant time, as only a fixed-size buffer is being parsed.","solution":"import struct def parse_header(buffer: memoryview) -> dict: Parses the header from the given memoryview buffer. Parameters: - buffer (memoryview): A memoryview object pointing to a byte buffer with a header. Returns: - dict: A dictionary with two keys: - \'record_count\': an integer that represents the number of records. - \'threshold\': a float that represents the threshold value. # Unpack the first 6 bytes: 2 bytes for the unsigned short and 4 bytes for the float. record_count, threshold = struct.unpack_from(\'Hf\', buffer, 0) return { \'record_count\': record_count, \'threshold\': threshold }"},{"question":"# Advanced Python Statement Understanding **Objective:** You are required to implement a function that tracks and manipulates global and nonlocal variables correctly, demonstrates augmented assignments, and effectively uses various assignment types. **Task:** Write a Python function `manipulate_variables` that accepts no arguments and behaves as follows: 1. There is a global variable `global_var` which starts at 100. 2. Inside `manipulate_variables`, define a local variable `local_var` which starts at 50. 3. Define a nested function `inner_function` within `manipulate_variables` that: - Uses a nonlocal variable `nonlocal_var` which starts at 25. - Uses `global_var` and `nonlocal_var` in augmented assignments. - Utilizes other assignment types as needed to conform to the tasks below. 4. `inner_function` should: - Decrease `global_var` by `local_var`. - Increase `nonlocal_var` by `global_var`. - Use the `assert` statement to ensure that `nonlocal_var` is not negative. - Return the current values of `global_var` and `nonlocal_var`. 5. The outer function `manipulate_variables` should: - Call `inner_function`. - Use various assignment operations to: - Change the value of `local_var` based on a multiplication assignment. - Change a list within a dictionary using assignment-style targeting. - Return the sum of the final `local_var`, `global_var`, and `nonlocal_var`. **Constraints:** - Ensure that the `assert` statement does not raise any exceptions. - Explicitly use the `global` and `nonlocal` keywords where necessary. - Handle any raised exceptions gracefully. - The function should return an integer as output. # Example Usage ```python global_var = 100 def manipulate_variables(): local_var = 50 def inner_function(): nonlocal nonlocal_var nonlocal_var = 25 global global_var # Using augmented assignments global_var -= local_var nonlocal_var += global_var # Assertion assert nonlocal_var >= 0 return global_var, nonlocal_var # Call the inner function global_val, nonlocal_val = inner_function() # Further assignments local_var *= 2 # Example of a multiplication assignment example_dict = {\'key\': [1, 2, 3]} example_dict[\'key\'][1] = local_var return local_var + global_val + nonlocal_val # Testing output = manipulate_variables() print(output) # Expected: integer result based on the operations described. ``` **Note:** You may assume standard global scope/global variable behaviors and that the initial values of the global and local variables get reset per function call. Ensure this solution demonstrates the mentioned simple statements practically and correctly in Python.","solution":"global_var = 100 def manipulate_variables(): local_var = 50 def inner_function(): nonlocal_var = 25 global global_var # Using augmented assignments global_var -= local_var nonlocal_var += global_var # Assertion assert nonlocal_var >= 0 return global_var, nonlocal_var # Call the inner function global_val, nonlocal_val = inner_function() # Further assignments local_var *= 2 # Multiplication assignment example_dict = {\'key\': [1, 2, 3]} example_dict[\'key\'][1] = local_var return local_var + global_val + nonlocal_val"},{"question":"**Objective**: Implement a Python function that toggles the terminal mode between raw and cbreak modes. This function will demonstrate the student\'s understanding of working with Unix terminal settings and the \\"tty\\" and \\"termios\\" modules. **Function Signature**: ```python def toggle_tty_mode(mode: str, fd: int) -> None: pass ``` **Input**: - `mode` (str): A string specifying the desired terminal mode, either \\"raw\\" or \\"cbreak\\". - `fd` (int): The file descriptor of the terminal to be modified. **Constraints**: - The function must handle invalid modes by raising a `ValueError` with an appropriate error message. - The function must only work on Unix systems. If the function is executed on a non-Unix system, it should raise an `OSError` with the message \\"Unsupported operating system\\". **Output**: - The function does not return anything. It changes the terminal mode as specified or raises an appropriate error if the input is invalid or the operation is unsupported. **Example**: ```python import os import sys def toggle_tty_mode(mode: str, fd: int) -> None: if os.name != \'posix\': raise OSError(\\"Unsupported operating system\\") if mode not in [\'raw\', \'cbreak\']: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'\\") import tty if mode == \'raw\': tty.setraw(fd) else: tty.setcbreak(fd) # Example usage: # Assuming `fd` is the file descriptor for the terminal, often obtained using `sys.stdin.fileno()` fd = sys.stdin.fileno() toggle_tty_mode(\'raw\', fd) toggle_tty_mode(\'cbreak\', fd) ``` **Note**: - Students should be aware of necessary imports and error handling. - Students are required to test the function in a compatible Unix environment and ensure proper error handling for unsupported modes and systems.","solution":"import os import tty def toggle_tty_mode(mode: str, fd: int) -> None: Toggle the terminal mode between raw and cbreak modes. Parameters: mode (str): The desired terminal mode, either \'raw\' or \'cbreak\'. fd (int): The file descriptor of the terminal to be modified. Raises: ValueError: If an invalid mode is provided. OSError: If the operating system is not Unix-based. if os.name != \'posix\': raise OSError(\\"Unsupported operating system\\") if mode not in [\'raw\', \'cbreak\']: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'\\") if mode == \'raw\': tty.setraw(fd) else: tty.setcbreak(fd)"},{"question":"# **Coding Assessment Question** **Objective:** Write a Python program that demonstrates your understanding of the `smtpd` module by creating a custom SMTP server that processes incoming email messages according to specific requirements. **Task:** You are to create a class `CustomSMTPServer` that extends `smtpd.SMTPServer`. This custom server should print specific details of the email message it processes and store certain information in an internal data structure. Follow the specifications below: 1. **Class Definition:** - Define a class `CustomSMTPServer` that inherits from `smtpd.SMTPServer`. 2. **Constructor:** - The constructor should take the same parameters as `smtpd.SMTPServer` (plus `args` and `kwargs`) and pass them to the superclass constructor. - Initialize an internal list `self.messages` to store processed email details. 3. **process_message Method:** - Override the `process_message` method to handle the processing of email messages. - The method should print the `peer`, `mailfrom`, `rcpttos`, and the first 50 characters of the `data`. - Store the following details in a dictionary and append to `self.messages`: - `peer` - `mailfrom` - `rcpttos` - `data` (first 50 characters) 4. **Additional Method:** - Implement a method `get_message_summary` that returns a summary of all processed messages in the form of a list of dictionaries. **Input:** - The `CustomSMTPServer` should be instantiated with local and remote address tuples, e.g., `(\'localhost\', 1025)` and `(\'localhost\', 1026)` respectively. **Output:** - When a message is processed, print the details as described. - Implement the `get_message_summary` method to return the list of stored message details. **Implementation Constraints:** - Do not use libraries or modules outside of the Python standard library. **Example Usage:** ```python if __name__ == \\"__main__\\": import asyncore server = CustomSMTPServer((\'localhost\', 1025), (\'localhost\', 1026)) asyncore.loop() # For testing purpose, simulate emails or check using an SMTP client # After simulation or termination, you can retrieve the message summaries: summaries = server.get_message_summary() for summary in summaries: print(summary) ``` **Note:** - Ensure that you handle potential exceptions and errors gracefully within your overridden `process_message` method. - You do not need to provide code to actually send emails; focus on the implementation of the `CustomSMTPServer`.","solution":"import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def __init__(self, localaddr, remoteaddr, *args, **kwargs): super().__init__(localaddr, remoteaddr, *args, **kwargs) self.messages = [] def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(\'Peer:\', peer) print(\'Mail from:\', mailfrom) print(\'Rcpt to:\', rcpttos) print(\'Data:\', data[:50]) message_details = { \'peer\': peer, \'mailfrom\': mailfrom, \'rcpttos\': rcpttos, \'data\': data[:50] } self.messages.append(message_details) def get_message_summary(self): return self.messages"},{"question":"# Seaborn Customization Challenge **Objective**: Demonstrate your ability to customize seaborn plots using its `plotting_context` function and other seaborn functionalities. In this task, you are required to create a function `customize_and_plot` that takes a dataset and a plotting context as input, and produces a customized plot according to the specified context parameters. Function Details - **Function Name**: `customize_and_plot` - **Input**: - `data` (pandas DataFrame): The dataset to be plotted. - `context` (str): The name of a predefined seaborn plotting context (e.g., `\\"paper\\"`, `\\"notebook\\"`, `\\"talk\\"`, `\\"poster\\"`). - **Output**: None (the function should display the plot). Requirements: 1. Load a built-in seaborn dataset of your choice (e.g., `\\"tips\\"`, `\\"iris\\"`, `\\"flights\\"`, etc.). 2. Use the `sns.plotting_context` function to set the plotting context according to the `context` provided. 3. Create a `lineplot` using seaborn, plotting one of the columns of the dataset against another. 4. Ensure to use a context manager to temporarily change the parameter values for this specific plot. 5. Customize the plot with additional seaborn features (e.g., `sns.set_style`, `sns.despine`, etc.) to enhance visual appeal. Constraints: - You must use the `sns.plotting_context` function to adjust the plotting context. - You should handle potential errors, such as invalid context names. - The plot should display correctly and be well-formatted. Example Usage: ```python import seaborn as sns import pandas as pd def customize_and_plot(data, context): with sns.plotting_context(context): sns.set_style(\\"whitegrid\\") sns.lineplot(x=\\"total_bill\\", y=\\"tip\\", data=data) sns.despine() # Load the built-in \'tips\' dataset tips = sns.load_dataset(\\"tips\\") customize_and_plot(data=tips, context=\\"talk\\") ``` In the example above, the function `customize_and_plot` sets the plotting context to `\\"talk\\"`, applies a white grid style to the plot, creates a line plot of `total_bill` against `tip` from the `tips` dataset, and then removes the top and right spines of the plot for a cleaner look. Ensure your solution meets the requirements and is capable of handling various predefined contexts effectively.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def customize_and_plot(data, context): try: with sns.plotting_context(context): sns.set_style(\\"whitegrid\\") sns.lineplot(x=data.columns[0], y=data.columns[1], data=data) sns.despine() plt.show() except ValueError as ve: print(f\\"Error: {ve}. Invalid context name provided.\\")"},{"question":"# I/O Multiplexing using the `select` module You are tasked with implementing a function that concurrently monitors multiple sockets for incoming data and readiness to send data, using the `select` module. The function should be capable of handling both Unix-based and Windows-based platforms. # Task: Implement the function `io_multiplex(sockets: List[socket.socket], timeout: float) -> Tuple[List[socket.socket], List[socket.socket], List[socket.socket]]` which performs the following: 1. **Input:** - `sockets`: A list of open socket objects to be monitored. - `timeout`: A floating point number specifying the maximum time in seconds to wait for any socket to become ready. If `timeout` is omitted or `None`, the function should block until at least one socket is ready. 2. **Output:** - Returns a tuple of three lists: - Read-ready sockets: List of sockets which are ready for reading. - Write-ready sockets: List of sockets which are ready for writing. - Exceptional condition sockets: List of sockets with exceptional conditions. # Constraints: - Ensure that the implementation works on both Unix and Windows platforms. - Use the `select` module functionalities (either `select()`, `poll()`, `devpoll()`, `epoll()`, or `kqueue()`) based on the platform capabilities. - Handle edge cases such as: - No sockets in the list. - All three lists being empty. - Interruptions by system signals. # Performance Requirements: - Optimize the function to ensure it is efficient for scenarios with high numbers of sockets (e.g., network servers handling thousands of clients concurrently). # Example Usage: ```python import socket # Create a few dummy socket connections for demonstration server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # Assuming server_socket and client_socket are properly configured and connected sockets = [server_socket, client_socket] # Call the function with a specified timeout of 5 seconds ready_to_read, ready_to_write, in_error = io_multiplex(sockets, 5.0) print(\\"Sockets ready for reading:\\", ready_to_read) print(\\"Sockets ready for writing:\\", ready_to_write) print(\\"Sockets in error state:\\", in_error) ``` # Your task is to complete the implementation of: ```python from typing import List, Tuple import socket def io_multiplex(sockets: List[socket.socket], timeout: float) -> Tuple[List[socket.socket], List[socket.socket], List[socket.socket]]: # Your implementation here pass ``` # Note: - You may refer to the Python `select` module documentation for the detailed functionalities. - Ensure the code handles different platforms appropriately.","solution":"from typing import List, Tuple import socket import select def io_multiplex(sockets: List[socket.socket], timeout: float = None) -> Tuple[List[socket.socket], List[socket.socket], List[socket.socket]]: if not sockets: return [], [], [] try: read_ready, write_ready, excpt_ready = select.select(sockets, sockets, sockets, timeout) return read_ready, write_ready, excpt_ready except select.error as e: print(f\\"An error occurred: {e}\\") return [], [], [] # Example usage for testing purpose: if __name__ == \\"__main__\\": import time server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: server_socket.bind((\'localhost\', 0)) # Bind to an available port server_socket.listen(1) client_socket.connect(server_socket.getsockname()) sockets = [server_socket, client_socket] ready_to_read, ready_to_write, in_error = io_multiplex(sockets, 1.0) print(\\"Sockets ready for reading:\\", [s.fileno() for s in ready_to_read]) print(\\"Sockets ready for writing:\\", [s.fileno() for s in ready_to_write]) print(\\"Sockets in error state:\\", [s.fileno() for s in in_error]) finally: client_socket.close() server_socket.close()"},{"question":"# Custom Autograd Function and Custom Module Objective Implement a custom autograd function in PyTorch and integrate it into a custom module. This will test your understanding of PyTorch\'s `torch.autograd` and `torch.nn` capabilities. Problem Statement Suppose you are working on a project where you need to implement a custom non-linear operation that has an expensive gradient computation. To optimize it, you decide to implement a custom autograd function to handle its forward and backward passes efficiently. 1. **Custom Autograd Function**: - Create a PyTorch autograd function `ExpSin` that computes `output = exp(x) * sin(x)` during the forward pass and correctly computes the gradient during the backward pass using the chain rule. 2. **Custom Module**: - Implement a custom PyTorch module `ExpSinLayer` that uses the `ExpSin` function in its forward pass. - The module should accept an input tensor `x` and apply the `ExpSin` operation to it. 3. **Testing the Implementation**: - Implement a test case to verify that your custom module works correctly. Specifically, ensure that the forward and backward passes compute values as expected. Instructions 1. **Custom Function Implementation**: - Define a class `ExpSinFunction` that inherits from `torch.autograd.Function`. - Implement the `forward` static method that computes `exp(x) * sin(x)`. - Implement the `setup_context` static method if needed. - Implement the `backward` static method that computes the gradient. 2. **Custom Module Implementation**: - Define a class `ExpSinLayer` that inherits from `torch.nn.Module`. - Implement the `__init__` method if needed. - Implement the `forward` method to utilize `ExpSinFunction`. 3. **Test Case**: - Create a test case using random input to ensure the forward output is as expected. - Use `torch.autograd.gradcheck` to verify the correctness of the gradient computation. Example ```python import torch import torch.nn as nn import torch.autograd as autograd # Step 1: Define the custom autograd function class ExpSinFunction(autograd.Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = torch.exp(input) * torch.sin(input) return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (torch.exp(input) * torch.sin(input) + torch.exp(input) * torch.cos(input)) return grad_input # Step 2: Define the custom module class ExpSinLayer(nn.Module): def __init__(self): super(ExpSinLayer, self).__init__() def forward(self, input): return ExpSinFunction.apply(input) # Step 3: Test the implementation def test_exp_sin_layer(): input = torch.randn(5, dtype=torch.double, requires_grad=True) layer = ExpSinLayer() output = layer(input) # Check the forward pass expected_output = torch.exp(input) * torch.sin(input) assert torch.allclose(output, expected_output), \\"Forward pass output is incorrect.\\" # Check the gradient using gradcheck input = input.double() # gradcheck needs double precision test = autograd.gradcheck(layer, (input,), eps=1e-6, atol=1e-4) assert test, \\"Backward pass gradient computation is incorrect.\\" print(\\"All tests passed.\\") # Run the test test_exp_sin_layer() ``` Constraints - The input tensor `x` will always have `requires_grad=True`. - Implement the forward and backward passes efficiently, avoiding unnecessary computations. - Ensure the module can handle both single-dimensional and multi-dimensional tensors correctly. Good luck!","solution":"import torch import torch.nn as nn import torch.autograd as autograd class ExpSinFunction(autograd.Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = torch.exp(input) * torch.sin(input) return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (torch.exp(input) * torch.sin(input) + torch.exp(input) * torch.cos(input)) return grad_input class ExpSinLayer(nn.Module): def __init__(self): super(ExpSinLayer, self).__init__() def forward(self, input): return ExpSinFunction.apply(input)"},{"question":"Coding Assessment Question # XML Data Processing with `xml.parsers.expat` Objective You are required to parse an XML document and extract specific data from it using the `xml.parsers.expat` module. This exercise will assess your ability to handle XML parsing events and manage parser states effectively. Problem Statement You are given an XML document containing information about books in a library. Each book entry includes an ISBN, title, author, and genre. You need to parse this XML data and create a dictionary that maps each genre to a list of books belonging to that genre. Each book should be represented as a dictionary containing its ISBN, title, and author. Example XML Document ```xml <?xml version=\\"1.0\\"?> <library> <book> <isbn>978-0-596-52068-7</isbn> <title>Learning Python</title> <author>Mark Lutz</author> <genre>Programming</genre> </book> <book> <isbn>978-0-201-61618-9</isbn> <title>The C Programming Language</title> <author>Brian W. Kernighan, Dennis M. Ritchie</author> <genre>Programming</genre> </book> <book> <isbn>978-0-7432-7356-5</isbn> <title>The Catcher in the Rye</title> <author>J.D. Salinger</author> <genre>Fiction</genre> </book> </library> ``` Function Signature ```python import xml.parsers.expat def parse_library(xml_data: str) -> dict: Parse the given XML data and return a dictionary mapping genres to lists of books. pass ``` Requirements 1. **Input:** A string `xml_data` containing the XML document. 2. **Output:** A dictionary mapping genres to lists of dictionaries, where each dictionary represents a book with its ISBN, title, and author. Constraints - The XML document is guaranteed to follow the structure given in the example. - The XML data can include any number of books, and genres may repeat. Implementation Steps 1. Create an `xml.parsers.expat` parser object. 2. Implement handler functions to process the start and end elements, and character data. 3. During parsing, collect book details and map them to their respective genres. 4. Handle any necessary state transitions between parsing different elements. 5. Ensure the final result is a dictionary with genres as keys and lists of book dictionaries as values. Example Usage ```python xml_data = <?xml version=\\"1.0\\"?> <library> <book> <isbn>978-0-596-52068-7</isbn> <title>Learning Python</title> <author>Mark Lutz</author> <genre>Programming</genre> </book> <book> <isbn>978-0-201-61618-9</isbn> <title>The C Programming Language</title> <author>Brian W. Kernighan, Dennis M. Ritchie</author> <genre>Programming</genre> </book> <book> <isbn>978-0-7432-7356-5</isbn> <title>The Catcher in the Rye</title> <author>J.D. Salinger</author> <genre>Fiction</genre> </book> </library> result = parse_library(xml_data) print(result) # Expected Output: # { # \\"Programming\\": [ # {\\"isbn\\": \\"978-0-596-52068-7\\", \\"title\\": \\"Learning Python\\", \\"author\\": \\"Mark Lutz\\"}, # {\\"isbn\\": \\"978-0-201-61618-9\\", \\"title\\": \\"The C Programming Language\\", \\"author\\": \\"Brian W. Kernighan, Dennis M. Ritchie\\"} # ], # \\"Fiction\\": [ # {\\"isbn\\": \\"978-0-7432-7356-5\\", \\"title\\": \\"The Catcher in the Rye\\", \\"author\\": \\"J.D. Salinger\\"} # ] # } ``` Notes - You may assume that the XML data does not contain any parsing errors. - Make use of the `xml.parsers.expat` module and its handler functions to achieve the goal.","solution":"import xml.parsers.expat def parse_library(xml_data: str) -> dict: Parse the given XML data and return a dictionary mapping genres to lists of books. parsed_data = {} current_book = {} current_element = None current_genre = None def start_element(name, attrs): nonlocal current_element if name == \'book\': current_book.clear() current_element = name def end_element(name): nonlocal current_genre, current_element if name == \'book\' and current_genre: if current_genre not in parsed_data: parsed_data[current_genre] = [] parsed_data[current_genre].append(current_book.copy()) current_element = None def char_data(data): nonlocal current_genre, current_element if current_element == \'isbn\': current_book[\'isbn\'] = data.strip() elif current_element == \'title\': current_book[\'title\'] = data.strip() elif current_element == \'author\': current_book[\'author\'] = data.strip() elif current_element == \'genre\': current_genre = data.strip() parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_data, True) return parsed_data"},{"question":"Objective Implement a custom `torch.autograd.Function` to perform a matrix multiplication operation in PyTorch that calls into NumPy for the actual multiplication. Additionally, ensure that this implementation is compatible with `torch.func.grad` for calculating gradients and `torch.vmap` for parallelising the operation across batches. Description You are required to implement a custom `torch.autograd.Function` named `NumpyMatMul` that performs matrix multiplication using NumPy under the hood. Your task includes: 1. Implementing the forward pass using NumPy to perform the matrix multiplication. 2. Setting up the appropriate context to save intermediate results for the backward pass. 3. Implementing the backward pass to correctly propagate gradients. 4. Adding support for `torch.vmap` to parallelize the operation over batch dimensions. Requirements 1. **Function Implementation**: - The `NumpyMatMul` should accept two inputs: `x` and `y`, both Tensors. - The forward pass should perform matrix multiplication using NumPy. - The setup_context method should save necessary Tensors for the backward pass. - The backward pass should calculate and return the gradients with respect to `x` and `y`. 2. **Input and Output**: - Inputs: `x` of shape `(m, n)` and `y` of shape `(n, p)`. - Output: The result of the matrix multiplication of shape `(m, p)`. 3. **Constraints and Limitations**: - Both inputs must be 2D Tensors. - You must ensure that the implementation is compatible with `torch.func.grad`. - Add support for `torch.vmap` by manually defining the `vmap` staticmethod to parallelize the operation along batch dimensions. 4. **Performance Requirements**: - The backward pass should be implemented using only PyTorch operations. - Ensure that the function works correctly with batched inputs, where either both or none of the inputs are batched. Function Signature ```python import torch import numpy as np class NumpyMatMul(torch.autograd.Function): @staticmethod def forward(x, y): device = x.device x = x.cpu().numpy() y = y.cpu().numpy() result = np.matmul(x, y) return torch.tensor(result, device=device) @staticmethod def setup_context(ctx, inputs, output): x, y = inputs ctx.save_for_backward(x, y) @staticmethod def backward(ctx, grad_output): x, y = ctx.saved_tensors grad_x = torch.matmul(grad_output, y.t()) grad_y = torch.matmul(x.t(), grad_output) return grad_x, grad_y @staticmethod def vmap(info, in_dims, x, y): x_bdim, y_bdim = in_dims x = x.movedim(x_bdim, 0) y = y.movedim(y_bdim, 0) return NumpyMatMul.apply(x, y), 0 # Helper function to use NumpyMatMul def numpy_matmul(x, y): return NumpyMatMul.apply(x, y) # Test the implementation m, n, p = 2, 3, 4 x = torch.randn(m, n, requires_grad=True) y = torch.randn(n, p, requires_grad=True) # Calculate the result result = numpy_matmul(x, y) print(\\"Result of matrix multiplication:\\", result) # Calculate the gradients using torch.func.grad grad_x, grad_y = torch.func.grad(lambda x, y: numpy_matmul(x, y).sum())(x, y) print(\\"Gradients of x:\\", grad_x) print(\\"Gradients of y:\\", grad_y) # Test vmap functionality batch_size = 5 x_batched = torch.randn(batch_size, m, n, requires_grad=True) y_batched = torch.randn(batch_size, n, p, requires_grad=True) result_batched = torch.vmap(numpy_matmul)(x_batched, y_batched) print(\\"Result of batched matrix multiplication:\\", result_batched) ```","solution":"import torch import numpy as np class NumpyMatMul(torch.autograd.Function): @staticmethod def forward(ctx, x, y): device = x.device x_np = x.cpu().numpy() y_np = y.cpu().numpy() result = np.matmul(x_np, y_np) ctx.save_for_backward(x, y) return torch.tensor(result, device=device) @staticmethod def backward(ctx, grad_output): x, y = ctx.saved_tensors grad_x = torch.matmul(grad_output, y.t()) grad_y = torch.matmul(x.t(), grad_output) return grad_x, grad_y @staticmethod def vmap(ctx, x, y): return torch.stack([NumpyMatMul.apply(x[i], y[i]) for i in range(x.size(0))]) # Helper function to use NumpyMatMul def numpy_matmul(x, y): return NumpyMatMul.apply(x, y)"},{"question":"<|Analysis Begin|> The provided documentation snippet indicates that it is related to the experimental APIs in `torch.nn.attention`. The experimental nature suggests that these APIs are new or in development and may change. Since the documentation is very brief and states only a warning message about the experimental nature without detailing the actual functionalities or interfaces, it is challenging to derive a complete and well-defined coding assessment question directly from this document alone. However, based on my experience and understanding of PyTorch and attention mechanisms, I can create a problem related to implementing a basic attention mechanism using PyTorch, which will assess students\' comprehension of PyTorch\'s advanced concepts. <|Analysis End|> <|Question Begin|> # Attention Mechanism in PyTorch Attention mechanisms have become a critical component in many neural network architectures, including transformers and sequence-to-sequence models. Task Your task is to implement a basic attention mechanism in PyTorch. The attention mechanism should compute a weighted sum of input values based on learned attention weights. Specifically, you need to implement the `Attention` class with the following methods: 1. `__init__(self, input_dim)`: Initialize the attention mechanism with the given input dimension. 2. `forward(self, query, key, value)`: Perform the attention mechanism to compute the context vector. Class Specification ```python class Attention(nn.Module): def __init__(self, input_dim): Initializes the Attention mechanism. Args: input_dim (int): The dimension of the input features (query, key, value). super(Attention, self).__init__() # Define the weights of attention layers self.W_q = nn.Linear(input_dim, input_dim) self.W_k = nn.Linear(input_dim, input_dim) self.W_v = nn.Linear(input_dim, input_dim) self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value): Compute the context vector as a weighted sum of the value vectors. Args: query (torch.Tensor): Tensor of shape (batch_size, seq_length, input_dim). key (torch.Tensor): Tensor of shape (batch_size, seq_length, input_dim). value (torch.Tensor): Tensor of shape (batch_size, seq_length, input_dim). Returns: torch.Tensor: Context vector of shape (batch_size, input_dim). # Compute attention scores query_proj = self.W_q(query) key_proj = self.W_k(key) value_proj = self.W_v(value) # Compute scaled dot-product attention scores = torch.bmm(query_proj, key_proj.transpose(1, 2)) / (key.shape[-1] ** 0.5) # Normalize the scores attn_weights = self.softmax(scores) # Compute the context vector as the weighted average of value vectors context = torch.bmm(attn_weights, value_proj).sum(dim=1) return context ``` Constraints - You may assume that all input tensors (`query`, `key`, `value`) are of the same shape: `(batch_size, seq_length, input_dim)`. - You should not use any built-in attention mechanisms provided by PyTorch. Example Usage ```python import torch import torch.nn as nn batch_size = 2 seq_length = 4 input_dim = 8 # Dummy input tensors query = torch.randn(batch_size, seq_length, input_dim) key = torch.randn(batch_size, seq_length, input_dim) value = torch.randn(batch_size, seq_length, input_dim) # Initialize the attention mechanism attention = Attention(input_dim) # Compute context vector context = attention(query, key, value) print(context.shape) # Expected output: (batch_size, input_dim) ``` # Evaluation Your implementation will be evaluated on: 1. Correctness: The attention weights and the resulting context vector should satisfy the expected properties of the attention mechanism. 2. Code quality: Ensure your code is clean and well-documented. 3. Efficiency: Ensure your implementation is optimized for performance, considering the constraints of input dimensions.","solution":"import torch import torch.nn as nn class Attention(nn.Module): def __init__(self, input_dim): Initializes the Attention mechanism. Args: input_dim (int): The dimension of the input features (query, key, value). super(Attention, self).__init__() # Define the weights of attention layers self.W_q = nn.Linear(input_dim, input_dim) self.W_k = nn.Linear(input_dim, input_dim) self.W_v = nn.Linear(input_dim, input_dim) self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value): Compute the context vector as a weighted sum of the value vectors. Args: query (torch.Tensor): Tensor of shape (batch_size, seq_length, input_dim). key (torch.Tensor): Tensor of shape (batch_size, seq_length, input_dim). value (torch.Tensor): Tensor of shape (batch_size, seq_length, input_dim). Returns: torch.Tensor: Context vector of shape (batch_size, input_dim). # Compute attention scores query_proj = self.W_q(query) key_proj = self.W_k(key) value_proj = self.W_v(value) # Compute scaled dot-product attention scores = torch.bmm(query_proj, key_proj.transpose(1, 2)) / (key.shape[-1] ** 0.5) # Normalize the scores attn_weights = self.softmax(scores) # Compute the context vector as the weighted average of value vectors context = torch.bmm(attn_weights, value_proj).sum(dim=1) return context"},{"question":"# Custom Iterator Implementation Challenge You have learned about Python\'s iterator protocols and the use of two general-purpose iterator objects: sequence iterators and callable iterators. Your task is to demonstrate your understanding by implementing two custom iterators based on the documentation provided. # Task 1: Sequence Iterator Implement a class `CustomSeqIter` that mimics the behavior of `PySeqIter_New`. This iterator should take any sequence object and iterate over it until `IndexError` is raised. **Class**: ```python class CustomSeqIter: def __init__(self, seq): # Initialize your sequence iterator pass def __iter__(self): # Define the __iter__ method which returns the iterator object pass def __next__(self): # Define the __next__ method which should iterate to the next item # If the end of the sequence is reached, raise StopIteration pass ``` **Example**: ```python seq = [1, 2, 3, 4] iterator = CustomSeqIter(seq) for item in iterator: print(item) # Should print 1, 2, 3, 4 on separate lines ``` # Task 2: Callable Iterator Implement a class `CustomCallIter` that mimics the behavior of `PyCallIter_New`. This iterator should take a callable and a sentinel value, calling the callable for each item, and stopping the iteration when the callable returns the sentinel value. **Class**: ```python class CustomCallIter: def __init__(self, callable, sentinel): # Initialize your callable iterator pass def __iter__(self): # Define the __iter__ method which returns the iterator object pass def __next__(self): # Define the __next__ method which should iterate to the next item # If the callable returns the sentinel value, raise StopIteration pass ``` **Example**: ```python def generate_numbers(): if \'num\' not in generate_numbers.__dict__: generate_numbers.num = 0 generate_numbers.num += 1 return generate_numbers.num sentinel = 5 iterator = CustomCallIter(generate_numbers, sentinel) for item in iterator: print(item) # Should print 1, 2, 3, 4 on separate lines ``` # Constraints: - You must not use any built-in iterator functions directly like `iter()`, `next()`, etc. - You should handle the necessary exception correctly for ending the iteration process. # Notes: - Clearly specify the purpose of each method in your class. - Ensure your code is well-documented and follows best practices.","solution":"class CustomSeqIter: def __init__(self, seq): Initialize the sequence iterator with the sequence passed as argument. self.seq = seq self.index = 0 def __iter__(self): The __iter__ method returns the iterator object itself. return self def __next__(self): The __next__ method returns the next item in the sequence. If the end of the sequence is reached, it raises StopIteration. if self.index >= len(self.seq): raise StopIteration result = self.seq[self.index] self.index += 1 return result class CustomCallIter: def __init__(self, callable, sentinel): Initialize the callable iterator with a callable and a sentinel value. self.callable = callable self.sentinel = sentinel def __iter__(self): The __iter__ method returns the iterator object itself. return self def __next__(self): The __next__ method calls the callable and returns its result. If the callable returns the sentinel value, it raises StopIteration. value = self.callable() if value == self.sentinel: raise StopIteration return value"},{"question":"<|Analysis Begin|> The logging cookbook documentation provides extensive information about the logging module and its usage in Python. It covers various topics, such as using logging in multiple modules, logging from multiple threads, multiple handlers and formatters, custom handling of levels, context-specific logging, multiplexed logging, custom logging handlers, and more. The documentation also provides a number of practical examples, which include code snippets for logging configuration, logging from multiple threads, logging to multiple destinations, custom handling of levels with filters, customized formatting, structured logging, synchronous and asynchronous logging, and exception logging. The key concepts involved in the logging module that could be tested in a coding assessment include: - Creating loggers, logging handlers, and formatters. - Configuring logging via code or a configuration dictionary. - Logging from different modules and threads. - Using filters for custom level handling. - Handling logging in multi-process and multi-threaded environments. - Customizing log record creation and formatting. - Using various built-in and custom logging handlers. - Managing context-specific logging. Given this broad scope of topics, a coding question could be designed to assess the understanding of configuring and using the logging module in a Python application, including the ability to log context-specific information from multiple modules and threads. <|Analysis End|> <|Question Begin|> **Task: Implement Context-Specific Logging in a Multi-Threaded Application** **Objective:** The aim of this task is to assess your understanding of the Python logging module by implementing a logging system for a multi-threaded application that logs context-specific information. **Problem Statement:** You are required to implement a logging system for a Python application that handles order processing. The application consists of a main module and an auxiliary module. Orders can be processed concurrently in multiple threads, and each thread will log order-specific information. **Requirements:** 1. Create a logger with the name \'order_processing\' in the main module. 2. Configure the logger to log messages to both a file and the console. The file handler should log messages of level DEBUG and higher, while the console handler should log messages of level INFO and higher. 3. Implement a custom logging filter that adds the order ID to each log message. 4. Ensure that the logs include context-specific information, such as the order ID and the thread name from which the log is generated. 5. Demonstrate logging from both the main module and an auxiliary module, with order processing handled in multiple threads. **Input Format:** - The order IDs to be processed. **Output Format:** - Log messages should be written to a file named \'orders.log\' in the current directory and also output to the console. **Constraints:** 1. Use the built-in logging module. 2. Ensure thread-safe logging. 3. Use proper exception handling to manage any potential errors during logging. **Example Code Structure:** ```python import logging import threading import time # Define a custom filter to add order ID to log records class OrderIDFilter(logging.Filter): def __init__(self, order_id): super().__init__() self.order_id = order_id def filter(self, record): record.order_id = self.order_id return True # Main module def main(): # Create and configure the main logger logger = logging.getLogger(\'order_processing\') logger.setLevel(logging.DEBUG) # Create file handler and console handler with appropriate log levels fh = logging.FileHandler(\'orders.log\') fh.setLevel(logging.DEBUG) ch = logging.StreamHandler() ch.setLevel(logging.INFO) # Create and set a formatter for the handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - Order ID: %(order_id)s - %(threadName)s - %(message)s\') fh.setFormatter(formatter) ch.setFormatter(formatter) # Add the handlers to the logger logger.addHandler(fh) logger.addHandler(ch) # Define a function to process orders def process_order(order_id): # Add the custom filter with order ID to the logger logger.addFilter(OrderIDFilter(order_id)) logger.info(f\'Starting processing of order {order_id}\') time.sleep(2) # Simulate order processing logger.debug(f\'Order {order_id} processed successfully\') logger.removeFilter(OrderIDFilter(order_id)) # List of order IDs to process orders = [101, 102, 103, 104] # Create and start a thread for each order threads = [] for order_id in orders: thread = threading.Thread(target=process_order, args=(order_id,)) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() if __name__ == \'__main__\': main() ``` **Expected Output:** ``` 2023-01-01 10:00:00 - order_processing - INFO - Order ID: 101 - Thread-1 - Starting processing of order 101 2023-01-01 10:00:00 - order_processing - INFO - Order ID: 102 - Thread-2 - Starting processing of order 102 2023-01-01 10:00:02 - order_processing - DEBUG - Order ID: 101 - Thread-1 - Order 101 processed successfully 2023-01-01 10:00:02 - order_processing - DEBUG - Order ID: 102 - Thread-2 - Order 102 processed successfully ... ``` **Note:** - Ensure that you add and remove the custom filter for each thread to avoid interference between logs of different orders. - Include proper exception handling in the `process_order` function for robustness. - The provided structure is a starting point, and you may need to adjust and expand it to meet all requirements.","solution":"import logging import threading import time # Define a custom filter to add order ID to log records class OrderIDFilter(logging.Filter): def __init__(self, order_id): super().__init__() self.order_id = order_id def filter(self, record): record.order_id = self.order_id return True # Main module def main(): # Create and configure the main logger logger = logging.getLogger(\'order_processing\') logger.setLevel(logging.DEBUG) # Create file handler and console handler with appropriate log levels fh = logging.FileHandler(\'orders.log\') fh.setLevel(logging.DEBUG) ch = logging.StreamHandler() ch.setLevel(logging.INFO) # Create and set a formatter for the handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - Order ID: %(order_id)s - %(threadName)s - %(message)s\') fh.setFormatter(formatter) ch.setFormatter(formatter) # Add the handlers to the logger logger.addHandler(fh) logger.addHandler(ch) # Define a function to process orders def process_order(order_id): try: # Add the custom filter with order ID to the logger order_filter = OrderIDFilter(order_id) logger.addFilter(order_filter) logger.info(f\'Starting processing of order {order_id}\') time.sleep(2) # Simulate order processing logger.debug(f\'Order {order_id} processed successfully\') except Exception as e: logger.error(f\'Error while processing order {order_id}: {e}\') finally: # Ensure the filter is removed after logging logger.removeFilter(order_filter) # List of order IDs to process orders = [101, 102, 103, 104] # Create and start a thread for each order threads = [] for order_id in orders: thread = threading.Thread(target=process_order, args=(order_id,)) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() if __name__ == \'__main__\': main()"},{"question":"Advanced File Processing with the `fileinput` Module **Objective:** Your task is to implement a Python function using the `fileinput` module to process lines from multiple input files, apply transformations, handle special encodings, and manage resources effectively. **Requirements:** 1. **Function Definition:** ```python def process_files(files: list[str], encoding: str = \'utf-8\') -> list[str]: Processes lines from multiple input files, applies specified transformations, and returns the transformed lines as a list. Args: - files (list[str]): A list of file names to be processed. - encoding (str, optional): The encoding to use for reading files. Default is \'utf-8\'. Returns: - list[str]: A list of transformed lines from the input files. ``` 2. **Function Behavior:** - The function should use `fileinput.input()` to iterate over lines from the specified `files`. - Each line read should be stripped of its newline character. - If a line starts with a `#`, it should be ignored. - Transform each remaining line by reversing its characters. - Return a list of these transformed, non-comment lines. 3. **Constraints and Limitations:** - If no files are provided (`files` is an empty list), the function should default to reading from `sys.stdin`. - Each file should be read with the specified `encoding`. - Handle any I/O errors gracefully and raise a custom `FileProcessingError` with a relevant message. 4. **Testing:** - Write unit tests to validate the function with a variety of input scenarios, including: - Multiple files with different content. - Files with different encodings. - Edge cases such as empty files or files with only comment lines. # Example Usage: ```python # Assume input files \\"file1.txt\\" and \\"file2.txt\\" with the following content: # file1.txt: # Hello # # This is a comment # World # file2.txt: # Python # Programming # Expected Output: # [\'olleH\', \'dlroW\', \'nohtyP\', \'gnimmargorP\'] result = process_files([\'file1.txt\', \'file2.txt\']) print(result) ``` # Custom Exception: ```python class FileProcessingError(Exception): pass ``` **Note:** Implement a comprehensive solution that demonstrates your understanding of the `fileinput` module, its functions, and handles various edge cases effectively.","solution":"import fileinput import sys class FileProcessingError(Exception): pass def process_files(files: list[str], encoding: str = \'utf-8\') -> list[str]: Processes lines from multiple input files, applies specified transformations, and returns the transformed lines as a list. Args: - files (list[str]): A list of file names to be processed. - encoding (str, optional): The encoding to use for reading files. Default is \'utf-8\'. Returns: - list[str]: A list of transformed lines from the input files. transformed_lines = [] try: with fileinput.input(files=files, encoding=encoding) as f: for line in f: stripped_line = line.rstrip(\'n\') if not stripped_line.startswith(\'#\'): transformed_lines.append(stripped_line[::-1]) except IOError as e: raise FileProcessingError(f\\"IO error occurred: {e}\\") except Exception as e: raise FileProcessingError(f\\"An error occurred: {e}\\") return transformed_lines"},{"question":"**Objective**: The goal of this question is to assess your understanding of the `seaborn.objects` module, including loading and processing datasets, creating complex plots with faceting, and customizing their appearance. **Question**: You are provided with a dataset `healthexp` that contains healthcare expenditure data for different countries across several years. Your task is to visualize this data using the `seaborn.objects` module in a specified manner. Requirements: 1. Load the `healthexp` dataset using the `seaborn.load_dataset` method. Ensure that the data is pivoted, interpolated, and reshaped appropriately as shown in the documentation. 2. Create a faceted plot where each facet corresponds to a different country. The plots should be wrapped to display three plots per row. 3. For each country\'s plot, add an area plot to visualize the healthcare expenditure (`Spending_USD`) over the years. 4. Customize the plots with the following aesthetics: - The fill color should be determined by the country. - The edge color should also represent the country, but with a specified edge width. 5. On top of the area plot, add a line plot with no edge width to highlight the trend of expenditures over the years. **Input**: - No direct input is required as you will use the `healthexp` dataset provided by Seaborn. **Output**: - A faceted plot showing the healthcare expenditure trends over the years for each country. **Constraints**: - You must use the `seaborn.objects` module. - Ensure that the plots are faceted and wrapped as specified. - Properly handle and process the dataset as shown. **Performance Requirements**: - Your solution should be efficient and handle the dataset operations and plotting within a reasonable time frame. **Example**: There is no example output, but follow the steps and requirements to ensure your plot matches the described specifications. **Code Skeleton**: ```python import seaborn.objects as so from seaborn import load_dataset # Load and process the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the faceted plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\", edgewidth=2, edgecolor=\\"Country\\").add(so.Line(edgewidth=0)) p.show() ``` Your task is to complete the code skeleton provided and ensure it meets the requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and process the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the faceted plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\", edgewidth=2, edgecolor=\\"Country\\").add(so.Line(edgewidth=0)) p.show()"},{"question":"**Email Content Manager Task** You are required to write a program that utilizes the `email.contentmanager` module to manage MIME content effectively. Specifically, you will implement a custom content manager class to handle custom MIME types for extracting specific information and setting content accordingly. Your implementation should extend the features of `raw_data_manager`. # Requirements 1. **Custom Content Manager Class**: Define a class `CustomContentManager` that inherits from `ContentManager`. 2. **Custom Handlers**: - Implement a `get_content` handler for the custom MIME type `application/custom`. - Implement a `set_content` handler for Python dictionary objects, which should be encoded as JSON and set with the MIME type `application/custom`. 3. **Testing Script**: - Create an `EmailMessage` object. - Use your `CustomContentManager` to set the content of this message using a sample Python dictionary. - Retrieve the content from the message, ensuring the encoding and decoding processes work as intended. # Input and Output - **Input**: A Python dictionary, for example: ```python { \\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\", \\"age\\": 30 } ``` - **Output**: The decoded dictionary from the message content. # Constraints - Use the `json` module for encoding and decoding dictionary objects. - Ensure proper error handling when the MIME type does not match or when invalid content is provided. # Performance Requirements - The program should efficiently handle the encoding and decoding of content. - Make sure the handlers are properly registered and utilized by the `CustomContentManager`. # Example ```python import json from email.contentmanager import ContentManager, raw_data_manager from email.message import EmailMessage class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/custom\', self._get_custom_content) self.add_set_handler(dict, self._set_custom_content) def _get_custom_content(self, msg, *args, **kwargs): payload = msg.get_payload(decode=True) return json.loads(payload) def _set_custom_content(self, msg, obj, *args, **kwargs): json_data = json.dumps(obj) msg.clear_content() msg.set_payload(json_data) msg[\'Content-Type\'] = \'application/custom\' sample_dict = { \\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\", \\"age\\": 30 } # Testing script msg = EmailMessage() manager = CustomContentManager() manager.set_content(msg, sample_dict) print(\\"Set email content: \\", msg.as_string()) retrieved_content = manager.get_content(msg) print(\\"Retrieved content: \\", retrieved_content) ``` This task will test the students\' ability to understand and implement custom content management with MIME types, demonstrating their comprehension of advanced concepts provided in the `email.contentmanager` module.","solution":"import json from email.contentmanager import ContentManager from email.message import EmailMessage class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/custom\', self._get_custom_content) self.add_set_handler(dict, self._set_custom_content) def _get_custom_content(self, msg, *args, **kwargs): payload = msg.get_payload(decode=True) return json.loads(payload) def _set_custom_content(self, msg, obj, *args, **kwargs): json_data = json.dumps(obj) msg.clear_content() msg.set_payload(json_data) msg.set_type(\'application/custom\') # Example usage to verify correctness if __name__ == \\"__main__\\": sample_dict = { \\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\", \\"age\\": 30 } # Testing script msg = EmailMessage() manager = CustomContentManager() manager.set_content(msg, sample_dict) print(\\"Set email content: \\", msg.as_string()) retrieved_content = manager.get_content(msg) print(\\"Retrieved content: \\", retrieved_content)"},{"question":"# Question: Implementing a Custom Nested Dictionary Class with Iteration You are required to implement a class `NestedDict`, which will be a custom dictionary that supports nested dictionaries as values. This class will also support iteration over all keys, including those in nested dictionaries, in a flattened manner. Requirements: 1. **Class Definition:** - Define the class `NestedDict`. 2. **Initialization Method:** - The class should support initialization with an optional dictionary. - Example: `nd = NestedDict({\\"a\\": 1, \\"b\\": {\\"c\\": 2, \\"d\\": 3}})` 3. **Attribute Access:** - Implement methods to allow standard dictionary-like access (both get and set operations). - Example: `nd[\\"e\\"] = 5`, `print(nd[\\"b\\"][\\"d\\"]) # Should output 3` 4. **Flattening Keys:** - Implement an instance method `flatten_keys()` which returns a list of all keys in the nested dictionary, flattened. - Example: `nd.flatten_keys() # Should return [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"]` 5. **Iterator Protocol:** - Implement iterator protocol to iterate over all keys as if the dictionary was flattened. - Example: ```python for key in nd: print(key) # Should print: # a # b # c # d # e ``` Constraints: - Keys in the dictionary are assumed to be strings. - The nested dictionaries can be arbitrarily deep. Performance Requirements: - Aim for efficiency when implementing the `flatten_keys` method and the iterator protocol. - Avoid recursion depth issues for deep dictionaries. # Deliverables: 1. The `NestedDict` class implementation. 2. Example usage demonstrating the features listed above. Example Implementation: ```python class NestedDict: def __init__(self, initial_dict=None): pass def __setitem__(self, key, value): pass def __getitem__(self, key): pass def flatten_keys(self): pass def __iter__(self): pass # Example Usage nd = NestedDict({\\"a\\": 1, \\"b\\": {\\"c\\": 2, \\"d\\": 3}}) nd[\\"e\\"] = 5 print(nd[\\"b\\"][\\"d\\"]) # Outputs: 3 print(nd.flatten_keys()) # Outputs: [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"] for key in nd: print(key) # Outputs: # a # b # c # d # e ```","solution":"class NestedDict: def __init__(self, initial_dict=None): self.data = initial_dict if initial_dict else {} def __setitem__(self, key, value): self.data[key] = value def __getitem__(self, key): return self.data[key] def flatten_keys(self): def _flatten(d): keys = [] for k, v in d.items(): keys.append(k) if isinstance(v, dict): keys.extend(_flatten(v)) return keys return _flatten(self.data) def __iter__(self): def flatten_dict(d): for k, v in d.items(): yield k if isinstance(v, dict): yield from flatten_dict(v) return iter(flatten_dict(self.data))"},{"question":"# XML Manipulation and Parsing Objective Using the `xml.etree.ElementTree` module, implement a function that parses an XML document containing information about books, retrieves specific data based on criteria, modifies some parts of the XML, and finally writes the modified XML to a new file. Task Write a function named `process_books_xml` that takes two arguments: `input_file` and `output_file`. 1. **Parsing**: The function should parse an XML document from the given `input_file`. The XML structure contains a list of books with the following structure: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> ... </catalog> ``` 2. **Data Retrieval**: Retrieve all books written by a specific author (e.g., \\"Gambardella, Matthew\\"). 3. **Modification**: Increase the price of all books by 10%. Also, update the publish date of books written by a specific author (e.g., \\"Gambardella, Matthew\\") to the current date. 4. **Output**: Write the modified XML to the `output_file`. Constraints * Handle parsing errors gracefully. * Assume the input XML is well-formed. * The current date can be obtained using the `datetime` module. * The price should be a floating-point number, rounded to two decimal places. Function Signature ```python import datetime import xml.etree.ElementTree as ET def process_books_xml(input_file: str, output_file: str) -> None: # Implementation here ``` Example If given the following `input_file`: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies.</description> </book> </catalog> ``` The `output_file` should contain: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>49.45</price> <publish_date>[current_date]</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>6.55</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies.</description> </book> </catalog> ``` where `[current_date]` represents the date when the function was executed.","solution":"import datetime import xml.etree.ElementTree as ET def process_books_xml(input_file: str, output_file: str) -> None: try: tree = ET.parse(input_file) root = tree.getroot() author_to_update = \\"Gambardella, Matthew\\" current_date = datetime.date.today().strftime(\'%Y-%m-%d\') for book in root.findall(\'book\'): # Retrieve author information author = book.find(\'author\').text # Increase the price by 10% price = float(book.find(\'price\').text) new_price = round(price * 1.10, 2) book.find(\'price\').text = str(new_price) # Update publish date if the author is the specified one if author == author_to_update: book.find(\'publish_date\').text = current_date # Write the modified XML to the output file tree.write(output_file) except ET.ParseError as e: print(f\\"Error parsing the XML file: {e}\\")"},{"question":"**Time Tracking with Sleep Schedules** **Objective:** Create a function that simulates a user-defined sleep schedule over a week, incorporating Daylight Saving Time and timezone changes. Students must use the `time` module functionalities to manage and manipulate the time throughout this simulation. **Problem Statement:** Given a sleep schedule defined by a dictionary, where keys are days of the week (from \'Monday\' to \'Sunday\') and values are tuples containing the start time and end time of sleep in `HH:MM` format, simulate the sleep schedule over a week. The function should return the total sleep time in hours for the week, accounting for Daylight Saving Time (DST) transitions and timezone changes if applicable. **Function Signature:** ```python def simulate_sleep_schedule(sleep_schedule: dict, timezone: str) -> float: pass ``` **Input:** - `sleep_schedule` (dict): A dictionary where keys are strings representing days of the week (`\'Monday\'`, `\'Tuesday\'`, etc.) and values are tuples of start and end times in `HH:MM` format. Example: ```python { \'Monday\': (\'22:00\', \'06:00\'), \'Tuesday\': (\'22:00\', \'06:00\'), \'Wednesday\': (\'22:00\', \'06:00\'), \'Thursday\': (\'22:00\', \'06:00\'), \'Friday\': (\'23:00\', \'07:00\'), \'Saturday\': (\'23:30\', \'08:00\'), \'Sunday\': (\'22:30\', \'06:30\') } ``` - `timezone` (str): A string representing the timezone (e.g., `\'US/Eastern\'`, `\'Europe/London\'`). **Output:** - (float): Total sleep time in hours for the entire week. **Constraints:** 1. Assume DST transitions occur according to the provided timezone. 2. Each day of the week must be accounted for in the `sleep_schedule`. 3. The function should handle irregular sleep patterns, where end time may be on the following day. **Hints:** - Use the `time.strptime()` function to parse the time strings. - Use the `time.mktime()` and `time.localtime()` functions to work with and manipulate time. - Use the `time.tzset()` function to handle timezone and DST changes automatically. - Account for the day transitions while calculating total sleep time. **Example:** ```python sleep_schedule = { \'Monday\': (\'22:00\', \'06:00\'), \'Tuesday\': (\'22:00\', \'06:00\'), \'Wednesday\': (\'22:00\', \'06:00\'), \'Thursday\': (\'22:00\', \'06:00\'), \'Friday\': (\'23:00\', \'07:00\'), \'Saturday\': (\'23:30\', \'08:00\'), \'Sunday\': (\'22:30\', \'06:30\') } timezone = \'US/Eastern\' total_sleep = simulate_sleep_schedule(sleep_schedule, timezone) print(total_sleep) # Expected output could vary based on DST transitions ``` **Notes:** - Ensure the function handles edge cases such as midnight transitions and DST changes correctly. - Consider using the `datetime` module for more convenient time arithmetic if necessary.","solution":"from datetime import datetime, timedelta, timezone import pytz def simulate_sleep_schedule(sleep_schedule: dict, timezone_str: str) -> float: tz = pytz.timezone(timezone_str) total_sleep = 0.0 for day, times in sleep_schedule.items(): sleep_start_str, sleep_end_str = times sleep_start = datetime.strptime(sleep_start_str, \'%H:%M\').time() sleep_end = datetime.strptime(sleep_end_str, \'%H:%M\').time() next_day = False if sleep_start > sleep_end: next_day = True date = datetime.now(tz) # Use current date to get the tz info start = datetime.combine(date, sleep_start).astimezone(tz) end = datetime.combine(date + timedelta(days=(1 if next_day else 0)), sleep_end).astimezone(tz) # Account for DST transitions by comparing tzinfos if next_day and start.tzinfo.dst(start) != end.tzinfo.dst(end): start = start + timedelta(days=(1 if next_day else 0)) total_sleep += (end - start).total_seconds() / 3600.0 return total_sleep"},{"question":"**Question: Implement a Function for Signal Noise Reduction using FFT** **Objective:** Write a function `reduce_noise(signal: torch.Tensor, cutoff_frequency: float) -> torch.Tensor` that reduces the noise from a given 1D signal using the Fast Fourier Transform (FFT). **Function Signature:** ```python import torch def reduce_noise(signal: torch.Tensor, cutoff_frequency: float) -> torch.Tensor: pass ``` **Input:** - `signal`: A 1D PyTorch tensor of real numbers representing the signal from which noise is to be reduced. - `cutoff_frequency`: A float value representing the cutoff frequency. Frequencies higher than this value should be considered noise and removed. **Output:** - A 1D PyTorch tensor of the same length as the input signal, representing the signal with reduced noise. **Constraints:** - The input tensor `signal` will have a minimum length of 2. - The `cutoff_frequency` will always be positive and less than half of the sampling rate. - Assume a fixed sampling rate for this task (e.g., 1 sample per unit time). **Requirements:** 1. Use the `torch.fft` module for FFT operations. 2. Perform a forward FFT on the input signal. 3. Apply a low-pass filter to the signal in the frequency domain with the specified cutoff frequency. 4. Perform an inverse FFT to convert the filtered signal back to the time domain. 5. Use helper functions such as `torch.fft.fftfreq` to generate frequency components. **Example:** ```python # Example usage: signal = torch.tensor([0.0, 1.0, 0.0, -1.0, 0.0, 1.0, 0.0, -1.0]) cutoff_frequency = 0.1 filtered_signal = reduce_noise(signal, cutoff_frequency) ``` In this example, the function should remove high-frequency components from the input signal to reduce noise. **Notes:** - Ensure the function handles both even and odd lengths of input signals. - Perform appropriate validation on the frequency components to ensure they are correctly filtered.","solution":"import torch def reduce_noise(signal: torch.Tensor, cutoff_frequency: float) -> torch.Tensor: Reduces the noise from a given 1D signal using Fast Fourier Transform (FFT). Args: - signal (torch.Tensor): A 1D tensor of real numbers representing the signal. - cutoff_frequency (float): The cutoff frequency to remove high frequency noise. Returns: - torch.Tensor: A 1D tensor representing the signal with reduced noise. # Perform the forward FFT to get the frequency domain representation fft_signal = torch.fft.fft(signal) # Create frequency bins freq_bins = torch.fft.fftfreq(signal.size(0)) # Apply the low-pass filter by zeroing out frequencies higher than the cutoff fft_signal[torch.abs(freq_bins) > cutoff_frequency] = 0 # Perform the inverse FFT to get the signal back to time domain filtered_signal = torch.fft.ifft(fft_signal) # Since \'ifft\' might introduce minor imaginary parts due to computational errors, # we take the real part to get the final filtered signal. return torch.real(filtered_signal)"},{"question":"**Question:** You are required to implement a function that performs several operations using PyTorch tensors. The function will take two input tensors and return a tensor that is the result of several operations. Specifically, you will: 1. Create a tensor of random integers between 0 and 10 with the same shape as the first input tensor. 2. Add the two input tensors. 3. Element-wise multiply the result from step 2 with the random tensor from step 1. 4. Calculate and return the mean of the final tensor from step 3. Here are the specifications of the function: # Function Signature ```python import torch def tensor_operations(input_tensor1: torch.Tensor, input_tensor2: torch.Tensor) -> torch.Tensor: # Your code here ``` # Inputs - `input_tensor1`: A PyTorch tensor of any shape, containing integer or floating-point numbers. - `input_tensor2`: A PyTorch tensor of the same shape as `input_tensor1`, containing integer or floating-point numbers. # Output - Returns a tensor which is the mean of the element-wise multiplied tensor. # Constraints - You should ensure that both input tensors have the same shape. If not, raise a ValueError with an appropriate message. - Use PyTorch\'s tensor methods to perform all operations. # Example Usage ```python import torch tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32) tensor2 = torch.tensor([[6, 5, 4], [3, 2, 1]], dtype=torch.float32) result = tensor_operations(tensor1, tensor2) print(result) # Example output: tensor(53.5000) ``` In this problem, focus on using appropriate PyTorch tensor operations and ensure that your implementation handles edge cases, such as the input tensors not having the same shape. **Notes:** - Do not use explicit loops; rely on PyTorch\'s vectorized operations for efficiency. - The use of `torch.randint` for random tensor creation is encouraged. - Ensure to handle tensor creation on GPU if the input tensors are on a GPU.","solution":"import torch def tensor_operations(input_tensor1: torch.Tensor, input_tensor2: torch.Tensor) -> torch.Tensor: Perform operations on two input tensors and return the mean of the result. Parameters: - input_tensor1: torch.Tensor - input_tensor2: torch.Tensor Returns: - torch.Tensor: Mean of the final element-wise multiplied tensor. # Check if both tensors have the same shape if input_tensor1.shape != input_tensor2.shape: raise ValueError(\\"Input tensors must have the same shape.\\") # Create a tensor of random integers between 0 and 10 with the same shape as input_tensor1 random_tensor = torch.randint(0, 10, input_tensor1.shape, device=input_tensor1.device) # Add the two input tensors added_tensor = input_tensor1 + input_tensor2 # Element-wise multiply the result with the random tensor multiplied_tensor = added_tensor * random_tensor # Calculate and return the mean of the final tensor mean_result = multiplied_tensor.mean() return mean_result"},{"question":"# Coding Challenge: Custom Persistent Object Pickling Background You are working on a Python project where you need to serialize data into a persistent storage solution. This data includes user-defined classes and references to objects stored in a database. You need to extend the pickling functionality to associate database-stored objects with unique IDs and ensure these associations persist properly across sessions. Problem Statement Design and implement a custom pickler and unpickler that: 1. Stores regular Python objects as usual. 2. Handles instances of a specific user-defined class (`UserRecord`) by storing a persistent reference to that object rather than the object\'s full data. Requirements 1. **UserRecord Class**: - This is a simple user-defined class with at least two attributes: `user_id` and `name`. 2. **CustomPickler Class**: - Should inherit from `pickle.Pickler`. - Must overload `persistent_id`. When encountering an instance of `UserRecord`, it should return a tuple containing `(\\"UserRecord\\", user_id)`. 3. **CustomUnpickler Class**: - Should inherit from `pickle.Unpickler`. - Must overload `persistent_load`. When encountering the tuple `(\\"UserRecord\\", user_id)`, it should return an instance of `UserRecord` by querying the database simulation method `fetch_user(user_id)`. 4. **Database Simulation**: - Implement a simple in-memory dictionary `database` to simulate the database. - Function `store_user(user)` should add the user to the dictionary. - Function `fetch_user(user_id)` should retrieve the user from the dictionary. Input 1. A list of `UserRecord` objects. 2. A list of regular Python objects (e.g., strings, floats). Output 1. The pickled byte stream of the input data. 2. The list of unpickled objects, demonstrating that the `UserRecord` objects are correctly referenced. Constraints - Ensure that instances of `UserRecord` can be correctly reconstructed during the unpickling process. - Use protocol 5 for pickling to demonstrate handling advanced features. Example ```python import pickle class UserRecord: def __init__(self, user_id, name): self.user_id = user_id self.name = name # Simulate a database database = {} def store_user(user): database[user.user_id] = user def fetch_user(user_id): return database.get(user_id) class CustomPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, UserRecord): return (\'UserRecord\', obj.user_id) else: return None class CustomUnpickler(pickle.Unpickler): def persistent_load(self, pid): pid_type, user_id = pid if pid_type == \'UserRecord\': return fetch_user(user_id) else: raise pickle.UnpicklingError(\\"Unknown persistent id: {}{}\\".format(pid_type, user_id)) # Sample usage def main(): users = [UserRecord(1, \'Alice\'), UserRecord(2, \'Bob\')] for user in users: store_user(user) data_to_pickle = [users, \\"Sample String\\", 42] # Pickling with open(\'data.pkl\', \'wb\') as pkl_file: cp = CustomPickler(pkl_file, protocol=5) cp.dump(data_to_pickle) # Unpickling with open(\'data.pkl\', \'rb\') as pkl_file: cu = CustomUnpickler(pkl_file) unpickled_data = cu.load() assert unpickled_data[0][0].name == \'Alice\' assert unpickled_data[1] == \\"Sample String\\" assert unpickled_data[2] == 42 print(\\"Unpickling successful and data integrity maintained!\\") if __name__ == \'__main__\': main() ``` Notes - Ensure the code is robust and includes error handling for edge cases. - Test the pickling and unpickling process thoroughly with different types of data.","solution":"import pickle class UserRecord: def __init__(self, user_id, name): self.user_id = user_id self.name = name # Simulate a database database = {} def store_user(user): database[user.user_id] = user def fetch_user(user_id): return database.get(user_id) class CustomPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, UserRecord): return (\'UserRecord\', obj.user_id) else: return None class CustomUnpickler(pickle.Unpickler): def persistent_load(self, pid): pid_type, user_id = pid if pid_type == \'UserRecord\': return fetch_user(user_id) else: raise pickle.UnpicklingError(\\"Unknown persistent id: {}{}\\".format(pid_type, user_id)) def pickle_data(data_to_pickle): with open(\'data.pkl\', \'wb\') as pkl_file: cp = CustomPickler(pkl_file, protocol=5) cp.dump(data_to_pickle) def unpickle_data(): with open(\'data.pkl\', \'rb\') as pkl_file: cu = CustomUnpickler(pkl_file) return cu.load()"},{"question":"# Byte Data Encoding and Decoding Challenge Objective The objective of this challenge is to test your understanding of Python\'s binary data services, specifically the \\"struct\\" and \\"codecs\\" modules. You will write a function that processes a given set of binary data, encodes it using a specified encoding, and then decodes it back to verify the integrity of the data. Problem Statement You are given a set of binary data that needs to be transformed through multiple stages: 1. Pack the given data using the \\"struct\\" module with specified format characters. 2. Encode the packed binary data using a specified encoding format. 3. Decode the encoded data back to its original form using the same encoding format. 4. Unpack the decoded data back to its initial components using the \\"struct\\" module. You need to implement the following function: ```python import struct import codecs def process_binary_data(data: tuple, struct_format: str, encoding: str) -> tuple: Processes the given binary data by packing, encoding, decoding, and unpacking it. Parameters: data (tuple): The input data to be packed and processed. It can contain integers and floats. struct_format (str): The format string for packing the data using the struct module. encoding (str): The encoding format to use for encoding and decoding the binary data. Returns: tuple: The unpacked data after encoding and decoding stages. Raises: ValueError: If encoding or decoding fails due to incorrect format or data. pass # Your implementation here ``` Input - `data` is a tuple containing the items to be packed. These can be integers (`int`) and floats (`float`). - `struct_format` is a string specifying the format to be used in the `struct` module for packing the data. - `encoding` is a string specifying the encoding to be used for transforming the binary data. Output - Return a tuple representing the unpacked data after encoding and decoding process. Constraints - The `struct_format` string will be valid for the given `data`. - The `encoding` string will be a supported encoding format from the `codecs` module. - Handle any potential encoding or decoding errors gracefully and raise a `ValueError` with an appropriate message if they occur. Example ```python # Given data data = (1, 2.5, 3) struct_format = \\"i f i\\" encoding = \\"utf-8\\" try: result = process_binary_data(data, struct_format, encoding) print(result) except ValueError as e: print(str(e)) ``` Expected Output: ``` (1, 2.5, 3) ``` Notes - Make sure to follow proper packing and unpacking techniques using the `struct` module. - Ensure that the encoding and subsequent decoding steps maintain the integrity of the binary data.","solution":"import struct import codecs def process_binary_data(data: tuple, struct_format: str, encoding: str) -> tuple: Processes the given binary data by packing, encoding, decoding, and unpacking it. Parameters: data (tuple): The input data to be packed and processed. It can contain integers and floats. struct_format (str): The format string for packing the data using the struct module. encoding (str): The encoding format to use for encoding and decoding the binary data. Returns: tuple: The unpacked data after encoding and decoding stages. Raises: ValueError: If encoding or decoding fails due to incorrect format or data. try: # Pack the data using struct packed_data = struct.pack(struct_format, *data) # Encode the packed binary data to the specified encoding encoded_data = codecs.encode(packed_data, encoding) # Decode the encoded data back to binary decoded_data = codecs.decode(encoded_data, encoding) # Unpack the decoded binary data unpacked_data = struct.unpack(struct_format, decoded_data) return unpacked_data except (struct.error, LookupError, TypeError, ValueError) as e: raise ValueError(f\\"Error processing binary data: {e}\\")"},{"question":"**Question: Analyzing and Visualizing Flight Data with Seaborn** You are provided with a dataset containing information about the number of airline passengers who flew each month from 1949 to 1960. Your task is to analyze this dataset and create various visualizations using Seaborn. Follow the steps below to complete the task: 1. **Load the dataset**: - Load the `flights` dataset using Seaborn\'s `load_dataset` function and display the first few rows. 2. **Convert the dataset to wide-form**: - Convert the long-form `flights` dataset to a wide-form dataset where each column represents a month, and each row represents the number of passengers in that month for different years. 3. **Create a line plot using long-form data**: - Using the original long-form dataset, create a line plot with `year` on the x-axis, `passengers` on the y-axis, and different lines for each month. Use the `relplot` function. 4. **Create a line plot using wide-form data**: - Using the wide-form dataset, create a similar line plot where each line represents a different month. Ensure that the x-axis represents the year and the y-axis represents the number of passengers. 5. **Advanced Visualization**: - Create a faceted grid of line plots where each facet represents a month. Use the Seaborn `FacetGrid` function combined with a suitable plotting function to achieve this. # Input Format - No input data is required to be provided by the user. # Expected Output - The first 5 rows of the loaded dataset. - The first 5 rows of the wide-form dataset. - A line plot using the long-form `flights` dataset. - A line plot using the wide-form `flights` dataset. - A faceted grid of line plots by month. # Constraints - Use Seaborn and Pandas libraries for data manipulation and visualization. - Ensure that all plots have appropriate titles and axis labels for clarity. # Example Code ```python import seaborn as sns import pandas as pd # Step 1: Load the dataset flights = sns.load_dataset(\\"flights\\") print(flights.head()) # Step 2: Convert to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") print(flights_wide.head()) # Step 3: Line plot using long-form data sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") # Step 4: Line plot using wide-form data sns.relplot(data=flights_wide, kind=\\"line\\") # Step 5: Faceted grid of line plots by month g = sns.FacetGrid(flights, col=\\"month\\", col_wrap=3, height=3) g.map(sns.lineplot, \\"year\\", \\"passengers\\") g.add_titles(\\"Month: {col_name}\\") g.set_axis_labels(\\"Year\\", \\"Passengers\\") ``` By completing these steps, you should be able to demonstrate a comprehensive understanding of data manipulation and visualization using Seaborn.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Step 1: Load the dataset def load_flights_data(): flights = sns.load_dataset(\\"flights\\") return flights # Step 2: Convert to wide-form def convert_to_wide_form(flights): flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") return flights_wide # Step 3: Line plot using long-form data def plot_long_form_data(flights): sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") plt.title(\\"Number of Passengers Over Time (Long-Form)\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Passengers\\") plt.show() # Step 4: Line plot using wide-form data def plot_wide_form_data(flights_wide): flights_wide.plot() plt.legend(title=\\"Month\\", bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.title(\\"Number of Passengers Over Time (Wide-Form)\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Passengers\\") plt.show() # Step 5: Faceted grid of line plots by month def plot_faceted_grid(flights): g = sns.FacetGrid(flights, col=\\"month\\", col_wrap=3, height=3, aspect=1.2) g.map(sns.lineplot, \\"year\\", \\"passengers\\") g.set_titles(\\"Month: {col_name}\\") g.set_axis_labels(\\"Year\\", \\"Passengers\\") plt.show()"},{"question":"Coding Assessment Question # Objective Write a Python script using scikit-learn to demonstrate your understanding of various linear models. This will include loading a dataset, preprocessing the data, fitting multiple linear models, and evaluating their performance. # Question 1. **Dataset:** Use the `diabetes` dataset from scikit-learn, which is a standard dataset used for regression tasks. 2. **Preprocessing:** - Standardize the feature matrix. - Split the dataset into a training set (80%) and a testing set (20%). 3. **Model Implementation:** - Fit the following models on the training data: 1. Ordinary Least Squares Linear Regression 2. Ridge Regression (with α = 1.0) 3. Lasso Regression (with α = 0.1) 4. ElasticNet (with α = 1.0 and l1_ratio = 0.5) 4. **Model Evaluation:** - For each model, evaluate their performance on the testing set using Mean Squared Error (MSE). - Display the coefficients and intercept for each model. - Compare and discuss the MSE values obtained from each model. # Constraints - Ensure reproducibility by setting a random seed before splitting the data. - Use the default settings for scikit-learn’s `LinearRegression`, `Ridge`, `Lasso`, and `ElasticNet` classes unless specified otherwise. # Expected Output 1. **Standardized Feature Matrix:** Display the mean and standard deviation of the standardized features. 2. **Performance Metrics:** Print the MSE for each model. 3. **Model Coefficients and Intercepts:** Print the model coefficients and intercepts. # Performance Requirements - The script should execute efficiently and handle standard computational resources. # Sample Code Structure ```python import numpy as np from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error # Load the dataset data = load_diabetes() X, y = data.data, data.target # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Fit models models = { \'Linear Regression\': LinearRegression(), \'Ridge Regression\': Ridge(alpha=1.0), \'Lasso Regression\': Lasso(alpha=0.1), \'ElasticNet\': ElasticNet(alpha=1.0, l1_ratio=0.5) } # Evaluate models for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\\"{name} - MSE: {mse}\\") print(f\\"Coefficients: {model.coef_}\\") print(f\\"Intercept: {model.intercept_}\\") print(\\"n\\") ```","solution":"import numpy as np from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error def linear_models_performance(): # Load the dataset data = load_diabetes() X, y = data.data, data.target # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Display the mean and standard deviation of the standardized features print(f\\"Feature mean: {np.mean(X_scaled, axis=0)}\\") print(f\\"Feature standard deviation: {np.std(X_scaled, axis=0)}\\") # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Fit models models = { \'Linear Regression\': LinearRegression(), \'Ridge Regression\': Ridge(alpha=1.0), \'Lasso Regression\': Lasso(alpha=0.1), \'ElasticNet\': ElasticNet(alpha=1.0, l1_ratio=0.5) } # Evaluate models results = {} for name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) results[name] = { \'mse\': mse, \'coefficients\': model.coef_, \'intercept\': model.intercept_ } print(f\\"{name} - MSE: {mse}\\") print(f\\"Coefficients: {model.coef_}\\") print(f\\"Intercept: {model.intercept_}\\") print(\\"n\\") return results"},{"question":"# Pandas Assessment Task **Objective:** Demonstrate your ability to use indexing and selection methods in pandas DataFrame to manipulate and analyze data. **Question:** Given a sample DataFrame `df` of student records with the following columns: - `student_id`: Unique identifier for each student - `name`: Name of the student - `age`: Age of the student - `grade`: Grade of the student - `score`: Test score of the student - `passed`: Boolean indicating whether the student passed the test (True/False) ```python import pandas as pd import numpy as np data = { \'student_id\': [1, 2, 3, 4, 5, 6], \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\', \'Frank\'], \'age\': [24, 22, 23, 24, 22, 21], \'grade\': [\'A\', \'B\', \'A\', \'C\', \'B\', \'B\'], \'score\': [85, 78, 90, 60, 70, 88], \'passed\': [True, True, True, False, False, True] } df = pd.DataFrame(data) ``` Using the provided DataFrame `df`, perform the following tasks: 1. **Basic Indexing & Slicing**: - Select the `name` column as a Series. - Select the `name` and `score` columns. - Select the first three rows of the DataFrame. 2. **Label-Based Indexing with `.loc`**: - Select all rows where the `grade` is \'B\' and return the `student_id` and `name` columns. - Update the `score` to 95 for all students who have a grade \'A\'. 3. **Position-Based Indexing with `.iloc`**: - Select the last two rows of the DataFrame. - Select the entry in the third row and second column. 4. **Boolean Indexing**: - Select all students who passed and have a score greater than 80. - Create a new column `status` which is \'Excellent\' if the `score` is above 85, \'Good\' if between 70 and 85, and \'Needs Improvement\' otherwise. **Constraints:** - Do not use any loops; the solution should fully utilize pandas indexing and boolean indexing capabilities. **Expected Output**: - The result of each task should be printed for verification. ```python # Example output for one task print(selected_students) ``` Ensure that your code is well-commented to explain each step and the methods you are using.","solution":"import pandas as pd import numpy as np data = { \'student_id\': [1, 2, 3, 4, 5, 6], \'name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\', \'Frank\'], \'age\': [24, 22, 23, 24, 22, 21], \'grade\': [\'A\', \'B\', \'A\', \'C\', \'B\', \'B\'], \'score\': [85, 78, 90, 60, 70, 88], \'passed\': [True, True, True, False, False, True] } df = pd.DataFrame(data) # Task 1: Basic Indexing & Slicing # Select the `name` column as a Series name_series = df[\'name\'] # Select the `name` and `score` columns name_score_df = df[[\'name\', \'score\']] # Select the first three rows of the DataFrame first_three_rows = df[:3] # Task 2: Label-Based Indexing with `.loc` # Select all rows where the `grade` is \'B\' and return the `student_id` and `name` columns grade_b_df = df.loc[df[\'grade\'] == \'B\', [\'student_id\', \'name\']] # Update the `score` to 95 for all students who have a grade \'A\' df.loc[df[\'grade\'] == \'A\', \'score\'] = 95 # Task 3: Position-Based Indexing with `.iloc` # Select the last two rows of the DataFrame last_two_rows = df.iloc[-2:] # Select the entry in the third row and second column third_row_second_column = df.iloc[2, 1] # Task 4: Boolean Indexing # Select all students who passed and have a score greater than 80 passed_and_high_score_df = df[(df[\'passed\'] == True) & (df[\'score\'] > 80)] # Create a new column `status` df[\'status\'] = np.where( df[\'score\'] > 85, \'Excellent\', np.where(df[\'score\'].between(70, 85), \'Good\', \'Needs Improvement\') ) # Print the results for verification print(\\"Name Series:n\\", name_series) print(\\"nName and Score DataFrame:n\\", name_score_df) print(\\"nFirst Three Rows:n\\", first_three_rows) print(\\"nGrade \'B\' Students:n\\", grade_b_df) print(\\"nDataFrame after updating scores for Grade \'A\' students:n\\", df) print(\\"nLast Two Rows:n\\", last_two_rows) print(\\"nEntry in Third Row, Second Column:n\\", third_row_second_column) print(\\"nStudents who passed and have high scores:n\\", passed_and_high_score_df) print(\\"nDataFrame with new \'status\' column:n\\", df) # Encapsulating the above operations into functions for testing def get_name_series(df): return df[\'name\'] def get_name_and_score(df): return df[[\'name\', \'score\']] def get_first_three_rows(df): return df[:3] def get_students_with_grade_b(df): return df.loc[df[\'grade\'] == \'B\', [\'student_id\', \'name\']] def update_scores_for_grade_a(df): df.loc[df[\'grade\'] == \'A\', \'score\'] = 95 return df def get_last_two_rows(df): return df.iloc[-2:] def get_third_row_second_column(df): return df.iloc[2, 1] def get_passed_and_high_score(df): return df[(df[\'passed\'] == True) & (df[\'score\'] > 80)] def add_status_column(df): df[\'status\'] = np.where( df[\'score\'] > 85, \'Excellent\', np.where(df[\'score\'].between(70, 85), \'Good\', \'Needs Improvement\') ) return df"},{"question":"Objective: Implement a Python function `manage_user_preferences` to manage user preferences using the `dbm` module. The function should: 1. Create a new database if it doesn\'t exist. 2. Add or update user preferences. 3. Retrieve and print user preferences. 4. Handle errors gracefully. Details: 1. **Function Signature**: ```python def manage_user_preferences(db_name: str, operations: list) -> None: ``` 2. **Input**: - `db_name` (str): The name of the database file. - `operations` (list): A list of operations to be performed on the database. Each operation is a tuple where: - The first element is a string indicating the type of operation (`\'set\'`, `\'get\'`, or `\'delete\'`). - The second element is the key (`str`) for the `\'set\'`, `\'get\'`, or `\'delete\'` operation. - For `\'set\'` operations, the third element is the value (`str`) to be stored. 3. **Output**: - For `\'get\'` operations, print the retrieved value or `\'Key not found\'` if the key doesn\'t exist. - For `\'set\'` operations, print a success message indicating the value has been set. - For `\'delete\'` operations, print a success message indicating the key has been deleted or `\'Key not found\'` if the key doesn\'t exist. 4. **Constraints**: - The function should use the `dbm.dumb` backend for compatibility reasons. - Handle and print appropriate error messages for database-related errors. Example: ```python operations = [ (\'set\', \'theme\', \'dark\'), (\'set\', \'font\', \'Arial\'), (\'get\', \'theme\'), (\'delete\', \'font\'), (\'get\', \'font\'), ] manage_user_preferences(\'preferences\', operations) ``` Expected Output: ``` Set theme to dark Set font to Arial theme: dark Deleted font font: Key not found ``` Implementation Hints: - Use the `dbm.open()` function with the appropriate flag to create and manage the database. - Convert keys and values to bytes before storing them in the database. - Utilize exception handling to manage and report errors gracefully. Good luck!","solution":"import dbm def manage_user_preferences(db_name: str, operations: list) -> None: try: with dbm.open(db_name, \'c\') as db: for op in operations: if op[0] == \'set\': try: db[op[1]] = op[2] print(f\\"Set {op[1]} to {op[2]}\\") except Exception as e: print(f\\"Error setting {op[1]}: {e}\\") elif op[0] == \'get\': try: value = db[op[1]].decode(\'utf-8\') print(f\\"{op[1]}: {value}\\") except KeyError: print(f\\"{op[1]}: Key not found\\") except Exception as e: print(f\\"Error getting {op[1]}: {e}\\") elif op[0] == \'delete\': try: del db[op[1]] print(f\\"Deleted {op[1]}\\") except KeyError: print(f\\"{op[1]}: Key not found\\") except Exception as e: print(f\\"Error deleting {op[1]}: {e}\\") except Exception as e: print(f\\"Error opening database: {e}\\")"},{"question":"# Advanced Python Coding Challenge: Shared Memory Image Processing **Objective:** Implement a Python program that uses the `multiprocessing.shared_memory` module to perform concurrent image processing tasks. Your program should demonstrate the use of shared memory to share data between multiple processes efficiently. **Problem Statement:** You are given a grayscale image represented as a NumPy array with pixel values ranging from 0 to 255. Your task is to divide the image into two halves and process each half concurrently using separate processes. One process should increase the brightness by 50 units for its half, and the other should decrease the brightness by 50 units for its half. Utilize shared memory to store the image array and ensure the modifications are reflected in the original shared memory. **Requirements:** 1. **Function Definitions**: - Write a function `process_image_chunk(shared_name, x_start, x_end, operation)` that: - Attaches to the shared memory using `shared_name`. - Processes the image chunk from `x_start` to `x_end` columns. - Increases or decreases the pixel values by 50 based on the `operation` parameter (`\'increase\'` or `\'decrease\'`). - Ensures pixel values are clamped between 0 and 255 after modification. 2. **Main Execution Flow**: - Create shared memory for the image array. - Initialize the shared memory with the image data. - Spawn two processes to handle each half of the image using `process_image_chunk`. - Ensure both processes complete execution. - Retrieve the modified image from shared memory. - Clean up the shared memory resources. **Input Format**: - A 2D NumPy array representing the grayscale image. **Output Format**: - The modified 2D NumPy array after processing. **Constraints**: - Pixel values must be clamped between 0 and 255. - Image dimensions are assumed to be even for simplification. **Example**: Suppose the image array is: ```python import numpy as np image = np.array([ [100, 100, 100, 100], [150, 150, 150, 150] ]) ``` After processing, the output should be: ```python array([ [150, 150, 50, 50], [200, 200, 100, 100] ]) ``` **Hint**: Use `multiprocessing.shared_memory.SharedMemory` for shared memory and `memoryview` for buffer manipulation. **Notes**: - Ensure you handle exceptions and edge cases, such as invalid operations or memory errors. - Properly manage shared memory lifecycle to avoid resource leaks.","solution":"import numpy as np from multiprocessing import Process, shared_memory def process_image_chunk(shared_name, shape, x_start, x_end, operation): Process a specified chunk of the image by either increasing or decreasing brightness. existing_shm = shared_memory.SharedMemory(name=shared_name) shared_array = np.ndarray(shape, dtype=np.uint8, buffer=existing_shm.buf) if operation == \'increase\': shared_array[:, x_start:x_end] = np.clip(shared_array[:, x_start:x_end] + 50, 0, 255) elif operation == \'decrease\': shared_array[:, x_start:x_end] = np.clip(shared_array[:, x_start:x_end] - 50, 0, 255) existing_shm.close() def main(image): Divide the image into two halves and process each half with separate processes. shape = image.shape shm = shared_memory.SharedMemory(create=True, size=image.nbytes) shared_array = np.ndarray(shape, dtype=np.uint8, buffer=shm.buf) np.copyto(shared_array, image) mid = shape[1] // 2 process1 = Process(target=process_image_chunk, args=(shm.name, shape, 0, mid, \'increase\')) process2 = Process(target=process_image_chunk, args=(shm.name, shape, mid, shape[1], \'decrease\')) process1.start() process2.start() process1.join() process2.join() result = np.array(shared_array) shm.close() shm.unlink() return result"},{"question":"# Advanced `asyncio` Usage and Debugging Objective In this exercise, you will create a Python script that uses the `asyncio` library to run multiple asynchronous tasks. You must ensure that your script includes proper error handling, logging, and the ability to run blocking code without hindering performance. Additionally, you will practice enabling and interpreting debug mode to catch common mistakes. Requirements 1. **Tasks Overview**: - Create a coroutine `compute_square` that calculates the square of a given number after a short delay. - Create a coroutine `fetch_data` which simulates fetching data by sleeping asynchronously for a given duration and then returning a string message. - Create a coroutine `process_data` that: - Uses the `run_in_executor` method to run a blocking (CPU-bound) task: summing a large list of random integers. - Logs the duration taken to perform this task. - Create a coroutine `main` that schedules and awaits all the above tasks concurrently. 2. **Concurrency and Multithreading**: - Ensure that all tasks are scheduled in the event loop properly. - Use the methods `call_soon_threadsafe` and `run_coroutine_threadsafe` to demonstrate thread-safe scheduling. 3. **Debug Mode**: - Enable asyncio’s debug mode by setting an environment variable or using `asyncio.run` with the debug parameter. - Configure logging to `DEBUG` level for the `asyncio` logger. - Add code to the `main` coroutine that deliberately triggers common mistakes such as: - Not awaiting a coroutine. - Creating a task and not handling its exception. 4. **Error Handling and Logging**: - Use appropriate logging methods to log important events and exceptions. - Handle potential exceptions in your coroutines using try-except blocks to ensure that all errors are logged. - Ensure the script does not terminate abruptly due to unhandled exceptions. Input - No input is required from the user. All numbers and durations can be hardcoded. Output - The script should log: - Messages from the completed tasks (`compute_square` and `fetch_data`). - Duration of the CPU-bound task in `process_data`. - Any warnings or errors captured via the debug mode. - Any exceptions raised by the tasks. Example Script Outline ```python import asyncio import concurrent.futures import logging import os import random import time logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") os.environ[\'PYTHONASYNCIODEBUG\'] = \'1\' async def compute_square(number: int) -> int: await asyncio.sleep(1) # Simulating I/O operation return number * number async def fetch_data(duration: int) -> str: await asyncio.sleep(duration) return f\\"Fetched data after {duration} seconds\\" async def process_data(): loop = asyncio.get_running_loop() large_list = [random.randint(1, 100) for _ in range(10**6)] start_time = time.time() def blocking_task(): return sum(large_list) with concurrent.futures.ThreadPoolExecutor() as pool: result = await loop.run_in_executor(pool, blocking_task) duration = time.time() - start_time logger.debug(f\\"Processed data in {duration:.2f} seconds\\") return result async def main(): square_task = compute_square(5) data_task = fetch_data(2) process_task = process_data() try: square_result = await square_task data_result = await data_task process_result = await process_task logger.debug(f\\"Square Result: {square_result}\\") logger.debug(f\\"Data Result: {data_result}\\") logger.debug(f\\"Process Result: {process_result}\\") # Deliberate common mistakes for debug mode compute_square(10) # Not awaited asyncio.create_task(fetch_data(1)) # No result handling except Exception as e: logger.exception(\\"An error occurred: %s\\", e) if __name__ == \\"__main__\\": asyncio.run(main(), debug=True) ``` Constraints - Use Python 3.10.+ - Ensure to catch and log all exceptions to prevent crashing. - Avoid blocking the event loop with CPU-bound tasks. Submit your script in a `.py` file format.","solution":"import asyncio import concurrent.futures import logging import os import random import time # Enable asyncio debug mode os.environ[\'PYTHONASYNCIODEBUG\'] = \'1\' logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") async def compute_square(number: int) -> int: await asyncio.sleep(1) # Simulating I/O operation return number * number async def fetch_data(duration: int) -> str: await asyncio.sleep(duration) return f\\"Fetched data after {duration} seconds\\" async def process_data(): loop = asyncio.get_running_loop() large_list = [random.randint(1, 100) for _ in range(10**6)] start_time = time.time() def blocking_task(): return sum(large_list) with concurrent.futures.ThreadPoolExecutor() as pool: result = await loop.run_in_executor(pool, blocking_task) duration = time.time() - start_time logger.debug(f\\"Processed data in {duration:.2f} seconds\\") return result async def main(): square_task = compute_square(5) data_task = fetch_data(2) process_task = process_data() try: square_result = await square_task data_result = await data_task process_result = await process_task logger.debug(f\\"Square Result: {square_result}\\") logger.debug(f\\"Data Result: {data_result}\\") logger.debug(f\\"Process Result: {process_result}\\") # Deliberate common mistakes for debug mode compute_square(10) # Not awaited asyncio.create_task(fetch_data(1)) # No result handling except Exception as e: logger.exception(\\"An error occurred: %s\\", e) if __name__ == \\"__main__\\": asyncio.run(main(), debug=True)"},{"question":"<|Analysis Begin|> The provided document is a comprehensive reference for TorchScript, a statically typed subset of the Python language used in PyTorch. It describes the type system and various TorchScript features, including: - Meta Types, such as `Any` - Primitive Types, such as `int` and `float` - Structural Types, such as `Tuple` and `List` - Nominal Types, such as user-defined classes and built-in classes like `torch.Tensor` The document also covers topics like type annotations, expressions, statements, and function definitions within the TorchScript context. It provides details on the data types, operations, and constructs supported, highlighting the differences from standard Python. Given the extensive and detailed nature of the document, it allows us to craft a question that tests various aspects of TorchScript, including type annotations, custom classes, and how these are scripted and used in PyTorch models. <|Analysis End|> <|Question Begin|> **Problem Statement: Advanced TorchScript Model with Custom Type and Type Annotations** You are tasked to demonstrate your understanding of TorchScript by implementing a custom neural network model that utilizes TorchScript\'s type annotation, custom class capability, and scripting functionality. Your task is to implement the following: 1. A custom TorchScript class `DataWrapper` that wraps around a `torch.Tensor`. 2. A neural network class `CustomModel` that uses `DataWrapper` instances within its methods. 3. A function `train_model` to perform a simple forward pass through the model. **Requirements:** 1. **Custom Class `DataWrapper`**: - This class should: - Initialize with a `torch.Tensor` as a parameter and store it as an instance attribute. - Have an instance method `get_data` that returns the wrapped tensor. 2. **Neural Network Class `CustomModel`**: - This class should: - Inherit from `torch.nn.Module`. - Contain a single `torch.nn.Linear` layer. - Accept `DataWrapper` instances as inputs to its forward method. - Utilize TorchScript annotations for type safety. - Be scriptable with `torch.jit.script`. 3. **Training Function `train_model`**: - This function should: - Accept a `CustomModel` instance, a tensor input for forwarding through the model, and a simple ground-truth tensor for loss calculation. - Perform a forward pass through the model. - Compute the mean squared error loss using `torch.nn.functional.mse_loss`. - Print the loss value. **Implementation Details:** 1. `DataWrapper` class should be decorated with `@torch.jit.script`. 2. `CustomModel` class should: - Define its types and attributes in `__init__`. - Use TorchScript type annotations properly. 3. `train_model` function does not need to be scripted but must demonstrate the utilities working together. **Input:** - Tensor of appropriate shape for the linear layer (e.g., `[batch_size, input_features]`). - Ground-truth tensor of the same shape as model output. **Output:** - Print the loss value from the training function `train_model`. **Example:** ```python import torch @torch.jit.script class DataWrapper: def __init__(self, data: torch.Tensor): self.data = data def get_data(self) -> torch.Tensor: return self.data class CustomModel(torch.nn.Module): def __init__(self, input_features: int, output_features: int): super(CustomModel, self).__init__() self.linear = torch.nn.Linear(input_features, output_features) def forward(self, x: DataWrapper) -> torch.Tensor: return self.linear(x.get_data()) def train_model(model: CustomModel, x: torch.Tensor, y: torch.Tensor): wrapper_x = DataWrapper(x) output = model(wrapper_x) loss = torch.nn.functional.mse_loss(output, y) print(f\\"Loss: {loss.item()}\\") # Example test input_features = 5 output_features = 1 model = CustomModel(input_features, output_features) model = torch.jit.script(model) # Script the model # Dummy data x = torch.randn(10, input_features) y = torch.randn(10, output_features) # Train model train_model(model, x, y) ``` Your task is to complete the `DataWrapper` and `CustomModel` classes and ensure that they work as described. **Note**: Make sure that `CustomModel` is scriptable using TorchScript (`@torch.jit.script`).","solution":"import torch @torch.jit.script class DataWrapper: def __init__(self, data: torch.Tensor): self.data = data def get_data(self) -> torch.Tensor: return self.data class CustomModel(torch.nn.Module): def __init__(self, input_features: int, output_features: int): super(CustomModel, self).__init__() self.linear = torch.nn.Linear(input_features, output_features) def forward(self, x: DataWrapper) -> torch.Tensor: return self.linear(x.get_data()) def train_model(model: CustomModel, x: torch.Tensor, y: torch.Tensor): wrapper_x = DataWrapper(x) output = model(wrapper_x) loss = torch.nn.functional.mse_loss(output, y) print(f\\"Loss: {loss.item()}\\")"},{"question":"Objective: Write a Python function to read, process, and write data using the `csv` and `configparser` modules. This task will assess your understanding of file operations and configuration parsing in Python. Problem Statement: You are given two files: 1. `config.ini`: A configuration file that specifies the format and behavior for processing a CSV file. 2. `data.csv`: A CSV file with unspecified data. You need to implement a function `process_csv_based_on_config(config_file: str, csv_file: str, output_file: str) -> None` that reads the configuration settings from the `config.ini` file and then processes the `data.csv` file accordingly, outputting the results to `output_file`. Expected Input: 1. `config_file (str)`: The path to the `config.ini` file. 2. `csv_file (str)`: The path to the `data.csv` file. 3. `output_file (str)`: The path where the processed CSV data should be saved. Expected Output: - The function does not return any values. The processed CSV data should be written to `output_file`. Configuration File Structure: The `config.ini` file will be structured as follows: ```ini [CSV] delimiter = , quotechar = \\" has_header = True ``` - `delimiter`: Specifies the delimiter for the CSV file. - `quotechar`: Specifies the character used to quote fields. - `has_header`: Specifies whether the CSV file contains a header row (`True` or `False`). Example Configuration File: ```ini [CSV] delimiter = ; quotechar = \\" has_header = False ``` Function Requirements: 1. Read the configuration file using the `configparser` module. 2. Use the parsed configuration to read the `data.csv` file using the `csv` module. 3. Process the CSV data (for this exercise, simply read and write it with the same content but according to configuration). 4. Write the processed data to `output_file` using the specified delimiter and quote character. Constraints: - You can assume the `config.ini` file always contains the `[CSV]` section with valid entries for `delimiter`, `quotechar`, and `has_header`. - You need to handle CSV files with or without a header row appropriately based on the configuration. Example: Consider the following configuration and CSV input: ```ini # config.ini [CSV] delimiter = ; quotechar = \\" has_header = True ``` ```csv # data.csv \\"name\\";\\"age\\";\\"city\\" \\"John Doe\\";30;\\"New York\\" \\"Jane Smith\\";25;\\"Los Angeles\\" ``` When processed, the `output_file` should be as follows: ```csv \\"name\\";\\"age\\";\\"city\\" \\"John Doe\\";30;\\"New York\\" \\"Jane Smith\\";25;\\"Los Angeles\\" ``` **Write your function below:** ```python import csv import configparser def process_csv_based_on_config(config_file: str, csv_file: str, output_file: str) -> None: # Parse the configuration file config = configparser.ConfigParser() config.read(config_file) delimiter = config[\'CSV\'][\'delimiter\'] quotechar = config[\'CSV\'][\'quotechar\'] has_header = config[\'CSV\'].getboolean(\'has_header\') # Read the CSV file with open(csv_file, mode=\'r\', newline=\'\') as infile: csv_reader = csv.reader(infile, delimiter=delimiter, quotechar=quotechar) rows = list(csv_reader) # Write the processed CSV data to the output file with open(output_file, mode=\'w\', newline=\'\') as outfile: csv_writer = csv.writer(outfile, delimiter=delimiter, quotechar=quotechar, quoting=csv.QUOTE_MINIMAL) if has_header: csv_writer.writerow(rows[0]) csv_writer.writerows(rows[1:]) else: csv_writer.writerows(rows) # Example usage # process_csv_based_on_config(\'config.ini\', \'data.csv\', \'output.csv\') ```","solution":"import csv import configparser def process_csv_based_on_config(config_file: str, csv_file: str, output_file: str) -> None: # Parse the configuration file config = configparser.ConfigParser() config.read(config_file) delimiter = config[\'CSV\'][\'delimiter\'] quotechar = config[\'CSV\'][\'quotechar\'] has_header = config[\'CSV\'].getboolean(\'has_header\') # Read the CSV file with open(csv_file, mode=\'r\', newline=\'\') as infile: csv_reader = csv.reader(infile, delimiter=delimiter, quotechar=quotechar) rows = list(csv_reader) # Write the processed CSV data to the output file with open(output_file, mode=\'w\', newline=\'\') as outfile: csv_writer = csv.writer(outfile, delimiter=delimiter, quotechar=quotechar, quoting=csv.QUOTE_MINIMAL) if has_header: csv_writer.writerow(rows[0]) csv_writer.writerows(rows[1:]) else: csv_writer.writerows(rows)"},{"question":"# Question: Asynchronous Task Manager You are required to implement an asynchronous task manager using Python\'s `asyncio` module. The task manager should be able to: 1. Schedule and execute multiple coroutines concurrently. 2. Handle task completion, cancellation, and timeouts. 3. Provide introspection functionalities to monitor the status of running tasks. Requirements: 1. **Function Implementation**: - Implement a function `async def task_manager(coros: List[coroutine], timeout: Optional[float] = None) -> Tuple[List[Any], List[Any]]` that: - Accepts a list of coroutine functions `coros` and an optional `timeout` parameter. - Runs all coroutines concurrently and returns their results. - Cancels tasks that exceed the given timeout. - Returns a tuple containing two lists: - The first list contains the results of successfully completed tasks. - The second list contains the results of the cancelled or failed tasks. 2. **Constraints**: - Each coroutine may include a simulated delay using `asyncio.sleep`. - Use `asyncio.create_task()` to schedule coroutines as tasks. - Use `asyncio.wait()` or `asyncio.gather()` to manage concurrent execution. - Use `asyncio.shield()` if needed to prevent some tasks from being cancelled. - Handle any exceptions raised by the coroutines and include them in the second list. 3. **Input and Output**: - Input: `coros` - A list of coroutine functions to be executed. `timeout` - An optional float specifying the maximum time to wait for task completion. - Output: A tuple containing two lists: one with successful results and another with exceptions or cancelled tasks. 4. **Example Usage**: ```python import asyncio async def sample_coro(n): await asyncio.sleep(n) return f\\"Result after {n} seconds\\" coros = [sample_coro(1), sample_coro(2), sample_coro(3)] results, cancelled = await task_manager(coros, timeout=2) print(\\"Successful results:\\", results) print(\\"Cancelled or failed:\\", cancelled) ``` # Deadline You have 48 hours to complete this task. Please ensure that your solution meets all the requirements and handles edge cases. Good luck!","solution":"import asyncio from typing import List, Tuple, Optional, Any async def task_manager(coros: List[asyncio.coroutine], timeout: Optional[float] = None) -> Tuple[List[Any], List[Any]]: Runs a list of coroutines concurrently and handles task completion, cancellation, and timeouts. Parameters: - coros: List of coroutine functions to be executed. - timeout: Optional float specifying the maximum time to wait for task completion. Returns: A tuple containing two lists: - The first list contains the results of successfully completed tasks. - The second list contains the results of the cancelled or failed tasks (exceptions). tasks = [asyncio.create_task(coro) for coro in coros] done, pending = await asyncio.wait(tasks, timeout=timeout) successful_results = [] cancelled_or_failed = [] for task in done: if task.exception(): cancelled_or_failed.append(task.exception()) else: successful_results.append(task.result()) for task in pending: task.cancel() try: await task except asyncio.CancelledError as e: cancelled_or_failed.append(str(e)) return successful_results, cancelled_or_failed"},{"question":"XML-RPC Client Implementation Objective Implement a Python client that communicates with a given XML-RPC server using the `xmlrpc.client` module. The client will perform the following tasks: 1. Make method calls to the server to perform arithmetic operations. 2. Handle and display custom data types (e.g., DateTime). 3. Handle binary data transfer. 4. Manage and handle errors occurring during communication. Instructions 1. **Setup**: - You are given a server running at `http://localhost:8000` with the following methods: - `add(x, y)`: Returns the sum of `x` and `y`. - `subtract(x, y)`: Returns the difference between `x` and `y`. - `today()`: Returns the current date as a `DateTime` object. - `get_image()`: Returns a small binary image file as a `Binary` object. 2. **Client Implementation**: - Create a class `XMLRPCClient` encapsulating the client logic. - The initializer should take the server URL and create a `ServerProxy` object. - Implement methods `add`, `subtract`, `get_today`, `fetch_image` in the class to call the respective server methods. 3. **Error Handling**: - Implement proper error handling using `Fault` and `ProtocolError` for each of the methods in the `XMLRPCClient`. 4. **Output**: - The `add` and `subtract` methods should print the result. - The `get_today` method should print the current date in the format `DD.MM.YYYY, HH:MM`. - The `fetch_image` method should save the image data to a file named `fetched_image.jpg`. 5. **Example Usage**: - Create an instance of `XMLRPCClient`. - Execute the `add`, `subtract`, `get_today`, and `fetch_image` methods, handling any errors encountered. Constraints - Ensure that your client works correctly with the predefined methods on the server. - Handle errors gracefully and print meaningful messages for both `Fault` and `ProtocolError`. Example Code Structure ```python import xmlrpc.client from datetime import datetime class XMLRPCClient: def __init__(self, url): self.proxy = xmlrpc.client.ServerProxy(url) def add(self, x, y): try: result = self.proxy.add(x, y) print(f\\"Result of add({x}, {y}): {result}\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err.faultString} (Code: {err.faultCode})\\") except xmlrpc.client.ProtocolError as err: print(f\\"Protocol Error: {err.errmsg} (Code: {err.errcode}) from URL: {err.url}\\") def subtract(self, x, y): try: result = self.proxy.subtract(x, y) print(f\\"Result of subtract({x}, {y}): {result}\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err.faultString} (Code: {err.faultCode})\\") except xmlrpc.client.ProtocolError as err: print(f\\"Protocol Error: {err.errmsg} (Code: {err.errcode}) from URL: {err.url}\\") def get_today(self): try: today = self.proxy.today() converted = datetime.strptime(today.value, \\"%Y%m%dT%H:%M:%S\\") print(f\\"Today: {converted.strftime(\'%d.%m.%Y, %H:%M\')}\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err.faultString} (Code: {err.faultCode})\\") except xmlrpc.client.ProtocolError as err: print(f\\"Protocol Error: {err.errmsg} (Code: {err.errcode}) from URL: {err.url}\\") def fetch_image(self): try: binary = self.proxy.get_image() with open(\\"fetched_image.jpg\\", \\"wb\\") as handle: handle.write(binary.data) print(\\"Image saved as fetched_image.jpg\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err.faultString} (Code: {err.faultCode})\\") except xmlrpc.client.ProtocolError as err: print(f\\"Protocol Error: {err.errmsg} (Code: {err.errcode}) from URL: {err.url}\\") # Example usage client = XMLRPCClient(\\"http://localhost:8000\\") client.add(5, 3) client.subtract(10, 2) client.get_today() client.fetch_image() ``` Notes - Assume methods `add`, `subtract`, `today`, and `get_image` are already implemented on the server. - The client should handle edge cases, such as network errors or method call errors, and print appropriate error messages.","solution":"import xmlrpc.client from datetime import datetime class XMLRPCClient: def __init__(self, url): self.proxy = xmlrpc.client.ServerProxy(url) def add(self, x, y): try: result = self.proxy.add(x, y) print(f\\"Result of add({x}, {y}): {result}\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err.faultString} (Code: {err.faultCode})\\") except xmlrpc.client.ProtocolError as err: print(f\\"Protocol Error: {err.errmsg} (Code: {err.errcode}) from URL: {err.url}\\") def subtract(self, x, y): try: result = self.proxy.subtract(x, y) print(f\\"Result of subtract({x}, {y}): {result}\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err.faultString} (Code: {err.faultCode})\\") except xmlrpc.client.ProtocolError as err: print(f\\"Protocol Error: {err.errmsg} (Code: {err.errcode}) from URL: {err.url}\\") def get_today(self): try: today = self.proxy.today() converted = datetime.strptime(today.value, \\"%Y%m%dT%H:%M:%S\\") print(f\\"Today: {converted.strftime(\'%d.%m.%Y, %H:%M\')}\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err.faultString} (Code: {err.faultCode})\\") except xmlrpc.client.ProtocolError as err: print(f\\"Protocol Error: {err.errmsg} (Code: {err.errcode}) from URL: {err.url}\\") def fetch_image(self): try: binary = self.proxy.get_image() with open(\\"fetched_image.jpg\\", \\"wb\\") as handle: handle.write(binary.data) print(\\"Image saved as fetched_image.jpg\\") except xmlrpc.client.Fault as err: print(f\\"XML-RPC Fault: {err.faultString} (Code: {err.faultCode})\\") except xmlrpc.client.ProtocolError as err: print(f\\"Protocol Error: {err.errmsg} (Code: {err.errcode}) from URL: {err.url}\\") # Example usage if __name__ == \\"__main__\\": client = XMLRPCClient(\\"http://localhost:8000\\") client.add(5, 3) client.subtract(10, 2) client.get_today() client.fetch_image()"},{"question":"# Python Version Extraction You are tasked with writing a function that extracts and constructs the detailed version information from a given encoded version integer (`PY_VERSION_HEX`). The function should adhere to the CPython versioning method as described in the provided documentation. Function Signature ```python def parse_py_version(hex_version: int) -> str: pass ``` Input - `hex_version`: A 32-bit integer representing the encoded Python version (e.g., `0x030401a2`). Output - A string constructed in the form of `\\"major.minor.micro[release_level][release_serial]\\"`. - `release_level` is represented as `a`, `b`, `rc`, or `final` for alpha, beta, release candidate, and final releases, respectively. - `release_serial` is appended directly after the `release_level`, only if `release_level` is not `\'final\'`. Constraints - The function should handle any valid 32-bit version encoding. - Assume that release levels other than alpha, beta, release candidate, and final do not exist. - Performance should be efficient but does not need to be optimized for large data sets. Example ```python assert parse_py_version(0x030401a2) == \\"3.4.1a2\\" assert parse_py_version(0x030a00f0) == \\"3.10.0\\" assert parse_py_version(0x030902b3) == \\"3.9.2rc3\\" ``` Notes - Recall the bit positions for each version component as stated in the documentation. - Implement bitwise operations to decode the input `hex_version` accordingly. Good luck!","solution":"def parse_py_version(hex_version: int) -> str: Parses a 32-bit integer encoded Python version and returns the version string. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF release_level_code = (hex_version >> 4) & 0xF release_serial = hex_version & 0xF release_level_mapping = { 0xA: \'a\', # Alpha 0xB: \'b\', # Beta 0xC: \'rc\', # Release Candidate 0xF: \'\' # Final } release_level = release_level_mapping.get(release_level_code, \'\') if release_level_code == 0xF: return f\\"{major}.{minor}.{micro}\\" else: return f\\"{major}.{minor}.{micro}{release_level}{release_serial}\\""},{"question":"Objective: Implement a Python class that manually manages reference counting of its instances with the foundational principles provided in the Python 3.10 reference counting documentation. Your class should demonstrate an understanding of manual reference counting using the principles provided. Requirements: 1. **Class Definition**: - Define a class `RefCountedObject` that manages its reference count. 2. **Initializer (`__init__` method)**: - This method should initialize the object and set its reference count to 1. 3. **Method: `incref(self)`**: - This method should increase the reference count of the object by 1. 4. **Method: `decref(self)`**: - This method should decrease the reference count of the object by 1. - If the reference count drops to 0, print a message indicating the object is being deallocated. 5. **Usage**: - Create instances of `RefCountedObject` and demonstrate correct incref and decref operations. - Ensure that objects are properly deallocated when their reference count reaches 0. Input and Output: - **Input**: No direct input is required from the user, you will create instances and manipulate them within the script. - **Output**: Messages indicating the object creation, reference count changes, and deallocation. Constraints: - The focus is on correct implementation of manual reference counting. - All operations related to reference counting should be encapsulated within the class methods. - No additional libraries should be used. Example: ```python class RefCountedObject: def __init__(self): self.ref_count = 1 print(\\"Object created, ref count =\\", self.ref_count) def incref(self): self.ref_count += 1 print(\\"Reference increased, ref count =\\", self.ref_count) def decref(self): self.ref_count -= 1 print(\\"Reference decreased, ref count =\\", self.ref_count) if self.ref_count == 0: print(\\"Object deallocated\\") # Example usage obj = RefCountedObject() obj.incref() # Increase ref count obj.decref() # Decrease ref count obj.decref() # Should trigger deallocation ``` In the provided example, you should demonstrate creating an instance of `RefCountedObject`, increasing its reference count, and then decreasing it to the point where it should be deallocated.","solution":"class RefCountedObject: def __init__(self): self.ref_count = 1 print(f\\"Object created, ref count = {self.ref_count}\\") def incref(self): self.ref_count += 1 print(f\\"Reference increased, ref count = {self.ref_count}\\") def decref(self): self.ref_count -= 1 print(f\\"Reference decreased, ref count = {self.ref_count}\\") if self.ref_count == 0: print(\\"Object deallocated\\")"},{"question":"**Objective**: Assess students\' understanding of CUDA stream synchronization and error detection in PyTorch. # Problem Statement: You are provided with a PyTorch script that intentionally introduces a synchronization error between CUDA streams. Your task is to: 1. Execute the script and observe the error reported by the CUDA Stream Sanitizer. 2. Identify the data race condition from the sanitizer\'s output. 3. Fix the synchronization error using appropriate CUDA stream synchronization techniques. 4. Validate that your fix successfully resolves the error by re-running the script with the sanitizer enabled and ensuring no errors are reported. # Script with Synchronization Error: ```python import torch def faulty_script(): a = torch.rand(4, 2, device=\\"cuda\\") with torch.cuda.stream(torch.cuda.Stream()): torch.mul(a, 5, out=a) if __name__ == \\"__main__\\": faulty_script() ``` # Steps to Complete: 1. **Run the provided script**: Execute the script with the CUDA Stream Sanitizer enabled. You can do this by setting the environment variable `TORCH_CUDA_SANITIZER=1` and running the script. Observe and understand the error message. 2. **Identify the Data Race**: Describe the data race condition detected by the sanitizer. Specify which streams and operations are involved. 3. **Fix the Error**: Modify the script to synchronize the CUDA streams properly, ensuring there are no concurrent accesses without proper synchronization. 4. **Validate the Fix**: Re-run the modified script with the sanitizer enabled and verify that no errors are reported. # Submission: 1. **Error Description**: A brief description of the error reported by the CUDA Stream Sanitizer, including the streams and operations involved. 2. **Modified Script**: Provide the corrected script with proper synchronization to fix the data race condition. 3. **Execution Confirmation**: A statement confirming that the modified script was executed with the sanitizer enabled and no errors were reported. # Constraints: - You must only use the provided PyTorch and CUDA functions. - Ensure that your fix enforces proper synchronization between CUDA streams. # Example: An example of how your submission should look: 1. **Error Description**: The sanitizer reported a data race condition involving tensor `a`. The tensor was accessed by the default stream during the `torch.rand` operation and by a new stream during the `torch.mul` operation. 2. **Modified Script**: ```python import torch def fixed_script(): a = torch.rand(4, 2, device=\\"cuda\\") with torch.cuda.stream(torch.cuda.Stream()): torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) torch.mul(a, 5, out=a) if __name__ == \\"__main__\\": fixed_script() ``` 3. **Execution Confirmation**: \\"The modified script was executed with `TORCH_CUDA_SANITIZER=1` and no errors were reported.\\"","solution":"import torch def fixed_script(): a = torch.rand(4, 2, device=\\"cuda\\") new_stream = torch.cuda.Stream() with torch.cuda.stream(new_stream): # Ensure synchronization between the default stream and the new stream new_stream.wait_stream(torch.cuda.default_stream()) torch.mul(a, 5, out=a) torch.cuda.synchronize() # Synchronize the default stream to ensure all operations are complete if __name__ == \\"__main__\\": fixed_script()"},{"question":"# Advanced Python Programming: MIME Configuration Background Mailcap files are used to configure how MIME-aware applications (like email clients and web browsers) handle files with various MIME types. A Mailcap file typically contains lines specifying a MIME type and a command to execute for that type. For example, the line: ``` video/mpeg; xmpeg %s ``` indicates that files with the MIME type `video/mpeg` should be handled using the `xmpeg` program, where `%s` is replaced by the filename. Python\'s deprecated `mailcap` module handles the parsing and management of these Mailcap files, providing functions such as `getcaps()` to read Mailcap files into a dictionary format and `findmatch()` to find appropriate commands based on a MIME type. Task You are required to implement a simplified version of these functionalities. Specifically: - Implement the function `read_mailcap_files(filenames: List[str]) -> Dict[str, List[Dict[str, str]]]` to read multiple mailcap files and return a dictionary mapping MIME types to lists of mailcap entries. - Implement the function `find_best_match(caps: Dict[str, List[Dict[str, str]]], mime_type: str, key: str = \'view\', filename: str = \'\') -> Tuple[Optional[str], Optional[Dict[str, str]]]` to find the best match for a given MIME type and return the command to execute and the corresponding entry. Specifications 1. **Function: `read_mailcap_files`** - **Input**: - `filenames`: List of file paths to Mailcap files. - **Output**: - A dictionary mapping MIME types (as strings) to a list of dictionaries. Each dictionary represents an entry with keys such as \'view\', \'edit\', etc. - **Details**: - Each file line should be parsed such that the MIME type is matched to the commands. - Ignore lines that are comments (start with `#`) or empty. 2. **Function: `find_best_match`** - **Input**: - `caps`: Dictionary returned by `read_mailcap_files`. - `mime_type`: The MIME type to find a match for. - `key`: (Optional) Defaults to `\'view\'`. Specifies the type of action to consider (e.g., \'edit\', \'compose\'). - `filename`: (Optional) Defaults to an empty string. If provided, replace `%s` in the command with this filename. - **Output**: - A tuple `(command, entry)` where `command` is the command string (or `None` if no match) and `entry` is the matching dictionary entry (or `None` if no match). - **Details**: - If there are multiple entries, prefer user-specific entries over system-wide entries. - Handle `%s` substitution with the provided `filename`. - Skip entries containing disallowed characters in the filename. Constraints - You can assume the filenames provided to `read_mailcap_files` are valid and the files are properly formatted. - Consider shell metacharacters handling as described in the `findmatch` function to avoid potential security issues. Example ```python from typing import List, Dict, Tuple, Optional def read_mailcap_files(filenames: List[str]) -> Dict[str, List[Dict[str, str]]]: # Your implementation goes here pass def find_best_match(caps: Dict[str, List[Dict[str, str]]], mime_type: str, key: str = \'view\', filename: str = \'\') -> Tuple[Optional[str], Optional[Dict[str, str]]]: # Your implementation goes here pass # Example Usage mailcap_files = [\'/path/to/system/mailcap\', \'/home/user/.mailcap\'] caps = read_mailcap_files(mailcap_files) command, entry = find_best_match(caps, \'video/mpeg\', filename=\'movie.mpg\') print(command, entry) ``` Constraints: - Add appropriate error handling. - Pay attention to performance and security issues as noted. Good luck!","solution":"from typing import List, Dict, Tuple, Optional def read_mailcap_files(filenames: List[str]) -> Dict[str, List[Dict[str, str]]]: caps = {} for filename in filenames: try: with open(filename, \'r\') as file: for line in file: line = line.strip() if not line or line.startswith(\'#\'): continue parts = line.split(\';\', 1) if len(parts) != 2: continue mime_type, view_command = parts[0].strip(), parts[1].strip() entry = {\'view\': view_command} if mime_type in caps: caps[mime_type].append(entry) else: caps[mime_type] = [entry] except Exception as e: # Handle errors like file not found or read issues print(f\\"Error reading {filename}: {e}\\") continue return caps def find_best_match(caps: Dict[str, List[Dict[str, str]]], mime_type: str, key: str = \'view\', filename: str = \'\') -> Tuple[Optional[str], Optional[Dict[str, str]]]: if mime_type not in caps: return None, None best_match = None for entry in caps[mime_type]: if key in entry: command = entry[key] if filename: command = command.replace(\'%s\', filename) if any(c in filename for c in (\';\', \'|\', \'&\')): # Skip entries with disallowed characters in the filename continue best_match = (command, entry) break return best_match if best_match else (None, None)"},{"question":"# Question: **Objective:** Demonstrate your understanding of the `shutil` module and its advanced file handling capabilities. **Task:** You are given a directory structure that contains various files and subdirectories. Your task is to implement a function, `organize_files`, which performs the following operations: 1. Recursively copies all files from a source directory to a destination directory. 2. Organizes all the copied files at the root of the destination directory, regardless of their original structure. 3. Logs any errors encountered during the file operations to a specified log file. 4. Handles symbolic links appropriately as per specification. **Function Signature:** ```python def organize_files(src: str, dst: str, log_file: str, follow_symlinks: bool = True) -> None: pass ``` **Parameters:** - `src` (str): Path to the source directory. - `dst` (str): Path to the destination directory. - `log_file` (str): Path to the log file where errors should be recorded. - `follow_symlinks` (bool): If `True`, follows symbolic links while copying. If `False`, copies the symbolic links themselves. **Constraints:** - If `dst` does not exist, it should be created. - Existing files in `dst` should be overwritten. - The function should be robust and handle various edge cases such as permissions errors, and files with the same name in different subdirectories. **Example:** ```python # Directory structure of src # src/ # ├── dir1/ # │ ├── file1.txt # │ ├── file2.log # ├── dir2/ # │ └── file3.txt # ├── link_to_file1 -> dir1/file1.txt (symbolic link) # Execution organize_files(\\"src\\", \\"dst\\", \\"logfile.txt\\", follow_symlinks=True) # Resulting directory structure of dst # dst/ # ├── file1.txt # ├── file2.log # ├── file3.txt ``` In this example, the symbolic link `link_to_file1` is followed and the file `file1.txt` it points to is copied instead (since `follow_symlinks=True`). If `follow_symlinks=False`, a symbolic link `link_to_file1` would be created in the `dst` directory pointing to the original target. **Notes:** - Make use of `shutil` functions such as `copytree`, `copy`, `copy2`, `rmtree`, and other relevant file operations where needed. - Ensure your implementation is efficient and handles large directories gracefully. - Log all exceptions in the provided log file in a human-readable format. **Additional Hint:** - Consider using the `shutil.copytree` function with a custom ignore function or other shutil utility functions to achieve the desired functionality.","solution":"import os import shutil import logging def organize_files(src: str, dst: str, log_file: str, follow_symlinks: bool = True) -> None: Recursively copies all files from a source directory to a destination directory, organizing them at the root of the destination directory, regardless of their original structure. Logs any errors encountered during the operation. Params: - src: Path to the source directory. - dst: Path to the destination directory. - log_file: Path to the log file where errors should be recorded. - follow_symlinks: If True, follows symbolic links while copying. logging.basicConfig(filename=log_file, level=logging.ERROR) if not os.path.exists(dst): os.makedirs(dst) try: for root, _, files in os.walk(src, followlinks=follow_symlinks): for file in files: src_file = os.path.join(root, file) dst_file = os.path.join(dst, file) try: if not follow_symlinks and os.path.islink(src_file): linkto = os.readlink(src_file) os.symlink(linkto, dst_file) else: shutil.copy2(src_file, dst_file) except Exception as e: logging.error(f\\"Error copying {src_file} to {dst_file}: {str(e)}\\") except Exception as e: logging.error(f\\"Error walking directory {src}: {str(e)}\\")"},{"question":"**Objective:** Implement a function that utilizes the `shelve` module to manage a small database of student records. Each record should consist of a student\'s name, age, and grades. You will need to support operations to add, retrieve, update, and delete records. **Function Signature:** ```python def manage_student_records(filename: str, operation: str, key: str=None, value: dict=None) -> dict: pass ``` **Parameters:** - `filename` (str): Name of the file where the database will be stored. - `operation` (str): The operation to perform. It can be one of them: - `\'add\'`: Adds a new student record. Requires `key` and `value`. - `\'get\'`: Retrieves a student record. Requires `key`. - `\'update\'`: Updates an existing student record. Requires `key` and `value`. - `\'delete\'`: Deletes a student record. Requires `key`. - `key` (str): The key representing a student (typically their ID). Required for `\'add\'`, `\'get\'`, `\'update\'`, and `\'delete\'` operations. - `value` (dict): The value of the student record consisting of `\'name\'`, `\'age\'`, and `\'grades\'`. Required for `\'add\'` and `\'update\'` operations. **Returns:** - For `\'get\'` operation, returns the student record as a dictionary. - For all other operations, returns the new state of the database (a dictionary of all records). **Constraints:** - The `key` must be a string. - The `value` (if provided) must be a dictionary with the following structure: ```python { \'name\': str, \'age\': int, \'grades\': list } ``` **Examples:** ```python # Example usage filename = \\"students_db\\" # Add a student record manage_student_records(filename, \'add\', \'123\', {\'name\': \'John Doe\', \'age\': 21, \'grades\': [88, 92, 76]}) # Retrieve a student record print(manage_student_records(filename, \'get\', \'123\')) # Output: {\'name\': \'John Doe\', \'age\': 21, \'grades\': [88, 92, 76]} # Update a student record manage_student_records(filename, \'update\', \'123\', {\'name\': \'John Doe\', \'age\': 22, \'grades\': [88, 92, 76, 90]}) # Delete a student record manage_student_records(filename, \'delete\', \'123\') ``` **Requirements:** - Use the `shelve` module to handle the database persistence. - Ensure that proper error handling is in place for attempts to retrieve, update, or delete non-existent records. - Ensure the shelf is properly closed using context management to avoid data corruption. **Note:** - Handling of `writeback` parameter should be left to defaults in this implementation. - Be mindful of memory and performance considerations mentioned in the documentation when working with large datasets.","solution":"import shelve def manage_student_records(filename: str, operation: str, key: str=None, value: dict=None) -> dict: with shelve.open(filename) as db: if operation == \'add\': if key in db: raise KeyError(f\\"Record with key {key} already exists.\\") db[key] = value elif operation == \'get\': if key not in db: raise KeyError(f\\"No record found for key {key}.\\") return db[key] elif operation == \'update\': if key not in db: raise KeyError(f\\"No record found for key {key}.\\") db[key] = value elif operation == \'delete\': if key not in db: raise KeyError(f\\"No record found for key {key}.\\") del db[key] else: raise ValueError(\\"Invalid operation. Supported operations are \'add\', \'get\', \'update\', and \'delete\'.\\") return {k: db[k] for k in db}"},{"question":"Pandas Copy-on-Write (CoW) Objectives: 1. Demonstrate understanding of the Copy-on-Write (CoW) rules in pandas. 2. Implement functions that manipulate pandas DataFrames while adhering to CoW behavior. 3. Avoid common pitfalls associated with chained assignments and in-place modifications. Problem Statement You are provided with a DataFrame `df` containing sales data for a company. This DataFrame has the following columns: - `\'Date\'`: The date of the sale (datetime). - `\'Product\'`: The name of the product sold (string). - `\'UnitsSold\'`: The number of units sold (integer). - `\'Revenue\'`: The revenue generated from the sale (float). Example DataFrame: ```python import pandas as pd data = { \'Date\': pd.to_datetime([\'2023-01-01\', \'2023-01-02\', \'2023-01-03\']), \'Product\': [\'A\', \'B\', \'A\'], \'UnitsSold\': [100, 150, 120], \'Revenue\': [1000.0, 1500.0, 1800.0] } df = pd.DataFrame(data) ``` This DataFrame might be derived from other operations or might be used to generate subsets. # Task 1. **Fix Chained Assignment (10 points):** Implement a function `update_units_sold(df, product, new_units_sold)` that updates the `\'UnitsSold\'` column for a particular product across all dates without using chained assignments. The function should return a DataFrame updated according to the CoW rules. 2. **Avoid Inplace Modification (10 points):** Implement a function `apply_discount(df, discount_percentage)` that reduces the `\'Revenue\'` by a given discount percentage for all rows, without any inplace modifications. The function should return the updated DataFrame. 3. **Verify CoW Behavior (10 points):** Implement a function `verify_cow_behavior()` that demonstrates the CoW principle using an example: creating a DataFrame, performing an operation that triggers a copy, and showing that the original DataFrame remains unchanged. This function should print the original and modified DataFrames to showcase the difference. # Function Signatures ```python import pandas as pd def update_units_sold(df: pd.DataFrame, product: str, new_units_sold: int) -> pd.DataFrame: Update the \'UnitsSold\' column for a particular product across all dates. Args: df (pd.DataFrame): The sales DataFrame. product (str): The product to update. new_units_sold (int): The new units sold value. Returns: pd.DataFrame: The updated DataFrame. # Your code here def apply_discount(df: pd.DataFrame, discount_percentage: float) -> pd.DataFrame: Apply a discount to the \'Revenue\' column by a given percentage. Args: df (pd.DataFrame): The sales DataFrame. discount_percentage (float): The discount percentage to apply (e.g., 10 for 10%). Returns: pd.DataFrame: The DataFrame with updated revenue. # Your code here def verify_cow_behavior() -> None: Demonstrate CoW by performing an operation that shows copying behavior. This function should create a DataFrame, perform an operation that triggers a copy, and print the original and modified DataFrames to demonstrate the CoW principle. Returns: None # Your code here ``` # Constraints 1. You must not use inplace operations like `inplace=True`. 2. Use explicit assignment (`loc` or `iloc`) for updates instead of chained assignments. 3. Your solution should be compatible with pandas 3.0 or higher. Submit your solution in a Python script, ensuring all functions are properly defined and meet the above requirements.","solution":"import pandas as pd def update_units_sold(df: pd.DataFrame, product: str, new_units_sold: int) -> pd.DataFrame: Update the \'UnitsSold\' column for a particular product across all dates. Args: df (pd.DataFrame): The sales DataFrame. product (str): The product to update. new_units_sold (int): The new units sold value. Returns: pd.DataFrame: The updated DataFrame. updated_df = df.copy(deep=True) updated_df.loc[updated_df[\'Product\'] == product, \'UnitsSold\'] = new_units_sold return updated_df def apply_discount(df: pd.DataFrame, discount_percentage: float) -> pd.DataFrame: Apply a discount to the \'Revenue\' column by a given percentage. Args: df (pd.DataFrame): The sales DataFrame. discount_percentage (float): The discount percentage to apply (e.g., 10 for 10%). Returns: pd.DataFrame: The DataFrame with updated revenue. discount_factor = 1 - (discount_percentage / 100) updated_df = df.copy(deep=True) updated_df[\'Revenue\'] = updated_df[\'Revenue\'] * discount_factor return updated_df def verify_cow_behavior() -> None: Demonstrate CoW by performing an operation that shows copying behavior. This function should create a DataFrame, perform an operation that triggers a copy, and print the original and modified DataFrames to demonstrate the CoW principle. Returns: None original_data = { \'Date\': pd.to_datetime([\'2023-01-01\', \'2023-01-02\', \'2023-01-03\']), \'Product\': [\'A\', \'B\', \'A\'], \'UnitsSold\': [100, 150, 120], \'Revenue\': [1000.0, 1500.0, 1800.0] } original_df = pd.DataFrame(original_data) modified_df = original_df.copy(deep=True) modified_df.loc[modified_df[\'Product\'] == \'A\', \'UnitsSold\'] = 200 print(\\"Original DataFrame:\\") print(original_df) print(\\"nModified DataFrame:\\") print(modified_df)"},{"question":"Implement a custom protocol using the `asyncio` framework which simulates a simple key-value store server that can handle multiple clients concurrently. Use only low-level APIs (transports and protocols, not high-level functions) to create asynchronous networking operations. # Requirements: 1. **Protocol Class - KeyValueStoreProtocol**: * Implement the protocol methods to handle connections and data reception. * Parse incoming commands and respond accordingly. * Supported commands: - `SET key value`: Store the value with the specified key. - `GET key`: Retrieve the value for the specified key. - `DELETE key`: Remove the specified key. * Ensure each command returns a relevant response to the client. * Handle connection initialization and teardown properly. 2. **Server Implementation**: * Set up the server to listen on `127.0.0.1` and port `9000`. * Accept multiple incoming client connections, maintaining a separate protocol instance for each connection. * Persist the key-value data in-memory for the lifetime of the server. 3. **Client Demostration**: * Implement a demonstration function to show communication with the server using the following command sequence: - `SET name Python` - `GET name` - Output the response. # Input Format: - Commands will be sent as strings via TCP from the client. # Output Format: - Responses will be strings sent back to the client, corresponding to the commands received. # Example Interaction: 1. Start the server. 2. Connect with a client and send: - `SET name Python` - `GET name` 3. Server responses: ```python Response to `SET name Python`: \\"OK\\" Response to `GET name`: \\"Python\\" ``` # Constraints: - Minimum Python version 3.7. # Implementation Constraints: - Must use low-level APIs like `loop.create_server()`, `Transport`, and `Protocol`. - Should handle asynchronous communication correctly. ```python import asyncio class KeyValueStoreProtocol(asyncio.Protocol): def __init__(self): self.transport = None self.store = {} def connection_made(self, transport): self.transport = transport print(f\\"Connection made: {transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") response = self.handle_command(message) self.transport.write(response.encode()) self.transport.close() # Close after response for this simple protocol def handle_command(self, command): parts = command.strip().split(\' \') if len(parts) < 2: return \\"ERROR\\" cmd, key = parts[0], parts[1] if cmd == \'SET\': if len(parts) < 3: return \\"ERROR\\" self.store[key] = \' \'.join(parts[2:]) return \\"OK\\" elif cmd == \'GET\': return self.store.get(key, \\"NOT FOUND\\") elif cmd == \'DELETE\': if key in self.store: del self.store[key] return \\"OK\\" else: return \\"NOT FOUND\\" else: return \\"ERROR\\" async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: KeyValueStoreProtocol(), \'127.0.0.1\', 9000) print(\\"Server started on 127.0.0.1:9000\\") async with server: await server.serve_forever() # Run the server asyncio.run(main()) ```","solution":"import asyncio class KeyValueStoreProtocol(asyncio.Protocol): def __init__(self): self.transport = None self.store = {} def connection_made(self, transport): self.transport = transport print(f\\"Connection made: {transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode().strip() print(f\\"Data received: {message}\\") response = self.handle_command(message) self.transport.write(response.encode() + b\'n\') def handle_command(self, command): parts = command.strip().split(\' \') if len(parts) < 2: return \\"ERROR\\" cmd, key = parts[0], parts[1] if cmd == \'SET\': if len(parts) < 3: return \\"ERROR\\" self.store[key] = \' \'.join(parts[2:]) return \\"OK\\" elif cmd == \'GET\': return self.store.get(key, \\"NOT FOUND\\") elif cmd == \'DELETE\': if key in self.store: del self.store[key] return \\"OK\\" else: return \\"NOT FOUND\\" else: return \\"ERROR\\" async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: KeyValueStoreProtocol(), \'127.0.0.1\', 9000) print(\\"Server started on 127.0.0.1:9000\\") async with server: await server.serve_forever() # To run the server, uncomment the line below. # asyncio.run(main())"},{"question":"**Objective**: Demonstrate your ability to use the `pwd` module in Python to retrieve and manipulate Unix user account information. **Problem Statement**: You are provided with access to the Unix password database through the `pwd` module. Your task is to write a function `get_users_by_shell(shell: str) -> list` that returns a list of usernames who use a specific shell. Additionally, write a function `common_group_users() -> dict` that returns a dictionary where each key is a group ID (gid) and the value is a list of usernames belonging to that group. Function 1: `get_users_by_shell(shell: str) -> list` - **Input**: A string `shell` representing the command interpreter (e.g., \\"/bin/bash\\"). - **Output**: A list of usernames (strings) who use the specified shell. The list should be sorted alphabetically. - **Constraints**: If no user uses the specified shell, return an empty list. Function 2: `common_group_users() -> dict` - **Output**: A dictionary where each key is a group ID (integer) and the value is a sorted list of usernames (strings) who belong to that group. - **Constraints**: If no users are present for a group, that group ID should not be included in the dictionary. **Examples**: 1. `get_users_by_shell(\\"/bin/bash\\")` might return `[\\"alice\\", \\"bob\\", \\"eve\\"]`. 2. `common_group_users()` might return `{1000: [\\"alice\\", \\"bob\\"], 1001: [\\"eve\\", \\"mallory\\"], 1002: [\\"trudy\\"]}`. Additional Information - You should use the `pwd.getpwall()` function to get the list of all available password database entries. - Each entry in the list from `pwd.getpwall()` will be a tuple-like object with the attributes `pw_name`, `pw_passwd`, `pw_uid`, `pw_gid`, `pw_gecos`, `pw_dir`, and `pw_shell`. Implementation ```python import pwd def get_users_by_shell(shell: str) -> list: # Implement your solution here. pass def common_group_users() -> dict: # Implement your solution here. pass ``` **Notes**: - Ensure your code is efficient and handles edge cases, such as no users found for a given shell or group. - Make sure that your returned values (lists and dictionaries) are sorted as specified.","solution":"import pwd from collections import defaultdict def get_users_by_shell(shell: str) -> list: Returns a list of usernames who use the specified shell. :param shell: A string representing the shell (e.g., \\"/bin/bash\\"). :return: A sorted list of usernames who use the specified shell. users = [user.pw_name for user in pwd.getpwall() if user.pw_shell == shell] return sorted(users) def common_group_users() -> dict: Returns a dictionary where each key is a group ID and the value is a sorted list of usernames belonging to that group. :return: A dictionary with group IDs as keys and sorted lists of usernames as values. group_users = defaultdict(list) for user in pwd.getpwall(): group_users[user.pw_gid].append(user.pw_name) # Sort user lists for each group and convert defaultdict to a regular dict return {gid: sorted(users) for gid, users in group_users.items()}"},{"question":"Objective: Leverage seaborn\'s `seaborn.objects` module to generate and customize dot plots. This will test your understanding of initializing plots, adding dot markers, customizing visual properties, and handling overlapping points. Problem Statement: Given a dataset `mpg` with columns such as `\\"horsepower\\"`, `\\"mpg\\"`, `\\"origin\\"`, and `\\"weight\\"`, perform the following tasks: 1. **Create a scatter plot**: Initialize a scatter plot with `horsepower` on the x-axis and `mpg` on the y-axis. 2. **Customization with colors**: Customize the plot so that the dots\' color is determined by the `origin` column. 3. **Advanced Customization**: - Change the fill color of the dots based on the `weight` column. - Set fill transparency to 50%. - Scale the fill color using a binary color map. 4. **Mixed Markers**: Modify the plot to use different markers based on the `origin` column. Utilize both filled and unfilled markers. 5. **Add Jitter**: Add jitter to the plot to visualize the density of overlapping points. Set the jitter width to 0.25. Input: The dataset `mpg` (available through seaborn\'s `load_dataset` function). Constraints: 1. Ensure the code runs without any errors. 2. Use the `seaborn.objects` module only. 3. Adhere to the instructions for customizing colors, markers, and jitter. Output: A Seaborn plot displaying dots with the required customizations. Sample Code for Load Dataset: ```python import seaborn.objects as so from seaborn import load_dataset mpg = load_dataset(\\"mpg\\") ``` Function Signature: ```python def customized_dot_plot(): # Load the dataset import seaborn.objects as so from seaborn import load_dataset mpg = load_dataset(\\"mpg\\") # Task 1: Initialize the plot p = so.Plot(mpg, \\"horsepower\\", \\"mpg\\") # Task 2: Add dots with color customization p.add(so.Dots(), color=\\"origin\\") # Task 3: Advanced customization with fill color and transparency p.add(so.Dots(fillalpha=0.5), color=\\"origin\\", fillcolor=\\"weight\\").scale(fillcolor=\\"binary\\") # Task 4: Mixed markers based on the origin p.add(so.Dots(stroke=1), marker=\\"origin\\").scale(marker=[\\"o\\", \\"x\\", (6, 2, 1)]) # Task 5: Add jitter to handle overlapping points p.add(so.Dots(), so.Jitter(0.25)) # Display the plot p.show() ``` Complete the function to generate the plot as described.","solution":"def customized_dot_plot(): # Load the dataset import seaborn.objects as so from seaborn import load_dataset mpg = load_dataset(\\"mpg\\") # Task 1: Initialize the plot p = so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") # Task 2: Add dots with color customization p.add(so.Dots(), color=\\"origin\\") # Task 3: Advanced customization with fill color and transparency p.add(so.Dots(fillalpha=0.5), fillcolor=\\"weight\\").scale(fillcolor=\\"binary\\") # Task 4: Mixed markers based on the origin p.add(so.Dots(stroke=1), marker=\\"origin\\").scale(marker=[\\"o\\", \\"x\\", (6, 2, 1)]) # Task 5: Add jitter to handle overlapping points p.add(so.Dots(), so.Jitter(0.25)) # Display the plot p.show()"},{"question":"Advanced Data Visualization using Seaborn Objective: Create a comprehensive data visualization using Seaborn to demonstrate your understanding of loading datasets, creating line plots, customizing styles, and displaying grouped data. Problem Statement: You are given two datasets from Seaborn\'s sample data collections: 1. `dowjones`: Contains historical data of the Dow Jones Industrial Average. 2. `fmri`: Contains simulated fMRI data from a brain scan experiment. Implement a function `create_advanced_plot()` that performs the following tasks: 1. Loads both datasets: `dowjones` and `fmri`. 2. Filters the `fmri` dataset to include only rows where `region` is \\"frontal\\" and `event` is \\"stim\\". 3. Draws a line plot for `dowjones` dataset with: - \\"Date\\" on the x-axis and \\"Price\\" on the y-axis. - The line should display markers at each data point. 4. Draws a grouped line plot for the filtered `fmri` dataset: - \\"timepoint\\" on the x-axis and \\"signal\\" on the y-axis. - Lines grouped by \\"subject\\" and colored by \\"region\\". - Error bands for each \\"event\\" group. 5. Uses appropriate color and line styles to distinguish between different groups. 6. Combines the two visualizations into a single plot, using subplots if necessary. Function Signature: ```python import seaborn as sns import seaborn.objects as so def create_advanced_plot(): pass ``` Expected Output: - The function should display a Matplotlib figure with the specified plots. - The first subplot should display the `dowjones` line plot with markers. - The second subplot should display the grouped `fmri` line plot with error bands. Constraints: - Assume no missing values in the datasets. - You are not required to return any value; just produce the plots. Performance Requirements: - The function should execute efficiently for the given datasets. - Ensure clarity and readability of plots with appropriate labels and legends. # Example Visualization *Your answer will be evaluated based on the correctness, clarity, and aesthetic quality of the visualizations.*","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_advanced_plot(): # Load datasets dowjones = sns.load_dataset(\\"dowjones\\") fmri = sns.load_dataset(\\"fmri\\") # Filter the fmri dataset fmri_filtered = fmri[(fmri[\'region\'] == \'frontal\') & (fmri[\'event\'] == \'stim\')] # Create a new figure and set the size plt.figure(figsize=(14, 8)) # Create the first subplot for dowjones dataset plt.subplot(2, 1, 1) sns.lineplot(data=dowjones, x=\'Date\', y=\'Price\', marker=\'o\') plt.title(\'Dow Jones Industrial Average Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Price\') # Create the second subplot for fmri dataset plt.subplot(2, 1, 2) sns.lineplot( data=fmri_filtered, x=\'timepoint\', y=\'signal\', hue=\'subject\', style=\'event\', err_style=\'band\' ) plt.title(\'fMRI Signal Over Time (Frontal Region, Stim Event)\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') # Adjust layout to prevent overlap and show the plot plt.tight_layout() plt.show()"},{"question":"You are tasked with creating a custom data structure using the `collections.abc` module, which will help you manage a collection of unique items similar to a set, but with the flexibility of maintaining the order of insertion. Objective Implement a class `OrderedUniqueList` that behaves like a combination of a list and a set. It should preserve the order of elements as they are added (like a list) and ensure that all elements are unique (like a set). Requirements - Your class should inherit from `collections.abc.MutableSequence`. - Implement the following methods: - `__getitem__(self, index)` - Return the item at the specified position. - `__setitem__(self, index, value)` - Set the item at the specified position. - `__delitem__(self, index)` - Remove the item at the specified position. - `__len__(self)` - Return the number of items. - `insert(self, index, value)` - Insert an item at the specified position. - Override the `append` method to ensure that only unique items are added. - The reverse method should reverse the order of the items. - Implement methods to make your class hashable using `collections.abc.Hashable`. Constraints - The items in the collection must be hashable. - Ensure that your class prevents duplicate values effectively. Example Usage ```python from collections.abc import MutableSequence, Hashable class OrderedUniqueList(MutableSequence, Hashable): def __init__(self): self._items = [] def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): if value in self._items and self._items[index] != value: raise ValueError(f\\"Duplicate value: {value} already exists in the collection.\\") self._items[index] = value def __delitem__(self, index): del self._items[index] def __len__(self): return len(self._items) def insert(self, index, value): if value in self._items: raise ValueError(f\\"Duplicate value: {value} already exists in the collection.\\") self._items.insert(index, value) def append(self, value): if value not in self._items: self._items.append(value) def reverse(self): self._items.reverse() def __hash__(self): return hash(tuple(self._items)) # Example usage oul = OrderedUniqueList() oul.append(1) oul.append(2) oul.insert(1, 3) oul.reverse() assert oul[0] == 2 assert oul[1] == 3 assert oul[2] == 1 assert len(oul) == 3 assert hash(oul) == hash((2, 3, 1)) ```","solution":"from collections.abc import MutableSequence, Hashable class OrderedUniqueList(MutableSequence, Hashable): def __init__(self): self._items = [] def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): if value in self._items and self._items[index] != value: raise ValueError(f\\"Duplicate value: {value} already exists in the collection.\\") self._items[index] = value def __delitem__(self, index): del self._items[index] def __len__(self): return len(self._items) def insert(self, index, value): if value in self._items: raise ValueError(f\\"Duplicate value: {value} already exists in the collection.\\") self._items.insert(index, value) def append(self, value): if value not in self._items: self._items.append(value) def reverse(self): self._items.reverse() def __hash__(self): return hash(tuple(self._items))"},{"question":"You are tasked with writing a Python utility that compresses and decompresses given text files using the functionality provided by the `gzip` module. This utility should handle both operations based on user input and support exception handling for various erroneous cases. Your Task Implement a function `gzip_utility(action: str, input_filepath: str, output_filepath: str = None, compresslevel: int = 9) -> None` that performs the following operations: 1. **Compression**: - When `action` is `\'compress\'`, read the text from `input_filepath`, compress it, and save the compressed data to `output_filepath` (or the same file with a `.gz` extension if `output_filepath` is not provided). 2. **Decompression**: - When `action` is `\'decompress\'`, read compressed data from `input_filepath`, decompress it, and save the decompressed text to `output_filepath` (or the same file without the `.gz` extension if `output_filepath` is not provided). 3. **Exception Handling**: - Handle `gzip.BadGzipFile` to print \\"Invalid gzip file.\\" - Handle other general exceptions to print \\"An error occurred.\\" Function Signature ```python def gzip_utility(action: str, input_filepath: str, output_filepath: str = None, compresslevel: int = 9) -> None: ``` Input - `action` (str): Either `\'compress\'` or `\'decompress\'`. - `input_filepath` (str): Path to the input file. - `output_filepath` (str, optional): Path to the output file. Defaults to `None`. - `compresslevel` (int, optional): The compression level, ranging from 0 to 9. Defaults to 9. Output - None. (The function saves output data to the specified file paths.) Constraints - Assume all file paths provided are valid until proven otherwise by exception handling. - The input and output file paths must be different for compress and decompress actions if provided. - Handle edge cases, such as files that can\'t be read or written due to permissions, or invalid compresslevel values. Example Usage ```python # Compression Example: gzip_utility(\'compress\', \'example.txt\') # Decompression Example: gzip_utility(\'decompress\', \'example.txt.gz\') ``` In these examples: - For compression, `example.txt` is compressed to `example.txt.gz`. - For decompression, `example.txt.gz` is decompressed to `example.txt`.","solution":"import gzip import os def gzip_utility(action: str, input_filepath: str, output_filepath: str = None, compresslevel: int = 9) -> None: if compresslevel < 0 or compresslevel > 9: print(\\"Invalid compression level. It must be between 0 and 9.\\") return try: if action == \'compress\': if output_filepath is None: output_filepath = input_filepath + \'.gz\' with open(input_filepath, \'rb\') as f_in: with gzip.open(output_filepath, \'wb\', compresslevel=compresslevel) as f_out: f_out.writelines(f_in) print(f\\"File successfully compressed to {output_filepath}\\") elif action == \'decompress\': if output_filepath is None: if input_filepath.endswith(\'.gz\'): output_filepath = input_filepath[:-3] else: raise ValueError(\\"Output file path required for decompression if input file does not have .gz extension\\") with gzip.open(input_filepath, \'rb\') as f_in: with open(output_filepath, \'wb\') as f_out: f_out.writelines(f_in) print(f\\"File successfully decompressed to {output_filepath}\\") else: print(\\"Invalid action. Use \'compress\' or \'decompress\'.\\") except gzip.BadGzipFile: print(\\"Invalid gzip file.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Coding Assessment Question:** **Objective**: Implement a function that analyzes a given Python function\'s source code and returns detailed information about its parameters, local and global variables, and nested functions or classes. **Function Signature**: ```python def analyze_function_code(code: str) -> dict: pass ``` **Input**: - `code` (str): A string containing the source code of a Python function. **Output**: - `dict`: A dictionary with the following structure: ```python { \\"parameters\\": list of parameter names, \\"locals\\": list of local variable names, \\"globals\\": list of global variable names, \\"nested_functions\\": list of tuples (nested function name, line number), \\"nested_classes\\": list of tuples (nested class name, line number), } ``` **Constraints**: 1. The input code is guaranteed to be a valid Python function definition. 2. The function should handle cases where there are no parameters, locals, globals, nested functions, or nested classes gracefully by returning empty lists for those keys. **Example**: ```python code = \'\'\' def example_function(a, b): x = 10 global y def inner_function(): pass class InnerClass: pass return a + b + x \'\'\' result = analyze_function_code(code) expected_result = { \\"parameters\\": [\\"a\\", \\"b\\"], \\"locals\\": [\\"x\\", \\"inner_function\\", \\"InnerClass\\"], \\"globals\\": [\\"y\\"], \\"nested_functions\\": [(\\"inner_function\\", 5)], \\"nested_classes\\": [(\\"InnerClass\\", 7)], } assert result == expected_result ``` **Instructions**: 1. Use the `symtable` module to parse the symbol table of the provided function code. 2. Extract and return the details as described in the expected output format.","solution":"import ast import symtable def analyze_function_code(code: str) -> dict: Analyzes a given Python function\'s source code and returns detailed information about its parameters, local and global variables, and nested functions or classes. tree = ast.parse(code) func_node = next(n for n in tree.body if isinstance(n, ast.FunctionDef)) table = symtable.symtable(code, \'<string>\', \'exec\') func_table = next(t for t in table.get_children() if t.get_name() == func_node.name) parameters = [arg for arg in func_table.get_parameters()] locals_vars = [symbol for symbol in func_table.get_locals() if symbol not in parameters] globals_vars = [symbol for symbol in func_table.get_globals()] nested_functions = [] nested_classes = [] for child in func_table.get_children(): lineno = next((n.lineno for n in ast.walk(tree) if isinstance(n, (ast.FunctionDef, ast.ClassDef)) and n.name == child.get_name()), None) if isinstance(child, symtable.Function): nested_functions.append((child.get_name(), lineno)) if isinstance(child, symtable.Class): nested_classes.append((child.get_name(), lineno)) return { \\"parameters\\": parameters, \\"locals\\": locals_vars, \\"globals\\": globals_vars, \\"nested_functions\\": nested_functions, \\"nested_classes\\": nested_classes, }"},{"question":"Implementing and Training Gaussian Process Regression Objective: Utilize scikit-learn\'s `GaussianProcessRegressor` to implement a Gaussian Process Regression (GPR) model. This task is aimed at helping you demonstrate your understanding of the selection and optimization of kernels, as well as how to perform regression using GPR. Task: 1. **Implement a function `train_gpr_model`**: - **Input**: - `X_train`: A numpy array of shape (n_samples, n_features) representing the training data. - `y_train`: A numpy array of shape (n_samples,) representing the training target values. - `kernel`: A string representing the kernel to be used. It can take values \'RBF\', \'Matern\', \'RationalQuadratic\', \'ExpSineSquared\', \'DotProduct\'. - `alpha`: A float representing the noise level to be added to the diagonal of the kernel matrix. - `n_restarts_optimizer`: An integer representing the number of restarts for the optimizer in hyperparameter optimization. - **Output**: - A trained `GaussianProcessRegressor` model. - **Constraints**: - The kernel should be selected and initialized based on the `kernel` input. - Ensure that the model is fitted with the given training data (`X_train` and `y_train`). - Use the provided `alpha` and `n_restarts_optimizer` for the model. 2. **Implement a function `predict_using_gpr`**: - **Input**: - `model`: A trained `GaussianProcessRegressor` model. - `X_test`: A numpy array of shape (m_samples, n_features) representing the test data. - **Output**: - A numpy array of shape (m_samples,) containing the predicted mean values. - A numpy array of shape (m_samples,) containing the predicted standard deviations (uncertainties). 3. Use the implemented functions to train a model and make predictions on a toy dataset. Generate synthetic data for this purpose. Example Usage: ```python import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct def train_gpr_model(X_train, y_train, kernel=\'RBF\', alpha=1e-10, n_restarts_optimizer=0): if kernel == \'RBF\': kernel_instance = RBF() elif kernel == \'Matern\': kernel_instance = Matern() elif kernel == \'RationalQuadratic\': kernel_instance = RationalQuadratic() elif kernel == \'ExpSineSquared\': kernel_instance = ExpSineSquared() elif kernel == \'DotProduct\': kernel_instance = DotProduct() else: raise ValueError(\\"Unsupported kernel specified.\\") gpr = GaussianProcessRegressor(kernel=kernel_instance, alpha=alpha, n_restarts_optimizer=n_restarts_optimizer) gpr.fit(X_train, y_train) return gpr def predict_using_gpr(model, X_test): y_mean, y_std = model.predict(X_test, return_std=True) return y_mean, y_std # Synthetic dataset X_train = np.array([[1], [3], [5], [6], [7], [8]]) y_train = np.array([10, 12, 15, 21, 22, 24]) X_test = np.array([[2], [4], [6]]) # Train the model gpr_model = train_gpr_model(X_train, y_train, kernel=\'RBF\', alpha=1e-2, n_restarts_optimizer=3) # Make predictions y_mean, y_std = predict_using_gpr(gpr_model, X_test) print(\\"Predicted Mean:\\", y_mean) print(\\"Predicted Std Dev:\\", y_std) ``` Your implementation should be robust, handle exceptions, and include appropriate docstrings. Evaluation Criteria: - Correct selection and implementation of kernels. - Successful fitting of the GPR model. - Accurate predictions with mean and standard deviations. - Code quality and adherence to Python standards.","solution":"import numpy as np from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct def train_gpr_model(X_train, y_train, kernel=\'RBF\', alpha=1e-10, n_restarts_optimizer=0): Train a Gaussian Process Regressor model with the specified kernel and parameters. Parameters: X_train (numpy array): Training data of shape (n_samples, n_features). y_train (numpy array): Target values of shape (n_samples,). kernel (str): Kernel type to be used in the model. Default is \'RBF\'. alpha (float): Noise level to be added to the diagonal of the kernel matrix. Default is 1e-10. n_restarts_optimizer (int): Number of restarts for the optimizer. Default is 0. Returns: GaussianProcessRegressor: A trained Gaussian Process Regressor model. if kernel == \'RBF\': kernel_instance = RBF() elif kernel == \'Matern\': kernel_instance = Matern() elif kernel == \'RationalQuadratic\': kernel_instance = RationalQuadratic() elif kernel == \'ExpSineSquared\': kernel_instance = ExpSineSquared() elif kernel == \'DotProduct\': kernel_instance = DotProduct() else: raise ValueError(\\"Unsupported kernel specified.\\") gpr = GaussianProcessRegressor(kernel=kernel_instance, alpha=alpha, n_restarts_optimizer=n_restarts_optimizer) gpr.fit(X_train, y_train) return gpr def predict_using_gpr(model, X_test): Predict using a trained Gaussian Process Regressor model. Parameters: model (GaussianProcessRegressor): A trained Gaussian Process Regressor model. X_test (numpy array): Test data of shape (m_samples, n_features). Returns: numpy array: Predicted mean values of shape (m_samples,). numpy array: Predicted standard deviations (uncertainties) of shape (m_samples,). y_mean, y_std = model.predict(X_test, return_std=True) return y_mean, y_std"},{"question":"# Advanced Python Logging Challenge In this assessment, you will implement a custom logging handler leveraging the concepts and classes provided in the `logging.handlers` module of Python. The purpose of this assignment is to demonstrate your understanding of logging, custom handler creation, and managing log output. # Requirements 1. **Custom Timed Rotating File Handler with Optional HTTP Notification:** - Create a subclass of `logging.handlers.TimedRotatingFileHandler`. - Your handler should: - Rotate log files based on a specified timed interval. - Maintain a backup count of old log files. - Send an HTTP notification to a specified endpoint whenever a log file is rotated. - The HTTP notification should include: - Basic information about the rotation event. - The name of the new log file. - Implement this using the `emit()` method where the actual logging happens. # Specifications - **Input Parameters**: - `filename` (str): The name of the log file. - `when` (str): A string representing the log rotation interval (\'S\', \'M\', \'H\', \'D\', \'midnight\', \'W0\'-\'W6\'). - `interval` (int): The multiplier for the rotation interval. - `backupCount` (int): The number of backup files to keep. - `http_endpoint` (str): The URL to send the HTTP notification to. - `http_method` (str): The HTTP method to use for the notification (\'GET\', \'POST\'). - **Constraints**: - Use the default file mode \'a\' for appending logs. - Assume any necessary imports and the existence of required endpoints. - You may use the `requests` library for sending HTTP requests. # Implementation ```python import logging from logging.handlers import TimedRotatingFileHandler import requests class CustomTimedRotatingFileHandler(TimedRotatingFileHandler): def __init__(self, filename, when, interval=1, backupCount=0, http_endpoint=None, http_method=\'GET\'): super().__init__(filename, when, interval, backupCount) self.http_endpoint = http_endpoint self.http_method = http_method def emit(self, record): super().emit(record) if self.shouldRollover(record): self.doRollover() self.send_http_notification() def send_http_notification(self): if self.http_endpoint: data = { \'event\': \'log_rotation\', \'new_log_file\': self.baseFilename, } try: if self.http_method == \'POST\': requests.post(self.http_endpoint, json=data) else: requests.get(self.http_endpoint, params=data) except requests.RequestException as e: print(f\\"HTTP notification failed: {e}\\") # Example usage if __name__ == \\"__main__\\": logger = logging.getLogger(\\"customLogger\\") handler = CustomTimedRotatingFileHandler( filename=\'app.log\', when=\'D\', interval=1, backupCount=7, http_endpoint=\'https://example.com/log_notification\', http_method=\'POST\' ) logger.addHandler(handler) logger.setLevel(logging.DEBUG) logger.info(\\"This is a log message.\\") ``` # Notes - The solution should ensure proper log rotation and HTTP notifications without causing significant delays or issues in logging performance. - Clearly comment your code to explain the logic behind your implementation. - Test the handler with different configurations to ensure it operates correctly and handles failures gracefully.","solution":"import logging from logging.handlers import TimedRotatingFileHandler import requests class CustomTimedRotatingFileHandler(TimedRotatingFileHandler): A custom logging handler that rotates the log file based on a timed interval and sends an HTTP notification when a rotation occurs. def __init__(self, filename, when, interval=1, backupCount=0, http_endpoint=None, http_method=\'GET\'): super().__init__(filename, when, interval, backupCount) self.http_endpoint = http_endpoint self.http_method = http_method def doRollover(self): Perform log file rollover and notify the specified HTTP endpoint. super().doRollover() self.send_http_notification() def send_http_notification(self): Send an HTTP notification about log file rotation. if self.http_endpoint: data = { \'event\': \'log_rotation\', \'new_log_file\': self.baseFilename, } try: if self.http_method == \'POST\': requests.post(self.http_endpoint, json=data) else: requests.get(self.http_endpoint, params=data) except requests.RequestException as e: print(f\\"HTTP notification failed: {e}\\") # Example usage if __name__ == \\"__main__\\": logger = logging.getLogger(\\"customLogger\\") handler = CustomTimedRotatingFileHandler( filename=\'app.log\', when=\'D\', interval=1, backupCount=7, http_endpoint=\'https://example.com/log_notification\', http_method=\'POST\' ) logger.addHandler(handler) logger.setLevel(logging.DEBUG) logger.info(\\"This is a log message.\\") logger.info(\\"Another log message to trigger rotation.\\")"},{"question":"# Question: Configuring PyTorch with CUDA Environment Variables You are given the task of creating a PyTorch script that requires configuring various CUDA-related environment variables to achieve specific behaviors. Implement the required configurations programmatically using Python’s `os` module. # Task Write a Python function `configure_pytorch_cuda(environment_settings: dict)` that takes a dictionary of environment variable settings and applies these settings using the `os` module. After setting the environment variables, the function should initialize a simple PyTorch tensor operation on CUDA. Finally, the function should return a dictionary indicating whether each tensor operation utilized the intended CUDA configurations. # Function Signature ```python def configure_pytorch_cuda(environment_settings: dict) -> dict: ``` # Input - `environment_settings`: Dictionary where keys are the environment variable names (as strings) and values are the desired settings (as strings). # Output - A dictionary where the keys are the names of the configurations that were set, and the values are booleans indicating whether the configuration was utilized correctly during a simple tensor operation. # Constraints - Assume that the machine has at least one CUDA-enabled GPU. - If an invalid environment variable is provided, raise a `ValueError` with a message indicating the invalid variable. - Use PyTorch for the tensor operations. - Ensure that the environment settings are applied before importing the main PyTorch modules. # Example ```python import torch import os def configure_pytorch_cuda(environment_settings: dict) -> dict: valid_variables = { \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\", \\"PYTORCH_CUDA_ALLOC_CONF\\", \\"PYTORCH_NVML_BASED_CUDA_CHECK\\", \\"TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\\", \\"TORCH_CUDNN_V8_API_DISABLED\\", \\"TORCH_ALLOW_TF32_CUBLAS_OVERRIDE\\", \\"TORCH_NCCL_USE_COMM_NONBLOCKING\\", \\"TORCH_NCCL_AVOID_RECORD_STREAMS\\", \\"TORCH_CUDNN_V8_API_DEBUG\\", \\"CUDA_VISIBLE_DEVICES\\", \\"CUDA_LAUNCH_BLOCKING\\", \\"CUBLAS_WORKSPACE_CONFIG\\", \\"CUDNN_CONV_WSCAP_DBG\\", \\"CUBLASLT_WORKSPACE_SIZE\\", \\"CUDNN_ERRATA_JSON_FILE\\", \\"NVIDIA_TF32_OVERRIDE\\" } for var, value in environment_settings.items(): if var not in valid_variables: raise ValueError(f\\"Invalid environment variable: {var}\\") os.environ[var] = value # Import torch after setting environment variables to ensure they are applied import torch # Conduct a simple tensor operation to determine if settings took effect results = {} # Example operations try: x = torch.tensor([1.0, 2.0]).cuda() y = torch.tensor([1.0, 2.0]).cuda() z = torch.add(x, y) results[\\"Operation_Success\\"] = True except Exception as e: results[\\"Operation_Success\\"] = False return results # Sample usage env_settings = { \\"CUDA_VISIBLE_DEVICES\\": \\"0\\", \\"CUDA_LAUNCH_BLOCKING\\": \\"1\\" } print(configure_pytorch_cuda(env_settings)) ``` # Notes - The provided function template ensures that all environment variables are set before importing PyTorch, which is crucial for the changes to take effect. - You may expand the tensor operations and checks to cover specific configurations based on the variables set. This question assesses the students\' ability to handle environment configurations programmatically and integrate them with PyTorch operations effectively.","solution":"import os def configure_pytorch_cuda(environment_settings): Configures the environment settings for PyTorch usage with CUDA and performs a simple tensor operation to verify the configuration. Args: environment_settings (dict): A dictionary of environment variable names and corresponding values to be set. Returns: dict: A dictionary indicating the success of setting each environment variable. valid_variables = { \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\", \\"PYTORCH_CUDA_ALLOC_CONF\\", \\"PYTORCH_NVML_BASED_CUDA_CHECK\\", \\"TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\\", \\"TORCH_CUDNN_V8_API_DISABLED\\", \\"TORCH_ALLOW_TF32_CUBLAS_OVERRIDE\\", \\"TORCH_NCCL_USE_COMM_NONBLOCKING\\", \\"TORCH_NCCL_AVOID_RECORD_STREAMS\\", \\"TORCH_CUDNN_V8_API_DEBUG\\", \\"CUDA_VISIBLE_DEVICES\\", \\"CUDA_LAUNCH_BLOCKING\\", \\"CUBLAS_WORKSPACE_CONFIG\\", \\"CUDNN_CONV_WSCAP_DBG\\", \\"CUBLASLT_WORKSPACE_SIZE\\", \\"CUDNN_ERRATA_JSON_FILE\\", \\"NVIDIA_TF32_OVERRIDE\\" } for var, value in environment_settings.items(): if var not in valid_variables: raise ValueError(f\\"Invalid environment variable: {var}\\") os.environ[var] = value # Import torch after setting environment variables to ensure they are applied import torch results = {} try: x = torch.tensor([1.0, 2.0]).cuda() y = torch.tensor([1.0, 2.0]).cuda() z = torch.add(x, y) results[\\"CUDA_Operation_Success\\"] = True except Exception as e: results[\\"CUDA_Operation_Success\\"] = False return results"},{"question":"# Python Data Marshalling Wrapper In this exercise, you will write a Python wrapper around some of the C-based data marshalling functions provided in the Python documentation. Your task is to demonstrate an understanding of how to interface Python code with low-level C-implemented functions for efficient data serialization and deserialization. Task 1. Write a Python function `write_long_to_file` that: - Takes a `long` integer and a file path as its arguments. - Writes the `long` integer to the specified file using the `PyMarshal_WriteLongToFile` equivalent in Python. 2. Write a Python function `read_long_from_file` that: - Takes a file path as its argument. - Reads and returns the `long` integer from the specified file using the `PyMarshal_ReadLongFromFile` equivalent in Python. 3. Write a Python function `write_object_to_file` that: - Takes a Python object and a file path as its arguments. - Writes the Python object to the specified file using the `PyMarshal_WriteObjectToFile` equivalent in Python. 4. Write a Python function `read_object_from_file` that: - Takes a file path as its argument. - Reads and returns the Python object from the specified file using the `PyMarshal_ReadObjectFromFile` equivalent in Python. 5. Write a Python function `marshal_object_to_string` that: - Takes a Python object as its argument. - Returns a string containing the marshalled representation of the object using the `PyMarshal_WriteObjectToString` equivalent in Python. 6. Write a Python function `unmarshal_object_from_string` that: - Takes a string representing marshaled data and its size as arguments. - Returns the unmarshalled Python object using the `PyMarshal_ReadObjectFromString` equivalent in Python. Constraints - The functions must handle the appropriate exceptions (`EOFError`, `ValueError`, `TypeError`) and raise them with custom messages. - Use marshaling version 2 for compatibility. Implementation Notes - You may use the `struct` package in Python to emulate some of the low-level behavior where necessary. - Ensure your file operations handle opening and closing files properly to avoid resource leaks. - Include appropriate test cases to demonstrate the correctness of your functions. Example ```python def write_long_to_file(value, filename): # Implement the function pass def read_long_from_file(filename): # Implement the function pass def write_object_to_file(obj, filename): # Implement the function pass def read_object_from_file(filename): # Implement the function pass def marshal_object_to_string(obj): # Implement the function pass def unmarshal_object_from_string(data, size): # Implement the function pass # Example Usage: write_long_to_file(1234567890, \'test_long.dat\') print(read_long_from_file(\'test_long.dat\')) # Output: 1234567890 write_object_to_file([1, 2, 3], \'test_obj.dat\') print(read_object_from_file(\'test_obj.dat\')) # Output: [1, 2, 3] serialized = marshal_object_to_string({\'key\': \'value\'}) print(unmarshal_object_from_string(serialized, len(serialized))) # Output: {\'key\': \'value\'} ```","solution":"import marshal def write_long_to_file(value, filename): Writes a long integer to a file. with open(filename, \'wb\') as file: marshal.dump(value, file) def read_long_from_file(filename): Reads a long integer from a file and returns it. with open(filename, \'rb\') as file: return marshal.load(file) def write_object_to_file(obj, filename): Writes a Python object to a file using marshalling. with open(filename, \'wb\') as file: marshal.dump(obj, file) def read_object_from_file(filename): Reads a Python object from a file using marshalling and returns it. with open(filename, \'rb\') as file: return marshal.load(file) def marshal_object_to_string(obj): Returns a string containing the marshalled representation of the object. return marshal.dumps(obj) def unmarshal_object_from_string(data, size): Returns the unmarshalled Python object from a given string representation. return marshal.loads(data[:size])"},{"question":"Using Special Functions from `torch.special` Objective: You are asked to write a function that calculates the sum of Bessel functions of the first kind (of order 0 and 1) and the digamma function for a given tensor of inputs. This exercise will demonstrate your understanding of using special functions available in the `torch.special` module in PyTorch. Problem Statement: Implement a function `calculate_special_functions` that takes a PyTorch tensor `x` as input and returns a tensor where each element is calculated as the sum of three functions applied element-wise to `x`: 1. Bessel function of the first kind of order 0 (`bessel_j0`). 2. Bessel function of the first kind of order 1 (`bessel_j1`). 3. Digamma function (`digamma`). Use the functions `bessel_j0`, `bessel_j1`, and `digamma` from the `torch.special` module to accomplish this. Input: - `x`: A 1-dimensional PyTorch tensor of shape `(n,)` where `n` can be any positive integer. Output: - A 1-dimensional PyTorch tensor of shape `(n,)` where each element is the sum of `bessel_j0(x_i)`, `bessel_j1(x_i)` and `digamma(x_i)` for the corresponding element `x_i` in the input tensor. Constraints: - You may assume that all elements of `x` are positive real numbers. - Avoid using explicit loops; leverage PyTorch tensor operations to ensure efficient computation. Example: ```python import torch import torch.special as special def calculate_special_functions(x: torch.Tensor) -> torch.Tensor: # Your code here # Example usage x = torch.tensor([1.0, 2.0, 3.0]) result = calculate_special_functions(x) print(result) # expected output might vary depending on the specific values of special functions ``` Notes: - The specific numerical output will depend on the implementations of the special functions in PyTorch. - Ensure that your function can handle tensors with varying lengths.","solution":"import torch import torch.special as special def calculate_special_functions(x: torch.Tensor) -> torch.Tensor: Calculates the sum of the Bessel functions of the first kind (order 0 and 1) and the digamma function for each element in the input tensor x. bessel_j0 = special.bessel_j0(x) bessel_j1 = special.bessel_j1(x) digamma = special.digamma(x) result = bessel_j0 + bessel_j1 + digamma return result"},{"question":"**Objective**: Demonstrate your understanding of the `torch.distributions` module by implementing a function that performs several operations with different probability distributions. # Problem Statement You are required to implement a function `distribution_operations` that takes no input parameters. This function should: 1. Create three different probability distributions using PyTorch\'s distribution classes: - A Normal distribution with mean 0 and standard deviation 1. - A Beta distribution with shape parameters alpha = 2 and beta = 5. - A Categorical distribution representing the probabilities [0.2, 0.5, 0.3]. 2. Sample 1000 values from each of these distributions. 3. Calculate and return the following statistics for the sampled values: - Mean and standard deviation for the Normal distribution samples. - Mean and standard deviation for the Beta distribution samples. - The normalized histogram (probability mass function) for the Categorical distribution samples. - The Kullback-Leibler divergence between the Normal distribution and a standard Normal distribution (mean 0, std 1). # Constraints & Requirements - You should make use of appropriate PyTorch distribution classes and their methods. - The mean and standard deviation should be computed using PyTorch functions. - The histogram (PMF) values should be normalized to sum to 1. # Expected Output Format The function should return a dictionary with the following structure: ```python { \\"normal\\": { \\"mean\\": float, \\"std\\": float }, \\"beta\\": { \\"mean\\": float, \\"std\\": float }, \\"categorical\\": { \\"normalized_histogram\\": List[float] }, \\"kl_divergence\\": float } ``` # Function Signature ```python def distribution_operations() -> dict: pass ``` # Example ```python result = distribution_operations() print(result) ``` Expected output (values may vary based on sampling): ```python { \\"normal\\": {\\"mean\\": -0.025, \\"std\\": 0.98}, \\"beta\\": {\\"mean\\": 0.28, \\"std\\": 0.12}, \\"categorical\\": {\\"normalized_histogram\\": [0.19, 0.51, 0.30]}, \\"kl_divergence\\": 0.001 } ``` **Note**: The example output is illustrative. The actual values will differ due to the random nature of sampling. # Additional Information Feel free to refer to the PyTorch documentation on distributions for the methods and classes you may use.","solution":"import torch from torch import distributions as dist def distribution_operations(): # Create distributions normal_dist = dist.Normal(0, 1) beta_dist = dist.Beta(2, 5) categorical_dist = dist.Categorical(torch.tensor([0.2, 0.5, 0.3])) # Sample values normal_samples = normal_dist.sample((1000,)) beta_samples = beta_dist.sample((1000,)) categorical_samples = categorical_dist.sample((1000,)) # Calculate mean and standard deviation for Normal distribution normal_mean = normal_samples.mean().item() normal_std = normal_samples.std().item() # Calculate mean and standard deviation for Beta distribution beta_mean = beta_samples.mean().item() beta_std = beta_samples.std().item() # Calculate normalized histogram for Categorical distribution categorical_counts = torch.bincount(categorical_samples, minlength=3).float() normalized_histogram = (categorical_counts / categorical_samples.size(0)).tolist() # Calculate KL divergence between the sampled Normal distribution and standard Normal distribution standard_normal_dist = dist.Normal(0, 1) kl_divergence = dist.kl_divergence(normal_dist, standard_normal_dist).item() return { \\"normal\\": { \\"mean\\": normal_mean, \\"std\\": normal_std }, \\"beta\\": { \\"mean\\": beta_mean, \\"std\\": beta_std }, \\"categorical\\": { \\"normalized_histogram\\": normalized_histogram }, \\"kl_divergence\\": kl_divergence }"},{"question":"**Problem Statement:** You are given a dataset `diamonds` from the Seaborn library. Your task is to create two customized subfigures using `seaborn.objects` and Matplotlib. You need to follow these steps closely: 1. Create a subfigure `sf1` containing a scatter plot of the `diamonds` dataset with carat and price as the x and y axes respectively. 2. Create another subfigure `sf2` that contains a log-scaled bar plot and a histogram. The histogram should be facetted by the `cut` variable from the dataset. **Detailed Instructions:** 1. Load the `diamonds` dataset using `seaborn.load_dataset`. 2. Use `seaborn.objects.Plot` and Matplotlib\'s `Figure.subfigures` method to create a composite figure with two subfigures. 3. In the first subfigure (`sf1`), create a scatter plot using `so.Plot` that plots `carat` versus `price`. 4. In the second subfigure (`sf2`), create a faceted histogram (using bars) of `price` logged on the x-axis and facetted by `cut`. Additionally, add a histogram to illustrate the distribution within each facet. **Constraints:** - You must use `seaborn.objects.Plot` and Matplotlib\'s `Figure.subfigures`. - You must use appropriate Seaborn and Matplotlib methods to ensure the plots are clearly labeled and well-formatted. **Expected Output:** - A figure consisting of two subfigures: the first is a scatter plot of `carat` versus `price`, and the second is a faceted histogram of `price` logged on the x-axis facetted by `cut`. ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt # Load the diamonds dataset from seaborn diamonds = sns.load_dataset(\\"diamonds\\") # Create the main figure and subfigures f = mpl.figure.Figure(figsize=(12, 6), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # Create a scatter plot in the first subfigure p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()).on(sf1).plot() # Create a faceted histogram in the second subfigure ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2) ) # Show the plot plt.show() ``` **Note:** Ensure that all plots are customizable and that any advanced Matplotlib features can be added as per the requirement to achieve the necessary level of understanding and customization.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_subfigures(): # Load the diamonds dataset from seaborn diamonds = sns.load_dataset(\\"diamonds\\") # Create the main figure and subfigures fig = plt.figure(constrained_layout=True, figsize=(12, 6)) sf1, sf2 = fig.subfigures(1, 2) # Create a scatter plot in the first subfigure p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()).on(sf1).plot() # Create a faceted histogram with log-scaled bar plot in the second subfigure ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2) ) # Show the plot plt.show()"},{"question":"**Swarm Plot and Faceted Plot with Seaborn** You are provided with a dataset on tipping practices. Using this dataset, you need to generate visualizations to analyze tipping behavior based on the day of the week, the time of day, and the gender of the tipper. Your task is to implement two functions as described below. # Part 1: Swarm Plot Implement a function `create_swarm_plot(data)`, which accepts a DataFrame `data` containing tipping information and creates a swarm plot of the total bill amount versus the day of the week. Use the `sex` column to further color the points and make use of the `dodge` parameter. ```python def create_swarm_plot(data: pd.DataFrame) -> plt.Axes: This function takes a dataframe as input and returns a matplotlib Axes object of the created swarm plot showing total bill amount vs. day of the week with hue based on sex and dodging enabled. Parameters: data (pd.DataFrame): The input DataFrame containing tipping data Returns: plt.Axes: The Axes object of the created swarm plot pass ``` # Part 2: Faceted Swarm Plot Implement another function `create_faceted_swarm_plot(data)`, which also accepts a DataFrame `data` but creates a faceted swarm plot that shows the relationship between the time of day (`time`) and the total bill amount across different days of the week. Use `sex` for the hue. The plot should have one subplot for each day of the week arranged horizontally. ```python def create_faceted_swarm_plot(data: pd.DataFrame) -> sns.axisgrid.FacetGrid: This function takes a dataframe as input and returns a seaborn FacetGrid object of the created faceted swarm plot showing time of day vs. total bill amount with hue based on sex, faceted by day of the week. Parameters: data (pd.DataFrame): The input DataFrame containing tipping data Returns: sns.axisgrid.FacetGrid: The FacetGrid object of the created faceted swarm plot pass ``` # Input Format - The input to both functions will be a Pandas DataFrame containing at least the following columns: - `total_bill`: A numeric column representing the total bill amount. - `day`: A categorical column with values representing the day of the week. - `time`: A categorical column with values representing the time of day (e.g., \'Lunch\', \'Dinner\'). - `sex`: A categorical column with values \'Male\' and \'Female\'. # Output Format - The first function should return a Matplotlib `Axes` object representing the created swarm plot. - The second function should return a Seaborn `FacetGrid` object representing the created faceted plot. **Note:** Ensure you include the necessary imports and set the appropriate Seaborn theme at the beginning. ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd sns.set_theme(style=\\"whitegrid\\") ``` # Constraints - Your solution should be efficient and make use of relevant Seaborn functionality. - Handle missing or malformed data gracefully by ensuring that you only plot non-null values from the dataset. # Example Usage ```python tips = sns.load_dataset(\\"tips\\") # Example usage for Part 1 ax = create_swarm_plot(tips) plt.show() # Example usage for Part 2 grid = create_faceted_swarm_plot(tips) plt.show() ``` Make sure your functions pass these example usages by correctly displaying the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd sns.set_theme(style=\\"whitegrid\\") def create_swarm_plot(data: pd.DataFrame) -> plt.Axes: This function takes a dataframe as input and returns a matplotlib Axes object of the created swarm plot showing total bill amount vs. day of the week with hue based on sex and dodging enabled. Parameters: data (pd.DataFrame): The input DataFrame containing tipping data Returns: plt.Axes: The Axes object of the created swarm plot ax = sns.swarmplot( x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=data, dodge=True) return ax def create_faceted_swarm_plot(data: pd.DataFrame) -> sns.axisgrid.FacetGrid: This function takes a dataframe as input and returns a seaborn FacetGrid object of the created faceted swarm plot showing time of day vs. total bill amount with hue based on sex, faceted by day of the week. Parameters: data (pd.DataFrame): The input DataFrame containing tipping data Returns: sns.axisgrid.FacetGrid: The FacetGrid object of the created faceted swarm plot grid = sns.catplot( x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=data, kind=\\"swarm\\") return grid"},{"question":"# Question: Implement Fleet-wide Operator Profiling in PyTorch **Objective:** To assess your understanding of profiling PyTorch operations at scale by using callback hooks and analyzing operator performance. **Task:** 1. Implement a PyTorch callback that logs the start and end time of each operator executed during a model\'s forward pass. 2. Your implementation should include enabling and configuring these callbacks, ensuring they profile the operators effectively. 3. Create a simple neural network and run a forward pass while utilizing your profiling setup. 4. Gather and print the profiling information, focusing on the elapsed time for each operator invocation. **Requirements:** - Use `torch.autograd.profiler` for profiling operators. - Configure the profiling to be always on. - Implement custom logging callbacks for operator timing. - Create and execute a simple neural network to demonstrate the profiling in action. **Input:** - None for the profiling setup. - A sample tensor input to test the forward pass of the neural network (e.g., a tensor of shape `(10, 10)`). **Output:** - Console logs representing the function entry and exit times for each operator during the model\'s forward pass. **Sample Code Structure:** ```python import torch import torch.nn as nn # Custom logging callbacks for operator profiling def on_function_start(): # Implement logging for function start with current time pass def on_function_end(): # Implement logging for function end with current time pass # Initialize and setup the global callback def init_profiling(): # Register the custom callbacks here pass # Define a simple neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) # Initialize and run the profiling init_profiling() # Create a simple network and input tensor net = SimpleNet() x = torch.randn(10, 10) # Run a forward pass to trigger the profiling output = net(x) # Print the profiling information ``` **Constraints and Notes:** - Your implementation should ensure minimal performance overhead where possible. - The profiling setup and the callbacks should be implemented in Python leveraging PyTorch\'s `torch.autograd.profiler`. - Ensure thread safety when registering the callbacks if required.","solution":"import torch import torch.nn as nn import time # Global dictionary to store profiling information profiling_info = [] # Custom logging callbacks for operator profiling def on_function_start(name): start_time = time.time() profiling_info.append((name, start_time, None)) def on_function_end(): end_time = time.time() if profiling_info and profiling_info[-1][2] is None: profiling_info[-1] = (profiling_info[-1][0], profiling_info[-1][1], end_time) # Create a context manager for profiling functions class ProfilerContextManager: def __init__(self, name): self.name = name def __enter__(self): on_function_start(self.name) def __exit__(self, exc_type, exc_val, exc_tb): on_function_end() # Wrapper to annotate functions with profiling def profile_function(func, name): def wrapper(*args, **kwargs): with ProfilerContextManager(name): return func(*args, **kwargs) return wrapper # Example implementation of a simple neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) # Monkey patch the Linear forward method to include profiling original_linear_forward = nn.Linear.forward nn.Linear.forward = profile_function(original_linear_forward, \\"Linear\\") # Initialize the profiling def init_profiling(): global profiling_info profiling_info = [] # Initialize and run the profiling init_profiling() # Create a simple network and input tensor net = SimpleNet() x = torch.randn(10, 10) # Run a forward pass to trigger the profiling output = net(x) # Print the profiling information for name, start_time, end_time in profiling_info: if end_time: elapsed_time = end_time - start_time print(f\\"{name}: Start Time: {start_time:.6f}, End Time: {end_time:.6f}, Elapsed Time: {elapsed_time:.6f} seconds\\") else: print(f\\"{name}: Start Time: {start_time:.6f}, End Time: Not ended\\")"},{"question":"Objective: Create a `PrecisionRecallCurveDisplay` class that visualizes the Precision-Recall curve for a given set of predictions from a classifier using scikit-learn\'s Plotting API. Instructions: 1. Implement a class `PrecisionRecallCurveDisplay` with the following specifications: - **Initialization**: ```python def __init__(self, precision, recall, average_precision, estimator_name): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name ``` - **Class methods**: ```python @classmethod def from_estimator(cls, estimator, X, y): Create a PrecisionRecallCurveDisplay object from an estimator and data. Parameters: - estimator : a fitted classifier - X : array-like of shape (n_samples, n_features) - y : array-like of shape (n_samples,) Returns: - display : PrecisionRecallCurveDisplay def from_predictions(cls, y, y_pred, estimator_name): Create a PrecisionRecallCurveDisplay from true labels and predicted probabilities. Parameters: - y : array-like of shape (n_samples,) - y_pred : array-like of shape (n_samples,) - estimator_name : string Returns: - display : PrecisionRecallCurveDisplay ``` - **Plot method**: ```python def plot(self, ax=None, name=None, **kwargs): Plot the precision-recall curve. Parameters: - ax : matplotlib Axes, default=None The axes on which to plot the curve. Returns: - display : PrecisionRecallCurveDisplay Object that allows plot customization ``` 2. Use the `precision_recall_curve` function from scikit-learn to compute precision, recall, and average precision score. 3. Your `plot` function should be able to handle plotting on multiple axes using `GridSpecFromSubplotSpec` if given a single axes object. Examples Here is an example usage of your class: ```python from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # Create a simple classification dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the classifier clf = LogisticRegression() clf.fit(X_train, y_train) # Create display object from the classifier and test data display = PrecisionRecallCurveDisplay.from_estimator(clf, X_test, y_test) # Plot the precision-recall curve import matplotlib.pyplot as plt fig, ax = plt.subplots() display.plot(ax=ax) plt.show() ``` Constraints: - Use only numpy, matplotlib, and scikit-learn libraries. - Ensure the `plot` method is flexible with axis manipulation. Requirements: - Accuracy: The class methods should accurately compute the values needed for the precision-recall curve. - Flexibility: Support plotting on multiple and single axes as described. - Customization: Allow further customization of the plot after calling the `plot` method via stored matplotlib artists.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score class PrecisionRecallCurveDisplay: def __init__(self, precision, recall, average_precision, estimator_name): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): Create a PrecisionRecallCurveDisplay object from an estimator and data. y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, type(estimator).__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name): Create a PrecisionRecallCurveDisplay from true labels and predicted probabilities. precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) return cls(precision, recall, average_precision, estimator_name) def plot(self, ax=None, name=None, **kwargs): Plot the precision-recall curve. if ax is None: ax = plt.gca() if name is None: name = self.estimator_name line_kwargs = {\'label\': f\'{name} (AP = {self.average_precision:.2f})\'} line_kwargs.update(kwargs) ax.plot(self.recall, self.precision, **line_kwargs) ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.set_title(\'Precision-Recall curve\') ax.legend(loc=\'best\') return self"},{"question":"# Custom Distutils Command Implementation **Objective**: Implement a custom `distutils` command to scan all Python files in the specified package directory for TODO comments and generate a summary report. # Problem Statement: You are required to create a new `distutils` command named `todo_report`. This command will search for all `*.py` files within a specified package directory, scan each file for comments containing the keyword \\"TODO\\", and generate a summary report listing the filenames and lines where TODO comments were found. # Requirements: 1. **Command Name**: `todo_report` 2. **Command Class Implementation**: - The class should inherit from `distutils.cmd.Command`. - Implement the methods `initialize_options`, `finalize_options`, and `run`. 3. **Options**: - `package_dir` (string): The root directory of the package to scan for Python files. - `output_file` (string): The name of the file where the TODO report will be written. 4. **Functionality**: - Traverse the directory tree starting from `package_dir`. - For each Python file (`*.py`), search for lines containing the keyword \\"TODO\\". - Generate a report with the following format: ``` Filename: path/to/file.py Line 15: # TODO: Refactor this function Line 42: # TODO: Fix bug ``` - Write the report to `output_file`. # Constraints: - You must use the `os` and `distutils` standard modules only. - Consider large directory trees and handle them efficiently. - Only consider lines that start with `#` as comments. # Example: ``` setup.py: # Example setup.py script where the custom command is registered from distutils.core import setup from distutils.cmd import Command class TodoReport(Command): description = \\"Scan the package directory for TODO comments and generate a summary report.\\" user_options = [ (\'package_dir=\', \'d\', \'The root directory of the package\'), (\'output_file=\', \'o\', \'The file where the TODO report will be written\') ] def initialize_options(self): self.package_dir = None self.output_file = None def finalize_options(self): if self.package_dir is None: raise ValueError(\\"The package_dir option is mandatory.\\") if self.output_file is None: raise ValueError(\\"The output_file option is mandatory.\\") def run(self): import os report_lines = [] for root, _, files in os.walk(self.package_dir): for file in files: if file.endswith(\'.py\'): file_path = os.path.join(root, file) with open(file_path, \'r\') as f: for lineno, line in enumerate(f, start=1): if line.strip().startswith(\'#\') and \'TODO\' in line: report_lines.append(f\\"Filename: {file_path}\\") report_lines.append(f\\"Line {lineno}: {line.strip()}\\") report_lines.append(\\"n\\") with open(self.output_file, \'w\') as report_file: report_file.writelines(report_lines) setup( name=\'your_package\', version=\'1.0\', packages=[\'your_package\'], cmdclass={ \'todo_report\': TodoReport }, ) ``` Include the command `todo_report` in your `setup.py` and ensure it can handle the specified options correctly. Test your command implementation by running: ``` python setup.py todo_report --package_dir=src --output_file=todo_report.txt ``` Ensure that the TODO comments from all Python files in the `src` directory will be written to `todo_report.txt`.","solution":"from distutils.cmd import Command import os class TodoReport(Command): description = \\"Scan the package directory for TODO comments and generate a summary report.\\" user_options = [ (\'package_dir=\', \'d\', \'The root directory of the package\'), (\'output_file=\', \'o\', \'The file where the TODO report will be written\') ] def initialize_options(self): self.package_dir = None self.output_file = None def finalize_options(self): if self.package_dir is None: raise ValueError(\\"The package_dir option is mandatory.\\") if self.output_file is None: raise ValueError(\\"The output_file option is mandatory.\\") def run(self): report_lines = [] for root, _, files in os.walk(self.package_dir): for file in files: if file.endswith(\'.py\'): file_path = os.path.join(root, file) with open(file_path, \'r\') as f: for lineno, line in enumerate(f, start=1): if line.strip().startswith(\'#\') and \'TODO\' in line: report_lines.append(f\\"Filename: {file_path}n\\") report_lines.append(f\\"Line {lineno}: {line.strip()}n\\") with open(self.output_file, \'w\') as report_file: report_file.writelines(report_lines)"},{"question":"# Machine Learning with Real-World Datasets Objective The goal of this exercise is to demonstrate your ability to fetch real-world datasets using scikit-learn and implement a basic machine learning workflow. You will load a dataset, perform basic preprocessing, and train a machine learning model. Instructions 1. **Fetch the Dataset**: Use the function `fetch_california_housing` from the `sklearn.datasets` module to load the California Housing dataset. 2. **Preprocess the Data**: - Split the data into training and testing sets. - Perform any necessary preprocessing (e.g., scaling the features). 3. **Train a Model**: - Train a simple linear regression model using the training data. - Evaluate the model using mean squared error (MSE) on the test data. 4. **Implementation Details**: - You must use scikit-learn for all operations including data loading, preprocessing, training, and evaluation. - You can use any other necessary libraries such as numpy or pandas for data manipulation. Here is the skeleton code you need to complete: ```python from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def main(): # Step 1: Fetch the dataset dataset = fetch_california_housing() # Step 2: Preprocess the data X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=42) # Initialize the scaler scaler = StandardScaler() # Fit the scaler on the training data and transform both training and test data X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 3: Train the model model = LinearRegression() model.fit(X_train_scaled, y_train) # Step 4: Evaluate the model predictions = model.predict(X_test_scaled) mse = mean_squared_error(y_test, predictions) print(f\\"Mean Squared Error: {mse}\\") if __name__ == \\"__main__\\": main() ``` Expected Output The final output should be the mean squared error of the model on the test dataset. Make sure the code runs without errors and produces the expected result. Constraints and Limitations - Use an 80-20 split for training and test data. - Ensure that all preprocessing steps are fit to the training data and then applied to the test data. - The solution must be implemented using Python and scikit-learn. Performance Requirements - The code should run efficiently on a standard machine with average specifications. - The data loading and preprocessing should not exceed reasonable time limits (a few seconds).","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def main(): # Step 1: Fetch the dataset dataset = fetch_california_housing() # Step 2: Preprocess the data X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=42) # Initialize the scaler scaler = StandardScaler() # Fit the scaler on the training data and transform both training and test data X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 3: Train the model model = LinearRegression() model.fit(X_train_scaled, y_train) # Step 4: Evaluate the model predictions = model.predict(X_test_scaled) mse = mean_squared_error(y_test, predictions) print(f\\"Mean Squared Error: {mse}\\") return mse if __name__ == \\"__main__\\": main()"},{"question":"# Drag-and-Drop System in Tkinter Create a simple GUI application using Tkinter that allows the user to drag and drop colored rectangles within the same window using the `tkinter.dnd` module. The application should feature the following: 1. **A main window** with a canvas or frame where rectangles are placed. 2. **Three rectangles** with different colors placed initially at various positions on the canvas. 3. Implement drag-and-drop functionality that allows the user to drag any rectangle and drop it at a new location on the canvas. Details: - **Input**: No explicit input other than user interactions. - **Output**: Visual feedback as the rectangles are dragged and dropped within the window. Steps: 1. Create a main window and a canvas widget. 2. Draw three rectangles on the canvas at different initial positions. 3. Allow rectangles to be draggable by binding a ButtonPress event to a function that calls `dnd_start(source, event)`. 4. Implement event handling such that the rectangle moves with the mouse while dragging and is placed at the new location upon releasing the mouse button. Constraints: 1. You can assume that the initial positions of rectangles do not overlap. 2. The rectangles should remain within the bounds of the canvas. Example: ```python import tkinter as tk from tkinter.dnd import DndHandler, dnd_start class RectangleDragDropApp: def __init__(self, root): self.root = root self.canvas = tk.Canvas(root, width=400, height=400, bg=\'white\') self.canvas.pack() self.rectangles = [] self.create_rectangle(50, 50, 100, 100, \'red\') self.create_rectangle(150, 50, 200, 100, \'green\') self.create_rectangle(250, 50, 300, 100, \'blue\') def create_rectangle(self, x1, y1, x2, y2, color): rect = self.canvas.create_rectangle(x1, y1, x2, y2, fill=color, tags=\\"draggable\\") self.rectangles.append(rect) self.canvas.tag_bind(rect, \\"<ButtonPress-1>\\", self.on_start_drag) self.canvas.tag_bind(rect, \\"<ButtonRelease-1>\\", self.on_stop_drag) self.canvas.tag_bind(rect, \\"<B1-Motion>\\", self.on_drag) def on_start_drag(self, event): rectangle = self.canvas.find_withtag(\\"current\\")[0] event.widget.tag_raise(rectangle) self.drag_data = { \\"rectangle\\": rectangle, \\"x\\": event.x, \\"y\\": event.y, } dnd_start(event.widget, event) def on_stop_drag(self, event): self.drag_data = None def on_drag(self, event): if self.drag_data: dx = event.x - self.drag_data[\\"x\\"] dy = event.y - self.drag_data[\\"y\\"] self.canvas.move(self.drag_data[\\"rectangle\\"], dx, dy) self.drag_data[\\"x\\"] = event.x self.drag_data[\\"y\\"] = event.y if __name__ == \\"__main__\\": root = tk.Tk() app = RectangleDragDropApp(root) root.mainloop() ``` This example sets up the basic structure. Your task is to complete and refine the `RectangleDragDropApp` class, ensuring all mentioned functionalities work as described.","solution":"import tkinter as tk class Rectangle: def __init__(self, canvas, x1, y1, x2, y2, color): self.canvas = canvas self.id = canvas.create_rectangle(x1, y1, x2, y2, fill=color, tags=\\"draggable\\") self.color = color # Bind the object for interaction self.canvas.tag_bind(self.id, \'<ButtonPress-1>\', self.on_start_drag) self.canvas.tag_bind(self.id, \'<ButtonRelease-1>\', self.on_stop_drag) self.canvas.tag_bind(self.id, \'<B1-Motion>\', self.on_drag) self.drag_data = {\\"x\\": 0, \\"y\\": 0} def on_start_drag(self, event): self.drag_data[\\"x\\"] = event.x self.drag_data[\\"y\\"] = event.y def on_stop_drag(self, event): self.drag_data[\\"x\\"] = 0 self.drag_data[\\"y\\"] = 0 def on_drag(self, event): dx = event.x - self.drag_data[\\"x\\"] dy = event.y - self.drag_data[\\"y\\"] self.canvas.move(self.id, dx, dy) self.drag_data[\\"x\\"] = event.x self.drag_data[\\"y\\"] = event.y class RectangleDragDropApp: def __init__(self, root): self.root = root self.canvas = tk.Canvas(root, width=400, height=400, bg=\'white\') self.canvas.pack() self.rectangles = [] self.rectangles.append(Rectangle(self.canvas, 50, 50, 100, 100, \'red\')) self.rectangles.append(Rectangle(self.canvas, 150, 50, 200, 100, \'green\')) self.rectangles.append(Rectangle(self.canvas, 250, 50, 300, 100, \'blue\')) if __name__ == \\"__main__\\": root = tk.Tk() app = RectangleDragDropApp(root) root.mainloop()"},{"question":"Implementing a Custom Sequence Class Objective: Implement a custom sequence class that mimics Python\'s list behavior. This question assesses your understanding of the Sequence Protocol and basic Python object interaction. Requirements: 1. Define a class `CustomList` that mimics basic list functionalities. 2. Implement the following methods: - `__init__(self, *args)`: Constructor to initialize the list with variable number of arguments. - `__getitem__(self, index)`: Gets the item at the specified index. - `__setitem__(self, index, value)`: Sets the item at the specified index to a new value. - `__delitem__(self, index)`: Deletes the item at the specified index. - `__len__(self)`: Returns the length of the list. - `__iter__(self)`: Returns an iterator for the list. - `__contains__(self, item)`: Checks if the item exists in the list. Additional Constraints: - You should not use Python’s built-in list type or its methods (except for iteration and basic interactions such as appending to a list for internal storage or representation). Performance Requirements: - Your implementation should have optimal complexity for basic list operations (e.g., O(1) for accessing an element, O(n) for iteration). Example Usage: ```python # Create a custom list with initial elements custom_list = CustomList(1, 2, 3, 4) # Access elements print(custom_list[2]) # Output: 3 # Set elements custom_list[2] = 10 print(custom_list[2]) # Output: 10 # Delete elements del custom_list[1] print(custom_list) # Output: CustomList([1, 10, 4]) # Length of the list print(len(custom_list)) # Output: 3 # Check for containment print(10 in custom_list) # Output: True # Iteration for item in custom_list: print(item) # Output: # 1 # 10 # 4 ``` You are required to write a class `CustomList` that fulfills the above specifications and demonstrate its usage and correctness through the provided example.","solution":"class CustomList: def __init__(self, *args): self.data = list(args) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def __iter__(self): return iter(self.data) def __contains__(self, item): return item in self.data def __repr__(self): return f\\"CustomList({self.data})\\""},{"question":"# Hashing a File for Integrity Verification Objective In this task, you are required to write a Python function that uses the `hashlib` module to compute the hash of a given file. This hash will be used to verify the file\'s integrity. Function Signature ```python def compute_file_hash(filepath: str, algorithm: str = \'sha256\') -> str: pass ``` Input - `filepath` (str): The path to the file for which the hash should be computed. - `algorithm` (str, optional): The hash algorithm to use (default is \'sha256\'). Other valid options include \'md5\', \'sha1\', \'sha224\', \'sha256\', \'sha384\', \'sha512\', and \'blake2b\'. Output - Returns a string that represents the hexadecimal digest of the file\'s contents. Constraints - The function should handle file reading errors gracefully. - Only the algorithms provided in the `algorithm` parameter should be used. - The function should efficiently read large files to avoid memory issues. Examples ```python # Example 1: hash_value = compute_file_hash(\'example.txt\') print(hash_value) # Output will be a SHA-256 hash value of the contents of \'example.txt\' # Example 2: hash_value = compute_file_hash(\'example.txt\', algorithm=\'md5\') print(hash_value) # Output will be an MD5 hash value of the contents of \'example.txt\' ``` Notes - You are expected to use the `hashlib` module to create a hash object and compute the hash. - Ensure that the function is efficient and can handle large files without loading the entire file into memory.","solution":"import hashlib def compute_file_hash(filepath: str, algorithm: str = \'sha256\') -> str: Computes the hash of a given file using the specified algorithm. Parameters: - filepath (str): The path to the file for which the hash should be computed. - algorithm (str, optional): The hash algorithm to use (default is \'sha256\'). Valid options include \'md5\', \'sha1\', \'sha224\', \'sha256\', \'sha384\', \'sha512\', \'blake2b\'. Returns: - str: The hexadecimal digest of the file\'s contents. try: hash_func = hashlib.new(algorithm) except ValueError: raise ValueError(f\\"Invalid algorithm: {algorithm}\\") try: with open(filepath, \'rb\') as file: while chunk := file.read(8192): # Read in 8k chunks hash_func.update(chunk) return hash_func.hexdigest() except FileNotFoundError: raise FileNotFoundError(f\\"File {filepath} not found.\\") except IOError as e: raise IOError(f\\"Error reading file {filepath}: {e}\\")"},{"question":"Objective The objective of this exercise is to test your ability to manipulate pandas display options using the pandas options API. You will need to understand and apply functions like `get_option`, `set_option`, `reset_option`, and `option_context`. Problem Statement You have been given a pandas DataFrame with random numerical data. Your task is to implement a function `customize_dataframe_display` that takes a DataFrame as input and performs the following steps: 1. Set the maximum number of rows displayed (`display.max_rows`) to 10. 2. Set the maximum number of columns displayed (`display.max_columns`) to 7. 3. Temporarily (only within the function scope) set the floating-point display precision (`display.precision`) to 4. 4. Return the DataFrame. Additionally, implement a function `reset_default_options` that resets all display options back to their default values. Input - A pandas DataFrame `df` with random numerical data. Output - A DataFrame configured to display according to the customized options. Constraints - Your function should handle DataFrames of any shape. - Ensure the floating-point display precision setting reverts back to its original state after your function finishes execution. Example ```python import pandas as pd import numpy as np # Example DataFrame data = np.random.randn(15, 8) df = pd.DataFrame(data) # Function definitions def customize_dataframe_display(df): # Your code here pass def reset_default_options(): # Your code here pass # Usage custom_df = customize_dataframe_display(df) print(custom_df) # Reset options reset_default_options() ``` # Requirements 1. Implement the `customize_dataframe_display` function that sets the display settings as described and returns the DataFrame. 2. Implement the `reset_default_options` function to reset all display options to their default values. You can use the provided example DataFrame to test your functions. Ensure that custom settings only affect the DataFrame display within the scope of the function.","solution":"import pandas as pd def customize_dataframe_display(df): # Set max rows and columns to custom values pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 7) # Use option_context to temporarily set precision with pd.option_context(\'display.precision\', 4): return df def reset_default_options(): # Reset all options to their default values pd.reset_option(\'all\')"},{"question":"Tuple and Struct Sequence Manipulation **Objective:** Implement a set of functions to manipulate and interact with tuples and struct sequences using the Python/C API. These functions will demonstrate your understanding of constructing, resizing, and accessing elements with memory management in mind. **Task 1: Tuple Operations** 1.1. Write a function `create_and_fill_tuple` that: - Accepts a list of Python objects. - Returns a new tuple containing these objects using the `PyTuple_New` and `PyTuple_SetItem` (or corresponding macros). 1.2. Write a function `resize_tuple` that: - Accepts a tuple and a new size. - Returns the resized tuple using the `_PyTuple_Resize`. Consider memory management and constraints mentioned in the documentation. 1.3. Write a function `get_tuple_slice` that: - Accepts a tuple, along with start and end indices. - Returns a new tuple that is a slice from the original tuple using `PyTuple_GetSlice`. **Task 2: Struct Sequence Operations** 2.1. Define a new struct sequence type `Person` with the fields: `name` and `age`. 2.2. Write a function `create_person_struct_sequence` that: - Accepts a name (string) and age (integer). - Returns an instance of the `Person` struct sequence with the specified name and age using `PyStructSequence_New`. 2.3. Write a function `get_struct_sequence_field` that: - Accepts a struct sequence object and a field index. - Returns the value of that field using `PyStructSequence_GetItem`. **Constraints:** 1. For all functions, ensure proper error handling as described in the documentation. 2. Use memory management techniques correctly, particularly handle reference counting appropriately (e.g., managing borrowed and stolen references). **Expected Input and Output Formats:** 1. `create_and_fill_tuple`: - Input: [\\"a\\", 1, {}, []] - Output: (\\"a\\", 1, {}, []) 2. `resize_tuple`: - Input: (1, 2, 3, 4), 6 - Output: (1, 2, 3, 4, None, None) 3. `get_tuple_slice`: - Input: (1, 2, 3, 4, 5, 6), 1, 4 - Output: (2, 3, 4) 4. `create_person_struct_sequence`: - Input: \\"Alice\\", 30 - Output: Person(name=\\"Alice\\", age=30) 5. `get_struct_sequence_field`: - Input: Person(name=\\"Alice\\", age=30), 0 - Output: \\"Alice\\" **Performance Requirements:** - All functions should handle inputs efficiently within typical memory and CPU constraints. **Note:** Implement your solution using Python and C API interfaces. Do not use built-in high-level Python abstractions for tuple and namedtuple manipulations directly.","solution":"from collections import namedtuple # Task 1: Tuple Operations def create_and_fill_tuple(py_list): Accepts a list of Python objects and returns a tuple containing these objects. return tuple(py_list) def resize_tuple(py_tuple, new_size): Accepts a tuple and a new size, returns the resized tuple. if new_size < len(py_tuple): return py_tuple[:new_size] else: return py_tuple + (None,) * (new_size - len(py_tuple)) def get_tuple_slice(py_tuple, start, end): Accepts a tuple along with start and end indices, returns a new tuple that is a slice from the original tuple. return py_tuple[start:end] # Task 2: Struct Sequence Operations Person = namedtuple(\'Person\', [\'name\', \'age\']) def create_person_struct_sequence(name, age): Accepts a name (string) and age (integer), returns a struct sequence (namedtuple) with the specified name and age. return Person(name=name, age=age) def get_struct_sequence_field(struct_seq, index): Accepts a struct sequence object and a field index, returns the value of that field. return struct_seq[index]"},{"question":"# Clustering and Evaluation with Scikit-learn Description In this task, you are required to implement two clustering algorithms, K-Means and DBSCAN, using scikit-learn. You will then evaluate the results using silhouette score and Davies-Bouldin index, both of which are suitable for evaluating clustering quality when the ground truth labels are not known. Instructions 1. **Implement Clustering Algorithms**: - Load the provided dataset. - Implement K-Means clustering. - Implement DBSCAN clustering. 2. **Evaluation**: - Evaluate the clustering results using the silhouette score. - Evaluate the clustering results using the Davies-Bouldin index. 3. **Comparison**: - Compare the results of both algorithms based on the scores. Dataset A dataset is provided in the format `(n_samples, n_features)` and can be generated using the following code: ```python import numpy as np from sklearn.datasets import make_blobs # Generating a sample dataset X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.70, random_state=0) ``` Requirements - **Input**: The data matrix `X` of shape `(n_samples, n_features)`. - **Output**: - Two dictionaries, each containing the silhouette score and Davies-Bouldin index for K-Means and DBSCAN respectively. Constraints - Use `n_clusters=5` for the K-Means algorithm. - Use `eps=0.3` and `min_samples=10` for the DBSCAN algorithm. - The silhouette score and Davies-Bouldin index should be calculated directly from the clustering results. Performance Requirements - Ensure that the implementation is efficient and handles the provided dataset without significant performance issues. Example ```python from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, davies_bouldin_score # Generating the dataset X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.70, random_state=0) # Implementing K-Means kmeans = KMeans(n_clusters=5, random_state=0).fit(X) kmeans_labels = kmeans.labels_ kmeans_silhouette = silhouette_score(X, kmeans_labels) kmeans_davies_bouldin = davies_bouldin_score(X, kmeans_labels) # Implementing DBSCAN dbscan = DBSCAN(eps=0.3, min_samples=10).fit(X) dbscan_labels = dbscan.labels_ dbscan_silhouette = silhouette_score(X, dbscan_labels) dbscan_davies_bouldin = davies_bouldin_score(X, dbscan_labels) # Output kmeans_scores = {\'silhouette_score\': kmeans_silhouette, \'davies_bouldin_index\': kmeans_davies_bouldin} dbscan_scores = {\'silhouette_score\': dbscan_silhouette, \'davies_bouldin_index\': dbscan_davies_bouldin} print(kmeans_scores) print(dbscan_scores) ``` Submission Submit a Python script that performs the clustering and outputs the evaluation results in the specified format.","solution":"from sklearn.datasets import make_blobs from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import silhouette_score, davies_bouldin_score def clustering_and_evaluation(X): Perform K-Means and DBSCAN clustering on dataset X and evaluate using silhouette score and Davies-Bouldin index. Parameters: X (array-like): Data matrix of shape (n_samples, n_features) Returns: dict: A dictionary containing two dictionaries for K-Means and DBSCAN scores # K-Means Clustering kmeans = KMeans(n_clusters=5, random_state=0).fit(X) kmeans_labels = kmeans.labels_ kmeans_silhouette = silhouette_score(X, kmeans_labels) kmeans_davies_bouldin = davies_bouldin_score(X, kmeans_labels) kmeans_scores = { \'silhouette_score\': kmeans_silhouette, \'davies_bouldin_index\': kmeans_davies_bouldin } # DBSCAN Clustering dbscan = DBSCAN(eps=0.3, min_samples=10).fit(X) dbscan_labels = dbscan.labels_ dbscan_silhouette = silhouette_score(X, dbscan_labels) dbscan_davies_bouldin = davies_bouldin_score(X, dbscan_labels) dbscan_scores = { \'silhouette_score\': dbscan_silhouette, \'davies_bouldin_index\': dbscan_davies_bouldin } return {\'KMeans\': kmeans_scores, \'DBSCAN\': dbscan_scores} # Example usage if __name__ == \\"__main__\\": X, _ = make_blobs(n_samples=500, centers=5, cluster_std=0.70, random_state=0) scores = clustering_and_evaluation(X) print(scores)"},{"question":"Objective: Demonstrate understanding of out-of-core learning in scikit-learn by building an incremental learning pipeline. Problem Statement: You are given a text dataset that is too large to fit into memory at once. Your task is to implement an out-of-core learning pipeline to classify text documents using scikit-learn. Requirements: 1. **Data Streaming**: - Implement a function `stream_data(file_path, batch_size)` that reads the data from a given file in mini-batches. Each batch should contain `batch_size` documents. 2. **Feature Extraction**: - Use `HashingVectorizer` for transforming the text documents into feature vectors. 3. **Incremental Learning**: - Implement an incremental learning classifier using `SGDClassifier`. 4. **Performance Evaluation**: - After processing each batch, keep track of the model’s accuracy on a separate validation set. - Plot the accuracy over time. Constraints: - Use a batch size of 1000 documents. - Assume the text data file is a large CSV file with two columns: \\"text\\" and \\"label\\". Input: - `file_path`: a string representing the path to the CSV file containing the dataset. - `batch_size`: an integer representing the number of documents in each batch. Output: - A plot of the model’s accuracy over time (as each batch is processed). Function Signature: ```python def stream_data(file_path: str, batch_size: int): pass def out_of_core_learning_pipeline(file_path: str, batch_size: int): pass ``` Example: ```python file_path = \'path/to/large_text_dataset.csv\' batch_size = 1000 # Function to stream data in mini-batches def stream_data(file_path, batch_size): # Your implementation here pass # Function to implement out-of-core learning pipeline def out_of_core_learning_pipeline(file_path, batch_size): import pandas as pd from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Initialize vectorizer and classifier vectorizer = HashingVectorizer(decode_error=\'ignore\') clf = SGDClassifier() accuracy_list = [] batch = 0 for X_batch, y_batch in stream_data(file_path, batch_size): # Transform the batch of text data X_batch_transformed = vectorizer.transform(X_batch) if batch == 0: # Fit the classifier on the first batch clf.partial_fit(X_batch_transformed, y_batch, classes=np.unique(y_batch)) else: # Continue training on subsequent batches clf.partial_fit(X_batch_transformed, y_batch) # Your code to evaluate accuracy on a separate validation set # Append accuracy to accuracy_list batch += 1 # Plot accuracy over time plt.plot(range(batch), accuracy_list) plt.xlabel(\'Batch number\') plt.ylabel(\'Accuracy\') plt.title(\'Accuracy over time\') plt.show() # Call the pipeline function out_of_core_learning_pipeline(file_path, batch_size) ``` In this script, replace streaming and validation steps with actual implementations as per your environment and data specifics. **Notes:** - The main challenge is handling the streaming of data and maintaining the incremental learning process. - Ensure the implementation is robust and can handle various edge cases.","solution":"import pandas as pd import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt def stream_data(file_path, batch_size): Function to stream data in mini-batches from a CSV file with columns \'text\' and \'label\'. for chunk in pd.read_csv(file_path, chunksize=batch_size): if \'text\' in chunk.columns and \'label\' in chunk.columns: yield chunk[\'text\'].values, chunk[\'label\'].values else: raise ValueError(\\"CSV file must contain \'text\' and \'label\' columns\\") def out_of_core_learning_pipeline(file_path, batch_size): Implements an out-of-core learning pipeline to classify text documents. # Initialize vectorizer and classifier vectorizer = HashingVectorizer(decode_error=\'ignore\', n_features=2**20) clf = SGDClassifier() accuracy_list = [] batch_number = 0 # Iterate over batches of streamed data for X_batch, y_batch in stream_data(file_path, batch_size): # Transform the batch of text data into feature vectors X_batch_transformed = vectorizer.transform(X_batch) if batch_number == 0: # Fit the classifier on the first batch clf.partial_fit(X_batch_transformed, y_batch, classes=np.unique(y_batch)) else: # Continue training on subsequent batches clf.partial_fit(X_batch_transformed, y_batch) # Evaluate model using a small validation set validation_data = next(stream_data(file_path, batch_size)) X_val, y_val = validation_data X_val_transformed = vectorizer.transform(X_val) y_pred = clf.predict(X_val_transformed) accuracy = accuracy_score(y_val, y_pred) accuracy_list.append(accuracy) batch_number += 1 # Plot accuracy over time plt.plot(range(batch_number), accuracy_list) plt.xlabel(\'Batch number\') plt.ylabel(\'Accuracy\') plt.title(\'Accuracy over time\') plt.show()"},{"question":"# Question: Warning Management in Python You are tasked with writing a Python function named `custom_warning_handler()` that will utilize the `warnings` module to manage and process warnings in a specific manner. Requirements: 1. **Custom Warning Category**: - Define a custom warning category named `CustomWarning` which is a subclass of `UserWarning`. 2. **Warning Issuance and Filtering**: - Within the function `custom_warning_handler()`, issue a warning of type `CustomWarning`. - Configure the warning filter to always display `CustomWarning`. 3. **Temporary Suppression**: - Utilize the `catch_warnings` context manager to suppress warnings of type `CustomWarning` temporarily. - While warnings are suppressed, call a helper function named `emit_warning()` that will issue a `CustomWarning`. This warning should not be displayed or logged due to suppression. - After exiting the context manager, call `emit_warning()` again to ensure the warning is displayed. Functional Specifications: - **Function Name**: `custom_warning_handler` - **Input**: None - **Output**: - Return a list of warnings captured during the execution. Each entry in the list should be a string representation of the warning message. ```python import warnings # Define a custom warning category class CustomWarning(UserWarning): pass def emit_warning(): warnings.warn(\\"This is a custom warning\\", CustomWarning) def custom_warning_handler(): captured_warnings = [] # Filter to always show CustomWarning warnings.simplefilter(\\"always\\", CustomWarning) # Issue a CustomWarning and capture it with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\", CustomWarning) emit_warning() # Capture warnings issued within context manager for warning in w: captured_warnings.append(str(warning.message)) # Temporarily suppress CustomWarning with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\", CustomWarning) emit_warning() # This warning should not be captured # Issue another CustomWarning and capture it with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\", CustomWarning) emit_warning() # Capture warnings issued within context manager for warning in w: captured_warnings.append(str(warning.message)) return captured_warnings # Example usage: # warnings_list = custom_warning_handler() # print(warnings_list) ``` Constraints: - Do not change the global warning configuration outside the function. - Ensure the function does not produce any output other than the return value. Evaluation Criteria: - Correct definition and use of the custom warning category. - Proper issuance and capturing of warnings. - Accurate implementation of temporary suppression using context manager. - Adherence to the specified input and output formats.","solution":"import warnings # Define a custom warning category class CustomWarning(UserWarning): pass def emit_warning(): warnings.warn(\\"This is a custom warning\\", CustomWarning) def custom_warning_handler(): captured_warnings = [] # Filter to always show CustomWarning warnings.simplefilter(\\"always\\", CustomWarning) # Issue a CustomWarning and capture it with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\", CustomWarning) emit_warning() # Capture warnings issued within context manager for warning in w: captured_warnings.append(str(warning.message)) # Temporarily suppress CustomWarning with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\", CustomWarning) emit_warning() # This warning should not be captured # Issue another CustomWarning and capture it with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\", CustomWarning) emit_warning() # Capture warnings issued within context manager for warning in w: captured_warnings.append(str(warning.message)) return captured_warnings"},{"question":"<|Analysis Begin|> The provided documentation is about the \\"enum\\" module in Python, which supports enumerations, a distinct data type consisting of a set of named values. The module includes classes like `Enum`, `IntEnum`, `Flag`, and `IntFlag`, along with the decorator `unique()` and helper `auto`. The documentation covers how to create and use these enumerations, including defining enums, iterating over them, combining flags, ensuring uniqueness, and using automatic values. Key points include: - Enum members have unique, constant values and can be compared by identity. - `IntEnum` and `IntFlag` are variations for integer-based enumerations and bitwise-combinable flags, respectively. - Customizing enum behaviors by defining methods such as `_generate_next_value_()` and using decorators like `@unique`. - Enum members and attributes are accessed programmatically, and can support methods, special methods, and properties. The feature set of `enum` includes defining enum members with custom values, auto values, iteration, membership testing, comparisons, pickling/unpickling, custom methods, and restricting subclassing. <|Analysis End|> <|Question Begin|> # Advanced Coding Assessment Question Enumeration Combinations with Constraints **Objective:** Write a Python class using the `enum` module that defines a combinable enumeration for a set of user permissions. You will demonstrate the ability to create bitwise-combinable flags, ensure unique values, and implement custom methods. **Problem Statement:** 1. Define an enumeration class `Permission` using `IntFlag` from the `enum` module. The members should include `READ`, `WRITE`, `EXECUTE`, and `ADMIN`, with automatic values. 2. Ensure that no two permissions can have the same value by using the `@unique` decorator. 3. Implement a method `has_permission` within the `Permission` class to check whether a given set of permissions includes a specific permission. 4. Create a method `describe` that returns a string representation of all permissions set in a given `Permission` instance. 5. Implement the `__str__` method to provide a human-readable string representation of the permission set. **Constraints:** - Use `IntFlag` to make the permissions combinable with bitwise operators. - Utilize `auto()` to automatically assign values to the permissions. - Ensure unique permission values using the `@unique` decorator. **Input and Output:** - **Input:** An instance of `Permission` and individual permissions to check. - **Output:** Boolean values for `has_permission` and string representations for `describe` and `__str__`. **Example Usage:** ```python from enum import IntFlag, auto, unique @unique class Permission(IntFlag): READ = auto() WRITE = auto() EXECUTE = auto() ADMIN = auto() def has_permission(self, perm): return (self & perm) == perm def describe(self): return \', \'.join([perm.name for perm in Permission if perm in self]) def __str__(self): return self.describe() # Test Cases perms = Permission.READ | Permission.WRITE print(perms.describe()) # Output: \\"READ, WRITE\\" print(perms.has_permission(Permission.EXECUTE)) # Output: False print(perms.has_permission(Permission.READ)) # Output: True print(str(perms)) # Output: \\"READ, WRITE\\" ``` In this task, implement the `Permission` class and verify the correctness of your methods using the provided test cases.","solution":"from enum import IntFlag, auto, unique @unique class Permission(IntFlag): READ = auto() WRITE = auto() EXECUTE = auto() ADMIN = auto() def has_permission(self, perm): Checks if the current set of permissions includes the specified permission. return (self & perm) == perm def describe(self): Returns a comma-separated string of all permissions present in the current set. return \', \'.join([perm.name for perm in Permission if perm in self]) def __str__(self): Provides a human-readable string representation of the permission set. return self.describe()"},{"question":"# Pandas Coding Assessment Question Objective Demonstrate understanding and practical skills with the Copy-on-Write (CoW) mechanism introduced in pandas, including proper handling of indexing, assignments, and avoiding unintended side effects. Question: You are given a dataset as a pandas DataFrame. Implement a function `modify_dataframe()` that performs the following operations in compliance with the Copy-on-Write (CoW) principles: 1. Return a new DataFrame that has an additional column named `\\"sum\\"` which is the sum of columns `\\"a\\"` and `\\"b\\"`. 2. Replace all negative values in the newly added `\\"sum\\"` column with `0`, but ensure that no unintended copies are made and the original DataFrame remains unchanged. 3. Create another column named `\\"mean\\"` that applies the mean of columns `\\"a\\"`, `\\"b\\"`, and `\\"sum\\"` for each row. 4. Ensure any operations that might share data between the original and new DataFrame avoid any side-effect issues. Use the following template to complete your implementation: ```python import pandas as pd def modify_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Add a new column \\"sum\\" result_df = df.copy() result_df[\\"sum\\"] = result_df[\\"a\\"] + result_df[\\"b\\"] # Step 2: Replace all negative values in \\"sum\\" with 0 result_df[\\"sum\\"] = result_df[\\"sum\\"].mask(result_df[\\"sum\\"] < 0, 0) # Step 3: Add \\"mean\\" column result_df[\\"mean\\"] = result_df[[\\"a\\", \\"b\\", \\"sum\\"]].mean(axis=1) return result_df # Sample DataFrame data = { \\"a\\": [1, 2, -3], \\"b\\": [4, -5, 6] } df = pd.DataFrame(data) # Function call new_df = modify_dataframe(df) print(new_df) print(df) # Ensure the original DataFrame is unchanged ``` Input Format: - A pandas DataFrame `df` with at least columns `\\"a\\"` and `\\"b\\"`. (Values can be integers or floats.) Output Format: - Return a new modified DataFrame that includes the newly added columns `\\"sum\\"` and `\\"mean\\"` as per the specifications above. Constraints: - Do not use chained assignments. - Ensure the original DataFrame `df` remains unchanged. - Utilize appropriate methods to adhere to the Copy-on-Write (CoW) principles. Performance Requirements: - Ensure no unnecessary copies are made to maintain performance. **Hint**: Use pandas methods to avoid directly manipulating NumPy arrays shared with the DataFrame to comply with CoW rules.","solution":"import pandas as pd def modify_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Add a new column \\"sum\\" result_df = df.copy() result_df[\\"sum\\"] = result_df[\\"a\\"] + result_df[\\"b\\"] # Step 2: Replace all negative values in \\"sum\\" with 0 result_df[\\"sum\\"] = result_df[\\"sum\\"].mask(result_df[\\"sum\\"] < 0, 0) # Step 3: Add \\"mean\\" column result_df[\\"mean\\"] = result_df[[\\"a\\", \\"b\\", \\"sum\\"]].mean(axis=1) return result_df # Sample DataFrame data = { \\"a\\": [1, 2, -3], \\"b\\": [4, -5, 6] } df = pd.DataFrame(data) # Function call new_df = modify_dataframe(df) print(new_df) print(df) # Ensure the original DataFrame is unchanged"},{"question":"**Title: Implement Custom Browser Controller and URL Opener** **Objective:** The purpose of this task is to assess your understanding of the `webbrowser` module in Python and your ability to extend its functionality. **Problem Statement:** You are tasked with implementing a custom browser controller using the `webbrowser` module. Your custom controller should follow the behavior: 1. First, attempt to open the URL in Google Chrome. 2. If Google Chrome is not available, attempt to open the URL in Mozilla Firefox. 3. If neither browser is available, raise a `webbrowser.Error` exception. Additionally, implement a function `open_url_custom(url)` that uses this custom browser controller to open the provided URL. **Function Specifications:** - **Function name:** `open_url_custom(url: str) -> bool` - Inputs: - `url` (str): The URL to be opened. - Returns: - `bool`: True if the URL was successfully opened, False otherwise. - Raises: - `webbrowser.Error`: If neither Chrome nor Firefox are available. **Constraints:** - You are only allowed to use the `webbrowser` module and the standard library. - The implementation should work on Unix, Windows, and macOS platforms. **Example Usage:** ```python try: success = open_url_custom(\\"https://www.example.com\\") if success: print(\\"URL opened successfully\\") else: print(\\"Failed to open URL\\") except webbrowser.Error as e: print(f\\"Error occurred: {e}\\") ``` **Additional Information:** You are allowed to define helper functions or classes if needed. **Evaluation Criteria:** - Correctness of implementation. - Handling of different platforms/browser availability. - Clean and readable code. Start by reading the `webbrowser` module documentation provided and implement the solution accordingly.","solution":"import webbrowser import os def open_url_custom(url: str) -> bool: Attempts to open the URL in Google Chrome. If Google Chrome is not available, attempts to open it in Mozilla Firefox. If neither browser is available, raises a webbrowser.Error. :param url: The URL to be opened :return: True if the URL was successfully opened, False otherwise. :raises: webbrowser.Error if neither Chrome nor Firefox are available. try: # Attempt to use Google Chrome chrome = webbrowser.get(using=\'google-chrome\') chrome.open(url) return True except webbrowser.Error: pass try: # Attempt to use Mozilla Firefox firefox = webbrowser.get(using=\'firefox\') firefox.open(url) return True except webbrowser.Error: pass # If neither browser is available, raise an error raise webbrowser.Error(\\"Neither Google Chrome nor Mozilla Firefox is available\\") # For testing purposes: Ensure correct browser names are used based on the platform def _get_chrome_name(): if os.name == \\"posix\\": return \'google-chrome\' elif os.name == \'nt\': # Windows return \'chrome\' elif os.name == \'mac\': # macOS return \'open -a /Applications/Google Chrome.app\' return \'unknown\' def _get_firefox_name(): if os.name == \\"posix\\": return \'firefox\' elif os.name == \'nt\': # Windows return \'firefox\' elif os.name == \'mac\': # macOS return \'open -a /Applications/Firefox.app\' return \'unknown\'"},{"question":"**Question: Implement and Utilize Python Iterator Objects** In this task, you are required to implement two types of iterators in Python: a sequence iterator and a callable iterator. You will then use these iterators in various scenarios to demonstrate your understanding of their functionality. # Part 1: Sequence Iterator Implement a custom class `SequenceIterator` that mimics the behavior of `PySeqIter_Type`. This iterator will: - Take a sequence (list, string, etc.) as input. - Provide a method to iterate over the sequence. **Constructor:** ```python def __init__(self, sequence): Initialize SequenceIterator with a sequence. :param sequence: A sequence that supports __getitem__() method. ``` **Methods:** ```python def __iter__(self): Returns the iterator object itself. :return: self def __next__(self): Returns the next item from the sequence. :return: Next item in the sequence. :raises StopIteration: When the end of the sequence is reached. ``` # Part 2: Callable Iterator Implement a custom class `CallableIterator` that mimics the behavior of `PyCallIter_Type`. This iterator will: - Take a callable (function) and a sentinel value as input. - Call the callable until it returns the sentinel value. **Constructor:** ```python def __init__(self, callable, sentinel): Initialize CallableIterator with a callable and sentinel value. :param callable: A callable object that returns the next item. :param sentinel: A value indicating the end of the iteration. ``` **Methods:** ```python def __iter__(self): Returns the iterator object itself. :return: self def __next__(self): Returns the next item by calling the callable. :return: Next item returned by the callable. :raises StopIteration: When callable returns the sentinel value. ``` # Part 3: Utilizing the Iterators Write a function `process_iterators(sequences, callable, sentinel)` that: 1. Takes a list of sequences and applies the `SequenceIterator` to each sequence. 2. Takes a callable and a sentinel value, and applies the `CallableIterator` to them. 3. Collects all output values from each iterator in their respective order and returns them as a list. **Function:** ```python def process_iterators(sequences, callable, sentinel): Process a list of sequences and a callable with a sentinel value. :param sequences: List of sequences to iterate over. :param callable: A callable object for CallableIterator. :param sentinel: Sentinel value for CallableIterator. :return: List of all values produced by the iterators. ``` # Example: ```python # Example callable def generate_numbers(): yield 1 yield 2 yield 3 yield \'done\' yield 4 # Example usage sequences = [[1, 2, 3], \\"hello\\"] callable = lambda nums=generate_numbers(): next(nums) sentinel = \'done\' result = process_iterators(sequences, callable, sentinel) print(result) # Output: [1, 2, 3, \'h\', \'e\', \'l\', \'l\', \'o\', 1, 2, 3] ``` **Constraints:** - The sequence in `SequenceIterator` will be provided as a list or string. - The callable in `CallableIterator` returns the sentinel value to indicate the end of iteration. - Each sequence and the callable will produce no more than 100 elements. **Performance Requirements:** - The solution should be efficient with a time complexity that is linear with respect to the total number of elements produced.","solution":"class SequenceIterator: def __init__(self, sequence): Initialize SequenceIterator with a sequence. :param sequence: A sequence that supports __getitem__() method. self.sequence = sequence self.index = 0 def __iter__(self): Returns the iterator object itself. :return: self return self def __next__(self): Returns the next item from the sequence. :return: Next item in the sequence. :raises StopIteration: When the end of the sequence is reached. if self.index < len(self.sequence): item = self.sequence[self.index] self.index += 1 return item else: raise StopIteration class CallableIterator: def __init__(self, callable, sentinel): Initialize CallableIterator with a callable and sentinel value. :param callable: A callable object that returns the next item. :param sentinel: A value indicating the end of the iteration. self.callable = callable self.sentinel = sentinel def __iter__(self): Returns the iterator object itself. :return: self return self def __next__(self): Returns the next item by calling the callable. :return: Next item returned by the callable. :raises StopIteration: When callable returns the sentinel value. result = self.callable() if result == self.sentinel: raise StopIteration return result def process_iterators(sequences, callable, sentinel): Process a list of sequences and a callable with a sentinel value. :param sequences: List of sequences to iterate over. :param callable: A callable object for CallableIterator. :param sentinel: Sentinel value for CallableIterator. :return: List of all values produced by the iterators. result = [] for sequence in sequences: seq_iter = SequenceIterator(sequence) for item in seq_iter: result.append(item) call_iter = CallableIterator(callable, sentinel) for item in call_iter: result.append(item) return result"},{"question":"# URL Utility Functions Given the comprehensive functionality available in the `urllib.parse` module, write a function `analyze_url(url: str) -> dict` that accepts a URL string and returns a dictionary containing detailed analysis of the URL. Your function should utilize `urllib.parse` methods to extract and analyze the URL components. **Requirements**: 1. Parse the URL using `urlparse()`. 2. Extract the following components: scheme, netloc, path, params, query, fragment, username, password, hostname, port. 3. If the URL contains a query string, parse it using `parse_qs()` and include the parsed query in the analysis. 4. If the URL contains any percent-encoded components, decode them using appropriate unquote functions. 5. Ensure that the dictionary includes all the extracted components, parsed query parameters, and decoded components wherever applicable. **Function Signature**: ```python def analyze_url(url: str) -> dict: pass ``` **Input**: - `url`: A valid URL string. **Output**: - A dictionary with the following keys: - `\'scheme\'`: The URL scheme specifier. - `\'netloc\'`: The network location component. - `\'path\'`: The hierarchical path. - `\'params\'`: The parameters for the last path element. - `\'query\'`: The query component. - `\'fragment\'`: The fragment identifier. - `\'username\'`: The username component. - `\'password\'`: The password component. - `\'hostname\'`: The hostname component. - `\'port\'`: The port number as an integer. - `\'parsed_query\'`: The parsed query parameters (as a dictionary). - `\'decoded_path\'`: The decoded path (if encoded). - `\'decoded_query\'`: The decoded query string. - `\'decoded_fragment\'`: The decoded fragment identifier. **Example**: ```python url = \\"http://username:password@hostname:80/path;params?query=val#fragment\\" result = analyze_url(url) # Example Output # { # \'scheme\': \'http\', # \'netloc\': \'username:password@hostname:80\', # \'path\': \'/path\', # \'params\': \'params\', # \'query\': \'query=val\', # \'fragment\': \'fragment\', # \'username\': \'username\', # \'password\': \'password\', # \'hostname\': \'hostname\', # \'port\': 80, # \'parsed_query\': {\'query\': [\'val\']}, # \'decoded_path\': \'/path\', # \'decoded_query\': \'query=val\', # \'decoded_fragment\': \'fragment\' # } ``` **Constraints**: - Assume the input URL string is well-formed and valid. - Proper exception handling should be incorporated where necessary to ensure the function does not break on unexpected input.","solution":"from urllib.parse import urlparse, parse_qs, unquote def analyze_url(url: str) -> dict: parsed_url = urlparse(url) parsed_query = parse_qs(parsed_url.query) decoded_path = unquote(parsed_url.path) decoded_query = unquote(parsed_url.query) decoded_fragment = unquote(parsed_url.fragment) return { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment, \'username\': parsed_url.username, \'password\': parsed_url.password, \'hostname\': parsed_url.hostname, \'port\': parsed_url.port, \'parsed_query\': parsed_query, \'decoded_path\': decoded_path, \'decoded_query\': decoded_query, \'decoded_fragment\': decoded_fragment }"},{"question":"# Advanced Coding Assessment: Constructing a Complex MIME Email As part of your work as a software developer, you need to send out daily reports that include a text summary, an image chart, and an audio briefing. Additionally, these reports should be sent as MIME-compliant emails that email clients can easily interpret and display correctly. **Task:** Write a Python function named `create_mime_email` that constructs a MIME email containing: 1. A plain text summary. 2. An image in PNG format. 3. An MP3 audio briefing. The function should take the following parameters: - `text_summary` (string): The plain text summary of the report. - `image_data` (bytes): The byte content of the PNG image. - `audio_data` (bytes): The byte content of the MP3 audio. The function should return the MIME email as a string. **Function Signature:** ```python import email from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio def create_mime_email(text_summary: str, image_data: bytes, audio_data: bytes) -> str: pass ``` **Requirements:** 1. The email should be a `MIMEMultipart` message. 2. The text summary should be added using `MIMEText`. 3. The image should be added using `MIMEImage` with appropriate headers. 4. The audio should be added using `MIMEAudio` with appropriate headers. 5. The email should be properly encoded for transport. You should also include error handling to ensure that: - The `image_data` is in valid PNG format. - The `audio_data` is in valid MP3 format. **Example Usage:** ```python text_summary = \\"Here is your daily report.\\" with open(\'chart.png\', \'rb\') as img_file: image_data = img_file.read() with open(\'briefing.mp3\', \'rb\') as audio_file: audio_data = audio_file.read() mime_email = create_mime_email(text_summary, image_data, audio_data) print(mime_email) ``` *Note: For the purpose of this task, you can assume the `chart.png` and `briefing.mp3` are valid PNG and MP3 files respectively.* **Constraints:** 1. The function should work efficiently with the provided data types. 2. Ensure compatibility with Python 3.6+. **Grading Criteria:** - Correct implementation of MIME classes. - Proper error handling and validation. - Clear and maintainable code.","solution":"import email from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio def create_mime_email(text_summary: str, image_data: bytes, audio_data: bytes) -> str: # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = \'Daily Report\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' # Add the text summary text_part = MIMEText(text_summary, \'plain\') msg.attach(text_part) # Add the image (assuming it\'s in PNG format) try: image_part = MIMEImage(image_data, _subtype=\'png\') image_part.add_header(\'Content-Disposition\', \'attachment\', filename=\\"chart.png\\") msg.attach(image_part) except Exception as e: print(f\\"Error attaching image: {e}\\") raise ValueError(\\"Invalid PNG image data\\") # Add the audio (assuming it\'s in MP3 format) try: audio_part = MIMEAudio(audio_data, _subtype=\'mp3\') audio_part.add_header(\'Content-Disposition\', \'attachment\', filename=\\"briefing.mp3\\") msg.attach(audio_part) except Exception as e: print(f\\"Error attaching audio: {e}\\") raise ValueError(\\"Invalid MP3 audio data\\") # Encode the email in string format return msg.as_string()"},{"question":"# Question: Log File Analysis with Regular Expressions You are tasked to analyze a log file from a server application to extract useful information. The log file contains entries in the following format: ``` [DATE TIME] [LOG_LEVEL] [SOURCE]: MESSAGE ``` For example: ``` [2023-07-15 10:43:21] [INFO] [Auth]: User login succeeded for user \'john_doe\' [2023-07-15 10:44:05] [ERROR] [Database]: Connection failed to database \'user_db\' [2023-07-15 10:45:11] [WARNING] [Disk]: Disk space low on \'/var/log\' ``` Your goal is to: 1. Extract and print all error messages with their timestamps. 2. Identify all unique log sources and their frequencies. 3. Identify all the timestamps when a specific user (`john_doe`) performed any action based on the logs. Implement a function named `analyze_log` that satisfies the following requirements: - **Input:** A list of log entries (strings). - **Output:** A dictionary with three keys: - \\"errors\\": A list of tuples containing the timestamp and error message. - \\"sources\\": A dictionary with log source names as keys and their frequencies as values. - \\"actions\\": A list of timestamps when the specified user (`john_doe`) performed an action. # Constraints 1. The log entries conform to the specified format. 2. Log levels can be \\"INFO\\", \\"ERROR\\", \\"WARNING\\", or \\"DEBUG\\". 3. Timestamps are in the format \\"YYYY-MM-DD HH:MM:SS\\". 4. Ensure the function handles any volume of log entries efficiently. # Example Input: ```python log_entries = [ \\"[2023-07-15 10:43:21] [INFO] [Auth]: User login succeeded for user \'john_doe\'\\", \\"[2023-07-15 10:44:05] [ERROR] [Database]: Connection failed to database \'user_db\'\\", \\"[2023-07-15 10:45:11] [WARNING] [Disk]: Disk space low on \'/var/log\'\\", \\"[2023-07-15 10:46:17] [INFO] [Auth]: User \'john_doe\' logged out\\" ] ``` Output: ```python { \\"errors\\": [(\\"2023-07-15 10:44:05\\", \\"Connection failed to database \'user_db\'\\")], \\"sources\\": {\\"Auth\\": 2, \\"Database\\": 1, \\"Disk\\": 1}, \\"actions\\": [\\"2023-07-15 10:43:21\\", \\"2023-07-15 10:46:17\\"] } ``` # Solution Template ```python import re from collections import defaultdict def analyze_log(log_entries): error_pattern = re.compile(r\\"^[(.*?)] [ERROR] [.*?]: (.*)\\") source_pattern = re.compile(r\\"[.*?] [.*?] [(.*?)]:\\") action_pattern = re.compile(r\\"^[(.*?)] [.*?] [.*?]: .*?\'john_doe\'.*?\\") errors = [] sources = defaultdict(int) actions = [] for entry in log_entries: error_match = error_pattern.match(entry) if error_match: timestamp, message = error_match.groups() errors.append((timestamp, message)) source_match = source_pattern.search(entry) if source_match: source = source_match.group(1) sources[source] += 1 action_match = action_pattern.match(entry) if action_match: timestamp = action_match.group(1) actions.append(timestamp) return { \\"errors\\": errors, \\"sources\\": dict(sources), \\"actions\\": actions } ```","solution":"import re from collections import defaultdict def analyze_log(log_entries): error_pattern = re.compile(r\\"^[(.*?)] [ERROR] [.*?]: (.*)\\") source_pattern = re.compile(r\\"[.*?] [.*?] [(.*?)]:\\") action_pattern = re.compile(r\\"^[(.*?)] [.*?] [.*?]: .*?\'john_doe\'.*?\\") errors = [] sources = defaultdict(int) actions = [] for entry in log_entries: error_match = error_pattern.match(entry) if error_match: timestamp, message = error_match.groups() errors.append((timestamp, message)) source_match = source_pattern.search(entry) if source_match: source = source_match.group(1) sources[source] += 1 action_match = action_pattern.match(entry) if action_match: timestamp = action_match.group(1) actions.append(timestamp) return { \\"errors\\": errors, \\"sources\\": dict(sources), \\"actions\\": actions }"},{"question":"**Gaussian Mixture Models and Variational Bayesian Gaussian Mixture Models Implementation** **Objective:** Implement and compare Gaussian Mixture Models (GMM) and Variational Bayesian Gaussian Mixture Models (VBGMM) on a provided dataset. Analyze the performance and selection of the number of components for each model using the Bayesian Information Criterion (BIC). **Instructions:** 1. Load the provided dataset `data.csv` containing 2-dimensional data points. 2. Implement the following steps using scikit-learn\'s `GaussianMixture` and `BayesianGaussianMixture`: - Fit a Gaussian Mixture Model (GMM) with varying number of components (from 1 to 10). - Fit a Variational Bayesian Gaussian Mixture Model (VBGMM) with an appropriate upper bound on the number of components. 3. For each number of components, compute the Bayesian Information Criterion (BIC) for the GMM. 4. Identify the optimal number of components for the GMM based on the BIC. 5. Plot the BIC values against the number of components for GMM. 6. Visualize the clustering results for both GMM and VBGMM on the 2D data. 7. Compare and contrast the results and discuss the advantages and disadvantages of each approach. **Constraints:** - Use scikit-learn\'s `GaussianMixture` and `BayesianGaussianMixture` classes. - Ensure the code is well-commented and follows PEP 8 guidelines. **Expected Input and Output:** - Input: A CSV file `data.csv` with 2-dimensional data points. - Output: - A plot showing BIC values vs. number of components for GMM. - Scatter plots showing clustering results for both GMM and VBGMM. - A written analysis comparing the two models. ```python # Sample code structure import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture # Load the dataset data = pd.read_csv(\'data.csv\') # Extract data points X = data[[\'feature1\', \'feature2\']].values # Part 1: Fit Gaussian Mixture Model (GMM) with varying number of components n_components = np.arange(1, 11) gmm_bics = [] for n in n_components: gmm = GaussianMixture(n_components=n, random_state=42) gmm.fit(X) gmm_bics.append(gmm.bic(X)) # Part 2: Fit Variational Bayesian Gaussian Mixture Model (VBGMM) vbgmm = BayesianGaussianMixture(n_components=10, random_state=42) vbgmm.fit(X) # Part 3: Identify optimal number of components for GMM optimal_n = n_components[np.argmin(gmm_bics)] print(f\'Optimal number of components for GMM based on BIC: {optimal_n}\') # Part 4: Plot BIC values plt.plot(n_components, gmm_bics, marker=\'o\') plt.xlabel(\'Number of components\') plt.ylabel(\'BIC\') plt.title(\'BIC vs Number of components for GMM\') plt.show() # Part 5: Visualize clustering results def plot_clusters(model, X, title): y_pred = model.predict(X) plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=40, cmap=\'viridis\') plt.title(title) plt.show() plot_clusters(GaussianMixture(n_components=optimal_n).fit(X), X, \'GMM Clustering\') plot_clusters(vbgmm, X, \'VBGMM Clustering\') # Part 6: Compare and contrast the models # **Describe your observations and conclusions here** ``` **Performance Requirements:** - The code should handle data files with up to 10000 data points efficiently. - Visualization should provide clear and distinguishable clusters. **Submission:** - Submit a Jupyter notebook or a Python script file (.ipynb or .py) with implemented code and analysis.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture def load_data(file_path): return pd.read_csv(file_path) def fit_gmm_bic(X, n_components_range): bics = [] for n in n_components_range: gmm = GaussianMixture(n_components=n, random_state=42) gmm.fit(X) bics.append(gmm.bic(X)) return bics def fit_vbgmm(X, max_components): vbgmm = BayesianGaussianMixture(n_components=max_components, random_state=42) vbgmm.fit(X) return vbgmm def plot_bic_vs_components(n_components, bics): plt.plot(n_components, bics, marker=\'o\') plt.xlabel(\'Number of components\') plt.ylabel(\'BIC\') plt.title(\'BIC vs Number of components for GMM\') plt.show() def plot_clusters(model, X, title): y_pred = model.predict(X) plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=40, cmap=\'viridis\') plt.title(title) plt.show() def main(file_path): data = load_data(file_path) X = data.values n_components_range = np.arange(1, 11) bics = fit_gmm_bic(X, n_components_range) optimal_n = n_components_range[np.argmin(bics)] print(f\'Optimal number of components for GMM based on BIC: {optimal_n}\') plot_bic_vs_components(n_components_range, bics) gmm = GaussianMixture(n_components=optimal_n, random_state=42).fit(X) plot_clusters(gmm, X, \'GMM Clustering\') vbgmm = fit_vbgmm(X, max_components=10) plot_clusters(vbgmm, X, \'VBGMM Clustering\') if __name__ == \'__main__\': main(\'data.csv\')"},{"question":"# UUID Generation and Validation Task Objective Write a Python function that generates a UUID of a specific version and validates its attributes. Function Signature ```python def generate_and_validate_uuid(version: int, name: str = \'\', namespace: uuid.UUID = None) -> str: pass ``` Input - `version` (int): An integer value of 1, 3, 4 or 5 specifying the UUID generation method. - `name` (str, optional): A string for versions 3 and 5 UUID generation. Default is an empty string. - `namespace` (uuid.UUID, optional): A UUID for versions 3 and 5 UUID generation. Default is `None`. Output - A string representation of the generated UUID. Constraints - For `version 1`: `name` and `namespace` should not be provided (should be ignored). - For `version 3` and `version 5`: Both `name` and `namespace` must be provided. - For `version 4`: `name` and `namespace` should not be provided (should be ignored). Performance Requirements - Efficiently handle standard input sizes. Task Details 1. The function should generate a UUID based on the given version: - For version 1: Use `uuid.uuid1()`. - For version 3: Use `uuid.uuid3(namespace, name)`. - For version 4: Use `uuid.uuid4()`. - For version 5: Use `uuid.uuid5(namespace, name)`. 2. After generating the UUID, validate its major attributes: - Ensure that the version attribute matches the input version. - Check the variant attribute to confirm it adheres to the RFC 4122 standard. 3. Return the string representation of the generated UUID. Example ```python import uuid # Suppose these are sample namespace UUIDs namespace_dns = uuid.uuid4() # Generating and validating version 1 UUID uuid_v1 = generate_and_validate_uuid(1) assert uuid.UUID(uuid_v1).version == 1 assert uuid.UUID(uuid_v1).variant == uuid.RFC_4122 print(uuid_v1) # Generating and validating version 3 UUID using a namespace and name uuid_v3 = generate_and_validate_uuid(3, \'example.name\', namespace_dns) assert uuid.UUID(uuid_v3).version == 3 assert uuid.UUID(uuid_v3).variant == uuid.RFC_4122 print(uuid_v3) # Generating and validating version 4 UUID uuid_v4 = generate_and_validate_uuid(4) assert uuid.UUID(uuid_v4).version == 4 assert uuid.UUID(uuid_v4).variant == uuid.RFC_4122 print(uuid_v4) # Generating and validating version 5 UUID using a namespace and name uuid_v5 = generate_and_validate_uuid(5, \'example.anothername\', namespace_dns) assert uuid.UUID(uuid_v5).version == 5 assert uuid.UUID(uuid_v5).variant == uuid.RFC_4122 print(uuid_v5) ``` # Notes - Use the `uuid` module for UUID functions and validations. - Ensure your code handles invalid inputs gracefully (e.g., missing required parameters for specific versions).","solution":"import uuid def generate_and_validate_uuid(version: int, name: str = \'\', namespace: uuid.UUID = None) -> str: Generates a UUID of the specified version and performs basic validation. Args: version (int): The version of the UUID to generate (1, 3, 4 or 5). name (str): The name for generating version 3 or 5 UUID (optional for these versions). namespace (uuid.UUID): The namespace UUID for generating version 3 or 5 UUID (optional for these versions). Returns: str: The generated UUID as a string. if version == 1: generated_uuid = uuid.uuid1() elif version == 3: if not name or not namespace: raise ValueError(\\"Both name and namespace must be provided for UUID version 3.\\") generated_uuid = uuid.uuid3(namespace, name) elif version == 4: generated_uuid = uuid.uuid4() elif version == 5: if not name or not namespace: raise ValueError(\\"Both name and namespace must be provided for UUID version 5.\\") generated_uuid = uuid.uuid5(namespace, name) else: raise ValueError(\\"Invalid UUID version specified. Only versions 1, 3, 4, and 5 are supported.\\") # Validate the generated UUID if generated_uuid.version != version: raise AssertionError(f\\"Generated UUID version does not match the specified version {version}.\\") if generated_uuid.variant != uuid.RFC_4122: raise AssertionError(\\"Generated UUID does not conform to RFC 4122 standard.\\") return str(generated_uuid)"},{"question":"# Python Code Analysis with AST Module **Problem Statement:** You are tasked with writing a function that performs static analysis on a given piece of Python code. The function should identify all the function definitions within the code and return a list of dictionaries. Each dictionary should contain the following details about the function: - `name`: The name of the function. - `arguments`: A list of arguments that the function takes. - `returns`: The type annotation of the return value if specified, otherwise `None`. - `async`: A boolean indicating whether the function is asynchronous. **Function Signature:** ```python def analyze_code(code: str) -> list: pass ``` **Input:** - `code` (str): A string containing valid Python source code. **Output:** - A list of dictionaries where each dictionary contains information about a function definition. **Example:** ```python code = \'\'\' def add(a: int, b: int) -> int: return a + b async def fetch_data(url: str) -> dict: pass def no_annotation(c, d): return c + d \'\'\' result = analyze_code(code) print(result) ``` **Expected Output:** ```python [ { \\"name\\": \\"add\\", \\"arguments\\": [\\"a\\", \\"b\\"], \\"returns\\": \\"int\\", \\"async\\": False }, { \\"name\\": \\"fetch_data\\", \\"arguments\\": [\\"url\\"], \\"returns\\": \\"dict\\", \\"async\\": True }, { \\"name\\": \\"no_annotation\\", \\"arguments\\": [\\"c\\", \\"d\\"], \\"returns\\": None, \\"async\\": False } ] ``` **Constraints:** - The input code will always be a valid Python code. - Function arguments might or might not have type annotations. - Function return values might or might not have type annotations. - Consider both synchronous and asynchronous functions. **Notes:** - You should use the `ast` module to parse and analyze the code. - Assume that the code does not contain nested function definitions. - The focus is to analyze top-level function definitions. # Guidelines: 1. Parse the input code using the `ast` module. 2. Traverse the parsed AST to find all function definitions. 3. Extract the required details from each function definition. 4. Construct the list of dictionaries with the extracted details. 5. Return the list as the output.","solution":"import ast def analyze_code(code: str) -> list: Analyzes the given Python code and extracts details about all top-level function definitions. Args: code (str): A string containing valid Python source code. Returns: list: A list of dictionaries each containing information about a function definition. parsed_code = ast.parse(code) func_details = [] for node in ast.walk(parsed_code): if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)): func_detail = { \\"name\\": node.name, \\"arguments\\": [arg.arg for arg in node.args.args], \\"returns\\": ast.unparse(node.returns) if node.returns else None, \\"async\\": isinstance(node, ast.AsyncFunctionDef) } func_details.append(func_detail) return func_details"},{"question":"You are tasked with analyzing a dataset of sales transactions. You need to perform various operations to clean, manipulate, and analyze the data using pandas. # Dataset You are provided with two CSV files: 1. `transactions.csv` 2. `products.csv` `transactions.csv` contains: - `transaction_id` (int): Unique identifier for each transaction - `product_id` (int): Identifier for the product sold - `quantity` (int): Quantity of the product sold - `transaction_date` (str): Date of transaction in \'YYYY-MM-DD\' format - `price_per_unit` (float): Price per unit of product sold `products.csv` contains: - `product_id` (int): Unique identifier for each product - `product_name` (str): Name of the product - `category` (str): Category of the product # Task 1. Read both `transactions.csv` and `products.csv` into pandas DataFrames. 2. Merge these DataFrames on `product_id` to create a new DataFrame that includes all the columns from both original DataFrames. 3. Handle any missing data by: - Dropping rows where `product_id` is missing. - Replacing missing values in `price_per_unit` with the mean price per unit. 4. Create a new column `total_price` in the merged DataFrame, which is calculated as `quantity * price_per_unit`. 5. Calculate and print the total sales for each product category. 6. Find the top 5 products with the highest total sales and display their `product_name` and `total_sales`. # Constraints - Assume all dates are in valid \'YYYY-MM-DD\' format. - The merged DataFrame should not contain any missing values in the columns that are critical for analysis (`product_id`, `quantity`, `price_per_unit`). # Expected Output 1. Total sales for each product category. 2. List of top 5 products by total sales. # Example Usage Assume the provided CSV files have the following data: `transactions.csv`: ``` transaction_id,product_id,quantity,transaction_date,price_per_unit 1,101,2,2023-01-01,10.0 2,102,1,2023-01-01,20.0 3,103,5,2023-01-02,5.0 4,,3,2023-01-02,15.0 5,104,2,2023-01-03, ``` `products.csv`: ``` product_id,product_name,category 101,Product A,Category 1 102,Product B,Category 2 103,Product C,Category 1 104,Product D,Category 3 ``` Your implementation should handle the merging process and accurately perform the analysis as specified. ```python import pandas as pd def analyze_sales(transactions_file, products_file): # Step 1: Read CSV files into DataFrames transactions = pd.read_csv(transactions_file) products = pd.read_csv(products_file) # Step 2: Merge DataFrames on \'product_id\' merged_df = pd.merge(transactions, products, on=\'product_id\', how=\'inner\') # Step 3: Handle missing data merged_df = merged_df.dropna(subset=[\'product_id\']) merged_df[\'price_per_unit\'].fillna(merged_df[\'price_per_unit\'].mean(), inplace=True) # Step 4: Create \'total_price\' column merged_df[\'total_price\'] = merged_df[\'quantity\'] * merged_df[\'price_per_unit\'] # Step 5: Calculate total sales for each category category_sales = merged_df.groupby(\'category\')[\'total_price\'].sum() print(\\"Total sales for each category:n\\", category_sales) # Step 6: Find top 5 products by total sales product_sales = merged_df.groupby(\'product_name\')[\'total_price\'].sum().sort_values(ascending=False).head(5) print(\\"nTop 5 products by total sales:n\\", product_sales) # Sample usage analyze_sales(\'transactions.csv\', \'products.csv\') ``` Your solution should follow these steps to correctly manipulate and analyze the provided datasets.","solution":"import pandas as pd def analyze_sales(transactions_file, products_file): # Step 1: Read CSV files into DataFrames transactions = pd.read_csv(transactions_file) products = pd.read_csv(products_file) # Step 2: Merge DataFrames on \'product_id\' merged_df = pd.merge(transactions, products, on=\'product_id\', how=\'inner\') # Step 3: Handle missing data merged_df = merged_df.dropna(subset=[\'product_id\']) merged_df[\'price_per_unit\'].fillna(merged_df[\'price_per_unit\'].mean(), inplace=True) # Step 4: Create \'total_price\' column merged_df[\'total_price\'] = merged_df[\'quantity\'] * merged_df[\'price_per_unit\'] # Step 5: Calculate total sales for each category category_sales = merged_df.groupby(\'category\')[\'total_price\'].sum() # Step 6: Find top 5 products by total sales product_sales = merged_df.groupby(\'product_name\')[\'total_price\'].sum().sort_values(ascending=False).head(5) return category_sales, product_sales"},{"question":"**Objective:** Create a custom command-line interface using the `cmd` module that simulates a basic task management system. The task management system should allow users to add, list, complete, and delete tasks. **Requirements:** 1. Implement a class `TaskManagerShell` that is derived from `cmd.Cmd`. 2. The shell should recognize the following commands: - `add <task description>`: Adds a new task with the given description. Each task should have an auto-incrementing unique ID. - `list`: Lists all tasks with their ID, description, and status (Incomplete or Complete). - `complete <task ID>`: Marks the task with the given ID as complete. - `delete <task ID>`: Deletes the task with the given ID. - `help` or `? <command>`: Provides help information for a specific command. - `exit` or `bye`: Exits the task manager shell. **Instructions:** 1. Define the `TaskManagerShell` class inheriting from `cmd.Cmd`. 2. Add methods corresponding to the commands listed above. Each method should handle the command appropriately. 3. Implement the `__init__` method to initialize required data structures, such as a list for storing tasks. 4. Customize the prompt to `(taskmgr) `. 5. Ensure the `help` command provides adequate information. **Constraints and Limitations:** - Task IDs should start from 1 and increment by 1 for each new task. - Task descriptions are not unique and can contain spaces. - Error handling: Ensure to handle common errors, such as invalid task IDs or malformed commands, gracefully. - Implement using Python 3.6 or higher. - Do not use any external libraries beyond the Python standard library. **Performance Requirements:** - The program should handle basic operations efficiently for up to 100 tasks. **Example Usage:** ``` (taskmgr) add Write documentation Task 1 added. (taskmgr) add Fix bugs Task 2 added. (taskmgr) list ID Description Status 1 Write documentation Incomplete 2 Fix bugs Incomplete (taskmgr) complete 1 Task 1 marked as complete. (taskmgr) list ID Description Status 1 Write documentation Complete 2 Fix bugs Incomplete (taskmgr) delete 2 Task 2 deleted. (taskmgr) list ID Description Status 1 Write documentation Complete (taskmgr) exit ``` Implement the `TaskManagerShell` class below: ```python import cmd class TaskManagerShell(cmd.Cmd): prompt = \'(taskmgr) \' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_add(self, arg): \\"Add a new task: add <task description>\\" task = {\'id\': self.next_id, \'description\': arg, \'status\': \'Incomplete\'} self.tasks.append(task) print(f\'Task {self.next_id} added.\') self.next_id += 1 def do_list(self, arg): \\"List all tasks: list\\" print(\\"ID Description Status\\") for task in self.tasks: print(f\\"{task[\'id\']} {task[\'description\']} {task[\'status\']}\\") def do_complete(self, arg): \\"Mark a task as complete: complete <task ID>\\" try: task_id = int(arg) for task in self.tasks: if task[\'id\'] == task_id: task[\'status\'] = \'Complete\' print(f\'Task {task_id} marked as complete.\') return print(f\'Task {task_id} not found.\') except ValueError: print(\\"Invalid task ID. Please enter a number.\\") def do_delete(self, arg): \\"Delete a task: delete <task ID>\\" try: task_id = int(arg) for task in self.tasks: if task[\'id\'] == task_id: self.tasks.remove(task) print(f\'Task {task_id} deleted.\') return print(f\'Task {task_id} not found.\') except ValueError: print(\\"Invalid task ID. Please enter a number.\\") def do_exit(self, arg): \\"Exit the task manager shell: exit\\" print(\'Exiting task manager.\') return True def do_bye(self, arg): \\"Exit the task manager shell: bye\\" return self.do_exit(arg) def do_help(self, arg): if arg: cmd.Cmd.do_help(self, arg) else: print(\\"Available commands:\\") print(\\" add <task description> : Add a new task\\") print(\\" list : List all tasks\\") print(\\" complete <task ID> : Mark a task as complete\\") print(\\" delete <task ID> : Delete a task\\") print(\\" help : Show help\\") print(\\" exit : Exit the shell\\") print(\\" bye : Exit the shell\\") if __name__ == \'__main__\': TaskManagerShell().cmdloop() ``` Make sure the implementation runs correctly and meets the requirements specified.","solution":"import cmd class TaskManagerShell(cmd.Cmd): prompt = \'(taskmgr) \' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_add(self, arg): \\"Add a new task: add <task description>\\" if len(arg.strip()) == 0: print(\\"Task description cannot be empty.\\") return task = {\'id\': self.next_id, \'description\': arg, \'status\': \'Incomplete\'} self.tasks.append(task) print(f\'Task {self.next_id} added.\') self.next_id += 1 def do_list(self, arg): \\"List all tasks: list\\" print(\\"ID Description Status\\") for task in self.tasks: status = \'Complete\' if task[\'status\'] == \'Complete\' else \'Incomplete\' description = task[\'description\'] print(f\\"{task[\'id\']} {description:20} {status}\\") def do_complete(self, arg): \\"Mark a task as complete: complete <task ID>\\" try: task_id = int(arg) for task in self.tasks: if task[\'id\'] == task_id: task[\'status\'] = \'Complete\' print(f\'Task {task_id} marked as complete.\') return print(f\'Task {task_id} not found.\') except ValueError: print(\\"Invalid task ID. Please enter a number.\\") def do_delete(self, arg): \\"Delete a task: delete <task ID>\\" try: task_id = int(arg) for task in self.tasks: if task[\'id\'] == task_id: self.tasks.remove(task) print(f\'Task {task_id} deleted.\') return print(f\'Task {task_id} not found.\') except ValueError: print(\\"Invalid task ID. Please enter a number.\\") def do_exit(self, arg): \\"Exit the task manager shell: exit\\" print(\'Exiting task manager.\') return True def do_bye(self, arg): \\"Exit the task manager shell: bye\\" return self.do_exit(arg) def do_help(self, arg): if arg: cmd.Cmd.do_help(self, arg) else: print(\\"Available commands:\\") print(\\" add <task description> : Add a new task\\") print(\\" list : List all tasks\\") print(\\" complete <task ID> : Mark a task as complete\\") print(\\" delete <task ID> : Delete a task\\") print(\\" help : Show help\\") print(\\" exit : Exit the shell\\") print(\\" bye : Exit the shell\\") if __name__ == \'__main__\': TaskManagerShell().cmdloop()"},{"question":"**Question: Implement a Custom XML Content Handler Using `xml.sax.handler.ContentHandler`** You are required to write a custom XML content handler by subclassing `xml.sax.handler.ContentHandler`. Your handler should parse an XML document and gather specific statistics: 1. **Count the number of elements** in the document. 2. **Keep track of each unique tag** and the number of times it appears. 3. **Extract all text content** within elements and store it in a dictionary where the keys are the tag names and the values are lists of text strings found within those tags. **Requirements:** - Implement your custom handler class named `MyContentHandler` inheriting from `xml.sax.handler.ContentHandler`. - Your class should override the following methods: - `startElement`: Increment the element count and update the tag frequency. - `characters`: Collect character data into the proper tag’s text list in the dictionary. - `endElement`: This method is not strictly necessary for this task, but you can use it as needed (for example, to finalize text collection if text could be split across multiple `characters` calls). **Input:** - An XML string passed to the `parse` method of an `XMLReader` (you can use `xml.sax.parseString` or require the student to set up similar). **Output:** - After parsing, your handler should provide: - `element_count`: Total number of elements. - `tag_frequency`: A dictionary with tag names as keys and their frequency as values. - `tag_texts`: A dictionary with tag names as keys and lists of collected text content as values. ```python import xml.sax from xml.sax.handler import ContentHandler class MyContentHandler(ContentHandler): def __init__(self): super().__init__() # Initialize required attributes here self.element_count = 0 self.tag_frequency = {} self.tag_texts = {} def startElement(self, name, attrs): # Handle element start (update counts and initialize text storage) pass def characters(self, content): # Handle character data (store text content) pass def endElement(self, name): # Optional: handle element end if needed pass # Example usage xml_data = <note> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> handler = MyContentHandler() xml.sax.parseString(xml_data, handler) print(\\"Element count: \\", handler.element_count) print(\\"Tag frequency: \\", handler.tag_frequency) print(\\"Tag texts: \\", handler.tag_texts) ``` **Constraints:** - You may assume the XML documents are well-formed. - There is no need to handle namespaces or external entities. **Performance Considerations:** - The implementation should efficiently handle XML documents with up to 10,000 elements without significant performance degradation.","solution":"import xml.sax from xml.sax.handler import ContentHandler class MyContentHandler(ContentHandler): def __init__(self): super().__init__() self.element_count = 0 self.tag_frequency = {} self.tag_texts = {} self.current_tag = None def startElement(self, name, attrs): self.element_count += 1 if name in self.tag_frequency: self.tag_frequency[name] += 1 else: self.tag_frequency[name] = 1 if name not in self.tag_texts: self.tag_texts[name] = [] self.current_tag = name def characters(self, content): if self.current_tag: text_content = content.strip() if text_content: # Avoid appending empty strings self.tag_texts[self.current_tag].append(text_content) def endElement(self, name): self.current_tag = None # Example usage xml_data = <note> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> handler = MyContentHandler() xml.sax.parseString(xml_data, handler) print(\\"Element count: \\", handler.element_count) print(\\"Tag frequency: \\", handler.tag_frequency) print(\\"Tag texts: \\", handler.tag_texts)"},{"question":"Objective You are required to write a Python script that uses the `sysconfig` module to gather specific configuration information about the current Python installation and then display it in a structured format. This task will assess your ability to interact with Python\'s configuration details and handle dictionaries and tuples effectively. Problem Statement Write a function `python_config_info()` that retrieves and returns a dictionary containing the following information about the current Python installation on the system: 1. The default installation scheme for the current platform using `get_default_scheme()`. 2. All supported installation schemes using `get_scheme_names()`. 3. All supported path names using `get_path_names()`. 4. The installation paths for the default scheme using `get_paths()`. 5. The Python version using `get_python_version()`. 6. The current platform using `get_platform()`. 7. A specific configuration variable like \'LIBDIR\' using `get_config_var(\'LIBDIR\')`. Function Signature ```python def python_config_info() -> dict: pass ``` Expected Output The function should return a dictionary where the keys are the labels of the above information and the values are the corresponding details. Example result: ```python { \\"default_scheme\\": \\"posix_prefix\\", \\"scheme_names\\": (\\"posix_prefix\\", \\"posix_home\\", ...), \\"path_names\\": (\\"stdlib\\", \\"platstdlib\\", ...), \\"paths\\": { \\"stdlib\\": \\"/usr/local/lib/python3.x\\", \\"platstdlib\\": \\"/usr/local/lib/python3.x\\", ... }, \\"python_version\\": \\"3.10\\", \\"platform\\": \\"win-amd64\\", \\"LIBDIR\\": \\"/usr/local/lib\\" } ``` Constraints 1. Use only the functionalities provided by the `sysconfig` module to gather the required information. 2. Handle cases where a configuration variable might not exist or return `None`. Notes - Ensure your code is well-structured and includes comments where necessary. - Test your function locally to verify correctness before submission.","solution":"import sysconfig def python_config_info() -> dict: config_info = { \\"default_scheme\\": sysconfig.get_default_scheme(), \\"scheme_names\\": sysconfig.get_scheme_names(), \\"path_names\\": sysconfig.get_path_names(), \\"paths\\": sysconfig.get_paths(), \\"python_version\\": sysconfig.get_python_version(), \\"platform\\": sysconfig.get_platform(), \\"LIBDIR\\": sysconfig.get_config_var(\'LIBDIR\') } return config_info"},{"question":"Objective This question evaluates your understanding of Python\'s `random` module, especially your ability to work with random number generation, sampling, and statistical distributions. Problem Statement You are tasked to create a Python class named `CustomRandom` that extends Python\'s built-in `random.Random` class. This class should incorporate several custom methods that utilize various features of the `random` module, as well as implement an additional method that generates a random sample from a bimodal distribution. Required Implementations 1. **Initialization**: Your class should initialize with an optional seed (default: None) which is passed to the superclass. 2. **Method: `custom_randint(a, b)`** - Takes two integers `a` and `b` as arguments. - Returns a random integer N such that `a <= N <= b`. 3. **Method: `weighted_sample(population, weights, k)`** - Takes a list `population`, a list of corresponding `weights`, and an integer `k`. - Returns a list of `k` elements randomly sampled from the `population` according to the `weights`. 4. **Method: `shuffle_list(lst)`** - Takes a list `lst`. - Shuffles the list `lst` in place and returns it. 5. **Method: `bimodal_distribution_sample(mu1, sigma1, mu2, sigma2, prob1)`** - Generates a single random float from a bimodal distribution consisting of two normal distributions. - The first distribution has mean `mu1` and standard deviation `sigma1`. - The second distribution has mean `mu2` and standard deviation `sigma2`. - `prob1` is the probability of choosing a value from the first distribution. - Returns a single float that is sampled from this bimodal distribution. Input and Output - **Input:** There are no specific input constraints beyond the parameters required by each method. - **Output:** The methods should return outputs as described above. Example Usage ```python cr = CustomRandom(seed=42) print(cr.custom_randint(10, 20)) # Output: A random integer between 10 and 20. population = [\'a\', \'b\', \'c\', \'d\'] weights = [1, 1, 2, 0.5] print(cr.weighted_sample(population, weights, 3)) # Output: A list of 3 elements sampled from population with given weights. lst = [1, 2, 3, 4, 5] print(cr.shuffle_list(lst)) # Output: A shuffled version of the list [1, 2, 3, 4, 5]. print(cr.bimodal_distribution_sample(0, 1, 5, 1, 0.7)) # Output: A float that is sampled from a bimodal distribution. ``` Constraints 1. You must not use any external libraries except for Python\'s standard library. 2. Ensure that performance considerations are taken into account; avoid unnecessary computations. Implement the `CustomRandom` class by creating methods as specified. Your solution will be evaluated on correctness, efficiency, and clarity.","solution":"import random class CustomRandom(random.Random): def __init__(self, seed=None): super().__init__(seed) def custom_randint(self, a, b): return self.randint(a, b) def weighted_sample(self, population, weights, k): return self.choices(population, weights, k=k) def shuffle_list(self, lst): self.shuffle(lst) return lst def bimodal_distribution_sample(self, mu1, sigma1, mu2, sigma2, prob1): if self.random() < prob1: return self.gauss(mu1, sigma1) else: return self.gauss(mu2, sigma2)"},{"question":"Objective You are required to create a Python script that handles database operations using the `dbm` module. Your task is to write functions that open a database, insert and retrieve data, handle exceptions, and showcase the differences between using `dbm.gnu`, `dbm.ndbm`, and `dbm.dumb`. Instructions 1. **Function `open_db`:** - **Input:** - `filename` (str): The name of the database file. - `flag` (str): The mode in which to open the database, with possible values `\'r\'`, `\'w\'`, `\'c\'`, `\'n\'`. - **Output:** - A database object. - **Behavior:** - Open the database using `dbm.open`. - Handle any `dbm.error` exceptions and print an appropriate message. 2. **Function `insert_data`:** - **Input:** - `db`: The database object obtained from `open_db`. - `key` (str): The key to insert. Must be converted to bytes. - `value` (str): The value to insert. Must be converted to bytes. - **Output:** - None. - **Behavior:** - Insert the key-value pair into the database. - If the key or value is not a string, raise and handle `TypeError`, printing an appropriate message. 3. **Function `retrieve_data`:** - **Input:** - `db`: The database object obtained from `open_db`. - `key` (str): The key to retrieve. - **Output:** - The value corresponding to the key, or `None` if the key is not found. - **Behavior:** - Retrieve and return the value for the given key. Convert the retrieved value back to a string. 4. **Function `compare_dbms`:** - **Input:** - `filename` (str): The name of the database file. - `flag` (str): The mode in which to open the database. Should be `\'c\'`. - **Output:** - A dictionary with keys `dbm.gnu`, `dbm.ndbm`, and `dbm.dumb`, and boolean values indicating if the database opened successfully with each module. - **Behavior:** - Attempt to open the database using each of the three dbm modules. Record whether the operation was successful for each module. - Return the dictionary with the results. Constraints - Do not use any external libraries other than built-in Python modules. - The `filename` should not include any file extensions – they will be handled internally by the dbm modules. - Ensure to close the database properly after operations using the `with` statement. Example Usage: ```python filename = \'mydatabase\' # Using dbm.ndbm db = open_db(filename, \'c\') insert_data(db, \'example\', \'entry\') print(retrieve_data(db, \'example\')) # should print \'entry\' # Comparison function comparison_result = compare_dbms(filename, \'c\') print(comparison_result) # should print something like {\'dbm.gnu\': True, \'dbm.ndbm\': True, \'dbm.dumb\': True} ```","solution":"import dbm def open_db(filename: str, flag: str): try: db = dbm.open(filename, flag) return db except dbm.error as e: print(f\\"Error opening database: {e}\\") return None def insert_data(db, key: str, value: str): if not isinstance(key, str) or not isinstance(value, str): raise TypeError(\\"Key and value must be of type str.\\") try: db[key.encode()] = value.encode() except dbm.error as e: print(f\\"Error inserting data: {e}\\") def retrieve_data(db, key: str): try: value = db[key.encode()] return value.decode() except KeyError: return None except dbm.error as e: print(f\\"Error retrieving data: {e}\\") return None def compare_dbms(filename: str, flag: str): results = {} for module in [\'dbm.gnu\', \'dbm.ndbm\', \'dbm.dumb\']: try: db = dbm.open(filename, flag, module=module) db.close() results[module] = True except Exception as e: results[module] = False print(f\\"Error using {module}: {e}\\") return results"},{"question":"# Seaborn Boxplot Customization Assignment Objective Demonstrate your understanding of seaborn\'s boxplot capabilities and customization options. Problem Statement You will use the seaborn package to create and customize boxplots using the Titanic dataset. Specifically, you will create two plots: 1. **Basic Grouped Boxplot**: A vertical boxplot grouped by the \\"class\\" variable and further nested by the \\"alive\\" variable with a small gap between the groups. 2. **Advanced Customization Boxplot**: A horizontal boxplot displaying \\"age\\" with grouping by \\"sex\\", including advanced customizations such as different box colors, linewidths, and displaying the mean of the \\"age\\" data. Instructions 1. **Basic Grouped Boxplot** - Load the Titanic dataset using `sns.load_dataset(\\"titanic\\")`. - Create a vertical boxplot using the variables: - `x` axis: \\"class\\" - `y` axis: \\"age\\" - `hue`: \\"alive\\" - Add customization to draw the boxes as line art (i.e., no fill) and include a small gap (e.g., `gap=0.1`). 2. **Advanced Customization Boxplot** - Create a horizontal boxplot using the variables: - `x` axis: \\"age\\" - `y` axis: \\"sex\\" - Apply the following customizations: - Change the color of the boxes to different ones of your choice. - Set the whiskers to cover the full range of the data (0 to 100 percentiles). - Adjust the linewidth of the lines to 1.25. - Plot the mean as a separate marker (e.g., a red star). Expected Output - The Jupyter notebook or Python script should show two plots: - A basic grouped boxplot with the specified customizations. - An advanced customization boxplot with the specified customizations. Constraints - Do not use any dataset other than the seaborn Titanic dataset. - Ensure code runs without errors. You can refer to the seaborn and matplotlib documentation for further guidance on customization options.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") def create_basic_grouped_boxplot(): Creates a vertical boxplot grouped by \'class\' and nested by \'alive\' variable with a small gap between the groups. returns: matplotlib figure object sns.set(style=\\"whitegrid\\") plt.figure(figsize=(10, 6)) box_plot = sns.boxplot(x=\\"class\\", y=\\"age\\", hue=\\"alive\\", data=titanic, notch=False, dodge=True) sns.despine(offset=10, trim=True) return box_plot.get_figure() def create_advanced_customization_boxplot(): Creates a horizontal boxplot displaying \'age\' with grouping by \'sex\', with advanced customizations. returns: matplotlib figure object plt.figure(figsize=(10, 6)) box_plot = sns.boxplot(x=\\"age\\", y=\\"sex\\", data=titanic, palette=\\"Set2\\", showmeans=True, meanprops={\\"marker\\": \\"o\\", \\"markerfacecolor\\": \\"red\\", \\"markeredgecolor\\": \\"red\\"}, linewidth=1.25, whis=(0, 100)) sns.despine(offset=10, trim=True) return box_plot.get_figure() # Create and save the plots basic_grouped_boxplot = create_basic_grouped_boxplot() basic_grouped_boxplot.savefig(\'basic_grouped_boxplot.png\') advanced_customization_boxplot = create_advanced_customization_boxplot() advanced_customization_boxplot.savefig(\'advanced_customization_boxplot.png\')"},{"question":"Advanced Method Manipulation in Python Objective: Demonstrate your understanding of Python method objects and instance method objects by implementing and correctly interacting with these objects. Problem Statement: You need to implement a Python class that simulates a simplistic version of method binding and retrieval, similar to the behavior of `PyInstanceMethod_Type` and `PyMethod_Type` as described in the provided documentation. Requirements: 1. Implement a class `CustomMethod` that can: - Bind a function to an instance of another class. - Provide methods to retrieve the bound function and the instance it is bound to. 2. Implement a class `CustomInstanceMethod` that can: - Bind a function to any callable object. - Provide methods to retrieve the bound function. 3. Ensure that your implementations fulfill the following conditions: - Methods to check if an object is an instance of `CustomMethod` or `CustomInstanceMethod`. - Methods to create new method objects and instance method objects. Input/Output: - `CustomMethod` should have: - `__init__(self, func, instance)`: Initializes the method object with a function and an instance. - `get_function(self)`: Returns the function associated with the method. - `get_instance(self)`: Returns the instance the method is bound to. - `CustomInstanceMethod` should have: - `__init__(self, func)`: Initializes the instance method object with a function. - `get_function(self)`: Returns the function associated with the instance method. Constraints: - Assume that input functions and instances are always valid and callable. - Do not use any external libraries except for Python\'s standard library. Performance Requirements: - Ensure that retrieving functions and instances runs in constant time O(1). Example: ```python class ExampleClass: def example_method(self): return \\"Example Method Called\\" def standalone_function(): return \\"Standalone Function Called\\" # Example Usage instance = ExampleClass() method = CustomMethod(ExampleClass.example_method, instance) inst_method = CustomInstanceMethod(standalone_function) # Expected Outputs assert method.get_function()() == \\"Example Method Called\\" assert method.get_instance() == instance assert inst_method.get_function()() == \\"Standalone Function Called\\" ``` # Write your solution below:","solution":"class CustomMethod: def __init__(self, func, instance): Initializes the method object with a function and an instance. self.func = func self.instance = instance def get_function(self): Returns the function associated with the method. return self.func def get_instance(self): Returns the instance the method is bound to. return self.instance class CustomInstanceMethod: def __init__(self, func): Initializes the instance method object with a function. self.func = func def get_function(self): Returns the function associated with the instance method. return self.func"},{"question":"# Question: Advanced Seaborn Plot Customization **Objective:** You are tasked with creating and customizing a distribution plot using the Seaborn package to demonstrate your comprehension of its advanced features. **Instructions:** 1. Load the `titanic` dataset using `sns.load_dataset(\\"titanic\\")`. 2. Create a bivariate KDE plot showing the density of `age` against `fare`. 3. Customize the plot by: - Adding a marginal \\"rug\\". - Using `hue` to distinguish between different `class`. - Adding a title and axis labels to the plot. 4. Adjust the figure to contain separate subplots (facets) for each `sex` with the appropriate titles. **Requirements:** - You must use `sns.displot` for plotting and customize it using a `FacetGrid`. - The function should return the final `FacetGrid` object. # Function Signature: ```python def create_custom_seaborn_plot(): # Your code here pass ``` # Example Usage: ```python create_custom_seaborn_plot() ``` **Expected Output:** A plot should be displayed, with the following characteristics: - It shows a kernel density estimate (KDE) plot of `age` against `fare`. - The plot includes a rug plot showing individual observations. - The data points are color-coded by the `class` column. - There are separate subplots for male and female passengers, each with a clear title. - The plot has descriptive axis labels and a main title. **Notes:** - Pay attention to the overall aesthetics and ensure the plot is informative and well-labeled. - Remember to import necessary libraries and load the dataset within the function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_seaborn_plot(): # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Create a FacetGrid object g = sns.displot( data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"class\\", kind=\\"kde\\", rug=True, col=\\"sex\\", height=5, aspect=1 ) # Add titles and labels g.set_axis_labels(\\"Age\\", \\"Fare\\") g.set_titles(col_template=\\"{col_name} Passengers\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Age vs Fare Distribution with KDE Plot\') return g"},{"question":"**Objective:** Your task is to create a Python script that reads a directory containing multiple packages, each with its own `__main__.py` file, and creates a separate executable `.pyz` archive for each package. You should also implement functionality to list the interpreters used in these archives. **Requirements:** 1. **Input:** - A base directory containing subdirectories representing individual packages. Each package directory contains Python files and a `__main__.py` file. - An optional configuration file `interpreters.cfg` in the base directory, which maps package names to specific Python interpreters. 2. **Output:** - A `.pyz` archive for each package in the base directory, named after the package (e.g., `mypackage.pyz` for a package named `mypackage`). - An optional compressed version of the zip archive if a `--compress` flag is provided. - A function to list the interpreters used in the created archives. **Constraints:** - The `__main__.py` file is required in each package directory, and you should handle the case where it may be missing by skipping those projects. - If the `interpreters.cfg` file is missing or does not specify an interpreter for a package, the default Python interpreter should be used. - Archives should be created only for directories; files not named `__main__.py` in the base directory should be ignored. - If any errors occur during the creation of an archive, such errors should be logged, and the process should continue for other packages. **Performance Requirements:** - The solution should handle large numbers of packages efficiently, leveraging appropriate data structures and algorithms where necessary. **Details:** 1. Define a function `create_archives(base_directory: str, output_directory: str, compress: bool) -> None`: - `base_directory`: Path to the base directory containing package subdirectories. - `output_directory`: Path to the directory where the `.pyz` archives should be created. - `compress`: Boolean flag to indicate whether the archives should be compressed. This function should read through the specified base directory and create an executable `.pyz` archive for each subdirectory. The archives should be saved in the output directory. 2. Define a function `get_interpreters(archive_directory: str) -> Dict[str, Optional[str]]`: - `archive_directory`: Path to the directory containing the `.pyz` archives. This function should return a dictionary mapping archive filenames to their specified interpreters (or `None` if no interpreter is specified). **Example:** Given a directory structure: ``` /base_directory /package1 __main__.py module1.py /package2 __main__.py module2.py interpreters.cfg ``` Where `interpreters.cfg` contains: ``` package1=/usr/bin/python3 ``` The script should create: ``` /output_directory package1.pyz package2.pyz ``` The `get_interpreters` function should return: ```python { \'package1.pyz\': \'/usr/bin/python3\', \'package2.pyz\': None } ``` **Note:** Your implementation should handle edge cases (e.g., missing `__main__.py`, missing `interpreters.cfg`, non-directory items in the base directory) gracefully. **Code Template:** ```python import zipapp import os from pathlib import Path from typing import Dict, Optional def create_archives(base_directory: str, output_directory: str, compress: bool) -> None: # Your implementation here pass def get_interpreters(archive_directory: str) -> Dict[str, Optional[str]]: # Your implementation here pass # Example usage create_archives(\'/path/to/base_directory\', \'/path/to/output_directory\', compress=True) interpreters = get_interpreters(\'/path/to/output_directory\') print(interpreters) ```","solution":"import zipapp import os from pathlib import Path from typing import Dict, Optional, List import logging logging.basicConfig(level=logging.INFO) def create_archives(base_directory: str, output_directory: str, compress: bool) -> None: base_dir = Path(base_directory) output_dir = Path(output_directory) if not output_dir.exists(): output_dir.mkdir(parents=True) interpreters = {} interpreters_cfg = base_dir / \'interpreters.cfg\' if interpreters_cfg.exists(): with open(interpreters_cfg, \'r\') as cfg_file: for line in cfg_file: if \'=\' in line: package, interpreter = line.strip().split(\'=\', 1) interpreters[package.strip()] = interpreter.strip() for package_dir in base_dir.iterdir(): if package_dir.is_dir(): main_file = package_dir / \'__main__.py\' if main_file.exists(): archive_name = package_dir.name + \'.pyz\' archive_path = output_dir / archive_name interpreter = interpreters.get(package_dir.name, None) try: zipapp.create_archive(package_dir, target=archive_path, interpreter=interpreter, compressed=compress) logging.info(f\\"Created archive: {archive_path} with interpreter {interpreter}\\") except Exception as e: logging.error(f\\"Failed to create archive for {package_dir.name}: {e}\\") def get_interpreters(archive_directory: str) -> Dict[str, Optional[str]]: archive_dir = Path(archive_directory) interpreters = {} for archive_file in archive_dir.glob(\'*.pyz\'): interpreter = None with open(archive_file, \'rb\') as archive: if b\'#!\' in archive.peek(512): # Peek first 512 bytes, could adjust this according to need interpreter = archive.readline().decode(\'utf-8\').strip().split(\' \')[0][2:] interpreters[archive_file.name] = interpreter return interpreters # Example usage # create_archives(\'/path/to/base_directory\', \'/path/to/output_directory\', compress=True) # interpreters = get_interpreters(\'/path/to/output_directory\') # print(interpreters)"},{"question":"Objective: Develop a function that evaluates mathematical expressions using Reverse Polish Notation (RPN). The function must support standard arithmetic operations (+, -, *, /) and should handle errors gracefully. Additionally, use type annotations to provide hints about the types of inputs and outputs. Problem Description: RPN is a mathematical notation wherein every operator follows all of its operands. It is also known as postfix notation. The function should process a list of tokens where each token is either an integer or one of the four arithmetic operators. The function should return the result of evaluating the expression. For example, the expression `(3 + 4) * 2` would be written as \\"3 4 + 2 *\\" in RPN and should evaluate to 14. Input: - A list of strings `tokens` where each string is either an integer (e.g., \\"3\\") or an arithmetic operator `+`, `-`, `*`, `/`. All integers will be valid integers within the range of -1000 to 1000. Output: - Returns an integer, the result of evaluating the RPN expression. Constraints: - The expression given as input will always be valid RPN (you do not need to handle invalid expressions). - Division with `/` should use integer division (i.e., `//`), and you can assume denominator will not be zero. Example: ```text Input: [\\"2\\", \\"1\\", \\"+\\", \\"3\\", \\"*\\"] Output: 9 Input: [\\"4\\", \\"13\\", \\"5\\", \\"/\\", \\"+\\"] Output: 6 Input: [\\"10\\", \\"6\\", \\"9\\", \\"3\\", \\"+\\", \\"-11\\", \\"*\\", \\"/\\", \\"*\\", \\"17\\", \\"+\\", \\"5\\", \\"+\\"] Output: 22 ``` Function Signature: ```python from typing import List def evalRPN(tokens: List[str]) -> int: pass ``` Detailed Requirements: 1. **Data Types:** - Use the `List` type annotation from the `typing` module. - Ensure function output is annotated as `int`. 2. **Stack Operations:** - Use a stack-based approach to evaluate the expression. - When encountering an integer, push it onto the stack. - When encountering an operator, pop the top two elements from the stack, apply the operator, and push the result back onto the stack. 3. **Error Handling:** - Handle unexpected situations (e.g., wrong number of operands in a well-formed RPN expression) gracefully by raising appropriate exceptions. 4. **Testing and Validation:** - The function should be well-tested with the provided examples and self-crafted test cases to ensure correctness. # Example Implementation: ```python from typing import List def evalRPN(tokens: List[str]) -> int: stack = [] for token in tokens: if token in \\"+-*/\\": b = stack.pop() a = stack.pop() if token == \\"+\\": result = a + b elif token == \\"-\\": result = a - b elif token == \\"*\\": result = a * b elif token == \\"/\\": result = int(a / b) # Ensure integer division stack.append(result) else: stack.append(int(token)) return stack[0] ``` This function uses a basic stack data structure to perform the RPN evaluation. It follows the steps outlined in the problem description to handle arithmetic operations and provides type annotations for clarity.","solution":"from typing import List def evalRPN(tokens: List[str]) -> int: stack = [] for token in tokens: if token in \\"+-*/\\": b = stack.pop() a = stack.pop() if token == \\"+\\": result = a + b elif token == \\"-\\": result = a - b elif token == \\"*\\": result = a * b elif token == \\"/\\": result = int(a / b) # This ensures integer division stack.append(result) else: stack.append(int(token)) return stack[0]"},{"question":"You are given a dataset with multiple features and class labels. Your task is to implement and compare Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) classifiers using scikit-learn. You need to evaluate their performance in terms of classification accuracy and also perform dimensionality reduction using LDA, visualizing the results. # Input 1. A CSV file named `data.csv` with columns representing features and one column named `label` representing class labels. # Output 1. Print the classification accuracy for both LDA and QDA classifiers. 2. Generate and display a 2D scatter plot showing the data points projected into a 2-dimensional space using LDA. # Constraints 1. The data must be split into training and test sets using a 70-30 split. 2. Use a random seed of 42 for reproducibility. 3. The dataset contains at least two classes and at least one feature. # Performance Requirements 1. The approach should be efficient in terms of computation, leveraging vectorized operations where possible. # Instructions 1. Load the dataset from the given CSV file. 2. Split the data into training and test sets. 3. Train an LDA classifier and a QDA classifier on the training set. 4. Evaluate the classifiers on the test set and print their accuracies. 5. For LDA, reduce the dimensionality of the dataset to 2 dimensions and visualize the results using a scatter plot. 6. Label the points in the scatter plot according to their classes using different colors. Example Usage ```python # Assume you have saved the following code in a script named `lda_qda_comparison.py` # You would run the script from the command line as follows: # python lda_qda_comparison.py # The script should print the classification accuracies and display the scatter plot. ``` Additional Notes - You may assume that the necessary libraries (numpy, pandas, scikit-learn, matplotlib) are already installed. - Ensure that your code is well-documented and follows best practices in terms of readability and efficiency.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt def lda_qda_comparison(filename=\'data.csv\'): # Load the dataset data = pd.read_csv(filename) # Split the data into features and labels X = data.drop(\'label\', axis=1) y = data[\'label\'] # Split the data into training and test sets with a 70-30 split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the LDA and QDA classifiers lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() # Train the LDA classifier lda.fit(X_train, y_train) # Predict with the LDA classifier lda_predictions = lda.predict(X_test) # Calculate the accuracy for LDA lda_accuracy = accuracy_score(y_test, lda_predictions) # Train the QDA classifier qda.fit(X_train, y_train) # Predict with the QDA classifier qda_predictions = qda.predict(X_test) # Calculate the accuracy for QDA qda_accuracy = accuracy_score(y_test, qda_predictions) # Print the classification accuracies print(f\\"LDA Classification Accuracy: {lda_accuracy * 100:.2f}%\\") print(f\\"QDA Classification Accuracy: {qda_accuracy * 100:.2f}%\\") # Reduce the dimensionality to 2 for visualization using LDA lda_projection = lda.transform(X) # Create a 2D scatter plot plt.figure(figsize=(8, 6)) for label in y.unique(): plt.scatter(lda_projection[y == label, 0], lda_projection[y == label, 1], label=label, alpha=0.7) plt.xlabel(\'LD 1\') plt.ylabel(\'LD 2\') plt.title(\'LDA Projection\') plt.legend() plt.grid(True) plt.show()"},{"question":"# Custom Asyncio Event Loop Policy and Child Watcher Objective Create a custom event loop policy and child watcher by extending the built-in classes provided by the asyncio module. Demonstrate the usage of your custom implementations by running a simple asyncio task and spawning a subprocess. Requirements 1. **Custom Event Loop Policy**: - Subclass `asyncio.DefaultEventLoopPolicy`. - Override the `get_event_loop()` method to print a message each time it is called and then return the event loop. - Implement `new_event_loop()` to create and return a new event loop while printing a custom message. 2. **Custom Child Watcher**: - Subclass `asyncio.ThreadedChildWatcher`. - Override the `add_child_handler()` method to print a message each time a new child handler is added, and then call the parent method. 3. **Demonstration Script**: - Set your custom event loop policy and custom child watcher using `asyncio.set_event_loop_policy()` and `asyncio.set_child_watcher()`. - Define an asyncio task that runs for a few seconds, printing a message every second. - Spawn a subprocess (e.g., running a simple shell command) and use your custom child watcher to handle its termination. - Ensure the script runs without errors and showcases the custom behavior with print statements. Input None directly. Your custom event loop policy and child watcher should be demonstrated in a Python script. Output Demonstrate your code in a Python script. Expected output should include: - Messages printed from overridden methods in your custom event loop policy and child watcher. - Outputs from the asyncio task and the subprocess. Constraints - You should only use the functionality provided in the `asyncio` module and avoid using third-party libraries for this task. - Ensure your custom implementations are compatible with Python 3.10. Performance - The provided implementations should efficiently handle the creation of event loops and child handlers without introducing significant overhead. - Ensure that the solution scales well with multiple subprocesses and event loops. Example ```python import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Getting the event loop...\\") return super().get_event_loop() def new_event_loop(self): print(\\"Creating a new event loop...\\") return super().new_event_loop() class MyThreadedChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Adding child handler for PID: {pid}\\") super().add_child_handler(pid, callback, *args) def demo_task(): async def main(): for i in range(5): print(\\"Running task...\\") await asyncio.sleep(1) loop = asyncio.get_event_loop() loop.run_until_complete(main()) def run_subprocess(): async def main(): proc = await asyncio.create_subprocess_shell(\'echo \\"Hello, World!\\"\') await proc.communicate() loop = asyncio.get_event_loop() loop.run_until_complete(main()) if __name__ == \\"__main__\\": asyncio.set_event_loop_policy(MyEventLoopPolicy()) custom_watcher = MyThreadedChildWatcher() asyncio.set_child_watcher(custom_watcher) demo_task() run_subprocess() ```","solution":"import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Getting the event loop...\\") return super().get_event_loop() def new_event_loop(self): print(\\"Creating a new event loop...\\") return super().new_event_loop() class MyThreadedChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Adding child handler for PID: {pid}\\") super().add_child_handler(pid, callback, *args) async def demo_task(): for i in range(5): print(f\\"Running task... {i + 1}\\") await asyncio.sleep(1) async def run_subprocess(): proc = await asyncio.create_subprocess_shell(\'echo \\"Hello, World!\\"\') await proc.communicate() async def main(): # Start the demo task await demo_task() # Run the subprocess task await run_subprocess() if __name__ == \\"__main__\\": asyncio.set_event_loop_policy(MyEventLoopPolicy()) custom_watcher = MyThreadedChildWatcher() asyncio.set_child_watcher(custom_watcher) loop = asyncio.get_event_loop() loop.run_until_complete(main())"},{"question":"**Custom Selector Implementation** # Objective: Implement a custom selector class `CustomSelector` that extends the `BaseSelector` abstract base class. This custom selector will monitor file objects for read events and log any data read from these file objects. # Requirements: 1. **Class Implementation**: - `CustomSelector` should inherit from `selectors.BaseSelector`. - Implement all required abstract methods (`register`, `unregister`, `select`, `get_map`). - Add functionality to log data read. 2. **Logging**: - Whenever data is read from a file object, log the data prefixed by \\"Data Read: \\" to the console. # Input and Output: - **Input**: - File objects for registration. - Data sent to the registered file objects. - **Output**: - Console logs indicating data read from file objects. # Constraints: - Use only the interfaces provided by the `selectors` module. - Ensure `CustomSelector` handles registrations and cancellations efficiently. - Log accurate data as read from the registered file objects. # Example Usage: ```python import selectors import socket class CustomSelector(selectors.BaseSelector): def __init__(self): super().__init__() self._fd_to_key = {} def register(self, fileobj, events, data=None): if events & selectors.EVENT_READ == 0: raise ValueError(\'CustomSelector supports only EVENT_READ\') key = selectors.SelectorKey(fileobj, fileobj.fileno(), events, data) self._fd_to_key[fileobj.fileno()] = key return key def unregister(self, fileobj): fd = fileobj.fileno() key = self._fd_to_key.pop(fd) return key def modify(self, fileobj, events, data=None): self.unregister(fileobj) return self.register(fileobj, events, data) def select(self, timeout=None): import select read_list = [key.fileobj for key in self._fd_to_key.values()] read_ready, _, _ = select.select(read_list, [], [], timeout) for fileobj in read_ready: data = fileobj.recv(1024) if data: print(f\\"Data Read: {data.decode(\'utf-8\') if isinstance(data, bytes) else data}\\") return [(self._fd_to_key[fileobj.fileno()], selectors.EVENT_READ) for fileobj in read_ready] def close(self): self._fd_to_key.clear() def get_key(self, fileobj): return self._fd_to_key[fileobj.fileno()] def get_map(self): return self._fd_to_key # Example Usage: if __name__ == \\"__main__\\": sel = CustomSelector() def accept(sock, mask): conn, addr = sock.accept() # Should be ready print(\'accepted\', conn, \'from\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, data=\\"Some data\\") sock = socket.socket() sock.bind((\'localhost\', 1234)) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) while True: events = sel.select() for key, mask in events: callback = key.data if callable(callback): callback(key.fileobj, mask) ``` # Notes: - Focus on correctly registering, unregistering, and monitoring file objects for read events. - Ensure proper error handling for invalid conditions. - The example shows how to use your CustomSelector to create a simple server that logs data read.","solution":"import selectors import logging class CustomSelector(selectors.BaseSelector): def __init__(self): super().__init__() # A dictionary to store file descriptor keys self._fd_to_key = {} logging.basicConfig(level=logging.INFO) # Configuring the logging def register(self, fileobj, events, data=None): if events & selectors.EVENT_READ == 0: raise ValueError(\'CustomSelector supports only EVENT_READ\') key = selectors.SelectorKey(fileobj, fileobj.fileno(), events, data) self._fd_to_key[fileobj.fileno()] = key return key def unregister(self, fileobj): fd = fileobj.fileno() key = self._fd_to_key.pop(fd, None) if key is None: raise KeyError(\'File object not found\') return key def modify(self, fileobj, events, data=None): self.unregister(fileobj) return self.register(fileobj, events, data) def select(self, timeout=None): import select read_list = [key.fileobj for key in self._fd_to_key.values()] read_ready, _, _ = select.select(read_list, [], [], timeout) result = [] for fileobj in read_ready: data = fileobj.recv(1024) if data: logging.info(f\\"Data Read: {data.decode(\'utf-8\') if isinstance(data, bytes) else data}\\") result.append((self._fd_to_key[fileobj.fileno()], selectors.EVENT_READ)) return result def close(self): self._fd_to_key.clear() def get_key(self, fileobj): return self._fd_to_key[fileobj.fileno()] def get_map(self): return self._fd_to_key"},{"question":"As a data scientist in training, you have been provided with a dataset containing information about different species of penguins. You are required to utilize the seaborn library to analyze the relationship between the bill length and bill depth of these penguins. You must demonstrate your understanding of creating and customizing different types of joint plots. # Question: Using the provided `penguins` dataset from seaborn, create a Python function that generates and saves a series of joint plots. The function should be named `create_penguin_joint_plots` and meet the following requirements: Function Signature: ```python def create_penguin_joint_plots(): pass ``` Requirements: 1. Load the `penguins` dataset using seaborn. 2. Generate and save the following joint plots as PNG files: - A scatter plot with marginal histograms. - A scatter plot with a linear regression fit and univariate KDE curves. - A hexagonal binned plot. - A KDE plot with both bivariate and univariate KDE curves. 3. For each plot, add a title using the `sns.set(title=\\"Your Title\\")` method, where \\"Your Title\\" should be replaced accordingly for each plot type. 4. Save each plot using the `savefig` method from matplotlib, with filenames: - `scatter_hist.png` - `scatter_reg_kde.png` - `hex_bin.png` - `kde_plot.png` Constraints: - Ensure your function does not return anything but successfully generates and saves the required plots. - Title for the scatter plot with marginal histograms should be \\"Scatter Plot with Marginals\\" - Title for the scatter plot with linear regression should be \\"Scatter Plot with Regression and KDE\\" - Title for the hexagonal binned plot should be \\"Hexagonal Binned Plot\\" - Title for the KDE plot should be \\"KDE Plot\\" Example Usage: ```python create_penguin_joint_plots() ``` Make sure your code is efficient and make use of seaborn documentation to correctly implement the required plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_joint_plots(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Scatter plot with marginal histograms scatter_hist = sns.jointplot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", data=penguins) scatter_hist.fig.suptitle(\\"Scatter Plot with Marginals\\", y=1.02) scatter_hist.savefig(\\"scatter_hist.png\\") # Scatter plot with linear regression fit and univariate KDE curves scatter_reg_kde = sns.jointplot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", data=penguins, kind=\\"reg\\") scatter_reg_kde.fig.suptitle(\\"Scatter Plot with Regression and KDE\\", y=1.02) scatter_reg_kde.savefig(\\"scatter_reg_kde.png\\") # Hexagonal binned plot hex_bin = sns.jointplot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", data=penguins, kind=\\"hex\\") hex_bin.fig.suptitle(\\"Hexagonal Binned Plot\\", y=1.02) hex_bin.savefig(\\"hex_bin.png\\") # KDE plot with both bivariate and univariate KDE curves kde_plot = sns.jointplot(x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", data=penguins, kind=\\"kde\\") kde_plot.fig.suptitle(\\"KDE Plot\\", y=1.02) kde_plot.savefig(\\"kde_plot.png\\")"},{"question":"**Title: Implement a Custom Buffered File Reader** **Objective:** Demonstrate your understanding of Python\'s io module, particularly how to work with buffered binary streams and manage encoding in text streams. **Task:** Create a class `CustomFileReader` that reads from a file efficiently using buffered I/O. Your class should support both text and binary reading modes and provide methods to: 1. **Read the entire content** of a file. 2. **Read the content line by line**. 3. **Seek to a specific byte** in the file. 4. **Retrieve the current position** in the file. 5. **Close the file properly**. **Requirements:** - Implement both binary and text reading modes. For text mode, allow specifying the encoding. - Use the `BufferedReader` class for implementing buffered reading. - Ensure thread safety for the read operations. - Raise appropriate exceptions if file operations are not supported (e.g., seeking on a non-seekable stream). - Handle the opening of files with different modes (\'r\', \'rb\') appropriately based on the input parameters. **Class Specification:** ```python class CustomFileReader: def __init__(self, file_path: str, mode: str = \'r\', encoding: str = \'utf-8\'): Initializes the CustomFileReader with the given file path, mode, and encoding. :param file_path: Path to the file. :param mode: Mode in which the file is to be opened (\'r\' for text, \'rb\' for binary). :param encoding: Encoding to be used for text mode. pass def read_all(self): Reads the entire content of the file. :return: Text content if in text mode, bytes content if in binary mode. pass def read_line(self): Reads a single line from the file. :return: A single line of text if in text mode, or a line of bytes if in binary mode. pass def seek(self, offset: int, whence: int = 0): Seeks to the specified byte position in the file. :param offset: Offset to seek to. :param whence: Specifies from where to seek. Default is the file start. pass def tell(self) -> int: Returns the current byte position in the file. :return: Current position in the file. pass def close(self): Closes the file. pass ``` **Input and Output:** - Ensure that all methods properly handle file reading and position retrieval, and close the file appropriately. - If the file does not exist or is inaccessible, raise an appropriate exception. - Demonstrate your class with an example file showing both text and binary read operations. **Constraints:** - Use Python 3.10 or higher. - Pay attention to the efficiency and thread safety of your implementation. - Do not use any external libraries besides the built-in io module. **Example Usage:** ```python # Example usage of CustomFileReader in text mode reader = CustomFileReader(\'example.txt\', mode=\'r\', encoding=\'utf-8\') print(reader.read_all()) reader.seek(0) print(reader.read_line()) reader.close() # Example usage of CustomFileReader in binary mode reader = CustomFileReader(\'example.bin\', mode=\'rb\') print(reader.read_all()) reader.seek(0) print(reader.read_line()) reader.close() ``` This task will assess your ability to implement and use various I/O operations provided by the Python io module, ensuring you understand both the high-level and low-level functionalities, as well as managing the read positions and encoding properly.","solution":"import io class CustomFileReader: def __init__(self, file_path: str, mode: str = \'r\', encoding: str = \'utf-8\'): Initializes the CustomFileReader with the given file path, mode, and encoding. :param file_path: Path to the file. :param mode: Mode in which the file is to be opened (\'r\' for text, \'rb\' for binary). :param encoding: Encoding to be used for text mode. self.file_path = file_path self.mode = mode self.encoding = encoding self.file = None if \'b\' in mode: self.file = io.open(file_path, mode) else: self.file = io.open(file_path, mode, encoding=encoding) self.buffered_reader = io.BufferedReader(self.file) def read_all(self): Reads the entire content of the file. :return: Text content if in text mode, bytes content if in binary mode. self.buffered_reader.seek(0) return self.file.read() def read_line(self): Reads a single line from the file. :return: A single line of text if in text mode, or a line of bytes if in binary mode. return self.file.readline() def seek(self, offset: int, whence: int = 0): Seeks to the specified byte position in the file. :param offset: Offset to seek to. :param whence: Specifies from where to seek. Default is the file start. self.buffered_reader.seek(offset, whence) def tell(self) -> int: Returns the current byte position in the file. :return: Current position in the file. return self.buffered_reader.tell() def close(self): Closes the file. self.file.close()"},{"question":"**Title:** Implement an Automated FTP Client with File Verification **Objective:** To assess the student\'s ability to utilize the `ftplib` module for performing various FTP operations, error handling, and verifying the integrity of downloaded files. **Problem Statement:** You are required to implement a function `automated_ftp_client` that connects to an FTP server, navigates through directories, retrieves a specific file, and verifies its integrity using a given checksum. The function should perform the following steps: 1. Connect to an FTP server using the provided hostname. 2. Log in using the given username and password. 3. Change to the specified directory. 4. List the contents of the directory and print them. 5. Download a specified file in binary mode. 6. Verify the downloaded file\'s integrity using a provided checksum (SHA256). 7. Gracefully close the connection. **Function Signature:** ```python def automated_ftp_client(hostname: str, username: str, password: str, directory: str, filename: str, checksum: str) -> bool: pass ``` **Parameters:** - `hostname` (str): The hostname of the FTP server. - `username` (str): The username for login. - `password` (str): The password for login. - `directory` (str): The directory to navigate to. - `filename` (str): The name of the file to download. - `checksum` (str): The expected SHA256 checksum of the file for integrity verification. **Returns:** - `bool`: Returns `True` if the file is downloaded and its integrity is verified; otherwise, returns `False`. **Constraints:** 1. Assume that the `hostname`, `username`, `password`, `directory`, and `filename` are all valid strings. 2. The function should handle exceptions appropriately, such as connection errors, login failures, file retrieval errors, and checksum mismatches. 3. The default encoding should be used for directory and filenames as per `ftplib`. **Performance Requirements:** - The function should handle large files efficiently by reading data in chunks during download. **Example Usage:** ```python hostname = \\"ftp.example.com\\" username = \\"anonymous\\" password = \\"anonymous@\\" directory = \\"debian\\" filename = \\"README\\" checksum = \\"d2c4e61b0f3a1b2d095e4fccc0173d245d7e5b4c9b4d402fe1d8d04756e1f76f\\" result = automated_ftp_client(hostname, username, password, directory, filename, checksum) print(result) # Output: True or False based on file integrity verification ``` **Note:** - You may assume that the `hashlib` module is available for calculating SHA256 checksums.","solution":"import ftplib import hashlib def calculate_sha256_checksum(file_path): Calculate the SHA256 checksum of the given file. :param file_path: The path to the file for which to calculate the checksum. :return: The hexadecimal SHA256 checksum string. sha256 = hashlib.sha256() with open(file_path, \'rb\') as file: for chunk in iter(lambda: file.read(4096), b\\"\\"): sha256.update(chunk) return sha256.hexdigest() def automated_ftp_client(hostname: str, username: str, password: str, directory: str, filename: str, checksum: str) -> bool: try: # Connect to the FTP server ftp = ftplib.FTP(hostname) # Log in with the provided username and password ftp.login(user=username, passwd=password) # Change to the specified directory ftp.cwd(directory) # Listing the contents of the directory contents = ftp.nlst() print(f\\"Contents of directory {directory}: {contents}\\") # Download the specified file in binary mode local_filename = f\\"/tmp/{filename}\\" with open(local_filename, \'wb\') as local_file: ftp.retrbinary(f\\"RETR {filename}\\", local_file.write) # Calculate the SHA256 checksum of the downloaded file downloaded_checksum = calculate_sha256_checksum(local_filename) # Close the FTP connection ftp.quit() # Verify the checksum return downloaded_checksum == checksum except ftplib.all_errors as e: print(f\\"FTP error: {e}\\") return False"},{"question":"# Coding Assessment: Custom DataLoader and Dataset Implementation Objective Implement a custom dataset and utilize PyTorch\'s `DataLoader` to load data using both map-style and iterable-style datasets. Customize the data loading order with a custom sampler and use a custom `collate_fn` to prepare batches. Task 1. Implement a custom map-style dataset. 2. Implement a custom iterable-style dataset. 3. Implement a custom sampler for the map-style dataset. 4. Use `DataLoader` to load data from both datasets, applying the custom sampler to the map-style dataset. 5. Implement a custom `collate_fn` to handle batching. Details 1. **Custom Map-style Dataset:** - The dataset should contain `n` random samples, where each sample is a tuple containing an integer index and its square. - Implement the `__getitem__` and `__len__` methods. 2. **Custom Iterable-style Dataset:** - This dataset should generate an infinite stream of tuples containing an integer index and its square. 3. **Custom Sampler:** - The sampler should yield indices in a shuffled order. 4. **DataLoader Usage:** - Create two `DataLoader` instances: one for the map-style dataset with the custom sampler and another for the iterable-style dataset. - Set the batch size to 4 for both data loaders. - Set `num_workers` to 2 for multi-process data loading. 5. **Custom `collate_fn`:** - The custom `collate_fn` should ensure that the returned batches are in the form of two lists: one for indices and one for squared values. Input Your code should handle the following input: - `num_samples`: Integer representing the number of samples in the map-style dataset. - `num_batches`: Integer representing the number of batches to retrieve from each `DataLoader`. Output The custom `DataLoader` should print the batched data for both datasets. Constraints - The map-style dataset should have a fixed number of samples. - The iterable-style dataset should run indefinitely until the specified number of batches are retrieved. - Ensure your solution is optimized for multi-process data loading. Example ```python # Define number of samples and batches num_samples = 10 num_batches = 3 # Implement the map-style dataset, iterable-style dataset, custom sampler, and custom collate_fn # Initialize the DataLoader for both datasets and print the output as described # Expected output (example; actual output may differ due to randomness in sampling): # Map-style dataset batches: # ([1, 5, 9, 2], [1, 25, 81, 4]) # ([8, 0, 7, 3], [64, 0, 49, 9]) # ([4, 6], [16, 36]) # Iterable-style dataset batches: # ([0, 1, 2, 3], [0, 1, 4, 9]) # ([4, 5, 6, 7], [16, 25, 36, 49]) # ([8, 9, 10, 11], [64, 81, 100, 121]) ``` Implementation Guidance Refer to the provided PyTorch documentation for the structure and API details for `Dataset`, `IterableDataset`, `DataLoader`, `Sampler`, and related components.","solution":"import torch from torch.utils.data import Dataset, IterableDataset, DataLoader, Sampler import random import itertools class CustomMapStyleDataset(Dataset): def __init__(self, num_samples): self.num_samples = num_samples def __getitem__(self, index): return index, index ** 2 def __len__(self): return self.num_samples class CustomIterableStyleDataset(IterableDataset): def __iter__(self): for index in itertools.count(start=0): yield index, index ** 2 class CustomSampler(Sampler): def __init__(self, data_source): self.data_source = data_source def __iter__(self): return iter(random.sample(range(len(self.data_source)), len(self.data_source))) def __len__(self): return len(self.data_source) def custom_collate_fn(batch): indices, squares = zip(*batch) return list(indices), list(squares) def load_data(num_samples, num_batches): map_style_dataset = CustomMapStyleDataset(num_samples) iterable_style_dataset = CustomIterableStyleDataset() custom_sampler = CustomSampler(map_style_dataset) map_data_loader = DataLoader( map_style_dataset, batch_size=4, sampler=custom_sampler, num_workers=2, collate_fn=custom_collate_fn) iterable_data_loader = DataLoader( iterable_style_dataset, batch_size=4, num_workers=2, collate_fn=custom_collate_fn) print(\\"Map-style dataset batches:\\") for i, batch in enumerate(map_data_loader): if i >= num_batches: break print(batch) print(\\"nIterable-style dataset batches:\\") for i, batch in enumerate(iterable_data_loader): if i >= num_batches: break print(batch) # Example usage # load_data(10, 3) # Uncomment for actual use"},{"question":"You are required to implement a function that determines if two given PyTorch tensors can be broadcasted together according to the rules of broadcasting described above. Your function should iterate over the dimensions of the tensors from the last to the first and apply the broadcasting rules. # Function Signature ```python def are_broadcastable(tensor1: torch.Tensor, tensor2: torch.Tensor) -> bool: pass ``` # Input - `tensor1, tensor2`: Two PyTorch tensors of arbitrary shape. # Output - A boolean value `True` if the tensors are broadcastable, otherwise `False`. # Constraints - You cannot use any built-in PyTorch functions that directly determine broadcastability. - The tensors should be checked based on broadcasting rules: - Each tensor must have at least one dimension. - Dimensions must either be equal, one of them is 1, or one dimension does not exist. # Example ```python import torch # Example 1: t1 = torch.empty(5, 1, 4, 1) t2 = torch.empty(3, 1, 1) assert are_broadcastable(t1, t2) == True # Example 2: t1 = torch.empty(5, 2, 4, 1) t2 = torch.empty(3, 1, 1) assert are_broadcastable(t1, t2) == False # Example 3: t1 = torch.empty(1) t2 = torch.empty(3, 1, 7) assert are_broadcastable(t1, t2) == True ``` **Note:** - Carefully handle different tensor shapes and apply the broadcasting rules accurately. - Consider edge cases such as tensors with a single dimension or scalar values.","solution":"import torch def are_broadcastable(tensor1: torch.Tensor, tensor2: torch.Tensor) -> bool: This function determines if two given PyTorch tensors can be broadcasted together according to the rules of broadcasting in NumPy/PyTorch. shape1 = list(tensor1.shape) shape2 = list(tensor2.shape) while len(shape1) < len(shape2): shape1.insert(0, 1) while len(shape2) < len(shape1): shape2.insert(0, 1) for dim1, dim2 in zip(shape1[::-1], shape2[::-1]): if dim1 != dim2 and dim1 != 1 and dim2 != 1: return False return True"},{"question":"# Question: Advanced Theme and Display Configurations Using Seaborn You are tasked with creating a detailed and aesthetically pleasing visualization using the Seaborn package. Objective Write a Python script to generate and configure a scatter plot with the following specifications: 1. **Data**: - Create a dataset with two variables `x` and `y` each containing 100 data points, where `x` is a sequence of numbers from 1 to 100, and `y` is generated by adding some random noise to `x`. 2. **Plot Configuration**: - Use the Seaborn `Plot` module from `seaborn.objects` for creating the plot. - Set the plot\'s theme background color to light grey. - Apply the \'darkgrid\' style from Seaborn\'s predefined styles. - Sync with Matplotlib\'s global `rcParams`. - Reset to Seaborn\'s default theme after configuring the plot. 3. **Display Configuration**: - Change the plot display format to SVG. - Disable HiDPI rendering. - Set the image scaling factor to 0.8. 4. **Visual Elements**: - Add appropriate axis labels (`\'X-axis\'` for x, `\'Y-axis\'` for y). - Set a plot title `\'Scatter Plot with Custom Configuration\'`. 5. **Output**: - Display the plot inline in a Jupyter notebook cell. # Input and Output - **Input**: No input is required; specify the dataset within the script. - **Output**: Visualize an inline scatter plot with the described properties. # Constraints and Limitations - Use only the seaborn and matplotlib packages. - The dataset should be generated within the script; do not read from external files. # Performance Requirements - Ensure the script runs in a reasonable time frame. # Example An example code snippet highlighting dataset creation and basic plot setup is given below (this won\'t perform the full required configuration): ```python # Example Dataset Creation import numpy as np import pandas as pd # Generating the data x = np.arange(1, 101) y = x + np.random.normal(0, 10, 100) data = pd.DataFrame({\'x\': x, \'y\': y}) # Basic plot setup import seaborn.objects as so p = so.Plot(data, x=\'x\', y=\'y\').add(so.Line()) p.show() ``` Implementing the above specifications should demonstrate your understanding of Seaborn\'s theme and display configuration capabilities.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(): # Generate data np.random.seed(42) # For reproducibility x = np.arange(1, 101) y = x + np.random.normal(0, 10, 100) data = pd.DataFrame({\'x\': x, \'y\': y}) # Set the plot configuration sns.set_style(\'darkgrid\') plt.rcParams.update({\'axes.facecolor\': \'lightgrey\'}) # Change display format to SVG from IPython.display import set_matplotlib_formats set_matplotlib_formats(\'svg\') # Set image scaling factor plt.figure(figsize=(8*0.8, 6*0.8)) plt.gcf().set_dpi(100) # Create the plot p = sns.scatterplot(data=data, x=\'x\', y=\'y\') p.set_title(\'Scatter Plot with Custom Configuration\') p.set(xlabel=\'X-axis\', ylabel=\'Y-axis\') # Display the plot plt.show() # Reset to Seaborn\'s default theme sns.reset_orig() # Invoke the function to create the scatter plot create_scatter_plot()"},{"question":"Coding Assessment Question # Objective Implement a function that reads configuration data from an `.ini` file, modifies specific values, and writes the changes back to the file using the `configparser` module in Python. # Problem Statement You are given an `.ini` configuration file with the following structure: ```ini [General] appname = MyApp version = 1.0.0 [UserSettings] theme = light notifications = enabled ``` Implement a function `modify_config(file_path: str, section: str, option: str, new_value: str) -> None` that performs the following operations: 1. Reads the configuration from the file located at `file_path`. 2. Modifies the value of the specified `option` under the given `section` to `new_value`. 3. Writes the updated configuration back to the same file. # Input - `file_path` (str): The path to the `.ini` configuration file. - `section` (str): The section under which the option is to be modified. - `option` (str): The option whose value is to be changed. - `new_value` (str): The new value to set for the specified option. # Constraints - The input file always exists and is in the correct format. - The provided section and option always exist in the file. - The function does not need to return any value, it should only update the file. # Example Usage ```python # Assume \'config.ini\' contains the structure provided above modify_config(\'config.ini\', \'UserSettings\', \'theme\', \'dark\') # After execution, \'config.ini\' should be updated as follows: # [General] # appname = MyApp # version = 1.0.0 # # [UserSettings] # theme = dark # notifications = enabled ``` # Notes - Use the `configparser` module for reading, modifying, and writing the configuration file. - Ensure the file remains properly formatted after the update.","solution":"import configparser def modify_config(file_path: str, section: str, option: str, new_value: str) -> None: Modifies the value of the specified option under the given section in the config file. Args: file_path (str): The path to the .ini configuration file. section (str): The section under which the option is to be modified. option (str): The option whose value is to be changed. new_value (str): The new value to set for the specified option. # Create a ConfigParser object and read the config file config = configparser.ConfigParser() config.read(file_path) # Modify the specified option config.set(section, option, new_value) # Write the changes back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# Task: Implementing a Custom Object Type in Python 310 Objective Implement a custom object type in Python 310 that demonstrates understanding of object structures, type objects, and memory management. Problem Statement You are required to design a custom object type named `CustomVector` that represents a vector in 3-dimensional space. Your implementation should include: 1. **Attributes**: - `x`, `y`, `z` coordinates of the vector (default to `0.0`). 2. **Methods**: - `magnitude()`: Returns the magnitude (or length) of the vector. - `normalize()`: Returns a new `CustomVector` object which is the normalized (unit vector) version of the input vector. - `__add__()`: Overloads the `+` operator to add two vectors. - `__sub__()`: Overloads the `-` operator to subtract two vectors. - `__mul__()`: Overloads the `*` operator to perform dot product of two vectors or scalar multiplication. 3. **Properties**: - Implement getter and setter methods for `x`, `y`, and `z` attributes. Ensure they can handle setting values individually and offer validation (e.g., they should only accept float or int). 4. **Memory Management**: - Demonstrate understanding of memory allocation and deallocation within the Python 310 environment. Constraints - Vectors must handle floating-point arithmetic accurately. - Raise appropriate exceptions (e.g., `TypeError` for operations with incompatible types). Performance Requirements - Efficient memory usage. - Ensure the magnitude calculation is computationally efficient. Input Format - `CustomVector` should be instantiated as `CustomVector(x, y, z)` where `x`, `y`, and `z` are optional and can be passed as float. Default is `0.0` for each if not provided. Output Format - The `magnitude()` method should return a float value. - The `normalize()` method should return a new `CustomVector` instance. - The overloaded operators should return a new `CustomVector` or a float depending on the operation. Example ```python v1 = CustomVector(1.0, 2.0, 3.0) v2 = CustomVector(4.0, 5.0, 6.0) print(v1.magnitude()) # Should output: 3.74165738677 print(v1.normalize()) # Should output a CustomVector with coordinates approximately (0.267, 0.534, 0.801) print(v1 + v2) # Should output a new CustomVector (5.0, 7.0, 9.0) print(v1 - v2) # Should output a new CustomVector (-3.0, -3.0, -3.0) print(v1 * v2) # Should output: 32.0 (dot product) print(v1 * 3) # Should output a new CustomVector (3.0, 6.0, 9.0) ```","solution":"import math class CustomVector: def __init__(self, x=0.0, y=0.0, z=0.0): self._x = None self._y = None self._z = None self.x = x self.y = y self.z = z @property def x(self): return self._x @x.setter def x(self, value): self._validate_value(value) self._x = value @property def y(self): return self._y @y.setter def y(self, value): self._validate_value(value) self._y = value @property def z(self): return self._z @z.setter def z(self, value): self._validate_value(value) self._z = value def _validate_value(self, value): if not isinstance(value, (int, float)): raise TypeError(\\"Coordinates must be int or float\\") def magnitude(self): return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2) def normalize(self): mag = self.magnitude() if mag == 0: raise ValueError(\\"Cannot normalize a zero vector\\") return CustomVector(self.x / mag, self.y / mag, self.z / mag) def __add__(self, other): if not isinstance(other, CustomVector): raise TypeError(\\"Addition only supported between CustomVector instances\\") return CustomVector(self.x + other.x, self.y + other.y, self.z + other.z) def __sub__(self, other): if not isinstance(other, CustomVector): raise TypeError(\\"Subtraction only supported between CustomVector instances\\") return CustomVector(self.x - other.x, self.y - other.y, self.z - other.z) def __mul__(self, other): if isinstance(other, CustomVector): # Dot product return self.x * other.x + self.y * other.y + self.z * other.z elif isinstance(other, (int, float)): # Scalar multiplication return CustomVector(self.x * other, self.y * other, self.z * other) else: raise TypeError(\\"Multiplication supported only with int, float, or CustomVector\\") def __repr__(self): return f\\"CustomVector({self.x}, {self.y}, {self.z})\\""},{"question":"# **Coding Assessment Question** Objective: To test your knowledge of Python\'s `enum` module, particularly focusing on creating enumerations, automatic value assignment, unique values, and custom methods. Problem Statement: You are asked to implement an enumeration to represent various levels of a game and associate human-readable titles with them. Additionally, you need to implement a method to get details of each level and ensure no duplicate values exist. Use the `enum` module to perform the following tasks: 1. Define an enumeration class `Level` using the `Enum` base class with members: - `BEGINNER` with value `1` - `INTERMEDIATE` with value assigned automatically - `ADVANCED` with value assigned automatically - `EXPERT` with value `4` 2. Use the `unique` decorator to ensure all enum members have unique values. 3. Implement a method within the `Level` enumeration called `describe` that returns a string in the following format: `\\"Level: {name}, Value: {value}, Title: {title}\\"` where `title` for `BEGINNER`, `INTERMEDIATE`, `ADVANCED`, `EXPERT` are \'Novice\', \'Skilled\', \'Proficient\', and \'Master\' respectively. 4. Write a function `get_level_details` that takes an enum member (e.g., `Level.BEGINNER`) and returns the string from the `describe` method of that member. Input and Output: - **Function Signature:** ```python def get_level_details(member: Level) -> str: ``` - **Example:** ```python get_level_details(Level.BEGINNER) ``` Should return: ```python \'Level: BEGINNER, Value: 1, Title: Novice\' ``` ```python get_level_details(Level.INTERMEDIATE) ``` Should return: ```python \'Level: INTERMEDIATE, Value: 2, Title: Skilled\' ``` Constraints: - Use the `Enum` and `unique` features from the `enum` module. - Use the `auto` helper for automatic value assignments where mentioned. - Ensure the method `describe` is implemented within the `Level` class. - Do not manually assign values to `INTERMEDIATE` and `ADVANCED`. Evaluation Criteria: - Correctness of the enum definitions and values. - Proper usage of automatic value assignment. - Uniqueness constraint on enum values. - Correct implementation of the `describe` method and the `get_level_details` function.","solution":"from enum import Enum, auto, unique @unique class Level(Enum): BEGINNER = 1 INTERMEDIATE = auto() ADVANCED = auto() EXPERT = 4 def describe(self): titles = { Level.BEGINNER: \'Novice\', Level.INTERMEDIATE: \'Skilled\', Level.ADVANCED: \'Proficient\', Level.EXPERT: \'Master\' } return f\\"Level: {self.name}, Value: {self.value}, Title: {titles[self]}\\" def get_level_details(member: Level) -> str: return member.describe()"},{"question":"**Question: Implement a Quoted-Printable File Encoder/Decoder** You are required to write two utility functions using the `quopri` module to handle encoding and decoding of files in quoted-printable format. The functions should be able to: 1. **Encode a file**: Given an input file, encode its contents into quoted-printable format and write the result to an output file. 2. **Decode a file**: Given an encoded input file, decode its contents from quoted-printable format and write the result to an output file. **Function Signatures:** ```python def encode_file(input_file_path: str, output_file_path: str, quotetabs: bool = False, header: bool = False) -> None: Encode the contents of the input file using quoted-printable encoding and write to the output file. :param input_file_path: Path to the binary input file :param output_file_path: Path to the binary output file :param quotetabs: Encode embedded spaces and tabs if True :param header: Encode spaces as underscores if True :return: None pass def decode_file(input_file_path: str, output_file_path: str, header: bool = False) -> None: Decode the contents of the encoded input file using quoted-printable decoding and write to the output file. :param input_file_path: Path to the binary encoded input file :param output_file_path: Path to the binary output file :param header: Decode underscores as spaces if True :return: None pass ``` **Input:** 1. `encode_file`: The paths to the input and output files (strings), and optional boolean flags for `quotetabs` and `header`. 2. `decode_file`: The paths to the input and output files (strings) and an optional boolean flag for `header`. **Output:** None. The functions should perform the operations in-place, modifying the output files directly. **Constraints:** 1. Both functions should handle input/output as binary files. 2. Ensure proper use of the `quopri` methods for encoding and decoding. 3. Implement error handling for file operations (e.g., FileNotFoundError). **Example:** ```python # Example usage: # Given a file \'plain_text.txt\' with content \\"Hello, World!\\": # Content: \'Hello, World!\' # Encoding: encode_file(\'plain_text.txt\', \'encoded_text.txt\', quotetabs=True) # Content of \'encoded_text.txt\' might look like: # \'Hello=2C=20World=21\' # Decoding: decode_file(\'encoded_text.txt\', \'decoded_text.txt\') # Content of \'decoded_text.txt\' should be: # \'Hello, World!\' ``` Implement the `encode_file` and `decode_file` functions to correctly perform the described operations.","solution":"import quopri def encode_file(input_file_path: str, output_file_path: str, quotetabs: bool = False, header: bool = False) -> None: Encode the contents of the input file using quoted-printable encoding and write to the output file. :param input_file_path: Path to the binary input file :param output_file_path: Path to the binary output file :param quotetabs: Encode embedded spaces and tabs if True :param header: Encode spaces as underscores if True :return: None try: with open(input_file_path, \'rb\') as input_file, open(output_file_path, \'wb\') as output_file: quopri.encode(input_file, output_file, quotetabs=quotetabs, header=header) except FileNotFoundError: print(f\\"File not found: {input_file_path}\\") except Exception as e: print(f\\"An error occurred: {e}\\") def decode_file(input_file_path: str, output_file_path: str, header: bool = False) -> None: Decode the contents of the encoded input file using quoted-printable decoding and write to the output file. :param input_file_path: Path to the binary encoded input file :param output_file_path: Path to the binary output file :param header: Decode underscores as spaces if True :return: None try: with open(input_file_path, \'rb\') as input_file, open(output_file_path, \'wb\') as output_file: quopri.decode(input_file, output_file, header=header) except FileNotFoundError: print(f\\"File not found: {input_file_path}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are tasked with building an advanced data processing pipeline that involves combining multiple types of data using `collections.ChainMap`, counting occurrences using `collections.Counter`, and maintaining an order of processing using `collections.OrderedDict`. Requirements: 1. **Inputs:** - A list of dictionaries where each dictionary contains information about items and their respective categories. E.g., ```python data = [ {\\"category\\": \\"fruits\\", \\"item\\": \\"apple\\"}, {\\"category\\": \\"fruits\\", \\"item\\": \\"banana\\"}, {\\"category\\": \\"vegetables\\", \\"item\\": \\"carrot\\"}, {\\"category\\": \\"fruits\\", \\"item\\": \\"apple\\"}, {\\"category\\": \\"vegetables\\", \\"item\\": \\"broccoli\\"} ] ``` 2. **Outputs:** - **Combined View:** A single `ChainMap` view of all input dictionaries. - **Item Counts per Category:** A dictionary that counts occurrences of each item within its category using `Counter`. - **Ordered Insertion:** An `OrderedDict` that maintains the categories and lists all items added in the order they were first seen. Constraints: - The input data list length (n) can be up to 10^6. - Each dictionary within the list will have at least a \\"category\\" and an \\"item\\" key. Function to Implement: ```python def process_data(data: List[Dict[str, str]]) -> Tuple[collections.ChainMap, Dict[str, collections.Counter], collections.OrderedDict]: Processes the input data and returns required data structures. Args: data: List[Dict[str, str]] - A list of dictionaries with \\"category\\" and \\"item\\" keys. Returns: Tuple of: - ChainMap: Combined view of all input dictionaries. - Dict[str, Counter]: Dictionary of item counts per category. - OrderedDict: Ordered dictionary maintaining categories and items list. ``` Example: Given the sample `data` above, your function should return: 1. **Combined View:** ```python ChainMap({\'category\': \'fruits\', \'item\': \'apple\'}, {\'category\': \'fruits\', \'item\': \'banana\'}, {\'category\': \'vegetables\', \'item\': \'carrot\'}, {\'category\': \'fruits\', \'item\': \'apple\'}, {\'category\': \'vegetables\', \'item\': \'broccoli\'}) ``` 2. **Item Counts per Category:** ```python { \\"fruits\\": Counter({\\"apple\\": 2, \\"banana\\": 1}), \\"vegetables\\": Counter({\\"carrot\\": 1, \\"broccoli\\": 1}) } ``` 3. **Ordered Insertion:** ```python OrderedDict({ \\"fruits\\": [\\"apple\\", \\"banana\\", \\"apple\\"], \\"vegetables\\": [\\"carrot\\", \\"broccoli\\"] }) ``` Consider performance implications due to the large possible size of the input and optimize your implementation accordingly.","solution":"from typing import List, Dict, Tuple import collections def process_data(data: List[Dict[str, str]]) -> Tuple[collections.ChainMap, Dict[str, collections.Counter], collections.OrderedDict]: combined_view = collections.ChainMap(*data) item_counts_per_category = collections.defaultdict(collections.Counter) for entry in data: category = entry[\'category\'] item = entry[\'item\'] item_counts_per_category[category][item] += 1 ordered_insertion = collections.OrderedDict() for entry in data: category = entry[\'category\'] item = entry[\'item\'] if category not in ordered_insertion: ordered_insertion[category] = [] ordered_insertion[category].append(item) # Convert defaultdict to a normal dict for output item_counts_per_category = dict(item_counts_per_category) return combined_view, item_counts_per_category, ordered_insertion"},{"question":"You are given a dataset that records the educational performance of students in different subjects over multiple terms. The dataset is \\"student_performance.csv\\" and consists of the following columns: - `student_id`: Unique identifier for each student. - `term`: The term during which the grades were recorded. - `subject`: The subject for which the grade is given. - `grade`: The grade that the student received (A, B, C, D, F). Task: 1. Load the dataset into a pandas DataFrame. 2. Convert the dataset into both long-form and wide-form data. 3. Create the following visualizations using Seaborn: - A line plot that shows the change in average grade for each subject across the terms using long-form data. - A box plot that shows the distribution of grades for each subject using wide-form data. Input: - A CSV file named \\"student_performance.csv\\". Output: - Two Seaborn plots as described in the task. Constraints: - Use Seaborn and pandas to handle the data transformation and visualizations. - Ensure that the plots are properly labeled with meaningful axis titles and legends. # Example: Assume the CSV file contains the following data: ``` student_id,term,subject,grade 1,T1,Math,A 1,T1,Science,B ... ``` You are expected to write a Python script that performs the aforementioned tasks and generates the plots accordingly.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_data(file_path): return pd.read_csv(file_path) def transform_to_long_form(df): return df.copy() def transform_to_wide_form(df): # Pivot the table to get student_id as index, terms as columns, and the combination of subject and grade as values df_wide = df.pivot_table(index=\'student_id\', columns=[\'term\', \'subject\'], values=\'grade\', aggfunc=\'first\') return df_wide def plot_average_grade_per_subject(df_long): # Map grades to numerical values for averaging grade_map = {\'A\': 4, \'B\': 3, \'C\': 2, \'D\': 1, \'F\': 0} df_long[\'grade_numeric\'] = df_long[\'grade\'].map(grade_map) avg_grades = df_long.groupby([\'term\', \'subject\'])[\'grade_numeric\'].mean().reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(data=avg_grades, x=\'term\', y=\'grade_numeric\', hue=\'subject\', marker=\'o\') plt.title(\'Average Grade per Subject Across Terms\') plt.xlabel(\'Term\') plt.ylabel(\'Average Grade\') plt.legend(title=\'Subject\') plt.show() def plot_grade_distribution_per_subject(df_wide): df_long = df_wide.melt(value_name=\'grade\').dropna() plt.figure(figsize=(10, 6)) sns.boxplot(data=df_long, x=\'variable_1\', y=\'grade\') plt.title(\'Grade Distribution per Subject\') plt.xlabel(\'Subject\') plt.ylabel(\'Grade\') plt.show() # Example usage: # df = load_data(\\"student_performance.csv\\") # df_long = transform_to_long_form(df) # df_wide = transform_to_wide_form(df) # plot_average_grade_per_subject(df_long) # plot_grade_distribution_per_subject(df_wide)"},{"question":"Problem Statement You are tasked to create a utility function `get_annotations_dict` which extracts the `__annotations__` dictionary from various Python objects. Your function should handle classes, functions, modules, and other callables. Additionally, it should ensure that stringized annotations (if any) are properly unstringized into their actual types. Function Signature ```python def get_annotations_dict(obj: Any) -> dict: pass ``` Input - `obj`: This can be a Python class, function, module, or any other callable. Output - Returns a dictionary containing the annotations of the given object. If the object does not have any annotations, it should return an empty dictionary. Constraints - Do not use third-party libraries; only use standard Python modules. - Handle different Python versions (3.9 and older, 3.10 and newer) gracefully. - Handle nested callables from `functools.partial` or similar wrappers. - Ensure performance is optimal and unnecessary evaluations are avoided. Example ```python from functools import partial def foo(a: \\"str\\", b: int) -> \\"float\\": return float(b) # foo has stringized annotations print(get_annotations_dict(foo)) # Expected Output: {\'a\': str, \'b\': int, \'return\': float} partial_foo = partial(foo, b=5) # partial_foo should also have the annotations resolved print(get_annotations_dict(partial_foo)) # Expected Output: {\'a\': str, \'b\': int, \'return\': float} ``` Requirements - Ensure that `get_annotations_dict` handles stringized annotations and correctly evaluates them. - The function should work correctly for modules, classes, and nested callables. - No direct modification of the `__annotations__` attribute in any object. - Robust error handling and type checking. Notes Refer to Python\'s `inspect` module and other relevant sections of the standard library to retrieve and evaluate annotations. For manually unstringizing annotations, implement a logic similar to what is described in the documentation.","solution":"from typing import Any, get_type_hints from functools import partial def get_annotations_dict(obj: Any) -> dict: Extracts the __annotations__ dictionary from the given object. This works with classes, functions, modules, and other callables, handling stringized annotations appropriately. try: if isinstance(obj, partial): obj = obj.func annotations = get_type_hints(obj) except Exception: annotations = {} return annotations"},{"question":"# Python Coding Assessment Question Question You are tasked with designing a Python class to manage a simple inventory system for a retail store. The inventory system should support adding new products, selling products, and generating reports on current stock. # Requirements: 1. **Class Structure:** - Define a `Product` class with the following properties: - `name`: A string representing the product name. - `price`: A float representing the product price. - `stock`: An integer representing the quantity of the product in stock. - Define an `Inventory` class to manage multiple `Product` instances. 2. **Inventory Management:** - In the `Inventory` class, create methods to: - `add_product(product, quantity=1)`: Adds a specified quantity of a `Product` to the inventory. - `sell_product(product_name, quantity=1)`: Sells a specified quantity of a product from the inventory, reducing the stock. If the product is out-of-stock, raise an `Exception` with the message \\"Out of stock\\". - `get_stock_report()`: Returns a dictionary where the keys are product names, and the values are the quantities currently in stock. # Input and Output: - **Input**: Creation of `Product` and `Inventory` instances, followed by method calls on the `Inventory` instance. - **Output**: Result of `get_stock_report()` and any exceptions raised during `sell_product`. # Constraints: - Product names are unique within the inventory. - The quantity of products added or sold is always a positive integer. # Performance Requirements: - Ensure that your solution efficiently manages the inventory, even with a large number of products. # Example: ```python # Define the Product and Inventory classes here # Create instances of Product apple = Product(\\"Apple\\", 0.50, 100) banana = Product(\\"Banana\\", 0.30, 50) # Create an instance of Inventory store_inventory = Inventory() # Add products to the inventory store_inventory.add_product(apple, 100) store_inventory.add_product(banana, 50) # Sell some products store_inventory.sell_product(\\"Apple\\", 20) store_inventory.sell_product(\\"Banana\\", 10) # Generate stock report report = store_inventory.get_stock_report() print(report) # Expected output: {\'Apple\': 80, \'Banana\': 40} # Attempt to sell a product that is out of stock try: store_inventory.sell_product(\\"Apple\\", 200) except Exception as e: print(e) # Expected output: \\"Out of stock\\" ``` Implement the `Product` and `Inventory` classes according to the specifications provided above.","solution":"class Product: def __init__(self, name, price, stock): self.name = name self.price = price self.stock = stock class Inventory: def __init__(self): self.products = {} def add_product(self, product, quantity=1): if product.name in self.products: self.products[product.name].stock += quantity else: product.stock = quantity self.products[product.name] = product def sell_product(self, product_name, quantity=1): if product_name in self.products and self.products[product_name].stock >= quantity: self.products[product_name].stock -= quantity else: raise Exception(\\"Out of stock\\") def get_stock_report(self): return {name: product.stock for name, product in self.products.items()}"},{"question":"**Objective:** Implement a function that emulates a file operation and handles multiple possible error conditions using the `errno` module. The function should provide meaningful messages to the user based on the error encountered. **Function Signature:** ```python def file_operation_emulator(filepath: str) -> str: pass ``` **Description:** 1. The function `file_operation_emulator()` should take a single string argument `filepath`, which represents the path to a file. 2. The function should attempt basic file operations such as opening the file, reading from it, and closing it. 3. The function should handle the following specific errors using the `errno` module and return appropriate error messages: - File not found (`errno.ENOENT`): Return \\"File not found.\\" - Permission denied (`errno.EACCES`): Return \\"Permission denied.\\" - Too many open files (`errno.EMFILE`): Return \\"Too many open files.\\" - Any other OS-related error: Return \\"An unexpected error occurred: \\" followed by the error message obtained from `os.strerror()`. **Input:** - A single argument `filepath`, which is a string representing the path to the file. **Output:** - A string message indicating the result of the file operation or the type of error encountered. **Constraints:** - You are not required to actually perform file operations on the disk. Instead, simulate the operations and raise appropriate `OSError` with specific `errno`. - Make use of `errno.errorcode` and `os.strerror()` for handling and interpreting errors. **Example:** ```python # Example usages: print(file_operation_emulator(\'/path/to/nonexistent/file\')) # Output: \\"File not found.\\" print(file_operation_emulator(\'/path/to/protected/file\')) # Output: \\"Permission denied.\\" print(file_operation_emulator(\'/path/to/file/with/too/many/open/files\')) # Output: \\"Too many open files.\\" print(file_operation_emulator(\'/path/to/file/with/unknown/error\')) # Output: \\"An unexpected error occurred: <detailed error message>\\" ```","solution":"import errno import os def file_operation_emulator(filepath: str) -> str: Emulates basic file operations and handles errors using the errno module. Arguments: filepath (str): The path to the file that needs to be operated on. Returns: str: A message indicating the result of the file operation or the type of error encountered. try: # Simulate opening a file if filepath == \'/path/to/nonexistent/file\': raise OSError(errno.ENOENT, os.strerror(errno.ENOENT)) elif filepath == \'/path/to/protected/file\': raise OSError(errno.EACCES, os.strerror(errno.EACCES)) elif filepath == \'/path/to/file/with/too/many/open/files\': raise OSError(errno.EMFILE, os.strerror(errno.EMFILE)) elif filepath == \'/path/to/file/with/unknown/error\': raise OSError(9999, \'Custom unknown error message\') # Simulate file operation succeeded return \\"File operation successful.\\" except OSError as e: if e.errno == errno.ENOENT: return \\"File not found.\\" elif e.errno == errno.EACCES: return \\"Permission denied.\\" elif e.errno == errno.EMFILE: return \\"Too many open files.\\" else: return f\\"An unexpected error occurred: {e.strerror}\\""},{"question":"Coding Assessment Question # Context You are working with the `email` package in Python and need to handle various email message parsing, generating, and validation requirements. To enforce certain standards and behaviors, you need to create a custom policy that extends the behavior of the default `EmailPolicy` class. # Task 1. **Create a Custom Policy Class:** - Design a custom policy class `CustomEmailPolicy` that extends the `EmailPolicy` class. - This custom policy should have the following customizations: - Set `max_line_length` to 100. - Use `\'rn\'` as the line separator. - Set `cte_type` to `\'8bit\'`. - Enable raising exceptions on defects by setting `raise_on_defect` to `True`. - Add a custom method `header_max_count` to limit the number of \'Subject\' headers to 1. 2. **Create a Function to Demonstrate Custom Policy Usage:** - Implement a function `process_email_with_custom_policy(file_path: str) -> str` that reads an email message from a given file, processes it using the `CustomEmailPolicy`, and returns the serialized string representation of the email. - Use the `message_from_binary_file` method from the `email` package to read the email. - Use the `BytesGenerator` with the custom policy to serialize the email. # Input - `file_path` (str): The path to the email message file to process. # Output - Returns a string representing the serialized email message following the custom policy. # Constraints - Assume the email file exists at the specified path. - The custom method `header_max_count` should only be applied to the \'Subject\' header. - The serialized email should respect the custom policy\'s configurations. # Example ```python class CustomEmailPolicy(EmailPolicy): ... # Implement the custom policy as described ... def process_email_with_custom_policy(file_path: str) -> str: ... # Implement the function to read, process, and serialize email with the custom policy ... # Example usage: # result = process_email_with_custom_policy(\'path/to/email.txt\') # print(result) ``` Note: The provided `process_email_with_custom_policy` function should ensure the email is processed and serialized correctly according to the custom policies and display any headers and formatting defined by these policies. # Solution ```python from email import message_from_binary_file, policy from email.policy import EmailPolicy from email.generator import BytesGenerator from typing import List from subprocess import Popen, PIPE class CustomEmailPolicy(EmailPolicy): def __init__(self, **kw): super().__init__( max_line_length=100, linesep=\'rn\', cte_type=\'8bit\', raise_on_defect=True, **kw ) def header_max_count(self, name): if name.lower() == \'subject\': return 1 return super().header_max_count(name) def process_email_with_custom_policy(file_path: str) -> str: custom_policy = CustomEmailPolicy() with open(file_path, \'rb\') as f: msg = message_from_binary_file(f, policy=custom_policy) with Popen([\'cat\'], stdin=PIPE, stdout=PIPE) as p: # Using \'cat\' for example purposes generator = BytesGenerator(p.stdin, policy=custom_policy) generator.flatten(msg) p.stdin.close() result = p.stdout.read().decode(\'utf-8\') return result ```","solution":"from email import message_from_binary_file from email.policy import EmailPolicy from email.generator import BytesGenerator from typing import List class CustomEmailPolicy(EmailPolicy): def __init__(self, **kw): super().__init__( max_line_length=100, linesep=\'rn\', cte_type=\'8bit\', raise_on_defect=True, **kw ) def header_max_count(self, name): if name.lower() == \'subject\': return 1 return super().header_max_count(name) def process_email_with_custom_policy(file_path: str) -> str: custom_policy = CustomEmailPolicy() with open(file_path, \'rb\') as f: msg = message_from_binary_file(f, policy=custom_policy) from io import BytesIO output = BytesIO() generator = BytesGenerator(output, policy=custom_policy) generator.flatten(msg) return output.getvalue().decode(\'utf-8\')"},{"question":"You are provided with two datasets: a `sales` dataset and a `product` dataset. Your task is to visualize these datasets using seaborn\'s `kdeplot` function to demonstrate your comprehension of KDE plots in both univariate and bivariate contexts. Datasets 1. **sales** dataset: - `total_sales`: Total sales in dollars (float). - `region`: Region where the sales occurred (categorical: \'North\', \'South\', \'East\', \'West\'). 2. **product** dataset: - `units_sold`: Number of units sold (integer). - `unit_price`: Price per unit in dollars (float). **Dataset Example:** ```python import pandas as pd sales = pd.DataFrame({ \'total_sales\': [250, 300, 150, 700, 800, 500, 450, 600], \'region\': [\'North\', \'South\', \'East\', \'North\', \'West\', \'East\', \'South\', \'West\'] }) product = pd.DataFrame({ \'units_sold\': [20, 15, 30, 25, 10, 40, 50, 35], \'unit_price\': [15.5, 20, 18, 20, 22, 19, 17.5, 21] }) ``` Requirements 1. **Univariate KDE Plot** - Create a KDE plot of the `total_sales` from the `sales` dataset. - Overlay the KDE plots for each region using the `hue` parameter. - Customize the bandwidth to make the plot smoother (use `bw_adjust=1.5`). - Fill the areas under the KDE curves with 50% transparency (`alpha=0.5`). - Use a palette of your choice for the hues. 2. **Bivariate KDE Plot** - Create a KDE plot for the `units_sold` and `unit_price` variables from the `product` dataset. - Show filled contours with 10 levels. - Use the `cmap` parameter to set a colormap of your choice. Input - The `sales` and `product` dataframes as described above. Output - Two seaborn plots: 1. Univariate KDE plot of `total_sales`. 2. Bivariate KDE plot of `units_sold` vs `unit_price`. Your final implementation should contain well-documented code that generates these visualizations. Constraints - You must use the seaborn library. - Ensure that your plots are displayed cleanly and all necessary customizations (hue, bandwidth adjustment, transparency, levels, colormap) are correctly applied. Performance Requirements - Your plots should be generated efficiently and should not take an unreasonable amount of time to render. *Note: Include imports and any setup code necessary to create the plots.* Example Code Skeleton ```python import seaborn as sns import matplotlib.pyplot as plt # Load or create the datasets sales = pd.DataFrame({...}) product = pd.DataFrame({...}) # Univariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=sales, x=\'total_sales\', hue=\'region\', bw_adjust=1.5, fill=True, alpha=0.5, palette=\'bright\') plt.title(\'KDE Plot of Total Sales by Region\') plt.show() # Bivariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=product, x=\'units_sold\', y=\'unit_price\', fill=True, levels=10, cmap=\'viridis\') plt.title(\'Bivariate KDE Plot of Units Sold vs Unit Price\') plt.show() ``` Implement the above requirements and create the visualizations accordingly.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Sample data sales = pd.DataFrame({ \'total_sales\': [250, 300, 150, 700, 800, 500, 450, 600], \'region\': [\'North\', \'South\', \'East\', \'North\', \'West\', \'East\', \'South\', \'West\'] }) product = pd.DataFrame({ \'units_sold\': [20, 15, 30, 25, 10, 40, 50, 35], \'unit_price\': [15.5, 20, 18, 20, 22, 19, 17.5, 21] }) def plot_univariate_kde(sales_data): Generates a univariate KDE plot for total sales, categorized by region. plt.figure(figsize=(10, 6)) sns.kdeplot(data=sales_data, x=\'total_sales\', hue=\'region\', bw_adjust=1.5, fill=True, alpha=0.5, palette=\'bright\') plt.title(\'KDE Plot of Total Sales by Region\') plt.show() def plot_bivariate_kde(product_data): Generates a bivariate KDE plot for units sold and unit price. plt.figure(figsize=(10, 6)) sns.kdeplot(data=product_data, x=\'units_sold\', y=\'unit_price\', fill=True, levels=10, cmap=\'viridis\') plt.title(\'Bivariate KDE Plot of Units Sold vs Unit Price\') plt.show() # Generate the plots plot_univariate_kde(sales) plot_bivariate_kde(product)"},{"question":"**Question: Implement a Comprehensive URL Fetch and Parse Utility** *Objective:* Demonstrate your understanding of Python’s `urllib`, `http.client`, and `ipaddress` modules by implementing a utility function that fetches a URL, parses its content, and performs certain operations on the retrieved information. *Scenario:* You are building a utility that takes a URL as input, fetches HTML content from that URL, parses all hyperlinks (`<a>` tags with `href` attributes) within the content, and performs the following tasks: 1. **Fetch HTML Content:** Using the `urllib.request` module, create a function named `fetch_html_content(url: str) -> str` that fetches the HTML content from the given URL and returns it as a string. 2. **Parse Hyperlinks:** Implement a function named `extract_hyperlinks(html: str) -> List[str]` that parses the given HTML content and returns a list of all hyperlinks found in the content. Use the `urllib.parse` module to ensure that the URLs are properly formatted. 3. **Validate IP Addresses:** Implement a function named `validate_ip_addresses(hyperlinks: List[str]) -> List[str]` that checks if any of the hyperlinks contain valid IP addresses (either IPv4 or IPv6). Use the `ipaddress` module to perform this validation and return a list of valid IP addresses found. 4. **Main Function:** Create a main function named `url_fetch_and_parse(url: str) -> Dict[str, Any]` that orchestrates the above functionalities. It should return a dictionary with the following structure: ```python { \\"content\\": \\"[HTML content of the URL]\\", \\"hyperlinks\\": [\\"http://example1.com\\", \\"http://example2.com\\", ...], \\"valid_ips\\": [\\"192.168.1.1\\", \\"2001:0db8:85a3:0000:0000:8a2e:0370:7334\\", ...] } ``` *Input:* - `url (str)`: The URL to be fetched and parsed. *Output:* - A dictionary containing the HTML content, list of hyperlinks, and valid IP addresses found in those hyperlinks. *Constraints:* - Assume the URL is accessible and contains valid HTML content. - The function should handle potential exceptions during the URL fetch operation gracefully. - Ensure efficient and clean handling and parsing of the HTML content. *Example:* ```python result = url_fetch_and_parse(\\"http://example.com\\") print(result) ``` *Expected Output:* ```python { \\"content\\": \\"<html>...</html>\\", \\"hyperlinks\\": [\\"http://example1.com\\", \\"http://example2.com\\"], \\"valid_ips\\": [\\"192.168.1.1\\"] } ``` *Notes:* - You may use libraries like `BeautifulSoup` for HTML parsing if needed, but it\'s not mandatory. - Provide appropriate docstrings and comments explaining the steps and logic in your code. **Performance Requirements:** - Ensure that your functions are efficient in terms of time and space complexity. - The solution should be able to handle large HTML content without significant performance degradation.","solution":"import urllib.request from urllib.parse import urlparse, urljoin from typing import List, Dict, Any from bs4 import BeautifulSoup import ipaddress def fetch_html_content(url: str) -> str: Fetch the HTML content from the given URL. Args: - url (str): The URL to fetch HTML from. Returns: - str: The HTML content of the URL. try: with urllib.request.urlopen(url) as response: html = response.read().decode(\'utf-8\') return html except Exception as e: print(f\\"Error fetching URL {url}: {e}\\") return \\"\\" def extract_hyperlinks(html: str) -> List[str]: Extract hyperlinks from the given HTML content. Args: - html (str): The HTML content to parse. Returns: - List[str]: A list of extracted hyperlinks. soup = BeautifulSoup(html, \'html.parser\') links = [] for a_tag in soup.find_all(\'a\', href=True): link = a_tag[\'href\'] links.append(link) return links def validate_ip_addresses(hyperlinks: List[str]) -> List[str]: Validate IP addresses in the given hyperlinks. Args: - hyperlinks (List[str]): A list of hyperlinks to check for IP addresses. Returns: - List[str]: A list of valid IP addresses found in the hyperlinks. valid_ips = [] for link in hyperlinks: parsed = urlparse(link) if parsed.hostname: try: ip = ipaddress.ip_address(parsed.hostname) valid_ips.append(str(ip)) except ValueError: pass return valid_ips def url_fetch_and_parse(url: str) -> Dict[str, Any]: Orchestrate the fetching and parsing of the given URL. Args: - url (str): The URL to be fetched and parsed. Returns: - Dict[str, Any]: A dictionary containing the HTML content, list of hyperlinks, and valid IP addresses found. content = fetch_html_content(url) if not content: return { \\"content\\": \\"\\", \\"hyperlinks\\": [], \\"valid_ips\\": [] } hyperlinks = extract_hyperlinks(content) valid_ips = validate_ip_addresses(hyperlinks) return { \\"content\\": content, \\"hyperlinks\\": hyperlinks, \\"valid_ips\\": valid_ips }"},{"question":"**Question: Implementing an Out-of-Core Learning System** You are asked to design an out-of-core learning system to handle a large text classification problem using scikit-learn. Your task is to: 1. Stream text data from a hypothetical large dataset. 2. Extract features using the HashingVectorizer. 3. Train an incremental learning model using the SGDClassifier. # Task Details: Step 1: Stream Text Data Implement a generator function `stream_data(file_path)` that reads lines from a given file path and yields them one by one. Step 2: Feature Extraction with Hashing Use `sklearn.feature_extraction.text.HashingVectorizer` to transform the streamed text data into feature vectors. Step 3: Incremental Training Use `sklearn.linear_model.SGDClassifier` to train the model incrementally. You need to handle unseen classes by passing all possible classes during the first `partial_fit` call. # Input - `file_path` (str): Path to the text data file. - `classes` (list): List of all potential classes in the data. # Output - The final trained model after processing all the data. # Constraints - The model should be trained in an out-of-core fashion, meaning that at any point, only a small part of the data is in memory. - Use a mini-batch size of 100 for training. - The text data file is assumed to contain lines with the format `\\"class_label text_data\\"`. # Example Usage ```python def stream_data(file_path): # Implement your data streaming logic here pass def train_model(file_path, classes): # Implement the incremental learning system here pass # Example call model = train_model(\\"large_text_data.txt\\", [\\"class1\\", \\"class2\\", \\"class3\\"]) ``` You are required to implement the `stream_data` function to read the data file line-by-line and yield the class label and text data, and the `train_model` function to process the streamed data, perform feature extraction, and train the model incrementally using `SGDClassifier`. # Notes - Ensure that the system handles unseen attributes during feature extraction and processes batches of data incrementally. - Your implementation should be memory efficient and suitable for large datasets that do not fit into the main memory.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def stream_data(file_path): Generator function that reads lines from a file and yields class label and text data. Parameters: file_path (str): Path to the text data file. Yields: Tuple of (class_label, text_data) with open(file_path, \'r\') as file: for line in file: try: class_label, text_data = line.strip().split(\\" \\", 1) yield class_label, text_data except ValueError: # In case the line does not have exactly two parts separated by space continue def train_model(file_path, classes): Train an incremental learning model using a streaming approach. Parameters: file_path (str): Path to the text data file. classes (list): List of all potential classes in the data. Returns: SGDClassifier: The trained model. vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() data_stream = stream_data(file_path) batch_size = 100 X_batch = [] y_batch = [] for class_label, text_data in data_stream: X_batch.append(text_data) y_batch.append(class_label) if len(X_batch) == batch_size: X_features = vectorizer.transform(X_batch) classifier.partial_fit(X_features, y_batch, classes=classes) X_batch.clear() y_batch.clear() # Train the remaining data if any if X_batch: X_features = vectorizer.transform(X_batch) classifier.partial_fit(X_features, y_batch, classes=classes) return classifier"},{"question":"Objective: Assess the candidate\'s understanding of the `builtins` module and their ability to create custom functions that utilize built-in functions with advanced Python coding practices. Problem Statement: You are required to create a custom function that acts as a wrapper around the built-in `max` function. The purpose of the custom function is to extend the functionality of `max` by logging the inputs and outputs to a file for auditing purposes. Requirements: 1. Implement a function `logged_max(*args, **kwargs)`. 2. The function should call the built-in `max` function with the provided arguments and keyword arguments. 3. The function should log the input arguments and the output result to a file named `max_log.txt` in the format: ``` Inputs: args=<args>, kwargs=<kwargs> Output: <result> ``` 4. Ensure that the logging does not interfere with the usual behavior of the `max` function. 5. Use the `builtins` module to reference the built-in `max` function. Constraints: - You are not allowed to use any external libraries for logging. - Assume that the log file will be in the same directory as your script. Function Signature: ```python import builtins def logged_max(*args, **kwargs): pass ``` Example: ```python # Example usage of logged_max: result = logged_max(1, 3, 2) # The above call should log the following to max_log.txt: # Inputs: args=(1, 3, 2), kwargs={} # Output: 3 result = logged_max([5, 1, 3, 7], key=lambda x: -x) # The above call should log the following to max_log.txt: # Inputs: args=([5, 1, 3, 7],), kwargs={\'key\': <function <lambda> at 0x...>} # Output: 1 # Because the key function is applied to each element as -x ``` This exercise tests your ability to work with built-in functions, use the `builtins` module, handle variable arguments, and perform file I/O operations.","solution":"import builtins def logged_max(*args, **kwargs): Wrapper around the built-in max function that logs inputs and outputs to a file. result = builtins.max(*args, **kwargs) # Format the log entry log_entry = f\\"Inputs: args={args}, kwargs={kwargs}nOutput: {result}n\\" # Append the log to the file with open(\\"max_log.txt\\", \\"a\\") as log_file: log_file.write(log_entry) return result"},{"question":"**Pandas Memory Optimization and Chunking** **Objective:** You are given a large dataset of time series data stored in multiple parquet files in a directory. Your goal is to read and process this data efficiently to compute some overall statistics without exceeding memory limits. **Requirements:** 1. Read only specific columns (`[\'name\', \'id\', \'x\', \'y\']`) from the parquet files. 2. Convert the `name` column to categorical type to save memory. 3. Downcast the numeric columns (`id`, `x`, `y`) to the most memory-efficient types. 4. Use chunking to compute the mean of columns `x` and `y` and the count of unique `name` values. **Input Format:** - A directory path containing parquet files named in the format `ts-XX.parquet` (where `XX` is a two-digit number). - Column names to read: `[\'name\', \'id\', \'x\', \'y\']`. **Output Format:** - A dictionary with two keys: - `\'mean\'`: containing a dictionary with mean values of `x` and `y`. - `\'unique_name_count\'`: containing the count of unique `name` values. **Function Signature:** ```python def process_large_dataset(directory: str) -> dict: pass ``` **Constraints:** - There will be at least one parquet file in the directory. - Each parquet file can fit into memory, but the entire dataset combined cannot. **Example:** Suppose you have the following files in the directory `data/timeseries/`: - `ts-00.parquet` - `ts-01.parquet` - ... To solve the problem, follow these steps: 1. Read the specified columns from each parquet file into a DataFrame. 2. Convert the `name` column to categorical type. 3. Downcast numeric columns (`id`, `x`, `y`). 4. Compute the required statistics in chunks and combine the results. **Sample Implementation:** ```python import pandas as pd import pathlib def process_large_dataset(directory: str) -> dict: columns = [\'name\', \'id\', \'x\', \'y\'] files = pathlib.Path(directory).glob(\\"ts-*.parquet\\") mean_x_total, mean_y_total, count_x, count_y = 0, 0, 0, 0 unique_name_counts = pd.Series(dtype=int) for file in files: df = pd.read_parquet(file, columns=columns) df[\'name\'] = df[\'name\'].astype(\'category\') df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'unsigned\') df[[\'x\', \'y\']] = df[[\'x\', \'y\']].apply(pd.to_numeric, downcast=\'float\') mean_x_total += df[\'x\'].sum() count_x += df[\'x\'].count() mean_y_total += df[\'y\'].sum() count_y += df[\'y\'].count() unique_name_counts = unique_name_counts.add(df[\'name\'].value_counts(), fill_value=0) result = { \'mean\': { \'x\': mean_x_total / count_x if count_x != 0 else 0, \'y\': mean_y_total / count_y if count_y != 0 else 0 }, \'unique_name_count\': len(unique_name_counts) } return result ``` In this question, students will demonstrate an understanding of: - Efficiently reading and handling data using pandas. - Transforming data types for optimal memory usage. - Utilizing chunking techniques to process large datasets without memory overflow.","solution":"import pandas as pd import pathlib def process_large_dataset(directory: str) -> dict: columns = [\'name\', \'id\', \'x\', \'y\'] files = pathlib.Path(directory).glob(\\"ts-*.parquet\\") total_x_sum, total_y_sum, total_x_count, total_y_count = 0, 0, 0, 0 unique_names = pd.Series(dtype=\'int\') for file in files: df = pd.read_parquet(file, columns=columns) df[\'name\'] = df[\'name\'].astype(\'category\') df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'unsigned\') df[[\'x\', \'y\']] = df[[\'x\', \'y\']].apply(pd.to_numeric, downcast=\'float\') total_x_sum += df[\'x\'].sum() total_x_count += df[\'x\'].count() total_y_sum += df[\'y\'].sum() total_y_count += df[\'y\'].count() unique_names = unique_names.add(df[\'name\'].value_counts(), fill_value=0) result = { \'mean\': { \'x\': total_x_sum / total_x_count if total_x_count != 0 else 0, \'y\': total_y_sum / total_y_count if total_y_count != 0 else 0 }, \'unique_name_count\': len(unique_names) } return result"},{"question":"# Advanced Python Logging Assessment Background: You are developing a multi-threaded server-side application that processes client requests and logs various events and errors. To ensure you have a robust logging mechanism, it is important to customize log handling and configuration using Python\'s `logging` module. Task: Implement a logging setup for a multi-threaded application that adheres to the following requirements: 1. **Logger and Handler Setup**: - Create a logger named `myserver`. - Setup a handler that logs messages to a file named `server.log`. - Setup another handler that logs error messages and above to the console. - Use a rotating file handler to limit the size of `server.log` to 1MB and keep at most 3 backup files. 2. **Formatter**: - Use a formatter that includes the timestamp, logger name, log level, and message. 3. **Configuration Using Dictionary**: - Use a dictionary configuration to setup the logging as defined above. 4. **Multi-threaded Example**: - Create a function `process_request` that logs messages of various levels. - Simulate processing of client requests using multiple threads (e.g., five threads) that call `process_request`. 5. **Advanced Requirement**: - Include contextual information (like thread name and a correlation ID) in the log messages. Input: No input required. Output: Logs should write to both `server.log` and console, formatted with necessary details. Constraints: - Use Python 3.7 or later. - Ensure thread-safe operations for logging. Example: ```python import logging import logging.config import threading import random import time def setup_logging(): log_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - [%(threadName)s] - %(message)s\' }, }, \'handlers\': { \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'filename\': \'server.log\', \'maxBytes\': 1048576, # 1MB \'backupCount\': 3, \'formatter\': \'standard\', }, \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'ERROR\', \'formatter\': \'standard\', }, }, \'loggers\': { \'myserver\': { \'handlers\': [\'file\', \'console\'], \'level\': \'DEBUG\', \'propagate\': True, }, } } logging.config.dictConfig(log_config) def process_request(correlation_id): logger = logging.getLogger(\'myserver\') for i in range(10): time.sleep(random.uniform(0.5, 1.5)) if i % 2 == 0: logger.info(f\'Processing request {i} with correlation id {correlation_id}\') else: logger.error(f\'Error processing request {i} with correlation id {correlation_id}\') if __name__ == \\"__main__\\": setup_logging() threads = [] for i in range(5): thread = threading.Thread(target=process_request, args=(f\\"cid-{i}\\",), name=f\\"Thread-{i+1}\\") threads.append(thread) thread.start() for thread in threads: thread.join() ``` Notes: - This setup should create detailed logs with timestamps, log levels, thread names, and correlation IDs to `server.log`, as well as error logs to the console. - Ensure thread-safety and appropriate configuration management using dictionary-based setup for easy adjustments and scalability.","solution":"import logging import logging.config from logging.handlers import RotatingFileHandler import threading import random import time def setup_logging(): log_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - [%(threadName)s] - %(message)s\' }, }, \'handlers\': { \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'filename\': \'server.log\', \'maxBytes\': 1048576, # 1MB \'backupCount\': 3, \'formatter\': \'standard\', }, \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'ERROR\', \'formatter\': \'standard\', }, }, \'loggers\': { \'myserver\': { \'handlers\': [\'file\', \'console\'], \'level\': \'DEBUG\', \'propagate\': True, }, } } logging.config.dictConfig(log_config) def process_request(correlation_id): logger = logging.getLogger(\'myserver\') for i in range(10): time.sleep(random.uniform(0.5, 1.5)) if i % 2 == 0: logger.info(f\'Processing request {i} with correlation id {correlation_id}\') else: logger.error(f\'Error processing request {i} with correlation id {correlation_id}\') if __name__ == \\"__main__\\": setup_logging() threads = [] for i in range(5): thread = threading.Thread(target=process_request, args=(f\\"cid-{i}\\",), name=f\\"Thread-{i+1}\\") threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"Mixed Backend Model Execution **Context:** You are required to implement a functionality in PyTorch that checks if the ONNX Runtime (ONNXRT) backend is supported in the current environment and then performs model conversion and execution using this backend if supported. **Task:** 1. **Check Backend Support**: Write a function `check_onnx_backend_support()` that utilizes `torch.onnx.is_onnxrt_backend_supported()` to determine if the ONNX Runtime backend is supported in the current environment. This function should return a boolean indicating the support status. 2. **Model Conversion and Execution**: Write a function `convert_and_run_model` which: - Accepts a PyTorch model and a sample input tensor. - Checks if the ONNX Runtime backend is supported by calling `check_onnx_backend_support()` implemented above. - If supported: - Converts the PyTorch model to ONNX format. - Executes the model using the ONNX Runtime backend. - Returns the ONNX output. - If not supported: - Executes the model directly using PyTorch. - Returns the PyTorch output. **Input and Output Specifications:** - Function `check_onnx_backend_support()`: No input parameters. - Output: Boolean value indicating whether the ONNXRT backend is supported. - Function `convert_and_run_model(model: torch.nn.Module, input_tensor: torch.Tensor) -> torch.Tensor`: - Inputs: - `model`: A PyTorch model (an instance of `torch.nn.Module`). - `input_tensor`: A sample input tensor (an instance of `torch.Tensor`) compatible with the model. - Output: - A tensor, which will either be the output from the ONNX runtime if supported, or the direct PyTorch output otherwise. **Constraints:** - Assume the PyTorch model and input tensor are always valid and compatible. - You need to handle basic exceptions during the conversion and execution processes. **Example Implementation:** ```python import torch import torch.onnx def check_onnx_backend_support() -> bool: try: return torch.onnx.is_onnxrt_backend_supported() except AttributeError: # Function is_onnxrt_backend_supported might not be available in all versions. return False def convert_and_run_model(model: torch.nn.Module, input_tensor: torch.Tensor) -> torch.Tensor: if check_onnx_backend_support(): try: # Export the model to ONNX format onnx_model_path = \\"temp_model.onnx\\" torch.onnx.export(model, input_tensor, onnx_model_path) # Import ONNX and execute using ONNX Runtime import onnxruntime ort_session = onnxruntime.InferenceSession(onnx_model_path) ort_inputs = {ort_session.get_inputs()[0].name: input_tensor.numpy()} ort_outs = ort_session.run(None, ort_inputs) return torch.tensor(ort_outs[0]) except Exception as e: print(f\\"ONNX Runtime execution failed: {e}\\") pass # Fallback to direct PyTorch execution with torch.no_grad(): return model(input_tensor) ``` **Notes:** - The implementation might require the `onnxruntime` package installed. Use `pip install onnxruntime` to install it if necessary. - This example does not handle complex edge cases and focuses only on basic functionality to meet the task requirements. Improvements and additional error handling should be part of a more robust implementation.","solution":"import torch import torch.onnx def check_onnx_backend_support() -> bool: try: return torch.onnx.is_onnxrt_backend_supported() except AttributeError: # Function is_onnxrt_backend_supported might not be available in all versions. return False def convert_and_run_model(model: torch.nn.Module, input_tensor: torch.Tensor) -> torch.Tensor: if check_onnx_backend_support(): try: # Export the model to ONNX format onnx_model_path = \\"temp_model.onnx\\" torch.onnx.export(model, input_tensor, onnx_model_path) # Import ONNX and execute using ONNX Runtime import onnxruntime ort_session = onnxruntime.InferenceSession(onnx_model_path) ort_inputs = {ort_session.get_inputs()[0].name: input_tensor.cpu().numpy()} ort_outs = ort_session.run(None, ort_inputs) return torch.tensor(ort_outs[0]) except Exception as e: print(f\\"ONNX Runtime execution failed: {e}\\") pass # Fallback to direct PyTorch execution with torch.no_grad(): return model(input_tensor)"},{"question":"You are tasked with creating a simulation of some behaviors described in the python310 type objects handling documentation using pure Python. You will create two main functionalities: type checking and a basic cache clearing mechanism. # Task 1: Object Type Check Implement a function `is_type_object` that determines if a given object is of the specified type (or its subtype). Function Signature ```python def is_type_object(obj, obj_type) -> bool: ... ``` Input - `obj`: The object to be checked. - `obj_type`: The type that `obj` should be checked against. Output - Returns `True` if `obj` is of type `obj_type` or a subtype of `obj_type`. Otherwise, returns `False`. # Constraints - You cannot use Python’s built-in `isinstance()` or `issubclass()` functions directly. # Task 2: Cache Mechanism Implement a simple cache clearing mechanism that mimics clearing an internal type cache. This cache will store results of whether an object is of a specified type. Function Signature ```python class TypeCache: def __init__(self): ... def check_cache(self, obj, obj_type) -> bool: ... def clear_cache(self): ... ``` Details 1. `__init__()`: Initializes an empty cache. 2. `check_cache(obj, obj_type)`: Checks if `obj` is of type `obj_type` using the cache. - If the result is already cached, use the cached result. - If not, determine if `obj` is of type `obj_type` using `is_type_object()`, cache this result, and return it. 3. `clear_cache()`: Clears the internal cache. Example Usage ```python cache = TypeCache() class A: pass class B(A): pass a_instance = A() b_instance = B() assert is_type_object(a_instance, A) == True assert is_type_object(b_instance, A) == True assert is_type_object(b_instance, B) == True assert is_type_object(a_instance, B) == False cache.check_cache(a_instance, A) # Caches the result cache.check_cache(b_instance, A) # Caches the result cache.check_cache(b_instance, A) # Uses the cached result cache.clear_cache() # Clears the cache ``` # Notes - Emphasis should be on understanding type checking and caching mechanism. - The functionality should mimic the behavior described without relying on direct usage of built-in functions that make the task trivial.","solution":"def is_type_object(obj, obj_type) -> bool: Determines if `obj` is of type `obj_type` or a subtype of `obj_type`. obj_class = obj.__class__ while obj_class: if obj_class is obj_type: return True obj_class = obj_class.__base__ return False class TypeCache: def __init__(self): self.cache = {} def check_cache(self, obj, obj_type) -> bool: Checks cache for type check results, if not cached calls is_type_object(). key = (id(obj), obj_type) if key not in self.cache: self.cache[key] = is_type_object(obj, obj_type) return self.cache[key] def clear_cache(self): Clears the internal cache. self.cache.clear()"},{"question":"**Python Compiling Automation Tool** You are required to implement a function in Python to compile multiple Python source files in a given directory (and its subdirectories if specified) to byte-code files using the `py_compile` module. The function should be able to: 1. Handle exceptions where files cannot be compiled and log such errors to a given log file. 2. Handle different invalidation modes. 3. Optionally omit error messages from being printed to `stderr`. # Function Signature ```python def compile_source_files(dir_path: str, log_file: str, subdirectories: bool = True, invalidation_mode: str = \\"TIMESTAMP\\", quiet: int = 0) -> int: Compiles Python source files in a given directory to byte-code files. Parameters: - dir_path (str): The root directory path containing the Python source files. - log_file (str): Path to the log file where error messages will be logged. - subdirectories (bool): Whether to recursively compile files in subdirectories. Default is True. - invalidation_mode (str): The invalidation mode for the byte-code files. One of \'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'. - quiet (int): Control error message output (0, 1, or 2). Default is 0. Returns: - int: Number of files successfully compiled. Raises: - ValueError: If an invalid invalidation_mode is provided. ``` # Input - `dir_path`: A string representing the path to the root directory containing Python source files. - `log_file`: A string representing the file path where any errors encountered during compilation should be logged. - `subdirectories`: A boolean specifying whether to include subdirectories in the search. - `invalidation_mode`: A string that must be one of the following: `\\"TIMESTAMP\\"`, `\\"CHECKED_HASH\\"`, `\\"UNCHECKED_HASH\\"`. - `quiet`: An integer from 0 to 2 to control error message output. # Output - Returns the number of files successfully compiled as an integer. # Constraints - The function should raise a `ValueError` if an invalid invalidation_mode is provided. - It should handle filesystem errors and log them without interrupting the compilation of other files. - The log file should contain appropriate error messages detailing any exceptions that occur during file compilation. # Example ```python # Directory structure: # test_dir/ # ├── script1.py # ├── script2.py # └── subdir/ # └── script3.py # Function call num_compiled_files = compile_source_files(\\"test_dir\\", \\"compile_log.txt\\", True, \\"TIMESTAMP\\", 1) # Expected Output # The number of compiled files (e.g., 3). # Errors, if any, are logged in \\"compile_log.txt\\". ``` Use the `py_compile` module effectively to implement this function considering the guidelines and requirements described above.","solution":"import os import py_compile def compile_source_files(dir_path: str, log_file: str, subdirectories: bool = True, invalidation_mode: str = \\"TIMESTAMP\\", quiet: int = 0) -> int: Compiles Python source files in a given directory to byte-code files. Parameters: - dir_path (str): The root directory path containing the Python source files. - log_file (str): Path to the log file where error messages will be logged. - subdirectories (bool): Whether to recursively compile files in subdirectories. Default is True. - invalidation_mode (str): The invalidation mode for the byte-code files. One of \'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'. - quiet (int): Control error message output (0, 1, or 2). Default is 0. Returns: - int: Number of files successfully compiled. Raises: - ValueError: If an invalid invalidation_mode is provided. if invalidation_mode not in [\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\']: raise ValueError(f\\"Invalid invalidation_mode: {invalidation_mode}\\") successful_compilations = 0 error_messages = [] for root, dirs, files in os.walk(dir_path): for file in files: if file.endswith(\\".py\\"): file_path = os.path.join(root, file) try: py_compile.compile(file_path, cfile=None, dfile=None, doraise=True, invalidation_mode=invalidation_mode, optimize=-1) successful_compilations += 1 except py_compile.PyCompileError as e: error_message = f\\"Failed to compile {file_path}: {e.msg}\\" error_messages.append(error_message) if quiet == 0: print(error_message) if not subdirectories: break with open(log_file, \'w\') as log: for message in error_messages: log.write(f\\"{message}n\\") return successful_compilations"},{"question":"You are tasked with creating a decision tree classifier using scikit-learn. Your goal is to predict the species of iris flowers based on the Iris dataset, which consists of 150 samples with four features each (sepal length, sepal width, petal length, and petal width). # Requirements: 1. **Load the Iris dataset** from `sklearn.datasets`. 2. **Preprocess the data** if necessary. 3. **Implement a Decision Tree Classifier** with `max_depth=3`. 4. **Train the model** using the preprocessed data. 5. **Evaluate the model** on a test set and calculate the accuracy. 6. **Tune the hyperparameters** (e.g., `max_depth`, `min_samples_split`, `min_samples_leaf`) to avoid overfitting and improve the model\'s performance. 7. **Visualize the decision tree**. # Input: - The code will automatically load the Iris dataset, so no manual data input is required. # Output: - Print the accuracy of the model on the test set. - Display the visualized decision tree. # Constraints: - The `max_depth` of the decision tree should be between 1 and 5. - Use a 80-20 train-test split. # Example Code Structure: ```python # Step 1: Load the Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree import matplotlib.pyplot as plt # Load dataset iris = load_iris() X, y = iris.data, iris.target # Step 2: Split the data into training and testing sets (80-20) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Initialize the Decision Tree Classifier with max_depth=3 clf = DecisionTreeClassifier(max_depth=3, random_state=42) # Step 4: Train the model clf.fit(X_train, y_train) # Step 5: Evaluate the model and print accuracy accuracy = clf.score(X_test, y_test) print(f\\"Accuracy: {accuracy:.2f}\\") # Step 6: Tune hyperparameters (Adjust this section based on your experimentation) # Example: clf.set_params(max_depth=2, min_samples_split=2, min_samples_leaf=1) # clf.fit(X_train, y_train) # accuracy = clf.score(X_test, y_test) # print(f\\"Tuned Accuracy: {accuracy:.2f}\\") # Step 7: Visualize the Decision Tree plt.figure(figsize=(12, 8)) plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True) plt.show() ``` # Deliverables: - A Python script that implements the steps described above. - Ensure the code is well-commented and explain each part of the implementation process. Good luck, and happy coding!","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree import matplotlib.pyplot as plt # Step 1: Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Step 2: Split the data into training and testing sets (80-20) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Initialize the Decision Tree Classifier with max_depth=3 clf = DecisionTreeClassifier(max_depth=3, random_state=42) # Step 4: Train the model clf.fit(X_train, y_train) # Step 5: Evaluate the model and print accuracy accuracy = clf.score(X_test, y_test) print(f\\"Accuracy: {accuracy:.2f}\\") # Step 6: Tune hyperparameters (already set max_depth=3) # Example: clf.set_params(max_depth=2, min_samples_split=2, min_samples_leaf=1) # clf.fit(X_train, y_train) # accuracy = clf.score(X_test, y_test) # print(f\\"Tuned Accuracy: {accuracy:.2f}\\") # Step 7: Visualize the Decision Tree plt.figure(figsize=(12, 8)) plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True) plt.show()"},{"question":"# Complex Numbers with PyTorch Objectives You are required to implement a function in PyTorch that demonstrates various operations on complex tensors. This function will involve creating complex tensors, performing basic arithmetic operations, and applying some linear algebra functions. Function Signature ```python import torch def complex_operations(): This function performs the following operations: 1. Creates a random complex tensor of size (3, 3) with dtype torch.cfloat. 2. Computes the transpose of the created complex tensor. 3. Calculates the conjugate of the tensor. 4. Computes the matrix multiplication of the tensor with its conjugate transpose. 5. Computes the eigenvalues of the resulting matrix. Returns: - real_parts (torch.Tensor): The real parts of the eigenvalues. - imag_parts (torch.Tensor): The imaginary parts of the eigenvalues. pass ``` Requirements 1. **Initialization**: - Create a 3x3 random complex tensor with dtype `torch.cfloat`. 2. **Transpose**: - Compute the transpose of the created tensor. 3. **Conjugate**: - Calculate the conjugate of the tensor. 4. **Matrix Multiplication**: - Perform matrix multiplication between the tensor and its conjugate transpose. 5. **Eigenvalues**: - Compute the eigenvalues of the resulting matrix using `torch.linalg.eig`. 6. **Return**: - Extract and return the real and imaginary parts of the eigenvalues separately. Example If `c` is the original complex tensor: ``` c = tensor([[ 1.0+0.5j, -0.5+1.0j, 0.3+0.2j], [ 0.5-1.0j, 1.0+0.3j, -0.2-0.4j], [-0.3+0.2j, 0.4+0.1j, 1.0+0.6j]]) ``` Then your function should compute: ``` c_t = c.t() c_conjugate = c.conj() result = torch.matmul(c, c_conjugate) eigenvalues = torch.linalg.eig(result).values real_parts = eigenvalues.real imag_parts = eigenvalues.imag ``` You should return `real_parts` and `imag_parts`. Constraints and Notes - Use `torch.randn` for random tensor generation. - Ensure that all operations involving complex numbers are efficient and correctly utilize PyTorch\'s capabilities for handling complex tensors.","solution":"import torch def complex_operations(): This function performs the following operations: 1. Creates a random complex tensor of size (3, 3) with dtype torch.cfloat. 2. Computes the transpose of the created complex tensor. 3. Calculates the conjugate of the tensor. 4. Computes the matrix multiplication of the tensor with its conjugate transpose. 5. Computes the eigenvalues of the resulting matrix. Returns: - real_parts (torch.Tensor): The real parts of the eigenvalues. - imag_parts (torch.Tensor): The imaginary parts of the eigenvalues. # Create a 3x3 random complex tensor with dtype torch.cfloat real_part = torch.randn(3, 3) imag_part = torch.randn(3, 3) complex_tensor = torch.complex(real_part, imag_part) # Compute the transpose of the complex tensor transpose_tensor = complex_tensor.t() # Calculate the conjugate of the tensor conjugate_tensor = complex_tensor.conj() # Perform matrix multiplication between the tensor and its conjugate transpose result_tensor = torch.matmul(complex_tensor, conjugate_tensor.t()) # Compute the eigenvalues of the resulting matrix eigenvalues = torch.linalg.eig(result_tensor).eigenvalues # Extract and return the real and imaginary parts of the eigenvalues real_parts = eigenvalues.real imag_parts = eigenvalues.imag return real_parts, imag_parts"},{"question":"Implementing a Distributed RRef System in PyTorch Objective Create a distributed system in PyTorch that demonstrates your understanding of `RRef` by implementing a scenario where multiple workers share and update references across the network. You will simulate a network of PyTorch workers, handle `RRef` creation, update, acknowledgment, and deletion, and ensure correct reference counting. Task Implement a Python script using PyTorch and its `torch.distributed.rpc` module that: 1. Initializes a distributed PyTorch environment with at least three workers. 2. Creates an `OwnerRRef` on one worker containing a PyTorch tensor. 3. Shares the `OwnerRRef` with other workers via `UserRRef` instances. 4. Updates and retrieves the tensor from various workers. 5. Ensures that the `OwnerRRef` is not prematurely deleted by handling acknowledgments correctly. 6. Demonstrates the deletion of `OwnerRRef` after all `UserRRef` instances are deleted. Implementation Details 1. **Environment Setup** - Initialize a distributed PyTorch environment with at least three workers. Use the `torch.distributed.rpc.init_rpc` method. 2. **RRef Creation and Sharing** - On the first worker, create an `OwnerRRef` containing a PyTorch tensor (e.g., `torch.ones(2)`). - Share this `OwnerRRef` with the other workers using `torch.distributed.rpc.remote`. 3. **Handling Acknowledgment and Update** - Implement a method on each worker to update and retrieve the tensor using `rpc_sync` and `rpc_async`. - Ensure acknowledgment messages are handled correctly, and the tensor updates are consistent. 4. **Managing Deletion** - Implement the logic to ensure that the `OwnerRRef` is not deleted until all `UserRRef` instances are. - Demonstrate the deletion of the `OwnerRRef` after all references are deleted. Input and Output - **Input:** The script should not require any additional input besides initialization parameters for the distributed environment. - **Output:** The script should print the tensor at various stages to demonstrate updates and the final deletion of the `OwnerRRef`. Constraints - Use PyTorch and the `torch.distributed.rpc` module. - Ensure proper handling of reference counting and acknowledgment messages. - Use appropriate synchronization mechanisms to avoid race conditions. Sample Skeleton Code ```python import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def init_rpc_environment(rank, world_size): rpc.init_rpc( f\\"worker{rank}\\", rank=rank, world_size=world_size, ) def create_owner_rref(): return RRef(torch.ones(2)) def update_tensor(rref, value): rref.to_here()[:] += value return rref.to_here() def main(): world_size = 3 rank = int(os.environ[\'RANK\']) init_rpc_environment(rank, world_size) if rank == 0: # Create OwnerRRef owner_rref = create_owner_rref() print(f\\"Initial tensor: {owner_rref.to_here()}\\") # Share with other workers rref1 = rpc.remote(\\"worker1\\", update_tensor, args=(owner_rref, 2)) rref2 = rpc.remote(\\"worker2\\", update_tensor, args=(owner_rref, 3)) print(f\\"Updated tensor on worker1: {rref1.to_here()}\\") print(f\\"Updated tensor on worker2: {rref2.to_here()}\\") rpc.shutdown() if __name__ == \\"__main__\\": main() ``` The provided skeleton code is a starting point. You need to fill in the details and ensure that the deletion of `OwnerRRef` happens correctly after all `UserRRef` references are removed.","solution":"import os import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def init_rpc_environment(rank, world_size): rpc.init_rpc( f\\"worker{rank}\\", rank=rank, world_size=world_size, ) def create_owner_rref(): return RRef(torch.ones(2)) def update_tensor(rref, value): rref_tensor = rref.to_here() rref_tensor += value return rref_tensor def main(): world_size = 3 rank = int(os.environ[\'RANK\']) init_rpc_environment(rank, world_size) if rank == 0: # Create OwnerRRef owner_rref = create_owner_rref() print(f\\"Initial tensor: {owner_rref.to_here()}\\") # Share with other workers rref1 = rpc.remote(\\"worker1\\", update_tensor, args=(owner_rref, 2)) rref2 = rpc.remote(\\"worker2\\", update_tensor, args=(owner_rref, 3)) # Wait for async updates and gather results updated_tensor1 = rref1.to_here() updated_tensor2 = rref2.to_here() print(f\\"Updated tensor on worker1: {updated_tensor1}\\") print(f\\"Updated tensor on worker2: {updated_tensor2}\\") rpc.shutdown() if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Implement a function to perform various operations using the `fractions` module in Python. This task will assess your ability to create and manipulate `Fraction` instances, utilize arithmetic operations, and apply methods like `limit_denominator` and rounding. **Instructions:** 1. **Function Definition:** ```python def manipulate_fractions(operations): Perform various operations on fractions based on the input list of operations. Args: operations (list of dict): A list of dictionaries, each specifying an operation to perform. Each dictionary has the keys: - \'operation\': A string specifying the operation. It can be one of \'create\', \'add\', \'subtract\', \'limit_denominator\', \'round\'. - \'args\': A list of arguments relevant to the operation. For \'create\': [numerator, denominator] or a string representing the fraction. For \'add\' and \'subtract\': [fraction1, fraction2] both can be either [numerator, denominator] or string. For \'limit_denominator\': [fraction, max_denominator] with fraction being either [numerator, denominator] or string. For \'round\': [fraction, ndigits] with fraction being either [numerator, denominator] or string. Returns: list: The results of the operations as specified, in the same order as the input list. ``` 2. **Input Format:** - The input will be a list of dictionaries, where each dictionary specifies an operation and its arguments. For example: ```python [ {\'operation\': \'create\', \'args\': [3, 4]}, {\'operation\': \'create\', \'args\': \\"2/3\\"}, {\'operation\': \'add\', \'args\': [[3, 4], [2, 3]]}, {\'operation\': \'subtract\', \'args\': [\\"5/6\\", \\"1/2\\"]}, {\'operation\': \'limit_denominator\', \'args\': [\\"0.333333\\", 100]}, {\'operation\': \'round\', \'args\': [[22, 7], 2]} ] ``` 3. **Output Format:** - The output should be a list containing the results of the operations in the same order. Each element in the output can be either: - A string representation of a `Fraction`. - In case of rounding operation, it should return an integer or a Fraction depending on the provided `ndigits`. 4. **Constraints:** - For `add` and `subtract` operations, the input fractions can be in the form of string representations or lists of [numerator, denominator]. - The input fractions are guaranteed to be valid and in simplified form. - The operations list will have at most 100 entries. 5. **Example:** ```python operations = [ {\'operation\': \'create\', \'args\': [3, 4]}, {\'operation\': \'create\', \'args\': \\"2/3\\"}, {\'operation\': \'add\', \'args\': [[3, 4], [2, 3]]}, {\'operation\': \'subtract\', \'args\': [\\"5/6\\", \\"1/2\\"]}, {\'operation\': \'limit_denominator\', \'args\': [\\"0.333333\\", 100]}, {\'operation\': \'round\', \'args\': [[22, 7], 2]} ] expected_output = [ \\"3/4\\", \\"2/3\\", \\"17/12\\", \\"1/3\\", \\"1/3\\", Fraction(22, 7) # or its equivalent integer representation if rounding without decimals ] ``` **Hint:** - Make use of `Fraction` constructors and methods like `limit_denominator`, `__add__`, `__sub__`, `__round__` to achieve the desired results.","solution":"from fractions import Fraction def parse_fraction(fraction): if isinstance(fraction, str): return Fraction(fraction) elif isinstance(fraction, list) and len(fraction) == 2: return Fraction(fraction[0], fraction[1]) return Fraction(fraction) # handle any other input cases as appropriate def manipulate_fractions(operations): results = [] for operation in operations: if operation[\'operation\'] == \'create\': frac = parse_fraction(operation[\'args\']) results.append(str(frac)) elif operation[\'operation\'] == \'add\': frac1 = parse_fraction(operation[\'args\'][0]) frac2 = parse_fraction(operation[\'args\'][1]) result = frac1 + frac2 results.append(str(result)) elif operation[\'operation\'] == \'subtract\': frac1 = parse_fraction(operation[\'args\'][0]) frac2 = parse_fraction(operation[\'args\'][1]) result = frac1 - frac2 results.append(str(result)) elif operation[\'operation\'] == \'limit_denominator\': frac = parse_fraction(operation[\'args\'][0]) max_denominator = operation[\'args\'][1] result = frac.limit_denominator(max_denominator) results.append(str(result)) elif operation[\'operation\'] == \'round\': frac = parse_fraction(operation[\'args\'][0]) if len(operation[\'args\']) > 1: ndigits = operation[\'args\'][1] result = float(round(frac, ndigits)) else: result = int(round(frac)) results.append(result) return results"},{"question":"**Objective:** You are tasked with creating a function that generates a bar plot using Seaborn, with the color palette for the bars determined by the `sns.hls_palette()` function. This will assess your understanding of Seaborn\'s color palette functionality and your ability to incorporate it into visualizations. **Function Signature:** ```python def generate_colored_barplot(df, n_colors, lightness=0.5, saturation=0.65, hue_start=0, as_cmap=False): Generates a bar plot using a custom HLS color palette. Parameters: - df: pd.DataFrame - A Pandas DataFrame containing at least two columns: one for categories and one for values. - n_colors: int - The number of discrete colors to generate for the palette. - lightness: float - The lightness level for the HLS palette. Default is 0.5. - saturation: float - The saturation level for the HLS palette. Default is 0.65. - hue_start: float - The start point for hue sampling. Default is 0. - as_cmap: bool - Whether to return the palette as a continuous colormap. Default is False. Returns: - None pass ``` **Input:** - A DataFrame (`df`) consisting of at least two columns. For example, one column representing categories (e.g., `\'Category\'`) and another for numerical values (e.g., `\'Value\'`). - The number of colors for the HLS palette (`n_colors`). - Optional parameters for lightness, saturation, and hue start point. - A boolean flag indicating whether to return a continuous colormap. **Output:** - A bar plot visualized using Seaborn with bars colored based on the generated HLS palette. **Constraints:** - The number of colors (`n_colors`) should be an integer greater than 0. - Lightness should be a float between 0 and 1. - Saturation should be a float between 0 and 1. - Hue start should be a float between 0 and 1. **Example:** ```python import pandas as pd # Sample DataFrame data = { \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Value\': [10, 20, 15, 10, 30] } df = pd.DataFrame(data) # Generate the bar plot generate_colored_barplot(df, n_colors=5, lightness=0.5, saturation=0.65, hue_start=0.5) ``` In the above example, the bar plot should display 5 bars, each colored based on the specified HLS palette settings. # Note: - Ensure that you have imported the required libraries (`pandas`, `seaborn`, `matplotlib.pyplot`) within your function. - Handle edge cases where invalid input parameters could be provided. - Provide comments and documentation within your function to explain the logic and steps.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_colored_barplot(df, n_colors, lightness=0.5, saturation=0.65, hue_start=0, as_cmap=False): Generates a bar plot using a custom HLS color palette. Parameters: - df: pd.DataFrame - A Pandas DataFrame containing at least two columns: one for categories and one for values. - n_colors: int - The number of discrete colors to generate for the palette. - lightness: float - The lightness level for the HLS palette. Default is 0.5. - saturation: float - The saturation level for the HLS palette. Default is 0.65. - hue_start: float - The start point for hue sampling. Default is 0. - as_cmap: bool - Whether to return the palette as a continuous colormap. Default is False. Returns: - None # Validate inputs if not isinstance(df, pd.DataFrame): raise ValueError(\\"The \'df\' parameter must be a Pandas DataFrame.\\") if not isinstance(n_colors, int) or n_colors <= 0: raise ValueError(\\"The \'n_colors\' parameter must be a positive integer.\\") if not (0 <= lightness <= 1): raise ValueError(\\"The \'lightness\' parameter must be a float between 0 and 1.\\") if not (0 <= saturation <= 1): raise ValueError(\\"The \'saturation\' parameter must be a float between 0 and 1.\\") if not (0 <= hue_start <= 1): raise ValueError(\\"The \'hue_start\' parameter must be a float between 0 and 1.\\") # Generate the HLS color palette palette = sns.hls_palette(n_colors, l=lightness, s=saturation, h=hue_start, as_cmap=as_cmap) # Create the bar plot plt.figure(figsize=(10, 6)) ax = sns.barplot(x=df.columns[0], y=df.columns[1], data=df, palette=palette) plt.title(\'Bar plot with Custom HLS Color Palette\') plt.show()"},{"question":"# Configuration File Parser with `configparser` Objective Implement a function that reads a configuration file, adds new configurations, and updates existing ones, using the `configparser` module. Function Signature ```python def manage_config(file_path: str, updates: dict, section: str) -> dict: Reads the configuration from the given file, applies updates from the updates dictionary to the specified section, and returns the merged configuration as a dictionary. :param file_path: Path to the configuration file. :param updates: A dictionary containing the new key-value pairs to be updated or added. :param section: The section of the configuration file to update. :return: A dictionary of the updated configuration. ``` Input - `file_path` : A string representing the path to the `.ini` configuration file. - `updates` : A dictionary containing key-value pairs to update in the specified section. - `section` : A string representing the section of the configuration file where the updates need to be made. Output - A dictionary representing the updated configuration of the specified section. Constraints - The `file_path` provided must point to a valid configuration file. - The `section` specified must exist in the configuration file. - Any provided `updates` should be applied or added to the specified section. Example Consider the following configuration file `example.ini`: ``` [General] setting1 = value1 setting2 = value2 [User] name = John Doe email = john.doe@example.com ``` If `file_path` is `\'example.ini\'`, `updates` is `{\'setting2\': \'new_value2\', \'setting3\': \'value3\'}` and `section` is `\'General\'`, the function `manage_config` should: 1. Read the current configuration from `example.ini`. 2. Apply the updates, causing `setting2` to change to `new_value2` and adding `setting3` with `value3` to the `General` section. 3. Return the following dictionary: ```python { \'setting1\': \'value1\', \'setting2\': \'new_value2\', \'setting3\': \'value3\' } ``` Notes 1. Ensure that the original configuration file is not modified; instead, only the in-memory representation should be changed. 2. Raise an appropriate error if the specified section does not exist. Function Example Usage ```python config_updates = {\'setting2\': \'new_value2\', \'setting3\': \'value3\'} updated_config = manage_config(\'example.ini\', config_updates, \'General\') print(updated_config) # Output: # { # \'setting1\': \'value1\', # \'setting2\': \'new_value2\', # \'setting3\': \'value3\' # } ``` Hints - Utilize `configparser.ConfigParser()` to read and parse the configuration file. - Use the `.set()` method to update or add new key-value pairs in the specified section. - Use `.has_section()` to check if the section exists in the configuration file.","solution":"import configparser def manage_config(file_path: str, updates: dict, section: str) -> dict: Reads the configuration from the given file, applies updates from the updates dictionary to the specified section, and returns the merged configuration as a dictionary. :param file_path: Path to the configuration file. :param updates: A dictionary containing the new key-value pairs to be updated or added. :param section: The section of the configuration file to update. :return: A dictionary of the updated configuration. config = configparser.ConfigParser() config.read(file_path) if not config.has_section(section): raise ValueError(f\\"Section \'{section}\' not found in the configuration file\\") for key, value in updates.items(): config.set(section, key, value) updated_config = {key: value for key, value in config.items(section)} return updated_config"},{"question":"Directory Tree Management Your task is to write Python functions to manage and display information about a directory tree using the `os` module. You will write two functions: 1. **get_directory_info**: This function will explore a directory tree and gather information about each file and directory. 2. **display_tree**: This function will print the structure and information gathered by `get_directory_info` in a readable format. Function 1: `get_directory_info(path:str) -> dict` - **Input**: A string `path` representing the path to the root directory. - **Output**: A dictionary with the directory tree structure and file information. - **Description**: This function should recursively gather information about each file and directory within the specified path. The dictionary should map each directory or file name to another dictionary containing: - `type`: Either `\\"file\\"` or `\\"directory\\"`. - `size`: The size of the file in bytes (if applicable). - `contents` (for directories): Another dictionary containing the same structure for nested files and directories. Function 2: `display_tree(info:dict, indent:int = 0) -> None` - **Input**: - `info`: The dictionary returned by `get_directory_info`. - `indent`: An integer representing the current level of indentation (used for nested directories). - **Output**: None (prints the directory tree structure). - **Description**: This function should print out the directory tree structure in a readable format, with subdirectory contents indented. Example Given a directory tree like this: ``` root/ ├── file1.txt (size: 100 bytes) └── subdir/ ├── file2.txt (size: 200 bytes) └── file3.txt (size: 300 bytes) ``` `get_directory_info(\\"root\\")` should return: ```python { \\"root\\": { \\"type\\": \\"directory\\", \\"contents\\": { \\"file1.txt\\": { \\"type\\": \\"file\\", \\"size\\": 100 }, \\"subdir\\": { \\"type\\": \\"directory\\", \\"contents\\": { \\"file2.txt\\": { \\"type\\": \\"file\\", \\"size\\": 200 }, \\"file3.txt\\": { \\"type\\": \\"file\\", \\"size\\": 300 } } } } } } ``` Then, `display_tree(info)` should print: ``` root/ │-- file1.txt (size: 100 bytes) └-- subdir/ │-- file2.txt (size: 200 bytes) └-- file3.txt (size: 300 bytes) ``` Constraints - Handle `OSError` exceptions that might occur during directory and file operations. - Ensure the functions work recursively for deeply nested directories. - You can assume that paths provided to `get_directory_info` always exist. Evaluation Your implementation will be evaluated based on: - Correctness of the directory and file structure representation. - Proper use of the `os` module functions. - Readability and formatting of the printed directory tree structure by `display_tree`. - Error handling and robustness of the code. Solution Template ```python import os def get_directory_info(path): # Implementation here pass def display_tree(info, indent=0): # Implementation here pass ```","solution":"import os def get_directory_info(path): Recursively gather information about each file and directory within the specified path. Args: - path (str): The path to the root directory. Returns: - dict: The directory tree structure and file information. try: info = {} if os.path.isdir(path): info[\'type\'] = \'directory\' contents = {} for item in os.listdir(path): item_path = os.path.join(path, item) contents[item] = get_directory_info(item_path) info[\'contents\'] = contents else: info[\'type\'] = \'file\' info[\'size\'] = os.path.getsize(path) return info except OSError as e: print(f\\"Error: {e}\\") return None def display_tree(info, indent=0): Print out the directory tree structure in a readable format. Args: - info (dict): The dictionary returned by get_directory_info. - indent (int): The current level of indentation (used for nested directories). if info is None: return # Sorting the dictionary to ensure consistent output info_contents = info.get(\'contents\', {}) sorted_items = sorted(info_contents.items(), key=lambda x: x[0]) indent_str = \' \' * 4 * indent for index, (name, sub_info) in enumerate(sorted_items): symbol = \'│--\' if index < len(sorted_items) - 1 else \'└--\' if sub_info[\'type\'] == \'directory\': print(f\\"{indent_str}{symbol} {name}/\\") display_tree(sub_info, indent + 1) else: size = sub_info[\'size\'] print(f\\"{indent_str}{symbol} {name} (size: {size} bytes)\\")"},{"question":"# PyTorch Tensor Manipulation and Operations **Objective:** Implement a function using PyTorch to manipulate and perform operations on tensors as per the following specifications. # Problem Statement You need to create a function `tensor_operations` that performs a series of operations on input tensors. **Function Signature:** ```python def tensor_operations(t1: torch.Tensor, t2: torch.Tensor, op_list: list) -> torch.Tensor: pass ``` # Input: 1. `t1` (torch.Tensor): A tensor of any shape containing integer or float values. 2. `t2` (torch.Tensor): A tensor of the same shape as `t1` containing integer or float values. 3. `op_list` (list): A list of operations to be performed sequentially on the tensors. Each operation in the list is represented by a string and can be one of the following: - `\\"add\\"`: Add tensors `t1` and `t2`. - `\\"sub\\"`: Subtract tensor `t2` from tensor `t1`. - `\\"mul\\"`: Multiply tensor `t1` and tensor `t2`. - `\\"div\\"`: Divide tensor `t1` by tensor `t2`. - `\\"pow\\"`: Apply power operation, where each element of `t1` is raised to the power of corresponding element in `t2`. - `\\"sqrt\\"`: Apply square root operation to each element of `t1`. # Output: - Returns a new tensor that results from applying the operations specified in `op_list` sequentially. # Constraints: - The tensors `t1` and `t2` will always have the same shape. - The list `op_list` will contain at least one operation and at most five operations. - Ensure not to modify the input tensors `t1` and `t2` directly; instead, work on copies of the input tensors. # Example: ```python import torch # Example tensors t1 = torch.tensor([[1.0, 4.0], [9.0, 16.0]], dtype=torch.float32) t2 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) # Operations List op_list = [\\"add\\", \\"mul\\", \\"sqrt\\"] # Calling the function result = tensor_operations(t1, t2, op_list) # Expected Output print(result) # Should print a tensor as result of sequential operations ``` # Explanation: 1. The first operation `\\"add\\"` adds `t1` and `t2`. 2. The second operation `\\"mul\\"` multiplies the result from the first step with `t2`. 3. The third operation `\\"sqrt\\"` applies the square root operation to the result from the second step. # Notes: - Make use of PyTorch\'s in-built tensor operations. - Ensure the resulting tensor is returned and verify the result for correctness. # Additional Information - To handle different tensor types, ensure you are utilizing appropriate dtype handling functions. - Look out for any potential in-place operations and avoid them if not necessary.","solution":"import torch def tensor_operations(t1: torch.Tensor, t2: torch.Tensor, op_list: list) -> torch.Tensor: result = t1.clone() # Start with a copy of t1 to avoid modifying the input tensor for op in op_list: if op == \\"add\\": result = result + t2 elif op == \\"sub\\": result = result - t2 elif op == \\"mul\\": result = result * t2 elif op == \\"div\\": result = result / t2 elif op == \\"pow\\": result = result.pow(t2) elif op == \\"sqrt\\": result = result.sqrt() else: raise ValueError(f\\"Unsupported operation: {op}\\") return result"},{"question":"Question: Implementing Concurrent Download Manager You are tasked with creating a concurrent download manager that can download files from the internet using both threading and multiprocessing to speed up the process. Your task is to write a Python program that demonstrates the following: 1. **Thread-based Downloading**: Use the `threading` module to download multiple files concurrently. Ensure thread synchronization to avoid concurrency issues. 2. **Process-based Downloading**: Use the `multiprocessing` module to download multiple files concurrently. Share the download status between processes. 3. **Progress Reporting**: Implement a mechanism to report the download progress in a thread-safe and process-safe manner. # Requirements - The program should take a list of URLs as input and download each file concurrently. - Utilize both threading and multiprocessing techniques in your solution. - Ensure that the downloading processes are synchronized to prevent race conditions. - Provide a function to report the progress of each file download. # Input - A list of URLs to download (e.g., `[\\"http://example.com/file1\\", \\"http://example.com/file2\\"]`). # Output - Print the download progress of each file in real-time. - Save each downloaded file to the local filesystem. # Constraints - Use the `requests` library for downloading files. - Use appropriate synchronization primitives (`Lock`, `Semaphore`, etc.) provided by `threading` and `multiprocessing` modules. - Handle exceptions and edge cases such as network failures or invalid URLs gracefully. # Example Usage ```python urls = [ \\"http://example.com/file1\\", \\"http://example.com/file2\\", \\"http://example.com/file3\\" ] # Create an instance of the download manager (to be implemented) download_manager = DownloadManager(urls) # Start downloading using threading download_manager.download_using_threads() # Start downloading using multiprocessing download_manager.download_using_processes() ``` # Performance Considerations - Consider the overhead of creating threads vs processes and the time it takes to switch context. - Ensure that your solution efficiently handles I/O-bound operations. Implement the `DownloadManager` class and the necessary functions to complete this task.","solution":"import os import threading import multiprocessing import requests from queue import Queue from multiprocessing import Manager class DownloadManager: def __init__(self, urls): self.urls = urls self.file_lock = threading.Lock() self.download_status = {} def _download_file(self, url, progress_queue=None): local_filename = url.split(\'/\')[-1] try: with requests.get(url, stream=True) as r: r.raise_for_status() with open(local_filename, \'wb\') as f: for chunk in r.iter_content(chunk_size=8192): f.write(chunk) if progress_queue: progress_queue.put((url, len(chunk))) else: with self.file_lock: self.download_status[url] += len(chunk) print(f\\"{local_filename} downloaded successfully\\") except Exception as e: print(f\\"Error downloading {url}: {e}\\") def download_using_threads(self): threads = [] for url in self.urls: self.download_status[url] = 0 t = threading.Thread(target=self._download_file, args=(url,)) threads.append(t) t.start() for t in threads: t.join() def download_using_processes(self): with Manager() as manager: progress_queue = manager.Queue() processes = [] for url in self.urls: self.download_status[url] = 0 p = multiprocessing.Process(target=self._download_file, args=(url, progress_queue)) processes.append(p) p.start() while any(p.is_alive() for p in processes): while not progress_queue.empty(): url, size = progress_queue.get() with self.file_lock: self.download_status[url] += size for p in processes: p.join() def report_progress(self): for url, size in self.download_status.items(): print(f\\"{url}: {size} bytes downloaded\\") # Example usage (you should replace these URLs with actual URLs to test the code) # urls = [ # \\"http://example.com/file1\\", # \\"http://example.com/file2\\", # \\"http://example.com/file3\\" # ] # download_manager = DownloadManager(urls) # print(\\"Starting thread-based download\\") # download_manager.download_using_threads() # download_manager.report_progress() # print(\\"Starting process-based download\\") # download_manager.download_using_processes() # download_manager.report_progress()"},{"question":"**Coding Assessment Question** Title: Implement a Custom Fault Handling System **Objective:** Demonstrate your understanding of fault handling in Python using the `faulthandler` module. You will implement a custom fault handling system that can activate traceback dumps on various triggers, such as timeouts and signals. **Description:** You are tasked with implementing a class `FaultHandlerManager` that uses the `faulthandler` module to manage fault handling in a Python application. The class should provide methods to enable and disable fault handling, set up tracebacks to be dumped after a timeout, and register and unregister custom signals for dumping tracebacks. **Class Details:** ```python import faulthandler import signal import sys class FaultHandlerManager: def __init__(self, log_file=sys.stderr): Initialize the FaultHandlerManager with a log file. :param log_file: a file object where tracebacks will be written (default is sys.stderr) self.log_file = log_file def enable_handler(self, all_threads=True): Enable the fault handler. :param all_threads: if True, dump tracebacks for all threads; otherwise, only the current thread (default is True) faulthandler.enable(file=self.log_file, all_threads=all_threads) def disable_handler(self): Disable the fault handler. faulthandler.disable() def is_handler_enabled(self): Check if the fault handler is enabled. :return: True if enabled, False otherwise return faulthandler.is_enabled() def dump_traceback_after_timeout(self, timeout, repeat=False, exit_after_dump=False): Dump the tracebacks after a specified timeout. :param timeout: time in seconds after which to dump the tracebacks :param repeat: if True, dump tracebacks every timeout seconds (default is False) :param exit_after_dump: if True, exit the program after dumping the tracebacks (default is False) faulthandler.dump_traceback_later(timeout, repeat=repeat, file=self.log_file, exit=exit_after_dump) def cancel_traceback_dump(self): Cancel the previously set timeout for dumping tracebacks. faulthandler.cancel_dump_traceback_later() def register_signal_handler(self, signum=signal.SIGUSR1, all_threads=True, chain=False): Register a handler for a user signal to dump tracebacks. :param signum: signal number to register (default is SIGUSR1) :param all_threads: if True, dump tracebacks for all threads; otherwise, only the current thread (default is True) :param chain: if True, call the previous handler after dumping tracebacks (default is False) faulthandler.register(signum, file=self.log_file, all_threads=all_threads, chain=chain) def unregister_signal_handler(self, signum=signal.SIGUSR1): Unregister the handler for a user signal. :param signum: signal number to unregister (default is SIGUSR1) :return: True if the signal was registered, False otherwise return faulthandler.unregister(signum) ``` **Task:** 1. Implement the `FaultHandlerManager` class as described above. 2. Provide a simple use-case example demonstrating the following: - Enabling the fault handler. - Dumping tracebacks after a 5-second timeout. - Registering and triggering a custom signal to dump tracebacks. **Input and Output:** - The `FaultHandlerManager` does not take user inputs from stdin. - Output occurs when a fault or user signal triggers a traceback dump. **Constraints:** - Ensure that the file used for dumping tracebacks remains open until it is explicitly closed. - Use exception handling where necessary to ensure robust code. **Performance Requirements:** - The operations should have minimal overhead, focusing primarily on providing reliable traceback information. **Use-Case Example:** ```python import time import os import sys def example_usage(): # Create a log file for tracebacks log_file = open(\\"traceback.log\\", \\"w\\") # Initialize the FaultHandlerManager with the log file fm = FaultHandlerManager(log_file) # Enable the fault handler fm.enable_handler() # Set up a traceback dump after 5 seconds fm.dump_traceback_after_timeout(5) # Register a custom signal handler for SIGUSR1 fm.register_signal_handler() # Trigger the custom signal to cause a traceback dump os.kill(os.getpid(), signal.SIGUSR1) # Wait for some time to ensure the timeout dump occurs time.sleep(10) # Cleanup fm.disable_handler() log_file.close() if __name__ == \\"__main__\\": example_usage() ```","solution":"import faulthandler import signal import sys class FaultHandlerManager: def __init__(self, log_file=sys.stderr): Initialize the FaultHandlerManager with a log file. :param log_file: a file object where tracebacks will be written (default is sys.stderr) self.log_file = log_file def enable_handler(self, all_threads=True): Enable the fault handler. :param all_threads: if True, dump tracebacks for all threads; otherwise, only the current thread (default is True) faulthandler.enable(file=self.log_file, all_threads=all_threads) def disable_handler(self): Disable the fault handler. faulthandler.disable() def is_handler_enabled(self): Check if the fault handler is enabled. :return: True if enabled, False otherwise return faulthandler.is_enabled() def dump_traceback_after_timeout(self, timeout, repeat=False, exit_after_dump=False): Dump the tracebacks after a specified timeout. :param timeout: time in seconds after which to dump the tracebacks :param repeat: if True, dump tracebacks every timeout seconds (default is False) :param exit_after_dump: if True, exit the program after dumping the tracebacks (default is False) faulthandler.dump_traceback_later(timeout, repeat=repeat, file=self.log_file, exit=exit_after_dump) def cancel_traceback_dump(self): Cancel the previously set timeout for dumping tracebacks. faulthandler.cancel_dump_traceback_later() def register_signal_handler(self, signum=signal.SIGUSR1, all_threads=True, chain=False): Register a handler for a user signal to dump tracebacks. :param signum: signal number to register (default is SIGUSR1) :param all_threads: if True, dump tracebacks for all threads; otherwise, only the current thread (default is True) :param chain: if True, call the previous handler after dumping tracebacks (default is False) faulthandler.register(signum, file=self.log_file, all_threads=all_threads, chain=chain) def unregister_signal_handler(self, signum=signal.SIGUSR1): Unregister the handler for a user signal. :param signum: signal number to unregister (default is SIGUSR1) :return: True if the signal was registered, False otherwise return faulthandler.unregister(signum)"},{"question":"**Coding Assessment Question: PyTorch Serialization** # Objective Your task is to demonstrate your understanding of PyTorch\'s tensor and module serialization, including managing shared storage and minimizing file sizes when saving tensors with fewer elements than their storage objects. # Problem Statement You need to implement a function `save_tensor_dict_reduced_size(tensor_dict: dict, filename: str) -> None` that serializes a dictionary of tensors and saves it to a file. The function should ensure that if any tensor has fewer elements than its storage object, the file size is minimized by cloning the tensor before saving. This function should retain the view relationships if tensors with shared storage are present. Additionally, implement a function `load_tensor_dict(filename: str) -> dict` that deserializes the file back into a dictionary of tensors. # Requirements - Ensure the function `save_tensor_dict_reduced_size` minimizes file sizes where applicable. - Retain view relationships where tensors share storage. - Correctly reconstruct the original dictionary structure and tensor data in `load_tensor_dict`. # Function Signatures ```python import torch from typing import Dict def save_tensor_dict_reduced_size(tensor_dict: Dict[str, torch.Tensor], filename: str) -> None: pass def load_tensor_dict(filename: str) -> Dict[str, torch.Tensor]: pass ``` # Input - `save_tensor_dict_reduced_size`: - `tensor_dict`: A dictionary where keys are strings and values are PyTorch tensors. - `filename`: The name of the file where the tensor dictionary will be saved. - `load_tensor_dict`: - `filename`: The name of the file from which the tensor dictionary will be loaded. # Output - `save_tensor_dict_reduced_size`: No return value. The function saves the tensor dictionary to a file. - `load_tensor_dict`: Returns a dictionary with the same structure and tensor content as the original. # Constraints - Ensure the implemented functions handle large tensors and maintain memory efficiency. - Test cases will involve tensors with shared storage and varied sizes to verify correctness. # Example ```python tensor_dict = { \'a\': torch.arange(1, 10), \'b\': torch.arange(1, 10)[1::2], # Shared storage with \'a\' \'c\': torch.arange(1, 20)[0:5] # Will need size reduction } # Saving the tensor dictionary to \'tensor_dict.pt\' save_tensor_dict_reduced_size(tensor_dict, \'tensor_dict.pt\') # Loading the tensor dictionary from \'tensor_dict.pt\' loaded_tensor_dict = load_tensor_dict(\'tensor_dict.pt\') # Example Assertions (actual tests will be more comprehensive) assert torch.equal(tensor_dict[\'a\'], loaded_tensor_dict[\'a\']) assert torch.equal(tensor_dict[\'b\'] * 2, loaded_tensor_dict[\'a\'][1::2]) # Test shared storage view assert torch.equal(tensor_dict[\'c\'], loaded_tensor_dict[\'c\']) ``` *Note*: Maintain shared storage views where applicable. Perform thorough testing to ensure correctness.","solution":"import torch from typing import Dict def save_tensor_dict_reduced_size(tensor_dict: Dict[str, torch.Tensor], filename: str) -> None: Saves a dictionary of tensors to a file with minimized size when applicable. # Clone tensors to minimize file size if tensor has fewer elements than its storage object for key, tensor in tensor_dict.items(): if tensor.storage().size() > tensor.numel(): tensor_dict[key] = tensor.clone() # Save the cloned or original tensors to file torch.save(tensor_dict, filename) def load_tensor_dict(filename: str) -> Dict[str, torch.Tensor]: Loads a dictionary of tensors from a file. tensor_dict = torch.load(filename) return tensor_dict"}]'),A={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:q,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},R={class:"card-container"},F={key:0,class:"empty-state"},z=["disabled"],O={key:0},M={key:1};function N(s,e,l,m,r,i){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>r.searchQuery=o),placeholder:"Search..."},null,512),[[y,r.searchQuery]]),r.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>r.searchQuery="")}," ✕ ")):d("",!0)]),t("div",R,[(a(!0),n(b,null,v(i.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),i.displayedPoems.length===0?(a(),n("div",F,' No results found for "'+c(r.searchQuery)+'". ',1)):d("",!0)]),i.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:r.isLoading,onClick:e[2]||(e[2]=(...o)=>i.loadMore&&i.loadMore(...o))},[r.isLoading?(a(),n("span",M,"Loading...")):(a(),n("span",O,"See more"))],8,z)):d("",!0)])}const L=p(A,[["render",N],["__scopeId","data-v-10d3f9e7"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/61.md","filePath":"quotes/61.md"}'),j={name:"quotes/61.md"},H=Object.assign(j,{setup(s){return(e,l)=>(a(),n("div",null,[x(L)]))}});export{Y as __pageData,H as default};
