import{_ as p,o as a,c as n,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(i,e,l,m,o,s){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-73014651"]]),I=JSON.parse('[{"question":"**Objective:** The goal of this question is to demonstrate your understanding of scikit-learn transformers and how to use them for data preprocessing, feature extraction, and combining transformations. **Question:** You are provided with a dataset containing numerical and categorical features. Your task is to preprocess this data by performing the following transformations: 1. Impute missing numerical values with the mean of the respective columns. 2. Standardize all numerical features (zero mean, unit variance). 3. Encode categorical features using one-hot encoding. 4. Combine the transformations into a single pipeline that can handle the given dataset and output preprocessed data ready for a machine learning model. **Dataset:** Assume the dataset is loaded into a pandas DataFrame `df` with the following columns: - `feature1` (numerical) - `feature2` (numerical) - `feature3` (categorical) - `feature4` (categorical) **Requirements:** 1. Implement a function `preprocess_data(df: pd.DataFrame) -> np.ndarray` that takes a pandas DataFrame as input and returns the preprocessed data as a numpy ndarray. 2. Use `SimpleImputer` for imputing missing values and `StandardScaler` for standardization. 3. Use `OneHotEncoder` for encoding categorical features. 4. Combine these transformations in a single `ColumnTransformer` and integrate them into a `Pipeline`. 5. Ensure that the pipeline can be fit and transformed on the provided dataset. **Constraints:** - Ensure the code is efficient and follows scikit-learn best practices. - You may assume that the dataset is not excessively large and fits into memory. - Handle any potential issues that might arise with missing values or categorical encoding. **Function Signature:** ```python import pandas as pd import numpy as np from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline def preprocess_data(df: pd.DataFrame) -> np.ndarray: pass ``` **Example:** ```python import pandas as pd # Example dataset data = { \\"feature1\\": [1.0, 2.0, np.nan, 4.0], \\"feature2\\": [np.nan, 2.5, 3.5, 4.5], \\"feature3\\": [\\"A\\", \\"B\\", \\"A\\", \\"B\\"], \\"feature4\\": [\\"X\\", \\"Y\\", \\"X\\", np.nan] } df = pd.DataFrame(data) # Expected output: Preprocessed data as np.ndarray processed_data = preprocess_data(df) print(processed_data) ``` **Note:** The actual values in the `processed_data` array will depend on the specific preprocessing steps applied, but it should be a fully numeric array suitable for feeding into a machine learning model.","solution":"import pandas as pd import numpy as np from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline def preprocess_data(df: pd.DataFrame) -> np.ndarray: # Split columns into numerical and categorical numerical_features = [\'feature1\', \'feature2\'] categorical_features = [\'feature3\', \'feature4\'] # Define transformations for numerical data numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Define transformations for categorical data categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine both transformers into a single ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Fit the preprocessor and transform the data processed_data = preprocessor.fit_transform(df) return processed_data"},{"question":"# Coding Assessment Question Implement a function `extract_non_empty_lines(filename: str) -> list` that reads all lines from the given file named `filename` and returns a list containing only the non-empty lines. This function should use the functionalities of the `linecache` module for line retrieval. Function Signature ```python def extract_non_empty_lines(filename: str) -> list: pass ``` Input - `filename` (str): The path to the file from which lines should be read. Output - A list of strings, each representing a non-empty line from the file. Constraints - You may assume that the file contains at most 10,000 lines. - Lines are considered non-empty if they contain any characters other than whitespace. Example Suppose `example.txt` contains the following lines: ``` This is line 1 This is line 3 # This is line 5 (whitespace before comment) ``` Calling `extract_non_empty_lines(\'example.txt\')` should return: ``` [\'This is line 1\', \'This is line 3\', \'# This is line 5 (whitespace before comment)\'] ``` Notes - Use the `linecache.getline(filename, lineno)` function to read each line from the file. - Avoid reading the entire file into memory at once. Instead, handle one line at a time. - Use `linecache.clearcache()` to clear the cache if necessary. Tips - You can use `str.strip()` to check if a line is non-empty. - Consider using `linecache.checkcache()` to ensure the file has not changed if reading it multiple times.","solution":"import linecache def extract_non_empty_lines(filename: str) -> list: Reads all lines from the given file and returns a list of non-empty lines. Args: filename (str): The path to the file from which lines should be read. Returns: list: A list of non-empty lines from the file. non_empty_lines = [] lineno = 1 while True: line = linecache.getline(filename, lineno) if not line: break if line.strip(): non_empty_lines.append(line.strip()) lineno += 1 linecache.clearcache() return non_empty_lines"},{"question":"Objective: Implement a function to create, record, and manipulate events using PyTorch\'s distributed elastic events API. This assessment will test your understanding of the event handling mechanisms within PyTorch. Description: You are required to implement the function `manage_pytorch_events` which demonstrates the following tasks: 1. Create an event with a given `event_name` and `event_source`. 2. Add metadata to the event. 3. Record the event. 4. Retrieve and return the logging handler. Function Signature: ```python import torch.distributed.elastic.events.api as events_api def manage_pytorch_events(event_name: str, event_source: str, metadata: dict) -> events_api.Event: # Your implementation here ``` Input: - `event_name` (str): The name of the event to be created. - `event_source` (str): The source of the event. - `metadata` (dict): A dictionary containing key-value pairs to be added as metadata to the event. Output: - Returns an `Event` object with the attached metadata and recorded using the provided PyTorch events API. Constraints: - The `event_name` and `event_source` are non-empty strings. - The `metadata` dictionary contains key-value pairs where both keys and values are strings. Example: ```python event_name = \\"TestEvent\\" event_source = \\"TestSource\\" metadata = { \\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\" } event = manage_pytorch_events(event_name, event_source, metadata) print(event.name) # Expected Output: TestEvent print(event.source) # Expected Output: TestSource print(event.metadata[\\"key1\\"]) # Expected Output: value1 print(event.metadata[\\"key2\\"]) # Expected Output: value2 ``` Notes: - Ensure that the function correctly handles the creation, metadata addition, and recording of the event. - You may refer to the PyTorch distributed elastic events API documentation for more details on the methods and classes used in this task.","solution":"import torch.distributed.elastic.events.api as events_api def manage_pytorch_events(event_name: str, event_source: str, metadata: dict) -> events_api.Event: Manage PyTorch events by creating, adding metadata, and recording the event. Args: event_name (str): The name of the event to be created. event_source (str): The source of the event. metadata (dict): A dictionary containing key-value pairs to be added as metadata to the event. Returns: events_api.Event: The created event with attached metadata. # Creating the event event = events_api.Event(name=event_name, source=event_source) # Adding metadata to the event for key, value in metadata.items(): event.metadata[key] = value # Recording the event event.record() return event"},{"question":"# Question: Implement Custom Pickling for a Complex Data Structure You are given a complex data structure `CustomList`, which essentially behaves like a Python list but adds some custom behavior. Your task is to make this custom data structure pickleable using Python\'s `copyreg` module. Objectives 1. Implement the `CustomList` class. 2. Write a custom pickling function for instances of `CustomList`. 3. Register this custom pickling function using `copyreg`. # Class Requirements Implement a class `CustomList` that: * Behaves like a list (supports indexing, appending, etc.). * Has an additional attribute `metadata` which stores a dictionary of metadata information. Methods to Implement - `__init__(self, data, metadata)`: Initializes the `CustomList` with `data` (a list of elements) and `metadata` (a dictionary). - Other necessary methods to ensure the class behaves like a list (such as `append`, etc.) # Custom Pickling Function Implement a function `pickle_custom_list(obj)` that: * Takes an instance of `CustomList`. * Returns the constructor of `CustomList` and a tuple with the data and metadata, i.e., it should return `CustomList, (obj.data, obj.metadata)`. # Register Pickling Function Use `copyreg.pickle` to register the `CustomList` type with your `pickle_custom_list` function. Constraints * You can assume metadata keys and values are always strings. * Implement the solution using Python standard libraries (`copyreg`, `pickle`). Example ``` >>> import copyreg, pickle >>> class CustomList: ... def __init__(self, data, metadata): ... self.data = data ... self.metadata = metadata ... def append(self, item): ... self.data.append(item) >>> def pickle_custom_list(obj): ... return CustomList, (obj.data, obj.metadata) >>> copyreg.pickle(CustomList, pickle_custom_list) ... >>> cl = CustomList([1, 2, 3], {\'author\': \'Jane Doe\'}) >>> p = pickle.dumps(cl) >>> print(p) >>> cl_loaded = pickle.loads(p) >>> print(cl_loaded.data) [1, 2, 3] >>> print(cl_loaded.metadata) {\'author\': \'Jane Doe\'} ``` # Evaluation Criteria * Correct implementation of the `CustomList` class. * Correct implementation of the pickling function. * Accurate registration of the pickling function. * Code should be syntactically correct and follow standard Python best practices.","solution":"import copyreg import pickle class CustomList: def __init__(self, data, metadata): self.data = data self.metadata = metadata def append(self, item): self.data.append(item) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __len__(self): return len(self.data) def pickle_custom_list(obj): return CustomList, (obj.data, obj.metadata) copyreg.pickle(CustomList, pickle_custom_list)"},{"question":"**Question: Implement a Custom PyTorch Autograd Function** **Objective:** You are required to implement a custom PyTorch autograd Function to demonstrate understanding of how autograd works under the hood, including saving tensors for the backward pass, handling in-place operations, and utilizing hooks. **Task:** 1. Implement a custom PyTorch autograd Function called `MySpecialOp` that performs an element-wise operation on an input tensor `x`, defined as `y = x * torch.abs(x)`. The backward pass should compute the gradient considering the operation\'s custom nature. 2. Save the intermediate tensors needed during the forward pass for the backward operation. 3. Implement hooks to pack and unpack saved tensors to/from a string representation (custom implementation for saving and loading). 4. Manage scenarios with `requires_grad`, and demonstrate the use of this function within different grad modes. **Requirements:** 1. Define the class `MySpecialOp` as a subclass of `torch.autograd.Function`. 2. Implement the static methods `forward` and `backward` for the class. 3. In the `forward` method, save any necessary tensors using `ctx.save_for_backward`. 4. In the `backward` method, retrieve the saved tensors and compute the gradients accordingly. 5. Define hooks (`pack_hook` and `unpack_hook`) to store saved tensors as strings. 6. Register these hooks on the saved tensors. 7. Demonstrate the usage of `MySpecialOp` in the following scenarios: - Regular computation with gradient tracking. - Computation within `torch.no_grad()` context. - Computation within `torch.inference_mode()` context. **Input:** - A single tensor `x` of shape `(N,)` with `requires_grad=True`. **Output:** - The tensor `y` after `MySpecialOp` is applied. - The gradient of `y` with respect to `x`. ```python import torch from torch.autograd import Function class MySpecialOp(Function): @staticmethod def forward(ctx, x): # Save input tensor ctx.save_for_backward(x) # Apply the custom operation y = x * torch.abs(x) return y @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor x, = ctx.saved_tensors # Compute the gradient considering the operation y = x * |x| grad_input = grad_output * (2 * torch.abs(x)) return grad_input # Implement the hooks def pack_hook(tensor): return str(tensor.tolist()) def unpack_hook(packed_tensor): return torch.tensor(eval(packed_tensor)) # Usage: # Register hooks for the custom function x = torch.randn(5, requires_grad=True) y = MySpecialOp.apply(x) grad_fn = y.grad_fn grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook) # Compute forward and backward pass y.sum().backward() # Print gradients and tensors print(\\"Input Tensor:\\", x) print(\\"Output Tensor:\\", y) print(\\"Gradient:\\", x.grad) # Demonstrate in no_grad and inference_mode contexts with torch.no_grad(): y_no_grad = MySpecialOp.apply(x) print(\\"Output Tensor in no_grad:\\", y_no_grad) with torch.inference_mode(): y_inference = MySpecialOp.apply(x) print(\\"Output Tensor in inference_mode:\\", y_inference) ``` **Constraints:** - Ensure the correctness of the gradient computation. - Verify the hook registration by examining the packed and unpacked tensors. - Illustrate the effects of different grad modes on your custom operation. **Performance Requirements:** - The implementation should effectively handle tensors up to `(1000,)` without significant performance degradation.","solution":"import torch from torch.autograd import Function class MySpecialOp(Function): @staticmethod def forward(ctx, x): # Save input tensor ctx.save_for_backward(x) # Apply the custom operation y = x * torch.abs(x) return y @staticmethod def backward(ctx, grad_output): # Retrieve saved tensor x, = ctx.saved_tensors # Compute the gradient considering the operation y = x * |x| grad_input = grad_output * (2 * x) return grad_input # Implement the hooks def pack_hook(tensor): return str(tensor.tolist()) def unpack_hook(packed_tensor): return torch.tensor(eval(packed_tensor)) # Register hooks for demonstration x = torch.randn(5, requires_grad=True) y = MySpecialOp.apply(x) grad_fn = y.grad_fn for saved_tensor in grad_fn.saved_tensors: saved_tensor.register_hook(lambda grad: print(f\\"Gradient through hook: {grad}\\")) # Compute forward and backward pass y.sum().backward() # Print gradients and tensors print(\\"Input Tensor:\\", x) print(\\"Output Tensor:\\", y) print(\\"Gradient:\\", x.grad) # Demonstrate in no_grad and inference_mode contexts with torch.no_grad(): y_no_grad = MySpecialOp.apply(x) print(\\"Output Tensor in no_grad:\\", y_no_grad) with torch.inference_mode(): y_inference = MySpecialOp.apply(x) print(\\"Output Tensor in inference_mode:\\", y_inference)"},{"question":"**Coding Assessment Question** **Objective**: This question assesses your ability to work with the Python `site` module, manage module search paths, and handle site-specific customizations. You are required to implement a function that adds a given directory to the Python module search path and verifies its addition. # Problem Description **Function Signature**: ```python def add_directory_to_site_path(dir_path: str) -> bool: Adds the specified directory to the Python module search path and verifies its addition. Parameters: - dir_path (str): The directory path to be added to the Python module search path. Returns: - bool: True if the directory was successfully added to the module search path, False otherwise. ``` # Instructions 1. Implement the function `add_directory_to_site_path(dir_path)` which: - Uses the `site.addsitedir()` function to add the `dir_path` to the Python module search path. - Checks if the `dir_path` has been added to `sys.path`. 2. Your implementation should: - Handle any exceptions that may occur during the addition of the directory. - Return `True` if the directory was successfully added to the path, otherwise return `False`. 3. **Constraints**: - The `dir_path` provided will always be a valid directory path. - Ensure that the function has no side-effects other than adding the directory to the current `sys.path`. # Example ```python # Example Usage result = add_directory_to_site_path(\\"/usr/local/lib/python3.10/site-packages\\") print(result) # Output: True or False based on whether the directory was added successfully ``` # Note - Ensure that your function properly interacts with the `site.addsitedir()` function. - You may want to use the `sys` module to verify the changes in the module search path.","solution":"import site import sys def add_directory_to_site_path(dir_path: str) -> bool: Adds the specified directory to the Python module search path and verifies its addition. Parameters: - dir_path (str): The directory path to be added to the Python module search path. Returns: - bool: True if the directory was successfully added to the module search path, False otherwise. try: site.addsitedir(dir_path) return dir_path in sys.path except Exception: return False"},{"question":"You are tasked with managing a key-value store to maintain user session information using the `dbm` module. Your goal is to implement a Python function `manage_user_sessions` that performs various operations on a session database. The database will store session data with session IDs as keys and usernames as values. # Requirements: 1. **Function Definition:** Implement a function `manage_user_sessions(database_file: str, operations: List[Tuple[str, str, Optional[str]]]) -> List[Optional[str]]` that performs session management operations and returns results of `get` operations. 2. **Parameters:** - `database_file` (str): Path to the database file. - `operations` (List[Tuple[str, str, Optional[str]]]): A list of operations to be performed. Each operation is represented as a tuple with: - An operation type: either `\'get\'`, `\'set\'`, or `\'delete\'`. - Session ID (str): The session ID for the operation. - Username (Optional[str]): The username for the `\'set\'` operation. This will be `None` for `\'get\'` and `\'delete\'` operations. 3. **Returns:** - List[Optional[str]]: A list containing the results of the `\'get\'` operations in the order they are performed. Each result will either be the username associated with the session ID or `None` if the session ID does not exist. Other operations should return `None`. # Constraints: - The function should handle cases where the database file may not exist initially. - The function should ensure that all keys and values in the database are properly encoded as bytes. - Assume session IDs and usernames are non-empty strings and consist of alphanumeric characters. # Example: ```python def manage_user_sessions(database_file: str, operations: List[Tuple[str, str, Optional[str]]]) -> List[Optional[str]]: # Your implementation here # Example usage: operations = [ (\'set\', \'session1\', \'user1\'), (\'set\', \'session2\', \'user2\'), (\'get\', \'session1\', None), (\'delete\', \'session2\', None), (\'get\', \'session2\', None) ] results = manage_user_sessions(\'session_db\', operations) print(results) # Output: [b\'user1\', None] ``` # Functionality to Implement: 1. **Opening the Database:** - Open the database file with the `\'c\'` flag to create if it does not exist. 2. **Performing Operations:** - For `\'set\'` operation: Store the session ID and username in the database. Ensure keys and values are bytes. - For `\'get\'` operation: Retrieve the username for the given session ID and add it to the results list (if exists). - For `\'delete\'` operation: Remove the session ID from the database if it exists. 3. **Closing the Database:** - Ensure the database is closed properly after all operations are performed. # Notes: - You may use the `dbm` module\'s functions such as `dbm.open()`. - Ensure that the function handles exceptions gracefully, especially when trying to access non-existing keys. Implement the function and test it with the provided example to verify its correctness.","solution":"import dbm from typing import List, Tuple, Optional def manage_user_sessions(database_file: str, operations: List[Tuple[str, str, Optional[str]]]) -> List[Optional[str]]: results = [] with dbm.open(database_file, \'c\') as db: for operation, session_id, username in operations: session_id_bytes = session_id.encode(\'utf-8\') if operation == \'set\': db[session_id_bytes] = username.encode(\'utf-8\') results.append(None) elif operation == \'get\': user = db.get(session_id_bytes) results.append(user.decode(\'utf-8\') if user else None) elif operation == \'delete\': if session_id_bytes in db: del db[session_id_bytes] results.append(None) return results"},{"question":"**Objective**: Implement a function that processes and normalizes a list of URLs using the `urllib.parse` module. **Problem Statement**: You are given a list of URLs. Each URL can be either absolute or relative. You need to: 1. Normalize the URLs to ensure they have a consistent format. 2. Convert all the URLs to absolute URLs using a given base URL. 3. Remove any fragment identifiers from the URLs. 4. Convert the query components of the URLs into dictionaries. # Function Signature ```python def process_and_normalize_urls(urls: List[str], base_url: str) -> List[Dict[str, Any]]: pass ``` # Input - `urls`: A list of strings where each string is a URL. Example: `[\\"http://example.com/path?query=1#fragment\\", \\"relative/path?name=foo&age=30\\"]` - `base_url`: A string representing the base URL to use for converting relative URLs to absolute URLs. Example: `\\"http://example.com\\"` # Output - Return a list of dictionaries. Each dictionary should contain the following keys: - `normalized_url`: The normalized absolute URL without fragment. - `components`: A dictionary with the keys (scheme, netloc, path, params, query, fragment) corresponding to the parts of the URL. - `query_dict`: A dictionary representing the query string component of the URL. # Example ```python urls = [ \\"http://example.com/path?query=1#fragment\\", \\"relative/path?name=foo&age=30\\" ] base_url = \\"http://example.com\\" process_and_normalize_urls(urls, base_url) ``` Expected output: ```python [ { \'normalized_url\': \'http://example.com/path?query=1\', \'components\': { \'scheme\': \'http\', \'netloc\': \'example.com\', \'path\': \'/path\', \'params\': \'\', \'query\': \'query=1\', \'fragment\': \'\' }, \'query_dict\': { \'query\': [\'1\'] } }, { \'normalized_url\': \'http://example.com/relative/path?name=foo&age=30\', \'components\': { \'scheme\': \'http\', \'netloc\': \'example.com\', \'path\': \'/relative/path\', \'params\': \'\', \'query\': \'name=foo&age=30\', \'fragment\': \'\' }, \'query_dict\': { \'name\': [\'foo\'], \'age\': [\'30\'] } } ] ``` # Constraints - All URLs in the input list are valid URLs. - The URLs are case-sensitive. - The length of the `urls` list does not exceed 1000. - The length of any single URL does not exceed 2000 characters. # Requirements - Use the `urllib.parse` module for URL parsing and processing. - The function should handle both absolute and relative URLs. - Ensure the URLs are normalized and fragment identifiers are removed. - Convert query components into dictionaries. # Notes - Be sure to use the appropriate functions such as `urljoin()`, `urlparse()`, `urlsplit()`, `urldefrag()`, and `parse_qs()` to achieve the desired operations.","solution":"from urllib.parse import urljoin, urlparse, urldefrag, parse_qs from typing import List, Dict, Any def process_and_normalize_urls(urls: List[str], base_url: str) -> List[Dict[str, Any]]: result = [] for url in urls: # Ensure the URL is absolute using the base URL absolute_url = urljoin(base_url, url) # Remove the fragment defragged_url, fragment = urldefrag(absolute_url) # Parse the URL components parsed_url = urlparse(defragged_url) # Convert query to dictionary query_dict = parse_qs(parsed_url.query) # Prepare the result dictionary result.append({ \'normalized_url\': defragged_url, \'components\': { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment }, \'query_dict\': query_dict, }) return result"},{"question":"**Objective:** Implement a function that takes a DataFrame and a set of logical operations to perform using nullable boolean arrays. **Function Specification:** ```python def boolean_operations(df, column_name, fill_value=None): Apply a series of logical operations on a nullable boolean column in a DataFrame and return the modified DataFrame. Parameters: df (pd.DataFrame): Input DataFrame. column_name (str): Name of the column to perform operations on. fill_value (bool or None): Optional value to fill NA values before performing operations. If None, NA values are left unchanged. Returns: pd.DataFrame: DataFrame with the modified column after logical operations. ``` **Input:** - `df` is a pandas DataFrame containing at least one nullable boolean column specified by `column_name`. - `column_name` is a string indicating which column in `df` to work on. - `fill_value` is an optional boolean value (`True` or `False`). If provided, NA values in the specified column are filled with this value before performing logical operations. If `None`, NA values remain unchanged. **Output:** - Returns the original DataFrame with the specified column modified by performing these two logical operations: 1. Perform an AND operation (`&`) between the specified column and an array `[True, False, NA]` cyclically. 2. Perform an OR operation (`|`) between the result of step 1 and an array `[False, NA, True]` cyclically. **Constraints:** - The specified column must have the nullable boolean dtype. **Example:** Given the following DataFrame `df` and `column_name`: ```python df = pd.DataFrame({ \'id\': [1, 2, 3], \'nullable_bool\': pd.array([True, False, pd.NA], dtype=\\"boolean\\") }) ``` - Calling `boolean_operations(df, \'nullable_bool\', fill_value=True)` would: 1. Fill `NA` values with `True`. 2. Perform the AND operation: `[True, False, True] & [True, False, NA]` -> `[True, False, NA]`. 3. Perform the OR operation: `[True, False, NA] | [False, NA, True]` -> `[True, NA, True]`. The returned DataFrame should look like this: ```python id nullable_bool 0 1 True 1 2 NaN 2 3 True ``` **Note:** - Be mindful of Kleene Logic rules when handling `NA` values in logical operations.","solution":"import pandas as pd def boolean_operations(df, column_name, fill_value=None): Apply a series of logical operations on a nullable boolean column in a DataFrame and return the modified DataFrame. Parameters: df (pd.DataFrame): Input DataFrame. column_name (str): Name of the column to perform operations on. fill_value (bool or None): Optional value to fill NA values before performing operations. If None, NA values are left unchanged. Returns: pd.DataFrame: DataFrame with the modified column after logical operations. if column_name not in df.columns: raise ValueError(f\\"Column \'{column_name}\' not found in DataFrame\\") # Fill NA values if fill_value is provided if fill_value is not None: df[column_name] = df[column_name].fillna(fill_value) # Define the cyclic arrays for operations and_array = pd.array([True, False, pd.NA], dtype=\\"boolean\\") or_array = pd.array([False, pd.NA, True], dtype=\\"boolean\\") # Perform AND operation df[column_name] = df[column_name] & and_array[:len(df)] # Perform OR operation df[column_name] = df[column_name] | or_array[:len(df)] return df"},{"question":"# Advanced Python Tuples and Struct Sequences **Objective:** The goal of this question is to assess your understanding of Python tuples and the equivalent operations in the C API as described in the provided documentation. You will need to implement a series of Python functions that mimic the behavior of various C API functions for tuples using pure Python. **Task:** Implement the following Python functions to perform operations on tuples: 1. **`is_tuple(obj)`**: - Simulates `PyTuple_Check`. - **Input**: An object `obj`. - **Output**: Returns `True` if `obj` is a tuple or an instance of a subclass of a tuple, otherwise `False`. 2. **`is_exact_tuple(obj)`**: - Simulates `PyTuple_CheckExact`. - **Input**: An object `obj`. - **Output**: Returns `True` if `obj` is exactly a tuple, otherwise `False`. 3. **`create_tuple(size)`**: - Simulates `PyTuple_New`. - **Input**: An integer `size`. - **Output**: Returns a new tuple of the specified `size`, initialized with `None` values. 4. **`get_tuple_item(tpl, pos)`**: - Simulates `PyTuple_GetItem`. - **Input**: A tuple `tpl` and an integer `pos`. - **Output**: Returns the item at the specified `pos` in `tpl`. Raises `IndexError` if `pos` is out of bounds. 5. **`set_tuple_item(tpl, pos, item)`**: - Simulates `PyTuple_SetItem`. - **Input**: A tuple `tpl`, an integer `pos`, and an object `item`. - **Output**: Returns a new tuple with the item at `pos` set to `item`. Raises `IndexError` if `pos` is out of bounds. 6. **`resize_tuple(tpl, new_size)`**: - Simulates `_PyTuple_Resize`. - **Input**: A tuple `tpl` and an integer `new_size`. - **Output**: Returns a new tuple resized to `new_size`, with extra elements initialized to `None` or truncated. **Constraints:** - You are not allowed to use any additional libraries except those built into Python 3.10. - The `set_tuple_item` function should not modify the original tuple, as tuples are immutable. Instead, return a new tuple reflecting the change. - Handle exceptions appropriately, raising `IndexError` for out-of-bounds access where necessary. **Example Usage:** ```python # Check if object is a tuple print(is_tuple((1, 2, 3))) # True print(is_tuple([1, 2, 3])) # False # Check if object is exactly a tuple print(is_exact_tuple((1, 2, 3))) # True print(is_exact_tuple(tuple([1, 2, 3]))) # True print(is_exact_tuple(TupleSubclass())) # False (assuming TupleSubclass subclasses tuple) # Create a new tuple of specified size print(create_tuple(3)) # (None, None, None) # Get an item from a tuple example_tuple = (1, 2, 3) print(get_tuple_item(example_tuple, 1)) # 2 # Set an item in a tuple print(set_tuple_item(example_tuple, 1, 99)) # (1, 99, 3) # Resize a tuple print(resize_tuple((1, 2, 3), 5)) # (1, 2, 3, None, None) print(resize_tuple((1, 2, 3), 2)) # (1, 2) ``` Include all these methods in a single Python script file or Jupyter notebook. Test each function thoroughly with various edge cases and input variations.","solution":"def is_tuple(obj): Returns True if obj is a tuple or an instance of a subclass of a tuple, otherwise False. return isinstance(obj, tuple) def is_exact_tuple(obj): Returns True if obj is exactly a tuple, otherwise False. return type(obj) is tuple def create_tuple(size): Returns a new tuple of the specified size initialized with None values. return tuple([None] * size) def get_tuple_item(tpl, pos): Returns the item at the specified pos in tpl. Raises IndexError if pos is out of bounds. if pos < 0 or pos >= len(tpl): raise IndexError(\\"tuple index out of range\\") return tpl[pos] def set_tuple_item(tpl, pos, item): Returns a new tuple with the item at pos set to item. Raises IndexError if pos is out of bounds. if pos < 0 or pos >= len(tpl): raise IndexError(\\"tuple index out of range\\") return tpl[:pos] + (item,) + tpl[pos+1:] def resize_tuple(tpl, new_size): Returns a new tuple resized to new_size with extra elements initialized to None or truncated. if new_size < 0: raise ValueError(\\"new_size must be non-negative\\") return tuple(tpl[:new_size] + (None,) * (new_size - len(tpl)))"},{"question":"# Question: Custom Package Importer You are tasked with creating a custom module importer using Python\'s import utilities. Specifically, you need to develop a solution to import modules from a zip file, check if certain modules exist within that zip file, and list all the available modules in the zip archive. Requirements: 1. **Function `import_from_zip(zip_path: str, module_name: str)`:** - Imports a module from the specified zip file. - The `zip_path` parameter is the path to the zip file. - The `module_name` parameter is the name of the module to import. - Returns the imported module object if successful, raises an `ImportError` if the module does not exist. 2. **Function `module_exists_in_zip(zip_path: str, module_name: str) -> bool`:** - Checks if the specified module exists within the zip file. - The `zip_path` parameter is the path to the zip file. - The `module_name` parameter is the name of the module to check. - Returns `True` if the module exists, `False` otherwise. 3. **Function `list_modules_in_zip(zip_path: str) -> list`:** - Lists all the available modules in the zip file. - The `zip_path` parameter is the path to the zip file. - Returns a list of module names available in the zip archive. Constraints: - Assume the modules are pure Python files (i.e., `.py`). - Handle potential errors such as non-existent zip files or malformed archives gracefully. Examples: ```python # Assume the zip file \\"example.zip\\" contains modules: \\"mod1\\", \\"mod2\\", \\"mod3\\" # Import a module from a zip file mod = import_from_zip(\\"example.zip\\", \\"mod1\\") print(mod) # <module \'mod1\' from \'example.zip/mod1.py\'> # Check if a module exists in the zip file exists = module_exists_in_zip(\\"example.zip\\", \\"mod2\\") print(exists) # True # List all modules in the zip file modules = list_modules_in_zip(\\"example.zip\\") print(modules) # [\'mod1\', \'mod2\', \'mod3\'] ``` Additional Information: - You might find the `zipimport` module particularly useful for this task. - Remember to handle any exceptions and edge cases that might arise, such as file not found or invalid zip file formats.","solution":"import zipimport import os def import_from_zip(zip_path: str, module_name: str): Imports a module from the specified zip file. :param zip_path: Path to the zip file. :param module_name: Name of the module to import. :returns: Imported module object. :raises ImportError: If the module does not exist. if not os.path.isfile(zip_path): raise ImportError(f\\"Zip file \'{zip_path}\' does not exist.\\") try: importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) return module except ImportError: raise ImportError(f\\"Module \'{module_name}\' does not exist in \'{zip_path}\'.\\") def module_exists_in_zip(zip_path: str, module_name: str) -> bool: Checks if the specified module exists within the zip file. :param zip_path: Path to the zip file. :param module_name: Name of the module to check. :returns: True if the module exists, False otherwise. if not os.path.isfile(zip_path): return False try: importer = zipimport.zipimporter(zip_path) modules = importer._files.keys() return any(module_name in f for f in modules if f.endswith(\'.py\')) except Exception: return False def list_modules_in_zip(zip_path: str) -> list: Lists all the available modules in the zip file. :param zip_path: Path to the zip file. :returns: List of module names available in the zip archive. if not os.path.isfile(zip_path): raise ImportError(f\\"Zip file \'{zip_path}\' does not exist.\\") try: importer = zipimport.zipimporter(zip_path) modules = [os.path.splitext(name)[0] for name in importer._files.keys() if name.endswith(\'.py\')] return modules except Exception as e: raise ImportError(f\\"Failed to list modules in \'{zip_path}\': {e}\\")"},{"question":"**Coding Assessment Question: Implement a Custom Python Module Using C API Functions** # Objective Your task is to demonstrate your understanding of creating and managing Python modules using the C API provided by `python310`. You will implement a simple custom module in Python using the exposed C API functions. # Problem Statement Write a Python function that creates a custom module named `custommodule` with the following requirements: 1. **Module Initialization**: - Implement single-phase initialization to create the module. - The module must have the name `custommodule`. - The module should have a docstring: `\\"This is a custom module implemented using the python310 C API.\\"` 2. **Module Contents**: - Add an integer constant `version` with value `1` to the module. - Add a string constant `author` with value `\\"Your_Name\\"` to the module. - Add a custom function `add_numbers` that takes two integers as arguments and returns their sum. 3. **Functions to Implement**: - `initialize_custom_module()`: This function initializes the module and sets up its contents as described above. - `custom_module_info()`: This function returns a dictionary containing the module\'s `__name__`, `__doc__`, `version`, and `author`. # Implementation Details - Use the functions like `PyModule_Create`, `PyModule_AddIntConstant`, `PyModule_AddStringConstant`, and `PyModule_AddObjectRef()` to set up the module correctly. - Ensure proper error handling throughout the function implementations. - Your solution should be clear and maintainable, following Python\'s best coding practices. # Example Usage ```python # Initialize the module initialize_custom_module() # Retrieve and display the module info info = custom_module_info() print(info) # Example output # { # \'__name__\': \'custommodule\', # \'__doc__\': \'This is a custom module implemented using the python310 C API.\', # \'version\': 1, # \'author\': \'Your_Name\' # } # Use the custom function result = custommodule.add_numbers(3, 5) print(result) # Output: 8 ``` # Constraints - You are not allowed to use any high-level import mechanisms directly. Use the C API functions provided in the documentation. - Ensure your implementation is compatible with Python 3.10. # Performance Requirements - Your solution should handle typical use cases efficiently. There are no specific performance constraints, but strive for clarity and correctness.","solution":"def initialize_custom_module(): Initializes a custom module with name \'custommodule\' and sets up its contents. import types # Create a new module module = types.ModuleType(\'custommodule\') module.__doc__ = \\"This is a custom module implemented using the python310 C API.\\" # Add constants module.version = 1 module.author = \\"Your_Name\\" # Add custom function def add_numbers(a, b): return a + b module.add_numbers = add_numbers # Add the module to sys.modules import sys sys.modules[\'custommodule\'] = module def custom_module_info(): Returns a dictionary containing the module\'s info. import custommodule return { \'__name__\': custommodule.__name__, \'__doc__\': custommodule.__doc__, \'version\': custommodule.version, \'author\': custommodule.author }"},{"question":"# Python310 Coding Assessment Question: You are expected to design a system that integrates Python script execution into a C program using the Python310 API. Your task is to implement a Python function that: 1. Executes a given Python script (from a string input). 2. Compiles and evaluates the code, handling global and local variable scopes. 3. Should provide functionalities for running scripts interactively, if needed. The function specifications are: ```python def execute_python_code(code_str: str, globals_dict: dict = None, locals_dict: dict = None) -> None: Executes Python code from a given string. Args: - code_str (str): The Python code to be executed. - globals_dict (dict, optional): A dictionary representing the global scope variables. Defaults to None. - locals_dict (dict, optional): A dictionary representing the local scope variables. Defaults to None. Returns: - None Note: - If the compilation fails, an appropriate exception should be raised. - Ensure any side effects or print statements within the code_str are visible to the user. ``` Requirements: - The function should use `PyRun_StringFlags` for execution. - Proper error handling must be implemented to capture and report exceptions. - Handle the environment setups for global and local variables appropriately within the function. Constraints: - The implementation must adhere to the Python310 API and should require minimal dependencies outside the standard Python library. - Ensure all resources (like files, if any) are managed properly to avoid leaks. Example Usage: ```python global_scope = {\\"x\\": 10} local_scope = {\\"y\\": 5} code = result = x + y print(\\"The result is:\\", result) execute_python_code(code, globals_dict=global_scope, locals_dict=local_scope) # Expected Output: The result is: 15 ``` Note: - If `globals_dict` or `locals_dict` are not provided, use an empty dictionary by default. - Any exceptions raised during code execution should be printed as part of the output.","solution":"def execute_python_code(code_str: str, globals_dict: dict = None, locals_dict: dict = None) -> None: Executes Python code from a given string. Args: - code_str (str): The Python code to be executed. - globals_dict (dict, optional): A dictionary representing the global scope variables. Defaults to None. - locals_dict (dict, optional): A dictionary representing the local scope variables. Defaults to None. Returns: - None Note: - If the compilation fails, an appropriate exception should be raised. - Ensure any side effects or print statements within the code_str are visible to the user. # Default empty dictionaries for globals and locals if none are provided if globals_dict is None: globals_dict = {} if locals_dict is None: locals_dict = {} try: # Compile the code string compiled_code = compile(code_str, \'<string>\', \'exec\') # Execute the compiled code exec(compiled_code, globals_dict, locals_dict) except Exception as e: # Print the exception if any occurs print(f\\"An exception occurred: {e}\\") raise"},{"question":"**Question: Visualizing and Interpreting Distributions** You are given a dataset of penguin measurements (from the Palmer Archipelago). Your task is to utilize seaborn to create and interpret several visualizations to understand different aspects of the data distributions. Follow the steps below and ensure each step is implemented as specified. **Instructions:** 1. Load the penguin dataset from seaborn\'s built-in datasets. 2. Plot the distribution of the `flipper_length_mm` variable using a histogram with seaborn\'s `displot`. Customize the bin size to 5 mm and provide a description of the plot insights. 3. Plot a kernel density estimate (KDE) for the `flipper_length_mm` variable. Adjust the bandwidth to `0.5` and explain how the KDE plot differs from the histogram. 4. Create an ECDF plot for the `flipper_length_mm` variable and compare it with the histogram and KDE plots. 5. Visualize the bivariate distribution of `bill_length_mm` and `bill_depth_mm` using a KDE plot. Add contour lines to the plot and explain any visible patterns. 6. Use `pairplot` to visualize the pairwise relationships between the `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm` variables. Include histograms and KDE plots in the diagonal and off-diagonal positions of the grid. Ensure your code is organized and commented, and provide interpretations for each plot. **Expected Outputs and Considerations:** - Your plots should be well-labeled and legible. - Comment on the key differences and insights each type of plot provides. - Highlight any patterns, trends, or outliers you observe in the data. **Dataset:** Use the `penguins` dataset available within seaborn by calling: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` **Example Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 2: Histogram with bin size of 5 mm sns.displot(penguins, x=\'flipper_length_mm\', binwidth=5) plt.title(\\"Histogram of Flipper Length (mm) with Binwidth 5\\") plt.show() # Provide description here # Task 3: KDE plot with bandwidth adjustment sns.displot(penguins, x=\'flipper_length_mm\', kind=\'kde\', bw_adjust=0.5) plt.title(\\"KDE of Flipper Length (mm) with Bandwidth Adjust 0.5\\") plt.show() # Provide description here # Task 4: ECDF plot sns.displot(penguins, x=\'flipper_length_mm\', kind=\'ecdf\') plt.title(\\"ECDF of Flipper Length (mm)\\") plt.show() # Provide description here # Task 5: Bivariate KDE plot with contour sns.displot(penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', kind=\'kde\') plt.title(\\"Bivariate KDE of Bill Length and Depth (mm)\\") plt.show() # Provide description here # Task 6: Pairplot of selected variables sns.pairplot(penguins[[\'bill_length_mm\', \'bill_depth_mm\', \'flipper_length_mm\']]) plt.title(\\"Pairplot of Bill Length, Bill Depth, and Flipper Length (mm)\\") plt.show() # Provide description here ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 2: Histogram with bin size of 5 mm sns.displot(penguins, x=\'flipper_length_mm\', binwidth=5) plt.title(\\"Histogram of Flipper Length (mm) with Binwidth 5\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # The histogram displays the frequency distribution of flipper lengths with a binwidth of 5mm. # Most penguins have flipper lengths concentrated in the range of 180-200mm. # Task 3: KDE plot with bandwidth adjustment sns.displot(penguins, x=\'flipper_length_mm\', kind=\'kde\', bw_adjust=0.5) plt.title(\\"KDE of Flipper Length (mm) with Bandwidth Adjust 0.5\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # The KDE plot provides a smoothed estimate of the distribution of flipper lengths. # The bandwidth of 0.5 results in a finer smoothness compared to the default setting. # The peaks in the KDE plot correspond to the modes observed in the histogram. # Task 4: ECDF plot sns.displot(penguins, x=\'flipper_length_mm\', kind=\'ecdf\') plt.title(\\"ECDF of Flipper Length (mm)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() # The ECDF plot shows the cumulative probability distribution of flipper lengths. # It provides a way to observe percentiles and the median value directly. # Task 5: Bivariate KDE plot with contour sns.displot(penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', kind=\'kde\') plt.title(\\"Bivariate KDE of Bill Length and Depth (mm)\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # The bivariate KDE plot visualizes the density distribution between bill length and bill depth. # Contour lines indicate regions with higher densities. # Patterns show that bill length and depth have certain common relationship clusters. # Task 6: Pairplot of selected variables sns.pairplot(penguins[[\'bill_length_mm\', \'bill_depth_mm\', \'flipper_length_mm\']].dropna()) plt.suptitle(\\"Pairplot of Bill Length, Bill Depth, and Flipper Length (mm)\\", y=1.02) plt.show() # The pairplot shows pairwise relationships between the three variables. # Diagonal subplots display KDE for each variable. # Off-diagonal subplots show bivariate scatter plots and KDE plots, revealing correlations among variables."},{"question":"**Question:** In this task, you are required to implement a function `inspect_execution_environment()` that gathers comprehensive information about the current execution environment. The function should return a dictionary with detailed information about built-ins, locals, globals, and the current execution frame, and should produce the following structure: ```python { \\"builtins\\": {...}, # A dictionary of built-in functions and objects \\"locals\\": {...}, # A dictionary of local variables \\"globals\\": {...}, # A dictionary of global variables \\"frame_info\\": { \\"code\\": \\"<code object>\\", # The code object being executed \\"line_number\\": <line_number>, # The current line number \\"back_frame_exists\\": <boolean> # Whether an outer frame exists } } ``` # Function Signature: ```python def inspect_execution_environment() -> dict: pass ``` # Constraints and Requirements: - You must use the functions mentioned in the provided documentation to achieve this. - Ensure that all borrowed references are handled correctly. - If no frame is currently executing or some information is not available, handle these cases gracefully by setting those dictionary values to `None`. - Avoid using any third-party libraries. Rely only on standard Python 3.10 library functions. # Example Usage: Imagine the following scenario: ```python def example_function(): x = 10 y = 20 env_info = inspect_execution_environment() print(env_info) return env_info # Calling example_function should print and return a dictionary # containing the described structure with actual runtime information. example_function() ``` In the above example, `env_info` might look something like this (actual contents will vary based on the environment): ```python { \\"builtins\\": {...}, # Built-in functions and objects in the current context \\"locals\\": {\\"x\\": 10, \\"y\\": 20}, # Local variables in example_function \\"globals\\": {...}, # Global variables in the current module \\"frame_info\\": { \\"code\\": \\"<code object example_function at 0x...>\\", \\"line_number\\": 4, \\"back_frame_exists\\": True } } ``` This assessment will test your ability to use Python\'s introspection capabilities to inspect and manipulate execution frames and environments.","solution":"import sys def inspect_execution_environment() -> dict: Gathers comprehensive information about the current execution environment. Returns: dict: A dictionary containing built-ins, locals, globals, and frame information. frame = sys._getframe(1) builtins = frame.f_builtins locals_ = frame.f_locals globals_ = frame.f_globals code = frame.f_code line_number = frame.f_lineno back_frame_exists = frame.f_back is not None frame_info = { \\"code\\": code, \\"line_number\\": line_number, \\"back_frame_exists\\": back_frame_exists } return { \\"builtins\\": builtins, \\"locals\\": locals_, \\"globals\\": globals_, \\"frame_info\\": frame_info }"},{"question":"You are provided with a list of integers and tasked with performing the following operations using Python\'s functional programming modules (`itertools` and `functools`): 1. Filter the list to include only prime numbers. 2. Create a new list where each element is the square of the prime numbers from the filtered list. 3. Sum all the squared prime numbers to obtain a final result. Write a function called `process_primes` that takes a list of integers as input and returns an integer which is the sum of the squared prime numbers. Input - A list of integers `numbers` (1 <= length of `numbers` <= 10^4, -10^6 <= `numbers[i]` <= 10^6). Output - An integer which is the sum of the squares of prime numbers from the list. Constraints - You must use `itertools` and/or `functools` modules to achieve this. - The solution should be efficient in terms of both time and space complexity. Example ```python def process_primes(numbers): #Your code here # Example usage numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] print(process_primes(numbers)) # Output: 87 (2^2 + 3^2 + 5^2 + 7^2 = 4 + 9 + 25 + 49 = 87) ``` # Additional Information: Here is a quick guide to prime numbers and squaring operation: - Prime numbers are greater than 1 and divisible only by 1 and themselves. - Squaring a number `n` means multiplying `n` by itself: `n*n`. Your implementation should include helper functions where necessary and should make full use of functional programming practices.","solution":"from itertools import filterfalse from functools import reduce def is_prime(n): Returns True if n is a prime number, otherwise False. if n <= 1: return False if n == 2: return True if n % 2 == 0: return False p = 3 while p * p <= n: if n % p == 0: return False p += 2 return True def process_primes(numbers): Processes a list of integers to filter out prime numbers, square them, and return the sum of the squared prime numbers. prime_numbers = filter(is_prime, numbers) squared_primes = map(lambda x: x * x, prime_numbers) return reduce(lambda acc, x: acc + x, squared_primes, 0)"},{"question":"Class Hierarchy and Custom Iterator Objective Design a class hierarchy for managing a collection of books in a library. You will implement multiple classes with inheritance and iterator functionality. Your solution will demonstrate the following concepts: - Class and instance variables - Method overriding and inheritance - Custom iterator implementation using special methods Requirements 1. Create a base class `Book` with the following attributes and methods: - Instance variables: - `title` (str): title of the book - `author` (str): author of the book - `year` (int): publication year - Class variable: - `genre` (str): default genre for all books (set to \\"Unknown\\") - Methods: - `__init__(self, title, author, year)`: initializes the instance variables. - `info(self)`: returns a string containing the book title, author, and year. 2. Create a subclass `Novel` that inherits from `Book`. Add or override the following: - Class variable: - `genre`: overrides the base class variable to \\"Fiction\\". - Instance variables: - `chapters` (list): list to hold titles of chapters in the novel. - Methods: - `__init__(self, title, author, year, chapters)`: initializes the base class and instance variables. - `info(self)`: overrides the base class method to include the genre information. 3. Create another subclass `ScienceBook` that inherits from `Book`. Add or override the following: - Class variable: - `genre`: overrides the base class variable to \\"Science\\". - Instance variables: - `citations` (int): number of citations the book has received. - Methods: - `__init__(self, title, author, year, citations)`: initializes the base class and instance variables. - `info(self)`: overrides the base class method to include the genre and citation information. 4. Implement an iterable class `Library` that can store multiple `Book` instances. This class should: - Instance variables: - `books` (list): a list to hold `Book` instances. - Methods: - `__init__(self)`: initializes the books list. - `add_book(self, book)`: adds a `Book` instance to the library. - `__iter__(self)`: returns an iterator object for the library. - `__next__(self)`: retrieves the next book in the library. Example Usage ```python # Create a library library = Library() # Add books to the library library.add_book(Book(\\"Python 101\\", \\"John Doe\\", 2020)) library.add_book(Novel(\\"Epic Tale\\", \\"Jane Smith\\", 2018, [\\"Chapter 1\\", \\"Chapter 2\\"])) library.add_book(ScienceBook(\\"Science of AI\\", \\"Alice Brown\\", 2021, 50)) # Iterate over books and print their information for book in library: print(book.info()) ``` Constraints - Title and author should be non-empty strings. - Year should be a positive integer. - Chapters should be a list of strings. - Citations should be a non-negative integer. Expected Output ``` Python 101 by John Doe (2020) Epic Tale by Jane Smith (2018) - Genre: Fiction Science of AI by Alice Brown (2021) - Genre: Science, Citations: 50 ``` Submission Implement the classes `Book`, `Novel`, `ScienceBook`, and `Library` according to the given requirements. Ensure your solution is efficient and handles edge cases appropriately.","solution":"class Book: genre = \\"Unknown\\" def __init__(self, title, author, year): if not title or not author or year <= 0: raise ValueError(\\"Invalid input for book details.\\") self.title = title self.author = author self.year = year def info(self): return f\\"{self.title} by {self.author} ({self.year})\\" class Novel(Book): genre = \\"Fiction\\" def __init__(self, title, author, year, chapters): super().__init__(title, author, year) if not isinstance(chapters, list) or not all(isinstance(ch, str) for ch in chapters): raise ValueError(\\"Chapters should be a list of strings.\\") self.chapters = chapters def info(self): return f\\"{self.title} by {self.author} ({self.year}) - Genre: {self.genre}\\" class ScienceBook(Book): genre = \\"Science\\" def __init__(self, title, author, year, citations): super().__init__(title, author, year) if not isinstance(citations, int) or citations < 0: raise ValueError(\\"Citations should be a non-negative integer.\\") self.citations = citations def info(self): return f\\"{self.title} by {self.author} ({self.year}) - Genre: {self.genre}, Citations: {self.citations}\\" class Library: def __init__(self): self.books = [] self.index = 0 def add_book(self, book): if not isinstance(book, Book): raise ValueError(\\"Only instances of Book or its subclasses can be added.\\") self.books.append(book) def __iter__(self): self.index = 0 return self def __next__(self): if self.index < len(self.books): result = self.books[self.index] self.index += 1 return result else: raise StopIteration"},{"question":"# Question: Advanced Histogram Plotting with Seaborn You are provided with the `planets` and `penguins` datasets from seaborn\'s built-in datasets. Your task is to create a function that generates customized histograms based on the parameters provided. Function Signature ```python def customized_histograms(data: str, plot_type: str, **kwargs) -> None: Generate various customized histograms using seaborn\'s histplot function. Parameters: - data (str): The dataset to use, either \'planets\' or \'penguins\'. - plot_type (str): The type of plot to generate. Options are: - \'univariate\': Generates a univariate histogram. - \'bivariate\': Generates a bivariate histogram. - **kwargs: Additional keyword arguments to pass to the sns.histplot function. Returns: - None: This function should display the plot(s). pass ``` Input 1. `data`: A string specifying the dataset to use. It can be either: - `\'planets\'` - `\'penguins\'` 2. `plot_type`: A string specifying the type of plot to generate. It can be either: - `\'univariate\'` - `\'bivariate\'` 3. `**kwargs`: Additional keyword arguments to customize the `sns.histplot` function. These can include options such as: - `x` or `y`: The variable to plot on the corresponding axis (required). - `hue`: The variable to map plot aspects to different colors. - `binwidth`: Width of the bins. - `bins`: Number of bins. - `kde`: Whether to add a Kernel Density Estimate. - `log_scale`: Whether to plot on a log scale. - `element`: The graphical representation of the histogram bars, such as `\'step\'`, `\'poly\'`, etc. - Any other relevant parameter used by `sns.histplot`. Output - The function should display the customized histogram based on the provided parameters. Constraints - The function should handle both univariate and bivariate histograms. - If `plot_type` is `\'bivariate\'`, both `x` and `y` must be specified in `**kwargs`. - The function should apply relevant options (log scale, different elements, etc.) based on the input parameters. Example Usage ```python # Example 1: Univariate histogram of flipper_length_mm with kde and hue customized_histograms(\'penguins\', \'univariate\', x=\'flipper_length_mm\', kde=True, hue=\'species\') # Example 2: Bivariate histogram of bill_depth_mm and body_mass_g customized_histograms(\'penguins\', \'bivariate\', x=\'bill_depth_mm\', y=\'body_mass_g\') # Example 3: Univariate histogram of distance with log scale customized_histograms(\'planets\', \'univariate\', x=\'distance\', log_scale=True) ``` Notes - Ensure appropriate error handling for incorrect or missing parameters. - Utilize the seaborn library for generating the plots, and ensure the plots are well-formatted and informative.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_histograms(data: str, plot_type: str, **kwargs) -> None: Generate various customized histograms using seaborn\'s histplot function. Parameters: - data (str): The dataset to use, either \'planets\' or \'penguins\'. - plot_type (str): The type of plot to generate. Options are: - \'univariate\': Generates a univariate histogram. - \'bivariate\': Generates a bivariate histogram. - **kwargs: Additional keyword arguments to pass to the sns.histplot function. Returns: - None: This function should display the plot(s). # Load the appropriate dataset if data == \'planets\': dataset = sns.load_dataset(\'planets\') elif data == \'penguins\': dataset = sns.load_dataset(\'penguins\') else: raise ValueError(\\"Invalid dataset. Choose \'planets\' or \'penguins\'\\") # Check if necessary parameters are provided if plot_type == \'univariate\': if \'x\' not in kwargs: raise ValueError(\\"\'x\' is a required parameter for univariate histograms.\\") elif plot_type == \'bivariate\': if \'x\' not in kwargs or \'y\' not in kwargs: raise ValueError(\\"\'x\' and \'y\' are required parameters for bivariate histograms.\\") else: raise ValueError(\\"Invalid plot_type. Choose \'univariate\' or \'bivariate\'\\") # Plot the histogram plt.figure(figsize=(10, 6)) sns.histplot(data=dataset, **kwargs) plt.show()"},{"question":"You have been hired by a game development company that is designing a new game. As part of the game, players can encounter a variety of random events that require generating different types of random numbers. Your task is to implement a function that simulates these events. The function should generate the following: 1. **Random Encounter in a Given Range:** - Generate a random integer within a given inclusive range `[a, b]`. 2. **Random Treasure Chest Selection:** - Randomly select a treasure chest from a list of chests. 3. **Random Health Boost:** - Generate a random floating point number representing a health boost between a given range `[low, high]` with a given mode for the triangular distribution. 4. **Random Enemy Stats:** - Generate random values for enemy stats (strength, agility, and intelligence) following a Gaussian distribution, with given means and standard deviations. Implement the function `simulate_events(seed, int_range, chests, float_range, float_mode, enemy_stats)` where: - `seed`: an integer seed for the random number generator to ensure reproducibility. - `int_range`: a tuple `(a, b)` representing the range for the random integer. - `chests`: a list of strings, each string representing a treasure chest. - `float_range`: a tuple `(low, high)` representing the range for the health boost. - `float_mode`: a float representing the mode for the triangular distribution for the health boost. - `enemy_stats`: a dictionary with keys `\\"strength\\"`, `\\"agility\\"`, and `\\"intelligence\\"`, each mapping to a tuple `(mean, stdev)`. The function should: 1. Seed the random number generator using the given `seed`. 2. Return a dictionary with the following keys and their corresponding values: - `random_int`: a random integer within the inclusive range `[a, b]`. - `random_chest`: a randomly selected chest from the list `chests`. - `health_boost`: a random floating point number representing a health boost. - `enemy_stats`: a dictionary containing random values for `\\"strength\\"`, `\\"agility\\"`, and `\\"intelligence\\"` based on the provided Gaussian distribution parameters. # Example ```python def simulate_events(seed, int_range, chests, float_range, float_mode, enemy_stats): import random # Seed the random number generator random.seed(seed) # Generate random integer random_int = random.randint(int_range[0], int_range[1]) # Select random chest random_chest = random.choice(chests) # Generate health boost health_boost = random.triangular(float_range[0], float_range[1], float_mode) # Generate enemy stats enemy_stats_generated = { \\"strength\\": random.gauss(enemy_stats[\\"strength\\"][0], enemy_stats[\\"strength\\"][1]), \\"agility\\": random.gauss(enemy_stats[\\"agility\\"][0], enemy_stats[\\"agility\\"][1]), \\"intelligence\\": random.gauss(enemy_stats[\\"intelligence\\"][0], enemy_stats[\\"intelligence\\"][1]), } return { \\"random_int\\": random_int, \\"random_chest\\": random_chest, \\"health_boost\\": health_boost, \\"enemy_stats\\": enemy_stats_generated, } # Example usage seed = 42 int_range = (1, 10) chests = [\\"gold\\", \\"silver\\", \\"bronze\\"] float_range = (10.0, 20.0) float_mode = 15.0 enemy_stats = { \\"strength\\": (10.0, 2.0), \\"agility\\": (8.0, 1.5), \\"intelligence\\": (12.0, 3.0) } print(simulate_events(seed, int_range, chests, float_range, float_mode, enemy_stats)) ``` # Constraints - Ensure the random values for enemy stats (strength, agility, intelligence) follow a Gaussian distribution. - Inputs will be valid and within reasonable ranges (e.g., ranges will not be reversed, chests list will not be empty). # Evaluation Criteria - Correct usage of the `random` module functions. - Proper seeding to ensure reproducibility. - Accurate generation and return of the specified random values. - Code readability and adherence to Python coding standards.","solution":"def simulate_events(seed, int_range, chests, float_range, float_mode, enemy_stats): import random # Seed the random number generator random.seed(seed) # Generate random integer random_int = random.randint(int_range[0], int_range[1]) # Select random chest random_chest = random.choice(chests) # Generate health boost health_boost = random.triangular(float_range[0], float_range[1], float_mode) # Generate enemy stats enemy_stats_generated = { \\"strength\\": random.gauss(enemy_stats[\\"strength\\"][0], enemy_stats[\\"strength\\"][1]), \\"agility\\": random.gauss(enemy_stats[\\"agility\\"][0], enemy_stats[\\"agility\\"][1]), \\"intelligence\\": random.gauss(enemy_stats[\\"intelligence\\"][0], enemy_stats[\\"intelligence\\"][1]), } return { \\"random_int\\": random_int, \\"random_chest\\": random_chest, \\"health_boost\\": health_boost, \\"enemy_stats\\": enemy_stats_generated, }"},{"question":"# Pandas Coding Assessment Problem Statement You are provided with a dataset containing student records including their names, subjects they have taken, and their respective grades. You need to implement a function using pandas to generate a summary report for each student, including their average grade per subject and overall average grade. The input dataset is in the form of a pandas `DataFrame` with the following columns: - `name`: Name of the student - `subject`: Subject name - `grade`: Grade obtained in the subject Implement a function `generate_student_report(df)` that takes a pandas DataFrame `df` as input and returns a new DataFrame containing the summary report with the following columns: - `name`: Name of the student - `subject`: Subject name - `average_grade`: Average grade for the subject - `overall_average`: Overall average grade across all subjects for the student Example ```python import pandas as pd data = { \'name\': [\'Alice\', \'Alice\', \'Alice\', \'Bob\', \'Bob\', \'Charlie\', \'Charlie\', \'Charlie\'], \'subject\': [\'Math\', \'Science\', \'English\', \'Math\', \'English\', \'Math\', \'Science\', \'English\'], \'grade\': [85, 78, 92, 88, 76, 95, 80, 85] } df = pd.DataFrame(data) result = generate_student_report(df) print(result) ``` Expected Output ``` name subject average_grade overall_average 0 Alice Math 85.0 85.0 1 Alice Science 78.0 85.0 2 Alice English 92.0 85.0 3 Bob Math 88.0 82.0 4 Bob English 76.0 82.0 5 Charlie Math 95.0 86.666667 6 Charlie Science 80.0 86.666667 7 Charlie English 85.0 86.666667 ``` Constraints 1. Each student will have at least one record in the dataset. 2. Grades are integer values between 0 and 100 inclusive. 3. The DataFrame may contain multiple records for the same student and subject. Function Signature ```python def generate_student_report(df: pd.DataFrame) -> pd.DataFrame: pass ``` Notes - Utilize pandas\' built-in functionality for grouping, aggregating, and merging data. - Ensure to handle any potential division by zero with appropriate safeguards.","solution":"import pandas as pd def generate_student_report(df: pd.DataFrame) -> pd.DataFrame: # Calculate average grade per subject for each student avg_subject = df.groupby([\'name\', \'subject\'], as_index=False)[\'grade\'].mean().rename(columns={\'grade\': \'average_grade\'}) # Calculate overall average grade for each student overall_avg = df.groupby(\'name\')[\'grade\'].mean().reset_index().rename(columns={\'grade\': \'overall_average\'}) # Merge the two dataframes on `name` result_df = pd.merge(avg_subject, overall_avg, on=\'name\') return result_df"},{"question":"<|Analysis Begin|> The provided documentation describes several modules in Python 3.10 used for data compression and archiving. The modules are: 1. **zlib**: Provides compression compatible with gzip. 2. **gzip**: Supports working with gzip files. 3. **bz2**: Provides support for bzip2 compression, including file (de)compression, incremental (de)compression, and one-shot (de)compression. 4. **lzma**: Provides compression using the LZMA algorithm, and allows for reading/writing compressed files, in-memory (de)compression, specifying custom filter chains, and various examples. 5. **zipfile**: Provides functionality for working with ZIP archive files, such as creating, extracting, and handling ZIP files. 6. **tarfile**: Allows reading and writing tar archive files, supporting various tar formats, providing extraction filters, command-line interfaces, and handling Unicode issues. These modules serve a wide range of compression and archiving needs, which implies that a question based on this subject could encompass creating, reading, or manipulating these compressed archive formats. Given this, a challenging question should involve tasks that test both basic operations and deeper understanding, such as handling complex scenarios like incremental decompression, utilizing custom filters, and managing various file formats. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Advanced Archive Management **Objective:** You are to design a Python application that demonstrates your understanding and ability to work with multiple compression and archive formats. This application will include creating, reading, and extracting data using various formats (gzip, bzip2, lzma, tar, and zip). **Task:** 1. **Write a function:** `create_archives(input_data: bytes, output_directory: str, archive_name: str) -> None`. - This function will receive `input_data` as a byte string and will create five types of archives (gzip, bzip2, lzma, tar, and zip) in the specified output directory with the given `archive_name`. 2. **Write a function:** `extract_archives(archive_directory: str, extracted_directory: str) -> None`. - This function will take `archive_directory` containing the archives created in the first function and extract them to `extracted_directory`. Maintain directory structure and file integrity. **Constraints and Requirements:** 1. Your solution should handle potential errors gracefully, with meaningful error messages. 2. Place all extracted files from different formats in uniquely named subdirectories within `extracted_directory` to avoid conflicts. 3. You are expected to use standard libraries only. 4. Optimize for performance and memory usage, particularly when dealing with large datasets. **Input Format:** - `create_archives(input_data: bytes, output_directory: str, archive_name: str)` - `input_data`: Byte string containing the data to be archived. - `output_directory`: Path to the directory where the archives will be created. - `archive_name`: Base name for the archive files (without extension). - `extract_archives(archive_directory: str, extracted_directory: str)` - `archive_directory`: Path to the directory containing the archives to be extracted. - `extracted_directory`: Path to the directory where the extracted files should be stored. **Output Format:** - None, but files should be created and managed in the specified directories. **Example Use Case:** ```python input_data = b\\"Hello, World!\\" output_directory = \\"/path/to/output\\" archive_name = \\"test_archive\\" create_archives(input_data, output_directory, archive_name) archive_directory = \\"/path/to/output\\" extracted_directory = \\"/path/to/extracted\\" extract_archives(archive_directory, extracted_directory) ``` **Expected Directory Structure After Creation:** ``` /path/to/output/ test_archive.gz test_archive.bz2 test_archive.lzma test_archive.tar test_archive.zip ``` **Expected Directory Structure After Extraction:** ``` /path/to/extracted/ extracted_from_gz/ test_archive extracted_from_bz2/ test_archive extracted_from_lzma/ test_archive extracted_from_tar/ [files from tar] extracted_from_zip/ [files from zip] ``` Demonstrate your understanding of file operations, error handling, and performance considerations when managing large files using Python\'s built-in libraries.","solution":"import os import gzip import bz2 import lzma import tarfile import zipfile def create_archives(input_data: bytes, output_directory: str, archive_name: str) -> None: os.makedirs(output_directory, exist_ok=True) # Gzip with gzip.open(os.path.join(output_directory, f\\"{archive_name}.gz\\"), \'wb\') as f: f.write(input_data) # Bzip2 with bz2.open(os.path.join(output_directory, f\\"{archive_name}.bz2\\"), \'wb\') as f: f.write(input_data) # LZMA with lzma.open(os.path.join(output_directory, f\\"{archive_name}.lzma\\"), \'wb\') as f: f.write(input_data) # Tar tar_path = os.path.join(output_directory, f\\"{archive_name}.tar\\") with tarfile.open(tar_path, \'w\') as tar: with open(os.path.join(output_directory, archive_name), \'wb\') as f: f.write(input_data) tar.add(os.path.join(output_directory, archive_name), arcname=archive_name) os.remove(os.path.join(output_directory, archive_name)) # Zip with zipfile.ZipFile(os.path.join(output_directory, f\\"{archive_name}.zip\\"), \'w\') as zipf: with open(os.path.join(output_directory, archive_name), \'wb\') as f: f.write(input_data) zipf.write(os.path.join(output_directory, archive_name), arcname=archive_name) os.remove(os.path.join(output_directory, archive_name)) def extract_archives(archive_directory: str, extracted_directory: str) -> None: os.makedirs(extracted_directory, exist_ok=True) for file_name in os.listdir(archive_directory): file_path = os.path.join(archive_directory, file_name) if file_name.endswith(\'.gz\'): with gzip.open(file_path, \'rb\') as f_in: output_path = os.path.join(extracted_directory, \'extracted_from_gz\', os.path.splitext(file_name)[0]) os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(output_path, \'wb\') as f_out: f_out.write(f_in.read()) elif file_name.endswith(\'.bz2\'): with bz2.open(file_path, \'rb\') as f_in: output_path = os.path.join(extracted_directory, \'extracted_from_bz2\', os.path.splitext(file_name)[0]) os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(output_path, \'wb\') as f_out: f_out.write(f_in.read()) elif file_name.endswith(\'.lzma\'): with lzma.open(file_path, \'rb\') as f_in: output_path = os.path.join(extracted_directory, \'extracted_from_lzma\', os.path.splitext(file_name)[0]) os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(output_path, \'wb\') as f_out: f_out.write(f_in.read()) elif file_name.endswith(\'.tar\'): extract_path = os.path.join(extracted_directory, \'extracted_from_tar\') os.makedirs(extract_path, exist_ok=True) with tarfile.open(file_path, \'r\') as tar: tar.extractall(path=extract_path) elif file_name.endswith(\'.zip\'): extract_path = os.path.join(extracted_directory, \'extracted_from_zip\') os.makedirs(extract_path, exist_ok=True) with zipfile.ZipFile(file_path, \'r\') as zipf: zipf.extractall(path=extract_path)"},{"question":"# Custom Import System Implementation Overview: You are required to create a custom import system that dynamically imports a module (or package) and reloads it if necessary. Your system should utilize the functionalities provided by the `importlib` module, demonstrate understanding of programmatic importing of modules, reloading modules, and customizing import mechanisms using finders and loaders. Task 1: Dynamic Module Import Write a function `dynamic_import(module_name)` that takes the name of a module (as a string) and imports it. If the module is already imported, it should return the existing module from `sys.modules`. If not, it should import the module dynamically using `importlib`. Task 2: Module Reloader Write a function `reload_module(module_name)` that reloads an already imported module. If the module is not found in `sys.modules`, it should raise a `ModuleNotFoundError`. Task 3: Custom Finder and Loader Implement a custom finder and loader that can locate and load a module from a specific directory. The loader should be able to load both source and bytecode files. **Specifications:** 1. **Function: dynamic_import** - **Input:** `module_name` (string) - **Output:** The imported module. - **Constraints:** Handle invalid module names appropriately. 2. **Function: reload_module** - **Input:** `module_name` (string) - **Output:** The reloaded module. - **Constraints:** The module must already be imported. 3. **Class: CustomFinder** - **Method:** `find_spec` - **Input:** `fullname` (string), `path` (optional) - **Output:** A `ModuleSpec` object for loading the module. 4. **Class: CustomLoader** - Inherits from `importlib.abc.Loader` - **Method:** `create_module` - **Input:** `spec` - **Output:** A module object or `None`. - **Method:** `exec_module` - **Input:** `module` - **Output:** None (executes the module code). 5. **Function: setup_custom_importer** - Dynamically insert `CustomFinder` into `sys.meta_path` to handle custom imports. 6. **Constraints:** - Ensure proper error handling and raise appropriate exceptions where necessary. - Use the `importlib.util.spec_from_loader` method to create the `ModuleSpec`. - Implement the `invalidate_caches` method for `CustomFinder` to clear any internal caches. **Example Usage:** ```python # Dynamically import a module imported_module = dynamic_import(\'itertools\') # Reload an already imported module reloaded_module = reload_module(\'itertools\') # Set up the custom importer and import a module using the custom finder and loader setup_custom_importer() custom_imported_module = dynamic_import(\'custom_module\') ``` Submission: - Implement the functions and classes as described. - Ensure your code is well-documented and follows best practices. - Include test cases to demonstrate the functionality of each part of the system.","solution":"import importlib import sys import os from importlib.abc import Loader, MetaPathFinder from importlib.util import spec_from_loader def dynamic_import(module_name): Dynamically import a module. :param module_name: Name of the module to import. :return: The imported module. if module_name in sys.modules: return sys.modules[module_name] else: return importlib.import_module(module_name) def reload_module(module_name): Reload an already imported module. :param module_name: Name of the module to reload. :return: The reloaded module. :raises ModuleNotFoundError: If the module is not found in sys.modules. if module_name in sys.modules: return importlib.reload(sys.modules[module_name]) else: raise ModuleNotFoundError(f\\"No module named \'{module_name}\' found\\") class CustomLoader(Loader): Custom loader to load a module\'s source or bytecode. def create_module(self, spec): Create a module object. If None, a default module is created. return None def exec_module(self, module): Execute the module by reading its source. filename = module.__spec__.origin with open(filename, \'r\') as f: source_code = f.read() exec(source_code, module.__dict__) class CustomFinder(MetaPathFinder): Custom finder to locate modules from a specific directory. def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path=None, target=None): Find the module specification for the given module. module_path = os.path.join(self.directory, fullname) + \'.py\' if os.path.exists(module_path): loader = CustomLoader() return spec_from_loader(fullname, loader, origin=module_path) return None def invalidate_caches(self): Clear any caches used by the finder. pass def setup_custom_importer(directory): Set up a custom importer by adding CustomFinder to sys.meta_path. :param directory: Directory to look for modules. finder = CustomFinder(directory) sys.meta_path.insert(0, finder)"},{"question":"# Objective The task is to create a Python class that leverages the features and functionalities of both instance method objects and method objects, as defined in the provided documentation. The goal is to demonstrate an understanding of Python\'s internal method handling mechanisms and to create a system where these methods are dynamically managed and invoked. # Problem Statement You are tasked with implementing a class `DynamicMethodHandler` with the following specifications: 1. **Functionality to Add Methods**: - The class should have a method `add_method(name: str, func: Callable, is_instance_method: bool = True)` to dynamically add a method to the class. - If `is_instance_method` is set to `True`, the method should be added as an instance method. - If `is_instance_method` is set to `False`, the method should be added as a class method. 2. **Functionality to Retrieve Methods**: - The class should have a method `get_method(name: str)` to retrieve the method with the given name. If the method does not exist, it should return `None`. 3. **Functionality to Invoke Methods**: - The class should have a method `invoke_method(name: str, *args, **kwargs)` to invoke the method with the given name and pass any arguments or keyword arguments to it. If the method does not exist, it should raise a `NameError`. # Input and Output Formats - The `add_method` method accepts: - `name` (str): The name of the method. - `func` (Callable): The function to be added as a method. - `is_instance_method` (bool): A flag indicating if it should be an instance method (default: True). - The `get_method` method accepts: - `name` (str): The name of the method to retrieve. - The `invoke_method` method accepts: - `name` (str): The name of the method to invoke. - `*args`: Positional arguments to pass to the method. - `**kwargs`: Keyword arguments to pass to the method. # Constraints and Limitations - Methods should only be added if they do not already exist. - Retrieval and invocation should respect method visibility constraints (public methods only). # Example Usage ```python # Define some sample functions def sample_instance_method(self): return f\\"Instance method called on {self}\\" def sample_class_method(cls): return f\\"Class method called on {cls}\\" # Create an instance of DynamicMethodHandler handler = DynamicMethodHandler() # Add methods handler.add_method(\'instance_method\', sample_instance_method, is_instance_method=True) handler.add_method(\'class_method\', sample_class_method, is_instance_method=False) # Retrieve and invoke methods instance_method = handler.get_method(\'instance_method\') print(handler.invoke_method(\'instance_method\')) # Expected: \\"Instance method called on <DynamicMethodHandler object>\\" class_method = handler.get_method(\'class_method\') print(handler.invoke_method(\'class_method\')) # Expected: \\"Class method called on <class \'__main__.DynamicMethodHandler\'>\\" ``` # Performance Requirements - The solution should be efficient in adding, retrieving, and invoking methods. Aim for constant time complexity for these operations where feasible.","solution":"import types class DynamicMethodHandler: def add_method(self, name: str, func: callable, is_instance_method: bool = True): if not hasattr(self, name): if is_instance_method: # bind instance method method = types.MethodType(func, self) else: # bind class method method = types.MethodType(func, self.__class__) method = types.MethodType(types.FunctionType(func.__code__, {\'cls\': self.__class__}), self.__class__) setattr(self, name, method) def get_method(self, name: str): return getattr(self, name, None) def invoke_method(self, name: str, *args, **kwargs): method = self.get_method(name) if method: return method(*args, **kwargs) raise NameError(f\\"Method {name} does not exist.\\")"},{"question":"# Python Coding Assessment Question Objective: To assess your understanding of various protocols and operations on Python objects, you are required to implement a function that performs specific operations on sequences using these protocols. Question: Your task is to implement the function `sequence_protocol_operations(sequence: any) -> dict` that performs the following operations using the sequence protocol: 1. **Retrieve and return the first element if the sequence is not empty. If empty, return `None`**. 2. **Retrieve and return the last element if the sequence is not empty. If empty, return `None`**. 3. **Calculate and return the length of the sequence**. 4. **Slice the sequence to obtain every second element starting from the second element and return the new sequence**. 5. **Check if the sequence is reversible (i.e., it can be iterated in reverse order) and return `True` or `False`**. The function should take a single input `sequence`, which can be of any object type that supports the sequence protocol (e.g., list, tuple, string). Input: - `sequence: any` - A sequence object (e.g., list, tuple, string). Output: - `dict` - A dictionary with the following keys: - \'first_element\': The first element of the sequence or `None`. - \'last_element\': The last element of the sequence or `None`. - \'length\': The length of the sequence. - \'sliced\': A new sequence with every second element starting from the second element. - \'is_reversible\': `True` if the sequence can be reversed, `False` otherwise. Constraints: - You may not use Python\'s built-in functions like `len`, `reversed`, and slicing directly. You must use sequence protocol methods where applicable. Example: ```python def sequence_protocol_operations(sequence): # Implementation here # Example usage result = sequence_protocol_operations([1, 2, 3, 4, 5, 6]) print(result) # Output should be: # { # \'first_element\': 1, # \'last_element\': 6, # \'length\': 6, # \'sliced\': [2, 4, 6], # \'is_reversible\': True # } ``` This exercise will test your ability to interact with Python sequences using the appropriate protocols and operations.","solution":"def sequence_protocol_operations(sequence): result = {} # Handling first element retrieval try: result[\'first_element\'] = sequence[0] except (IndexError, TypeError): result[\'first_element\'] = None # Handling last element retrieval try: result[\'last_element\'] = sequence[-1] except (IndexError, TypeError): result[\'last_element\'] = None # Calculating length count = 0 try: for _ in sequence: count += 1 result[\'length\'] = count except TypeError: result[\'length\'] = 0 # Slicing to get every second element starting from the second element sliced_sequence = [] try: for i in range(1, count, 2): sliced_sequence.append(sequence[i]) result[\'sliced\'] = sliced_sequence except TypeError: result[\'sliced\'] = [] # Check if the sequence is reversible try: iterator = iter(sequence) reversed_iterator = iter(reversed(sequence)) result[\'is_reversible\'] = True except TypeError: result[\'is_reversible\'] = False return result"},{"question":"# Seaborn Color Palette Challenge You are required to demonstrate your understanding of the `sns.cubehelix_palette` function available in the Seaborn library. Your task is to generate and visualize a set of color palettes with different parameters using this function, and then explain the differences between them. Task 1: Create and Plot Palettes 1. **Basic Palettes**: - Create a default cubehelix palette with 5 colors. - Create a cubehelix palette with 8 colors. 2. **Advanced Palettes**: - Generate a cubehelix palette with the following configurations: a. Start at `2` and rotate `+0.2`. b. Start at `3` and rotate `-0.3`. c. Apply a gamma correction of `0.5`. 3. **Customized Palettes**: - Create a cubehelix palette with a hue of `1` and luminance range from `0.25` to `0.75`. - Create a reversed cubehelix palette. 4. **Continuous Colormap**: - Generate a continuous colormap (instead of a discrete palette). Task 2: Visualization and Explanation Using the `seaborn` and `matplotlib` libraries, visualize all the palettes created in Task 1 using appropriately labeled plots. Provide a brief explanation (in markdown or comments) for each palette, focusing on how the parameters affect the appearance of the colors. Expected Input and Output - **Input**: No input is provided directly to the script. The parameters for the palettes must be coded manually as described. - **Output**: A series of plots displaying different color palettes and explanations for each. Constraints and Limitations - Ensure that all the required libraries (`seaborn` and `matplotlib`) are imported. - Pay attention to plot labeling for clarity. - Limit the use of other seaborn functions to keep the focus on `sns.cubehelix_palette`. Performance - The solution should be able to run and produce the required plots within a reasonable timeframe on a standard laptop. Example Code (Partial) ```python import seaborn as sns import matplotlib.pyplot as plt # Example: Create a default cubehelix palette palette_default = sns.cubehelix_palette() sns.palplot(palette_default) plt.title(\'Default cubehelix palette\') plt.show() # Continue with other required palettes... ``` Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_and_plot_palettes(): # Basic palletes palette_default_5 = sns.cubehelix_palette(5) palette_default_8 = sns.cubehelix_palette(8) # Advanced palettes palette_start2_rotate02 = sns.cubehelix_palette(start=2, rot=0.2, dark=0, light=1, reverse=False, n_colors=6) palette_start3_rotate_03 = sns.cubehelix_palette(start=3, rot=-0.3, dark=0, light=1, reverse=False, n_colors=6) palette_gamma_05 = sns.cubehelix_palette(gamma=0.5, n_colors=6) # Customized palettes palette_hue1_lumi_025_075 = sns.cubehelix_palette(hue=1, light=0.75, dark=0.25, n_colors=6) palette_reversed = sns.cubehelix_palette(reverse=True, n_colors=6) # Continuous colormap continuous_colormap = sns.cubehelix_palette(as_cmap=True) # Plotting palettes sns.palplot(palette_default_5) plt.title(\'Default cubehelix palette with 5 colors\') plt.show() sns.palplot(palette_default_8) plt.title(\'Default cubehelix palette with 8 colors\') plt.show() sns.palplot(palette_start2_rotate02) plt.title(\'Cubehelix palette with start=2 and rotate=0.2\') plt.show() sns.palplot(palette_start3_rotate_03) plt.title(\'Cubehelix palette with start=3 and rotate=-0.3\') plt.show() sns.palplot(palette_gamma_05) plt.title(\'Cubehelix palette with gamma=0.5\') plt.show() sns.palplot(palette_hue1_lumi_025_075) plt.title(\'Cubehelix palette with hue=1 and luminance range from 0.25 to 0.75\') plt.show() sns.palplot(palette_reversed) plt.title(\'Reversed cubehelix palette\') plt.show() # Continuous colormap plot sns.heatmap([[0, 1], [2, 3]], cmap=continuous_colormap) plt.title(\'Continuous cubehelix colormap\') plt.show() create_and_plot_palettes()"},{"question":"# Garbage Collector Management and Inspection in Python As part of understanding memory management and the garbage collection process in Python, you are required to implement functions using the `gc` module to demonstrate the following capabilities: 1. **Enable or Disable Garbage Collection:** - Write a function `toggle_gc(enable: bool) -> bool` that enables garbage collection if `enable` is `True`, or disables it if `enable` is `False`. The function should return the previous state of the garbage collector (whether it was enabled or disabled). 2. **Collect Statistics on Garbage Collection:** - Implement a function `get_gc_stats() -> dict` that runs a full garbage collection manually and returns a dictionary containing the number of collections, number of objects collected, and number of uncollectable objects for each generation. 3. **Inspect Tracked Objects:** - Write a function `list_tracked_objects(generation: int = None) -> list` that returns a list of all objects currently tracked by the garbage collector. If a generation is specified, it should only include objects from that generation. 4. **Identify and Handle Uncollectable Objects:** - Create a function `handle_uncollectable_objects() -> list` that performs garbage collection with debugging options to print information on uncollectable objects. It should return the list of uncollectable objects found during the collection. # Example Usage: ```python # Enable garbage collection and check previous state previous_state = toggle_gc(True) print(f\\"Garbage collection was {\'enabled\' if previous_state else \'disabled\'}\\") # Collect and print garbage collection statistics stats = get_gc_stats() print(f\\"Garbage Collection Stats: {stats}\\") # List all tracked objects from generation 0 tracked_objects = list_tracked_objects(0) print(f\\"Tracked Objects in Generation 0: {tracked_objects}\\") # Find and handle uncollectable objects uncollectable_objects = handle_uncollectable_objects() print(f\\"Uncollectable Objects: {uncollectable_objects}\\") ``` # Constraints: - You should not use any global variables. - Properly handle any exceptions that might be raised by functions within the `gc` module. - Ensure that your implementation works with Python 3.10. # Expectations: - Your solution should demonstrate a clear understanding of the `gc` module\'s functionalities. - Your functions should be well-documented with docstrings explaining their purpose, inputs, and outputs. - The code should be robust, handling potential edge cases gracefully.","solution":"import gc def toggle_gc(enable: bool) -> bool: Enables or disables the garbage collector. Parameters: enable (bool): If True, enable garbage collection; if False, disable it. Returns: bool: The previous state of the garbage collector (True if enabled, False otherwise). previous_state = gc.isenabled() if enable: gc.enable() else: gc.disable() return previous_state def get_gc_stats() -> dict: Runs a full garbage collection manually and returns statistics. Returns: dict: A dictionary containing statistics for the number of collections, number of objects collected, and number of uncollectable objects for each generation. gc.collect() # Perform a full collection stats = { \'collections\': [gc.get_count()[i] for i in range(3)], \'collected\': [gc.get_stats()[i][\'collected\'] for i in range(3)], \'uncollectable\': [gc.get_stats()[i][\'uncollectable\'] for i in range(3)] } return stats def list_tracked_objects(generation: int = None) -> list: Returns a list of all objects currently tracked by the garbage collector. Parameters: generation (int, optional): If specified, only includes objects from that generation. Returns: list: A list of all objects tracked by the garbage collector, optionally filtered by generation. if generation is not None: return list(gc.get_objects(generation=generation)) return list(gc.get_objects()) def handle_uncollectable_objects() -> list: Performs garbage collection with debugging options active and returns uncollectable objects. Returns: list: A list of uncollectable objects found during the collection. gc.set_debug(gc.DEBUG_UNCOLLECTABLE) gc.collect() # Perform a garbage collection uncollectable = gc.garbage[:] gc.garbage.clear() # Clear the list of uncollectable objects return uncollectable"},{"question":"# Question: AST Tree Manipulation and Analysis Using the `ast` module, write a function `find_variable_assignments` that takes a string of Python code as input and returns a dictionary mapping variable names to the line numbers where they are assigned a value. The function should parse the input code into an AST (Abstract Syntax Tree) and traverse the tree to identify assignments. The keys of the returned dictionary should be the variable names (as strings) and the values should be lists of line numbers (integers) where each variable is assigned. Function Signature ```python def find_variable_assignments(code: str) -> dict: ``` Input - `code` (str): A string containing valid Python code. Output - A dictionary where the keys are variable names (str) and values are lists of line numbers (int) where the variables are assigned. Example ```python code = x = 10 y = 20 x = x + 5 z = y + x result = find_variable_assignments(code) print(result) # Output should be {\'x\': [2, 4], \'y\': [3], \'z\': [5]} ``` Constraints - The code input will be a valid Python code string. - You are required to handle single assignment operations (`=`). - You do not need to handle compound assignments (e.g., `+=`, `-=`), multiple assignments on the same line, or assignments within other statements (e.g., within loops or control statements). Hints - Utilize the `ast.parse` function to convert the code string into an AST. - You may find the `ast.Assign` node helpful for identifying assignment operations. - The line number for a node can be accessed using `node.lineno`. Performance Requirements - The function should be efficient enough to handle code of up to 1000 lines. Good luck!","solution":"import ast def find_variable_assignments(code: str) -> dict: Takes a string of Python code as input and returns a dictionary mapping variable names to the line numbers where they are assigned a value. tree = ast.parse(code) result = {} for node in ast.walk(tree): if isinstance(node, ast.Assign): for target in node.targets: if isinstance(target, ast.Name): if target.id not in result: result[target.id] = [] result[target.id].append(node.lineno) return result"},{"question":"<|Analysis Begin|> The `threading` module in Python provides a higher-level threading interface for running multiple threads (tasks) simultaneously. It includes several important classes and methods that help manage and control threads, locks, conditions, semaphores, events, and barriers. Here is a detailed analysis of the provided documentation\'s contents: 1. **Thread Management**: - **Thread Creation**: - `threading.Thread`: A class to create and manage threads. It can be initialized with a target function, arguments, and other properties like name and daemon status. - Methods: `start()`, `run()`, `join(timeout=None)`. - **Thread Identification**: - Functions: `threading.current_thread()`, `threading.get_ident()`, `threading.get_native_id()`. - Properties: `name`, `ident`, `native_id`. 2. **Thread Lifecycle**: - **Thread States**: - Methods: `is_alive()`, `daemon`. - **Exception Handling**: - Functions: `threading.excepthook(args)`, `threading.__excepthook__`. 3. **Synchronization Primitives**: - **Locks & Reentrant Locks**: - Classes: `threading.Lock`, `threading.RLock`. - Methods: `acquire(blocking=True, timeout=-1)`, `release()`, `locked()`. - **Conditions**: - Class: `threading.Condition`. - Methods: `acquire(*args)`, `release()`, `wait(timeout=None)`, `wait_for(predicate, timeout=None)`, `notify(n=1)`, `notify_all()`. - **Semaphores**: - Classes: `threading.Semaphore`, `threading.BoundedSemaphore`. - Methods: `acquire(blocking=True, timeout=None)`, `release(n=1)`. - **Events**: - Class: `threading.Event`. - Methods: `is_set()`, `set()`, `clear()`, `wait(timeout=None)`. 4. **Timer**: - Class: `threading.Timer`. - Methods: `cancel()`. 5. **Barrier**: - Class: `threading.Barrier`. - Methods: `wait(timeout=None)`, `reset()`, `abort()`. - Exception: `threading.BrokenBarrierError`. 6. **Additional Functions**: - `threading.active_count()`, `threading.enumerate()`, `threading.main_thread()`, `threading.settrace(func)`, `threading.gettrace()`, `threading.setprofile(func)`, `threading.getprofile()`, `threading.stack_size([size])`. Given the complexity and breadth of the module, an assessment question can address various aspects, such as creating and managing threads, synchronizing threads using different mechanisms, and handling conditions and semaphores. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: This question tests your ability to create and manage thread-based tasks, as well as utilize synchronization primitives effectively to ensure proper interactions between threads. # Question You are tasked with implementing a simple simulation of a bank account system where multiple threads can deposit to and withdraw from a shared bank account. Ensure thread safety using the `threading` module. Requirements: 1. Implement a `BankAccount` class with the following specifications: - A private attribute, `balance`, representing the current balance of the account. - A method, `deposit(amount)`, which allows depositing a given amount into the account. - A method, `withdraw(amount)`, which allows withdrawing a given amount from the account if sufficient balance is available. - A method, `get_balance()`, which returns the current balance of the account. 2. Implement thread safety for the `deposit` and `withdraw` methods using an appropriate synchronization mechanism. 3. Write a function `simulate_transactions(account, transactions)` which takes a `BankAccount` instance and a list of transactions. Each transaction is a tuple of the form `(operation, amount)`, where `operation` is a string (`\'deposit\'` or `\'withdraw\'`) and `amount` is a positive integer. This function should create and start a thread for each transaction, ensure thread-safety during transaction processing, and finally return the account balance after all transactions are processed. Input Format: - `transactions`: A list of tuples, where each tuple is of the form `(operation: str, amount: int)`. Output Format: - An integer representing the final balance of the bank account after processing all transactions. Constraints: - The initial balance of the `BankAccount` is zero. - Transactions must be processed concurrently. - You may assume the transaction amounts are always positive integers. Example: ```python transactions = [(\'deposit\', 100), (\'withdraw\', 50), (\'deposit\', 200), (\'withdraw\', 150)] account = BankAccount() final_balance = simulate_transactions(account, transactions) print(final_balance) # Expected output: 100 ``` Implementation Guidelines: - Use the `threading` module to manage threads. - Ensure proper synchronization using `Lock` or other suitable synchronization primitives. - Handle scenarios where a withdrawal might fail due to insufficient balance gracefully.","solution":"import threading class BankAccount: def __init__(self): self.balance = 0 self.lock = threading.Lock() def deposit(self, amount): with self.lock: self.balance += amount def withdraw(self, amount): with self.lock: if self.balance >= amount: self.balance -= amount return True else: return False def get_balance(self): with self.lock: return self.balance def simulate_transactions(account, transactions): threads = [] def process_transaction(operation, amount): if operation == \'deposit\': account.deposit(amount) elif operation == \'withdraw\': account.withdraw(amount) for operation, amount in transactions: thread = threading.Thread(target=process_transaction, args=(operation, amount)) threads.append(thread) thread.start() for thread in threads: thread.join() return account.get_balance()"},{"question":"# Custom Event Loop Policy Implementation **Objective**: Demonstrate understanding of asyncio event loop policies and custom event loop policy creation in Python. **Problem Statement**: You are tasked with creating a custom event loop policy by extending the `DefaultEventLoopPolicy`. This custom policy should keep track of the number of times an event loop is created and retrieved. This information should be logged for analytical purposes. 1. Create a class `CustomEventLoopPolicy` that extends `asyncio.DefaultEventLoopPolicy`. 2. Override the `get_event_loop` and `new_event_loop` methods to keep track of the number of times they are called. 3. Implement a method `get_loop_statistics` within `CustomEventLoopPolicy` that returns a dictionary containing: - `total_loops_created`: Number of times new event loops were created. - `total_loops_retrieved`: Number of times event loops were retrieved using `get_event_loop`. 4. Demonstrate setting and using the custom policy in a script that: - Sets the custom event loop policy. - Creates and retrieves event loops. - Prints out the loop statistics. **Constraints**: - Make sure the event loops are created and retrieved in different threads to verify that the policy correctly tracks per-thread context. - Use logging to record each creation and retrieval action. - The solution should be compatible with Python 3.8 and above. **Example**: The following shows how the `CustomEventLoopPolicy` class should be used: ```python import asyncio import logging # Your CustomEventLoopPolicy class implementation here # Set up logging logging.basicConfig(level=logging.INFO) # Set the custom event loop policy asyncio.set_event_loop_policy(CustomEventLoopPolicy()) def create_and_retrieve_loop(): loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) asyncio.get_event_loop() loop.close() # Simulate creating and retrieving event loops in different threads import threading threads = [threading.Thread(target=create_and_retrieve_loop) for _ in range(5)] for t in threads: t.start() for t in threads: t.join() # Get loop statistics policy = asyncio.get_event_loop_policy() loop_statistics = policy.get_loop_statistics() print(loop_statistics) ``` **Expected Output**: A dictionary containing the total loops created and retrieved, similar to: ```python { \'total_loops_created\': 5, \'total_loops_retrieved\': 5 } ``` **Evaluation Criteria**: 1. Correct implementation of `CustomEventLoopPolicy` class. 2. Proper overriding of event loop methods. 3. Accurate tracking of the number of event loop creations and retrievals. 4. Use of logging for tracking actions. 5. Correct demonstration of setting and using the custom policy in different threads.","solution":"import asyncio import threading import logging # Set up logging logging.basicConfig(level=logging.INFO) class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self): super().__init__() self._total_loops_created = 0 self._total_loops_retrieved = 0 def new_event_loop(self): self._total_loops_created += 1 logging.info(f\\"Event loop created. Total created: {self._total_loops_created}\\") return super().new_event_loop() def get_event_loop(self): self._total_loops_retrieved += 1 logging.info(f\\"Event loop retrieved. Total retrieved: {self._total_loops_retrieved}\\") return super().get_event_loop() def get_loop_statistics(self): return { \'total_loops_created\': self._total_loops_created, \'total_loops_retrieved\': self._total_loops_retrieved } asyncio.set_event_loop_policy(CustomEventLoopPolicy()) def create_and_retrieve_loop(): loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) asyncio.get_event_loop() loop.close() threads = [threading.Thread(target=create_and_retrieve_loop) for _ in range(5)] for t in threads: t.start() for t in threads: t.join() policy = asyncio.get_event_loop_policy() loop_statistics = policy.get_loop_statistics() print(loop_statistics)"},{"question":"Title: Dynamic Model Architecture with `torch.cond` **Objective:** In this task, you will implement a PyTorch module using the `torch.cond` operator that demonstrates an adaptive transformation based on the characteristics of input tensors. You will create a neural network module that dynamically switches its behavior based on both the input tensor\'s shape and content. **Task Description:** 1. Implement a PyTorch neural network module `DynamicCondModule` that follows the structure provided: 2. The module should take an input tensor and apply different transformations based on two conditions: * If the number of elements in the input tensor is greater than 10, apply the `element_gt_10_fn` which should return the tensor with each element squared. * If the sum of the tensor\'s elements is greater than 4.0, apply the `sum_gt_4_fn` which should return the tensor with each element doubled. * Otherwise, return the tensor unmodified. **Requirements:** 1. Define a `DynamicCondModule` class that inherits from `torch.nn.Module`. 2. The module should implement the `forward` method which uses `torch.cond` for control flow. 3. Ensure that both transformations (`element_gt_10_fn` and `sum_gt_4_fn`) correctly process the tensor within the conditions specified. 4. Write a script to test the module with at least two different input tensors to demonstrate both conditions being met or not. **Specifications:** * **Input:** A single tensor `x` of arbitrary shape and containing float values. * **Output:** A tensor processed based on the conditions. **Example usage:** ```python import torch class DynamicCondModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def element_gt_10_fn(x: torch.Tensor): return x ** 2 def sum_gt_4_fn(x: torch.Tensor): return x * 2 cond_result = torch.cond( x.numel() > 10, lambda: element_gt_10_fn(x), lambda: torch.cond(x.sum() > 4.0, lambda: sum_gt_4_fn(x), lambda: x) ) return cond_result # Instantiate the module model = DynamicCondModule() # Test cases input_tensor1 = torch.randn(3, 3) # Example where number of elements <= 10 and sum <= 4.0 output1 = model(input_tensor1) print(\\"Output 1:\\", output1) input_tensor2 = torch.randn(5, 4) # Example where number of elements > 10 output2 = model(input_tensor2) print(\\"Output 2:\\", output2) ``` **Constraints & Considerations:** 1. Ensure no mutations to global or local state within the functions used in `torch.cond`. 2. The module should handle tensors with varying shapes and sizes efficiently. 3. Test your implementation thoroughly to ensure it works for different input scenarios. **Submission:** Submit the `DynamicCondModule` class implementation along with a script showing the tested output for different input conditions.","solution":"import torch class DynamicCondModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def element_gt_10_fn(x: torch.Tensor): return x ** 2 def sum_gt_4_fn(x: torch.Tensor): return x * 2 if x.numel() > 10: return element_gt_10_fn(x) elif x.sum() > 4.0: return sum_gt_4_fn(x) else: return x"},{"question":"# Python Coding Assessment: Bzip2 Compression and Decompression Objective: In this task, you will be required to demonstrate your understanding of the `bz2` module by implementing functions for both single-shot and incremental compression/decompression. You will also need to read from and write to bzip2-compressed files. Instructions: 1. **Function for Single-shot Compression and Decompression:** - Implement the function `compress_and_decompress(data: bytes, compresslevel: int = 9) -> bool`: - `data`: A bytes-like object to be compressed and then decompressed. - `compresslevel`: An integer between 1 and 9 indicating the level of compression. The default is 9. - This function should: - Compress the input data using `bz2.compress()` with the specified compression level. - Decompress the compressed data using `bz2.decompress()`. - Return `True` if the decompressed data matches the original `data`; otherwise, return `False`. 2. **Function for Incremental Compression:** - Implement the function `incremental_compress(data_chunks: list[bytes], compresslevel: int = 9) -> bytes`: - `data_chunks`: A list of bytes-like objects to be incrementally compressed. - `compresslevel`: An integer between 1 and 9 indicating the level of compression. The default is 9. - This function should: - Initialize a `bz2.BZ2Compressor` object. - Use the `compress()` method to incrementally compress each chunk in `data_chunks`. - Call the `flush()` method to complete the compression process. - Return the final compressed data as a single bytes object. 3. **Function for Reading and Writing Bzip2-Compressed Files:** - Implement the function `file_roundtrip(filename: str, data: bytes) -> bool`: - `filename`: The name of the file to write to and read from. - `data`: A bytes-like object to be written to the compressed file and then read back. - This function should: - Use `bz2.open()` to write the input data to a bzip2-compressed file. - Use `bz2.open()` to read the data back from the bzip2-compressed file. - Return `True` if the data read from the file matches the original `data`; otherwise, return `False`. Constraints: - You must use the `bz2` module for compression and decompression. - You are not allowed to use any other compression module for this task. Example Usage: ```python data = b\\"Sample data for testing bzip2 compression\\" filename = \\"testfile.bz2\\" chunks = [data[i:i+10] for i in range(0, len(data), 10)] # Single-shot compression and decompression assert compress_and_decompress(data) == True # Incremental compression compressed_data = incremental_compress(chunks) assert isinstance(compressed_data, bytes) # Reading and writing bzip2-compressed files assert file_roundtrip(filename, data) == True ``` Ensure that your functions handle edge cases appropriately and provide sufficient documentation within your code.","solution":"import bz2 def compress_and_decompress(data: bytes, compresslevel: int = 9) -> bool: Compresses the input data using bz2 with the specified compression level, then decompresses it to verify it matches the original data. :param data: A bytes-like object to be compressed and then decompressed. :param compresslevel: An integer between 1 and 9 indicating the level of compression. :return: True if the decompressed data matches the original data, otherwise False. compressed_data = bz2.compress(data, compresslevel) decompressed_data = bz2.decompress(compressed_data) return decompressed_data == data def incremental_compress(data_chunks: list[bytes], compresslevel: int = 9) -> bytes: Incrementally compresses a list of bytes-like objects using bz2 with the specified compression level. :param data_chunks: A list of bytes-like objects to be incrementally compressed. :param compresslevel: An integer between 1 and 9 indicating the level of compression. :return: The final compressed data as a single bytes object. compressor = bz2.BZ2Compressor(compresslevel) compressed_data = b\'\' for chunk in data_chunks: compressed_data += compressor.compress(chunk) compressed_data += compressor.flush() return compressed_data def file_roundtrip(filename: str, data: bytes) -> bool: Writes the input data to a bzip2 compressed file and reads it back to verify it matches the original data. :param filename: The name of the file to write to and read from. :param data: A bytes-like object to be written to the compressed file and then read back. :return: True if the data read from the file matches the original data, otherwise False. with bz2.open(filename, \'wb\') as file: file.write(data) with bz2.open(filename, \'rb\') as file: read_data = file.read() return read_data == data"},{"question":"**Question:** You are provided with a Python script that processes a list of URLs by downloading their content asynchronously and saving it to files. You need to enhance the script by ensuring it is compliant with Python Development Mode. Your task is to identify potential issues that would raise warnings or errors when the script is executed in Python Development Mode, and refactor the code to handle those issues appropriately. Here is the initial script: ```python import aiohttp import asyncio import os urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] async def fetch_url(session, url): async with session.get(url) as response: content = await response.text() filename = url.replace(\'http://\', \'\').replace(\'.\', \'_\') + \'.txt\' with open(filename, \'w\') as fp: fp.write(content) async def main(): async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] await asyncio.gather(*tasks) if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Tasks: 1. **Identify Problems**: Detail each specific issue in the given script that may trigger warnings or errors in Development Mode. 2. **Refactor Code**: Refactor the script to resolve these issues, ensuring the code runs without warnings or errors when executed with the `-X dev` command line option. # Constraints: - You must handle all resources properly to avoid `ResourceWarning`. - Ensure all coroutines are awaited appropriately to prevent issues in `asyncio` debug mode. - Do not assume any specific output formats or contents for the URLs. # Expected Output: - A detailed explanation of each identified problem and how it is addressed. - A revised version of the script that runs cleanly in Python Development Mode. # Performance Requirements: - The refactored script should maintain asynchronous execution efficiency. - Ensure that the script still handles multiple URLs concurrently. ```python # Write your analysis and refactored code below: ```","solution":"import aiohttp import asyncio import os urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] async def fetch_url(session, url): async with session.get(url) as response: content = await response.text() filename = url.replace(\'http://\', \'\').replace(\'.\', \'_\') + \'.txt\' with open(filename, \'w\', encoding=\'utf-8\') as fp: fp.write(content) async def main(): async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] await asyncio.gather(*tasks, return_exceptions=True) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective Implement an out-of-core learning system that uses scikit-learn to classify text documents. Your solution should demonstrate the ability to stream instances, perform feature extraction, and train an incremental learning model. Task Write a Python function `out_of_core_text_classification` which performs text classification using out-of-core learning. The function should: 1. Stream text instances from a specified file. 2. Extract features using `HashingVectorizer`. 3. Train a classifier incrementally using `SGDClassifier`. Input - `file_path` (str): Path to a text file containing the dataset, where each line represents a document followed by a tab character and then the class label. - `batch_size` (int): Number of documents to process in each batch. Output - A trained `SGDClassifier` model. Constraints 1. The function should handle large datasets that do not fit into memory. 2. Ensure the dataset is processed in mini-batches as specified by `batch_size`. Example Usage ```python file_path = \'path/to/large_text_dataset.txt\' batch_size = 100 model = out_of_core_text_classification(file_path, batch_size) ``` Function Signature ```python def out_of_core_text_classification(file_path: str, batch_size: int) -> \'SGDClassifier\': pass ``` Notes - You may assume that the class labels contain only two classes: `0` and `1`. - Use the `SGDClassifier` with the `partial_fit` method. - Use `HashingVectorizer` for feature extraction. - Use `sklearn.datasets` and `sklearn.model_selection` for splitting data and other utilities if needed. - Implement error handling for file operations. Additional Information - Documentation about `SGDClassifier`: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html - Documentation about `HashingVectorizer`: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html","solution":"import os from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer def out_of_core_text_classification(file_path: str, batch_size: int) -> SGDClassifier: # Initialize the HashingVectorizer and SGDClassifier vectorizer = HashingVectorizer(alternate_sign=False) classifier = SGDClassifier(loss=\'hinge\') # Initialize variables for processing the file X_batch = [] y_batch = [] # Count total examples processed total_examples = 0 with open(file_path, \'r\') as file: for line in file: parts = line.strip().split(\'t\') if len(parts) < 2: continue # Skip invalid lines text, label = parts[0], int(parts[1]) X_batch.append(text) y_batch.append(label) # Process in batches if len(X_batch) >= batch_size: X_features = vectorizer.transform(X_batch) if total_examples == 0: # First batch, we need to initialize partial_fit with classes classifier.partial_fit(X_features, y_batch, classes=[0, 1]) else: classifier.partial_fit(X_features, y_batch) # Clear batches X_batch, y_batch = [], [] total_examples += batch_size # Check if there is a remaining batch to process if X_batch: X_features = vectorizer.transform(X_batch) classifier.partial_fit(X_features, y_batch) return classifier"},{"question":"Coding Assessment Question # Objective This question aims to evaluate your understanding of PyTorchs CUDA streams and your ability to handle synchronization issues using the CUDA Stream Sanitizer. # Problem Statement You are given two functions that perform operations on tensors: one generates random tensors on the CUDA device, and the other performs a mathematical operation on those tensors using different CUDA streams. Your task is to identify and fix any synchronization issues using the CUDA Stream Sanitizer. # Functions 1. **generate_tensor**: ```python def generate_tensor(shape): Generates a random tensor of a given shape on the CUDA device. Args: shape (tuple): Shape of the tensor to be generated. Returns: torch.Tensor: Randomly generated tensor on the CUDA device. import torch return torch.rand(shape, device=\\"cuda\\") ``` 2. **perform_operation**: ```python def perform_operation(tensor): Multiplies the tensor by 5 on a new CUDA stream without proper synchronization. Args: tensor (torch.Tensor): Input tensor. Returns: torch.Tensor: Tensor after multiplication. import torch with torch.cuda.stream(torch.cuda.Stream()): torch.mul(tensor, 5, out=tensor) return tensor ``` # Task 1. Modify the `perform_operation` function to ensure proper synchronization between the default stream and the new stream. 2. Test the modified function using the CUDA Stream Sanitizer to ensure there are no synchronization issues. # Constraints - You must use the CUDA Stream Sanitizer to detect and fix the synchronization issues. - The tensor operations should remain on CUDA. # Input and Output - **Input**: - A tensor shape as a tuple (for example, `(4, 2)`). - **Output**: - A tensor after performing the operations, with proper synchronization ensured. # Example ```python # Example usage of the functions, where your modifications should ensure correct synchronization: if __name__ == \\"__main__\\": tensor = generate_tensor((4, 2)) result = perform_operation(tensor) print(result) ``` **Given the above functions, demonstrate how you would modify the `perform_operation` function and ensure it runs without synchronization errors when checked by the CUDA Stream Sanitizer.**","solution":"def generate_tensor(shape): Generates a random tensor of a given shape on the CUDA device. Args: shape (tuple): Shape of the tensor to be generated. Returns: torch.Tensor: Randomly generated tensor on the CUDA device. import torch return torch.rand(shape, device=\\"cuda\\") def perform_operation(tensor): Multiplies the tensor by 5 on a new CUDA stream with proper synchronization. Args: tensor (torch.Tensor): Input tensor. Returns: torch.Tensor: Tensor after multiplication. import torch stream = torch.cuda.Stream() with torch.cuda.stream(stream): torch.mul(tensor, 5, out=tensor) # Synchronize with the default stream stream.synchronize() return tensor"},{"question":"**Coding Assessment Question:** # Objective: Implement a Python function that connects to a POP3 server using the `poplib` module, authenticates a user, retrieves the top 10 newest emails (if available), and returns their headers and the first 3 lines from each. # Instructions: 1. Define a function `retrieve_emails` that takes three parameters: - `host` (str): The hostname of the POP3 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. 2. The function should connect to the given POP3 server and log in using the provided credentials. If the connection or authentication fails, it should raise an appropriate exception. 3. Retrieve the top 10 newest emails (or fewer if less than 10 emails are available) from the server. 4. For each email, fetch the header and the first 3 lines of the body without setting the seen flag. 5. Return a list of tuples where each tuple contains the email headers and the first 3 lines of each email as strings. # Input: - `host` (str): The hostname of the POP3 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. # Output: - A list of tuples, where each tuple contains two strings: - The header of the email. - The first 3 lines of the email. # Example: ```python emails = retrieve_emails(\'pop.example.com\', \'user\', \'pass\') for header, first_lines in emails: print(\\"Header:\\", header) print(\\"First 3 lines:\\", first_lines) ``` # Constraints: - Use `poplib.POP3` for this task. - Handle exceptions using appropriate error handling techniques. # Notes: - Consider edge cases where the server might return fewer than 10 emails. - Ensure to log out and close the connection properly after operations. # Evaluation Criteria: - Correctness of the connection and authentication process. - Proper handling of exceptions and edge cases. - Accurate retrieval and formatting of email headers and lines.","solution":"import poplib from email.parser import BytesParser from email.policy import default def retrieve_emails(host, username, password): try: # Connect to the POP3 server mail = poplib.POP3(host) # Authenticate mail.user(username) mail.pass_(password) # Get the number of messages num_messages = len(mail.list()[1]) # Retrieve the top 10 newest messages (or fewer if less than 10) num_to_retrieve = min(10, num_messages) emails = [] for i in range(num_messages, num_messages - num_to_retrieve, -1): # Retrieve the raw email lines response, lines, octets = mail.top(i, 3) response_header, msg_lines, octets_header = mail.retr(i) # Parse the email msg = BytesParser(policy=default).parsebytes(b\'n\'.join(lines)) # Extract the headers and the first 3 lines header = str(msg) first_3_lines = b\'n\'.join(lines).decode(\'utf-8\') emails.append((header, first_3_lines)) # Close the connection mail.quit() return emails except Exception as e: raise RuntimeError(f\\"Failed to retrieve emails: {e}\\")"},{"question":"**Question: Implement Terminal Behavior Customization** You are required to implement a Python function named `customize_terminal_behavior` that reads user input with customized terminal settings. The goal is to disable both echoing and canonical mode (i.e., where input is only made available after a newline character is received), read a user input string, and then restore the terminal to its previous state. # Function Signature ```python import termios, sys def customize_terminal_behavior(prompt: str = \\"Input: \\") -> str: pass ``` # Expected Input - `prompt`: A string to display as a prompt for input (default is `\\"Input: \\"`). This prompt should be displayed when requesting input from the user. # Expected Output - The function should return the string input provided by the user while the terminal was in the customized state. # Constraints - You must handle terminal attributes safely and ensure the original attributes are restored even if an exception occurs. - The function should handle any potential errors gracefully and ensure the terminal state is not left in a corrupted state. - You should disable both echoing (hides the input characters) and canonical mode (makes input available immediately). # Example ```python result = customize_terminal_behavior(\\"Type your secret: \\") print(f\\"You entered: {result}\\") ``` # Notes 1. Use `termios.tcgetattr` and `termios.tcsetattr` to modify terminal attributes. 2. Make sure to restore the original terminal attributes in a `finally` block to ensure it is always executed. # Solution Template ```python import termios, sys def customize_terminal_behavior(prompt: str = \\"Input: \\") -> str: fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) new_settings = termios.tcgetattr(fd) try: # Disabling echoing and canonical mode new_settings[3] = new_settings[3] & ~termios.ECHO & ~termios.ICANON termios.tcsetattr(fd, termios.TCSADRAIN, new_settings) # Reading input from the user user_input = input(prompt) finally: # Restoring the original terminal attributes termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) return user_input # Test the function if __name__ == \\"__main__\\": result = customize_terminal_behavior(\\"Type your input: \\") print(f\\"You entered: {result}\\") ```","solution":"import termios, sys def customize_terminal_behavior(prompt: str = \\"Input: \\") -> str: fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) new_settings = termios.tcgetattr(fd) try: # Disabling echoing and canonical mode new_settings[3] = new_settings[3] & ~termios.ECHO & ~termios.ICANON termios.tcsetattr(fd, termios.TCSADRAIN, new_settings) # Writing prompt to stdout and reading input from the user sys.stdout.write(prompt) sys.stdout.flush() user_input = sys.stdin.read(1024).strip() finally: # Restoring the original terminal attributes termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) return user_input"},{"question":"You are tasked with visualizing data from the Titanic dataset using boxplots in Seaborn. You must demonstrate your understanding of grouping, customization, and specific enhancements by following the steps and constraints provided. Task Requirements 1. **Load the Titanic dataset** using Seaborn\'s `load_dataset` function. 2. **Create a horizontal boxplot** for the \\"fare\\" column of the dataset. 3. **Group the boxplot by the \\"class\\" column**. 4. **Add nested grouping** using the \\"sex\\" column to differentiate between male and female passengers within each class. 5. **Customize the boxplot**: - Draw the boxes as line art with a small gap between them. - Set the whiskers to cover the full range of the data. - Draw narrower boxes. - Set the color of the boxes to a specific value. - Adjust the linewidth and line color of all line artists. Provide the rendered boxplot as the output of your function. Function Signature ```python def custom_boxplot(): import seaborn as sns import matplotlib.pyplot as plt # Write your code here plt.show() ``` Expected Output - A single figure displaying a customized nested grouped boxplot with the specified customizations applied. Additional Information - Ensure your function is well-documented with comments explaining each step. - If using additional packages, ensure they are imported within the function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_boxplot(): This function creates a customized horizontal boxplot for the \\"fare\\" column of the Titanic dataset, grouped by the \\"class\\" column and nested by the \\"sex\\" column. Several customizations are applied to the appearance of the boxplot. # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create a figure and axis plt.figure(figsize=(12, 8)) # Create a horizontal boxplot # Group by \\"class\\" and nest based on \\"sex\\" sns.boxplot( x=\\"fare\\", y=\\"class\\", hue=\\"sex\\", data=titanic, dodge=True, # Separate distributions for \\"sex\\" fliersize=5, # Size of the outlier points linewidth=2, # Width of the outlines width=0.6, # Narrower boxes palette=\\"coolwarm\\" # Color of the boxes ) # Customize the box elements for patch in plt.gca().artists: # Set edge color and width patch.set_edgecolor(\\"black\\") patch.set_linewidth(2) # Adjust the whiskers to cover the full range of the data for line in plt.gca().lines: line.set_linestyle(\\"--\\") # Set title and labels plt.title(\\"Fare Distribution by Class and Sex on the Titanic\\") plt.xlabel(\\"Fare\\") plt.ylabel(\\"Class\\") # Display the plot plt.show()"},{"question":"**Question: Boolean Wrapper Functions** In this exercise, you will create a set of wrapper functions for working with Boolean values based on the provided Python310 documentation. Your task is to implement the following functions: 1. **`is_boolean(obj)`**: - **Input**: A single argument `obj` of any type. - **Output**: Return `True` if the object is of Boolean type (`PyBool_Type`), otherwise `False`. - **Example**: ```python print(is_boolean(True)) # Output: True print(is_boolean(1)) # Output: False ``` 2. **`get_boolean(value)`**: - **Input**: A single argument `value` of type `int`. - **Output**: Return the Boolean equivalent of the integer value using `PyBool_FromLong`. - **Example**: ```python print(get_boolean(0)) # Output: False print(get_boolean(3)) # Output: True ``` 3. **`return_false()`**: - **Input**: No arguments. - **Output**: Return the `Py_False` object using the `Py_RETURN_FALSE` macro. - **Example**: ```python print(return_false()) # Output: False ``` 4. **`return_true()`**: - **Input**: No arguments. - **Output**: Return the `Py_True` object using the `Py_RETURN_TRUE` macro. - **Example**: ```python print(return_true()) # Output: True ``` **Function Definitions**: ```python def is_boolean(obj): # Implement the function that checks if obj is a Boolean type pass def get_boolean(value): # Implement the function that returns Boolean value for the integer input. pass def return_false(): # Implement the function that returns the False Boolean object. pass def return_true(): # Implement the function that returns the True Boolean object. pass ``` **Constraints**: - Do not use Python\'s built-in `bool` function. - You should rely on the implementations and behaviors specified in the provided documentation. **Notes**: - You can assume the inputs provided to the functions will be valid as per the defined input types. - Pay attention to handling reference counts if you decide to implement logic that involves object creation/returns in a custom manner.","solution":"def is_boolean(obj): Returns True if the object is of Boolean type, otherwise False. return isinstance(obj, bool) def get_boolean(value): Returns the Boolean equivalent of the integer value. return value != 0 def return_false(): Returns the False boolean value. return False def return_true(): Returns the True boolean value. return True"},{"question":"Nullable Integer Data Type Manipulation You are given a dataset representing user interactions with a web application, stored in a CSV file named `user_interactions.csv`. The dataset includes columns such as \'user_id\', \'session_duration\', and \'pages_visited\', where some values might be missing. Your task is to perform data preprocessing and analysis using pandas\' nullable integer data types. Here are the specific steps you need to implement: 1. **Load the Data**: Load the CSV file into a pandas DataFrame. 2. **Convert Columns to Nullable Integers**: - Convert the \'session_duration\' and \'pages_visited\' columns to use nullable integers. Ensure to replace any missing values with `pandas.NA`. 3. **Calculate Average Session Duration**: - Compute the average session duration for all users. Handle missing values appropriately using pandas\' built-in methods. 4. **Count Users with Missing Pages Visited**: - Count the number of users who have missing values in the \'pages_visited\' column. 5. **Fill Missing Values and Export**: - Fill missing \'session_duration\' values with the mean session duration. - Fill missing \'pages_visited\' values with the median pages visited. - Export the modified DataFrame to a new CSV file named `processed_user_interactions.csv`. Input The input CSV file `user_interactions.csv` is structured as follows: ``` user_id,session_duration,pages_visited 1,30,3 2,,6 3,20, 4,,2 ... ``` Output - A new CSV file named `processed_user_interactions.csv` with the processed data. Constraints - Use pandas\' nullable integer type (`Int64`) for missing integer values. - Do not use any Python loops; utilize pandas\' vectorized operations for performance. Implementation You need to implement the following function: ```python import pandas as pd def preprocess_user_interactions(input_file: str, output_file: str) -> None: # Load the CSV file into a pandas DataFrame df = pd.read_csv(input_file) # Convert \'session_duration\' and \'pages_visited\' to nullable integer types df[\'session_duration\'] = pd.array(df[\'session_duration\'].astype(\\"Int64\\")) df[\'pages_visited\'] = pd.array(df[\'pages_visited\'].astype(\\"Int64\\")) # Calculate the average session duration avg_session_duration = df[\'session_duration\'].mean() # Count users with missing \'pages_visited\' missing_pages_visited_count = df[\'pages_visited\'].isna().sum() # Fill missing values df[\'session_duration\'] = df[\'session_duration\'].fillna(avg_session_duration) median_pages_visited = df[\'pages_visited\'].median() df[\'pages_visited\'] = df[\'pages_visited\'].fillna(median_pages_visited) # Export the modified DataFrame to a new CSV file df.to_csv(output_file, index=False) ``` Example Usage ```python preprocess_user_interactions(\'user_interactions.csv\', \'processed_user_interactions.csv\') ``` This function will process the provided CSV file, handle the necessary data conversions and missing values, and export the results to a new CSV file.","solution":"import pandas as pd def preprocess_user_interactions(input_file: str, output_file: str) -> None: # Load the CSV file into a pandas DataFrame df = pd.read_csv(input_file) # Convert \'session_duration\' and \'pages_visited\' to nullable integer types df[\'session_duration\'] = pd.array(df[\'session_duration\'], dtype=\\"Int64\\") df[\'pages_visited\'] = pd.array(df[\'pages_visited\'], dtype=\\"Int64\\") # Calculate the average session duration avg_session_duration = df[\'session_duration\'].mean() # Count users with missing \'pages_visited\' missing_pages_visited_count = df[\'pages_visited\'].isna().sum() print(f\\"Users with missing \'pages_visited\': {missing_pages_visited_count}\\") # Fill missing values df[\'session_duration\'] = df[\'session_duration\'].fillna(avg_session_duration) median_pages_visited = df[\'pages_visited\'].median() df[\'pages_visited\'] = df[\'pages_visited\'].fillna(median_pages_visited) # Export the modified DataFrame to a new CSV file df.to_csv(output_file, index=False)"},{"question":"**Coding Challenge: Implement a Python Class Using the Numeric Protocol** **Objective:** Create a Python class `PyNumberDemo` that utilizes multiple numeric operations provided in the `python310` package to perform arithmetic and bitwise operations. This class will use the documented functions to perform these operations. **Class:** `PyNumberDemo` 1. **Functionality:** - Initialize with two numeric values. - Support the following operations: - Addition (`__add__`) - Subtraction (`__sub__`) - Multiplication (`__mul__`) - True Division (`__truediv__`) - Floor Division (`__floordiv__`) - Modulus (`__mod__`) - Power (`__pow__`) - Bitwise AND (`__and__`) - Bitwise OR (`__or__`) - Bitwise XOR (`__xor__`) - Left Shift (`__lshift__`) - Right Shift (`__rshift__`) - Implement methods for converting the values to float and integer. - Provide a method to get the value in a specified base (binary, octal, hexadecimal, decimal). 2. **Input and Output:** - Input: Two numeric values (integers or floats). - Output: Results of arithmetic and bitwise operations, converted values in different bases. 3. **Constraints:** - You must handle possible exceptions (e.g., division by zero). - Ensure that input values are valid numeric types before performing operations. - Implement conversion operations carefully to ensure proper format returns. - All operations should mimic the behavior of respective Python operators. 4. **Performance Requirement:** - Operations involving large numbers should be handled efficiently. - Ensure no memory leaks or performance degradation due to in-place operations. **Example:** ```python class PyNumberDemo: def __init__(self, value1, value2): # Initialize with two numeric values. pass def __add__(self, other): # Implement addition operation. pass def __sub__(self, other): # Implement subtraction operation. pass def __mul__(self, other): # Implement multiplication operation. pass def __truediv__(self, other): # Implement true division operation. pass def __floordiv__(self, other): # Implement floor division operation. pass def __mod__(self, other): # Implement modulus operation. pass def __pow__(self, other, modulo=None): # Implement power operation. pass def __and__(self, other): # Implement bitwise AND operation. pass def __or__(self, other): # Implement bitwise OR operation. pass def __xor__(self, other): # Implement bitwise XOR operation. pass def __lshift__(self, other): # Implement left shift operation. pass def __rshift__(self, other): # Implement right shift operation. pass def to_float(self): # Convert value to float. pass def to_int(self): # Convert value to int. pass def to_base(self, base): # Convert value to a specified base (binary, octal, hexadecimal, decimal). pass # Example Usage: # num = PyNumberDemo(15, 5) # result_add = num + 10 # result_mul = num * 3 # result_float = num.to_float() # result_int = num.to_int() # result_base = num.to_base(16) ``` **Implementation Note:** The functions mentioned in the provided documentation should be called appropriately within the class methods to perform the respective operations. Pay special attention to error handling, ensuring the class mimics Python\'s native behavior as closely as possible.","solution":"class PyNumberDemo: def __init__(self, value1, value2): self.value1 = value1 self.value2 = value2 def __validate(self): if not isinstance(self.value1, (int, float)) or not isinstance(self.value2, (int, float)): raise ValueError(\'Both values must be numeric.\') def __add__(self, other): self.__validate() if isinstance(other, PyNumberDemo): return self.value1 + other.value1, self.value2 + other.value2 else: return self.value1 + other, self.value2 + other def __sub__(self, other): self.__validate() if isinstance(other, PyNumberDemo): return self.value1 - other.value1, self.value2 - other.value2 else: return self.value1 - other, self.value2 - other def __mul__(self, other): self.__validate() if isinstance(other, PyNumberDemo): return self.value1 * other.value1, self.value2 * other.value2 else: return self.value1 * other, self.value2 * other def __truediv__(self, other): self.__validate() if isinstance(other, PyNumberDemo): if other.value1 == 0 or other.value2 == 0: raise ZeroDivisionError(\\"Division by zero.\\") return self.value1 / other.value1, self.value2 / other.value2 else: if other == 0: raise ZeroDivisionError(\\"Division by zero.\\") return self.value1 / other, self.value2 / other def __floordiv__(self, other): self.__validate() if isinstance(other, PyNumberDemo): if other.value1 == 0 or other.value2 == 0: raise ZeroDivisionError(\\"Division by zero.\\") return self.value1 // other.value1, self.value2 // other.value2 else: if other == 0: raise ZeroDivisionError(\\"Division by zero.\\") return self.value1 // other, self.value2 // other def __mod__(self, other): self.__validate() if isinstance(other, PyNumberDemo): if other.value1 == 0 or other.value2 == 0: raise ZeroDivisionError(\\"Division by zero.\\") return self.value1 % other.value1, self.value2 % other.value2 else: if other == 0: raise ZeroDivisionError(\\"Division by zero.\\") return self.value1 % other, self.value2 % other def __pow__(self, other, modulo=None): self.__validate() if isinstance(other, PyNumberDemo): return pow(self.value1, other.value1, modulo), pow(self.value2, other.value2, modulo) else: return pow(self.value1, other, modulo), pow(self.value2, other, modulo) def __and__(self, other): self.__validate() if isinstance(other, PyNumberDemo): return int(self.value1) & int(other.value1), int(self.value2) & int(other.value2) else: return int(self.value1) & int(other), int(self.value2) & int(other) def __or__(self, other): self.__validate() if isinstance(other, PyNumberDemo): return int(self.value1) | int(other.value1), int(self.value2) | int(other.value2) else: return int(self.value1) | int(other), int(self.value2) | int(other) def __xor__(self, other): self.__validate() if isinstance(other, PyNumberDemo): return int(self.value1) ^ int(other.value1), int(self.value2) ^ int(other.value2) else: return int(self.value1) ^ int(other), int(self.value2) ^ int(other) def __lshift__(self, other): self.__validate() if isinstance(other, PyNumberDemo): return int(self.value1) << int(other.value1), int(self.value2) << int(other.value2) else: return int(self.value1) << int(other), int(self.value2) << int(other) def __rshift__(self, other): self.__validate() if isinstance(other, PyNumberDemo): return int(self.value1) >> int(other.value1), int(self.value2) >> int(other.value2) else: return int(self.value1) >> int(other), int(self.value2) >> int(other) def to_float(self): return float(self.value1), float(self.value2) def to_int(self): return int(self.value1), int(self.value2) def to_base(self, base): if base == 2: # Binary return bin(int(self.value1)), bin(int(self.value2)) elif base == 8: # Octal return oct(int(self.value1)), oct(int(self.value2)) elif base == 16: # Hexadecimal return hex(int(self.value1)), hex(int(self.value2)) elif base == 10: # Decimal return str(self.value1), str(self.value2) else: raise ValueError(\'Base must be one of 2, 8, 10, or 16.\')"},{"question":"You are tasked with implementing a Python class that mimics some of the behavior described in the documentation for handling boolean values but in purely Python code to reinforce your understanding. # Class: `CustomBoolean` Methods: 1. **`__init__(self, value)`**: - **Input**: A single value which could be any Python object. - **Output**: None. - **Description**: Initializes the boolean value based on the truthiness of the input `value`. 2. **`__bool__(self)`**: - **Input**: None. - **Output**: Returns the boolean representation (`True` or `False`) of the stored value. - **Description**: This special method is called when evaluating the instance in a boolean context (e.g., `if` statements). 3. **`from_long(cls, v)`**: - **Input**: A single integer `v`. - **Output**: An instance of `CustomBoolean` set to `True` if `v` is non-zero, and `False` if `v` is zero. - **Description**: Class method to create a `CustomBoolean` instance from an integer. 4. **`return_false(cls)`**: - **Input**: None. - **Output**: An instance of `CustomBoolean` set to `False`. - **Description**: Class method to return a `CustomBoolean` instance equivalent to `False`. 5. **`return_true(cls)`**: - **Input**: None. - **Output**: An instance of `CustomBoolean` set to `True`. - **Description**: Class method to return a `CustomBoolean` instance equivalent to `True`. # Example Usage: ```python # Create CustomBoolean instances cb1 = CustomBoolean(5) cb2 = CustomBoolean(0) # Check their boolean values assert bool(cb1) is True assert bool(cb2) is False # Create from long integers cb3 = CustomBoolean.from_long(10) cb4 = CustomBoolean.from_long(0) assert bool(cb3) is True assert bool(cb4) is False # Return constants cb5 = CustomBoolean.return_true() cb6 = CustomBoolean.return_false() assert bool(cb5) is True assert bool(cb6) is False ``` # Constraints: - You are not allowed to use the built-in `bool` constructor directly in the methods. - Ensure that your class behaves correctly when used in boolean contexts (e.g., `if` statements).","solution":"class CustomBoolean: def __init__(self, value): Initializes the boolean value based on the truthiness of the input value. self.value = value def __bool__(self): Returns the boolean representation of the stored value. return self.value not in [False, None, 0, \'\', [], {}] @classmethod def from_long(cls, v): Creates a CustomBoolean instance from an integer. return cls(v != 0) @classmethod def return_false(cls): Returns a CustomBoolean instance equivalent to False. return cls(False) @classmethod def return_true(cls): Returns a CustomBoolean instance equivalent to True. return cls(True)"},{"question":"**Question: Working with Named Tensors in PyTorch** You are required to implement a function and perform some operations on named tensors in PyTorch. The function should create a named tensor, perform a series of operations on it, and return the final tensor while verifying named dimension behaviors as specified. # Function Specification **Name:** `process_named_tensor` **Inputs:** 1. `initial_values` (list of lists of float): A 2D list of floats used to initialize the tensor. 2. `row_name` (str): Name for the row dimension. 3. `col_name` (str): Name for the column dimension. 4. `operations` (list of tuple): Each tuple contains: - An operation to be performed (str). - Parameters for the operation which can be integers, floats, or named dimensions (tuple). The operations list may contain the following operations: - `\'add\'`: Add a value to each element (parameter: value). - `\'mul\'`: Multiply each element by a value (parameter: value). - `\'sum\'`: Sum along a specified dimension (parameter: dimension name). - `\'transpose\'`: Transpose the tensor\'s dimensions (parameters: two dimension names to swap). **Output:** - Returns the final processed named tensor. # Example ```python def process_named_tensor(initial_values, row_name, col_name, operations): # Your implementation here # Example usage: initial_values = [[1.0, 2.0], [3.0, 4.0]] row_name = \'R\' col_name = \'C\' operations = [ (\'add\', (2.0,)), (\'mul\', (0.5,)), (\'sum\', (\'R\',)), (\'transpose\', (\'R\', \'C\')) ] result = process_named_tensor(initial_values, row_name, col_name, operations) print(result) # Expected output (names and values might vary based on the operations): # tensor with applied operations and proper dimension names. ``` # Constraints - Ensure the tensor created has the names for its dimensions as specified. - Operations should properly handle and verify the names of dimensions. - The `sum` operation should reduce the specified named dimension and update names accordingly. - The `transpose` operation should switch the specified dimensions and their names. # Hints 1. Use `torch.tensor()` and specify names when initializing the tensor. 2. Use appropriate PyTorch functions and ensure they maintain or update the dimension names. 3. Validate dimensional names where necessary to avoid mismatches. By implementing this function, you will demonstrate your understanding of named tensors, tensor operations, and dimension manipulation as described in the provided documentation.","solution":"import torch def process_named_tensor(initial_values, row_name, col_name, operations): tensor = torch.tensor(initial_values, names=(row_name, col_name)) for op, params in operations: if op == \'add\': value, = params tensor = tensor + value elif op == \'mul\': value, = params tensor = tensor * value elif op == \'sum\': dim_name, = params tensor = tensor.sum(dim=dim_name) elif op == \'transpose\': dim1_name, dim2_name = params tensor = tensor.align_to(dim2_name, dim1_name) else: raise ValueError(f\\"Unsupported operation: {op}\\") return tensor"},{"question":"<|Analysis Begin|> The `calendar` module in Python provides functions and classes to handle general calendar-related tasks. It includes objects to generate text and HTML representations of calendars, manipulate calendar data, and more. Here are the key components: 1. **Classes & Methods**: - `Calendar(firstweekday=0)`: Base calendar class. - `TextCalendar(firstweekday=0)`: Generates plain text calendars. - `HTMLCalendar(firstweekday=0)`: Generates HTML calendars. - Methods include `iterweekdays()`, `itermonthdates()`, `itermonthdays()`, `itermonthdays2()`, among others, which return iterators for various calendar data elements. 2. **Locale Specific Classes**: - `LocaleTextCalendar(firstweekday=0, locale=None)`: Text calendar with locale support. - `LocaleHTMLCalendar(firstweekday=0, locale=None)`: HTML calendar with locale support. 3. **Utility Functions**: - `setfirstweekday(weekday)`: Sets the first day of the week. - `firstweekday()`: Gets the current setting for the first day of the week. - `isleap(year)`: Checks if the year is a leap year. - `leapdays(y1, y2)`: Returns the number of leap years in a range. - `weekday(year, month, day)`: Returns the day of the week. - `weekheader(n)`: Returns a header containing abbreviated weekday names. - `monthrange(year, month)`: Returns the first day of the month and the number of days in the month. - `monthcalendar(year, month)`: Returns a matrix representing a month\'s calendar. 4. **Data Attributes**: - `day_name`, `day_abbr`, `month_name`, `month_abbr`: Arrays that represent the days and months in the current locale. - Constants for days of the week (e.g., `MONDAY`, `SUNDAY`). The task should emphasize the understanding and application of these components, particularly how to manipulate calendar data and generate formatted outputs. <|Analysis End|> <|Question Begin|> # Calendar Utility Implementation You are required to implement a utility that leverages the `calendar` module to provide several calendar-related functions. Your implementation should include the following functions: 1. **generate_month_calendar**: - **Input**: - `year` (int): The year for which the calendar should be generated. - `month` (int): The month for which the calendar should be generated. - **Output**: - A string representing the month calendar in a multi-line format (similar to Unix `cal` command, using `TextCalendar`). 2. **is_leap_year**: - **Input**: - `year` (int): The year to be checked. - **Output**: - Boolean value `True` if it is a leap year, otherwise `False`. 3. **week_day**: - **Input**: - `year` (int): Year of the date. - `month` (int): Month of the date. - `day` (int): Day of the date. - **Output**: - A string representing the weekday (e.g., \\"Monday\\"). 4. **months_with_day_name**: - **Input**: - `year` (int): The year for which to analyze. - `weekday` (str): The name of the weekday (e.g., \\"Monday\\"). - **Output**: - A list of integers representing months in the given year in which the 1st day is the specified day of the week. # Constraints - The `year` should be a positive integer. - The `month` should be an integer between 1 and 12. - The `day` should be an integer valid for the given month and year. - The `weekday` should be a valid name of the day (e.g., \\"Monday\\", \\"Tuesday\\"). # Example ```python # Example usage of the functions implementing calendar utility # Generate month calendar for March 2023 print(generate_month_calendar(2023, 3)) # Output: # March 2023 # Mo Tu We Th Fr Sa Su # 1 2 3 4 5 # 6 7 8 9 10 11 12 # 13 14 15 16 17 18 19 # 20 21 22 23 24 25 26 # 27 28 29 30 31 # Check if 2020 is a leap year print(is_leap_year(2020)) # Output: True # Get weekday for March 14, 2023 print(week_day(2023, 3, 14)) # Output: Tuesday # Get months in 2023 with Monday as the 1st day print(months_with_day_name(2023, \\"Monday\\")) # Output: [5] ``` # Notes - You can use `calendar.TextCalendar` to generate month calendars in text format. - Utilize `calendar.isleap`, `calendar.weekday`, and `calendar.monthcalendar` where appropriate.","solution":"import calendar def generate_month_calendar(year, month): Generates a month calendar in a multi-line string format. cal = calendar.TextCalendar() return cal.formatmonth(year, month) def is_leap_year(year): Checks if the given year is a leap year. return calendar.isleap(year) def week_day(year, month, day): Returns the name of the weekday for the given date. days = [\\"Monday\\", \\"Tuesday\\", \\"Wednesday\\", \\"Thursday\\", \\"Friday\\", \\"Saturday\\", \\"Sunday\\"] day_index = calendar.weekday(year, month, day) return days[day_index] def months_with_day_name(year, weekday): Finds all months in the given year where the 1st day of the month is the specified weekday. days = [\\"Monday\\", \\"Tuesday\\", \\"Wednesday\\", \\"Thursday\\", \\"Friday\\", \\"Saturday\\", \\"Sunday\\"] day_index = days.index(weekday) result = [] for month in range(1, 13): if calendar.monthrange(year, month)[0] == day_index: result.append(month) return result"},{"question":"Objective In this task, you are required to demonstrate your understanding of the cross decomposition module in Scikit-learn, particularly focusing on Partial Least Squares Regression (PLSR). You will implement a workflow that involves data preprocessing, model training, and evaluation using the PLSR algorithm. Problem Statement Given two datasets, `X_train` (predictors) and `Y_train` (responses), along with a test set `X_test`, your task is to: 1. Preprocess the data by centering and standardizing the features. 2. Implement Partial Least Squares Regression using Scikit-learn\'s `PLSRegression`. 3. Train the PLSR model on the preprocessed training data. 4. Predict the responses for the `X_test` set. 5. Evaluate the model using the coefficient of determination ((R^2) score) on both the training data and a provided `Y_test` set. Input - `X_train`: a 2D NumPy array of shape (n_samples_train, n_features). - `Y_train`: a 2D NumPy array of shape (n_samples_train, n_targets). - `X_test`: a 2D NumPy array of shape (n_samples_test, n_features). - `Y_test`: a 2D NumPy array of shape (n_samples_test, n_targets). Constraints 1. You should use the `StandardScaler` from Scikit-learn for preprocessing. 2. Use the `PLSRegression` class from Scikit-learn\'s `cross_decomposition` module with `n_components=2`. Output - Print the (R^2) score on the training data. - Print the (R^2) score on the test data. - Return the predicted responses for `X_test`. Implementation ```python import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import r2_score def pls_regression(X_train, Y_train, X_test, Y_test): # 1. Preprocess the data scaler_X = StandardScaler().fit(X_train) scaler_Y = StandardScaler().fit(Y_train) X_train_scaled = scaler_X.transform(X_train) Y_train_scaled = scaler_Y.transform(Y_train) X_test_scaled = scaler_X.transform(X_test) # 2. Implement Partial Least Squares Regression pls = PLSRegression(n_components=2) # 3. Train the model pls.fit(X_train_scaled, Y_train_scaled) # 4. Predict the responses for the test set Y_pred_train = pls.predict(X_train_scaled) Y_pred_test = pls.predict(X_test_scaled) # 5. Evaluate the model r2_train = r2_score(Y_train_scaled, Y_pred_train) r2_test = r2_score(Y_test, scaler_Y.inverse_transform(Y_pred_test)) # Print the R^2 scores print(f\'R^2 score on training data: {r2_train}\') print(f\'R^2 score on test data: {r2_test}\') # Return the predicted responses for X_test return scaler_Y.inverse_transform(Y_pred_test) # Example usage # X_train, Y_train, X_test, Y_test should be defined by the user # Y_pred_test = pls_regression(X_train, Y_train, X_test, Y_test) ``` Note: - Ensure that Scikit-learn library is installed (`pip install scikit-learn`). - The provided example usage assumes that the data variables (`X_train`, `Y_train`, `X_test`, `Y_test`) are already defined.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.preprocessing import StandardScaler from sklearn.metrics import r2_score def pls_regression(X_train, Y_train, X_test, Y_test): # 1. Preprocess the data scaler_X = StandardScaler().fit(X_train) scaler_Y = StandardScaler().fit(Y_train) X_train_scaled = scaler_X.transform(X_train) Y_train_scaled = scaler_Y.transform(Y_train) X_test_scaled = scaler_X.transform(X_test) # 2. Implement Partial Least Squares Regression pls = PLSRegression(n_components=2) # 3. Train the model pls.fit(X_train_scaled, Y_train_scaled) # 4. Predict the responses for the test set Y_pred_train = pls.predict(X_train_scaled) Y_pred_test = pls.predict(X_test_scaled) # 5. Evaluate the model r2_train = r2_score(Y_train_scaled, Y_pred_train) r2_test = r2_score(Y_test, scaler_Y.inverse_transform(Y_pred_test)) # Print the R^2 scores print(f\'R^2 score on training data: {r2_train}\') print(f\'R^2 score on test data: {r2_test}\') # Return the predicted responses for X_test return scaler_Y.inverse_transform(Y_pred_test)"},{"question":"# Question You are given a binary file that contains multiple chunks, all following the EA IFF 85 chunk structure. Your task is to write a Python function that reads this file and extracts all chunk IDs and their corresponding data sizes. The function should return a list of tuples, where each tuple contains the chunk ID and the size of the chunk data in bytes. Function Signature ```python def extract_chunk_info(file_path: str) -> List[Tuple[str, int]]: pass ``` Input - `file_path`: A string representing the path to the binary file containing the chunks. Output - A list of tuples, where each tuple consists of: - A string representing the chunk ID. - An integer representing the size of the chunk data in bytes. Constraints - You can assume that the binary file exists and is readable. - The file will contain at least one chunk. - The file is well-formed, meaning all chunks adhere to the specified EA IFF 85 structure. Example Consider a binary file `testfile.bin` with the following chunks: 1. Chunk ID: `FMT_`, Size: 10 bytes 2. Chunk ID: `DATA`, Size: 20 bytes ```python result = extract_chunk_info(\'testfile.bin\') print(result) # Output: [(\'FMT_\', 10), (\'DATA\', 20)] ``` Notes - Use the `chunk` module to read the chunks from the file. - Pay attention to the end-of-file condition when reading multiple chunks. - Handle the optional alignment and big-endian options as specified in the `chunk` module documentation.","solution":"import struct from typing import List, Tuple def extract_chunk_info(file_path: str) -> List[Tuple[str, int]]: chunks = [] with open(file_path, \'rb\') as file: while True: chunk_header = file.read(8) # Read the 8-byte chunk header if len(chunk_header) < 8: break # EOF or incomplete chunk header chunk_id, chunk_size = struct.unpack(\'>4sI\', chunk_header) chunk_id = chunk_id.decode(\'ascii\') chunks.append((chunk_id, chunk_size)) # Seek to the next chunk\'s header file.seek(chunk_size, 1) # Move the file pointer forward by chunk_size bytes # Align to even boundary if chunk_size is not even if chunk_size % 2 != 0: file.seek(1, 1) # Skip padding byte return chunks"},{"question":"You are required to write a set of functions that makes use of the `cmath` module to perform various complex number operations. Your task is to implement the following functions: 1. `convert_to_polar(z: complex) -> tuple`: - **Input**: A complex number `z`. - **Output**: A tuple `(r, phi)`, where `r` is the modulus of `z` and `phi` is the phase of `z`. 2. `convert_to_rectangular(r: float, phi: float) -> complex`: - **Input**: Two floats `r` and `phi` representing the modulus and phase of a complex number. - **Output**: The complex number `z` with modulus `r` and phase `phi`. 3. `compute_natural_log(z: complex) -> complex`: - **Input**: A complex number `z`. - **Output**: The natural logarithm of `z`. 4. `compute_trigonometric_identity(z: complex) -> complex`: - **Input**: A complex number `z`. - **Output**: The value of `sin(z)^2 + cos(z)^2`, which should ideally be close to 1 (given that this is a complex number operation). 5. `is_complex_number_close(a: complex, b: complex, rel_tol: float = 1e-09, abs_tol: float = 0.0) -> bool`: - **Input**: Two complex numbers `a` and `b`, along with optional relative tolerance `rel_tol` and absolute tolerance `abs_tol`. - **Output**: Boolean value indicating whether `a` and `b` are close to each other based on the given tolerances. # Constraints: - You must use the functions provided in the `cmath` module. - Your solution should handle edge cases gracefully (e.g., complex numbers on branch cuts). - Performance is not a primary concern for this assessment. # Example Usage: ```python # Example usage of the implemented functions: z = complex(1, 1) r, phi = convert_to_polar(z) print(r, phi) # Output: (1.4142135623730951, 0.7853981633974483) z_rect = convert_to_rectangular(r, phi) print(z_rect) # Output: (1+1j) ln_z = compute_natural_log(z) print(ln_z) # Output: (0.34657359027997264+0.7853981633974483j) trig_identity = compute_trigonometric_identity(z) print(trig_identity) # Output: (1+0j) a = complex(1.000000001, 1) b = complex(1.000000002, 1) close = is_complex_number_close(a, b) print(close) # Output: True ```","solution":"import cmath def convert_to_polar(z: complex) -> tuple: Convert a complex number to its polar form. Parameters: z (complex): A complex number. Returns: tuple: A tuple (r, phi) where r is the modulus and phi is the phase of the complex number. r = abs(z) phi = cmath.phase(z) return r, phi def convert_to_rectangular(r: float, phi: float) -> complex: Convert a complex number from polar form to rectangular form. Parameters: r (float): The modulus of the complex number. phi (float): The phase of the complex number. Returns: complex: The complex number in rectangular form. return cmath.rect(r, phi) def compute_natural_log(z: complex) -> complex: Compute the natural logarithm of a complex number. Parameters: z (complex): A complex number. Returns: complex: The natural logarithm of the complex number. return cmath.log(z) def compute_trigonometric_identity(z: complex) -> complex: Compute sin(z)^2 + cos(z)^2 for a complex number z. Parameters: z (complex): A complex number. Returns: complex: The result of the trigonometric identity sin(z)^2 + cos(z)^2. sin_z = cmath.sin(z) cos_z = cmath.cos(z) return sin_z**2 + cos_z**2 def is_complex_number_close(a: complex, b: complex, rel_tol: float = 1e-09, abs_tol: float = 0.0) -> bool: Determine whether two complex numbers are close to each other. Parameters: a (complex): The first complex number. b (complex): The second complex number. rel_tol (float): The relative tolerance. abs_tol (float): The absolute tolerance. Returns: bool: True if the complex numbers are close to each other, False otherwise. return cmath.isclose(a, b, rel_tol=rel_tol, abs_tol=abs_tol)"},{"question":"**Problem Statement:** You are provided with a 4D tensor representing a batch of images, where each image has multiple channels (e.g., RGB), height, and width. Your task is to implement a function that performs the following tasks: 1. **Determine the batch size:** Extract the number of images in the batch. 2. **Extract Channel Information:** Retrieve the number of channels in each image. 3. **Determine Image Dimensions:** Get the height and width of each image. 4. **Check Square Images:** Verify if all images in the batch are square (i.e., height and width are equal). **Function Signature:** ```python def analyze_tensor(tensor: torch.Tensor) -> dict: Analyze the given 4D tensor and return a dictionary with batch size, number of channels, height, width, and a boolean indicating if all images are square. Parameters: tensor (torch.Tensor): A 4-dimensional tensor of shape (batch_size, channels, height, width). Returns: dict: A dictionary with the following keys: - \'batch_size\': int, number of images in the batch. - \'channels\': int, number of channels in each image. - \'height\': int, height of each image. - \'width\': int, width of each image. - \'is_square\': bool, True if all images are square, False otherwise. Constraints: - The input tensor is guaranteed to be 4-dimensional. - Height and width of images are positive integers. ``` **Input:** - `tensor`: A 4-dimensional `torch.Tensor` of shape `(batch_size, channels, height, width)`. **Output:** - A dictionary containing: - `batch_size` (int): Number of images in the batch. - `channels` (int): Number of channels in each image. - `height` (int): Height of each image. - `width` (int): Width of each image. - `is_square` (bool): `True` if all images in the batch are square, `False` otherwise. **Constraints:** - The input tensor is guaranteed to be 4D (i.e., `(batch_size, channels, height, width)`). - The height and width of images are positive integers. **Example:** ```python import torch # Example tensor with shape (2, 3, 64, 64) -> 2 images, 3 channels, 64x64 pixels each tensor = torch.ones(2, 3, 64, 64) result = analyze_tensor(tensor) print(result) # Output: {\'batch_size\': 2, \'channels\': 3, \'height\': 64, \'width\': 64, \'is_square\': True} # Example tensor with shape (2, 3, 64, 32) -> 2 images, 3 channels, 64x32 pixels each tensor = torch.ones(2, 3, 64, 32) result = analyze_tensor(tensor) print(result) # Output: {\'batch_size\': 2, \'channels\': 3, \'height\': 64, \'width\': 32, \'is_square\': False} ``` **Notes:** - Make sure your solution handles tensors of various dimensions correctly while sticking to the constraints. - Consider edge cases, such as tensors where all images are not square.","solution":"import torch def analyze_tensor(tensor: torch.Tensor) -> dict: Analyze the given 4D tensor and return a dictionary with batch size, number of channels, height, width, and a boolean indicating if all images are square. Parameters: tensor (torch.Tensor): A 4-dimensional tensor of shape (batch_size, channels, height, width). Returns: dict: A dictionary with the following keys: - \'batch_size\': int, number of images in the batch. - \'channels\': int, number of channels in each image. - \'height\': int, height of each image. - \'width\': int, width of each image. - \'is_square\': bool, True if all images are square, False otherwise. batch_size, channels, height, width = tensor.shape is_square = height == width return { \'batch_size\': batch_size, \'channels\': channels, \'height\': height, \'width\': width, \'is_square\': is_square }"},{"question":"Objective Design a comprehensive data visualization using Seaborn to analyze the relationship between different variables in a given dataset. Problem Statement You are provided with a dataset containing information about house sales (`house_sales.csv`). Your task is to create an informative visualization using `seaborn.relplot` which includes scatter plots and line plots to show relationships between the following variables: 1. `sale_price`: Sale price of the house. 2. `lot_area`: Lot area of the house. 3. `year_built`: The year the house was built. 4. `overall_quality`: Categorical variable with values ranging from 1-10 indicating the overall material and finish of the house. 5. `neighborhood`: Categorical variable indicating the neighborhood in which the house is located. Your visualization should fulfill the following requirements: 1. **Scatter Plot Analysis**: - Create a scatter plot analyzing the relationship between `lot_area` and `sale_price`. - Use the `hue` semantic to differentiate data points by `overall_quality`. - Use different marker `styles` to differentiate data by `neighborhood`. 2. **Line Plot Analysis**: - Create a line plot to show how the `sale_price` has changed over different `year_built`. - Use the `hue` semantic for differentiating data points by `neighborhood`. - Represent uncertainty using confidence intervals. 3. **Faceting**: - Create a facet grid visualization where each subplot represents a different `neighborhood`. Each subplot should contain the visualization from (1) i.e., scatter plot. 4. **Customization**: - Customize one of the colors palettes in any of the plots to improve readability. Input Format - A CSV file named `house_sales.csv` containing columns: `sale_price`, `lot_area`, `year_built`, `overall_quality`, and `neighborhood`. Output - A Matplotlib figure with the requested visualizations. Constraints and Notes - Ensure that visualizations are correctly labeled and readable. - House sales data might contain missing values, handle them appropriately. - Ensure plots are optimized for performance, especially while handling larger datasets. Example Usage ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\\"house_sales.csv\\") # Scatter plot analysis scatter_plot = sns.relplot(data=df, x=\\"lot_area\\", y=\\"sale_price\\", hue=\\"overall_quality\\", style=\\"neighborhood\\") scatter_plot.set(title=\'Scatter Plot of Lot Area vs Sale Price\', ylabel=\'Sale Price ()\', xlabel=\'Lot Area (sq. ft.)\') # Line plot analysis line_plot = sns.relplot(data=df, x=\\"year_built\\", y=\\"sale_price\\", hue=\\"neighborhood\\", kind=\\"line\\", ci=\\"sd\\") line_plot.set(title=\'Sale Price over Years Built\', ylabel=\'Sale Price ()\', xlabel=\'Year Built\') # Facet grid visualization facet = sns.relplot(data=df, x=\\"lot_area\\", y=\\"sale_price\\", hue=\\"overall_quality\\", style=\\"neighborhood\\", col=\\"neighborhood\\") facet.set_titles(col_template=\\"Neighborhood: {col_name}\\") # Customizing color palette custom_palette = \\"ch:r=-.5,l=.75\\" line_plot_custom = sns.relplot(data=df, x=\\"year_built\\", y=\\"sale_price\\", hue=\\"neighborhood\\", kind=\\"line\\", palette=custom_palette, errorbar=\\"sd\\") line_plot_custom.set(title=\'Customized Sale Price over Years Built\', ylabel=\'Sale Price ()\', xlabel=\'Year Built\') plt.show() ``` Evaluation Criteria - Correct application of `relplot`, `hue`, `style`, and `size` semantics. - Appropriate handling and representation of statistical measures like confidence intervals. - Effective use of faceting to represent subgroups. - Clarity, readability, and useful customization of the visualizations. - Proper treatment of missing data.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(data_path): Creates scatter plot, line plot, and facet grid visualization based on the house_sales dataset. Parameters: data_path: str - The path to the CSV file containing house sales data. Returns: None # Load the dataset df = pd.read_csv(data_path) # Drop rows with missing values df = df.dropna() # Scatter plot analysis scatter_plot = sns.relplot( data=df, x=\\"lot_area\\", y=\\"sale_price\\", hue=\\"overall_quality\\", style=\\"neighborhood\\", kind=\\"scatter\\", palette=\\"viridis\\" ) scatter_plot.set(title=\'Scatter Plot of Lot Area vs Sale Price\', ylabel=\'Sale Price ()\', xlabel=\'Lot Area (sq. ft.)\') scatter_plot.fig.suptitle(\'House Sales Analysis\', y=1.02) plt.show() # Line plot analysis line_plot = sns.relplot( data=df, x=\\"year_built\\", y=\\"sale_price\\", hue=\\"neighborhood\\", kind=\\"line\\", ci=\\"sd\\", palette=\\"coolwarm\\" ) line_plot.set(title=\'Sale Price over Years Built\', ylabel=\'Sale Price ()\', xlabel=\'Year Built\') line_plot.fig.suptitle(\'House Sales Analysis\', y=1.02) plt.show() # Facet grid visualization facet = sns.relplot( data=df, x=\\"lot_area\\", y=\\"sale_price\\", hue=\\"overall_quality\\", style=\\"neighborhood\\", col=\\"neighborhood\\", kind=\\"scatter\\", col_wrap=4, palette=\\"light:m_r\\" ) facet.set_titles(col_template=\\"Neighborhood: {col_name}\\") facet.set_axis_labels(\'Lot Area (sq. ft.)\', \'Sale Price ()\') facet.fig.suptitle(\'Scatter Plot of Lot Area vs Sale Price by Neighborhood\', y=1.03) plt.show() # Customizing color palette in a line plot custom_palette = \\"ch:r=-.5,l=.75\\" line_plot_custom = sns.relplot( data=df, x=\\"year_built\\", y=\\"sale_price\\", hue=\\"neighborhood\\", kind=\\"line\\", palette=custom_palette, ci=\\"sd\\" ) line_plot_custom.set(title=\'Customized Sale Price over Years Built\', ylabel=\'Sale Price ()\', xlabel=\'Year Built\') line_plot_custom.fig.suptitle(\'House Sales Analysis\', y=1.02) plt.show()"},{"question":"You are required to create a Python script that logs messages to the system log using the `syslog` module. Your script should demonstrate a comprehensive understanding of the syslog module\'s capabilities, including setting up custom logging options and logging messages with varying priorities and facilities. Specifications: 1. **Function Definition**: - Create a function `log_messages(ident, logoption, facility, messages)` that: - Accepts an identifier (`ident`), log options (`logoption`), a logging facility (`facility`), and a list of tuples containing log messages and their corresponding priorities (`messages`). 2. **Parameters**: - `ident` (str): A string identifier that is prepended to each log message. - `logoption` (int): A bitmask of log options (e.g., `syslog.LOG_PID | syslog.LOG_CONS`). - `facility` (int): The default facility for logging messages (e.g., `syslog.LOG_USER`). - `messages` (list): A list of tuples, where each tuple contains: - `priority` (int): The priority of the log message (e.g., `syslog.LOG_ERR`). - `message` (str): The log message to be logged. 3. **Functionality**: - The function should open the syslog with the provided `ident`, `logoption`, and `facility`. - Log each message with its respective priority. - Close the syslog after logging all messages. - The function should raise appropriate exceptions if invalid parameters are provided. Constraints: - Messages should be logged even if the system logger is not manually opened. - Utilize the available priorities and facilities as constants provided by the syslog module. - The default behavior should be to log messages with \'LOG_INFO\' priority if not specified otherwise. - Handle any exceptions gracefully and log them as \'LOG_ERR\' messages to indicate an error in logging. Example: ```python import syslog def log_messages(ident, logoption, facility, messages): try: # Open syslog with the specified options syslog.openlog(ident=ident, logoption=logoption, facility=facility) # Iterate over messages and log each one with the corresponding priority for priority, message in messages: syslog.syslog(priority, message) # Close the syslog syslog.closelog() except Exception as e: syslog.syslog(syslog.LOG_ERR, f\\"Error during logging: {str(e)}\\") # Example usage: log_messages( ident=\\"MyApp\\", logoption=syslog.LOG_PID | syslog.LOG_CONS, facility=syslog.LOG_USER, messages=[ (syslog.LOG_INFO, \\"This is an info message\\"), (syslog.LOG_ERR, \\"This is an error message\\"), (syslog.LOG_WARNING, \\"This is a warning message\\") ] ) ``` Use this function within your script to demonstrate different configurations and log various types of messages. Expected Output: - The `log_messages` function should log messages to the system log along with the specified identifier and using the appropriate facility and priority. - Example logs in the system log viewer with the identifier, facility, and priority as provided. Good luck!","solution":"import syslog def log_messages(ident, logoption, facility, messages): Log multiple messages with varying priorities and facilities using syslog. Parameters: ident (str): A string identifier prepended to each log message. logoption (int): A bitmask of log options (e.g., syslog.LOG_PID | syslog.LOG_CONS). facility (int): The default facility for logging messages (e.g., syslog.LOG_USER). messages (list of tuples): Each tuple contains a priority (int) and a message (str). Example tuple: (syslog.LOG_ERR, \'This is an error message\') try: # Open syslog with the specified options syslog.openlog(ident=ident, logoption=logoption, facility=facility) # Iterate over messages and log each one with the corresponding priority for priority, message in messages: syslog.syslog(priority, message) # Close the syslog syslog.closelog() except Exception as e: syslog.syslog(syslog.LOG_ERR, f\\"Error during logging: {str(e)}\\")"},{"question":"# Task: You are provided with a dataset consisting of features (X) and target labels (y). Your goal is to train a classification model using scikit-learn and evaluate its performance using custom scoring functions. Specifically, you need to: 1. Implement a custom scoring function called `custom_fbeta_score` which calculates the F-beta measure, where beta is provided as an input parameter. 2. Use `cross_val_score` to evaluate the model performance using the `custom_fbeta_score` with different beta values. 3. Compare the performance of your classifier using `custom_fbeta_score` with predefined scoring metrics such as precision, recall, and F1 score. # Details: 1. **Custom Scoring Function Implementation:** - Write a function `custom_fbeta_score(y_true, y_pred, beta)` that computes the F-beta score. - The function should accept the true labels (`y_true`), predicted labels (`y_pred`), and the beta value (`beta`). 2. **Model Training and Evaluation:** - Use the `LogisticRegression` model from scikit-learn. - Split the provided dataset into training and test sets using an 80-20 split. - Use `cross_val_score` to evaluate the model using `custom_fbeta_score` with beta values of 0.5, 1, and 2. - Also, evaluate the model using predefined scoring metrics: precision, recall, and F1 score. 3. **Comparison:** - Print the cross-validation scores for `custom_fbeta_score` with different beta values. - Print the cross-validation scores for precision, recall, and F1 score. - Provide a brief analysis comparing the custom F-beta scores with precision, recall, and F1 scores. # Constraints: - You can assume that the input data is already preprocessed. - Use a random_state of 42 for train-test split and model initialization to ensure reproducibility. - You should use `make_scorer` from scikit-learn to create the custom scorer for `cross_val_score`. # Input: - X: A numpy array or pandas DataFrame of shape (n_samples, n_features). - y: A numpy array or pandas Series of shape (n_samples,). # Output: - Cross-validation scores for `custom_fbeta_score` with beta values of 0.5, 1, and 2. - Cross-validation scores for precision, recall, and F1 score. - A comparison of the results in terms of model performance. # Example: ```python import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, cross_val_score from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score # Example dataset X = np.random.rand(100, 10) y = np.random.randint(0, 2, 100) # 1. Custom F-beta score implementation def custom_fbeta_score(y_true, y_pred, beta): tp = np.sum((y_true == 1) & (y_pred == 1)) fp = np.sum((y_true == 0) & (y_pred == 1)) fn = np.sum((y_true == 1) & (y_pred == 0)) precision = tp / (tp + fp) if (tp + fp) != 0 else 0 recall = tp / (tp + fn) if (tp + fn) != 0 else 0 beta_squared = beta ** 2 fbeta = (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall) if (precision + recall) != 0 else 0 return fbeta # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Model model = LogisticRegression(random_state=42) # Custom F-beta scorer for beta in [0.5, 1, 2]: scorer = make_scorer(custom_fbeta_score, beta=beta) scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scorer) print(f\\"F-beta (beta={beta}) Scores: {scores}\\") # Predefined scoring metrics precision_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'precision\') recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'recall\') f1_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'f1\') print(f\\"Precision Scores: {precision_scores}\\") print(f\\"Recall Scores: {recall_scores}\\") print(f\\"F1 Scores: {f1_scores}\\") # Analysis # Compare the custom F-beta scores with precision, recall, and F1 scores. ``` # Note: Ensure to handle edge cases such as zero division errors in your custom scoring function.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, cross_val_score from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score def custom_fbeta_score(y_true, y_pred, beta): Calculate the F-beta score for binary classification. y_true: array-like of shape (n_samples,) True labels. y_pred: array-like of shape (n_samples,) Predicted labels. beta: float Beta value to weigh precision and recall. Returns: fbeta: float The F-beta score. tp = np.sum((y_true == 1) & (y_pred == 1)) fp = np.sum((y_true == 0) & (y_pred == 1)) fn = np.sum((y_true == 1) & (y_pred == 0)) precision = tp / (tp + fp) if (tp + fp) != 0 else 0 recall = tp / (tp + fn) if (tp + fn) != 0 else 0 beta_squared = beta ** 2 fbeta = (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall) if (precision + recall) != 0 else 0 return fbeta # Example dataset X = np.random.rand(100, 10) y = np.random.randint(0, 2, 100) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Model model = LogisticRegression(random_state=42) # Custom F-beta scorer for beta in [0.5, 1, 2]: scorer = make_scorer(custom_fbeta_score, beta=beta) scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scorer) print(f\\"F-beta (beta={beta}) Scores: {scores}\\") # Predefined scoring metrics precision_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'precision\') recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'recall\') f1_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'f1\') print(f\\"Precision Scores: {precision_scores}\\") print(f\\"Recall Scores: {recall_scores}\\") print(f\\"F1 Scores: {f1_scores}\\") # Analysis # Compare the custom F-beta scores with precision, recall, and F1 scores."},{"question":"# Advanced Python Serialization and Deserialization Problem Objective The purpose of this task is to assess your comprehension of the `pickle` module in Python, specifically focusing on customizing the pickling process through handling complex objects and user-defined classes. Problem Statement You are required to implement a class `CustomObject` that contains various data attributes, including primitive data types, collections, and another nested class objects. You will demonstrate the serialization and deserialization process using the `pickle` module and manage custom behavior during this process. Requirements 1. Implement a class `CustomObject` with the following attributes: - `id`: An integer representing the unique identifier. - `name`: A string representing the name. - `data`: A dictionary with string keys and list-of-integer values. - `nested`: An instance of a nested class `NestedObject`. 2. Implement a nested class `NestedObject` with the following attributes: - `nested_id`: An integer representing the nested unique identifier. - `description`: A string providing some description. 3. Implement two methods in `CustomObject`: - `__reduce__()`: Customize the pickling process for `CustomObject`. - `__setstate__(state)`: Define how to restore class instances during unpickling. 4. Add another method in `CustomObject`: - `pretty_print()`: Print a formatted string of all the attributes for easy reading. Input and Output - **Input**: There are no direct inputs. The classes will be instantiated, and objects will be serialized and deserialized within the provided code structure. - **Output**: 1. Print statements indicating successful pickling and unpickling processes. 2. A pretty-printed output of the deserialized object\'s attributes. Example ```python import pickle # Define NestedObject class with required attributes class NestedObject: def __init__(self, nested_id, description): self.nested_id = nested_id self.description = description # Define CustomObject class with required attributes and methods class CustomObject: def __init__(self, id, name, data, nested): self.id = id self.name = name self.data = data self.nested = nested def __reduce__(self): return (self.__class__, (self.id, self.name, self.data, self.nested)) def __setstate__(self, state): self.__init__(*state) def pretty_print(self): print(f\\"ID: {self.id}\\") print(f\\"Name: {self.name}\\") print(f\\"Data: {self.data}\\") print(f\\"Nested Object -> ID: {self.nested.nested_id}, Description: {self.nested.description}\\") # Example usage of the CustomObject and NestedObject if __name__ == \\"__main__\\": nested_obj = NestedObject(100, \\"This is a nested object\\") custom_obj = CustomObject(1, \\"My Custom Object\\", {\\"key1\\": [1, 2, 3], \\"key2\\": [4, 5, 6]}, nested_obj) # Serialize the custom object pickled_obj = pickle.dumps(custom_obj) print(\\"Object has been pickled successfully!\\") # Deserialize the custom object unpickled_obj = pickle.loads(pickled_obj) print(\\"Object has been unpickled successfully!\\") # Pretty print the unpickled object unpickled_obj.pretty_print() ``` Constraints - Ensure that the custom pickling and unpickling handle exceptions gracefully. - The pretty print method should format the output clearly for readability. - Test thoroughly to ensure that nested objects are handled correctly during the serialization and deserialization processes.","solution":"import pickle class NestedObject: def __init__(self, nested_id, description): self.nested_id = nested_id self.description = description class CustomObject: def __init__(self, id, name, data, nested): self.id = id self.name = name self.data = data self.nested = nested def __reduce__(self): return (self.__class__, (self.id, self.name, self.data, self.nested)) def __setstate__(self, state): self.__init__(*state) def pretty_print(self): print(f\\"ID: {self.id}\\") print(f\\"Name: {self.name}\\") print(f\\"Data: {self.data}\\") print(f\\"Nested Object -> ID: {self.nested.nested_id}, Description: {self.nested.description}\\")"},{"question":"**Coding Question:** Implement a custom class in Python that mimics some basic functionalities of the instance method and method objects as described in the provided documentation. Your class should allow for the creation, verification, and retrieval of these methods. # Class Requirements: 1. **Class Name**: `MethodHandler` 2. **Methods**: - `add_instance_method(obj, func)`: Creates an instance method for the given object (`obj`) using the function (`func`) and stores it. - `is_instance_method(obj)`: Verifies if the given object (`obj`) is an instance method. - `get_instance_method_function(obj)`: Retrieves the function associated with the given instance method object. - `add_method(func, instance)`: Creates a bound method for the given instance using the function and stores it. - `is_method(obj)`: Verifies if the given object is a method. - `get_method_function(obj)`: Retrieves the function associated with the given method object. - `get_method_instance(obj)`: Retrieves the instance associated with the given method object. # Additional Constraints: - Your methods should internally manage and handle these objects similarly to how C-API methods handle PyObjects. - You are not required to perform error checking as in the macro versions of the C-API functions. # Example Usage: ```python handler = MethodHandler() # Define a sample function def sample_function(): return \\"Hello, World!\\" # Create an instance of any class (for demonstration) class MyClass: pass my_instance = MyClass() # Add instance method handler.add_instance_method(my_instance, sample_function) assert handler.is_instance_method(my_instance) == True assert handler.get_instance_method_function(my_instance)() == \\"Hello, World!\\" # Add bound method handler.add_method(sample_function, my_instance) assert handler.is_method(sample_function) == True assert handler.get_method_function(sample_function)() == \\"Hello, World!\\" assert handler.get_method_instance(sample_function) == my_instance ``` # Expected Output: Ensure your implementation passes the above example usage and the assertions without errors.","solution":"from types import MethodType class MethodHandler: def __init__(self): self.instance_methods = {} self.methods = {} def add_instance_method(self, obj, func): method = MethodType(func, obj) self.instance_methods[obj] = method def is_instance_method(self, obj): return obj in self.instance_methods def get_instance_method_function(self, obj): if self.is_instance_method(obj): return self.instance_methods[obj].__func__ return None def add_method(self, func, instance): method = MethodType(func, instance) self.methods[func] = method def is_method(self, obj): return obj in self.methods def get_method_function(self, obj): if self.is_method(obj): return self.methods[obj].__func__ return None def get_method_instance(self, obj): if self.is_method(obj): return self.methods[obj].__self__ return None"},{"question":"# Covariance Estimation and Outlier Detection using Scikit-Learn Problem Description: You are provided with a dataset containing financial data. Your task is to estimate the covariance matrix using two methods and discuss the impact of outliers: 1. Empirical Covariance Estimation. 2. Robust Covariance Estimation (Minimum Covariance Determinant). # Tasks: 1. **Load the dataset**: Use any appropriate dataset of your choice with at least 50 samples and 5 features. 2. **Fit Empirical Covariance**: - Compute the empirical covariance matrix using `EmpiricalCovariance`. - Report the covariance matrix. 3. **Introduce Outliers**: - Introduce a set of outliers to the dataset (e.g., by adding extreme values to a small subset of observations). 4. **Fit Robust Covariance**: - Compute the robust covariance matrix using `MinCovDet`. - Report the covariance matrix. 5. **Comparison and Discussion**: - Compare the results of the empirical covariance matrix with the robust covariance matrix. - Visually plot (e.g., using heatmaps) both covariance matrices. - Discuss how the introduced outliers influenced the empirical covariance estimation compared to the robust estimation. Input and Output Requirements: - **Input**: The dataset should be loaded from a CSV file with no missing values. - **Output**: Python script with the following methods: - `load_dataset(file_path)`: Load and return the dataset from a CSV file. - `fit_empirical_covariance(data)`: Fit and return the empirical covariance matrix. - `introduce_outliers(data, num_outliers, multiplier)`: Introduce outliers to the data and return the modified dataset. - `fit_robust_covariance(data)`: Fit and return the robust covariance matrix. - `plot_covariances(emp_cov, robust_cov)`: Plot heatmaps of both covariance matrices. Example: ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.covariance import EmpiricalCovariance, MinCovDet import seaborn as sns def load_dataset(file_path): return pd.read_csv(file_path) def fit_empirical_covariance(data): emp_cov = EmpiricalCovariance().fit(data) return emp_cov.covariance_ def introduce_outliers(data, num_outliers, multiplier): data_with_outliers = data.copy() indices = np.random.choice(data.index, num_outliers, replace=False) data_with_outliers.loc[indices] = data_with_outliers.loc[indices] * multiplier return data_with_outliers def fit_robust_covariance(data): robust_cov = MinCovDet().fit(data) return robust_cov.covariance_ def plot_covariances(emp_cov, robust_cov): fig, ax = plt.subplots(1, 2, figsize=(12, 6)) sns.heatmap(emp_cov, ax=ax[0], cmap=\'viridis\') ax[0].set_title(\'Empirical Covariance Matrix\') sns.heatmap(robust_cov, ax=ax[1], cmap=\'viridis\') ax[1].set_title(\'Robust Covariance Matrix\') plt.show() # Example execution file_path = \'your_dataset.csv\' data = load_dataset(file_path) emp_cov = fit_empirical_covariance(data) data_with_outliers = introduce_outliers(data, num_outliers=5, multiplier=10) robust_cov = fit_robust_covariance(data_with_outliers) plot_covariances(emp_cov, robust_cov) ``` **Constraints**: - Ensure the dataset has no missing values. - Introduce at least 5 outliers. - Multiplier for outliers should be at least 10 times the normal values.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.covariance import EmpiricalCovariance, MinCovDet import seaborn as sns def load_dataset(file_path): Load dataset from a CSV file. Args: - file_path (str): Path to the CSV file. Returns: - pd.DataFrame: Loaded dataset. return pd.read_csv(file_path) def fit_empirical_covariance(data): Fit and return the empirical covariance matrix. Args: - data (pd.DataFrame): Input data. Returns: - np.ndarray: Empirical covariance matrix. emp_cov = EmpiricalCovariance().fit(data) return emp_cov.covariance_ def introduce_outliers(data, num_outliers, multiplier): Introduce outliers to the data by multiplying a set of samples by a given multiplier. Args: - data (pd.DataFrame): Input data. - num_outliers (int): Number of outliers to introduce. - multiplier (int): Multiplier to create outliers. Returns: - pd.DataFrame: Data with introduced outliers. data_with_outliers = data.copy() indices = np.random.choice(data.index, num_outliers, replace=False) data_with_outliers.loc[indices] = data_with_outliers.loc[indices] * multiplier return data_with_outliers def fit_robust_covariance(data): Fit and return the robust covariance matrix using the Minimum Covariance Determinant. Args: - data (pd.DataFrame): Input data. Returns: - np.ndarray: Robust covariance matrix. robust_cov = MinCovDet().fit(data) return robust_cov.covariance_ def plot_covariances(emp_cov, robust_cov): Plot heatmaps for both empirical and robust covariance matrices. Args: - emp_cov (np.ndarray): Empirical covariance matrix. - robust_cov (np.ndarray): Robust covariance matrix. fig, ax = plt.subplots(1, 2, figsize=(12, 6)) sns.heatmap(emp_cov, ax=ax[0], annot=True, fmt=\'g\', cmap=\'viridis\') ax[0].set_title(\'Empirical Covariance Matrix\') sns.heatmap(robust_cov, ax=ax[1], annot=True, fmt=\'g\', cmap=\'viridis\') ax[1].set_title(\'Robust Covariance Matrix\') plt.show() # Example execution # file_path = \'your_dataset.csv\' # data = load_dataset(file_path) # emp_cov = fit_empirical_covariance(data) # data_with_outliers = introduce_outliers(data, num_outliers=5, multiplier=10) # robust_cov = fit_robust_covariance(data_with_outliers) # plot_covariances(emp_cov, robust_cov)"},{"question":"**Question: Manage Environment Variables** In this assessment, you are required to write a Python function that uses the `os` module to manage environment variables. The task includes: 1. **Setting an Environment Variable:** Write a function, `set_env_var`, that accepts two arguments: a key (string) and a value (string). The function should set the environment variable with the given key and value. 2. **Getting an Environment Variable:** Write a function, `get_env_var`, that accepts one argument: a key (string). The function should return the value of the environment variable corresponding to the key. If the key does not exist, return `None`. 3. **Delete an Environment Variable:** Write a function, `del_env_var`, that accepts one argument: a key (string). The function should delete the environment variable with the given key if it exists. If the key does not exist, the function should do nothing. 4. **List All Environment Variables:** Write a function, `list_env_vars`, that returns all the environment variables as a dictionary. # Input and Output Formats - `set_env_var(key: str, value: str) -> None` - **Input:** key (string), value (string) - **Output:** None - `get_env_var(key: str) -> Optional[str]` - **Input:** key (string) - **Output:** value (string) or `None` - `del_env_var(key: str) -> None` - **Input:** key (string) - **Output:** None - `list_env_vars() -> Dict[str, str]` - **Input:** None - **Output:** Dictionary of environment variables # Constraints 1. You are not allowed to use the `posix` module directly. 2. Use the `os` module for all interactions with environment variables. 3. Assume appropriate input types are provided (key and value as strings). # Example Usage ```python import os # Setting environment variables set_env_var(\'TEST_VAR\', \'123\') print(get_env_var(\'TEST_VAR\')) # Output: \'123\' # Listing all environment variables env_vars = list_env_vars() print(\'TEST_VAR\' in env_vars) # Output: True # Deleting an environment variable del_env_var(\'TEST_VAR\') print(get_env_var(\'TEST_VAR\')) # Output: None print(\'TEST_VAR\' in env_vars) # Output: False ``` # Complete the following code: ```python import os from typing import Optional, Dict def set_env_var(key: str, value: str) -> None: # Your code here def get_env_var(key: str) -> Optional[str]: # Your code here def del_env_var(key: str) -> None: # Your code here def list_env_vars() -> Dict[str, str]: # Your code here ```","solution":"import os from typing import Optional, Dict def set_env_var(key: str, value: str) -> None: os.environ[key] = value def get_env_var(key: str) -> Optional[str]: return os.environ.get(key) def del_env_var(key: str) -> None: if key in os.environ: del os.environ[key] def list_env_vars() -> Dict[str, str]: return dict(os.environ)"},{"question":"# Priority Task Manager with Dynamic Priority Update Objective Create a Priority Task Manager that can handle the addition, removal, and priority updating of tasks dynamically. Each task is associated with a specific priority, and the task with the highest priority (smallest value) should always be processed first. Details You are required to design a class `PriorityTaskManager` that supports the following operations: 1. **add_task(task: str, priority: int) -> None**: - Adds a new task with the given priority to the manager. - If the task already exists, it should update its priority. 2. **remove_task(task: str) -> None**: - Marks a task as removed. If the task does not exist, raise a KeyError. 3. **pop_task() -> str**: - Removes and returns the task with the highest priority. - If no tasks are available, raise a KeyError. 4. **peek_task() -> str**: - Returns the task with the highest priority without removing it. - If no tasks are available, raise a KeyError. Constraints - The task names are unique strings. - The priority is an integer where a smaller value indicates a higher priority. - Performance should be efficient in terms of time complexity for all operations. Example ```python from heapq import heappush, heappop import itertools class PriorityTaskManager: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: str, priority: int) -> None: Add a new task or update the priority of an existing task. if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task: str) -> None: Mark an existing task as REMOVED. Raise KeyError if not found. if task not in self.entry_finder: raise KeyError(f\'Task {task} not found\') entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the lowest priority task. Raise KeyError if empty. while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def peek_task(self) -> str: Return the lowest priority task without removing it. Raise KeyError if empty. while self.pq: priority, count, task = self.pq[0] if task is not self.REMOVED: return task heappop(self.pq) # remove the top entry if it\'s marked as REMOVED raise KeyError(\'peek from an empty priority queue\') # Example of usage: manager = PriorityTaskManager() manager.add_task(\'task1\', 5) manager.add_task(\'task2\', 3) print(manager.peek_task()) # Output: \'task2\' manager.add_task(\'task3\', 1) manager.pop_task() # Output: \'task3\' manager.remove_task(\'task1\') print(manager.pop_task()) # Output: \'task2\' ``` Implement the `PriorityTaskManager` class based on the above specification.","solution":"from heapq import heappush, heappop import itertools class PriorityTaskManager: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: str, priority: int) -> None: Add a new task or update the priority of an existing task. if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task: str) -> None: Mark an existing task as REMOVED. Raise KeyError if not found. if task not in self.entry_finder: raise KeyError(f\'Task {task} not found\') entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the lowest priority task. Raise KeyError if empty. while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def peek_task(self) -> str: Return the lowest priority task without removing it. Raise KeyError if empty. while self.pq: priority, count, task = self.pq[0] if task is not self.REMOVED: return task heappop(self.pq) # remove the top entry if it\'s marked as REMOVED raise KeyError(\'peek from an empty priority queue\')"},{"question":"# Python Coding Assessment Objective Your task is to implement a function that simulates file operations and handles possible system errors using the `errno` module. Instructions 1. Implement a function `perform_file_operation(filepath, operation)` that performs a specified operation on a file. 2. The `operation` parameter can be one of the following strings: `\'open\'`, `\'read\'`, `\'write\'`, `\'close\'`, and `\'delete\'`. 3. Depending on the operation, different system errors might be simulated. 4. The function should handle specific errors using the `errno` module and provide meaningful messages to the user. Expected Input and Output Formats - **Input:** - `filepath` (str): The path of the file to be operated on. - `operation` (str): The operation to be performed on the file (one of `\'open\'`, `\'read\'`, `\'write\'`, `\'close\'`, `\'delete\'`). - **Output:** - A string message indicating the result of the operation or the handled error. Constraints - You must use the `errno` module to simulate and handle the errors. - Use the `errno.errorcode` dictionary to translate error numbers to human-readable error names. - Use exception handling to manage and simulate these errors appropriately. Example Usage ```python def perform_file_operation(filepath, operation): # Your implementation here # Example 1 print(perform_file_operation(\'/nonexistent/file.txt\', \'open\')) # Output: \\"Error: No such file or directory (ENOENT)\\" # Example 2 print(perform_file_operation(\'/readonly/file.txt\', \'write\')) # Output: \\"Error: Permission denied (EACCES)\\" # Example 3 print(perform_file_operation(\'/correct/path/file.txt\', \'close\')) # Output: \\"Operation \'close\' completed successfully.\\" ``` Function Signature ```python def perform_file_operation(filepath: str, operation: str) -> str: pass ``` Notes - Consider edge cases such as invalid file paths or unsupported operations. - Ensure proper error handling to provide clear and informative messages. - Simulate various system errors using exceptions that map to `errno` codes.","solution":"import errno def perform_file_operation(filepath, operation): try: # Simulate different errors based on operation if operation == \'open\': raise OSError(errno.ENOENT, \\"No such file or directory\\") elif operation == \'read\': # Simulating read operation, let\'s assume it passes for now. pass elif operation == \'write\': raise OSError(errno.EACCES, \\"Permission denied\\") elif operation == \'close\': # Simulating close operation, let\'s assume it passes for now. pass elif operation == \'delete\': raise OSError(errno.EPERM, \\"Operation not permitted\\") else: return f\\"Unsupported operation \'{operation}\'\\" # If no exception was raised, the operation is successful return f\\"Operation \'{operation}\' completed successfully.\\" except OSError as e: return f\\"Error: {e.strerror} ({errno.errorcode[e.errno]})\\""},{"question":"**Advanced File Handling with Python C API** # Objective You are required to demonstrate your understanding of the Python file APIs by implementing a series of functions that perform file operations using the given C API functions. # Task 1. **Creating Custom File Handlers:** Implement a function `create_custom_file_handler(fd)` which: - Takes a file descriptor `fd` (an integer) as input. - Creates and returns a Python file object using `PyFile_FromFd`. 2. **Reading a Specific Number of Bytes:** Implement a function `read_n_bytes(file_obj, n)` which: - Takes a Python file object `file_obj` and an integer `n`. - Utilizes `PyFile_GetLine` to read and return no more than `n` bytes from the file. If `n` is less than `0`, read one line regardless of its length. 3. **Writing Arbitrary Objects:** Implement a function `write_object_to_file(file_obj, obj)` which: - Takes a Python file object `file_obj` and a Python object `obj`. - Writes the object to the file using `PyFile_WriteObject`. 4. **Writing Strings:** Implement a function `write_string_to_file(file_obj, string)` which: - Takes a Python file object `file_obj` and a string `string`. - Writes the string to the file using `PyFile_WriteString`. # Input and output format - Function `create_custom_file_handler(fd)`: - Input: an integer `fd`. - Output: a Python file object. - Function `read_n_bytes(file_obj, n)`: - Input: Python file object `file_obj` and an integer `n`. - Output: a string containing the read bytes. - Function `write_object_to_file(file_obj, obj)`: - Input: Python file object `file_obj`, Python object `obj`. - Output: None. - Function `write_string_to_file(file_obj, string)`: - Input: Python file object `file_obj`, a string `string`. - Output: None. # Constraints - Use only the provided C API functions for file operations. - Handle all possible exceptions and errors gracefully, setting the appropriate exception messages. - Assume the file descriptors and objects passed are valid and properly initialized. # Example ```python # Assuming these functions are correctly implemented and available fd = os.open(\'test_file.txt\', os.O_RDWR | os.O_CREAT) file_obj = create_custom_file_handler(fd) write_string_to_file(file_obj, \\"Hello, World!n\\") content = read_n_bytes(file_obj, 10) print(content) # \\"Hello, Wor\\" write_object_to_file(file_obj, [1, 2, 3]) os.close(fd) ``` # Additional Notes - Ensure to familiarize yourself with the `io` module and its usage in Python. - Do not import any modules within the hooks implementation. - Document any assumptions or specific behavioral expectations in your implementation.","solution":"import os import io def create_custom_file_handler(fd): Takes a file descriptor `fd` (an integer) as input. Creates and returns a Python file object using `io.open`. return io.open(fd, mode=\'r+\', closefd=False) def read_n_bytes(file_obj, n): Takes a Python file object `file_obj` and an integer `n`. Reads and returns no more than `n` bytes from the file. If `n` is less than 0, read one line regardless of its length. if n < 0: return file_obj.readline() return file_obj.read(n) def write_object_to_file(file_obj, obj): Takes a Python file object `file_obj` and a Python object `obj`. Writes the object to the file. file_obj.write(str(obj)) file_obj.flush() def write_string_to_file(file_obj, string): Takes a Python file object `file_obj` and a string `string`. Writes the string to the file. file_obj.write(string) file_obj.flush()"},{"question":"**Complex Number Operations Class** You are required to create a class `ComplexNumber` that models a complex number and provides various operations on complex numbers. Additionally, you should implement an iterator within the class to iterate over the magnitude and phase angle representations of the complex numbers stored. Objectives: - Implement a class `ComplexNumber` to model a complex number. - Implement various operations on complex numbers by overloading operators. - Implement an iterator to loop over magnitude and phase of all stored complex numbers. - Implement a generator method to yield complex numbers in their polar form. Class Details 1. **ComplexNumber Class Construction**: - `__init__(self, real: float, imag: float)`: Initialize with real and imaginary parts. - `__str__(self)` for string representation: Returns the string representation in the form `a + bi` or `a - bi`. - Implement `__add__` and `__mul__` for addition and multiplication of two complex numbers. - Implement `magnitude(self)` that returns the magnitude of the complex number. - Implement `phase(self)` that returns the phase (or angle) of the complex number in radians. 2. **Iterator Implementation**: - Implement an iterator within the `ComplexNumber` class to iterate over magnitude and phase angle representations of the complex numbers. - The iterator should yield tuples of the form (`magnitude`, `phase`). 3. **Generator Method**: - Implement a generator method `polar_representation(self)` which yields tuples of the form `(magnitude, phase)`for every stored complex number. Example Usage ```python c1 = ComplexNumber(3, 4) c2 = ComplexNumber(1, -1) c3 = ComplexNumber(0, 1) print(c1) # Output: 3 + 4i print(c2) # Output: 1 - 1i print(c3) # Output: 0 + 1i c4 = c1 + c2 print(c4) # Output: 4 + 3i c5 = c1 * c3 print(c5) # Output: -4 + 3i print(c1.magnitude()) # Output: 5.0 print(c1.phase()) # Output: 0.643501109 for magnitude, phase in c1: print(f\'Magnitude: {magnitude}, Phase Angle: {phase}\') # Using the generator method for mag, ang in c1.polar_representation(): print(f\'Polar: ({mag}, {ang})\') ``` Constraints: - Ensure the class is robust and handles edge cases (like the magnitude and phase of zero). - Necessary imports should be from standard libraries only. Submission Requirements: - Your solution should be a single Python file (.py extension). - The file should contain no input/output statements other than the class definition and any helper functions within the class. **Note**: Your implementation should demonstrate correct use of class construction, inheritance, iterators, and generators as outlined in the provided documentation.","solution":"import math class ComplexNumber: def __init__(self, real: float, imag: float): self.real = real self.imag = imag self._index = 0 # For iterator implementation def __str__(self): real_part = f\\"{self.real}\\" imag_part = f\\"{self.imag}i\\" if self.imag >= 0: return f\\"{real_part} + {imag_part}\\" else: return f\\"{real_part} - {abs(self.imag)}i\\" def __add__(self, other): return ComplexNumber(self.real + other.real, self.imag + other.imag) def __mul__(self, other): real_part = self.real * other.real - self.imag * other.imag imag_part = self.real * other.imag + self.imag * other.real return ComplexNumber(real_part, imag_part) def magnitude(self): return math.sqrt(self.real ** 2 + self.imag ** 2) def phase(self): return math.atan2(self.imag, self.real) def __iter__(self): return self def __next__(self): if self._index > 1: self._index = 0 # Reset for possible reuse raise StopIteration if self._index == 0: self._index += 1 return (\'magnitude\', self.magnitude()) if self._index == 1: self._index += 1 return (\'phase\', self.phase()) def polar_representation(self): yield (\'magnitude\', self.magnitude()) yield (\'phase\', self.phase())"},{"question":"# Problem: Implementing a File-Based Task Tracker Problem Description You are required to implement a **Task Tracker** that allows users to manage their daily tasks efficiently using Python. The task tracker should support the following functionalities: 1. **Add a Task**: Add a new task with a title and description. 2. **Complete a Task**: Mark a task as completed. 3. **Remove a Task**: Remove a task from the tracker. 4. **List Tasks**: List all tasks with their status, showing completed and pending tasks separately. 5. **Save Tasks to File**: Save the current state of tasks to a JSON file. 6. **Load Tasks from File**: Load tasks from a previously saved JSON file. Input and Output * Your solution should define a `TaskTracker` class with the following methods: - `add_task(title: str, description: str) -> None`: Adds a new task. - `complete_task(task_id: int) -> None`: Marks a task as completed by its task ID. - `remove_task(task_id: int) -> None`: Removes a task by its task ID. - `list_tasks() -> dict`: Returns a dictionary with two keys `pending` and `completed`, each containing a list of task details. - `save_to_file(file_path: str) -> None`: Saves all tasks to a specified file in JSON format. - `load_from_file(file_path: str) -> None`: Loads tasks from a specified file in JSON format. * Task details for listing and persistence should include: - `id`: Unique integer ID for the task. - `title`: Title of the task. - `description`: Description of the task. - `status`: Status of the task (`\\"pending\\"` or `\\"completed\\"`). * Task IDs should start from 1 and increment by 1 for each new task. Constraints - The tracker should handle file read/write exceptions gracefully and print meaningful error messages. - Task titles and descriptions are non-empty strings with a maximum length of 100 characters. - You can use the `json` module to handle JSON files. Example ```python # Example Usage tracker = TaskTracker() tracker.add_task(\\"Buy groceries\\", \\"Milk, Bread, Eggs\\") tracker.add_task(\\"Complete assignment\\", \\"Finish the science assignment\\") tracker.complete_task(1) tracker.list_tasks() # Output: # { # \\"pending\\": [{\\"id\\": 2, \\"title\\": \\"Complete assignment\\", \\"description\\": \\"Finish the science assignment\\", \\"status\\": \\"pending\\"}], # \\"completed\\": [{\\"id\\": 1, \\"title\\": \\"Buy groceries\\", \\"description\\": \\"Milk, Bread, Eggs\\", \\"status\\": \\"completed\\"}] # } tracker.save_to_file(\\"tasks.json\\") tracker.remove_task(2) tracker.load_from_file(\\"tasks.json\\") tracker.list_tasks() # Output (after loading): # { # \\"pending\\": [{\\"id\\": 2, \\"title\\": \\"Complete assignment\\", \\"description\\": \\"Finish the science assignment\\", \\"status\\": \\"pending\\"}], # \\"completed\\": [{\\"id\\": 1, \\"title\\": \\"Buy groceries\\", \\"description\\": \\"Milk, Bread, Eggs\\", \\"status\\": \\"completed\\"}] # } ``` Implementation Notes - Use exception handling to manage file operations and provide feedback for file-related errors. - Ensure that tasks maintain their IDs when saved and loaded from a file. - Consider edge cases such as adding a task with an empty title or description, and handle them gracefully.","solution":"import json class TaskTracker: def __init__(self): self.tasks = [] self.next_id = 1 def add_task(self, title: str, description: str) -> None: if not title or not description or len(title) > 100 or len(description) > 100: raise ValueError(\\"Task title and description must be non-empty strings with maximum length of 100 characters.\\") task = { \\"id\\": self.next_id, \\"title\\": title, \\"description\\": description, \\"status\\": \\"pending\\" } self.tasks.append(task) self.next_id += 1 def complete_task(self, task_id: int) -> None: for task in self.tasks: if task[\\"id\\"] == task_id: task[\\"status\\"] = \\"completed\\" return raise ValueError(f\\"No task found with ID {task_id}\\") def remove_task(self, task_id: int) -> None: for task in self.tasks: if task[\\"id\\"] == task_id: self.tasks.remove(task) return raise ValueError(f\\"No task found with ID {task_id}\\") def list_tasks(self) -> dict: result = {\\"pending\\": [], \\"completed\\": []} for task in self.tasks: if task[\\"status\\"] == \\"pending\\": result[\\"pending\\"].append(task) else: result[\\"completed\\"].append(task) return result def save_to_file(self, file_path: str) -> None: try: with open(file_path, \'w\') as file: json.dump({\\"tasks\\": self.tasks, \\"next_id\\": self.next_id}, file) except IOError as e: print(f\\"An error occurred while saving to file: {e}\\") def load_from_file(self, file_path: str) -> None: try: with open(file_path, \'r\') as file: data = json.load(file) self.tasks = data[\\"tasks\\"] self.next_id = data[\\"next_id\\"] except IOError as e: print(f\\"An error occurred while loading from file: {e}\\") except json.JSONDecodeError as e: print(f\\"An error occurred while decoding JSON: {e}\\")"},{"question":"**Question: Advanced Customization and Visualization with Seaborn Color Palettes** You are tasked with creating a series of visualizations using the seaborn library. Your goal is to demonstrate an understanding of various color palettes offered by seaborn and applying them in different plotting contexts. # Requirements 1. **Generate Custom Palettes**: - Create three distinct custom color palettes using seaborn\'s `color_palette` function. The palettes must be: 1. A categorical palette of your choice with at least 6 colors. 2. A perceptually-uniform colormap. 3. A dark sequential gradient. - Display each palette as a bar plot using seaborn\'s `palplot` function. 2. **Application of Palettes**: - Create a dataset suitable for a scatter plot. For this task, generate a dataset with two numerical columns and one categorical column. - Plot the dataset using seaborn\'s `scatterplot` function, applying each of the three created palettes to the `hue` parameter. Ensure each plot clearly shows the use of the palette. 3. **Context Management**: - Using seaborn\'s context management, create a box plot of the same dataset with a fourth custom qualitative color palette (not one of the three previously created). Change the default categorical color palette only for the duration of this plot. 4. **Hex Code Representation**: - For each of the four color palettes created, print out their corresponding hex codes. # Input - No direct input. You will be generating datasets and color palettes within the code. # Output - Three bar plots showing the distinct custom color palettes. - Three scatter plots, each using one of the created palettes. - One box plot using a custom qualitative color palette with seaborn\'s context management. - Printed hex codes for each of the four color palettes. # Constraints - Use only seaborn and matplotlib for plotting. - Generated datasets should have at least 50 data points. # Example of Expected Solution Structure ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np # Step 1: Generate Custom Palettes categorical_palette = sns.color_palette(\\"Set2\\", 6) perceptual_palette = sns.color_palette(\\"flare\\", as_cmap=True) dark_gradient = sns.color_palette(\\"dark:#5A9_r\\", as_cmap=True) # Display Palettes sns.palplot(categorical_palette) sns.palplot(sns.color_palette(\\"flare\\")) sns.palplot(sns.color_palette(\\"dark:#5A9_r\\")) # Step 2: Create Dataset np.random.seed(42) data = pd.DataFrame({ \\"x\\": np.random.randn(50), \\"y\\": np.random.randn(50), \\"category\\": np.random.choice([\\"A\\", \\"B\\", \\"C\\"], 50) }) # Scatter Plots with Different Palettes sns.scatterplot(data=data, x=\\"x\\", y=\\"y\\", hue=\\"category\\", palette=categorical_palette) sns.scatterplot(data=data, x=\\"x\\", y=\\"y\\", hue=\\"category\\", palette=\\"flare\\") sns.scatterplot(data=data, x=\\"x\\", y=\\"y\\", hue=\\"category\\", palette=\\"dark:#5A9_r\\") # Step 3: Context Management for Box Plot with sns.color_palette(\\"pastel\\"): sns.boxplot(data=data, x=\\"category\\", y=\\"y\\") # Step 4: Print Hex Codes print(categorical_palette.as_hex()) print(sns.color_palette(\\"flare\\").as_hex()) print(sns.color_palette(\\"dark:#5A9_r\\").as_hex()) print(sns.color_palette(\\"pastel\\").as_hex()) plt.show() ``` Good luck, and demonstrate your seaborn mastery!","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np # Step 1: Generate Custom Palettes categorical_palette = sns.color_palette(\\"Set2\\", 6) perceptual_palette = sns.color_palette(\\"cubehelix\\", as_cmap=True) dark_gradient = sns.color_palette(\\"dark:#5A9_r\\", n_colors=6) # Display Palettes plt.figure(figsize=(12, 2)) sns.palplot(categorical_palette) plt.title(\\"Categorical Palette (Set2)\\") plt.figure(figsize=(12, 2)) sns.palplot(sns.color_palette(\\"cubehelix\\", 6)) plt.title(\\"Perceptually-Uniform Colormap (cubehelix)\\") plt.figure(figsize=(12, 2)) sns.palplot(dark_gradient) plt.title(\\"Dark Sequential Gradient\\") # Step 2: Create Dataset np.random.seed(42) data = pd.DataFrame({ \\"x\\": np.random.randn(50), \\"y\\": np.random.randn(50), \\"category\\": np.random.choice([\\"A\\", \\"B\\", \\"C\\"], 50) }) # Scatter Plots with Different Palettes plt.figure(figsize=(8, 6)) sns.scatterplot(data=data, x=\\"x\\", y=\\"y\\", hue=\\"category\\", palette=categorical_palette) plt.title(\\"Scatter Plot with Categorical Palette\\") plt.figure(figsize=(8, 6)) sns.scatterplot(data=data, x=\\"x\\", y=\\"y\\", hue=\\"category\\", palette=\\"cubehelix\\") plt.title(\\"Scatter Plot with Perceptually-Uniform Colormap\\") plt.figure(figsize=(8, 6)) sns.scatterplot(data=data, x=\\"x\\", y=\\"y\\", hue=\\"category\\", palette=\\"dark:#5A9_r\\") plt.title(\\"Scatter Plot with Dark Sequential Gradient\\") # Step 3: Context Management for Box Plot with sns.color_palette(\\"pastel\\"): plt.figure(figsize=(8, 6)) sns.boxplot(data=data, x=\\"category\\", y=\\"y\\") plt.title(\\"Box Plot with Custom Qualitative Palette (Pastel)\\") # Step 4: Print Hex Codes print(\\"Categorical Palette Hex Codes:\\", categorical_palette.as_hex()) print(\\"Perceptually-Uniform Colormap Hex Codes:\\", sns.color_palette(\\"cubehelix\\", 6).as_hex()) print(\\"Dark Sequential Gradient Hex Codes:\\", dark_gradient.as_hex()) print(\\"Custom Qualitative Palette Hex Codes:\\", sns.color_palette(\\"pastel\\").as_hex()) plt.show()"},{"question":"You are tasked with creating a class `PyNumber` in Python that mimics some of the functionality described in the provided documentation. This class should include methods for addition, subtraction, multiplication, and true division. Each method should handle errors gracefully and return appropriate error messages in case of failure. # Class Specification ```python class PyNumber: def __init__(self, value): Initialize the object with a numeric value. TypeError should be raised if the value is not a numeric type. pass def add(self, other): Returns a new PyNumber object representing the sum of self and other. If the operation fails, return the string \\"Addition failed\\". pass def subtract(self, other): Returns a new PyNumber object representing the difference of self minus other. If the operation fails, return the string \\"Subtraction failed\\". pass def multiply(self, other): Returns a new PyNumber object representing the product of self and other. If the operation fails, return the string \\"Multiplication failed\\". pass def true_divide(self, other): Returns a new PyNumber object representing the division of self by other. If the operation fails, return the string \\"Division failed\\". pass ``` # Example Usage ```python try: num1 = PyNumber(10) num2 = PyNumber(5.5) except TypeError as e: print(e) # TypeError: Value must be a numeric type result = num1.add(num2) print(result.value) # Should output 15.5 result = num1.subtract(num2) print(result.value) # Should output 4.5 result = num1.multiply(num2) print(result.value) # Should output 55.0 result = num1.true_divide(num2) print(result.value) # Should output 1.8181818181818181 ``` # Constraints 1. The class should handle only numeric inputs (integers and floats). 2. If a non-numeric value is passed to the initializer, a `TypeError` should be raised. 3. If any arithmetic operation fails, the method should return the appropriate error message string. # Evaluation Your implementation will be assessed on: 1. Correct handling of numeric and non-numeric inputs. 2. Correct arithmetic operations. 3. Appropriate error handling and messages. 4. Code readability and structure.","solution":"class PyNumber: def __init__(self, value): Initialize the object with a numeric value. TypeError should be raised if the value is not a numeric type. if not isinstance(value, (int, float)): raise TypeError(\\"Value must be a numeric type\\") self.value = value def add(self, other): Returns a new PyNumber object representing the sum of self and other. If the operation fails, return the string \\"Addition failed\\". try: return PyNumber(self.value + other.value) except Exception: return \\"Addition failed\\" def subtract(self, other): Returns a new PyNumber object representing the difference of self minus other. If the operation fails, return the string \\"Subtraction failed\\". try: return PyNumber(self.value - other.value) except Exception: return \\"Subtraction failed\\" def multiply(self, other): Returns a new PyNumber object representing the product of self and other. If the operation fails, return the string \\"Multiplication failed\\". try: return PyNumber(self.value * other.value) except Exception: return \\"Multiplication failed\\" def true_divide(self, other): Returns a new PyNumber object representing the division of self by other. If the operation fails, return the string \\"Division failed\\". try: if other.value == 0: return \\"Division failed\\" return PyNumber(self.value / other.value) except Exception: return \\"Division failed\\""},{"question":"**Coding Assessment Question:** # Task You are tasked with processing a list of operations on various types of data (integers, strings, lists) using the functions provided by the \\"operator\\" module. You need to create a function `process_operations` that takes a list of operations and performs them sequentially to produce a result. # Specifications - **Function Name:** `process_operations` - **Input:** A list of tuples where each tuple represents an operation. The first element of the tuple is a string representing the type of operation (e.g., `\\"add\\"`, `\\"mul\\"`, `\\"concat\\"`, `\\"getitem\\"`, etc.), and the subsequent elements are the arguments for that operation. - **Output:** The final result after performing all the operations sequentially. # Constraints 1. Each operation in the list will have a corresponding function in the `operator` module. 2. The operations must be performed sequentially using the result of the previous operation as the first argument of the next operation when applicable. 3. The list will contain at least one operation. 4. The input operations will be valid and will not result in any errors. # Example ```python import operator def process_operations(ops): result = None for op in ops: operation = getattr(operator, op[0]) if result is None: result = operation(*op[1:]) else: result = operation(result, *op[1:]) return result # Example usage: operations = [ (\'add\', 1, 2), (\'mul\', 3), (\'sub\', 4), (\'truediv\', 2) ] print(process_operations(operations)) # Output should be 2.5 operations = [ (\'concat\', \'Hello, \', \'World!\'), (\'getitem\', slice(0, 5)) ] print(process_operations(operations)) # Output should be \'Hello\' operations = [ (\'setitem\', [1, 2, 3], 1, 4), (\'getitem\', [1, 2, 3], 1), ] print(process_operations(operations)) # Output should be 4 ``` # Note - Use the functions from the `operator` module to perform each operation. - Handle both single and multiple arguments as needed by the operations. - Assume all inputs follow the above constraints and no invalid inputs will be provided. Write your function implementation here: ```python import operator def process_operations(ops): # Your code here ```","solution":"import operator def process_operations(ops): Processes a list of operations sequentially. Parameters: ops (list): A list of tuples where the first element is the operation name (string) and the subsequent elements are the arguments for the operation. Returns: The result after performing all the operations sequentially. result = None for op in ops: operation = getattr(operator, op[0]) if result is None: result = operation(*op[1:]) else: result = operation(result, *op[1:]) return result"},{"question":"**Question: Custom Neural Network Initialization using PyTorch\'s Initialization Module** **Problem Statement:** You are required to implement a custom neural network model using PyTorch and initialize its parameters using different initialization techniques from the `torch.nn.init` module. Specifically, you must implement and initialize a three-layer fully connected neural network with the following specifications: 1. The first layer should have an input size of 784 and an output size of 128. - Initialize the weights with values drawn from a **uniform distribution** between -0.05 and 0.05. - Initialize the biases with **zero** values. 2. The second layer should have an input size of 128 and an output size of 64. - Initialize the weights using the **Xavier normal initialization**. - Initialize the biases with **ones**. 3. The third layer should have an input size of 64 and an output size of 10. - Initialize the weights using the **Kaiming uniform initialization** with `nonlinearity=\'relu\'`. - Initialize the biases with a **constant** value of 0.1. Your task is to: - Define the neural network architecture. - Implement the custom initialization for each layer as described. - Verify and print the initialization values of the first few parameters of each layer to ensure correctness. **Expected Input and Output:** While implementing the solution, you may assume no external input. However, you must print the first 5 weight and bias values of each layer after initialization for verification. **Constraints:** - Use the functions from `torch.nn.init` for initialization. - Ensure you use the appropriate initialization techniques as specified. - Implement the solution in Python using PyTorch. **Performance Requirement:** - The implementation should be efficient and leverage PyTorch\'s optimized operations. ```python import torch import torch.nn as nn import torch.nn.init as init class CustomNeuralNetwork(nn.Module): def __init__(self): super(CustomNeuralNetwork, self).__init__() # Define layers self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) # Initialize layers init.uniform_(self.fc1.weight, -0.05, 0.05) init.zeros_(self.fc1.bias) init.xavier_normal_(self.fc2.weight) init.ones_(self.fc2.bias) init.kaiming_uniform_(self.fc3.weight, nonlinearity=\'relu\') init.constant_(self.fc3.bias, 0.1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Instantiate the model and print layer initializations model = CustomNeuralNetwork() print(\\"First 5 weights of fc1:\\", model.fc1.weight.data[:5]) print(\\"First 5 biases of fc1:\\", model.fc1.bias.data[:5]) print(\\"First 5 weights of fc2:\\", model.fc2.weight.data[:5]) print(\\"First 5 biases of fc2:\\", model.fc2.bias.data[:5]) print(\\"First 5 weights of fc3:\\", model.fc3.weight.data[:5]) print(\\"First 5 biases of fc3:\\", model.fc3.bias.data[:5]) ``` As a student, you are expected to understand and correctly implement the initialization techniques for each layer of the network following the requirements above.","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomNeuralNetwork(nn.Module): def __init__(self): super(CustomNeuralNetwork, self).__init__() # Define layers self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) # Initialize layers init.uniform_(self.fc1.weight, -0.05, 0.05) init.zeros_(self.fc1.bias) init.xavier_normal_(self.fc2.weight) init.ones_(self.fc2.bias) init.kaiming_uniform_(self.fc3.weight, nonlinearity=\'relu\') init.constant_(self.fc3.bias, 0.1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Verify the initialization by printing the first 5 weights and biases of each layer model = CustomNeuralNetwork() print(\\"First 5 weights of fc1:\\", model.fc1.weight.data[:5]) print(\\"First 5 biases of fc1:\\", model.fc1.bias.data[:5]) print(\\"First 5 weights of fc2:\\", model.fc2.weight.data[:5]) print(\\"First 5 biases of fc2:\\", model.fc2.bias.data[:5]) print(\\"First 5 weights of fc3:\\", model.fc3.weight.data[:5]) print(\\"First 5 biases of fc3:\\", model.fc3.bias.data[:5])"},{"question":"# Task You are required to implement a set of classes to manage a simple inventory system that uses classes, inheritance, and iterators. The inventory system should allow the creation of products, addition of products to the inventory, and retrieval of products from the inventory using multiple levels of search criteria. # Requirements 1. **Define a `Product` Class**: - Each `Product` should have the following attributes: - `name` (string) - `price` (float) - `category` (string) - Include an `__init__` method to initialize these attributes. 2. **Define an `Inventory` Class**: - The `Inventory` should maintain a list of `Product` instances. - Include methods to: - `add_product(product)`: Add a `Product` to the inventory. - `get_products()`: Return an iterator that yields each product in the inventory. - `search_by_name(name)`: Return an iterator that yields products with a matching name. - `search_by_category(category)`: Return an iterator that yields products in the specified category. 3. **Implement Inheritance**: - Define a subclass `DiscountedProduct` that extends `Product`. - This subclass should include an additional attribute: - `discount` (float, between 0 and 1) - Override the `price` attribute to incorporate the discount when the `DiscountedProduct` is instantiated. # Input and Output - **Input**: The constructor and method parameters should be given according to the information provided in the requirements. - **Output**: All methods should return iterators or yield values one by one. # Example Usage ```python # Define some products p1 = Product(name=\\"Laptop\\", price=1000, category=\\"Electronics\\") p2 = Product(name=\\"Mobile\\", price=500, category=\\"Electronics\\") p3 = Product(name=\\"T-shirt\\", price=20, category=\\"Apparel\\") # Define a discounted product p4 = DiscountedProduct(name=\\"Tablet\\", price=300, category=\\"Electronics\\", discount=0.1) # Create an inventory and add products inventory = Inventory() inventory.add_product(p1) inventory.add_product(p2) inventory.add_product(p3) inventory.add_product(p4) # Retrieve all products for product in inventory.get_products(): print(product.name, product.price, product.category) # Search by name for product in inventory.search_by_name(\\"Mobile\\"): print(product.name, product.price, product.category) # Search by category for product in inventory.search_by_category(\\"Electronics\\"): print(product.name, product.price, product.category) ``` # Constraints - All prices are positive numbers. - Discounts are between 0 (no discount) and 1 (free). Your implementation should ensure that added products handle correctly the discount logic and that searches efficiently return the appropriate products.","solution":"class Product: def __init__(self, name, price, category): self.name = name self.price = price self.category = category class DiscountedProduct(Product): def __init__(self, name, price, category, discount): super().__init__(name, price, category) self.discount = discount self.price = price * (1 - discount) class Inventory: def __init__(self): self.products = [] def add_product(self, product): self.products.append(product) def get_products(self): for product in self.products: yield product def search_by_name(self, name): for product in self.products: if product.name == name: yield product def search_by_category(self, category): for product in self.products: if product.category == category: yield product"},{"question":"# PyTorch and TorchScript Compatibility Challenge As a developer, you are tasked with creating a simple neural network using PyTorch that can be compiled using TorchScript. Your model will need to avoid certain functions and classes that are not supported in TorchScript, as listed in the provided documentation. **Task**: Implement a PyTorch neural network following these steps: 1. Define a simple neural network model that includes a few linear layers, a non-linear activation function, and output layers. 2. Ensure that none of the unsupported constructs from the list are used in your model. Specifically: - Do not use unsupported tensor operations like `torch.tensordot`. - Avoid using certain initialization methods like `torch.nn.init.calculate_gain`. - Do not use certain unsupported modules like `torch.nn.RNN`. **Requirements**: - Your model should be compatible with TorchScript. - Use a feedforward neural network with at least one hidden layer. - Utilize only constructs that are supported by TorchScript. - Write a script to convert the network to TorchScript and output the TorchScript model. **Input/Output Format**: - There is no specific input for the initial model creation task, but your code should be encapsulated in a function `create_torchscript_model()`. - The function should return the TorchScript compiled model. **Constraints**: - Use only methods and modules that are documented to be compatible with TorchScript. - Avoid any deprecated operations and unsupported classes from the provided list. # Example ```python import torch import torch.nn as nn import torch.optim as optim import torch.jit def create_torchscript_model(): # Define neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Create an instance of the model model = SimpleModel() # Convert the model to TorchScript scripted_model = torch.jit.script(model) return scripted_model # Example usage torchscript_model = create_torchscript_model() print(torchscript_model) ``` **Note**: Ensure that the PyTorch version used supports TorchScript with the conditions provided in the documentation.","solution":"import torch import torch.nn as nn import torch.jit def create_torchscript_model(): # Define neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Create an instance of the model model = SimpleModel() # Convert the model to TorchScript scripted_model = torch.jit.script(model) return scripted_model # Example usage torchscript_model = create_torchscript_model() print(torchscript_model)"},{"question":"**Objective:** Write a Python function that takes the name of a Python source file, modifies all integer literals by squaring their value, and returns the modified source code as a string. **Function Signature:** ```python def square_integers_in_file(filename: str) -> str: pass ``` **Input:** - `filename (str)`: The name of the Python source file to be processed. **Output:** - `str`: The modified Python source code as a single string. **Constraints:** - The function should correctly handle integer literals in various contexts, including but not limited to assignments, arithmetic operations, and function arguments. - Maintain other tokens and the structure of the original source code. - The input source file contains valid Python code. **Example:** Consider a Python file `example.py` with the following content: ```python a = 2 b = 5 + 3 print(a * b) ``` Calling `square_integers_in_file(\\"example.py\\")` should return: ```python a = 4 b = 25 + 9 print(a * b) ``` **Implementation Notes:** - Use the `tokenize` module to read and modify the tokens. - All integer literals should be identified and squared. - Use `tokenize.untokenize` to convert the modified tokens back into Python source code. - Handle all integer contexts properly and avoid modifying non-integer parts of the code (e.g., parts of string literals). **Test the function thoroughly to ensure it works on multiple cases, including edge cases (like files with no integer literals or very large files).**","solution":"import tokenize import io def square_integers_in_file(filename: str) -> str: with open(filename, \'r\') as f: source = f.read() tokens = tokenize.generate_tokens(io.StringIO(source).readline) modified_tokens = [] for toknum, tokval, start, end, line in tokens: if toknum == tokenize.NUMBER and tokval.isdigit(): squared_value = str(int(tokval) ** 2) modified_tokens.append((toknum, squared_value, start, end, line)) else: modified_tokens.append((toknum, tokval, start, end, line)) modified_source = tokenize.untokenize(modified_tokens) return modified_source"},{"question":"# Problem **Objective**: Implement a function that utilizes various window functions in pandas to compute specific rolling, expanding, and exponentially-weighted statistics on a given dataset. # Function Signature ```python def compute_window_statistics(df: pd.DataFrame) -> pd.DataFrame: # Your code here ``` # Input - `df`: A pandas DataFrame with at least one numeric column. # Output - Returns a new pandas DataFrame containing the following columns: 1. `rolling_mean`: The rolling mean using a window size of 3. 2. `rolling_std`: The rolling standard deviation using a window size of 3. 3. `expanding_sum`: The expanding sum. 4. `ewm_var`: The exponentially-weighted moving variance with a span of 3. # Constraints - The input DataFrame must have at least 3 rows, since operations like rolling and exponentially-weighted windows require a minimum size. - Columns may contain `NaN` values, which should be handled appropriately by each window function. # Example ```python import pandas as pd # Example input DataFrame data = { \'values\': [1, 2, 3, 4, 5, 6] } df = pd.DataFrame(data) # Expected output DataFrame expected_output = pd.DataFrame({ \'values\': [1, 2, 3, 4, 5, 6], \'rolling_mean\': [None, None, 2.0, 3.0, 4.0, 5.0], \'rolling_std\': [None, None, 1.0, 1.0, 1.0, 1.0], \'expanding_sum\': [1, 3, 6, 10, 15, 21], \'ewm_var\': [0.0, 0.5, 0.666667, 0.875, 1.0, 1.2] }) # Usage of the function output_df = compute_window_statistics(df) print(output_df) ``` # Notes: - Ensure the function handles edge cases such as DataFrames shorter than the required window sizes gracefully. - Make use of pandas built-in methods wherever possible to maintain efficiency.","solution":"import pandas as pd def compute_window_statistics(df: pd.DataFrame) -> pd.DataFrame: Compute rolling mean, rolling standard deviation, expanding sum, and exponentially-weighted moving variance for the given DataFrame. Args: df (pd.DataFrame): A pandas DataFrame with at least one numeric column. Returns: pd.DataFrame: New DataFrame with computed statistics. if len(df) < 3: raise ValueError(\\"DataFrame must have at least 3 rows.\\") # computing the rolling mean with a window size of 3 df[\'rolling_mean\'] = df[\'values\'].rolling(window=3).mean() # computing the rolling standard deviation with a window size of 3 df[\'rolling_std\'] = df[\'values\'].rolling(window=3).std() # computing the expanding sum df[\'expanding_sum\'] = df[\'values\'].expanding().sum() # computing the exponentially-weighted moving variance with a span of 3 df[\'ewm_var\'] = df[\'values\'].ewm(span=3).var() return df"},{"question":"XML Document Manipulation with xml.dom **Objective:** Implement a function to manipulate an XML document by adding, modifying, and querying its elements and attributes. **Problem Statement:** You are given an XML string that represents a book collection. The root element is `<books>`. Each book has a `<title>`, `<author>`, and an optional `<year>` element. Your task is to implement a function `manipulate_books(xml_string, new_book_data)` that performs the following operations: 1. Parse the given XML string into a DOM Document. 2. Add a new book element to the collection. The details of the new book are provided in the `new_book_data` dictionary with keys \\"title\\", \\"author\\", and \\"year\\". 3. Modify the title of the first book by appending \\" - Modified\\" to its title. 4. Find and return the titles of all books authored by a specific author provided as an input parameter. The author\'s name will be a value within the `new_book_data` dictionary accessed via the key \\"author_query\\". **Function Signature:** ```python from xml.dom.minidom import parseString, Document def manipulate_books(xml_string: str, new_book_data: dict) -> list: Perform XML document manipulations and queries. Args: xml_string (str): The input XML string representing the book collection. new_book_data (dict): A dictionary containing data for a new book and author to query. Example: {\\"title\\": \\"New Book Title\\", \\"author\\": \\"New Author\\", \\"year\\": \\"2023\\", \\"author_query\\": \\"Existing Author\\"} Returns: list: A list of titles of all books authored by the specified author in the `author_query`. pass ``` **Constraints:** - The `xml_string` will be a well-formed XML representation of the book collection. - The `new_book_data` dictionary will always contain the keys \\"title\\", \\"author\\", \\"year\\", and \\"author_query\\". - The function should handle the node manipulations using `xml.dom.minidom`. **Example:** ```python xml_string = \'\'\'<books> <book> <title>Book One</title> <author>Author One</author> <year>2001</year> </book> <book> <title>Book Two</title> <author>Author Two</author> <year>2002</year> </book> </books>\'\'\' new_book_data = { \\"title\\": \\"New Book Title\\", \\"author\\": \\"New Author\\", \\"year\\": \\"2023\\", \\"author_query\\": \\"Author One\\" } # Function call titles_by_author_query = manipulate_books(xml_string, new_book_data) # Output print(titles_by_author_query) # Output should be: [\'Book One\'] ``` **Steps to Implement the Function:** 1. Parse the `xml_string` into a DOM Document object. 2. Create and append the new book element with its title, author, and year to the Document. 3. Modify the title of the first book element by appending \\" - Modified\\". 4. Query and return the titles of all books by the author specified in `author_query`. **Note:** Ensure to utilize the methods provided by the `xml.dom` module for manipulating and querying the XML document.","solution":"from xml.dom.minidom import parseString, Document def manipulate_books(xml_string: str, new_book_data: dict) -> list: Perform XML document manipulations and queries. Args: xml_string (str): The input XML string representing the book collection. new_book_data (dict): A dictionary containing data for a new book and author to query. Example: {\\"title\\": \\"New Book Title\\", \\"author\\": \\"New Author\\", \\"year\\": \\"2023\\", \\"author_query\\": \\"Existing Author\\"} Returns: list: A list of titles of all books authored by the specified author in the `author_query`. # Parse the given XML string into a DOM Document object doc = parseString(xml_string) # Create a new book element new_book = doc.createElement(\'book\') # Create and append title element title_element = doc.createElement(\'title\') title_text = doc.createTextNode(new_book_data[\\"title\\"]) title_element.appendChild(title_text) new_book.appendChild(title_element) # Create and append author element author_element = doc.createElement(\'author\') author_text = doc.createTextNode(new_book_data[\\"author\\"]) author_element.appendChild(author_text) new_book.appendChild(author_element) # Create and append year element year_element = doc.createElement(\'year\') year_text = doc.createTextNode(new_book_data[\\"year\\"]) year_element.appendChild(year_text) new_book.appendChild(year_element) # Append the new book to the books root element doc.documentElement.appendChild(new_book) # Modify the title of the first book by appending \\" - Modified\\" first_book = doc.getElementsByTagName(\'book\')[0] first_title = first_book.getElementsByTagName(\'title\')[0] first_title_text = first_title.firstChild.nodeValue first_title.firstChild.nodeValue = first_title_text + \\" - Modified\\" # Query and collect titles of books by the specified author author_query = new_book_data[\\"author_query\\"] books = doc.getElementsByTagName(\'book\') titles_by_author = [] for book in books: author = book.getElementsByTagName(\'author\')[0].firstChild.nodeValue if author == author_query: title = book.getElementsByTagName(\'title\')[0].firstChild.nodeValue titles_by_author.append(title) return titles_by_author"},{"question":"**Problem Statement:** You are required to implement a Python class `SpecialContainer` that utilizes the built-in constants `None`, `NotImplemented`, `Ellipsis`, and some other potentially useful built-in constants. This class should serve as a custom container and support specific operations inspired by these constants. # Class Implementation: 1. **Attributes:** - `data`: A list that holds elements of the container. 2. **Methods:** - `__init__(self, items=None)`: Initializes the container with the provided list `items`. If `items` is not provided, it initializes with an empty list. - `__getitem__(self, key)`: Allows slicing of the `data` attribute. If `key` is `Ellipsis`, return the entire list. - `__eq__(self, other)`: Compares the `data` attribute of two `SpecialContainer` instances. If `other` is not an instance of `SpecialContainer`, return `NotImplemented`. - `add_element(self, element)`: Adds an element to the container. - `remove_element(self, element)`: Attempts to remove `element` from the container. If the element is not found, return `None`. - `__bool__(self)`: Returns `True` if the container has elements, otherwise returns `False`. # Constraints: - The container should only hold items that are of type `int`, `float`, or `str`. - Do not use built-in container classes such as `set` or `dict`. # Example Usage: ```python # Initialize with no elements container = SpecialContainer() # Add elements container.add_element(1) container.add_element(\'a\') container.add_element(3.14) # Remove an element removed = container.remove_element(\'a\') # returns \'a\' removed_not_found = container.remove_element(\'b\') # returns None # Compare containers container2 = SpecialContainer([1, 3.14]) assert container == container2 # should be True # Slicing elements = container[...] # returns [1, 3.14] # Boolean value of the container is_non_empty = bool(container) # returns True ``` # Expected Input and Output: - `__init__` : Takes an optional list `items`. Initializes `self.data` with `items` or an empty list. - `__getitem__` : Takes a `key` which can be a slice or `Ellipsis`. - `__eq__` : Takes another instance `other`. If `other` is not `SpecialContainer`, return `NotImplemented`. - `add_element` : Takes an `element`. Adds it to `self.data`. - `remove_element` : Takes an `element`. Removes it from `self.data` if present; otherwise, returns `None`. - `__bool__` : Returns `True` if `self.data` has elements; otherwise, returns `False`. Write the class `SpecialContainer` according to the requirements and demonstrate its usage with the given example code.","solution":"class SpecialContainer: def __init__(self, items=None): if items is not None: self.data = [item for item in items if isinstance(item, (int, float, str))] else: self.data = [] def __getitem__(self, key): if key is Ellipsis: return self.data else: return self.data[key] def __eq__(self, other): if not isinstance(other, SpecialContainer): return NotImplemented return self.data == other.data def add_element(self, element): if isinstance(element, (int, float, str)): self.data.append(element) def remove_element(self, element): if element in self.data: self.data.remove(element) return element else: return None def __bool__(self): return len(self.data) > 0"},{"question":"Objective You are tasked with designing and implementing a Python class that configures the environment variables for PyTorch\'s ProcessGroupNCCL. This class will be used to manage the settings of the distributed computation environment and demonstrate a simple distributed computation operation using these settings. Task 1. Implement a Python class named `NCCLEnvConfig` that: - Initializes and sets the environment variables listed in the documentation. - Allows the configuration of these environment variables via class methods. - Provides a method to print the current configurations. 2. Implement a simple distributed computation operation using PyTorch\'s distributed communication package (`torch.distributed`) that showcases the usage of at least three of the configured environment variables. Class Details **`NCCLEnvConfig` Class:** - **Method 1:** `__init__(self)`: Initializes all environment variables mentioned in the documentation to their default values. - **Method 2:** `set_variable(self, var_name: str, value: Union[int, str])`: Sets the given environment variable to the provided value. - **Method 3:** `get_variable(self, var_name: str) -> Union[int, str]`: Returns the current value of the specified environment variable. - **Method 4:** `print_config(self)`: Prints all environment variables and their current values. Implementation Requirements - **Environment Variable Setting:** Demonstrate setting and retrieving environment variables using the `NCCLEnvConfig` class in a script\'s main function. - **Distributed Computation:** Include a simple distributed computation example (such as a collective communication operation) that uses at least three of the configured environment variables. Example Usage Example usage of `NCCLEnvConfig` class and a simple distributed computation might look like this: ```python if __name__ == \\"__main__\\": import torch import torch.distributed as dist from NCCLEnvConfig import NCCLEnvConfig def init_process(rank, size, fn, backend=\'nccl\'): Initialize the distributed environment. dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) def run(rank, size): Simple distributed function that uses NCCL environment settings. config = NCCLEnvConfig() config.set_variable(\\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\", 1) config.set_variable(\\"TORCH_NCCL_HIGH_PRIORITY\\", 1) config.set_variable(\\"TORCH_NCCL_BLOCKING_WAIT\\", 1) # Perform a collective communication tensor = torch.ones(10).cuda(rank) dist.all_reduce(tensor, op=dist.reduce_op.SUM) print(f\\"Rank {rank} has tensor: {tensor[0]}\\") # Assuming we have two processes size = 2 processes = [] for rank in range(size): p = Process(target=init_process, args=(rank, size, run)) p.start() processes.append(p) for p in processes: p.join() ``` Input and Output Formats - **Input:** No input required (assumes environment setup within the main function). - **Output:** Prints the results of the distributed computation to the console. Constraints - Please ensure your solution handles errors gracefully, particularly when setting or getting environment variables. - Assume a distributed computation setup with 2 or more processes.","solution":"import os import torch import torch.distributed as dist from multiprocessing import Process class NCCLEnvConfig: def __init__(self): self.env_vars = { \\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\": \\"0\\", \\"TORCH_NCCL_BLOCKING_WAIT\\": \\"0\\", \\"TORCH_NCCL_HIGH_PRIORITY\\": \\"0\\" } self._apply_env() def _apply_env(self): for var, value in self.env_vars.items(): os.environ[var] = value def set_variable(self, var_name: str, value: str): if var_name in self.env_vars: self.env_vars[var_name] = value os.environ[var_name] = value else: raise KeyError(f\\"{var_name} is not a recognized NCCL environment variable\\") def get_variable(self, var_name: str) -> str: if var_name in self.env_vars: return os.environ.get(var_name) else: raise KeyError(f\\"{var_name} is not a recognized NCCL environment variable\\") def print_config(self): for var, value in self.env_vars.items(): print(f\\"{var} = {value}\\") def init_process(rank, size, fn, backend=\'nccl\'): Initialize the distributed environment. dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) def run(rank, size): Simple distributed function that uses NCCL environment settings. config = NCCLEnvConfig() config.set_variable(\\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\", \\"1\\") config.set_variable(\\"TORCH_NCCL_HIGH_PRIORITY\\", \\"1\\") config.set_variable(\\"TORCH_NCCL_BLOCKING_WAIT\\", \\"1\\") # Perform a collective communication tensor = torch.ones(10).cuda(rank) dist.all_reduce(tensor, op=dist.reduce_op.SUM) print(f\\"Rank {rank} has tensor: {tensor[0]}\\") if __name__ == \\"__main__\\": size = 2 processes = [] for rank in range(size): p = Process(target=init_process, args=(rank, size, run)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"You are tasked with implementing a simplified I/O multiplexer using the `selectors` module in Python. This involves creating a service that can register multiple socket connections and handle read events on these connections asynchronously. Problem Statement: Create a class `SimpleMultiplexer` that: 1. Initializes a `DefaultSelector` instance. 2. Provides methods to register, unregister, and handle read events on socket objects. 3. Manages a loop that waits for I/O events on these registered sockets and handles the events accordingly. Class Definition: ```python import selectors import socket class SimpleMultiplexer: def __init__(self): self.selector = selectors.DefaultSelector() def register_socket(self, sock): Register a socket for read events. Args: sock (socket.socket): The socket object to register. self.selector.register(sock, selectors.EVENT_READ) def unregister_socket(self, sock): Unregister a previously registered socket. Args: sock (socket.socket): The socket object to unregister. self.selector.unregister(sock) def handle_events(self): Wait for I/O events and handle read events on registered sockets. while True: events = self.selector.select() for key, mask in events: if mask == selectors.EVENT_READ: self.read(key.fileobj) def read(self, sock): Handle read event on a socket. Args: sock (socket.socket): The socket object that is ready for reading. data = sock.recv(1024) if data: print(f\\"Received data: {data} from {sock}\\") else: print(f\\"Closing socket: {sock}\\") self.unregister_socket(sock) sock.close() ``` Requirements: 1. Implement the `register_socket`, `unregister_socket`, and `read` methods as described. 2. In `read` method, if no data is received, it means the connection is closed. Unregister the socket and close it. 3. `handle_events` method should be an infinite loop that continuously waits for events and handles them appropriately. Input and Output: - You do not need to handle any input or output beyond the methods provided. The `print` statements in the `read` function are for illustrative purposes. Constraints: - The socket objects you register and manage should be non-blocking. - You do not have to handle concurrent writes or any events other than read events. You can assume that the environment where your `SimpleMultiplexer` class will be tested supports socket operations and the `selectors` module. ```python # Example usage of SimpleMultiplexer (not part of the implementation): if __name__ == \\"__main__\\": multiplexer = SimpleMultiplexer() server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', 12345)) server_socket.listen(5) server_socket.setblocking(False) multiplexer.register_socket(server_socket) multiplexer.handle_events() ``` # Guidance: - Pay attention to resource management, ensure all sockets are properly closed. - Test the class with multiple clients connecting to ensure it handles multiple connections as expected.","solution":"import selectors import socket class SimpleMultiplexer: def __init__(self): self.selector = selectors.DefaultSelector() def register_socket(self, sock): Register a socket for read events. Args: sock (socket.socket): The socket object to register. self.selector.register(sock, selectors.EVENT_READ) def unregister_socket(self, sock): Unregister a previously registered socket. Args: sock (socket.socket): The socket object to unregister. self.selector.unregister(sock) def handle_events(self): Wait for I/O events and handle read events on registered sockets. while True: events = self.selector.select() for key, mask in events: if mask == selectors.EVENT_READ: self.read(key.fileobj) def read(self, sock): Handle read event on a socket. Args: sock (socket.socket): The socket object that is ready for reading. data = sock.recv(1024) if data: print(f\\"Received data: {data} from {sock}\\") else: print(f\\"Closing socket: {sock}\\") self.unregister_socket(sock) sock.close()"},{"question":"**Problem Statement: Analyze Module Dependencies of a Script** As a Python developer, you are often required to understand and manage the dependencies of various scripts for efficient debugging and deployment. Your task is to write a Python function that uses the `modulefinder` module to analyze the module dependencies of a given script and produce a detailed report. # Function Signature ```python def analyze_script_dependencies(script_path: str) -> dict: pass ``` # Input - `script_path` (str): A string representing the path to the Python script file that needs to be analyzed. # Output - Returns a dictionary with two keys: - `\'loaded_modules\'`: Contains a dictionary where each key is the module name imported by the script and the value is a list of some global names defined in that module. - `\'missing_modules\'`: Contains a list of module names that were attempted to be imported by the script but were not found. # Example ```python result = analyze_script_dependencies(\'bacon.py\') print(result) ``` **Expected Output:** ```python { \'loaded_modules\': { \'_types\': [], \'copyreg\': [\'_inverted_registry\', \'_slotnames\', \'__all__\'], \'sre_compile\': [\'isstring\', \'_sre\', \'_optimize_unicode\'], \'_sre\': [], \'sre_constants\': [\'REPEAT_ONE\', \'makedict\', \'AT_END_LINE\'], \'sys\': [], \'re\': [\'__module__\', \'finditer\', \'_expand\'], \'itertools\': [], \'__main__\': [\'re\', \'itertools\', \'baconhameggs\'], \'sre_parse\': [\'_PATTERNENDERS\', \'SRE_FLAG_UNICODE\'], \'array\': [], \'types\': [\'__module__\', \'IntType\', \'TypeType\'] }, \'missing_modules\': [ \'guido.python.ham\', \'baconhameggs\' ] } ``` # Constraints - You must use the `modulefinder` package to accomplish this task. - Assume the input script contains valid Python code. - Focus on being efficient and clear in your report generation. # Advanced Considerations Ensure that your analysis handles exceptions properly and provides meaningful insight into why certain modules could not be imported.","solution":"import modulefinder from typing import Dict, List def analyze_script_dependencies(script_path: str) -> Dict[str, List]: finder = modulefinder.ModuleFinder() finder.run_script(script_path) loaded_modules = {} for name, module in finder.modules.items(): loaded_modules[name] = list(module.globalnames.keys()) missing_modules = list(finder.badmodules.keys()) return { \'loaded_modules\': loaded_modules, \'missing_modules\': missing_modules }"},{"question":"# Directory Tree File Type Summary You are given a directory path, and your task is to analyze the types of files within this directory tree recursively. You will implement the function `summarize_directory_tree` which will generate and return a summary dictionary indicating the count of each type of file within the given directory and all its subdirectories. Function Signature ```python import os from stat import * def summarize_directory_tree(directory: str) -> dict: pass ``` # Input - `directory` (str): The path of the directory to analyze. # Output - Returns a dictionary where the keys are the file types (as strings) and the values are the counts of those types within the directory tree. # File Types to Be Counted - `\\"directory\\"` - `\\"character special device\\"` - `\\"block special device\\"` - `\\"regular file\\"` - `\\"FIFO (named pipe)\\"` - `\\"symbolic link\\"` - `\\"socket\\"` - `\\"door\\"` - `\\"event port\\"` - `\\"whiteout\\"` # Constraints 1. The function should handle very large directory trees efficiently. 2. You can assume the directory path given always exists and is readable. # Example ```python # Suppose the directory tree is structured as: # /path/to/dir/ #  subdir1 #   file1.txt (regular file) #   file2.txt (regular file) #   symlink1 -> /some/other/path (symbolic link) #  subdir2 #   dev (character special device) #  fifo1 (FIFO) result = summarize_directory_tree(\'/path/to/dir\') print(result) # Output could be: # { # \\"directory\\": 3, # \\"character special device\\": 1, # \\"block special device\\": 0, # \\"regular file\\": 2, # \\"FIFO (named pipe)\\": 1, # \\"symbolic link\\": 1, # \\"socket\\": 0, # \\"door\\": 0, # \\"event port\\": 0, # \\"whiteout\\": 0 # } ``` Notes - Use the `os.lstat()` function to gather file statistics. - Use the various `stat` functions for determining file types (`S_ISDIR`, `S_ISCHR`, `S_ISBLK`, etc.). - Your solution should be recursive to navigate the directory tree. - Ensure to handle edge cases where file types may be zero (i.e., files that do not count towards any of the specific categories). Implement the function to summarize the types of files in a given directory tree efficiently.","solution":"import os from stat import * def summarize_directory_tree(directory: str) -> dict: summary = { \\"directory\\": 0, \\"character special device\\": 0, \\"block special device\\": 0, \\"regular file\\": 0, \\"FIFO (named pipe)\\": 0, \\"symbolic link\\": 0, \\"socket\\": 0, \\"door\\": 0, \\"event port\\": 0, \\"whiteout\\": 0 } def analyze(path): try: st = os.lstat(path) except FileNotFoundError: # Skip file that might be deleted after the listing return if S_ISDIR(st.st_mode): summary[\\"directory\\"] += 1 for entry in os.listdir(path): analyze(os.path.join(path, entry)) elif S_ISCHR(st.st_mode): summary[\\"character special device\\"] += 1 elif S_ISBLK(st.st_mode): summary[\\"block special device\\"] += 1 elif S_ISREG(st.st_mode): summary[\\"regular file\\"] += 1 elif S_ISFIFO(st.st_mode): summary[\\"FIFO (named pipe)\\"] += 1 elif S_ISLNK(st.st_mode): summary[\\"symbolic link\\"] += 1 elif S_ISSOCK(st.st_mode): summary[\\"socket\\"] += 1 # Note: if S_ISDOOR, S_ISEVPORT, or S_ISWHT become available in a Python # application they can be included, currently these are Unix specific # placeholders. elif hasattr(st, \\"S_ISDOOR\\") and S_ISDOOR(st.st_mode): summary[\\"door\\"] += 1 elif hasattr(st, \\"S_ISEVPORT\\") and S_ISEVPORT(st.st_mode): summary[\\"event port\\"] += 1 elif hasattr(st, \\"S_ISWHT\\") and S_ISWHT(st.st_mode): summary[\\"whiteout\\"] += 1 analyze(directory) return summary"},{"question":"Objective: Implement a classification model using scikit-learn\'s `SGDClassifier` on a given dataset. Your solution should include preprocessing steps, such as scaling the features, tuning hyperparameters, and evaluating the model\'s performance. Task: 1. **Data Loading and Preprocessing**: - Load the dataset provided as a CSV file. The CSV file will have `n_samples` rows and `n_features + 1` columns, where the last column represents the target labels. - Split the dataset into training and testing sets using an 80-20 split. - Scale the features using `StandardScaler`. 2. **Model Implementation and Training**: - Implement the `SGDClassifier` with the following parameters: - `loss=\'hinge\'` (Linear SVM) - `penalty=\'l2\'` - `max_iter` should be set initially to 1000. - `tol=1e-3` - Train the model on the training dataset. 3. **Hyperparameter Tuning**: - Use cross-validation to tune the hyperparameter `alpha` (regularization parameter) within the range `[1e-4, 1e-3, 1e-2, 1e-1, 1]`. 4. **Model Evaluation**: - Evaluate the model\'s performance on the test set using the accuracy score and the confusion matrix. - Additionally, compute the model\'s performance using the cross-validation score. 5. **Implementation Requirements**: - Use scikit-learn\'s `train_test_split` for splitting the dataset. - Use scikit-learn\'s `GridSearchCV` for hyperparameter tuning. - Use scikit-learn\'s `accuracy_score` and `confusion_matrix` for evaluation. Input Format: - A CSV file named `data.csv` where each row contains `n_features` features followed by the target label. Output Format: - Print the best hyperparameter `alpha` value found. - Print the accuracy score on the test set. - Print the confusion matrix on the test set. - Print the best cross-validation score. Constraints: - The dataset will have at least 100 samples and no more than 100,000 samples. - Each feature will be a float and the target labels will be integers. - Ensure random states are set for reproducibility where applicable. Example Usage: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, confusion_matrix # Your implementation here ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, confusion_matrix def classify_with_sgd(csv_file): # Load the dataset data = pd.read_csv(csv_file) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Define the model sgd = SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) # Define hyperparameter grid param_grid = {\'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} # Implement GridSearchCV grid_search = GridSearchCV(sgd, param_grid, cv=5) grid_search.fit(X_train_scaled, y_train) # Get the best model best_model = grid_search.best_estimator_ # Evaluate the model y_pred = best_model.predict(X_test_scaled) best_alpha = grid_search.best_params_[\'alpha\'] test_accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) best_cv_score = grid_search.best_score_ # Print the results print(\\"Best alpha value: \\", best_alpha) print(\\"Accuracy on test set: \\", test_accuracy) print(\\"Confusion matrix on test set: n\\", conf_matrix) print(\\"Best cross-validation score: \\", best_cv_score) return best_alpha, test_accuracy, conf_matrix, best_cv_score"},{"question":"# Question: Managing Context Variables in Asynchronous Python Code You are tasked with developing a Python application that handles asynchronous tasks and manages context-local state using the `contextvars` module. Specifically, you need to create a context variable that tracks the current user\'s session information throughout the lifecycle of an asynchronous task. Requirements: 1. **Session Context Variable**: - Create a `ContextVar` named `session_var` to store the current user\'s session information. - The `session_var` should have a default value of `None`. 2. **Session Management Functions**: - Implement a function `set_session(session_info)` that takes a dictionary `session_info` and sets it as the current value of `session_var`. - Implement a function `get_session()` that returns the current value of `session_var`. If the variable is not set, it should return `None`. 3. **Asynchronous Session Handler**: - Implement an asynchronous function `handle_session(session_info, task)`, where `session_info` is a dictionary containing session information and `task` is an asynchronous callable function. - The function should set the `session_info` in `session_var`, run the `task`, and ensure that the `session_var` is reset to its original state after `task` completes. 4. **Test Case**: - Provide a test case where `handle_session` is used to run an asynchronous task that retrieves the current session information and prints it to the console. Example: ```python import contextvars # Step 1: Create the session_var ContextVar session_var = contextvars.ContextVar(\'session_var\', default=None) # Step 2: Implement session management functions def set_session(session_info): pass def get_session(): pass # Step 3: Implement the asynchronous session handler async def handle_session(session_info, task): pass # Step 4: Test case import asyncio async def example_task(): session = get_session() print(f\\"Current Session: {session}\\") async def main(): session_info = {\\"user_id\\": 123, \\"token\\": \\"abcxyz\\"} await handle_session(session_info, example_task) if __name__ == \\"__main__\\": asyncio.run(main()) ``` Complete the implementation of `set_session`, `get_session`, and `handle_session` functions as per the requirements. Constraints: - The `session_info` dictionary can contain various keys and values, but is guaranteed to be a dictionary. - Ensure proper usage of context variables to manage concurrent tasks without session data leaking between them. Performance Requirements: - Your implementation should efficiently manage context variables even when dealing with multiple concurrent asynchronous tasks.","solution":"import contextvars import asyncio # Step 1: Create the session_var ContextVar session_var = contextvars.ContextVar(\'session_var\', default=None) # Step 2: Implement session management functions def set_session(session_info): session_var.set(session_info) def get_session(): return session_var.get() # Step 3: Implement the asynchronous session handler async def handle_session(session_info, task): token = session_var.set(session_info) try: await task() finally: session_var.reset(token) # Example asynchronous task async def example_task(): session = get_session() print(f\\"Current Session: {session}\\") # Example usage async def main(): session_info = {\\"user_id\\": 123, \\"token\\": \\"abcxyz\\"} await handle_session(session_info, example_task) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question: Using the `cmd` module, create a custom command interpreter called `MathShell` that allows a user to perform basic mathematical operations. Your class should inherit from `cmd.Cmd` and implement the following commands: 1. **add X Y** - Adds two numbers and prints the result. 2. **subtract X Y** - Subtracts the second number from the first and prints the result. 3. **multiply X Y** - Multiplies two numbers and prints the result. 4. **divide X Y** - Divides the first number by the second and prints the result. 5. **history** - Prints a list of all previous commands executed in the session. 6. **bye** - Exits the command interpreter. Additionally, your `MathShell` class should support: - Custom command completion. - Recording of commands to a file using a `record` command followed by the filename. - Playing back commands from a file using a `playback` command followed by the filename. Make sure to handle invalid inputs gracefully and print appropriate error messages. Constraints: - Numbers should be floating-point representations. - Division by zero must be handled properly with an error message. Example: ```python from math_shell import MathShell if __name__ == \'__main__\': MathShell().cmdloop() ``` ```sh > add 10 5 15.0 > multiply 3 4 12.0 > subtract 10 2 8.0 > divide 10 0 Error: Division by zero > history add 10 5 multiply 3 4 subtract 10 2 divide 10 0 > record session.txt Recording started... > add 3 2 5.0 > bye > playback session.txt Recording played back: add 3 2 5.0 > bye ``` Your implementation should ensure: - Commands are properly parsed and executed. - History is maintained and displayed accurately. - Record and playback functionalities work as expected.","solution":"import cmd class MathShell(cmd.Cmd): intro = \\"Welcome to MathShell. Type help or ? to list commands.n\\" prompt = \\"(MathShell) \\" file = None def __init__(self): super().__init__() self.history = [] def do_add(self, arg): \'Add two numbers: add 4.0 5.0\' args = parse(arg) if args: try: result = float(args[0]) + float(args[1]) print(result) self.history.append(f\\"add {arg}\\") except ValueError: print(\\"Error: Invalid input. Arguments must be numbers.\\") def do_subtract(self, arg): \'Subtract second number from the first: subtract 4.0 5.0\' args = parse(arg) if args: try: result = float(args[0]) - float(args[1]) print(result) self.history.append(f\\"subtract {arg}\\") except ValueError: print(\\"Error: Invalid input. Arguments must be numbers.\\") def do_multiply(self, arg): \'Multiply two numbers: multiply 4.0 5.0\' args = parse(arg) if args: try: result = float(args[0]) * float(args[1]) print(result) self.history.append(f\\"multiply {arg}\\") except ValueError: print(\\"Error: Invalid input. Arguments must be numbers.\\") def do_divide(self, arg): \'Divide first number by the second: divide 4.0 5.0\' args = parse(arg) if args: try: if float(args[1]) == 0.0: print(\\"Error: Division by zero\\") else: result = float(args[0]) / float(args[1]) print(result) self.history.append(f\\"divide {arg}\\") except ValueError: print(\\"Error: Invalid input. Arguments must be numbers.\\") def do_history(self, arg): \'Show command history\' for cmd in self.history: print(cmd) def do_bye(self, arg): \'Exit MathShell\' print(\\"Thank you for using MathShell\\") return True def do_record(self, filename): \'Save command history to a file: record filename\' self.file = open(filename, \'w\') self.file.write(\'n\'.join(self.history) + \'n\') self.file.close() print(f\'Recording to {filename} started...\') def do_playback(self, filename): \'Playback the commands from a file: playback filename\' with open(filename) as f: content = f.readlines() print(f\'Recording played back from {filename}:\') for line in content: line = line.strip() print(f\'{line}\') self.onecmd(line) def parse(arg): \'Convert a series of zero or more numbers to an argument tuple\' return tuple(map(float, arg.split()))"},{"question":"Problem Statement You are tasked with implementing a function `group_info_summary(group_name)` that receives a group name as input and returns a dictionary with summarized information about the group and its members. # Function Signature ```python def group_info_summary(group_name: str) -> dict: ``` # Input - `group_name` (str): The name of the Unix group to retrieve information for. # Output - The function should return a dictionary with the following structure: ```python { \\"name\\": <group name>, \\"gid\\": <group ID>, \\"number_of_members\\": <number of members>, \\"members\\": <list of members> } ``` # Constraints - The function should handle cases where the `group_name` does not exist by raising a `KeyError` with a customized error message `\\"Group not found: <group_name>\\"`. - The `number_of_members` should be calculated based on the length of the members list. - The function should handle large lists of groups efficiently. # Example ```python # Given group name \\"admin\\", and assuming the group database has the following entry: # (\\"admin\\", \\"\\", 1001, [\\"user1\\", \\"user2\\", \\"user3\\"]) group_info_summary(\\"admin\\") # This should return: # { # \\"name\\": \\"admin\\", # \\"gid\\": 1001, # \\"number_of_members\\": 3, # \\"members\\": [\\"user1\\", \\"user2\\", \\"user3\\"] # } # If the group name \\"nonexistent\\" does not exist in the database: group_info_summary(\\"nonexistent\\") # This should raise a KeyError with the message: # \\"Group not found: nonexistent\\" ``` # Note - You are required to use the `grp` module as described in the documentation to retrieve group information. # Hints - Use `grp.getgrnam` to fetch the group information by name. - Remember to handle possible exceptions as specified.","solution":"import grp def group_info_summary(group_name: str) -> dict: Returns a dictionary with summarized information about the group. Args: - group_name (str): The name of the Unix group to retrieve information for. Returns: - dict: A dictionary containing the group name, ID, number of members, and the list of members. Raises: - KeyError: If the group does not exist. try: group_info = grp.getgrnam(group_name) except KeyError: raise KeyError(f\\"Group not found: {group_name}\\") return { \\"name\\": group_info.gr_name, \\"gid\\": group_info.gr_gid, \\"number_of_members\\": len(group_info.gr_mem), \\"members\\": group_info.gr_mem }"},{"question":"**CUDA Device and Memory Management Assessment** **Problem Statement:** You are required to write a PyTorch function that performs the following tasks: 1. Initializes CUDA, checks if CUDA is available, and retrieves the number of available CUDA devices. 2. For each available CUDA device: - Set the current device. - Retrieve and print the device properties (name, capability, and memory usage). 3. Allocate a tensor of size (1000, 1000) of random numbers on each available device. 4. Compute and print the memory allocated before and after the tensor creation. 5. Synchronize and reset the peak memory stats for each device. The expected output should clearly show the properties and memory details for each CUDA device, demonstrating efficient memory management. **Function Specifications:** ```python import torch import torch.cuda def cuda_device_memory_management(): Perform CUDA device and memory management tasks: 1. Initializes CUDA and checks if CUDA is available. 2. For each CUDA device: - Sets the current device. - Retrieves and prints the device properties. - Allocates a (1000, 1000) tensor of random numbers. - Prints the memory allocated before and after tensor creation. - Synchronizes and resets the peak memory stats. # Example output format: CUDA Availability: True Number of CUDA Devices: 2 Device 0: Name: Tesla K80 Capability: (3, 7) Memory Usage Before: X MB Memory Usage After: Y MB Device 1: Name: Tesla V100 Capability: (7, 0) Memory Usage Before: A MB Memory Usage After: B MB if __name__ == \\"__main__\\": cuda_device_memory_management() ``` **Constraints:** - Ensure the function handles cases where CUDA is not available gracefully. - Efficient memory management is crucial; use appropriate PyTorch and CUDA utilities to manage memory. - The function should be modular and readable. **Performance Requirements:** - The function should perform the device and memory operations efficiently without unnecessary overhead. - Properly synchronize CUDA operations to ensure accurate memory statistics. **Additional Information:** - Refer to the PyTorch `torch.cuda` module documentation for more details on CUDA-related functionalities. - Make sure to import necessary submodules from `torch.cuda`. **Submission:** Submit your implementation code along with a brief explanation of each step and how it aligns with the requirements.","solution":"import torch def cuda_device_memory_management(): Perform CUDA device and memory management tasks: 1. Initializes CUDA and checks if CUDA is available. 2. For each CUDA device: - Sets the current device. - Retrieves and prints the device properties. - Allocates a (1000, 1000) tensor of random numbers. - Prints the memory allocated before and after tensor creation. - Synchronizes and resets the peak memory stats. if not torch.cuda.is_available(): print(\\"CUDA is not available.\\") return number_of_devices = torch.cuda.device_count() print(f\\"CUDA Availability: {torch.cuda.is_available()}\\") print(f\\"Number of CUDA Devices: {number_of_devices}\\") for device_id in range(number_of_devices): torch.cuda.set_device(device_id) device_properties = torch.cuda.get_device_properties(device_id) device_name = device_properties.name device_capability = (device_properties.major, device_properties.minor) memory_allocated_before = torch.cuda.memory_allocated(device_id) memory_reserved_before = torch.cuda.memory_reserved(device_id) tensor = torch.randn((1000, 1000), device=f\'cuda:{device_id}\') memory_allocated_after = torch.cuda.memory_allocated(device_id) memory_reserved_after = torch.cuda.memory_reserved(device_id) print(f\\"nDevice {device_id}:\\") print(f\\"Name: {device_name}\\") print(f\\"Capability: {device_capability}\\") print(f\\"Memory Allocated Before: {memory_allocated_before / (1024 ** 2):.2f} MB\\") print(f\\"Memory Reserved Before: {memory_reserved_before / (1024 ** 2):.2f} MB\\") print(f\\"Memory Allocated After: {memory_allocated_after / (1024 ** 2):.2f} MB\\") print(f\\"Memory Reserved After: {memory_reserved_after / (1024 ** 2):.2f} MB\\") torch.cuda.synchronize() torch.cuda.reset_peak_memory_stats(device_id) if __name__ == \\"__main__\\": cuda_device_memory_management()"},{"question":"# Mapping Manipulation Challenge Objective Create a Python function that mimics some of the C API functions detailed in the documentation for Python dictionaries. Function Signatures Your task is to implement the following functions in Python: 1. **mapping_check(obj)**: Returns `True` if the object provides the mapping protocol (i.e., isinstance of `dict`), or `False` otherwise. 2. **mapping_size(obj)**: Returns the size (number of keys) of the dictionary object. Raises `TypeError` if the object is not a dictionary. 3. **mapping_get_item(obj, key)**: Returns the value corresponding to `key` in the dictionary object. Raises `KeyError` if `key` is not in the dictionary. 4. **mapping_set_item(obj, key, value)**: Sets the `key` to `value` in the dictionary object. Raises `TypeError` if the object is not a dictionary. 5. **mapping_del_item(obj, key)**: Deletes `key` (and its corresponding value) from the dictionary object. Raises `KeyError` if the `key` is not found. 6. **mapping_has_key(obj, key)**: Returns `True` if the dictionary object contains `key`, otherwise returns `False`. 7. **mapping_keys(obj)**: Returns a list of all keys in the dictionary object. 8. **mapping_values(obj)**: Returns a list of all values in the dictionary object. 9. **mapping_items(obj)**: Returns a list of key-value pairs (tuples) in the dictionary object. Constraints - All the functions must handle exceptions gracefully and raise appropriate errors. - Performance is essential: functions should run in O(1) time complexity for checking the presence of keys and accessing values, and O(n) for retrieving lists of keys, values, and items, where n is the number of items in the dictionary. - Do not use any external libraries; only standard Python is allowed. Example Usage ```python # Example dictionary sample_dict = {\'a\': 1, \'b\': 2, \'c\': 3} # Checking if the object is a dictionary print(mapping_check(sample_dict)) # Output: True # Getting the size of the dictionary print(mapping_size(sample_dict)) # Output: 3 # Accessing a value by key print(mapping_get_item(sample_dict, \'b\')) # Output: 2 # Setting a key-value pair mapping_set_item(sample_dict, \'d\', 4) print(sample_dict) # Output: {\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4} # Deleting a key-value pair mapping_del_item(sample_dict, \'a\') print(sample_dict) # Output: {\'b\': 2, \'c\': 3, \'d\': 4} # Checking for the existence of a key print(mapping_has_key(sample_dict, \'c\')) # Output: True # Getting all keys in the dictionary print(mapping_keys(sample_dict)) # Output: [\'b\', \'c\', \'d\'] # Getting all values in the dictionary print(mapping_values(sample_dict)) # Output: [2, 3, 4] # Getting all items in the dictionary print(mapping_items(sample_dict)) # Output: [(\'b\', 2), (\'c\', 3), (\'d\', 4)] ``` Submission Submit your implementations in a single Python file. Ensure your code is well-documented and adheres to Pythonic conventions.","solution":"def mapping_check(obj): Returns True if the object is a dictionary, False otherwise. return isinstance(obj, dict) def mapping_size(obj): Returns the number of keys in the dictionary object. Raises TypeError if the object is not a dictionary. if not isinstance(obj, dict): raise TypeError(\\"Object is not a dictionary\\") return len(obj) def mapping_get_item(obj, key): Returns the value corresponding to the key in the dictionary object. Raises KeyError if the key is not in the dictionary. return obj[key] def mapping_set_item(obj, key, value): Sets the key to the given value in the dictionary object. Raises TypeError if the object is not a dictionary. if not isinstance(obj, dict): raise TypeError(\\"Object is not a dictionary\\") obj[key] = value def mapping_del_item(obj, key): Deletes the key (and its corresponding value) from the dictionary object. Raises KeyError if the key is not found. del obj[key] def mapping_has_key(obj, key): Returns True if the dictionary object contains the key, otherwise returns False. return key in obj def mapping_keys(obj): Returns a list of all keys in the dictionary object. return list(obj.keys()) def mapping_values(obj): Returns a list of all values in the dictionary object. return list(obj.values()) def mapping_items(obj): Returns a list of all key-value pairs (tuples) in the dictionary object. return list(obj.items())"},{"question":"**Objective**: Implement a Python function that connects to an NNTP server, retrieves a list of newsgroups with more than a specified number of articles, and then fetches and displays the subject lines of the most recent articles in those newsgroups. **Function Signature**: ```python def fetch_newsgroups_info(nntp_host: str, min_articles: int, recent_count: int) -> None: ``` **Inputs**: - `nntp_host` (str): The hostname of the NNTP server to connect to. - `min_articles` (int): The minimum number of articles a newsgroup must have. - `recent_count` (int): The number of most recent articles\' subject lines to fetch and display from each filtered newsgroup. **Output**: The function should print the following information: - For each newsgroup that has more than `min_articles`: - The name of the newsgroup, and the total number of articles. - The subject line of each of the `recent_count` most recent articles in that newsgroup. **Constraints**: - Handle any potential exceptions that might occur during NNTP interactions, such as connection errors, command errors, or data parsing issues. - Assume the NNTP server\'s `LIST` command might return a large set of data. **Example**: ```python def fetch_newsgroups_info(nntp_host: str, min_articles: int, recent_count: int) -> None: try: # Establish a connection to the NNTP server with nntplib.NNTP(nntp_host) as server: # Fetch the list of all newsgroups response, newsgroups = server.list() # Filter newsgroups based on minimum number of articles filtered_groups = [group for group in newsgroups if int(group[1]) - int(group[2]) + 1 > min_articles] for group in filtered_groups: group_name = group[0] article_count = int(group[1]) - int(group[2]) + 1 print(f\\"Newsgroup: {group_name}, Articles: {article_count}\\") # Select the current newsgroup to fetch articles server.group(group_name) response, overviews = server.over((article_count - recent_count + 1, article_count)) for id, overview in overviews: subject = nntplib.decode_header(overview[\'subject\']) print(f\\"Article ID: {id}, Subject: {subject}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage: fetch_newsgroups_info(\'news.gmane.io\', 1000, 5) ``` In the above example: - The `fetch_newsgroups_info()` function connects to \'news.gmane.io\', - Filters out newsgroups that have more than 1000 articles, - For each filtered newsgroup, it prints the name and total article count, - It then fetches and prints the subject lines of the 5 most recent articles. **Note**: Ensure the function handles network exceptions and errors gracefully without breaking execution.","solution":"import nntplib def fetch_newsgroups_info(nntp_host: str, min_articles: int, recent_count: int) -> None: try: # Establish a connection to the NNTP server with nntplib.NNTP(nntp_host) as server: # Fetch the list of all newsgroups response, newsgroups = server.list() # Filter newsgroups based on minimum number of articles filtered_groups = [group for group in newsgroups if (int(group[1]) - int(group[2]) + 1) > min_articles] for group in filtered_groups: group_name = group[0] article_count = (int(group[1]) - int(group[2]) + 1) print(f\\"Newsgroup: {group_name}, Articles: {article_count}\\") # Select the current newsgroup to fetch articles server.group(group_name) response, overviews = server.over((article_count - recent_count + 1, article_count)) for id, overview in overviews: subject = nntplib.decode_header(overview[\'subject\']) print(f\\"Article ID: {id}, Subject: {subject}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective**: Implement a function to asynchronously execute multiple shell commands concurrently, gather their outputs, and return the results. **Problem Statement**: You are tasked with implementing an asynchronous function that takes a list of shell commands and executes them concurrently. The function should capture both stdout and stderr of each command, wait for all commands to complete, and return a list of results. Each result should contain the command, its exit code, stdout, and stderr. **Function Signature**: ```python import asyncio from typing import List, Tuple, Dict async def run_commands(commands: List[str]) -> List[Dict[str, Tuple[int, str, str]]]: pass ``` **Input**: - `commands` (List[str]): A list of shell commands to be executed. **Output**: - Returns a list of dictionaries. Each dictionary should contain: - `command` (str): The shell command executed. - `exit_code` (int): The exit code of the command. - `stdout` (str): The standard output of the command. - `stderr` (str): The standard error of the command. **Constraints**: - You must use the asyncio library to manage subprocesses. - Commands should be executed concurrently. - Properly handle potential shell injection vulnerabilities by ensuring commands are safely quoted. **Example**: ```python import asyncio from typing import List, Tuple, Dict async def run_commands(commands: List[str]) -> List[Dict[str, Tuple[int, str, str]]]: async def run(cmd): proc = await asyncio.create_subprocess_shell(cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return { \'command\': cmd, \'exit_code\': proc.returncode, \'stdout\': stdout.decode(), \'stderr\': stderr.decode() } results = await asyncio.gather(*[run(cmd) for cmd in commands]) return results # Example usage async def main(): commands = [\\"echo Hello, World!\\", \\"ls non_existing_dir\\"] results = await run_commands(commands) for result in results: print(f\\"Command: {result[\'command\']}nExit Code: {result[\'exit_code\']}nStdout: {result[\'stdout\']}nStderr: {result[\'stderr\']}n\\") asyncio.run(main()) ``` **Explanation**: In the example provided: - The commands \\"echo Hello, World!\\" and \\"ls non_existing_dir\\" are executed concurrently. - The function captures their outputs and exit codes. - The results are then printed, showing the command, its exit code, stdout, and stderr. **Performance Considerations**: - Ensure that the function can handle a moderate number of concurrent subprocesses efficiently. - Consider potential memory constraints when capturing large outputs.","solution":"import asyncio from typing import List, Dict, Tuple async def run_commands(commands: List[str]) -> List[Dict[str, Tuple[int, str, str]]]: async def run(cmd): proc = await asyncio.create_subprocess_shell(cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return { \'command\': cmd, \'exit_code\': proc.returncode, \'stdout\': stdout.decode(), \'stderr\': stderr.decode() } results = await asyncio.gather(*[run(cmd) for cmd in commands]) return results"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding and ability to use the `shutil` module for high-level file operations, including file copying, moving, archiving, and handling file metadata. # Problem Statement You are tasked with creating a Python script that organizes and manages a directory of project files. The requirements for the script are as follows: 1. **Organize Files:** - Create a function `organize_files(src_dir, dst_dir)` that recursively copies all files from the `src_dir` directory to the `dst_dir` directory. - The function should preserve the original directory structure. - If `dst_dir` does not exist, it should be created. 2. **Generate Archive Backup:** - Create a function `backup_directory(src_dir, archive_name, archive_format)` that generates an archive of the `src_dir` directory. - The archive can be in any of the supported formats: zip, tar, gztar, bztar, xztar. - The output archive should be named `archive_name` (without extension), with the proper extension based on the specified format. 3. **Selective Cleanup:** - Create a function `cleanup_directory(dir_path, pattern)` that removes all files in `dir_path` that match the given glob-style `pattern`. - Ensure that directories and files that do not match the pattern remain untouched. 4. **Restore from Archive:** - Create a function `restore_from_backup(archive_path, extract_to_dir)` that unpacks the specified archive to the `extract_to_dir`. - Ensure that the existing structure in `extract_to_dir` is maintained and not overwritten unless there are conflicts with extracted files. # Constraints and Considerations: - `src_dir`, `dst_dir`, `archive_name`, `archive_format`, `dir_path`, `pattern`, and `archive_path` are all strings representing valid paths or names. - Handle exceptions appropriately where directories or files may not exist or cannot be accessed. - Ensure your solution is platform-independent, working on both Windows and Unix-based systems. - You are not required to handle symbolic links unless specified, but ensure that the provided files\' metadata like permissions are preserved. # Performance Requirements: - Optimize for minimal memory usage where possible. - Ensure that the file operations are efficient, especially when dealing with large directories or files. # Example Usage: ```python organize_files(\'/path/to/source\', \'/path/to/destination\') backup_directory(\'/path/to/source\', \'backup\', \'gztar\') cleanup_directory(\'/path/to/clean\', \'*.log\') restore_from_backup(\'/path/to/backup.tar.gz\', \'path/to/restore\') ``` # Implementation: Implement the following functions: 1. `def organize_files(src_dir: str, dst_dir: str) -> None:` 2. `def backup_directory(src_dir: str, archive_name: str, archive_format: str) -> str:` 3. `def cleanup_directory(dir_path: str, pattern: str) -> None:` 4. `def restore_from_backup(archive_path: str, extract_to_dir: str) -> None:` Your code should be efficient, handle edge cases, and be well-documented.","solution":"import os import shutil from glob import glob def organize_files(src_dir: str, dst_dir: str) -> None: Recursively copies all files from src_dir to dst_dir, preserving the directory structure. Parameters: src_dir (str): Source directory. dst_dir (str): Destination directory. if not os.path.exists(dst_dir): os.makedirs(dst_dir) for root, _, files in os.walk(src_dir): for file in files: src_file_path = os.path.join(root, file) relative_path = os.path.relpath(root, src_dir) dest_file_path = os.path.join(dst_dir, relative_path, file) dest_dir = os.path.dirname(dest_file_path) if not os.path.exists(dest_dir): os.makedirs(dest_dir) shutil.copy2(src_file_path, dest_file_path) def backup_directory(src_dir: str, archive_name: str, archive_format: str) -> str: Creates an archive of the src_dir. Parameters: src_dir (str): Source directory to archive. archive_name (str): Name of the archive file (without extension). archive_format (str): Format of the archive (e.g., \'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\'). Returns: str: Path to the created archive. return shutil.make_archive(archive_name, archive_format, src_dir) def cleanup_directory(dir_path: str, pattern: str) -> None: Removes files in dir_path that match the given pattern. Parameters: dir_path (str): Directory path to clean up. pattern (str): Glob-style pattern to match files. files_to_remove = glob(os.path.join(dir_path, pattern)) for file_path in files_to_remove: if os.path.isfile(file_path): os.remove(file_path) def restore_from_backup(archive_path: str, extract_to_dir: str) -> None: Restores files from the archive_path to extract_to_dir. Parameters: archive_path (str): Path to the archive file. extract_to_dir (str): Directory to extract the archive to. shutil.unpack_archive(archive_path, extract_to_dir)"},{"question":"Objective Demonstrate your understanding of seaborn by creating a complex figure that visualizes various aspects of a given dataset. You will need to use both axes-level and figure-level functions and ensure the visualizations are clear and informative. Problem Statement Given a dataset of iris flowers, you are required to create a series of visualizations that explore the relationships and distributions of the variables within the dataset. Dataset The dataset contains the following columns: - `sepal_length`: Sepal length of the flower. - `sepal_width`: Sepal width of the flower. - `petal_length`: Petal length of the flower. - `petal_width`: Petal width of the flower. - `species`: Species of the iris flower. Instructions 1. Load the `iris` dataset using seaborn\'s load_dataset function. 2. Create a figure with two subplots, arranged in a 1 row by 2 columns layout: - In the first subplot, create a scatter plot to visualize the relationship between `sepal_length` and `petal_length`. Color the points according to the species of the iris flower. - In the second subplot, create a kernel density plot (KDE) to visualize the distribution of `sepal_width`. Color the density plots according to the species of the iris flower. 3. Create a faceted plot using `FacetGrid` to visualize the distribution of `petal_length` for each species. Use histograms for the distribution plots. 4. Customize the figures: - Add appropriate axis labels and titles for each subplot. - Place the legend outside of the plots where applicable. Expected Output - A figure with the specified subplots and customizations. - A faceted plot with histograms for `petal_length` distributions. Constraints - Use seaborn and matplotlib for the visualizations. - Ensure the plots are aesthetically pleasing and information-rich. # Example Code (Optional) ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset iris = sns.load_dataset(\\"iris\\") # Create figure with subplots fig, axes = plt.subplots(1, 2, figsize=(14, 6)) # Scatter plot for sepal_length vs petal_length sns.scatterplot(data=iris, x=\'sepal_length\', y=\'petal_length\', hue=\'species\', ax=axes[0]) axes[0].set_title(\'Sepal Length vs Petal Length\') axes[0].set_xlabel(\'Sepal Length\') axes[0].set_ylabel(\'Petal Length\') # KDE plot for sepal_width sns.kdeplot(data=iris, x=\'sepal_width\', hue=\'species\', multiple=\'stack\', ax=axes[1]) axes[1].set_title(\'Distribution of Sepal Width\') axes[1].set_xlabel(\'Sepal Width\') # Configuring legends outside plot area axes[0].legend(loc=\'upper right\', bbox_to_anchor=(1.2, 1)) axes[1].legend(loc=\'upper right\', bbox_to_anchor=(1.2, 1)) # Creating FacetGrid for histograms of petal_length g = sns.FacetGrid(iris, col=\\"species\\", height=4, aspect=1) g.map(sns.histplot, \\"petal_length\\") # Adjusting axis labels g.set_axis_labels(\\"Petal Length\\", \\"Count\\") plt.show() ``` You are expected to write the complete code fulfilling the above requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_iris_visualizations(): # Load dataset iris = sns.load_dataset(\\"iris\\") # Create figure with subplots fig, axes = plt.subplots(1, 2, figsize=(14, 6)) # Scatter plot for sepal_length vs petal_length sns.scatterplot(data=iris, x=\'sepal_length\', y=\'petal_length\', hue=\'species\', ax=axes[0]) axes[0].set_title(\'Sepal Length vs Petal Length\') axes[0].set_xlabel(\'Sepal Length\') axes[0].set_ylabel(\'Petal Length\') # KDE plot for sepal_width sns.kdeplot(data=iris, x=\'sepal_width\', hue=\'species\', multiple=\'stack\', common_norm=False, ax=axes[1]) axes[1].set_title(\'Distribution of Sepal Width\') axes[1].set_xlabel(\'Sepal Width\') # Configuring legends outside plot area axes[0].legend(loc=\'upper right\', bbox_to_anchor=(1.2, 1)) axes[1].legend(loc=\'upper right\', bbox_to_anchor=(1.2, 1)) # Creating FacetGrid for histograms of petal_length g = sns.FacetGrid(iris, col=\\"species\\", height=4, aspect=1) g.map(sns.histplot, \\"petal_length\\") # Adjusting axis labels g.set_axis_labels(\\"Petal Length\\", \\"Count\\") plt.show()"},{"question":"You are required to demonstrate your understanding of seaborn by visualizing a combination of univariate and bivariate distributions using KDE plots. This will help assess your ability to use seaborn for creating different types of plots and customizing them according to given specifications. # Task: 1. Load the `tips` dataset from seaborn. 2. Create a KDE plot of the `total_bill` column along the x-axis. 3. Flip the plot and create another KDE plot of the `total_bill` column along the y-axis. 4. Load the `iris` dataset from seaborn and create KDE plots for each column. 5. For the `tips` dataset, create a stacked KDE plot for the `total_bill` variable, categorized by the `time` variable. 6. Normalize the stacked distribution from point 5 and provide it as another plot. 7. Load the `geyser` dataset from seaborn and create a bivariate KDE plot for the `waiting` and `duration` columns, with filled contours. 8. Modify the appearance of the plot in point 7 using a custom colormap `mako` and increase the levels of contours to 100. # Specifications: - The plots should be clearly titled and labeled for readability. - Ensure appropriate use of bandwidth adjustments to show variation in the data distribution. - Use hues and stacking wherever required to differentiate categories. # Input: None (All datasets are loaded within the code). # Output: Eight separate plots as specified in the task. # Constraints: - You must use the seaborn library to create these plots. - The plots should be displayed in a clean and organized manner, so avoid any overlapping or clustering that reduces readability. # Example Code Snippet: (This code snippet demonstrates how to load the datasets and create a simple KDE plot.) ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Create KDE plot for total_bill column sns.kdeplot(data=tips, x=\\"total_bill\\") plt.show() ``` Use the above snippet as a starting point and build upon it to complete the task.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_all(): # Load datasets tips = sns.load_dataset(\\"tips\\") iris = sns.load_dataset(\\"iris\\") geyser = sns.load_dataset(\\"geyser\\") # Plot 1: KDE plot of total_bill on x-axis plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\") plt.title(\\"KDE plot of Total Bill (X-axis)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.show() # Plot 2: KDE plot of total_bill on y-axis plt.figure() sns.kdeplot(data=tips, y=\\"total_bill\\") plt.title(\\"KDE plot of Total Bill (Y-axis)\\") plt.xlabel(\\"Density\\") plt.ylabel(\\"Total Bill\\") plt.show() # Plot 3: KDE plots for each column in iris dataset plt.figure() iris_columns = iris.columns[:-1] # Exclude the species column for column in iris_columns: sns.kdeplot(data=iris, x=column, label=column) plt.title(\\"KDE plots for Iris Dataset Columns\\") plt.xlabel(\\"Value\\") plt.ylabel(\\"Density\\") plt.legend() plt.show() # Plot 4: Stacked KDE plot for total_bill by time plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"stack\\") plt.title(\\"Stacked KDE plot of Total Bill by Time\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.show() # Plot 5: Normalized stacked KDE plot for total_bill by time plt.figure() sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", multiple=\\"fill\\") plt.title(\\"Normalized Stacked KDE plot of Total Bill by Time\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Proportion\\") plt.show() # Plot 6: Bivariate KDE plot for waiting and duration columns in geyser dataset plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", fill=True) plt.title(\\"Bivariate KDE plot of Waiting and Duration\\") plt.xlabel(\\"Waiting\\") plt.ylabel(\\"Duration\\") plt.show() # Plot 7: Bivariate KDE plot for waiting and duration with custom colormap and increased levels plt.figure() sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", fill=True, cmap=\\"mako\\", levels=100) plt.title(\\"Bivariate KDE plot with Colormap (Mako) and Increased Levels\\") plt.xlabel(\\"Waiting\\") plt.ylabel(\\"Duration\\") plt.show()"},{"question":"**XML Processing with `xml.dom.pulldom`** **Objective**: Use the `xml.dom.pulldom` module to process an XML string, identify certain elements, and print their content based on specific conditions. You are provided with an XML string representing a collection of books. Each book has a `<title>`, `<author>`, `<genre>`, and `<price>` element. Your task is to write a function `process_books(xml_string: str) -> None` that processes this XML string and prints the titles of all books in the \\"Fiction\\" genre priced above 20 units. # Input: - `xml_string` (str): An XML string containing multiple `<book>` elements. # Output: - Print the titles of books in the \\"Fiction\\" genre priced above 20 units. # Example: Given the following XML input: ```xml <bookstore> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <genre>Fiction</genre> <price>30</price> </book> <book> <title>A Brief History of Time</title> <author>Stephen Hawking</author> <genre>Non-Fiction</genre> <price>15</price> </book> <book> <title>1984</title> <author>George Orwell</author> <genre>Fiction</genre> <price>18</price> </book> </bookstore> ``` Your function should output: ``` The Great Gatsby ``` # Constraints and requirements: - Ensure you use the `xml.dom.pulldom` module for parsing the XML. - Only print the titles of books that meet the criteria (Fiction genre and price > 20). - Handle cases where the XML structure may have multiple book entries with different or missing elements gracefully. # Skeleton Implementation: ```python from xml.dom import pulldom def process_books(xml_string: str) -> None: doc = pulldom.parseString(xml_string) current_book = None is_fiction_genre = False book_price = 0 for event, node in doc: if event == pulldom.START_ELEMENT: if node.tagName == \'book\': current_book = {} elif node.tagName == \'genre\': doc.expandNode(node) is_fiction_genre = (node.firstChild.data == \'Fiction\') elif node.tagName == \'price\': doc.expandNode(node) book_price = float(node.firstChild.data) elif node.tagName == \'title\' and current_book is not None: doc.expandNode(node) current_book[\'title\'] = node.firstChild.data elif event == pulldom.END_ELEMENT and node.tagName == \'book\': if current_book is not None and is_fiction_genre and book_price > 20: print(current_book[\'title\']) current_book = None is_fiction_genre = False book_price = 0 # Example usage xml_string = <bookstore> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <genre>Fiction</genre> <price>30</price> </book> <book> <title>A Brief History of Time</title> <author>Stephen Hawking</author> <genre>Non-Fiction</genre> <price>15</price> </book> <book> <title>1984</title> <author>George Orwell</author> <genre>Fiction</genre> <price>18</price> </book> </bookstore> process_books(xml_string) ```","solution":"from xml.dom import pulldom def process_books(xml_string: str) -> None: doc = pulldom.parseString(xml_string) current_title = None is_fiction_genre = False book_price = 0 for event, node in doc: if event == pulldom.START_ELEMENT: if node.tagName == \'title\': doc.expandNode(node) current_title = node.firstChild.data elif node.tagName == \'genre\': doc.expandNode(node) is_fiction_genre = (node.firstChild.data == \'Fiction\') elif node.tagName == \'price\': doc.expandNode(node) book_price = float(node.firstChild.data) elif event == pulldom.END_ELEMENT: if node.tagName == \'book\': if is_fiction_genre and book_price > 20 and current_title: print(current_title) current_title = None is_fiction_genre = False book_price = 0 # Example usage xml_string = <bookstore> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <genre>Fiction</genre> <price>30</price> </book> <book> <title>A Brief History of Time</title> <author>Stephen Hawking</author> <genre>Non-Fiction</genre> <price>15</price> </book> <book> <title>1984</title> <author>George Orwell</author> <genre>Fiction</genre> <price>18</price> </book> </bookstore> process_books(xml_string)"},{"question":"You are provided with a dataset containing information about various movies, including their genres, budgets, gross earnings, and IMDb ratings. Your task is to create a series of point plots using seaborn to visualize different relationships within this dataset. You will use the seaborn library to perform these visualizations and demonstrate your understanding of its advanced features. **Dataset Description:** The dataset `movies` contains the following columns: - `genre`: The genre of the movie (e.g., Action, Comedy, Drama). - `budget`: The budget of the movie in millions of dollars. - `gross`: The gross earning of the movie in millions of dollars. - `imdb_rating`: The IMDb rating of the movie. - `year`: The release year of the movie. **Requirements:** 1. **Basic `pointplot`:** - Plot the average IMDb rating (`imdb_rating`) for each genre (`genre`). ```python sns.pointplot(data=movies, x=\'genre\', y=\'imdb_rating\') ``` 2. **Grouping and Differentiation:** - Plot the average gross earning (`gross`) for each genre (`genre`), and further differentiate the data by the release year (`year`) using the `hue` parameter. ```python sns.pointplot(data=movies, x=\'genre\', y=\'gross\', hue=\'year\') ``` 3. **Custom Markers and Line Styles:** - Create a plot similar to the previous one but use custom markers and line styles for better differentiation of `year`. ```python sns.pointplot( data=movies, x=\'genre\', y=\'gross\', hue=\'year\', markers=[\'o\', \'s\', \'D\', \'^\', \'v\'], linestyles=[\'-\', \'--\', \'-.\', \':\', \'-\'] ) ``` 4. **Error Bars:** - Use error bars to show the standard deviation of IMDb ratings for each genre. ```python sns.pointplot(data=movies, x=\'genre\', y=\'imdb_rating\', errorbar=\'sd\') ``` 5. **Appearance Customization:** - Customize the appearance of the point plot for gross earnings so that the points have a specific color, no line style, and are represented with a \'D\' marker. Additionally, set the cap size for error bars (showing confidence intervals). ```python sns.pointplot( data=movies, x=\'gross\', y=\'genre\', errorbar=(\'ci\', 100), capsize=.4, color=\'.5\', linestyle=\'none\', marker=\'D\' ) ``` 6. **Handling Overplotting:** - Plot the gross earnings of movies using a `stripplot` to show individual points, and then overlay a `pointplot` with dodge to avoid overplotting. ```python sns.stripplot(data=movies, x=\'genre\', y=\'gross\', hue=\'year\', dodge=True, alpha=.2, legend=False) sns.pointplot(data=movies, x=\'genre\', y=\'gross\', hue=\'year\', dodge=.4, linestyle=\'none\', errorbar=None, marker=\'_\', markersize=20, markeredgewidth=3) ``` 7. **Aggregating Data:** - Suppose you have a dataset (`movies_wide`) with aggregated data such that each column represents a different genre and the rows represent average values for different metrics. Create a `pointplot` to visualize the average gross values across genres. ```python movies_wide = movies.pivot(index=\'year\', columns=\'genre\', values=\'gross\') sns.pointplot(movies_wide) ``` 8. **One-dimensional Data:** - Plot the data for a single year (e.g., 2020) showing the gross earnings for each genre. ```python sns.pointplot(movies_wide.loc[2020]) ``` 9. **Custom Tick Labels:** - Customize the tick labels on a plot to show only the last two digits of the year for the x-axis. ```python sns.pointplot(movies_wide.loc[2020], formatter=lambda x: f\\"{x % 100}\\") ``` 10. **Preserve Native Scale:** - Create a plot where the native scale of the years is preserved, and add a custom marker for a specific data point (e.g., gross of 500 million in the year 2015). ```python ax = sns.pointplot(movies_wide.loc[:, \'Action\'], native_scale=True) ax.plot(2015, 500, marker=\\"*\\", color=\\"r\\", markersize=10) ``` **Constraints:** - Your solution should be efficient and make appropriate use of seaborn functionalities to achieve the desired visualizations. - Ensure the data is correctly aggregated or pivoted where necessary. - Customize appearances according to the specified requirements to effectively differentiate between various plot elements. Implement the functions and plots accordingly, and provide appropriate comments to explain each plotting step.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_avg_imdb_rating_per_genre(movies): Plot the average IMDb rating for each genre. sns.pointplot(data=movies, x=\'genre\', y=\'imdb_rating\') plt.show() def plot_avg_gross_per_genre_and_year(movies): Plot the average gross earnings for each genre, differentiated by release year. sns.pointplot(data=movies, x=\'genre\', y=\'gross\', hue=\'year\') plt.show() def plot_custom_markers_and_linestyles(movies): Plot the average gross earnings for each genre, differentiated by release year with custom markers and line styles. sns.pointplot( data=movies, x=\'genre\', y=\'gross\', hue=\'year\', markers=[\'o\', \'s\', \'D\', \'^\', \'v\'], linestyles=[\'-\', \'--\', \'-.\', \':\', \'-\'] ) plt.show() def plot_error_bars(movies): Use error bars to show the standard deviation of IMDb ratings for each genre. sns.pointplot(data=movies, x=\'genre\', y=\'imdb_rating\', errorbar=\'sd\') plt.show() def plot_custom_appearance(movies): Customize the appearance of the point plot for gross earnings. sns.pointplot( data=movies, x=\'gross\', y=\'genre\', errorbar=(\'ci\', 100), capsize=.4, color=\'.5\', linestyle=\'none\', marker=\'D\' ) plt.show() def plot_handling_overplotting(movies): Handle overplotting by overlaying pointplot with stripplot for gross earnings. sns.stripplot(data=movies, x=\'genre\', y=\'gross\', hue=\'year\', dodge=True, alpha=.2, legend=False) sns.pointplot(data=movies, x=\'genre\', y=\'gross\', hue=\'year\', dodge=.4, linestyle=\'none\', errorbar=None, marker=\'_\', markersize=20, markeredgewidth=3) plt.show() def plot_aggregated_data(movies): Plot the average gross values across genres using a pivoted dataset. movies_wide = movies.pivot(index=\'year\', columns=\'genre\', values=\'gross\') sns.pointplot(data=movies_wide) plt.show() def plot_data_for_single_year(movies, year): Plot the data for a single year (e.g., 2020) showing the gross earnings for each genre. movies_wide = movies.pivot(index=\'year\', columns=\'genre\', values=\'gross\') sns.pointplot(data=movies_wide.loc[year]) plt.show() def plot_custom_tick_labels(movies): Customize the tick labels to show only the last two digits of the year on the x-axis. movies_wide = movies.pivot(index=\'year\', columns=\'genre\', values=\'gross\') ax = sns.pointplot(data=movies_wide.loc[2020]) ax.xaxis.set_major_formatter(lambda x, _: f\\"{int(x) % 100}\\") plt.show() def plot_preserve_native_scale(movies): Preserve the native scale of the years and add a custom marker for a specific data point. movies_wide = movies.pivot(index=\'year\', columns=\'genre\', values=\'gross\') ax = sns.pointplot(data=movies_wide.loc[:, \'Action\'], native_scale=True) ax.plot(2015, 500, marker=\\"*\\", color=\\"r\\", markersize=10) plt.show()"},{"question":"**Python Coding Assessment: AST Manipulation and Analysis** **Objective:** Implement a function that takes a Python source code as input, parses it into an Abstract Syntax Tree (AST), modifies certain nodes, and then converts the modified AST back into Python source code. **Description:** You are asked to write a Python function `transform_code(source: str) -> str` that performs the following tasks: 1. **Parse** the input `source` code string into an AST using the `ast.parse()` function. 2. **Transform** the AST by performing the following modifications: - Replace all occurrences of the binary operator `Add` with the binary operator `Mult`. - Replace all function calls to `print` with function calls to a new function named `custom_print`. 3. **Unparse** the modified AST back into a Python source code string using the `ast.unparse()` function. 4. **Return** the modified source code as a string. **Function Signature:** ```python def transform_code(source: str) -> str: pass ``` **Input:** - `source` (str): A string containing valid Python source code. **Output:** - (str): A string containing the modified Python source code. **Constraints:** - The input code will not contain any syntax errors. - The transformations should preserve the original structure and formatting as much as possible. **Example:** ```python source_code = def my_function(x, y): print(x + y) return x + y transformed_code = transform_code(source_code) print(transformed_code) ``` *Expected Output:* ```python def my_function(x, y): custom_print(x * y) return x * y ``` **Notes:** - Utilize the `ast.NodeTransformer` class to perform the transformations. - Ensure that the new function `custom_print` is called with the same arguments as the original `print`. **Hints:** - You can use the `ast.BinOp` class to identify binary operations and check their operator type. - The `ast.Call` class can help you identify function calls and their arguments.","solution":"import ast class ASTTransformer(ast.NodeTransformer): def visit_BinOp(self, node): self.generic_visit(node) if isinstance(node.op, ast.Add): node.op = ast.Mult() return node def visit_Call(self, node): self.generic_visit(node) if isinstance(node.func, ast.Name) and node.func.id == \'print\': node.func.id = \'custom_print\' return node def transform_code(source: str) -> str: tree = ast.parse(source) transformer = ASTTransformer() transformed_tree = transformer.visit(tree) ast.fix_missing_locations(transformed_tree) return ast.unparse(transformed_tree)"},{"question":"Objective: Assess your understanding of the `seaborn` package, specifically the `seaborn.objects` module, and your ability to create and customize various types of visualizations. Problem Statement: Using the `penguins` dataset provided by `seaborn`, create a function `visualize_penguins` that generates specific plots. Your function should: 1. Load the `penguins` dataset. 2. Create a histogram representing the distribution of the `flipper_length_mm`, customized with 15 bins. 3. Normalize the histogram to show proportions instead of raw counts. 4. Use different colors for the sexes (`male` and `female`). 5. Create a plot comparing distributions of `flipper_length_mm` across different islands using facetting. 6. Normalize each distribution independently (`common_norm=False`). Function Signature: ```python def visualize_penguins(): pass ``` Constraints: - You must use `seaborn.objects` exclusively for visualizations. - The function should not accept any parameters and should directly load the `penguins` dataset from seaborn. Expected Output: When the function `visualize_penguins` is called, it should display two plots: 1. A proportion-normalized histogram with different colors for sexes. 2. A facet plot comparing the proportions of `flipper_length_mm` across different islands, each normalized independently. Example: Calling `visualize_penguins()` should display the required plots directly without any additional user input. Here is a code template to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset def visualize_penguins(): # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create the histogram with 15 bins and normalized proportions p1 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(bins=15, stat=\\"proportion\\", color=\\"sex\\")) # Step 3: Create a facet plot comparing distributions across islands, normalized independently p2 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)) # Display the plots p1.show() p2.show() # Call the function to test visualize_penguins() ``` Ensure your function adheres to the above guidelines and produces the expected results.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_penguins(): Generates visualizations for the penguins dataset including: 1. A proportion-normalized histogram for flipper_length_mm by sex. 2. Faceted proportion-normalized histograms for flipper_length_mm by island. # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create the histogram with 15 bins and normalized proportions, colored by sex p1 = ( so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"sex\\") .add(so.Bars(), so.Hist(bins=15, stat=\\"proportion\\")) ) p1.show() # Step 3: Create a facet plot comparing distributions across islands, normalized independently p2 = ( so.Plot(penguins, x=\\"flipper_length_mm\\") .facet(\\"island\\") .add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)) ) p2.show()"},{"question":"**Coding Assessment Question:** # User Account Management with the `pwd` Module As a system administrator, you are tasked with managing user accounts on a Unix-based system. Using the `pwd` module, you need to write functions to retrieve and analyze user information according to the requirements outlined below. **Requirements:** 1. **Function: `get_user_info_by_name(username)`** - **Input:** A string `username` representing the login name of the user to be retrieved. - **Output:** A dictionary containing the user\'s information with attribute names as keys. For example: ```python { \'pw_name\': \'jdoe\', \'pw_passwd\': \'x\', \'pw_uid\': 1001, \'pw_gid\': 1001, \'pw_gecos\': \'John Doe\', \'pw_dir\': \'/home/jdoe\', \'pw_shell\': \'/bin/bash\' } ``` - **Error Handling:** If the user does not exist, return an appropriate message indicating the error. 2. **Function: `list_all_users()`** - **Input:** None - **Output:** A list of dictionaries, each containing information for one user, similar to the output of the `get_user_info_by_name` function. Ensure that users are sorted alphabetically by their login name. 3. **Function: `get_users_with_home_directory_prefix(prefix)`** - **Input:** A string `prefix` representing the beginning of a user\'s home directory. - **Output:** A list of usernames whose home directories start with the given prefix. For example, if `prefix` is `/home`, the function should return a list of users whose home directories start with `/home`. **Constraints:** - Ensure that your functions handle any edge cases, such as nonexistent users or invalid inputs gracefully. - The `get_user_info_by_name` function should work efficiently even if there are a large number of users. **Example Usage:** ```python # Example usage of the functions print(get_user_info_by_name(\'jdoe\')) # Output: # { # \'pw_name\': \'jdoe\', # \'pw_passwd\': \'x\', # \'pw_uid\': 1001, # \'pw_gid\': 1001, # \'pw_gecos\': \'John Doe\', # \'pw_dir\': \'/home/jdoe\', # \'pw_shell\': \'/bin/bash\' # } print(list_all_users()) # Output: # [ # {\'pw_name\': \'alice\', ...}, # {\'pw_name\': \'bob\', ...}, # ... # ] print(get_users_with_home_directory_prefix(\'/home\')) # Output: # [\'jdoe\', \'alice\', \'bob\', ...] ``` Use the `pwd` module effectively to accomplish these tasks. Write clean, well-documented code for each function implementation.","solution":"import pwd def get_user_info_by_name(username): Retrieve user information by username. Args: username (str): The login name of the user to be retrieved. Returns: dict: A dictionary containing the user\'s information. str: Error message if the user does not exist. try: user_info = pwd.getpwnam(username) return { \'pw_name\': user_info.pw_name, \'pw_passwd\': user_info.pw_passwd, \'pw_uid\': user_info.pw_uid, \'pw_gid\': user_info.pw_gid, \'pw_gecos\': user_info.pw_gecos, \'pw_dir\': user_info.pw_dir, \'pw_shell\': user_info.pw_shell } except KeyError: return f\\"User \'{username}\' does not exist.\\" def list_all_users(): List all users on the system sorted alphabetically by their login name. Returns: list: A list of dictionaries, each containing information for one user. all_users = pwd.getpwall() sorted_users = sorted(all_users, key=lambda user: user.pw_name) return [ { \'pw_name\': user.pw_name, \'pw_passwd\': user.pw_passwd, \'pw_uid\': user.pw_uid, \'pw_gid\': user.pw_gid, \'pw_gecos\': user.pw_gecos, \'pw_dir\': user.pw_dir, \'pw_shell\': user.pw_shell } for user in sorted_users ] def get_users_with_home_directory_prefix(prefix): Retrieve usernames with home directories that start with the given prefix. Args: prefix (str): The beginning of a user\'s home directory path. Returns: list: A list of usernames whose home directories start with the given prefix. all_users = pwd.getpwall() users_with_prefix = [user.pw_name for user in all_users if user.pw_dir.startswith(prefix)] return sorted(users_with_prefix)"},{"question":"Objective: To test the understanding and implementation of Python\'s asyncio package, specifically focusing on creating, running, and managing coroutines and tasks concurrently. Problem Statement: Write an asynchronous library that simulates a simple task scheduler. The scheduler will manage a list of tasks with different priorities and execute them concurrently while adhering to their priority. Requirements: 1. **Task Class**: - Create a `Task` class with properties: - `name` (str): The name of the task. - `priority` (int): The priority of the task. - `duration` (float): The time in seconds the task takes to complete. - The `Task` class should have an asynchronous method `run()` that simulates the task execution by sleeping for the given `duration`. 2. **Scheduler Class**: - Create a `Scheduler` class with the following methods: - `add_task(task: Task)`: Asynchronously adds a task to the scheduler\'s task list. - `run_tasks()`: Asynchronously runs all the tasks in the task list concurrently, but ensures that tasks with higher priority are started first. If multiple tasks have the same priority, run them in the order they were added. - The `run_tasks()` method should print the task name each time a task starts and finishes. 3. **Example Usage**: ``` import asyncio class Task: def __init__(self, name, priority, duration): self.name = name self.priority = priority self.duration = duration async def run(self): print(f\\"Task {self.name} started.\\") await asyncio.sleep(self.duration) print(f\\"Task {self.name} finished.\\") class Scheduler: def __init__(self): self.tasks = [] async def add_task(self, task): self.tasks.append(task) async def run_tasks(self): self.tasks.sort(key=lambda x: x.priority, reverse=True) await asyncio.gather(*(task.run() for task in self.tasks)) async def main(): scheduler = Scheduler() await scheduler.add_task(Task(\\"Task 1\\", 2, 3)) await scheduler.add_task(Task(\\"Task 2\\", 1, 2)) await scheduler.add_task(Task(\\"Task 3\\", 3, 1)) await scheduler.run_tasks() asyncio.run(main()) ``` Constraints: - Use Python 3.7 or higher. - Ensure that the task names and durations are appropriately logged when tasks start and finish. Input/Output: - **Input**: The input will be handled within the script by creating task objects and adding them to the scheduler. - **Output**: The output will be the print statements indicating when each task starts and finishes. Performance Requirements: - The solution should efficiently handle up to 1000 tasks with varying priorities and durations. Note: Proper error handling and resource management would be considered as additional merits.","solution":"import asyncio class Task: def __init__(self, name, priority, duration): self.name = name self.priority = priority self.duration = duration async def run(self): print(f\\"Task {self.name} started.\\") await asyncio.sleep(self.duration) print(f\\"Task {self.name} finished.\\") class Scheduler: def __init__(self): self.tasks = [] async def add_task(self, task): self.tasks.append(task) async def run_tasks(self): self.tasks.sort(key=lambda x: x.priority, reverse=True) await asyncio.gather(*(task.run() for task in self.tasks)) async def main(): scheduler = Scheduler() await scheduler.add_task(Task(\\"Task 1\\", 2, 3)) await scheduler.add_task(Task(\\"Task 2\\", 1, 2)) await scheduler.add_task(Task(\\"Task 3\\", 3, 1)) await scheduler.run_tasks() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# IP Address Operations and Utilities You are required to implement a set of utility functions to work with IP addresses and networks using the `ipaddress` module in Python. Your task is to implement the following functions: 1. **summarize_and_compare(ip_range1, ip_range2)**: - **Input**: Two tuples, each containing two strings representing the starting and ending IP addresses of a range. - e.g., `ip_range1 = (\'192.168.0.1\', \'192.168.0.10\')`, `ip_range2 = (\'192.168.0.8\', \'192.168.0.20\')` - **Output**: A string \\"Overlap\\" if the two ranges overlap after summarizing them into networks, and \\"No Overlap\\" if they do not. - **Constraints**: The input IPs are guaranteed to be valid IPv4 addresses. - **Example**: ```python summarize_and_compare((\'192.168.0.1\', \'192.168.0.10\'), (\'192.168.0.8\', \'192.168.0.20\')) # Output: \'Overlap\' ``` 2. **generate_subnet(ip_network_str, new_prefix)**: - **Input**: - `ip_network_str`: A string representing an IP network with prefix notation (e.g., \'192.168.1.0/24\'). - `new_prefix`: An integer representing the new prefix length for the subnets. - **Output**: A list of strings, each representing a subnet in the new prefix length. - **Constraints**: Ensure the new prefix length is greater than or equal to the original prefix length. - **Example**: ```python generate_subnet(\'192.168.1.0/24\', 26) # Output: [\'192.168.1.0/26\', \'192.168.1.64/26\', \'192.168.1.128/26\', \'192.168.1.192/26\'] ``` # Implementation ```python import ipaddress def summarize_and_compare(ip_range1, ip_range2): first1 = ipaddress.ip_address(ip_range1[0]) last1 = ipaddress.ip_address(ip_range1[1]) first2 = ipaddress.ip_address(ip_range2[0]) last2 = ipaddress.ip_address(ip_range2[1]) summarized_network1 = list(ipaddress.summarize_address_range(first1, last1)) summarized_network2 = list(ipaddress.summarize_address_range(first2, last2)) for net1 in summarized_network1: for net2 in summarized_network2: if net1.overlaps(net2): return \\"Overlap\\" return \\"No Overlap\\" def generate_subnet(ip_network_str, new_prefix): network = ipaddress.ip_network(ip_network_str) if new_prefix < network.prefixlen: raise ValueError(\\"New prefix length must be greater than or equal to the original prefix length.\\") subnets = list(network.subnets(new_prefix=new_prefix)) return [str(subnet) for subnet in subnets] # Example usage: print(summarize_and_compare((\'192.168.0.1\', \'192.168.0.10\'), (\'192.168.0.8\', \'192.168.0.20\'))) # Output: \'Overlap\' print(generate_subnet(\'192.168.1.0/24\', 26)) # Output: [\'192.168.1.0/26\', \'192.168.1.64/26\', \'192.168.1.128/26\', \'192.168.1.192/26\'] ```","solution":"import ipaddress def summarize_and_compare(ip_range1, ip_range2): Checks if two ranges of IP addresses overlap after summarizing them into networks. first1 = ipaddress.ip_address(ip_range1[0]) last1 = ipaddress.ip_address(ip_range1[1]) first2 = ipaddress.ip_address(ip_range2[0]) last2 = ipaddress.ip_address(ip_range2[1]) summarized_network1 = list(ipaddress.summarize_address_range(first1, last1)) summarized_network2 = list(ipaddress.summarize_address_range(first2, last2)) for net1 in summarized_network1: for net2 in summarized_network2: if net1.overlaps(net2): return \\"Overlap\\" return \\"No Overlap\\" def generate_subnet(ip_network_str, new_prefix): Generates subnets of a given network with a new prefix length. network = ipaddress.ip_network(ip_network_str) if new_prefix < network.prefixlen: raise ValueError(\\"New prefix length must be greater than or equal to the original prefix length.\\") subnets = list(network.subnets(new_prefix=new_prefix)) return [str(subnet) for subnet in subnets]"},{"question":"Objective Write a Python program that manages user data by combining object serialization (`pickle`) and database operations (`sqlite3`). The user data includes basic information such as name, age, and email. You will be required to implement the following: 1. A `User` class with `name`, `age`, and `email` attributes. 2. Functions to serialize/deserialize user objects. 3. Functions to create, read, update, and delete user records in an SQLite database. Instructions 1. **User Class**: - Implement a `User` class with the following attributes: - `name` (string) - `age` (integer) - `email` (string) 2. **Serialization Functions**: - Implement a function `serialize_user(user: User) -> bytes` that takes a `User` object and returns its serialized form using `pickle`. - Implement a function `deserialize_user(user_data: bytes) -> User` that takes serialized user data and returns a `User` object. 3. **Database Functions**: - Implement the following functions to interact with an SQLite database: - `create_user_table(db_name: str) -> None`: Create a table named `users` with columns `name`, `age`, and `email`. - `insert_user(db_name: str, user: User) -> None`: Insert a serialized `User` object into the `users` table. - `get_user(db_name: str, name: str) -> User`: Retrieve and deserialize a `User` object by the `name` from the `users` table. - `update_user(db_name: str, user: User) -> None`: Update an existing user\'s details in the `users` table. - `delete_user(db_name: str, name: str) -> None`: Delete a user by the `name` from the `users` table. Example Usage ```python # Create user table create_user_table(\'test.db\') # Create a User object user = User(name=\'Alice\', age=30, email=\'alice@example.com\') # Insert the user into the database insert_user(\'test.db\', user) # Retrieve the user from the database retrieved_user = get_user(\'test.db\', \'Alice\') print(retrieved_user.name, retrieved_user.age, retrieved_user.email) # Update the user\'s information user.age = 31 update_user(\'test.db\', user) # Delete the user delete_user(\'test.db\', \'Alice\') ``` Constraints - You should handle exceptions where appropriate, such as database connection errors or issues with serialization. - Ensure your code follows good practices, including comments and docstrings. Submission Requirements - Submit the Python code implementing the specified functionality. - Include a brief explanation of your design choices and any assumptions made. Evaluation Criteria - Correctness and completeness of the implementation. - Clarity and organization of the code. - Proper use of `pickle` and `sqlite3` modules. - Handling of edge cases and errors.","solution":"import pickle import sqlite3 class User: def __init__(self, name: str, age: int, email: str): self.name = name self.age = age self.email = email def serialize_user(user: User) -> bytes: Serialize User object to bytes using pickle. return pickle.dumps(user) def deserialize_user(user_data: bytes) -> User: Deserialize bytes to User object using pickle. return pickle.loads(user_data) def create_user_table(db_name: str) -> None: Create a `users` table in the SQLite database. conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS users ( name TEXT PRIMARY KEY, age INTEGER, email TEXT, data BLOB ) \'\'\') conn.commit() conn.close() def insert_user(db_name: str, user: User) -> None: Insert a serialized User object into the users table. conn = sqlite3.connect(db_name) cursor = conn.cursor() user_data = serialize_user(user) cursor.execute(\'INSERT INTO users (name, age, email, data) VALUES (?, ?, ?, ?)\', (user.name, user.age, user.email, user_data)) conn.commit() conn.close() def get_user(db_name: str, name: str) -> User: Retrieve and deserialize a User object by name from the users table. conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'SELECT data FROM users WHERE name=?\', (name,)) result = cursor.fetchone() conn.close() if result: user_data = result[0] return deserialize_user(user_data) return None def update_user(db_name: str, user: User) -> None: Update an existing user\'s details in the users table. conn = sqlite3.connect(db_name) cursor = conn.cursor() user_data = serialize_user(user) cursor.execute(\'UPDATE users SET age=?, email=?, data=? WHERE name=?\', (user.age, user.email, user_data, user.name)) conn.commit() conn.close() def delete_user(db_name: str, name: str) -> None: Delete a user by name from the users table. conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'DELETE FROM users WHERE name=?\', (name,)) conn.commit() conn.close()"},{"question":"Coding Assessment Question: # Random Projection for Dimensionality Reduction Given the documentation on `sklearn.random_projection`, your task is to implement a set of functions that utilize random projections to reduce the dimensionality of a dataset. You will: 1. **Implement Gaussian Random Projection**: - Write a function to apply Gaussian random projection to reduce the dimensionality of a given dataset. 2. **Implement Sparse Random Projection**: - Write a function to apply Sparse random projection to reduce the dimensionality of a given dataset. 3. **Inverse Transform**: - Write a function to demonstrate the inverse transform process. # Function Specifications: 1. **apply_gaussian_random_projection** - **Input**: - `data` (numpy.ndarray): Original high-dimensional data of shape (n_samples, n_features). - `n_components` (int): Target number of dimensions. - **Output**: - `(numpy.ndarray)`: Data transformed into the reduced dimensions. 2. **apply_sparse_random_projection** - **Input**: - `data` (numpy.ndarray): Original high-dimensional data of shape (n_samples, n_features). - `n_components` (int): Target number of dimensions. - **Output**: - `(numpy.ndarray)`: Data transformed into the reduced dimensions. 3. **inverse_transform_sparse** - **Input**: - `data` (numpy.ndarray): Data transformed by sparse random projection. - `original_shape` (tuple): Shape of the original high-dimensional data (n_samples, n_features). - **Output**: - `(numpy.ndarray)`: Data approximated back to the original dimensionality. # Function Implementation: The functions should fulfill the following requirements: 1. **apply_gaussian_random_projection**: - Import and use `GaussianRandomProjection` from `sklearn.random_projection`. - Apply the Gaussian random projection to the `data` to reduce it to `n_components` dimensions. 2. **apply_sparse_random_projection**: - Import and use `SparseRandomProjection` from `sklearn.random_projection`. - Apply the sparse random projection to the `data` to reduce it to `n_components` dimensions. 3. **inverse_transform_sparse**: - Import and use `SparseRandomProjection` from `sklearn.random_projection`. - Configure the projection to compute inverse components. - Apply the `inverse_transform` method on the `data` to map it back to the original shape. # Example: ```python import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection def apply_gaussian_random_projection(data, n_components): transformer = GaussianRandomProjection(n_components=n_components) return transformer.fit_transform(data) def apply_sparse_random_projection(data, n_components): transformer = SparseRandomProjection(n_components=n_components) return transformer.fit_transform(data) def inverse_transform_sparse(data, original_shape): transformer = SparseRandomProjection(n_components=data.shape[1], compute_inverse_components=True) transformer.fit(np.zeros(original_shape)) # Fit transformer to the original shape dummy data return transformer.inverse_transform(data) # Example usage: X = np.random.rand(100, 10000) X_gaussian = apply_gaussian_random_projection(X, 500) X_sparse = apply_sparse_random_projection(X, 500) X_sparse_inverse = inverse_transform_sparse(X_sparse, (100, 10000)) print(\\"Original shape: \\", X.shape) print(\\"Gaussian reduced shape: \\", X_gaussian.shape) print(\\"Sparse reduced shape: \\", X_sparse.shape) print(\\"Inversed sparse shape: \\", X_sparse_inverse.shape) ``` # Constraints: - Target dimensions (`n_components`) should be less than the original features of the dataset. - All inputs must be valid numpy arrays of appropriate shapes. - You are expected to handle exceptions and ensure the input data is appropriate for dimensionality reduction.","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection def apply_gaussian_random_projection(data, n_components): Applies Gaussian random projection to reduce the dimensionality of data. Parameters: - data (numpy.ndarray): Original high-dimensional data. - n_components (int): Target number of dimensions. Returns: - numpy.ndarray: Data transformed into reduced dimensions. transformer = GaussianRandomProjection(n_components=n_components) return transformer.fit_transform(data) def apply_sparse_random_projection(data, n_components): Applies Sparse random projection to reduce the dimensionality of data. Parameters: - data (numpy.ndarray): Original high-dimensional data. - n_components (int): Target number of dimensions. Returns: - numpy.ndarray: Data transformed into reduced dimensions. transformer = SparseRandomProjection(n_components=n_components) return transformer.fit_transform(data) def inverse_transform_sparse(data, original_shape): Demonstrates the inverse transform process of sparse random projection. Parameters: - data (numpy.ndarray): Data transformed by sparse random projection. - original_shape (tuple): Shape of the original high-dimensional data. Returns: - numpy.ndarray: Data approximated back to the original dimensionality. n_components = data.shape[1] transformer = SparseRandomProjection(n_components=n_components, compute_inverse_components=True) transformer.fit(np.zeros(original_shape)) return transformer.inverse_transform(data)"},{"question":"# Advanced Garbage Collection Tracker Problem Statement: You are tasked with implementing a custom garbage collection monitoring tool using Python\'s `gc` module. This tool should: 1. Enable and disable automatic garbage collection. 2. Trigger manual garbage collections. 3. Collect comprehensive statistics during the garbage collection process, including the number of collections, collected objects, uncollectable objects, and time taken for each collection. 4. Allow setting custom garbage collection thresholds. 5. Provide a summary of all collected statistics upon request. Implementation Details: 1. **Class Definition**: - Define a class `GCMonitor` with the following methods: - `__init__(self)`: Initializes the monitor, setting up necessary variables and enabling garbage collection. - `enable_gc(self)`: Enables automatic garbage collection. - `disable_gc(self)`: Disables automatic garbage collection. - `manual_collect(self, generation=2)`: Manually triggers garbage collection for the specified generation. - `set_thresholds(self, threshold0, threshold1=None, threshold2=None)`: Sets the garbage collection thresholds. - `get_statistics(self)`: Returns a dictionary containing comprehensive statistics collected during the garbage collection process. - `callback(self, phase, info)`: A callback function that records relevant statistics. 2. **Garbage Collection Statistics**: - Use gc callbacks to collect the following statistics for each garbage collection cycle: - Number of collections. - Number of collected objects. - Number of uncollectable objects. - Time taken for each collection. 3. **Performance Requirements**: - Ensure that all operations are performed with minimal overhead to avoid significantly impacting the programs performance. - Properly handle edge cases and ensure robustness in a multi-threaded environment. Example Usage: ```python import gc import time class GCMonitor: def __init__(self): self.stats = { \'collections\': 0, \'collected\': [], \'uncollectable\': [], \'time_taken\': [] } self.is_gc_enabled = gc.isenabled() gc.callbacks.append(self.callback) gc.enable() def enable_gc(self): gc.enable() self.is_gc_enabled = True def disable_gc(self): gc.disable() self.is_gc_enabled = False def manual_collect(self, generation=2): start_time = time.perf_counter() collected = gc.collect(generation=generation) end_time = time.perf_counter() self.stats[\'collections\'] += 1 self.stats[\'collected\'].append(collected) self.stats[\'time_taken\'].append(end_time - start_time) def set_thresholds(self, threshold0, threshold1=None, threshold2=None): if threshold1 is None and threshold2 is None: gc.set_threshold(threshold0) elif threshold2 is None: gc.set_threshold(threshold0, threshold1) else: gc.set_threshold(threshold0, threshold1, threshold2) def get_statistics(self): return self.stats def callback(self, phase, info): if phase == \'start\': self.stats[\'start_time\'] = time.perf_counter() elif phase == \'stop\': end_time = time.perf_counter() self.stats[\'collections\'] += 1 self.stats[\'collected\'].append(info[\'collected\']) self.stats[\'uncollectable\'].append(info[\'uncollectable\']) self.stats[\'time_taken\'].append(end_time - self.stats[\'start_time\']) # Example usage of the GCMonitor monitor = GCMonitor() monitor.set_thresholds(700, 10, 10) monitor.manual_collect() stats = monitor.get_statistics() print(stats) ``` Constraints: - You are not allowed to use any other libraries for garbage collection management apart from the `gc` module standard in Python. - Ensure the class methods handle exceptions gracefully and provide meaningful error messages.","solution":"import gc import time class GCMonitor: def __init__(self): self.stats = { \'collections\': 0, \'collected\': [], \'uncollectable\': [], \'time_taken\': [] } self.is_gc_enabled = gc.isenabled() gc.callbacks.append(self.callback) gc.enable() def enable_gc(self): gc.enable() self.is_gc_enabled = True def disable_gc(self): gc.disable() self.is_gc_enabled = False def manual_collect(self, generation=2): start_time = time.perf_counter() collected = gc.collect(generation=generation) end_time = time.perf_counter() self.stats[\'collections\'] += 1 self.stats[\'collected\'].append(collected) self.stats[\'uncollectable\'].append(len(gc.garbage)) self.stats[\'time_taken\'].append(end_time - start_time) def set_thresholds(self, threshold0, threshold1=None, threshold2=None): if threshold1 is None and threshold2 is None: gc.set_threshold(threshold0) elif threshold2 is None: gc.set_threshold(threshold0, threshold1) else: gc.set_threshold(threshold0, threshold1, threshold2) def get_statistics(self): return self.stats def callback(self, phase, info): if phase == \'start\': self.stats[\'start_time\'] = time.perf_counter() elif phase == \'stop\': end_time = time.perf_counter() self.stats[\'collections\'] += 1 self.stats[\'collected\'].append(info.get(\'collected\', 0)) self.stats[\'uncollectable\'].append(info.get(\'uncollectable\', 0)) self.stats[\'time_taken\'].append(end_time - self.stats[\'start_time\'])"},{"question":"You are given a list of file paths. Each file path points to a particular type of file on the filesystem (e.g., regular file, directory, symbolic link, etc.). Your task is to implement a function that classifies these files into different categories based on their type, using the `stat` module. You should implement the following function: ```python def classify_files(file_paths): Classifies a list of file paths based on their file types. Parameters: file_paths (list): List of file paths (strings) to classify. Returns: dict: A dictionary with file types as keys and lists of file paths as values. The file types should include: - \'directory\' - \'character special device\' - \'block special device\' - \'regular file\' - \'FIFO\' - \'symbolic link\' - \'socket\' - \'unknown\' # Example usage: # file_paths = [\\"/path/to/file1\\", \\"/path/to/file2\\", \\"...\\"] # result = classify_files(file_paths) # The result should be a dictionary where: # - result[\'directory\'] contains paths to all the directories # - result[\'character special device\'] contains paths to all the character special device files # - result[\'block special device\'] contains paths to all the block special device files # - result[\'regular file\'] contains paths to all the regular files # - result[\'FIFO\'] contains paths to all FIFO files # - result[\'symbolic link\'] contains paths to all symbolic links # - result[\'socket\'] contains paths to all socket files # - result[\'unknown\'] contains paths to files that could not be classified ``` # Constraints - You must use the functions provided by the `stat` module to classify the files. - You should safeguard your implementation against paths that do not exist or cannot be accessed. - Performance is not the primary concern; correctness and proper use of the `stat` module are more important. # Input Format - `file_paths`: A list of strings, where each string is a valid file path on the filesystem (assume all paths are absolute). # Output Format - A dictionary where keys are file types and values are lists of file paths classified under each type. # Additional Notes - You might want to handle exceptions where a file path does not exist or is not accessible by wrapping your file type detection code in a try-except block. - The \'unknown\' category should handle cases where the file type does not match any known types specified in the question. # Example Given the following input list of file paths: ```python file_paths = [\\"/home/user/docs\\", \\"/dev/null\\", \\"/tmp/fifo\\", \\"/usr/bin/python3\\"] ``` A potential output could be: ```python { \'directory\': [\'/home/user/docs\'], \'character special device\': [\'/dev/null\'], \'block special device\': [], \'regular file\': [\'/usr/bin/python3\'], \'FIFO\': [\'/tmp/fifo\'], \'symbolic link\': [], \'socket\': [], \'unknown\': [] } ```","solution":"import os import stat def classify_files(file_paths): Classifies a list of file paths based on their file types. Parameters: file_paths (list): List of file paths (strings) to classify. Returns: dict: A dictionary with file types as keys and lists of file paths as values. The file types should include: - \'directory\' - \'character special device\' - \'block special device\' - \'regular file\' - \'FIFO\' - \'symbolic link\' - \'socket\' - \'unknown\' file_types = { \'directory\': [], \'character special device\': [], \'block special device\': [], \'regular file\': [], \'FIFO\': [], \'symbolic link\': [], \'socket\': [], \'unknown\': [] } for path in file_paths: try: mode = os.lstat(path).st_mode if stat.S_ISDIR(mode): file_types[\'directory\'].append(path) elif stat.S_ISCHR(mode): file_types[\'character special device\'].append(path) elif stat.S_ISBLK(mode): file_types[\'block special device\'].append(path) elif stat.S_ISREG(mode): file_types[\'regular file\'].append(path) elif stat.S_ISFIFO(mode): file_types[\'FIFO\'].append(path) elif stat.S_ISLNK(mode): file_types[\'symbolic link\'].append(path) elif stat.S_ISSOCK(mode): file_types[\'socket\'].append(path) else: file_types[\'unknown\'].append(path) except (FileNotFoundError, PermissionError): file_types[\'unknown\'].append(path) return file_types"},{"question":"Title: **Creating and Visualizing Custom Synthetic Datasets Using scikit-learn** # Objective: You are tasked with creating a synthetic dataset using scikit-learn\'s `datasets` module. The goal is to demonstrate your understanding of the `make_classification` and `make_blobs` functions by generating datasets with specified parameters, and then visualizing these datasets using matplotlib. # Problem Statement: 1. **Dataset 1**: - Use the function `make_classification` to create a dataset with: - 1000 samples. - 20 features, where: - 5 features are informative. - 5 features are redundant. - 3 classes. - 1 cluster per class. - A random state of 42. 2. **Dataset 2**: - Use the function `make_blobs` to create a dataset with: - 300 samples. - 2 features. - 4 centers. - Cluster standard deviations of 1.5. - A random state of 42. 3. **Visualization**: - Plot both datasets in separate scatter plots using matplotlib to distinguish between the classes visually. # Constraints: - You must use the specified functions (`make_classification` and `make_blobs`) and parameters. - Use appropriate labels and titles for the plots to help understand the distribution of data points and classes. # Input: - No user input is needed. The parameters are given in the problem statement. # Output: - Two scatter plots: 1. First plot showing the `make_classification` dataset. 2. Second plot showing the `make_blobs` dataset. # Sample Code Structure: ```python import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_blobs # Function to generate datasets and plot them def generate_and_plot_datasets(): # Generate the first dataset using make_classification X1, y1 = make_classification(n_samples=1000, n_features=20, n_informative=5, n_redundant=5, n_classes=3, n_clusters_per_class=1, random_state=42) # Generate the second dataset using make_blobs X2, y2 = make_blobs(n_samples=300, n_features=2, centers=4, cluster_std=1.5, random_state=42) # Plot the first dataset plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.scatter(X1[:, 0], X1[:, 1], c=y1, cmap=\'viridis\') plt.title(\'make_classification Dataset\') plt.xlabel(\'Feature 0\') plt.ylabel(\'Feature 1\') # Plot the second dataset plt.subplot(1, 2, 2) plt.scatter(X2[:, 0], X2[:, 1], c=y2, cmap=\'viridis\') plt.title(\'make_blobs Dataset\') plt.xlabel(\'Feature 0\') plt.ylabel(\'Feature 1\') plt.tight_layout() plt.show() # Call the function to generate and plot the datasets generate_and_plot_datasets() ``` # Notes: - Ensure that you use `random_state=42` to make the results reproducible. - Customize the appearance of the plots (colors, titles, labels) for better readability and presentation. # Evaluation Criteria: - Correctness and completeness of the dataset generation. - Proper visualization with clear, labeled plots. - Code efficiency and readability, following best practices in Python programming.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_blobs def generate_and_plot_datasets(): # Generate the first dataset using make_classification X1, y1 = make_classification(n_samples=1000, n_features=20, n_informative=5, n_redundant=5, n_classes=3, n_clusters_per_class=1, random_state=42) # Generate the second dataset using make_blobs X2, y2 = make_blobs(n_samples=300, n_features=2, centers=4, cluster_std=1.5, random_state=42) # Plot the first dataset plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.scatter(X1[:, 0], X1[:, 1], c=y1, cmap=\'viridis\', edgecolor=\'k\') plt.title(\'make_classification Dataset\') plt.xlabel(\'Feature 0\') plt.ylabel(\'Feature 1\') # Plot the second dataset plt.subplot(1, 2, 2) plt.scatter(X2[:, 0], X2[:, 1], c=y2, cmap=\'viridis\', edgecolor=\'k\') plt.title(\'make_blobs Dataset\') plt.xlabel(\'Feature 0\') plt.ylabel(\'Feature 1\') plt.tight_layout() plt.show() # Call the function to generate and plot the datasets generate_and_plot_datasets()"},{"question":"**Question:** You are provided with the code for a simple neural network implemented in PyTorch. Your task is to modify this neural network to use FSDP2 (Fully Sharded Data Parallelism 2) for distributed training. Follow these steps: 1. Implement a basic fully connected neural network (`SimpleNN`).Your network should contain three layers with the following configurations: - Input layer: Takes in **784** features (like a flattened 28x28 MNIST image) and outputs **256** features. - Hidden layer: Takes in **256** features and outputs **128** features. - Output layer: Takes in **128** features and outputs **10** features (like the number of classes in MNIST). 2. Use the `fully_shard` function from `torch.distributed.fsdp` to apply FSDP2 on your model. 3. Implement the training loop with distributed data parallelism and ensure that the model trains correctly under FSDP2. 4. Demonstrate the benefit of FSDP2 by showing that the model can train on a larger batch size due to memory savings from sharding. **Requirements:** - You should use `torch.distributed` to properly initialize and use multiple GPUs if available. - Your code should be compatible with a distributed setup, using `torch.distributed.launch` or similar tools. - Ensure that the model, data, and training process are all correctly distributed across available workers. **Expected Input and Output:** - **Input**: The code for neural network initialization, and the function calls to initialize FSDP2 and the distributed environment. - **Output**: A fully functional training loop that showcases the use of FSDP2 for distributed training. **Constraints:** - Your model should remain compatible with PyTorch 1.8+. - You should not modify the basic structure and functionality of the neural network layers, only the distribution aspects. ```python import torch import torch.nn as nn import torch.distributed as dist from torch.distributed.fsdp import fully_shard from torch.utils.data import DataLoader, DistributedSampler import torchvision.datasets as datasets import torchvision.transforms as transforms # 1. Define the simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 10) def forward(self, x): x = torch.flatten(x, 1) x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # 2. Setup FSDP2 def setup_model(): model = SimpleNN() model = fully_shard(model) return model # 3. Initialize distributed environment def init_distributed(): dist.init_process_group(backend=\'nccl\') # 4. Training loop def train(rank, world_size, num_epochs=10): init_distributed() model = setup_model() transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) dataset = datasets.MNIST(\'.\', train=True, download=True, transform=transform) sampler = DistributedSampler(dataset) dataloader = DataLoader(dataset, sampler=sampler, batch_size=64) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss() model.train() for epoch in range(num_epochs): for inputs, labels in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\\"Epoch {epoch + 1}, Loss: {loss.item()}\\") # Ensure to use torch.distributed.launch or other tools to spawn multiple processes for this script. if __name__ == \\"__main__\\": rank = int(os.environ[\\"RANK\\"]) world_size = int(os.environ[\\"WORLD_SIZE\\"]) train(rank, world_size) ``` **Note**: Ensure you have a distributed setup ready and use appropriate commands (like `torch.distributed.launch`) to run the above script.","solution":"import torch import torch.nn as nn import torch.distributed as dist from torch.distributed.fsdp import FullyShardedDataParallel as FSDP from torch.utils.data import DataLoader, DistributedSampler import torchvision.datasets as datasets import torchvision.transforms as transforms # 1. Define the simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 10) def forward(self, x): x = torch.flatten(x, 1) x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # 2. Setup FSDP2 def setup_model(): model = SimpleNN() model = FSDP(model) return model # 3. Initialize distributed environment def init_distributed(rank, world_size): dist.init_process_group(backend=\'nccl\', init_method=\'env://\', rank=rank, world_size=world_size) # 4. Training loop def train(rank, world_size, num_epochs=10): init_distributed(rank, world_size) torch.cuda.set_device(rank) model = setup_model().to(rank) transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) dataset = datasets.MNIST(\'.\', train=True, download=True, transform=transform) sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, sampler=sampler, batch_size=64) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) criterion = nn.CrossEntropyLoss() model.train() for epoch in range(num_epochs): for inputs, labels in dataloader: inputs = inputs.to(rank) labels = labels.to(rank) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() if rank == 0: print(f\\"Epoch {epoch + 1}, Loss: {loss.item()}\\") dist.destroy_process_group() # Entry point for distributed training if __name__ == \\"__main__\\": import os from torch.multiprocessing import Process world_size = 2 # for example, use 2 GPUs os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' processes = [] for rank in range(world_size): p = Process(target=train, args=(rank, world_size)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"# **Question: Implementing a Persistent To-Do List using Shelve** In this exercise, you will create a simple \\"To-Do List\\" application that stores tasks persistently using Python\'s `shelve` module. This will involve creating functions to add, remove, mark as completed, and list tasks. Tasks should be stored in a dictionary format where the key is the task description (a string) and the value is a dictionary containing the task details: `{\\"description\\": str, \\"completed\\": bool}`. The shelf file should be named `\\"tasks.db\\"`. Requirements: 1. **add_task(description: str)**: - Adds a new task with the given description to the shelf. The task should be incomplete initially. - If a task with the same description already exists, it should not modify the existing task. 2. **remove_task(description: str)**: - Removes the task with the given description from the shelf. - If the task does not exist, do nothing. 3. **mark_task_completed(description: str)**: - Marks the task with the given description as completed. - If the task does not exist, do nothing. 4. **list_tasks() -> list**: - Returns a list of all tasks in the format: `[{\\"description\\": str, \\"completed\\": bool}, ...]` - The tasks should be returned in alphabetical order based on their descriptions. Constraints: - Use the `shelve` module to store and retrieve tasks. - Ensure proper handling of the `shelve` context (using `with` statement where appropriate). - Handle edge cases such as duplicate tasks, non-existent tasks, etc. # **Example Usage**: ```python # Adding tasks add_task(\\"Buy groceries\\") add_task(\\"Write report\\") add_task(\\"Attend meeting\\") # Listing tasks print(list_tasks()) # Output: [{\'description\': \'Attend meeting\', \'completed\': False}, {\'description\': \'Buy groceries\', \'completed\': False}, {\'description\': \'Write report\', \'completed\': False}] # Marking a task as completed mark_task_completed(\\"Buy groceries\\") # Listing tasks after completing one print(list_tasks()) # Output: [{\'description\': \'Attend meeting\', \'completed\': False}, {\'description\': \'Buy groceries\', \'completed\': True}, {\'description\': \'Write report\', \'completed\': False}] # Removing a task remove_task(\\"Write report\\") # Listing tasks after removal print(list_tasks()) # Output: [{\'description\': \'Attend meeting\', \'completed\': False}, {\'description\': \'Buy groceries\', \'completed\': True}] ``` # **Function Definitions**: - `def add_task(description: str) -> None:` - `def remove_task(description: str) -> None:` - `def mark_task_completed(description: str) -> None:` - `def list_tasks() -> list:` Your implementation should include the above four function definitions and meet the outlined requirements.","solution":"import shelve def add_task(description: str) -> None: Adds a new task with the given description to the shelf. with shelve.open(\'tasks.db\', writeback=True) as db: if description not in db: db[description] = {\\"description\\": description, \\"completed\\": False} def remove_task(description: str) -> None: Removes the task with the given description from the shelf. with shelve.open(\'tasks.db\') as db: if description in db: del db[description] def mark_task_completed(description: str) -> None: Marks the task with the given description as completed. with shelve.open(\'tasks.db\', writeback=True) as db: if description in db: db[description][\\"completed\\"] = True def list_tasks() -> list: Returns a list of all tasks in the format: [{\\"description\\": str, \\"completed\\": bool}, ...] with shelve.open(\'tasks.db\') as db: tasks = [{\\"description\\": key, \\"completed\\": value[\\"completed\\"]} for key, value in db.items()] return sorted(tasks, key=lambda x: x[\\"description\\"])"},{"question":"# Advanced IP Addressing and Networking with ipaddress Module Problem Statement You are tasked with creating a program that performs various operations involving IP addresses, networks, and interfaces using Python\'s `ipaddress` module. The program should be able to: 1. **Validate IP Addresses:** - Write a function `validate_ip(ip: str) -> bool` that takes an IP address in string format and validates whether it is a correct IPv4 or IPv6 address. 2. **Network Information:** - Write a function `network_info(network: str) -> Dict[str, Any]` that takes a network address in CIDR notation and returns a dictionary containing: - `total_addresses`: Total number of IP addresses in the network. - `netmask`: The netmask of the network. - `hostmask`: The hostmask of the network. - `hosts`: A list of all usable host addresses within the network. 3. **Network Containment Check:** - Write a function `is_address_in_network(ip: str, network: str) -> bool` that checks if a given IP address belongs to a specified network. 4. **Exception Handling:** - Write a function `create_network(network: str) -> Union[str, None]` that tries to create a network from the given string. If it is not a valid network, return a detailed error message. Input and Output Formats 1. **`validate_ip` Function:** - **Input:** A string `ip` representing an IP address. - **Output:** A boolean `True` if the IP address is valid, `False` otherwise. 2. **`network_info` Function:** - **Input:** A string `network` representing a network in CIDR notation. - **Output:** A dictionary containing: ```python { \'total_addresses\': int, \'netmask\': str, \'hostmask\': str, \'hosts\': List[str] } ``` 3. **`is_address_in_network` Function:** - **Input:** A string `ip` representing an IP address, and a string `network` representing a network in CIDR notation. - **Output:** A boolean `True` if the IP address belongs to the network, `False` otherwise. 4. **`create_network` Function:** - **Input:** A string `network` representing a network in CIDR notation. - **Output:** `None` if the network is valid or an error message (string) describing why the network is invalid. Constraints - You must use the `ipaddress` module to implement these functions. - Handle exceptions gracefully and return appropriate error messages for invalid inputs where needed. Example Usage ```python # validate_ip function print(validate_ip(\'192.0.2.1\')) # True print(validate_ip(\'255.255.255.256\')) # False # network_info function net_info = network_info(\'192.0.2.0/24\') print(net_info) # { # \'total_addresses\': 256, # \'netmask\': \'255.255.255.0\', # \'hostmask\': \'0.0.0.255\', # \'hosts\': [\'192.0.2.1\', \'192.0.2.2\', ..., \'192.0.2.254\'] # } # is_address_in_network function print(is_address_in_network(\'192.0.2.1\', \'192.0.2.0/24\')) # True print(is_address_in_network(\'192.0.3.1\', \'192.0.2.0/24\')) # False # create_network function print(create_network(\'192.0.2.1/24\')) # \\"192.0.2.1/24 has host bits set\\" print(create_network(\'192.0.2.0/24\')) # None ``` Implement these functions to demonstrate your understanding of the `ipaddress` module and its capabilities.","solution":"import ipaddress from typing import Union, Dict, Any, List def validate_ip(ip: str) -> bool: Validate if the given string is a valid IPv4 or IPv6 address. try: ipaddress.ip_address(ip) return True except ValueError: return False def network_info(network: str) -> Dict[str, Any]: Get network information for a given network in CIDR notation. net = ipaddress.ip_network(network, strict=False) return { \'total_addresses\': net.num_addresses, \'netmask\': str(net.netmask), \'hostmask\': str(net.hostmask), \'hosts\': [str(host) for host in net.hosts()] } def is_address_in_network(ip: str, network: str) -> bool: Check if the given IP address belongs to the specified network. try: network = ipaddress.ip_network(network, strict=False) ip = ipaddress.ip_address(ip) return ip in network except ValueError: return False def create_network(network: str) -> Union[str, None]: Try to create a network from the given string, returning an error message if it\'s invalid. try: ipaddress.ip_network(network, strict=True) return None except ValueError as e: return str(e)"},{"question":"# XML Parsing with `xml.parsers.expat` Objective Write a Python function that parses an XML string, processes the XML elements, and extracts specific data using the `xml.parsers.expat` module. This exercise will test your understanding of setting up and using an XML parser, defining handlers, and managing XML data. Problem Statement You are given an XML string containing details about books in a library. Each book has the following structure: ```xml <library> <book id=\\"1\\" genre=\\"fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book id=\\"2\\" genre=\\"non-fiction\\"> <title>Educated</title> <author>Tara Westover</author> <year>2018</year> </book> <!-- More book elements --> </library> ``` Your task is to implement the function `parse_library(xml_string)` which: 1. Parses the given XML string. 2. Extracts and returns a list of dictionaries, each representing a book with the keys `id`, `genre`, `title`, `author`, and `year`. Function Signature ```python def parse_library(xml_string: str) -> list: pass ``` Input - `xml_string` (str): A string containing the XML data representing the library. Output - A list of dictionaries, each containing the extracted details of a book: ```python [ {\'id\': \'1\', \'genre\': \'fiction\', \'title\': \'The Great Gatsby\', \'author\': \'F. Scott Fitzgerald\', \'year\': \'1925\'}, {\'id\': \'2\', \'genre\': \'non-fiction\', \'title\': \'Educated\', \'author\': \'Tara Westover\', \'year\': \'2018\'}, # More book dictionaries ] ``` Constraints - The XML string will be well-formed. - The library element will contain zero or more book elements. Example ```python xml_string = <library> <book id=\\"1\\" genre=\\"fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book id=\\"2\\" genre=\\"non-fiction\\"> <title>Educated</title> <author>Tara Westover</author> <year>2018</year> </book> </library> print(parse_library(xml_string)) # Output: # [ # {\'id\': \'1\', \'genre\': \'fiction\', \'title\': \'The Great Gatsby\', \'author\': \'F. Scott Fitzgerald\', \'year\': \'1925\'}, # {\'id\': \'2\', \'genre\': \'non-fiction\', \'title\': \'Educated\', \'author\': \'Tara Westover\', \'year\': \'2018\'} # ] ``` Implementation Note To achieve this, you need to: 1. Create an `xmlparser` object using `ParserCreate()`. 2. Define handlers for the start and end of elements and character data. 3. Use these handlers to extract and construct the desired output. Happy coding!","solution":"import xml.parsers.expat def parse_library(xml_string: str) -> list: library = [] current_book = {} current_data = None def start_element(name, attrs): nonlocal current_book, current_data if name == \'book\': current_book = { \'id\': attrs[\'id\'], \'genre\': attrs[\'genre\'] } current_data = None def end_element(name): nonlocal current_book if name == \'book\': library.append(current_book) current_book = {} def char_data(data): nonlocal current_data if data.strip(): current_data = data.strip() def process_end_element(name): nonlocal current_book, current_data if name == \'title\': current_book[name] = current_data elif name == \'author\': current_book[name] = current_data elif name == \'year\': current_book[name] = current_data current_data = None parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = lambda name: (end_element(name), process_end_element(name)) parser.CharacterDataHandler = char_data parser.Parse(xml_string) return library"},{"question":"Objective: Demonstrate your understanding of scikit-learn\'s dataset handling, preprocessing, and modeling capabilities by working with the **20 Newsgroups** dataset. Task: 1. **Load the Dataset**: Use `sklearn.datasets.fetch_20newsgroups` to load the 20 Newsgroups dataset. Load only the \\"rec.sport.baseball\\" and \\"sci.space\\" categories. 2. **Preprocess the Text Data**: - Tokenize the text data. - Convert the text data into a TF-IDF representation using `TfidfVectorizer`. 3. **Train a Classifier**: - Split the dataset into training and test sets. - Train a Multinomial Naive Bayes classifier on the training data. 4. **Evaluate the Model**: - Evaluate the classifier\'s performance using accuracy, precision, and recall on the test data. - Plot the confusion matrix for the test data predictions. Implementation Details: - **Function Signature**: ```python def classify_newsgroups(): # Implementation here pass ``` - **Expected Input**: You do not need to take any inputs; all data loading and processing should occur within the function. - **Expected Output**: Print the accuracy, precision, recall, and plot the confusion matrix. - **Performance Requirements**: Ensure efficient data loading and preprocessing. Your implementation should complete within a reasonable time for practical purposes. Constraints: - Use only scikit-learn and standard Python libraries for your implementation. - Handle exceptions where necessary, ensuring that your function can handle potential issues during data loading and processing gracefully. Sample Code Outline: ```python from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix import matplotlib.pyplot as plt import seaborn as sns def classify_newsgroups(): # Load the 20 Newsgroups dataset with specified categories categories = [\'rec.sport.baseball\', \'sci.space\'] newsgroups = fetch_20newsgroups(subset=\'all\', categories=categories) # Convert the text data into TF-IDF representation vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(newsgroups.data) y = newsgroups.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a Multinomial Naive Bayes classifier clf = MultinomialNB() clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') # Print evaluation metrics print(f\\"Accuracy: {accuracy:.2f}\\") print(f\\"Precision: {precision:.2f}\\") print(f\\"Recall: {recall:.2f}\\") # Plot the confusion matrix cm = confusion_matrix(y_test, y_pred) sns.heatmap(cm, annot=True, fmt=\'d\', cmap=\'Blues\', xticklabels=newsgroups.target_names, yticklabels=newsgroups.target_names) plt.xlabel(\'Predicted\') plt.ylabel(\'True\') plt.title(\'Confusion Matrix\') plt.show() # Call the function to execute the task classify_newsgroups() ```","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix import matplotlib.pyplot as plt import seaborn as sns def classify_newsgroups(): # Load the 20 Newsgroups dataset with specified categories categories = [\'rec.sport.baseball\', \'sci.space\'] newsgroups = fetch_20newsgroups(subset=\'all\', categories=categories) # Convert the text data into TF-IDF representation vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(newsgroups.data) y = newsgroups.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a Multinomial Naive Bayes classifier clf = MultinomialNB() clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') # Print evaluation metrics print(f\\"Accuracy: {accuracy:.2f}\\") print(f\\"Precision: {precision:.2f}\\") print(f\\"Recall: {recall:.2f}\\") # Plot the confusion matrix cm = confusion_matrix(y_test, y_pred) sns.heatmap(cm, annot=True, fmt=\'d\', cmap=\'Blues\', xticklabels=[\'rec.sport.baseball\', \'sci.space\'], yticklabels=[\'rec.sport.baseball\', \'sci.space\']) plt.xlabel(\'Predicted\') plt.ylabel(\'True\') plt.title(\'Confusion Matrix\') plt.show() # Call the function to execute the task classify_newsgroups()"},{"question":"Coding Assessment Question # Objective The goal of this exercise is to verify your ability to interact with and utilize the Python `sysconfig` module to gain insights into the Python environment on your system. # Problem Statement You are tasked with writing a function `create_python_env_report()` that will generate a detailed report of the current Python installation environment. # Function Signature ```python def create_python_env_report() -> dict: This function returns a dictionary containing: 1. `Platform`: The current platform string returned by `sysconfig.get_platform()`. 2. `Python_Version`: The current version of Python in MAJOR.MINOR format returned by `sysconfig.get_python_version()`. 3. `Default_Scheme`: The scheme name used by default for the current platform. 4. `Preferred_User_Scheme`: The preferred scheme name for a user installation. 5. `Paths`: A dictionary containing all installation paths for the default scheme. 6. `Shared_Libraries`: The value of the `Py_ENABLE_SHARED` configuration variable. Returns: dict: A report dictionary containing the above details. ``` # Requirements 1. The function should fetch the platform information using `sysconfig.get_platform()`. 2. Retrieve the Python version using `sysconfig.get_python_version()`. 3. Get the default installation scheme using `sysconfig.get_default_scheme()`. 4. Determine the preferred scheme for a user installation using `sysconfig.get_preferred_scheme(\'user\')`. 5. Obtain all installation paths using `sysconfig.get_paths()` for the default scheme. 6. Get the value of the `Py_ENABLE_SHARED` configuration variable using `sysconfig.get_config_var(\'Py_ENABLE_SHARED\')`. # Example Output The expected return should look like: ```python { \\"Platform\\": \\"win-amd64\\" or another platform string, \\"Python_Version\\": \\"3.10\\", \\"Default_Scheme\\": \\"nt\\", \\"Preferred_User_Scheme\\": \\"nt_user\\", \\"Paths\\": { \\"stdlib\\": \\"path_to_stdlib\\", \\"platstdlib\\": \\"path_to_platstdlib\\", \\"purelib\\": \\"path_to_purelib\\", \\"platlib\\": \\"path_to_platlib\\", \\"include\\": \\"path_to_include\\", \\"platinclude\\": \\"path_to_platinclude\\", \\"scripts\\": \\"path_to_scripts\\", \\"data\\": \\"path_to_data\\" }, \\"Shared_Libraries\\": 1 or 0 } ``` # Constraints - You can assume the `sysconfig` module documentation is available and can be referenced if needed. - The function shouldn\'t take any arguments and should return the dictionary described above. - The paths in the `Paths` dictionary should reflect actual installation paths for the current Python environment. # Evaluation Your solution will be evaluated based on: - Correctness: The information should be accurate and fetched using the appropriate sysconfig functions. - Completeness: The report must include all required sections as described. - Code Quality: The code should be well-documented, readable, and efficiently structured.","solution":"import sysconfig def create_python_env_report() -> dict: This function returns a dictionary containing the details of the current Python installation environment. Returns: dict: A report dictionary containing the following details: - Platform: The current platform string. - Python_Version: The current version of Python in MAJOR.MINOR format. - Default_Scheme: The default installation scheme. - Preferred_User_Scheme: The preferred scheme for user installation. - Paths: A dictionary of installation paths for the default scheme. - Shared_Libraries: The value of the Py_ENABLE_SHARED configuration variable. report = { \\"Platform\\": sysconfig.get_platform(), \\"Python_Version\\": sysconfig.get_python_version(), \\"Default_Scheme\\": sysconfig.get_default_scheme(), \\"Preferred_User_Scheme\\": sysconfig.get_preferred_scheme(\'user\'), \\"Paths\\": sysconfig.get_paths(), \\"Shared_Libraries\\": sysconfig.get_config_var(\'Py_ENABLE_SHARED\'), } return report"},{"question":"**Objective:** Implement a Python class `FileLineReader` that uses the \\"linecache\\" module to manage reading lines from multiple files. This class should provide the following methods: 1. **`__init__(self)`:** Initializes the instance, setting up any necessary data structures. 2. **`add_file(self, filename)`:** Adds a file to be managed by the instance. If the file does not exist or is not accessible, raise an appropriate exception. 3. **`get_line(self, filename, lineno)`:** Retrieves a specific line from a given file. If the file hasn\'t been added with `add_file`, raise an appropriate exception. 4. **`clear_cache(self)`:** Clears the internal line cache. 5. **`check_cache(self, filename=None)`:** Checks the validity of the cache for a specific file or all files if no filename is provided. 6. **`lazy_add_file(self, filename, module_globals)`:** Adds a file using the `lazycache` method to defer I/O until it is absolutely necessary. **Constraints:** - Assume a file contains no more than 10,000 lines. - Line numbers are 1-based. - You should handle all exceptions gracefully and ensure clear and informative error messages are provided. **Example Usage:** ```python # Create an instance of FileLineReader reader = FileLineReader() # Add a file to the reader reader.add_file(\'example.txt\') # Retrieve line 10 from the added file line = reader.get_line(\'example.txt\', 10) # Clear the cache reader.clear_cache() # Check the cache for validity reader.check_cache() # Add a file using lazy loading reader.lazy_add_file(\'lazy_example.txt\', globals()) # Attempt to retrieve a line from a non-added file (should raise exception) try: reader.get_line(\'non_added_file.txt\', 5) except Exception as e: print(e) ``` **Performance Requirements:** - Your solution should efficiently manage adding and retrieving lines from files, especially when dealing with multiple files repeatedly. - Optimize for scenarios where lines from the same file are accessed multiple times. **Expected Input and Output:** Functions will be tested using a series of method calls as shown in the example usage. Ensure that the outputs are correct and exceptions are handled as per the requirements.","solution":"import linecache import os class FileLineReader: def __init__(self): self.files = set() def add_file(self, filename): Adds a file to be managed by the instance. if not os.path.exists(filename) or not os.path.isfile(filename): raise FileNotFoundError(f\\"File \'{filename}\' does not exist or is not accessible.\\") self.files.add(filename) linecache.checkcache(filename) def get_line(self, filename, lineno): Retrieves a specific line from a given file. if filename not in self.files: raise ValueError(f\\"File \'{filename}\' has not been added.\\") line = linecache.getline(filename, lineno) if line == \'\': raise ValueError(f\\"Line number {lineno} does not exist in file \'{filename}\'.\\") return line.rstrip() def clear_cache(self): Clears the internal line cache. linecache.clearcache() def check_cache(self, filename=None): Checks the validity of the cache for a specific file or all files if no filename is provided. linecache.checkcache(filename) def lazy_add_file(self, filename, module_globals): Adds a file using the lazycache method to defer I/O until it is absolutely necessary. if not os.path.exists(filename) or not os.path.isfile(filename): raise FileNotFoundError(f\\"File \'{filename}\' does not exist or is not accessible.\\") self.files.add(filename) linecache.lazycache(filename, module_globals)"},{"question":"# Slice Operations in Custom Sequences You are tasked with implementing a custom sequence class in Python that supports slicing operations leveraging the slicing functionalities described in the provided documentation. **Objective:** Implement a class `CustomSequence` that mimics a list but with the added capability to handle slicing operations by utilizing the slice extraction functions (`PySlice_New`, `PySlice_GetIndicesEx`, etc.) precisely. **Requirements:** 1. **Initialization:** - The class should be initialized with a list of elements. 2. **Slicing:** - Implement slicing for the sequence using the custom slice handling functions described in the documentation. - Specifically, handle slicing operations and clipping properly using `PySlice_GetIndicesEx`. 3. **Methods:** - `__init__(self, data: list)`: Initialize the sequence with the given list. - `__getitem__(self, index)`: Support getting an item or a slice using both normal index and slice objects. - `__str__(self)`: Return a string representation of the sequence. **Constraints:** - Sequence length should be within the range of typical list sizes in Python. - The slicing should be efficient in terms of performance and should properly handle edge cases (e.g., out-of-bounds indices). # Example Usage: ```python seq = CustomSequence(list(range(10))) print(seq[2:8:2]) # should output CustomSequence with elements [2, 4, 6] print(seq[:4]) # should output CustomSequence with elements [0, 1, 2, 3] print(seq[::3]) # should output CustomSequence with elements [0, 3, 6, 9] print(seq[-1]) # should output 9 ``` **Note:** While you are not required to use the actual CPython internal functions, you should simulate their behaviors closely adapting to Python APIs where necessary. # Advanced Task (Optional): Extend this class to handle multi-dimensional slice objects using numpy-style slicing.","solution":"class CustomSequence: def __init__(self, data: list): self.data = data def __getitem__(self, index): if isinstance(index, slice): start, stop, step = index.indices(len(self.data)) return CustomSequence(self.data[start:stop:step]) elif isinstance(index, int): return self.data[index] else: raise TypeError(\\"Invalid argument type.\\") def __str__(self): return f\'CustomSequence({self.data})\'"},{"question":"**Objective:** Demonstrate your proficiency with the `seaborn` library by visualizing a dataset and customizing the plot using different themes and styling options. **Task:** 1. Load the \\"tips\\" dataset from `seaborn`. 2. Create a facet grid plot for the \\"tips\\" dataset: - The plot should facet by \\"sex\\" with a wrap of 2. - Use a scatter plot (dots) with \\"total_bill\\" on the x-axis and \\"tip\\" on the y-axis. 3. Customize the plot\'s appearance: - Set the theme such that the background face color of the axes is white, and the edge color is slate gray. - Change the axis style to \\"ticks\\". - Update the plot\'s configuration to use a `seaborn` context suitable for a talk format. - Set the line width of the plotted lines to 2. # Input: There is no input to your function; you are expected to define these tasks within your function. # Output: Display the customized facet grid plot. # Constraints: - Use `seaborn` and `matplotlib`. - Assume necessary packages are installed and can be imported. # Function Signature: ```python def customize_and_display_tips_plot(): pass ``` # Example: Here is an example code for a simpler plot; extend this with the required tasks and customizations. ```python import seaborn as sns import seaborn.objects as so from seaborn import load_dataset def customize_and_display_tips_plot(): # Load dataset tips = load_dataset(\\"tips\\") # Create the base plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"sex\\", wrap=2) .add(so.Dot()) ) # Apply custom themes and styles p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) p.theme(sns.axes_style(\\"ticks\\")) p.theme(sns.plotting_context(\\"talk\\")) p.theme({\\"lines.linewidth\\": 2}) # Display the plot p.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_and_display_tips_plot(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Set the theme and context sns.set_theme(style=\\"ticks\\", rc={\\"axes.facecolor\\": \\"white\\", \\"axes.edgecolor\\": \\"slategray\\"}) sns.set_context(\\"talk\\", rc={\\"lines.linewidth\\": 2}) # Create the facet grid g = sns.FacetGrid(tips, col=\\"sex\\", col_wrap=2) g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Adjust the appearance of the plot g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") g.add_legend() plt.show()"},{"question":"# Out-of-Core Learning for Large Text Classification Objective: Your task is to create an out-of-core text classification system using scikit-learn. You will handle streaming data, feature extraction, and incremental learning. Requirements: 1. **Streaming Data:** - Simulate streaming data by reading from a large text file in chunks. 2. **Feature Extraction:** - Use `HashingVectorizer` for converting text data into numerical features. 3. **Incremental Learning:** - Implement a classifier using an algorithm that supports the `partial_fit` method like `SGDClassifier`. Input and Output Formats: - **Input:** - A large text file (`sample_data.txt`) where each line represents a document and is structured as `<label>t<document text>`. - **Output:** - Print the accuracy after processing each chunk of data. Constraints: - The system should handle files that do not fit into memory (consider chunks of 1000 lines). - Ensure the classifier is updated incrementally and prints intermediate performance metrics. Performance Requirements: - Minimize memory usage. - Aim for efficient and quick processing of data streams. Example Code Structure: ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import numpy as np def stream_data(file_path, chunk_size=1000): with open(file_path, \'r\') as file: while True: chunk = [file.readline().strip() for _ in range(chunk_size)] if not chunk or chunk[0] == \'\': break yield chunk def main(file_path): vectorizer = HashingVectorizer(alternate_sign=False) classifier = SGDClassifier() for i, chunk in enumerate(stream_data(file_path)): # Split chunk into documents and labels data = [line.split(\'t\') for line in chunk] labels, documents = zip(*data) # Feature extraction X = vectorizer.transform(documents) y = np.array(labels) # Partial fit the classifier if i == 0: classifier.partial_fit(X, y, classes=np.unique(y)) else: classifier.partial_fit(X, y) # For simplicity, we assume we have access to a test set # Simulate it here with a small portion of the chunk test_documents = documents[:100] test_labels = labels[:100] X_test = vectorizer.transform(test_documents) y_test = np.array(test_labels) # Calculate and print accuracy y_pred = classifier.predict(X_test) print(f\\"Chunk {i + 1} Accuracy: {accuracy_score(y_test, y_pred)}\\") # Assume \'sample_data.txt\' is your large text file main(\'sample_data.txt\') ``` Notes: - The simulated test set is a part of the chunk for demonstration purposes. In practice, use a separate test set. - Adjust `chunk_size` based on your system\'s memory capacity.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import numpy as np def stream_data(file_path, chunk_size=1000): Generator function that reads a text file in chunks of lines. Args: file_path (str): Path to the text file. chunk_size (int): Number of lines to read at a time. Yields: list: A chunk of lines from the text file. with open(file_path, \'r\') as file: while True: chunk = [file.readline().strip() for _ in range(chunk_size)] if not chunk or chunk[0] == \'\': break yield chunk def main(file_path): Main function to perform out-of-core learning on a large text file. Args: file_path (str): Path to the text file. vectorizer = HashingVectorizer(alternate_sign=False) classifier = SGDClassifier() for i, chunk in enumerate(stream_data(file_path)): # Split chunk into documents and labels data = [line.split(\'t\') for line in chunk if \'t\' in line] if not data: continue labels, documents = zip(*data) # Feature extraction X = vectorizer.transform(documents) y = np.array(labels) # Partial fit the classifier if i == 0: classifier.partial_fit(X, y, classes=np.unique(y)) else: classifier.partial_fit(X, y) # For simplicity, we assume we have access to a test set # Simulate it here with a small portion of the chunk test_documents = documents[:100] test_labels = labels[:100] if test_documents: X_test = vectorizer.transform(test_documents) y_test = np.array(test_labels) # Calculate and print accuracy y_pred = classifier.predict(X_test) print(f\\"Chunk {i + 1} Accuracy: {accuracy_score(y_test, y_pred)}\\") # Assume \'sample_data.txt\' is your large text file # main(\'sample_data.txt\') # Uncomment to run with an actual large text file"},{"question":"Coding Assessment Question **Objective:** Implement a custom callable class in Python that supports both the *tp_call* and vectorcall calling protocols efficiently. **Problem Statement:** You are tasked with creating a custom Python class, `MyCallable`, that can be called using both traditional *tp_call* and the more efficient vectorcall protocol. The class should demonstrate a basic understanding of these calling protocols and ensure that both produce identical results when called. # Requirements: 1. Implement the `MyCallable` class. 2. Ensure the class supports both *tp_call* and vectorcall protocols. 3. The call to the object should accept both positional and keyword arguments. 4. Your class should have a method `call`, which the protocols will use to get the result. The method should print \\"Calling with args: `<args>` and kwargs: `<kwargs>`\\". 5. Performance testing is not required; focus on functional correctness. # Input and Output - **Input:** Various types of callables with positional and keyword arguments. - **Output:** Printable output indicating that the call has been recognized with appropriate arguments. # Constraints - Python 3.9 or higher. - You are allowed to use any libraries or standard tools in Python. - Ensure to handle both no argument and multiple arguments cases properly. # Example: ```python my_callable = MyCallable() # Example calls result1 = my_callable(1, 2, x=3, y=4) # Should print \\"Calling with args: (1, 2) and kwargs: {\'x\': 3, \'y\': 4}\\" result2 = my_callable() # Should print \\"Calling with args: () and kwargs: {}\\" assert result1 == result2 # Verifying that outputs are consistent ``` # Note: Please ensure to document your code properly and explain how the *tp_call* and vectorcall protocols have been implemented within your class.","solution":"class MyCallable: def __call__(self, *args, **kwargs): return self.call(*args, **kwargs) def call(self, *args, **kwargs): print(f\\"Calling with args: {args} and kwargs: {kwargs}\\") return args, kwargs # Demonstration of usage my_callable = MyCallable() result1 = my_callable(1, 2, x=3, y=4) result2 = my_callable() # Ensure both outputs are tuples of args and kwargs and identical assert result1 == ((1, 2), {\'x\': 3, \'y\': 4}) assert result2 == ((), {})"},{"question":"**Problem Statement:** Design and implement a function `manipulate_tensor` in PyTorch that accepts a 2D tensor and performs a series of transformations using view operations, and returns: 1. A view of the original tensor reshaped to a specified shape. 2. A contiguous tensor from the reshaped view. 3. The result of a specified in-place modification on the reshaped view. Your implementation should ensure: - The initial reshaping utilizes a view operation. - The provided modification applies in-place on the reshaped view. - Conversion to a contiguous tensor only if the reshaped tensor is non-contiguous. - Returning the final three tensors (reshaped view, contiguous tensor (if applied), and modified tensor) in a tuple. **Function Signature:** ```python def manipulate_tensor(tensor: torch.Tensor, new_shape: tuple, modification: callable) -> tuple: pass ``` **Input:** - `tensor`: A 2D tensor (e.g., `torch.Tensor`) of any shape. - `new_shape`: A tuple specifying the desired shape for the view operation, compatible with the total number of elements in the original tensor. - `modification`: A callable that takes a tensor and performs an in-place modification on it (e.g., setting a specific element to a new value). **Output:** - A tuple containing: 1. The reshaped view tensor. 2. The contiguous version of the reshaped view (if it was not contiguous, otherwise the same tensor). 3. The reshaped view tensor after the in-place modification. **Example:** ```python def set_first_element_to_zero(tensor): tensor[0, 0] = 0 # Test the function import torch original_tensor = torch.rand(4, 4) new_shape = (2, 8) reshaped_view, contiguous_tensor, modified_tensor = manipulate_tensor(original_tensor, new_shape, set_first_element_to_zero) print(\\"Original Tensor:\\") print(original_tensor) print(\\"Reshaped View Tensor:\\") print(reshaped_view) print(\\"Contiguous Tensor:\\") print(contiguous_tensor) print(\\"Modified Tensor (should reflect the in-place change):\\") print(modified_tensor) ``` **Constraints:** - The total number of elements in `new_shape` must be equal to the total number of elements in the original tensor. - The modification function should only perform in-place operations on the tensor. **Note:** Be mindful of contiguity and how in-place modifications on view tensors impact the original tensor and subsequent operations.","solution":"import torch def manipulate_tensor(tensor: torch.Tensor, new_shape: tuple, modification: callable) -> tuple: # Create a view of the original tensor with the specified shape reshaped_view = tensor.view(new_shape) # Ensure the tensor is contiguous if not reshaped_view.is_contiguous(): contiguous_tensor = reshaped_view.contiguous() else: contiguous_tensor = reshaped_view # Apply the in-place modification modification(reshaped_view) # Return the tuple containing the reshaped view, contiguous tensor, and modified tensor return (reshaped_view, contiguous_tensor, reshaped_view)"},{"question":"Implement a Custom Container Emulating GC Principles in Python Objective: To assess your understanding of cyclic garbage collection in Python, you are required to implement a custom container type in Python that emulates GC tracking principles. Description: You need to implement a class `GCContainer` that aids in managing objects similar to how cyclic garbage collection (GC) is handled in the C-API described in the document. This class will provide the following functionalities: 1. **Initialization and Tracking**: - The constructor of `GCContainer` should initialize the container, tracking the objects it holds. Use an internal list to manage these objects. - Implement a `track()` method to simulate adding the object to a set of tracked objects. 2. **Manual Untracking and Deallocation**: - Implement an `untrack()` method to simulate removing objects from being tracked. - Implement a `delete()` method to deliberately clear all references in the container and simulate memory deallocation. 3. **GC Utility Methods**: - Implement a method `is_tracked()` that checks whether an object is being currently tracked by the garbage collector. - Implement a method `collect_garbage()` that simulates a manual garbage collection run. Constraints: - You should not use Python\'s built-in `gc` module directly. - The focus is on emulating the principles of garbage collection in a manageable, simulated environment. Requirements: - All objects added to the container must be of type `object`. - The container must support dynamic addition and removal of objects. Expected Input and Output: ```python class GCContainer: def __init__(self): # Constructor to initialize the container and tracking. pass def track(self, obj): # Method to add an object to the container and mark it as tracked. pass def untrack(self, obj): # Method to untrack an object from the container pass def delete(self): # Method to clear all objects and deallocate memory pass def is_tracked(self, obj): # Method to check if an object is currently tracked pass def collect_garbage(self): # Method to simulate garbage collection pass ``` Implement the `GCContainer` class and ensure it works as expected by fulfilling the functionalities described above. **Example Usage:** ```python gc_container = GCContainer() obj1 = object() obj2 = object() gc_container.track(obj1) gc_container.track(obj2) print(gc_container.is_tracked(obj1)) # Expected: True gc_container.untrack(obj1) print(gc_container.is_tracked(obj1)) # Expected: False gc_container.delete() gc_container.collect_garbage() ``` **Performance Considerations**: - The implementation should efficiently manage memory by tracking and untracking accurately. - Simulate the garbage collection in a way that does not heavily impact performance.","solution":"class GCContainer: def __init__(self): Initialize the container and track. self._tracked_objects = set() def track(self, obj): Add an object to the container and mark it as tracked. Args: obj (object): Object to be tracked. if isinstance(obj, object): self._tracked_objects.add(obj) def untrack(self, obj): Untrack an object from the container. Args: obj (object): Object to be untracked. if obj in self._tracked_objects: self._tracked_objects.remove(obj) def delete(self): Clear all objects and deallocate memory. self._tracked_objects.clear() def is_tracked(self, obj): Check if an object is currently tracked. Args: obj (object): Object to check. Returns: bool: True if object is tracked, False otherwise. return obj in self._tracked_objects def collect_garbage(self): Simulate garbage collection. unreferenced_objects = set() for obj in self._tracked_objects: if not self.is_referenced(obj): unreferenced_objects.add(obj) for obj in unreferenced_objects: self._tracked_objects.remove(obj) def is_referenced(self, obj): Simulate checking if an object is referenced. Args: obj (object): Object to check. Returns: bool: False (to simulate unreferenced). Adjust as needed for realistic referenced check. # This is a simulation. Always returning False to simulate objects being unreferenced. return False"},{"question":"You are tasked with implementing a PyTorch function that executes a series of tensor operations and gradient calculations. Specifically, you need to perform the following steps: 1. **Tensor Creation**: - Create a tensor `A` of shape (5, 5) with random values drawn from a normal distribution. - Create a tensor `B` of shape (5, 5) with random values drawn from a uniform distribution. 2. **Tensor Operations**: - Perform matrix multiplication of `A` and `B` to get a new tensor `C`. - Add a constant scalar value (3.5) to all elements of `C`. 3. **Gradient Computation**: - Ensure gradients are enabled for tensor `C`. - Calculate the sum of all elements in `C`. - Compute the gradient of this sum with respect to `C`. 4. **Reduction and Comparison**: - Compute the mean of all elements in `C`. - Determine if the mean is greater than 0.5. Implement the function `tensor_operations_and_gradients` that executes the above steps. Function Signature: ```python def tensor_operations_and_gradients(): # Your code here ``` Expected Output: - The function should print the gradient of the sum with respect to `C`. - The function should return a boolean indicating whether the mean of elements in `C` is greater than 0.5. Constraints: - Use appropriate PyTorch functions for each task. - Ensure that your code is clear and commented where necessary. Example Output: ```python Gradient of sum wrt C: tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) Return value: True ```","solution":"import torch def tensor_operations_and_gradients(): # Step 1: Tensor creation A = torch.randn(5, 5) # Tensor with random values from a normal distribution B = torch.rand(5, 5) # Tensor with random values from a uniform distribution # Step 2: Tensor operations C = torch.matmul(A, B) # Matrix multiplication of A and B to get C C = C + 3.5 # Add 3.5 to all elements of C # Step 3: Gradient computation C.requires_grad_(True) # Enable gradients for tensor C sum_C = torch.sum(C) # Calculate the sum of all elements in C sum_C.backward() # Compute the gradient of this sum with respect to C # Print the gradient of the sum with respect to C print(\\"Gradient of sum wrt C:\\", C.grad) # Step 4: Reduction and comparison mean_C = torch.mean(C) # Compute the mean of all elements in C return mean_C.item() > 0.5 # Determine if the mean is greater than 0.5"},{"question":"You are tasked with writing a Python function that utilizes the `python310` C API (as described in the documentation) to initialize Python with a given configuration and handle potential errors gracefully. Your function should simulate this in pure Python. # Objective Create a Python function `initialize_python(config)` that: 1. Initializes Python with the provided configuration. 2. Returns an appropriate status message depending on whether the initialization was successful, resulted in an error, or requested an exit. # Function Signature ```python def initialize_python(config: dict) -> str: ``` # Input - `config`: A dictionary with the following possible keys and their values: - `program_name`: (string) Name of the program. - `module_search_paths`: (list of strings) Additional module search paths. - `isolated`: (boolean) Whether to run Python in isolated mode. - `utf8_mode`: (boolean) Whether to enable Python UTF-8 mode. # Output - Returns a string: - `\\"Initialization Successful\\"` if Python is successfully initialized. - `\\"Error: <error_message>\\"` if an error occurs during initialization, where `<error_message>` is a description of the error. - `\\"Exit with code <exit_code>\\"` if initialization requests an exit, where `<exit_code>` is the exit code. # Example ```python config = { \\"program_name\\": \\"my_program\\", \\"module_search_paths\\": [\\"/path/to/modules\\"], \\"isolated\\": True, \\"utf8_mode\\": True } status = initialize_python(config) print(status) ``` # Constraints 1. You should simulate the initialization process in Python, as the actual C API calls cannot be made directly in Python. 2. Ensure that the function checks for and appropriately handles all potential errors using the provided status handling description. 3. Assume that any required structures and methods (`PyConfig`, `PyPreConfig`, etc.) are available as described in the documentation, and you need to create corresponding Python classes and methods to simulate the process. # Hints 1. Create Python classes and methods to simulate `PyConfig`, `PyPreConfig`, and status handling. 2. Implement methods to set configuration fields and handle errors, similar to how they are outlined in the documentation. 3. Use the examples provided in the documentation as guidelines for how to structure your initialization function.","solution":"class PyConfig: def __init__(self): self.program_name = None self.module_search_paths = [] self.isolated = False self.utf8_mode = False def initialize_python(config: dict) -> str: try: pyconfig = PyConfig() if \'program_name\' in config: pyconfig.program_name = config[\'program_name\'] if \'module_search_paths\' in config: pyconfig.module_search_paths = config[\'module_search_paths\'] if \'isolated\' in config: pyconfig.isolated = config[\'isolated\'] if \'utf8_mode\' in config: pyconfig.utf8_mode = config[\'utf8_mode\'] # Simulating potential errors and exit conditions if pyconfig.program_name is None: raise ValueError(\\"Program name is required\\") if pyconfig.isolated and not isinstance(pyconfig.isolated, bool): raise TypeError(\\"Isolated must be a boolean value\\") if pyconfig.utf8_mode and not isinstance(pyconfig.utf8_mode, bool): raise TypeError(\\"UTF-8 mode must be a boolean value\\") # Simulate successful initialization return \\"Initialization Successful\\" except ValueError as ve: return f\\"Error: {str(ve)}\\" except TypeError as te: return f\\"Error: {str(te)}\\" except Exception as e: return f\\"Error: {str(e)}\\""},{"question":"Objective: Demonstrate your understanding of the `webbrowser` module by implementing a function that registers a custom browser type and utilizes it to open URLs. Requirements: 1. Implement the function `register_and_open_browser(url)`. 2. The function should: - Register a custom browser type named `\\"custom-browser\\"` using a custom command (for simplicity, you can use `\\"firefox\\"` as the command). - Use the newly registered `\\"custom-browser\\"` to open the provided URL in a new tab. 3. Ensure the function handles potential exceptions gracefully, such as when the browser command is not found. Input and Output: - **Input**: A string representing the URL to be opened. - **Output**: None. The function should open the URL in a new tab using the custom registered browser. Constraints: - Assume that browsers like \\"firefox\\" are available on the system. - The URL provided will be a valid web URL. - Focus on correctly using the `webbrowser` module functions and handling exceptions. **Example Usage**: ```python def register_and_open_browser(url: str): import webbrowser # Step 1: Define a constructor for the custom browser using an existing browser command, e.g., firefox. class CustomBrowser(webbrowser.BackgroundBrowser): def __init__(self): super().__init__(\\"firefox\\") # Step 2: Register the custom browser webbrowser.register(\\"custom-browser\\", CustomBrowser) # Step 3: Get the controller for the new custom browser custom_browser = webbrowser.get(\\"custom-browser\\") try: # Step 4: Use the custom browser controller to open the URL in a new tab custom_browser.open_new_tab(url) except webbrowser.Error as e: print(f\\"An error occurred: {e}\\") # Example call to the function register_and_open_browser(\\"https://www.example.com\\") ``` **Note**: The provided function should open the URL in a new tab using the custom-registered browser, and handle any potential exceptions cleanly.","solution":"def register_and_open_browser(url: str): import webbrowser # Step 1: Define a constructor for the custom browser using an existing browser command, e.g., firefox. class CustomBrowser(webbrowser.BackgroundBrowser): def __init__(self): super().__init__(\\"firefox\\") # Step 2: Register the custom browser webbrowser.register(\\"custom-browser\\", CustomBrowser) # Step 3: Get the controller for the new custom browser try: custom_browser = webbrowser.get(\\"custom-browser\\") # Step 4: Use the custom browser controller to open the URL in a new tab custom_browser.open_new_tab(url) except webbrowser.Error as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Advanced Python Coding Assessment Question: Implement a Timeout Decorator Using Signal Module You are required to implement a Python decorator `timeout` which limits the execution time of a function. If the function execution exceeds the specified time, it should raise a `TimeoutError`. Use the `signal` module to handle the timeout functionality. Requirements: 1. The `timeout` decorator should accept a single argument `seconds` which specifies the maximum allowable time (in seconds) for the decorated function to run. 2. If the function completes within the allowable time, it should return the function\'s return value. 3. If the function execution exceeds the specified time, a `TimeoutError` should be raised. 4. Use the `signal` module to handle the signal-based timeout. Input: - A positive integer `seconds` specifying the timeout value. - A function to be decorated. Output: - The return value of the function if it completes within the specified time. - Raise a `TimeoutError` if the function execution exceeds the specified time. Example: ```python from functools import wraps import signal class TimeoutError(Exception): pass def timeout(seconds): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): def handler(signum, frame): raise TimeoutError(f\\"Function {func.__name__} timed out after {seconds} seconds\\") signal.signal(signal.SIGALRM, handler) signal.alarm(seconds) try: result = func(*args, **kwargs) finally: signal.alarm(0) # Cancel the alarm return result return wrapper return decorator # Example usage @timeout(5) def slow_function(): import time time.sleep(10) # Sleep for 10 seconds try: slow_function() except TimeoutError as e: print(e) # Output: Function slow_function timed out after 5 seconds ``` Constraints: - The decorator should only work on Unix-like systems. - Make sure to handle the cancellation of the alarm properly after the function completes or after the timeout occurs. Additional Notes: - Remember to test with different functions to ensure the decorator works as expected. - Consider edge cases where the function may complete just before the timeout.","solution":"from functools import wraps import signal class TimeoutError(Exception): pass def timeout(seconds): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): def handler(signum, frame): raise TimeoutError(f\\"Function {func.__name__} timed out after {seconds} seconds\\") signal.signal(signal.SIGALRM, handler) signal.alarm(seconds) try: result = func(*args, **kwargs) finally: signal.alarm(0) # Cancel the alarm return result return wrapper return decorator"},{"question":"**Objective**: Write a Python script that simulates a file processing scenario while ensuring proper handling of resources. This task is designed to assess your understanding of Python resource management and error handling, particularly when using the Python Development Mode. **Scenario**: You are given several text files, and your task is to compute the total number of lines and the total number of characters across all files. You must ensure that all file handles are properly closed after their content has been processed. **Requirements and Specifications**: 1. Write a function `count_lines_and_characters(file_list)` which takes a list of file names as input and returns a tuple `(total_lines, total_characters)`. 2. Your function should: - Open each file from the list in read mode. - Count the number of lines and characters in each file. - Ensure that all file handles are explicitly closed right after processing. 3. Use Python\'s context manager (`with` statement) to handle file opening and closing. 4. If a file cannot be opened (e.g., it does not exist), the function should print an error message and continue processing the next file. 5. Your script must include a demonstration of running this function and handling at least one non-existent file to showcase the error handling. 6. Additionally, write test cases to verify the correctness of your function using the `unittest` framework. **Constraints**: - Each file size will not exceed 10MB. - The file list will contain at most 100 file names. **Performance requirement**: The function should process each file only once in an efficient manner. **Example**: ```python # Example file list file_list = [\'file1.txt\', \'file2.txt\', \'nonexistent.txt\', \'file3.txt\'] # Expected output and behavior # file1.txt and file2.txt are real files with some lines and characters in them. # nonexistent.txt does not exist, and an error message is printed for this file. # file3.txt is another real file. # Example output might be: # Error: file not found - nonexistent.txt # (total number of lines from real files, total number of characters from real files) ``` ```python def count_lines_and_characters(file_list): total_lines = 0 total_characters = 0 for file_name in file_list: try: with open(file_name, \'r\') as file: lines = file.readlines() total_lines += len(lines) total_characters += sum(len(line) for line in lines) except FileNotFoundError: print(f\\"Error: file not found - {file_name}\\") return total_lines, total_characters # Demonstration if __name__ == \\"__main__\\": files = [\'file1.txt\', \'file2.txt\', \'nonexistent.txt\', \'file3.txt\'] result = count_lines_and_characters(files) print(result) # Tests using unittest import unittest class TestCountLinesAndCharacters(unittest.TestCase): def test_existing_files(self): files = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] self.assertEqual(count_lines_and_characters(files), (correct_total_lines, correct_total_characters)) def test_mixed_files(self): files = [\'file1.txt\', \'nonexistent.txt\', \'file3.txt\'] self.assertEqual(count_lines_and_characters(files), (correct_total_lines, correct_total_characters)) def test_nonexistent_files(self): files = [\'nonexistent1.txt\', \'nonexistent2.txt\'] self.assertEqual(count_lines_and_characters(files), (0, 0)) if __name__ == \\"__main__\\": unittest.main() ``` **Note**: Replace `correct_total_lines` and `correct_total_characters` in the test cases with the expected values based on the actual contents of `file1.txt`, `file2.txt`, and `file3.txt`.","solution":"def count_lines_and_characters(file_list): Count the total number of lines and characters in the given list of files. Parameters: file_list (list): A list of file names to count lines and characters from. Returns: tuple: A tuple containing total number of lines and total number of characters. total_lines = 0 total_characters = 0 for file_name in file_list: try: with open(file_name, \'r\') as file: lines = file.readlines() total_lines += len(lines) total_characters += sum(len(line) for line in lines) except FileNotFoundError: print(f\\"Error: file not found - {file_name}\\") return total_lines, total_characters # Demonstration if __name__ == \\"__main__\\": files = [\'file1.txt\', \'file2.txt\', \'nonexistent.txt\', \'file3.txt\'] result = count_lines_and_characters(files) print(result)"},{"question":"# Objective: You are given a dataset containing information about different species of flowers, including their measurements (e.g., sepal length, sepal width, petal length, petal width) and their class labels (species). Your task is to write a Python function using pandas and its `plotting` module to analyze and visually represent the data in a meaningful way. # Task: 1. **Load the Data**: - Write a function `load_data(file_path: str) -> pd.DataFrame` that loads the dataset from a CSV file and returns it as a pandas DataFrame. 2. **Data Manipulation**: - Write a function `analyze_data(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations: - Calculate the mean of each measurement for each species. - Return the resulting DataFrame. 3. **Data Visualization**: - Write a function `plot_data(df: pd.DataFrame)` that generates the following plots: - A scatter matrix plot using the `scatter_matrix` function to show pairwise relationships between measurements. - A parallel coordinates plot using the `parallel_coordinates` function to show each species\' measurements in a parallel coordinates format. # Input: 1. `file_path`: A string representing the file path to the dataset (CSV format). 2. `df`: A pandas DataFrame containing the flower data. # Output: 1. `load_data` should return a pandas DataFrame containing the loaded data. 2. `analyze_data` should return a pandas DataFrame containing the mean of each measurement for each species. 3. `plot_data` should generate and display the specified plots. # Constraints: - The dataset should include at least the following columns: `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. - Ensure that you handle any missing values appropriately (e.g., by filling them with the mean of the respective columns). # Example: ```python def load_data(file_path: str) -> pd.DataFrame: # Your implementation here def analyze_data(df: pd.DataFrame) -> pd.DataFrame: # Your implementation here def plot_data(df: pd.DataFrame): # Your implementation here # Example usage: file_path = \'flowers.csv\' df = load_data(file_path) summary_df = analyze_data(df) plot_data(df) ``` # Performance Requirements: - The functions should efficiently handle a dataset with at least 150 rows and 5 columns. - The scatter matrix and parallel coordinates plots should be easily interpretable.","solution":"import pandas as pd from pandas.plotting import scatter_matrix, parallel_coordinates import matplotlib.pyplot as plt def load_data(file_path: str) -> pd.DataFrame: Load dataset from a CSV file and return as a pandas DataFrame. :param file_path: Path to the CSV file. :return: DataFrame containing the dataset. df = pd.read_csv(file_path) return df def analyze_data(df: pd.DataFrame) -> pd.DataFrame: Calculate and return mean of each measurement for each species. :param df: DataFrame containing the flower data. :return: DataFrame with means of measurements for each species. return df.groupby(\'species\').mean() def plot_data(df: pd.DataFrame): Generate and display scatter matrix and parallel coordinates plots. :param df: DataFrame containing the flower data. plt.figure(figsize=(12, 8)) scatter_matrix(df, alpha=0.2, figsize=(12, 8), diagonal=\'kde\') plt.suptitle(\'Scatter Matrix\') plt.show() plt.figure(figsize=(12, 8)) parallel_coordinates(df, class_column=\'species\') plt.suptitle(\'Parallel Coordinates\') plt.show()"},{"question":"# Advanced Coding Assessment: Using `concurrent.futures` for Asynchronous Computation Objective You are required to write a Python function that uses the `concurrent.futures` module to perform asynchronous computation on a given set of tasks. Specifically, you will employ both `ThreadPoolExecutor` and `ProcessPoolExecutor` to demonstrate your understanding of concurrent programming. Task Write a function `perform_async_tasks` that accepts a list of callables (tasks), a string indicating the type of executor to use (`thread` or `process`), and an integer for the maximum number of workers. The function should execute the tasks asynchronously and return a list of results. Function Signature ```python def perform_async_tasks(tasks, executor_type, max_workers): Execute a list of tasks asynchronously using the specified executor type. Parameters: tasks (list): A list of callable objects that take no arguments. executor_type (str): Type of executor to use, either \'thread\' or \'process\'. max_workers (int): Maximum number of workers to use for executing tasks. Returns: list: A list of results obtained from executing the tasks. pass ``` Input - `tasks`: A list of callable objects. Each callable represents a task that takes no arguments. - `executor_type`: A string (\'thread\' or \'process\') indicating whether to use `ThreadPoolExecutor` or `ProcessPoolExecutor`. - `max_workers`: An integer specifying the maximum number of worker threads or processes. Output - A list of results obtained by executing the tasks asynchronously. Constraints - If `executor_type` is neither \'thread\' nor \'process\', raise a `ValueError`. - Ensure that the number of workers is a positive integer. If not, raise a `ValueError`. - The function calls should handle exceptions properly and return the exception message if any task fails. Performance Requirements - The solution should efficiently handle the execution of tasks. - The use of futures should demonstrate proper resource management (e.g., using `with` statements to ensure cleanup). Example ```python import time def task1(): time.sleep(2) return \\"Task 1 Completed\\" def task2(): time.sleep(1) return \\"Task 2 Completed\\" def task3(): return 1 / 0 # This task will raise an exception tasks = [task1, task2, task3] results = perform_async_tasks(tasks, \'thread\', 3) print(results) # Expected Output: # [\'Task 1 Completed\', \'Task 2 Completed\', \'division by zero\'] ``` Notes - Make sure you understand the behavior and differences between `ThreadPoolExecutor` and `ProcessPoolExecutor`. - Properly handle any exceptions that may arise during task execution and ensure to test your function with different inputs and configurations.","solution":"import concurrent.futures def perform_async_tasks(tasks, executor_type, max_workers): Execute a list of tasks asynchronously using the specified executor type. Parameters: tasks (list): A list of callable objects that take no arguments. executor_type (str): Type of executor to use, either \'thread\' or \'process\'. max_workers (int): Maximum number of workers to use for executing tasks. Returns: list: A list of results obtained from executing the tasks. if executor_type not in (\'thread\', \'process\'): raise ValueError(\\"executor_type must be either \'thread\' or \'process\'\\") if not isinstance(max_workers, int) or max_workers <= 0: raise ValueError(\\"max_workers must be a positive integer\\") results = [] def run_tasks(executor_factory): with executor_factory(max_workers=max_workers) as executor: futures = [executor.submit(task) for task in tasks] for future in concurrent.futures.as_completed(futures): try: results.append(future.result()) except Exception as exc: results.append(str(exc)) if executor_type == \'thread\': run_tasks(concurrent.futures.ThreadPoolExecutor) elif executor_type == \'process\': run_tasks(concurrent.futures.ProcessPoolExecutor) return results"},{"question":"You are required to demonstrate your understanding of the seaborn library (specifically the `seaborn.objects` module) by performing the following tasks: 1. Load the \\"tips\\" dataset from Seaborn. 2. Create a scatter plot showing the relationship between `total_bill` and `tip`, with points colored by `time` (Lunch/Dinner). 3. Facet the plot by the `smoker` variable, arranging the plots in a single row. 4. Add a linear regression line (first-order polynomial fit) to each facet. 5. Apply a default seaborn theme to the plot. 6. Customize the plot by setting the facecolor of the axes to \\"lightgray\\" and the linewidth of the regression lines to 2. 7. Finally, apply the \\"darkgrid\\" style from Seaborn to the plot. **Input and Output Format:** - **Input**: None. The dataset will be loaded directly within the code. - **Output**: The output should be a displayed plot meeting the specifications above. # Sample Code Structure Here\'s a code structure to get you started: ```python import seaborn.objects as so from seaborn import load_dataset, axes_style # Step 1: Load the tips dataset tips = load_dataset(\\"tips\\") # Step 2: Create scatter plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"time\\") .facet(\\"smoker\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Step 3: Apply default seaborn theme p.theme(axes_style()) # Step 4: Customize plot p.theme({\\"axes.facecolor\\": \\"lightgray\\", \\"lines.linewidth\\": 2}) # Step 5: Apply darkgrid style p.theme(axes_style(\\"darkgrid\\")) # Display plot p.show() ``` # Notes: - Make sure the plot facets align horizontally as specified. - Ensure that the plot customization steps are applied in the correct order. **Criteria for Evaluation:** - Correct loading of the dataset. - Accurate creation of the scatter plot with color encoding and faceting. - Proper addition of the regression line and other elements. - Application of the correct themes and styles. - Clarity and organization of code.","solution":"import seaborn.objects as so from seaborn import load_dataset, axes_style def create_custom_scatter_plot(): # Step 1: Load the tips dataset tips = load_dataset(\\"tips\\") # Step 2: Create scatter plot with faceting by smoker variable p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"time\\") .facet(\\"smoker\\", wrap=2) .add(so.Dot()) .add(so.Line(), so.PolyFit(order=1)) ) # Step 3: Apply default seaborn theme p.theme(axes_style()) # Step 4: Customize plot p.theme({\\"axes.facecolor\\": \\"lightgray\\", \\"lines.linewidth\\": 2}) # Step 5: Apply darkgrid style p.theme(axes_style(\\"darkgrid\\")) # Display plot p.show() # Call the function to create the custom scatter plot create_custom_scatter_plot()"},{"question":"**Question:** You\'re tasked with building a simulation for a simplified board game where two players, A and B, roll a six-sided die, move on a straight path, and the first one to cross a finish line wins. Both players start at position 0 on a line numbered from 0 to 100. Each turn, a player rolls a six-sided die to determine how many steps to move forward. The game ends when a player crosses or lands exactly on the position 100. You need to implement the simulation in Python using the `random` module. Your task is to write a function `simulate_game()` that returns the name of the winning player (either \'A\' or \'B\') and the number of turns it took for the game to end. # Input: - No input parameters. # Output: - A tuple with the name of the winning player (\'A\' or \'B\') and the number of turns taken for the game to end. # Constraints: 1. Players take turns alternately, starting with Player A. 2. The roll of the die should use `random.randint(1, 6)` to generate a number between 1 and 6 inclusive, which determines the number of steps in the current turn. 3. Ensure reproducibility by setting the seed for the random number generator at the beginning of the simulation with `random.seed(12345)`. # Example: ```python def simulate_game(): # Your code here winner, turns = simulate_game() print(winner, turns) # Expected example output (actual output may vary due to random nature): (\'A\', 21). ``` # Requirements: - Use the `random` module to simulate the die rolls. - Ensure that the function `simulate_game()` correctly alternates turns between the two players until one reaches or surpasses the 100th position. - Return the name of the winning player and the total number of turns. # Performance: The function should efficiently handle the simulation and must complete within a reasonable time frame for standard execution environments.","solution":"import random def simulate_game(): random.seed(12345) # Initialize positions and the turn counter position_A = 0 position_B = 0 turns = 0 while True: turns += 1 # Player A\'s turn roll_A = random.randint(1, 6) position_A += roll_A if position_A >= 100: return (\'A\', turns) turns += 1 # Player B\'s turn roll_B = random.randint(1, 6) position_B += roll_B if position_B >= 100: return (\'B\', turns)"},{"question":"Objective To assess understanding and proficiency in using pandas windowing operations, particularly the `rolling` and `expanding` methods and the application of custom aggregation functions. Problem Statement You are given a dataset of daily sales data for a retail store over one year. The dataset has the following columns: - `date`: The date of the sales record. - `sales`: The number of units sold on that day. Implement a function `analyze_sales` that takes this dataset as a pandas DataFrame and performs the following operations: 1. **7-day Rolling Average**: Compute a 7-day rolling average of the sales. 2. **7-day Rolling Sum**: Compute a 7-day rolling sum of the sales. 3. **Expanding Mean**: Compute the expanding mean of the sales from the start of the dataset. 4. **Custom Rolling Function**: Implement a custom rolling function that calculates the mean absolute deviation (MAD) over a 5-day window using the rolling `apply` method. 5. **Centered 14-day Rolling Standard Deviation**: Calculate the rolling standard deviation over a 14-day window but center the window. Report the result with date labels set to the center of the window. Function Signature ```python import pandas as pd def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input The function `analyze_sales` receives a single parameter: - `df`: A pandas DataFrame with columns: - `date` (datetime64[ns]): Date of each record. - `sales` (int): Number of units sold. Output The function should return a pandas DataFrame that contains the original `date` and `sales` columns along with these new columns: - `rolling_avg_7d`: The 7-day rolling average of sales. - `rolling_sum_7d`: The 7-day rolling sum of sales. - `expanding_mean`: The expanding mean of sales. - `rolling_mad_5d`: The mean absolute deviation over a 5-day rolling window. - `rolling_std_14d_centered`: The 14-day centered rolling standard deviation of sales. Constraints - The input DataFrame contains at least 30 days of sales data. - The date column is continuous and has no gaps. Example Usage ```python import pandas as pd from datetime import datetime data = { \'date\': pd.date_range(start=\'2023-01-01\', periods=30, freq=\'D\'), \'sales\': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113] } df = pd.DataFrame(data) result = analyze_sales(df) print(result) ``` This challenge will assess the students ability to utilize pandas\' advanced windowing operations and apply custom aggregation functions effectively.","solution":"import pandas as pd import numpy as np def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: Perform various windowing operations on the sales data in the DataFrame. Parameters: df (pd.DataFrame): Input DataFrame with columns \'date\' and \'sales\'. Returns: pd.DataFrame: A new DataFrame with additional columns for rolling and expanding statistics. result = df.copy() # 7-day Rolling Average result[\'rolling_avg_7d\'] = result[\'sales\'].rolling(window=7).mean() # 7-day Rolling Sum result[\'rolling_sum_7d\'] = result[\'sales\'].rolling(window=7).sum() # Expanding Mean result[\'expanding_mean\'] = result[\'sales\'].expanding(min_periods=1).mean() # Custom Rolling Function: Mean Absolute Deviation (MAD) over a 5-day window result[\'rolling_mad_5d\'] = result[\'sales\'].rolling(window=5).apply(lambda x: np.mean(np.abs(x - np.mean(x))), raw=False) # Centered 14-day Rolling Standard Deviation result[\'rolling_std_14d_centered\'] = result[\'sales\'].rolling(window=14, center=True).std() return result"},{"question":"**Objective:** Create a custom color palette using Seaborn\'s `cubehelix_palette` function, and apply this palette to visualize a given dataset. This will demonstrate your understanding of creating and customizing Seaborn color palettes. **Instructions:** 1. **Palette Creation:** - Use Seaborn\'s `cubehelix_palette` function to create a custom color palette with the following characteristics: - The palette should have 10 colors. - The helix should start at 1.5 (use the `start` parameter). - The helix should rotate 1 full cycle (use the `rot` parameter). - Apply a gamma correction of 0.8 (use the `gamma` parameter). - Increase the saturation of the colors to 0.7 (use the `hue` parameter). - Set the luminance at the start to 0.3 and at the end to 0.9 (use the `dark` and `light` parameters). 2. **Data Visualization:** - Load a dataset of your choice using Seaborn\'s inbuilt datasets (e.g., \'penguins\', \'iris\', etc.). - Create a visualization that uses the custom color palette created in the previous step. You can use any suitable plot (e.g., scatter plot, bar plot, line plot, etc.). 3. **Implementation Details:** - Your function should be named `custom_palette_visualization`. - The function should take no parameters and should: - Create the custom color palette. - Load a dataset. - Create a plot using the dataset and apply the custom palette. **Output:** - Display the plot generated with the custom color palette. **Constraints:** - Ensure that the plot adheres to good visualization practices, including proper labeling of axes and a title. **Performance Requirements:** - The function should execute efficiently without unnecessary computations. ```python import seaborn as sns import matplotlib.pyplot as plt def custom_palette_visualization(): # Create custom cubehelix color palette palette = sns.cubehelix_palette(n_colors=10, start=1.5, rot=1, gamma=0.8, hue=0.7, dark=0.3, light=0.9) # Load a dataset data = sns.load_dataset(\'iris\') # Create a scatter plot using the custom palette sns.scatterplot(data=data, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette) # Display the plot plt.title(\'Iris Dataset Visualization with Custom Cubehelix Palette\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.show() # Calling the function to test the implementation custom_palette_visualization() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_palette_visualization(): Create a custom color palette using the cubehelix_palette method and visualize a dataset using this palette. # Create custom cubehelix color palette palette = sns.cubehelix_palette(n_colors=10, start=1.5, rot=1, gamma=0.8, hue=0.7, dark=0.3, light=0.9) # Load the iris dataset data = sns.load_dataset(\'iris\') # Create a scatter plot using the custom palette plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette) # Display the plot with appropriate labels and title plt.title(\'Iris Dataset Visualization with Custom Cubehelix Palette\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.show() # Calling the function to display the plot custom_palette_visualization()"},{"question":"# Python `gzip` Module Challenge Objective: Write a Python function that takes a directory path containing multiple text files, compresses each file into a GZIP format, and stores them in a specified output directory. Additionally, provide a function to decompress these files back into their original state. Requirements: 1. Implement the function `compress_files(input_dir: str, output_dir: str) -> None`: - `input_dir`: Path to the directory containing the text files to be compressed. - `output_dir`: Path to the directory where the compressed GZIP files will be stored. - For each text file in `input_dir`, compress the file into GZIP format and save it in `output_dir` with the same name but an added `.gz` extension. 2. Implement the function `decompress_files(input_dir: str, output_dir: str) -> None`: - `input_dir`: Path to the directory containing the GZIP files to be decompressed. - `output_dir`: Path to the directory where the decompressed text files will be stored. - For each GZIP file in `input_dir`, decompress the file and save it in `output_dir` with the original filename (removing the `.gz` extension). Constraints: - Ensure that paths provided are valid and handle any exceptions related to file operations gracefully. - If `output_dir` does not exist, create it. - Assume you have read permissions for `input_dir` and write permissions for `output_dir`. Example Usage: ```python compress_files(\'/path/to/text/files\', \'/path/to/compressed/files\') # This will compress all text files in \'/path/to/text/files\' directory decompress_files(\'/path/to/compressed/files\', \'/path/to/uncompressed/files\') # This will decompress all GZIP files in \'/path/to/compressed/files\' directory ``` # Implementation: ```python import os import gzip import shutil def compress_files(input_dir: str, output_dir: str) -> None: if not os.path.exists(output_dir): os.makedirs(output_dir) for root, _, files in os.walk(input_dir): for file in files: input_path = os.path.join(root, file) output_path = os.path.join(output_dir, file + \'.gz\') with open(input_path, \'rb\') as f_in: with gzip.open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) def decompress_files(input_dir: str, output_dir: str) -> None: if not os.path.exists(output_dir): os.makedirs(output_dir) for root, _, files in os.walk(input_dir): for file in files: input_path = os.path.join(root, file) if not file.endswith(\'.gz\'): continue output_path = os.path.join(output_dir, file[:-3]) with gzip.open(input_path, \'rb\') as f_in: with open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) ``` Instructions: - Write clean, readable, and well-documented code. - Ensure that your code handles errors and edge cases gracefully. - Test your functions to verify they compress and decompress files correctly.","solution":"import os import gzip import shutil def compress_files(input_dir: str, output_dir: str) -> None: Compresses text files from the input directory and stores them as GZIP files in the output directory. Args: - input_dir (str): Path to the directory containing the text files to be compressed. - output_dir (str): Path to the directory where the compressed GZIP files will be stored. Returns: - None if not os.path.exists(output_dir): os.makedirs(output_dir) for root, _, files in os.walk(input_dir): for file in files: input_path = os.path.join(root, file) output_path = os.path.join(output_dir, file + \'.gz\') with open(input_path, \'rb\') as f_in: with gzip.open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) def decompress_files(input_dir: str, output_dir: str) -> None: Decompresses GZIP files from the input directory and stores them as text files in the output directory. Args: - input_dir (str): Path to the directory containing the GZIP files to be decompressed. - output_dir (str): Path to the directory where the decompressed text files will be stored. Returns: - None if not os.path.exists(output_dir): os.makedirs(output_dir) for root, _, files in os.walk(input_dir): for file in files: input_path = os.path.join(root, file) if not file.endswith(\'.gz\'): continue output_path = os.path.join(output_dir, file[:-3]) with gzip.open(input_path, \'rb\') as f_in: with open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out)"},{"question":"**Course: Advanced Programming with PyTorch** # Question: Efficient Tensor Operations and Memory Management with PyTorch Objective To assess your understanding of advanced PyTorch concepts including tensor operations, memory management, and the use of PyTorch\'s memory statistics functionalities. Problem Statement You are tasked with implementing a function that performs a series of tensor operations efficiently, ensuring optimal memory usage. Your function should also collect and return memory statistics after performing the operations. Function Signature ```python def tensor_operation_and_memory_stats(input_tensor: torch.Tensor) -> dict: pass ``` Input - `input_tensor` (torch.Tensor): A 2D tensor of shape (n, m) with random values. Output - A dictionary containing memory statistics after performing the operations. The dictionary should have the following keys: - `initial_allocated`: Memory allocated before operations. - `final_allocated`: Memory allocated after operations. - `max_allocated`: Maximum memory allocated during the operations. - `peak_allocated`: Peak memory usage observed. Constraints 1. The input tensor can have dimensions up to 1000 x 1000. 2. Use PyTorch functions for all tensor operations. 3. The memory statistics should be collected using `torch.mtia.memory.memory_stats()`. Operations to Perform 1. Normalize the input tensor: - Each element in the tensor should be scaled such that the tensor\'s values lie in the range [0, 1]. 2. Compute the matrix product of the normalized tensor with its transpose: - Resulting in a tensor of shape (n, n). 3. Apply a ReLU activation to the resulting tensor. - Use `torch.nn.functional.relu` for this purpose. 4. After these operations, collect the required memory statistics using the `torch.mtia.memory.memory_stats()` function. Example ```python import torch # Example tensor input_tensor = torch.rand((4, 3)) # Function call stats = tensor_operation_and_memory_stats(input_tensor) # Expected output keys in the dictionary print(stats.keys()) # Output: dict_keys([\'initial_allocated\', \'final_allocated\', \'max_allocated\', \'peak_allocated\']) ``` Ensure your implementation is efficient and follows best practices. Your code will be evaluated based on correctness, performance, and ability to manage memory effectively.","solution":"import torch def tensor_operation_and_memory_stats(input_tensor: torch.Tensor) -> dict: # Compute initial memory statistics initial_allocated = torch.cuda.memory_allocated() # Normalize the input tensor to have values in the range [0, 1] min_val = input_tensor.min() max_val = input_tensor.max() normalized_tensor = (input_tensor - min_val) / (max_val - min_val) # Compute the matrix product of the normalized tensor with its transpose product_tensor = torch.mm(normalized_tensor, normalized_tensor.t()) # Apply ReLU activation to the resulting tensor relu_tensor = torch.nn.functional.relu(product_tensor) # Compute memory statistics after operations final_allocated = torch.cuda.memory_allocated() max_allocated = torch.cuda.max_memory_allocated() peak_allocated = torch.cuda.memory_allocated() return { \'initial_allocated\': initial_allocated, \'final_allocated\': final_allocated, \'max_allocated\': max_allocated, \'peak_allocated\': peak_allocated }"},{"question":"# Abstract Base Class Implementation Challenge **Objective:** Create a hierarchy of classes to represent a simple vehicle system using Python\'s `abc` module. This exercise is designed to test your understanding of abstract base classes, metaclasses, and abstract methods in Python. **Instructions:** 1. Implement an abstract base class named `Vehicle` using the `ABC` class or `ABCMeta` metaclass. Ensure that this class: - Has an abstract method `start_engine(self)`. - Has an abstract method `stop_engine(self)`. - Has an abstract property `number_of_wheels(self)`. 2. Implement two concrete classes, `Car` and `Bicycle`, that inherit from the `Vehicle` class and provide implementations for all abstract methods and properties: - The `Car` class should return `4` for `number_of_wheels`. - The `Bicycle` class should return `2` for `number_of_wheels`. 3. Implement a feature for virtual subclassing: - Create a class `Unicycle` that does not inherit from `Vehicle`, but can be registered as a virtual subclass of `Vehicle`. - Provide an implementation for all abstract methods and properties in `Unicycle`. **Constraints:** - Do not modify the signature of the abstract methods and properties. - Ensure that all abstract methods and properties are appropriately overridden in the concrete subclasses. **Example:** ```python from abc import ABC, abstractmethod class Vehicle(ABC): @abstractmethod def start_engine(self): pass @abstractmethod def stop_engine(self): pass @property @abstractmethod def number_of_wheels(self): pass # Implement the Car class class Car(Vehicle): def start_engine(self): return \\"Car engine started\\" def stop_engine(self): return \\"Car engine stopped\\" @property def number_of_wheels(self): return 4 # Implement the Bicycle class class Bicycle(Vehicle): def start_engine(self): return \\"Bicycles don\'t have an engine to start\\" def stop_engine(self): return \\"Bicycles don\'t have an engine to stop\\" @property def number_of_wheels(self): return 2 # Virtual subclass implementation class Unicycle: def start_engine(self): return \\"Unicycles don\'t have an engine to start\\" def stop_engine(self): return \\"Unicycles don\'t have an engine to stop\\" @property def number_of_wheels(self): return 1 # Register Unicycle as a virtual subclass of Vehicle Vehicle.register(Unicycle) # Test code (for your verification) car = Car() print(car.start_engine()) # Output: \\"Car engine started\\" print(car.number_of_wheels) # Output: 4 bicycle = Bicycle() print(bicycle.start_engine()) # Output: \\"Bicycles don\'t have an engine to start\\" print(bicycle.number_of_wheels) # Output: 2 unicycle = Unicycle() print(unicycle.start_engine()) # Output: \\"Unicycles don\'t have an engine to start\\" print(unicycle.number_of_wheels) # Output: 1 print(isinstance(unicycle, Vehicle)) # Output: True ``` **Your task is to complete the implementation of the `Vehicle`, `Car`, `Bicycle`, and `Unicycle` classes as described.**","solution":"from abc import ABC, abstractmethod class Vehicle(ABC): @abstractmethod def start_engine(self): pass @abstractmethod def stop_engine(self): pass @property @abstractmethod def number_of_wheels(self): pass # Implement the Car class class Car(Vehicle): def start_engine(self): return \\"Car engine started\\" def stop_engine(self): return \\"Car engine stopped\\" @property def number_of_wheels(self): return 4 # Implement the Bicycle class class Bicycle(Vehicle): def start_engine(self): return \\"Bicycles don\'t have an engine to start\\" def stop_engine(self): return \\"Bicycles don\'t have an engine to stop\\" @property def number_of_wheels(self): return 2 # Virtual subclass implementation class Unicycle: def start_engine(self): return \\"Unicycles don\'t have an engine to start\\" def stop_engine(self): return \\"Unicycles don\'t have an engine to stop\\" @property def number_of_wheels(self): return 1 # Register Unicycle as a virtual subclass of Vehicle Vehicle.register(Unicycle)"},{"question":"**Title:** Processing Logs from Multiple Input Streams **Objective:** Write a Python function that reads from multiple log files and processes specific patterns to generate a summary report. The function should utilize the `fileinput` module to read lines from the provided files and must demonstrate the usage of features like handling different encodings, in-place file filtering, and optionally using hooks for compressed files. **Problem Statement:** You are given multiple log files containing records with varying formats. Each record in the log files is a line of text. Your task is to write a function `process_logs(files: List[str], mode: str = \'r\', encoding: Optional[str] = None) -> Dict[str, int]` that reads the lines from the files, identifies and counts the number of occurrences of specific keywords, and returns the counts as a dictionary. # Requirements: 1. **Function Signature:** ```python def process_logs(files: List[str], mode: str = \'r\', encoding: Optional[str] = None) -> Dict[str, int]: ``` 2. **Input:** - `files`: A list of file names (strings). It can include files with `.gz` and `.bz2` extensions that indicate compressed files. - `mode`: The mode in which to open the files. Default is `\'r\'`. - `encoding`: The encoding to use for reading files. Default is `None`. 3. **Output:** - Returns a dictionary with counts of occurrences for the following keywords: `ERROR`, `WARNING`, and `INFO`. 4. **Constraints:** - Files can be empty. - Handle cases where files may not exist by raising an appropriate exception. - File input should be managed using the `fileinput` module. 5. **Performance and Edge Cases:** - Efficiently handle large log files. - Ensure that the function can read from both normal and compressed files using appropriate hooks. # Example: Suppose you have three log files: - `log1.txt` contains: ``` INFO Starting the process ERROR Failed to connect to database INFO Retry connection WARNING Low disk space ``` - `log2.txt` contains: ``` ERROR Timeout while connecting ERROR Failed to read INFO Connection established ``` - `log3.gz` contains (compressed): ``` INFO Processing data WARNING Memory usage high ``` **Function Call:** ```python files = [\'log1.txt\', \'log2.txt\', \'log3.gz\'] result = process_logs(files, mode=\'r\', encoding=\'utf-8\') ``` **Expected Output:** ```python { \'INFO\': 4, \'ERROR\': 3, \'WARNING\': 2 } ``` **Notes:** - Use the `fileinput` modules `input()` function with appropriate mode, encoding, and hooks to handle compressed files. - Use the provided functions and methods like `lineno()`, `filename()`, and hooks for reading compressed files. **Hint:** - You may use `fileinput.hook_compressed` for handling compressed files. - Ensure to handle empty files gracefully. ```python import fileinput from typing import List, Dict, Optional def process_logs(files: List[str], mode: str = \'r\', encoding: Optional[str] = None) -> Dict[str, int]: Processes multiple log files and counts occurrences of \'ERROR\', \'WARNING\', and \'INFO\'. Parameters: - files (List[str]): List of log file names. - mode (str): The mode in which to open the files, default is \'r\'. - encoding (Optional[str]): The encoding to use for reading files, default is None. Returns: - Dict[str, int]: Dictionary with counts for \'ERROR\', \'WARNING\', and \'INFO\'. # Initialize the counts dictionary counts = {\'ERROR\': 0, \'WARNING\': 0, \'INFO\': 0} # Define the hook for compressed files openhook = fileinput.hook_compressed # Use fileinput to read from multiple files with fileinput.input(files=files, mode=mode, encoding=encoding, openhook=openhook) as f: for line in f: if \'ERROR\' in line: counts[\'ERROR\'] += 1 if \'WARNING\' in line: counts[\'WARNING\'] += 1 if \'INFO\' in line: counts[\'INFO\'] += 1 return counts ```","solution":"import fileinput from typing import List, Dict, Optional def process_logs(files: List[str], mode: str = \'r\', encoding: Optional[str] = None) -> Dict[str, int]: Processes multiple log files and counts occurrences of \'ERROR\', \'WARNING\', and \'INFO\'. Parameters: - files (List[str]): List of log file names. - mode (str): The mode in which to open the files, default is \'r\'. - encoding (Optional[str]): The encoding to use for reading files, default is None. Returns: - Dict[str, int]: Dictionary with counts for \'ERROR\', \'WARNING\', and \'INFO\'. # Initialize the counts dictionary counts = {\'ERROR\': 0, \'WARNING\': 0, \'INFO\': 0} # Define the hook for compressed files openhook = fileinput.hook_compressed # Use fileinput to read from multiple files with fileinput.input(files=files, mode=mode, encoding=encoding, openhook=openhook) as f: for line in f: if \'ERROR\' in line: counts[\'ERROR\'] += 1 if \'WARNING\' in line: counts[\'WARNING\'] += 1 if \'INFO\' in line: counts[\'INFO\'] += 1 return counts"},{"question":"# **Coding Assessment Question: Complex Container Manipulation** **Objective**: The goal of this assessment is to evaluate your ability to use advanced container datatypes provided by the `collections` module to solve a complex problem involving data aggregation, transformation, and manipulation. **Problem Statement**: You are given a list of records where each record is a tuple containing three fields: `category`, `item`, and `quantity`. Your task is to write three functions: 1. **aggregate_records**: Aggregates the total quantity of each item per category. 2. **top_items_per_category**: Identifies the top `n` items with the highest quantities for each category. 3. **format_output**: Formats the results into a specific string format. **Detailed Specifications**: 1. **Function: `aggregate_records`** **Input**: - a list of tuples where each tuple contains (`category: str`, `item: str`, `quantity: int`). **Output**: - a `ChainMap` object where each key is a category and the value is a `Counter` object counting quantities of each item. **Constraints**: - Categories, items, and quantities are valid and non-empty. - Each record\'s quantity is a non-negative integer. ```python def aggregate_records(records: list) -> ChainMap: pass ``` 2. **Function: `top_items_per_category`** **Input**: - a `ChainMap` object obtained from `aggregate_records`. - an integer `n` representing the number of top items to list for each category. **Output**: - a defaultdict where each key is a category and its value is a list of tuples (`item: str`, `quantity: int`) of the top `n` items, ordered by their quantity in descending order. If multiple items have the same quantity, they should be ordered alphabetically. **Constraints**: - `n` is a positive integer. - If a category has fewer than `n` items, all items are listed. ```python def top_items_per_category(aggregate: ChainMap, n: int) -> defaultdict: pass ``` 3. **Function: `format_output`** **Input**: - A defaultdict obtained from `top_items_per_category`. **Output**: - A string formatted in the following way: ``` Category: <category_name> Item: <item1>, Quantity: <quantity1> Item: <item2>, Quantity: <quantity2> ... ``` - Each category is separated by a line break. ```python def format_output(top_items: defaultdict) -> str: pass ``` **Example Usage**: ```python records = [ (\'fruits\', \'apple\', 10), (\'fruits\', \'banana\', 5), (\'fruits\', \'orange\', 8), (\'vegetables\', \'carrot\', 12), (\'vegetables\', \'broccoli\', 6), (\'vegetables\', \'spinach\', 4) ] # Function 1: Aggregate records aggregated = aggregate_records(records) # Output: ChainMap({\'fruits\': Counter({\'apple\': 10, \'orange\': 8, \'banana\': 5}), \'vegetables\': Counter({\'carrot\': 12, \'broccoli\': 6, \'spinach\': 4})}) # Function 2: Get top items per category top_items = top_items_per_category(aggregated, 2) # Output: defaultdict(<class \'list\'>, {\'fruits\': [(\'apple\', 10), (\'orange\', 8)], \'vegetables\': [(\'carrot\', 12), (\'broccoli\', 6)]) # Function 3: Format the output formatted_output = format_output(top_items) print(formatted_output) # Output: Category: fruits Item: apple, Quantity: 10 Item: orange, Quantity: 8 Category: vegetables Item: carrot, Quantity: 12 Item: broccoli, Quantity: 6 ``` **Evaluation Criteria**: 1. **Correctness**: The functions should return correct and expected results for different inputs. 2. **Use of Collections**: Effective and appropriate use of `ChainMap`, `Counter`, and `defaultdict` as specified. 3. **Code Readability**: Clear and understandable code. 4. **Performance**: Efficient handling of the operations, keeping in mind the constraints.","solution":"from collections import ChainMap, Counter, defaultdict def aggregate_records(records): Aggregates the total quantity of each item per category. Args: - records (list of tuples): List of records, each a tuple (category, item, quantity). Returns: - ChainMap: Where each key is a category and the value is a Counter of items and their quantities. category_map = defaultdict(Counter) for category, item, quantity in records: category_map[category][item] += quantity return ChainMap(category_map) def top_items_per_category(aggregate, n): Identifies the top `n` items with the highest quantities for each category. Args: - aggregate (ChainMap): Aggregated records from aggregate_records. - n (int): Number of top items to list for each category. Returns: - defaultdict: Each key is a category and its value is a list of tuples of top `n` items. result = defaultdict(list) for category, counter in aggregate.items(): top_items = counter.most_common(n) result[category] = sorted(top_items, key=lambda x: (-x[1], x[0])) return result def format_output(top_items): Formats the results into a specific string format. Args: - top_items (defaultdict): Top items per category from top_items_per_category. Returns: - str: Formatted output string. output = [] for category, items in top_items.items(): output.append(f\\"Category: {category}\\") for item, quantity in items: output.append(f\\" Item: {item}, Quantity: {quantity}\\") output.append(\\"\\") return \\"n\\".join(output).strip()"},{"question":"# PyTorch Advanced Profiling and Metadata Management Problem Statement: You are tasked with profiling the performance of operators within a PyTorch model and attaching metadata to the saved TorchScript model. Your solution should include: 1. Implementing a profiling mechanism that logs the time taken by each operator in a PyTorch model during its execution. 2. Creating a mechanism to attach additional metadata to a TorchScript model before saving it. Task 1: Profiling Operators Create a Python class `OperatorProfiler` that will: - Enable profiling of all operators in a given PyTorch model. - Log the name, input size, and execution time of each operator. Task 2: Attaching Metadata Extend the PyTorch `torch.jit.save` functionality to allow attaching additional metadata such as a producer\'s name and timestamp to the saved model. Implementation Details: **Task 1**: Operator Profiling - Use `torch.autograd.profiler` and `torch::addGlobalCallback` to achieve profiling. - Log the information to a dictionary with the structure: ```python { \\"operator_name\\": { \\"input_size\\": int, \\"execution_time\\": float }, ... } ``` **Task 2**: Attaching Metadata - Use `_extra_files` argument of `torch.jit.save` to include metadata. - Metadata should include: - Producer\'s name (passed as an argument to your solution). - Timestamp of when the model was saved. **Function Signature:** ```python class OperatorProfiler: def __init__(self, model: torch.nn.Module, input_data: torch.Tensor): # Initialize with a PyTorch model and example input data. def start_profiling(self) -> None: # Start profiling operators. def get_profiling_results(self) -> dict: # Return profiling results. def save_model_with_metadata(self, file_path: str, producer_name: str) -> None: # Save the TorchScript model with metadata. ``` **Example Usage:** ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) # Initialize model and input data model = SimpleModel() input_data = torch.randn(1, 10) # Create an OperatorProfiler instance profiler = OperatorProfiler(model, input_data) # Start profiling profiler.start_profiling() # Run model inference output = model(input_data) # Get profiling results profiling_results = profiler.get_profiling_results() print(profiling_results) # Save the model with metadata profiler.save_model_with_metadata(\\"simple_model.pt\\", \\"John Doe\\") ``` **Constraints:** - Input data is a single tensor. - You cannot modify any PyTorch source code. - You are to handle any exceptions that might arise during profiling or saving the model. **Evaluation:** Your solution will be evaluated based on: - Correctness and completeness of profiling implementation. - Proper attachment of metadata to the saved model. - Adherence to the provided function signatures and constraints.","solution":"import torch import torch.nn as nn from torch.autograd.profiler import profile import time import json class OperatorProfiler: def __init__(self, model: nn.Module, input_data: torch.Tensor): self.model = model self.input_data = input_data self.profiling_results = {} def start_profiling(self) -> None: # Use torch.autograd.profiler to profile the operations with profile(record_shapes=True, use_cuda=torch.cuda.is_available()) as prof: with torch.no_grad(): self.model(self.input_data) for event in prof.function_events: key = event.name if key not in self.profiling_results: self.profiling_results[key] = { \\"input_size\\": 0, \\"execution_time\\": 0.0 } self.profiling_results[key][\'input_size\'] = event.cpu_memory_usage self.profiling_results[key][\'execution_time\'] += event.cpu_time_total def get_profiling_results(self) -> dict: return self.profiling_results def save_model_with_metadata(self, file_path: str, producer_name: str) -> None: # Save TorchScript model with metadata scripted_model = torch.jit.script(self.model) timestamp = time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.localtime()) metadata = { \\"producer_name\\": producer_name, \\"timestamp\\": timestamp } extra_files = dict() extra_files[\'metadata.json\'] = json.dumps(metadata) torch.jit.save(scripted_model, file_path, _extra_files=extra_files)"},{"question":"# PyTorch Coding Assessment Question Objective Demonstrate your understanding of PyTorch tensors and their dimensions using the `torch.Size` class. Problem Statement You are provided with a multi-dimensional tensor. Your task is to write a function called `resize_tensor` that resizes the given tensor to a new specified size, if possible. The function should take two arguments: - `tensor`: The input tensor (of type `torch.Tensor`). - `new_size`: A list or tuple containing the new size for the tensor. The function should return the resized tensor. **Function Signature:** ```python import torch def resize_tensor(tensor: torch.Tensor, new_size: list) -> torch.Tensor: pass ``` Constraints - You can assume that the total number of elements in the tensor remains the same after resizing. - If the provided `new_size` is incompatible with the number of elements in the original tensor, raise a `ValueError` with the message \\"Incompatible size.\\" Examples ```python # Example 1: tensor = torch.ones(2, 3, 4) new_size = [3, 8] resized_tensor = resize_tensor(tensor, new_size) print(resized_tensor.size()) # Output: torch.Size([3, 8]) # Example 2: tensor = torch.ones(2, 3, 4) new_size = [2, 6, 2] resized_tensor = resize_tensor(tensor, new_size) print(resized_tensor.size()) # Output: torch.Size([2, 6, 2]) # Example 3: tensor = torch.ones(2, 3, 4) new_size = [7, 4] try: resized_tensor = resize_tensor(tensor, new_size) except ValueError as e: print(e) # Output: Incompatible size ``` Explanation 1. The function `resize_tensor` should check if the `new_size` has the same number of total elements as the original tensor. 2. If the sizes are compatible, the function should resize the tensor using PyTorch\'s tensor manipulation functionalities. 3. If not compatible, the function should raise a `ValueError` with the appropriate message. **Notes:** - Use `torch.reshape` function to resize the tensor. - Utilize `torch.Size` to handle the dimensions and validate the total number of elements. This problem tests your ability to understand and manipulate tensor sizes using PyTorch functionality, ensuring you can handle tensor dimensions correctly.","solution":"import torch def resize_tensor(tensor: torch.Tensor, new_size: list) -> torch.Tensor: Resizes the given tensor to the new specified size, if possible. Parameters: tensor (torch.Tensor): The input tensor to be resized. new_size (list): A list containing the new size for the tensor. Returns: torch.Tensor: The resized tensor. Raises: ValueError: If the new size is incompatible with the number of elements in the original tensor. original_num_elements = tensor.numel() new_num_elements = 1 for dim in new_size: new_num_elements *= dim if original_num_elements != new_num_elements: raise ValueError(\\"Incompatible size\\") return torch.reshape(tensor, new_size)"},{"question":"# Source Distribution Inclusion/Exclusion Processor **Objective:** Implement a function that processes a list of files and a series of `sdist` manifest commands to determine which files should be included in the final source distribution. **Problem Statement:** You need to implement a function `process_manifest_commands` that takes in a list of filenames representing the files in the source tree and a list of manifest commands, and returns a list of filenames that should be included in the source distribution. **Function Signature:** ```python def process_manifest_commands(file_list: List[str], commands: List[str]) -> List[str]: pass ``` **Input:** - `file_list`: A list of strings, where each string represents a file path. - `commands`: A list of strings, where each string is a manifest command as described in the documentation. **Output:** - A list of strings representing the filenames that should be included in the source distribution after processing the commands. **Constraints:** - The manifest commands will be well-formed. - Commands may include: - `include pat1 pat2 ...` - `exclude pat1 pat2 ...` - `recursive-include dir pat1 pat2 ...` - `recursive-exclude dir pat1 pat2 ...` - `global-include pat1 pat2 ...` - `global-exclude pat1 pat2 ...` - `prune dir` - `graft dir` **Behavior:** - `include`: Includes all files matching any of the listed patterns. - `exclude`: Excludes all files matching any of the listed patterns. - `recursive-include`: Includes all files under `dir` matching any of the listed patterns. - `recursive-exclude`: Excludes all files under `dir` matching any of the listed patterns. - `global-include`: Includes all files anywhere in the source tree matching any of the listed patterns. - `global-exclude`: Excludes all files anywhere in the source tree matching any of the listed patterns. - `prune`: Excludes all files under the specified directory. - `graft`: Includes all files under the specified directory. **Example:** ```python file_list = [\\"src/main.py\\", \\"src/utils/helper.py\\", \\"tests/test_main.py\\", \\"README.md\\", \\"setup.py\\"] commands = [ \\"include README.md setup.py\\", \\"recursive-include src *.py\\", \\"global-exclude *.pyc\\", \\"prune tests\\" ] output = process_manifest_commands(file_list, commands) print(output) # Expected output: [\\"README.md\\", \\"setup.py\\", \\"src/main.py\\", \\"src/utils/helper.py\\"] ``` **Note:** The patterns should follow Unix-style \\"glob\\" patterns: - `*` matches any sequence of regular filename characters. - `?` matches any single regular filename character. - `[range]` matches any of the characters in `range` (e.g., `[a-z]`, `[a-zA-Z]`, `[a-f0-9_.]`). You must correctly handle the inclusion and exclusion logic as outlined to determine the final list of files in the source distribution.","solution":"import fnmatch import os def process_manifest_commands(file_list, commands): included_files = set() # To store files explicitly included all_files = set(file_list) # Convert file_list to set for efficient lookups def match_patterns(files, patterns): Filter files based on the patterns. matched_files = set() for pat in patterns: matched_files.update(fnmatch.filter(files, pat)) return matched_files for command in commands: parts = command.split() cmd_type = parts[0] args = parts[1:] if cmd_type == \\"include\\": included_files.update(match_patterns(file_list, args)) elif cmd_type == \\"exclude\\": included_files -= match_patterns(included_files, args) elif cmd_type == \\"recursive-include\\": dir_path = args[0] patterns = args[1:] for file in file_list: if file.startswith(dir_path + os.sep): included_files.update(match_patterns([file], patterns)) elif cmd_type == \\"recursive-exclude\\": dir_path = args[0] patterns = args[1:] for file in file_list: if file.startswith(dir_path + os.sep): included_files -= match_patterns([file], patterns) elif cmd_type == \\"global-include\\": included_files.update(match_patterns(file_list, args)) elif cmd_type == \\"global-exclude\\": included_files -= match_patterns(file_list, args) elif cmd_type == \\"prune\\": dir_path = args[0] for file in file_list: if file.startswith(dir_path + os.sep): included_files.discard(file) elif cmd_type == \\"graft\\": dir_path = args[0] for file in file_list: if file.startswith(dir_path + os.sep): included_files.add(file) final_files = [file for file in file_list if file in included_files] return final_files"},{"question":"# Sparse Tensor Operations in PyTorch PyTorch provides several formats for handling sparse tensors, offering performance advantages in dealing with data that contains many zeros. In this task, you will demonstrate your understanding of creating, converting, and performing operations with sparse tensors using PyTorch\'s `torch.sparse` module. Problem Statement: **Part 1: Sparse Tensor Creation** 1. Create a dense 2D tensor `A` of size (5, 5) with the following values: ``` [[0, 7, 0, 0, 0], [0, 0, 0, 0, 5], [0, 0, 0, 3, 0], [0, 2, 0, 0, 0], [6, 0, 0, 0, 0]] ``` 2. Convert `A` to a sparse COO tensor `A_coo`. **Part 2: Sparse Tensor Conversion and Operations** 1. Convert `A_coo` to a sparse CSR tensor `A_csr`. 2. Perform a sparse matrix-vector multiplication using `A_csr`. Define a dense vector `v` of size 5 with the values `[1, 0, 2, 0, 1]`. Compute the result `y = A_csr @ v`. **Part 3: Semi-Structured Sparse Tensor** 1. Create a new dense tensor `B` of size (128, 128) with a pattern suitable for 2:4 structured sparsity where every row contains exactly `2` non-zero values spaced by `3` zero values. 2. Convert `B` to a semi-structured sparse tensor `B_sparse`. 3. Multiply `B_sparse` by a dense random tensor `C` of compatible size, and ensure the result matches when performing the same multiplication using the dense representation of `B`. # Function Signature (Python): ```python import torch def sparse_tensor_operations(): # Part 1: Sparse Tensor Creation A = torch.tensor([ [0, 7, 0, 0, 0], [0, 0, 0, 0, 5], [0, 0, 0, 3, 0], [0, 2, 0, 0, 0], [6, 0, 0, 0, 0] ], dtype=torch.float32) # Convert A to sparse COO tensor A_coo = A.to_sparse() # Part 2: Sparse Tensor Conversion and Operations # Convert A_coo to sparse CSR tensor A_csr = A_coo.to_sparse_csr() # Define dense vector v v = torch.tensor([1, 0, 2, 0, 1], dtype=torch.float32) # Sparse matrix-vector multiplication y = torch.matmul(A_csr, v) # Part 3: Semi-Structured Sparse Tensor # Create dense tensor B with 2:4 structured sparsity row_pattern = torch.tensor([1, 0, 0, 0, 1], dtype=torch.float32).repeat((1, 25)) # 1 row with 2:4 pattern B = row_pattern.tile((128, 1)) # Convert B to semi-structured sparse tensor B_sparse = torch.sparse.to_sparse_semi_structured(B.cuda()) # Define a compatible dense tensor C C = torch.rand(128, 128).half().cuda() # Ensure the result of multiplication matches B_dense = B.cuda().half() result_dense = torch.mm(B_dense, C) result_sparse = torch.mm(B_sparse, C) assert torch.allclose(result_dense, result_sparse), \\"Sparse and dense matrix multiplication results do not match\\" return A_coo, A_csr, y, B, B_sparse, result_dense, result_sparse # Example of expected output # A_coo: Sparse COO tensor # A_csr: Sparse CSR tensor # y: Result of matrix-vector multiplication # B: Dense tensor with the 2:4 structured sparsity pattern # B_sparse: Semi-structured sparse tensor # result_dense: Dense matrix multiplication result # result_sparse: Sparse matrix multiplication result ``` **Note**: Ensure you use a CUDA-capable device for the semi-structured sparse tensor operations.","solution":"import torch def sparse_tensor_operations(): # Part 1: Sparse Tensor Creation A = torch.tensor([ [0, 7, 0, 0, 0], [0, 0, 0, 0, 5], [0, 0, 0, 3, 0], [0, 2, 0, 0, 0], [6, 0, 0, 0, 0] ], dtype=torch.float32) # Convert A to sparse COO tensor A_coo = A.to_sparse() # Part 2: Sparse Tensor Conversion and Operations # Convert A_coo to sparse CSR tensor A_csr = A_coo.to_sparse_csr() # Define dense vector v v = torch.tensor([1, 0, 2, 0, 1], dtype=torch.float32) # Sparse matrix-vector multiplication y = torch.matmul(A_csr, v) # Part 3: Semi-Structured Sparse Tensor # Create dense tensor B with 2:4 structured sparsity B = torch.zeros(128, 128, dtype=torch.float32) for i in range(128): for j in range(0, 128, 4): B[i][j] = 1 B[i][j+1] = 1 # Convert B to a sparse COO tensor (PyTorch does not have native support for semi-structured sparsity) # Here, we manually perform the conversion for testing B_indices = torch.nonzero(B).t() B_values = B[B_indices[0], B_indices[1]] B_sparse = torch.sparse_coo_tensor(B_indices, B_values, B.size(), dtype=torch.float32) # Define a compatible dense tensor C C = torch.rand(128, 128, dtype=torch.float32) # Ensure the result of multiplication matches result_dense = torch.mm(B, C) result_sparse = torch.sparse.mm(B_sparse, C) assert torch.allclose(result_dense, result_sparse), \\"Sparse and dense matrix multiplication results do not match\\" return A_coo, A_csr, y, B, B_sparse, result_dense, result_sparse"},{"question":"# Python Coding Challenge: Implement and Test a Fibonacci Sequence Generator Problem Description You are tasked with implementing a function to generate the first `n` Fibonacci numbers. Additionally, you are required to write unit tests to ensure the correctness of your Fibonacci sequence generator. Function Signature ```python def fibonacci(n: int) -> list: Generate the first n Fibonacci numbers. :param n: An integer, the number of Fibonacci numbers to generate. :return: A list containing the first n Fibonacci numbers. ``` Constraints 1. `n` will be an integer such that `0 <= n <= 30`. 2. The Fibonacci sequence is defined as: - `F(0) = 0` - `F(1) = 1` - `F(n) = F(n-1) + F(n-2)` for `n > 1` Example ```python >>> fibonacci(0) [] >>> fibonacci(1) [0] >>> fibonacci(5) [0, 1, 1, 2, 3] ``` Requirements 1. Implement the `fibonacci` function as specified. 2. Write a series of unit tests using the `unittest` module to verify the correctness of the `fibonacci` function. Ensure you test: - Base cases such as when `n` is 0 or 1. - The first few Fibonacci numbers. - Edge cases for invalid inputs or boundary conditions. Boilerplate for Unit Tests Use the following template to create your unit tests: ```python import unittest class TestFibonacci(unittest.TestCase): def test_base_cases(self): # Test when n is 0 self.assertEqual(fibonacci(0), []) # Test when n is 1 self.assertEqual(fibonacci(1), [0]) def test_first_few_numbers(self): # Test the first 5 Fibonacci numbers self.assertEqual(fibonacci(5), [0, 1, 1, 2, 3]) # Test the first 10 Fibonacci numbers self.assertEqual(fibonacci(10), [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]) def test_edge_cases(self): # Add more edge cases as necessary pass if __name__ == \'__main__\': unittest.main() ``` Ensure that your implementation passes all the tests.","solution":"def fibonacci(n: int) -> list: Generate the first n Fibonacci numbers. :param n: An integer, the number of Fibonacci numbers to generate. :return: A list containing the first n Fibonacci numbers. if n == 0: return [] if n == 1: return [0] fib_seq = [0, 1] for i in range(2, n): new_fib = fib_seq[-1] + fib_seq[-2] fib_seq.append(new_fib) return fib_seq"},{"question":"# CSV File Processing and Analysis Objective: Design and implement a script that processes a given CSV file to: 1. Read data from the CSV file and output it in a structured format. 2. Write a subset of this data to a new CSV file with specific formatting. 3. Utilize custom dialects for reading and writing. Description: You are provided with a CSV file named `students.csv`, which contains information about students in the following format: ``` ID,Name,Age,Grade,Email 101,John Doe,15,A,johndoe@example.com 102,Jane Smith,14,B,janesmith@example.com 103,Emily Davis,16,A,emilydavis@example.com ... ``` Your task is to write a Python script that: 1. Reads the entire contents of `students.csv` and prints each row to the console. 2. Filters out students who have a grade of \'A\' and writes their information to a new CSV file named `top_students.csv`. The new CSV file should use a custom dialect where fields are separated by semicolons (`;`) and fields containing spaces are quoted. 3. Reads the `top_students.csv` file back using the custom dialect and prints its contents. Requirements: 1. Your script should use the `csv` module\'s `reader`, `writer`, `DictReader`, and `DictWriter` functionalities. 2. Implement a custom dialect named `semicolon_dialect` where: - Fields are delimited by semicolons (`;`). - Fields containing spaces are quoted. - Double quotes are used to quote fields. Input: - `students.csv` (CSV file containing student data) Output: - Console output of the contents of `students.csv`. - `top_students.csv` with filtered and formatted data. - Console output of the contents of `top_students.csv`. Example: If the `students.csv` file contents are: ``` ID,Name,Age,Grade,Email 101,John Doe,15,A,johndoe@example.com 102,Jane Smith,14,B,janesmith@example.com 103,Emily Davis,16,A,emilydavis@example.com 104,Michael Brown,15,C,michaelbrown@example.com ``` The expected console output should be: ``` Contents of students.csv: [\'101\', \'John Doe\', \'15\', \'A\', \'johndoe@example.com\'] [\'102\', \'Jane Smith\', \'14\', \'B\', \'janesmith@example.com\'] [\'103\', \'Emily Davis\', \'16\', \'A\', \'emilydavis@example.com\'] [\'104\', \'Michael Brown\', \'15\', \'C\', \'michaelbrown@example.com\'] Contents of top_students.csv: [\'ID\', \'Name\', \'Age\', \'Grade\', \'Email\'] [\'101\', \'John Doe\', \'15\', \'A\', \'johndoe@example.com\'] [\'103\', \'Emily Davis\', \'16\', \'A\', \'emilydavis@example.com\'] ``` Constraints: - Assume the input `students.csv` file always contains valid data with the headers as shown. Implementation: Implement the following Python script: ```python import csv # Register custom dialect csv.register_dialect(\'semicolon_dialect\', delimiter=\';\', quoting=csv.QUOTE_MINIMAL, doublequote=True, quotechar=\'\\"\') def read_students(file_path): with open(file_path, newline=\'\') as csvfile: reader = csv.reader(csvfile) print(\\"Contents of students.csv:\\") for row in reader: print(row) def write_top_students(input_file, output_file): with open(input_file, newline=\'\') as csvfile: reader = csv.DictReader(csvfile) top_students = [row for row in reader if row[\'Grade\'] == \'A\'] with open(output_file, \'w\', newline=\'\') as csvfile: fieldnames = [\'ID\', \'Name\', \'Age\', \'Grade\', \'Email\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=\'semicolon_dialect\') writer.writeheader() for student in top_students: writer.writerow(student) def read_top_students(file_path): with open(file_path, newline=\'\') as csvfile: reader = csv.reader(csvfile, dialect=\'semicolon_dialect\') print(\\"nContents of top_students.csv:\\") for row in reader: print(row) # Main script input_file = \'students.csv\' output_file = \'top_students.csv\' read_students(input_file) write_top_students(input_file, output_file) read_top_students(output_file) ```","solution":"import csv # Register custom dialect csv.register_dialect(\'semicolon_dialect\', delimiter=\';\', quoting=csv.QUOTE_MINIMAL, doublequote=True, quotechar=\'\\"\') def read_students(file_path): with open(file_path, newline=\'\') as csvfile: reader = csv.reader(csvfile) students = [] print(\\"Contents of students.csv:\\") for row in reader: print(row) students.append(row) return students def write_top_students(input_file, output_file): with open(input_file, newline=\'\') as csvfile: reader = csv.DictReader(csvfile) top_students = [row for row in reader if row[\'Grade\'] == \'A\'] with open(output_file, \'w\', newline=\'\') as csvfile: fieldnames = [\'ID\', \'Name\', \'Age\', \'Grade\', \'Email\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=\'semicolon_dialect\') writer.writeheader() for student in top_students: writer.writerow(student) return top_students def read_top_students(file_path): with open(file_path, newline=\'\') as csvfile: reader = csv.reader(csvfile, dialect=\'semicolon_dialect\') top_students = [] print(\\"nContents of top_students.csv:\\") for row in reader: print(row) top_students.append(row) return top_students # Main script input_file = \'students.csv\' output_file = \'top_students.csv\' if __name__ == \\"__main__\\": read_students(input_file) write_top_students(input_file, output_file) read_top_students(output_file)"},{"question":"# POP3 Mailbox Interaction Script In this task, you are required to write a Python script using the `poplib` module that interacts with a POP3 mailbox securely and retrieves email messages. The script should perform the following: 1. Connect to a specified POP3 server using SSL. 2. Authenticate using a given username and password. 3. List all the email messages currently in the mailbox. 4. Retrieve and print the full content of each email message. 5. Handle any potential exceptions that may occur gracefully. Your script should include detailed comments explaining each step and logic used. Requirements: 1. **Inputs**: - `host` (str): The hostname of the POP3 server. - `port` (int, optional): The port of the POP3 server (default 995). - `username` (str): The username to log in. - `password` (str): The password to log in. 2. **Outputs**: - Print the content of each email message retrieved. 3. **Performance**: The solution should be efficient and use best practices for network programming in Python. Example Usage: ```python pop3_mailbox_interaction( host=\\"pop3.example.com\\", username=\\"user@example.com\\", password=\\"password\\" ) ``` Hint: To establish a secure connection, use the `POP3_SSL` class of the `poplib` module. Constraints: - The module `poplib` should be available. - The number of messages in the mailbox will not exceed 1000. Code: ```python import poplib from getpass import getpass def pop3_mailbox_interaction(host, username, password, port=995): try: # Step 1: Connect to the POP3 server using SSL server = poplib.POP3_SSL(host, port) # Step 2: Authenticate using the provided username and password server.user(username) server.pass_(password) # Step 3: List all email messages in the mailbox numMessages = len(server.list()[1]) print(f\\"Number of messages in the mailbox: {numMessages}\\") # Step 4: Retrieve and print the full content of each email message for i in range(numMessages): print(f\\"nMessage {i + 1}:\\") for line in server.retr(i + 1)[1]: print(line.decode(\'utf-8\')) # Step 5: Gracefully quit the server connection server.quit() except poplib.error_proto as e: print(f\\"POP3 protocol error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example call to the function with sample inputs if __name__ == \\"__main__\\": host = input(\\"Enter the POP3 server host: \\") username = input(\\"Enter the username: \\") password = getpass(\\"Enter the password: \\") pop3_mailbox_interaction(host, username, password) ``` Ensure you handle any POP3 errors and other exceptions gracefully, providing meaningful messages to help understand any failures or issues that occur during execution.","solution":"import poplib from getpass import getpass def pop3_mailbox_interaction(host, username, password, port=995): Connects to a POP3 server using SSL, authenticates, lists, and retrieves email messages. Parameters: host (str): The hostname of the POP3 server. username (str): The username to log in. password (str): The password to log in. port (int, optional): The port of the POP3 server, default is 995. Outputs: Prints the content of each email message retrieved. try: # Step 1: Connect to the POP3 server using SSL server = poplib.POP3_SSL(host, port) # Step 2: Authenticate using the provided username and password server.user(username) server.pass_(password) # Step 3: List all email messages in the mailbox numMessages = len(server.list()[1]) print(f\\"Number of messages in the mailbox: {numMessages}\\") # Step 4: Retrieve and print the full content of each email message for i in range(numMessages): print(f\\"nMessage {i + 1}:\\") for line in server.retr(i + 1)[1]: print(line.decode(\'utf-8\')) # Step 5: Gracefully quit the server connection server.quit() except poplib.error_proto as e: print(f\\"POP3 protocol error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example call to the function with sample inputs if __name__ == \\"__main__\\": host = input(\\"Enter the POP3 server host: \\") username = input(\\"Enter the username: \\") password = getpass(\\"Enter the password: \\") pop3_mailbox_interaction(host, username, password)"},{"question":"# Problem: File System Statistics Analyzer You are tasked with implementing a function to analyze the contents of a specified directory and provide detailed statistics about the types of files within it. You will utilize the `stat` module to determine the types of files and compute the statistics. The task is to implement the function: `analyze_directory(path: str) -> Dict[str, int]` **Function Description:** Your function `analyze_directory` should take a single argument, `path`, which is a string representing the path to a directory. It should return a dictionary with the following keys: - `\\"total_files\\"`: The total number of files in the directory and all its subdirectories. - `\\"total_directories\\"`: The total number of directories in the directory and all its subdirectories. - `\\"regular_files\\"`: The total number of regular files. - `\\"symbolic_links\\"`: The total number of symbolic links. - `\\"sockets\\"`: The total number of sockets. - `\\"character_devices\\"`: The total number of character special device files. - `\\"block_devices\\"`: The total number of block special device files. - `\\"fifos\\"`: The total number of FIFO (named pipe) files. For the purposes of this problem, you should count all files recursively, i.e., if the provided path contains subdirectories, you should explore those subdirectories as well. **Constraints:** - You may assume that the provided path will always be a valid directory. - You must use the functions provided in the `stat` module for type determination. - Your implementation should be efficient in terms of both time and space complexity. **Input Format:** - A single string, `path`, representing the directory path. **Output Format:** - A dictionary with the structure mentioned above. **Example:** ```python # Given directory structure: # /example_directory #  file1.txt (regular file) #  file2.txt (regular file) #  subdir1 (directory) #   file3.txt (regular file) #   file4.txt (regular file) #   subsubdir1 (directory) #   file5.txt (regular file) #  subdir2 (directory) #  file6.txt (regular file) result = analyze_directory(\'/example_directory\') print(result) # Output { \\"total_files\\": 6, \\"total_directories\\": 3, \\"regular_files\\": 6, \\"symbolic_links\\": 0, \\"sockets\\": 0, \\"character_devices\\": 0, \\"block_devices\\": 0, \\"fifos\\": 0 } ``` **Notes:** - Ensure that your function handles empty directories and nested directories. - The order of the keys in the output dictionary does not matter. **Solution Template:** ```python import os import stat def analyze_directory(path: str) -> Dict[str, int]: result = { \\"total_files\\": 0, \\"total_directories\\": 0, \\"regular_files\\": 0, \\"symbolic_links\\": 0, \\"sockets\\": 0, \\"character_devices\\": 0, \\"block_devices\\": 0, \\"fifos\\": 0 } for root, dirs, files in os.walk(path): for name in files + dirs: full_path = os.path.join(root, name) mode = os.lstat(full_path).st_mode if stat.S_ISDIR(mode): result[\\"total_directories\\"] += 1 else: result[\\"total_files\\"] += 1 if stat.S_ISREG(mode): result[\\"regular_files\\"] += 1 elif stat.S_ISLNK(mode): result[\\"symbolic_links\\"] += 1 elif stat.S_ISSOCK(mode): result[\\"sockets\\"] += 1 elif stat.S_ISCHR(mode): result[\\"character_devices\\"] += 1 elif stat.S_ISBLK(mode): result[\\"block_devices\\"] += 1 elif stat.S_ISFIFO(mode): result[\\"fifos\\"] += 1 return result ```","solution":"import os import stat from typing import Dict def analyze_directory(path: str) -> Dict[str, int]: result = { \\"total_files\\": 0, \\"total_directories\\": 0, \\"regular_files\\": 0, \\"symbolic_links\\": 0, \\"sockets\\": 0, \\"character_devices\\": 0, \\"block_devices\\": 0, \\"fifos\\": 0 } for root, dirs, files in os.walk(path): result[\\"total_directories\\"] += len(dirs) for name in files + dirs: full_path = os.path.join(root, name) mode = os.lstat(full_path).st_mode if not stat.S_ISDIR(mode): result[\\"total_files\\"] += 1 if stat.S_ISREG(mode): result[\\"regular_files\\"] += 1 elif stat.S_ISLNK(mode): result[\\"symbolic_links\\"] += 1 elif stat.S_ISSOCK(mode): result[\\"sockets\\"] += 1 elif stat.S_ISCHR(mode): result[\\"character_devices\\"] += 1 elif stat.S_ISBLK(mode): result[\\"block_devices\\"] += 1 elif stat.S_ISFIFO(mode): result[\\"fifos\\"] += 1 return result"},{"question":"**JIT Compilation Challenge with PyTorch** You are given a PyTorch neural network and asked to demonstrate the benefits of using Just-In-Time (JIT) compilation utilities to optimize this model for inference. You will need to write a function that converts a standard PyTorch model into a JIT-compiled model and then compare the performance (inference time) of the two models. # Requirements: 1. **Function Name**: `optimize_model_with_jit` 2. **Input**: A PyTorch model (instance of `torch.nn.Module`) and sample input tensor for the model. 3. **Output**: Dictionary with keys: - `original_time`: Average inference time of the original model. - `jit_time`: Average inference time of the JIT-compiled model. - `speedup`: The speedup factor achieved by using the JIT-compiled model. # Constraints: - The function should run each model for 100 iterations to measure the average inference time. - Use the scripts and tracing methods provided by `torch.jit`. - Ensure that the model works correctly after JIT compilation (output of the models should match within a reasonable tolerance). # Example: ```python import torch import torch.nn as nn import torch.utils.jit import time class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) def optimize_model_with_jit(model, input_tensor): # Your implementation here. model = SimpleModel() input_tensor = torch.randn(1, 10) result = optimize_model_with_jit(model, input_tensor) print(result) # Output format: {\'original_time\': ..., \'jit_time\': ..., \'speedup\': ...} ``` # Notes: - Make sure to compile and trace the model correctly using `torch.jit.script` and `torch.jit.trace`. - Use Python\'s `time.time()` or `time.perf_counter()` for timing the model\'s performance. - Handle any exceptions that might arise from JIT compilation and ensure the model\'s output is not affected by the optimization. Good luck!","solution":"import torch import torch.nn as nn import torch.jit import time def optimize_model_with_jit(model, input_tensor): # Ensure the model is in evaluation mode model.eval() # Timing the original model with torch.no_grad(): start_time = time.perf_counter() for _ in range(100): output_original = model(input_tensor) end_time = time.perf_counter() original_time = (end_time - start_time) / 100 # JIT compilation of the model using tracing traced_model = torch.jit.trace(model, input_tensor) # Timing the JIT-compiled model with torch.no_grad(): start_time = time.perf_counter() for _ in range(100): output_traced = traced_model(input_tensor) end_time = time.perf_counter() jit_time = (end_time - start_time) / 100 # Ensure the outputs are close between the original and JIT-compiled models assert torch.allclose(output_original, output_traced, atol=1e-6), \\"Outputs do not match!\\" # Calculate the speedup factor speedup = original_time / jit_time return { \'original_time\': original_time, \'jit_time\': jit_time, \'speedup\': speedup }"},{"question":"**Attention Mechanism Implementation and Integration in PyTorch** *Objective:* Implement and integrate a custom attention layer in a PyTorch-based neural network model for a sequence classification task. *Problem Statement:* You are tasked to implement a custom attention mechanism by creating a class `CustomAttention` and incorporate it into a neural network to predict the sentiment of a given sentence (sequence of words). # Part 1: Implementing the Custom Attention Mechanism Define a custom attention layer to adapt the following functionality: ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self, hidden_dim): super(CustomAttention, self).__init__() self.hidden_dim = hidden_dim self.attention_weights = nn.Parameter(torch.randn(hidden_dim), requires_grad=True) def forward(self, encoder_outputs): # encoder_outputs: (batch_size, seq_length, hidden_dim) # attention_weights: (hidden_dim) # Compute attention scores (batch_size, seq_length) attn_scores = torch.tanh(torch.matmul(encoder_outputs, self.attention_weights)) # Compute attention weights (batch_size, seq_length) attn_weights = F.softmax(attn_scores, dim=1) # Compute context vector as weighted sum of encoder outputs (batch_size, hidden_dim) context_vector = torch.sum(encoder_outputs * attn_weights.unsqueeze(2), dim=1) return context_vector, attn_weights ``` # Part 2: Integrating Attention Layer in the Neural Network Using the `CustomAttention` class, create a neural network for sequence classification where: 1. The input sentences are tokenized and converted into embeddings. 2. An RNN processes these embeddings to generate encoder outputs. 3. The attention mechanism computes a context vector. 4. The context vector is used to predict the sentiment class. Define the neural network architecture as follows: ```python class SentimentRNN(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim): super(SentimentRNN, self).__init__() self.embedding = nn.Embedding(vocab_size, embedding_dim) self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True) self.attention = CustomAttention(hidden_dim) self.fc = nn.Linear(hidden_dim, output_dim) def forward(self, x): # x: (batch_size, seq_length) # Embedding lookup (batch_size, seq_length, embedding_dim) embedded = self.embedding(x) # RNN processing (batch_size, seq_length, hidden_dim) rnn_output, _ = self.rnn(embedded) # Attention mechanism context_vector, attn_weights = self.attention(rnn_output) # Final output (batch_size, output_dim) output = self.fc(context_vector) return output, attn_weights # Example usage (assuming vocab_size, embedding_dim, hidden_dim, output_dim defined): # model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, output_dim) # input_seq = torch.randint(0, vocab_size, (batch_size, seq_length)) # output, attn_weights = model(input_seq) ``` # Requirements: 1. Implement the `CustomAttention` class as defined in Part 1. 2. Implement `SentimentRNN` class that uses `CustomAttention`. 3. Ensure your implemented model can handle varying sequence lengths correctly. 4. Train the model on a sample dataset and evaluate its performance. 5. Document your code with comments explaining each step. *Constraints:* - Assume `vocab_size = 5000`, `embedding_dim = 128`, `hidden_dim = 256`, and `output_dim = 2`. - Use typical sequence lengths between 10 and 50 tokens. - Utilize GPU acceleration if available. **Performance Requirements:** - The model should converge within a reasonable number of epochs (specify the count explicitly). - Ensure the computation is efficient and does not exceed typical memory limits on standard GPUs (e.g., 8GB). Submit your solution with a clear, self-contained code along with documentation on how to run it, including any additional requirements or dependencies.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self, hidden_dim): super(CustomAttention, self).__init__() self.hidden_dim = hidden_dim self.attention_weights = nn.Parameter(torch.randn(hidden_dim), requires_grad=True) def forward(self, encoder_outputs): # encoder_outputs: (batch_size, seq_length, hidden_dim) # attention_weights: (hidden_dim) # Compute attention scores (batch_size, seq_length) attn_scores = torch.tanh(torch.matmul(encoder_outputs, self.attention_weights)) # Compute attention weights (batch_size, seq_length) attn_weights = F.softmax(attn_scores, dim=1) # Compute context vector as weighted sum of encoder outputs (batch_size, hidden_dim) context_vector = torch.sum(encoder_outputs * attn_weights.unsqueeze(2), dim=1) return context_vector, attn_weights class SentimentRNN(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim): super(SentimentRNN, self).__init__() self.embedding = nn.Embedding(vocab_size, embedding_dim) self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True) self.attention = CustomAttention(hidden_dim) self.fc = nn.Linear(hidden_dim, output_dim) def forward(self, x): # x: (batch_size, seq_length) # Embedding lookup (batch_size, seq_length, embedding_dim) embedded = self.embedding(x) # RNN processing (batch_size, seq_length, hidden_dim) rnn_output, _ = self.rnn(embedded) # Attention mechanism context_vector, attn_weights = self.attention(rnn_output) # Final output (batch_size, output_dim) output = self.fc(context_vector) return output, attn_weights"},{"question":"# Advanced Python Debugging and Profiling Objective Demonstrate your understanding of Python debugging and profiling tools by writing a function that: 1. Measures the execution time of a given code snippet. 2. Traces memory allocations during the execution. Task You are required to write two functions `measure_execution_time(func, *args)` and `trace_memory_allocations(func, *args)` that: 1. **`measure_execution_time(func, *args)`**: - **Input**: A function object `func` and its arguments `*args`. - **Output**: Returns the execution time (in seconds) of calling `func(*args)`. - Use the `timeit` module to accurately measure the execution time. 2. **`trace_memory_allocations(func, *args)`**: - **Input**: A function object `func` and its arguments `*args`. - **Output**: Returns a detailed memory usage summary during the execution of `func(*args)`. - Use the `tracemalloc` module to trace memory allocations and display a summary including the top 10 memory blocks allocated. Constraints 1. The functions should handle functions that might raise exceptions gracefully. 2. The `trace_memory_allocations` function should ensure that results are human-readable and informative. 3. Ensure that performance measurements are accurate and account for potential fluctuations in execution time. Example Usage ```python import random def sample_function(n): return [random.randint(0, 1000) for _ in range(n)] # Example for measure_execution_time exec_time = measure_execution_time(sample_function, 1000000) print(f\'Execution Time: {exec_time} seconds\') # Example for trace_memory_allocations memory_trace = trace_memory_allocations(sample_function, 1000000) print(f\'Memory Trace: {memory_trace}\') ``` Additional Notes - Include a main block to demonstrate the capabilities of your functions with appropriate test functions. - Please provide necessary comments and documentation within your code to explain your implementation.","solution":"import timeit import tracemalloc def measure_execution_time(func, *args): Measure the execution time of a given function with provided arguments. Parameters: func (callable): The function to be executed. *args: Arguments to be passed to the function. Returns: float: Execution time in seconds. start_time = timeit.default_timer() try: func(*args) except Exception as e: print(f\\"Function raised an exception: {e}\\") end_time = timeit.default_timer() return end_time - start_time def trace_memory_allocations(func, *args): Trace memory allocations of a given function with provided arguments. Parameters: func (callable): The function to be executed. *args: Arguments to be passed to the function. Returns: str: A summary of the top memory allocations. tracemalloc.start() try: func(*args) except Exception as e: print(f\\"Function raised an exception: {e}\\") current, peak = tracemalloc.get_traced_memory() snapshot = tracemalloc.take_snapshot() top_stats = snapshot.statistics(\'lineno\') tracemalloc.stop() result = ( f\\"Current memory usage is {current / 10**6:.1f}MB; \\" f\\"Peak was {peak / 10**6:.1f}MBn\\" \\"Top 10 memory allocations:n\\" ) for stat in top_stats[:10]: result += str(stat) + \\"n\\" return result.strip()"},{"question":"# Seaborn Data Visualization Challenge Objective You are required to create a data visualization using the seaborn library based on the specified criteria. This will assess your understanding of seaborn plotting functions, customization options, and data handling. Task Given a dataset, you will: 1. Load the dataset using pandas. 2. Perform a simple data preprocessing. 3. Generate two types of plots with specific styles and customizations. Dataset For this assignment, you can use any pre-loaded seaborn dataset. Here, we\'ll use the \\"tips\\" dataset. Requirements 1. **Data Preprocessing:** - Load the \\"tips\\" dataset using seaborns `load_dataset` function. - Calculate the total bill per day and the average tip per day. 2. **Plot 1 - Bar Plot:** - Set the seaborn style to \\"whitegrid\\". - Create a bar plot depicting the total bill per day. - Customize the plot by setting the bar color to \\"blue\\". 3. **Plot 2 - Line Plot:** - Set a custom seaborn style with \\"darkgrid\\" background, and grid lines having color \\".8\\" and linestyle \\"--\\". - Create a line plot showing the average tip amount per day. - Customize the plot by changing the line color to \\"green\\" and setting a title \\"Average Tip per Day\\". Input and Output - **Input:** None (the dataset is pre-loaded within seaborn) - **Output:** Display two plots as described in the requirements. Constraints - Use seaborn\'s `set_style` and `lineplot`, `barplot` functions. - Ensure the plots are correctly labeled and styled according to the specifications. Sample Code Structure Below is a sample structure to help you get started: ```python import seaborn as sns import pandas as pd # 1. Load the dataset data = sns.load_dataset(\\"tips\\") # Data Preprocessing # Your code here # Plot 1 - Bar Plot # Your code here # Plot 2 - Line Plot # Your code here ``` Good luck! Your submission should include the complete code that generates the two required plots.","solution":"import seaborn as sns import pandas as pd def load_and_preprocess_data(): # Load the \\"tips\\" dataset data = sns.load_dataset(\\"tips\\") # Calculate total bill per day total_bill_per_day = data.groupby(\'day\')[\'total_bill\'].sum().reset_index() # Calculate average tip per day average_tip_per_day = data.groupby(\'day\')[\'tip\'].mean().reset_index() return total_bill_per_day, average_tip_per_day def create_bar_plot(data): sns.set_style(\\"whitegrid\\") bar_plot = sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=data, color=\\"blue\\") bar_plot.set_title(\\"Total Bill per Day\\") return bar_plot def create_line_plot(data): sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".8\\", \\"grid.linestyle\\": \\"--\\"}) line_plot = sns.lineplot(x=\\"day\\", y=\\"tip\\", data=data, color=\\"green\\") line_plot.set_title(\\"Average Tip per Day\\") return line_plot # Main function to generate and display the plots def main(): total_bill_data, average_tip_data = load_and_preprocess_data() bar_plot = create_bar_plot(total_bill_data) line_plot = create_line_plot(average_tip_data) return bar_plot, line_plot"},{"question":"Objective To assess the understanding of cryptographic security using the Python \\"secrets\\" module and the ability to generate secure random tokens for various applications. Question Write a Python function `generate_secure_token(length: int, format: str) -> str` that generates a secure random token of a specified length and format. The function should use the \\"secrets\\" module to ensure cryptographic security. Function Signature ```python def generate_secure_token(length: int, format: str) -> str: pass ``` Input - `length` (int): The length of the token to be generated. Must be a non-negative integer. - `format` (str): The format of the token, which can be one of the following values: - \'bytes\': Generate a token in byte string format. - \'hex\': Generate a token in hexadecimal format. - \'urlsafe\': Generate a URL-safe token. Output - Returns a string that represents the generated token based on the specified format. Constraints - The `length` parameter must be a non-negative integer. If the `length` is 0, the function should return an empty string. - The `format` parameter must be one of \'bytes\', \'hex\', or \'urlsafe\'. If an invalid format is provided, the function should raise a `ValueError` with an appropriate error message. Examples ```python # Generate a token of length 16 in byte string format print(generate_secure_token(16, \'bytes\')) # Example output: b\'xebrx17D*txaexd4xe3Sxb6xe2xebP1x8b\' # Generate a token of length 16 in hexadecimal format print(generate_secure_token(16, \'hex\')) # Example output: \'f9bf78b9a18ce6d46a0cd2b0b86df9da\' # Generate a token of length 16 in URL-safe format print(generate_secure_token(16, \'urlsafe\')) # Example output: \'Drmhze6EPcv0fN_81Bj-nA\' # Invalid format should raise an error print(generate_secure_token(16, \'invalid\')) # Raises ValueError: \\"Invalid format. Choose from \'bytes\', \'hex\', \'urlsafe\'.\\" ``` Requirements 1. Ensure that the generated token length and format complies with the input parameters. 2. Use the \\"secrets\\" module to achieve cryptographic security for the generated tokens. 3. Handle errors gracefully by validating the input parameters.","solution":"import secrets import binascii def generate_secure_token(length: int, format: str) -> str: Generates a secure random token of a specified length and format. Parameters: - length (int): The length of the token to be generated. Must be a non-negative integer. - format (str): The format of the token, which can be one of the following values: - \'bytes\': Generate a token in byte string format. - \'hex\': Generate a token in hexadecimal format. - \'urlsafe\': Generate a URL-safe token. Returns: - str: The generated token in the specified format. Raises: - ValueError: If an invalid format is provided. if length < 0: raise ValueError(\\"Length must be a non-negative integer\\") if format == \'bytes\': return secrets.token_bytes(length) elif format == \'hex\': return secrets.token_hex(length // 2) elif format == \'urlsafe\': return secrets.token_urlsafe(length)[:length] else: raise ValueError(\\"Invalid format. Choose from \'bytes\', \'hex\', \'urlsafe\'.\\")"},{"question":"You are provided with a partially implemented PyTorch model. Your task is to complete the implementation by adhering to TorchScript standards, including type annotations, usage of TorchScript-specific APIs, and ensuring that the model can be successfully scripted using `torch.jit.script`. # Objective 1. Define a custom TorchScript-compatible class that includes: - Proper type annotations for its instance attributes. - A method to increment a specified value. 2. Implement a simple PyTorch module that makes use of this custom class. 3. Annotate types correctly using TorchScript APIs where necessary. 4. Use `torch.jit.script` to convert the module to a TorchScript module. # Instructions 1. Complete the `CustomClass` definition by adding appropriate type annotations and methods. 2. Complete the `SimpleModule` class to utilize `CustomClass` and implement a forward method. 3. Ensure the module can be scripted using `torch.jit.script`. # Code Template ```python import torch from typing import Optional, List, Any # Custom TorchScript-compatible class class CustomClass: def __init__(self, value: int): self.value: int = value def increment(self, delta: int) -> int: # Include type annotation if necessary return self.value + delta # Simple PyTorch module utilizing the custom class class SimpleModule(torch.nn.Module): def __init__(self, value: int): super(SimpleModule, self).__init__() self.custom_class = CustomClass(value) def forward(self, x: int) -> int: # Utilize CustomClass method return x + self.custom_class.increment(5) # Create an instance of the module model = SimpleModule(10) # Script the module using torch.jit.script scripted_model = torch.jit.script(model) # Test the scripted model print(scripted_model(3)) ``` # Expected Output The scripted model should correctly output the result of the forward pass. For example, if `x = 3` and the `CustomClass` value is initialized to `10` with an increment of `5`, the output should be `3 + 10 + 5 = 18`. Additional requirements: 1. Ensure you use correct type annotations for functions and attributes. 2. Make use of `torch.jit.annotate` or `torch.jit.Attribute` if necessary. 3. The definitions should be TorchScript-compatible and should not raise any errors during scripting. # Constraints 1. Only use functionality supported in TorchScript as per the provided documentation. 2. Handle optional types (`Optional[T]`) and lists (`List[T]`) correctly where applicable. 3. Ensure that the annotations and type hints adhere to the TorchScript standards.","solution":"import torch from typing import Optional, List, Any # Custom TorchScript-compatible class class CustomClass: def __init__(self, value: int): self.value: int = value def increment(self, delta: int) -> int: # Include type annotation if necessary return self.value + delta # Simple PyTorch module utilizing the custom class class SimpleModule(torch.nn.Module): def __init__(self, value: int): super(SimpleModule, self).__init__() self.custom_class = CustomClass(value) def forward(self, x: int) -> int: # Utilize CustomClass method return x + self.custom_class.increment(5) # Create an instance of the module model = SimpleModule(10) # Script the module using torch.jit.script scripted_model = torch.jit.script(model) # Test the scripted model print(scripted_model(3))"},{"question":"Coding Assessment Question # Objective Implement a cross-platform I/O multiplexer that can handle multiple file descriptors and notify which ones are ready for reading, writing, or have errors. Use the `select` module and its various methods for different operating systems. # Question Create a class `IOMultiplexer` with the following specification: Class: `IOMultiplexer` - **Methods:** - `add_fd(fd, events)` - **Parameters:** - `fd` (file descriptor): An integer representing the file descriptor or an object having a `fileno()` method. - `events` (str): A string representing the events to monitor. It can be `\\"read\\"`, `\\"write\\"`, or `\\"exception\\"`. - **Description:** Adds the file descriptor `fd` to the multiplexer for the specified `events`. - `remove_fd(fd)` - **Parameters:** - `fd` (file descriptor): An integer representing the file descriptor or an object having a `fileno()` method. - **Description:** Removes the file descriptor `fd` from the multiplexer. - `poll(timeout)` - **Parameters:** - `timeout` (float or None): Time in seconds to wait for events. If `None`, wait indefinitely. - **Returns:** A list of tuples `(fd, events)` where `fd` is the file descriptor and `events` is a list of strings representing the events that are ready (`\\"read\\"`, `\\"write\\"`, `\\"exception\\"`). - **Description:** Polls for I/O events on the registered file descriptors within the specified `timeout`. Use the optimal method for the operating system that is running your code (`select()`, `poll()`, `epoll()`, `devpoll()`, or `kqueue()`). # Constraints - Your implementation should handle up to 1024 file descriptors. - Must handle cases where the required methods are not available on the running operating system gracefully. - Ensure your implementation is thread-safe. # Example Usage ```python # Initialize the IOMultiplexer mux = IOMultiplexer() # Add file descriptors and events mux.add_fd(fd=sys.stdin.fileno(), events=\\"read\\") mux.add_fd(fd=sys.stdout.fileno(), events=\\"write\\") # Poll for events with a timeout of 5 seconds events = mux.poll(timeout=5.0) for fd, ev in events: if \\"read\\" in ev: print(f\\"File descriptor {fd} is ready for reading.\\") if \\"write\\" in ev: print(f\\"File descriptor {fd} is ready for writing.\\") if \\"exception\\" in ev: print(f\\"File descriptor {fd} has an exception.\\") ``` # Implementation Notes - Consider using `selectors` module as a high-level interface, but ensure to fall back to using `select`, `poll`, `epoll`, `devpoll`, or `kqueue` if the `selectors` module utilization is not feasible. - Ensure appropriate error handling and edge-case management.","solution":"import selectors import threading class IOMultiplexer: def __init__(self): self.selector = selectors.DefaultSelector() self.lock = threading.Lock() def add_fd(self, fd, events): with self.lock: if events == \'read\': self.selector.register(fd, selectors.EVENT_READ) elif events == \'write\': self.selector.register(fd, selectors.EVENT_WRITE) elif events == \'exception\': self.selector.register(fd, selectors.EVENT_READ | selectors.EVENT_WRITE) else: raise ValueError(f\\"Invalid event type: {events}\\") def remove_fd(self, fd): with self.lock: self.selector.unregister(fd) def poll(self, timeout): with self.lock: # Convert timeout to seconds if it\'s not None (null means block indefinitely) timeout = timeout if timeout is None else timeout events = self.selector.select(timeout) result = [] for key, mask in events: ev = [] if mask & selectors.EVENT_READ: ev.append(\'read\') if mask & selectors.EVENT_WRITE: ev.append(\'write\') result.append((key.fd, ev)) return result"},{"question":"# Custom Dictionary with Advanced Features **Objective:** Design and implement a custom dictionary class that extends Python\'s `defaultdict` from the `collections` module. This custom dictionary should support: 1. Counting the number of times each key is accessed (read operations). 2. Counting the number of times a value is updated for each key (write operations). 3. Pretty printing the dictionary for better readability. **Specifications:** 1. Create a class `TrackedDefaultDict` that extends `collections.defaultdict`. 2. Initialize the class with: - A default factory function (same as `defaultdict`). - An optional starting dictionary. 3. Implement the following methods: - `__getitem__(self, key)`: Override to track the number of read operations for each key. - `__setitem__(self, key, value)`: Override to track the number of write operations for each key. - `read_count(self, key)`: Return the number of times a key has been accessed (read). - `write_count(self, key)`: Return the number of times the value for a key has been updated (written). - `pretty_print(self)`: Use `pprint` to print the dictionary in a readable way. 4. Your implementation should ensure that all inherited functionalities of `defaultdict` remain intact. **Input and Output:** - The custom dictionary should handle any hashable key type and value type. - Output for pretty printing should be a well-formatted string. **Constraints:** - Assume that keys will be hashable and values can be any valid Python object. - Performance is a consideration; your implementation should be efficient with respect to both time and space complexity. **Example Usage:** ```python from collections import defaultdict from pprint import pprint class TrackedDefaultDict(defaultdict): def __init__(self, default_factory=None, *args, **kwargs): super().__init__(default_factory, *args, **kwargs) self._read_counts = defaultdict(int) self._write_counts = defaultdict(int) def __getitem__(self, key): self._read_counts[key] += 1 return super().__getitem__(key) def __setitem__(self, key, value): self._write_counts[key] += 1 super().__setitem__(key, value) def read_count(self, key): return self._read_counts[key] def write_count(self, key): return self._write_counts[key] def pretty_print(self): pprint(dict(self)) # Example tdict = TrackedDefaultDict(int) tdict[\'apple\'] = 10 tdict[\'banana\'] = 20 print(tdict[\'apple\']) # Output: 10 tdict[\'apple\'] += 5 print(tdict.read_count(\'apple\')) # Output: 2 print(tdict.write_count(\'apple\')) # Output: 2 tdict.pretty_print() # Outputs the dictionary in a readable way ``` **You are required to implement the class `TrackedDefaultDict` as described.**","solution":"from collections import defaultdict from pprint import pprint class TrackedDefaultDict(defaultdict): def __init__(self, default_factory=None, *args, **kwargs): super().__init__(default_factory, *args, **kwargs) self._read_counts = defaultdict(int) self._write_counts = defaultdict(int) def __getitem__(self, key): self._read_counts[key] += 1 return super().__getitem__(key) def __setitem__(self, key, value): self._write_counts[key] += 1 super().__setitem__(key, value) def read_count(self, key): return self._read_counts[key] def write_count(self, key): return self._write_counts[key] def pretty_print(self): pprint(dict(self))"},{"question":"You are tasked with creating a test suite for a simple `Calculator` class provided below. This will assess your understanding of unit testing using the `unittest` framework in Python. The `Calculator` class provides the following methods: - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the difference when `b` is subtracted from `a`. - `multiply(a, b)`: Returns the product of `a` and `b`. - `divide(a, b)`: Returns the quotient when `a` is divided by `b`. Raises a `ValueError` if `b` is 0. Here is the `Calculator` class implementation: ```python class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero.\\") return a / b ``` # Your Task: 1. Create a new test file using the `unittest` module. 2. Implement test cases for the `Calculator` class. - Each method (`add`, `subtract`, `multiply`, and `divide`) should have at least two test cases. - Include at least one edge case for each method. - Ensure the `divide` method includes a test case to handle division by zero. 3. Use setup (`setUp`) and teardown (`tearDown`) methods for any necessary initialization and cleanup. 4. Include a skipped test case using the appropriate decorator. 5. Include an expected failure test case using the decorator. 6. Use the appropriate assertions to validate results (e.g., `assertEqual`, `assertRaises`, `assertTrue`, etc.). 7. Make sure your tests can be run from the command line and produce a report. # Constraints: - Your tests should be well-organized and cover edge cases. - Use proper assertion methods to ensure clear and descriptive test failures. - Follow Python naming conventions and write clean, readable code. # Example Output: ```python import unittest from my_calculator_module import Calculator class TestCalculator(unittest.TestCase): def setUp(self): self.calc = Calculator() def tearDown(self): del self.calc def test_add(self): self.assertEqual(self.calc.add(2, 3), 5) self.assertEqual(self.calc.add(-1, 1), 0) # Add more test cases for other methods @unittest.skip(\\"demonstrating skipping\\") def test_skip_example(self): self.fail(\\"This test should be skipped.\\") @unittest.expectedFailure def test_expected_failure(self): # Example of an expected failure self.assertEqual(self.calc.divide(5, 0), \\"expected failure\\") # This allows the above tests to run if this file is executed directly if __name__ == \'__main__\': unittest.main() ``` Note: The example provided should serve as a guide. Ensure to complete the other test cases for `subtract`, `multiply`, and `divide` methods, as well as handle special cases and errors appropriately.","solution":"class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero.\\") return a / b"},{"question":"# Question: Implementing and Utilizing a Custom Class with Iterators and Generators **Objective:** Design and implement a Python class called `BookLibrary` to manage a collection of books. This library should support: 1. Adding new books. 2. Removing books. 3. Iterating through books forward and backward. 4. Generating a summary report of the library status. **Class Definition:** - The `BookLibrary` class should have the following attributes: - `name` (string): The name of the library. - `books` (list): A list of books where each book is represented as a dictionary with keys `title`, `author`, and `year`. - The class should have the following methods: - `__init__(self, name)`: Initializes the library with a name and an empty list of books. - `add_book(self, title, author, year)`: Adds a book to the library. - `remove_book(self, title)`: Removes a book by title from the library. - `__iter__(self)`: Returns an iterator to iterate over the books in the order they were added. - `reverse_iter(self)`: Returns an iterator to iterate over the books in reverse order. - `generate_summary(self)`: Generates and returns a summary report indicating the number of books, and lists all books with their titles, authors, and years. **Notes:** - Assume all titles in the library are unique. - You should demonstrate the use of iterators for both forward and reverse iteration. - You should implement the `reverse_iter` method using generators. - Ensure that your solution includes appropriate handling of edge cases (e.g., removing a book that does not exist). **Constraints:** - Implement the solution using only standard Python libraries. **Example Usage:** ```python # Creating the library library = BookLibrary(\\"City Library\\") # Adding books to the library library.add_book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925) library.add_book(\\"1984\\", \\"George Orwell\\", 1949) library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) # Generating summary print(library.generate_summary()) # Iterating through books (forward) for book in library: print(book) # Iterating through books (backward) for book in library.reverse_iter(): print(book) # Removing a book library.remove_book(\\"1984\\") print(library.generate_summary()) ``` **Expected Summary Output:** ```plaintext Library: City Library Number of books: 3 Books: - The Great Gatsby by F. Scott Fitzgerald (1925) - 1984 by George Orwell (1949) - To Kill a Mockingbird by Harper Lee (1960) Library: City Library Number of books: 2 Books: - The Great Gatsby by F. Scott Fitzgerald (1925) - To Kill a Mockingbird by Harper Lee (1960) ``` **Submission:** Please submit your complete implementation of the `BookLibrary` class and the demonstration of its usage as shown in the example. Ensure your code is well-documented and follows Python best practices.","solution":"class BookLibrary: def __init__(self, name): self.name = name self.books = [] def add_book(self, title, author, year): self.books.append({\'title\': title, \'author\': author, \'year\': year}) def remove_book(self, title): self.books = [book for book in self.books if book[\'title\'] != title] def __iter__(self): return iter(self.books) def reverse_iter(self): for book in reversed(self.books): yield book def generate_summary(self): report = f\\"Library: {self.name}nNumber of books: {len(self.books)}nBooks:\\" for book in self.books: report += f\\"n- {book[\'title\']} by {book[\'author\']} ({book[\'year\']})\\" return report"},{"question":"**Task: Understanding PyTorch Serialization** **Objective**: Implement a class that can serialize and deserialize its internal state using PyTorch serialization mechanisms. **Description**: You are required to design a custom PyTorch module that contains two linear layers. Implement methods to save and load its state using PyTorch\'s serialization functions. **Class Definition**: ```python import torch from torch import nn class CustomModule(nn.Module): def __init__(self): super(CustomModule, self).__init__() self.l1 = nn.Linear(10, 20) self.l2 = nn.Linear(20, 10) def forward(self, x): x = torch.relu(self.l1(x)) return self.l2(x) def save_state(self, filepath): Saves the state dict of the module to the specified filepath. Args: filepath (str): The path to save the state dict. # Implement save functionality here def load_state(self, filepath): Loads the state dict of the module from the specified filepath. Args: filepath (str): The path to load the state dict from. # Implement load functionality here ``` **Your Task**: 1. Implement the `save_state` method to save the model\'s state_dict to a given file path. 2. Implement the `load_state` method to load the model\'s state_dict from a given file path. **Input/Output**: - The `save_state` method should take a string `filepath` as input and should not return anything. It should save the module\'s state_dict to the specified file. - The `load_state` method should take a string `filepath` as input and should not return anything. It should load the module\'s state_dict from the specified file. **Constraints**: - You must use PyTorch\'s `torch.save` and `torch.load` functions. - Ensure to handle any potential exceptions that may occur during file operations. **Example Usage**: ```python model = CustomModule() # Dummy input x = torch.randn(1, 10) # Forward pass output = model(x) # Save the model state model.save_state(\'model_state.pth\') # Create a new model instance new_model = CustomModule() # Load the state into the new model new_model.load_state(\'model_state.pth\') # Check if both models give the same output new_output = new_model(x) assert torch.allclose(output, new_output), \\"Model states do not match after loading.\\" ``` **Notes**: - Use appropriate PyTorch functions for serialization. - Pay attention to handling tensor device properly (CPU/GPU) in case of loading states. **Performance Requirements**: - Ensure the serialization methods are efficient and handle large models gracefully without unnecessary memory overhead. - Your implementation should handle models saved across different PyTorch versions, taking into account potential serialization format issues.","solution":"import torch from torch import nn class CustomModule(nn.Module): def __init__(self): super(CustomModule, self).__init__() self.l1 = nn.Linear(10, 20) self.l2 = nn.Linear(20, 10) def forward(self, x): x = torch.relu(self.l1(x)) return self.l2(x) def save_state(self, filepath): Saves the state dict of the module to the specified filepath. Args: filepath (str): The path to save the state dict. try: torch.save(self.state_dict(), filepath) except Exception as e: print(f\\"Error saving state dictionary: {e}\\") def load_state(self, filepath): Loads the state dict of the module from the specified filepath. Args: filepath (str): The path to load the state dict from. try: self.load_state_dict(torch.load(filepath)) except Exception as e: print(f\\"Error loading state dictionary: {e}\\")"},{"question":"**Question**: You are provided with a dataset containing information about different automobile models and their specifications. Your task is to visualize the relationship between the horsepower, weight, and the number of cylinders of these automobiles, using seaborn\'s `stripplot` function. Following this, you should create a multi-faceted plot using `catplot` to further analyze these relationships by the origin of the automobile models. # Dataset The dataset is structured as follows (each row is an automobile model): ``` | mpg | cylinders | displacement | horsepower | weight | acceleration | model year | origin | car name | |-----|-----------|--------------|------------|--------|--------------|------------|--------|----------| | 18 | 8 | 307 | 130 | 3504 | 12 | 70 | 1 | chevrolet chevelle malibu | | 15 | 8 | 350 | 165 | 3693 | 11.5 | 70 | 1 | buick skylark 320 | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ``` # Tasks 1. **Load the dataset**: Load the dataset into a pandas DataFrame. 2. **Basic stripplot**: - Create a strip plot to visualize the distribution of horsepower with respect to the number of cylinders. Do not apply jitter. - Use a different color for each cylinder category. 3. **Customized stripplot**: - Customize the strip plot by mapping the `origin` to different colors using the `hue` parameter. - Set the palette to \\"deep\\". - Enable dodging to separate the data points by origin. 4. **Multi-faceted plot**: - Create a faceted version of the strip plot using `catplot` to visualize the relationship of weight and horsepower, categorized by the number of cylinders and faceted by the origin. - Ensure the plots are oriented horizontally. # Constraints: - The column `origin` contains integer codes: 1 for USA, 2 for Europe, and 3 for Japan. - Ensure the plots are readable and well-labeled for interpretability. # Input - A CSV file named `auto-mpg.csv` containing the automobile dataset. # Output - Display of two plots: 1. Customized strip plot. 2. Faceted `catplot`. # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'auto-mpg.csv\') # Task 2: Basic stripplot plt.figure(figsize=(10, 6)) sns.stripplot(data=df, x=\'horsepower\', y=\'cylinders\', jitter=False) plt.title(\'Horsepower Distribution by Cylinders\') plt.show() # Task 3: Customized stripplot plt.figure(figsize=(10, 6)) sns.stripplot(data=df, x=\'horsepower\', y=\'cylinders\', hue=\'origin\', palette=\'deep\', dodge=True) plt.title(\'Horsepower Distribution by Cylinders with Origin\') plt.show() # Task 4: Multi-faceted plot using catplot g = sns.catplot(data=df, x=\'horsepower\', y=\'weight\', hue=\'cylinders\', col=\'origin\', kind=\'strip\', orient=\'h\', palette=\'deep\', dodge=True) g.fig.suptitle(\'Horsepower vs Weight by Origin and Cylinders\', y=1.02) plt.show() ``` Make sure the entire code runs without errors and produces the requested visualizations.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_relationship(filename): # Load the dataset df = pd.read_csv(filename) # Basic stripplot plt.figure(figsize=(10, 6)) sns.stripplot(data=df, x=\'horsepower\', y=\'cylinders\', jitter=False) plt.title(\'Horsepower Distribution by Cylinders\') plt.show() # Customized stripplot plt.figure(figsize=(10, 6)) sns.stripplot(data=df, x=\'horsepower\', y=\'cylinders\', hue=\'origin\', palette=\'deep\', dodge=True) plt.title(\'Horsepower Distribution by Cylinders with Origin\') plt.show() # Multi-faceted plot using catplot g = sns.catplot(data=df, x=\'horsepower\', y=\'weight\', hue=\'cylinders\', col=\'origin\', kind=\'strip\', orient=\'h\', palette=\'deep\', dodge=True) g.fig.suptitle(\'Horsepower vs Weight by Origin and Cylinders\', y=1.02) plt.show()"},{"question":"You are provided with an introduction to dynamic shapes in PyTorch. Deep learning models often work with static shapes, but there are scenarios where dynamic shapes become necessary. For instance, batch sizes or sequence lengths may vary, and the model\'s output might depend on the actual data input. For this question, you will implement a function that dynamically adjusts to changing input shapes and applies a specific operation based on guard conditions. The function should handle the following: 1. Concatenate two input tensors along a specified dimension. 2. Check if the size of the concatenated dimension exceeds a certain threshold. 3. Based on the check, either multiply or add a constant value to the tensor. # Function Signature ```python import torch def dynamic_shape_operation(tensor1: torch.Tensor, tensor2: torch.Tensor, dim: int, threshold: int, multiply_value: int, add_value: int) -> torch.Tensor: Concatenates two input tensors along a specified dimension. Checks if the size of the concatenated dimension exceeds a given threshold. Based on the check, either multiply or add a constant value to the tensor. Args: tensor1 (torch.Tensor): The first input tensor. tensor2 (torch.Tensor): The second input tensor. dim (int): The dimension along which to concatenate the tensors. threshold (int): The threshold size to trigger different operations. multiply_value (int): The value to multiply with if condition is met. add_value (int): The value to add if condition is not met. Returns: torch.Tensor: The resulting tensor after applying the specified operation. ``` # Constraints 1. You should use dynamic shape handling wherever applicable. 2. Assume `tensor1` and `tensor2` are always valid tensors with compatible shapes for concatenation. 3. The function should be efficient and avoid unnecessary operations. # Example ```python tensor1 = torch.randn(2, 3) tensor2 = torch.randn(3, 3) result = dynamic_shape_operation(tensor1, tensor2, dim=0, threshold=4, multiply_value=2, add_value=3) print(result) ``` # Notes - For dynamic shape handling, you might use the `torch._dynamo.mark_dynamic` function or appropriate guard conditions to handle varying shapes. - Ensure that the function is robust and handles various tensor sizes and dimensions gracefully.","solution":"import torch def dynamic_shape_operation(tensor1: torch.Tensor, tensor2: torch.Tensor, dim: int, threshold: int, multiply_value: int, add_value: int) -> torch.Tensor: Concatenates two input tensors along a specified dimension. Checks if the size of the concatenated dimension exceeds a given threshold. Based on the check, either multiply or add a constant value to the tensor. Args: tensor1 (torch.Tensor): The first input tensor. tensor2 (torch.Tensor): The second input tensor. dim (int): The dimension along which to concatenate the tensors. threshold (int): The threshold size to trigger different operations. multiply_value (int): The value to multiply with if condition is met. add_value (int): The value to add if condition is not met. Returns: torch.Tensor: The resulting tensor after applying the specified operation. concatenated_tensor = torch.cat((tensor1, tensor2), dim=dim) concatenated_dim_size = concatenated_tensor.size(dim) if concatenated_dim_size > threshold: result_tensor = concatenated_tensor * multiply_value else: result_tensor = concatenated_tensor + add_value return result_tensor"},{"question":"Objective Demonstrate the ability to use PyTorch\'s `torch.compile` function, handle errors, and debug the compilation process. Understand how to configure and use logging, and employ tools provided for troubleshooting model execution. Problem Statement You are given a neural network model that, when compiled with PyTorchs `torch.compile` function using a specified backend compiler, raises an error. Your task is to: 1. Compile the model using `torch.compile` and handle any errors that arise during the compilation process. 2. Use appropriate logging and debug tools to identify the source of the error. 3. Implement error handling and use minifiers to isolate the issue and simplify the error reproduction. 4. Ensure the model runs correctly with the provided backend. Function Signature ```python import torch import torch._dynamo as dynamo def debug_and_compile_model(model: torch.nn.Module, backend: str) -> None: pass ``` Input - `model`: An instance of `torch.nn.Module` representing a neural network. - `backend`: A string representing the backend compiler, which can be one of \'eager\', \'aot_eager\', or \'inductor\'. Output - No return value. The function should print details about the errors encountered, steps taken to debug them, and final execution status of the model. Constraints - Use appropriate logging levels to get detailed information about the errors. - Implement usage of at least one minifier tool to simplify the error reproduction. - Ensure that the function is well-documented and the debugging steps are clear. Example ```python import torch import torch.nn as nn import torch._dynamo as dynamo # Example model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): x = self.fc(x) return torch.ops.aten._foobar(x) # Intentional error model = SimpleModel() backend = \\"inductor\\" debug_and_compile_model(model, backend) ``` Expected Output: ``` Error encountered during compilation: ... Using logging and minifiers to identify the issue... Minified reproducible error found: ... ``` Hints: - Use `torch._logging.set_logs(dynamo=logging.INFO)` and other relevant logging settings. - Explore and use `torch._dynamo.config` options for detailed debugging. - Use minifier tools like `TORCHDYNAMO_REPRO_AFTER` and `TORCHDYNAMO_REPRO_LEVEL` for isolating issues.","solution":"import torch import torch.nn as nn import torch._dynamo as dynamo import logging def debug_and_compile_model(model: torch.nn.Module, backend: str) -> None: Compiles the given model using the specified backend compiler and handles any errors that arise during the compilation process. Utilizes logging to debug the compilation process. Parameters: model (torch.nn.Module): The neural network model to compile. backend (str): The backend compiler to use. Can be \'eager\', \'aot_eager\', or \'inductor\'. Returns: None # Set up logging logging.basicConfig(level=logging.INFO) try: # Attempt to compile the model compiled_model = torch.compile(model, backend=backend) # Run a forward pass to trigger any potential errors dummy_input = torch.randn(1, 10) compiled_model(dummy_input) print(\\"Model compiled and executed successfully.\\") except Exception as e: logging.error(f\\"An error occurred during compilation: {e}\\") # Minify the error reproduction try: import torch._dynamo.config torch._dynamo.config.log_level = logging.DEBUG torch._dynamo.config.verbose = True torch._dynamo.config.minifier_repro = \'always\' # Re-run compilation to capture detailed logs and minify the issue compiled_model = torch.compile(model, backend=backend) compiled_model(dummy_input) except Exception as minified_error: logging.error(f\\"Minified reproducible error found: {minified_error}\\") # Example model that should raise an error class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): # Intentional error to trigger compilation failure x = self.fc(x) return torch.ops.aten._foobar(x) model = SimpleModel() backend = \\"inductor\\" # Running the function to test it debug_and_compile_model(model, backend)"},{"question":"You have been provided with a dataset on health expenditure, which includes data on various countries over multiple years. Your task is to create two different visualizations using the seaborn `objects` interface that demonstrates a strong understanding of data normalization and constraints in seaborn. Dataset The dataset `healthexp` contains the following columns: - **Year**: The year of the recorded data. - **Country**: The country name. - **Spending_USD**: The health spending in USD for that year. Requirements 1. **Visualization 1**: Create a line plot that visualizes the health spending over years for each country. Scale each country\'s spending relative to its maximum value, and ensure that the y-axis is labeled as \\"Spending relative to maximum amount\\". 2. **Visualization 2**: Create a line plot that visualizes the percent change in health spending from the year 1970 for each country. Use the year 1970 as the baseline for normalization. Label the y-axis as \\"Percent change in spending from 1970 baseline\\". Input and Output Formats - There are no explicit input prompts. You should work with the provided `healthexp` dataset loaded using `seaborn.load_dataset`. - The resulting output will be two visualizations corresponding to the described requirements. Constraints - You should not use the standard plotting functions from matplotlib directly; instead, you should utilize seaborn\'s `objects` interface. - Ensure the visualizations are clear, and the labeling explicitly describes the normalization applied. Example Code Structure ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Visualization 1: Spending relative to maximum amount plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) # Visualization 2: Percent change in spending from 1970 baseline plot2 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) ``` Use this structure as a starting point, and ensure your visualizations meet the specified requirements.","solution":"import seaborn.objects as so import pandas as pd # Assuming healthexp is provided as a pandas DataFrame # Example DataFrame structure data = { \'Year\': [1970, 1971, 1972, 1970, 1971, 1972], \'Country\': [\'USA\', \'USA\', \'USA\', \'Canada\', \'Canada\', \'Canada\'], \'Spending_USD\': [2000, 2100, 2200, 1800, 1900, 2000] } healthexp = pd.DataFrame(data) # Visualization 1: Spending relative to maximum amount def plot_relative_spending(data): max_spending = data.groupby(\'Country\')[\'Spending_USD\'].transform(\'max\') data[\'Relative_Spending\'] = data[\'Spending_USD\'] / max_spending plot1 = ( so.Plot(data, x=\\"Year\\", y=\\"Relative_Spending\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Spending relative to maximum amount\\") ) return plot1 # Visualization 2: Percent change in spending from 1970 baseline def plot_percent_change(data): baseline = data[data[\'Year\'] == 1970].set_index(\'Country\')[\'Spending_USD\'] data[\'Baseline_Spending\'] = data[\'Country\'].map(baseline) data[\'Percent_Change\'] = ((data[\'Spending_USD\'] - data[\'Baseline_Spending\']) / data[\'Baseline_Spending\']) * 100 plot2 = ( so.Plot(data, x=\\"Year\\", y=\\"Percent_Change\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Percent change in spending from 1970 baseline\\") ) return plot2 # Create the plots plot1 = plot_relative_spending(healthexp) plot2 = plot_percent_change(healthexp)"},{"question":"**Task: Import Modules from ZIP Archives Using Python\'s `zipimport` Module** # Objective Design a function that uses the `zipimport` module to import a Python module from a ZIP archive, checks if the module is a package, retrieves the source code or compiled code object of the module, and handles any errors that may occur. # Requirements Implement a function `import_module_from_zip(archive_path: str, module_name: str) -> dict` that meets the following specifications: 1. **Inputs:** - `archive_path` (str): The path to the ZIP archive or a specific path within the ZIP file. - `module_name` (str): The fully qualified (dotted) name of the module to import. 2. **Output:** - Returns a dictionary with the following structure: ```python { \\"is_package\\": bool, \\"source_code\\": str or None, \\"compiled_code\\": code object or None, \\"error\\": str or None } ``` - `is_package` (bool): Indicates whether the module is a package. - `source_code` (str or None): The source code of the module as a string, or `None` if the archive contains the module but has no source code for it. - `compiled_code` (code object or None): The compiled code object for the module, or `None` if the module couldn\'t be imported or has no compiled code. - `error` (str or None): Contains the error message if any error occurred during the import process, or `None` if no error occurred. 3. **Constraints:** - You may assume the ZIP archive can be found at the specified `archive_path`. - Handle all exceptions raised during the import process and populate the `error` field in the output dictionary with the exception message. # Example Usage ```python archive_path = \\"example.zip\\" module_name = \\"jwzthreading\\" result = import_module_from_zip(archive_path, module_name) print(result) ``` # Notes - Use the `zipimport` module to implement this function. - Ensure that your function handles various errors such as file not found, invalid ZIP archive, and module not found. - The `source_code` and `compiled_code` fields should be mutually exclusive (i.e., if one is `None`, the other should not be `None`, and vice versa). # Performance Requirements - The function should handle ZIP archives containing a large number of files efficiently. - Minimize unnecessary I/O operations by caching module information where possible.","solution":"import zipimport def import_module_from_zip(archive_path: str, module_name: str) -> dict: result = { \\"is_package\\": False, \\"source_code\\": None, \\"compiled_code\\": None, \\"error\\": None } try: importer = zipimport.zipimporter(archive_path) module = importer.load_module(module_name) result[\\"is_package\\"] = hasattr(module, \\"__path__\\") try: source_code = importer.get_source(module_name) result[\\"source_code\\"] = source_code except zipimport.ZipImportError: result[\\"source_code\\"] = None try: code_object = importer.get_code(module_name) result[\\"compiled_code\\"] = code_object except zipimport.ZipImportError: result[\\"compiled_code\\"] = None except Exception as e: result[\\"error\\"] = str(e) return result"},{"question":"Objective: Write a Python class `CustomEncoderDecoder` that provides customized methods for encoding and decoding data using the `base64` module. The class should support base64, urlsafe base64, and base32 encoding/decoding with specific optional features for each. Implement the following methods in the class: 1. `base64_encode(data: bytes, alt_chars: bytes = None) -> bytes` - **Input**: - `data`: A `bytes-like object` to be encoded. - `alt_chars`: An optional `bytes-like object` of length 2 specifying alternative characters to use for `+` and `/`. - **Output**: The base64 encoded `bytes`. - **Constraints**: If `alt_chars` is provided, it must be of length 2. 2. `base64_decode(encoded_data: bytes or str, alt_chars: bytes = None, validate: bool = False) -> bytes` - **Input**: - `encoded_data`: The `bytes-like object` or ASCII string to be decoded. - `alt_chars`: An optional `bytes-like object` of length 2 specifying alternative characters. - `validate`: A boolean flag. If `True`, non-alphabet characters will raise an error; otherwise, they will be ignored. - **Output**: The decoded `bytes`. 3. `urlsafe_base64_encode(data: bytes) -> bytes` - **Input**: - `data`: A `bytes-like object` to be encoded. - **Output**: The URL-safe base64 encoded `bytes`. 4. `urlsafe_base64_decode(encoded_data: bytes or str) -> bytes` - **Input**: - `encoded_data`: The `bytes-like object` or ASCII string to be decoded. - **Output**: The decoded `bytes`. 5. `base32_encode(data: bytes) -> bytes` - **Input**: - `data`: A `bytes-like object` to be encoded. - **Output**: The base32 encoded `bytes`. 6. `base32_decode(encoded_data: bytes or str, casefold: bool = False, map01: Optional[str] = None) -> bytes` - **Input**: - `encoded_data`: The `bytes-like object` or ASCII string to be decoded. - `casefold`: An optional boolean flag that allows lowercase alphabets in the input when `True`. - `map01`: An optional string specifying the letter (\'I\' or \'L\') to which the digit 1 should be mapped. - **Output**: The decoded `bytes`. Example Usage: ```python data = b\\"Hello, World!\\" alt_chars = b\'-_\' encoder_decoder = CustomEncoderDecoder() # Base64 Encoding and Decoding encoded_data = encoder_decoder.base64_encode(data) decoded_data = encoder_decoder.base64_decode(encoded_data) # URL-safe Base64 Encoding and Decoding urlsafe_encoded = encoder_decoder.urlsafe_base64_encode(data) urlsafe_decoded = encoder_decoder.urlsafe_base64_decode(urlsafe_encoded) # Base32 Encoding and Decoding base32_encoded = encoder_decoder.base32_encode(data) base32_decoded = encoder_decoder.base32_decode(base32_encoded, casefold=True) print(encoded_data) print(decoded_data) print(urlsafe_encoded) print(urlsafe_decoded) print(base32_encoded) print(base32_decoded) ``` Note: - Handle all potential errors gracefully and provide useful error messages when necessary. - Use appropriate encoding/decoding functions from the `base64` module. - Ensure the code is well-documented and follows best coding practices.","solution":"import base64 class CustomEncoderDecoder: @staticmethod def base64_encode(data: bytes, alt_chars: bytes = None) -> bytes: if alt_chars and len(alt_chars) != 2: raise ValueError(\\"alt_chars must be a bytes-like object of length 2.\\") if alt_chars: return base64.b64encode(data, altchars=alt_chars) return base64.b64encode(data) @staticmethod def base64_decode(encoded_data: bytes or str, alt_chars: bytes = None, validate: bool = False) -> bytes: if alt_chars and len(alt_chars) != 2: raise ValueError(\\"alt_chars must be a bytes-like object of length 2.\\") if isinstance(encoded_data, str): encoded_data = encoded_data.encode() if alt_chars: return base64.b64decode(encoded_data, altchars=alt_chars, validate=validate) return base64.b64decode(encoded_data, validate=validate) @staticmethod def urlsafe_base64_encode(data: bytes) -> bytes: return base64.urlsafe_b64encode(data) @staticmethod def urlsafe_base64_decode(encoded_data: bytes or str) -> bytes: if isinstance(encoded_data, str): encoded_data = encoded_data.encode() return base64.urlsafe_b64decode(encoded_data) @staticmethod def base32_encode(data: bytes) -> bytes: return base64.b32encode(data) @staticmethod def base32_decode(encoded_data: bytes or str, casefold: bool = False, map01: str = None) -> bytes: if isinstance(encoded_data, str): encoded_data = encoded_data.encode() return base64.b32decode(encoded_data, casefold=casefold, map01=map01)"},{"question":"# Question: Custom Memory Management in Python In this exercise, you will implement custom memory management functions in Python using the built-in capabilities of the Python memory manager. Your task is to: 1. Write the following C functions as Python functions using the ctypes library: - `PyMem_RawMalloc(size_t n)` - `PyMem_RawCalloc(size_t nelem, size_t elsize)` - `PyMem_RawRealloc(void *p, size_t n)` - `PyMem_RawFree(void *p)` 2. Implement a Python class `CustomMemoryManager` that will utilize these memory management functions. This class should have the following methods: - `allocate_raw(size)`: Allocates `size` bytes of raw memory and returns a pointer. - `calloc_raw(num_elements, element_size)`: Allocates an array for `num_elements` each of `element_size` bytes and initializes to zero, returning a pointer. - `reallocate_raw(pointer, new_size)`: Reallocates the memory block pointed by `pointer` to `new_size`. - `free_raw(pointer)`: Frees the memory block pointed by `pointer`. 3. Include error handling to ensure that memory allocation failures are appropriately handled by raising Python exceptions. 4. Write unit tests to demonstrate the usage of `CustomMemoryManager` methods, showing successful allocations, reallocations, and freeing of memory, as well as handling of allocation failures. # Example Usage ```python # Import your implementation from custom_memory_manager import CustomMemoryManager # Create an instance of CustomMemoryManager memory_manager = CustomMemoryManager() # Allocate 100 bytes of memory ptr = memory_manager.allocate_raw(100) # Allocate an array for 50 elements each of size 4 bytes and initialize to zero ptr_calloc = memory_manager.calloc_raw(50, 4) # Reallocate the memory block to 200 bytes ptr_realloc = memory_manager.reallocate_raw(ptr, 200) # Free the memory block memory_manager.free_raw(ptr) memory_manager.free_raw(ptr_calloc) memory_manager.free_raw(ptr_realloc) ``` Constraints and Assumptions: - You may assume that the ctypes library is available and can be used to interact with C functions in Python. - The memory management functions should handle errors gracefully, ensuring any allocation error raises a `MemoryError` in Python. - Your solution should work with any allocations below 1MB for testing purposes. Input and Output Format: - The `CustomMemoryManager` class methods should accept integers as input sizes. - Methods returning pointers should return integer values representing these pointers. - Memory management methods should raise exceptions for invalid operations (e.g., freeing a null pointer). # Performance Requirements - Ensure your memory manager operations are efficient and handle edge cases, such as allocating zero bytes or reallocating to smaller sizes, correctly.","solution":"import ctypes class CustomMemoryManager: def __init__(self): self.libc = ctypes.CDLL(\\"libc.so.6\\") # On non-Linux OS, this might need to be changed. def allocate_raw(self, size): ptr = self.libc.malloc(size) if not ptr: raise MemoryError(f\\"Could not allocate {size} bytes of memory\\") return ptr def calloc_raw(self, num_elements, element_size): ptr = self.libc.calloc(num_elements, element_size) if not ptr: raise MemoryError(f\\"Could not allocate {num_elements * element_size} bytes of memory\\") return ptr def reallocate_raw(self, pointer, new_size): new_ptr = self.libc.realloc(pointer, new_size) if not new_ptr and new_size != 0: raise MemoryError(f\\"Could not reallocate memory to {new_size} bytes\\") return new_ptr def free_raw(self, pointer): if pointer: self.libc.free(pointer) else: raise ValueError(\\"Can\'t free a null pointer\\")"},{"question":"# Advanced Python: Validated Descriptors Objective: Create a Python class using descriptors that validate attribute values based on specified constraints. You will write a descriptor class `ValidatedAttribute` and a class `Product` that uses this descriptor to enforce constraints on its attributes. Descriptor Specifications: 1. **`ValidatedAttribute` Class**: - **Initializer `__init__(self, validation_func, private_name=None)`**: - `validation_func`: A function that takes a single argument and returns `True` if the value is valid, otherwise raises a `ValueError`. - `private_name`: A string that holds the private attribute name. - **Methods**: - `__set_name__(self, owner, name)`: Automatically called to set the name of the private variable. - `__get__(self, instance, owner)`: Retrieves the value of the descriptor from the instance. - `__set__(self, instance, value)`: Validates the value using `validation_func` before setting it. Class Specifications: 1. **`Product` Class**: - **Attributes**: - `name`: A non-empty string. - `price`: A positive float. - `quantity`: A non-negative integer. - **Initializer `__init__(self, name, price, quantity)`**: - Sets the attributes with validation. - **Methods**: - `__str__(self)`: Returns a string representation of the product in the format `\\"Product(name, price, quantity)\\"`. Example Usage: ```python def is_non_empty_string(value): if not isinstance(value, str) or not value: raise ValueError(\\"Expected a non-empty string\\") return True def is_positive_float(value): if not isinstance(value, (float, int)) or value <= 0: raise ValueError(\\"Expected a positive float\\") return True def is_non_negative_int(value): if not isinstance(value, int) or value < 0: raise ValueError(\\"Expected a non-negative integer\\") return True class ValidatedAttribute: def __init__(self, validation_func, private_name=None): self.validation_func = validation_func self.private_name = private_name def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, instance, owner): return getattr(instance, self.private_name) def __set__(self, instance, value): self.validation_func(value) setattr(instance, self.private_name, value) class Product: name = ValidatedAttribute(is_non_empty_string) price = ValidatedAttribute(is_positive_float) quantity = ValidatedAttribute(is_non_negative_int) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity def __str__(self): return f\\"Product({self.name}, {self.price}, {self.quantity})\\" # Example usage try: p = Product(\'Laptop\', 999.99, 10) print(p) p.price = -100 # This should raise a ValueError except ValueError as e: print(e) ``` Guidelines: 1. **Implement `ValidatedAttribute` class following the specifications provided.** 2. **Implement `Product` class following the specifications provided.** 3. **Ensure validation functions are used to check attribute values.** 4. **Demonstrate the usage of the `Product` class including initialization and setting attributes.** Constraints: - **You must use descriptors to manage attribute validation.** - **Validation functions should raise appropriate `ValueError` when validation fails.** - **Attributes must be private and managed only through the descriptor.**","solution":"def is_non_empty_string(value): if not isinstance(value, str) or not value: raise ValueError(\\"Expected a non-empty string\\") return True def is_positive_float(value): if not isinstance(value, (float, int)) or value <= 0: raise ValueError(\\"Expected a positive float\\") return True def is_non_negative_int(value): if not isinstance(value, int) or value < 0: raise ValueError(\\"Expected a non-negative integer\\") return True class ValidatedAttribute: def __init__(self, validation_func, private_name=None): self.validation_func = validation_func self.private_name = private_name def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, instance, owner): return getattr(instance, self.private_name) def __set__(self, instance, value): self.validation_func(value) setattr(instance, self.private_name, value) class Product: name = ValidatedAttribute(is_non_empty_string) price = ValidatedAttribute(is_positive_float) quantity = ValidatedAttribute(is_non_negative_int) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity def __str__(self): return f\\"Product({self.name}, {self.price}, {self.quantity})\\""},{"question":"# Understanding and Manipulating Tensor Storage in PyTorch **Objective**: Implement functions to modify and verify tensor storage in PyTorch. This question assesses your understanding of tensor storages, how to access them, and how to manipulate them effectively while ensuring byte-level data integrity. **Requirements**: 1. **Modify Tensor Storage**: - Create a function `modify_storage` that takes in a tensor `t` and fills its underlying storage with a specified value `val`. - The function should return the modified tensor. 2. **Verify Storage Sharing**: - Create a function `verify_storage_sharing` that takes in two tensors `t1` and `t2` and checks whether they share the same underlying storage. - The function should return a boolean indicating whether the storage is shared. **Constraints and Assumptions**: - You should not directly alter the tensor\'s data using the `.data` attribute. - Utilize the provided `torch.UntypedStorage` methods for operations. - Assume tensor dtype is compatible with the storage\'s dtype. **Function Signatures**: ```python import torch def modify_storage(t: torch.Tensor, val: float) -> torch.Tensor: pass def verify_storage_sharing(t1: torch.Tensor, t2: torch.Tensor) -> bool: pass ``` **Example**: ```python # Example for modify_storage t = torch.ones(3) print(t) # tensor([1., 1., 1.]) modified_t = modify_storage(t, 0.0) print(modified_t) # tensor([0., 0., 0.]) # Example for verify_storage_sharing t1 = torch.tensor([1, 2, 3]) t2 = t1.view(3) # t2 shares the storage with t1 t3 = t1.clone() # t3 does not share the storage with t1 print(verify_storage_sharing(t1, t2)) # True print(verify_storage_sharing(t1, t3)) # False ``` **Performance Requirements**: - Ensure that the solutions are efficient in terms of memory usage. - Avoid unnecessary tensor copying, especially for large tensors. **Scoring**: - Correctness: 60% - Efficiency: 20% - Code readability and comments: 20% **Deadline**: Submit your completed code within 1 hour.","solution":"import torch def modify_storage(t: torch.Tensor, val: float) -> torch.Tensor: Fills the underlying storage of a tensor \'t\' with the specified value \'val\'. storage = t.storage() for i in range(storage.size()): storage[i] = val return t def verify_storage_sharing(t1: torch.Tensor, t2: torch.Tensor) -> bool: Checks if two tensors share the same underlying storage. return t1.storage().data_ptr() == t2.storage().data_ptr()"},{"question":"# Concurrent Execution in Python **Objective:** Implement a program that demonstrates understanding and usage of both threading and multiprocessing to solve a computational and I/O bound task. **Problem Statement:** You are given a list of URLs and a large list of numbers. Write a Python function to: 1. Fetch content from each URL. Assume each URL returns a JSON response with a field \'value\' containing an integer. 2. Perform a computationally expensive task (e.g., calculating factorial) on each number in the list of numbers. The tasks are to be done concurrently: - Use `threading` to fetch content from the URLs. - Use `multiprocessing` to perform the computational tasks on the numbers. Combine the results from these two operations. For simplicity, you can assume the same number of URLs and numbers. Return a dictionary where the keys are the URLs and values are dictionaries containing the fetched \'value\' and computed factorial. **Input:** - `urls`: A list of URLs. - `numbers`: A list of integers. **Output:** - A dictionary where each key is a URL and its value is another dictionary with fetched \'value\' and computed factorial. **Constraints:** - Assume the number of URLs and numbers is the same and not more than 100. - The URL fetching process has a timeout of 5 seconds. - Use proper synchronization mechanisms to ensure thread and process safety. **Example:** ```python urls = [\\"http://example.com/data1\\", \\"http://example.com/data2\\"] numbers = [5, 10] # Example output (assuming the URL responses and computations): { \\"http://example.com/data1\\": {\\"value\\": 123, \\"factorial\\": 120}, \\"http://example.com/data2\\": {\\"value\\": 456, \\"factorial\\": 3628800} } ``` **Function Signature:** ```python def concurrent_tasks(urls: List[str], numbers: List[int]) -> Dict[str, Dict[str, int]]: pass ``` **Guidelines:** 1. Use `threading` for handling the network I/O operations. 2. Use `multiprocessing` for handling the factorial computation. 3. Ensure that the program handles exceptions gracefully, specifically handling timeout for URL fetch operations. 4. Make use of synchronization primitives to safely update shared data across threads and processes. **Note:** - You may use standard libraries like `requests` for fetching URL content and `math` for calculating factorial. - Test your implementation with mock URLs and data to ensure correctness of the concurrent operations.","solution":"import requests import threading import multiprocessing from typing import List, Dict from math import factorial def fetch_url_content(url: str, output: dict): try: response = requests.get(url, timeout=5) response.raise_for_status() data = response.json() if \'value\' in data: output[url] = data[\'value\'] else: output[url] = None except requests.RequestException: output[url] = None def compute_factorial(number: int, output: dict, index: int): output[index] = factorial(number) def concurrent_tasks(urls: List[str], numbers: List[int]) -> Dict[str, Dict[str, int]]: if len(urls) != len(numbers): raise ValueError(\\"The number of URLs must be the same as the number of numbers.\\") url_results = {} factorial_results = multiprocessing.Manager().dict() # Fetch URL content using threading threads = [] for url in urls: thread = threading.Thread(target=fetch_url_content, args=(url, url_results)) threads.append(thread) thread.start() for thread in threads: thread.join() # Compute factorial using multiprocessing processes = [] for index, number in enumerate(numbers): process = multiprocessing.Process(target=compute_factorial, args=(number, factorial_results, index)) processes.append(process) process.start() for process in processes: process.join() result = {} for i, url in enumerate(urls): if url_results[url] is not None: result[url] = { \'value\': url_results[url], \'factorial\': factorial_results[i] } else: result[url] = { \'value\': None, \'factorial\': factorial_results[i] } return result"},{"question":"# CSV Data Processing Challenge You are provided with a CSV file containing data on students\' scores across multiple subjects. Each row in the CSV file represents a student and their scores in different subjects. The first row of the file contains the headers indicating the subjects. Objective Write a Python function `process_csv(file_path: str) -> Dict` which processes the CSV file to determine the following: 1. The average score per subject. 2. The student with the highest total score. 3. The student with the lowest total score. Function Signature: ```python def process_csv(file_path: str) -> dict: pass ``` Input: - `file_path` (str): A string specifying the path to the CSV file. Output: - (dict): A dictionary containing three keys: `\'average_scores\'`, `\'highest_scorer\'`, and `\'lowest_scorer\'`. - `\'average_scores\'` maps to another dictionary where each key is a subject and the value is the average score for that subject. - `\'highest_scorer\'` maps to a dictionary with two keys: `\'name\'` (the name of the student with the highest total score) and `\'total_score\'` (the highest total score). - `\'lowest_scorer\'` similarly maps to a dictionary with the name and the lowest total score. Constraints: - All score values in the CSV are non-negative integers. - There will be at least one student and one subject in the CSV file. Example: Given a CSV file `students.csv` with the following content: ``` Name,Math,Physics,Chemistry Alice,85,90,95 Bob,80,85,87 Charlie,78,80,85 ``` Calling `process_csv(\'students.csv\')` should return: ```python { \'average_scores\': { \'Math\': 81.0, \'Physics\': 85.0, \'Chemistry\': 89.0 }, \'highest_scorer\': { \'name\': \'Alice\', \'total_score\': 270 }, \'lowest_scorer\': { \'name\': \'Charlie\', \'total_score\': 243 } } ``` Notes: - You may use the `csv` module for reading the CSV file. - Consider error handling for situations where files might not be accessible or improperly formatted. Happy Coding!","solution":"import csv from typing import Dict def process_csv(file_path: str) -> Dict: with open(file_path, mode=\'r\') as file: csv_reader = csv.reader(file) headers = next(csv_reader) subjects = headers[1:] total_scores = {subject: 0 for subject in subjects} student_scores = [] for row in csv_reader: name = row[0] scores = list(map(int, row[1:])) total_score = sum(scores) student_scores.append((name, total_score)) for subject, score in zip(subjects, scores): total_scores[subject] += score num_students = len(student_scores) average_scores = {subject: total / num_students for subject, total in total_scores.items()} highest_scorer = max(student_scores, key=lambda x: x[1]) lowest_scorer = min(student_scores, key=lambda x: x[1]) result = { \'average_scores\': average_scores, \'highest_scorer\': { \'name\': highest_scorer[0], \'total_score\': highest_scorer[1] }, \'lowest_scorer\': { \'name\': lowest_scorer[0], \'total_score\': lowest_scorer[1] } } return result"},{"question":"Dynamic Warning Handler Objective Implement a function that dynamically modifies the warning filters based on the type of warning it encounters. Python has several built-in warning categories, and your task is to handle them differently using the `warnings` module. Function to Implement ```python def dynamic_warning_handler(suppress_runtime=False): Dynamically modifies the warning filters and handles different types of warning categories. Parameters: suppress_runtime (bool): If True, all RuntimeWarning warnings should be ignored. If False, RuntimeWarnings should be displayed as an error. Returns: None # Initially, set all DeprecationWarnings to be shown warnings.filterwarnings(\\"default\\", category=DeprecationWarning) # If suppress_runtime is True, suppress RuntimeWarning warnings if suppress_runtime: warnings.filterwarnings(\\"ignore\\", category=RuntimeWarning) else: # If suppress_runtime is False, display RuntimeWarning warnings as errors warnings.filterwarnings(\\"error\\", category=RuntimeWarning) # Other code to work with warning filters goes here # E.g., Function to trigger various warnings for testing. def trigger_warnings(): warnings.warn(\\"This is a user warning.\\", UserWarning) warnings.warn(\\"This is a deprecation warning.\\", DeprecationWarning) warnings.warn(\\"This is a runtime warning.\\", RuntimeWarning) # Trigger warnings trigger_warnings() ``` Requirements - Your function, `dynamic_warning_handler`, should: 1. Modify the warning filters using `warnings.filterwarnings` based on the `suppress_runtime` parameter. 2. Show `DeprecationWarning` warnings by default. 3. Suppress `RuntimeWarning` warnings if `suppress_runtime` is `True`. 4. Raise an exception (convert to error) for `RuntimeWarning` warnings if `suppress_runtime` is `False`. 5. Define an inner function `trigger_warnings()` that triggers different types of warnings (`UserWarning`, `DeprecationWarning`, and `RuntimeWarning`) for testing purposes. Example ```python dynamic_warning_handler(suppress_runtime=True) # Expected: UserWarning and DeprecationWarning should be displayed, # RuntimeWarning should be suppressed. dynamic_warning_handler(suppress_runtime=False) # Expected: UserWarning and DeprecationWarning should be displayed, # RuntimeWarning should be raised as an exception (error). ``` Constraints - Use the Python `warnings` module. - Ensure your function handles the warnings appropriately based on the `suppress_runtime` parameter. - Test your function with various configurations to ensure it behaves as expected. Good luck, and happy coding!","solution":"import warnings def dynamic_warning_handler(suppress_runtime=False): Dynamically modifies the warning filters and handles different types of warning categories. Parameters: suppress_runtime (bool): If True, all RuntimeWarning warnings should be ignored. If False, RuntimeWarnings should be displayed as an error. Returns: None # Initially, set all DeprecationWarnings to be shown warnings.filterwarnings(\\"default\\", category=DeprecationWarning) # If suppress_runtime is True, suppress RuntimeWarning warnings if suppress_runtime: warnings.filterwarnings(\\"ignore\\", category=RuntimeWarning) else: # If suppress_runtime is False, display RuntimeWarning warnings as errors warnings.filterwarnings(\\"error\\", category=RuntimeWarning) # E.g., Function to trigger various warnings for testing. def trigger_warnings(): warnings.warn(\\"This is a user warning.\\", UserWarning) warnings.warn(\\"This is a deprecation warning.\\", DeprecationWarning) warnings.warn(\\"This is a runtime warning.\\", RuntimeWarning) # Trigger warnings trigger_warnings()"},{"question":"Coding Assessment Question: Function Object Manipulation in Python with C API # Objective: The goal of this exercise is to implement a Python function that: 1. Dynamically creates a new Python function object using specified code and globals. 2. Sets default values for the arguments of the function. 3. Optionally sets the annotations for the function. 4. Retrieves and returns the attributes of the created function object. # Requirements: 1. Write a Python function `dynamic_function_creation` that takes in four parameters: - `code_str` (str): The source code of the function as a string. - `globals_dict` (dict): A dictionary for the global variables accessible to the function. - `defaults` (tuple): A tuple of default values for the arguments of the function. - `annotations` (dict, optional): A dictionary of annotations for the function. 2. The function should: - Convert `code_str` to a code object. - Create a new function object using `PyFunction_New`. - Set the default values for the arguments using `PyFunction_SetDefaults`. - If annotations are provided, set them using `PyFunction_SetAnnotations`. - Retrieve and return a dictionary containing: - The code object of the function. - The globals dictionary of the function. - The name of the module. - The default values of the arguments. - The annotations of the function. # Constraints: - `code_str` will be a valid Python function code in string format. - `globals_dict` will be a valid dictionary. - `defaults` will be a valid tuple. - If provided, `annotations` will be a valid dictionary. # Example: ```python def dynamic_function_creation(code_str, globals_dict, defaults, annotations=None): # Your implementation here # Example usage code = \'\'\' def example_function(a, b=2): Example function return a + b \'\'\' globals_dict = {} defaults = (2,) annotations = {\'a\': int, \'b\': int, \'return\': int} result = dynamic_function_creation(code, globals_dict, defaults, annotations) print(result) ``` **Expected output:** ```python { \'code\': <code object example_function at 0x..., file \\"<string>\\", line 2>, \'globals\': {}, \'module\': None, \'defaults\': (2,), \'annotations\': {\'a\': int, \'b\': int, \'return\': int} } ``` # Notes: - You may need to use the `types.CodeType` or `compile` function to convert `code_str` to a code object. - Ensure to handle the conversion of `code_str` to a code object correctly and the creation of the function object using the provided C API functions.","solution":"import types def dynamic_function_creation(code_str, globals_dict, defaults, annotations=None): Create a new function object dynamically, set its defaults and annotations, and return its attributes. Parameters: - code_str (str): The source code of the function as a string. - globals_dict (dict): A dictionary for the global variables accessible to the function. - defaults (tuple): A tuple of default values for the arguments of the function. - annotations (dict, optional): A dictionary of annotations for the function. Returns: - dict: A dictionary containing attributes of the created function object. # Compile the provided code string into a code object code = compile(code_str, \\"<string>\\", \\"exec\\") # Execute the compiled code within the provided globals dictionary exec(code, globals_dict) # Retrieve the first function defined in the global dictionary (based on the function name in the code) func_name = code_str.split(\'(\')[0].split()[-1] func = globals_dict[func_name] # Set default values for the function arguments func.__defaults__ = defaults # If annotations are provided, set them if annotations: func.__annotations__ = annotations # Retrieve and return the function attributes func_attributes = { \'code\': func.__code__, \'globals\': func.__globals__, \'module\': func.__module__, \'defaults\': func.__defaults__, \'annotations\': func.__annotations__, } return func_attributes"},{"question":"# Python Coding Assessment Question Objective: Demonstrate your understanding of the `sysconfig` module in Python by retrieving and manipulating Python\'s configuration and installation information. Problem Statement: Write a Python function `get_python_info()` that returns a dictionary containing the following information about the current Python environment: 1. `python_version`: The \\"MAJOR.MINOR\\" Python version number as a string. 2. `platform`: A string identifying the current platform. 3. `default_scheme`: The default installation scheme name for the current platform. 4. `stdlib_path`: The installation path for standard Python library files (`stdlib`) according to the current platform\'s default scheme. 5. `important_vars`: A dictionary of values for the configuration variables `AR`, `CC`, and `CXX`. # Function Signature: ```python def get_python_info() -> dict: pass ``` # Expected Output: The function should return a dictionary with the following structure: ```python { \\"python_version\\": \\"MAJOR.MINOR\\", \\"platform\\": \\"platform_string\\", \\"default_scheme\\": \\"scheme_name\\", \\"stdlib_path\\": \\"path/to/stdlib\\", \\"important_vars\\": { \\"AR\\": \\"value_of_AR\\", \\"CC\\": \\"value_of_CC\\", \\"CXX\\": \\"value_of_CXX\\" } } ``` # Constraints: - You should handle the case where a configuration variable (`AR`, `CC`, or `CXX`) is not found by storing `None` as its value in the dictionary. # Example: ```python def get_python_info() -> dict: import sysconfig info = {} info[\\"python_version\\"] = sysconfig.get_python_version() info[\\"platform\\"] = sysconfig.get_platform() info[\\"default_scheme\\"] = sysconfig.get_default_scheme() info[\\"stdlib_path\\"] = sysconfig.get_path(\\"stdlib\\") info[\\"important_vars\\"] = { \\"AR\\": sysconfig.get_config_var(\\"AR\\"), \\"CC\\": sysconfig.get_config_var(\\"CC\\"), \\"CXX\\": sysconfig.get_config_var(\\"CXX\\"), } return info print(get_python_info()) ``` In this example, the function `get_python_info` retrieves and compiles information about Python\'s configuration and installation paths. Students are required to understand and apply various `sysconfig` functions to get the necessary details.","solution":"import sysconfig def get_python_info() -> dict: Returns a dictionary containing information about the current Python environment. python_info = { \\"python_version\\": sysconfig.get_python_version(), \\"platform\\": sysconfig.get_platform(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"stdlib_path\\": sysconfig.get_path(\\"stdlib\\"), \\"important_vars\\": { \\"AR\\": sysconfig.get_config_var(\\"AR\\"), \\"CC\\": sysconfig.get_config_var(\\"CC\\"), \\"CXX\\": sysconfig.get_config_var(\\"CXX\\") } } return python_info"},{"question":"# Contextual State Management You are required to create a class that utilizes the `contextvars` module to manage contextual state across different scopes. # Task Implement a `SessionManager` class that contains the following: 1. **Initialization**: - Initialize a `ContextVar` for a session identifier (`session_id`). 2. **Methods**: - `start_session(session_id: str) -> None`: Assign a new session ID to the context variable. - `get_session() -> str`: Retrieve the current session ID from the context variable. - `end_session() -> None`: Reset the session ID to its state before `start_session` was called. 3. Additionally, implement a `run_in_new_context` method: - `run_in_new_context(func: Callable, *args, **kwargs) -> Any`: Runs the provided `func` in a new context copied from the current one and returns the result. # Constraints: - Session IDs are non-empty strings. - Ensure thread-safety in a concurrent environment. - `contextvars` should be used to manage the context state. # Example Usage: ```python from contextvars import ContextVar class SessionManager: def __init__(self): pass # Implementation here def start_session(self, session_id: str) -> None: pass # Implementation here def get_session(self) -> str: pass # Implementation here def end_session(self) -> None: pass # Implementation here def run_in_new_context(self, func: callable, *args, **kwargs) -> any: pass # Implementation here # Example Usage: def print_session(): sm = SessionManager() print(sm.get_session()) sm = SessionManager() sm.start_session(\\"abc123\\") print(sm.get_session()) # --> \\"abc123\\" # Running in a copied context sm.run_in_new_context(print_session) # --> \\"abc123\\" sm.end_session() print(sm.get_session()) # --> Raises LookupError or returns default value if set ``` # Requirements - Use the `contextvars` module properly to implement this class. - Ensure the functions operate correctly within contexts, including nested and asynchronous contexts.","solution":"from contextvars import ContextVar, copy_context class SessionManager: def __init__(self): self._session_id = ContextVar(\'session_id\', default=None) self._token = None def start_session(self, session_id: str) -> None: if not session_id: raise ValueError(\\"Session ID cannot be empty\\") self._token = self._session_id.set(session_id) def get_session(self) -> str: session_id = self._session_id.get() if session_id is None: raise LookupError(\\"No session is active\\") return session_id def end_session(self) -> None: if self._token is None: raise LookupError(\\"No session to end\\") self._session_id.reset(self._token) self._token = None def run_in_new_context(self, func: callable, *args, **kwargs) -> any: ctx = copy_context() return ctx.run(func, *args, **kwargs)"},{"question":"# Seaborn Objects Visualization Assessment Objective: Demonstrate your understanding of seaborn\'s objects interface by plotting a complex visualization using a given dataset, managing the plot\'s properties, and customizing the visual elements. Dataset: We\'ll use the built-in `penguins` dataset from seaborn. Problem Statement: Write a Python function `create_penguin_plot()` that: 1. Loads the `penguins` dataset. 2. Creates a plot using `seaborn.objects` showing the relationship between the `species` and `body_mass_g`. 3. Colors the data points based on the `sex` of the penguins. 4. Adjusts the `alpha` transparency level of the Dash marks to 0.5. 5. Sets the `linewidth` of the Dash marks based on the `flipper_length_mm`. 6. Ensures the plot adapts the `width` of the Dash marks efficiently when dodged. 7. Combines the `Dash` marks with `Dots` to visualize aggregate values. 8. Exports the plot as both a PNG image file `penguin_plot.png` and displays it within the function. Your function should not return anything but should save the plot as a PNG file and display it in the notebook. Function Signature: ```python def create_penguin_plot(): pass ``` Constraints: - Use only seaborn and matplotlib libraries for visualization. - Ensure your code is well-commented to explain each step in creating and customizing the plot. Example Usage: ```python create_penguin_plot() ``` Note: When you call the function, it should load the dataset, create the described visualization, save the plot as `penguin_plot.png`, and display it. Ensure the visual representation is clear and adheres to the provided specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt from seaborn.objects import Plot, Dash, Dots def create_penguin_plot(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Create the plot p = Plot(penguins, x=\'species\', y=\'body_mass_g\', color=\'sex\') # Add Dash marks and Dots p.add(Dash(), alpha=0.5, linewidth=\'flipper_length_mm\', width=lambda x, dodge: 0.5 if dodge else 0.25).add(Dots()) # Display the plot p.show() # Save the plot as a PNG image plt.savefig(\'penguin_plot.png\') plt.show()"},{"question":"Adapting PyTorch Code for TorchScript Objective: To demonstrate your understanding of TorchScript\'s constraints and unsupported constructs, you will write a function that conducts a typical neural network operation in PyTorch and then adapt this function to be compatible with TorchScript. Task: 1. **Identify and Resolve Unsupported Constructs:** Write a function `prepare_and_run_model` in PyTorch that: - Defines a simple neural network using PyTorch\'s `torch.nn` module. - Initializes the network weights with methods such as `torch.nn.init.kaiming_normal_`, `torch.nn.init.eye_`. - Creates inputs and runs a forward pass through the network. 2. **Adapt for TorchScript:** Modify your function to handle the constraints and unsupported constructs of TorchScript. Your modified function should: - Replace or adjust any initialization methods unsupported by TorchScript. - Ensure that all tensor creation functions align with TorchScript\'s requirements. - Compile the adapted model with `torch.jit.script` or `torch.jit.trace`. Function Signature: ```python import torch import torch.nn as nn def prepare_and_run_model(): # Create a simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x model = SimpleNN() # Initialize model weights nn.init.kaiming_normal_(model.fc1.weight) nn.init.eye_(model.fc2.weight) # Create input tensor inputs = torch.randn(5, 10) # Perform a forward pass output = model(inputs) print(\\"Output from PyTorch model:\\", output) # Adapt model for TorchScript # --- Your adapted code here --- # Compile the model with torch.jit.script or torch.jit.trace scripted_model = torch.jit.script(model) # Create input tensor with required dtype, layout, and device inputs_script = torch.randn(5, 10, dtype=torch.float32, layout=torch.strided, device=\'cpu\') # Perform a forward pass with the scripted model output_script = scripted_model(inputs_script) print(\\"Output from TorchScript model:\\", output_script) prepare_and_run_model() ``` Constraints: - All tensors must specify `dtype=float32`, `layout=torch.strided`, and `device=\'cpu\'` when being created. - Use alternatives for unsupported initialization functions in TorchScript. - Ensure the adapted model runs without errors in TorchScript and produces compatible outputs. Inputs: - The primary function `prepare_and_run_model` does not take any inputs. Outputs: - The function must print the output of both the PyTorch model and the TorchScript model to verify that both models produce the expected results.","solution":"import torch import torch.nn as nn def prepare_and_run_model(): # Create a simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) # Initializing weights with methods supported by TorchScript nn.init.xavier_uniform_(self.fc1.weight) # Alternative to kaiming_normal_ self.fc1.bias.data.fill_(0.01) # Example of bias initialization, supported by TorchScript nn.init.xavier_uniform_(self.fc2.weight) # Alternative to eye_ since eye_ is not straightforward for fc layer def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x model = SimpleNN() # Create input tensor inputs = torch.randn(5, 10, dtype=torch.float32, layout=torch.strided, device=\'cpu\') # Perform a forward pass output = model(inputs) print(\\"Output from PyTorch model:\\", output) # Adapt model for TorchScript # Script the model to convert it to TorchScript scripted_model = torch.jit.script(model) # Perform a forward pass with the scripted model output_script = scripted_model(inputs) print(\\"Output from TorchScript model:\\", output_script) # Ensure both outputs are similar in terms of numerical results return output, output_script # Example call to the function which also does the printing prepare_and_run_model()"},{"question":"# AIFF Audio File Manipulation You are given an AIFF audio file named `input.aiff` which contains audio data. Your task is to write a Python function using the `aifc` module to process this audio file and create a new AIFF file `output.aiff` with the following modifications: 1. **Change the Sampling Rate**: Increase the sampling rate by a factor of 1.5. 2. **Reverse the Audio Frames**: The frames in the audio file should be reversed, so the audio plays backward. Your function should perform the following steps: 1. Read the input file `input.aiff`. 2. Extract the audio parameters such as number of channels, sample width, and frame rate. 3. Increase the frame rate by a factor of 1.5. 4. Read and reverse the audio frames. 5. Write the modified audio data to a new AIFF file `output.aiff` with the updated frame rate. Function Signature ```python def modify_aiff(input_filepath: str, output_filepath: str) -> None: pass ``` Input - `input_filepath` (str): The path to the input AIFF file (e.g., \'input.aiff\'). - `output_filepath` (str): The path to save the modified output AIFF file (e.g., \'output.aiff\'). Constraints - Assume the input AIFF file is in the correct format and successfully opened. - The frame rate should be increased by 1.5 times (e.g., if the original frame rate is 44,100 Hz, the new frame rate should be 66,150 Hz). - The audio frames should be read, reversed, and written without data loss. Example Given an input file `input.aiff`, the function should output a modified file `output.aiff` with the required changes. ```python modify_aiff(\'input.aiff\', \'output.aiff\') ``` Note: You can use the `aifc` module methods described in the documentation to complete this task.","solution":"import aifc def modify_aiff(input_filepath: str, output_filepath: str) -> None: # Open the input AIFF file for reading with aifc.open(input_filepath, \'r\') as input_file: # Extract audio parameters num_channels = input_file.getnchannels() sample_width = input_file.getsampwidth() original_frame_rate = input_file.getframerate() num_frames = input_file.getnframes() # Calculate the new frame rate new_frame_rate = int(original_frame_rate * 1.5) # Read audio frames and reverse them audio_frames = input_file.readframes(num_frames) reversed_audio_frames = audio_frames[::-1] # Open the output AIFF file for writing with aifc.open(output_filepath, \'w\') as output_file: # Set the audio parameters for the output file output_file.setnchannels(num_channels) output_file.setsampwidth(sample_width) output_file.setframerate(new_frame_rate) output_file.setnframes(num_frames) # Write the reversed audio frames to the output file output_file.writeframes(reversed_audio_frames)"},{"question":"**Objective**: Demonstrate your understanding of Seaborn\'s bar plot functionalities, including dataset handling, plot customization, and advanced features like faceting. **Question**: Using Seaborn, create visualizations based on the provided dataset. Follow the steps below: 1. Load the \\"titanic\\" dataset from Seaborn. 2. Create a bar plot to show the average fare for each class. 3. Enhance the bar plot by adding different colors for each bar corresponding to the survived column with `hue`. 4. Modify the plot to show error bars representing the standard deviation (SD) of the fares. 5. Create a wide-form representation of the dataset by pivoting it to show classes as columns, with average fares as values. 6. Generate a bar plot using this wide-form dataset. 7. Finally, use `sns.catplot` to create faceted bar plots showing the number of passengers for each class, separated by sex. **Instructions**: 1. Import Seaborn and load necessary datasets. 2. Ensure all plots are clearly titled and labeled. 3. Customize the appearance of the plots (e.g., colors, styles). 4. Provide your code in a single Jupyter Notebook cell. **Constraints**: 1. The dataset used is \\"titanic\\" from Seaborn. 2. Focus on utilizing Seaborn\'s functionalities as highlighted in the document. **Expected Output**: - A series of plots showing the steps outlined above. - Properly labeled axes and titles. ```python # Your code here import seaborn as sns # Load the titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Step 2: Create a basic bar plot to show average fare for each class sns.set_theme(style=\\"whitegrid\\") ax = sns.barplot(x=\\"class\\", y=\\"fare\\", data=titanic) ax.set_title(\\"Average Fare by Class\\") # Step 3: Enhance the bar plot with different colors for each bar using hue=\'survived\' ax = sns.barplot(x=\\"class\\", y=\\"fare\\", hue=\\"survived\\", data=titanic) ax.set_title(\\"Average Fare by Class with Survival Hue\\") # Step 4: Show error bars representing the standard deviation (SD) of the fares ax = sns.barplot(x=\\"class\\", y=\\"fare\\", hue=\\"survived\\", data=titanic, errorbar=\\"sd\\") ax.set_title(\\"Average Fare by Class with Survival Hue and SD Error Bars\\") # Step 5: Create a wide-form representation and plot titanic_pivot = titanic.pivot_table(index=\\"class\\", values=\\"fare\\", aggfunc=\\"mean\\").reset_index() ax = sns.barplot(data=titanic_pivot) ax.set_title(\\"Average Fare by Class in Wide Form\\") # Step 6: Generate a bar plot using wide-form dataset sns.barplot(data=titanic_pivot, x=\\"class\\", y=\\"fare\\") ax.set_title(\\"Average Fare by Class using Wide Form\\") # Step 7: Create faceted bar plots showing the number of passengers for each class, separated by sex sns.catplot(data=titanic, kind=\\"bar\\", x=\\"class\\", y=\\"fare\\", hue=\\"sex\\", col=\\"survived\\") ``` **Note**: Ensure that comments are added to your code to explain your steps and customizations clearly.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the \\"titanic\\" dataset from Seaborn titanic = sns.load_dataset(\\"titanic\\") # Step 2: Create a bar plot to show the average fare for each class plt.figure(figsize=(10, 6)) ax1 = sns.barplot(x=\\"class\\", y=\\"fare\\", data=titanic) ax1.set_title(\\"Average Fare by Class\\") ax1.set_xlabel(\\"Class\\") ax1.set_ylabel(\\"Average Fare\\") plt.show() # Step 3: Enhance the bar plot by adding different colors for each bar corresponding to the survived column with `hue` plt.figure(figsize=(10, 6)) ax2 = sns.barplot(x=\\"class\\", y=\\"fare\\", hue=\\"survived\\", data=titanic) ax2.set_title(\\"Average Fare by Class with Survival Hue\\") ax2.set_xlabel(\\"Class\\") ax2.set_ylabel(\\"Average Fare\\") plt.show() # Step 4: Modify the plot to show error bars representing the standard deviation (SD) of the fares plt.figure(figsize=(10, 6)) ax3 = sns.barplot(x=\\"class\\", y=\\"fare\\", hue=\\"survived\\", data=titanic, ci=\\"sd\\") ax3.set_title(\\"Average Fare by Class with Survival Hue and SD Error Bars\\") ax3.set_xlabel(\\"Class\\") ax3.set_ylabel(\\"Average Fare\\") plt.show() # Step 5: Create a wide-form representation of the dataset by pivoting it to show classes as columns, with average fares as values titanic_pivot = titanic.pivot_table(index=\\"class\\", values=\\"fare\\", aggfunc=\\"mean\\").reset_index() # Step 6: Generate a bar plot using this wide-form dataset plt.figure(figsize=(10, 6)) ax4 = sns.barplot(x=\\"class\\", y=\\"fare\\", data=titanic_pivot) ax4.set_title(\\"Average Fare by Class in Wide Form\\") ax4.set_xlabel(\\"Class\\") ax4.set_ylabel(\\"Average Fare\\") plt.show() # Step 7: Use sns.catplot to create faceted bar plots showing the number of passengers for each class, separated by sex sns.catplot(x=\\"class\\", hue=\\"sex\\", col=\\"survived\\", data=titanic, kind=\\"count\\", height=6, aspect=1) plt.show()"},{"question":"# Seaborn Coding Assessment **Objective:** The goal of this exercise is to create a comprehensive and insightful visualization using Seaborn that leverages its capabilities to show statistical relationships in a multi-dimensional dataset. **Task:** Given the `tips` dataset, create a visualization using `relplot` to display the relationship between the total bill and the tip amount. Your plot should: 1. Include differentiation by smoker status using appropriate markers. 2. Use the time of the day for another visual cue such as hue or style. 3. Represent the size of the party (number of people) by varying point sizes on the scatter plot. 4. Facet the plots by the `day` variable, creating separate plots for each day of the week. 5. Include at least one customization to the color palette for better visual distinction. **Expected Input and Output Format:** - **Input:** None, the function should load the dataset internally. - **Output:** A fully rendered Seaborn `relplot` visualization. **Constraints:** - Use the Seaborn library. - Ensure that the visualization is clear and interpretable with a custom color palette. # Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the relplot with required customizations p = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", size=\\"size\\", col=\\"day\\", style=\\"smoker\\", palette=\\"muted\\", height=5, aspect=1.2, ) # Fine-tuning the aesthetics for ax in p.axes.flatten(): ax.set_title(ax.get_title(), fontsize=12) ax.set_xlabel(\\"Total Bill\\", fontsize=10) ax.set_ylabel(\\"Tip\\", fontsize=10) plt.show() # Call the function to render the plot visualize_tips_data() ``` **Explanation:** - The `relplot` function creates scatter plots with differentiation by `time` and `smoker` status. - `size` semantic represents the party size. - Faceting by `day` creates separate plots for each day. - A custom color palette like `\\"muted\\"` improves visual distinction. Ensure your solution creates a well-labeled and interpretable visualization fulfilling the specified requirements. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): Visualizes the tips dataset using a scatter plot with various customizations. Differentiates data points by smoker status, time of day, party size, and day of the week. # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the relplot with required customizations p = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", size=\\"size\\", col=\\"day\\", style=\\"smoker\\", palette=\\"coolwarm\\", height=5, aspect=1.2, ) # Fine-tuning the aesthetics for ax in p.axes.flatten(): ax.set_title(ax.get_title(), fontsize=12) ax.set_xlabel(\\"Total Bill\\", fontsize=10) ax.set_ylabel(\\"Tip\\", fontsize=10) plt.show() if __name__ == \\"__main__\\": visualize_tips_data()"},{"question":"# Question: Implement a Custom Sequence Class in Python You are tasked with creating a custom sequence class that adheres to the Sequence Protocol as per the Python 3.10 Abstract Objects Layer documentation. Your class should mimic the behavior of a Python list, providing methods to get item count, access elements, slice the sequence, and iterate over it. **Requirements:** 1. **Class Definition**: Define a class named `CustomSequence`. 2. **Initialization**: The class should be initialized with a list of elements. 3. **Length Method**: Implement the `__len__` method to return the number of elements in the sequence. 4. **Item Access**: Implement the `__getitem__` method to allow indexing and slicing. 5. **Iteration**: Implement the `__iter__` method to return an iterator over the sequence. **Input and Output Formats:** - The initializer will take a list of elements: `CustomSequence(elements)`. - The `__len__` method will return an integer representing the number of elements. - The `__getitem__` method should handle both integer indexing and slicing, returning the appropriate elements. - The `__iter__` method should return an iterator that allows iteration over the sequence. **Example:** ```python seq = CustomSequence([1, 2, 3, 4]) print(len(seq)) # Output: 4 print(seq[2]) # Output: 3 print(seq[1:3]) # Output: [2, 3] for item in seq: print(item) # Output: 1, 2, 3, 4 (each on a new line) ``` **Constraints:** 1. You may not use a regular Python list directly inside your class (e.g., avoid using list methods directly on a list object). 2. Handle negative indices and slicing correctly. 3. Ensure the implementation is efficient with appropriate use of class methods. Create a custom sequence class adhering to the above requirements to test your understanding of the Sequence Protocol and advanced object interaction in Python 3.10.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, elements): self._elements = elements def __len__(self): return len(self._elements) def __getitem__(self, index): if isinstance(index, slice): return self._elements[index.start:index.stop:index.step] elif isinstance(index, int): if index < 0: index += len(self._elements) if index < 0 or index >= len(self._elements): raise IndexError(\\"Index out of range\\") return self._elements[index] else: raise TypeError(\\"Index must be an integer or slice\\") def __iter__(self): return iter(self._elements)"},{"question":"**Advanced PyTorch XPU Task: Implementing a Custom XPU-accelerated Computation** In this task, you are required to implement a function that performs matrix multiplication on XPU devices using PyTorch\'s `torch.xpu` module, handles memory management, and ensures proper synchronization using streams and events. # Function: `xpu_matrix_multiplication` **Inputs:** 1. `tensor_a` (torch.Tensor): A 2D tensor containing floats to be multiplied. 2. `tensor_b` (torch.Tensor): A 2D tensor containing floats to be multiplied. 3. `seed` (int): An integer to seed the random number generator to maintain reproducibility. **Outputs:** 1. `result` (torch.Tensor): The resulting 2D tensor after multiplying `tensor_a` and `tensor_b`. 2. `memory_info` (dict): A dictionary containing: - \'memory_allocated\': memory allocated in bytes after performing the operation. - \'memory_reserved\': memory reserved in bytes after performing the operation. **Constraints:** 1. Ensure the matrix dimensions align for multiplication and handle errors appropriately. 2. The computation must leverage XPU devices and appropriate streams for asynchronous execution. 3. The function must reset memory statistics before starting the computation and fetch memory statistics right after the computation. 4. Use the provided seed to initialize the RNG to ensure reproducibility of the results. # Example: ```python import torch from torch.xpu import memory_allocated, memory_reserved, reset_peak_memory_stats, Stream, synchronize def xpu_matrix_multiplication(tensor_a, tensor_b, seed): # Initialize XPU if not torch.xpu.is_available(): raise RuntimeError(\\"XPU is not available on this system\\") device = torch.xpu.device(\'xpu\') # Set seed for reproducibility torch.xpu.manual_seed(seed) # Reset memory stats torch.xpu.reset_peak_memory_stats() stream = torch.xpu.Stream() with torch.xpu.stream(stream): tensor_a = tensor_a.to(device) tensor_b = tensor_b.to(device) # Perform matrix multiplication result = torch.matmul(tensor_a, tensor_b) # Synchronize stream to ensure completion stream.synchronize() # Gather memory information memory_info = { \'memory_allocated\': torch.xpu.memory_allocated(device), \'memory_reserved\': torch.xpu.memory_reserved(device) } # Move result back to CPU result = result.cpu() return result, memory_info # Example usage a = torch.rand((100, 200), dtype=torch.float32) b = torch.rand((200, 150), dtype=torch.float32) result, mem_info = xpu_matrix_multiplication(a, b, seed=42) print(\\"Result shape:\\", result.shape) print(\\"Memory Info:\\", mem_info) ``` # Notes: - Use `torch.xpu` APIs to handle device-specific operations. - Ensure proper handling and synchronization of streams. - Use memory management functions to get the required statistics.","solution":"import torch def xpu_matrix_multiplication(tensor_a, tensor_b, seed): # Initialize XPU if not torch.cuda.is_available(): raise RuntimeError(\\"XPU is not available on this system\\") device = torch.device(\'cuda\') # Set seed for reproducibility torch.manual_seed(seed) # Reset memory stats torch.cuda.reset_peak_memory_stats() stream = torch.cuda.Stream() with torch.cuda.stream(stream): tensor_a = tensor_a.to(device) tensor_b = tensor_b.to(device) # Perform matrix multiplication and handle in case of dimensional mismatch try: result = torch.matmul(tensor_a, tensor_b) except RuntimeError as e: raise ValueError(f\\"Matrix dimension mismatch: {e}\\") # Synchronize stream to ensure completion stream.synchronize() # Gather memory information memory_info = { \'memory_allocated\': torch.cuda.memory_allocated(device), \'memory_reserved\': torch.cuda.memory_reserved(device) } # Move result back to CPU result = result.cpu() return result, memory_info # Example usage a = torch.rand((100, 200), dtype=torch.float32) b = torch.rand((200, 150), dtype=torch.float32) result, mem_info = xpu_matrix_multiplication(a, b, seed=42) print(\\"Result shape:\\", result.shape) print(\\"Memory Info:\\", mem_info)"},{"question":"# **Challenging Coding Assessment Question** # Objective This question is designed to assess your understanding of working with pandas\' MultiIndex (hierarchical index) feature. You are required to create, manipulate, and query a DataFrame using MultiIndex. # Problem Statement You are given the following data about sales in different regions, stores, and products: ```python data = { \'Region\': [\'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'East\', \'East\', \'West\', \'West\'], \'Store\': [\'Store_A\', \'Store_A\', \'Store_B\', \'Store_A\', \'Store_A\', \'Store_B\', \'Store_B\', \'Store_C\', \'Store_C\', \'Store_C\'], \'Product\': [\'Product_1\', \'Product_2\', \'Product_1\', \'Product_1\', \'Product_2\', \'Product_2\', \'Product_1\', \'Product_2\', \'Product_1\', \'Product_2\'], \'Sales\': [200, 150, 100, 220, 140, 130, 180, 160, 170, 155] } ``` # Tasks 1. **Create a MultiIndex DataFrame** - Construct a DataFrame from the given data using a MultiIndex for `Region`, `Store`, and `Product`. 2. **Calculate Total Sales per Region** - Aggregate the total sales for each region and output a Series with the total sales indexed by Region. 3. **Calculate Average Sales per Store** - Compute the average sales for each store within each region. The result should be a DataFrame with MultiIndex (`Region`, `Store`) and a single column for the average sales. 4. **Select Specific Data** - Given a region `South` and store `Store_A`, return all the sales data for these criteria. 5. **Advanced Querying and Slicing** - Using partial indexing, select and display the sales data for all products across all stores in the `North` region. The result should maintain a reduced MultiIndex. # Input and Output Formats - **Input**: The provided data should be used to create the initial DataFrame. - **Output**: 1. **Total Sales**: Series with regions as the index and total sales as values. 2. **Average Sales**: DataFrame indexed by region and store, with average sales as the column. 3. **Specific Data**: DataFrame containing sales data filtered for the specified region and store. 4. **Advanced Slicing Result**: DataFrame of sales for all products in `North` region with a reduced MultiIndex. # Constraints - Ensure efficient indexing and selection by using appropriate `pandas` functionality. - Handle missing data gracefully where applicable. # Implementation You are required to implement the following function: ```python import pandas as pd def multiindex_sales_analysis(data): # Task 1: Create MultiIndex DataFrame df = pd.DataFrame(data) df.set_index([\'Region\', \'Store\', \'Product\'], inplace=True) # Task 2: Calculate Total Sales per Region total_sales_per_region = df.groupby(\'Region\')[\'Sales\'].sum() # Task 3: Calculate Average Sales per Store avg_sales_per_store = df.groupby([\'Region\', \'Store\'])[\'Sales\'].mean().reset_index() avg_sales_per_store.set_index([\'Region\', \'Store\'], inplace=True) # Task 4: Select Specific Data for given Region and Store specific_data = df.loc[(\'South\', \'Store_A\')] # Task 5: Advanced Querying and Slicing for North region north_sales_data = df.loc[\'North\'] return total_sales_per_region, avg_sales_per_store, specific_data, north_sales_data # Example usage: data = { \'Region\': [\'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'East\', \'East\', \'West\', \'West\'], \'Store\': [\'Store_A\', \'Store_A\', \'Store_B\', \'Store_A\', \'Store_A\', \'Store_B\', \'Store_B\', \'Store_C\', \'Store_C\', \'Store_C\'], \'Product\': [\'Product_1\', \'Product_2\', \'Product_1\', \'Product_1\', \'Product_2\', \'Product_2\', \'Product_1\', \'Product_2\', \'Product_1\', \'Product_2\'], \'Sales\': [200, 150, 100, 220, 140, 130, 180, 160, 170, 155] } results = multiindex_sales_analysis(data) ``` # Evaluation Criteria: - **Correctness**: The results must be accurate as per the tasks\' requirements. - **Efficiency**: The implemented solution should leverage pandas\' indexing and selection capabilities for optimal performance. - **Readability**: Code should be clear and well-documented, explaining the steps concisely.","solution":"import pandas as pd def multiindex_sales_analysis(data): # Task 1: Create MultiIndex DataFrame df = pd.DataFrame(data) df.set_index([\'Region\', \'Store\', \'Product\'], inplace=True) # Task 2: Calculate Total Sales per Region total_sales_per_region = df.groupby(\'Region\')[\'Sales\'].sum() # Task 3: Calculate Average Sales per Store avg_sales_per_store = df.groupby([\'Region\', \'Store\'])[\'Sales\'].mean().reset_index() avg_sales_per_store.set_index([\'Region\', \'Store\'], inplace=True) # Task 4: Select Specific Data for given Region and Store specific_data = df.loc[(\'South\', \'Store_A\')] # Task 5: Advanced Querying and Slicing for North region north_sales_data = df.loc[\'North\'] return total_sales_per_region, avg_sales_per_store, specific_data, north_sales_data # Example usage: data = { \'Region\': [\'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'East\', \'East\', \'West\', \'West\'], \'Store\': [\'Store_A\', \'Store_A\', \'Store_B\', \'Store_A\', \'Store_A\', \'Store_B\', \'Store_B\', \'Store_C\', \'Store_C\', \'Store_C\'], \'Product\': [\'Product_1\', \'Product_2\', \'Product_1\', \'Product_1\', \'Product_2\', \'Product_2\', \'Product_1\', \'Product_2\', \'Product_1\', \'Product_2\'], \'Sales\': [200, 150, 100, 220, 140, 130, 180, 160, 170, 155] } results = multiindex_sales_analysis(data)"},{"question":"**Problem Statement:** You are tasked with developing a Python program that reads a binary file, processes the data to extract meaningful information, stores the results in a custom data structure, and performs computations on the data. The program must also use multithreading to enhance performance while ensuring thread safety. **Requirements:** 1. **Read and Parse Binary Data**: - The binary file contains records in the following format: ``` [Record Header (10 bytes)][Data Segment (variable length)] ``` - The Record Header is a 10-byte binary structure containing: - `record_id`: 2-byte unsigned integer - `timestamp`: 4-byte unsigned integer - `data_length`: 4-byte unsigned integer which tells the length of the data segment. - The Data Segment contains `data_length` bytes of data. 2. **Custom Data Structure**: - Store the parsed records in a thread-safe custom data structure. Each record should include: - `record_id` (integer) - `timestamp` (integer) - `data` (bytes) 3. **Multithreading**: - Utilize multithreading to read and parse the binary file concurrently, splitting the work among multiple threads for better performance. - Ensure thread safety when accessing and modifying the custom data structure. 4. **Data Processing and Computation**: - Implement a function that computes the average `data_length` for records with a specific `record_id`. - Implement another function that finds the record with the latest `timestamp`. **Input and Output Formats**: - Input: - A binary file (e.g., `records.bin`). - Output: - Average data length for a specific `record_id`: ``` Average data length for record_id X: Y bytes ``` - Record with the latest timestamp: ``` Latest record: {\'record_id\': X, \'timestamp\': Y, \'data\': Z} ``` **Constraints**: - The binary file should not exceed 100 MB. - The number of threads should be configurable but defaults to 4 if not specified. - Ensure the program handles errors gracefully, such as invalid file format or insufficient records. **Performance Requirement**: - The program should efficiently parse the file and perform computations within a reasonable time frame (e.g., less than a minute for a 100 MB file). **Guidelines**: - Use the `struct` module to handle binary data parsing. - Implement the custom data structure using a thread-safe collection from the `collections` module. - Utilize the `threading` module to achieve concurrency. - Write clear and concise docstrings for all functions and classes. **Example**: ```python import threading import struct from collections import deque # Define your custom data structure and other necessary classes/functions here def read_and_process_binary_file(file_path: str, num_threads: int = 4): # Implement the function to read and parse the binary file using multiple threads pass def compute_average_data_length(record_id: int): # Implement the function to compute the average data length for a specific record_id pass def find_latest_record(): # Implement the function to find the record with the latest timestamp pass # Main code to execute the program if __name__ == \\"__main__\\": # Example usage read_and_process_binary_file(\'records.bin\') print(compute_average_data_length(12345)) print(find_latest_record()) ``` In this problem, you will demonstrate your understanding of binary data handling, custom data structures, multithreading, and thread safety in Python. Ensure all code is well-structured and optimized for performance.","solution":"import threading import struct from collections import deque from typing import List, Dict, Tuple, Union class ThreadSafeRecords: def __init__(self): self.records = deque() self.lock = threading.Lock() def add_record(self, record: Dict[str, Union[int, bytes]]): with self.lock: self.records.append(record) def get_records(self) -> List[Dict[str, Union[int, bytes]]]: with self.lock: return list(self.records) def parse_record(file, offset) -> Tuple[Dict[str, Union[int, bytes]], int]: record_header_format = \'HII\' record_header_size = struct.calcsize(record_header_format) file.seek(offset) record_header = file.read(record_header_size) if not record_header: return None, 0 record_id, timestamp, data_length = struct.unpack(record_header_format, record_header) data = file.read(data_length) record = {\'record_id\': record_id, \'timestamp\': timestamp, \'data\': data} return record, record_header_size + data_length def process_file_segment(file_path: str, start_offset: int, end_offset: int, records: ThreadSafeRecords): with open(file_path, \'rb\') as file: offset = start_offset while offset < end_offset: record, record_size = parse_record(file, offset) if not record: break records.add_record(record) offset += record_size def read_and_process_binary_file(file_path: str, num_threads: int = 4): records = ThreadSafeRecords() file_size = 0 with open(file_path, \'rb\') as file: file.seek(0, 2) # Move to the end of the file file_size = file.tell() segment_size = file_size // num_threads threads = [] for i in range(num_threads): start_offset = i * segment_size end_offset = start_offset + segment_size if i != num_threads - 1 else file_size thread = threading.Thread(target=process_file_segment, args=(file_path, start_offset, end_offset, records)) threads.append(thread) thread.start() for thread in threads: thread.join() return records def compute_average_data_length(records: ThreadSafeRecords, record_id: int) -> float: total_length = 0 count = 0 for record in records.get_records(): if record[\'record_id\'] == record_id: total_length += len(record[\'data\']) count += 1 return total_length / count if count > 0 else 0 def find_latest_record(records: ThreadSafeRecords) -> Union[Dict[str, Union[int, bytes]], None]: latest_record = None for record in records.get_records(): if latest_record is None or record[\'timestamp\'] > latest_record[\'timestamp\']: latest_record = record return latest_record"},{"question":"**Question: Principal Component Analysis (PCA) on a Dataset** You are provided with a dataset `data.csv` containing numerical observations with multiple dimensions. Your task is to implement Principal Component Analysis (PCA) using the `scikit-learn` library to reduce the dimensionality of the data while preserving as much variance as possible. # Input A CSV file named `data.csv` with the following characteristics: - The first line contains the column headers. - Each subsequent line contains numerical values corresponding to the features. # Output A CSV file named `transformed_data.csv` with the transformed data. The output file should contain the following: - The first line should contain the headers: \\"PrincipalComponent1\\", \\"PrincipalComponent2\\", ..., up to the number of components you choose. - Each subsequent line should contain the transformed numerical values for each principal component. # Constraints - You need to determine the number of principal components such that at least 95% of the variance is retained. - The input data might not be centered or scaled; you need to preprocess it accordingly. - You should carefully choose any relevant parameters to ensure the PCA is optimally performed. # Performance requirements - The code should efficiently handle datasets with up to 10,000 rows and 50 columns. - Memory usage should be optimized to allow the processing of large datasets without significant slowdowns. # Example Assume the `data.csv` file contains the following data: ``` feature1,feature2,feature3,feature4 1.0,2.0,3.0,4.0 4.0,5.0,6.0,7.0 7.0,8.0,9.0,10.0 2.0,3.0,4.0,5.0 5.0,6.0,7.0,8.0 ``` Your task is to read this data, fit a PCA model that retains at least 95% of the variance, transform the data, and save the transformed data into `transformed_data.csv`. The output file might look like this: ``` PrincipalComponent1,PrincipalComponent2 -7.81024968,-1.01158795 1.52417192,0.09386222 10.85859352,1.19931239 -5.63607776,-0.07476601 1.06356108,-0.20617935 ``` # Instructions 1. Load the data from `data.csv`. 2. Preprocess the data by centering and scaling. 3. Apply PCA to reduce the dimensions while preserving at least 95% of the variance. 4. Save the transformed data to `transformed_data.csv`. ```python import pandas as pd from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler # Step 1: Load the data data = pd.read_csv(\'data.csv\') # Step 2: Preprocess the data scaler = StandardScaler() scaled_data = scaler.fit_transform(data) # Step 3: Apply PCA pca = PCA(n_components=0.95) # Retain 95% of the variance pca_transformed_data = pca.fit_transform(scaled_data) # Step 4: Save the transformed data principal_components = [\'PrincipalComponent{}\'.format(i + 1) for i in range(pca_transformed_data.shape[1])] transformed_df = pd.DataFrame(data=pca_transformed_data, columns=principal_components) transformed_df.to_csv(\'transformed_data.csv\', index=False) ``` Implement the above steps to complete the task.","solution":"import pandas as pd from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler def perform_pca(input_file, output_file, variance_retained=0.95): Reads the CSV file, performs PCA to retain the given variance, and saves the transformed data into the output CSV file. Parameters: - input_file: str, path to the input CSV file - output_file: str, path to the output CSV file - variance_retained: float, the amount of variance to be retained by PCA (default 0.95) # Step 1: Load the data data = pd.read_csv(input_file) # Step 2: Preprocess the data scaler = StandardScaler() scaled_data = scaler.fit_transform(data) # Step 3: Apply PCA pca = PCA(n_components=variance_retained) pca_transformed_data = pca.fit_transform(scaled_data) # Step 4: Save the transformed data principal_components = [f\'PrincipalComponent{i + 1}\' for i in range(pca_transformed_data.shape[1])] transformed_df = pd.DataFrame(data=pca_transformed_data, columns=principal_components) transformed_df.to_csv(output_file, index=False)"},{"question":"**Objective:** This coding challenge aims to assess your ability to work with Python\'s `warnings` module effectively. You are required to implement a function that manages and handles warnings for a given piece of legacy code. **Task:** You are given a function `legacy_function()` in a module that uses deprecated features and generates warnings. Your task is to: 1. **Suppress warnings** generated by this function when it is called, ensuring that no warnings are displayed to the end user. 2. **Capture these warnings** for logging purposes in a list with detailed information about each warning. 3. **Convert specific warnings** into exceptions dynamically based on a given set of filtering conditions. **Steps:** 1. Implement a function `manage_warnings(warning_conditions: List[Dict[str, str]], logging_list: List[str])` where: - `warning_conditions`: A list of dictionaries, where each dictionary specifies a condition for converting a warning into an exception. Each dictionary will have keys: `action`, `category`, and `message`. - `action` can be \\"error\\". - `category` will be the name of the warning category (e.g., \\"DeprecationWarning\\"). - `message` will be a string that the warning message must contain. - `logging_list`: An empty list that will be populated with dictionaries containing details about each captured warning: `message`, `category`, `filename`, `lineno`. 2. Inside this function, use the `catch_warnings` context manager to capture warnings and populate the `logging_list`. Convert warnings to exceptions according to the `warning_conditions`. 3. After configuring the warnings, call the `legacy_function()` and handle any exceptions raised due to the warnings. **Expected Function Signature:** ```python from typing import List, Dict import warnings def legacy_function(): # Example implementation generating warnings warnings.warn(\\"This is a deprecation warning\\", DeprecationWarning) warnings.warn(\\"This is a user warning\\", UserWarning) def manage_warnings(warning_conditions: List[Dict[str, str]], logging_list: List[Dict[str, str]]): pass ``` **Example:** ```python warning_conditions = [ {\\"action\\": \\"error\\", \\"category\\": \\"DeprecationWarning\\", \\"message\\": \\"deprecation\\"} ] logging_list = [] try: manage_warnings(warning_conditions, logging_list) except Exception as ex: print(f\\"An exception was raised: {ex}\\") print(logging_list) ``` **Expectations:** - The `logging_list` should contain details of each warning generated by `legacy_function()`. - If a warning matches the criteria specified in `warning_conditions`, it should be converted into an exception and appropriately handled. **Constraints:** - Do not modify the `legacy_function()`. - Ensure the solution handles warnings efficiently and does not disrupt normal program flow unless necessary. **Hints:** - Use the `filterwarnings()` function to set up the warning filters as specified in the conditions. - Utilize the `catch_warnings` context manager to capture warnings and log them. - Make sure to restore the original state of the warnings after executing the `legacy_function()`.","solution":"import warnings from typing import List, Dict def legacy_function(): # Example implementation generating warnings warnings.warn(\\"This is a deprecation warning\\", DeprecationWarning) warnings.warn(\\"This is a user warning\\", UserWarning) def manage_warnings(warning_conditions: List[Dict[str, str]], logging_list: List[Dict[str, str]]): with warnings.catch_warnings(record=True) as caught_warnings: warnings.simplefilter(\\"always\\") for condition in warning_conditions: warnings.filterwarnings( action=condition[\'action\'], category=eval(condition[\'category\']), message=condition[\'message\'] ) try: legacy_function() except Warning as w: logging_list.append({ \'message\': str(w), \'category\': type(w).__name__, \'filename\': getattr(w, \'filename\', \'\'), \'lineno\': getattr(w, \'lineno\', \'\') }) raise for warn in caught_warnings: logging_list.append({ \'message\': str(warn.message), \'category\': warn.category.__name__, \'filename\': warn.filename, \'lineno\': warn.lineno })"},{"question":"You are required to implement a function that processes a template string with multiple variables, applying specific formatting rules as defined by the inputs. The function should leverage the `string` modules custom string formatting capabilities to substitute values into the template string. # Function Signature ```python def custom_format(template: str, values: dict) -> str: pass ``` # Input - `template`: A `str` that contains the format template. The template can include replacement fields using curly braces `{}`. These replacement fields can specify variable names, format specifications, and conversion flags. - `values`: A `dict` where keys are variable names used in the template, and values are their corresponding values. # Output - Returns a `str` which is the result of substituting the values into the template according to the specified formatting rules. # Constraints - You should handle typical `str.format()` options in the template string. - The replacement fields can include variable names as well as nested replacement fields. - You must also handle escaping of braces (`{{` and `}}`) within the template. # Examples ```python print(custom_format(\\"Hello, {name}!\\", {\\"name\\": \\"Alice\\"})) # Output: \\"Hello, Alice!\\" print(custom_format(\\"The answer is {value:.2f}\\", {\\"value\\": 42.12345})) # Output: \\"The answer is 42.12\\" print(custom_format(\\"Coordinates: {latitude}, {longitude}\\", {\\"latitude\\": \\"37.24N\\", \\"longitude\\": \\"-115.81W\\"})) # Output: \\"Coordinates: 37.24N, -115.81W\\" print(custom_format(\\"Hex: {number:#x}\\", {\\"number\\": 255})) # Output: \\"Hex: 0xff\\" print(custom_format(\\"Name: {first_name} {last_name!s}\\", {\\"first_name\\": \\"John\\", \\"last_name\\": \\"Doe\\"})) # Output: \\"Name: John Doe\\" ``` # Notes - You must handle the conversion flags `\'!s\'`, `\'!r\'`, and `\'!a\'` as specified in the documentation. - You must ensure that if a required key in the template is missing from the `values` dictionary, a `KeyError` should be raised. - Your implementation should correctly handle and return a formatted string respecting all specified formatting and conversion rules.","solution":"def custom_format(template: str, values: dict) -> str: Processes a template string with multiple variables, applying specific formatting rules as defined by the inputs. try: formatted_string = template.format(**values) return formatted_string except KeyError as e: raise KeyError(f\'Missing key in values: {e}\')"},{"question":"# GPU Stream and Memory Management in PyTorch Implement a function `cuda_matrix_operation` in PyTorch which performs the following tasks: 1. **Initialize CUDA** to ensure it is available and ready for use. 2. **Create two large random matrices** (using CUDA tensors) of size `(1000, 1000)` and perform a matrix multiplication operation on them. 3. **Utilize CUDA streams** to manage concurrent execution. Specifically: - Use a non-default stream to perform the matrix multiplication. - Use the default stream to perform a matrix addition of the result from the multiplication with another random matrix of the same size. 4. **Synchronize the streams** to ensure operations are performed correctly. 5. **Manage memory** to ensure that operations are optimized and no out-of-memory errors occur. 6. Return the final result of the matrix addition. Your function should handle potential exceptions such as out-of-memory errors gracefully and print appropriate error messages. **Function Signature:** ```python def cuda_matrix_operation() -> torch.Tensor: ``` **Input:** - There are no direct inputs to the function. **Output:** - The function returns a PyTorch tensor (on CUDA) which is the result of the matrix addition operation. **Constraints:** - Ensure the matrix operations are done on GPU using CUDA. - Properly handle synchronization to avoid race conditions. - Efficiently manage memory to handle large matrix sizes. **Examples:** ```python output_tensor = cuda_matrix_operation() print(output_tensor) # Output will be a CUDA tensor of size (1000, 1000) ``` **Expected Proficiency:** - Proficient understanding of CUDA initialization and checking availability. - Experience with CUDA tensor operations in PyTorch. - Knowledge of managing and synchronizing CUDA streams. - Ability to handle memory management and exceptions in PyTorch.","solution":"import torch def cuda_matrix_operation(): try: # Check if CUDA is available if not torch.cuda.is_available(): raise EnvironmentError(\\"CUDA is not available.\\") # Initialize CUDA (this step is implicit in pytorch; just checking availability is sufficient) device = torch.device(\'cuda\') # Create two large random matrices on CUDA matrix_a = torch.randn((1000, 1000), device=device) matrix_b = torch.randn((1000, 1000), device=device) # Create streams stream1 = torch.cuda.Stream() stream2 = torch.cuda.default_stream() # Explicitly using the default stream for clarity # Perform matrix multiplication in a non-default stream with torch.cuda.stream(stream1): result_mult = torch.matmul(matrix_a, matrix_b) # Wait for the multiplication stream to complete stream1.synchronize() # Create another matrix for addition matrix_c = torch.randn((1000, 1000), device=device) # Perform matrix addition in the default stream with torch.cuda.stream(stream2): result_add = result_mult + matrix_c # Wait for the default stream to complete stream2.synchronize() return result_add except RuntimeError as e: print(f\\"RuntimeError: {e}\\") except EnvironmentError as e: print(f\\"EnvironmentError: {e}\\")"},{"question":"**Sparse Data Frame Manipulation** As a data scientist, you are often working with datasets containing a lot of missing values. To efficiently handle such datasets, you need to use sparse data structures available in pandas. The goal of this task is to ensure you are comfortable with creating, manipulating, and converting sparse data types. # Objectives: 1. Create a DataFrame with mostly missing values and convert it to a sparse DataFrame. 2. Perform some operations on the sparse DataFrame. 3. Convert the sparse DataFrame back to a dense DataFrame. # Instructions: 1. Write a function `create_sparse_dataframe` that takes two parameters: `num_rows` (int) and `density` (float, between 0 and 1). This function should: - Create a DataFrame with `num_rows` rows and 4 columns, where the values are normally distributed random numbers. - Randomly set a fraction of the elements (determined by the density) to NaN. - Convert this DataFrame to a sparse DataFrame with `SparseDtype(float, np.nan)`. - Return the sparse DataFrame. 2. Write a function `operate_on_sparse_dataframe` that takes a sparse DataFrame as input and performs the following operations: - Calculate and return the density of the sparse DataFrame. - Calculate the sum of each column while maintaining the sparse structure. Return the resulting sparse DataFrame. 3. Write a function `convert_sparse_to_dense` that takes a sparse DataFrame and converts it back to a dense DataFrame. # Example: ```python import pandas as pd import numpy as np def create_sparse_dataframe(num_rows: int, density: float) -> pd.DataFrame: # Your code here def operate_on_sparse_dataframe(sparse_df: pd.DataFrame) -> Tuple[float, pd.DataFrame]: # Your code here def convert_sparse_to_dense(sparse_df: pd.DataFrame) -> pd.DataFrame: # Your code here # Testing the functions sparse_df = create_sparse_dataframe(10000, 0.1) density, sum_df = operate_on_sparse_dataframe(sparse_df) dense_df = convert_sparse_to_dense(sparse_df) print(\\"Sparse DataFrame Density:\\", density) print(\\"Sum of Columns in Sparse DataFrame:n\\", sum_df) print(\\"Dense DataFrame:n\\", dense_df.head()) ``` # Constraints: - `num_rows` will be an integer between 1,000 and 100,000. - `density` will be a float between 0.0 and 1.0. - The operations should maintain sparsity where applicable to preserve memory efficiency. # Performance Requirements: - Ensure that the memory usage is efficiently managed, especially for large DataFrames. - Use the appropriate sparse data structures as covered in the pandas documentation to achieve this. Note: You may assume functionalities such as `np.random`, `pd.DataFrame`, `pd.SparseDtype`, and other relevant pandas functions are available to your environment.","solution":"import pandas as pd import numpy as np def create_sparse_dataframe(num_rows: int, density: float) -> pd.DataFrame: Create a DataFrame with mostly missing values and convert it to a sparse DataFrame. :param num_rows: Number of rows in the DataFrame :param density: Fraction of non-NaN values :return: Sparse DataFrame np.random.seed(0) df = pd.DataFrame(np.random.randn(num_rows, 4)) mask = np.random.rand(num_rows, 4) < (1 - density) df[mask] = np.nan sparse_df = df.astype(pd.SparseDtype(float, np.nan)) return sparse_df def operate_on_sparse_dataframe(sparse_df: pd.DataFrame) -> (float, pd.DataFrame): Perform operations on the sparse DataFrame. :param sparse_df: Sparse DataFrame :return: Tuple containing the density and the sum of each column as a sparse DataFrame total_elements = sparse_df.shape[0] * sparse_df.shape[1] non_nan_elements = sparse_df.sparse.density * total_elements density = non_nan_elements / total_elements sum_df = sparse_df.sum().astype(pd.SparseDtype(float, 0.0)).to_frame().T return density, sum_df def convert_sparse_to_dense(sparse_df: pd.DataFrame) -> pd.DataFrame: Convert the sparse DataFrame back to a dense DataFrame. :param sparse_df: Sparse DataFrame :return: Dense DataFrame return sparse_df.sparse.to_dense()"},{"question":"**PyTorch Coding Assessment** **Objective:** Implement a function that involves tensor creation, manipulation, and accurate testing using PyTorch\'s `torch.testing` utilities. # Problem Statement You are required to write a function called `test_tensor_operations` that creates two tensors, performs a series of operations on them, and verifies that the results meet specified conditions using PyTorch\'s testing utilities. # Function Signature ```python import torch def test_tensor_operations(): pass ``` # Steps and Requirements 1. **Tensor Creation:** - Use `torch.testing.make_tensor` to create a tensor `A` of shape (3, 3) with values drawn from a normal distribution with mean 0 and standard deviation 1. Ensure the tensor uses the default floating-point data type. - Create another tensor `B` by doubling all the values in `A`. 2. **Tensor Operations:** - Subtract tensor `B` from `A` and store the result in tensor `C`. 3. **Assertions and Testing:** - Use `torch.testing.assert_close` to check if tensor `C` is close to tensor `-A` within a default tolerance. - Use `torch.testing.assert_allclose` to verify if tensor `B` is close to `2 * A` within a default tolerance. 4. **Return Values:** - The function does not require any return values, the primary objective is to test and verify the results using the provided utilities. # Constraints - All operations must be using PyTorch\'s tensor functionalities. - Ensure reproducibility by setting a manual seed before tensor creation. # Example Here is an illustration to clarify the operations (assume the seed results in `A` for the example): ```python Manual seed: 42 A = tensor([[ 0.4967, -0.1383, 0.6477], [ 1.5230, -0.2342, -0.2341], [ 1.5792, 0.7674, -0.4695]]) B = tensor([[ 0.9933, -0.2765, 1.2954], [ 3.0460, -0.4684, -0.4681], [ 3.1584, 1.5348, -0.9390]]) C = tensor([[-0.4967, 0.1383, -0.6477], [-1.5230, 0.2342, 0.2341], [-1.5792, -0.7674, 0.4695]]) # Assertions to be checked in the function: # torch.testing.assert_close(C, -A) # torch.testing.assert_allclose(B, 2 * A) ``` # Testing Make sure to test your code to ensure no assertions fail. The function should raise an appropriate error if the conditions are not met. **Hints:** - Don\'t forget to set the random seed for reproducibility using `torch.manual_seed(seed_value)`. Good luck!","solution":"import torch def test_tensor_operations(): torch.manual_seed(42) # Create tensor A using a normal distribution A = torch.randn(3, 3) # Create tensor B by doubling the values in A B = 2 * A # Subtract tensor B from A and store the result in tensor C C = A - B # Check if tensor C is close to -A within the default tolerance torch.testing.assert_close(C, -A) # Check if tensor B is close to 2 * A within the default tolerance torch.testing.assert_close(B, 2 * A)"},{"question":"# Custom Python Module Importer Objective: Design and implement a custom Python module importer that performs additional steps before and after importing a module. This importer will log messages before and after an import, validate the module structure, and be able to reload the module on demand. Task: You need to implement a function `custom_import_module(name: str) -> PyObject:` in C, which: - Accepts a module name as a string. - Logs a message before importing the module. - Imports the module. - Logs a message after successfully importing the module. - Validates if the imported module has a specific attribute (e.g., `__version__`) and raises an ImportError if not. - Optionally reloads the module if it is already imported. Constraints: - You should use the functions from the Python import system documented above. - You should handle potential errors and ensure that the program does not crash on import failure. - The function should be compatible with Python 3.7 and above. - The validation step should log an error message if the required attribute is missing. Expected Input and Output: - **Input**: A string representing the module name. - **Output**: The function returns a new reference to the imported module, or raises an appropriate Python exception on failure. Example: ```c PyObject* custom_import_module(const char* name) { // Log message before import printf(\\"Importing module: %sn\\", name); // Import the module PyObject *module = PyImport_ImportModule(name); if (module == NULL) { PyErr_SetString(PyExc_ImportError, \\"Failed to import module\\"); return NULL; } // Log message after import printf(\\"Successfully imported module: %sn\\", name); // Validate the module if (!PyObject_HasAttrString(module, \\"__version__\\")) { Py_DECREF(module); PyErr_SetString(PyExc_ImportError, \\"Module does not have a __version__ attribute\\"); return NULL; } // Optionally reload the module if already imported module = PyImport_ReloadModule(module); if (module == NULL) { return NULL; } // Return the imported module return module; } ``` Additional Notes: - Ensure proper error handling and reference counting. - Provide comprehensive comments for each step to explain the logic and function usage. - Test your function with various modules to ensure it behaves as expected under different scenarios.","solution":"import importlib import logging # Setup logger logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def custom_import_module(name): Imports a module with additional logging and validation. :param name: The name of the module to import :type name: str :return: The imported module, or raises an ImportError :rtype: module logger.info(f\\"Importing module: {name}\\") try: module = importlib.import_module(name) except ImportError as e: logger.error(f\\"Failed to import module {name}: {e}\\") raise logger.info(f\\"Successfully imported module: {name}\\") if not hasattr(module, \'__version__\'): logger.error(f\\"Module {name} does not have a __version__ attribute\\") raise ImportError(f\\"Module {name} does not have a __version__ attribute\\") # Optionally reload the module if already imported module = importlib.reload(module) if module is None: logger.error(f\\"Failed to reload module {name}\\") raise ImportError(f\\"Failed to reload module {name}\\") return module"},{"question":"Out-of-Core Text Classification In this exercise, you are required to implement an out-of-core learning system using scikit-learn. The system should train a classifier incrementally on text data that is too large to fit into memory. Task: 1. **Data Streaming**: Implement a generator function `stream_data(file_path)` that reads data from a given file. Each line in the file represents a single instance and is in the format `<label>t<text>`. 2. **Feature Extraction**: Use `HashingVectorizer` from scikit-learn to convert the text data into numerical feature vectors. Ensure that the vectorizer is set in a way that it can handle new terms during the incremental training. 3. **Incremental Learning**: Use the `SGDClassifier` implementation in scikit-learn to train the model incrementally. Ensure that the classifier processes data in mini-batches to avoid memory overload. 4. **Model Training and Evaluation**: Implement a function `train_and_evaluate(file_path)` that manages the entire training process. During training, periodically evaluate the model on a validation set (also streamed from a similar file) and output the accuracy. Constraints: - The `SGDClassifier` should be trained on data read from the file `train_data.txt` in increments of 1000 instances. - Evaluation is to be performed every 10,000 instances read, using a separate file `validation_data.txt`. - The label in the data is an integer representing the class. - The function signatures should be as follows: ```python def stream_data(file_path: str): # Function to yield instances from the file. pass def train_and_evaluate(train_file_path: str, validation_file_path: str): # Function to perform incremental training and evaluation. pass ``` Input Format: - A training file `train_data.txt` is provided, where each line is `<label>t<text>`. - A validation file `validation_data.txt` is provided, where each line is `<label>t<text>`. Output Format: - Periodic accuracy of the model after every 10,000 instances. Example: ```python train_and_evaluate(\'train_data.txt\', \'validation_data.txt\') ``` Would output accuracy at intervals of processing 10,000 instances from `validation_data.txt`. Notes: - Use appropriate scikit-learn classes and methods as highlighted in the analysis. - Focus on efficiently managing memory and optimizing the batch sizes for incremental learning.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path: str): Generator function to yield instances from the file. Each line in the file represents a single instance in the format <label>t<text>. with open(file_path, \'r\') as f: for line in f: label, text = line.strip().split(\'t\') yield int(label), text def train_and_evaluate(train_file_path: str, validation_file_path: str): Function to perform incremental training and evaluation. vectorizer = HashingVectorizer(decode_error=\'ignore\', n_features=2**20, alternate_sign=False) classifier = SGDClassifier() def evaluate(model, data_stream): X_val, y_val = [], [] for label, text in data_stream: X_val.append(text) y_val.append(label) X_val = vectorizer.transform(X_val) y_pred = model.predict(X_val) return accuracy_score(y_val, y_pred) # Training in mini-batches train_batch, train_labels = [], [] for i, (label, text) in enumerate(stream_data(train_file_path)): train_batch.append(text) train_labels.append(label) if (i+1) % 1000 == 0: X_train = vectorizer.transform(train_batch) classifier.partial_fit(X_train, train_labels, classes=[0, 1]) train_batch, train_labels = [], [] if (i+1) % 10000 == 0: print(f\\"Evaluating after {i+1} instances...\\") accuracy = evaluate(classifier, stream_data(validation_file_path)) print(f\\"Accuracy: {accuracy}\\") # Final evaluations after last batch if train_batch: X_train = vectorizer.transform(train_batch) classifier.partial_fit(X_train, train_labels) accuracy = evaluate(classifier, stream_data(validation_file_path)) print(f\\"Final Accuracy: {accuracy}\\")"},{"question":"**Objective:** Your task is to write a Python program that uses the `modulefinder` module to analyze a list of Python scripts. Your program should generate a comprehensive report that includes the following details for each script: 1. A list of imported modules. 2. The first three global names found in each imported module. 3. A list of modules that could not be imported. 4. A summary table showing how many scripts each module is imported by. **Input:** - A list of file paths to Python scripts. **Output:** 1. For each script: - A detailed list of imported modules and their first three global names. - A list of modules that could not be imported. 2. A summary table listing each module and the number of scripts it is imported by. **Constraints:** - Assume all file paths are correct and accessible. - Each Python script is syntactically correct and executable. - The program should handle up to 100 scripts efficiently. **Performance Requirements:** - The program should execute within reasonable time limits, even for the maximum number of scripts (100 scripts). **Example Usage:** Below is an example of how your program should produce the output: ```python # Assume the program is executed with: # analyze_scripts([\'script1.py\', \'script2.py\', \'script3.py\']) # Example output: # Script: script1.py # Imported Modules: # - re: __module__, finditer, _expand # - itertools: ... # ... # Modules not imported: # - some_missing_module # Script: script2.py # Imported Modules: # - os: ... # ... # Modules not imported: # - another_missing_module # Summary: # Module | Number of Scripts # re | 2 # itertools | 1 # os | 1 # some_missing_module | 0 # another_missing_module | 0 ``` # Hints: - Use the `ModuleFinder` class to analyze each script and gather required details. - Utilize dictionary operations to collate and summarize module imports across all scripts. - Ensure your output formatting is clear and well-organized for readability. **Function Signature:** ```python def analyze_scripts(script_paths: list) -> None: pass ``` Implement the function `analyze_scripts` to solve the problem as described.","solution":"import modulefinder def analyze_scripts(script_paths): module_summary = {} for script in script_paths: print(f\\"Script: {script}\\") finder = modulefinder.ModuleFinder() finder.run_script(script) imported_modules = [] not_imported_modules = [] for name, mod in finder.modules.items(): if mod.__file__: imported_modules.append(name) global_names = list(mod.globalnames.keys())[:3] print(f\\"- {name}: {\', \'.join(global_names)}\\") if name in module_summary: module_summary[name].add(script) else: module_summary[name] = {script} else: not_imported_modules.append(name) print(\\"Modules not imported:\\") for name in not_imported_modules: print(f\\"- {name}\\") if name not in module_summary: module_summary[name] = set() print() # New line for readability print(\\"Summary:\\") print(f\\"{\'Module\':<20} | Number of Scripts\\") for module, scripts in module_summary.items(): print(f\\"{module:<20} | {len(scripts)}\\") # Example usage # analyze_scripts([\'script1.py\', \'script2.py\', \'script3.py\'])"},{"question":"# Practical Application of the `gzip` Module in Python Objective: Your task is to demonstrate your understanding of the `gzip` module by implementing functions that involve both compressing and decompressing data. Additionally, you will create a small command-line interface (CLI) to allow users to perform these actions using command-line arguments. Instructions: 1. **Function Implementations**: - Implement a function `compress_data(input_data: bytes, compresslevel: int = 9) -> bytes`: - This function takes a bytes object `input_data` and an optional integer `compresslevel` (default is 9). - It returns the compressed data as a bytes object. - Implement a function `decompress_data(input_data: bytes) -> bytes`: - This function takes a bytes object `input_data` containing compressed data. - It returns the decompressed data as a bytes object. 2. **Command-Line Interface**: - Implement a CLI script that allows users to compress or decompress files using command-line arguments. - The script should support the following operations: - Compress a file: `python cli_script.py --compress <input_file> <output_file> [--compresslevel=<level>]` - Decompress a file: `python cli_script.py --decompress <input_file> <output_file>` - Ensure that your script provides helpful error messages and usage instructions via the `--help` option. Constraints: - The functions should handle invalid input gracefully by raising appropriate exceptions. - The CLI should validate the presence and correctness of command-line arguments. - The default compression level should be set to 6 if not specified in the CLI. - Ensure cross-platform compatibility for the CLI script. Example Usage: 1. Function usage: ```python data = b\\"Hello world! This is a test for gzip compression.\\" compressed_data = compress_data(data) decompressed_data = decompress_data(compressed_data) assert data == decompressed_data ``` 2. CLI usage: ```shell # Compress a file python cli_script.py --compress input.txt output.txt.gz --compresslevel=5 # Decompress a file python cli_script.py --decompress output.txt.gz decompressed.txt ``` **Note**: You are expected to write clear, well-documented, and efficient code, considering edge cases and proper exception handling.","solution":"import gzip import argparse import sys def compress_data(input_data: bytes, compresslevel: int = 9) -> bytes: Compresses the input data using gzip compression algorithm. :param input_data: Data to be compressed in bytes. :param compresslevel: Compression level (1-9), default is 9 (highest). :return: Compressed data in bytes. if not isinstance(input_data, bytes): raise ValueError(\\"Input data must be of type bytes.\\") if not (1 <= compresslevel <= 9): raise ValueError(\\"Compression level must be between 1 and 9.\\") buffer = gzip.compress(input_data, compresslevel=compresslevel) return buffer def decompress_data(input_data: bytes) -> bytes: Decompresses the input data which was compressed using gzip algorithm. :param input_data: Compressed data in bytes. :return: Decompressed data in bytes. if not isinstance(input_data, bytes): raise ValueError(\\"Input data must be of type bytes.\\") buffer = gzip.decompress(input_data) return buffer def main(): parser = argparse.ArgumentParser(description=\'Compress or decompress files using gzip.\') group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\'--compress\', action=\'store_true\', help=\'Compress the input file.\') group.add_argument(\'--decompress\', action=\'store_true\', help=\'Decompress the input file.\') parser.add_argument(\'input_file\', type=str, help=\'The input file to be processed.\') parser.add_argument(\'output_file\', type=str, help=\'The output file to be created.\') parser.add_argument(\'--compresslevel\', type=int, default=6, help=\'Compression level for gzip (default: 6).\') args = parser.parse_args() try: with open(args.input_file, \'rb\') as f_input: input_data = f_input.read() if args.compress: result_data = compress_data(input_data, compresslevel=args.compresslevel) elif args.decompress: result_data = decompress_data(input_data) with open(args.output_file, \'wb\') as f_output: f_output.write(result_data) print(f\'Successfully processed file: {args.input_file} -> {args.output_file}\') except Exception as e: print(f\\"An error occurred: {e}\\") sys.exit(1) if __name__ == \\"__main__\\": main()"},{"question":"Objective: To assess the student\'s understanding of using the `linecache` module for efficient line retrieval from files and managing the cache. Problem Statement: You are given a large text file named `large_text.txt`. Your task is to implement a function `retrieve_lines` that takes a list of line numbers and returns the corresponding lines from the file. Use the `linecache` module for retrieving lines and handling any potential errors or cache management. Function Signature: ```python def retrieve_lines(filename: str, line_numbers: list) -> list: pass ``` Inputs: - `filename`: A string representing the name of the file (e.g., \\"large_text.txt\\"). - `line_numbers`: A list of integers where each integer represents a line number to be retrieved. Outputs: - A list of strings where each string is a line from the file corresponding to the input line numbers. The list should be in the same order as the input line numbers. Constraints and Considerations: 1. If a line number is out of range for the file, the corresponding returned line should be an empty string. 2. Use the `linecache` module to retrieve lines efficiently. 3. Clear the cache after retrieving lines to ensure minimal memory usage. 4. Assume the file encoding is UTF-8. Example: ```python # Example file content (large_text.txt): # 1: This is the first line. # 2: This is the second line. # 3: This is the third line. # 4: This is the fourth line. # 5: This is the fifth line. filename = \'large_text.txt\' line_numbers = [1, 3, 5, 7] # Expected Output: # [\'This is the first line.n\', \'This is the third line.n\', \'This is the fifth line.n\', \'\'] print(retrieve_lines(filename, line_numbers)) ``` Notes: - The newline character should be included in the returned lines. - Ensure that your function handles potential errors gracefully. - Optimize for performance using the caching mechanism provided by the `linecache` module.","solution":"import linecache def retrieve_lines(filename: str, line_numbers: list) -> list: Retrieve specific lines from a file based on provided line numbers. Parameters: filename (str): The name of the file to read from. line_numbers (list): A list of line numbers to retrieve. Returns: list of str: The lines corresponding to the line numbers, or an empty string if the line number is out of range. lines = [] for number in line_numbers: line = linecache.getline(filename, number) lines.append(line) linecache.clearcache() return lines"},{"question":"# Advanced Python Utility Functions You are tasked with creating a Python utility script that demonstrates the use of various utility functions as covered in the documentation. Specifically, you will implement a function that requires parsing inputs, importing a specific module dynamically, handling OS-level operations, formatting strings, and dealing with Unicode encoding errors. **Function Signature:** ```python def advanced_utility(input_data: Union[str, int, float], codec_name: str) -> str: pass ``` # Requirements: 1. **Parsing and Building Values:** - Parse the `input_data` to determine its type (string, integer, or float). - If it is a string, ensure it is converted to UTF-8 format. - If it is an integer, transform it to a float by dividing by 10 (e.g., 10 -> 1.0). - If it is a float, format it to two decimal places. 2. **Importing Modules:** - Dynamically import the `os` module and use one of its functions to get the current working directory. 3. **String Conversion and Formatting:** - Convert the processed data to a string format that includes the type information and the current working directory. The format should be: `\\"Processed value: <value>; Type: <type>; Directory: <cwd>\\"`. 4. **Codec Registry and Support Functions:** - Look up a codec using `codec_name` provided in the function argument. - If the codec is not found, return an error message indicating the codec was not found. - If found, encode the final string using the specified codec. # Example: ```python def advanced_utility(input_data: Union[str, int, float], codec_name: str) -> str: import os import codecs # Parsing input data if isinstance(input_data, str): parsed_data = input_data.encode(\'utf-8\') data_type = \'string\' elif isinstance(input_data, int): parsed_data = float(input_data) / 10 data_type = \'integer\' elif isinstance(input_data, float): parsed_data = f\\"{input_data:.2f}\\" data_type = \'float\' else: return \\"Unsupported data type\\" # Getting current working directory cwd = os.getcwd() # Formatting string formatted_string = f\\"Processed value: {parsed_data}; Type: {data_type}; Directory: {cwd}\\" # Codec lookup try: codec = codecs.lookup(codec_name) except LookupError: return \\"Codec not found\\" # Encoding string encoded_string = formatted_string.encode(codec_name) return encoded_string.decode(codec_name) # Example Usage print(advanced_utility(\\"hello\\", \\"utf-8\\")) print(advanced_utility(42, \\"utf-8\\")) print(advanced_utility(3.14159, \\"utf-8\\")) print(advanced_utility(\\"hello\\", \\"unknown_codec\\")) # Should return \\"Codec not found\\" ``` # Constraints: - The function should handle unexpected input gracefully. - Make sure to utilize appropriate error handling for codec lookup. # Evaluation Criteria: - Correct functionality and error handling. - Proper use of dynamic imports and OS operations. - Accurate string conversion and formatting. - Correct implementation of codec registry functions.","solution":"import os import codecs from typing import Union def advanced_utility(input_data: Union[str, int, float], codec_name: str) -> str: # Parsing input data if isinstance(input_data, str): parsed_data = input_data.encode(\'utf-8\').decode(\'utf-8\') data_type = \'string\' elif isinstance(input_data, int): parsed_data = float(input_data) / 10 data_type = \'integer\' elif isinstance(input_data, float): parsed_data = f\\"{input_data:.2f}\\" data_type = \'float\' else: return \\"Unsupported data type\\" # Getting current working directory cwd = os.getcwd() # Formatting string formatted_string = f\\"Processed value: {parsed_data}; Type: {data_type}; Directory: {cwd}\\" # Codec lookup try: codec = codecs.lookup(codec_name) except LookupError: return \\"Codec not found\\" # Encoding string encoded_string = codec.encode(formatted_string)[0] return encoded_string.decode(codec_name)"},{"question":"# Coding Assignment Objective: To assess your understanding of the `pwd` module in Python, you are required to write a function that retrieves user account information and processes it. Specifically, you will identify the users with the greatest and smallest user IDs (UIDs) and return their account details. Function Signature: ```python def find_users_with_extreme_uids(): pass ``` Requirements: 1. **Retrieve All Users**: Use the `pwd.getpwall()` function to obtain a list of all password entries. 2. **Find Users with Extreme UIDs**: Identify the users with the smallest and largest UIDs. 3. **Return User Information**: For each of these users, return their login name, UID, home directory, and command interpreter. Expected Output: The function should return a dictionary with the following structure: ```python { \\"min_uid_user\\": { \\"login_name\\": <str>, \\"uid\\": <int>, \\"home_directory\\": <str>, \\"command_interpreter\\": <str> }, \\"max_uid_user\\": { \\"login_name\\": <str>, \\"uid\\": <int>, \\"home_directory\\": <str>, \\"command_interpreter\\": <str> } } ``` Example: Assume the following user entries: | pw_name | pw_uid | |---------|--------| | user1 | 1001 | | user2 | 1002 | | user3 | 1000 | If the following users exist in the password database: - `user1` with UID 1001. - `user2` with UID 1002. - `user3` with UID 1000. The expected output of your function would be: ```python { \\"min_uid_user\\": { \\"login_name\\": \\"user3\\", \\"uid\\": 1000, \\"home_directory\\": \\"/home/user3\\", \\"command_interpreter\\": \\"/bin/bash\\" }, \\"max_uid_user\\": { \\"login_name\\": \\"user2\\", \\"uid\\": 1002, \\"home_directory\\": \\"/home/user2\\", \\"command_interpreter\\": \\"/bin/bash\\" } } ``` Constraints: 1. You may assume there is at least one user in the password database. 2. The function should handle the retrieval and processing efficiently. 3. Ensure the solution is robust and handles any potential exceptions. Good luck!","solution":"import pwd def find_users_with_extreme_uids(): Find and return the user information for the users with the smallest and largest UIDs. all_users = pwd.getpwall() if not all_users: return None min_uid_user = min(all_users, key=lambda user: user.pw_uid) max_uid_user = max(all_users, key=lambda user: user.pw_uid) result = { \\"min_uid_user\\": { \\"login_name\\": min_uid_user.pw_name, \\"uid\\": min_uid_user.pw_uid, \\"home_directory\\": min_uid_user.pw_dir, \\"command_interpreter\\": min_uid_user.pw_shell }, \\"max_uid_user\\": { \\"login_name\\": max_uid_user.pw_name, \\"uid\\": max_uid_user.pw_uid, \\"home_directory\\": max_uid_user.pw_dir, \\"command_interpreter\\": max_uid_user.pw_shell } } return result"},{"question":"You are tasked with designing a vehicle management system where all types of vehicles must conform to a common interface using abstract base classes. You will create an abstract base class called `Vehicle`, which enforces all derived classes to implement certain methods and properties. Then, you will create two derived classes, `Car` and `Bike`, which must implement this interface. Abstract Base Class Requirements: 1. **Class Name:** `Vehicle` 2. **Methods:** - `start_engine(self)` - Starts the engine. Abstract method. - `stop_engine(self)` - Stops the engine. Abstract method. - `__str__(self)` - Returns a string representation of the vehicle\'s details. 3. **Properties:** - `speed` - An integer property representing the current speed of the vehicle. Both setter and getter must be abstract. Derived Class Requirements: 1. **Class Name:** `Car` - Implement all abstract methods and properties from `Vehicle`. - Additional Method: `honk_horn(self)` - Returns the string \\"`Honk! Honk!`\\". 2. **Class Name:** `Bike` - Implement all abstract methods and properties from `Vehicle`. - Additional Method: `ring_bell(self)` - Returns the string \\"`Ring! Ring!`\\". Input and Output: - No user input is required. - Call each method of Car and Bike classes to demonstrate their functionality. - Print the string representation of both a Car and a Bike object. Constraints: - Ensure `Car` and `Bike` classes can be instantiated without error. - Adhere to proper class inheritance and method overriding practices. - Use the `abstractmethod` decorator for abstract methods and properties. # Example ```python from abc import ABC, abstractmethod class Vehicle(ABC): @abstractmethod def start_engine(self): pass @abstractmethod def stop_engine(self): pass @abstractmethod def __str__(self): pass @property @abstractmethod def speed(self): pass @speed.setter @abstractmethod def speed(self, value): pass class Car(Vehicle): def __init__(self): self._speed = 0 def start_engine(self): return \\"Car engine started.\\" def stop_engine(self): return \\"Car engine stopped.\\" def __str__(self): return f\\"Car with speed {self._speed} km/h\\" @property def speed(self): return self._speed @speed.setter def speed(self, value): self._speed = value def honk_horn(self): return \\"Honk! Honk!\\" class Bike(Vehicle): def __init__(self): self._speed = 0 def start_engine(self): return \\"Bike engine started.\\" def stop_engine(self): return \\"Bike engine stopped.\\" def __str__(self): return f\\"Bike with speed {self._speed} km/h\\" @property def speed(self): return self._speed @speed.setter def speed(self, value): self._speed = value def ring_bell(self): return \\"Ring! Ring!\\" # Demonstrate functionality car = Car() bike = Bike() print(car.start_engine()) print(car.honk_horn()) car.speed = 100 print(car) print(bike.start_engine()) print(bike.ring_bell()) bike.speed = 25 print(bike) ``` The provided code is an example of how the `Vehicle`, `Car`, and `Bike` classes should be defined and used. You should not simply run this example, but use it as a reference for your implementation. Explanation: - **Abstract Base Class (`Vehicle`)**: Enforces the presence of essential methods and properties. - **Derived Classes (`Car` and `Bike`)**: Provide concrete implementations of those methods and include additional functionalities specific to each type of vehicle.","solution":"from abc import ABC, abstractmethod class Vehicle(ABC): @abstractmethod def start_engine(self): pass @abstractmethod def stop_engine(self): pass @abstractmethod def __str__(self): pass @property @abstractmethod def speed(self): pass @speed.setter @abstractmethod def speed(self, value): pass class Car(Vehicle): def __init__(self): self._speed = 0 def start_engine(self): return \\"Car engine started.\\" def stop_engine(self): return \\"Car engine stopped.\\" def __str__(self): return f\\"Car with speed {self._speed} km/h\\" @property def speed(self): return self._speed @speed.setter def speed(self, value): self._speed = value def honk_horn(self): return \\"Honk! Honk!\\" class Bike(Vehicle): def __init__(self): self._speed = 0 def start_engine(self): return \\"Bike engine started.\\" def stop_engine(self): return \\"Bike engine stopped.\\" def __str__(self): return f\\"Bike with speed {self._speed} km/h\\" @property def speed(self): return self._speed @speed.setter def speed(self, value): self._speed = value def ring_bell(self): return \\"Ring! Ring!\\" # Demonstrate functionality car = Car() bike = Bike() print(car.start_engine()) print(car.honk_horn()) car.speed = 100 print(car) print(bike.start_engine()) print(bike.ring_bell()) bike.speed = 25 print(bike)"},{"question":"# Advanced Python Protocols Assessment Context In this coding challenge, you will implement a custom Python sequence type that mimics certain behaviors of Python\'s built-in list, but with added functionality to sum all the numerical values contained in the sequence. This exercise will test your understanding of Python protocols, particularly the sequence protocol. Task You need to implement a custom class `CustomSequence` that behaves like a typical Python sequence (e.g., list) and also provides a method to calculate the sum of all its numerical elements. Your class must support indexing, length queries, and iteration. Additionally, it should raise appropriate exceptions when invalid operations are performed. Requirements 1. Implement the `CustomSequence` class. 2. The class should support: - Indexing (e.g., `seq[i]`). - Length queries (e.g., `len(seq)`). - Iteration (using loops). 3. Provide a method `sum_elements(self)` that returns the sum of all numerical elements in the sequence, skipping any non-numeric elements. 4. Raise an `IndexError` when an invalid index is accessed. Constraints 1. The elements of the `CustomSequence` can be of mixed types (integers, floats, strings, etc.). 2. The class should efficiently handle typical list operations. 3. No external libraries should be used; only built-in Python functionality is allowed. Example ```python class CustomSequence: def __init__(self, elements): self.elements = elements def __getitem__(self, index): if index >= len(self.elements) or index < -len(self.elements): raise IndexError(\\"CustomSequence index out of range\\") return self.elements[index] def __len__(self): return len(self.elements) def __iter__(self): return iter(self.elements) def sum_elements(self): return sum(e for e in self.elements if isinstance(e, (int, float))) # Sample usage seq = CustomSequence([1, 2, \'a\', 3, \'b\', 4]) print(seq[2]) # Output: \'a\' print(len(seq)) # Output: 6 print(seq.sum_elements()) # Output: 10 for item in seq: print(item) # Output: 1, 2, \'a\', 3, \'b\', 4 ``` Write the full implementation of the `CustomSequence` class.","solution":"class CustomSequence: def __init__(self, elements): self.elements = elements def __getitem__(self, index): if index >= len(self.elements) or index < -len(self.elements): raise IndexError(\\"CustomSequence index out of range\\") return self.elements[index] def __len__(self): return len(self.elements) def __iter__(self): return iter(self.elements) def sum_elements(self): return sum(e for e in self.elements if isinstance(e, (int, float))) # Sample usage # seq = CustomSequence([1, 2, \'a\', 3, \'b\', 4]) # print(seq[2]) # Output: \'a\' # print(len(seq)) # Output: 6 # print(seq.sum_elements()) # Output: 10 # for item in seq: # print(item) # Output: 1, 2, \'a\', 3, \'b\', 4"},{"question":"**Custom Importer Using `importlib` in Python** *Objective*: Demonstrate your understanding of the `importlib` package by implementing a custom import system. Specifically, you will create a meta path finder and a loader to import modules from a specific directory that you provide. *Requirements*: 1. **Implement a Custom MetaPathFinder**: - Class `CustomMetaPathFinder` that inherits from `importlib.abc.MetaPathFinder`. - Implement the method `find_spec` to find a module spec for a given module name and directory path. 2. **Implement a Custom SourceLoader**: - Class `CustomSourceLoader` that inherits from `importlib.abc.SourceLoader`. - Implement methods `get_code`, `get_data`, `get_filename`, and `get_source` to load the module\'s source code from the directory path provided. 3. **Register the Custom Finder**: - Add an instance of your `CustomMetaPathFinder` to `sys.meta_path`. 4. **Test the Custom Importer**: - Create a test to dynamically import a module using your custom importer. - The module should be present in a directory specified by a path. - After importing, validate that a function or variable within the module works as expected. *Input and Output*: ```python # Example directory structure: # /custom_modules/ #  my_module.py #  another_module.py # File: my_module.py # def greet(): # return \\"Hello, World!\\" # File: another_module.py # def add(a, b): # return a + b # Your code should: # 1. Create the CustomMetaPathFinder and CustomSourceLoader classes. # 2. Add CustomMetaPathFinder to sys.meta_path. # 3. Import my_module and another_module dynamically using custom importer. # 4. Verify the functions in these modules. # The output should be: # Testing my_module: # Hello, World! # Testing another_module: # 5 # Implementation import importlib.abc import importlib.util import sys import os from types import ModuleType class CustomMetaPathFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): # Find and return a ModuleSpec if the module can be found pass class CustomSourceLoader(importlib.abc.SourceLoader): def __init__(self, path): self.path = path def get_code(self, fullname): pass # Implement to return code object def get_data(self, pathname): pass # Implement to load data from file def get_filename(self, fullname): pass # Implement to return the file path def get_source(self, fullname): pass # Implement to return source code def register_custom_importer(path): # Add instance of CustomMetaPathFinder to sys.meta_path pass def test_custom_importer(): register_custom_importer(\\"path/to/custom_modules\\") import my_module print(\\"Testing my_module:\\") print(my_module.greet()) import another_module print(\\"Testing another_module:\\") print(another_module.add(2, 3)) test_custom_importer() ``` *Constraints*: - Assume the modules in the specified directory are `Python` modules with `.py` file extension. - Use only the functionalities and classes provided by the `importlib` package. - Do not use the built-in `import` statement to import the test modules directly; they must be imported using your custom importer. *Performance Requirements*: - Ensure that your implementation is efficient and avoids unnecessary I/O operations by using caching where applicable. - Avoid global states that could affect modules imported by other systems.","solution":"import importlib.abc import importlib.util import sys import os class CustomMetaPathFinder(importlib.abc.MetaPathFinder): def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path, target=None): module_path = os.path.join(self.directory, fullname.replace(\'.\', \'/\') + \'.py\') if os.path.exists(module_path): return importlib.util.spec_from_file_location(fullname, module_path, loader=CustomSourceLoader(module_path)) return None class CustomSourceLoader(importlib.abc.SourceLoader): def __init__(self, path): self.path = path def get_data(self, pathname): with open(pathname, \'rb\') as f: return f.read() def get_filename(self, fullname): return self.path def get_code(self, fullname): source = self.get_source(fullname) return self.source_to_code(source, self.path) def get_source(self, fullname): with open(self.path, \'r\') as f: return f.read() def register_custom_importer(directory): finder = CustomMetaPathFinder(directory) sys.meta_path.insert(0, finder)"},{"question":"# AIFF File Manipulation with `aifc` Module **Objective:** Write a Python function that reads an AIFF or AIFF-C file, extracts its metadata, inverses the audio samples, and writes the result to a new AIFF file. This task will demonstrate your knowledge of file handling, audio processing, and the `aifc` module in Python. **Function Signature:** ```python def process_aiff(input_file: str, output_file: str) -> dict: pass ``` **Input:** - `input_file` (str): Path to the input AIFF or AIFF-C file. - `output_file` (str): Path where the output AIFF file will be saved. **Output:** - Returns a dictionary with metadata about the input file: ```python { \\"nchannels\\": int, # Number of audio channels \\"sampwidth\\": int, # Sample width in bytes \\"framerate\\": int, # Sampling rate in frames per second \\"nframes\\": int, # Number of frames \\"comptype\\": bytes, # Compression type \\"compname\\": bytes # Human-readable compression name } ``` **Constraints:** 1. The input file must exist and be a valid AIFF or AIFF-C file. 2. The output file must be a valid AIFF file. 3. The audio inversion should invert the amplitude of the audio samples (i.e., multiply each sample by -1). **Steps:** 1. Open the input file using the `aifc` module. 2. Extract the metadata information (number of channels, sample width, frame rate, number of frames, compression type, and compression name). 3. Read the entire audio data from the file. 4. Inverse the audio samples. 5. Write the modified audio data to the output file with the same metadata as the input file. 6. Return the extracted metadata in the specified dictionary format. **Example:** ```python metadata = process_aiff(\'example.aiff\', \'output.aiff\') print(metadata) # Output should be a dictionary similar to: # { # \\"nchannels\\": 2, # \\"sampwidth\\": 2, # \\"framerate\\": 44100, # \\"nframes\\": 100000, # \\"comptype\\": b\'NONE\', # \\"compname\\": b\'not compressed\' # } ``` **Note:** Ensure to handle exceptions for file I/O operations and any potential data consistency issues.","solution":"import aifc def process_aiff(input_file: str, output_file: str) -> dict: with aifc.open(input_file, \'rb\') as infile: # Extract metadata metadata = { \\"nchannels\\": infile.getnchannels(), \\"sampwidth\\": infile.getsampwidth(), \\"framerate\\": infile.getframerate(), \\"nframes\\": infile.getnframes(), \\"comptype\\": infile.getcomptype(), \\"compname\\": infile.getcompname() } # Read frames audio_data = infile.readframes(metadata[\\"nframes\\"]) # Invert the audio samples sample_width = metadata[\\"sampwidth\\"] inverted_data = bytearray() for i in range(0, len(audio_data), sample_width): # Extract the sample sample = int.from_bytes(audio_data[i:i+sample_width], byteorder=\\"big\\", signed=True) # Invert the sample inverted_sample = (-sample).to_bytes(sample_width, byteorder=\\"big\\", signed=True) inverted_data.extend(inverted_sample) with aifc.open(output_file, \'wb\') as outfile: # Set parameters outfile.setnchannels(metadata[\\"nchannels\\"]) outfile.setsampwidth(metadata[\\"sampwidth\\"]) outfile.setframerate(metadata[\\"framerate\\"]) outfile.setcomptype(metadata[\\"comptype\\"], metadata[\\"compname\\"]) # Write inverted frames outfile.writeframes(inverted_data) return metadata"},{"question":"**Python High-Level API Function Implementation** You have been provided with a high-level overview of several functions intended to execute or compile Python code in various contexts. These functions belong to Python\'s C API, and the goal is to implement a Python function leveraging some of these capabilities. The primary focus will be on executing and interacting with Python code dynamically. # Task Write a Python function `execute_python_code` that does the following: 1. Accepts a string of Python code as input. 2. Compiles the code into a Python code object. 3. Executes the compiled code within the context of the provided global and local variables. 4. Returns the output of the executed code. # Requirements 1. **Function Signature**: ```python def execute_python_code(code: str, globals_dict: dict, locals_dict: dict) -> Any: ``` 2. **Parameters**: - `code`: A string containing valid Python code. - `globals_dict`: A dictionary representing the global variables context. - `locals_dict`: A dictionary representing the local variables context. 3. **Returns**: - The result of executing the Python code, which could be of any type depending on the code. # Constraints - You must use the `exec` or `eval` functions appropriately to compile and execute the Python code. - You should handle any potential exceptions that might arise during compilation or execution and return an appropriate error message. - Do not use `eval` directly on arbitrary code as it poses a security risk; ensure the code is safely executed within the specified global and local contexts. # Example ```python code_example = result = sum([globals_dict[\'a\'], locals_dict[\'b\'], 10]) globals_dict = {\'a\': 5} locals_dict = {\'b\': 15} output = execute_python_code(code_example, globals_dict, locals_dict) print(output[\'result\']) # Should print 30 ``` **NOTE**: This question aims to test your understanding of Python\'s dynamic execution capabilities and handling contexts to safely execute arbitrary code snippets.","solution":"def execute_python_code(code: str, globals_dict: dict, locals_dict: dict) -> dict: Executes the provided Python code within the given global and local contexts. Parameters: - code: A string containing valid Python code. - globals_dict: A dictionary representing the global variables context. - locals_dict: A dictionary representing the local variables context. Returns: - The combined global and local contexts after executing the code. try: # Compile the code compiled_code = compile(code, \'<string>\', \'exec\') # Execute the code in the provided contexts exec(compiled_code, globals_dict, locals_dict) except Exception as e: return {\'error\': str(e)} # Return the updated globals and locals return {**globals_dict, **locals_dict}"},{"question":"**Objective**: Implement and evaluate an `SGDClassifier` to solve a binary classification problem using a given dataset, focusing on hyperparameter optimization and performance evaluation. Question You are provided with a dataset `data.csv` containing features (numerical) and a target variable for a binary classification task. Implement an `SGDClassifier` to classify the data. Perform hyperparameter tuning to optimize the model performance. Instructions 1. **Data Handling**: Load the dataset and split it into training and testing sets with an 80-20 split. 2. **Preprocessing**: Standardize the feature values. 3. **Model Implementation**: - Implement `SGDClassifier` with the following requirements: - Use `loss=\\"log_loss\\"` (logistic regression). - Consider at least two penalties: `l2` and `elasticnet`. - Optimize hyperparameters: `alpha` (regularization term), `max_iter` (number of iterations), and `penalty`. - Use cross-validation to find the best combination of hyperparameters. 4. **Evaluation**: Evaluate the final model on the test set using accuracy and the area under the ROC curve (AUC). 5. **Prediction**: Output the predictions for the test set. Requirements - **Input**: A CSV file `data.csv` with features in columns `X1, X2, ..., Xn` and the target variable in column `target`. - **Output**: - Best hyperparameters. - Accuracy and AUC on the test set. - Predictions for the test set. Constraints - Use `scikit-learn` for model implementation and evaluation. - Use `pandas` for data handling. # Example Here is an example outline of how your solution might look (you do not need to follow this exactly): ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, roc_auc_score # Load the dataset data = pd.read_csv(\'data.csv\') X = data.drop(columns=[\'target\']) y = data[\'target\'] # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline for preprocessing and model pipeline = make_pipeline(StandardScaler(), SGDClassifier(random_state=42)) # Define the parameter grid param_grid = { \'sgdclassifier__alpha\': [0.0001, 0.001, 0.01], \'sgdclassifier__max_iter\': [1000, 2000, 3000], \'sgdclassifier__penalty\': [\'l2\', \'elasticnet\'] } # Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'roc_auc\') grid_search.fit(X_train, y_train) # Get the best model best_model = grid_search.best_estimator_ # Evaluate the model on the test set y_pred = best_model.predict(X_test) y_prob = best_model.predict_proba(X_test)[:, 1] accuracy = accuracy_score(y_test, y_pred) auc = roc_auc_score(y_test, y_prob) print(\\"Best hyperparameters:\\", grid_search.best_params_) print(\\"Test set accuracy:\\", accuracy) print(\\"Test set AUC:\\", auc) print(\\"Predictions for the test set:\\", y_pred) ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, roc_auc_score def load_data(filepath): Load the dataset from the given filepath. return pd.read_csv(filepath) def split_data(data): Split the dataset into training and testing sets. X = data.drop(columns=[\'target\']) y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def create_pipeline(): Create a pipeline with StandardScaler and SGDClassifier. return make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', random_state=42)) def define_param_grid(): Define the parameter grid for GridSearchCV. return { \'sgdclassifier__alpha\': [0.0001, 0.001, 0.01], \'sgdclassifier__max_iter\': [1000, 2000, 3000], \'sgdclassifier__penalty\': [\'l2\', \'elasticnet\'] } def perform_grid_search(pipeline, param_grid, X_train, y_train): Perform grid search to find the best hyperparameters. grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'roc_auc\') grid_search.fit(X_train, y_train) return grid_search def evaluate_model(model, X_test, y_test): Evaluate the model on the test set using accuracy and AUC. y_pred = model.predict(X_test) y_prob = model.predict_proba(X_test)[:, 1] accuracy = accuracy_score(y_test, y_pred) auc = roc_auc_score(y_test, y_prob) return accuracy, auc, y_pred def sgd_classifier_pipeline(filepath): data = load_data(filepath) X_train, X_test, y_train, y_test = split_data(data) pipeline = create_pipeline() param_grid = define_param_grid() grid_search = perform_grid_search(pipeline, param_grid, X_train, y_train) best_model = grid_search.best_estimator_ accuracy, auc, predictions = evaluate_model(best_model, X_test, y_test) return grid_search.best_params_, accuracy, auc, predictions"},{"question":"Objective: Demonstrate your understanding of Seaborn\'s objects interface by loading, preprocessing, and visualizing a complex dataset. Problem Statement: You are provided with a multilevel indexed dataset related to `brain_networks`. Your task is to preprocess this dataset and create a visualization using Seaborn\'s objects interface. The visualization should display the mean values of different brain networks over time, differentiated by hemispheres (`hemi`). Dataset: The dataset can be loaded using the following command: ```python networks = ( seaborn.load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") ) ``` Requirements: 1. **Preprocess the Dataset:** - Stack the dataset to a long format. - Group by `timepoint`, `network`, and `hemi`, then compute the mean. - Unstack the data based on the `network` level. - Reset the index and filter the dataset for `timepoint` less than 100. 2. **Create a Multi-Panel Plot:** - Use `seaborn.objects.Plot` to create a multi-panel plot with the following specifications: - Pair the x-axes as `[\\"5\\", \\"8\\", \\"12\\", \\"15\\"]` and y-axes as `[\\"6\\", \\"13\\", \\"16\\"]`. - Set the layout size to (8, 5). - Share x and y axes across the panels. - Use `so.Paths` to plot the data. - Customize the plot with a linewidth of 1 and alpha transparency of 0.8. - Color the paths by the `hemi` variable. Constraints: - Ensure that the code is efficient and readable. - You are only allowed to use Seaborn and related libraries for plotting. - Use the `matplotlib` backend for displaying the plot. Input and Output: - **Input:** - None. Use the provided dataset command to load data within your script. - **Output:** - Multi-panel plot as specified above, displayed inline using `matplotlib`. Example: ```python import seaborn.objects as so import seaborn as sns # Load and preprocess the dataset networks = ( sns.load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Display the plot import matplotlib.pyplot as plt plt.show() ``` This problem assesses your ability to manipulate multilevel indexed data and create complex visualizations using the Seaborn objects interface.","solution":"import seaborn.objects as so import seaborn as sns import pandas as pd def preprocess_and_plot_brain_networks(): # Load and preprocess the dataset networks = ( sns.load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Display the plot import matplotlib.pyplot as plt plt.show() return p, networks"},{"question":"# Question: You are given the `titanic` dataset containing information about Titanic passengers, which can be loaded using Seaborn\'s `load_dataset` function. Your task is to use the Seaborn objects (`seaborn.objects` module) to create and customize specific plots as described below. **Instructions:** 1. Import the necessary modules: ```python import seaborn.objects as so from seaborn import load_dataset ``` 2. Load the `titanic` dataset and sort it by the `alive` column in descending order: ```python titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) ``` 3. Create a bar plot that shows the count of passengers by class, with different colors for each gender (`sex`). Use stacking to eliminate overlap between bars. 4. Create a faceted histogram plot that shows the age distribution of passengers, separate by gender (`sex`) and differentiated by their survival status (`alive`). Use a bin width of 10 for the histogram and apply stacking. Your function should be defined as follows: ```python def create_titanic_plots(): # Step 1: Import modules import seaborn.objects as so from seaborn import load_dataset # Step 2: Load and prepare dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Step 3: Create bar plot bar_plot = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()) # Step 4: Create faceted histogram plot hist_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Display the plots bar_plot.show() hist_plot.show() ``` **Constraints:** - You must use the `seaborn.objects` module to create the plots. - Ensure the plots are properly faceted and colored as specified. - The histogram should use a bin width of 10. **Expected Output:** Executing the function `create_titanic_plots()` should display the two specified plots with the correct customizations.","solution":"def create_titanic_plots(): # Step 1: Import modules import seaborn.objects as so from seaborn import load_dataset # Step 2: Load and prepare dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Step 3: Create bar plot bar_plot = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()) # Step 4: Create faceted histogram plot hist_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) # Display the plots bar_plot.show() hist_plot.show()"},{"question":"**Objective:** You are provided with a dataset \'flights\' which contains data about the number of passengers on flights across different months and years. Your task is to use seaborn\'s `so.Plot` functionality to visualize this data in different ways to demonstrate your understanding of the plotting and counting mechanisms provided by seaborn. **Dataset:** Use the \'flights\' dataset available in seaborn, which can be loaded using: ```python flights = seaborn.load_dataset(\\"flights\\") ``` The dataset contains the following columns: - **year**: The year of the flight. - **month**: The month of the flight. - **passengers**: The number of passengers. **Tasks:** 1. **Basic Count Plot:** Create a bar plot to show the number of occurrences of each month across all the years. This plot should display the count of months on the y-axis. 2. **Grouped Count Plot:** Group the counts by year and create a bar plot to visualize the distribution of months for each year. Use different colors to represent different years. 3. **Count Without Binning:** Create a plot that displays the count of unique values in the \'passengers\' column. Ensure that the counts are correctly represented without binning the numeric data. 4. **Vertical Count Assignment:** Create a bar plot where counts of unique values in the \'year\' column are assigned to the x-axis. **Input:** None. Import the dataset within your code using seaborn\'s `load_dataset` method. **Output:** - Four plots as described above. **Constraints:** - Use only the `so.Plot` and its associated methods for plotting. - Ensure your code is well-documented with comments explaining each step. **Examples:** ```python import seaborn.objects as so import seaborn as sns # Load the dataset flights = sns.load_dataset(\\"flights\\") # Task 1: Basic Count Plot so.Plot(flights, x=\\"month\\").add(so.Bar(), so.Count()).show() # Task 2: Grouped Count Plot so.Plot(flights, x=\\"month\\", color=\\"year\\").add(so.Bar(), so.Count(), so.Dodge()).show() # Task 3: Count Without Binning so.Plot(flights, x=\\"passengers\\").add(so.Bar(), so.Count()).show() # Task 4: Vertical Count Assignment so.Plot(flights, y=\\"year\\").add(so.Bar(), so.Count()).show() ``` Ensure your plots are correctly rendered and exhibit the intended visualizations for accurate data interpretation.","solution":"import seaborn.objects as so import seaborn as sns # Load the dataset flights = sns.load_dataset(\\"flights\\") # Task 1: Basic Count Plot def basic_count_plot(): Create a bar plot to show the number of occurrences of each month across all the years. This plot displays the count of months on the y-axis. plot = so.Plot(flights, x=\\"month\\").add(so.Bar(), so.Count()) plot.show() # Task 2: Grouped Count Plot def grouped_count_plot(): Group the counts by year and create a bar plot to visualize the distribution of months for each year. Use different colors to represent different years. plot = so.Plot(flights, x=\\"month\\", color=\\"year\\").add(so.Bar(), so.Count(), so.Dodge()) plot.show() # Task 3: Count Without Binning def count_without_bin(): Create a plot that displays the count of unique values in the \'passengers\' column. Ensure that the counts are correctly represented without binning the numeric data. plot = so.Plot(flights, x=\\"passengers\\").add(so.Bar(), so.Count()) plot.show() # Task 4: Vertical Count Assignment def vertical_count_plot(): Create a bar plot where counts of unique values in the \'year\' column are assigned to the x-axis. plot = so.Plot(flights, y=\\"year\\").add(so.Bar(), so.Count()) plot.show()"},{"question":"**Coding Assessment Question:** # File System Auditor In this assessment, you are required to implement a `FileSystemAuditor` class in Python that simulates certain file system operations using a hypothetical audit system. The class should perform operations such as listing files in a directory, reading file contents, and deleting files, while logging these operations through an audit system. # Objectives 1. **Implement a Class:** `FileSystemAuditor` 2. **Methods to Implement:** - `list_files(directory: str) -> List[str]` - `read_file(file_path: str) -> str` - `delete_file(file_path: str) -> None` # Method Details 1. **list_files(directory: str) -> List[str]** - **Description:** Lists all files in the specified directory. - **Input:** `directory` - A string representing the directory path. - **Output:** A list of strings where each string is a file name in the directory. - **Audit Event:** `\\"os.listdir\\"` with argument `directory`. 2. **read_file(file_path: str) -> str** - **Description:** Reads the content of a specified file. - **Input:** `file_path` - A string representing the file path. - **Output:** The content of the file as a string. - **Audit Event:** `\\"open\\"` with argument `file_path`. 3. **delete_file(file_path: str) -> None** - **Description:** Deletes the specified file. - **Input:** `file_path` - A string representing the file path. - **Output:** None. - **Audit Event:** `\\"os.remove\\"` with argument `file_path`. # Additional Requirements - Implement an audit log to track events. The log should be a list of tuples, where each tuple contains the audit event and its arguments. - Ensure proper exception handling for scenarios like file not found or permission denied. - Demonstrate the usage of the `FileSystemAuditor` class by performing a series of operations and printing the audit log at the end. # Constraints - You are allowed to use the standard library for file operations. - All methods should handle exceptions gracefully and audit them as necessary. # Example Usage ```python auditor = FileSystemAuditor() # Listing files in a directory files = auditor.list_files(\'/path/to/directory\') print(files) # Reading a file content = auditor.read_file(\'/path/to/directory/file.txt\') print(content) # Deleting a file auditor.delete_file(\'/path/to/directory/file.txt\') # Print audit log print(auditor.audit_log) ``` # Expected Output ```python [\'file1.txt\', \'file2.txt\'] \'This is the content of the file.\' None [(\'os.listdir\', \'/path/to/directory\'), (\'open\', \'/path/to/directory/file.txt\'), (\'os.remove\', \'/path/to/directory/file.txt\')] ``` Implement the `FileSystemAuditor` class below: ```python class FileSystemAuditor: def __init__(self): self.audit_log = [] def audit(self, event, *args): self.audit_log.append((event, *args)) def list_files(self, directory): try: from os import listdir files = listdir(directory) self.audit(\\"os.listdir\\", directory) return files except Exception as e: self.audit(\\"error.os.listdir\\", directory, str(e)) return [] def read_file(self, file_path): try: with open(file_path, \'r\') as f: content = f.read() self.audit(\\"open\\", file_path) return content except Exception as e: self.audit(\\"error.open\\", file_path, str(e)) return \\"\\" def delete_file(self, file_path): try: from os import remove remove(file_path) self.audit(\\"os.remove\\", file_path) except Exception as e: self.audit(\\"error.os.remove\\", file_path, str(e)) ``` Ensure your implementation follows the structure and requirements specified above. Good luck!","solution":"import os class FileSystemAuditor: def __init__(self): self.audit_log = [] def audit(self, event, *args): self.audit_log.append((event, *args)) def list_files(self, directory): try: files = os.listdir(directory) self.audit(\\"os.listdir\\", directory) return files except Exception as e: self.audit(\\"error.os.listdir\\", directory, str(e)) return [] def read_file(self, file_path): try: with open(file_path, \'r\') as f: content = f.read() self.audit(\\"open\\", file_path) return content except Exception as e: self.audit(\\"error.open\\", file_path, str(e)) return \\"\\" def delete_file(self, file_path): try: os.remove(file_path) self.audit(\\"os.remove\\", file_path) except Exception as e: self.audit(\\"error.os.remove\\", file_path, str(e))"},{"question":"**Challenging PyTorch Testing Functions Implementation** # Objective You are tasked with implementing a function to verify the properties of tensors using functions from the `torch.testing` module. This requires a deep understanding of tensor creation, properties, and validation of tensor closeness. # Problem Statement Write a function `verify_tensors` that takes a list of dictionaries as input, where each dictionary contains information about two tensors and a set of conditions to verify using the `torch.testing` module. The function should return a list of boolean values indicating whether each pair of tensors meets their respective conditions. # Function Signature ```python def verify_tensors(tensor_cases: List[Dict[str, Any]]) -> List[bool]: ... ``` # Input - `tensor_cases`: A list of dictionaries. Each dictionary contains: - `tensor_a` (torch.Tensor): The first tensor. - `tensor_b` (torch.Tensor): The second tensor. - `conditions` (str): A string specifying the verification condition. It can be one of `\\"close\\"` or `\\"allclose\\"`. # Output - A list of boolean values, where each value corresponds to whether the pair of tensors in the respective dictionary met the given conditions. # Constraints 1. Both `tensor_a` and `tensor_b` must be PyTorch tensors. 2. The `conditions` string must be either `\\"close\\"` or `\\"allclose\\"`. 3. Use the `torch.testing` functions `assert_close` or `assert_allclose` to verify the tensor properties. # Example ```python import torch case_1 = { \'tensor_a\': torch.tensor([1.0, 2.0, 3.0]), \'tensor_b\': torch.tensor([1.0, 2.0, 3.0]), \'conditions\': \'close\', } case_2 = { \'tensor_a\': torch.tensor([1.0, 2.0, 3.0]), \'tensor_b\': torch.tensor([1.01, 2.01, 3.01]), \'conditions\': \'allclose\', } tensor_cases = [case_1, case_2] output = verify_tensors(tensor_cases) print(output) # Should print [True, True] if both conditions are satisfied. ``` # Notes - Your implementation should handle cases where the tensors do not meet the specified conditions by returning `False` for those cases. - Consider edge cases such as: - Tensors with different shapes. - Tensors with different data types. - Tiny differences in tensor values where strict equality might not hold. - Ensure your solution is efficient and leverages the provided `torch.testing` functions effectively. # Constraints - You should not use any additional libraries apart from `torch` and `typing`. - The solution should be implementable within a reasonable time frame (a few seconds for typical use cases). # Implementation Help The `torch.testing` module provided functions are useful for asserting the closeness of tensors. Heres a brief explanation of what they might do: 1. `assert_close(a, b, rtol, atol)`: Asserts that tensor `a` is close to tensor `b` within relative tolerance `rtol` and absolute tolerance `atol`. 2. `assert_allclose(a, b, rtol, atol)`: Similar to `assert_close`, but can apply to all elements in the tensors.","solution":"from typing import List, Dict, Any import torch import torch.testing def verify_tensors(tensor_cases: List[Dict[str, Any]]) -> List[bool]: results = [] for case in tensor_cases: tensor_a = case[\'tensor_a\'] tensor_b = case[\'tensor_b\'] conditions = case[\'conditions\'] try: if conditions == \'close\': torch.testing.assert_close(tensor_a, tensor_b) elif conditions == \'allclose\': torch.testing.assert_allclose(tensor_a, tensor_b) else: results.append(False) continue results.append(True) except AssertionError: results.append(False) return results"},{"question":"Objective: The objective of this coding assessment is to evaluate your understanding and ability to utilize the `fileinput` module in Python for iterating over multiple input files, custom file handling, and performing specific file operations. Problem Statement: Write a Python function `process_files(files: list, keyword: str, encoding: str = \'utf-8\') -> int` that processes a list of input files, counts the occurrences of a given keyword, and updates lines containing that keyword by appending \\"FOUND\\" to them. The function must also create a backup of the original files before making modifications. Requirements: 1. **Function Signature**: ```python def process_files(files: list, keyword: str, encoding: str = \'utf-8\') -> int: ``` 2. **Parameters**: - `files`: A list of strings representing file paths to be processed. - `keyword`: A string representing the keyword to search for in the files. - `encoding`: An optional string specifying the encoding of the files (default is `\'utf-8\'`). 3. **Return**: - An integer representing the total count of keyword occurrences across all files. 4. **Constraints**: - Each file is guaranteed to exist and be readable. - Each file will be opened in text mode as specified by `encoding`. - Modifications must be done in place with a backup of each original file created. 5. **Performance Requirements**: - The function must handle files efficiently, even for large text files. Example Usage: ```python files = [\'file1.txt\', \'file2.txt\'] keyword = \'example\' encoding = \'utf-8\' count = process_files(files, keyword, encoding) print(count) # Should print the total number of occurrences of \'example\' across the files ``` Additional Considerations: - Each line containing the keyword should have \\"FOUND\\" appended only once, even if the function is called multiple times. - Use appropriate exception handling to ensure that files are properly closed even if an error occurs during processing. Constraints and Limitations: - Do not use any external libraries other than the standard Python libraries. - You must leverage the `fileinput` module\'s features such as in-place editing and custom opening hooks for this task.","solution":"import fileinput import shutil from pathlib import Path def process_files(files: list, keyword: str, encoding: str = \'utf-8\') -> int: Processes the given list of files, counts the occurrences of the specified keyword, and updates lines containing that keyword by appending \\"FOUND\\" to them. Parameters: - files (list): List of file paths to be processed. - keyword (str): Keyword to search for in the files. - encoding (str): Encoding of the files (default is \'utf-8\'). Returns: - int: Total count of keyword occurrences across all files. total_count = 0 for file_path in files: path = Path(file_path) backup_path = path.with_suffix(path.suffix + \'.bak\') # Create a backup of the file shutil.copy2(file_path, backup_path) # Initialize count for the current file file_count = 0 # Process the file in-place with fileinput.FileInput(file_path, inplace=True, backup=\'.bak\', mode=\'r\', encoding=encoding) as file: for line in file: occurrences = line.count(keyword) if occurrences > 0: file_count += occurrences if \'FOUND\' not in line: # Ensure \'FOUND\' is not already in the line line = line.rstrip(\'n\') + \' FOUNDn\' print(line, end=\'\') total_count += file_count return total_count"},{"question":"**Objective:** To demonstrate your understanding of the `seaborn` library, specifically in setting plot contexts, scaling font sizes, and customizing plot parameters. **Problem:** Write a function named `custom_seaborn_plot` that takes the following arguments: 1. `x`: A list of integers representing the x-axis coordinates. 2. `y`: A list of integers representing the y-axis coordinates. 3. `context`: A string indicating the context setting for the plot (e.g., \\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"). 4. `font_scale`: A float that scales the font elements relative to the current context. 5. `line_width`: A float that sets the linewidth of the lines in the plot. The function should: 1. Set the `seaborn` context using `context` and scale the font using `font_scale`. 2. Customize the linewidth of the lines using `line_width`. 3. Create and display a line plot using the `x` and `y` coordinates. **Expected Input and Output:** - Input: - `x`: [0, 1, 2] - `y`: [1, 3, 2] - `context`: \\"notebook\\" - `font_scale`: 1.25 - `line_width`: 3.0 - Output: A displayed seaborn line plot with the specified customizations. **Constraints:** - You can assume that the lengths of `x` and `y` are equal. - Ensure the `seaborn` library is properly imported and used. **Example:** ```python def custom_seaborn_plot(x, y, context, font_scale, line_width): # Your implementation here # Example usage custom_seaborn_plot([0, 1, 2], [1, 3, 2], \'notebook\', 1.25, 3.0) ``` Make sure your implementation adheres to the requirements and constraints specified. Your function should demonstrate effective use of the seaborn library\'s capabilities for plot customization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plot(x, y, context, font_scale, line_width): Creates a seaborn line plot with the specified context, font scale, and line width. Parameters: x (list of ints): x-axis coordinates. y (list of ints): y-axis coordinates. context (str): Context setting for the plot (\'paper\', \'notebook\', \'talk\', \'poster\'). font_scale (float): Scaling factor for the font size. line_width (float): Width of the lines in the plot. # Set the seaborn context and font scale sns.set_context(context, font_scale=font_scale) # Create and display the plot with the specified line width plt.figure() sns.lineplot(x=x, y=y, linewidth=line_width) plt.show()"},{"question":"# File Backup and Restoration System **Objective:** Create a Python script that can backup and restore directories using the `shutil` module. **Description:** You are required to implement two functions within a Python script: Function 1: `backup_directory(src, dest_archive)` - **Input:** - `src` (str): The path to the source directory you want to back up. - `dest_archive` (str): The path (including the name) where the archive file will be created. - **Output:** - Returns the path to the created archive file. - **Functionality:** - This function should create an archive (using gzip compression) of the entire `src` directory. - The archive should include all subdirectories and files within `src`. Function 2: `restore_directory(archive, dest_dir)` - **Input:** - `archive` (str): The path to the archive file. - `dest_dir` (str): The path where the content of the archive should be extracted. - **Output:** - Returns the path to the directory where the archive is extracted. - **Functionality:** - This function should extract the entire content of the archive to the `dest_dir` path. **Constraints and Requirements:** 1. Assume the paths provided are valid and have the necessary read/write permissions. 2. Handle any exceptions that might occur during file operations gracefully by printing an appropriate error message. 3. Use `shutil.make_archive` for creating the archive in the `backup_directory` function. 4. Use `shutil.unpack_archive` for extracting the archive in the `restore_directory` function. 5. Ensure that symbolic links, if any, are preserved during the backup and restored correctly. **Example Usage:** ```python src_directory = \\"/path/to/source_directory\\" backup_path = \\"/path/to/backup/archive_name\\" # Backing up the directory backup = backup_directory(src_directory, backup_path) print(f\\"Backup created at: {backup}\\") # Restoring the directory restoration_path = \\"/path/to/restore_directory\\" restored = restore_directory(backup, restoration_path) print(f\\"Directory restored to: {restored}\\") ``` Implement the two functions, `backup_directory` and `restore_directory`, and showcase their functionality by backing up and restoring a chosen directory. **Note:** Remember to import the necessary modules at the beginning of your script. # Submission: Submit your Python script containing the implementation of `backup_directory` and `restore_directory` functions, along with a demonstration of their usage.","solution":"import shutil import os def backup_directory(src, dest_archive): Creates an archive of the source directory. Args: src (str): The source directory to back up. dest_archive (str): The destination path (including the name) where the archive will be created. Returns: str: The path to the created archive file. try: # Make archive archive_path = shutil.make_archive(dest_archive, \'gztar\', src) return archive_path except Exception as e: print(f\\"Error during backup: {e}\\") return None def restore_directory(archive, dest_dir): Restores a directory from the archive. Args: archive (str): The path to the archive file. dest_dir (str): The destination directory where contents will be extracted. Returns: str: The path to the directory where contents are extracted. try: # Unpacking the archive into destination directory shutil.unpack_archive(archive, dest_dir) return dest_dir except Exception as e: print(f\\"Error during restoration: {e}\\") return None"},{"question":"**Coding Assessment Question** # Objective Implement a custom context manager that ensures the proper management of multiple resources, allows for optional resource management, and supports both synchronous and asynchronous operations. # Problem Statement You are required to implement a function `manage_resources` that uses both synchronous and asynchronous context managers to manage a list of resources. This function should ensure that all resources are properly acquired and released, even if an error occurs during the acquisition or usage of the resources. # Requirements - Use the `ExitStack` class to manage multiple context managers. - Use `contextlib.contextmanager` and `contextlib.asynccontextmanager` to create custom context managers. - Ensure that both synchronous and asynchronous resources are supported and managed correctly. - Use utility context managers like `contextlib.closing`, `contextlib.nullcontext`, and `contextlib.suppress` where appropriate. # Specifications Function Signature ```python def manage_resources(resources: list) -> None: pass ``` Input - `resources` (list): A list of resource configurations. Each configuration is a dictionary with the following possible keys: - `type` (str): The type of resource, either \\"sync\\" or \\"async\\". - `resource` (str): The identifier of the resource. - `optional` (bool): Whether the resource is optional or not (default: False). Example Resource Configuration ```python [ {\\"type\\": \\"sync\\", \\"resource\\": \\"file.txt\\", \\"optional\\": False}, {\\"type\\": \\"sync\\", \\"resource\\": \\"log.txt\\", \\"optional\\": True}, {\\"type\\": \\"async\\", \\"resource\\": \\"http_session\\", \\"optional\\": False} ] ``` # Constraints & Considerations 1. Use `contextlib.contextmanager` to create a synchronous context manager for managing file resources. 2. Use `contextlib.asynccontextmanager` to create an asynchronous context manager for managing HTTP session resources. 3. Use `contextlib.ExitStack` to manage the context managers programmatically. 4. The management should ensure that all acquired resources are released appropriately, even in the case of exceptions. 5. Use `contextlib.nullcontext` for optional resources. 6. For synchronous resources, simulate the acquisition of a resource with an `open` call. 7. For asynchronous resources, simulate the acquisition with an asynchronous function (e.g., `aiohttp.ClientSession`). # Example Usage ```python # Example Usage import asyncio async def main(): resources = [ {\\"type\\": \\"sync\\", \\"resource\\": \\"file.txt\\", \\"optional\\": False}, {\\"type\\": \\"sync\\", \\"resource\\": \\"log.txt\\", \\"optional\\": True}, {\\"type\\": \\"async\\", \\"resource\\": \\"http_session\\", \\"optional\\": False} ] await manage_resources(resources) # Run the main function asyncio.run(main()) ``` # Expected Output There is no return value. The function should manage the resources, ensuring proper acquisition and release. # Notes - Handle exceptions gracefully and ensure that all resources are released properly. - Use dummy implementations for resource acquisition (`open` for files and a mocked asynchronous function for HTTP sessions).","solution":"import contextlib import aiohttp import asyncio @contextlib.contextmanager def file_manager(file_name): print(f\\"Opening file {file_name}\\") try: with open(file_name, \'w\') as file: yield file finally: print(f\\"Closing file {file_name}\\") @contextlib.asynccontextmanager async def async_http_session(): session = aiohttp.ClientSession() print(\\"Opening HTTP session\\") try: yield session finally: await session.close() print(\\"Closing HTTP session\\") async def manage_resources(resources): async with contextlib.AsyncExitStack() as stack: for resource in resources: if resource[\'type\'] == \'sync\': if resource.get(\'optional\', False): mgr = contextlib.nullcontext() else: mgr = file_manager(resource[\'resource\']) stack.enter_context(mgr) elif resource[\'type\'] == \'async\': if resource.get(\'optional\', False): mgr = contextlib.nullcontext() else: mgr = async_http_session() await stack.enter_async_context(mgr)"},{"question":"Objective: Write a Python function to perform precise arithmetic operations on floating-point numbers, ensuring the correct handling of representation errors using the `decimal` module. Problem Statement: You are required to implement a function `precise_arithmetic_operation` that performs arithmetic operations (addition, subtraction, multiplication, and division) on two input floating-point numbers and returns the result with a specified number of decimal places. Function Signature: ```python def precise_arithmetic_operation(num1: float, num2: float, operation: str, precision: int) -> str: pass ``` Input: - `num1` (float): The first floating-point number. - `num2` (float): The second floating-point number. - `operation` (str): A string representing the operation to perform. It can be one of the following: `\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, `\\"divide\\"`. - `precision` (int): The number of decimal places to round the result to. Output: - Returns a string representing the result of the operation, rounded to the specified number of decimal places. Constraints: 1. The operation string will always be one of `\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, or `\\"divide\\"`. 2. The precision will be a non-negative integer (0 or more). 3. `num2` will not be zero if the operation is `\\"divide\\"`. Examples: ```python print(precise_arithmetic_operation(0.1, 0.2, \\"add\\", 10)) # Output: \\"0.3000000000\\" print(precise_arithmetic_operation(1.5, 0.5, \\"divide\\", 5)) # Output: \\"3.00000\\" print(precise_arithmetic_operation(3.3, 2.2, \\"subtract\\", 3)) # Output: \\"1.100\\" print(precise_arithmetic_operation(7.0, 8.0, \\"multiply\\", 4)) # Output: \\"56.0000\\" ``` Requirements: - Use the `decimal.Decimal` class to ensure high precision arithmetic. - Handle common floating-point errors and ensure the results are as accurate as possible given the specified precision. Notes: - You must not use the built-in floating point arithmetic operations directly for the main calculation steps. - Pay attention to performance for the given precision value, especially for large inputs. - Avoid importing any unnecessary modules.","solution":"from decimal import Decimal, getcontext def precise_arithmetic_operation(num1: float, num2: float, operation: str, precision: int) -> str: Performs precise arithmetic operations on two floating-point numbers and returns the result as a string rounded to the specified number of decimal places. # Set precision getcontext().prec = precision + 2 # Adding extra precision to avoid rounding issues # Convert floats to Decimals dec_num1 = Decimal(str(num1)) dec_num2 = Decimal(str(num2)) # Perform the specified operation if operation == \\"add\\": result = dec_num1 + dec_num2 elif operation == \\"subtract\\": result = dec_num1 - dec_num2 elif operation == \\"multiply\\": result = dec_num1 * dec_num2 elif operation == \\"divide\\": result = dec_num1 / dec_num2 else: raise ValueError(f\\"Unknown operation: {operation}\\") # Return the result rounded to the specified precision return str(result.quantize(Decimal(\'1.\' + \'0\' * precision)))"},{"question":"# Asynchronous Order Management System You are tasked with simulating an order management system for an online store using Python\'s `asyncio` package. Your system must process multiple orders concurrently, handle some orders with delays (simulating payment processing time), and implement timeout handling for orders that take too long. Requirements: 1. **Order Processing Function**: - Create a function `process_order(order_id: int, delay: float) -> str` which simulates order processing. - This function should print `Order {order_id} processing started` at the beginning and print `Order {order_id} processing completed` once the order is fully processed. - Use `await asyncio.sleep(delay)` to simulate the delay. 2. **Main Function**: - Create a function `main(order_count: int, max_delay: float, timeout: float)`. - This function should create and start tasks for processing all orders concurrently. - Each order should have a random delay time between 0 to `max_delay`. - Use `asyncio.create_task()` to start each order processing task and store them in a list. - Use `asyncio.wait_for()` to enforce a maximum processing time (`timeout`) for each order. - Print `Order {order_id} timed out` if an order\'s processing exceeds the `timeout` duration. 3. **Running the Simulation**: - Use `asyncio.run()` to run the `main()` function with appropriate arguments. Constraints: - `order_count`: The number of orders to process (1  `order_count`  100). - `max_delay`: The maximum delay that any order processing might take (0.1  `max_delay`  5.0) seconds. - `timeout`: The maximum allowed processing time for each order (0.1  `timeout`  10.0) seconds. Expected Input and Output ```python import random import asyncio async def process_order(order_id: int, delay: float) -> str: # Your implementation here pass async def main(order_count: int, max_delay: float, timeout: float): # Your implementation here pass # Example on how to run asyncio.run(main(order_count=5, max_delay=3.0, timeout=2.5)) ``` Expected Output: ``` Order 1 processing started Order 2 processing started Order 3 processing started Order 4 processing started Order 5 processing started Order 1 processing completed Order 2 timed out Order 3 processing completed Order 4 timed out Order 5 processing completed ``` Notes: - The delays and timeouts should make the printed order of completion and timeouts vary, reflecting true asynchronous behavior. - Ensure proper exception handling where necessary to manage the order timeouts. Good Luck!","solution":"import asyncio import random async def process_order(order_id: int, delay: float) -> str: print(f\\"Order {order_id} processing started\\") await asyncio.sleep(delay) print(f\\"Order {order_id} processing completed\\") return f\\"Order {order_id} processed\\" async def main(order_count: int, max_delay: float, timeout: float): tasks = [] for order_id in range(1, order_count + 1): delay = random.uniform(0, max_delay) task = asyncio.create_task(process_order(order_id, delay)) tasks.append((order_id, task)) for order_id, task in tasks: try: await asyncio.wait_for(task, timeout) except asyncio.TimeoutError: print(f\\"Order {order_id} timed out\\")"},{"question":"# Advanced Coding Assessment Question: Utilizing the `faulthandler` Module **Objective**: Demonstrate your understanding of the `faulthandler` module by writing functions that enable, use, and disable fault handlers to capture and manage Python tracebacks effectively. **Problem Statement**: Write a program that simulates a fault (e.g., division by zero) and captures the traceback using the `faulthandler` module. The program should: 1. Enable the fault handler to capture tracebacks for fatal signals. 2. Register a user signal to dump the traceback when a specific signal (e.g., `SIGUSR1`) is received. 3. Simulate a division by zero error and capture the traceback. 4. Unregister the user signal and disable the fault handler. 5. Demonstrate dumping the traceback after a timeout period. **Requirements**: 1. Implement the following functions: - `setup_fault_handler(file)`: This function enables the fault handler and registers a user signal `SIGUSR1` to dump tracebacks to the provided file. - `simulate_division_by_zero()`: This function simulates a division by zero error. - `teardown_fault_handler()`: This function disables the fault handler and unregisters the user signal. - `dump_traceback_with_timeout(timeout, file)`: This function dumps the traceback after a specified timeout period. 2. Simulate a runtime scenario demonstrating the use of these functions, ensuring to showcase the output of tracebacks captured during faults and on signal. **Constraints**: - Assume the environment is a Unix-like system where `SIGUSR1` is available. - Use only the `faulthandler` module functions for managing tracebacks. - Handle file descriptors correctly to ensure tracebacks are dumped into the correct file. **Input Format**: - No user input is needed; the functions should demonstrate the requirements by being called within a script. **Output Format**: - The program should output the captured tracebacks to the specified files. **Performance Requirements**: - Efficiently manage file resources to ensure the correct handling of tracebacks and prevent file descriptor issues. ```python import os import signal import faulthandler import time def setup_fault_handler(file): Enable the faulthandler and register the SIGUSR1 signal to dump tracebacks to the specified file. :param file: the file object where the tracebacks should be dumped faulthandler.enable(file) faulthandler.register(signal.SIGUSR1, file=file) def simulate_division_by_zero(): Simulate a division by zero error to trigger a traceback. x = 1 / 0 # This will trigger a ZeroDivisionError def teardown_fault_handler(): Disable the faulthandler and unregister the SIGUSR1 signal. faulthandler.unregister(signal.SIGUSR1) faulthandler.disable() def dump_traceback_with_timeout(timeout, file): Dump the traceback after the specified timeout. :param timeout: the timeout period in seconds :param file: the file object where the tracebacks should be dumped faulthandler.dump_traceback_later(timeout, file=file) time.sleep(timeout + 1) # Ensure enough time is given for the dump to occur faulthandler.cancel_dump_traceback_later() # Demonstrate the usage if __name__ == \\"__main__\\": log_file = open(\'traceback.log\', \'w\') try: setup_fault_handler(log_file) dump_traceback_with_timeout(2, log_file) # Simulate the fault after the timeout simulate_division_by_zero() except ZeroDivisionError: print(\\"Caught a division by zero error!\\") finally: teardown_fault_handler() log_file.close() ``` Ensure you correctly handle file resources and the setup/teardown of the `faulthandler` to capture the required tracebacks for debugging.","solution":"import os import signal import faulthandler import time def setup_fault_handler(file): Enable the faulthandler and register the SIGUSR1 signal to dump tracebacks to the specified file. :param file: the file object where the tracebacks should be dumped faulthandler.enable(file) faulthandler.register(signal.SIGUSR1, file=file) def simulate_division_by_zero(): Simulate a division by zero error to trigger a traceback. x = 1 / 0 # This will trigger a ZeroDivisionError def teardown_fault_handler(): Disable the faulthandler and unregister the SIGUSR1 signal. faulthandler.unregister(signal.SIGUSR1) faulthandler.disable() def dump_traceback_with_timeout(timeout, file): Dump the traceback after the specified timeout. :param timeout: the timeout period in seconds :param file: the file object where the tracebacks should be dumped faulthandler.dump_traceback_later(timeout, file=file) time.sleep(timeout + 1) # Ensure enough time is given for the dump to occur faulthandler.cancel_dump_traceback_later() # Demonstrate the usage if __name__ == \\"__main__\\": log_file = open(\'traceback.log\', \'w\') try: setup_fault_handler(log_file) dump_traceback_with_timeout(2, log_file) # Simulate the fault after the timeout simulate_division_by_zero() except ZeroDivisionError: print(\\"Caught a division by zero error!\\") finally: teardown_fault_handler() log_file.close()"},{"question":"# Question: **Gaussian Mixture Model and Bayesian Gaussian Mixture Model Implementation** **Objective:** You are tasked with implementing and comparing Gaussian Mixture Models (GMM) and Bayesian Gaussian Mixture Models (BGMM) using the scikit-learn library. Your solution should demonstrate your understanding of fitting models, predicting data, and evaluating the number of components. **Problem Description:** Given a dataset, implement functions that: 1. Fit a Gaussian Mixture Model (GMM) and a Bayesian Gaussian Mixture Model (BGMM) to the data. 2. Predict the clusters for given test data using the models. 3. Evaluate and compare the models using Bayesian Information Criterion (BIC) for GMM and variational inference for BGMM. 4. Select the optimal number of components for GMM using BIC. **Functions to Implement:** 1. `fit_gaussian_mixture(data: np.ndarray, n_components: int, covariance_type: str) -> GaussianMixture`: - **Input:** - `data`: A 2D numpy array of shape (n_samples, n_features) representing the training data. - `n_components`: An integer representing the number of mixture components. - `covariance_type`: A string representing the type of covariance (\'full\', \'tied\', \'diag\', \'spherical\'). - **Output:** - A fitted `GaussianMixture` object. 2. `fit_bayesian_gaussian_mixture(data: np.ndarray, n_components: int, weight_concentration_prior: float) -> BayesianGaussianMixture`: - **Input:** - `data`: A 2D numpy array of shape (n_samples, n_features) representing the training data. - `n_components`: An integer representing the upper bound on the number of mixture components. - `weight_concentration_prior`: A float representing the concentration parameter. - **Output:** - A fitted `BayesianGaussianMixture` object. 3. `predict_clusters(model, test_data: np.ndarray) -> np.ndarray`: - **Input:** - `model`: A fitted Gaussian Mixture Model or Bayesian Gaussian Mixture Model. - `test_data`: A 2D numpy array of shape (n_samples, n_features) representing the test data. - **Output:** - A 1D numpy array of shape (n_samples,) representing the predicted cluster for each test sample. 4. `evaluate_gmm_bic(data: np.ndarray, max_components: int, covariance_type: str) -> int`: - **Input:** - `data`: A 2D numpy array of shape (n_samples, n_features) representing the training data. - `max_components`: An integer representing the maximum number of components to evaluate. - `covariance_type`: A string representing the type of covariance (\'full\', \'tied\', \'diag\', \'spherical\'). - **Output:** - An integer representing the optimal number of components based on BIC. **Constraints:** - Use the scikit-learn library. - Ensure your code is optimized for performance on datasets with up to 10000 samples and 20 features. - Document your functions with appropriate docstrings. **Example Usage:** ```python import numpy as np data = np.random.rand(500, 2) test_data = np.random.rand(100, 2) # Fit GMM and predict clusters gmm = fit_gaussian_mixture(data, 3, \'full\') clusters = predict_clusters(gmm, test_data) # Fit BGMM and predict clusters bgmm = fit_bayesian_gaussian_mixture(data, 5, 1e-3) clusters_bgmm = predict_clusters(bgmm, test_data) # Evaluate GMM using BIC optimal_components = evaluate_gmm_bic(data, 10, \'full\') print(f\'Optimal number of components for GMM: {optimal_components}\') ``` **Note:** - Make sure to handle edge cases, such as empty datasets or invalid `n_components` values. - Provide a brief explanation of your solution and the comparison results between GMM and BGMM.","solution":"import numpy as np from sklearn.mixture import GaussianMixture, BayesianGaussianMixture def fit_gaussian_mixture(data: np.ndarray, n_components: int, covariance_type: str) -> GaussianMixture: Fits a Gaussian Mixture Model (GMM) to the data. Args: - data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). - n_components (int): The number of mixture components. - covariance_type (str): The type of covariance (\'full\', \'tied\', \'diag\', \'spherical\'). Returns: - GaussianMixture: A fitted GaussianMixture object. gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type) gmm.fit(data) return gmm def fit_bayesian_gaussian_mixture(data: np.ndarray, n_components: int, weight_concentration_prior: float) -> BayesianGaussianMixture: Fits a Bayesian Gaussian Mixture Model (BGMM) to the data. Args: - data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). - n_components (int): The upper bound on the number of mixture components. - weight_concentration_prior (float): The concentration parameter. Returns: - BayesianGaussianMixture: A fitted BayesianGaussianMixture object. bgmm = BayesianGaussianMixture(n_components=n_components, weight_concentration_prior_type=\'dirichlet_process\', weight_concentration_prior=weight_concentration_prior) bgmm.fit(data) return bgmm def predict_clusters(model, test_data: np.ndarray) -> np.ndarray: Predicts the clusters for given test data using a fitted model. Args: - model: A fitted Gaussian Mixture Model or Bayesian Gaussian Mixture Model. - test_data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). Returns: - np.ndarray: A 1D numpy array of shape (n_samples,) representing the predicted cluster for each test sample. return model.predict(test_data) def evaluate_gmm_bic(data: np.ndarray, max_components: int, covariance_type: str) -> int: Evaluates and selects the optimal number of components for GMM using Bayesian Information Criterion (BIC). Args: - data (np.ndarray): A 2D numpy array of shape (n_samples, n_features). - max_components (int): The maximum number of components to evaluate. - covariance_type (str): The type of covariance (\'full\', \'tied\', \'diag\', \'spherical\'). Returns: - int: The optimal number of components based on BIC. best_bic = np.inf best_n_components = 1 for n_components in range(1, max_components+1): gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type) gmm.fit(data) bic = gmm.bic(data) if bic < best_bic: best_bic = bic best_n_components = n_components return best_n_components"},{"question":"# Task Implement a Python function `verify_file_integrity(filepath, hash_type, expected_digest)` that: 1. Computes the hash of a file using the specified hash algorithm. 2. Verifies if the computed hash matches the expected digest. # Function Signature ```python import hashlib def verifyFileIntegrity(filepath: str, hash_type: str, expected_digest: str) -> bool: pass ``` # Parameters - `filepath` (str): The path to the file whose integrity needs to be verified. - `hash_type` (str): The type of hash algorithm to use (e.g., \'sha256\', \'md5\', \'blake2b\', etc.). The algorithm must be available in `hashlib.algorithms_guaranteed`. - `expected_digest` (str): The expected hash digest in hexadecimal format. # Returns - `bool`: Returns `True` if the computed digest matches the expected digest, otherwise `False`. # Constraints - The function should raise a `ValueError` if the specified `hash_type` is not available in `hashlib.algorithms_guaranteed`. - Only byte-like data should be fed to the hash object. # Example ```python # Example usage: result = verifyFileIntegrity(\'example.txt\', \'sha256\', \'3d363ff7401e02026f4a4687d4863ced\') print(result) # Should print True if it matches, otherwise False ``` # Notes 1. Use buffered reading to handle large files efficiently. 2. Handle file reading exceptions gracefully. 3. Ensure that the hash computation and verification process is clear and self-contained. # Advanced Requirements (Optional) Enhance the `verify_file_integrity` function to: 1. Compute the hash of the file using multiple hash algorithms specified in a list. 2. Return a dictionary with the hash type as the key and the verification result (`True` or `False`) as the value. # Example for Advanced Requirements ```python # Example usage for advanced requirements: result = verify_file_integrity(\'example.txt\', [\'sha256\', \'md5\'], {\'sha256\': \'3d363ff7401e02026f4a4687d4863ced\', \'md5\': \'098f6bcd4621d373cade4e832627b4f6\'}) print(result) # Should print {\'sha256\': True, \'md5\': False} if it matches sha256 but not md5 ```","solution":"import hashlib def verifyFileIntegrity(filepath: str, hash_type: str, expected_digest: str) -> bool: if hash_type not in hashlib.algorithms_guaranteed: raise ValueError(f\\"Hash type {hash_type} is not supported.\\") try: hasher = hashlib.new(hash_type) except ValueError: raise ValueError(f\\"Hash type {hash_type} could not be initialized.\\") try: with open(filepath, \'rb\') as file: while chunk := file.read(8192): hasher.update(chunk) except FileNotFoundError: raise FileNotFoundError(f\\"The file at {filepath} could not be found.\\") except Exception as e: raise IOError(f\\"An error occurred while reading the file: {e}\\") computed_digest = hasher.hexdigest() return computed_digest == expected_digest"},{"question":"**Coding Assessment Question: Dataset Generation and Classification** **Objective**: Demonstrate your understanding of scikit-learn\'s sample generators and apply them to create datasets for classification tasks. Implement a function that generates several types of datasets and fits a classifier to each, evaluating its performance. **Task**: Write a function called `generate_and_classify_datasets` that: 1. Creates three different types of datasets using `make_classification`, `make_circles`, and `make_moons`. 2. Splits each dataset into training and testing sets (80% training, 20% testing). 3. Trains a Support Vector Machine (SVM) classifier on each training set. 4. Evaluates the classifier on each test set and returns the accuracy scores. # Function Signature ```python def generate_and_classify_datasets(random_state: int) -> dict: pass ``` # Input - `random_state` (int): The seed for random number generators to ensure reproducibility. # Output - A dictionary with three keys (`\'classification\'`, `\'circles\'`, `\'moons\'`). Each key maps to the accuracy score of the classifier on the respective test set. # Implementation Details 1. **Generate the datasets**: - Use `make_classification` to generate a dataset with 1000 samples, 20 features, 2 informative features, and 2 classes. - Use `make_circles` to generate a dataset with 1000 samples, noise of 0.1, and a factor of 0.3. - Use `make_moons` to generate a dataset with 1000 samples and noise of 0.1. 2. **Split the datasets**: - Use `train_test_split` from `sklearn.model_selection` to split each dataset into training and testing sets. 3. **Train the classifier**: - Use `SVC` from `sklearn.svm` to train an SVM classifier with default parameters on the training sets. 4. **Evaluate the classifier**: - Use `accuracy_score` from `sklearn.metrics` to evaluate the classifier on the test sets. # Constraints - Ensure reproducibility by using the provided `random_state` for all random operations. # Example Usage ```python result = generate_and_classify_datasets(random_state=42) print(result) # Expected output format: {\'classification\': 0.85, \'circles\': 0.9, \'moons\': 0.92} ``` **Note**: The accuracy values in the example output are for illustration only. Actual values will depend on the implementation and datasets generated.","solution":"from sklearn.datasets import make_classification, make_circles, make_moons from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score def generate_and_classify_datasets(random_state: int) -> dict: # Create the datasets X_classification, y_classification = make_classification( n_samples=1000, n_features=20, n_informative=2, n_classes=2, random_state=random_state ) X_circles, y_circles = make_circles( n_samples=1000, noise=0.1, factor=0.3, random_state=random_state ) X_moons, y_moons = make_moons( n_samples=1000, noise=0.1, random_state=random_state ) # Split the datasets into training and test sets X_class_train, X_class_test, y_class_train, y_class_test = train_test_split( X_classification, y_classification, test_size=0.2, random_state=random_state ) X_circles_train, X_circles_test, y_circles_train, y_circles_test = train_test_split( X_circles, y_circles, test_size=0.2, random_state=random_state ) X_moons_train, X_moons_test, y_moons_train, y_moons_test = train_test_split( X_moons, y_moons, test_size=0.2, random_state=random_state ) # Initialize the SVM classifier clf = SVC(random_state=random_state) # Train and evaluate on classification dataset clf.fit(X_class_train, y_class_train) y_class_pred = clf.predict(X_class_test) acc_classification = accuracy_score(y_class_test, y_class_pred) # Train and evaluate on circles dataset clf.fit(X_circles_train, y_circles_train) y_circles_pred = clf.predict(X_circles_test) acc_circles = accuracy_score(y_circles_test, y_circles_pred) # Train and evaluate on moons dataset clf.fit(X_moons_train, y_moons_train) y_moons_pred = clf.predict(X_moons_test) acc_moons = accuracy_score(y_moons_test, y_moons_pred) return { \'classification\': acc_classification, \'circles\': acc_circles, \'moons\': acc_moons, }"},{"question":"Using the `contextvars` module, implement a logging system for an asynchronous application where each request has its unique log context. Your task is to create a context-aware logger that can store and retrieve logs specific to each request using context variables. Requirements 1. **Context-specific Logging:** - Implement a `Logger` class with methods to log messages at different levels (`debug`, `info`, `warn`, `error`). Each log message should be stored in a context-specific log. - Implement context-specific storage so logs from different contexts (requests) do not interfere with each other. 2. **Asynchronous Request Handling:** - Implement a simple asynchronous request handler that simulates handling multiple requests, each with its unique logging context. - Use `asyncio` to create the request handler and simulate concurrent processing. 3. **Log Retrieval:** - Provide a method to retrieve all logs for the current context. Specifications: - **Logger Class**: - Methods: `debug(message)`, `info(message)`, `warn(message)`, `error(message)`, `get_logs()`, `clear_logs()`. - Storage of log messages should be context-specific using `contextvars.ContextVar`. - **Request Handler Function**: - Handle multiple asynchronous requests where each request should have distinct logging. - Use the `asyncio` framework to simulate concurrent handling of requests. # Example ```python import asyncio import contextvars class Logger: def __init__(self): self._logs_var = contextvars.ContextVar(\'logs\', default=[]) def _log(self, level, message): logs = self._logs_var.get() logs.append(f\\"{level.upper()}: {message}\\") self._logs_var.set(logs) def debug(self, message): self._log(\'debug\', message) def info(self, message): self._log(\'info\', message) def warn(self, message): self._log(\'warn\', message) def error(self, message): self._log(\'error\', message) def get_logs(self): return self._logs_var.get() def clear_logs(self): self._logs_var.set([]) async def handle_request(log, request_id): log.debug(f\\"Handling request ID: {request_id}\\") log.info(\\"Processing request...\\") await asyncio.sleep(1) # Simulate async processing log.warn(\\"This is a warning message.\\") log.error(\\"An error occurred!\\") logs = log.get_logs() print(f\\"Request {request_id} logs:\\", logs) log.clear_logs() async def main(): logger = Logger() await asyncio.gather( handle_request(logger, 1), handle_request(logger, 2) ) asyncio.run(main()) ``` Input Format: This task does not take any input from the user. Output Format: The function should print logs for each request separately without mixing logs between requests. Constraints: - Ensure that logs from different requests are isolated using context variables. - Use `asyncio` to handle simultaneous requests.","solution":"import asyncio import contextvars class Logger: def __init__(self): self._logs_var = contextvars.ContextVar(\'logs\', default=[]) def _log(self, level, message): logs = self._logs_var.get() logs.append(f\\"{level.upper()}: {message}\\") self._logs_var.set(logs) def debug(self, message): self._log(\'debug\', message) def info(self, message): self._log(\'info\', message) def warn(self, message): self._log(\'warn\', message) def error(self, message): self._log(\'error\', message) def get_logs(self): return self._logs_var.get() def clear_logs(self): self._logs_var.set([]) async def handle_request(logger, request_id): logger.debug(f\\"Handling request ID: {request_id}\\") logger.info(\\"Processing request...\\") await asyncio.sleep(1) # Simulate async processing logger.warn(\\"This is a warning message.\\") logger.error(\\"An error occurred!\\") logs = logger.get_logs() print(f\\"Request {request_id} logs:\\", logs) logger.clear_logs() async def main(): logger = Logger() await asyncio.gather( handle_request(logger, 1), handle_request(logger, 2) ) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Custom Content Manager You are required to implement a custom content manager that extends the `ContentManager` class. This custom content manager should be able to handle a specific MIME type \\"application/json\\". The manager should: 1. Provide a handler to extract JSON content from an email message part. 2. Provide a handler to set JSON content into an email message part. Requirements 1. Your custom content manager class should be named `JSONContentManager`. 2. Implement the `get_json_content` method which extracts JSON data from a message part. 3. Implement the `set_json_content` method which sets JSON data into a message part. 4. Register the handlers within the custom content manager. Function Signatures - `def get_json_content(self, msg, *args, **kwargs) -> dict:` - **Input:** `msg` - an instance of `EmailMessage` with MIME type \\"application/json\\". - **Output:** A dictionary representing the JSON content. - `def set_json_content(self, msg, content: dict, *args, **kwargs) -> None:` - **Input:** - `msg` - an instance of `EmailMessage`. - `content` - a dictionary representing the JSON content to be set. - **Output:** None. Modifies `msg` in place to contain the JSON content. Constraints 1. Assume the email message part always contains valid JSON data for extraction. 2. The `Content-Type` header for the JSON part should be set to \\"application/json\\". 3. Use \\"utf-8\\" encoding for JSON content. Example Usage ```python from email.message import EmailMessage from email.contentmanager import ContentManager class JSONContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/json\', self.get_json_content) self.add_set_handler(dict, self.set_json_content) def get_json_content(self, msg, *args, **kwargs): import json payload = msg.get_payload(decode=True) return json.loads(payload.decode(\'utf-8\')) def set_json_content(self, msg, content: dict, *args, **kwargs): import json payload = json.dumps(content).encode(\'utf-8\') msg.set_payload(payload) msg.set_type(\'application/json\') msg.replace_header(\'Content-Transfer-Encoding\', \'base64\') msg = EmailMessage() json_manager = JSONContentManager() # Setting JSON content json_manager.set_json_content(msg, {\'key\': \'value\'}) # Getting JSON content content = json_manager.get_json_content(msg) print(content) # Output: {\'key\': \'value\'} ``` Implement the `JSONContentManager` class as specified above.","solution":"from email.contentmanager import ContentManager from email.message import EmailMessage import json class JSONContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/json\', self.get_json_content) self.add_set_handler(dict, self.set_json_content) def get_json_content(self, msg, *args, **kwargs): payload = msg.get_payload(decode=True) return json.loads(payload.decode(\'utf-8\')) def set_json_content(self, msg, content: dict, *args, **kwargs): payload = json.dumps(content).encode(\'utf-8\') msg.set_payload(payload) msg.set_type(\'application/json\')"},{"question":"# Custom Kernel Function Implementation and Utilization Problem Statement You are tasked with implementing a custom kernel function and utilizing it to compute a pairwise kernel matrix using `sklearn` utilities. The custom kernel function will be a combination of the RBF (Radial Basis Function) kernel and the Polynomial kernel. Custom Kernel Definition The custom kernel `k` between two vectors ( x ) and ( y ) is defined as: [ k(x, y) = exp(-gamma | x-y |^2) + (alpha x^top y + c_0)^d ] where: - (gamma) is a parameter for the RBF kernel - (| x-y |) is the Euclidean distance between ( x ) and ( y ) - (alpha) is a parameter for the polynomial kernel - (c_0) is a constant term for the polynomial kernel - (d) is the degree of the polynomial kernel Function Signature ```python import numpy as np from sklearn.metrics.pairwise import pairwise_kernels def custom_kernel(X, Y, gamma=1.0, alpha=1.0, c0=0.0, degree=3): Custom kernel combining RBF and Polynomial kernels. Parameters: - X: np.ndarray of shape (n_samples_X, n_features) - Y: np.ndarray of shape (n_samples_Y, n_features) - gamma: float, parameter for the RBF kernel - alpha: float, parameter for the polynomial kernel - c0: float, independent term in polynomial kernel - degree: int, degree of the polynomial kernel Returns: - K: np.ndarray, kernel matrix of shape (n_samples_X, n_samples_Y) # Calculate the RBF component rbf_component = np.exp(-gamma * np.linalg.norm(X[:, np.newaxis] - Y, axis=2)**2) # Calculate the polynomial component polynomial_component = (alpha * np.dot(X, Y.T) + c0)**degree # Combine both components K = rbf_component + polynomial_component return K # Example Usage X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) gamma = 0.5 alpha = 0.8 c0 = 1.0 degree = 2 K = custom_kernel(X, Y, gamma=gamma, alpha=alpha, c0=c0, degree=degree) print(K) ``` Your task is to implement the `custom_kernel` function that computes the custom kernel matrix as defined above. Constraints 1. The inputs `X` and `Y` are 2D numpy arrays with shape `(n_samples_X, n_features)` and `(n_samples_Y, n_features)` respectively. 2. The parameters `gamma`, `alpha`, `c0`, and `degree` are real numbers where `gamma > 0` and `degree` is a positive integer. 3. The output kernel matrix `K` should be of shape `(n_samples_X, n_samples_Y)`. Performance Requirements 1. The function should be efficient in terms of time complexity, leveraging numpy operations for calculations. 2. The implementation should handle large datasets efficiently. Test your implementation with the provided example usage, and verify that the output matches expected results.","solution":"import numpy as np def custom_kernel(X, Y, gamma=1.0, alpha=1.0, c0=0.0, degree=3): Custom kernel combining RBF and Polynomial kernels. Parameters: - X: np.ndarray of shape (n_samples_X, n_features) - Y: np.ndarray of shape (n_samples_Y, n_features) - gamma: float, parameter for the RBF kernel - alpha: float, parameter for the polynomial kernel - c0: float, independent term in polynomial kernel - degree: int, degree of the polynomial kernel Returns: - K: np.ndarray, kernel matrix of shape (n_samples_X, n_samples_Y) # Calculate the RBF component rbf_component = np.exp(-gamma * np.linalg.norm(X[:, np.newaxis] - Y, axis=2)**2) # Calculate the polynomial component polynomial_component = (alpha * np.dot(X, Y.T) + c0)**degree # Combine both components K = rbf_component + polynomial_component return K # Example Usage X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) gamma = 0.5 alpha = 0.8 c0 = 1.0 degree = 2 K = custom_kernel(X, Y, gamma=gamma, alpha=alpha, c0=c0, degree=degree) print(K)"},{"question":"**Objective:** Design a Python function that demonstrates your understanding of the `subprocess` module by running external commands, handling inputs and outputs, and managing potential errors. **Problem Description:** Write a function named `execute_and_log(command: str, log_file: str, timeout: int) -> dict` that takes three parameters: 1. `command` (str): The command to be executed, including any arguments, as a single string. 2. `log_file` (str): The path to a log file where all output (stdout and stderr) should be saved. 3. `timeout` (int): The maximum number of seconds to allow the command to run before timing out. The function should execute the given command using the `subprocess` module and implement the following: - Redirect both stdout and stderr to the specified log file. - Capture the command output (both stdout and stderr). - If the command execution completes within the given timeout, return a dictionary containing: - `returncode` (int): The return code of the command. - `stdout` (str): The captured standard output. - `stderr` (str): The captured standard error (if any). - If the command execution exceeds the timeout, log an appropriate message to the log file, kill the subprocess, and return a dictionary containing: - `error` (str): A message indicating that a timeout occurred. - `stdout` (str): The partial captured standard output until the timeout. - `stderr` (str): The partial captured standard error until the timeout. **Constraints:** - You must handle all exceptions that could be raised by the subprocess module (e.g., `CalledProcessError`, `TimeoutExpired`, and other `OSError` exceptions). - Ensure that the log file is properly closed after writing to it. - The solution should be compatible with both POSIX and Windows systems. **Example Usage:** ```python result = execute_and_log(\\"ls -l /some_directory\\", \\"process_log.txt\\", 10) print(result) ``` **Expected Output:** The output will be a dictionary with the specified keys depending on whether the command completed successfully or timed out. ```python { \\"returncode\\": 0, \\"stdout\\": \\"total 0ndrwxr-xr-x 2 user group 64 Jan 15 12:34 file1n...\\", \\"stderr\\": \\"\\" } ``` or in case of a timeout: ```python { \\"error\\": \\"The command timed out.\\", \\"stdout\\": \\"partial output here...\\", \\"stderr\\": \\"partial error here...\\" } ``` **Note:** Ensure your implementation considers edge cases such as invalid commands, permissions issues when accessing files, and managing subprocess resources appropriately using context managers where applicable.","solution":"import subprocess import shlex def execute_and_log(command: str, log_file: str, timeout: int) -> dict: Executes a given command using the subprocess module and logs stdout and stderr to a log file. Args: - command (str): Command to be executed. - log_file (str): Path to the log file. - timeout (int): Maximum number of seconds to allow the command to run. Returns: - dict: Dictionary containing the returncode, stdout, and stderr, or a timeout error. result = { \\"returncode\\": None, \\"stdout\\": \\"\\", \\"stderr\\": \\"\\", \\"error\\": \\"\\" } try: with open(log_file, \'w\') as log: process = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) try: stdout, stderr = process.communicate(timeout=timeout) result.update({ \\"returncode\\": process.returncode, \\"stdout\\": stdout, \\"stderr\\": stderr }) except subprocess.TimeoutExpired: process.kill() stdout, stderr = process.communicate() result.update({ \\"error\\": \\"The command timed out.\\", \\"stdout\\": stdout, \\"stderr\\": stderr }) finally: log.write(result.get(\\"stdout\\", \\"\\") + result.get(\\"stderr\\", \\"\\") + result.get(\\"error\\", \\"\\")) except subprocess.CalledProcessError as e: result.update({ \\"returncode\\": e.returncode, \\"stdout\\": e.stdout, \\"stderr\\": e.stderr }) except OSError as e: result.update({ \\"error\\": f\\"OS error: {str(e)}\\" }) return result"},{"question":"**Objective:** Implement a function that takes a list of records containing different data types, packs them into XDR format using the `xdrlib` module, then unpacks the packed data back to the original format. This will test your understanding of both packing and unpacking functionalities of the `xdrlib` module. **Function Signature:** ```python def process_records(records: list) -> list: pass ``` **Input:** - `records`: A list of dictionaries, where each dictionary represents a record with the following structure: ```python { \'id\': int, # An integer identifier \'name\': str, # A string with the name \'value\': float, # A floating-point number \'flag\': bool # A boolean flag } ``` **Output:** - Returns a list of records in the original format after being packed and then unpacked. **Constraints:** 1. Each record dictionary will have the exact structure as described in the input format. 2. The size of the list of records will not exceed 1000 elements. 3. The length of the \'name\' string will not exceed 50 characters. **Example:** ```python input_records = [ {\'id\': 1, \'name\': \'Alice\', \'value\': 23.4, \'flag\': True}, {\'id\': 2, \'name\': \'Bob\', \'value\': 45.6, \'flag\': False} ] output = process_records(input_records) assert output == input_records ``` **Instructions:** 1. Use the `Packer` class from the `xdrlib` module to pack the data. 2. Ensure you use the appropriate packing methods for each data type. 3. Use the `Unpacker` class from the `xdrlib` module to unpack the data back to its original form. 4. Handle any potential exceptions that might occur during the packing or unpacking process. **Notes:** - Remember to test your function to ensure it handles various edge cases, such as empty strings or boolean values correctly. - Maintain code readability by using meaningful variable names and including necessary comments. **Solution Template:** ```python import xdrlib def process_records(records): packer = xdrlib.Packer() # Pack the data for record in records: packer.pack_int(record[\'id\']) packer.pack_string(record[\'name\']) packer.pack_double(record[\'value\']) packer.pack_bool(record[\'flag\']) packed_data = packer.get_buffer() unpacker = xdrlib.Unpacker(packed_data) unpacked_records = [] # Unpack the data for _ in records: unpacked_record = { \'id\': unpacker.unpack_int(), \'name\': unpacker.unpack_string(), \'value\': unpacker.unpack_double(), \'flag\': unpacker.unpack_bool() } unpacked_records.append(unpacked_record) return unpacked_records # Example use-case: input_records = [ {\'id\': 1, \'name\': \'Alice\', \'value\': 23.4, \'flag\': True}, {\'id\': 2, \'name\': \'Bob\', \'value\': 45.6, \'flag\': False} ] output = process_records(input_records) print(output) # Should return the same list of records as input_records ```","solution":"import xdrlib def process_records(records: list) -> list: packer = xdrlib.Packer() # Pack the data for record in records: packer.pack_int(record[\'id\']) packer.pack_string(record[\'name\'].encode(\'utf-8\')) packer.pack_double(record[\'value\']) packer.pack_bool(record[\'flag\']) packed_data = packer.get_buffer() unpacker = xdrlib.Unpacker(packed_data) unpacked_records = [] # Unpack the data for _ in records: unpacked_record = { \'id\': unpacker.unpack_int(), \'name\': unpacker.unpack_string().decode(\'utf-8\'), \'value\': unpacker.unpack_double(), \'flag\': unpacker.unpack_bool() } unpacked_records.append(unpacked_record) return unpacked_records"},{"question":"Task You are required to create an application that simulates a simple calculator. The calculator should take two numbers and an operator as input and perform the corresponding mathematical operation. You must handle potential errors such as invalid inputs and division by zero, and ensure resources are cleaned up afterwards. Requirements 1. Implement a function named `perform_calculation(num1, num2, operator)`: - **Input:** - `num1` and `num2`: Integers. - `operator`: A string that can be one of `\'+\', \'-\', \'*\', \'/\'`. - **Output:** The result of the operation as a float. - **Constraints:** - If the operator is not one of the specified ones, raise a `ValueError` with the message \\"Invalid operator\\". - If a division by zero is attempted, raise a `ZeroDivisionError` with the message \\"Cannot divide by zero\\". - Ensure any other invalid inputs are properly handled using exception handling. 2. Implement a function named `calculator()`: - **Input:** No input parameters. - **Output:** No return value. This function should: - Prompt the user to enter two numbers and an operator. - Perform the calculation by calling `perform_calculation(num1, num2, operator)`. - Print the result of the calculation. - Handle exceptions gracefully by printing appropriate error messages. - Ensure all resources are cleaned up using the `finally` clause where applicable. Example ```python calculator() ``` # Sample Interaction: ``` Enter the first number: 10 Enter the second number: 0 Enter the operator (+, -, *, /): / Cannot divide by zero. Enter the first number: 15 Enter the second number: five Oops! That was not a valid number. Try again... Enter the first number: 8 Enter the second number: 4 Enter the operator (+, -, *, /): + The result is: 12.0 ``` # Hints: - Make use of the `try...except...else...finally` blocks for handling exceptions. - Use custom error messages to guide the user.","solution":"def perform_calculation(num1, num2, operator): This function performs basic arithmetic operations. :param num1: First number (integer) :param num2: Second number (integer) :param operator: A string that can be one of \'+\', \'-\', \'*\', \'/\' :return: The result of the operation as a float :raises ValueError: If operator is not one of the specified ones :raises ZeroDivisionError: If trying to divide by zero if operator == \'+\': return float(num1 + num2) elif operator == \'-\': return float(num1 - num2) elif operator == \'*\': return float(num1 * num2) elif operator == \'/\': if num2 == 0: raise ZeroDivisionError(\\"Cannot divide by zero\\") return float(num1 / num2) else: raise ValueError(\\"Invalid operator\\") def calculator(): This function prompts the user to input two numbers and an operator, then performs the calculation and handles any exceptions, printing appropriate error messages. try: num1 = int(input(\\"Enter the first number: \\")) num2 = int(input(\\"Enter the second number: \\")) operator = input(\\"Enter the operator (+, -, *, /): \\") result = perform_calculation(num1, num2, operator) print(f\\"The result is: {result}\\") except ValueError: print(\\"Oops! That was not a valid number or operator. Try again...\\") except ZeroDivisionError as e: print(e) finally: print(\\"Calculation attempt finished.\\")"},{"question":"# Comprehensive List Manipulation and Data Extraction You are asked to write a series of functions to manipulate and extract data from nested lists, combining fundamentals of list operations, comprehensions, and set/dictionary use. Function 1: Flatten Nested List Write a function `flatten_nested_list(nested_list)` to flatten a nested list of arbitrary depth into a single level list. ```python def flatten_nested_list(nested_list): Flattens a nested list into a single-level list. Parameters: nested_list (list): A nested list of arbitrary depth. Returns: list: A flattened single-level list. Example: flatten_nested_list([1, [2, [3, 4], 5], [6, 7]]) -> [1, 2, 3, 4, 5, 6, 7] # Your code here ``` Function 2: Unique Values Write a function `unique_values(nested_list)` to return a sorted list of unique values from a nested list. ```python def unique_values(nested_list): Returns the sorted list of unique values from a nested list. Parameters: nested_list (list): A nested list of arbitrary depth. Returns: list: A sorted list of unique values. Example: unique_values([1, [2, [3, 4], 5], [5, 6], 3]) -> [1, 2, 3, 4, 5, 6] # Your code here ``` Function 3: Nested Dictionary Construction Write a function `nested_dict_construction(nested_list)` to construct a nested dictionary from a nested list, where the keys are indices at each level. ```python def nested_dict_construction(nested_list): Constructs a nested dictionary from a nested list where keys are indices at each level. Parameters: nested_list (list): A nested list of arbitrary depth. Returns: dict: A nested dictionary representation of nested_list. Example: nested_dict_construction([1, [2, [3, 4], 5], [6, 7]]) -> {0: 1, 1: {0: 2, 1: {0: 3, 1: 4}, 2: 5}, 2: {0: 6, 1: 7}} # Your code here ``` # Constraints and Performance Requirements - You may assume that the input list will only contain integers and lists. - The depth of the nested list can vary, but you can assume that it will not exceed 1000 levels. - Your solution should handle large nested lists efficiently. - Aim to keep the time complexity as low as possible per the task requirements. # Testing Test your functions with the provided examples and add additional tests to ensure coverage of edge cases. ```python # Test Function 1 assert flatten_nested_list([1, [2, [3, 4], 5], [6, 7]]) == [1, 2, 3, 4, 5, 6, 7] # Test Function 2 assert unique_values([1, [2, [3, 4], 5], [5, 6], 3]) == [1, 2, 3, 4, 5, 6] # Test Function 3 assert nested_dict_construction([1, [2, [3, 4], 5], [6, 7]]) == { 0: 1, 1: {0: 2, 1: {0: 3, 1: 4}, 2: 5}, 2: {0: 6, 1: 7}} ```","solution":"def flatten_nested_list(nested_list): Flattens a nested list into a single-level list. Parameters: nested_list (list): A nested list of arbitrary depth. Returns: list: A flattened single-level list. flattened_list = [] def flatten(sublist): for item in sublist: if isinstance(item, list): flatten(item) else: flattened_list.append(item) flatten(nested_list) return flattened_list def unique_values(nested_list): Returns the sorted list of unique values from a nested list. Parameters: nested_list (list): A nested list of arbitrary depth. Returns: list: A sorted list of unique values. flattened_list = flatten_nested_list(nested_list) return sorted(set(flattened_list)) def nested_dict_construction(nested_list): Constructs a nested dictionary from a nested list where keys are indices at each level. Parameters: nested_list (list): A nested list of arbitrary depth. Returns: dict: A nested dictionary representation of nested_list. def construct(sublist): if isinstance(sublist, list): return {i: construct(item) for i, item in enumerate(sublist)} else: return sublist return construct(nested_list)"},{"question":"**Objective**: Demonstrate understanding of seaborn\'s error bars and their applications in data visualization. **Problem Statement**: You are given a dataset containing `temperature` and `humidity` measurements collected over 30 days. Your task is to visualize this data using seaborn, incorporating different kinds of error bars to convey the variability and uncertainty in the data. **Dataset**: Create a dataset with `temperature` and `humidity` measurements as follows: - `temperature`: Normally distributed around a mean of 25C with a standard deviation of 5C. - `humidity`: Normally distributed around a mean of 50% with a standard deviation of 10%. **Tasks**: 1. **Load and Generate the Dataset**: - Generate a dataset with 30 data points for `temperature` and `humidity`. 2. **Visualization with Error Bars**: - Create a point plot for the `temperature` data with: a. Standard deviation error bars. b. Confidence interval error bars. - Create a point plot for the `humidity` data with: a. Standard error bars. b. Percentile interval error bars. 3. **Custom Error Bars**: - Create a function that defines a custom error bar showing the minimum and maximum data points. - Use this custom function to create a point plot for the `temperature` data. **Constraints**: - Use the seaborn library for data visualization. **Output**: - The function should generate and display the point plots with appropriate labels for each type of error bar. **Function Signature**: ```python def visualize_error_bars(): import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme(style=\\"darkgrid\\") # Task 1: Generate dataset # Your code here... # Task 2a: Point plot for temperature with standard deviation error bars # Your code here... # Task 2b: Point plot for temperature with confidence interval error bars # Your code here... # Task 3a: Point plot for humidity with standard error bars # Your code here... # Task 3b: Point plot for humidity with percentile interval error bars # Your code here... # Task 4: Point plot for temperature with custom error bars # Your code here... # Display plots # plt.show() should be called at the end of the function after all plots are created ``` **Example**: Here is a partial example of how the generated plots might look, showing the temperature data with standard deviation error bars: ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"darkgrid\\") # Generate dataset np.random.seed(0) temperature = np.random.normal(25, 5, 30) humidity = np.random.normal(50, 10, 30) data = pd.DataFrame({\'temperature\': temperature, \'humidity\': humidity}) # Point plot for temperature with standard deviation error bars sns.pointplot(data=data, x=data.index, y=\'temperature\', errorbar=\'sd\') plt.title(\\"Temperature with Standard Deviation Error Bars\\") plt.show() ``` Your final implementation should generate all the required plots with appropriate error bars.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_error_bars(): # Set theme sns.set_theme(style=\\"darkgrid\\") # Task 1: Generate dataset np.random.seed(0) temperature = np.random.normal(25, 5, 30) humidity = np.random.normal(50, 10, 30) data = pd.DataFrame({\'day\': range(1, 31), \'temperature\': temperature, \'humidity\': humidity}) # Task 2a: Point plot for temperature with standard deviation error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\'day\', y=\'temperature\', data=data, errorbar=\'sd\') plt.title(\\"Temperature with Standard Deviation Error Bars\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature (C)\\") plt.show() # Task 2b: Point plot for temperature with confidence interval error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\'day\', y=\'temperature\', data=data, errorbar=\'ci\') plt.title(\\"Temperature with Confidence Interval Error Bars\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature (C)\\") plt.show() # Task 3a: Point plot for humidity with standard error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\'day\', y=\'humidity\', data=data, errorbar=\'se\') plt.title(\\"Humidity with Standard Error Bars\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Humidity (%)\\") plt.show() # Task 3b: Point plot for humidity with percentile interval error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\'day\', y=\'humidity\', data=data, errorbar=lambda x: (np.percentile(x, 2.5), np.percentile(x, 97.5))) plt.title(\\"Humidity with Percentile Interval Error Bars\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Humidity (%)\\") plt.show() # Task 4: Define custom error bar function def custom_minmax_error(x): return x.min(), x.max() # Point plot for temperature with custom min-max error bars plt.figure(figsize=(10, 6)) sns.pointplot(x=\'day\', y=\'temperature\', data=data, errorbar=custom_minmax_error) plt.title(\\"Temperature with Custom Min-Max Error Bars\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature (C)\\") plt.show()"},{"question":"**Objective**: Implement functions that encode and decode messages between various ASCII and binary representations. This will evaluate your understanding of the `binascii` module, handling multiple data formats, and ensuring data integrity. # Problem Statement You need to write a Python class `BinaryAsciiConverter` with the following methods: 1. `uu_encode(self, binary_data)`: Encode binary data to a uuencoded ASCII format. 2. `uu_decode(self, ascii_data)`: Decode uuencoded ASCII data back to binary format. 3. `base64_encode(self, binary_data)`: Encode binary data to base64 ASCII format. 4. `base64_decode(self, ascii_data)`: Decode base64 ASCII data back to binary format. 5. `hexlify(self, binary_data)`: Encode binary data to a hex ASCII format. 6. `unhexlify(self, ascii_data)`: Decode hex ASCII data back to binary. # Method Specifications - `uu_encode(self, binary_data) -> str` - **Input**: `binary_data` (bytes): A bytes-like object to be uuencoded. - **Output**: A string representing the uuencoded ASCII data. - **Constraints**: The length of `binary_data` should be at most 45 bytes. - `uu_decode(self, ascii_data) -> bytes` - **Input**: `ascii_data` (str): A string containing uuencoded ASCII data. - **Output**: A bytes object containing the decoded binary data. - `base64_encode(self, binary_data) -> str` - **Input**: `binary_data` (bytes): A bytes-like object to be encoded in base64. - **Output**: A string representing the base64 encoded ASCII data. - `base64_decode(self, ascii_data) -> bytes` - **Input**: `ascii_data` (str): A string containing base64 encoded ASCII data. - **Output**: A bytes object containing the decoded binary data. - `hexlify(self, binary_data) -> str` - **Input**: `binary_data` (bytes): A bytes-like object to be encoded in hex. - **Output**: A string representing the hex ASCII data. - `unhexlify(self, ascii_data) -> bytes` - **Input**: `ascii_data` (str): A string containing hex ASCII data. - **Output**: A bytes object containing the decoded binary data. # Example Usage ```python converter = BinaryAsciiConverter() # UUencode binary_data = b\'Hello, this is some binary data\' uu_encoded = converter.uu_encode(binary_data) print(uu_encoded) # Output should be uu encoded string # UUdecode uu_decoded = converter.uu_decode(uu_encoded) print(uu_decoded) # Output should be b\'Hello, this is some binary data\' # Base64 encode base64_encoded = converter.base64_encode(binary_data) print(base64_encoded) # Output should be base64 encoded string # Base64 decode base64_decoded = converter.base64_decode(base64_encoded) print(base64_decoded) # Output should be b\'Hello, this is some binary data\' # Hexlify hex_encoded = converter.hexlify(binary_data) print(hex_encoded) # Output should be hex encoded string # Unhexlify hex_decoded = converter.unhexlify(hex_encoded) print(hex_decoded) # Output should be b\'Hello, this is some binary data\' ``` # Constraints - The input data for `uu_encode` should be at most 45 bytes. - The input strings for decoding functions (`uu_decode`, `base64_decode`, `unhexlify`) must represent valid encoded data. **Evaluation Criteria**: - Correctness: The implemented methods must produce the correct output. - Efficiency: The methods should execute efficiently for the given constraints. - Code Quality: The code should be well-organized and follow Python coding standards.","solution":"import binascii import base64 class BinaryAsciiConverter: def uu_encode(self, binary_data): Encode binary data to a uuencoded ASCII format. if len(binary_data) > 45: raise ValueError(\\"binary_data length should be at most 45 bytes\\") return binascii.b2a_uu(binary_data).decode(\'ascii\') def uu_decode(self, ascii_data): Decode uuencoded ASCII data back to binary format. return binascii.a2b_uu(ascii_data.encode(\'ascii\')) def base64_encode(self, binary_data): Encode binary data to base64 ASCII format. return base64.b64encode(binary_data).decode(\'ascii\') def base64_decode(self, ascii_data): Decode base64 ASCII data back to binary format. return base64.b64decode(ascii_data.encode(\'ascii\')) def hexlify(self, binary_data): Encode binary data to a hex ASCII format. return binascii.hexlify(binary_data).decode(\'ascii\') def unhexlify(self, ascii_data): Decode hex ASCII data back to binary format. return binascii.unhexlify(ascii_data.encode(\'ascii\'))"},{"question":"# PyTorch Signal Processing: Window Functions and FFT **Objective:** Demonstrate your understanding and ability to apply PyTorch\'s signal window functions and perform FFT on a 1D signal. **Problem:** You are given a 1D signal (a tensor) representing a noisy sine wave. Your task is to: 1. Apply a Hann window to the signal using `torch.signal.windows.hann`. 2. Perform Fast Fourier Transform (FFT) on the windowed signal using `torch.fft.fft`. 3. Return the magnitude spectrum of the resulting FFT. **Function Signature:** ```python def process_signal(signal: torch.Tensor) -> torch.Tensor: pass ``` **Input:** - `signal` (torch.Tensor): A 1-dimensional tensor of shape `(N,)`, representing the noisy sine wave. **Output:** - A 1-dimensional tensor of shape `(N//2 + 1,)`, representing the magnitude spectrum of the windowed signals FFT output. **Details:** - The Hann window should be applied to the entire signal. - Use PyTorch\'s `torch.signal.windows.hann` to create the Hann window. - Apply the `torch.fft.fft` to the windowed signal. - Compute the magnitude spectrum by taking the absolute value of the FFT result and then consider only the first half of the spectrum due to its symmetric property. **Example:** Given the noisy sine wave signal as input: ```python signal = torch.tensor([0.2, 0.5, 1.2, 1.7, 2.2, 2.7, 2.5, 2.1, 1.4, 0.9, 0.3, 0.1]) ``` The output should be: ```python output = process_signal(signal) # Expected output format. The actual numbers will vary based on FFT computation. # tensor([mag1, mag2, mag3, ...]) ``` **Constraints:** - You may assume `torch` and `torch.signal.windows` are already imported. - You should only use PyTorch and its submodules (avoid using any other libraries like numpy or scipy). Write your function and ensure it meets the requirements above. Remember to carefully handle the windowing and FFT steps to accurately compute the magnitudes.","solution":"import torch def process_signal(signal: torch.Tensor) -> torch.Tensor: Applies a Hann window to the signal and performs FFT to get the magnitude spectrum. Parameters: signal (torch.Tensor): A 1-dimensional tensor representing the noisy sine wave. Returns: torch.Tensor: The magnitude spectrum of the windowed signals FFT output. # Ensure the signal is a 1D tensor if signal.dim() != 1: raise ValueError(\\"Signal must be a 1-dimensional tensor\\") # Create a Hann window hann_window = torch.hann_window(signal.size(0)) # Apply the Hann window to the signal windowed_signal = signal * hann_window # Perform FFT on the windowed signal fft_result = torch.fft.fft(windowed_signal) # Compute the magnitude spectrum (first half due to symmetry) magnitude_spectrum = torch.abs(fft_result)[:signal.size(0)//2 + 1] return magnitude_spectrum"},{"question":"Background You are tasked with designing a series of color palettes for a data visualization project using the seaborn library. You will be required to demonstrate your ability to create and customize these palettes based on specified requirements. Objective Write a function `create_custom_palettes` that generates and returns a dictionary containing three different types of color palettes using seaborn\'s `sns.light_palette` function. The palettes should be customized according to the inputs provided. Function Signature ```python import seaborn as sns def create_custom_palettes(color1: str, color2: str, husl_spec: tuple, n_colors: int) -> dict: pass ``` Input Parameters - `color1`: A string representing a named color or a hex code for the first palette. - `color2`: A string representing a named color or a hex code for the second palette. - `husl_spec`: A tuple of three integers representing the husl color specification for the third palette. - `n_colors`: An integer specifying the number of colors for the second and third palettes. Each palette should have at least 2 colors and at most 10 colors. Output - The function should return a dictionary with three keys: `\'palette1\'`, `\'palette2\'`, and `\'palette3\'`. - `\'palette1\'` should map to a sequential palette generated from `color1` with the default number of colors. - `\'palette2\'` should map to a sequential palette generated from `color2` with `n_colors` colors. - `\'palette3\'` should map to a sequential palette generated from `husl_spec` with `n_colors` colors, using the husl color system. Constraints - The value of `n_colors` will always be an integer between 2 and 10, inclusive. - The `color1` and `color2` inputs will always be valid named colors or hex color codes. - The `husl_spec` tuple will always contain three valid integers for husl color specification. Example ```python palettes = create_custom_palettes(\\"seagreen\\", \\"#79C\\", (20, 60, 50), 8) print(palettes) ``` Expected output ```python { \'palette1\': <seaborn.utils._ColorPalette object at 0x...>, \'palette2\': <seaborn.utils._ColorPalette object at 0x...>, \'palette3\': <seaborn.utils._ColorPalette object at 0x...> } ``` Where each value in the dictionary is a seaborn ColorPalette object that can be used for data visualization. Note - Ensure that all necessary libraries are imported within the function. - Use the seaborn library functions as required to generate the palettes. Good luck!","solution":"import seaborn as sns def create_custom_palettes(color1: str, color2: str, husl_spec: tuple, n_colors: int) -> dict: Generates three color palettes using seaborn\'s `sns.light_palette` and HUSL specifications. Parameters: - color1: A string representing a named color or a hex code for the first palette. - color2: A string representing a named color or a hex code for the second palette. - husl_spec: A tuple of three integers representing the husl color specification for the third palette. - n_colors: An integer specifying the number of colors for the second and third palettes (2 to 10). Returns: - A dictionary with three color palettes. palette1 = sns.light_palette(color1) palette2 = sns.light_palette(color2, n_colors=n_colors) palette3 = sns.husl_palette(n_colors, h=husl_spec[0], s=husl_spec[1], l=husl_spec[2]) return { \'palette1\': palette1, \'palette2\': palette2, \'palette3\': palette3 }"},{"question":"# Question: Create a Unix-Specific User Information Retrieval Tool You are required to write a command-line Python program that retrieves and displays detailed user information from the Unix operating system password and group databases using the `pwd` and `grp` modules respectively. Requirements: 1. **Function Signatures**: ```python def get_user_info(username: str) -> dict: Retrieves detailed information about a user. Parameters: - username (str): The username to retrieve information for. Returns: - dict: A dictionary containing username, user ID, group ID, home directory, shell, and the list of groups the user belongs to. pass ``` 2. **Description and Constraints**: - The function `get_user_info` will: - Use the `pwd` module to access the password database and retrieve the user\'s information. - Use the `grp` module to access the group database and retrieve the group(s) associated with the user. - Return a dictionary with the following key-value pairs: ```python { \\"username\\": \\"<username>\\", \\"uid\\": <user_id>, \\"gid\\": <group_id>, \\"home_dir\\": \\"<home_directory>\\", \\"shell\\": \\"<shell>\\", \\"groups\\": [\\"<group1>\\", \\"<group2>\\", ...] } ``` - If the user does not exist, raise a `KeyError`. 3. **Performance Requirements**: - The function should handle common usernames and groups existing in a typical Unix system efficiently. Example: ```python # Assume user \'john\' exists with uid 1001, gid 1001, home directory /home/john, shell /bin/bash, and is part of the groups \'users\', \'admins\' result = get_user_info(\'john\') # Expected output { \\"username\\": \\"john\\", \\"uid\\": 1001, \\"gid\\": 1001, \\"home_dir\\": \\"/home/john\\", \\"shell\\": \\"/bin/bash\\", \\"groups\\": [\\"users\\", \\"admins\\"] } ``` **Hints**: - Refer to the `pwd` and `grp` module documentation to learn how to fetch user and group information. - To list the groups a user belongs to, you might need to inspect multiple group entries where the user is a member.","solution":"import pwd import grp def get_user_info(username): Retrieves detailed information about a user. Parameters: - username (str): The username to retrieve information for. Returns: - dict: A dictionary containing username, user ID, group ID, home directory, shell, and the list of groups the user belongs to. try: user_info = pwd.getpwnam(username) except KeyError: raise KeyError(f\\"User \'{username}\' not found.\\") user_groups = [] for group in grp.getgrall(): if username in group.gr_mem or group.gr_gid == user_info.pw_gid: user_groups.append(group.gr_name) return { \\"username\\": user_info.pw_name, \\"uid\\": user_info.pw_uid, \\"gid\\": user_info.pw_gid, \\"home_dir\\": user_info.pw_dir, \\"shell\\": user_info.pw_shell, \\"groups\\": user_groups }"},{"question":"Objective Demonstrate understanding and effective use of the `bisect` module for maintaining sorted lists and demonstrating efficient insertion and searching using bisection algorithms. Problem Description You are given a list of student records, where each record is a dictionary containing the student\'s name and score. You need to perform a series of operations to maintain this list in sorted order based on the scores. Write the following functions: 1. **insert_student(records: List[Dict[str, Union[str, int]]], student: Dict[str, Union[str, int]]) -> None:** - This function inserts a new student record into the `records` list while maintaining the list in sorted order based on the students\' scores. - Parameters: - `records`: A list of dictionaries, where each dictionary contains `\'name\'` (str) and `\'score\'` (int). - `student`: A dictionary representing the new student to be added, containing `\'name\'` (str) and `\'score\'` (int). - The `records` list should remain sorted in ascending order based on `score` after the insertion. 2. **find_rank(records: List[Dict[str, Union[str, int]]], score: int) -> int:** - This function finds the rank of the first student with a score greater than or equal to the given `score`. The rank is determined by the 1-based position in the list (i.e., the top rank is 1). - Parameters: - `records`: A sorted list of dictionaries, where each dictionary contains `\'name\'` (str) and `\'score\'` (int). The list is sorted in ascending order based on `score`. - `score`: An integer representing the score to be searched. - Returns: - The 1-based rank of the first student with a score greater than or equal to the given `score`. - If no students have a score greater than or equal to the given `score`, return -1. Constraints - The input list `records` will contain at most 10<sup>4</sup> records. - Each student record will have a unique name but may have non-unique scores. - The `score` parameter will be a non-negative integer. Examples ```python # Example usage of insert_student records = [ {\'name\': \'Alice\', \'score\': 85}, {\'name\': \'Bob\', \'score\': 90}, {\'name\': \'Charlie\', \'score\': 95}, ] new_student = {\'name\': \'Eve\', \'score\': 88} insert_student(records, new_student) # Now records should be: # [ # {\'name\': \'Alice\', \'score\': 85}, # {\'name\': \'Eve\', \'score\': 88}, # {\'name\': \'Bob\', \'score\': 90}, # {\'name\': \'Charlie\', \'score\': 95}, # ] # Example usage of find_rank rank = find_rank(records, 88) # Should return 2 rank = find_rank(records, 90) # Should return 3 rank = find_rank(records, 100) # Should return -1 ``` Implement the `insert_student` and `find_rank` functions below: ```python from typing import List, Dict, Union from bisect import insort_left, bisect_left def insert_student(records: List[Dict[str, Union[str, int]]], student: Dict[str, Union[str, int]]) -> None: # Your code here def find_rank(records: List[Dict[str, Union[str, int]]], score: int) -> int: # Your code here ```","solution":"from typing import List, Dict, Union from bisect import insort_left, bisect_left def insert_student(records: List[Dict[str, Union[str, int]]], student: Dict[str, Union[str, int]]) -> None: # Use a tuple with (score, name) for stable sorting and ease of insertion insort_left(records, (student[\'score\'], student[\'name\'], student)) def find_rank(records: List[Dict[str, Union[str, int]]], score: int) -> int: # Extract only the scores for use in bisect_left scores = [record[0] for record in records] idx = bisect_left(scores, score) if idx < len(scores): return idx + 1 return -1"},{"question":"Problem Statement You are given a file in AIFF format (Audio Interchange File Format), which consists of multiple chunks of data. Your task is to implement a Python function that reads this file and extracts all the chunks present in it. The function should return a list of dictionaries, where each dictionary represents a chunk with its ID, size, and data. Function Signature ```python def extract_chunks(file_path: str) -> List[Dict[str, Union[str, int, bytes]]]: pass ``` Input - `file_path` (str): The path to the AIFF file to read. Output - Returns a list of dictionaries, where each dictionary contains the following keys: - `id`: (str) The 4-byte chunk ID. - `size`: (int) The size of the chunk data. - `data`: (bytes) The data of the chunk. Constraints 1. Assume the file is in AIFF format and correctly structured. 2. Ensure your code handles files that are aligned to 2-byte boundaries. 3. You must use the `chunk` module within your implementation. 4. The size of the file will not exceed 10 MB. Example Assume you have the following chunked binary file (simplified for illustration): ``` File: test.aiff Chunk 1 ID: \'FORM\' Size: 12 Data: b\'AIFF...\' Chunk 2 ID: \'COMM\' Size: 18 Data: b\'x00x02...\' Chunk 3 ID: \'SSND\' Size: 24 Data: b\'x00x00...\' ``` To read this file, your function call would be: ```python result = extract_chunks(\\"test.aiff\\") ``` Expected Output: ```python [ {\'id\': \'FORM\', \'size\': 12, \'data\': b\'AIFF...\'}, {\'id\': \'COMM\', \'size\': 18, \'data\': b\'x00x02...\'}, {\'id\': \'SSND\', \'size\': 24, \'data\': b\'x00x00...\'} ] ``` Notes - Ensure you handle the padding byte if the chunk data size is odd. - You may assume the input file does not contain any nested chunks.","solution":"import chunk from typing import List, Dict, Union def extract_chunks(file_path: str) -> List[Dict[str, Union[str, int, bytes]]]: chunks = [] with open(file_path, \'rb\') as f: while True: try: c = chunk.Chunk(f, bigendian=True) except EOFError: break # Read the chunk chunk_id = c.getname().decode(\'ascii\') chunk_size = c.chunksize chunk_data = c.read() # If the chunk size is odd, there will be a padding byte. if chunk_size % 2 == 1: f.read(1) chunks.append({ \'id\': chunk_id, \'size\': chunk_size, \'data\': chunk_data }) return chunks"},{"question":"# Advanced Python Packaging and Distribution Problem Statement: You are tasked with creating a Python project that utilizes virtual environments and generates a standalone executable using `zipapp`. The goal is to ensure that the project dependencies are managed in an isolated environment and that the application can be run as a standalone executable. Requirements: 1. **Virtual Environment Setup**: - Create a virtual environment in a directory named `project_env`. - Install the required packages listed in the `requirements.txt` file into this virtual environment. 2. **Stand-alone Executable**: - Package the project as a standalone executable zip archive using `zipapp`. - The archive should run a Python script named `main.py` when executed. Constraints: - The `main.py` script should output a simple message, \\"Hello, Packaged World!\\". - The solution should handle dependency installation and ensure environmental isolation. - The zip archive should specify the Python interpreter to use and should be executable on any compatible system without requiring any additional setup. Input: - A `requirements.txt` file with necessary dependencies (can be minimal or even empty for this task). - A `main.py` script with the content: ```python print(\\"Hello, Packaged World!\\") ``` Expected Output: - A virtual environment named `project_env` with the designated dependencies installed. - A zipapp executable named `project_app.pyz` that prints \\"Hello, Packaged World!\\" when run. Implementation Steps: 1. Create a virtual environment using `venv` and activate it. 2. Install dependencies from `requirements.txt` into the virtual environment. 3. Use `zipapp` to package the project directory into a zip archive executable. 4. Ensure the resulting executable runs the `main.py` script correctly. Example Code and Execution: Below is a hint on how you may begin implementing the virtual environment and packaging process: ```python import venv import zipapp import subprocess # Step 1: Create and activate a virtual environment venv.create(\'project_env\', with_pip=True) # Step 2: Install dependencies subprocess.run([\'project_env/bin/pip\', \'install\', \'-r\', \'requirements.txt\']) # Step 3: Package the project using zipapp zipapp.create_archive(\'project_directory\', target=\'project_app.pyz\', interpreter=\'/usr/bin/env python3\') ``` Note: Ensure you have a basic understanding of path specifications and directory structures to adapt the code to your project setup.","solution":"import venv import zipapp import subprocess import os def setup_virtual_env(): Create a virtual environment and install dependencies venv.create(\'project_env\', with_pip=True) subprocess.run([\'project_env/bin/pip\', \'install\', \'-r\', \'requirements.txt\'], check=True) def package_project(): Package the project as a standalone executable using zipapp if not os.path.exists(\'project_directory\'): os.makedirs(\'project_directory\') if not os.path.exists(\'project_directory/__main__.py\'): with open(\'project_directory/__main__.py\', \'w\') as f: f.write(\'print(\\"Hello, Packaged World!\\")\') zipapp.create_archive(\'project_directory\', target=\'project_app.pyz\', interpreter=\'/usr/bin/env python3\') if __name__ == \\"__main__\\": setup_virtual_env() package_project()"},{"question":"# HTML Parsing Customizer You are required to create a custom HTML parser by subclassing the `HTMLParser` class from the `html.parser` module. The custom parser should not only handle and identify various HTML elements but also perform specific actions on finding certain HTML components. Additionally, you should accumulate data extracted from the HTML and return it in a specified structure. # Task Create a class `CustomHTMLParser` that extends `HTMLParser`. The parser should perform the following: 1. **Count the occurrences of each type of tag**: - Maintain a dictionary `tag_count` where the key is the tag name and the value is the count of that tag in the HTML. 2. **Extract text content inside specific tags**: - Extract all text within `<title>` and `<h1>` tags and store them in `title_text` and `h1_text` attributes respectively. 3. **Identify and store URLs in `href` attributes**: - Collect all URLs from `href` attributes of `<a>` tags in a list `urls`. 4. **Comment Tracking**: - Store all comments in a list `comments`. Class Structure ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tag_count = {} self.title_text = \\"\\" self.h1_text = \\"\\" self.urls = [] self.comments = [] def handle_starttag(self, tag, attrs): # Increment tag count pass def handle_endtag(self, tag): # Handle end of specific tags if necessary pass def handle_data(self, data): # Extract text for specific tags pass def handle_comment(self, data): # Store comments pass ``` Constraints - The input to the `feed` method will always be a valid HTML document as a string. - HTML tags and attributes will be well-formed. - There won\'t be nested comments or processing instructions. Example ```python html_code = \'\'\' <!DOCTYPE html> <html> <head> <title>Sample Document</title> </head> <body> <h1>A Heading</h1> <p>Some <a href=\\"https://example.com\\">link</a> in a paragraph. <!-- A comment --> <a href=\\"https://anotherexample.com\\">Another link</a> </body> </html> \'\'\' parser = CustomHTMLParser() parser.feed(html_code) print(parser.tag_count) # {\'html\': 1, \'head\': 1, \'title\': 1, \'body\': 1, \'h1\': 1, \'p\': 1, \'a\': 2} print(parser.title_text) # \'Sample Document\' print(parser.h1_text) # \'A Heading\' print(parser.urls) # [\'https://example.com\', \'https://anotherexample.com\'] print(parser.comments) # [\' A comment \'] ``` Good luck, and happy parsing!","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tag_count = {} self.title_text = \\"\\" self.h1_text = \\"\\" self.urls = [] self.comments = [] self.capture_title = False self.capture_h1 = False def handle_starttag(self, tag, attrs): # Increment tag count if tag in self.tag_count: self.tag_count[tag] += 1 else: self.tag_count[tag] = 1 # Check if capturing the titles or h1s if tag == \'title\': self.capture_title = True elif tag == \'h1\': self.capture_h1 = True # Capture href attributes if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.urls.append(attr[1]) def handle_endtag(self, tag): # Turn off capturing when title or h1 tag ends if tag == \'title\': self.capture_title = False elif tag == \'h1\': self.capture_h1 = False def handle_data(self, data): # Extract text for specific tags if self.capture_title: self.title_text += data if self.capture_h1: self.h1_text += data def handle_comment(self, data): # Store comments self.comments.append(data)"},{"question":"# PyTorch Coding Assessment Question: CUDA Memory Management Objective: To assess your understanding of PyTorch\'s CUDA memory management functionalities and your ability to diagnose and optimize memory usage in a deep learning workflow. Problem Statement: You are given a hypothetical deep learning model that unexpectedly runs into CUDA out-of-memory (OOM) errors during training. Your task is to write a function to diagnose the memory usage and recommend if any optimizations can be made. Requirements: 1. Write a Python function `diagnose_cuda_memory(model, input_data, output_file)` that: - Records the memory usage history using `torch.cuda.memory._record_memory_history()`. - Runs one forward pass of the model with the given input data. - Dumps the memory snapshot into a file using `torch.cuda.memory._dump_snapshot(output_file)`. 2. The function `diagnose_cuda_memory` should have the following signature: ```python def diagnose_cuda_memory(model: torch.nn.Module, input_data: torch.Tensor, output_file: str) -> None: pass ``` 3. Ensure proper usage of the model and data to simulate a real-world scenario: - You may assume the provided model is a `torch.nn.Module` and the input data is a `torch.Tensor`. - Only use the specified PyTorch CUDA memory management functions for recording and dumping snapshots. Constraints: - You are not required to implement the actual model training or optimization logic. - Focus on capturing and saving the CUDA memory usage data effectively. Example: Here is a sample usage of the `diagnose_cuda_memory` function: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 5) def forward(self, x): return self.fc(x) # Initialize model and data model = SimpleModel().cuda() # Move model to GPU input_data = torch.randn(1, 10).cuda() # Generate random input data and move to GPU # Diagnose CUDA memory usage diagnose_cuda_memory(model, input_data, \\"memory_snapshot.pickle\\") ``` This usage example demonstrates how to initialize a simple model and input data, and then diagnose the CUDA memory usage by creating a snapshot file \\"memory_snapshot.pickle\\". Submission: Submit your Python function `diagnose_cuda_memory` that meets the above requirements. Ensure your code is properly commented to explain your logic.","solution":"import torch def diagnose_cuda_memory(model: torch.nn.Module, input_data: torch.Tensor, output_file: str) -> None: Diagnose CUDA memory usage by recording memory history and dumping a memory snapshot to a file. Args: - model (torch.nn.Module): The model to be diagnosed. - input_data (torch.Tensor): The input data for the model. - output_file (str): The file where the memory snapshot will be saved. Returns: - None # Start recording the memory history torch.cuda.memory._record_memory_history(True) # Run a forward pass with torch.no_grad(): # Ensure no gradients are calculated model(input_data) # Dump the memory snapshot torch.cuda.memory._dump_snapshot(output_file)"},{"question":"# Adding Custom Install Commands and File Filtering You are tasked with creating a script to manage the installation of a Python project\'s components and files. The script should provide functionalities similar to those described in the `install` and `sdist` command families. Part 1: Implementing File Filtering Implement a function `filter_files(paths, include_patterns, exclude_patterns)` that takes the following inputs: - `paths`: A list of file paths (strings) to be considered. - `include_patterns`: A list of Unix-style glob patterns (strings) specifying which files to include. - `exclude_patterns`: A list of Unix-style glob patterns (strings) specifying which files to exclude. The function should return a list of file paths that match the include patterns but do not match any of the exclude patterns. **Constraints:** - The function should support Unix-style glob patterns (\\"*\\", \\"?\\", \\"[range]\\"). - The function should handle platforms where the file path separator could be either \\"/\\" (Unix) or \\"\\" (Windows). **Function Signature:** ```python import fnmatch def filter_files(paths, include_patterns, exclude_patterns): # Your code here pass ``` **Example:** ```python paths = [ \\"src/module1.py\\", \\"src/module2.py\\", \\"tests/test_module1.py\\", \\"docs/README.md\\", \\"docs/index.html\\" ] include_patterns = [\\"*.py\\", \\"*.md\\"] exclude_patterns = [\\"tests/*\\", \\"docs/index.html\\"] filtered = filter_files(paths, include_patterns, exclude_patterns) print(filtered) # Output: [\\"src/module1.py\\", \\"src/module2.py\\", \\"docs/README.md\\"] ``` Part 2: Implementing Custom Install Commands Expand your script to support custom install commands `install_data` and `install_scripts`. Implement two functions `install_data` and `install_scripts`. Each function should take a list of file paths and a target directory, copying the specified files to the target directory. - `install_data(data_files, target_directory)`: Copies data files to the target directory. - `install_scripts(scripts, target_directory)`: Copies script files to the target directory. **Constraints:** - Assume that the input paths are always valid. - Ensure your function maintains the directory structure relative to the target directory. **Function Signatures:** ```python import os import shutil def install_data(data_files, target_directory): # Your code here pass def install_scripts(scripts, target_directory): # Your code here pass ``` **Example:** ```python data_files = [\\"data/config.json\\", \\"data/info.txt\\"] scripts = [\\"scripts/setup.py\\", \\"scripts/deploy.sh\\"] target_directory = \\"install_dir\\" install_data(data_files, target_directory) install_scripts(scripts, target_directory) # Expected folder structure inside \'install_dir\': # install_dir/ #  data/ #   config.json #   info.txt #  scripts/ #   setup.py #   deploy.sh ``` Your task is to implement both `filter_files`, `install_data`, and `install_scripts` functions to demonstrate your understanding of file operations, pattern matching, and proper command behavior in Python.","solution":"import fnmatch import os import shutil def filter_files(paths, include_patterns, exclude_patterns): Filters the given file paths based on include and exclude patterns. :param paths: List of file paths to consider. :param include_patterns: List of Unix-style glob patterns specifying which files to include. :param exclude_patterns: List of Unix-style glob patterns specifying which files to exclude. :return: List of file paths that match the include patterns but do not match any of the exclude patterns. included_files = set() excluded_files = set() for path in paths: if any(fnmatch.fnmatch(path, pattern) for pattern in include_patterns): included_files.add(path) if any(fnmatch.fnmatch(path, pattern) for pattern in exclude_patterns): excluded_files.add(path) return list(included_files - excluded_files) def install_data(data_files, target_directory): Copies data files to the target directory, preserving their relative structure. :param data_files: List of data file paths to copy. :param target_directory: The target directory where the data files should be copied. for file_path in data_files: target_path = os.path.join(target_directory, file_path) os.makedirs(os.path.dirname(target_path), exist_ok=True) shutil.copy(file_path, target_path) def install_scripts(scripts, target_directory): Copies script files to the target directory, preserving their relative structure. :param scripts: List of script file paths to copy. :param target_directory: The target directory where the script files should be copied. for file_path in scripts: target_path = os.path.join(target_directory, file_path) os.makedirs(os.path.dirname(target_path), exist_ok=True) shutil.copy(file_path, target_path)"},{"question":"You are required to develop a small web application script in Python that intentionally raises an exception and uses the `cgitb` module to handle and display the traceback in both text and HTML formats. Requirements 1. **Exception Handling**: Your script must intentionally raise an exception. 2. **cgitb Integration**: Use the `cgitb` module to handle the exception. 3. **Logging**: Save the HTML formatted traceback to a file named `error_log.html`. 4. **Output**: Display the traceback in text format in the console. Instructions 1. Import the necessary modules (`cgitb` and `sys`). 2. Write a function `raise_exception()` that deliberately raises a `ValueError` with a descriptive error message. 3. In your main script, use `cgitb.enable(logdir=\'.\', format=\'html\')` to enable HTML logging. 4. Within a `try-except` block, call the `raise_exception()` function and catch the exception. 5. Use `cgitb.text()` to convert the traceback info to text and print it to the console. 6. Use `cgitb.html()` to convert the traceback info to HTML format and save it to `error_log.html`. Input There is no user input required for the script. Output The console output should display the traceback information in text format, and an HTML file `error_log.html` should be created in the current directory containing the HTML formatted traceback. Script Template ```python import cgitb import sys def raise_exception(): # Intentionally raise an exception raise ValueError(\\"This is a deliberate exception\\") if __name__ == \\"__main__\\": # Enable cgitb with HTML formatting and logging to the current directory cgitb.enable(logdir=\'.\', format=\'html\') try: raise_exception() except Exception as e: # Capture the exception info info = sys.exc_info() # Get the text formatted traceback text_traceback = cgitb.text(info) print(text_traceback) # Get the HTML formatted traceback html_traceback = cgitb.html(info) # Save the HTML formatted traceback to a file with open(\'error_log.html\', \'w\') as log_file: log_file.write(html_traceback) ``` This task assesses the student\'s grasp of exception handling in Python, as well as their ability to utilize the `cgitb` module for debugging and logging purposes.","solution":"import cgitb import sys def raise_exception(): # Intentionally raise an exception raise ValueError(\\"This is a deliberate exception\\") if __name__ == \\"__main__\\": # Enable cgitb with HTML formatting and logging to the current directory cgitb.enable(logdir=\'.\', format=\'html\') try: raise_exception() except Exception as e: # Capture the exception info info = sys.exc_info() # Get the text formatted traceback text_traceback = cgitb.text(info) print(text_traceback) # Get the HTML formatted traceback html_traceback = cgitb.html(info) # Save the HTML formatted traceback to a file with open(\'error_log.html\', \'w\') as log_file: log_file.write(html_traceback)"},{"question":"# Custom Wrapper Function using Python builtins Module Problem Statement You are required to implement a function `custom_open(path: str, method: str) -> custom_file`, which wraps the built-in `open()` function to add custom functionality. The `custom_file` class should behave like a standard file object but must include additional features. Features to Implement: 1. **Line Reversal:** Modify the `readline()` method of the file object to return each line in reverse order. 2. **Character Frequency:** Implement a method `char_frequency(char: str) -> int` in the `custom_file` class that returns the frequency of the specified character in the entire file. 3. **Case Insensitive Read:** Implement a method `read_case_insensitive() -> List[str]` in the `custom_file` class. This should return the content of the file in a list of lines, where each line is in lower case. Function Signature ```python import builtins class custom_file: def __init__(self, file_obj): self.file_obj = file_obj self.content = self.file_obj.read() self.file_obj.seek(0) # Reset file pointer to the beginning def readline(self) -> str: line = self.file_obj.readline() return line[::-1] def char_frequency(self, char: str) -> int: return self.content.count(char) def read_case_insensitive(self) -> List[str]: return [line.lower() for line in self.content.splitlines()] def custom_open(path: str, method: str) -> custom_file: f = builtins.open(path, method) return custom_file(f) ``` Input - `path`: A string representing the path to the file to be opened. - `method`: A string indicating the mode in which the file will be opened (e.g., `\'r\'` for read). Output - Returns an instance of the `custom_file` class that wraps the built-in file object with additional functionality. Constraints - You can assume the file exists and is accessible using the provided path. - The methods should handle typical file content gracefully and should be optimized for performance. Example Usage ```python # Assuming \'example.txt\' contains the following lines: # Hello World # Python is great file = custom_open(\'example.txt\', \'r\') print(file.readline()) # Output: \\"dlroW olleH\\" print(file.char_frequency(\'o\')) # Output: 3 print(file.read_case_insensitive()) # Output: [\\"hello world\\", \\"python is great\\"] ``` Notes - Ensure that class methods handle file streaming operations smoothly and that file pointers are managed correctly. - The `char_frequency` method should be case sensitive when counting character occurrences.","solution":"import builtins from typing import List class custom_file: def __init__(self, file_obj): self.file_obj = file_obj self.content = self.file_obj.read() self.file_obj.seek(0) # Reset file pointer to the beginning def readline(self) -> str: line = self.file_obj.readline() return line[::-1] def char_frequency(self, char: str) -> int: return self.content.count(char) def read_case_insensitive(self) -> List[str]: return [line.lower() for line in self.content.splitlines()] def close(self): self.file_obj.close() def custom_open(path: str, method: str) -> custom_file: f = builtins.open(path, method) return custom_file(f)"},{"question":"Objective: Demonstrate your understanding of the `itertools` module by composing and transforming iterators. You are required to use multiple `itertools` functions to solve a realistic problem involving both finite and potentially infinite sequences. Problem Statement: You are given an infinite sequence of integers starting from 1. Implement a function `calculate_moving_sum` that computes a moving sum (rolling sum) over a given window size for this sequence. The function should return the first `n` elements of this moving sum sequence. Specifications: - Function Signature: `def calculate_moving_sum(window_size: int, n: int) -> List[int]:` - **Input:** - `window_size` (int): Size of the window for the moving sum. - `n` (int): Number of elements to return from the moving sum sequence. - **Output:** - List of the first `n` elements of the moving sum sequence. Details: - **Moving Sum**: The moving sum over a window size of `3` for the sequence `1, 2, 3, 4, 5, ...` results in the sequence `6, 9, 12, 15, ...` calculated as `(1+2+3), (2+3+4), (3+4+5), ...`. - Use the `itertools` module to handle the infinite nature of the input sequence and to efficiently compute the moving sums. - Utilize the `islice` function from `itertools` to limit the output to `n` elements. Constraints: - `1 <= window_size <= 100` - `1 <= n <= 1000` Example: ```python def calculate_moving_sum(window_size: int, n: int) -> List[int]: # Your implementation here pass # Example Usage print(calculate_moving_sum(3, 5)) # Output: [6, 9, 12, 15, 18] ``` **Notes:** - Ensure your solution is efficient and leverages `itertools` to handle the potentially infinite input sequence. - Pay attention to the functional style of combining iterators as demonstrated in the documentation. Hints: - Consider `itertools.count` for generating the infinite sequence. - Use `itertools.islice` to restrict the output to `n` elements. - Think about how `itertools.accumulate` can be used in combination with `itertools.islice` to compute the moving sums efficiently.","solution":"from typing import List import itertools def calculate_moving_sum(window_size: int, n: int) -> List[int]: Computes a moving sum (rolling sum) over a given window size for an infinite sequence of integers starting from 1. Returns the first `n` elements of the moving sum sequence. # Generate an infinite sequence starting from 1 infinite_sequence = itertools.count(1) # Create a rolling window iterator rolling_windows = itertools.tee(infinite_sequence, window_size) # Offset each window by its position for i, window in enumerate(rolling_windows): for _ in range(i): next(window, None) # Sum over the window size summed_windows = (sum(values) for values in zip(*rolling_windows)) # Fetch the first `n` elements from the resultant sequence result = list(itertools.islice(summed_windows, n)) return result"},{"question":"Objective Demonstrate comprehension of Python\'s `PyCapsule` API by implementing functions to create, manage, and validate capsules. Description You are required to write a Python C-Extension module named `capsule_module`. This module should provide the following functions: 1. **create_capsule**: - **Input**: A `void*` pointer and an optional `char*` name. - **Output**: A `PyObject*` which is a new `PyCapsule`. - **Constraints**: The pointer must not be `NULL`. 2. **get_capsule_pointer**: - **Input**: A `PyCapsule*` and an optional `char*` name. - **Output**: The `void*` pointer stored in the capsule if the `name` matches; otherwise, raise an appropriate error. 3. **destroy_capsule**: - **Input**: A `PyCapsule*`. - **Output**: None. The capsule should be appropriately destroyed using its destructor. 4. **validate_capsule**: - **Input**: A `PyCapsule*` and a `char*` name. - **Output**: A boolean indicating whether the capsule is valid and its name matches the provided name. 5. **set_capsule_name**: - **Input**: A `PyCapsule*` and a `char*` name. - **Output**: None. Set the name of the capsule to the provided name. Specific functionalities: - The `PyCapsule_Destructor` should simply log that the capsule is being destroyed. - Provide error handling for the case where the capsule operations fail. - Ensure the functions return the proper Python objects and handle Python exceptions where necessary. Python Example Usage Assuming the module is correctly compiled and imported as `capsule_module`, the following Python code would demonstrate its usage: ```python import capsule_module # Create a new capsule capsule = capsule_module.create_capsule(pointer_value, \\"example.name\\") # Validate the capsule valid = capsule_module.validate_capsule(capsule, \\"example.name\\") print(f\\"Capsule valid: {valid}\\") # Retrieve the pointer from the capsule pointer = capsule_module.get_capsule_pointer(capsule, \\"example.name\\") print(f\\"Pointer retrieved: {pointer}\\") # Set a new name for the capsule capsule_module.set_capsule_name(capsule, \\"new.name\\") # Destroy the capsule capsule_module.destroy_capsule(capsule) ``` Constraints - The `void*` pointer used in `create_capsule` should not be `NULL`. - Ensure robust error handling for all operations. - Properly manage memory and ensure that no memory leaks occur during the lifecycle of the capsule. Provide the complete C-Extension module code in a single `.c` file.","solution":"import ctypes class Capsule: def __init__(self, data, name): if data is None: raise ValueError(\\"data pointer cannot be None\\") self.data = data self.name = name self.is_destroyed = False def get_pointer(self, name=None): if self.is_destroyed: raise ValueError(\\"capsule has been destroyed\\") if name and self.name != name: raise ValueError(\\"name does not match\\") return self.data def destroy(self): self.is_destroyed = True def validate(self, name): if self.is_destroyed: return False return self.name == name def set_name(self, name): if self.is_destroyed: raise ValueError(\\"capsule has been destroyed\\") self.name = name def create_capsule(data, name=None): return Capsule(data, name) def get_capsule_pointer(capsule, name=None): return capsule.get_pointer(name) def destroy_capsule(capsule): capsule.destroy() def validate_capsule(capsule, name): return capsule.validate(name) def set_capsule_name(capsule, name): capsule.set_name(name)"},{"question":"**Question:** You are provided with a dataset called `penguins` that contains information about penguin species, their physical measurements, and island locations. Your task is to create a complex multi-plot grid using Seaborn\'s `FacetGrid` and `PairGrid` functionalities to explore this dataset. To complete this task, follow these steps: 1. Load the `penguins` dataset using Seaborn\'s `load_dataset` function. 2. Create a `FacetGrid` that shows the distribution of one continuous variable (`bill_length_mm`) across different categories (`species` and `island`). Use histograms for the plots and set the figure height to 3 and the aspect ratio to 0.8. 3. Customize the appearance of the `FacetGrid` by adding titles, labels, and a legend. 4. Create a `PairGrid` that explores pairwise relationships between continuous variables (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`). Use scatter plots for the off-diagonal plots and KDE plots for the diagonal. 5. Color the observations by the `species` variable in the `PairGrid`. 6. Further customize the `PairGrid` by modifying the markers and adding a legend. **Input Format:** The `penguins` dataset can be loaded using: ```python penguins = sns.load_dataset(\\"penguins\\") ``` **Output:** Your code should generate two plots: 1. A `FacetGrid` with histograms of `bill_length_mm` across `species` and `island`. 2. A `PairGrid` with pairwise scatter and KDE plots of the continuous variables, colored by `species`. **Constraints:** - Ensure that all plots are well-labeled and aesthetically pleasing. - Use appropriate customizations to enhance the readability of the plots. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a FacetGrid facet = sns.FacetGrid(penguins, col=\\"island\\", row=\\"species\\", height=3, aspect=0.8) facet.map(sns.histplot, \\"bill_length_mm\\") facet.set_axis_labels(\\"Bill Length (mm)\\", \\"Count\\") facet.add_legend() facet.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") plt.show() # Task 2: Create a PairGrid pair = sns.PairGrid(penguins, vars=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"], hue=\\"species\\") pair.map_diag(sns.kdeplot) pair.map_offdiag(sns.scatterplot) pair.add_legend() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a FacetGrid def plot_facetgrid(): facet = sns.FacetGrid(penguins, col=\\"island\\", row=\\"species\\", height=3, aspect=0.8) facet.map(sns.histplot, \\"bill_length_mm\\") facet.set_axis_labels(\\"Bill Length (mm)\\", \\"Count\\") facet.add_legend() facet.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") plt.show() # Task 2: Create a PairGrid def plot_pairgrid(): pair = sns.PairGrid(penguins, vars=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"], hue=\\"species\\") pair.map_diag(sns.kdeplot) pair.map_offdiag(sns.scatterplot) pair.add_legend() plt.show() # Function calls for plotting (to be used for actual plotting, not in unit tests) # plot_facetgrid() # plot_pairgrid()"},{"question":"Objective To assess the understanding and application of the Seaborn `objects` interface for creating layered plots with various customizations. Problem Statement Using the Seaborn `objects` interface, visualize the relationship between total bill and tip amounts from the `tips` dataset. You need to implement a Python function `visualize_tips_data` that creates a comprehensive plot as described below: 1. **Scatter Plot Layer**: - Create a scatter plot (dots) showing the relationship between `total_bill` and `tip`. - The size of the dots should be mapped to the `size` column. - The color of the dots should represent the `sex` column. 2. **Line Plot Layer**: - Add a line that represents a polynomial regression (using `PolyFit`) to fit the relationship between `total_bill` and `tip`. - Color the line in gray and set its linewidth to 2. 3. **Facet by Day**: - Facet the plot by the `day` of the week, creating a small multiples plot for each day. 4. **Customization**: - Set a meaningful title for the overall plot and for the y-axis label. - Customize the legend to indicate what the point sizes mean and what colors represent the gender. Function Signature ```python def visualize_tips_data(): pass ``` Requirements - Use the `seaborn.objects` interface. - Use appropriate `Mark`, `Stat`, and transformation functionalities. - Ensure that the plot is well-labeled and easy to interpret. Example The function does not take any input nor return any output. It should directly display the plot as described. ```python def visualize_tips_data(): # Your implementation here import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\") # Creating the plot plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\") .facet(col=\\"day\\") .add(so.Dot(), pointsize=\\"size\\", color=\\"sex\\") .add(so.Line(color=\\".5\\", linewidth=2), so.PolyFit()) .scale(pointsize=(2, 10)) .label(y=\\"Tip Amount\\") .label(title=\\"Relationship between Total Bill and Tip Amount across Days\\") ) return plot # Execute the function to visualize visualize_tips_data() ``` This problem assesses the ability to use advanced functionalities of the Seaborn `objects` interface, including layering, transformations, and faceting, as well as the ability to create informative and aesthetically pleasing visualizations.","solution":"def visualize_tips_data(): import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\") # Creating the plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") .facet(col=\\"day\\") .add(so.Dot(), pointsize=\\"size\\", color=\\"sex\\") .add(so.Line(color=\\".5\\", linewidth=2), so.PolyFit(order=2)) .scale(pointsize=(2, 10)) .label(y=\\"Tip Amount\\") .label(title=\\"Relationship between Total Bill and Tip Amount across Days\\") ) # Display the plot plot.show()"},{"question":"# Floating Point Precision and Exact Arithmetic in Python Python\'s floating-point arithmetic can often produce surprising results due to the way numbers are represented in binary. This problem aims to test your understanding of these issues and your ability to address them using Python\'s tools. Problem Statement You are given a list of floating-point numbers representing prices of items in USD. Your task is to: 1. Implement a function `calculate_total` which takes a list of floating-point numbers (prices) and returns the total sum using floating-point arithmetic. 2. Implement the same as above using Python\'s `decimal.Decimal` for exact decimal arithmetic. 3. Implement another function `calculate_total_fractions` which uses Python\'s `fractions.Fraction` for exact arithmetic based on rational numbers. 4. Implement another function `calculate_total_fsum` which uses `math.fsum` to return the total sum. Additionally, you are required to: - Compare the results of all four implementations. - Determine and explain (via comments in your code) which implementations give the most accurate results and why. Function Signatures ```python def calculate_total(prices: list) -> float: pass def calculate_total_decimal(prices: list) -> str: pass def calculate_total_fractions(prices: list) -> str: pass def calculate_total_fsum(prices: list) -> float: pass ``` Input - `prices`: A list of floating-point numbers (1  len(prices)  10^5). Each element (price) will be within the range [0.01, 1000.00]. Output - The output from `calculate_total` and `calculate_total_fsum` will be a floating-point number representing the total sum. - The output from `calculate_total_decimal` and `calculate_total_fractions` will be a string representing the total sum to avoid floating-point rounding issues. Constraints - You must ensure that the precision is preserved and demonstrate the effects of floating-point arithmetic on summation. Example ```python prices = [0.1, 0.1, 0.1, 0.3] print(calculate_total(prices)) # Expected output may not be exactly 0.6 due to floating-point inaccuracies print(calculate_total_decimal(prices)) # Expected: \'0.6\' print(calculate_total_fractions(prices)) # Expected: \'0.6\' print(calculate_total_fsum(prices)) # Expected: 0.6 ``` Explanation - The `calculate_total` function will show the floating-point summation of the prices, demonstrating potential inaccuracies. - The `calculate_total_decimal` function will use the `decimal.Decimal` module to show precise decimal arithmetic. - The `calculate_total_fractions` function will use the `fractions.Fraction` module for rational number arithmetic. - The `calculate_total_fsum` function will use `math.fsum` to mitigate loss of precision during summation. By comparing these implementations, you will understand how different approaches handle floating-point precision and which provides the most accurate results.","solution":"from decimal import Decimal from fractions import Fraction import math def calculate_total(prices: list) -> float: Returns the total sum of the prices using floating-point arithmetic. return sum(prices) def calculate_total_decimal(prices: list) -> str: Returns the total sum of the prices using decimal.Decimal for exact arithmetic. total = Decimal(\'0\') for price in prices: total += Decimal(str(price)) return str(total) def calculate_total_fractions(prices: list) -> str: Returns the total sum of the prices using fractions.Fraction for exact arithmetic. total = Fraction(0) for price in prices: total += Fraction(price) return str(float(total)) def calculate_total_fsum(prices: list) -> float: Returns the total sum of the prices using math.fsum for improved floating-point accuracy. return math.fsum(prices)"},{"question":"# Python Curses Programming Assessment Question Objective: To assess your understanding of Python\'s `curses` module and ability to use it to create a simple text-based user interface. Problem Statement: Create a Python program using the `curses` module that implements a simple text-based user interface. Your program should display a menu, allow text input, and perform actions based on user input. Requirements: 1. **Menu Display**: The program should display a menu with the following options: - View Message - Edit Message - Clear Message - Quit 2. **User Input Handling**: - When the user selects **View Message**, the current message should be displayed on the screen. - When the user selects **Edit Message**, the user should be prompted to enter a new message. Use `curses.textpad` for this input operation. - When the user selects **Clear Message**, the current message should be erased. - When the user selects **Quit**, the program should terminate gracefully, restoring the terminal to its original state. 3. **Initial Setup and Cleanup**: Ensure you handle the initialization and termination of the curses application using best practices. Handle any exceptions to ensure the terminal is not left in an inconsistent state. 4. **Attributes and Colors**: Use at least one text attribute or color in the program, such as highlighting the menu options or displaying the message in a different color. Input: - User interaction through the terminal to select menu options and enter text. Output: - The terminal-based UI displaying the menu, the current message, or prompts for editing the message. Constraints: - The program should not use external libraries other than `curses`. Example Layout: ``` +=========================+ | Simple Menu | +=========================+ | 1. View Message | | 2. Edit Message | | 3. Clear Message | | 4. Quit | +=========================+ ``` Hints: - Use `wrapper(main)` from the `curses` module to simplify initialization and cleanup. - Use `addstr()` with coordinates to display text at specific locations. - Use `curses.textpad.Textbox` for the text editing functionality. - Handle user input using `getkey()` or `getch()`. Submission: Submit your Python code implementing the above requirements. Ensure your code is well-documented and handles terminal cleanup properly.","solution":"import curses from curses import wrapper import curses.textpad def main(stdscr): # Setup curses.curs_set(0) stdscr.clear() stdscr.refresh() current_message = \\"\\" while True: stdscr.clear() # Display Menu stdscr.addstr(1, 2, \\"Simple Menu\\", curses.A_BOLD) stdscr.addstr(3, 2, \\"1. View Message\\") stdscr.addstr(4, 2, \\"2. Edit Message\\") stdscr.addstr(5, 2, \\"3. Clear Message\\") stdscr.addstr(6, 2, \\"4. Quit\\") stdscr.refresh() # Get user input key = stdscr.getch() if key == ord(\'1\'): # View Message stdscr.clear() stdscr.addstr(1, 2, \\"Current Message:\\", curses.A_BOLD) stdscr.addstr(3, 2, current_message) stdscr.addstr(5, 2, \\"Press any key to return to the menu.\\") stdscr.refresh() stdscr.getch() elif key == ord(\'2\'): # Edit Message stdscr.clear() stdscr.addstr(1, 2, \\"Enter new message (press Ctrl-G to save):\\", curses.A_BOLD) editwin = curses.newwin(1, 40, 3, 2) rectangle(stdscr, 2, 1, 4, 43) stdscr.refresh() box = curses.textpad.Textbox(editwin) current_message = box.edit() elif key == ord(\'3\'): # Clear Message current_message = \\"\\" elif key == ord(\'4\'): # Quit break def rectangle(stdscr, uly, ulx, lry, lrx): Draw a rectangle with the given corners. stdscr.hline(uly, ulx, curses.ACS_HLINE, lrx - ulx) stdscr.hline(lry, ulx, curses.ACS_HLINE, lrx - ulx) stdscr.vline(uly, ulx, curses.ACS_VLINE, lry - uly) stdscr.vline(uly, lrx, curses.ACS_VLINE, lry - uly) stdscr.addch(uly, ulx, curses.ACS_ULCORNER) stdscr.addch(uly, lrx, curses.ACS_URCORNER) stdscr.addch(lry, ulx, curses.ACS_LLCORNER) stdscr.addch(lry, lrx, curses.ACS_LRCORNER) if __name__ == \\"__main__\\": wrapper(main)"},{"question":"**Question: Persistent Storage Manager with shelve** Using Python\'s `shelve` module, implement a class `PersistentStorage` that manages a persistent dictionary with additional safe handling and convenience methods. Your implementation should demonstrate comprehension of persistent storage concepts and context management. # Implementation Details: 1. **Class Definition**: Define a class `PersistentStorage` that accepts a filename and optional `writeback` parameter during initialization. 2. **Initialization**: In the `__init__` method, open the shelf with the provided filename and writeback option, and store the reference to the open shelf. 3. **Context Management**: Implement context management methods (`__enter__` and `__exit__`) to automatically handle opening and closing the shelf. 4. **Add Data**: Implement a method `add_data(self, key, value)` to store a key-value pair in the shelf. 5. **Retrieve Data**: Implement a method `get_data(self, key)` to retrieve data associated with a key. If the key does not exist, return `None`. 6. **Delete Data**: Implement a method `delete_data(self, key)` to delete the data associated with a key. If the key does not exist, raise a `KeyError`. 7. **List Keys**: Implement a method `list_keys(self)` to return a list of all keys in the shelf. 8. **Close Storage**: Implement a method `close(self)` to close the shelf explicitly. # Constraints: - Keys should be strings. - Values can be arbitrary Python objects that the `pickle` module can handle. - Ensure that methods handle exceptions where applicable (e.g., non-existing keys). # Example Usage: ```python # Usage example: ps = PersistentStorage(\'my_shelf.db\', writeback=True) # Adding data ps.add_data(\'name\', \'Alice\') ps.add_data(\'age\', 30) # Retrieving data print(ps.get_data(\'name\')) # Output: Alice print(ps.get_data(\'age\')) # Output: 30 print(ps.get_data(\'unknown\')) # Output: None # Listing all keys print(ps.list_keys()) # Output: [\'name\', \'age\'] # Deleting data ps.delete_data(\'name\') # List keys after deletion print(ps.list_keys()) # Output: [\'age\'] # Close the shelf ps.close() ``` Implement the `PersistentStorage` class adhering to the above specifications and example.","solution":"import shelve class PersistentStorage: def __init__(self, filename, writeback=False): self.filename = filename self.writeback = writeback self.shelf = shelve.open(self.filename, writeback=self.writeback) def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): self.shelf.close() def add_data(self, key, value): self.shelf[key] = value def get_data(self, key): return self.shelf.get(key, None) def delete_data(self, key): if key in self.shelf: del self.shelf[key] else: raise KeyError(f\'Key {key} not found.\') def list_keys(self): return list(self.shelf.keys()) def close(self): self.shelf.close()"},{"question":"**Objective:** Implement a Python function which initializes a Python configuration using provided settings and simulates the initialization process as described in the given documentation. **Description:** You are required to create a function `initialize_python(settings: dict) -> str` which initializes Python based on configuration settings provided in the `settings` dictionary. The function should perform the following tasks: 1. **Create and preinitialize Python configuration**: - Preinitialize Python using the `Py_PreInitialize` method. 2. **Initialize Python configuration**: - Use `PyConfig_InitPythonConfig` or `PyConfig_InitIsolatedConfig` based on the `settings[\'isolation_mode\']` value. - Set various configuration settings as specified in the `settings` dictionary. 3. **Handle command line arguments**: - Set command line arguments using `PyConfig_SetBytesArgv`. 4. **Simulate initialization process**: - Use `Py_InitializeFromConfig` to finalize Python initialization. - Return proper status messages (\\"Success\\" or \\"Failure\\") based on the initialization process. **Function Signature:** ```python def initialize_python(settings: dict) -> str: # Implementation here pass ``` **Input:** - A dictionary `settings` containing: - `isolation_mode`: (Boolean) True for isolated mode, False for regular mode. - `argv`: (list) command line arguments to be passed. - Other keys corresponding to advanced configuration fields as required. **Output:** - Return \\"Success\\" if the initialization process completes successfully. - Return \\"Failure\\" if an error or exit status is encountered at any step in the process. **Example:** ```python settings = { \\"isolation_mode\\": False, \\"argv\\": [\\"python\\", \\"-c\\", \\"print(\'Hello, World!\')\\"], \\"utf8_mode\\": 1, \\"use_environment\\": 1, \\"dev_mode\\": 0, } result = initialize_python(settings) print(result) # Should output \\"Success\\" if implementation is correct ``` Implement the `initialize_python` function based on the above requirements using the information from the provided documentation. **Constraints:** - The function should handle various field configurations as outlined in the documentation. - Properly manage memory and handle exceptions using `PyStatus` functions to ensure no memory leaks or unhandled exit statuses.","solution":"def initialize_python(settings: dict) -> str: try: import sysconfig sysconfig.get_config_var(\'Py_ENABLE_SHARED\') # Simulated Preinitialize and Config initialization def Py_PreInitialize(): pass # Here we assume pre-initialization logic def PyConfig_InitPythonConfig(config): config.update({ \'isolation_mode\': False, \'utf8_mode\': 0, \'use_environment\': 1, \'dev_mode\': 0 }) def PyConfig_InitIsolatedConfig(config): config.update({ \'isolation_mode\': True, \'utf8_mode\': 0, \'use_environment\': 1, \'dev_mode\': 0 }) def PyConfig_SetBytesArgv(config, argv): config[\'argv\'] = argv def Py_InitializeFromConfig(config): return \\"Success\\" # Simulated success for example purposes # Step 1: Preinitialize Python Py_PreInitialize() # Step 2: Initialize Python config config = {} if settings[\'isolation_mode\']: PyConfig_InitIsolatedConfig(config) else: PyConfig_InitPythonConfig(config) # Update config with settings from the dictionary for key, value in settings.items(): if key in config: config[key] = value # Step 3: Handle command line arguments if \'argv\' in settings: PyConfig_SetBytesArgv(config, settings[\'argv\']) # Step 4: Simulate initialization process status = Py_InitializeFromConfig(config) return status except Exception as e: return \\"Failure\\" settings = { \\"isolation_mode\\": False, \\"argv\\": [\\"python\\", \\"-c\\", \\"print(\'Hello, World!\')\\"], \\"utf8_mode\\": 1, \\"use_environment\\": 1, \\"dev_mode\\": 0, } initialize_python(settings)"},{"question":"Objective Your task is to demonstrate the understanding of the `pipes.Template` class by creating a pipeline that processes a text file using various shell commands. Problem Statement Write a Python function `process_text_file(input_file: str, output_file: str) -> None` that reads the contents of a given text file, processes it using a series of shell commands, and writes the processed contents to an output file. Specifically, the pipeline should convert all text to uppercase, sort the lines alphabetically, and remove duplicate lines. You must use the `pipes.Template` class to implement this function. Requirements 1. The function should read from `input_file`. 2. It should use the `pipes.Template` class to create a pipeline that: - Converts all text to uppercase (`tr a-z A-Z`). - Sorts the lines alphabetically (`sort`). - Removes duplicate lines (`uniq`). 3. The function should write the processed text to `output_file`. Constraints - Assume that the input file will contain only plain text. - Ensure proper handling of file I/O operations. - Do not use external libraries apart from the standard library imports (`pipes` and other essential modules). Input - `input_file` (str): The path to the input text file. - `output_file` (str): The path to the output text file. Output - None (The result should be written to `output_file`). Example Suppose `input_file` contains the following content: ``` banana Apple apple Orange Banana orange apple Banana ``` After processing, `output_file` should contain: ``` APPLE BANANA ORANGE ``` Function Signature ```python def process_text_file(input_file: str, output_file: str) -> None: pass ``` Implementation Note You need to use the `pipes` module to create the necessary pipeline and handle file operations following the provided documentation.","solution":"import pipes def process_text_file(input_file: str, output_file: str) -> None: Processes a text file by converting text to uppercase, sorting lines alphabetically, and removing duplicates. Parameters: - input_file: Path to the input file - output_file: Path to the output file t = pipes.Template() t.append(\'tr a-z A-Z\', \'--\') t.append(\'sort\', \'--\') t.append(\'uniq\', \'--\') with t.open(output_file, \'w\') as f_out: with open(input_file, \'r\') as f_in: f_out.write(f_in.read())"},{"question":"# Unicode String Manipulation in Python Objective Implement a function `normalize_compare` that takes two strings and returns whether they are equivalent under Unicode normalization. Description Write a function `normalize_compare` that checks if two Unicode strings are equivalent after normalizing them. The function will take two strings as input and normalize them to a specified normalization form. The normalization form can be \'NFC\', \'NFKC\', \'NFD\', or \'NFKD\'. Input - `str1` (String): The first Unicode string. - `str2` (String): The second Unicode string. - `form` (String): The normalization form to be used. It will be one of \'NFC\', \'NFKC\', \'NFD\', \'NFKD\'. Output - Return `True` if the two strings are equivalent under the specified normalization; otherwise, return `False`. Constraints 1. The input strings can contain any Unicode characters. 2. The normalization form will always be a valid option among \'NFC\', \'NFKC\', \'NFD\', \'NFKD\'. 3. The function should handle strings efficiently even if they contain a large number of characters. Example ```python def normalize_compare(str1, str2, form): import unicodedata normalized_str1 = unicodedata.normalize(form, str1) normalized_str2 = unicodedata.normalize(form, str2) return normalized_str1 == normalized_str2 # Example usage: str1 = \\"\\" str2 = \\"eu0302\\" # \'e\' + \'COMBINING CIRCUMFLEX ACCENT\' form = \\"NFC\\" print(normalize_compare(str1, str2, form)) # Output: True ``` Write the implementation for the `normalize_compare` function.","solution":"def normalize_compare(str1, str2, form): import unicodedata normalized_str1 = unicodedata.normalize(form, str1) normalized_str2 = unicodedata.normalize(form, str2) return normalized_str1 == normalized_str2"},{"question":"# Configuring Logging with `logging.config` in Python **Objective**: Demonstrate your understanding of the `logging.config` module by configuring the logging system using a dictionary and a file, then dynamically updating the logging configuration. **Task**: 1. **Initial Configuration**: - Create a logging configuration using a dictionary. - The configuration should include: - One `StreamHandler` with a `DEBUG` level and a custom formatter. - One `RotatingFileHandler` with an `ERROR` level, a custom formatter, and the following properties: - `filename`: \\"app.log\\" - `maxBytes`: 2000 - `backupCount`: 5 2. **Custom Formatter**: - Implement a custom formatter class `CustomFormatter` that inherits from `logging.Formatter`. - The formatter should format messages as: `[TIME] LEVEL: logger_name - message` - Integrate this formatter with both handlers. 3. **Save Configuration to File**: - Save the logging configuration to a configuration file `logging.conf` in `configparser` format. - The file should replicate the initial dictionary configuration. 4. **Dynamic Update**: - Implement a method to dynamically update the logging configuration using a new dictionary that modifies: - The `StreamHandler` to `INFO` level. - Add a new `SMTPHandler` with the following properties: - `mailhost`: \\"localhost\\" - `fromaddr`: \\"from@example.com\\" - `toaddrs`: [\\"to1@example.com\\", \\"to2@example.com\\"] - `subject`: \\"Error Log\\" - `level`: `ERROR` **Requirements**: - Your solution should incorporate exception handling to gracefully handle configuration errors. - Demonstrate logging with different log levels and output destinations both before and after the configuration update. - Provide a README file explaining how to run your code and test the configurations. **Input/Output Formats**: - **Input**: N/A - **Output**: Log messages will be directed to the console and \\"app.log\\" file, and after the dynamic update, an SMTP handler should send emails on error logs. **Constraints**: - Use `logging.config.dictConfig()` and `logging.config.fileConfig()` appropriately. - Ensure proper use of a custom formatter class. - Handle exceptions that are raised during logging configuration updates. **Performance**: - Ensure your solution efficiently updates the logging configuration without restarting the application. Good luck!","solution":"import logging import logging.config from logging.handlers import RotatingFileHandler, SMTPHandler import configparser class CustomFormatter(logging.Formatter): def format(self, record): record.msg = f\\"[{self.formatTime(record)}] {record.levelname}: {record.name} - {record.msg}\\" return super().format(record) initial_config_dict = { \'version\': 1, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'custom\' }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'ERROR\', \'formatter\': \'custom\', \'filename\': \'app.log\', \'maxBytes\': 2000, \'backupCount\': 5 }, }, \'formatters\': { \'custom\': { \'()\': CustomFormatter, \'format\': \'%(message)s\' } }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'] } } def save_config_to_file(config_dict, filename): config = configparser.ConfigParser() config[\'formatters\'] = { \'keys\': \'custom\' } config[\'formatter_custom\'] = { \'format\': \'%(message)s\' } config[\'handlers\'] = { \'keys\': \'console,file\' } config[\'handler_console\'] = { \'class\': \'StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'custom\', \'args\': \'()\', } config[\'handler_file\'] = { \'class\': \'handlers.RotatingFileHandler\', \'level\': \'ERROR\', \'formatter\': \'custom\', \'args\': \\"(\'app.log\', \'a\', 2000, 5)\\", } config[\'loggers\'] = { \'keys\': \'root\' } config[\'logger_root\'] = { \'level\': \'DEBUG\', \'handlers\': \'console,file\' } with open(filename, \'w\') as configfile: config.write(configfile) def update_logging_config(new_config_dict): try: logging.config.dictConfig(new_config_dict) logging.info(\\"Successfully updated logging configuration\\") except Exception as e: logging.error(f\\"Error updating logging configuration: {e}\\") # Initial logging configuration logging.config.dictConfig(initial_config_dict) # Save configuration to file save_config_to_file(initial_config_dict, \'logging.conf\') # Log some messages to demonstrate initial configuration logger = logging.getLogger() logger.debug(\\"This is a debug message\\") logger.error(\\"This is an error message\\") # New configuration dictionary for dynamic update new_config_dict = { \'version\': 1, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'INFO\', \'formatter\': \'custom\' }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'ERROR\', \'formatter\': \'custom\', \'filename\': \'app.log\', \'maxBytes\': 2000, \'backupCount\': 5 }, \'mail\': { \'class\': \'logging.handlers.SMTPHandler\', \'level\': \'ERROR\', \'mailhost\': \'localhost\', \'fromaddr\': \'from@example.com\', \'toaddrs\': [\'to1@example.com\', \'to2@example.com\'], \'subject\': \'Error Log\', \'credentials\': None, }, }, \'formatters\': { \'custom\': { \'()\': CustomFormatter, \'format\': \'%(message)s\' } }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\', \'mail\'] } } # Update logging configuration dynamically update_logging_config(new_config_dict) # Log some messages to demonstrate updated configuration logger.info(\\"This is an info message\\") logger.error(\\"This is another error message\\")"},{"question":"**Objective:** Demonstrate understanding of the `queue` module in Python by implementing a multi-threaded producer-consumer pattern using a `queue.PriorityQueue` and tracking task completion. **Problem Statement:** You are required to implement a simulation of a multi-threaded task scheduler using a priority queue. The scheduler will have multiple producer threads generating tasks with various priorities, and multiple consumer threads processing these tasks. Your task is to ensure all tasks are added to the queue with the correct priority and processed in the appropriate order. **Requirements:** 1. Implement a class `TaskScheduler` with the following methods: - `add_task(priority: int, task: str)`: Adds a new task with the given priority to the queue. - `start_consumers(num_consumers: int)`: Starts the specified number of consumer threads. - `wait_for_all_tasks()`: Blocks until all tasks in the queue have been processed. 2. The consumer threads should process tasks in priority order (lower numbers indicate higher priority). Each consumer should print the task it is processing. 3. Use the `queue.PriorityQueue` class for managing the tasks. 4. Use threading to create producer and consumer threads. **Constraints:** - Use the `PrioritizedItem` data class as shown in the provided documentation to manage task priorities. - Ensure thread safety where necessary. - The number of tasks, priorities, and threads should be flexible and not hardcoded. **Example:** ```python from queue import PriorityQueue from dataclasses import dataclass, field from typing import Any import threading @dataclass(order=True) class PrioritizedItem: priority: int item: Any=field(compare=False) class TaskScheduler: def __init__(self): self.queue = PriorityQueue() self.lock = threading.Lock() def add_task(self, priority: int, task: str): self.queue.put(PrioritizedItem(priority, task)) def start_consumers(self, num_consumers: int): for _ in range(num_consumers): t = threading.Thread(target=self._consume) t.start() def _consume(self): while True: item = self.queue.get() with self.lock: print(f\'Processing task: {item.item} with priority {item.priority}\') self.queue.task_done() def wait_for_all_tasks(self): self.queue.join() # Example usage scheduler = TaskScheduler() # Adding tasks with different priorities scheduler.add_task(3, \'Task A\') scheduler.add_task(1, \'Task B\') scheduler.add_task(2, \'Task C\') # Starting consumers scheduler.start_consumers(2) # Wait for all tasks to be processed scheduler.wait_for_all_tasks() ``` **Expected Output:** The tasks should be processed in the order of their priority: ``` Processing task: Task B with priority 1 Processing task: Task C with priority 2 Processing task: Task A with priority 3 ``` **Note:** - Ensure your implementation is thread-safe. - Consider edge cases such as when there are no tasks in the queue.","solution":"from queue import PriorityQueue from dataclasses import dataclass, field from typing import Any import threading @dataclass(order=True) class PrioritizedItem: priority: int item: Any=field(compare=False) class TaskScheduler: def __init__(self): self.queue = PriorityQueue() self.lock = threading.Lock() self._consumers = [] def add_task(self, priority: int, task: str): self.queue.put(PrioritizedItem(priority, task)) def start_consumers(self, num_consumers: int): for _ in range(num_consumers): t = threading.Thread(target=self._consume, daemon=True) t.start() self._consumers.append(t) def _consume(self): while True: item = self.queue.get() with self.lock: print(f\'Processing task: {item.item} with priority {item.priority}\') self.queue.task_done() def wait_for_all_tasks(self): self.queue.join() # Example usage if __name__ == \\"__main__\\": scheduler = TaskScheduler() # Adding tasks with different priorities scheduler.add_task(3, \'Task A\') scheduler.add_task(1, \'Task B\') scheduler.add_task(2, \'Task C\') # Starting consumers scheduler.start_consumers(2) # Wait for all tasks to be processed scheduler.wait_for_all_tasks()"},{"question":"# Seaborn Coding Assessment Question You are tasked with analyzing a dataset to diagnose any violations in linear regression assumptions using Seaborn\'s `residplot` function. # Requirements 1. **Load the Dataset:** - Load the \\"mpg\\" dataset using seaborn\'s `load_dataset` function. 2. **Create Basic Residual Plot:** - Generate a basic residual plot where the independent variable (x) is \\"weight\\" and the dependent variable (y) is \\"displacement.\\" 3. **Advanced Residual Analysis:** - Create another residual plot with \\"horsepower\\" as the independent variable and \\"mpg\\" as the dependent variable. - Modify this plot to: - Include a quadratic trend line (`order=2`). - Add a LOWESS curve to the plot with the line in red (`lowess=True, line_kws=dict(color=\\"r\\")`). # Expected Output 1. A function `analyze_mpg_data` that: - Loads the \\"mpg\\" dataset. - Creates and displays the required plots as specified in the requirements. # Constraints - Use only the Seaborn library for plotting. - The dataset should be loaded using the Seaborn library. # Example Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_mpg_data(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Basic residual plot plt.figure() sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual plot of weight vs displacement\\") # Advanced residual analysis plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual plot of horsepower vs mpg with quadratic trend\\") plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual plot of horsepower vs mpg with LOWESS smoothing\\") plt.show() # Call the function to display the plots analyze_mpg_data() ``` Ensure your function meets the outlined requirements and correctly displays the intended plots with necessary modifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_mpg_data(): Loads the \'mpg\' dataset and creates residual plots to analyze linear regression assumptions. # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Basic residual plot: weight vs displacement plt.figure() sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual plot of weight vs displacement\\") # Advanced residual analysis: horsepower vs mpg with quadratic trend plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual plot of horsepower vs mpg with quadratic trend\\") # Advanced residual analysis: horsepower vs mpg with LOWESS curve plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"red\\")) plt.title(\\"Residual plot of horsepower vs mpg with LOWESS smoothing\\") # Show all plots plt.show()"},{"question":"# Python Coding Assessment Question Objective This question is designed to test your understanding of cell objects in Python, specifically focusing on creating, checking, retrieving, and updating cell objects. Problem Statement You are required to write a Python class that simulates the behavior of cell objects using Python\'s built-in functionalities. Your implementation should include methods corresponding to the following C functions for cell objects: 1. `PyCell_Check(ob)` 2. `PyCell_New(PyObject *ob)` 3. `PyCell_Get(PyObject *cell)` 4. `PyCell_SET(PyObject *cell, PyObject *value)` Class Specification Implement a class named `SimulatedCell` with the following methods: 1. **`__init__(self, value=None)`** - Initializes a new `SimulatedCell` object containing the initial value `value`. - **Parameters:** 1. `value` (default is `None`): Initial value to be stored in the cell object. - **Output:** - None 2. **`is_cell(self)`** - Checks if the current object is a cell object. - **Parameters:** - None - **Output:** - Returns `True` if the object is a cell object, otherwise `False`. 3. **`get(self)`** - Retrieves the current value stored in the cell object. - **Parameters:** - None - **Output:** - Returns the current value stored in the cell object. 4. **`set(self, value)`** - Sets a new value in the cell object. - **Parameters:** - `value`: New value to be stored in the cell object. - **Output:** - None Constraints and Performance Requirements 1. Your implementation should correctly handle `None` values. 2. The class should be designed in such a way that it ensures proper encapsulation and follows Python coding conventions. Example ```python # Creating a new cell object with initial value 10 cell = SimulatedCell(10) # Checking if the created object is a cell object print(cell.is_cell()) # Output: True # Retrieving the current value stored in the cell object print(cell.get()) # Output: 10 # Setting a new value in the cell object cell.set(20) # Retrieving the updated value stored in the cell object print(cell.get()) # Output: 20 ``` Note - You should focus on implementing the above methods as described. - This is a simulation of low-level C functionalities in Python and should be implemented using Python\'s built-in features.","solution":"class SimulatedCell: def __init__(self, value=None): self._value = value def is_cell(self): return True def get(self): return self._value def set(self, value): self._value = value"},{"question":"**Configuration File Handling with `configparser`** **Objective:** Write a Python function that reads a configuration file, updates its contents, and retrieves interpolated values. **Problem Statement:** You are given a configuration file, `settings.ini`, which contains sections with various settings. Each section has key-value pairs, and some values may contain placeholders that need to be interpolated with values from other keys within the same section. Write a function `process_config(file_path: str, updates: dict) -> dict` that performs the following tasks: 1. Reads the configuration file from the specified `file_path`. 2. Updates the configuration with the contents of the `updates` dictionary. The dictionary has the following structure: ```python updates = { \\"section1\\": { \\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\" }, \\"section2\\": { \\"key1\\": \\"value3\\" } } ``` 3. Retrieves and returns all key-value pairs from each section after applying interpolation. **Input:** - `file_path` (str): The path to the configuration file. - `updates` (dict): A dictionary containing the updates to be applied to the configuration. **Output:** - Returns a dictionary with the updated and interpolated values from all sections of the configuration file. **Constraints:** - The configuration file follows the INI file structure. - Interpolation syntax follows the format `{key}`, where `key` refers to another key within the same section. **Example:** Suppose the content of `settings.ini` is: ```ini [section1] key1 = original_value1 key2 = original_value2 key3 = {key1} [section2] key1 = original_value3 key2 = {key1} ``` And the `updates` dictionary is: ```python updates = { \\"section1\\": { \\"key1\\": \\"new_value1\\" }, \\"section2\\": { \\"key1\\": \\"new_value3\\" } } ``` The function should return: ```python { \\"section1\\": { \\"key1\\": \\"new_value1\\", \\"key2\\": \\"original_value2\\", \\"key3\\": \\"new_value1\\" }, \\"section2\\": { \\"key1\\": \\"new_value3\\", \\"key2\\": \\"new_value3\\" } } ``` **Note:** - Ensure proper error handling for any interpolation issues or invalid updates. - You may use the `configparser` module from the Python Standard Library. **Function Signature:** ```python def process_config(file_path: str, updates: dict) -> dict: # TODO: Implement the function ``` **Write your solution as a function implementation below.**","solution":"import configparser def process_config(file_path: str, updates: dict) -> dict: config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation()) config.read(file_path) # Apply updates to the configuration for section, keys in updates.items(): if not config.has_section(section): config.add_section(section) for key, value in keys.items(): config.set(section, key, value) # Retrieve the values after interpolation result = {} for section in config.sections(): result[section] = {key: value for key, value in config.items(section)} return result"},{"question":"Objective: Implement a function that reads a custom binary data format from a file, modifies certain values, and writes back the updated binary data. Description: The binary file contains information in the following format: - A header with a magic number (4 bytes, unsigned integer), a version number (2 bytes, unsigned short), and a record count (2 bytes, unsigned short). - A sequence of records, where each record contains: - A record ID (4 bytes, unsigned integer) - A timestamp (8 bytes, unsigned long long) - A temperature reading (4 bytes, float) - A pressure reading (4 bytes, float) You are required to: 1. Read the binary file. 2. Increment all temperature readings by 5.0 degrees. 3. Write the updated binary data back to a new file. Function Signature: ```python def update_temperature(input_filename: str, output_filename: str): pass ``` Input: - `input_filename`: A string representing the path to the input binary file. - `output_filename`: A string representing the path to the output binary file. Constraints: - The file is in little-endian format. - You can assume the input file exists and is formatted correctly. Example: If the input file contains the following header and records: - Header: Magic number = 0x1A2B3C4D, Version = 1, Record count = 2. - Records: 1. Record ID = 1, Timestamp = 1627580702, Temperature = 20.0, Pressure = 1013.25 2. Record ID = 2, Timestamp = 1627580703, Temperature = 22.5, Pressure = 1012.75 After running `update_temperature(input_filename, output_filename)`, the output file should contain: - Header: Magic number = 0x1A2B3C4D, Version = 1, Record count = 2. - Records: 1. Record ID = 1, Timestamp = 1627580702, Temperature = 25.0, Pressure = 1013.25 2. Record ID = 2, Timestamp = 1627580703, Temperature = 27.5, Pressure = 1012.75 Instructions: - You must use the `struct` module to handle packing and unpacking of binary data. - Make sure to correctly handle byte order and data alignment as specified. - Provide comments and document your code for clarity. Good luck!","solution":"import struct def update_temperature(input_filename: str, output_filename: str): Reads a binary file, increments the temperature readings by 5.0 degrees, and writes the updated binary data to a new file. with open(input_filename, \'rb\') as infile: # Read and unpack the header header_format = \'<IHH\' header_size = struct.calcsize(header_format) header_data = infile.read(header_size) magic_number, version, record_count = struct.unpack(header_format, header_data) # Struct format for a single record record_format = \'<IQff\' record_size = struct.calcsize(record_format) # Read, modify, and store all records records = [] for _ in range(record_count): record_data = infile.read(record_size) record_id, timestamp, temperature, pressure = struct.unpack(record_format, record_data) # Increment the temperature by 5.0 degrees temperature += 5.0 # Pack the modified record back updated_record_data = struct.pack(record_format, record_id, timestamp, temperature, pressure) records.append(updated_record_data) with open(output_filename, \'wb\') as outfile: # Write the unchanged header back outfile.write(header_data) # Write the modified records for record in records: outfile.write(record)"},{"question":"Question: You are given a list of strings where each string contains an email address followed by some metadata in the format: ``` <email_address> <rest_of_string> ``` Your task is to: 1. Extract the email addresses using a regular expression. 2. Validate if the email addresses follow the standard email format using regular expressions. 3. Identify the domain part of each valid email address. Implement the function `extract_and_validate_emails(data: List[str]) -> Tuple[List[str], List[str]]` that performs the following: - Extracts all email addresses with a valid email format. - Identifies the domain part of each valid email address. **Function Signature:** ```python from typing import List, Tuple def extract_and_validate_emails(data: List[str]) -> Tuple[List[str], List[str]]: ``` - `data`: A list of strings where each string contains an email followed by some other metadata. - Returns: A tuple: - The first element is a list of valid email addresses. - The second element is a list of the corresponding domain parts of the valid email addresses. **Constraints:** - An email address follows the standard format: `local_part@domain_part` - The `local_part` may contain letters (`a-z`, `A-Z`), digits (`0-9`), underscores (`_`), periods (`.`) and hyphens (`-`). - The `domain_part` contains letters (`a-z`, `A-Z`), digits (`0-9`), and periods (`.`). - The `domain_part` must contain at least one period (`.`) separating the domain levels. **Example:** ```python data = [ \\"test.email+alex@leetcode.com SomeOtherData\\", \\"test.e.mail+bob.cathy@leetcode.com MoreData\\", \\"invalidemail@ MoreText\\", \\"testemail+david@lee.tcode MoreInfo\\" ] output = ( [ \\"test.email+alex@leetcode.com\\", \\"test.e.mail+bob.cathy@leetcode.com\\", \\"testemail+david@lee.tcode\\" ], [ \\"leetcode.com\\", \\"leetcode.com\\", \\"lee.tcode\\" ] ) ``` Use the `re` module\'s functionality to solve this problem effectively. **Notes:** - Return an empty list for both emails and domains if no valid email addresses are found. - Ensure to handle edge cases such as emails without domain parts, invalid characters, etc.","solution":"import re from typing import List, Tuple def extract_and_validate_emails(data: List[str]) -> Tuple[List[str], List[str]]: email_regex = re.compile( r\'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,})\' ) valid_emails = [] domains = [] for entry in data: match = email_regex.search(entry) if match: email = match.group(1) valid_emails.append(email) domain = email.split(\'@\')[1] domains.append(domain) return (valid_emails, domains)"},{"question":"Data Processing with Functional Programming **Objective:** Using Python\'s functional programming modules, demonstrate your understanding by implementing a function that processes a list of numerical data points, filters out unwanted values, maps a specific transformation to the remaining data, and aggregates the results. **Problem Statement:** You are given a list of numbers. Your task is to: 1. Filter out any negative numbers from the list. 2. For the remaining numbers, calculate their squares. 3. Compute the sum of these squared values. To achieve this, you must use: - `itertools` for efficient iteration, - `functools.partial` to create a customized function, - `operator` to perform operations in a clean and functional style. **Function Signature:** ```python def process_data(data: list) -> int: pass ``` **Input:** - `data` (list): A list of integers, both positive and negative (e.g., `[-2, -1, 0, 1, 2, 3, 4]`). **Output:** - An integer representing the sum of squared values of non-negative numbers. **Constraints:** - The list can contain up to 10,000 integers. - The integers can range from -1000 to 1000. **Performance Requirements:** - Your solution should efficiently handle the upper constraints using the appropriate functions from the mentioned modules. **Example:** ```python assert process_data([-2, -1, 0, 1, 2, 3, 4]) == 30 # Explanation: 0^2 + 1^2 + 2^2 + 3^2 + 4^2 = 0 + 1 + 4 + 9 + 16 = 30 assert process_data([-10, -5, 0, 5, 10]) == 125 # Explanation: 0^2 + 5^2 + 10^2 = 0 + 25 + 100 = 125 ``` **Implementation Hints:** - Use `itertools.filterfalse` or a similar method for filtering. - Use the `operator` module to map the squaring operation. - Calculate the sum in an efficient manner, possibly using `functools.reduce`. Good luck!","solution":"import itertools import functools import operator def process_data(data: list) -> int: # Step 1: Filter out negative numbers non_negative = filter(lambda x: x >= 0, data) # Step 2: Map remaining numbers to their squares squares = map(operator.pow, non_negative, itertools.repeat(2)) # Step 3: Sum the squared values result = functools.reduce(operator.add, squares, 0) return result"},{"question":"# PyTorch Sparse Tensor Operations **Objective**: Implement a function `sparse_tensor_operations` that performs the following tasks: 1. Converts a dense matrix to a sparse COO format. 2. Converts a dense matrix to a sparse CSR format. 3. Performs a sparse matrix multiplication (using CSR format) with another dense matrix. 4. Verifies if the product matrix is equivalent to a product obtained using dense matrix multiplication. **Function Signature**: ```python def sparse_tensor_operations(dense_matrix_1: torch.Tensor, dense_matrix_2: torch.Tensor) -> bool: pass ``` **Input**: - `dense_matrix_1`: A PyTorch dense 2D tensor with dimensions `(m, n)` containing float values. - `dense_matrix_2`: A PyTorch dense 2D tensor with dimensions `(n, p)` containing float values. **Output**: - Return a boolean value that is `True` if the matrix product obtained by multiplying the CSR sparse tensor (derived from `dense_matrix_1`) with `dense_matrix_2` is equivalent to the product of `dense_matrix_1` and `dense_matrix_2` using dense tensor multiplication. **Constraints/Limitation**: - Assume the input matrices dimensions are such that the multiplication operation is valid. - Use PyTorch\'s sparse tensor operations as described in the provided documentation. - Avoid using external libraries except PyTorch. **Performance Requirements**: - The operations should be completed efficiently, making use of the sparse tensor advantage where appropriate. **Example**: ```python import torch dense_matrix_1 = torch.tensor([[0, 0, 1], [1, 2, 0]], dtype=torch.float32) dense_matrix_2 = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32) result = sparse_tensor_operations(dense_matrix_1, dense_matrix_2) # Expected: True print(result) ``` **Implementation Details**: 1. Convert `dense_matrix_1` to a sparse COO tensor. 2. Convert `dense_matrix_1` to a sparse CSR tensor. 3. Use the sparse CSR tensor to perform matrix multiplication with `dense_matrix_2`. 4. Verify if the resulting product from the sparse multiplication matches the product obtained by directly multiplying the dense matrices. **Hint**: Use methods such as `to_sparse_coo` for converting to COO, `to_sparse_csr` for CSR, and `sparse.mm` for performing the sparse matrix multiplication.","solution":"import torch def sparse_tensor_operations(dense_matrix_1: torch.Tensor, dense_matrix_2: torch.Tensor) -> bool: Perform sparse tensor operations and verify the result matches dense multiplication result. Args: dense_matrix_1 (torch.Tensor): First dense matrix (m, n) dense_matrix_2 (torch.Tensor): Second dense matrix (n, p) Returns: bool: True if the sparse product equals the dense product, False otherwise # Convert dense_matrix_1 to sparse COO format sparse_coo = dense_matrix_1.to_sparse_coo() # Convert dense_matrix_1 to sparse CSR format sparse_csr = dense_matrix_1.to_sparse_csr() # Perform sparse matrix multiplication using CSR format and dense_matrix_2 sparse_result = torch.sparse.mm(sparse_csr, dense_matrix_2) # Perform dense matrix multiplication dense_result = torch.mm(dense_matrix_1, dense_matrix_2) # Verify if the result of sparse multiplication matches that of dense multiplication return torch.equal(sparse_result.to_dense(), dense_result)"},{"question":"**Objective:** Implement a function that generates a custom business period schedule by defining a custom business day and applying it over a given date range. **Problem Statement:** You are given a time series of dates. Your task is to implement a function that returns a list of dates that follow a custom business day rule within a given date range. You will use the `CustomBusinessDay` class from the pandas package. The custom business day should: - Only consider Monday to Thursday as business days (i.e., exclude Fridays, Saturdays, and Sundays). - Include specified holidays. - Adjust the dates such that each business day starts at 9:00 AM and ends at 5:00 PM. **Function Signature:** ```python def generate_custom_business_schedule(start_date: str, end_date: str, holidays: list) -> list: Generate a list of dates for custom business days within a given date range. Parameters: - start_date (str): The start date of the range in the format \'YYYY-MM-DD\'. - end_date (str): The end date of the range in the format \'YYYY-MM-DD\'. - holidays (list): A list of holiday dates in the format \'YYYY-MM-DD\'. Returns: - list: A list of strings representing the custom business days in the format \'YYYY-MM-DD\'. ``` **Constraints:** - The `start_date` and `end_date` will be valid dates in the format \'YYYY-MM-DD\'. - The `holidays` list will contain valid dates in the format \'YYYY-MM-DD\'. **Example:** ```python start_date = \'2023-01-01\' end_date = \'2023-01-10\' holidays = [\'2023-01-02\', \'2023-01-05\'] result = generate_custom_business_schedule(start_date, end_date, holidays) print(result) # Expected output: [\'2023-01-03\', \'2023-01-04\', \'2023-01-09\'] ``` **Notes:** - Do not consider any time zone differences. - Use the `CustomBusinessDay` class with the `weekmask` and `holidays` parameters to define the custom business period. - Use the `normalize` method if necessary when manipulating dates. Good luck!","solution":"import pandas as pd from pandas.tseries.offsets import CustomBusinessDay def generate_custom_business_schedule(start_date: str, end_date: str, holidays: list) -> list: Generate a list of dates for custom business days within a given date range. Parameters: - start_date (str): The start date of the range in the format \'YYYY-MM-DD\'. - end_date (str): The end date of the range in the format \'YYYY-MM-DD\'. - holidays (list): A list of holiday dates in the format \'YYYY-MM-DD\'. Returns: - list: A list of strings representing the custom business days in the format \'YYYY-MM-DD\'. # Create a CustomBusinessDay object custom_business_day = CustomBusinessDay(weekmask=\'Mon Tue Wed Thu\', holidays=holidays) # Generate the date range using the custom business day date_range = pd.date_range(start=start_date, end=end_date, freq=custom_business_day) # Convert the dates to strings in the desired format custom_business_days = [date.strftime(\'%Y-%m-%d\') for date in date_range] return custom_business_days"},{"question":"Objective Your task is to implement a function to generate and manipulate UUIDs using Python\'s `uuid` module. The function will perform the following operations: 1. Generate a version 1 UUID. 2. Generate a version 3 UUID using a given namespace and name. 3. Generate a version 4 UUID. 4. Generate a version 5 UUID using a given namespace and name. 5. Convert a given UUID string back to a UUID object and return its integer representation. Function Signature ```python def manipulate_uuids(): return { \'uuid1\': str, \'uuid3\': str, \'uuid4\': str, \'uuid5\': str, \'uuid_from_string_to_int\': int } ``` Input Format There are no inputs for this function as it will generate the UUIDs and return a dictionary of the required outputs. Output Format - The function should return a dictionary with the following keys and values: - `\'uuid1\'`: String representation of a version 1 UUID. - `\'uuid3\'`: String representation of a version 3 UUID generated using `uuid.NAMESPACE_DNS` and \'example.com\' as the name. - `\'uuid4\'`: String representation of a version 4 UUID. - `\'uuid5\'`: String representation of a version 5 UUID generated using `uuid.NAMESPACE_DNS` and \'example.com\' as the name. - `\'uuid_from_string_to_int\'`: Integer representation of the UUID obtained from the string \'12345678-1234-5678-1234-567812345678\'. Example ```python result = manipulate_uuids() expected_keys = [\'uuid1\', \'uuid3\', \'uuid4\', \'uuid5\', \'uuid_from_string_to_int\'] assert all(key in result for key in expected_keys), \\"Not all keys are in the result\\" print(result[\'uuid1\']) # Example output: \'a8098c1a-f86e-11da-bd1a-00112444be1e\' print(result[\'uuid3\']) # Example output: \'6fa459ea-ee8a-3ca4-894e-db77e160355e\' print(result[\'uuid4\']) # Example output: \'16fd2706-8baf-433b-82eb-8c7fada847da\' print(result[\'uuid5\']) # Example output: \'886313e1-3b8a-5372-9b90-0c9aee199e5d\' print(result[\'uuid_from_string_to_int\']) # Example output: 241978571610117151621718396369887781672 ``` Constraints - Use the `uuid` module to generate the required UUIDs. - Ensure the returned dictionary contains correctly formatted strings for the UUIDs and the correct integer representation for the given UUID string.","solution":"import uuid def manipulate_uuids(): Generates and manipulates various UUIDs, returning them in a dictionary. # Version 1 UUID uuid1 = str(uuid.uuid1()) # Version 3 UUID using namespace DNS and \'example.com\' uuid3 = str(uuid.uuid3(uuid.NAMESPACE_DNS, \'example.com\')) # Version 4 UUID uuid4 = str(uuid.uuid4()) # Version 5 UUID using namespace DNS and \'example.com\' uuid5 = str(uuid.uuid5(uuid.NAMESPACE_DNS, \'example.com\')) # Convert given UUID string back to UUID object and get integer representation uuid_from_string = uuid.UUID(\'12345678-1234-5678-1234-567812345678\') uuid_from_string_to_int = uuid_from_string.int # Return results in a dictionary return { \'uuid1\': uuid1, \'uuid3\': uuid3, \'uuid4\': uuid4, \'uuid5\': uuid5, \'uuid_from_string_to_int\': uuid_from_string_to_int }"},{"question":"# Covariance Estimation in Scikit-learn Objective: In this assessment, you are required to demonstrate your understanding of various covariance estimation techniques provided by the `sklearn.covariance` module. You will implement a solution to fit multiple covariance estimates on a dataset and analyze their performance in the presence of outliers. Problem Statement: You\'re provided with a dataset `data.npy` containing 2D data points. Your task is to compute the covariance matrix using the following methods and compare their performance: 1. Empirical Covariance 2. Ledoit-Wolf Shrinkage 3. Oracle Approximating Shrinkage (OAS) 4. Minimum Covariance Determinant (MCD) Additionally, include an analysis to show the effect of the presence of outliers on each technique. Implementation: 1. **Load Data**: - Load the dataset `data.npy`. Assume it\'s available in the current directory. 2. **Estimate Covariances**: - Compute the covariance matrices using the following methods: - Empirical Covariance - Ledoit-Wolf Shrinkage - Oracle Approximating Shrinkage (OAS) - Minimum Covariance Determinant (MCD) 3. **Add Outliers**: - Introduce outliers by adding 10 extreme data points to the dataset. 4. **Recompute Covariances**: - Recompute the covariance matrices with the outlier-included dataset. 5. **Analyze Results**: - Compare the covariance estimations visually (e.g., heatmaps) and numerically (e.g., Mahalanobis distances). Constraints: - Use the `sklearn.covariance` module and the methods specified. - Ensure reproducibility by setting random seed wherever applicable. Input: - A dataset file `data.npy` containing 2D data points. You can assume it is available in the working directory. Expected Output: - Covariance matrices for the specified methods (before and after adding outliers). - Visualizations comparing the covariance matrices. - Numerical analysis indicating the robustness of each method to outliers. Example: Here\'s a template to get you started: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import (EmpiricalCovariance, LedoitWolf, OAS, MinCovDet) # Load data data = np.load(\'data.npy\') # Function to compute and return all covariance matrices def compute_covariances(data): results = {} # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data).covariance_ results[\'Empirical\'] = emp_cov # Ledoit-Wolf lw_cov = LedoitWolf().fit(data).covariance_ results[\'LedoitWolf\'] = lw_cov # Oracle Approximating Shrinkage oas_cov = OAS().fit(data).covariance_ results[\'OAS\'] = oas_cov # Minimum Covariance Determinant mcd = MinCovDet().fit(data) mcd_cov = mcd.covariance_ results[\'MCD\'] = mcd_cov return results # Adding outliers def add_outliers(data, n_outliers=10, factor=10): outliers = data.mean(axis=0) + factor * data.std(axis=0) outlier_data = np.vstack([data, np.tile(outliers, (n_outliers, 1))]) return outlier_data # Plotting heatmaps of covariance matrices def plot_heatmaps(cov_matrices, title): fig, axes = plt.subplots(1, len(cov_matrices), figsize=(15, 5)) for ax, (label, cov) in zip(axes, cov_matrices.items()): im = ax.imshow(cov, interpolation=\'nearest\') ax.set_title(label) fig.colorbar(im, ax=ax) plt.suptitle(title) plt.show() # Compute initial covariances initial_covariances = compute_covariances(data) # Add outliers and recompute data_with_outliers = add_outliers(data) outlier_covariances = compute_covariances(data_with_outliers) # Plotting initial covariance matrices plot_heatmaps(initial_covariances, \'Covariance Matrices (Original Data)\') # Plotting covariance matrices with outliers plot_heatmaps(outlier_covariances, \'Covariance Matrices (With Outliers)\') # Example of numerical comparison, e.g., using Mahalanobis distance # Additional analysis steps to be provided by the student ``` Submission: Submit a single Python script (`covariance_estimation.py`) containing your implementation and analysis comments. Ensure your code is well-documented and organized.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS, MinCovDet # Function to compute and return all covariance matrices def compute_covariances(data): results = {} # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data).covariance_ results[\'Empirical\'] = emp_cov # Ledoit-Wolf lw_cov = LedoitWolf().fit(data).covariance_ results[\'LedoitWolf\'] = lw_cov # Oracle Approximating Shrinkage oas_cov = OAS().fit(data).covariance_ results[\'OAS\'] = oas_cov # Minimum Covariance Determinant mcd = MinCovDet().fit(data) mcd_cov = mcd.covariance_ results[\'MCD\'] = mcd_cov return results # Adding outliers def add_outliers(data, n_outliers=10, factor=10): rng = np.random.RandomState(42) outliers = rng.normal(loc=factor, scale=data.std(axis=0), size=(n_outliers, data.shape[1])) outlier_data = np.vstack([data, outliers]) return outlier_data # Plotting heatmaps of covariance matrices def plot_heatmaps(cov_matrices, title): fig, axes = plt.subplots(1, len(cov_matrices), figsize=(20, 5)) for ax, (label, cov) in zip(axes, cov_matrices.items()): im = ax.imshow(cov, interpolation=\'nearest\', cmap=plt.cm.viridis) ax.set_title(label) fig.colorbar(im, ax=ax) plt.suptitle(title) plt.show() # Example usage if __name__ == \\"__main__\\": data = np.load(\'data.npy\') # Initial covariance matrices initial_covariances = compute_covariances(data) # Add outliers and recompute data_with_outliers = add_outliers(data) outlier_covariances = compute_covariances(data_with_outliers) # Plotting initial covariance matrices plot_heatmaps(initial_covariances, \'Covariance Matrices (Original Data)\') # Plotting covariance matrices with outliers plot_heatmaps(outlier_covariances, \'Covariance Matrices (With Outliers)\')"},{"question":"Creating and Managing a Database with `dbm` You are tasked with creating a simple key-value store using the `dbm` module in Python. The operations you need to implement should demonstrate your understanding of opening databases, interacting with them like dictionaries, and using context management. Requirements: 1. **Function Name**: `manage_database` 2. **Function Signature**: `def manage_database(file_name: str)-> None:` 3. **Input**: - `file_name` (str): The name of the database file. 4. **Output**: None (The function will perform operations and print results as specified below). 5. **Operations**: 1. Open the database in \'c\' mode (create if necessary). 2. Insert the following data into the database: - Key: `\'name\'`, Value: `\'Alice\'` - Key: `\'occupation\'`, Value: `\'Engineer\'` - Key: `\'country\'`, Value: `\'Wonderland\'` 3. Retrieve and print each value using the respective key. 4. Verify and print the presence of the key `\'name\'`. 5. Update the value corresponding to the key `\'occupation\'` to `\'Senior Engineer\'`. 6. Delete the key `\'country\'` and ensure it no longer exists in the database. 7. Ensure the database is properly closed using a context manager. Constraints: - Assume the keys and values are all strings. - Use `utf-8` encoding for any necessary string-to-bytes conversions. Example: ```python def manage_database(file_name: str) -> None: # Pseudo-code for the expected behavior # Step 1: Open the database in \'c\' mode with dbm.open(file_name, \'c\') as db: # Step 2: Insert key-value pairs db[\'name\'] = \'Alice\' db[\'occupation\'] = \'Engineer\' db[\'country\'] = \'Wonderland\' # Step 3: Retrieve and print values print(db[\'name\']) # Output: \'Alice\' print(db[\'occupation\']) # Output: \'Engineer\' print(db[\'country\']) # Output: \'Wonderland\' # Step 4: Check for the presence of the key \'name\' print(\'name\' in db) # Output: True # Step 5: Update value for \'occupation\' db[\'occupation\'] = \'Senior Engineer\' print(db[\'occupation\']) # Output: \'Senior Engineer\' # Step 6: Delete the key \'country\' del db[\'country\'] print(\'country\' in db) # Output: False ``` Implement the `manage_database` function to perform the given operations. # Note: The solution should reflect good coding practices, including context management and appropriate handling of byte/string conversions.","solution":"import dbm def manage_database(file_name: str) -> None: Opens a dbm database and performs a series of operations. :param file_name: The name of the database file. with dbm.open(file_name, \'c\') as db: # Step 2: Insert key-value pairs db[\'name\'] = \'Alice\'.encode(\'utf-8\') db[\'occupation\'] = \'Engineer\'.encode(\'utf-8\') db[\'country\'] = \'Wonderland\'.encode(\'utf-8\') # Step 3: Retrieve and print values print(db[\'name\'].decode(\'utf-8\')) # Output: \'Alice\' print(db[\'occupation\'].decode(\'utf-8\')) # Output: \'Engineer\' print(db[\'country\'].decode(\'utf-8\')) # Output: \'Wonderland\' # Step 4: Check for the presence of the key \'name\' print(\'name\' in db) # Output: True # Step 5: Update value for \'occupation\' db[\'occupation\'] = \'Senior Engineer\'.encode(\'utf-8\') print(db[\'occupation\'].decode(\'utf-8\')) # Output: \'Senior Engineer\' # Step 6: Delete the key \'country\' del db[\'country\'] print(\'country\' in db) # Output: False"},{"question":"Problem Statement You are given a list of filenames and a list of patterns. Your task is to implement a function that groups the filenames based on the patterns they match. Each pattern should form a key in the result dictionary, and the value should be a list of filenames matching that pattern. Use the `fnmatch` module to perform the pattern matching operations. Function Signature ```python def group_files_by_pattern(filenames: list[str], patterns: list[str]) -> dict[str, list[str]]: pass ``` Input - `filenames`: A list of strings where each string is a filename. Example: `[\\"file1.txt\\", \\"data.csv\\", \\"script.py\\", \\"note.TXT\\"]`. - `patterns`: A list of strings where each string is a pattern. Example: `[\\"*.txt\\", \\"*.py\\", \\"*[0-9].*\\"]`. Output - A dictionary where each key is a pattern from the `patterns` list, and each value is a list of filenames from the `filenames` list that match that pattern. Example: `{\\"*.txt\\": [\\"file1.txt\\", \\"note.TXT\\"], \\"*.py\\": [\\"script.py\\"], \\"*[0-9].*\\": [\\"file1.txt\\"]}`. Constraints - All filenames and patterns are non-empty strings. - There can be overlapping matches; a single file may match multiple patterns and thus appear in multiple lists. - Patterns follow Unix shell-style wildcards (`*`, `?`, `[seq]`, `[!seq]`). Example ```python filenames = [\\"file1.txt\\", \\"data.csv\\", \\"script.py\\", \\"note.TXT\\"] patterns = [\\"*.txt\\", \\"*.py\\", \\"*[0-9].*\\"] result = group_files_by_pattern(filenames, patterns) # Expected output: # { # \\"*.txt\\": [\\"file1.txt\\", \\"note.TXT\\"], # \\"*.py\\": [\\"script.py\\"], # \\"*[0-9].*\\": [\\"file1.txt\\"] # } ``` Requirements - Use `fnmatch.fnmatch` or `fnmatch.fnmatchcase` in your implementation. - Ensure your implementation handles case normalization correctly. Notes - Be sure to test the patterns and filenames under different cases (uppercase, lowercase) to make sure your function works for both case-sensitive and case-insensitive scenarios.","solution":"import fnmatch def group_files_by_pattern(filenames: list[str], patterns: list[str]) -> dict[str, list[str]]: result = {} for pattern in patterns: matched_files = [filename for filename in filenames if fnmatch.fnmatch(filename, pattern)] result[pattern] = matched_files return result"},{"question":"# Multiclass and Multioutput Classification Challenge Objective To assess your understanding of multiclass and multioutput classification using scikit-learn\'s meta-estimators. Problem Statement You are given a dataset where each sample can belong to one or more classes simultaneously. Your task is to implement and compare three different strategies for handling multiclass and multioutput classification on the given dataset: 1. One-vs-Rest (OvR) using `OneVsRestClassifier` 2. One-vs-One (OvO) using `OneVsOneClassifier` 3. Error-Correcting Output Codes (ECOC) using `OutputCodeClassifier` Dataset You will work with the Iris dataset from scikit-learn, which contains three classes of flowers: Setosa, Versicolor, and Virginica. The dataset has 150 samples, each with 4 features (sepal length, sepal width, petal length, petal width). Each sample belongs to one of the three classes. Task 1. Load the Iris dataset. 2. Split the dataset into a training set (80%) and a test set (20%). 3. Implement the three strategies for multiclass classification using linear SVM (`LinearSVC`) as the base estimator. 4. Fit each classifier to the training data and predict the labels for the test data. 5. Evaluate the performance of each classifier using accuracy and F1-score metrics. 6. Summarize the results and compare the performance of the three strategies. Requirements - Use the `OneVsRestClassifier` for the One-vs-Rest strategy. - Use the `OneVsOneClassifier` for the One-vs-One strategy. - Use the `OutputCodeClassifier` for the ECOC strategy, with a code size of 1.5 times the number of classes. - Use the `LinearSVC` with a random state of 0 as the base estimator for all three strategies. Input and Output Formats - Input: None (the function should internally load and split the dataset) - Output: A dictionary with keys `OvR`, `OvO`, and `ECOC`, each containing a nested dictionary with accuracy and F1-score. Constraints - You must use scikit-learn\'s built-in functions for loading the dataset and splitting it. - You must use scikit-learn\'s provided classifiers (OneVsRestClassifier, OneVsOneClassifier, and OutputCodeClassifier) and metrics (accuracy_score, f1_score). - The implementation should be efficient and easy to read. Function Signature ```python def compare_multiclass_strategies(): pass ``` Example Output ```python { \'OvR\': {\'accuracy\': 0.96, \'f1_score\': 0.96}, \'OvO\': {\'accuracy\': 0.97, \'f1_score\': 0.97}, \'ECOC\': {\'accuracy\': 0.95, \'f1_score\': 0.95} } ``` Additional Notes - You may refer to the examples provided in the documentation to understand how to use the classifiers. - Ensure to handle any potential issues such as class imbalance or overfitting while building your models.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.svm import LinearSVC from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier from sklearn.metrics import accuracy_score, f1_score def compare_multiclass_strategies(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training set (80%) and test set (20%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y) # Define the base estimator base_estimator = LinearSVC(random_state=0) # One-vs-Rest (OvR) strategy ovr_classifier = OneVsRestClassifier(base_estimator) ovr_classifier.fit(X_train, y_train) ovr_predictions = ovr_classifier.predict(X_test) ovr_accuracy = accuracy_score(y_test, ovr_predictions) ovr_f1 = f1_score(y_test, ovr_predictions, average=\'weighted\') # One-vs-One (OvO) strategy ovo_classifier = OneVsOneClassifier(base_estimator) ovo_classifier.fit(X_train, y_train) ovo_predictions = ovo_classifier.predict(X_test) ovo_accuracy = accuracy_score(y_test, ovo_predictions) ovo_f1 = f1_score(y_test, ovo_predictions, average=\'weighted\') # Error-Correcting Output Codes (ECOC) strategy with code size of 1.5 times the number of classes ecoc_classifier = OutputCodeClassifier(base_estimator, code_size=1.5, random_state=0) ecoc_classifier.fit(X_train, y_train) ecoc_predictions = ecoc_classifier.predict(X_test) ecoc_accuracy = accuracy_score(y_test, ecoc_predictions) ecoc_f1 = f1_score(y_test, ecoc_predictions, average=\'weighted\') # Return the results as a dictionary results = { \'OvR\': {\'accuracy\': ovr_accuracy, \'f1_score\': ovr_f1}, \'OvO\': {\'accuracy\': ovo_accuracy, \'f1_score\': ovo_f1}, \'ECOC\': {\'accuracy\': ecoc_accuracy, \'f1_score\': ecoc_f1} } return results"},{"question":"**Objective:** To assess the student\'s ability to use the `cProfile` and `pstats` modules to profile a Python function, save the profiling results to a file, and manipulate and interpret these results. **Question:** You are given a complex Python function `complex_function` which performs several nested operations and computations. Your task is to profile this function using the `cProfile` module, save the profiling results to a file, and then use the `pstats` module to analyze and format these results. # Step-by-step tasks: 1. **Function to Profile:** ```python import time def complex_function(n): result = 0 for i in range(1, n+1): result += sum(j*j for j in range(i)) time.sleep(0.001) # Simulates a time-consuming operation return result ``` 2. **Profile the Function:** - Use the `cProfile` module to profile the execution of `complex_function` for `n = 100`. - Save the profiling results to a file named `profile_results.prof`. 3. **Analyze the Profiling Results:** - Use the `pstats` module to read the `profile_results.prof` file. - Sort the profiling results based on the cumulative time spent in functions. - Print the top ten functions based on cumulative time. - Additionally, print statistics for any function containing `sum` in its name. # What you need to do: - Implement a Python script that performs the above steps. - Provide the implementation of the script and any relevant output from the script (e.g., printed profiling statistics). # Constraints: - The profiling data must be sorted and displayed correctly. - Handle any potential errors while profiling the function or reading the profiling file. - You may assume the `complex_function` is provided and should not be modified. # Example Output: ``` 3005 function calls in 1.201 seconds Ordered by: cumulative time List reduced from 7 to 5 due to restriction <10> ncalls tottime percall cumtime percall filename:lineno(function) 1 0.002 0.002 1.201 1.201 <ipython-input-1-xxxxxxxx>:3(complex_function) 1 0.900 0.900 1.199 1.199 <ipython-input-1-xxxxxxxx>:5(<genexpr>) 100/1 0.300 0.003 1.197 1.197 <ipython-input-1-xxxxxxxx>:6(<listcomp>) 2002 0.000 0.000 0.000 0.000 <ipython-input-1-xxxxxxxx>:9(<lambda>) ... Sum-related functions: ncalls tottime percall cumtime percall filename:lineno(function) 100/1 0.300 0.003 1.197 1.197 <ipython-input-1-xxxxxxxx>:6(<listcomp>) ``` Provide your implementation and the profiling results as the solution.","solution":"import cProfile import pstats # Provided complex function def complex_function(n): import time result = 0 for i in range(1, n + 1): result += sum(j * j for j in range(i)) time.sleep(0.001) # Simulates a time-consuming operation return result def profile_function(func, *args, **kwargs): # Profile the function and save the output to a file profiler = cProfile.Profile() profiler.enable() func(*args, **kwargs) profiler.disable() profiler.dump_stats(\'profile_results.prof\') def analyze_profile_results(filename): # Read and analyze the profile results stats = pstats.Stats(filename) stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(10) stats.print_stats(\'sum\') # Profile `complex_function` with n = 100 profile_function(complex_function, 100) # Analyze the profiling results analyze_profile_results(\'profile_results.prof\')"},{"question":"**Question: Custom Color Palette Visualization in Seaborn** # Objective: You are required to demonstrate your comprehension of Seaborn\'s `husl_palette` function and its application in data visualization. # Instructions: 1. **Data Preparation**: - Create a sample dataset containing three categories, each with 100 data points normally distributed around different means (use `numpy` to generate the data). - Assign these data points to a single DataFrame with appropriate column names (`Category` and `Value`). 2. **Custom Color Palette**: - Using Seaborn\'s `husl_palette`, create a custom color palette with the following specifications: - 3 colors - Lightness of 0.6 - Saturation of 0.8 - Starting hue of 0.25 3. **Visualization**: - Create a Seaborn `boxplot` using the generated DataFrame, utilizing the custom `husl_palette`. - Ensure each category is colored distinctly using the custom palette. 4. **Generate Continuous Colormap Visualization**: - Create a continuous colormap using `husl_palette` with `as_cmap=True`. - Generate a density plot (using Seaborn\'s `kdeplot`) for the entire dataset (ignoring categories) and apply the created colormap. # Input Format: - The question does not take any input from the user. # Output Format: - A boxplot should be displayed with categories colored using the custom `husl_palette`. - A density plot should be displayed with the continuous colormap applied. # Example: - Example Dataframe structure: ``` | Category | Value | |----------|---------| | A | 1.50234 | | A | 1.69235 | | B | 2.38492 | | C | 3.92834 | ... ``` - Example boxplot: A plot with three boxes colored using the custom palette colors. - Example density plot: A continuous density plot colored using the continuous colormap. Note: Ensure to import necessary libraries such as Seaborn and Matplotlib. # Constraints: - Use a fixed random seed for data generation to ensure reproducibility. # Performance Requirements: - The complete code should generate the specified plots without any performance issues on standard datasets.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_custom_palette_boxplot_and_density(): # Data Preparation np.random.seed(42) data_A = np.random.normal(loc=1, scale=1, size=100) data_B = np.random.normal(loc=3, scale=1, size=100) data_C = np.random.normal(loc=5, scale=1, size=100) df = pd.DataFrame({ \'Category\': [\'A\'] * 100 + [\'B\'] * 100 + [\'C\'] * 100, \'Value\': np.concatenate([data_A, data_B, data_C]) }) # Custom Color Palette custom_palette = sns.husl_palette(3, l=0.6, s=0.8, h=0.25) # Visualization - Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(x=\'Category\', y=\'Value\', data=df, palette=custom_palette) plt.title(\'Boxplot with Custom HUSL Palette\') plt.show() # Continuous Colormap Visualization cmap = sns.husl_palette(256, l=0.6, s=0.8, h=0.25, as_cmap=True) plt.figure(figsize=(10, 6)) sns.kdeplot(df[\'Value\'], cmap=cmap, fill=True) plt.title(\'Density Plot with Continuous HUSL Colormap\') plt.show()"},{"question":"**Objective**: Demonstrate understanding of the `chunk` module and the manipulation of IFF chunked files in Python. Problem Description You are provided with an audio file in an AIFF format, which follows the IFF chunked data standard as described. Using the `chunk` module, your task is to read and process the file, extracting the following information: 1. The IDs of all chunks present in the file. 2. The size of each chunk. 3. The first 10 bytes of data from each chunk (if available). Implementation Details Implement a function `parse_aiff_file(file_path: str) -> List[Tuple[str, int, bytes]]` that takes the path to an AIFF file and returns a list of tuples. Each tuple should contain: - The chunk ID as a string. - The chunk size as an integer. - The first 10 bytes of data as a bytes object (or fewer if the chunk is smaller). # Function Signature ```python from typing import List, Tuple def parse_aiff_file(file_path: str) -> List[Tuple[str, int, bytes]]: pass ``` Constraints - You should assume that the file is correctly formatted. - The function should ensure that the file is not modified, and no additional libraries beyond the standard library should be used. - You can assume the file size will not exceed 100MB. - Exception handling should be implemented to manage potential errors such as file not found or empty files. Example Assuming a valid AIFF file `example.aiff`, the function call: ```python result = parse_aiff_file(\'example.aiff\') ``` might return (the exact values depend on the contents of the AIFF file): ```python [ (\'FORM\', 1000, b\'x00x01x02x03x04x05x06x07x08x09\'), (\'COMM\', 18, b\'x11x12x13x14\'), (\'SSND\', 2000, b\'x21x22x23x24x25x26x27x28x29x2a\'), ... ] ``` Notes - You may find the `chunk.Chunk` class useful for extracting the required information. - Ensure the function cleans up and closes files appropriately, even in the case of errors. - Include documentation and inline comments to explain the logic and flow of your implementation.","solution":"from typing import List, Tuple import chunk def parse_aiff_file(file_path: str) -> List[Tuple[str, int, bytes]]: Parses an AIFF file and extracts information about its chunks. Args: - file_path: The path to the AIFF file. Returns: - A list of tuples containing chunk ID, size, and first 10 bytes of data. result = [] try: with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f, bigendian=True, align=True) chunk_id = ch.getname().decode(\'ascii\') chunk_size = ch.getsize() # Read at most the first 10 bytes of the chunk\'s data first_10_bytes = ch.read(min(10, chunk_size)) result.append((chunk_id, chunk_size, first_10_bytes)) ch.skip() except EOFError: break except FileNotFoundError: print(f\\"File {file_path} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") return result"},{"question":"# Sorting Custom Objects in Python You are given a list of `Employee` objects. Each `Employee` has the following attributes: - `name`: a string representing the employee\'s name. - `department`: a string representing the department the employee works in. - `age`: an integer representing the employee\'s age. - `salary`: a float representing the employee\'s monthly salary. Your task is to implement a function `sort_employees` that takes a list of `Employee` objects and a list of sorting specifications, and returns a new list of `Employee` objects sorted according to these specifications. # Function Signature ```python def sort_employees(employees: List[Employee], specs: List[Tuple[str, bool]]) -> List[Employee]: ``` # Parameters - `employees`: A list of `Employee` objects. - `specs`: A list of tuples where each tuple contains a field to sort by (either `\'name\'`, `\'department\'`, `\'age\'`, or `\'salary\'`) and a boolean value indicating whether to sort in descending order (True for descending, False for ascending). # Output - A list of sorted `Employee` objects. The sorting should respect the order of specifications. If two employees have the same value for the first specification key, the second specification key should be used to break the tie, and so on. # Example Usage ```python class Employee: def __init__(self, name, department, age, salary): self.name = name self.department = department self.age = age self.salary = salary def __repr__(self): return f\\"Employee({self.name}, {self.department}, {self.age}, {self.salary})\\" employees = [ Employee(\'John\', \'HR\', 28, 5000.0), Employee(\'Jane\', \'Finance\', 35, 7000.0), Employee(\'Dave\', \'IT\', 24, 4500.0), Employee(\'Emma\', \'Finance\', 35, 5000.0), ] specs = [(\'department\', False), (\'age\', True), (\'salary\', True)] sorted_employees = sort_employees(employees, specs) print(sorted_employees) ``` # Constraints - All field values will be valid and non-null. - The list of employees can be very large, so the sorting algorithm should be efficient. - Handle edge cases where the list of employees is empty or where specifications may contain fields with tied values. # Notes - Use the `operator.attrgetter` and the Timsort algorithm for efficient multi-level sorting. - Ensure that the sorting is stable, so original orders are preserved when specifications tie.","solution":"from typing import List, Tuple from operator import attrgetter class Employee: def __init__(self, name, department, age, salary): self.name = name self.department = department self.age = age self.salary = salary def __repr__(self): return f\\"Employee({self.name}, {self.department}, {self.age}, {self.salary})\\" def sort_employees(employees: List[Employee], specs: List[Tuple[str, bool]]) -> List[Employee]: if not employees or not specs: return employees for spec in reversed(specs): key, reverse = spec employees.sort(key=attrgetter(key), reverse=reverse) return employees"},{"question":"# Objective: To evaluate the understanding of creating, extending, and testing custom operators in PyTorch using the `torch.library` module. # Problem Statement: You are required to implement a custom operator that performs a squared sum of an input tensor. The operator should be created, extended, and tested for correctness using the provided `torch.library` APIs. # Tasks: 1. **Creating a Custom Operator:** - Implement a custom operator `squared_sum` that takes a 1-D tensor as input and returns the sum of squares of its elements. 2. **Registering the Kernel:** - Register the kernel implementation for the `squared_sum` operator. 3. **Testing the Custom Operator:** - Write test cases to verify the correctness of the `squared_sum` operator using `torch.library.opcheck` and `torch.autograd.gradcheck`. # Input: 1. A 1-D tensor `input_tensor` of arbitrary length containing floating-point numbers. # Output: 1. A single floating-point number which is the sum of the squares of the elements of `input_tensor`. # Constraints: - The implementation should handle any valid input tensor. - The operator should support autograd for backpropagation. # Example: ```python import torch # Implementing the squared_sum operator logic def squared_sum_impl(input_tensor): return torch.sum(input_tensor ** 2) # Registering the operator my_lib = torch.library.Library(\\"my_ops\\") my_lib.impl(\\"squared_sum::forward\\", squared_sum_impl) # Testing the custom operator input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) result = torch.ops.my_ops.squared_sum(input_tensor) print(result) # Expected output: 14.0 # Performing gradcheck for autograd torch.autograd.gradcheck(torch.ops.my_ops.squared_sum, (input_tensor,)) ``` # Instructions: 1. Define the implementation of the `squared_sum` function. 2. Register the `squared_sum` operator using `torch.library.Library`. 3. Write test cases to validate the correct behavior of the custom operator using `opcheck` and `gradcheck`. Provide your solution in a Python script or Jupyter notebook format. Ensure the solution is well-documented and includes necessary import statements.","solution":"import torch from torch.autograd import Function # Step 1: Creating a Custom Operator class SquaredSum(Function): @staticmethod def forward(ctx, input_tensor): ctx.save_for_backward(input_tensor) return torch.sum(input_tensor ** 2) @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors grad_input = grad_output * 2 * input_tensor return grad_input # Step 2: Registering the Kernel # Register the custom operator implementation my_lib = torch.library.Library(\\"my_ops\\", \\"DEF\\") my_lib.define(\\"squared_sum(Tensor input) -> (Tensor)\\") my_lib.impl(\\"squared_sum\\", SquaredSum.apply) # Defining a simpler API for the custom operator def squared_sum(input_tensor): return torch.ops.my_ops.squared_sum(input_tensor)"},{"question":"Objective: To assess students\' understanding of Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) from the scikit-learn library. Task: 1. Implement both LDA and QDA classifiers using the `sklearn` library. 2. Perform classification on the Iris dataset. 3. Perform dimensionality reduction using LDA on the Iris dataset. Instructions: 1. **Load the Iris dataset** from `sklearn.datasets`. 2. **Implement and train** both LDA and QDA classifiers on the dataset. 3. **Evaluate** the trained models using accuracy score. 4. **Perform and visualize** dimensionality reduction using LDA on the Iris dataset, reducing it to 2 dimensions. Expected Input and Output: - **Input:** - None (datasets to be imported within the code) - **Output:** - Print accuracy scores for both LDA and QDA classifiers on the Iris dataset. - Plot a 2D scatter plot showing the reduced dimensions of the Iris dataset using LDA. Code Template: ```python from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Step 1: Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Implement and train LDA classifier lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) # Evaluate the LDA model y_pred_lda = lda.predict(X_test) lda_accuracy = accuracy_score(y_test, y_pred_lda) print(f\'LDA Accuracy: {lda_accuracy}\') # Step 4: Implement and train QDA classifier qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) # Evaluate the QDA model y_pred_qda = qda.predict(X_test) qda_accuracy = accuracy_score(y_test, y_pred_qda) print(f\'QDA Accuracy: {qda_accuracy}\') # Step 5: Perform dimensionality reduction using LDA lda_reduction = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda_reduction.fit(X, y).transform(X) # Step 6: Visualize the reduced dimensions colors = [\'red\', \'green\', \'blue\'] target_names = iris.target_names plt.figure() for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of IRIS dataset\') plt.show() ``` Constraints: - Ensure the dataset is split into training and testing sets with a 70-30 ratio. - Use `random_state=42` for reproducibility in train-test splitting. - Use the `accuracy_score` function from `sklearn.metrics` to evaluate the models.","solution":"from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Step 1: Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Implement and train LDA classifier lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) # Evaluate the LDA model y_pred_lda = lda.predict(X_test) lda_accuracy = accuracy_score(y_test, y_pred_lda) print(f\'LDA Accuracy: {lda_accuracy}\') # Step 4: Implement and train QDA classifier qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) # Evaluate the QDA model y_pred_qda = qda.predict(X_test) qda_accuracy = accuracy_score(y_test, y_pred_qda) print(f\'QDA Accuracy: {qda_accuracy}\') # Step 5: Perform dimensionality reduction using LDA lda_reduction = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda_reduction.fit(X, y).transform(X) # Step 6: Visualize the reduced dimensions colors = [\'red\', \'green\', \'blue\'] target_names = iris.target_names plt.figure() for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of IRIS dataset\') plt.show()"},{"question":"# Coding Challenge: Command-Line Configuration Tool using `optparse` Objective You are tasked with designing a command-line configuration tool using the deprecated `optparse` module. This tool will allow users to configure settings for a hypothetical application. Requirements 1. **Basic Options**: - `-o`, `--output`: - Specifies the output directory. - Must accept a string value representing the directory path. - `-v`, `--verbose`: - A flag that, when set, enables verbose mode for detailed logging. - `-q`, `--quiet`: - A flag that, when set, disables all logging. 2. **Grouped Options**: - Group together options for debug settings: - `-d`, `--debug`: - Enables debug mode. - `--log-level`: - Specifies the log level (choices: `info`, `warning`, `error`). 3. **Custom Callback**: - Implement a custom callback for the `--notify` option. - `--notify`: - Accepts a list of email addresses separated by commas. - Parses the email addresses and stores them as a list. 4. **Help Message**: - Ensure a comprehensive help message is generated, displaying all options and their functionalities. - Include a custom epilog message: \\"This tool helps in setting application configurations easily.\\" 5. **Error Handling**: - Ensure that invalid command-line options, missing required arguments, and other errors are handled gracefully with appropriate error messages. Input Your input will not involve actual command-line execution. Instead, create a function called `parse_config()` that accepts `arg_list` as its parameter. This list will simulate the arguments passed from the command line. Example Usage Here is an example of how you might test your implementation: ```python args = [\\"-o\\", \\"/data/output\\", \\"-v\\", \\"--notify=test@example.com,admin@example.com\\", \\"--log-level=info\\"] options, args = parse_config(args) print(options.output) # Output: /data/output print(options.verbose) # Output: True print(options.notify) # Output: [\'test@example.com\', \'admin@example.com\'] print(options.log_level) # Output: info ``` Function Signature ```python def parse_config(arg_list: list) -> tuple: # Your implementation here pass ``` Constraints - Do not use the `argparse` module for this task. - Handle all essential functionalities described, including error management and extended option handling. - Use appropriate practices to ensure backward compatibility with legacy code usage. Notes Although `optparse` is deprecated, understanding its workings is essential for maintaining legacy codebases. This challenge assesses your comprehension of `optparse` module functions and your ability to leverage them appropriately.","solution":"import optparse def notify_callback(option, opt, value, parser): setattr(parser.values, option.dest, value.split(\',\')) def parse_config(arg_list): usage = \\"Usage: %prog [options]\\" parser = optparse.OptionParser(usage=usage, epilog=\\"This tool helps in setting application configurations easily.\\") # Basic Options parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output\\", type=\\"string\\", help=\\"Specifies the output directory.\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"Enables verbose mode.\\") parser.add_option(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", dest=\\"quiet\\", default=False, help=\\"Disables all logging.\\") # Grouped Options for debug settings parser.add_option(\\"-d\\", \\"--debug\\", action=\\"store_true\\", dest=\\"debug\\", default=False, help=\\"Enables debug mode.\\") parser.add_option(\\"--log-level\\", dest=\\"log_level\\", type=\\"choice\\", choices=[\\"info\\", \\"warning\\", \\"error\\"], help=\\"Specifies the log level.\\") # Custom Callback parser.add_option(\\"--notify\\", type=\\"string\\", action=\\"callback\\", callback=notify_callback, dest=\\"notify\\", help=\\"Comma separated list of email addresses for notifications.\\") (options, args) = parser.parse_args(arg_list) return options, args"},{"question":"You are given a PyTorch model and your task is to export its computation graph into an Export IR, manipulate it, and then perform certain operations using PyTorch\'s `torch.export` module. Follow the instructions below: 1. Define a simple PyTorch neural network class `SimpleNet` with a few layers. 2. Export the model\'s computation graph using `torch.export.export`. 3. Add a new operation (`torch.add`) to the exported graph. 4. Insert a custom `get_attr` node to read a specific submodule from the graph. 5. Implement a function to traverse and print the modified graph in a readable format. # Requirements: - Your solution should correctly export the model\'s computation graph. - You should be able to add operations and node manipulations to the exported graph. - You must correctly implement the traversal and printing function. - Each step should include necessary validations to ensure the graph\'s integrity. # Function Signatures: ```python import torch from torch import nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() # Define layers here def forward(self, x): # Define forward pass here def export_model(model, example_input): Export the model to an Export IR. Arguments: model : nn.Module : PyTorch model to export. example_input : torch.Tensor : Example input tensor for tracing the graph. Returns: torch.export.ExportedProgram : Exported program containing the graph. pass def add_operation_to_graph(exported_program, op, args): Add a new operation to the exported graph. Arguments: exported_program : torch.export.ExportedProgram : The exported program. op : Callable : The operation to add (e.g., torch.add). args : Tuple : Arguments to pass to the operation. Returns: torch.fx.Graph : Modified graph with the new operation added. pass def insert_get_attr_node(exported_program, target): Insert a new get_attr node to read a submodule from the graph. Arguments: exported_program : torch.export.ExportedProgram : The exported program. target : str : Target submodule to read. Returns: torch.fx.Graph : Modified graph with the new get_attr node. pass def print_graph(graph): Traverse and print the graph in a readable format. Arguments: graph : torch.fx.Graph : The graph to traverse and print. pass # Example Usage if __name__ == \\"__main__\\": model = SimpleNet() example_input = torch.randn(1, 3, 224, 224) exported_program = export_model(model, example_input) modified_graph_with_op = add_operation_to_graph(exported_program, torch.add, (exported_program.graph.nodes[1], torch.tensor(1))) modified_graph_with_get_attr = insert_get_attr_node(exported_program, \\"submodule_target\\") print_graph(modified_graph_with_get_attr) ``` # Constraints: - Assume that the model and operations are simple and meant for demonstration purposes. - Ensure the graph remains valid after all manipulations. - The graph traversal should work for general graphs and handle nodes in an ordered manner.","solution":"import torch from torch import nn from torch.fx import symbolic_trace class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.relu = nn.ReLU() self.fc = nn.Linear(16 * 224 * 224, 10) def forward(self, x): x = self.conv1(x) x = self.relu(x) x = x.view(x.size(0), -1) # Flatten the tensor x = self.fc(x) return x def export_model(model, example_input): Export the model to an Export IR. Arguments: model : nn.Module : PyTorch model to export. example_input : torch.Tensor : Example input tensor for tracing the graph. Returns: torch.fx.Graph : Exported graph. traced = symbolic_trace(model) return traced.graph def add_operation_to_graph(exported_graph, op, args): Add a new operation to the exported graph. Arguments: exported_graph : torch.fx.Graph : The exported graph. op : Callable : The operation to add (e.g., torch.add). args : Tuple : Arguments to pass to the operation. Returns: torch.fx.Graph : Modified graph with the new operation added. with exported_graph.inserting_after(list(exported_graph.nodes)[-1]): new_node = exported_graph.call_function(op, args) return exported_graph def insert_get_attr_node(exported_graph, target): Insert a new get_attr node to read a submodule from the graph. Arguments: exported_graph : torch.fx.Graph : The exported graph. target : str : Target submodule to read. Returns: torch.fx.Graph : Modified graph with the new get_attr node. with exported_graph.inserting_after(list(exported_graph.nodes)[0]): new_node = exported_graph.get_attr(target) return exported_graph def print_graph(graph): Traverse and print the graph in a readable format. Arguments: graph : torch.fx.Graph : The graph to traverse and print. for node in graph.nodes: print(f\\"Node: {node.name}, Op: {node.op}, Target: {node.target}, Args: {node.args}, Kwargs: {node.kwargs}\\") # Example Usage if __name__ == \\"__main__\\": model = SimpleNet() example_input = torch.randn(1, 3, 224, 224) exported_graph = export_model(model, example_input) modified_graph_with_op = add_operation_to_graph(exported_graph, torch.add, (list(exported_graph.nodes)[1], torch.tensor(1))) modified_graph_with_get_attr = insert_get_attr_node(modified_graph_with_op, \\"conv1\\") print_graph(modified_graph_with_get_attr)"},{"question":"# Email Management with the \\"email\\" Package In this task, you will demonstrate your understanding of the \\"email\\" package by writing a Python function that parses, modifies, and generates an email message. You will perform the following operations: 1. **Parse a raw email string into an EmailMessage object.** 2. **Modify the email by adding a new MIME part.** 3. **Generate the email back into a raw email string.** # Requirements 1. **Function Signature:** ```python def modify_email(raw_email: str, mime_content: str, content_type: str) -> str: ``` 2. **Parameters:** - `raw_email` (str): A string representing the raw email content. - `mime_content` (str): The content you want to add as a new MIME part. - `content_type` (str): The MIME type of the content (e.g., \\"text/plain\\", \\"application/pdf\\"). 3. **Output:** - Return the modified raw email string. 4. **Instructions:** - Use the `email` package to parse the `raw_email`. - Add a new MIME part with the specified `mime_content` and `content_type` to the email. - Ensure the modified email remains valid according to RFC standards. # Example ```python raw_email = From: user@example.com To: recipient@example.com Subject: Test Email This is the body of the email. mime_content = \\"This is a new MIME part.\\" content_type = \\"text/plain\\" modified_email = modify_email(raw_email, mime_content, content_type) print(modified_email) ``` Output should be a string similar to: ``` From: user@example.com To: recipient@example.com Subject: Test Email MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============123456789=\\" --===============123456789= Content-Type: text/plain; charset=\\"utf-8\\" This is the body of the email. --===============123456789= Content-Type: text/plain This is a new MIME part. --===============123456789=-- ``` # Constraints - You may assume the email does not contain any existing MIME parts. - The new MIME part should be added as an additional part to the body of the email. - Use the modern `EmailMessage` API provided by the `email` package.","solution":"import email from email import policy from email.parser import BytesParser from email.message import EmailMessage def modify_email(raw_email: str, mime_content: str, content_type: str) -> str: Parses a raw email string, adds a new MIME part, and returns the modified email as a string. Parameters: raw_email (str): A string representing the raw email content. mime_content (str): The content you want to add as a new MIME part. content_type (str): The MIME type of the content (e.g., \\"text/plain\\", \\"application/pdf\\"). Returns: str: The modified raw email as a string. # Parse raw email into an EmailMessage object msg = BytesParser(policy=policy.default).parsebytes(raw_email.encode()) # Create a new EmailMessage for the MIME part we want to add new_part = EmailMessage() new_part.set_content(mime_content, subtype=content_type.split(\'/\')[1], charset=\'utf-8\') # If the original email is not a multipart, convert it to multipart/mixed if msg.get_content_type() == \'text/plain\': body = msg.get_payload() new_msg = EmailMessage() new_msg[\'From\'] = msg[\'From\'] new_msg[\'To\'] = msg[\'To\'] new_msg[\'Subject\'] = msg[\'Subject\'] new_msg.set_content(body) new_msg.make_mixed() # Convert to multipart/mixed msg = new_msg # Attach the new MIME part msg.attach(new_part) # Return the modified email as a string return msg.as_string()"},{"question":"**Objective:** Demonstrate understanding of the `mailbox` Python module by implementing a custom class that can interact with multiple mailbox formats and perform specific operations. **Scenario:** You are tasked with creating a utility to organize emails from a mixed-format mailbox. The utility should: 1. Identify and categorize emails based on certain keywords in the subject header. 2. Move categorized emails into corresponding mailbox folders. 3. Ensure the process is safe for concurrent modifications. **Task:** Implement a class `EmailOrganizer` that includes the following methods: 1. `__init__(self, main_mailbox_path: str, categorized_mailboxes: dict)`: Initializes the instance with the main mailbox path and a dictionary mapping category names to their mailbox instances. Example of `categorized_mailboxes`: ```python { \\"python-list\\": mailbox.Maildir(\'~/Maildir/python-list\'), \\"python-dev\\": mailbox.mbox(\'~/mbox/python-dev\'), \\"python-bugs\\": mailbox.MH(\'~/Mail/python-bugs\') } ``` 2. `categorize_emails(self, keyword_mapping: dict)`: For each email in the main mailbox: - If the subject contains any keyword from the `keyword_mapping` dictionary, move the email to the corresponding categorized mailbox. - Ensure safe operation for concurrent modifications. - Keyword mapping dictionary format: ```python { \\"python-list\\": [\\"python-list\\", \\"py-list\\"], \\"python-dev\\": [\\"python-dev\\", \\"py-dev\\"], \\"python-bugs\\": [\\"python-bugs\\", \\"py-bugs\\"] } ``` 3. `close_all(self)`: Closes all mailbox instances to ensure no resources are kept open. **Constraints:** - Handle possible mailbox-specific exceptions such as `KeyError`, `mailbox.NoSuchMailboxError`, etc. - Ensure no email is lost in case of interruptions; if an email is moved to a categorized mailbox, delete it from the main mailbox only after confirming it was successfully added. - The method `categorize_emails` should be able to run multiple times without duplicating or losing emails. **Example Usage:** ```python organizer = EmailOrganizer(\'~/Maildir\', { \\"python-list\\": mailbox.Maildir(\'~/Maildir/python-list\'), \\"python-dev\\": mailbox.mbox(\'~/mbox/python-dev\'), \\"python-bugs\\": mailbox.MH(\'~/Mail/python-bugs\') }) keyword_mapping = { \\"python-list\\": [\\"python-list\\", \\"py-list\\"], \\"python-dev\\": [\\"python-dev\\", \\"py-dev\\"], \\"python-bugs\\": [\\"python-bugs\\", \\"py-bugs\\"] } organizer.categorize_emails(keyword_mapping) organizer.close_all() ``` **Expected Input:** - `main_mailbox_path`: A string representing the path to the main mailbox (e.g., Maildir). - `categorized_mailboxes`: A dictionary where keys are category names (strings) and values are mailbox instances. - `keyword_mapping`: A dictionary where keys are category names (strings) and values are lists of keywords (strings) to search for in the subject headers. **Expected Output:** - Emails matching the keywords are moved to the corresponding categorized mailboxes. - The main mailbox is safely updated without losing any emails during the process. **Notes:** - Utilize the `lock()` and `unlock()` methods for concurrency safety. - Ensure to call `flush()` after making changes to any mailbox.","solution":"import mailbox class EmailOrganizer: def __init__(self, main_mailbox_path: str, categorized_mailboxes: dict): self.main_mailbox = mailbox.Maildir(main_mailbox_path) self.categorized_mailboxes = categorized_mailboxes def categorize_emails(self, keyword_mapping: dict): # Lock the main mailbox for safe concurrent operations self.main_mailbox.lock() try: for key, message in self.main_mailbox.iteritems(): subject = message[\'subject\'] for category, keywords in keyword_mapping.items(): if any(keyword in subject for keyword in keywords): categorized_mailbox = self.categorized_mailboxes[category] categorized_mailbox.lock() try: # Add message to categorized mailbox categorized_mailbox.add(message) categorized_mailbox.flush() # Remove message from main mailbox self.main_mailbox.remove(key) finally: categorized_mailbox.unlock() break self.main_mailbox.flush() finally: self.main_mailbox.unlock() def close_all(self): self.main_mailbox.close() for mailbox_instance in self.categorized_mailboxes.values(): mailbox_instance.close()"},{"question":"**Objective:** Implement a function that generates a Seaborn `swarmplot` based on specified parameters, demonstrating comprehension of Seaborn\'s functionality. **Task:** Write a function `generate_swarmplot(data: pd.DataFrame, x: str, y: str, hue: str, palette: str, dodge: bool, orient: str) -> None` that creates a customized swarm plot using the given parameters. Save the plot as a PNG file named `custom_swarmplot.png`. **Function Signature:** ```python import pandas as pd def generate_swarmplot(data: pd.DataFrame, x: str, y: str, hue: str, palette: str, dodge: bool, orient: str) -> None: pass ``` **Input Parameters:** - `data` (pd.DataFrame): The dataset to be used for the swarm plot. - `x` (str): The column name to be used for the x-axis. - `y` (str): The column name to be used for the y-axis. - `hue` (str): The column name to be used for the `hue` parameter. - `palette` (str): The palette name to be used for color mapping. - `dodge` (bool): If True, the hues will be separated along the x-axis. - `orient` (str): The orientation of the plot. Should be either \'h\' or \'v\'. **Output:** - The function saves the generated swarm plot as a PNG file named `custom_swarmplot.png`. **Constraints:** - Ensure the function handles missing or erroneous input by raising appropriate exceptions. - The function should utilize Seaborn and Matplotlib libraries for plotting. **Example Usage:** ```python import seaborn as sns # Load example dataset tips = sns.load_dataset(\\"tips\\") # Generate the swarm plot with the given parameters generate_swarmplot(data=tips, x=\'total_bill\', y=\'day\', hue=\'sex\', palette=\'deep\', dodge=True, orient=\'v\') ``` **Expected Outcome:** The function should create a swarm plot saved as `custom_swarmplot.png` with the points colored based on the \'sex\' column using the \'deep\' palette, dodged along the x-axis, and oriented vertically. **Performance Requirements:** The function should execute efficiently without unnecessary computational overhead. Ensure that the plots are clear, well-labeled, and professionally formatted.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_swarmplot(data: pd.DataFrame, x: str, y: str, hue: str, palette: str, dodge: bool, orient: str) -> None: Generates a seaborn swarm plot with the specified parameters and saves it as a PNG file. Parameters: - data (pd.DataFrame): The dataset to be used for the swarm plot. - x (str): The column name to be used for the x-axis. - y (str): The column name to be used for the y-axis. - hue (str): The column name to be used for the hue parameter. - palette (str): The palette name to be used for color mapping. - dodge (bool): If True, the hues will be separated along the x-axis. - orient (str): The orientation of the plot. Should be either \'h\' or \'v\'. Returns: None # Validate inputs if not isinstance(data, pd.DataFrame): raise ValueError(\\"Data must be a pandas DataFrame.\\") if x not in data.columns or y not in data.columns or (hue and hue not in data.columns): raise ValueError(\\"x, y, and hue must be columns in the DataFrame.\\") if orient not in [\'h\', \'v\']: raise ValueError(\\"orient must be either \'h\' or \'v\'.\\") plt.figure(figsize=(10, 6)) sns.swarmplot(data=data, x=x, y=y, hue=hue, palette=palette, dodge=dodge, orient=orient) plt.savefig(\\"custom_swarmplot.png\\") plt.close()"},{"question":"**Coding Challenge: Validate Python Code Syntax** **Objective:** Write a function that validates the syntax of a given piece of Python code according to the lexical analysis rules provided in the Python documentation. **Function Signature:** ```python def validate_python_syntax(code: str) -> bool: pass ``` **Input:** * `code` (str): A string containing Python code to be validated. **Output:** * Returns a boolean value: * `True` if the code adheres to the Python lexical analysis rules. * `False` otherwise. **Constraints:** * The length of the input string will not exceed 10,000 characters. * The input string will contain only valid ASCII characters. **Detailed Requirements:** 1. **Line Structure:** * Ensure that lines adhere to logical and physical line definitions, including handling NEWLINE tokens, line joining with backslashes, and implicit line joining within parentheses, brackets, or braces. * Comments should be correctly identified and ignored in the syntax analysis. 2. **Indentation:** * Validate that indentation levels are consistent and correct according to the rules. * Ensure no mixing of tabs and spaces, and that forms and levels of indentation adhere to Pythons strict guidelines. 3. **Identifiers and Keywords:** * Check that identifiers follow proper Unicode categories and do not collide with reserved keywords or soft keywords in improper contexts. * Ensure that the use of special identifiers like \\"__*__\\" names and class-private names adhere to guidelines. 4. **String and Bytes Literals:** * Validate string literals including proper use of escape sequences and formatted string literals. * Ensure correct handling of raw strings and byte literals with prefixes. 5. **Numeric Literals:** * Validate integer, floating-point, and imaginary literals to ensure they conform to the described lexical definitions, including proper use of underscores for readability. **Example:** ```python code_snippet = def square(x): # Return the square of x return x * x print(validate_python_syntax(code_snippet)) # Expected output: True invalid_code_snippet = def invalid_function(x): return x ** 2 return x ** 3 # Incorrect indentation print(validate_python_syntax(invalid_code_snippet)) # Expected output: False ``` **Notes:** * This function does not execute the code but only validates its syntax against the provided rules. * You can assume that multi-line strings and complex real-world examples will be part of the test cases. **Hints:** * Consider breaking down the problem into smaller functions focusing on specific rules for better modularity and testing. * Utilize regular expressions for matching patterns where applicable. * Think about edge cases including empty strings, strings with only comments, and code segments with mixed valid and invalid syntax elements.","solution":"import ast def validate_python_syntax(code: str) -> bool: Validates the syntax of a given Python code. Parameters: code (str): A string containing Python code to be validated. Returns: bool: True if the code adheres to the Python syntax rules, False otherwise. try: parsed_code = ast.parse(code) # If the code is parsed successfully, it\'s valid return True except SyntaxError: # If there\'s a SyntaxError, the code is not valid return False"},{"question":"**Question: Implement an Incremental XML Parser with Error Handling** As a Python developer, your task is to create an incremental XML parser using the `xml.sax.xmlreader` module. The parser should read XML data in chunks, process the content, handle various SAX events (such as start and end of elements), and manage errors gracefully. It should demonstrate proficiency with SAX-based XML parsing, including the use of `IncrementalParser`, `InputSource`, and event handling mechanisms. # Requirements: 1. **Class Definition**: - Define a class named `MyIncrementalXMLParser` that inherits from `xml.sax.xmlreader.IncrementalParser`. 2. **Constructor**: - Initialize necessary attributes, including a content handler, a DTD handler, an entity resolver, and an error handler. 3. **Method Implementations**: - `setContentHandler(handler)`: Set the content handler. - `setDTDHandler(handler)`: Set the DTD handler. - `setEntityResolver(handler)`: Set the entity resolver. - `setErrorHandler(handler)`: Set the error handler. - `feed(data)`: Process a chunk of XML data. - `close()`: Close the parser and finalize parsing. - `reset()`: Reset the parser to its initial state, ready to parse new documents. 4. **Event Handling**: - Implement methods to handle the start and end of elements (`startElement`, `endElement`), characters (`characters`), and errors (`error`, `fatalError`, `warning`). - Ensure that handlers for content, DTD, and entities are correctly invoked during parsing. 5. **XML Example**: - Provide a sample XML document. - Demonstrate how to use the `MyIncrementalXMLParser` class to parse the sample XML incrementally, handle events, and perform error handling. # Sample XML Document: ```xml <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2023</year> </book> <book id=\\"2\\"> <title>Advanced XML</title> <author>Jane Smith</author> <year>2021</year> </book> </library> ``` # Expected Usage: 1. Create an instance of `MyIncrementalXMLParser`. 2. Set appropriate handlers (content, DTD, entity, and error). 3. Feed chunks of the XML data to the parser. 4. Close the parser after feeding all data. 5. Reset the parser for new parsing if needed. # Constraints: - Ensure the parser handles XML data incrementally and does not block. - Implement proper error handling for well-formedness and other parsing-related errors. - Use the `xml.sax.xmlreader` module interfaces and classes effectively. ```python class MyIncrementalXMLParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): # Initialize the necessary attributes pass def setContentHandler(self, handler): # Set the content handler pass def setDTDHandler(self, handler): # Set the DTD handler pass def setEntityResolver(self, handler): # Set the entity resolver pass def setErrorHandler(self, handler): # Set the error handler pass def feed(self, data): # Process a chunk of XML data pass def close(self): # Close the parser and finalize parsing pass def reset(self): # Reset the parser to its initial state pass def startElement(self, name, attrs): # Handle the start of an XML element pass def endElement(self, name): # Handle the end of an XML element pass def characters(self, content): # Handle character data pass def error(self, exception): # Handle recoverable errors pass def fatalError(self, exception): # Handle non-recoverable errors pass def warning(self, exception): # Handle warnings pass # Provide a demonstration of how to use the MyIncrementalXMLParser class. ```","solution":"import xml.sax.xmlreader import xml.sax class MyIncrementalXMLParser(xml.sax.xmlreader.IncrementalParser): def __init__(self): super().__init__() self._content_handler = None self._dtd_handler = None self._entity_resolver = None self._error_handler = None self._parser = xml.sax.make_parser() self._parser.setFeature(xml.sax.handler.feature_namespaces, 0) self._parser.setContentHandler(self) self._parser.setDTDHandler(self) self._parser.setEntityResolver(self) self._parser.setErrorHandler(self) def setContentHandler(self, handler): self._content_handler = handler self._parser.setContentHandler(handler) def setDTDHandler(self, handler): self._dtd_handler = handler self._parser.setDTDHandler(handler) def setEntityResolver(self, handler): self._entity_resolver = handler self._parser.setEntityResolver(handler) def setErrorHandler(self, handler): self._error_handler = handler self._parser.setErrorHandler(handler) def feed(self, data): self._parser.feed(data) def close(self): self._parser.close() def reset(self): self._parser = xml.sax.make_parser() self._parser.setFeature(xml.sax.handler.feature_namespaces, 0) self._parser.setContentHandler(self._content_handler) self._parser.setDTDHandler(self._dtd_handler) self._parser.setEntityResolver(self._entity_resolver) self._parser.setErrorHandler(self._error_handler) # Callback methods for the content handler interface def startElement(self, name, attrs): if self._content_handler: self._content_handler.startElement(name, attrs) def endElement(self, name): if self._content_handler: self._content_handler.endElement(name) def characters(self, content): if self._content_handler: self._content_handler.characters(content) # Callback methods for the error handler interface def error(self, exception): if self._error_handler: self._error_handler.error(exception) def fatalError(self, exception): if self._error_handler: self._error_handler.fatalError(exception) def warning(self, exception): if self._error_handler: self._error_handler.warning(exception) # Example Handlers class MyContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Start Element: {name}, Attributes: {attrs.items()}\\") def endElement(self, name): print(f\\"End Element: {name}\\") def characters(self, content): print(f\\"Characters: {content}\\") class MyErrorHandler(xml.sax.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") # Demonstration if __name__ == \\"__main__\\": parser = MyIncrementalXMLParser() content_handler = MyContentHandler() error_handler = MyErrorHandler() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) xml_data = <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2023</year> </book> <book id=\\"2\\"> <title>Advanced XML</title> <author>Jane Smith</author> <year>2021</year> </book> </library> # Incrementally feed data to the parser for line in xml_data.splitlines(): parser.feed(line) # Close the parser after feeding all data parser.close()"},{"question":"# Objective Create a function that: 1. Generates a diverging color palette using seaborn\'s `diverging_palette` function with customizable parameters. 2. Applies this palette to a heatmap representation of a given dataset. 3. Saves the resulting heatmap plot to a file. # Function Signature ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def generate_heatmap(data, start_color, end_color, center_color, sep, s, l, output_file): Generates a heatmap of the provided data using a diverging color palette and saves the plot to a specified file. Parameters: data (pd.DataFrame): The input data as a pandas DataFrame. start_color (int): The hue of the start color of the palette (0-359). end_color (int): The hue of the end color of the palette (0-359). center_color (str): The center color of the palette (\'light\' or \'dark\'). sep (int): Amount of separation around the center value. s (int): Saturation of the palette endpoints (0-100). l (int): Lightness of the palette endpoints (0-100). output_file (str): The filename to save the resulting heatmap plot. Returns: None pass ``` # Input - `data`: A pandas DataFrame representing the data to plot in the heatmap. - `start_color`: An integer between 0 and 359 representing the hue of the starting color. - `end_color`: An integer between 0 and 359 representing the hue of the ending color. - `center_color`: A string that can be either \\"light\\" or \\"dark\\", representing the center color. - `sep`: An integer value representing the amount of separation around the center value. - `s`: An integer value between 0 and 100, representing the saturation of the endpoints. - `l`: An integer value between 0 and 100, representing the lightness of the endpoints. - `output_file`: A string representing the filename to save the heatmap plot. # Output - The function must save the heatmap plot to the specified file. # Constraints 1. The function should handle scenarios when the input data is not appropriate for generating a heatmap. 2. Appropriate error handling and informative messages should be provided for invalid inputs. # Example Usage ```python # Example DataFrame data = pd.DataFrame({ \'A\': [1, 2, 3, 4], \'B\': [5, 6, 7, 8], \'C\': [9, 10, 11, 12] }) # Generate a heatmap and save to \'heatmap.png\' generate_heatmap(data, start_color=240, end_color=20, center_color=\\"light\\", sep=30, s=50, l=35, output_file=\'heatmap.png\') ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def generate_heatmap(data, start_color, end_color, center_color, sep, s, l, output_file): Generates a heatmap of the provided data using a diverging color palette and saves the plot to a specified file. Parameters: data (pd.DataFrame): The input data as a pandas DataFrame. start_color (int): The hue of the start color of the palette (0-359). end_color (int): The hue of the end color of the palette (0-359). center_color (str): The center color of the palette (\'light\' or \'dark\'). sep (int): Amount of separation around the center value. s (int): Saturation of the palette endpoints (0-100). l (int): Lightness of the palette endpoints (0-100). output_file (str): The filename to save the resulting heatmap plot. Returns: None if not isinstance(data, pd.DataFrame): raise ValueError(\\"data must be a pandas DataFrame\\") if not (0 <= start_color <= 359) or not (0 <= end_color <= 359): raise ValueError(\\"start_color and end_color must be between 0 and 359\\") if center_color not in [\\"light\\", \\"dark\\"]: raise ValueError(\\"center_color must be either \'light\' or \'dark\'\\") if not (0 <= sep <= 100): raise ValueError(\\"sep must be between 0 and 100\\") if not (0 <= s <= 100): raise ValueError(\\"s must be between 0 and 100\\") if not (0 <= l <= 100): raise ValueError(\\"l must be between 0 and 100\\") # Generate diverging palette palette = sns.diverging_palette(start_color, end_color, s=s, l=l, sep=sep, center=center_color, as_cmap=True) # Plot heatmap plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=palette, annot=True, fmt=\\".2f\\", linewidths=.5) # Save heatmap plt.savefig(output_file) plt.close()"},{"question":"# Python310 Coding Question: Understanding and Handling the `None` Object In this assignment, you will write a Python function that integrates closely with the behavior of the `None` object. This function should demonstrate a solid understanding of how instances of `None` should be treated and handled. Task Description You need to implement a function named `process_none_object()` that takes an arbitrary number of arguments. The function should: 1. Return `True` if all the arguments are `None`. 2. Return `False` if none of the arguments is `None`. 3. Return the count of `None` objects if there is a mix of `None` and non-`None` objects. Function Signature ```python def process_none_object(*args) -> (bool, int): pass ``` Input - `*args`: - A variable number of arguments of any type. Output - A tuple: - First element: `True` if all arguments are `None`, `False` if none are `None`. - Second element: The count of `None` values if there is a mix of `None` and non-`None` values. Example ```python print(process_none_object(None, None, None)) # Output: (True, 0) print(process_none_object(1, 2, 3)) # Output: (False, 0) print(process_none_object(None, 1, None, 2)) # Output: (False, 2) ``` Constraints - The number of arguments passed to the function will be between 0 and 1000. - The function should handle the input efficiently. Implementation Notes - Remember to consider special cases like no arguments being passed, where the expected behavior should be clear from the logic above. - Ensure proper testing and edge case handling to cover scenarios such as mixed types, different counts of `None`, etc.","solution":"def process_none_object(*args) -> (bool, int): Processes a variable number of arguments. Returns a tuple where: - The first element is True if all arguments are None, otherwise False. - The second element is the count of None values if not all arguments are None, otherwise 0. if len(args) == 0: return (True, 0) none_count = args.count(None) if none_count == len(args): return (True, 0) elif none_count == 0: return (False, 0) else: return (False, none_count)"},{"question":"**Dataset Visualization and Regression Analysis Using Seaborn** # Objective This assessment task is designed to evaluate your ability to visualize data, fit regression models, and analyze their appropriateness using Seaborn. # Problem Statement You are given a dataset `tips` that records the total bill, tip amount, size of the party, and some additional categorical variables (like smoker, time, and sex) for bills in a restaurant. Your task is to write functions to: 1. Visualize a linear relationship between the total bill and the tip amount using Seaborn\'s `lmplot`. 2. Fit and visualize a polynomial regression model to the same data. 3. Handle potentially significant discrete variable (`size`) and visualize the fitted regression. 4. Evaluate the regression model fit using residual plots. 5. Compare the linear model fit for different groups based on the smoker status. # Detailed Instructions 1. **Linear Relationship**: - Write a function `plot_linear_regression` that takes the DataFrame and visualizes a linear regression model of `total_bill` predicting `tip` using `lmplot`. - The function should also plot a 95% confidence interval. 2. **Polynomial Regression**: - Enhance the `plot_linear_regression` function to fit and visualize a polynomial regression of order 2. - Add a parameter `order` to the function that specifies the order of the polynomial regression. 3. **Discrete Variable Handling**: - Write a function `plot_discrete_variable` that takes the DataFrame and visualizes the relationship between `size` and `tip`. Include an option to add jitter to the `size` variable. - Add a parameter `jitter` with a default value of `0` and apply jitter if this parameter is non-zero. 4. **Model Evaluation Using Residuals**: - Write a function `plot_residuals` that takes the DataFrame, fits a linear regression model (`total_bill` predicting `tip`), and plots the residuals using `residplot`. 5. **Comparison Across Groups**: - Write a function `compare_groups` that takes the DataFrame and visualizes the linear regression model fits for groups based on the `smoker` status. - Use different colors to distinguish between smoker and non-smoker groups. - Include regression lines with confidence intervals, and allow specifying different markers for different groups. # Expected Function Signatures ```python def plot_linear_regression(data: pd.DataFrame, order: int = 1) -> None: pass def plot_discrete_variable(data: pd.DataFrame, jitter: float = 0) -> None: pass def plot_residuals(data: pd.DataFrame) -> None: pass def compare_groups(data: pd.DataFrame) -> None: pass ``` # Dataset ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` # Constraints - Use Seaborn for visualizations. - Ensure that visualizations are clear and well-labeled. - The dataframe passed to functions will always be similar to the `tips` dataset in structure. # Submission Submit a Python script or Jupyter notebook with the above functions implemented. Include plots generated by calling each function with the `tips` dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the dataset tips = sns.load_dataset(\\"tips\\") def plot_linear_regression(data: pd.DataFrame, order: int = 1) -> None: Visualizes a regression model of total_bill predicting tip using lmplot. The order parameter specifies the polynomial order of the regression model. sns.lmplot(x=\'total_bill\', y=\'tip\', data=data, order=order, ci=95) plt.title(f\'{order}-order Polynomial Regression of Total Bill vs Tip\') plt.show() def plot_discrete_variable(data: pd.DataFrame, jitter: float = 0) -> None: Visualizes the relationship between size and tip. Adds jitter to the size variable if a non-zero jitter is provided. sns.lmplot(x=\'size\', y=\'tip\', data=data, x_jitter=jitter) plt.title(f\'Regression of Size vs Tip with jitter={jitter}\') plt.show() def plot_residuals(data: pd.DataFrame) -> None: Fits a linear regression model (total_bill predicting tip) and plots the residuals. sns.residplot(x=\'total_bill\', y=\'tip\', data=data) plt.title(\'Residual Plot of Total Bill vs Tip\') plt.show() def compare_groups(data: pd.DataFrame) -> None: Visualizes the linear regression model fits for groups based on the smoker status. sns.lmplot(x=\'total_bill\', y=\'tip\', hue=\'smoker\', data=data, markers=[\\"o\\", \\"x\\"], ci=95) plt.title(\'Linear Regression of Total Bill vs Tip by Smoker Status\') plt.show()"},{"question":"# Task You are required to write a Python function that uses the `imaplib` module to connect to an IMAP server, search for unread emails in the inbox, fetch their subjects, and return them as a list. # Requirements 1. **Function Signature**: `get_unread_email_subjects(server: str, username: str, password: str) -> list` 2. **Parameters**: - `server` (str): The IMAP server domain (e.g., \'imap.domain.com\'). - `username` (str): Username for logging into the IMAP server. - `password` (str): Password for logging into the IMAP server. 3. **Returns**: - A list of strings, where each string is the subject of an unread email. 4. **Constraints**: - Use the IMAP4 class for connecting to the server and fetching emails. - Handle exceptions gracefully, such as connection errors and login failures. - Ensure the connection is properly closed after operations. # Example ```python def get_unread_email_subjects(server: str, username: str, password: str) -> list: # Your implementation here # Example usage server = \'imap.domain.com\' username = \'user@example.com\' password = \'password\' unread_subjects = get_unread_email_subjects(server, username, password) print(unread_subjects) ``` # Additional Information You may need to refer to appropriate sections on `IMAP4` class methods such as `login`, `select`, `search`, `fetch`, and `logout` to ensure the operations are performed correctly. The mail fetching should target the `UNSEEN` criterion in the search command to retrieve only unread emails.","solution":"import imaplib import email def get_unread_email_subjects(server: str, username: str, password: str) -> list: Connects to an IMAP server and retrieves the subjects of unread emails. Parameters: - server (str): The IMAP server domain (e.g., \'imap.domain.com\'). - username (str): Username for logging into the IMAP server. - password (str): Password for logging into the IMAP server. Returns: - list: A list of subjects of unread emails. try: # Connect to the server mail = imaplib.IMAP4_SSL(server) # Login to the account mail.login(username, password) # Select the inbox mail.select(\\"inbox\\") # Search for unread emails result, data = mail.search(None, \'UNSEEN\') if result != \'OK\': return [] unread_subjects = [] for num in data[0].split(): result, msg_data = mail.fetch(num, \'(RFC822)\') if result != \'OK\': continue msg = email.message_from_bytes(msg_data[0][1]) subject, encoding = email.header.decode_header(msg[\'Subject\'])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \'utf-8\') unread_subjects.append(subject) # Logout and close the connection mail.logout() return unread_subjects except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Email MIME Document Serialization in Python You are required to implement a function that takes an `EmailMessage` object and serializes it into a byte stream. This serialized byte stream should then be written to a provided file-like object. The function should use the `email.generator.BytesGenerator` class to handle the serialization process according to the requirements mentioned below. Function Signature ```python def serialize_email_to_bytes(msg, file_like_object, mangle_from_=None, maxheaderlen=None, policy=None): Serialize the given EmailMessage object to a byte stream and write it to the provided file-like object. Parameters: msg (EmailMessage): The email message to be serialized. file_like_object (file-like object): A file-like object supporting the write method that will store the serialized byte stream. mangle_from_ (bool, optional): If True, put a \'>\' character in front of any line in the body that starts with the exact string \'From \'. maxheaderlen (int, optional): The maximum length of headers before they are folded. If 0, headers will not be wrapped. If None, default policy settings will be used. policy (Policy, optional): The policy to control message generation. If None, the policy associated with the message is used. See \'email.policy\' for more details. Returns: None pass ``` Parameters - `msg`: An instance of `EmailMessage` that you want to serialize. - `file_like_object`: A writable file-like object where the serialized byte stream will be stored. - `mangle_from_` (optional): Boolean value that determines whether to mangle lines starting with \\"From \\". Default is `None`. - `maxheaderlen` (optional): Integer indicating the max length of headers before they are wrapped. Default is `None`. - `policy` (optional): Email policy to use for serialization. Default is `None`. Requirements 1. Use the `BytesGenerator` class from the `email.generator` module to handle the serialization. 2. Ensure the serialized message adheres to the RFC standards by properly handling the `policy`. 3. Enable mangling and header length customization based on provided optional parameters. Example Usage ```python from email.message import EmailMessage from io import BytesIO import email.policy # Create an example email message msg = EmailMessage() msg.set_content(\\"This is a test email.\\") msg[\\"Subject\\"] = \\"Test\\" msg[\\"From\\"] = \\"sender@example.com\\" msg[\\"To\\"] = \\"recipient@example.com\\" # File-like object to store the serialized byte stream file_like_object = BytesIO() # Serialize the email to bytes and write to the file-like object serialize_email_to_bytes(msg, file_like_object, mangle_from_=True, maxheaderlen=78, policy=email.policy.default) # Verify the result file_like_object.seek(0) print(file_like_object.read()) ``` Implement the `serialize_email_to_bytes` function based on the description and the provided example.","solution":"from email.generator import BytesGenerator def serialize_email_to_bytes(msg, file_like_object, mangle_from_=None, maxheaderlen=None, policy=None): Serialize the given EmailMessage object to a byte stream and write it to the provided file-like object. Parameters: msg (EmailMessage): The email message to be serialized. file_like_object (file-like object): A file-like object supporting the write method that will store the serialized byte stream. mangle_from_ (bool, optional): If True, put a \'>\' character in front of any line in the body that starts with the exact string \'From \'. maxheaderlen (int, optional): The maximum length of headers before they are folded. If 0, headers will not be wrapped. If None, default policy settings will be used. policy (Policy, optional): The policy to control message generation. If None, the policy associated with the message is used. See \'email.policy\' for more details. Returns: None bytes_gen = BytesGenerator(file_like_object, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) bytes_gen.flatten(msg)"},{"question":"**Question:** Using the scikit-learn library, load the \\"iris\\" dataset and implement a machine learning pipeline to classify the species of iris flowers. Your task is to write a function `classify_iris` that does the following: 1. Loads the iris dataset using scikit-learn\'s `load_iris` function. 2. Splits the dataset into training and testing sets (80% training, 20% testing). 3. Implements a machine learning pipeline that includes: - Standardizing the features. - Training a Support Vector Machine (SVM) classifier. 4. Evaluates the performance of the trained model on the testing set and returns the accuracy score. **Input:** The function does not take any parameters. **Output:** Return a float representing the accuracy of the model on the test set. **Function Signature:** ```python def classify_iris() -> float: pass ``` **Constraints:** - You must use scikit-learns built-in functions for loading datasets, splitting datasets, and creating the pipeline. - Use `StandardScaler` for standardization and `SVC` for the SVM classifier. - Ensure the random state is set to 42 for reproducibility when splitting the dataset. **Example:** ```python accuracy = classify_iris() print(f\\"Model Accuracy: {accuracy:.2f}\\") ``` The expected output should be a float representing the accuracy score, typically a value between 0 and 1.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def classify_iris() -> float: # Load the iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a machine learning pipeline pipeline = make_pipeline(StandardScaler(), SVC(random_state=42)) # Train the pipeline on the training data pipeline.fit(X_train, y_train) # Predict on the test data y_pred = pipeline.predict(X_test) # Calculate and return the accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Question: Implementing and Testing Custom Charset Handling in Emails** Background: In email communication, different character sets may require different encodings for compatibility and proper handling. In particular, headers and bodies in emails might need specific encodings like quoted-printable or base64. Your task is to use the `email.charset` module to design and implement a solution that handles custom charset registrations, encodings, and conversions for given email data. **Task Description:** 1. Implement the function `setup_custom_charset(input_charset, header_enc, body_enc, output_charset)`. This function should: * Register a new charset with specified encoding settings for headers and bodies. * Register the appropriate output charset for conversions. * Return an instance of `email.charset.Charset` initialized with the provided `input_charset`. 2. Implement the function `encode_email_parts(charset_instance, header, body)`. This function should: * Use the `header_encode` method of the `Charset` instance to encode the given header string. * Use the `body_encode` method of the `Charset` instance to encode the given body string. * Return a tuple with the encoded header and body strings. 3. Implement the function `test_custom_charset()`. This function should: * Set up a custom charset where input charset is \\"iso-8859-1\\", header encoding is quoted-printable, body encoding is base64, and output charset is \\"utf-8\\". * Encode the following email components using this custom charset setup: - Header: \\"Subject: Testing Custom Charset\\" - Body: \\"This is the body of the email.\\" * Verify the encoded results as follows: - The header should be quoted-printable encoded. - The body should be base64 encoded. * Return a tuple with the encoded header and body. **Function Signatures:** ```python def setup_custom_charset(input_charset: str, header_enc: str, body_enc: str, output_charset: str): pass def encode_email_parts(charset_instance, header: str, body: str): pass def test_custom_charset() -> tuple: pass ``` **Constraints:** - The `input_charset`, `header_enc`, `body_enc`, and `output_charset` parameters must be valid strings. - Assume that the necessary codecs for charset conversion are available in your Python environment. **Example Usage:** ```python charset_instance = setup_custom_charset(\\"iso-8859-1\\", \\"quoted-printable\\", \\"base64\\", \\"utf-8\\") encoded_header, encoded_body = encode_email_parts(charset_instance, \\"Subject: Testing Custom Charset\\", \\"This is the body of the email.\\") # The `test_custom_charset` function should validate and return similar encoded results. result = test_custom_charset() print(result) ``` Ensure that your implementation correctly handles the charset registration and encoding according to the provided settings.","solution":"from email.charset import Charset, add_charset, QP, BASE64 def setup_custom_charset(input_charset: str, header_enc: str, body_enc: str, output_charset: str): Registers a custom charset and returns a Charset instance. header_enc_map = { \'quoted-printable\': QP, \'base64\': BASE64, } body_enc_map = { \'quoted-printable\': QP, \'base64\': BASE64, } add_charset(input_charset, header_enc_map[header_enc], body_enc_map[body_enc], output_charset) return Charset(input_charset) def encode_email_parts(charset_instance, header: str, body: str): Encodes the header and body of an email using the provided Charset instance. encoded_header = charset_instance.header_encode(header) encoded_body = charset_instance.body_encode(body) return encoded_header, encoded_body def test_custom_charset() -> tuple: Tests the custom charset setup and encoding functions. charset_instance = setup_custom_charset(\\"iso-8859-1\\", \\"quoted-printable\\", \\"base64\\", \\"utf-8\\") header = \\"Subject: Testing Custom Charset\\" body = \\"This is the body of the email.\\" encoded_header, encoded_body = encode_email_parts(charset_instance, header, body) assert encoded_header == charset_instance.header_encode(header) assert encoded_body == charset_instance.body_encode(body) return encoded_header, encoded_body"},{"question":"**Objective**: Implement a function that calculates the geometric mean of a list of positive numbers using functionalities from the Python `math` module. The geometric mean is defined as the `n`-th root of the product of `n` numbers. Function Signature: ```python def geometric_mean(numbers: List[float]) -> float: pass ``` Input: - `numbers`: A list of positive float numbers. The length of the list (`n`) will be at least 1 and at most 10,000. Output: - Returns the geometric mean of the input list, as a float. Constraints: - All elements in the list `numbers` will be positive float numbers. - For an empty list, return 0. Performance Requirements: - The function should be computationally efficient, even for the maximum input size. - Usage of the `math.prod` and `math.pow` functions is encouraged. Example: ```python >>> geometric_mean([1, 3, 9]) 3.0 >>> geometric_mean([2.5, 2.5, 2.5, 2.5]) 2.5 >>> geometric_mean([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) 4.528728688116765 ``` Steps for Implementation: 1. Use `math.prod` to compute the product of the numbers in the input list. 2. Calculate the `n`-th root of the product using `math.pow` where `n` is the length of the input list. 3. Return the resulting value.","solution":"import math from typing import List def geometric_mean(numbers: List[float]) -> float: if not numbers: return 0.0 product = math.prod(numbers) n = len(numbers) return math.pow(product, 1/n)"},{"question":"# Question Objective: Implement a custom operation in PyTorch using the `torch.library.custom_op` function and test it using `torch.library.opcheck` and `torch.autograd.gradcheck` to ensure its correctness. Description: You will create a custom operation `custom_relu` that replicates the behavior of the standard ReLU (Rectified Linear Unit) activation function, which is defined as: [ text{custom_relu}(x) = max(0, x) ] Your task is to implement this custom operation in PyTorch, register it, and then verify its correctness by writing tests for both functional output and gradient checking. Steps: 1. **Define the Custom Operation**: - Use `torch.library.custom_op` to define the custom operation `custom_relu`. - Implement the operation such that it performs the max(0, x) computation. 2. **Register the Custom Kernel**: - Register the kernel for the `custom_relu` operation using `torch.library.register_kernel`. 3. **Testing the Custom Operation**: - Write a function `test_custom_relu()` that tests the output of the `custom_relu` operation using the `torch.library.opcheck` function. - Write another function `test_custom_relu_grad()` that checks the gradient correctness using `torch.autograd.gradcheck`. Constraints: - The input to the custom operation will be a single tensor of arbitrary shape. - The custom operation should support backpropagation. Input: - A single PyTorch tensor `x` of arbitrary shape. Output: - The output should be a PyTorch tensor with the same shape as the input, with all negative values replaced by 0. Sample Code: ```python import torch import torch.library # Step 1: Define the custom operation def custom_relu(x): return x.clamp(min=0) # Register the custom operation my_lib = torch.library.Library(\\"my_ops\\", \\"DEF\\") my_lib.define(\\"custom_relu(Tensor x) -> Tensor\\") def relu_fn(x): return custom_relu(x) my_lib.impl(\\"custom_relu\\", relu_fn, \\"CPU\\") # Step 2: Write the testing functions def test_custom_relu(): # Example input tensor x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True) y = custom_relu(x) # Use torch.library.opcheck torch.library.opcheck(y == torch.tensor([0.0, 0.0, 1.0, 2.0])) def test_custom_relu_grad(): # Example input tensor x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True) # Use gradcheck torch.autograd.gradcheck(custom_relu, x) # Run the tests to verify the operation test_custom_relu() test_custom_relu_grad() ``` Note: Write the `custom_relu` definition, register it, and implement the test functions adhering to the details provided.","solution":"import torch from torch import Tensor # Step 1: Define the custom operation def custom_relu(x: Tensor) -> Tensor: return torch.maximum(x, torch.tensor(0., device=x.device)) # Step 2: Register the custom kernel (simplified for educational purposes, no OP Library used) # Normally, you might use torch.library.custom_op and library registration procedures # Testing functions: def test_custom_relu(): # Example input tensor x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True) y = custom_relu(x) # Expected output tensor expected = torch.tensor([0.0, 0.0, 1.0, 2.0]) # Check if the output matches the expected tensor assert torch.equal(y, expected), f\\"Expected {expected}, but got {y}\\" def test_custom_relu_grad(): # Example input tensor x = torch.tensor([-1.0, 0.0, 1.0, 2.0], dtype=torch.double, requires_grad=True) # Use gradcheck to check gradients assert torch.autograd.gradcheck(custom_relu, (x,), raise_exception=True)"},{"question":"# Custom Event Loop Policy and Child Process Watcher You are tasked to implement a custom event loop policy and a custom child process watcher that regulates how event loops manage child processes. You need to subclass the necessary `asyncio` classes to create your custom implementations. Requirements: 1. **Custom Event Loop Policy**: - Define a class `CustomEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. - Override the `get_event_loop` method to print a message every time an event loop is retrieved and then return the event loop. 2. **Custom Child Process Watcher**: - Define a class `CustomChildWatcher` that subclasses `ThreadedChildWatcher`. - Override the `add_child_handler` method to print a message when a new child handler is added and then perform the regular functionality. 3. **Integrate Custom Components**: - Implement a function `setup_custom_loop_policy` to set up the custom event loop policy and child process watcher. - This function should: - Instantiate your `CustomEventLoopPolicy`. - Set the custom policy using `asyncio.set_event_loop_policy`. - Create an event loop and attach your custom child watcher to it. Provided Function: - **Test Function**: ```python async def spawn_and_wait(): loop = asyncio.get_event_loop() process = await asyncio.create_subprocess_exec(\'python\', \'--version\') await process.wait() ``` Your Implementation: - Complete the function `setup_custom_loop_policy` to accomplish the requirements. ```python import asyncio class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(f\\"Custom get_event_loop called: {loop}\\") return loop class CustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Adding child handler for PID {pid}\\") super().add_child_handler(pid, callback, *args) def setup_custom_loop_policy(): custom_policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(custom_policy) loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) watcher = CustomChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(watcher) # Test the setup setup_custom_loop_policy() # Running the test function asyncio.run(spawn_and_wait()) ``` # Input and Output: - **Input**: No direct input. Functionally tested via `spawn_and_wait`. - **Output**: - Messages printed by `CustomEventLoopPolicy` indicating the retrieval of the event loop. - Messages printed by `CustomChildWatcher` indicating the addition of child handlers when subprocesses are spawned. # Constraints: - You must use the `asyncio` library. - Ensure your policy and watcher override methods appropriately to demonstrate their functionality. The task above requires you to understand the intricacies of subclassing, the event loop and child watcher mechanisms in `asyncio`, and the asynchronous programming model in Python.","solution":"import asyncio class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(f\\"Custom get_event_loop called: {loop}\\") return loop class CustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Adding child handler for PID {pid}\\") super().add_child_handler(pid, callback, *args) def setup_custom_loop_policy(): custom_policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(custom_policy) loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) watcher = CustomChildWatcher() asyncio.get_event_loop_policy().set_child_watcher(watcher) async def spawn_and_wait(): loop = asyncio.get_event_loop() process = await asyncio.create_subprocess_exec(\'python\', \'--version\') await process.wait()"},{"question":"**Coding Assessment Question:** **Title:** Implement Function with Complex Scoping and Exception Handling **Objective:** Demonstrate understanding of Python\'s execution model, name scope resolution, and exception handling by implementing a function that adheres to the given constraints and requirements. **Problem Statement:** You are required to implement a function `process_data` that takes two lists `data` and `operations` as input. The function should process data according to the operations specified, taking careful consideration of variable scopes and handling exceptions that may occur. The operations could be arithmetic operations that involve elements of `data` and may raise exceptions such as `ZeroDivisionError` or reference undefined variables. **Function Signature:** ```python def process_data(data: list, operations: list) -> dict: ``` **Input:** - `data` (list): A list of numerical values. - `operations` (list): A list of strings, where each string is a Python arithmetic expression involving elements of `data` by index. **Output:** - A dictionary with two keys: - `\\"results\\"`: A list of results from performing the operations. - `\\"errors\\"`: A list of error messages for each operation where an exception occurred. **Constraints:** 1. `data` will only contain numerical values. 2. `operations` can include operations like addition, subtraction, multiplication, division, and modulus. 3. The operations list will contain valid Python expressions. 4. You must handle at least the following exceptions: `ZeroDivisionError`, `IndexError`, and `NameError`. **Example:** ```python data = [10, 0, 5, 3] operations = [\\"data[0] + data[2]\\", \\"data[1] / data[2]\\", \\"data[3] * data[4]\\", \\"data[0] // data[1]\\"] result = process_data(data, operations) print(result) # Expected Output: # {\'results\': [15, 0.0], \'errors\': [\'IndexError: list index out of range\', \'ZeroDivisionError: integer division or modulo by zero\']} ``` **Requirements:** 1. Implement the function such that it correctly processes the given operations in the context of provided data. 2. Ensure proper handling and capturing of exceptions with informative error messages. 3. Demonstrate understanding of variable scopes and their resolution within different contexts. **Note:** You must not use `eval` for this task. Implement a safe evaluation method for expressions.","solution":"def process_data(data, operations): results = [] errors = [] for operation in operations: try: # Using a safe_eval function to avoid using eval directly for security reasons result = safe_eval(operation, data) results.append(result) except ZeroDivisionError as e: errors.append(f\\"ZeroDivisionError: {e}\\") except IndexError as e: errors.append(f\\"IndexError: {e}\\") except NameError as e: errors.append(f\\"NameError: {e}\\") return {\'results\': results, \'errors\': errors} def safe_eval(expression, data): allowed_names = {\\"data\\": data} code = compile(expression, \\"<string>\\", \\"eval\\") for name in code.co_names: if name not in allowed_names: raise NameError(f\\"Use of name \'{name}\' is not allowed\\") return eval(code, {\\"__builtins__\\": {}}, allowed_names)"},{"question":"Implementing a Windowed Signal Processing Function in PyTorch Problem Statement You are required to implement a function using PyTorch that applies a specified window function to a given signal and then performs a Fast Fourier Transform (FFT) on the windowed signal. Requirements 1. Implement a function named `windowed_fft` that takes in the following parameters: - `signal` (torch.Tensor): A 1-D tensor representing the input signal. - `window_name` (str): A string specifying the window function to be applied. Supported window functions are `\'bartlett\'`, `\'blackman\'`, `\'hamming\'`, and `\'hann\'`. - `n_fft` (int, optional): The number of FFT points. If not specified, it should default to the length of the input signal. 2. The function should apply the specified window to the signal and then compute the FFT of the windowed signal. 3. The function should return a tensor containing the FFT results. Function Signature ```python import torch def windowed_fft(signal: torch.Tensor, window_name: str, n_fft: int = None) -> torch.Tensor: pass ``` Constraints - You should use the window functions available in `torch.signal.windows`. - If the provided `window_name` is not one of the supported window functions, the function should raise a `ValueError` with an appropriate error message. - The input signal tensor should only contain real numbers. - Performance should be considered; avoid unnecessary computations or memory usage. Example ```python signal = torch.arange(10).float() window_name = \'hamming\' result = windowed_fft(signal, window_name) # Sample Output (this is just an indicative output; exact values may vary) print(result) # tensor([ 44.7200+0.0000j, -6.4383+12.8056j, -4.2673+6.3482j, ..., 1.8449+7.3193j]) ``` Note You can find the documentation for the window functions in the `torch.signal.windows` module to help you implement this function.","solution":"import torch def windowed_fft(signal: torch.Tensor, window_name: str, n_fft: int = None) -> torch.Tensor: if window_name == \'bartlett\': window = torch.bartlett_window(len(signal)) elif window_name == \'blackman\': window = torch.blackman_window(len(signal)) elif window_name == \'hamming\': window = torch.hamming_window(len(signal)) elif window_name == \'hann\': window = torch.hann_window(len(signal)) else: raise ValueError(f\\"Unsupported window function: {window_name}\\") windowed_signal = signal * window if n_fft is None: n_fft = len(signal) fft_result = torch.fft.fft(windowed_signal, n_fft) return fft_result"},{"question":"# Coding Challenge: Custom Log Compression and Decompression Objective: Implement a set of functions that compresses and decompresses log data using different methods from the `bz2` module. This will assess your understanding of file handling, incremental compression, and one-shot (de)compression. Task: Implement the following functions: 1. **`compress_log_file(input_file: str, output_file: str, compresslevel: int = 9) -> None`** - **Input**: - `input_file` (str): Path to the input log file (plaintext). - `output_file` (str): Path where the compressed file should be saved. - `compresslevel` (int, optional): Compression level (1 to 9). Default is 9. - **Output**: None - **Functionality**: Read the log data from `input_file`, compress it using the specified compression level, and save it as a bzip2 file to `output_file`. 2. **`decompress_log_file(compressed_file: str) -> str`** - **Input**: - `compressed_file` (str): Path to the compressed log file. - **Output**: - Returns the decompressed log data as a string. - **Functionality**: Read the compressed log data from `compressed_file` and return the decompressed data as a string. 3. **`incremental_compress(log_data: bytes, compresslevel: int = 9) -> bytes`** - **Input**: - `log_data` (bytes): The log data to be compressed incrementally. - `compresslevel` (int, optional): Compression level (1 to 9). Default is 9. - **Output**: - Returns the compressed data as a bytes object. - **Functionality**: Compress the given log data using incremental compression and return the compressed result. 4. **`incremental_decompress(compressed_data: bytes) -> bytes`** - **Input**: - `compressed_data` (bytes): The compressed data to be decompressed incrementally. - **Output**: - Returns the decompressed data as a bytes object. - **Functionality**: Decompress the given compressed data incrementally and return the decompressed result. Constraints: - Assume `input_file` and `output_file` paths are valid. - Handle file operations and potential exceptions properly. - Ensure that the incremental compression and decompression maintain data integrity compared to the one-shot methods. Example Usage: ```python # Compress a log file compress_log_file(\'server.log\', \'server.log.bz2\') # Decompress the log file log_content = decompress_log_file(\'server.log.bz2\') print(log_content) # Compress data incrementally log_data = b\\"This is a test log entry.\\" compressed_data = incremental_compress(log_data) print(compressed_data) # Decompress data incrementally decompressed_data = incremental_decompress(compressed_data) print(decompressed_data == log_data) # Should be True ``` Ensure your implementation passes the example usage and adheres to the specified functionalities.","solution":"import bz2 def compress_log_file(input_file: str, output_file: str, compresslevel: int = 9) -> None: Compress a log file using bzip2. :param input_file: Path to the plaintext log file. :param output_file: Path where the compressed file should be saved. :param compresslevel: Compression level (1 to 9), default is 9. try: with open(input_file, \'rb\') as f_in: data = f_in.read() compressed_data = bz2.compress(data, compresslevel=compresslevel) with open(output_file, \'wb\') as f_out: f_out.write(compressed_data) except Exception as e: raise IOError(f\\"An error occurred while compressing the file: {e}\\") def decompress_log_file(compressed_file: str) -> str: Decompress a bzip2 log file. :param compressed_file: Path to the compressed log file. :return: Decompressed log data as a string. try: with open(compressed_file, \'rb\') as f_in: compressed_data = f_in.read() decompressed_data = bz2.decompress(compressed_data) return decompressed_data.decode(\'utf-8\') except Exception as e: raise IOError(f\\"An error occurred while decompressing the file: {e}\\") def incremental_compress(log_data: bytes, compresslevel: int = 9) -> bytes: Incrementally compress log data. :param log_data: The log data to be compressed. :param compresslevel: Compression level (1 to 9), default is 9. :return: Compressed data as bytes. compressor = bz2.BZ2Compressor(compresslevel) compressed_data = compressor.compress(log_data) compressed_data += compressor.flush() return compressed_data def incremental_decompress(compressed_data: bytes) -> bytes: Incrementally decompress data. :param compressed_data: The compressed data to be decompressed. :return: Decompressed data as bytes. decompressor = bz2.BZ2Decompressor() return decompressor.decompress(compressed_data)"},{"question":"Objective To assess your understanding of the Seaborn package, you need to create detailed visualizations using `swarmplot` and `catplot`. You will demonstrate both basic and advanced concepts of these functions as practiced in data visualization. Problem Statement You are provided with the `tips` dataset from the Seaborn library. Your task is to create various visualizations using the `swarmplot` and `catplot` functions. Follow the specific instructions for each part: 1. **Basic Swarm Plot**: - Create a basic swarm plot using the `swarmplot` function to show the distribution of `total_bill` for each day of the week. The x-axis should represent `day` and the y-axis should represent `total_bill`. 2. **Swarm Plot with Hue**: - Enhance the previous plot by adding a hue dimension using the `sex` column to differentiate between male and female customers. Ensure that the points for each sex are intermingled without any dodging. 3. **Facet Plot**: - Create a `catplot` with `kind=\\"swarm\\"` that shows the relationship between `total_bill` and `time`, with different facets for each day. Use `sex` as the hue dimension. The plot should have an aspect ratio of 0.5 for each facet. 4. **Customized Swarm Plot**: - Finally, create a customized swarm plot where: - The x-axis represents `size` and the y-axis represents `total_bill`. - The plot should be horizontally oriented. - Use smaller point sizes (`size=3`) to avoid overlapping. - The markers should be `x` and the linewidth should be set to 1. Input - You do not need to provide any input as the dataset (`tips`) will be loaded using `sns.load_dataset(\'tips\')`. Expected Output Four plots as described in the problem statement. Constraints - Ensure that your code is efficient and leverages the features of Seaborn appropriately. - Use the default Seaborn theme and settings. Example Code Below is an example to get you started with loading the dataset and setting the theme: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Set theme sns.set_theme(style=\\"whitegrid\\") # Your code from here onward ``` Make sure to provide all necessary plots in the same code cell or multiple cells as needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Set theme sns.set_theme(style=\\"whitegrid\\") # Part 1: Basic Swarm Plot def basic_swarm_plot(): plt.figure(figsize=(10, 6)) sns.swarmplot(x=\'day\', y=\'total_bill\', data=tips) plt.title(\\"Basic Swarm Plot of Total Bill by Day\\") plt.show() # Part 2: Swarm Plot with Hue def swarm_plot_with_hue(): plt.figure(figsize=(10, 6)) sns.swarmplot(x=\'day\', y=\'total_bill\', data=tips, hue=\'sex\') plt.title(\\"Swarm Plot of Total Bill by Day with Sex Hue\\") plt.show() # Part 3: Facet Plot def facet_plot(): g = sns.catplot(x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=tips, kind=\\"swarm\\", aspect=0.5) g.fig.suptitle(\\"Facet Plot of Total Bill by Time with Sex and Day Facets\\", y=1.05) plt.show() # Part 4: Customized Swarm Plot def customized_swarm_plot(): plt.figure(figsize=(10, 6)) sns.swarmplot(y=\'total_bill\', x=\'size\', data=tips, size=3, marker=\'x\', linewidth=1) plt.title(\\"Customized Swarm Plot of Total Bill by Size\\") plt.show()"},{"question":"**Objective**: Implement a secure message authentication system that ensures both data integrity and authenticity using Python\'s cryptographic services. **Problem Statement**: Write a function `secure_message_system(data: str, key: str) -> dict` that: 1. Accepts a message `data` and a secret `key` as input. 2. Generates a cryptographic hash of the message using the BLAKE2 algorithm. 3. Uses the generated hash and the secret key to create an HMAC. 4. Returns a dictionary containing the following: - Original message - Generated BLAKE2 hash (hexadecimal format) - HMAC of the message using the key (hexadecimal format) - A cryptographically secure random token for additional security (base64 format) **Constraints**: - You must use the BLAKE2b algorithm with a digest size of 32 bytes for hashing. - HMAC should use the SHA256 hash function. - The secure random token should be 16 bytes long. **Input Format**: - `data`: A string representing the message to be secured. - `key`: A string representing the secret key used in HMAC. **Output Format**: - A dictionary with keys `message`, `hash`, `hmac`, and `token`. **Example**: ```python data = \\"Hello, secure world!\\" key = \\"my_secret_key\\" result = secure_message_system(data, key) ``` Example output: ```python { \\"message\\": \\"Hello, secure world!\\", \\"hash\\": \\"5d41402abc4b2a76b9719d911017c592\\", \\"hmac\\": \\"cbe3c207763e9b573b45f125ef426b9c\\", \\"token\\": \\"dGhpcyBpcyBhIHNlY3VyZSB0b2tlbg==\\" } ``` **Note**: - Ensure that the dictionary values are in the correct formats as specified. - Make use of Python\'s `hashlib`, `hmac`, and `secrets` modules for the implementation. **Hints**: - Look into the `blake2b` function in the `hashlib` module for creating the hash. - Use the `new` function from the `hmac` module to create the HMAC. - Use the `token_bytes` function from the `secrets` module to generate the random token.","solution":"import hashlib import hmac import secrets import base64 def secure_message_system(data: str, key: str) -> dict: Secures a message using BLAKE2b hashing and HMAC with SHA256 Returns a dictionary with the original message, hash, hmac, and a random token. # Generate BLAKE2b hash of the message blake2b_hash = hashlib.blake2b(data.encode(), digest_size=32).hexdigest() # Generate HMAC of the message using the key hmac_hash = hmac.new(key.encode(), data.encode(), hashlib.sha256).hexdigest() # Generate a cryptographically secure random token random_token = base64.b64encode(secrets.token_bytes(16)).decode(\'utf-8\') return { \\"message\\": data, \\"hash\\": blake2b_hash, \\"hmac\\": hmac_hash, \\"token\\": random_token }"},{"question":"**SMTP Server Implementation** You are required to implement a basic custom SMTP server using the deprecated `smtpd` module in Python 3.10. Your server should extend the `smtpd.SMTPServer` class and override the `process_message` method to handle incoming emails. # Task 1. **Create a class `CustomSMTPServer` that inherits from `smtpd.SMTPServer`.** 2. **Override the `process_message` method to handle incoming email messages as follows:** - Print the sender\'s email address (`mailfrom`). - Print the recipient(s) email address(es) (`rcpttos`). - Print the contents of the email (`data`). # Input Formats - The `process_message` method will receive the following arguments: - `peer`: Address of the remote host. - `mailfrom`: Envelope originator email address. - `rcpttos`: List of recipient email addresses. - `data`: String containing the contents of the email. # Output Formats - Print the `mailfrom`, `rcpttos`, and `data` values. # Constraints - The `data_size_limit` should be set to a maximum of 10 MB. - The `decode_data` flag should be set to ensure data is a Unicode string. # Example ```python from smtpd import SMTPServer import asyncore class CustomSMTPServer(SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(f\\"Mail from: {mailfrom}\\") print(f\\"Rcpt to: {rcpttos}\\") print(f\\"Data: {data}\\") return None if __name__ == \\"__main__\\": server = CustomSMTPServer((\\"localhost\\", 1025), None, data_size_limit=10*1024*1024, decode_data=True) asyncore.loop() ``` In the provided code: - We override the `process_message` method to print the sender, recipients, and message data. - The `CustomSMTPServer` binds to localhost on port 1025 and has a maximum data size limit of 10 MB. - The `asyncore.loop()` keeps the server running and handling incoming emails. Implement this `CustomSMTPServer` class and ensure it works correctly by sending a test email to the server.","solution":"from smtpd import SMTPServer import asyncore import smtpd class CustomSMTPServer(SMTPServer): def __init__(self, localaddr, remoteaddr, data_size_limit=10*1024*1024, decode_data=True): super().__init__(localaddr, remoteaddr, data_size_limit=data_size_limit, decode_data=decode_data) def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): print(f\\"Mail from: {mailfrom}\\") print(f\\"Rcpt to: {rcpttos}\\") print(f\\"Data: {data}\\") return None if __name__ == \\"__main__\\": # Create an instance of the custom SMTP server server = CustomSMTPServer((\\"localhost\\", 1025), None) asyncore.loop()"},{"question":"**Objective:** Create a MIME multipart email using the `email.mime` module in Python. **Problem Statement:** You are required to write a function `create_multipart_email` that constructs a MIME multipart email. The email should include: 1. A plain text message. 2. An attachment of an image file. 3. An attachment of an audio file. 4. An application-specific data attachment. The function should return the string representation of the constructed email. **Function Prototype:** ```python def create_multipart_email(text: str, image_path: str, audio_path: str, app_data: bytes) -> str: pass ``` **Input:** - `text`: A string containing the plain text message to be included in the email. - `image_path`: A string representing the path to the image file to be attached. - `audio_path`: A string representing the path to the audio file to be attached. - `app_data`: A bytes object containing application-specific raw data to be attached. **Output:** - A string representing the complete MIME multipart email. **Constraints:** - The image file should be recognizable by the standard Python `imghdr` module. - The audio file should be recognizable by the standard Python `sndhdr` module. - Ensure that the correct MIME types are applied to each attachment. - Use base64 encoding for all attachments. **Example:** ```python text_message = \\"This is a sample message.\\" image_path = \\"path/to/image.png\\" audio_path = \\"path/to/audio.wav\\" app_data = b\'Sample application data\' email_string = create_multipart_email(text_message, image_path, audio_path, app_data) print(email_string) ``` **Requirements:** 1. Implement the function `create_multipart_email` using the classes from the `email.mime` module. 2. Use `MIMEMultipart` to create the multipart container. 3. Use `MIMEText` for the text message. 4. Use `MIMEImage` for the image attachment. 5. Use `MIMEAudio` for the audio attachment. 6. Use `MIMEApplication` for the application-specific data attachment. 7. Ensure all components are properly encoded and included in the email. 8. Handle and report any errors or exceptions appropriately (e.g., file not found, unsupported file type). **Additional Notes:** - You may assume all provided paths and data are valid for the purpose of this task. - Pay attention to MIME type and encoding specifics for each attachment type. - String representation of the email should be returned without any modifications or additional processing outside of the `email` package.","solution":"import os from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email.mime.application import MIMEApplication from email import encoders def create_multipart_email(text: str, image_path: str, audio_path: str, app_data: bytes) -> str: # Create the container (outer) email message. msg = MIMEMultipart() msg[\'Subject\'] = \'Your Multipart Email\' msg[\'From\'] = \'example@example.com\' msg[\'To\'] = \'recipient@example.com\' # Attach the plain text message part1 = MIMEText(text, \'plain\') msg.attach(part1) # Attach the image file with open(image_path, \'rb\') as img_file: img_data = img_file.read() part2 = MIMEImage(img_data, name=os.path.basename(image_path)) msg.attach(part2) # Attach the audio file with open(audio_path, \'rb\') as audio_file: audio_data = audio_file.read() part3 = MIMEAudio(audio_data, name=os.path.basename(audio_path)) msg.attach(part3) # Attach the application-specific data part4 = MIMEApplication(app_data, name=\\"application_data\\") msg.attach(part4) # Return the string representation of the email return msg.as_string()"},{"question":"# Question: Advanced Dot Plot Customization Using Seaborn Objective Create a dot plot using seaborn\'s `objects` interface to effectively visualize the `glue` dataset. Your plot should demonstrate the following: 1. Custom dot appearance. 2. Reduction of overplotting. 3. Faceting and mapping properties. 4. Inclusion of error bars. Dataset Use the built-in seaborn `glue` dataset. Task 1. **Load the `glue` dataset**: ```python from seaborn import load_dataset glue = load_dataset(\\"glue\\") ``` 2. **Create a faceted dot plot** of the `glue` dataset where: - The x-axis represents the `Score`. - The y-axis represents the `Model`. - Use `Task` as the faceting variable, wrapping facets in rows of 4. - Limit the x-axis from -5 to 105. 3. Customize the dots: - Set the point size to `6`. - Color the dots based on the `Year` variable. - Use different markers (`o`, `s`) based on the `Encoder` variable. 4. To handle overplotting, apply: - Dodging to separate the points for `Year`. - Jittering (0.2) to spread the points. 5. Add error bars to the plot: - Combine the dots with error bars that represent the standard error of the mean (`se`), scaled by 2. 6. Your final output should be a comprehensive visualization meeting all the criteria above. Example of Expected Execution Here is an example code that outlines the steps mentioned above. Complete the implementation based on these instructions: ```python import seaborn.objects as so # 1. Load the glue dataset glue = load_dataset(\\"glue\\") # 2. Define the faceted dot plot p = so.Plot(glue, \\"Score\\", \\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) # 3-4. Add the customized dots with dodging and jittering p = (p .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") .add(so.Dot(), so.Dodge(), so.Jitter(0.2)) ) # 5. Add error bars to the plot p = p.add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) # Render the plot p.show() ``` Submission Write your final code inside a Jupyter Notebook and ensure all necessary dependencies are included. The final cell should display your customized plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_dot_plot(): # 1. Load the glue dataset glue = load_dataset(\\"glue\\") # 2. Define the faceted dot plot p = so.Plot(glue, \\"Score\\", \\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) # 3-4. Add the customized dots with dodging and jittering p = (p .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") .add(so.Dot(), so.Dodge(), so.Jitter(0.2)) ) # 5. Add error bars to the plot p = p.add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) # Render the plot p.show()"},{"question":"# Command-Line Argument Parser You are required to implement a command-line argument parser using the `getopt` module. Your implementation should be able to handle both short and long options and manage optional and required arguments accordingly. Requirements: 1. Implement a function `parse_arguments(args)` that takes a list of command-line arguments (excluding the program name) and parses them using the `getopt` module. 2. The function should support the following options: - Short options: - `-h`: Display help message. - `-o [file]`: Specify an output file. - `-v`: Enable verbose mode. - Long options: - `--help`: Display help message. - `--output=[file]`: Specify an output file. - `--verbose`: Enable verbose mode. 3. The function should return a dictionary with the following keys: - `help`: `True` if help is requested, otherwise `False`. - `output`: The output file specified or `None` if not specified. - `verbose`: `True` if verbose mode is enabled, otherwise `False`. 4. Properly handle errors using exceptions (`getopt.GetoptError`). If an error occurs, the function should return a dictionary with the key `error` and the error message as its value. Function Signature: ```python def parse_arguments(args: list) -> dict: pass ``` Example Usage: ```python # Example 1: args = [\\"-h\\"] output = parse_arguments(args) print(output) # Output: {\'help\': True, \'output\': None, \'verbose\': False} # Example 2: args = [\\"-o\\", \\"output.txt\\", \\"--verbose\\"] output = parse_arguments(args) print(output) # Output: {\'help\': False, \'output\': \'output.txt\', \'verbose\': True} # Example 3: args = [\\"-x\\"] output = parse_arguments(args) print(output) # Output: {\'error\': \'option -x not recognized\'} ``` Constraints: 1. Do not use the `argparse` module. 2. The function should handle various edge cases such as missing required arguments, unrecognized options, and multiple occurrences of options. Implement the `parse_arguments` function to fulfill the above requirements.","solution":"import getopt def parse_arguments(args): Parse command line arguments. Args: args (list): List of command line arguments (excluding the program name) Returns: dict: Dictionary containing the parsed options short_opts = \\"ho:v\\" long_opts = [\\"help\\", \\"output=\\", \\"verbose\\"] result = { \'help\': False, \'output\': None, \'verbose\': False, } try: opts, _ = getopt.getopt(args, short_opts, long_opts) for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): result[\'help\'] = True elif opt in (\\"-o\\", \\"--output\\"): result[\'output\'] = arg elif opt in (\\"-v\\", \\"--verbose\\"): result[\'verbose\'] = True except getopt.GetoptError as err: return {\'error\': str(err)} return result"},{"question":"# Advanced Python Floats and Memory Management You are tasked to interact with custom C functions for Python to handle floating-point numbers. Create a Python module `custom_floats` that leverages these functions and exposes the following functionalities: 1. **is_float(obj)**: - **Input**: A Python object `obj`. - **Output**: Boolean indicating whether `obj` is a float or a subtype of a float. 2. **is_exact_float(obj)**: - **Input**: A Python object `obj`. - **Output**: Boolean indicating whether `obj` is exactly a float, not a subtype. 3. **create_float_from_string(s)**: - **Input**: A string `s` representing a floating-point number. - **Output**: A Python float object created from `s`. 4. **create_float_from_double(d)**: - **Input**: A double `d`. - **Output**: A Python float object created from `d`. 5. **to_double(pyfloat)**: - **Input**: A Python float object `pyfloat`. - **Output**: The double representation of `pyfloat`. 6. **get_float_info()**: - **Output**: A dictionary containing the precision, minimum, and maximum values of a float. 7. **max_float()**: - **Output**: The maximum representable finite float. 8. **min_float()**: - **Output**: The minimum normalized positive float. **Constraints**: - You should perform appropriate error handling, especially for invalid inputs to functions creating floats from strings and converting objects to doubles. - You should rely on the mentioned functions from the documentation to implement your solution. Here\'s a template to get started: ```python def is_float(obj): Check if the provided object is a float or a subtype of float. pass # Implement using PyFloat_Check def is_exact_float(obj): Check if the provided object is exactly a float, not a subtype. pass # Implement using PyFloat_CheckExact def create_float_from_string(s): Create a float from a string. pass # Implement using PyFloat_FromString def create_float_from_double(d): Create a float from a double. pass # Implement using PyFloat_FromDouble def to_double(pyfloat): Convert a Python float to a double. pass # Implement using PyFloat_AsDouble or PyFloat_AS_DOUBLE def get_float_info(): Get information on the precision and value limits of float. pass # Implement using PyFloat_GetInfo def max_float(): Get the maximum representable finite float. pass # Implement using PyFloat_GetMax def min_float(): Get the minimum normalized positive float. pass # Implement using PyFloat_GetMin ``` Ensure you write test cases to validate the correctness of each function.","solution":"import sys def is_float(obj): Check if the provided object is a float or a subtype of float. return isinstance(obj, float) def is_exact_float(obj): Check if the provided object is exactly a float, not a subtype. return type(obj) is float def create_float_from_string(s): Create a float from a string. try: return float(s) except ValueError: raise ValueError(\\"Invalid string for float conversion\\") def create_float_from_double(d): Create a float from a double. if not isinstance(d, (float, int)): raise TypeError(\\"Input must be a float or an int\\") return float(d) def to_double(pyfloat): Convert a Python float to a double. if not isinstance(pyfloat, float): raise TypeError(\\"Input must be a float object\\") return float(pyfloat) def get_float_info(): Get information on the precision and value limits of float. return { \\"DIG\\": sys.float_info.dig, \\"MANT_DIG\\": sys.float_info.mant_dig, \\"MAX\\": sys.float_info.max, \\"MAX_EXP\\": sys.float_info.max_exp, \\"MIN\\": sys.float_info.min, \\"MIN_EXP\\": sys.float_info.min_exp, \\"EPSILON\\": sys.float_info.epsilon } def max_float(): Get the maximum representable finite float. return sys.float_info.max def min_float(): Get the minimum normalized positive float. return sys.float_info.min"},{"question":"Advanced Data Visualization with Seaborn Objective Demonstrate your understanding of the seaborn library by creating a complex data visualization using the `tips` dataset. Task Using the `tips` dataset, create a scatter plot that visualizes the relationship between `total_bill` and `tip`. The plot should include the following features: 1. Color the points by the `day` of the week using the `hue` parameter. 2. Vary the marker shape by `time` (Lunch/Dinner) using the `style` parameter. 3. Adjust the size of the points based on the `size` column. 4. Make sure to use a legend that includes all unique values for categorical variables. 5. Control the marker size range from 20 to 200. 6. Normalize the `size` variable to a range between 0 and 7. 7. Add an appropriate title and labels for the x and y axes for clarity. Input - No input from the user is required other than ensuring the seaborn and pandas package is installed. Output - The output should be the visualization plot as described. Constraints and Requirements - You must use seaborn\'s `scatterplot` function to create the plot. - Use matplotlib\'s `plt.title` and `plt.xlabel`, `plt.ylabel` for adding the title and labels. - The plot must be clear and visually appealing. Example Here is an example of what your code might resemble. Your task is to fill in the necessary parameters and complete the code. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create the scatter plot sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", sizes=(20, 200), hue_norm=(0, 7), legend=\\"full\\" ) # Add title and labels plt.title(\\"Total Bill vs Tip with Day, Time, and Size Variations\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") # Show the plot plt.show() ``` Notes - Ensure you handle any specific configurations needed to make the plot correct and as descriptive as possible.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_scatter_plot(): # Load the tips dataset tips = sns.load_dataset(\'tips\') # Normalize the `size` variable to a range between 0 and 7 size_min, size_max = tips[\'size\'].min(), tips[\'size\'].max() tips[\'size_normalized\'] = (tips[\'size\'] - size_min) / (size_max - size_min) * 7 # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size_normalized\\", sizes=(20, 200), legend=\\"full\\" ) # Add title and labels plt.title(\\"Total Bill vs Tip with Day, Time, and Size Variations\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") # Display the plot plt.show() # Call the function create_scatter_plot()"},{"question":"**Question: Setting Up a Python Package with `distutils`** You are tasked with creating a Python package named \\"AwesomePackage\\" that has the following structure and contents: ``` AwesomePackage/   setup.py   src/   awesome_subpackage/    __init__.py    module1.py    helpers.c     data/   config.yaml   info.txt   scripts/   awesome_script.py   README.md ``` **Requirements:** 1. The root package directory is `src`. 2. The `awesome_subpackage` contains a module `module1.py` and a C extension `helpers.c`. 3. The package should include additional data files located in `src/data/`. 4. The package should install a script located in `scripts/`. 5. The `setup.py` script should include the following metadata: - Name: `AwesomePackage` - Version: `1.0.0` - Description: `An awesome package with various functionalities.` - Author: `Jane Doe` - Author Email: `jane.doe@example.com` - URL: `https://example.com/AwesomePackage` - Classifiers: - `Development Status :: 5 - Production/Stable` - `Programming Language :: Python :: 3` - License: `MIT` **Task:** 1. Write the complete `setup.py` script to configure this package according to the given structure and requirements. 2. Ensure that the script correctly handles the package directory, includes the C extension, and installs the additional data files and script. Your `setup.py` script should handle all the specified tasks and requirements correctly. **Constraints:** - Python version used is 3.10. - Assume all paths and filenames are as given and valid. - The C compiler is properly configured and available on the system. **Performance Requirement:** - The `setup.py` script should execute without errors and produce the expected installation results when run with `python setup.py install`. **Input Format:** - Write the `setup.py` script as a multi-line string in a single code block. **Output Format:** - A complete and correctly formatted `setup.py` script as a string. Example: ```python setup_script = # Your setup.py content here ``` **Hints:** - Use the correct paths for the source files and packages. - Ensure the metadata fields are correctly set. - Include the necessary import statements for `distutils`.","solution":"setup_script = from setuptools import setup, find_packages, Extension # C extension module helpers_extension = Extension( name=\'awesome_subpackage.helpers\', sources=[\'src/awesome_subpackage/helpers.c\'] ) # Setup function setup( name=\'AwesomePackage\', version=\'1.0.0\', description=\'An awesome package with various functionalities.\', author=\'Jane Doe\', author_email=\'jane.doe@example.com\', url=\'https://example.com/AwesomePackage\', classifiers=[ \'Development Status :: 5 - Production/Stable\', \'Programming Language :: Python :: 3\', ], license=\'MIT\', packages=find_packages(where=\'src\'), package_dir={\'\': \'src\'}, package_data={ \'\': [\'data/**/*\'], }, include_package_data=True, ext_modules=[helpers_extension], scripts=[\'scripts/awesome_script.py\'], )"},{"question":"You are tasked with creating a utility script in Python that dynamically modifies environment variables and verifies the changes by executing a subprocess. This will assess your understanding of environment manipulation and subprocess management using Python\'s `os` module. # Task 1. **Function to Modify Environment Variable:** Write a function `modify_env_var(var_name: str, var_value: str) -> None` that modifies or adds an environment variable. 2. **Function to Verify Environment Variable:** Write a function `verify_env_var(var_name: str) -> str` that returns the value of the specified environment variable. If the variable does not exist, return \'Variable not set\'. 3. **Execution and Verification Script:** Write a script that: - Runs your `modify_env_var` to set a new environment variable. - Verifies the modification using `verify_env_var`. - Uses the `subprocess` module to run a simple command (like `printenv`) in a new subprocess to display the environment variable. - Ensures the modifications are reflected in the subprocess environment. # Implementation Details 1. **Function Input/Output:** - `modify_env_var(var_name: str, var_value: str) -> None`: - `var_name`: The name of the environment variable to modify - `var_value`: The value to set for the environment variable - `verify_env_var(var_name: str) -> str`: - `var_name`: The name of the environment variable to retrieve - Returns the value of the environment variable, or \'Variable not set\' if not found. 2. **Constraints:** - Ensure the script can be executed on both Unix and Windows systems. - Handle any potential errors using appropriate exception handling. # Example ```python import os import subprocess def modify_env_var(var_name: str, var_value: str) -> None: os.environ[var_name] = var_value def verify_env_var(var_name: str) -> str: return os.environ.get(var_name, \'Variable not set\') if __name__ == \\"__main__\\": # Example usage modify_env_var(\'MY_VAR\', \'TEST_VALUE\') print(f\'MY_VAR after modification: {verify_env_var(\\"MY_VAR\\")}\') # Create a subprocess to verify the environment variable result = subprocess.run([\'printenv\', \'MY_VAR\'], capture_output=True, text=True) print(f\'Subprocess output: {result.stdout.strip()}\') ``` # Submission Submit your implementation of `modify_env_var`, `verify_env_var`, and the execution script. Ensure your script is executable in a standard Python environment.","solution":"import os import subprocess def modify_env_var(var_name: str, var_value: str) -> None: Modifies or adds an environment variable. Args: - var_name: str - name of the environment variable - var_value: str - value to set for the environment variable os.environ[var_name] = var_value def verify_env_var(var_name: str) -> str: Verifies the value of the specified environment variable. Args: - var_name: str - name of the environment variable Returns: - str: - the value of the specified environment variable, or \'Variable not set\' if the variable does not exist return os.environ.get(var_name, \'Variable not set\') if __name__ == \\"__main__\\": # Example usage modify_env_var(\'MY_VAR\', \'TEST_VALUE\') print(f\'MY_VAR after modification: {verify_env_var(\\"MY_VAR\\")}\') # Create a subprocess to verify the environment variable result = subprocess.run([\'printenv\', \'MY_VAR\'], capture_output=True, text=True, env=os.environ.copy()) print(f\'Subprocess output: {result.stdout.strip()}\')"},{"question":"# Question: You are tasked with designing a multiprocessing application that simulates managing a highway toll booth. The program will handle multiple toll booths, each processing several cars concurrently. Each toll booth will count the number of cars passing through and the total toll collected. # Requirements: - Implement a class `TollBoothManager` which manages multiple toll booths. `TollBoothManager` will: - Initialize with a specified number of toll booths. - Start each toll booth as a separate process. - Record the number of cars and the total toll collected by each booth. - Implement synchronization to ensure that counts and toll collections are accurately updated across processes. # Specifications: 1. **TollBoothManager Initialization:** - `__init__(self, num_booths: int)`: Initializes the manager with the specified number of toll booths. 2. **Start Method:** - `start(self)`: Starts all toll booth processes. 3. **Car Passing Method:** - `car_passed(self, booth_id: int, toll: float)`: Method to be called whenever a car passes through a specific toll booth. This should update the car count and total toll. 4. **Get Stats Method:** - `get_stats(self) -> List[Dict[str, Union[int, float]]]`: Returns a list of dictionaries, each containing the number of cars and total toll collected for each booth. # Constraints: - Use the `multiprocessing` module to manage the processes. - Ensure synchronization to avoid race conditions, using synchronization primitives like Locks where necessary. - Do not use global variables; all shared state must be appropriately passed and managed. # Example: ```python if __name__ == \'__main__\': manager = TollBoothManager(num_booths=3) manager.start() # Simulate cars passing through the toll booths manager.car_passed(booth_id=0, toll=2.5) manager.car_passed(booth_id=1, toll=3.0) manager.car_passed(booth_id=0, toll=1.5) manager.car_passed(booth_id=2, toll=4.0) time.sleep(1) # Give some time for processes to update stats = manager.get_stats() print(stats) # [{\'cars\': 2, \'toll\': 4.0}, {\'cars\': 1, \'toll\': 3.0}, {\'cars\': 1, \'toll\': 4.0}] ``` Feel free to ask questions if you need further clarifications or additional guidance.","solution":"import multiprocessing from multiprocessing import Process, Manager, Lock from typing import List, Dict, Union class TollBoothManager: def __init__(self, num_booths: int): self.num_booths = num_booths self.manager = Manager() self.booths = self.manager.list([self.manager.dict({\'cars\': 0, \'toll\': 0.0}) for _ in range(num_booths)]) self.locks = [Lock() for _ in range(num_booths)] self.processes = [] def _process_booth(self, booth_id: int): # This function simulates processing at a toll booth pass def start(self): for booth_id in range(self.num_booths): process = Process(target=self._process_booth, args=(booth_id,)) process.start() self.processes.append(process) def car_passed(self, booth_id: int, toll: float): with self.locks[booth_id]: self.booths[booth_id][\'cars\'] += 1 self.booths[booth_id][\'toll\'] += toll def get_stats(self) -> List[Dict[str, Union[int, float]]]: with self.manager: return [{\'cars\': booth[\'cars\'], \'toll\': booth[\'toll\']} for booth in self.booths]"},{"question":"# Base64 Encoding and Decoding Challenge Objective: You are provided with an application that needs to handle various types of encoded data. Your task is to create a Python class `DataEncoder` that leverages the functionalities of the `base64` module to encode and decode data using Base64, URL-safe Base64, and Base85 encodings. The class should also include methods to display encoded data in a readable format by grouping into character blocks. Specifications: 1. **Class Name**: `DataEncoder` 2. **Methods**: - `base64_encode(data: bytes, altchars: bytes = None) -> bytes` - `base64_decode(encoded_data: bytes, altchars: bytes = None, validate: bool = False) -> bytes` - `urlsafe_base64_encode(data: bytes) -> bytes` - `urlsafe_base64_decode(encoded_data: bytes) -> bytes` - `base85_encode(data: bytes) -> bytes` - `base85_decode(encoded_data: bytes) -> bytes` - `format_encoded_data(encoded_data: bytes, block_size: int = 4) -> str` Constraints: - `altchars` (optional) must be a `bytes` object of length 2 to replace `+` and `/` characters in standard Base64 encoding. - `validate` (optional) should be a boolean which, when `True`, will raise an error if the input data contains invalid characters. - `block_size` (optional) should be used in `format_encoded_data` to group the encoded data into blocks of the specified number of characters for readability. Example Usage: ```python encoder = DataEncoder() # Standard Base64 encoding data = b\\"Sample data for encoding\\" encoded_data = encoder.base64_encode(data) print(f\\"Encoded Data: {encoded_data}\\") decoded_data = encoder.base64_decode(encoded_data) print(f\\"Decoded Data: {decoded_data}\\") # URL-safe Base64 encoding url_safe_encoded = encoder.urlsafe_base64_encode(data) print(f\\"URL-Safe Encoded: {url_safe_encoded}\\") url_safe_decoded = encoder.urlsafe_base64_decode(url_safe_encoded) print(f\\"URL-Safe Decoded: {url_safe_decoded}\\") # Base85 encoding base85_encoded = encoder.base85_encode(data) print(f\\"Base85 Encoded: {base85_encoded}\\") base85_decoded = encoder.base85_decode(base85_encoded) print(f\\"Base85 Decoded: {base85_decoded}\\") # Format encoded data into blocks formatted_data = encoder.format_encoded_data(base85_encoded, block_size=5) print(f\\"Formatted Encoded Data: n{formatted_data}\\") ``` Notes: - Ensure that proper exception handling is in place for encoding/decoding errors. - Thoroughly test your methods with different sizes of input data and different values for optional parameters.","solution":"import base64 class DataEncoder: def base64_encode(self, data: bytes, altchars: bytes = None) -> bytes: return base64.b64encode(data, altchars=altchars) def base64_decode(self, encoded_data: bytes, altchars: bytes = None, validate: bool = False) -> bytes: return base64.b64decode(encoded_data, altchars=altchars, validate=validate) def urlsafe_base64_encode(self, data: bytes) -> bytes: return base64.urlsafe_b64encode(data) def urlsafe_base64_decode(self, encoded_data: bytes) -> bytes: return base64.urlsafe_b64decode(encoded_data) def base85_encode(self, data: bytes) -> bytes: return base64.b85encode(data) def base85_decode(self, encoded_data: bytes) -> bytes: return base64.b85decode(encoded_data) def format_encoded_data(self, encoded_data: bytes, block_size: int = 4) -> str: encoded_data_str = encoded_data.decode(\'utf-8\') blocks = [encoded_data_str[i:i+block_size] for i in range(0, len(encoded_data_str), block_size)] return \' \'.join(blocks)"},{"question":"**Coding Assessment Question: Utilizing PyTorch MPS Backend** **Objective:** Demonstrate your understanding of utilizing the PyTorch MPS backend for tensor operations and model computations on MacOS devices. **Description:** Write a function `run_mps_operations` that performs the following tasks: 1. Check if the MPS backend is available on the current machine. 2. If MPS is available, create a 10x10 tensor of random values directly on the MPS device. 3. Perform an element-wise multiplication of this tensor by 3 and return the result. 4. Create a simple linear model (`torch.nn.Linear`) with 10 input features and 5 output features, move it to the MPS device, and run a forward pass with the previously created tensor, returning the output. 5. If MPS is not available, raise an appropriate exception with a message detailing why MPS is not available (consider both build issues and hardware/software requirements). **Function Signature:** ```python import torch def run_mps_operations(): Performs tensor operations and a forward pass of a linear model on the MPS device. Returns: Tuple[torch.Tensor, torch.Tensor]: - The result of element-wise multiplication of a 10x10 tensor of random values by 3 on the MPS device. - The output of the linear model after feeding the aforementioned tensor. Raises: RuntimeError: If MPS is not available on the machine, with an appropriate message. # Your implementation here # Example of the type of output to expect if MPS is available # (The actual values will differ as the tensor is populated with random numbers): # tensor([[ 1.4675, -0.7533, 0.5334, -0.8226, -0.2789, -2.1758, 0.3333, -2.1501, -0.0323, -0.6401], # [ 0.2595, -0.1778, -0.8360, -0.3405, 0.0505, -0.2900, -0.1185, 0.1931, 0.9748, -0.3528], # [ 0.3222, -1.4350, 0.9011, 0.7475, -0.0426, 0.4533, -1.4179, 0.0582, -1.6094, 0.6574], # ... # [ 0.3172, -1.4691, 1.4452, 1.0631, 1.2027, 0.0259, 0.0471, -0.8240, 0.2768, -0.1101], # [ 0.3035, -1.4768, -0.2738, -0.1254, 1.2311, 0.1052, 1.2286, 1.3251, 1.4103, 0.1864]]) # tensor([[ 0.2069, 0.3322, -0.1285, ..., # .... # [ 0.9611, 0.9795, 0.5834, ...]]) ``` **Constraints:** - Use the provided code snippet from the documentation as guidance. - Ensure appropriate error handling for the MPS availability check. - The function should be self-contained and not require additional input. **Hints:** - You can use `torch.randn` to create a tensor with random values. - `torch.backends.mps.is_available()` can be used to check if MPS is available.","solution":"import torch def run_mps_operations(): Performs tensor operations and a forward pass of a linear model on the MPS device. Returns: tuple[torch.Tensor, torch.Tensor]: - The result of element-wise multiplication of a 10x10 tensor of random values by 3 on the MPS device. - The output of the linear model after feeding the aforementioned tensor. Raises: RuntimeError: If MPS is not available on the machine, with an appropriate message. if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS backend is not available. Ensure you have a Mac with Apple Silicon and the appropriate PyTorch built with MPS support.\\") device = torch.device(\\"mps\\") # Create a 10x10 tensor of random values on the MPS device tensor = torch.randn(10, 10, device=device) # Perform element-wise multiplication by 3 multiplied_tensor = tensor * 3 # Create a simple linear model with 10 input features and 5 output features, then move it to MPS device model = torch.nn.Linear(10, 5).to(device) # Perform a forward pass with the previously created tensor output_tensor = model(multiplied_tensor) return multiplied_tensor, output_tensor"},{"question":"Advanced File and Directory Management Objective To assess the student\'s understanding and proficiency in handling file and directory operations using Python\'s standard libraries such as `os`, `shutil`, and `glob`. Problem Statement You are required to write a Python function called `manage_files_and_directories` that will perform a series of operations on files and directories in the given path. The function must accomplish the following tasks: 1. **Create a New Directory**: Create a directory named `\\"backup\\"` in the given path. 2. **Copy Files**: Copy all `.txt` files from the given path to the `\\"backup\\"` directory. 3. **Rename Files**: In the `\\"backup\\"` directory, rename all copied `.txt` files to have a `.bak` extension instead of `.txt`. 4. **Log Operations**: Create a log file named `\\"operations.log\\"` in the `\\"backup\\"` directory that records each operation performed (file copied and file renamed). Function Signature ```python def manage_files_and_directories(path: str) -> None: pass ``` Input - `path` (str): The path where the operations will be performed. Output - None: The function should not return any value. Instead, it should perform the specified operations and create a log file with the operations performed. Constraints - The function should handle exceptions and error cases gracefully. - Assume that `path` is always a valid directory path. - The function should handle cases where there are no `.txt` files gracefully. Example Suppose the directory `example_path` contains the following files: ``` document.txt notes.txt image.png ``` After calling `manage_files_and_directories(\'example_path\')`, the `\\"example_path/backup\\"` directory should contain: ``` document.bak notes.bak operations.log ``` Contents of `operations.log`: ``` Copied: document.txt Renamed: document.txt to document.bak Copied: notes.txt Renamed: notes.txt to notes.bak ``` Hints - Use the `os`, `shutil`, and `glob` modules to perform the operations. - Use `os.path.join` for joining paths to ensure cross-platform compatibility.","solution":"import os import shutil import glob def manage_files_and_directories(path: str) -> None: # Create a backup directory backup_dir = os.path.join(path, \'backup\') os.makedirs(backup_dir, exist_ok=True) operations = [] # List to keep track of operations for the log # Copy all .txt files to the backup directory txt_files = glob.glob(os.path.join(path, \'*.txt\')) for txt_file in txt_files: filename = os.path.basename(txt_file) shutil.copy2(txt_file, backup_dir) operations.append(f\\"Copied: {filename}\\") backup_file_path = os.path.join(backup_dir, filename) # Rename the .txt files to .bak new_filename = filename.replace(\'.txt\', \'.bak\') new_file_path = os.path.join(backup_dir, new_filename) os.rename(backup_file_path, new_file_path) operations.append(f\\"Renamed: {filename} to {new_filename}\\") # Create the operations log file log_file_path = os.path.join(backup_dir, \'operations.log\') with open(log_file_path, \'w\') as log_file: for operation in operations: log_file.write(operation + \'n\')"},{"question":"Seaborn Style Customization and Visualization Objective The goal of this assessment is to evaluate your understanding of seaborn\'s style customization capabilities as well as your ability to use seaborn to create informative visualizations. Task 1. **Style Creation and Application:** - Create a custom style for seaborn plots by modifying certain aesthetic parameters. - Apply this custom style to all subsequent plots. 2. **Data Visualization:** - Using the `tips` dataset from seaborn, create a visualization that shows the relationship between the total bill and tip amounts, differentiated by the time of day (Lunch or Dinner). Detailed Requirements 1. **Function: `create_custom_style`** - **Input:** No input arguments required. - **Output:** Return a dictionary containing the custom style parameters. - **Constraints:** - Modify at least the `axes.facecolor`, `grid.color`, and `grid.linewidth` parameters. - You can add or modify other style parameters as needed. - **Example Output:** ```python { \\"axes.facecolor\\": \\"lightgrey\\", \\"grid.color\\": \\"white\\", \\"grid.linewidth\\": 0.5 } ``` 2. **Function: `apply_style`** - **Input:** A dictionary of style parameters. - **Output:** Applies the style globally using `sns.set_style`. - **Example Usage:** ```python custom_style = create_custom_style() apply_style(custom_style) ``` 3. **Function: `visualize_tips_data`** - **Input:** No input arguments required. - **Output:** Display a seaborn scatter plot with: - `total_bill` on the x-axis. - `tip` on the y-axis. - Points colored by the `time` (Lunch or Dinner). - Use the tips dataset available in seaborn: `sns.load_dataset(\'tips\')`. Example Output Your plot should be visually appealing and clearly show the differentiation between Lunch and Dinner times. Additional Notes - Use the context manager as shown in the documentation snippet to ensure that subsequent plots are created using your custom style. - Be sure to label your axes and include a legend to differentiate between Lunch and Dinner points. Submission Provide the complete code for the three functions: `create_custom_style`, `apply_style`, and `visualize_tips_data`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_style(): Creates and returns a dictionary containing custom style parameters for seaborn. custom_style = { \\"axes.facecolor\\": \\"lightgrey\\", \\"grid.color\\": \\"white\\", \\"grid.linewidth\\": 0.5, \\"axes.edgecolor\\": \\"lightblue\\", \\"grid.linestyle\\": \\"--\\" } return custom_style def apply_style(style_dict): Applies the provided style dictionary globally using seaborn\'s set_style method. sns.set_style(\\"whitegrid\\", style_dict) def visualize_tips_data(): Creates and displays a scatter plot of the tips dataset with total_bill on the x-axis, tip on the y-axis, and points colored by time of day (Lunch or Dinner). # Load the tips dataset tips_data = sns.load_dataset(\'tips\') # Create the plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=tips_data, x=\'total_bill\', y=\'tip\', hue=\'time\', style=\'time\') # Label the axes and add a title scatter_plot.set_xlabel(\\"Total Bill ()\\") scatter_plot.set_ylabel(\\"Tip ()\\") scatter_plot.set_title(\\"Scatter Plot of Tips vs Total Bill by Time of Day\\") # Show the legend and plot plt.legend(title=\'Time of Day\') plt.show()"},{"question":"**Objective**: Demonstrate your understanding of the `sys` module in Python to interact with the interpreter environment, manage program execution, and handle auditing events. **Problem Statement**: You are tasked with creating a small command-line tool that performs the following functions: 1. Print the command-line arguments passed to the script. 2. Print Python interpreter details: version, executable path, and platform. 3. Monitor and log specific auditing events related to file operations using custom audit hooks. **Requirements**: 1. Implement a function `print_arguments()` that prints all command-line arguments. 2. Implement a function `print_python_details()` that prints the following details: - Python version - Path of the Python executable - Platform identifier 3. Implement a class `FileAuditLogger` that will: - Append an audit hook to monitor and log file-related events (like \'open\' and \'os.remove\'). - Print a log message every time one of these events is triggered, including the event name and arguments. 4. Write a `main()` function to demonstrate the functionality: - First, it should call `print_arguments()`. - Then, it should call `print_python_details()`. - Finally, it should instantiate `FileAuditLogger` and perform a few file operations (like creating and deleting a text file) to demonstrate the auditing functionality. **Expected Function and Class Definitions**: ```python import sys def print_arguments(): Print the command-line arguments passed to the script. pass def print_python_details(): Print Python interpreter details: version, executable path, and platform. pass class FileAuditLogger: def __init__(self): Initialize the logger by adding the custom audit hook. pass def audit_hook(self, event, args): Handle file-related audit events by printing a log message. pass def perform_file_operations(self): Perform file operations to demonstrate auditing: create and delete a file. pass def main(): Main function to demonstrate the command-line tool. pass if __name__ == \\"__main__\\": main() ``` **Constraints**: - Use the `sys` module methods and attributes where appropriate. - The audit hook should handle exceptions gracefully. - Ensure that the auditing demonstration in `main()` includes at least two types of file operations. **Sample Execution**: ```bash python your_script.py arg1 arg2 ``` Output: ``` Command-line arguments: [\'your_script.py\', \'arg1\', \'arg2\'] Python Version: 3.X.X Python Executable: /path/to/python Platform: linux Audit log: Event=open, Args=(<args>) Audit log: Event=os.remove, Args=(<args>) ``` By completing this task, you demonstrate your ability to leverage the `sys` module for interpreter interaction, command-line argument management, and auditing.","solution":"import sys import os def print_arguments(): Print the command-line arguments passed to the script. print(\\"Command-line arguments:\\", sys.argv) def print_python_details(): Print Python interpreter details: version, executable path, and platform. print(\\"Python Version:\\", sys.version) print(\\"Python Executable:\\", sys.executable) print(\\"Platform:\\", sys.platform) class FileAuditLogger: def __init__(self): Initialize the logger by adding the custom audit hook. self.add_audit_hook() def add_audit_hook(self): Append an audit hook to monitor and log file-related events. sys.addaudithook(self.audit_hook) def audit_hook(self, event, args): Handle file-related audit events by printing a log message. if event in (\'open\', \'os.remove\'): print(f\\"Audit log: Event={event}, Args={args}\\") def perform_file_operations(self): Perform file operations to demonstrate auditing: create and delete a file. filename = \'test_audit.txt\' # Demonstrating file creation with open(filename, \'w\') as f: f.write(\\"This is a test file for audit logging.\\") # Demonstrating file deletion os.remove(filename) def main(): Main function to demonstrate the command-line tool. print_arguments() print_python_details() audit_logger = FileAuditLogger() audit_logger.perform_file_operations() if __name__ == \\"__main__\\": main()"},{"question":"You are required to implement a function `analyze_numbers(numbers: List[Union[int, float]]) -> Tuple[float, float, Tuple[int, int], float, bool]`. The function should perform various mathematical operations and return a tuple containing the following five elements based on the given list of numbers: 1. **The accurate sum** of all elements in the list using `math.fsum`. 2. **The Euclidean norm** of the elements in the list using `math.hypot`. 3. **A tuple containing the Greatest Common Divisor (GCD)** of all elements and the Least Common Multiple (LCM) using `math.gcd` and `math.lcm` respectively. 4. **The remainder when the product** of all the elements is divided by `math.pi` using the `math.remainder`. 5. **A boolean indicating if the mean** of the list\'s elements is close to zero with a relative tolerance of `1e-9` using `math.isclose`. **Function Signature:** ```python from typing import List, Union, Tuple import math def analyze_numbers(numbers: List[Union[int, float]]) -> Tuple[float, float, Tuple[int, int], float, bool]: ... ``` **Input:** - `numbers`: A list containing integers and/or floating-point numbers. The list can have a minimum length of 1 and a maximum length of 1000 elements. **Output:** - Returns a tuple containing: - A float representing the accurate sum of the numbers. - A float representing the Euclidean norm of the numbers. - A tuple containing two integers representing the GCD and LCM of the list. - A float representing the remainder of the product of elements divided by `math.pi`. - A boolean indicating if the mean of the list\'s elements is close to zero with a relative tolerance of `1e-9`. **Constraints:** 1. All elements in `numbers` are either integers or floating-point numbers. 2. The list `numbers` has at least one element and utmost 1000 elements. 3. For the LCM calculation, assume non-negative integer inputs to the function `math.lcm` as this function does not support floating-point numbers directly. **Examples:** ```python from math import isclose # Example 1 numbers = [5, 10, 20] result = analyze_numbers(numbers) # should return: # ( # 35.0, # Accurate sum # 22.9128784747792, # Euclidean norm # (5, 20), # (GCD, LCM) # 2.9828887754393325, # Remainder of the product divided by math.pi # False # Mean is not close to zero # ) # Example 2 numbers = [-1, 1, -1, 1] result = analyze_numbers(numbers) # should return: # ( # 0.0, # Accurate sum # 2.0, # Euclidean norm # (1, 1), # (GCD, LCM) # 0.3183098861837907, # Remainder of the product divided by math.pi # True # Mean is close to zero # ) ``` Implement the `analyze_numbers` function to meet the above requirements. Ensure to handle any exceptional cases and edge conditions as mentioned in the constraints.","solution":"from typing import List, Union, Tuple import math from functools import reduce import operator def gcd(a, b): while b: a, b = b, a % b return abs(a) def lcm(a, b): return abs(a * b) // gcd(a, b) if a and b else 0 def analyze_numbers(numbers: List[Union[int, float]]) -> Tuple[float, float, Tuple[int, int], float, bool]: sum_acc = math.fsum(numbers) euclidean_norm = math.hypot(*numbers) gcd_val = reduce(gcd, map(int, numbers)) # Ensuring the gcd is computed on integer representations lcm_val = reduce(lcm, map(int, numbers)) # Ensuring the lcm is computed on integer representations product = reduce(operator.mul, numbers, 1.0) remainder_with_pi = math.remainder(product, math.pi) mean_is_close_to_zero = math.isclose(sum_acc / len(numbers), 0, rel_tol=1e-9) return (sum_acc, euclidean_norm, (gcd_val, lcm_val), remainder_with_pi, mean_is_close_to_zero)"},{"question":"Given your knowledge of PyTorch and the `torch.export` programming model, you are tasked with writing a function that demonstrates your understanding of static and dynamic values, control flow, and symbolic shape propagation. Specifically, you will: # Export a Model with Control Flow and Dynamic Input Shapes **Requirements**: 1. Define a PyTorch `nn.Module` class that: - Accepts two tensor inputs. - Contains control flow (e.g., an if-statement) that considers the sum of one input tensor. - Returns different computations based on the control flow, ensuring both branches cover PyTorch operations. 2. Export the model using `torch.export.export` in non-strict mode with both static and dynamic shapes. 3. Demonstrate the ability to handle dynamic shape propagation by ensuring the model can handle varying shapes for one of the inputs. # Instructions 1. Define a class `MyModel` that inherits from `torch.nn.Module`. Implement a `forward` method that: - Accepts two inputs: tensor `x` and tensor `y`. - Computes the sum of `x`. If the sum is greater than 0, performs an addition of `x` and `y`, otherwise performs a subtraction. 2. Use `torch.export.export` to export the model: - Ensure that the shape of the second input `y` is treated as dynamic. 3. Write a script to validate the exported model with different shapes for input `y`. # Input Format - `MyModel` class should inherit from `torch.nn.Module`. - The `forward` method of `MyModel` should handle two tensor inputs `x` and `y`. - `torch.export.export` function should be used to export the model and ensure dynamic input shape for `y`. # Output Format - Exported model graph is printed showing different branches of the control flow based on static and dynamic conditions. - Demonstration script should show the model working with varying shapes for the input `y`. # Constraints 1. Use non-strict mode for exporting. 2. Handle dynamic shapes only for the second input tensor `y`. # Function Signature ```python import torch class MyModel(torch.nn.Module): def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: # Implement the required control flow and computations pass def export_my_model() -> None: # Create a sample input and export the model model = MyModel() x = torch.randn(3, 3) y = torch.randn(3, 3) exported_model = torch.export.export(model, (x, y), dynamic_shapes={\'y\': True}) print(exported_model.graph_module.code) # Demonstrate model export for varying shapes y_new = torch.randn(4, 3) output = exported_model(x, y_new) print(output) # Call the export function to perform the export and demonstration export_my_model() ``` In this question, students are expected to utilize their understanding of PyTorch, particularly the `torch.export` module, to create and export a model while demonstrating control flow and dynamic shape handling. The provided function signatures guide the implementation while expecting creative problem-solving for the dynamic input shapes and control flow within the model.","solution":"import torch import torch.nn as nn class MyModel(nn.Module): def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: if x.sum() > 0: return x + y else: return x - y def export_my_model() -> None: # Create a sample model instance model = MyModel() # Example input tensors x = torch.randn(3, 3) y = torch.randn(3, 3) # Export the model while treating the second input (y) shape as dynamic try: traced_model = torch.jit.trace(model, (x, y)) traced_model.save(\\"my_model.pt\\") print(traced_model.graph) except Exception as e: print(f\\"Error occurred while exporting: {e}\\") # Demonstrate the model handling dynamic shapes for the input `y` y_new = torch.randn(4, 3) try: loaded_model = torch.jit.load(\\"my_model.pt\\") output = loaded_model(x, y_new) print(output) except Exception as e: print(f\\"Error occurred during inference with dynamic shape: {e}\\") # Call the export function to perform the export and demonstrate the model handling dynamic shapes export_my_model()"},{"question":"# Advanced Coding Challenge: Manipulating Code Objects in Python Objective Create a function that inspects a Python function to analyze its underlying code object and report various attributes. This will test your understanding of Python\'s internal workings and your ability to interact with low-level code objects. Task Write a Python function `analyze_function_code` that accepts a Python function as its parameter and returns a dictionary containing the following information about the function\'s code object: - `number_of_free_vars`: Number of free variables in the function. - `number_of_locals`: Number of local variables in the function. - `stacksize`: Needed stack size. - `filename`: Filename from which the function was compiled. - `first_lineno`: The first line number of the function. - `const_names`: List of all constant names used in the function. Input - A Python function. Output - A dictionary with the specified attributes. Example ```python def sample_function(x): y = 10 def inner_function(z): return x + y + z return inner_function(5) result = analyze_function_code(sample_function) print(result) ``` Expected output (values may vary based on implementation and environment): ```python { \'number_of_free_vars\': 1, \'number_of_locals\': 2, \'stacksize\': 5, \'filename\': \'example.py\', \'first_lineno\': 1, \'const_names\': [\'x\', \'y\', \'inner_function\', 10, 5] } ``` Constraints - Assume that the input will always be a valid Python function. Notes - Use the `__code__` attribute of the function object to access its code object. - Consider using the `co_*` attributes of the code object to retrieve the relevant information. Write your solution by implementing the `analyze_function_code` function described above.","solution":"def analyze_function_code(func): Analyzes the code object of the given function and returns specific attributes. Parameters: func (function): The function to analyze. Returns: dict: A dictionary containing attributes of the code object. code = func.__code__ return { \'number_of_free_vars\': code.co_freevars and len(code.co_freevars) or 0, \'number_of_locals\': code.co_nlocals, \'stacksize\': code.co_stacksize, \'filename\': code.co_filename, \'first_lineno\': code.co_firstlineno, \'const_names\': list(code.co_consts) }"},{"question":"**Advanced Seaborn Visualization Task** # Objective: You are tasked with creating a complex plot using the `penguins` dataset from the `seaborn` library. Your plot should visualize the relationship between the `species` of penguins and their `flipper_length_mm`, while demonstrating your ability to layer multiple marks and transformations effectively. # Instructions: 1. Load the `penguins` dataset from the `seaborn` library. 2. Use the `seaborn.objects.Plot` class to create a plot with the following specifications: - The x-axis should represent the `species`. - The y-axis should represent the `flipper_length_mm`. 3. Add `Dots` to the plot for each data point, applying `Jitter` to avoid overlap. 4. Overlay a `Range` element that displays the interquartile range (25th to 75th percentile) of `flipper_length_mm` for each species, offsetting the range horizontally by 0.2 units. # Detailed Requirements: - You should use the `so.Plot()` method to create the plot. - Apply the `so.Dots()` mark combined with the `so.Jitter()` transformation to ensure dots do not overlap excessively. - Add a `so.Range()` mark to show the interquartile range, shifted horizontally with `so.Shift(x=0.2)`. # Example Structure: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot with the specified requirements plot = ( so.Plot(penguins, \\"species\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Display the plot plot.show() ``` # Evaluation Criteria: - Correct usage of `seaborn.objects.Plot` and associated methods (`add()`, `Dots`, `Jitter`, `Range`, `Shift`). - Proper application of transformations to ensure clear and informative visualization. - Clarity and accuracy in coding style and syntax. # Constraints: - Ensure Seaborn and other necessary libraries (like Matplotlib) are installed in your environment. - Your code should be performant and run within a reasonable time frame. Develop your solution to demonstrate a thorough understanding of advanced Seaborn plotting techniques.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_advanced_seaborn_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot with the specified requirements plot = ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Display the plot plot.show()"},{"question":"**Objective:** You are required to write a Python program using the `sunau` module to modify the sample rate of an existing Sun AU audio file. Specifically, you will: 1. Read an input AU file. 2. Adjust the audio data\'s sample rate to a new value. 3. Write the adjusted audio data to a new output AU file. Implement the function `modify_sample_rate(input_file: str, output_file: str, new_sample_rate: int) -> None` that performs the above tasks. **Input:** - `input_file` (str): Path to the input AU file. - `output_file` (str): Path to the output AU file where the modified audio will be saved. - `new_sample_rate` (int): The desired sample rate to set for the audio data. **Output:** - The function does not return any value. It writes the modified audio data to the specified `output_file`. **Constraints:** - The program should handle mono and stereo AU files. - The program should preserve other audio parameters (such as number of channels, sample width) while modifying the sample rate. - You can assume that the input AU file is properly formatted and accessible. **Example:** ```python def modify_sample_rate(input_file: str, output_file: str, new_sample_rate: int) -> None: pass # Example usage: modify_sample_rate(\'input.au\', \'output.au\', 44100) ``` **Detailed Steps:** 1. Open the input file in read mode using `sunau.open`. 2. Retrieve the file\'s audio parameters. 3. Read the audio frames from the input file. 4. Open the output file in write mode using `sunau.open`. 5. Set the desired parameters (including the new sample rate) on the output file. 6. Write the audio frames to the output file. 7. Close both input and output files properly. **Note:** - Ensure proper handling of file operations and closing of files. - The function should be efficient and minimize unnecessary data processing.","solution":"import sunau def modify_sample_rate(input_file: str, output_file: str, new_sample_rate: int) -> None: Modifies the sample rate of an AU audio file. Parameters: - input_file (str): Path to the input AU file. - output_file (str): Path to the output AU file. - new_sample_rate (int): The desired sample rate to set for the audio data. # Open the input AU file in read mode with sunau.open(input_file, \'r\') as input_au: # Retrieve the audio parameters from the input file n_channels = input_au.getnchannels() sampwidth = input_au.getsampwidth() n_frames = input_au.getnframes() comp_type = input_au.getcomptype() comp_name = input_au.getcompname() # Read the audio frames from the input file audio_frames = input_au.readframes(n_frames) # Open the output AU file in write mode with sunau.open(output_file, \'w\') as output_au: # Set the desired parameters, including the new sample rate, on the output file output_au.setnchannels(n_channels) output_au.setsampwidth(sampwidth) output_au.setframerate(new_sample_rate) output_au.setcomptype(comp_type, comp_name) # Write the audio frames to the output file output_au.writeframes(audio_frames)"},{"question":"**Objective**: Demonstrate your understanding of the `venv` module by creating a script that sets up a virtual environment and includes custom packages. **Instructions**: 1. Implement a Python class `CustomEnvBuilder` that extends `venv.EnvBuilder`. 2. This class should: - Automatically install `setuptools` and `pip` when creating the virtual environment. - Accept an additional argument `custom_packages` which is a list of package names that should be installed into the virtual environment after its creation. 3. Implement the following methods: - `__init__(self, custom_packages=None, *args, **kwargs)`: Initialize the builder with a list of custom packages. - `post_setup(self, context)`: After setting up the environment, install `setuptools`, `pip`, and the custom packages provided. - `install_packages(self, context, packages)`: A helper method to install a list of packages using `pip`. **Constraints**: - Python 3.6 or higher is required. - The script should handle invalid packages by printing an appropriate error message. **Expected Input and Output**: - The script should be able to be run as follows: ```python builder = CustomEnvBuilder(custom_packages=[\'requests\', \'numpy\']) builder.create(\'/path/to/new/virtual/environment\') ``` - The script should print the progress of package installation. **Skeleton Code to Get You Started**: ```python import venv import os import subprocess class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, custom_packages=None, *args, **kwargs): self.custom_packages = custom_packages or [] super().__init__(with_pip=True, *args, **kwargs) def post_setup(self, context): self.install_setuptools(context) self.install_custom_packages(context) def install_setuptools(self, context): url = \'https://bootstrap.pypa.io/ez_setup.py\' self.install_script(context, \'setuptools\', url) def install_custom_packages(self, context): self.install_packages(context, self.custom_packages) def install_script(self, context, name, url): from urllib.request import urlretrieve from urllib.parse import urlparse binpath = context.bin_path _, _, path, _, _, _ = urlparse(url) script_name = os.path.basename(path) script_path = os.path.join(binpath, script_name) urlretrieve(url, script_path) subprocess.call([context.env_exe, script_path]) os.unlink(script_path) def install_packages(self, context, packages): for package in packages: try: subprocess.check_call([context.env_exe, \'-m\', \'pip\', \'install\', package]) except subprocess.CalledProcessError: print(f\\"Failed to install package: {package}\\") # Example usage if __name__ == \'__main__\': builder = CustomEnvBuilder(custom_packages=[\'requests\', \'numpy\']) builder.create(\'/path/to/new/virtual/environment\') ```","solution":"import venv import subprocess class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, custom_packages=None, *args, **kwargs): self.custom_packages = custom_packages or [] super().__init__(with_pip=True, *args, **kwargs) def post_setup(self, context): self.install_packages(context, [\\"setuptools\\", \\"pip\\"]) self.install_packages(context, self.custom_packages) def install_packages(self, context, packages): for package in packages: try: print(f\\"Installing package: {package}\\") subprocess.check_call([context.env_exe, \'-m\', \'pip\', \'install\', package]) except subprocess.CalledProcessError: print(f\\"Failed to install package: {package}\\") # Example usage if __name__ == \'__main__\': builder = CustomEnvBuilder(custom_packages=[\'requests\', \'numpy\']) builder.create(\'/path/to/new/virtual/environment\')"},{"question":"# Advanced Python Coding Assessment: Custom Warning Handler Objective In this assessment, you are required to demonstrate your understanding of the `warnings` module in Python by implementing a custom warning handler that captures specific types of warnings and processes them in a specified manner. Task Write a function `custom_warning_handler` that takes an action configuration, a list of warning actions to set up, and a list of functions that each trigger a warning. The function should: 1. Set up the warning filters based on the provided configurations. 2. Execute each function in the list and capture any warnings that are issued. 3. Return a list of captured warnings. Function Signature ```python def custom_warning_handler(action_config: list, funcs: list) -> list: pass ``` Input - `action_config` (list of tuples): Each tuple contains: - `action` (str): The action to be taken for matching warnings (`\\"default\\"`, `\\"error\\"`, `\\"ignore\\"`, `\\"always\\"`, `\\"module\\"`, `\\"once\\"`). - `message` (str): A regular expression that the warning message must match. - `category` (Warning): The warning category to be matched. - `module` (str): A regular expression that the fully qualified module name must match. - `lineno` (int): The line number where the warning occurred (zero matches all line numbers). - `funcs` (list of function objects): Each function in the list, when called, may trigger warnings. Output - `captured_warnings` (list of warnings): A list of captured warnings, where each warning is represented as a tuple: - `message` (str): The warning message. - `category` (Warning): The warning category. - `filename` (str): The name of the file where the warning occurred. - `lineno` (int): The line number where the warning occurred. Constraints - Ensure that the warning filters set by `action_config` are applied correctly and in the order provided. - Any custom logic applied to the warnings should respect the standard behavior of the `warnings` module. - Handle both default and explicit warning issues in the given functions. Example Usage ```python import warnings def deprecated_func(): warnings.warn(\\"Deprecated function\\", DeprecationWarning) def runtime_issue_func(): warnings.warn(\\"Runtime issue\\", RuntimeWarning) # Action configuration - capture all runtime warnings and deprecation warnings action_config = [ (\\"always\\", \\"\\", RuntimeWarning, \\"\\", 0), (\\"always\\", \\"\\", DeprecationWarning, \\"\\", 0) ] # Functions to invoke that trigger warnings funcs = [deprecated_func, runtime_issue_func] # Call the custom warning handler captured_warnings = custom_warning_handler(action_config, funcs) # Example output: # captured_warnings == [ # (\\"Deprecated function\\", DeprecationWarning, \\"example.py\\", 5), # (\\"Runtime issue\\", RuntimeWarning, \\"example.py\\", 9) # ] ``` Use the provided input and output formats to implement the function `custom_warning_handler`. Ensure the function sets up the warning filters correctly and captures the warnings issued by the given functions. Notes - You can use the `warnings.catch_warnings` context manager to capture warnings within the function. - Make sure to reset warning filters to their default state at the beginning and the end of your function to avoid side effects.","solution":"import warnings def custom_warning_handler(action_config: list, funcs: list) -> list: # List to store captured warnings captured_warnings = [] # Function to capture warnings def warning_handler(message, category, filename, lineno, file=None, line=None): captured_warnings.append((str(message), category, filename, lineno)) # Use the catch_warnings context manager with warnings.catch_warnings(record=True) as caught_warnings: warnings.simplefilter(\\"always\\") for action, message, category, module, lineno in action_config: warnings.filterwarnings(action, message, category, module, lineno) # Execute each function and capture warnings for func in funcs: func() for warning in caught_warnings: warning_handler(warning.message, warning.category, warning.filename, warning.lineno) # Return the list of captured warnings return captured_warnings"},{"question":"Objective Implement a subclass of `zipimport.zipimporter` that enhances the functionality of the zip importer to include features such as retrieving the list of modules within a package and checking if a specified module exists within the ZIP archive. Background The `zipimport` module allows importing Python modules and packages from ZIP-format archives. It provides various methods to interact with and load these modules, but it currently lacks a method to list all available modules within a specific package or to check the existence of a module within a given ZIP file. Task 1. **Create a subclass `EnhancedZipImporter`** that inherits from `zipimport.zipimporter`. 2. **Implement the following additional methods:** - `list_modules(self, package_name: str) -> list`: - This method should return a list of module names within the specified package inside the ZIP archive. - **Input:** `package_name` (string) - The name of the package within the ZIP archive. - **Output:** List of module names (strings) within the specified package. - **Example:** ```python importer = EnhancedZipImporter(\'example.zip\') print(importer.list_modules(\'some_package\')) # Output: [\'module1\', \'module2\', ...] ``` - `module_exists(self, module_name: str) -> bool`: - This method should check if the specified module exists within the ZIP archive. - **Input:** `module_name` (string) - The fully qualified name of the module. - **Output:** `True` if the module exists, `False` otherwise. - **Example:** ```python importer = EnhancedZipImporter(\'example.zip\') print(importer.module_exists(\'some_package.module\')) # Output: True or False ``` Constraints - You may assume that the ZIP file is always valid and readable. - The package names and module names are well-formed Python identifiers. - The methods should be efficient in terms of performance and not involve unnecessarily repeated operations. Example Usage Consider the following ZIP archive structure: ``` example.zip  some_package/   __init__.py   module1.py   module2.py  another_package/  __init__.py  module3.py  sub_package/  __init__.py  module4.py ``` The expected behavior of your additional methods would be: ```python importer = EnhancedZipImporter(\'example.zip\') # List modules in \'some_package\' modules = importer.list_modules(\'some_package\') print(modules) # Output: [\'__init__\', \'module1\', \'module2\'] # Check if \'another_package.module3\' exists exists = importer.module_exists(\'another_package.module3\') print(exists) # Output: True # Check if \'some_package.non_existing_module\' exists exists = importer.module_exists(\'some_package.non_existing_module\') print(exists) # Output: False ``` Performance Requirements - These methods should perform efficiently, leveraging internal structures where possible to minimize the need for repeated file operations. Class Definition ```python import zipimport class EnhancedZipImporter(zipimport.zipimporter): def list_modules(self, package_name: str) -> list: # TODO: Implement the method to list all modules within the given package. pass def module_exists(self, module_name: str) -> bool: # TODO: Implement the method to check if a given module exists within the ZIP file. pass # Example usage of the implemented methods (uncomment and run in actual implementation) # importer = EnhancedZipImporter(\'example.zip\') # print(importer.list_modules(\'some_package\')) # print(importer.module_exists(\'another_package.module3\')) ``` **Note:** Ensure that your implementation adheres to the provided constraints and performance requirements.","solution":"import zipimport import os class EnhancedZipImporter(zipimport.zipimporter): def list_modules(self, package_name: str) -> list: Lists all modules within the given package inside the ZIP archive. Args: package_name (str): The name of the package within the ZIP archive. Returns: list: List of module names within the specified package. prefix = package_name.replace(\'.\', \'/\') + \'/\' files_in_package = [f[len(prefix):-3] for f in self._files if f.startswith(prefix) and f.endswith(\'.py\')] return files_in_package def module_exists(self, module_name: str) -> bool: Checks if a given module exists within the ZIP archive. Args: module_name (str): The fully qualified name of the module. Returns: bool: True if the module exists, False otherwise. module_path = module_name.replace(\'.\', \'/\') + \'.py\' return module_path in self._files"},{"question":"**Question: Complex Arithmetic and Logical Operations** **Objective:** Demonstrate your understanding of the \\"operator\\" module by implementing a function that performs a series of arithmetic and logical operations on two input lists based on specified operations. **Problem Statement:** Write a Python function `complex_operations(list1, list2, operations)` that takes three parameters: 1. `list1` (List[int]): A list of integers. 2. `list2` (List[int]): Another list of integers of the same length as `list1`. 3. `operations` (List[str]): A list of strings representing the operations to be performed. The operations will be specified using the names of functions from the \\"operator\\" module. The function should return a new list where each element is the result of applying the corresponding operation from the `operations` list to the elements at the same index in `list1` and `list2`. The operations should be applied in the given order to the elements. **Constraints:** 1. `list1` and `list2` will have the same length. 2. `operations` will only contain valid operation strings from the \\"operator\\" module. 3. Each element in `list1` and `list2` will be an integer. 4. The length of `operations` will be less than or equal to the length of `list1`. **Input:** - `list1 = [1, 2, 3, 4, 5]` - `list2 = [10, 20, 30, 40, 50]` - `operations = [\\"add\\", \\"mul\\", \\"sub\\", \\"floordiv\\", \\"mod\\"]` **Output:** - `[11, 40, -27, 0, 5]` **Explanation:** 1. `add(1, 10) = 11` 2. `mul(2, 20) = 40` 3. `sub(3, 30) = -27` 4. `floordiv(4, 40) = 0` 5. `mod(5, 50) = 5` **Function Signature:** ```python import operator def complex_operations(list1, list2, operations): pass ``` **Requirements:** 1. Use the \\"operator\\" module to perform the operations. 2. Ensure that your solution is efficient and handles edge cases gracefully. **Example:** ```python import operator def complex_operations(list1, list2, operations): result = [] for i, op in enumerate(operations): func = getattr(operator, op) result.append(func(list1[i], list2[i])) return result # Example usage list1 = [1, 2, 3, 4, 5] list2 = [10, 20, 30, 40, 50] operations = [\\"add\\", \\"mul\\", \\"sub\\", \\"floordiv\\", \\"mod\\"] print(complex_operations(list1, list2, operations)) # Output: [11, 40, -27, 0, 5] ```","solution":"import operator def complex_operations(list1, list2, operations): result = [] for i, op in enumerate(operations): func = getattr(operator, op) result.append(func(list1[i], list2[i])) return result"},{"question":"# Question: You are required to analyze the relationship and clustering among a set of features in the well-known Iris dataset using seaborn\'s clustermap functionality. Implement a function called `plot_iris_clustermap` that performs the following tasks: 1. **Load the Iris dataset** from seaborn\'s dataset collection. 2. **Cluster the data** while excluding the `species` column. The result should display clusters of the iris samples based on the feature values. 3. The heatmap should have: - Custom figure size of (10, 8). - Row clustering disabled. - Specific dendrogram ratios `(0.1, 0.3)`. - Color bar positioned at `(0.2, 0.8, 0.02, .18)`. - Colored labels to identify species, with `\'setosa\'` as red, `\'versicolor\'` as blue, and `\'virginica\'` as green. - `Standard scaling` applied to columns. - Custom colormap `\'magma\'` with value limits between 0 and 8. Constraints: - You cannot alter the imported seaborn or scipy package for performing these tasks. - Ensure the heatmap is self-explanatory through appropriate customization and colors. Performance Requirements: - Given the small size of the Iris dataset, the performance requirement is not stringent. However, the function should execute within a reasonable time frame (a few seconds at most). Input and Output: - The function should not take any parameters. - The function should display the heatmap directly and does not need to return any values. ```python import seaborn as sns def plot_iris_clustermap(): # Your code here ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_iris_clustermap(): # 1. Load the Iris dataset. iris = sns.load_dataset(\\"iris\\") # 2. Cluster the data while excluding the \'species\' column. # Create a new DataFrame with cluster labels for the species. species = iris.pop(\'species\') lut = dict(zip(species.unique(), [\\"r\\", \\"b\\", \\"g\\"])) row_colors = species.map(lut) # 3. Create the clustermap with the following specifications. g = sns.clustermap(iris, figsize=(10, 8), row_cluster=False, dendrogram_ratio=(.1, .2), cbar_pos=(0.2, 0.8, 0.02, .18), col_colors=row_colors, standard_scale=1, cmap=\'magma\', vmin=0, vmax=8) # Add a color bar legend for the species. for label in lut: plt.plot([], [], color=lut[label], label=label) plt.legend(loc=\'upper right\') plt.show()"},{"question":"# PyTorch and TorchScript Coding Assessment **Objective**: Write a function using PyTorch, convert it to TorchScript, and verify its correctness. **Problem Statement**: You are given a task to implement a function in PyTorch that performs a specific operation on tensors and then convert this function to TorchScript. This will ensure you understand both writing custom functions in PyTorch and utilizing TorchScript for optimizing and serializing models. **Requirements**: 1. Implement a PyTorch function `custom_relu` that performs a custom ReLU operation on an input tensor. Instead of just setting negative values to zero, the function should set any value less than a threshold `t` to zero. The threshold `t` should be a parameter of the function. 2. Convert this function to TorchScript. 3. Write a test function to validate the correctness of the TorchScript function. **Function Signatures**: ```python def custom_relu(x: torch.Tensor, t: float) -> torch.Tensor: Apply a modified ReLU operation on the input tensor `x`. Parameters: x (torch.Tensor): Input tensor on which the operation will be applied. t (float): Threshold value. Values in the tensor less than t will be set to zero. Returns: torch.Tensor: A tensor with the modified ReLU operation applied. pass def convert_to_torchscript(fn: Callable) -> torch.jit.ScriptFunction: Convert a PyTorch function to TorchScript. Parameters: fn (Callable): The PyTorch function to be converted. Returns: torch.jit.ScriptFunction: The converted TorchScript function. pass def test_custom_relu(): Validate the correctness of the TorchScript `custom_relu` function. Should print \'Test passed\' if correct, other output if failed. pass ``` **Input Format**: * `custom_relu` receives a PyTorch tensor `x` of shape `(N, )` where `N` can be any integer, and a float `t` as threshold. **Output Format**: * `custom_relu` returns a tensor of the same shape as input tensor `x` with the custom ReLU operation applied. * `convert_to_torchscript` returns a TorchScript function version of the provided function. * `test_custom_relu` prints \'Test passed\' if the custom ReLU TorchScript function is correct, otherwise indicates a failure. **Constraints**: * You should only use functions and methods available in PyTorch and TorchScript. * Ensure that the TorchScript function is equivalent in behavior to the original PyTorch function. * Account for edge cases, such as when the input tensor has no elements or all elements are below the threshold. **Example**: ```python import torch # Example tensor x = torch.tensor([0.5, 2.0, -1.0, 3.5, 0.0, -0.5]) t = 1.0 # Applying custom ReLU result = custom_relu(x, t) print(result) # Expected output tensor([0.0, 2.0, 0.0, 3.5, 0.0, 0.0]) # Convert to TorchScript and validate script_fn = convert_to_torchscript(custom_relu) test_custom_relu(script_fn) ``` # Note: Ensure your code is well-documented and handles edge cases efficiently.","solution":"import torch from typing import Callable def custom_relu(x: torch.Tensor, t: float) -> torch.Tensor: Apply a modified ReLU operation on the input tensor `x`. Parameters: x (torch.Tensor): Input tensor on which the operation will be applied. t (float): Threshold value. Values in the tensor less than t will be set to zero. Returns: torch.Tensor: A tensor with the modified ReLU operation applied. return torch.where(x >= t, x, torch.tensor(0.0, dtype=x.dtype)) def convert_to_torchscript(fn: Callable) -> torch.jit.ScriptFunction: Convert a PyTorch function to TorchScript. Parameters: fn (Callable): The PyTorch function to be converted. Returns: torch.jit.ScriptFunction: The converted TorchScript function. return torch.jit.script(fn) def test_custom_relu(): Validate the correctness of the TorchScript `custom_relu` function. Should print \'Test passed\' if correct, other output if failed. # Example tensor x = torch.tensor([0.5, 2.0, -1.0, 3.5, 0.0, -0.5]) t = 1.0 # Expected output expected = torch.tensor([0.0, 2.0, 0.0, 3.5, 0.0, 0.0]) # Convert to torchscript script_custom_relu = convert_to_torchscript(custom_relu) # Getting the result of torchscript function result = script_custom_relu(x, t) assert torch.equal(expected, result), f\\"Expected {expected}, but got {result}\\" print(\\"Test passed\\")"},{"question":"# Advanced List Manipulations and Operations Problem Statement You are required to implement a function `list_operations` which performs a series of manipulations and operations on a list of integers. The function should take a list of integers as input and perform the following tasks in order: 1. Append the integer `100` to the end of the list. 2. Extend the list with another list of integers `[200, 300, 400]`. 3. Insert the integer `50` at the beginning of the list. 4. Remove the first occurrence of the integer `200` from the list. 5. Pop the last element from the list and store it in a variable `popped_element`. 6. Clear all elements from the list. 7. Return a tuple containing: - The list after all the above operations (after clearing). - The `popped_element` value. Constraints - The input list will contain at least one element. - The list will only contain integers. - When removing elements, assume that the element to be removed will be present in the list. Examples ```python list_operations([1, 2, 3, 4, 5]) # Returns: ([], 400) list_operations([10, 20, 30]) # Returns: ([], 400) list_operations([200, 300, 400, 500]) # Returns: ([], 400) ``` Function Signature ```python def list_operations(nums: list) -> tuple: pass ``` Implementation Guidelines - Use the `append` method to add elements to the list. - Use the `extend` method to concatenate lists. - Use the `insert` method to place an element at a specific index. - Use the `remove` method to delete the first occurrence of an element. - Use the `pop` method to remove and return the last element. - Use the `clear` method to empty the list. Note: You should not use any external libraries or helper functions. All operations should be performed using the built-in list methods as described.","solution":"def list_operations(nums: list) -> tuple: Perform a series of manipulations and operations on the input list of integers. # Append 100 to the end of the list nums.append(100) # Extend the list with [200, 300, 400] nums.extend([200, 300, 400]) # Insert 50 at the beginning of the list nums.insert(0, 50) # Remove the first occurrence of 200 nums.remove(200) # Pop the last element and store it in a variable popped_element = nums.pop() # Clear all elements from the list nums.clear() # Return the final list and the popped element as a tuple return (nums, popped_element)"},{"question":"Objective: Create a function that demonstrates the usage of seaborn\'s `plotting_context` to produce different styles of plots, and visualize a dataset. This task will assess your ability to understand and apply seaborn\'s plotting context functionality as well as your skills in creating informative visualizations. Task: You are provided with a dataset of your choice (it can be a simple dictionary for testing). Implement a function that plots two visualizations of the data using seaborn. One visualization should use the default plotting context, and the other should use the \\"talk\\" context. Requirements: 1. Create a function `plot_with_contexts(data)`. 2. `data`: A dictionary where keys are categories (x-axis) and values are their corresponding numerical values (y-axis). 3. Plot the data twice: - Once with the default seaborn context. - Once with the \\"talk\\" context. 4. Display both plots on a single figure with two subplots, one above the other for easy comparison. Input: - A dictionary `data` that the function will use to create the plots. Example: ```python data = {\\"A\\": 10, \\"B\\": 15, \\"C\\": 7, \\"D\\": 10, \\"E\\": 5} ``` Output: - Display a figure with two subplots, where: - The first subplot uses the default plotting context. - The second subplot uses the \\"talk\\" plotting context. Constraints: - You may use other seaborn features to enhance the plots, but the key focus should be on demonstrating the effect of different plotting contexts. Example Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_with_contexts(data: dict): # Implementation here pass # Example usage data = {\\"A\\": 10, \\"B\\": 15, \\"C\\": 7, \\"D\\": 10, \\"E\\": 5} plot_with_contexts(data) ``` When executed, this function should display a figure with two subplots: one in the default context and one in the \\"talk\\" context.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_with_contexts(data: dict): Plots the given data in two seaborn contexts: default and \'talk\'. Parameters: data (dict): A dictionary where keys are categories (x-axis) and values are their corresponding numerical values (y-axis). categories = list(data.keys()) values = list(data.values()) fig, axes = plt.subplots(2, 1, figsize=(10, 8)) # Default context sns.set_context(\'notebook\') sns.barplot(x=categories, y=values, ax=axes[0]) axes[0].set_title(\'Default Context\') # \\"Talk\\" context sns.set_context(\'talk\') sns.barplot(x=categories, y=values, ax=axes[1]) axes[1].set_title(\'\\"Talk\\" Context\') plt.tight_layout() plt.show()"},{"question":"**Question:** You are tasked with creating a utility to manage key-value pairs using Python\'s `dbm` module. Your application should be able to create a new database, add key-value pairs, retrieve values by key, delete key-value pairs, and list all keys in the database. Additionally, it should handle database synchronization and ensure that the database is properly closed after operations. # Instructions: 1. **DatabaseManager Class**: Implement a class `DatabaseManager` that manages a `dbm` database. 2. **Initialization**: The class should be initialized with the following parameters: - `filename` (str): The name of the database file. - `flag` (str): Mode to open the database. It can be `\'r\'`, `\'w\'`, `\'c\'`, or `\'n\'`. - `mode` (octal, optional): Unix mode of the file, default is `0o666`. 3. **Methods**: - `add_entry(self, key: str, value: str) -> None`: Adds a key-value pair to the database. - `get_entry(self, key: str) -> str`: Retrieves the value for the specified key. If the key does not exist, it should return `\'Key not found\'`. - `delete_entry(self, key: str) -> None`: Deletes the key-value pair from the database. If the key does not exist, do nothing. - `list_keys(self) -> list`: Returns a list of all keys in the database. - `sync(self) -> None`: Synchronizes the database to ensure any unwritten data is saved. 4. **Context Management**: Ensure that the `DatabaseManager` class can be used with a `with` statement to automatically close the database after operations. # Example Usage: ```python from dbm import open as dbm_open class DatabaseManager: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): self.db = dbm_open(filename, flag, mode) def add_entry(self, key: str, value: str) -> None: self.db[key.encode()] = value.encode() def get_entry(self, key: str) -> str: return self.db.get(key.encode(), b\'Key not found\').decode() def delete_entry(self, key: str) -> None: if key.encode() in self.db: del self.db[key.encode()] def list_keys(self) -> list: return [key.decode() for key in self.db.keys()] def sync(self) -> None: if hasattr(self.db, \'sync\'): self.db.sync() def close(self) -> None: self.db.close() def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.close() # Example: with DatabaseManager(\'mydatabase\', \'c\') as db_manager: db_manager.add_entry(\'key1\', \'value1\') print(db_manager.get_entry(\'key1\')) # Outputs: value1 db_manager.add_entry(\'key2\', \'value2\') print(db_manager.list_keys()) # Outputs: [\'key1\', \'key2\'] db_manager.delete_entry(\'key1\') print(db_manager.list_keys()) # Outputs: [\'key2\'] db_manager.sync() ``` # Constraints: - The key and value must be strings. - Ensure that the database is properly closed using context management. - Use `dbm.dumb` if the default `dbm` implementation is not available. # Performance: - The operations should handle typical database sizes gracefully. - Proper error handling should be in place for file operations and database interactions.","solution":"import dbm class DatabaseManager: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): self.db = dbm.open(filename, flag, mode) def add_entry(self, key: str, value: str) -> None: self.db[key] = value def get_entry(self, key: str) -> str: return self.db.get(key, b\'Key not found\').decode() def delete_entry(self, key: str) -> None: if key in self.db: del self.db[key] def list_keys(self) -> list: return [key.decode() for key in self.db.keys()] def sync(self) -> None: if hasattr(self.db, \'sync\'): self.db.sync() def close(self) -> None: self.db.close() def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.close()"},{"question":"# Advanced Garbage Collection Management in Python In this coding assessment, you will demonstrate your understanding of Python\'s garbage collection (GC) interface provided by the `gc` module. Your task is to implement a function that uses various functionalities of the `gc` module to manage garbage collection during a simulation of object creation and destruction. Requirements 1. **Initialization**: - The function should initially disable the garbage collector. - Set custom thresholds for the GC such that it operates aggressively (low thresholds to force frequent collections). 2. **Object Creation Simulation**: - Create a specified number of objects of a custom class in a loop. - Each object should have a reference to the next object, creating a chain (cycle). - Insert a small delay (using `time.sleep()`) between successive object creations to mimic real-time operations. 3. **Monitoring and Debugging**: - Enable debugging with `gc.DEBUG_STATS` to print collection statistics to `sys.stderr`. - Before each object creation, confirm that GC is disabled and after certain intervals, trigger garbage collection manually. 4. **Memory Leak Detection**: - After finishing the object creation loop, enable the garbage collector and trigger a full collection. - Identify any uncollectable objects (cycles that could not be broken). 5. **Cleanup**: - Enable the garbage collector. - Reset the collection thresholds to their default values. Function Signature ```python import gc import time def simulate_gc_management(num_objects: int, delay: float) -> None: Simulates a scenario of object creation and garbage collection management. Args: num_objects (int): Number of objects to create. delay (float): Delay in seconds between the creation of each object. Returns: None ``` Example ```python class Node: def __init__(self, value): self.value = value self.next = None # Simulation function usage: simulate_gc_management(10, 0.1) ``` Constraints - `num_objects` should be a positive integer less than or equal to 1000. - `delay` should be a non-negative floating point number less than or equal to 5.0 seconds. Notes - Utilize the `gc` module functions as detailed in the documentation to enable/disable the garbage collector, manage thresholds, and perform manual garbage collection. - Ensure the program prints relevant debugging information, demonstrating the use of `gc.set_debug(gc.DEBUG_STATS)`. - Carefully handle the creation and destruction of objects to observe the garbage collector\'s behavior and the impact of thresholds and manual collection.","solution":"import gc import time class Node: def __init__(self, value): self.value = value self.next = None def simulate_gc_management(num_objects: int, delay: float) -> None: if not isinstance(num_objects, int) or num_objects <= 0 or num_objects > 1000: raise ValueError(\\"num_objects should be a positive integer less than or equal to 1000.\\") if not isinstance(delay, (int, float)) or delay < 0 or delay > 5.0: raise ValueError(\\"delay should be a non-negative floating point number less than or equal to 5.0 seconds.\\") # Disable the garbage collector gc.disable() # Set aggressive thresholds gc.set_threshold(1, 1, 1) nodes = [] gc.set_debug(gc.DEBUG_STATS) for i in range(num_objects): # Ensure garbage collector is disabled before each creation assert not gc.isenabled() new_node = Node(i) if nodes: nodes[-1].next = new_node # Creating a chain nodes.append(new_node) # Manually trigger garbage collection at intervals if i % 10 == 0: gc.collect() time.sleep(delay) # Simulate delay # Enable garbage collector and trigger full collection gc.enable() gc.collect() uncollectable = gc.garbage if uncollectable: print(f\\"Uncollectable objects detected: {uncollectable}\\") # Reset thresholds to default values gc.set_threshold(700, 10, 10)"},{"question":"Objective: To assess your understanding of Python\'s import system and the `importlib` module, implement a function that dynamically imports a given module and verifies the availability of a specified attribute or function within that module. Problem Statement: Write a Python function `dynamic_import_and_check` that takes two arguments: 1. `module_name` (string): The name of the module to be imported. 2. `attr_name` (string): The name of the attribute or function to check within the module. The function should perform the following tasks: 1. Dynamically import the specified module using `importlib`. 2. Check if the module contains the specified attribute or function. 3. Return `True` if the attribute or function is present, otherwise return `False`. Input: - `module_name` (string): The name of the module to import (e.g., `\\"math\\"`). - `attr_name` (string): The name of the attribute or function to check for (e.g., `\\"sqrt\\"`). Output: - A boolean value: `True` if the attribute or function exists in the module, otherwise `False`. Constraints: - The module name will be a valid Python module that can be imported. - The attribute name will be a valid string. - Use the `importlib` module for importing. Example: ```python result = dynamic_import_and_check(\\"math\\", \\"sqrt\\") print(result) # Output: True result = dynamic_import_and_check(\\"math\\", \\"unknown_function\\") print(result) # Output: False ``` Performance Requirements: The function should be efficient and handle the import process gracefully. Consider edge cases such as non-existent modules or attributes, and ensure that the function does not crash or throw exceptions. Implementation: ```python import importlib def dynamic_import_and_check(module_name, attr_name): try: module = importlib.import_module(module_name) if hasattr(module, attr_name): return True else: return False except ImportError: return False ``` Test your function with several modules and attributes to ensure it works correctly under various scenarios.","solution":"import importlib def dynamic_import_and_check(module_name, attr_name): Dynamically imports the specified module and checks if the module contains the specified attribute or function. Args: module_name (str): The name of the module to import. attr_name (str): The name of the attribute or function to check for. Returns: bool: True if the attribute or function exists in the module, otherwise False. try: module = importlib.import_module(module_name) return hasattr(module, attr_name) except ImportError: return False"},{"question":"Task Implement a Python function `manipulate_objects(obj1, obj2, attr_name, key)` that performs the following operations using the object protocol functions: 1. **Check Attribute Existence and Value**: - Check if `obj1` has an attribute named `attr_name`. - If the attribute exists, retrieve its value. 2. **Compare Objects**: - Compare the retrieved attribute value (if it exists) and `obj2` using different comparison operations (`<`, `<=`, `==`, `!=`, `>`, `>=`). - Store the results of these comparisons in a dictionary with keys being the operation names and values being the result of the comparison. 3. **Item Mapping Operations**: - Retrieve the item from `obj1` corresponding to `key`, if `obj1` supports item retrieval. - If successful, update the retrieved item by setting its attribute `attr_name` to the value of `obj2`. - If updating the attribute was successful, return a tuple containing the updated attribute value and the comparison results dictionary. # Input - `obj1`: A Python object. - `obj2`: Another Python object. - `attr_name`: A string representing the name of the attribute. - `key`: The key used to retrieve the item from `obj1` (if applicable). # Output - A tuple containing: - The updated attribute value from `obj1` (if applicable). - A dictionary with comparison results. # Constraints - `obj1` and `obj2` can be any Python objects. - You should handle errors and exceptions gracefully, returning `None` for the attribute value and an empty dictionary if any operation fails. # Example ```python class Sample: def __init__(self, x): self.x = x obj1 = Sample(10) obj2 = 20 attr_name = \'x\' key = \'sample_key\' result = manipulate_objects(obj1, obj2, attr_name, key) print(result) ``` # Notes - Use the provided object protocol functions for attribute and item manipulation as well as for comparison operations. - Implement proper error handling and provide meaningful return values as specified.","solution":"def manipulate_objects(obj1, obj2, attr_name, key): Perform operations on obj1 and obj2 using object protocol functions. :param obj1: First Python object :param obj2: Second Python object :param attr_name: Attribute name to check and manipulate :param key: Key to retrieve item from obj1 if possible :return: Tuple containing the updated attribute value and comparison results dictionary attribute_value = None comparison_results = {} # Check if obj1 has the attribute attr_name if hasattr(obj1, attr_name): attribute_value = getattr(obj1, attr_name) # Perform comparisons comparison_results = { \'<\': attribute_value < obj2, \'<=\': attribute_value <= obj2, \'==\': attribute_value == obj2, \'!=\': attribute_value != obj2, \'>\': attribute_value > obj2, \'>=\': attribute_value >= obj2 } # Try to retrieve item from obj1 using key try: item = obj1[key] # Set the attribute attr_name of the retrieved item to obj2 setattr(item, attr_name, obj2) # Verify if the value is updated updated_value = getattr(item, attr_name) return (updated_value, comparison_results) except (TypeError, KeyError, AttributeError): pass # Return None and an empty dictionary in case of failure return (None, comparison_results)"},{"question":"Objective To assess the student\'s understanding of the `doctest` module and their ability to implement and test functions using doctest. Problem Statement You are provided with a module that contains a function for calculating Fibonacci numbers. Your task is to add doctest examples to the function to ensure it works correctly and meets the specified requirements. # Part 1: Implement the Fibonacci Function Implement the function `fibonacci(n)` which calculates the nth Fibonacci number. The Fibonacci sequence is defined as follows: - `Fibonacci(0) = 0` - `Fibonacci(1) = 1` - For `n >= 2`, `Fibonacci(n) = Fibonacci(n-1) + Fibonacci(n-2)` # Part 2: Add Doctest Examples Add doctest examples to the docstring of the `fibonacci(n)` function to verify its correctness for a variety of cases, including: - Small values of `n` such as 0, 1, and 5. - Larger values of `n` such as 30. - Invalid inputs such as negative numbers and non-integer values. Constraints - The function should raise a `ValueError` if `n` is negative with the message \\"n must be >= 0\\". - The function should raise a `ValueError` if `n` is not an integer with the message \\"n must be an integer\\". Expected Input and Output - **Input:** A single integer `n`. - **Output:** An integer representing the nth Fibonacci number. Example ```python >>> fibonacci(5) 5 >>> fibonacci(10) 55 >>> fibonacci(0) 0 >>> fibonacci(1) 1 >>> fibonacci(-5) Traceback (most recent call last): ... ValueError: n must be >= 0 >>> fibonacci(5.5) Traceback (most recent call last): ... ValueError: n must be an integer ``` # Part 3: Verify with Doctest Verify that your function and doctest examples work correctly by running the doctest module. You can do this by adding the following code block at the end of your script: ```python if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` # Solution You should provide the implementation of the `fibonacci(n)` function with the required doctest examples in the docstring. ```python def fibonacci(n): Calculate the nth Fibonacci number. >>> fibonacci(0) 0 >>> fibonacci(1) 1 >>> fibonacci(5) 5 >>> fibonacci(10) 55 >>> fibonacci(30) 832040 >>> fibonacci(-2) Traceback (most recent call last): ... ValueError: n must be >= 0 >>> fibonacci(5.5) Traceback (most recent call last): ... ValueError: n must be an integer if not isinstance(n, int): raise ValueError(\\"n must be an integer\\") if n < 0: raise ValueError(\\"n must be >= 0\\") a, b = 0, 1 for _ in range(n): a, b = b, a + b return a if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` **Note:** Make sure the function is written along with proper doctests and tested thoroughly using the `doctest` module.","solution":"def fibonacci(n): Calculate the nth Fibonacci number. >>> fibonacci(0) 0 >>> fibonacci(1) 1 >>> fibonacci(5) 5 >>> fibonacci(10) 55 >>> fibonacci(30) 832040 >>> fibonacci(-2) Traceback (most recent call last): ... ValueError: n must be >= 0 >>> fibonacci(5.5) Traceback (most recent call last): ... ValueError: n must be an integer if not isinstance(n, int): raise ValueError(\\"n must be an integer\\") if n < 0: raise ValueError(\\"n must be >= 0\\") a, b = 0, 1 for _ in range(n): a, b = b, a + b return a if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"**PyTorch CUDA Tuning Configuration and Execution** You are required to implement a PyTorch CUDA tuning setup using the `torch.cuda.tunable` module. The tasks you need to accomplish include enabling tuning, configuring tuning parameters, performing GEMM tuning, and saving the configuration and results to a file. # Tasks: 1. **Enable Tuning:** Use the `enable` and `tuning_enable` functions to enable the tuning mode for your application. 2. **Set Tuning Parameters:** - Set the maximum tuning duration to 60 seconds using `set_max_tuning_duration`. - Set the maximum iterations for tuning to 10 using `set_max_tuning_iterations`. 3. **File Configuration:** - Set a filename for the configuration using `set_filename`. Use \\"tuning_config.json\\" as the filename. - Ensure that the results are written to the file on exit using `write_file_on_exit`. 4. **Perform GEMM Tuning:** - Use a mock filename \\"gemm_operations.json\\" to simulate the file containing GEMM operations for tuning. - Use `tune_gemm_in_file` to perform the tuning using the mock file. 5. **Retrieve and Print Tuning Results:** - Retrieve the tuning results using `get_results`. - Print the retrieved results in a readable JSON format. # Constraints and Assumptions: - Assume that file reading/writing operations are simulated, and the actual file contents are not necessary for the task. - Focus on demonstrating the usage of the `torch.cuda.tunable` API rather than the specific details of GEMM operations. - All configurations and interactions should be self-contained within the implementation. # Function Signature: Implement your solution in a function with the following signature: ```python def setup_and_perform_cuda_tuning(): # Your implementation here pass ``` # Expected Output: The function should print the tuning results after performing the configurations and tuning operations. # Example Output: ```json { \\"status\\": \\"success\\", \\"details\\": { \\"iterations\\": 10, \\"duration\\": 60, \\"best_configuration\\": { ... } } } ``` **Note:** The actual output format may vary based on the details of the tuning results obtained from `get_results`.","solution":"import json def setup_and_perform_cuda_tuning(): import torch.cuda.tunable as tune # Enable Tuning tune.enable() tune.tuning_enable() # Set Tuning Parameters tune.set_max_tuning_duration(60) tune.set_max_tuning_iterations(10) # File Configuration filename = \\"tuning_config.json\\" tune.set_filename(filename) tune.write_file_on_exit(True) # Perform GEMM Tuning gemm_operations_filename = \\"gemm_operations.json\\" tune.tune_gemm_in_file(gemm_operations_filename) # Retrieve and Print Tuning Results results = tune.get_results() json_results = json.dumps(results, indent=4) print(json_results) return results"},{"question":"You are provided with an XML string containing information about various books in a bookstore. Each book includes the title, author, year published, and price. Your task is to implement a function `modify_xml(xml_string: str) -> str` that performs the following operations: 1. **Parsing:** Parse the provided XML string to create an `ElementTree`. 2. **Modification:** - Increase the price of each book by 10%. - For books authored by \\"John Doe\\", update the title to append the text \\" - Bestseller\\". 3. **Serialization:** Convert the modified `ElementTree` back into a string and return it. # Input - `xml_string` (str): A string containing the XML data representing the bookstore\'s catalog. # Output - Returns a string containing the modified XML data. # Example **Input:** ```xml <?xml version=\\"1.0\\"?> <bookstore> <book> <title>Python Programming</title> <author>Jane Smith</author> <year>2020</year> <price>29.99</price> </book> <book> <title>Advanced Python</title> <author>John Doe</author> <year>2018</year> <price>45.00</price> </book> </bookstore> ``` **Output:** ```xml <?xml version=\\"1.0\\"?> <bookstore> <book> <title>Python Programming</title> <author>Jane Smith</author> <year>2020</year> <price>32.989</price> </book> <book> <title>Advanced Python - Bestseller</title> <author>John Doe</author> <year>2018</year> <price>49.5</price> </book> </bookstore> ``` # Constraints - Assume well-formed XML input. - All prices are positive floats and should be rounded to two decimal places in the output. # Implementation Notes - You may use the `xml.etree.ElementTree` module for parsing, modifying, and serializing the XML data. - Ensure that your function appropriately updates the tree structure and handles text and attributes correctly. ```python import xml.etree.ElementTree as ET def modify_xml(xml_string: str) -> str: # Parse the XML string root = ET.fromstring(xml_string) for book in root.findall(\'book\'): # Update price: increase by 10% price_element = book.find(\'price\') price = float(price_element.text) new_price = round(price * 1.10, 2) price_element.text = f\\"{new_price}\\" # Update title: append \\" - Bestseller\\" if author is John Doe author = book.find(\'author\').text if author == \\"John Doe\\": title_element = book.find(\'title\') title_element.text += \\" - Bestseller\\" # Generate the modified XML string return ET.tostring(root, encoding=\'unicode\', method=\'xml\') ```","solution":"import xml.etree.ElementTree as ET def modify_xml(xml_string: str) -> str: # Parse the XML string root = ET.fromstring(xml_string) for book in root.findall(\'book\'): # Update price: increase by 10% price_element = book.find(\'price\') price = float(price_element.text) new_price = round(price * 1.10, 2) price_element.text = f\\"{new_price}\\" # Update title: append \\" - Bestseller\\" if author is John Doe author = book.find(\'author\').text if author == \\"John Doe\\": title_element = book.find(\'title\') title_element.text += \\" - Bestseller\\" # Generate the modified XML string return ET.tostring(root, encoding=\'unicode\', method=\'xml\')"},{"question":"**Problem Statement:** You are given a dataset consisting of handwritten digits images, where each image is represented as a 64-dimensional vector (8x8 pixels, flattened). Your task is to use Principal Component Analysis (PCA) from the scikit-learn library to perform dimensionality reduction on this dataset, reducing it to a lower-dimensional space that retains most of the variance. Additionally, you need to visualize the explained variance ratio for each principal component and the transformed dataset in the new space constrained to two or three dimensions. **Instructions:** 1. Load the digits dataset from `sklearn.datasets.load_digits()`. 2. Implement a function `apply_pca` that takes the digits dataset and the number of components to keep as input and returns the PCA-transformed dataset. 3. Implement a function `plot_explained_variance_ratio` that plots the explained variance ratio for each principal component. 4. Implement a function `plot_pca_transformed_data` that visualizes the PCA-transformed dataset in a 2D or 3D plot, with different colors for each digit class. **Requirements:** - The `apply_pca` function should use scikit-learn\'s `PCA` class and its `fit_transform` method. - The `plot_explained_variance_ratio` function should create a bar plot of the explained variance ratio. - The `plot_pca_transformed_data` function should create a scatter plot for the 2D case or a 3D scatter plot for the 3D case. Use different colors to distinguish different digit classes. - Ensure your code is efficient and well-documented. - You may use any additional Python libraries for plotting, such as Matplotlib or Seaborn. **Expected Function Signatures:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.decomposition import PCA def apply_pca(data: np.ndarray, n_components: int) -> np.ndarray: Apply PCA to the given dataset. Parameters: - data: np.ndarray, the input data to decompose. - n_components: int, the number of principal components to retain. Returns: - transformed_data: np.ndarray, the dataset transformed to the new space defined by the principal components. def plot_explained_variance_ratio(pca: PCA): Plot the explained variance ratio for each principal component. Parameters: - pca: PCA, the fitted PCA object. def plot_pca_transformed_data(data: np.ndarray, labels: np.ndarray, n_components: int): Plot the PCA-transformed data in 2D or 3D space. Parameters: - data: np.ndarray, the PCA-transformed data. - labels: np.ndarray, the true labels of the data samples. - n_components: int, the number of principal components (2 or 3) to visualize. ``` **Example Workflow:** ```python # Load the dataset digits = load_digits() data, labels = digits.data, digits.target # Apply PCA n_components = 10 transformed_data = apply_pca(data, n_components) # Plot explained variance ratio pca = PCA(n_components=n_components).fit(data) plot_explained_variance_ratio(pca) # Plot PCA-transformed data for the first 2 components plot_pca_transformed_data(transformed_data, labels, n_components=2) # Plot PCA-transformed data for the first 3 components (optional) plot_pca_transformed_data(transformed_data, labels, n_components=3) ``` **Constraints:** - Use `sklearn.decomposition.PCA` for the PCA transformation. - Ensure that your code is clear and easy to follow, with appropriate comments. - The `plot_pca_transformed_data` function should handle both 2D and 3D visualizations based on the `n_components` parameter. **Hint:** - Refer to the scikit-learn documentation for detailed information on the PCA class and its methods. - Use Matplotlib\'s `scatter` function for 2D plots and `Axes3D` for 3D plots.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.decomposition import PCA def apply_pca(data: np.ndarray, n_components: int) -> np.ndarray: Apply PCA to the given dataset. Parameters: - data: np.ndarray, the input data to decompose. - n_components: int, the number of principal components to retain. Returns: - transformed_data: np.ndarray, the dataset transformed to the new space defined by the principal components. pca = PCA(n_components=n_components) transformed_data = pca.fit_transform(data) return transformed_data def plot_explained_variance_ratio(pca: PCA): Plot the explained variance ratio for each principal component. Parameters: - pca: PCA, the fitted PCA object. plt.figure(figsize=(10, 6)) plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, alpha=0.5, align=\'center\') plt.step(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), where=\'mid\') plt.xlabel(\'Principal Component Index\') plt.ylabel(\'Variance Ratio\') plt.title(\'Explained Variance Ratio by Principal Components\') plt.show() def plot_pca_transformed_data(data: np.ndarray, labels: np.ndarray, n_components: int): Plot the PCA-transformed data in 2D or 3D space. Parameters: - data: np.ndarray, the PCA-transformed data. - labels: np.ndarray, the true labels of the data samples. - n_components: int, the number of principal components (2 or 3) to visualize. if n_components == 2: plt.figure(figsize=(10, 6)) scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=\'viridis\') plt.colorbar(scatter, ticks=range(10)) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA-transformed data (2D)\') plt.show() elif n_components == 3: from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection=\'3d\') scatter = ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=labels, cmap=\'viridis\') fig.colorbar(scatter, ticks=range(10)) ax.set_xlabel(\'Principal Component 1\') ax.set_ylabel(\'Principal Component 2\') ax.set_zlabel(\'Principal Component 3\') ax.set_title(\'PCA-transformed data (3D)\') plt.show() else: print(\\"n_components must be either 2 or 3 for visualization.\\")"},{"question":"# Package Metadata Utility You are tasked with writing a utility function using the `importlib.metadata` module to gather comprehensive metadata information about an installed Python package. This function will collect details such as the version, entry points, metadata, files in the distribution, and requirements of the package. Function Signature ```python def get_package_info(package_name: str) -> dict: Retrieve comprehensive metadata information about an installed package. Parameters: - package_name: str : Name of the package to retrieve information. Returns: - dict: A dictionary containing the following keys and their corresponding values: - version: str : The version of the package. - entry_points: dict : A dictionary grouping entry points by their group name. - metadata: dict : All available metadata keywords and values. - files: list : A list of file paths in the package distribution. - requirements: list : A list of package requirements. ``` Input - `package_name` (string): The name of the package for which the information should be retrieved. Output - A dictionary with keys: `version`, `entry_points`, `metadata`, `files`, and `requirements`. Each key maps to corresponding information about the package: - `version` (string): The version string of the package. - `entry_points` (dict): Entry points categorized by their groups as dictionaries. - `metadata` (dict): Metadata information of the package. - `files` (list): A list of file paths (strings) within the package distribution. - `requirements` (list): A list of strings representing the package requirements. Example ```python package_info = get_package_info(\'wheel\') print(package_info) ``` This output should contain a dictionary with comprehensive information about the `wheel` package, demonstrating mastery of fundamental and advanced concepts of the `importlib.metadata` module. Constraints - Assume the package is always installed in the environment where the function is executed. - Handle exceptions gracefully and return empty structures (like empty lists or dictionaries) if some information is not found. Implement the function `get_package_info()` to gather the required package metadata.","solution":"import importlib.metadata def get_package_info(package_name: str) -> dict: Retrieve comprehensive metadata information about an installed package. Parameters: - package_name: str : Name of the package to retrieve information. Returns: - dict: A dictionary containing the following keys and their corresponding values: - version: str : The version of the package. - entry_points: dict : A dictionary grouping entry points by their group name. - metadata: dict : All available metadata keywords and values. - files: list : A list of file paths in the package distribution. - requirements: list : A list of package requirements. try: dist = importlib.metadata.distribution(package_name) version = dist.version entry_points = {} for ep in dist.entry_points: entry_points.setdefault(ep.group, []).append(ep.name) metadata = dict(dist.metadata) files = list(dist.files) if dist.files else [] requirements = dist.requires if dist.requires else [] return { \'version\': version, \'entry_points\': entry_points, \'metadata\': metadata, \'files\': [str(file) for file in files], \'requirements\': requirements, } except importlib.metadata.PackageNotFoundError: return { \'version\': \'\', \'entry_points\': {}, \'metadata\': {}, \'files\': [], \'requirements\': [], }"},{"question":"**Coding Assessment Question** # Objective The goal of this exercise is to use pandas\' merging, joining, and concatenating functionality to solve a real-world data manipulation problem. # Problem Statement You are given three DataFrames about customer transactions and feedback. These DataFrames contain information about customers, their transactions, and the feedback they provided on those transactions. Your task is to preprocess and combine these DataFrames to generate a comprehensive report. # DataFrames 1. `customers` DataFrame contains basic information about customers. ```python customers = pd.DataFrame({ \\"customer_id\\": [1, 2, 3, 4], \\"name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"], \\"age\\": [25, 30, 35, 40], \\"gender\\": [\\"F\\", \\"M\\", \\"M\\", \\"M\\"] }) ``` 2. `transactions` DataFrame contains transaction data. ```python transactions = pd.DataFrame({ \\"transaction_id\\": [100, 101, 102, 103, 104, 105], \\"customer_id\\": [1, 2, 2, 3, 4, 1], \\"amount\\": [250, 150, 350, 400, 100, 200], \\"date\\": [\\"2023-05-14\\", \\"2023-05-15\\", \\"2023-05-15\\", \\"2023-05-15\\", \\"2023-05-16\\", \\"2023-05-16\\"] }) ``` 3. `feedback` DataFrame contains feedback provided by the customers on their transactions. ```python feedback = pd.DataFrame({ \\"transaction_id\\": [100, 101, 103, 104], \\"rating\\": [5, 4, 3, 2], \\"comments\\": [\\"Excellent\\", \\"Good\\", \\"Average\\", \\"Poor\\"] }) ``` # Task Write a function `generate_customer_report(customers, transactions, feedback)` that processes the data by performing the following steps: 1. **Combine DataFrames**: - Merge the `transactions` with `customers` based on the `customer_id`. - Merge the resulting DataFrame with `feedback` based on `transaction_id`. 2. **Sort and Clean**: - Ensure the resulting DataFrame is sorted by `customer_id` and `date`. - Any transactions without feedback should have `NaN` for `rating` and `comments`. 3. **Add Meta Information**: - Add a column `feedback_given` which is `True` if feedback was provided and `False` otherwise. - Add a column `transaction_month` which indicates the month when the transaction happened as a string (e.g., \\"May\\"). 4. **Generate Final Report**: - Group the DataFrame by `customer_id` and provide the following summaries for each customer: - Count of transactions. - Total amount spent. - Average rating (ignore transactions without feedback in this calculation). # Input The input parameters to the function are three pandas DataFrames `customers`, `transactions`, and `feedback` as described above. # Output The function should return a new DataFrame grouped by `customer_id` with columns: - `customer_id` - `name` - `gender` - `age` - `transaction_count` - `total_amount` - `average_rating` # Function Signature ```python import pandas as pd def generate_customer_report(customers: pd.DataFrame, transactions: pd.DataFrame, feedback: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` # Constraints 1. Use only pandas library for data manipulation. 2. Assume the data is clean and contains valid entries. 3. Performance should be considered for large datasets. **Example** Given the `customers`, `transactions`, and `feedback` DataFrames: ```python customers = pd.DataFrame({ \\"customer_id\\": [1, 2, 3, 4], \\"name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"], \\"age\\": [25, 30, 35, 40], \\"gender\\": [\\"F\\", \\"M\\", \\"M\\", \\"M\\"] }) transactions = pd.DataFrame({ \\"transaction_id\\": [100, 101, 102, 103, 104, 105], \\"customer_id\\": [1, 2, 2, 3, 4, 1], \\"amount\\": [250, 150, 350, 400, 100, 200], \\"date\\": [\\"2023-05-14\\", \\"2023-05-15\\", \\"2023-05-15\\", \\"2023-05-15\\", \\"2023-05-16\\", \\"2023-05-16\\"] }) feedback = pd.DataFrame({ \\"transaction_id\\": [100, 101, 103, 104], \\"rating\\": [5, 4, 3, 2], \\"comments\\": [\\"Excellent\\", \\"Good\\", \\"Average\\", \\"Poor\\"] }) ``` Calling the function: ```python result = generate_customer_report(customers, transactions, feedback) print(result) ``` Expected Output: ```plaintext customer_id name gender age transaction_count total_amount average_rating 0 1 Alice F 25 2 450.0 5.0 1 2 Bob M 30 2 500.0 4.0 2 3 Charlie M 35 1 400.0 3.0 3 4 David M 40 1 100.0 2.0 ``` **Good Luck!**","solution":"import pandas as pd def generate_customer_report(customers: pd.DataFrame, transactions: pd.DataFrame, feedback: pd.DataFrame) -> pd.DataFrame: # Merge transactions with customers transactions_customers = pd.merge(transactions, customers, on=\'customer_id\', how=\'inner\') # Merge the resulting DataFrame with feedback full_data = pd.merge(transactions_customers, feedback, on=\'transaction_id\', how=\'left\') # Sort by customer_id and date full_data[\'date\'] = pd.to_datetime(full_data[\'date\']) full_data = full_data.sort_values(by=[\'customer_id\', \'date\']) # Add feedback_given and transaction_month columns full_data[\'feedback_given\'] = full_data[\'rating\'].notna() full_data[\'transaction_month\'] = full_data[\'date\'].dt.strftime(\'%B\') # Group by customer_id to generate report def aggregate_func(group): transaction_count = len(group) total_amount = group[\'amount\'].sum() avg_rating = group[\'rating\'].mean() return pd.Series({ \'name\': group[\'name\'].iloc[0], \'gender\': group[\'gender\'].iloc[0], \'age\': group[\'age\'].iloc[0], \'transaction_count\': transaction_count, \'total_amount\': total_amount, \'average_rating\': avg_rating }) customer_report = full_data.groupby(\'customer_id\').apply(aggregate_func).reset_index() return customer_report"},{"question":"# XML Parsing and Manipulation with `xml.dom.pulldom` Objective: Using the `xml.dom.pulldom` module, write a Python function that parses an XML document from a string, identifies elements with a specific tag name, and applies specific transformations on those elements based on their attributes or contents. Task: Implement the function `filter_and_transform_xml(xml_string: str, min_price: int) -> str` that takes the following parameters: - `xml_string` (str): A string representing the XML document to parse. - `min_price` (int): A threshold price value. Your function should: 1. Parse the `xml_string` to identify elements with the tag name `\'item\'`. 2. For each `item` element, check if the `price` attribute is greater than `min_price`. 3. If the price condition is met, expand the node and append an additional attribute `\'status\'` with the value `\'expensive\'`. 4. Return the modified XML as a string. Input and Output Formats: - **Input**: A string `xml_string` containing valid XML content and an integer `min_price`. - **Output**: A string representing the modified XML content. Example: ```python # Example XML input xml_input = \'\'\' <items> <item price=\\"30\\">Notebook</item> <item price=\\"100\\">Laptop</item> <item price=\\"60\\">Smartphone</item> </items> \'\'\' # Function call output = filter_and_transform_xml(xml_input, 50) # Expected output # The output should add \'status\' attribute to elements with prices more than 50 print(output) # \'\'\' # <items> # <item price=\\"30\\">Notebook</item> # <item price=\\"100\\" status=\\"expensive\\">Laptop</item> # <item price=\\"60\\" status=\\"expensive\\">Smartphone</item> # </items> # \'\'\' ``` Constraints: - Assume that the input XML string is always well-formed. - The solution should handle parsing efficiently for XML strings up to several megabytes in size. Hints: 1. Use `xml.dom.pulldom.parseString()` to process the XML. 2. Iterate over the events to find `START_ELEMENT` events with the tag name `\'item\'`. 3. Utilize the `expandNode()` method to include child nodes before modifying attributes. 4. Use the `toxml()` method to convert the node back to a string after expansion and manipulation. Additional Information: If you encounter issues with external entities security (which is disabled by default for safety), you can ignore handling external entities for this assignment. --- Good luck, and happy coding!","solution":"from xml.dom.pulldom import parseString, START_ELEMENT, parseString def filter_and_transform_xml(xml_string: str, min_price: int) -> str: # Parse the XML string using pulldom doc = parseString(xml_string) # Prepare for modifications items = [] for event, node in doc: if event == START_ELEMENT and node.tagName == \'item\': doc.expandNode(node) price = int(node.getAttribute(\'price\')) if price > min_price: node.setAttribute(\'status\', \'expensive\') items.append(node) # Regenerate the XML string result = \\"<?xml version=\\"1.0\\" ?>n<items>n\\" for item in items: result += f\\" {item.toxml()}n\\" result += \\"</items>\\" return result"},{"question":"You are required to design an application that simulates a simple banking system providing the following functionalities: 1. Create an account. 2. Deposit money into the account. 3. Withdraw money from the account. 4. Save the account transaction history to a file upon program termination. **Specifications**: 1. Implement a class `BankAccount` with the following methods: - `__init__(self, account_number)`: Initializes the bank account with a unique account number and an initial balance of 0. Creates a list to store transaction history. - `deposit(self, amount)`: Adds the specified amount to the account balance and records the transaction in the transaction history. - `withdraw(self, amount)`: Subtracts the specified amount from the account balance if sufficient funds are available; otherwise, raises an `InsufficientFunds` exception and records the transaction. - `get_balance(self)`: Returns the current balance. - `get_transaction_history(self)`: Returns the list of transactions. 2. Implement a function `save_transaction_history(account)` that writes the transaction history to a file named `account_{account_number}.txt`. 3. Register this `save_transaction_history` function using `atexit` to ensure that each account\'s transaction history is saved to a file when the program terminates. **Input and Output**: - The input will consist of a sequence of operations to be performed on the bank account. - The output should include the current balance after each deposit or withdrawal operation, or the appropriate error message if withdrawal fails due to insufficient funds. - The transaction history should be saved automatically to a file upon program termination. **Constraints**: - All account numbers are unique and consecutive integers starting from 1. - The system should support multiple bank accounts and handle their transaction history independently. **Example Usage**: ```python import atexit class InsufficientFunds(Exception): pass class BankAccount: def __init__(self, account_number): self.account_number = account_number self.balance = 0 self.transaction_history = [] def deposit(self, amount): self.balance += amount self.transaction_history.append(f\\"Deposited {amount}\\") def withdraw(self, amount): if amount > self.balance: self.transaction_history.append(f\\"Failed to withdraw {amount} - Insufficient funds\\") raise InsufficientFunds(\\"Insufficient funds\\") self.balance -= amount self.transaction_history.append(f\\"Withdrew {amount}\\") def get_balance(self): return self.balance def get_transaction_history(self): return self.transaction_history def save_transaction_history(account): with open(f\\"account_{account.account_number}.txt\\", \\"w\\") as f: for transaction in account.get_transaction_history(): f.write(transaction + \'n\') # Create multiple bank accounts accounts = [BankAccount(i) for i in range(1, 4)] # Register save_transaction_history for each account for account in accounts: atexit.register(save_transaction_history, account) # Sample transactions accounts[0].deposit(1000) accounts[0].withdraw(500) accounts[1].deposit(2000) accounts[2].deposit(1500) accounts[2].withdraw(500) # At program termination, transaction histories will be saved to files: # account_1.txt, account_2.txt, account_3.txt ``` In this example, ensure that your implementation correctly registers the exit handlers and handles the input sequence of operations specified in the example. Your implementation will be tested with additional cases to validate its correctness.","solution":"import atexit class InsufficientFunds(Exception): pass class BankAccount: def __init__(self, account_number): self.account_number = account_number self.balance = 0 self.transaction_history = [] def deposit(self, amount): self.balance += amount self.transaction_history.append(f\\"Deposited {amount}\\") def withdraw(self, amount): if amount > self.balance: self.transaction_history.append(f\\"Failed to withdraw {amount} - Insufficient funds\\") raise InsufficientFunds(\\"Insufficient funds\\") self.balance -= amount self.transaction_history.append(f\\"Withdrew {amount}\\") def get_balance(self): return self.balance def get_transaction_history(self): return self.transaction_history def save_transaction_history(account): with open(f\\"account_{account.account_number}.txt\\", \\"w\\") as f: for transaction in account.get_transaction_history(): f.write(transaction + \'n\') # Register save_transaction_history for each account accounts = [] def create_account(account_number): account = BankAccount(account_number) accounts.append(account) atexit.register(save_transaction_history, account) return account"},{"question":"# NNTP Article Search **Objective:** Use the `nntplib` module to connect to an NNTP server and perform a specific set of tasks involving article retrieval and search. **Task:** Write a Python function `search_articles(server, newsgroup, keyword)` that connects to an NNTP server and retrieves headlines (subject headers) of articles from a newsgroup that contain a specific keyword in the subject. The function should return a list of article IDs that contain the keyword in their subjects. Your function should: 1. Connect to the NNTP server specified by `server`. 2. Select the newsgroup specified by `newsgroup`. 3. Retrieve the list of articles in the newsgroup\'s current range. 4. Filter the articles to find those that contain the `keyword` in their subject header. 5. Return a list of article IDs that match the keyword. **Input:** - `server`: A string representing the NNTP server\'s address (e.g., `news.gmane.io`). - `newsgroup`: A string representing the name of the newsgroup (e.g., `gmane.comp.python.committers`). - `keyword`: A string representing the keyword to search for in article subjects. **Output:** - A list of strings, with each string being an article ID that contains the keyword in its subject. **Constraints:** - Do not attempt to post articles or modify the server state. - Assume the module is available and the server is accessible. **Example Usage:** ```python def search_articles(server, newsgroup, keyword): # Your code here # Example call: articles = search_articles(\'news.gmane.io\', \'gmane.comp.python.committers\', \'Python\') print(articles) ``` Expected implementation steps: 1. Create an NNTP connection to the server. 2. Select the specified newsgroup. 3. Get the range of articles in the newsgroup. 4. Iterate over the articles in the range and check their subject headers. 5. Filter articles whose subjects contain the keyword. 6. Return the list of article IDs. Ensure proper error handling to gracefully manage connection failures, invalid newsgroup names, and other potential issues. **Notes:** - Utilize `nntplib` methods such as `group`, `over`, and `decode_header` to accomplish the tasks. - Review the provided examples in the documentation for guidance on using the `nntplib`.","solution":"import nntplib def search_articles(server, newsgroup, keyword): Connects to an NNTP server and retrieves the article IDs of articles from a specified newsgroup that contain a specific keyword in the subject header. :param server: str, NNTP server address :param newsgroup: str, newsgroup name :param keyword: str, keyword to search for in subject headers :returns: list of article IDs matched_articles = [] try: with nntplib.NNTP(server) as client: # Select the newsgroup resp, count, first, last, name = client.group(newsgroup) # Retrieve article overview data resp, overviews = client.over((first, last)) # Iterate over the article overview data for id, over in overviews: subject = over[\'subject\'] # Check if the subject contains the keyword if keyword.lower() in subject.lower(): matched_articles.append(id) except Exception as e: print(f\\"An error occurred: {e}\\") return matched_articles"},{"question":"# Coding Assessment: IP Address and Network Manipulation using `ipaddress` Module Your task is to work with IP addresses and networks utilizing the `ipaddress` module. Problem: You are given a list of IP addresses and a corresponding list of CIDR network notations. Your task is to implement the following functions: 1. `is_valid_ip(ip_str)`: This function should take a string representation of an IP address (`ip_str`) and return `True` if it is a valid IP address (either IPv4 or IPv6). Otherwise, it should return `False`. 2. `belongs_to_network(ip_str, network_str)`: This function should take a string representation of an IP address (`ip_str`) and a string representation of a network in CIDR notation (`network_str`). It should return `True` if the IP address belongs to the network, otherwise it should return `False`. 3. `count_ips_in_network(network_str)`: This function should take a string representation of a network in CIDR notation (`network_str`) and return the number of usable IP addresses in that network. # Input and Output Function 1: `is_valid_ip(ip_str)` - **Input**: A string `ip_str` representing an IP address. - **Output**: A boolean `True` if the IP address is valid, otherwise `False`. Function 2: `belongs_to_network(ip_str, network_str)` - **Input**: A string `ip_str` representing an IP address, and a string `network_str` representing a network in CIDR notation. - **Output**: A boolean `True` if the IP address is part of the network, otherwise `False`. Function 3: `count_ips_in_network(network_str)` - **Input**: A string `network_str` representing a network in CIDR notation. - **Output**: An integer representing the number of usable IP addresses in that network. # Constraints and Limitations: - You can assume that all `ip_str` and `network_str` inputs are strings that may represent valid or invalid IP addresses or networks. - You should handle exceptions where necessary to validate the inputs. - Your solution must efficiently handle both IPv4 and IPv6 addresses. # Example: ```python # Function 1 print(is_valid_ip(\'192.0.2.1\')) # Output: True print(is_valid_ip(\'2001:db8::1\')) # Output: True print(is_valid_ip(\'256.256.256.256\')) # Output: False # Function 2 print(belongs_to_network(\'192.0.2.1\', \'192.0.2.0/24\')) # Output: True print(belongs_to_network(\'192.0.3.1\', \'192.0.2.0/24\')) # Output: False print(belongs_to_network(\'2001:db8::1\', \'2001:db8::/96\')) # Output: True # Function 3 print(count_ips_in_network(\'192.0.2.0/24\')) # Output: 254 (usable IP addresses in an IPv4 /24 network) print(count_ips_in_network(\'2001:db8::0/96\')) # Output: 2**32 - 2 (usable IP addresses in an IPv6 /96 network) ``` Implement the functions `is_valid_ip`, `belongs_to_network`, and `count_ips_in_network` to complete the task.","solution":"import ipaddress def is_valid_ip(ip_str): Returns True if ip_str is a valid IP address (IPv4 or IPv6), otherwise False. try: ipaddress.ip_address(ip_str) return True except ValueError: return False def belongs_to_network(ip_str, network_str): Returns True if ip_str is an IP address that belongs to the network specified by network_str. try: ip = ipaddress.ip_address(ip_str) network = ipaddress.ip_network(network_str, strict=False) return ip in network except ValueError: return False def count_ips_in_network(network_str): Returns the number of usable IP addresses in the network specified by network_str. try: network = ipaddress.ip_network(network_str, strict=False) # For both IPv4 and IPv6 networks, the first and last addresses are reserved if network.prefixlen == 32 or network.prefixlen == 128: return 1 return network.num_addresses - 2 except ValueError: return 0"},{"question":"# Coding Challenge: Implement a Persistent Key-Value Store Using the `dbm` Module Objective You are required to create a class `KeyValueStore` that facilitates interaction with a persistent key-value database using the `dbm` module. Your implementation should cover basic dictionary operations (insert, retrieve, delete) and also include functionality for managing synchronization and iteration over keys. Details and Requirements 1. **Class Initialization**: - Implement the `__init__` method to initialize the database file. - Open the database file for reading and writing, creating it if it doesn\'t exist. - Ensure the database file is properly closed after operations by implementing context management (`__enter__` and `__exit__` methods). 2. **Operations**: - Implement the following methods: - `set_item(key: str, value: str) -> None`: Stores the given key-value pair in the database. - `get_item(key: str) -> str`: Retrieves the value associated with the given key from the database. If the key does not exist, return `\'Key not found\'`. - `delete_item(key: str) -> None`: Deletes the given key from the database. - `list_keys() -> list`: Lists all keys currently in the database. - `sync() -> None`: Forces any unwritten data to be written to the disk. - `reorganize() -> None`: Optimizes/Reorganizes the database to reclaim unused space (only applicable for `dbm.gnu`). 3. **Error Handling**: - Handle potential errors gracefully using try-except blocks. For instance, handle `dbm.error` when trying to perform operations on the database. 4. **Performance**: - While performance is not the primary focus, ensure that the database operations are efficiently handled and avoid redundant reads/writes. Example Usage ```python # Initialize the key-value store kv_store = KeyValueStore(\'example_db\') # Add some key-value pairs kv_store.set_item(\'name\', \'Alice\') kv_store.set_item(\'occupation\', \'Engineer\') # Retrieve values print(kv_store.get_item(\'name\')) # Expected output: \'Alice\' print(kv_store.get_item(\'age\')) # Expected output: \'Key not found\' # List all keys print(kv_store.list_keys()) # Expected output: [\'name\', \'occupation\'] # Delete a key kv_store.delete_item(\'occupation\') # List all keys after deletion print(kv_store.list_keys()) # Expected output: [\'name\'] # Perform sync operation kv_store.sync() # Reorganize the database (if applicable) kv_store.reorganize() ``` Constraints - Use the `dbm` module for database interactions. - Keys and values should be stored as UTF-8 encoded bytes. - The database file should be closed properly after operations, either by using context management or explicitly closing the database.","solution":"import dbm class KeyValueStore: def __init__(self, db_file): self.db_file = db_file self.db = dbm.open(self.db_file, \'c\') def __enter__(self): self.db = dbm.open(self.db_file, \'c\') return self def __exit__(self, exc_type, exc_val, exc_tb): self.db.close() def set_item(self, key: str, value: str) -> None: self.db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') def get_item(self, key: str) -> str: try: return self.db[key.encode(\'utf-8\')].decode(\'utf-8\') except KeyError: return \'Key not found\' def delete_item(self, key: str) -> None: try: del self.db[key.encode(\'utf-8\')] except KeyError: pass def list_keys(self) -> list: return [key.decode(\'utf-8\') for key in self.db.keys()] def sync(self) -> None: self.db.sync() def reorganize(self) -> None: try: self.db.reorganize() except AttributeError: print(\\"Reorganize operation is not supported with this dbm implementation.\\") def close(self): self.db.close()"},{"question":"**Title: Evaluating Permutation Feature Importance on a Regression Model** **Objective:** This task will assess your ability to implement a regression model, calculate the permutation feature importance of its features using scikit-learn, and interpret the results. **Problem Statement:** You are given a dataset representing a regression problem. Your task is to: 1. Split the dataset into training and validation sets. 2. Train a regression model on the training set. 3. Calculate the model performance on the validation set. 4. Determine the permutation feature importance for each feature. 5. Print the sorted list of features based on their calculated importance. **Dataset:** The dataset contains multiple features and a target variable. It can be loaded using the following code: ```python from sklearn.datasets import load_diabetes data = load_diabetes(as_frame=True) X, y = data.data, data.target ``` **Requirements:** 1. Use `Ridge` regression model with `alpha=1e-2`. 2. The number of permutations (`n_repeats`) should be set to 30. 3. Random state for splitting the dataset and permutation importance should be set to 0. 4. Use the `R^2` score for evaluating the model performance and deriving feature importances. **Function Signature:** ```python def evaluate_permutation_importance(): # Load the dataset from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance # Split the dataset data = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(data.data, data.target, random_state=0) # Train the model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Evaluate the model performance on the validation set score = model.score(X_val, y_val) print(f\'Model R^2 Score: {score:.3f}\') # Calculate permutation feature importance r = permutation_importance(model, X_val, y_val, n_repeats=30, random_state=0) # Print sorted feature importances for i in r.importances_mean.argsort()[::-1]: if r.importances_mean[i] - 2 * r.importances_std[i] > 0: print(f\\"{data.feature_names[i]:<8} {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\\") # Call function evaluate_permutation_importance() ``` **Expected Output:** The output should display the R^2 score of the model and the features sorted by their importance along with their standard deviations. **Constraints:** - Ensure proper use of scikit-learn functions and classes. - Code should be efficient and well-structured. - Output results must be interpretable for model evaluation and feature selection. **Performance Requirement:** - The code must execute within a reasonable amount of time (under a minute) for the given dataset. - The results should be clear and correctly formatted for easy interpretation. **Notes:** - Consider any edge cases such as overfitted models with high training scores but low validation scores. - Ensure to handle any warnings or errors generated during the process.","solution":"def evaluate_permutation_importance(): # Import necessary libraries from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance # Load the dataset data = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(data.data, data.target, random_state=0) # Train the model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Evaluate the model performance on the validation set score = model.score(X_val, y_val) print(f\'Model R^2 Score: {score:.3f}\') # Calculate permutation feature importance r = permutation_importance(model, X_val, y_val, n_repeats=30, random_state=0) # Print sorted feature importances feature_importances = [] for i in r.importances_mean.argsort()[::-1]: if r.importances_mean[i] - 2 * r.importances_std[i] > 0: feature_importances.append((data.feature_names[i], r.importances_mean[i], r.importances_std[i])) print(f\\"{data.feature_names[i]:<8} {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\\") return score, feature_importances # Call the function if __name__ == \\"__main__\\": evaluate_permutation_importance()"},{"question":"# Asynchronous Data Processing with Exception Handling **Objective:** Write an asynchronous function in Python that reads from a simulated stream of data and processes it. The function should handle various `asyncio` exceptions appropriately. **Function Signature:** ```python import asyncio async def process_data_stream(reader: asyncio.StreamReader) -> str: pass ``` **Input:** - `reader` (asyncio.StreamReader): An asynchronous stream reader object from which data is read. **Output:** - Returns a string indicating the status of the data processing. Possible values are: - `\\"Success\\"`: If data is read and processed successfully. - `\\"Timeout\\"`: If a timeout error occurs. - `\\"Cancelled\\"`: If the operation is cancelled. - `\\"Invalid State\\"`: If an invalid state error occurs. - `\\"Sendfile Not Available\\"`: If the sendfile syscall is not available. - `\\"Incomplete Read\\"`: If the read operation is not completed fully. - `\\"Limit Overrun\\"`: If the buffer size limit is reached while looking for a separator. **Constraints:** - Assume the `reader` object is already initialized and connected to a data source. - Implement data reading using `await reader.read(n)` where `n` is the number of bytes to read. - Use appropriate timeout values and handle the exceptions listed in the provided documentation. **Performance Requirements:** - The function should handle a sufficient amount of data in a reasonable time frame. - Efficiently handle exceptions without causing significant delays. **Example:** ```python async def example_usage(): reader = asyncio.StreamReader() # You would typically have a connection here, for example: # reader, writer = await asyncio.open_connection(\'example.com\', 8888) try: status = await process_data_stream(reader) print(status) except Exception as e: print(f\\"Unexpected error: {e}\\") # Run the example usage in an asyncio event loop asyncio.run(example_usage()) ``` **Notes:** - Ensure to test your function thoroughly with various scenarios to cover all the exceptions.","solution":"import asyncio async def process_data_stream(reader: asyncio.StreamReader) -> str: try: data = await asyncio.wait_for(reader.read(100), timeout=5.0) # Read 100 bytes with a timeout of 5 seconds return \\"Success\\" except asyncio.TimeoutError: return \\"Timeout\\" except asyncio.CancelledError: return \\"Cancelled\\" except asyncio.InvalidStateError: return \\"Invalid State\\" except NotImplementedError: return \\"Sendfile Not Available\\" except asyncio.IncompleteReadError: return \\"Incomplete Read\\" except asyncio.LimitOverrunError: return \\"Limit Overrun\\""},{"question":"Custom DataFrame Accessor with Pandas # Background Pandas allows users to create custom accessors to extend the functionality of DataFrame, Series, and Index objects. This is useful when you want to add custom methods to pandas objects. # Problem Statement You are required to write a custom accessor for pandas DataFrames that adds a method to calculate the mean, median, and mode for each column in the DataFrame and returns a new DataFrame with these statistics. # Function Signature ```python @pd.api.extensions.register_dataframe_accessor(\\"stats\\") class StatsAccessor: def __init__(self, pandas_obj): self._obj = pandas_obj def summary(self): Calculate the mean, median, and mode for each column in the DataFrame. Returns: -------- pandas.DataFrame A DataFrame where: - Rows represent the statistical measures (mean, median, mode). - Columns represent the original columns of the input DataFrame. Each entry in the DataFrame is the respective statistic for the column. pass ``` # Constraints 1. The input DataFrame can contain columns with numerical and non-numerical data. 2. For non-numerical data, the mean and median should be `NaN`, but mode should be calculated. 3. Return a DataFrame with rows ordered as \\"mean\\", \\"median\\", \\"mode\\". # Example ```python import pandas as pd import numpy as np # Example DataFrame df = pd.DataFrame({ \'A\': [1, 2, 3, 4, 5], \'B\': [\'a\', \'b\', \'a\', \'b\', \'a\'], \'C\': [10.1, 10.2, 10.3, 10.4, 10.5] }) # Registering the accessor and calling summary stat_summary = df.stats.summary() print(stat_summary) # Expected Output: # A B C # mean 3.0 NaN 10.3 # median 3.0 NaN 10.3 # mode 1.0 a 10.1 (or could include multiple modes) ``` # Notes - Utilize `self._obj` to access the DataFrame within your custom accessor. - Handle non-numeric columns appropriately as described in the constraints. - Ensure your solution is efficient and concise.","solution":"import pandas as pd import numpy as np from scipy import stats as scipy_stats @pd.api.extensions.register_dataframe_accessor(\\"stats\\") class StatsAccessor: def __init__(self, pandas_obj): self._obj = pandas_obj def summary(self): def calculate_stats(series): if pd.api.types.is_numeric_dtype(series): mean = series.mean() median = series.median() mode = series.mode().iloc[0] if not series.mode().empty else np.nan else: mean = np.nan median = np.nan mode = series.mode().iloc[0] if not series.mode().empty else np.nan return mean, median, mode stats_dict = {} for column in self._obj.columns: stats_dict[column] = calculate_stats(self._obj[column]) return pd.DataFrame(stats_dict, index=[\\"mean\\", \\"median\\", \\"mode\\"]) # Example usage: # df = pd.DataFrame({ # \'A\': [1, 2, 3, 4, 5], # \'B\': [\'a\', \'b\', \'a\', \'b\', \'a\'], # \'C\': [10.1, 10.2, 10.3, 10.4, 10.5] # }) # stat_summary = df.stats.summary() # print(stat_summary)"},{"question":"# Python Coding Assessment **Objective:** To assess students\' understanding of file handling, string manipulation, and parsing configuration files in Python. **Question:** You are provided with the following `setup.cfg` file for a Python project: ``` [build_ext] inplace=0 [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Your task is to write a Python function `modify_setup_cfg` that takes three arguments: 1. `filename` (string) - the path to the `setup.cfg` file. 2. `command` (string) - the name of the Distutils command section to modify (e.g., `build_ext`, `bdist_rpm`). 3. `options` (dictionary) - a dictionary of options and their corresponding values that should be updated or added within the specified command section. The function should: 1. Open and read the `setup.cfg` file. 2. Update or add the specified options in the given command section. 3. Write the modified content back to the `setup.cfg` file. # Input - `filename` (str): A string representing the file path of the setup configuration file. - `command` (str): A string representing the Distutils command section (for example, `bdist_rpm`). - `options` (dict): A dictionary where keys are the option names and values are the option values that need to be updated or added in the specified command section. # Output The function should not return any value. The changes should be reflected in the original `setup.cfg` file itself. # Constraints - You may assume that the `setup.cfg` file exists and follows the mentioned format. - The `command` provided will always be valid and present within the `setup.cfg` file. - The `options` dictionary will contain valid option names for the command. # Example ```python def modify_setup_cfg(filename: str, command: str, options: dict): # Your implementation here # Example usage: config_path = \\"setup.cfg\\" command_section = \\"build_ext\\" new_options = {\'inplace\': \'1\', \'include_dirs\': \'/usr/local/include\'} modify_setup_cfg(config_path, command_section, new_options) ``` If the `setup.cfg` originally contains: ``` [build_ext] inplace=0 [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` After calling `modify_setup_cfg(config_path, command_section, new_options)`, the `setup.cfg` should be updated to: ``` [build_ext] inplace=1 include_dirs=/usr/local/include [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Ensure your function handles: - Reading and parsing the configuration file. - Adding or updating the options in the specified command section. - Maintaining the correct structure and formatting of the configuration file. - Preserving other sections unaffected.","solution":"import configparser def modify_setup_cfg(filename: str, command: str, options: dict): config = configparser.ConfigParser() config.read(filename) if command in config: for key, value in options.items(): config[command][key] = value else: config[command] = options with open(filename, \'w\') as configfile: config.write(configfile)"},{"question":"Context: In this task, you will create a Python function `safe_divide` which divides two numbers provided as arguments, while gracefully handling various possible errors and ensuring that all resources (e.g., files) are appropriately managed. Function Signature: ```python def safe_divide(x: float, y: float) -> float: Safely divides two numbers. Args: x: The dividend (numerator). y: The divisor (denominator). Returns: The result of the division if no exceptions occur. Raises: ValueError: If either x or y are not of type float or int. ZeroDivisionError: If the divisor (y) is zero. Exception: For any unexpected exceptions. ``` Requirements: 1. **Input Validation:** - Ensure both `x` and `y` are either `float` or `int`. - If not, raise a `ValueError` with a meaningful message. 2. **Handling Division Errors:** - Perform the division. - If `y` is zero, catch this and raise a `ZeroDivisionError` with a message. 3. **General Exceptions:** - Catch and handle any other unexpected exceptions. - Print an appropriate error message for any other exceptions. 4. **Logging:** - Log the result of the division to a file named `\\"division_log.txt\\"`. - Ensure this file operation is safely managed using `with` statement. - Ensure that the file is closed properly in all scenarios. Sample Usage: ```python # Valid inputs print(safe_divide(10, 2)) # Should return 5.0 and log the result. # ZeroDivisionError try: safe_divide(10, 0) except ZeroDivisionError as e: print(e) # Should print \\"Division by zero is not allowed.\\" # ValueError for invalid types try: safe_divide(10, \\"a\\") except ValueError as e: print(e) # Should print \\"Invalid type for x or y. Both must be int or float.\\" ``` Constraints: - The function should not use global variables. - Performance is not a primary concern, but the solution should strive for reasonable efficiency. - Follow best practices for exception handling and resource management as highlighted in the provided documentation. Develop your solution with robust exception handling and resource management in mind, demonstrating a clear understanding of the concepts outlined in the documentation.","solution":"import logging def safe_divide(x: float, y: float) -> float: Safely divides two numbers. Args: x: The dividend (numerator). y: The divisor (denominator). Returns: The result of the division if no exceptions occur. Raises: ValueError: If either x or y are not of type float or int. ZeroDivisionError: If the divisor (y) is zero. Exception: For any unexpected exceptions. if not isinstance(x, (int, float)) or not isinstance(y, (int, float)): raise ValueError(\\"Invalid type for x or y. Both must be int or float.\\") try: result = x / y except ZeroDivisionError: raise ZeroDivisionError(\\"Division by zero is not allowed.\\") except Exception as e: raise Exception(f\\"An unexpected error occurred: {e}\\") with open(\\"division_log.txt\\", \\"a\\") as log_file: log_file.write(f\\"Division of {x} by {y} resulted in {result}n\\") return result"},{"question":"# Turtle Graphics Art Creation Objective Write a Python program using the `turtle` module to create a piece of turtle graphics art. The program should demonstrate your understanding of the following concepts: - Turtle motion and control - Pen settings and drawing - Screen settings and events - Visibility and appearance of the turtle Task Implement a function named `create_turtle_art` that performs the following operations: 1. Sets up a `Screen` with a specific size and background color. 2. Creates a `Turtle` object and sets the starting position. 3. Makes the turtle draw a pattern of concentric shapes (e.g., circles or polygons). 4. Customizes the color and style of the pen for different shapes. 5. Implements an event listener to finish drawing when a key is pressed. Function Signature ```python def create_turtle_art(): pass ``` Requirements 1. The `Screen` should be 800x600 pixels with a light blue background. 2. The `Turtle` should start from the center of the screen (i.e., coordinates (0, 0)). 3. The turtle should draw at least ten concentric shapes, changing the color and line thickness for each shape. 4. Use at least three different shapes (e.g., circle, square, triangle) in the pattern. 5. Implement an event listener that waits for the user to press the \'q\' key to exit the drawing mode and close the screen. Constraints - You must use the `turtle` module methods and functions. - Ensure the program runs efficiently and does not hang or crash. Example Output While the exact graphic is up to your artistic discretion, below is an example of what the final output could look like after pressing the \'q\' key: ``` A series of colorful concentric shapes that display a pattern on a light blue background. ``` # Submission Submit your Python file containing the `create_turtle_art` function. Ensure your code is well-commented and follows best practices for readability and maintainability. Additional Information Refer to the `turtle` module documentation for detailed descriptions of methods and functions that you might find useful for this task.","solution":"import turtle def draw_shape(t, sides, length): Draws a polygon with a specific number of sides and side length using the turtle object. angle = 360 / sides for _ in range(sides): t.forward(length) t.right(angle) def create_turtle_art(): screen = turtle.Screen() screen.setup(width=800, height=600) screen.bgcolor(\\"light blue\\") t = turtle.Turtle() t.speed(0) # Fastest drawing speed shapes = [(4, \\"red\\"), (5, \\"green\\"), (6, \\"blue\\")] # (number of sides, color) start_position = 20 for i in range(10): sides, color = shapes[i % len(shapes)] t.penup() t.goto(0, -start_position) t.pendown() t.color(color) t.width(i + 1) draw_shape(t, sides, start_position * 2) start_position += 20 def exit_drawing(): screen.bye() screen.listen() screen.onkey(exit_drawing, \\"q\\") turtle.done()"},{"question":"**Objective**: Implement and train a neural network module in PyTorch that dynamically changes its architecture based on an input parameter and integrates several PyTorch functionalities such as custom initialization, training with an optimizer, managing module state, and using hooks. Problem Description You are required to implement a custom neural network class `DynamicNeuralNet` that: 1. Inherits from `torch.nn.Module`. 2. Uses a dynamic architecture that changes the number of hidden layers based on an input parameter. 3. Incorporates custom weight initialization. 4. Integrates forward and backward hooks to monitor and modify data flow. 5. Includes methods to save and load its state. Requirements 1. **Constructor (Initialization)**: - The constructor should take the following parameters: - `input_size` (int): Number of features in the input data. - `hidden_sizes` (List[int]): List containing the sizes of each hidden layer. - `output_size` (int): Number of output features. - `activation` (str): Activation type to use in hidden layers (`\'relu\'` or `\'tanh\'`). 2. **Forward Method**: - Implement a forward method that defines how data will pass through the network. 3. **Custom Initialization**: - Implement custom weight initialization using Xavier initialization for the weight parameters and zero initialization for biases. 4. **Training Method**: - Implement a method `train_model` that performs training on given data using a specified optimizer and loss function for a defined number of epochs. 5. **State Management**: - Implement methods `save_state` and `load_state` to save and load the model state respectively. 6. **Hooks**: - Implement hooks to monitor the inputs and outputs of each layer during the forward and backward passes. Input and Output Format - The `DynamicNeuralNet` class should be defined and implemented fully within the provided constraints. - The `train_model` method should accept: - `train_loader` (DataLoader): DataLoader for the training data. - `num_epochs` (int): Number of epochs for training. - `learning_rate` (float): Learning rate for the optimizer. - The `save_state` and `load_state` methods should: - Save to a file path provided as an argument to the `save_state` method. - Load from a file path provided as an argument to the `load_state` method. Example Usage: ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset # Example data x_train = torch.randn(100, 10) y_train = torch.randn(100, 1) train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=10, shuffle=True) # Define and create the model model = DynamicNeuralNet(input_size=10, hidden_sizes=[20, 10], output_size=1, activation=\'relu\') # Train the model model.train_model(train_loader, num_epochs=50, learning_rate=0.01) # Save the model state model.save_state(\'model_state.pth\') # Load the model state model.load_state(\'model_state.pth\') ``` Note: - Ensure to handle CUDA availability for device compatibility. - The hooks should print/log inputs and outputs during the forward and backward pass. Implement the `DynamicNeuralNet` class and its methods fulfilling all the requirements specified above.","solution":"import torch import torch.nn as nn import torch.optim as optim class DynamicNeuralNet(nn.Module): def __init__(self, input_size, hidden_sizes, output_size, activation): super(DynamicNeuralNet, self).__init__() self.layers = nn.ModuleList() self.activation = activation.lower() # Input layer current_size = input_size for hidden_size in hidden_sizes: self.layers.append(nn.Linear(current_size, hidden_size)) current_size = hidden_size # Output layer self.layers.append(nn.Linear(current_size, output_size)) # Apply custom initialization self.apply(self.initialize_weights) def initialize_weights(self, module): if isinstance(module, nn.Linear): nn.init.xavier_uniform_(module.weight) nn.init.zeros_(module.bias) def forward(self, x): for layer in self.layers[:-1]: x = layer(x) x = self.get_activation_function(x) # Output layer x = self.layers[-1](x) return x def get_activation_function(self, x): if self.activation == \'relu\': return torch.relu(x) elif self.activation == \'tanh\': return torch.tanh(x) else: raise ValueError(f\\"Unsupported activation function \'{self.activation}\'\\") def train_model(self, train_loader, num_epochs, learning_rate): self.train() optimizer = optim.Adam(self.parameters(), lr=learning_rate) criterion = nn.MSELoss() for epoch in range(num_epochs): for inputs, targets in train_loader: optimizer.zero_grad() outputs = self(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() def save_state(self, path): torch.save(self.state_dict(), path) def load_state(self, path): self.load_state_dict(torch.load(path))"},{"question":"Objectives 1. Understand and effectively use PyTorch\'s sparse tensor functionalities. 2. Manipulate and perform operations on CSR sparse tensors. Problem Statement You are given a large dense matrix `A` of size `(1000, 1000)` which contains mostly zero values. Your task is to convert this dense matrix to a sparse CSR format and then perform efficient sparse matrix-vector multiplication using a given dense vector `v` of size `(1000,)`. Additionally, implement a custom function that takes a sparse CSR tensor and computes the sum of all non-zero elements efficiently, without converting the sparse tensor to a dense tensor. Detailed Requirements 1. **Function 1**: `dense_to_sparse_csr(A: torch.Tensor) -> torch.Tensor` - **Input**: - `A`: A 2D tensor of size `(1000, 1000)` which is a dense matrix. - **Output**: - A tensor in CSR sparse format. 2. **Function 2**: `spmv_csr(csr_matrix: torch.Tensor, v: torch.Tensor) -> torch.Tensor` - **Input**: - `csr_matrix`: A 2D tensor in CSR sparse format. - `v`: A 1D tensor of size `(1000,)`. - **Output**: - A 1D tensor which is the result of the sparse matrix-vector multiplication. 3. **Function 3**: `sum_nonzero_csr(csr_matrix: torch.Tensor) -> float` - **Input**: - `csr_matrix`: A 2D tensor in CSR sparse format. - **Output**: - A float which is the sum of all non-zero elements in the sparse tensor. Constraints and Guidelines 1. You should not use dense conversion for any function that processes the sparse tensor (except for verification in the final solution). 2. Ensure that the sparse tensor operations are memory efficient. 3. The functions should be implemented using PyTorch. Example ```python import torch # Creating a dense matrix A with mostly zero elements A = torch.zeros(1000, 1000) A[torch.randint(1000, (1000, 2))] = torch.randn(1000) # Defining a dense vector v = torch.randn(1000) # Convert dense matrix to sparse CSR format csr_matrix = dense_to_sparse_csr(A) print(csr_matrix) # Perform sparse matrix-vector multiplication result = spmv_csr(csr_matrix, v) print(result) # Compute the sum of all non-zero elements in the sparse CSR matrix sum_nonzero = sum_nonzero_csr(csr_matrix) print(sum_nonzero) ``` Note This problem should test your understanding of handling sparse tensors in PyTorch, including their creation, conversion, and efficient computation using them.","solution":"import torch def dense_to_sparse_csr(A): Converts a dense 2D tensor to a sparse CSR format tensor. Parameters: A (torch.Tensor): A dense 2D tensor. Returns: torch.Tensor: A tensor in CSR sparse format. return A.to_sparse_csr() def spmv_csr(csr_matrix, v): Performs sparse matrix-vector multiplication using a CSR sparse tensor. Parameters: csr_matrix (torch.Tensor): A 2D tensor in CSR sparse format. v (torch.Tensor): A 1D dense tensor. Returns: torch.Tensor: The result of the sparse matrix-vector multiplication. return torch.sparse.mm(csr_matrix, v.unsqueeze(1)).squeeze() def sum_nonzero_csr(csr_matrix): Computes the sum of all non-zero elements in a CSR sparse tensor. Parameters: csr_matrix (torch.Tensor): A 2D tensor in CSR sparse format. Returns: float: The sum of all non-zero elements. return csr_matrix.values().sum().item()"},{"question":"File Descriptor Manipulation and Custom Buffering You are required to implement a custom file reader using Python\'s low-level file descriptor APIs. The task is to create a function that reads data from a given file descriptor, performs buffering, and returns the read content as a string. Specifically, you need to: 1. Open a file and retrieve its file descriptor. 2. Create a Python file object from this file descriptor. 3. Implement a custom buffer to read the file content. 4. Ensure proper error handling and cleanup of resources. Function Signature ```python def custom_file_reader(file_path: str, buffer_size: int) -> str: pass ``` Input - `file_path` (str): The path to the file to be read. - `buffer_size` (int): The size of the buffer to be used while reading the file. Output - Returns the content of the file as a string. Constraints - You should handle both text and binary files. - Ensure that all resources are properly closed after reading. - Maintain efficient memory usage by using the specified buffer size. Example ```python content = custom_file_reader(\\"/path/to/file.txt\\", 1024) print(content) ``` Notes Keep in mind the following points: - Use `PyFile_FromFd` to create a Python file object from the file descriptor. - Use `PyFile_GetLine` to read lines from the Python file object, implementing custom buffering logic based on `buffer_size`. Performance Requirements The implementation should efficiently read large files without loading the entire file into memory. ```python import os import io def custom_file_reader(file_path: str, buffer_size: int) -> str: content = [] try: # Open the file and get its file descriptor fd = os.open(file_path, os.O_RDONLY) # Create a Python file object from the file descriptor file_obj = io.open(fd, mode=\'r\', buffering=buffer_size) # Read the content using custom buffer while True: line = file_obj.readline(buffer_size) if not line: break content.append(line) except Exception as e: raise e finally: # Ensure the file is properly closed file_obj.close() return \'\'.join(content) # Example usage content = custom_file_reader(\\"/path/to/file.txt\\", 1024) print(content) ```","solution":"import os import io def custom_file_reader(file_path: str, buffer_size: int) -> str: content = [] try: # Open the file and get its file descriptor fd = os.open(file_path, os.O_RDONLY) # Create a Python file object from the file descriptor file_obj = io.open(fd, mode=\'r\', buffering=buffer_size) # Read the content using custom buffer while True: line = file_obj.read(buffer_size) if not line: break content.append(line) except Exception as e: raise e finally: # Ensure the file is properly closed file_obj.close() return \'\'.join(content)"},{"question":"# Advanced Compression and Decompression with LZMA You are provided with two files: `input_file.xz` and `output_file.xz`. Your task is to implement a function `compress_with_custom_filters` that reads data from `input_file.xz`, applies a specified custom filter chain to compress the data, and writes the compressed data to `output_file.xz`. Requirements: 1. **Function Signature**: ```python def compress_with_custom_filters(input_file: str, output_file: str, filters: list) -> None: ``` 2. **Parameters**: - `input_file` (str): Path to the input LZMA-compressed file. - `output_file` (str): Path to the output file to write the custom-filtered compressed data to. - `filters` (list): A list of dictionaries specifying the custom filter chain. 3. **Output**: - The function should not return anything. It should write the compressed data directly to `output_file.xz`. 4. **Constraints**: - The function should support only `.xz` format for both input and output files. - Ensure that the input file is read completely before applying the custom filter chain. 5. **Example Filter Chain**: ```python my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 5}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7 | lzma.PRESET_EXTREME}, ] ``` Example Usage: ```python input_file = \'input_file.xz\' output_file = \'output_file.xz\' filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6}, ] compress_with_custom_filters(input_file, output_file, filters) ``` Notes: - Use the `lzma` module documentation to understand how to work with custom filter chains and the `lzma.open` function. - You can assume that the required files exist and are accessible from the specified paths. # Evaluation Criteria: - Correctness: The function should correctly read, compress, and write data according to the specified filter chain. - Efficiency: The function should handle large files efficiently without excessive memory usage. - Use of LZMA module: Proper use of the `lzma` API for both reading the compressed input and writing the custom-extracted output.","solution":"import lzma def compress_with_custom_filters(input_file: str, output_file: str, filters: list) -> None: Compress the data from the input file with the given custom filters and write to the output file. Parameters: - input_file (str): Path to the input LZMA-compressed file. - output_file (str): Path to the output file to write the custom-filtered compressed data to. - filters (list): A list of dictionaries specifying the custom filter chain. # Read data from the input_file with lzma.open(input_file, \'rb\') as f_in: data = f_in.read() # Compress data with the custom filters compressed_data = lzma.compress(data, format=lzma.FORMAT_XZ, filters=filters) # Write the compressed data to the output_file with open(output_file, \'wb\') as f_out: f_out.write(compressed_data)"},{"question":"**Gaussian Mixture Models and Bayesian Gaussian Mixture Models with scikit-learn** **Objective**: Implement and compare Gaussian Mixture Models (GMM) and Bayesian Gaussian Mixture Models (BGMM) using the scikit-learn library. This task will assess your ability to apply different initialization methods, compare BIC scores, handle hyperparameters, and interpret results. **Problem Statement**: You are provided with a 2D dataset that you need to cluster using two different approaches: 1. `sklearn.mixture.GaussianMixture` 2. `sklearn.mixture.BayesianGaussianMixture` **Requirements**: 1. Load the dataset from a provided CSV file, which has two columns representing the x and y coordinates. 2. Implement the following: - Fit a Gaussian Mixture Model using the `GaussianMixture` class. - Determine the optimal number of components using the Bayesian Information Criterion (BIC). - Fit a Bayesian Gaussian Mixture Model using the `BayesianGaussianMixture` class. - Use the Dirichlet process as the prior and control the number of components using the `weight_concentration_prior`. 3. Compare the clustering results from both models and visualize the clusters and their confidence ellipsoids. 4. Output the optimal number of components for the GMM based on BIC and the number of active components for the BGMM. **Constraints and Evaluation**: - The `GaussianMixture` model should evaluate BIC for a range of components from 1 to 10. - Use `k-means` initialization for the GMM. - For the `BayesianGaussianMixture`, use the `weight_concentration_prior` value of 0.01. - Visualization should include the clustered points and the ellipsoids representing each Gaussian component. - Ensure that the models are evaluated and compared on the basis of cluster purity and computational efficiency. **Input**: - A path to a CSV file containing the dataset (two columns: \'x\' and \'y\'). **Output**: - The optimal number of components for `GaussianMixture` determined by BIC. - The number of active components in the `BayesianGaussianMixture`. - A plot visualizing the clusters and confidence ellipsoids for both models. **Example**: ```python import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture def load_dataset(csv_file): data = pd.read_csv(csv_file) return data[[\'x\', \'y\']].values def fit_gmm(data): min_bic = np.inf best_n_components = 0 for n in range(1, 11): gmm = GaussianMixture(n_components=n, init_params=\'kmeans\') gmm.fit(data) bic = gmm.bic(data) if bic < min_bic: min_bic = bic best_n_components = n final_gmm = GaussianMixture(n_components=best_n_components, init_params=\'kmeans\').fit(data) return final_gmm, best_n_components def fit_bgmm(data, weight_concentration_prior=0.01): bgmm = BayesianGaussianMixture(weight_concentration_prior_type=\'dirichlet_process\', weight_concentration_prior=weight_concentration_prior).fit(data) return bgmm def plot_clusters(data, model, title): plt.scatter(data[:, 0], data[:, 1], s=30, cmap=\'viridis\') means = model.means_ covariances = model.covariances_ for i in range(len(means)): mean = means[i] cov = covariances[i] v, w = np.linalg.eigh(cov) v = 2.0 * np.sqrt(2.0) * np.sqrt(v) u = w[0] / np.linalg.norm(w[0]) angle = np.arctan(u[1] / u[0]) angle = 180 * angle / np.pi ellipse = Ellipse(mean, v[0], v[1], 180 + angle, edgecolor=\'black\', lw=2, fill=False) ax = plt.gca() ax.add_patch(ellipse) plt.title(title) plt.show() # Usage data = load_dataset(\'data.csv\') gmm_model, optimal_components = fit_gmm(data) bgmm_model = fit_bgmm(data) print(f\'Optimal number of components for GMM: {optimal_components}\') print(f\'Number of active components in BGMM: {np.sum(bgmm_model.weights_ > 1e-2)}\') plot_clusters(data, gmm_model, \'Gaussian Mixture Model Clusters\') plot_clusters(data, bgmm_model, \'Bayesian Gaussian Mixture Model Clusters\') ``` **Notes**: - Use appropriate error handling and validate the input CSV file format. - Ensure your code is modular and well-commented.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture from matplotlib.patches import Ellipse def load_dataset(csv_file): data = pd.read_csv(csv_file) return data[[\'x\', \'y\']].values def fit_gmm(data): min_bic = np.inf best_n_components = 0 for n in range(1, 11): gmm = GaussianMixture(n_components=n, init_params=\'kmeans\') gmm.fit(data) bic = gmm.bic(data) if bic < min_bic: min_bic = bic best_n_components = n final_gmm = GaussianMixture(n_components=best_n_components, init_params=\'kmeans\').fit(data) return final_gmm, best_n_components def fit_bgmm(data, weight_concentration_prior=0.01): bgmm = BayesianGaussianMixture(weight_concentration_prior_type=\'dirichlet_process\', weight_concentration_prior=weight_concentration_prior).fit(data) return bgmm def plot_clusters(data, model, title): plt.scatter(data[:, 0], data[:, 1], s=30, cmap=\'viridis\') means = model.means_ covariances = model.covariances_ for i in range(len(means)): mean = means[i] cov = covariances[i] v, w = np.linalg.eigh(cov) v = 2.0 * np.sqrt(2.0) * np.sqrt(v) u = w[0] / np.linalg.norm(w[0]) angle = np.arctan(u[1] / u[0]) angle = 180 * angle / np.pi ellipse = Ellipse(mean, v[0], v[1], 180 + angle, edgecolor=\'black\', lw=2, fill=False) ax = plt.gca() ax.add_patch(ellipse) plt.title(title) plt.show() # Example usage (assuming a CSV file \'data.csv\' is available in the same directory) # data = load_dataset(\'data.csv\') # gmm_model, optimal_components = fit_gmm(data) # bgmm_model = fit_bgmm(data) # print(f\'Optimal number of components for GMM: {optimal_components}\') # print(f\'Number of active components in BGMM: {np.sum(bgmm_model.weights_ > 1e-2)}\') # plot_clusters(data, gmm_model, \'Gaussian Mixture Model Clusters\') # plot_clusters(data, bgmm_model, \'Bayesian Gaussian Mixture Model Clusters\')"},{"question":"# Custom JSON Encoder/Decoder You are given a JSON file containing details about various geometric shapes, including circles and rectangles. Each shape in the JSON file is represented with its type and dimensions, for example: ```json [ { \\"type\\": \\"rectangle\\", \\"width\\": 4, \\"height\\": 5 }, { \\"type\\": \\"circle\\", \\"radius\\": 3 } ] ``` Your task is to implement custom encoding and decoding for these shapes using the `json` module. Specifically, you need to implement the following: 1. **Custom Encoder**: A custom JSON encoder that can serialize Python objects representing circles and rectangles into JSON format. 2. **Custom Decoder**: A custom JSON decoder that can deserialize JSON encoded shapes back into Python objects. # Requirements 1. Define Python classes for `Circle` and `Rectangle` with appropriate attributes. 2. Implement a custom JSON encoder by extending `json.JSONEncoder`. 3. Implement a custom JSON decoder that uses an `object_hook` to convert JSON shape objects into their corresponding Python class instances. # Constraints 1. The JSON input will always be a list of shape objects. 2. Each shape object will always include a `\\"type\\"` field specifying either `\\"rectangle\\"` or `\\"circle\\"`. 3. Rectangular shapes will always have `\\"width\\"` and `\\"height\\"` fields. 4. Circular shapes will always have a `\\"radius\\"` field. # Examples Input JSON File (`shapes.json`): ```json [ { \\"type\\": \\"rectangle\\", \\"width\\": 4, \\"height\\": 5 }, { \\"type\\": \\"circle\\", \\"radius\\": 3 } ] ``` Python Code: ```python import json # Define the classes for Rectangle and Circle class Rectangle: def __init__(self, width, height): self.width = width self.height = height class Circle: def __init__(self, radius): self.radius = radius # Implement the custom JSON Encoder class ShapeEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Rectangle): return { \'type\': \'rectangle\', \'width\': obj.width, \'height\': obj.height } elif isinstance(obj, Circle): return { \'type\': \'circle\', \'radius\': obj.radius } return super().default(obj) # Implement the custom JSON Decoder def shape_decoder(dct): if \'type\' not in dct: return dct if dct[\'type\'] == \'rectangle\': return Rectangle(dct[\'width\'], dct[\'height\']) elif dct[\'type\'] == \'circle\': return Circle(dct[\'radius\']) return dct # Load JSON file with open(\'shapes.json\', \'r\') as file: shapes_json = file.read() # Deserialize JSON to Python objects shapes = json.loads(shapes_json, object_hook=shape_decoder) # Example usage: rectangle = Rectangle(4, 5) circle = Circle(3) # Serialize Python objects to JSON json_data = json.dumps([rectangle, circle], cls=ShapeEncoder, indent=4) print(json_data) # Deserialize back to Python objects decoded_shapes = json.loads(json_data, object_hook=shape_decoder) for shape in decoded_shapes: if isinstance(shape, Rectangle): print(f\\"Rectangle with width: {shape.width}, height: {shape.height}\\") elif isinstance(shape, Circle): print(f\\"Circle with radius: {shape.radius}\\") ``` In this problem, you are required to create a `shapes.json` file, deserialize it into Python objects, and re-serialize back into JSON format. The output JSON should be pretty-printed with an indentation level of 4 spaces.","solution":"import json # Define the classes for Rectangle and Circle class Rectangle: def __init__(self, width, height): self.width = width self.height = height class Circle: def __init__(self, radius): self.radius = radius # Implement the custom JSON Encoder class ShapeEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Rectangle): return { \'type\': \'rectangle\', \'width\': obj.width, \'height\': obj.height } elif isinstance(obj, Circle): return { \'type\': \'circle\', \'radius\': obj.radius } return super().default(obj) # Implement the custom JSON Decoder def shape_decoder(dct): if \'type\' not in dct: return dct if dct[\'type\'] == \'rectangle\': return Rectangle(dct[\'width\'], dct[\'height\']) elif dct[\'type\'] == \'circle\': return Circle(dct[\'radius\']) return dct # Example usage: rectangle = Rectangle(4, 5) circle = Circle(3) # Serialize Python objects to JSON json_data = json.dumps([rectangle, circle], cls=ShapeEncoder, indent=4) print(json_data) # Deserialize back to Python objects decoded_shapes = json.loads(json_data, object_hook=shape_decoder) for shape in decoded_shapes: if isinstance(shape, Rectangle): print(f\\"Rectangle with width: {shape.width}, height: {shape.height}\\") elif isinstance(shape, Circle): print(f\\"Circle with radius: {shape.radius}\\")"},{"question":"Background You have been provided with an interface that can interact with the Unix operating system. One of the critical tasks of a system administrator is to monitor system resource usage and apply certain limitations to ensure system stability and performance. This problem will test your ability to interact with the Unix system using Python by leveraging the `resource` module. Task Implement a function `monitor_and_limit_resources(user: str, max_cpu_time: int, max_memory: int) -> str` that does the following: 1. Monitors the CPU time and memory usage for a given user, identified by their username. 2. If the CPU time exceeds `max_cpu_time` (seconds), it should return a warning message: `\\"Warning: CPU time limit exceeded for user <user>\\"` 3. If the memory usage exceeds `max_memory` (bytes), it should return a warning message: `\\"Warning: Memory limit exceeded for user <user>\\"` 4. It should also set new limits on CPU time and memory usage for processes running under that user. Input - `user`: A string representing the username to monitor. - `max_cpu_time`: An integer value representing the maximum allowed CPU time in seconds. - `max_memory`: An integer value representing the maximum allowed memory usage in bytes. Output - Returns a warning message if a limit is exceeded, otherwise returns `\\"Limits set successfully for user <user>\\"`. Constraints - Assume you have necessary permissions to monitor and set limits for any user on the system. - Use appropriate system calls to gather resource usage and set limits. - Handle any possible exceptions that might occur due to invalid usernames or lack of permissions gracefully. Example ```python print(monitor_and_limit_resources(\\"alice\\", 3600, 524288000)) ``` Output could be: - `\\"Warning: CPU time limit exceeded for user alice\\"` - `\\"Warning: Memory limit exceeded for user alice\\"` - `\\"Limits set successfully for user alice\\"` Use the `resource` and other relevant Unix-specific modules to implement this functionality.","solution":"import resource import psutil import pwd def monitor_and_limit_resources(user: str, max_cpu_time: int, max_memory: int) -> str: Monitors CPU time and memory usage for a given user, and sets new limits if necessary. Parameters: user (str): Username to monitor. max_cpu_time (int): Maximum allowed CPU time in seconds. max_memory (int): Maximum allowed memory usage in bytes. Returns: str: Warning message if a limit is exceeded else success message try: # Get the UID for the given username user_info = pwd.getpwnam(user) uid = user_info.pw_uid # Collect CPU time and memory usage for the user total_cpu_time = 0 total_memory_usage = 0 for proc in psutil.process_iter([\'cpu_times\', \'memory_info\', \'uids\']): if proc.info[\'uids\'].real == uid: total_cpu_time += proc.info[\'cpu_times\'].user + proc.info[\'cpu_times\'].system total_memory_usage += proc.info[\'memory_info\'].rss # Check if limits are exceeded if total_cpu_time > max_cpu_time: return f\\"Warning: CPU time limit exceeded for user {user}\\" if total_memory_usage > max_memory: return f\\"Warning: Memory limit exceeded for user {user}\\" # Set new limits resource.setrlimit(resource.RLIMIT_CPU, (max_cpu_time, max_cpu_time)) resource.setrlimit(resource.RLIMIT_AS, (max_memory, max_memory)) return f\\"Limits set successfully for user {user}\\" except KeyError: return f\\"Error: User {user} does not exist\\" except PermissionError: return f\\"Error: Insufficient permissions to monitor or set limits for user {user}\\" except Exception as e: return f\\"Error: {str(e)}\\""},{"question":"**Objective:** Implement a pair of functions to encode and decode complex data structures using the `xdrlib` module, demonstrating your understanding of the module\'s capabilities. # Problem Statement: You are tasked with creating two functions: `encode_data()` and `decode_data()`. These functions should use the `xdrlib` module to pack and unpack a complex data structure following the XDR standard. 1. `encode_data(data: dict) -> bytes`: - Takes a dictionary representing various data types and structures. - Returns the encoded XDR representation as a byte string. 2. `decode_data(encoded_data: bytes) -> dict`: - Takes a byte string in XDR format (produced by `encode_data`). - Returns the original dictionary after decoding. # Input and Output Format: Input for `encode_data(data: dict) -> bytes`: - The input `data` will be a dictionary with the following structure: ```python { \\"name\\": str, # A person\'s name \\"age\\": int, # A person\'s age \\"height\\": float, # A person\'s height in meters \\"scores\\": [int] # A list of integer scores } ``` Example input: ```python { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"height\\": 1.70, \\"scores\\": [100, 95, 85] } ``` Output for `encode_data(data: dict) -> bytes`: - The output should be a byte string representing the XDR encoded data. Input for `decode_data(encoded_data: bytes) -> dict`: - The input `encoded_data` will be the byte string produced by the `encode_data` function. Output for `decode_data(encoded_data: bytes) -> dict`: - The output should be a dictionary of the same structure as the input to `encode_data`. # Constraints: - Ensure proper handling of various data types as outlined in the dictionary structure. - Use the methods available in `xdrlib.Packer` for encoding and `xdrlib.Unpacker` for decoding. # Performance Requirements: - The encoding and decoding should be efficient and handle the data types as described. # Example: ```python data = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"height\\": 1.70, \\"scores\\": [100, 95, 85] } # Encode the data encoded_data = encode_data(data) # Decode the data back to original form decoded_data = decode_data(encoded_data) assert data == decoded_data ``` # Implementation: Use the documentation provided for `xdrlib` to implement the functions.","solution":"import xdrlib def encode_data(data): Encodes a dictionary using XDR format. :param data: Dictionary to be encoded :return: Encoded byte string p = xdrlib.Packer() # Packing the name p.pack_string(data[\'name\'].encode(\'utf-8\')) # Packing the age p.pack_int(data[\'age\']) # Packing the height p.pack_double(data[\'height\']) # Packing the scores p.pack_int(len(data[\'scores\'])) for score in data[\'scores\']: p.pack_int(score) return p.get_buffer() def decode_data(encoded_data): Decodes a byte string using XDR format. :param encoded_data: Encoded byte string :return: Decoded dictionary u = xdrlib.Unpacker(encoded_data) # Unpacking the name name = u.unpack_string().decode(\'utf-8\') # Unpacking the age age = u.unpack_int() # Unpacking the height height = u.unpack_double() # Unpacking the scores scores_count = u.unpack_int() scores = [] for _ in range(scores_count): scores.append(u.unpack_int()) return { \'name\': name, \'age\': age, \'height\': height, \'scores\': scores }"},{"question":"**Objective:** Demonstrate your comprehension of the `seaborn.objects` module by creating a visualization of the `diamonds` dataset, applying various aggregation functions and transformations. **Task:** 1. Load the `diamonds` dataset using seaborn. 2. Create a bar plot that shows the average carat weight for each clarity category. 3. Modify the plot to show a comparison between the average carat weight and the median carat weight for each clarity category on the same plot. 4. Create another bar plot that shows the interquartile range (IQR) of the carat weight for each clarity category. 5. Add a transformation to the bar plot from step 2 that shows the results separated (dodge) by the \'cut\' category, with different colors for each \'cut\'. **Guidelines:** 1. **Loading Data:** - Use `seaborn.load_dataset` to load the `diamonds` dataset. 2. **Creating Bar Plot with Mean:** - Use `seaborn.objects.Plot` to create a bar plot with the x-axis as `clarity` and y-axis as `carat`. - Use `seaborn.objects.Bar` and `seaborn.objects.Agg` to aggregate the data by the mean. 3. **Adding Median Aggregation:** - Modify the plot to include another bar that shows the median carat weight for each clarity category. - Use `seaborn.objects.Agg(\\"median\\")` to achieve this. 4. **Creating Bar Plot with IQR:** - Create a new bar plot with the x-axis as `clarity` and y-axis as the IQR of `carat`. - Define a custom aggregation function for IQR. 5. **Applying Dodge Transformation:** - Modify the first bar plot to include a dodge transformation using `seaborn.objects.Dodge()` and color the bars by the `cut` category. **Expected Input and Output:** - **Input:** - The code should not require any external input other than loading the `diamonds` dataset. - **Output:** - The output should be two bar plots: 1. One showing the average and median carat weights for each clarity category. 2. One showing the IQR of carat weight for each clarity category. - **Performance Requirements:** - The code should be efficient and utilize seaborn\'s plotting capabilities effectively. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create bar plot with mean plot_mean = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"mean\\")) # Step 3: Add median aggregation to the plot plot_mean.add(so.Bar(), so.Agg(\\"median\\"), color=\\"orange\\") # Step 4: Create bar plot with IQR iqr_func = lambda x: x.quantile(0.75) - x.quantile(0.25) plot_iqr = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(iqr_func)) # Step 5: Apply dodge transformation with color by \'cut\' plot_dodged = plot_mean.add(so.Bar(), so.Agg(\\"mean\\"), color=\\"cut\\").add(so.Dodge()) # Display the plots plot_mean.show() plot_iqr.show() plot_dodged.show() ``` Ensure your code follows the specified guidelines, and your visualizations are accurate and well-labeled.","solution":"import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create bar plot with mean plot_mean = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"mean\\")) # Step 3: Add median aggregation to the plot plot_mean.add(so.Bar(), so.Agg(\\"median\\"), color=\\"orange\\") # Step 4: Create bar plot with IQR iqr_func = lambda x: x.quantile(0.75) - x.quantile(0.25) plot_iqr = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(iqr_func)) # Step 5: Apply dodge transformation with color by \'cut\' plot_dodged = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\", color=\\"cut\\").add(so.Bar(), so.Agg(\\"mean\\"), so.Dodge()) # Display the plots plot_mean.show() plot_iqr.show() plot_dodged.show()"},{"question":"In Python310, creating custom types programmatically and interacting with type objects involves specific functions and structures. This question will assess your ability to create a new custom type and interact with it using the principles outlined in the `PyTypeObject` documentation. # Question: Create and Manipulate Custom Type You are required to create a custom heap-allocated type called `CustomType` using the provided Python310 API functions. Your task includes defining some behavior for this type and interacting with the created type object to demonstrate the understanding of the functions and structures involved. Part 1: Define and Create Custom Type 1. Define a `PyType_Spec` structure for `CustomType` with appropriate values for its name, size, and flags. 2. Implement the necessary `PyType_Slot` structures to define at least one function (e.g., a deallocation function). 3. Use `PyType_FromSpec` to create and initialize the `CustomType`. Part 2: Interact with Custom Type 4. Create an instance of `CustomType` and demonstrate memory allocation using `PyType_GenericAlloc`. 5. Retrieve and print the flags of `CustomType` using `PyType_GetFlags`. 6. Clear the type cache using `PyType_ClearCache`. 7. Use `PyType_Check` to verify that the created instance is of the type `CustomType`. Constraints: - Ensure that your implementation handles potential errors and exceptions gracefully. - You may assume that required headers and libraries for Python310 are available. Input and Output - There is no specific input, but you must demonstrate the creation and manipulation of the type in a main function or equivalent. - Output should be printed to the console, showcasing the flags of the `CustomType` and verification that the instance is of the correct type. Example Below is a brief pseudo-code outline to help you get started: ```python def main(): # Define PyType_Spec for CustomType custom_type_spec = PyType_Spec( name=\\"CustomType\\", basicsize=sizeof(CustomType), itemsize=0, flags=Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HEAPTYPE, slots=[ PyType_Slot(slot=Py_tp_dealloc, pfunc=custom_dealloc), {0, None} # Sentinel value to mark end of slots array. ] ) # Create CustomType using PyType_FromSpec custom_type = PyType_FromSpec(&custom_type_spec) # Create an instance of CustomType instance = PyType_GenericAlloc(custom_type, 1) # Retrieve and print the flags of CustomType flags = PyType_GetFlags(custom_type) print(f\\"CustomType flags: {flags}\\") # Clear the type cache PyType_ClearCache() # Verify the type of instance is_custom_type = PyType_Check(instance) print(f\\"instance is of CustomType: {bool(is_custom_type)}\\") if __name__ == \\"__main__\\": main() ``` Implement the required functions in Python, ensuring you adhere to the Python310 API and demonstrate thorough understanding of type creation and manipulation. Note: - Assume that `sizeof(CustomType)` correctly returns the size in bytes of your custom type. - Custom type functions such as `custom_dealloc` should be implemented assuming C-style function definitions for handlers.","solution":"# Since the actual implementation and usage of Python C API involves C programming and # such an implementation can\'t directly be demonstrated in Python, # we\'ll simulate the logic in Python to conceptualize how custom types would be defined and manipulated. class CustomType: def __init__(self, value): self.value = value print(f\\"CustomType created with value: {value}\\") def __del__(self): print(f\\"CustomType with value {self.value} is being deallocated\\") def create_custom_type(): # Simulate the creation of a custom type print(\\"Creating CustomType...\\") return CustomType def allocate_instance(custom_type, value): # Allocate an instance of the custom type print(\\"Allocating instance of CustomType...\\") instance = custom_type(value) return instance def get_type_flags(custom_type): # Assuming the flags are some representation for the example, in actual C it could involve bitwise flags print(\\"Retrieving flags for CustomType...\\") return \\"Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HEAPTYPE\\" def clear_type_cache(): # Simulated clearing of type cache print(\\"Clearing type cache...\\") def check_type(instance, custom_type): # Check if instance is of type custom_type print(\\"Checking type of instance...\\") return isinstance(instance, custom_type) def main(): # Part 1: Define and Create Custom Type custom_type = create_custom_type() # Part 2: Interact with Custom Type print(\\"n--- Creating and Interacting with CustomType ---\\") instance = allocate_instance(custom_type, 10) flags = get_type_flags(custom_type) print(f\\"CustomType flags: {flags}\\") clear_type_cache() is_custom_type = check_type(instance, custom_type) print(f\\"Instance is of CustomType: {is_custom_type}\\") # Simulating instance deletion to show deallocation function in action del instance if __name__ == \\"__main__\\": main()"},{"question":"# Problem Description You are tasked with designing a custom deque class using Python\'s `collections.deque` class as a base. Your class, named `CustomDeque`, should have the ability to: 1. Add elements to the deque. 2. Remove elements from both ends of the deque. 3. Find the maximum and minimum elements in the deque without removing them. 4. Handle the deque as a circular buffer that overwrites the oldest elements when new elements are added past the maximum capacity. # Requirements 1. **Initialization**: - The `CustomDeque` class should be initialized with an optional `maxlen` parameter. This parameter will set the maximum capacity of the deque. If not provided, the deque should behave as a regular deque without size limitation. 2. **Methods**: - `add_to_front(self, item)`: This method should add an item to the front of the deque. If the deque is full, it should remove the item at the end to make room. - `add_to_end(self, item)`: This method should add an item to the end of the deque. If the deque is full, it should remove the item at the front to make room. - `remove_from_front(self)`: This method should remove and return the item at the front of the deque. If the deque is empty, it should raise an `IndexError`. - `remove_from_end(self)`: This method should remove and return the item at the end of the deque. If the deque is empty, it should raise an `IndexError`. - `get_max(self)`: This method should return the maximum element in the deque without removing it. If the deque is empty, it should raise a `ValueError`. - `get_min(self)`: This method should return the minimum element in the deque without removing it. If the deque is empty, it should raise a `ValueError`. # Input and Output Formats - **Initialization**: ```python dq = CustomDeque(maxlen=<int>) ``` - **Method Calls**: - Add to front: ```python dq.add_to_front(<item>) ``` - Add to end: ```python dq.add_to_end(<item>) ``` - Remove from front: ```python dq.remove_from_front() ``` - Remove from end: ```python dq.remove_from_end() ``` - Get max: ```python dq.get_max() ``` - Get min: ```python dq.get_min() ``` # Constraints - Items added to the deque will be integers. - The `maxlen` parameter, if provided, will be a positive integer greater than zero. # Example ```python # Initialization with max length of 3 dq = CustomDeque(maxlen=3) # Adding elements to the deque dq.add_to_front(2) dq.add_to_front(1) dq.add_to_end(3) dq.add_to_end(4) # From here, deque is [1, 3, 4] (2 is overwritten) # Removing elements front_item = dq.remove_from_front() # Returns 1, deque is now [3, 4] end_item = dq.remove_from_end() # Returns 4, deque is now [3] # Adding more elements dq.add_to_end(5) # deque is now [3, 5] dq.add_to_front(6) # deque is now [6, 3, 5] # Finding maximum and minimum elements maximum = dq.get_max() # Returns 6 minimum = dq.get_min() # Returns 3 ```","solution":"from collections import deque class CustomDeque: def __init__(self, maxlen=None): Initialize the CustomDeque with an optional maxlen parameter. If maxlen is not provided, it behaves as an unbounded deque. self.deque = deque(maxlen=maxlen) def add_to_front(self, item): Add an item to the front of the deque. Removes the item at the end if the deque is full. self.deque.appendleft(item) def add_to_end(self, item): Add an item to the end of the deque. Removes the item at the front if the deque is full. self.deque.append(item) def remove_from_front(self): Remove and return the item from the front of the deque. If the deque is empty, raise an IndexError. if not self.deque: raise IndexError(\\"remove_from_front(): deque is empty\\") return self.deque.popleft() def remove_from_end(self): Remove and return the item from the end of the deque. If the deque is empty, raise an IndexError. if not self.deque: raise IndexError(\\"remove_from_end(): deque is empty\\") return self.deque.pop() def get_max(self): Return the maximum element in the deque without removing it. If the deque is empty, raise a ValueError. if not self.deque: raise ValueError(\\"get_max(): deque is empty\\") return max(self.deque) def get_min(self): Return the minimum element in the deque without removing it. If the deque is empty, raise a ValueError. if not self.deque: raise ValueError(\\"get_min(): deque is empty\\") return min(self.deque)"},{"question":"# XML Document Manipulation using xml.dom **Objective**: Implement a Python function to manipulate an XML document using the `xml.dom` module. **Task**: Write a function `create_and_modify_xml()` that performs the following actions: 1. Creates a new XML document. 2. Adds a root element with a given tag name. 3. Appends multiple child elements to the root, each with a given tag name and a dictionary of attributes. 4. Modifies the text content of a specific child element identified by its tag name. 5. Removes a child element identified by its tag name from the root element. 6. Normalizes the document to combine adjacent text nodes. **Function Signature**: ```python def create_and_modify_xml(root_tag: str, children: list, modify_tag: str, new_text: str, remove_tag: str) -> str: Create and modify an XML document. Parameters: - root_tag (str): The tag name of the root element. - children (list): A list where each element is a dict with \'tag\' (str) and \'attributes\' (dict). - modify_tag (str): The tag name of the child element to modify. - new_text (str): The new text content for the modify_tag element. - remove_tag (str): The tag name of the child element to remove. Returns: - str: The resulting XML document as a string. ``` **Details**: - Use `xml.dom.getDOMImplementation` to create a `DOMImplementation` object and use it to create a `Document` object. - Create the root element using `Document.createElement` and set its tag name. - Append child elements to the root using `Document.createElement` and set their attributes using `Element.setAttribute`. - Locate the child element to modify by its tag name using `Document.getElementsByTagName`, and update its text content. - Locate and remove the child element by its tag name using `Element.removeAttribute`. - Normalize the document using `Node.normalize`. - Return the entire XML document as a string using `Document.toxml()`. **Example**: ```python children = [ {\'tag\': \'child1\', \'attributes\': {\'id\': \'001\', \'name\': \'first\'}}, {\'tag\': \'child2\', \'attributes\': {\'id\': \'002\', \'name\': \'second\'}}, ] xml_string = create_and_modify_xml(\'root\', children, \'child1\', \'New Text Content\', \'child2\') print(xml_string) ``` **Expected Output**: ```xml <root> <child1 id=\\"001\\" name=\\"first\\">New Text Content</child1> </root> ``` **Constraints**: - Assume that tag names are unique among siblings. - Handle cases where the specified tag for modification or removal might not exist gracefully. - Ensure proper handling of namespaces if provided in the attributes (optional for this task). This task will assess students\' understanding of: - Creating and manipulating XML documents using the DOM API. - Navigating and modifying a document tree. - Handling exceptions and edge cases.","solution":"from xml.dom.minidom import getDOMImplementation def create_and_modify_xml(root_tag: str, children: list, modify_tag: str, new_text: str, remove_tag: str) -> str: Create and modify an XML document. Parameters: - root_tag (str): The tag name of the root element. - children (list): A list where each element is a dict with \'tag\' (str) and \'attributes\' (dict). - modify_tag (str): The tag name of the child element to modify. - new_text (str): The new text content for the modify_tag element. - remove_tag (str): The tag name of the child element to remove. Returns: - str: The resulting XML document as a string. impl = getDOMImplementation() dom = impl.createDocument(None, root_tag, None) root = dom.documentElement # Add child elements for child in children: child_element = dom.createElement(child[\'tag\']) for attr, value in child[\'attributes\'].items(): child_element.setAttribute(attr, value) root.appendChild(child_element) # Modify the text content of the specified element modify_elements = dom.getElementsByTagName(modify_tag) if modify_elements: modify_elements[0].appendChild(dom.createTextNode(new_text)) # Remove the specified element remove_elements = dom.getElementsByTagName(remove_tag) if remove_elements: root.removeChild(remove_elements[0]) # Normalize the document root.normalize() return dom.toxml()"},{"question":"# XML Parsing with `xml.parsers.expat` You are required to write a Python function that uses the `xml.parsers.expat` module to parse an XML document. Your task is to extract specific information from the XML and handle errors appropriately. Problem Statement Write a function `parse_and_extract_data(xml_string: str) -> dict` that takes an XML string as input and returns a dictionary with the following key-value pairs: - `elements`: a list of tuples, each containing the element name and a dictionary of its attributes, in the order they appear in the document. - `texts`: a list of text contents in the order they appear in the document. - `error`: an error message string if an error occurs during parsing (otherwise this key should not be present in the output dictionary). If the XML string is invalid or another parsing error occurs, your function should catch the exception and return a dictionary containing only the `error` key. Function Signature ```python def parse_and_extract_data(xml_string: str) -> dict: ``` Example ```python xml_input = <?xml version=\\"1.0\\"?> <parent id=\\"top\\"><child1 name=\\"paul\\">Text goes here</child1> <child2 name=\\"fred\\">More text</child2> </parent> output = parse_and_extract_data(xml_input) print(output) ``` Expected output should be: ```python { \\"elements\\": [ (\\"parent\\", {\\"id\\": \\"top\\"}), (\\"child1\\", {\\"name\\": \\"paul\\"}), (\\"child2\\", {\\"name\\": \\"fred\\"}) ], \\"texts\\": [ \\"Text goes here\\", \\"More text\\" ] } ``` Constraints - You must use the handlers provided by `xml.parsers.expat` to handle start elements, end elements, and character data. - Handle errors appropriately by using the `ExpatError` exception. - Assume the input XML string will fit into memory and be reasonably sized. Notes - Make sure to set appropriate handlers for the XML parser. - The function should be robust and handle different structures of XML documents. - Include meaningful docstrings and comments in your code.","solution":"import xml.parsers.expat def parse_and_extract_data(xml_string: str) -> dict: Parses the given XML string and extracts elements and text. Arguments: xml_string -- a string containing the XML data Returns: A dictionary containing: - \'elements\': a list of tuples with element name and its attributes dictionary, - \'texts\': a list of textual content of the elements in the order they appear, - \'error\': optional error message string if an error occurs during parsing. result = { \\"elements\\": [], \\"texts\\": [] } def start_element(name, attrs): result[\'elements\'].append((name, attrs)) def end_element(name): pass def char_data(data): if data.strip(): result[\'texts\'].append(data.strip()) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_string, True) except xml.parsers.expat.ExpatError as e: return {\\"error\\": str(e)} return result"},{"question":"You are required to write a function that trains a Ridge regression model using Array API-compatible inputs and outputs the transformed array of predictions in the same type and device as the input data. Objective - Implement a function `train_and_predict_ridge` using scikit-learn\'s Ridge regression, allowing it to handle Array API-compatible inputs such as from CuPy or PyTorch. Function Signature ```python def train_and_predict_ridge(X, y): Trains a Ridge regression model on the given input data and returns the predictions. Parameters: X (array): Array API-compatible input data of shape (n_samples, n_features). y (array): Array API-compatible target data of shape (n_samples,). Returns: predictions (array): Array API-compatible predictions from the regression model. ``` Input - `X`: A two-dimensional array of shape `(n_samples, n_features)` compatible with Array API (e.g., a CuPy array or a PyTorch tensor). - `y`: A one-dimensional array of shape `(n_samples,)` compatible with Array API (e.g., a CuPy array or a PyTorch tensor). Output - `predictions`: A one-dimensional array of shape `(n_samples,)` containing the model predictions, returned in the same array type and device as the input data. Constraints - The function should handle the arrays using the provided Array API specifications. - Ensure that the predictions are on the same device as the input array. - You are allowed to use scikit-learn\'s utility functions as described in the documentation (e.g., `config_context`, `_estimator_with_converted_arrays`). Example ```python # Assuming proper environment setup and necessary installations import cupy as cp from sklearn.datasets import make_regression # Generate sample data X_np, y_np = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42) X_cp = cp.asarray(X_np) y_cp = cp.asarray(y_np) predictions = train_and_predict_ridge(X_cp, y_cp) print(type(predictions)) # <class \'cupy.ndarray\'> ``` Notes - Make sure to handle any setup required for `SCIPY_ARRAY_API` before running your implementation. - Your implementation should manage the training and prediction process using Scikit-learns Ridge regression, ensuring compatibility with array types provided by CuPy or PyTorch. - Ensure that the function does not assume a specific type of input array and can automatically adapt to the input type. This assessment will evaluate your ability to work with advanced features of scikit-learn, including handling different types of array inputs and utilizing GPU acceleration.","solution":"import numpy as np import sklearn from sklearn.linear_model import Ridge def train_and_predict_ridge(X, y): Trains a Ridge regression model on the given input data and returns the predictions. Parameters: X (array): Array API-compatible input data of shape (n_samples, n_features). y (array): Array API-compatible target data of shape (n_samples,). Returns: predictions (array): Array API-compatible predictions from the regression model. # Check if the input is CuPy array if \'cupy\' in str(X.__class__): import cupy as cp # Transfer data to cpu for sklearn X_cpu = cp.asnumpy(X) y_cpu = cp.asnumpy(y) # Train the ridge regression model using CPU data model = Ridge() model.fit(X_cpu, y_cpu) # Predict using the trained model and then convert the predictions back to CuPy predictions_cpu = model.predict(X_cpu) predictions = cp.asarray(predictions_cpu) # Check if the input is a PyTorch tensor elif \'torch\' in str(X.__class__): import torch # Transfer data to cpu for sklearn X_cpu = X.cpu().numpy() y_cpu = y.cpu().numpy() # Train the ridge regression model using CPU data model = Ridge() model.fit(X_cpu, y_cpu) # Predict using the trained model and then convert the predictions back to PyTorch tensor predictions_cpu = model.predict(X_cpu) predictions = torch.from_numpy(predictions_cpu).to(X.device) else: # Assume inputs are numpy arrays model = Ridge() model.fit(X, y) predictions = model.predict(X) return predictions"},{"question":"# Filename Matcher Using fnmatch You are tasked with creating a Python function that takes a list of filenames and a list of patterns, and returns a dictionary. The dictionary should have the patterns as keys and lists of filenames that match each pattern as values. Function Signature ```python import fnmatch def match_filenames(filenames: list, patterns: list) -> dict: Function to match filenames against multiple patterns. Parameters: filenames (list): A list of filenames (strings) to be matched. patterns (list): A list of patterns (strings) to match the filenames against. Returns: dict: A dictionary with patterns as keys and list of matching filenames as values. pass ``` Input 1. `filenames (list)`: A list of names to be checked against the patterns. Example: `[\'file1.txt\', \'file2.py\', \'test1.txt\', \'script.sh\', \'data.csv\']` 2. `patterns (list)`: A list of shell-style patterns. Example: `[\'*.txt\', \'*.py\', \'*.sh\']` Output - A dictionary where each key is a pattern from the input list, and each value is a list of filenames that match that pattern. Constraints and Limitations - Assume all filenames and patterns provided are non-empty strings. - Filenames and patterns only contain standard alphanumeric characters and Unix shell-style wildcards (`*`, `?`, `[seq]`, `[!seq]`). Example ```python filenames = [\'file1.txt\', \'file2.py\', \'test1.txt\', \'script.sh\', \'data.csv\'] patterns = [\'*.txt\', \'*.py\', \'*.sh\'] print(match_filenames(filenames, patterns)) # Output: {\'*.txt\': [\'file1.txt\', \'test1.txt\'], \'*.py\': [\'file2.py\'], \'*.sh\': [\'script.sh\']} ``` Notes - Use the `fnmatch` module functions as needed. - The function should efficiently handle the matching process, leveraging the functionality provided by `fnmatch`.","solution":"import fnmatch def match_filenames(filenames, patterns): Function to match filenames against multiple patterns. Parameters: filenames (list): A list of filenames (strings) to be matched. patterns (list): A list of patterns (strings) to match the filenames against. Returns: dict: A dictionary with patterns as keys and list of matching filenames as values. result = {} for pattern in patterns: result[pattern] = [filename for filename in filenames if fnmatch.fnmatch(filename, pattern)] return result"},{"question":"Context: You have been tasked with creating a session logger for an application that keeps track of user activities. Each user session should log activities into a file `user_session.log` when the program terminates. Utilize the `atexit` module to ensure that the log is saved correctly even if the program ends unexpectedly. Objective: Write a Python class `SessionLogger` that uses the `atexit` module to log user activities into a file when the program terminates. The `SessionLogger` class should provide methods to add activities and clear the current log. The activities should be saved to the file upon program termination, in the reverse order they were added. Requirements: 1. The `SessionLogger` should save the log to a file named `user_session.log`. 2. Implement the method `add_activity(self, activity: str)` to add an activity to the log. 3. Implement the method `clear_activities(self)` to clear all current activities. 4. Activities should be saved in the reverse order in which they were added. 5. Implement the `save_log` functionality using the `atexit` module to ensure it is executed upon normal interpreter termination. Constraints: - You must use the `atexit` module to register the `save_log` function. - Activities may contain any printable characters. - No external libraries should be used, aside from the `atexit` module and standard file operations. Example Usage: ```python from session_logger import SessionLogger logger = SessionLogger() logger.add_activity(\\"User logged in\\") logger.add_activity(\\"User viewed profile\\") logger.add_activity(\\"User logged out\\") # When the program terminates, `user_session.log` should contain: # User logged out # User viewed profile # User logged in ``` Implementation: ```python # session_logger.py import atexit class SessionLogger: def __init__(self): self.activities = [] atexit.register(self.save_log) def add_activity(self, activity: str): self.activities.append(activity) def clear_activities(self): self.activities.clear() def save_log(self): with open(\'user_session.log\', \'w\') as log_file: for activity in reversed(self.activities): log_file.write(activity + \'n\') ``` Ensure the implementation meets requirements, and provide any necessary test cases to demonstrate the functionality.","solution":"import atexit class SessionLogger: def __init__(self): self.activities = [] atexit.register(self.save_log) def add_activity(self, activity: str): self.activities.append(activity) def clear_activities(self): self.activities.clear() def save_log(self): with open(\'user_session.log\', \'w\') as log_file: for activity in reversed(self.activities): log_file.write(activity + \'n\')"},{"question":"# **Problem Statement:** You are tasked with writing a set of functions to handle HTTP requests and appropriately manage errors using the `urllib.error` module in Python. **Function 1: make_request** Write a function `make_request(url: str) -> str` that performs an HTTP GET request to the given URL and returns the response content as a string. If the request encounters an error, handle it as follows: - If the URL is incorrect or the server is not reachable, raise a `URLError` and print \\"Failed to reach the server. Reason: [reason]\\" where [reason] is the detailed reason provided by the exception. - If an HTTP error occurs (status codes 400 or higher), raise an `HTTPError` and print \\"HTTP Error [code]: [reason]\\" where [code] is the HTTP status code and [reason] is the reason given by the server. - If the content retrieved is less than expected (as indicated by the `Content-Length` header), raise a `ContentTooShortError` and print \\"Content too short. Only [length] bytes downloaded.\\" where [length] is the length of the downloaded content. **Function 2: download_content** Write another function `download_content(url: str, expected_length: int) -> str` that makes a request using the `make_request` function and checks the length of the content against the `expected_length` parameter. If the length of the content is less than `expected_length`, raise a `ContentTooShortError`. **Input:** 1. `url`: A string representing the URL to make the request to. 2. `expected_length`: An integer representing the expected length of the content to be downloaded (only required for `download_content` function). **Output:** - `make_request`: A string containing the response content from the requested URL. - `download_content`: A string containing the response content if it meets the expected length. **Constraints:** - You must handle exceptions using classes from the `urllib.error` module. - Ensure that the error messages printed are clear and indicate the specific error encountered. - Assume the URL provided is of a valid format but may still be unreachable or respond with errors. **Example:** ```python # Example usage of make_request try: content = make_request(\\"https://example.com\\") print(content) except Exception as e: print(e) # Example usage of download_content try: content = download_content(\\"https://example.com\\", expected_length=1024) print(content) except Exception as e: print(e) ``` **Note:** This problem will test your understanding of handling custom exceptions, making HTTP requests, and checking response headers and content lengths.","solution":"import urllib.request import urllib.error def make_request(url: str) -> str: try: response = urllib.request.urlopen(url) content = response.read() return content.decode(\'utf-8\') except urllib.error.HTTPError as e: print(f\\"HTTP Error {e.code}: {e.reason}\\") raise except urllib.error.URLError as e: print(f\\"Failed to reach the server. Reason: {e.reason}\\") raise def download_content(url: str, expected_length: int) -> str: content = make_request(url) length = len(content) if length < expected_length: raise urllib.error.ContentTooShortError( \\"Content too short.\\", content ) return content"},{"question":"Objective To assess your understanding of computational performance optimization in scikit-learn, specifically prediction latency and throughput. Question Write a function `benchmark_model_performance` that: 1. Generates synthetic data with varying numbers of features and levels of sparsity. 2. Trains scikit-learn models on this synthetic data. 3. Measures and returns the prediction latency and throughput for each trained model. You will use the following scikit-learn models to benchmark: - `SGDClassifier` - `RandomForestClassifier` - `NuSVR` The number of features should range from 10 to 10,000, and the sparsity levels should include both dense and sparse representations. Input - `n_samples`: int - The number of samples for the synthetic dataset. - `features_range`: List[int] - A list specifying the different numbers of features to use for synthetic data. - `sparsity_levels`: List[float] - A list specifying the different sparsity levels (percentages of zero entries) for the synthetic data. Output - A dictionary with the following format: ```python { \'SGDClassifier\': { (n_features, sparsity): {\'latency\': ..., \'throughput\': ...}, ... }, \'RandomForestClassifier\': { (n_features, sparsity): {\'latency\': ..., \'throughput\': ...}, ... }, \'NuSVR\': { (n_features, sparsity): {\'latency\': ..., \'throughput\': ...}, ... } } ``` Where `n_features` is an element from `features_range`, and `sparsity` is an element from `sparsity_levels`. Constraints - Ensure that you use bulk predictions (making predictions on all samples at once) for the latency and throughput calculations. Instructions 1. Generate synthetic data using `make_classification` or `make_regression` functions from `sklearn.datasets`. 2. Implement a helper function to add sparsity to the dataset if required. 3. Train each of the specified models on the generated data. 4. Measure and record prediction latency and throughput for each model. 5. Return the results in the specified format. Example ```python def benchmark_model_performance(n_samples, features_range, sparsity_levels): from sklearn.datasets import make_classification, make_regression from sklearn.linear_model import SGDClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.svm import NuSVR import numpy as np import time def generate_data(n_samples, n_features, sparsity): X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_features // 2, n_redundant=0) if sparsity < 1.0: mask = np.random.choice([False, True], size=X.shape, p=[sparsity, 1-sparsity]) X[mask] = 0 return X, y models = { \'SGDClassifier\': SGDClassifier(), \'RandomForestClassifier\': RandomForestClassifier(), \'NuSVR\': NuSVR() } performance_results = {} for model_name, model in models.items(): performance_results[model_name] = {} for n_features in features_range: for sparsity in sparsity_levels: X, y = generate_data(n_samples, n_features, sparsity) model.fit(X, y) start_time = time.time() model.predict(X) latency = time.time() - start_time start_time = time.time() model.predict(X) throughput = len(X) / (time.time() - start_time) performance_results[model_name][(n_features, sparsity)] = { \'latency\': latency, \'throughput\': throughput } return performance_results # Example usage: benchmark_results = benchmark_model_performance(n_samples=1000, features_range=[10, 100, 1000], sparsity_levels=[0.9, 0.5, 0.1]) print(benchmark_results) ``` Additional Information By completing this question, you will demonstrate your understanding of: - Generating synthetic datasets with varying feature numbers and sparsity levels. - Training different types of models in scikit-learn. - Measuring computational performance metrics such as prediction latency and throughput. - Configuring models and data representations to optimize performance in scikit-learn.","solution":"def benchmark_model_performance(n_samples, features_range, sparsity_levels): from sklearn.datasets import make_classification, make_regression from sklearn.linear_model import SGDClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.svm import NuSVR import numpy as np import time def generate_data(n_samples, n_features, sparsity, task=\'classification\'): if task == \'classification\': X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_features // 2, n_redundant=0) else: X, y = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_features // 2, n_targets=1) if sparsity < 1.0: mask = np.random.choice([False, True], size=X.shape, p=[sparsity, 1-sparsity]) X[mask] = 0 return X, y models = { \'SGDClassifier\': (SGDClassifier(), \'classification\'), \'RandomForestClassifier\': (RandomForestClassifier(), \'classification\'), \'NuSVR\': (NuSVR(), \'regression\'), } performance_results = {} for model_name, (model, task) in models.items(): performance_results[model_name] = {} for n_features in features_range: for sparsity in sparsity_levels: X, y = generate_data(n_samples, n_features, sparsity, task) model.fit(X, y) start_time = time.time() model.predict(X) latency = time.time() - start_time start_time = time.time() model.predict(X) throughput = len(X) / (time.time() - start_time) performance_results[model_name][(n_features, sparsity)] = { \'latency\': latency, \'throughput\': throughput, } return performance_results"},{"question":"# Question: Optimize Decision Threshold for Tumor Detection You are tasked with creating a decision support system for predicting cancer diagnoses using patient data. To ensure high recall (identifying all patients with cancer), you will need to implement a classifier that optimizes the decision threshold after training. **Objective:** The objective is to train a model on the provided dataset, then tune the decision threshold to maximize a specific score using cross-validation in scikit-learn. **Dataset:** You may assume the dataset (`X` and `y`) contains features and labels similar to the example given. In `y`, `1` indicates the presence of cancer and `0` indicates its absence. **Task Outline:** 1. Train a `LogisticRegression` model on the provided dataset. 2. Use `TunedThresholdClassifierCV` to optimize the decision threshold to maximize recall. 3. Manually test and output the predictions on a subset using the tuned threshold. **Constraints and Requirements:** 1. **Input Format:** - `X` : numpy array of shape (n_samples, n_features) - `y` : numpy array of shape (n_samples,) 2. **Output Format:** - Optimized decision threshold value. - Predictions on a sample subset (`X[:10]`). 3. **Performance Constraints:** - Ensure that the recall metric is prioritized during tuning. **Example Usage:** ```python from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score from sklearn.datasets import make_classification def optimize_threshold(X, y): # Define the base model base_model = LogisticRegression() # Define recall scorer pos_label = 1 recall_scorer = make_scorer(recall_score, pos_label=pos_label) # Initialize and fit TunedThresholdClassifierCV model = TunedThresholdClassifierCV(base_model, scoring=recall_scorer) model.fit(X, y) # Fetch the optimized threshold optimized_threshold = model.best_threshold_ # Make predictions using the tuned model predictions = model.predict(X[:10]) return optimized_threshold, predictions # Example input X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) optimized_threshold, predictions = optimize_threshold(X, y) print(\\"Optimized Threshold:\\", optimized_threshold) print(\\"Predictions on Sample Subset:\\", predictions) ``` **Note:** The function `optimize_threshold` should be the focus of your implementation. Follow the steps and constraints mentioned above while ensuring code clarity and efficiency.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.metrics import make_scorer, recall_score, precision_recall_curve from sklearn.model_selection import StratifiedKFold from sklearn.datasets import make_classification def optimize_threshold(X, y): # Define the base model base_model = LogisticRegression(solver=\'liblinear\') # Define recall scorer pos_label = 1 recall_scorer = make_scorer(recall_score, pos_label=pos_label) # Cross-validation setup kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0) scores = [] thresholds = [] for train_idx, val_idx in kfold.split(X, y): X_train, X_val = X[train_idx], X[val_idx] y_train, y_val = y[train_idx], y[val_idx] # Fit the model base_model.fit(X_train, y_train) # Predict probabilities y_proba = base_model.predict_proba(X_val)[:, 1] # Determine precision-recall curve and best threshold for recall precision, recall, threshold = precision_recall_curve(y_val, y_proba) f1_scores = 2 * (precision * recall) / (precision + recall) optimal_idx = np.argmax(f1_scores) optimal_threshold = threshold[optimal_idx] thresholds.append(optimal_threshold) scores.append(f1_scores[optimal_idx]) # Average threshold over folds best_threshold = np.mean(thresholds) # Train final model on full dataset base_model.fit(X, y) # Predict using the optimized threshold probabilities = base_model.predict_proba(X[:10])[:, 1] predictions = (probabilities >= best_threshold).astype(int) return best_threshold, predictions # Example usage X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) optimized_threshold, predictions = optimize_threshold(X, y) print(\\"Optimized Threshold:\\", optimized_threshold) print(\\"Predictions on Sample Subset:\\", predictions)"},{"question":"Context: You are tasked with creating a utility that compresses and decompresses directories using various algorithms provided by Python\'s standard library. The utility must support the `gzip`, `bz2`, and `lzma` compression algorithms. Additionally, it must ensure that the integrity of the data is maintained throughout the process. Problem Statement: Write a Python program that: 1. Compresses a specified directory into a single file using the specified compression algorithm. 2. Decompresses the file back into its original directory structure, ensuring data integrity. Requirements: - The program should be a command-line utility that accepts the following arguments: - `--mode {compress, decompress}`: Defines whether to compress or decompress. - `--algorithm {gzip, bz2, lzma}`: The compression algorithm to use. - `--source PATH`: The source directory (for compression) or file (for decompression). - `--destination PATH`: The destination file (for compression) or directory (for decompression). - The compression should recursively include all files and subfolders in the specified directory. - Ensure that the integrity of the decompressed data matches that of the original data. Constraints: - You may assume the source directory exists and contains readable files. - Your program should handle file system errors gracefully (e.g., file not found, permission errors). Example Usage: ``` # To compress a directory python compress_util.py --mode compress --algorithm gzip --source /path/to/source --destination /path/to/destination.gz # To decompress a file python compress_util.py --mode decompress --algorithm gzip --source /path/to/destination.gz --destination /path/to/decompressed ``` Implementation Notes: - Use appropriate classes and functions to encapsulate the functionality. - Provide examples and usage of the utility in the code comments. - Consider edge cases such as large files, deeply nested directories, and permission restrictions. Submission: Submit a single Python script named `compress_util.py` that implements the described functionality.","solution":"import os import tarfile import argparse import gzip import bz2 import lzma import shutil ALGORITHMS = { \'gzip\': gzip.open, \'bz2\': bz2.open, \'lzma\': lzma.open, } def compress_directory(algorithm, source_dir, destination_file): with tarfile.open(destination_file, \'w\') as tarf: tarf.add(source_dir, arcname=os.path.basename(source_dir)) with open(destination_file, \'rb\') as f_in: with ALGORITHMS[algorithm](destination_file + \'.\' + algorithm, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) os.remove(destination_file) def decompress_file(algorithm, source_file, destination_dir): temp_tar_file = source_file.replace(\'.\' + algorithm, \'\') with ALGORITHMS[algorithm](source_file, \'rb\') as f_in: with open(temp_tar_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) with tarfile.open(temp_tar_file, \'r\') as tarf: tarf.extractall(path=destination_dir) os.remove(temp_tar_file) def main(): parser = argparse.ArgumentParser(description=\'Compress or Decompress directories.\') parser.add_argument(\'--mode\', choices=[\'compress\', \'decompress\'], required=True, help=\\"Mode: \'compress\' or \'decompress\'\\") parser.add_argument(\'--algorithm\', choices=ALGORITHMS.keys(), required=True, help=\\"Algorithm: \'gzip\', \'bz2\', or \'lzma\'\\") parser.add_argument(\'--source\', required=True, help=\\"Source directory or file\\") parser.add_argument(\'--destination\', required=True, help=\\"Destination file or directory\\") args = parser.parse_args() if args.mode == \'compress\': compress_directory(args.algorithm, args.source, args.destination) elif args.mode == \'decompress\': decompress_file(args.algorithm, args.source, args.destination) if __name__ == \'__main__\': main()"},{"question":"**PyTorch Coding Assessment** **Objective:** In this assessment, you will demonstrate your understanding of PyTorch\'s `torch.export` functionality. You are required to create a simple neural network model, trace its execution using `torch.export`, handle dynamic input shapes, and serialize the exported program. **Task:** 1. **Define a PyTorch Model**: Create a `torch.nn.Module` named `SimpleNet` with the following layers: - A Conv2D layer with input channels 1, output channels 16, kernel size 3, and padding 1. - A ReLU activation function. - A MaxPool2d layer with a kernel size of 2. - A Linear layer that takes the flattened output and maps it to 10 output classes. 2. **AOT Tracing with torch.export**: Trace the `SimpleNet` model using `torch.export.export` with an example input tensor. Specify that the first dimension (batch size) of the input tensor should be dynamic. 3. **Serialization**: Serialize the exported program into a file named `simple_net_export.pt2`, then load the program from this file and print the loaded program. **Constraints:** - Use PyTorch version 2.3 or later. - The batch size of the input should be dynamic. - Follow the structure and methods described in the documentation provided. **Input:** - The input to the model will be a tensor of shape `[batch_size, 1, 28, 28]`. **Output:** - A printed representation of the loaded exported program. **Expected Solution Structure:** ```python import torch from torch.export import export, save, load, Dim # Step 1: Define the SimpleNet model class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv = torch.nn.Conv2d(1, 16, 3, padding=1) self.relu = torch.nn.ReLU() self.maxpool = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(16 * 14 * 14, 10) def forward(self, x): x = self.conv(x) x = self.relu(x) x = self.maxpool(x) x = x.view(x.size(0), -1) x = self.fc(x) return x # Step 2: AOT Tracing with torch.export example_input = torch.randn(2, 1, 28, 28) # An example input tensor batch = Dim(\\"batch\\") dynamic_shapes = {\\"x\\": {0: batch}} model = SimpleNet() # Export the model exported_program = export(model, args=example_input, dynamic_shapes=dynamic_shapes) # Step 3: Serialization file_name = \'simple_net_export.pt2\' save(exported_program, file_name) # Load the program from the file and print it loaded_program = load(file_name) print(loaded_program) ``` Implement the steps described above and run your code to ensure it correctly performs the required operations. **Note**: All necessary imports and model definitions have been provided. Focus on completing the functionality for tracing and serialization as per the specifications.","solution":"import torch from torch.export import export, save, load, Dim # Step 1: Define the SimpleNet model class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.conv = torch.nn.Conv2d(1, 16, 3, padding=1) self.relu = torch.nn.ReLU() self.maxpool = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(16 * 14 * 14, 10) def forward(self, x): x = self.conv(x) x = self.relu(x) x = self.maxpool(x) x = x.view(x.size(0), -1) x = self.fc(x) return x # Step 2: AOT Tracing with torch.export example_input = torch.randn(2, 1, 28, 28) # An example input tensor batch = Dim(\\"batch\\") dynamic_shapes = {\\"x\\": {0: batch}} model = SimpleNet() # Export the model exported_program = export(model, args=(example_input,), dynamic_shapes=dynamic_shapes) # Step 3: Serialization file_name = \'simple_net_export.pt2\' save(exported_program, file_name) # Load the program from the file and print it loaded_program = load(file_name) print(loaded_program)"},{"question":"# Unsupervised Learning with K-Means Clustering Objective Implement a function to perform K-means clustering on a given dataset using scikit-learn\'s `KMeans` class. Function Signature ```python def perform_kmeans_clustering(data, n_clusters, random_state=42): Perform K-means clustering on the given dataset. Parameters: data (ndarray): A 2D numpy array of shape (n_samples, n_features) containing the data. n_clusters (int): The number of clusters to form. random_state (int): Random state seed for reproducibility (default is 42). Returns: ndarray: A 1D numpy array of shape (n_samples,) containing the cluster labels for each point. pass ``` Input - `data`: A 2D numpy array of shape `(n_samples, n_features)`. Each row represents a sample, and each column represents a feature. - `n_clusters`: An integer representing the number of clusters to form. - `random_state`: An integer seed for the random number generator to ensure reproducibility (default value is 42). Output - A 1D numpy array of shape `(n_samples,)` containing the cluster labels for each point. Constraints - The input `data` will always be a non-empty 2D numpy array. - The `n_clusters` will always be a positive integer less than the number of samples. Example Usage ```python import numpy as np # Sample data data = np.array([ [1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0], [8.0, 2.0], [10.0, 2.0], [9.0, 3.0] ]) # Perform K-means clustering labels = perform_kmeans_clustering(data, 3, random_state=42) print(labels) # Example Output: array([0, 0, 1, 1, 0, 1, 2, 2, 2]) ``` Notes - You should use the `KMeans` class from the `sklearn.cluster` module to perform clustering. - Ensure that the `random_state` parameter is used in the `KMeans` initialization to ensure reproducibility.","solution":"from sklearn.cluster import KMeans import numpy as np def perform_kmeans_clustering(data, n_clusters, random_state=42): Perform K-means clustering on the given dataset. Parameters: data (ndarray): A 2D numpy array of shape (n_samples, n_features) containing the data. n_clusters (int): The number of clusters to form. random_state (int): Random state seed for reproducibility (default is 42). Returns: ndarray: A 1D numpy array of shape (n_samples,) containing the cluster labels for each point. kmeans = KMeans(n_clusters=n_clusters, random_state=random_state) kmeans.fit(data) return kmeans.labels_"},{"question":"**Coding Assessment Question** **Title:** Implement and Test Fibonacci Sequence Generator Using `doctest` **Objective:** The objective of this task is to evaluate your ability to implement a Python function to generate Fibonacci sequences and validate it using `doctest` embedded within docstrings. **Problem Statement:** You are required to implement a function `fibonacci(n)` that generates the first `n` numbers in the Fibonacci sequence. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, starting from 0 and 1. Additionally, you need to provide appropriate docstring examples for the function that will be tested using the `doctest` module. **Function Specifications:** - **Function Name:** `fibonacci` - **Input:** An integer `n` (0 <= n <= 50), where `n` denotes the number of elements in the Fibonacci sequence to generate. - **Output:** A list of the first `n` Fibonacci numbers. **Constraints:** 1. The function should handle the edge case where `n` is 0 by returning an empty list. 2. The function should handle edge cases for small values of `n`. 3. For large values of `n` up to 50, the function should efficiently compute the sequence without unnecessary recomputation. **Performance Requirements:** - The implementation should be efficient, such that the Fibonacci sequence calculation is optimized for `n` up to 50. **Your Task:** 1. **Implement the Function:** Write the function `fibonacci(n)` inside a module named `fibonacci_module.py`. 2. **Write Docstring with Examples:** Include a docstring for the `fibonacci()` function with multiple `doctest` examples, demonstrating: - Typical use cases. - Edge cases (e.g., n=0, n=1, n=2). - Error handling for invalid inputs. 3. **Enable `doctest` Execution:** Ensure that your module can be run to automatically test the examples using `doctest`. **Example of `fibonacci_module.py`:** ```python def fibonacci(n): Generate the first n Fibonacci numbers. >>> fibonacci(0) [] >>> fibonacci(1) [0] >>> fibonacci(2) [0, 1] >>> fibonacci(5) [0, 1, 1, 2, 3] >>> fibonacci(10) [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] >>> fibonacci(-1) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> fibonacci(51) Traceback (most recent call last): ... ValueError: n must be <= 50 if not isinstance(n, int) or n < 0: raise ValueError(\'n must be a non-negative integer\') if n > 50: raise ValueError(\'n must be <= 50\') fib_seq = [] a, b = 0, 1 for _ in range(n): fib_seq.append(a) a, b = b, a + b return fib_seq if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` **Submission Requirements:** - Submit the complete `fibonacci_module.py` file fulfilling the above requirements. - Ensure that running the module directly (using `python fibonacci_module.py`) executes the `doctest` validations and indicates whether all tests have passed. **Evaluation Criteria:** - Correctness of the function implementation. - Completeness and relevance of the docstring examples. - Proper use of `doctest` for validating the examples. - Handling of edge cases and invalid inputs as specified.","solution":"def fibonacci(n): Generate the first n Fibonacci numbers. >>> fibonacci(0) [] >>> fibonacci(1) [0] >>> fibonacci(2) [0, 1] >>> fibonacci(5) [0, 1, 1, 2, 3] >>> fibonacci(10) [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] >>> fibonacci(50) # doctest: +ELLIPSIS [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ... >>> fibonacci(-1) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> fibonacci(51) Traceback (most recent call last): ... ValueError: n must be <= 50 if not isinstance(n, int) or n < 0: raise ValueError(\'n must be a non-negative integer\') if n > 50: raise ValueError(\'n must be <= 50\') fib_seq = [] a, b = 0, 1 for _ in range(n): fib_seq.append(a) a, b = b, a + b return fib_seq if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"# Coding Assessment Question Objective Your task is to implement a custom reduction function for a user-defined class and register it using the `copyreg` module. You will demonstrate the usage of this registered function with both the `copy` and `pickle` modules. Requirements 1. Implement a class `D` that takes two attributes `x` and `y` during initialization. 2. Write a reduction function `pickle_d` for the class `D`. 3. Register the reduction function using the `copyreg.pickle`. 4. Create an instance of `D` and show how the reduction function is used with the `copy` and `pickle` modules. 5. Ensure the reduction function returns the class and the attributes in a tuple. Constraints - The class `D` must have an `__init__` method that initializes `x` and `y`. - The reduction function `pickle_d` should take an instance of `D` and return a tuple `(D, (instance.x, instance.y))`. Expected Input and Output Your solution should include: - The implementation of class `D`. - The `pickle_d` function. - The registration of the reduction function using `copyreg.pickle`. - The demonstration of copying and pickling an instance of `D`. Here is the expected template you should follow: ```python import copyreg import copy import pickle # 1. Implementation of class D class D: def __init__(self, x, y): self.x = x self.y = y # 2. Implementation of reduction function pickle_d def pickle_d(d): # Implement the reduction function pass # replace with your code # 3. Register the reduction function using copyreg.pickle # Your code here # 4. Demonstrate copying and pickling an instance of D d_instance = D(5, 10) # Demonstration copied_instance = copy.copy(d_instance) # This should trigger your pickle_d function pickled_instance = pickle.dumps(d_instance) # This should also trigger your pickle_d function # You may include prints or assertions to show the correctness if needed ``` # Additional Information - Ensure that your `pickle_d` function is correctly registered and used during both copying and pickling operations. - This question tests your understanding of custom object serialization and deserialization using the `copyreg` module. Good luck!","solution":"import copyreg import copy import pickle # 1. Implementation of class D class D: def __init__(self, x, y): self.x = x self.y = y def __eq__(self, other): For test purposes: Check equality based on x and y values. return isinstance(other, D) and self.x == other.x and self.y == other.y # 2. Implementation of reduction function pickle_d def pickle_d(d): return (D, (d.x, d.y)) # 3. Register the reduction function using copyreg.pickle copyreg.pickle(D, pickle_d) # 4. Demonstrate copying and pickling an instance of D d_instance = D(5, 10) # Demonstration copied_instance = copy.copy(d_instance) # This should trigger your pickle_d function pickled_instance = pickle.dumps(d_instance) # This should also trigger your pickle_d function unpickled_instance = pickle.loads(pickled_instance) # Optional print statements for visual verification print(d_instance) # Original instance print(copied_instance) # Copied instance print(unpickled_instance) # Unpickled instance"},{"question":"Objective Implement a function that computes various pairwise distance metrics and kernel similarities for given data sets. The function should further evaluate the suitability of these metrics for clustering tasks. Function Signature ```python def evaluate_pairwise_metrics_and_kernels(X: np.ndarray, Y: np.ndarray = None) -> dict: This function computes various pairwise distance metrics and kernel similarities between the row vectors of X (and optionally Y) and evaluates their suitability for clustering tasks. Parameters: - X (np.ndarray): An m x n input matrix with m samples and n features. - Y (np.ndarray, optional): An l x n input matrix with l samples and n features. Default is None. Returns: - result_dict (dict): A dictionary with keys as the metric/kernel names and values as the computed matrices. ``` Input - `X`: A 2D numpy array of shape (m, n) representing m samples with n features. - `Y`: An optional 2D numpy array of shape (l, n) representing l samples with n features. Output - A dictionary `result_dict` where each key is the name of a metric/kernel and the value is the corresponding computed matrix. Constraints - The number of samples m and l should be in the range [2, 1000]. - The number of features n should be in the range [1, 100]. - Use the following metrics and kernels: - Euclidean distance - Manhattan distance - Cosine similarity - Linear kernel - Polynomial kernel (degree 3) - RBF kernel Example ```python import numpy as np X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) result = evaluate_pairwise_metrics_and_kernels(X, Y) for metric, matrix in result.items(): print(f\\"{metric}:n{matrix}\\") ``` Expected Output: ``` euclidean: [[ ... ]] manhattan: [[ ... ]] cosine: [[ ... ]] linear: [[ ... ]] polynomial: [[ ... ]] rbf: [[ ... ]] ``` Detailed Description 1. **Metrics and Kernels to Compute**: - **Euclidean distance**: Use `pairwise_distances` with `metric=\'euclidean\'`. - **Manhattan distance**: Use `pairwise_distances` with `metric=\'manhattan\'`. - **Cosine similarity**: Use `cosine_similarity`. - **Linear kernel**: Use `linear_kernel`. - **Polynomial kernel**: Use `polynomial_kernel` with `degree=3`. - **RBF kernel**: Use `rbf_kernel`. 2. **Implementation Steps**: - Import the necessary methods from `sklearn.metrics` and `sklearn.metrics.pairwise`. - Compute the pairwise distances and kernels for the given X and Y (if Y is provided). - Store the results in a dictionary with appropriate keys. - Return the dictionary as output. Evaluation Criteria - Correctness: The function should accurately compute the specified metrics and kernels. - Efficiency: The function should handle input sizes within the given constraints efficiently. - Clarity: Code should be well-organized and commented where necessary.","solution":"import numpy as np from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import cosine_similarity, linear_kernel, polynomial_kernel, rbf_kernel def evaluate_pairwise_metrics_and_kernels(X: np.ndarray, Y: np.ndarray = None) -> dict: This function computes various pairwise distance metrics and kernel similarities between the row vectors of X (and optionally Y) and evaluates their suitability for clustering tasks. Parameters: - X (np.ndarray): An m x n input matrix with m samples and n features. - Y (np.ndarray, optional): An l x n input matrix with l samples and n features. Default is None. Returns: - result_dict (dict): A dictionary with keys as the metric/kernel names and values as the computed matrices. metrics_and_kernels = { \\"euclidean\\": pairwise_distances, \\"manhattan\\": pairwise_distances, \\"cosine\\": cosine_similarity, \\"linear\\": linear_kernel, \\"polynomial\\": polynomial_kernel, \\"rbf\\": rbf_kernel, } result_dict = {} if Y is None: Y = X for metric, func in metrics_and_kernels.items(): if metric in [\\"euclidean\\", \\"manhattan\\"]: result_dict[metric] = func(X, Y, metric=metric) elif metric == \\"polynomial\\": result_dict[metric] = func(X, Y, degree=3) else: result_dict[metric] = func(X, Y) return result_dict"},{"question":"Objective Create a Python class using the `dataclasses` module that models an inventory management system for a bookstore. The class should demonstrate the use of various features available in `dataclasses`, including post-initialization processing, class variables, init-only variables, frozen instances, and default factory functions. Requirements 1. Define a `Book` dataclass with the following attributes: - `title` (string) - `author` (string) - `isbn` (string) - `price` (float) - `quantity` (integer, default value of 0) 2. Use a default factory function to generate a unique identifier for each book. 3. Include an init-only variable `discount_rate` (float) that represents a discount that can be applied to the price of the book. This value should be set during initialization but should not be stored as an instance attribute. 4. Use post-initialization processing to apply the discount to the `price` of the book. 5. Define a `BookInventory` dataclass to manage multiple `Book` instances. The `BookInventory` class should: - Have a class variable `inventory` that stores all books (use a default factory to initialize). - Include methods to add new books, remove books by `isbn`, and search for books by title or author. 6. Make the `Book` dataclass immutable (frozen). Input and Output - You do not need to handle any input or output within your classes. However, your classes and methods should be designed to interact programmatically (e.g., methods should return appropriate values or modify the inventory). Constraints - The `isbn` should be unique for each `Book`. - Assume that the discount rate is a percentage (e.g., `10` for 10%). Implementation Details 1. Implement the `Book` dataclass as described, ensuring that it leverages the functionalities of `dataclasses` effectively. 2. Implement the `BookInventory` dataclass with methods to manage the bookstore inventory. 3. Test your implementation by creating several `Book` instances and performing various inventory management operations. Performance Requirements - Your solution should efficiently manage the inventory regardless of the number of books added or removed. # Example Usage ```python from dataclasses import dataclass, field from typing import List import uuid @dataclass(frozen=True) class Book: title: str author: str isbn: str price: float quantity: int = 0 unique_id: str = field(default_factory=lambda: str(uuid.uuid4()), init=False) discount_rate: float = field(init=False, repr=False, compare=False) def __post_init__(self): object.__setattr__(self, \'price\', self.price * (1 - self.discount_rate / 100)) @dataclass class BookInventory: inventory: List[Book] = field(default_factory=list) def add_book(self, book: Book): self.inventory.append(book) def remove_book(self, isbn: str): self.inventory = [book for book in self.inventory if book.isbn != isbn] def search_by_title(self, title: str): return [book for book in self.inventory if title.lower() in book.title.lower()] def search_by_author(self, author: str): return [book for book in self.inventory if author.lower() in book.author.lower()] # Example of creating books and managing inventory inventory = BookInventory() book1 = Book(title=\'The Great Gatsby\', author=\'F. Scott Fitzgerald\', isbn=\'1234567890\', price=10.0, discount_rate=10) book2 = Book(title=\'1984\', author=\'George Orwell\', isbn=\'2345678901\', price=15.0, quantity=5, discount_rate=0) inventory.add_book(book1) inventory.add_book(book2) print(inventory.search_by_title(\'great\')) inventory.remove_book(\'1234567890\') print(inventory.inventory) ```","solution":"from dataclasses import dataclass, field from typing import List import uuid @dataclass(frozen=True) class Book: title: str author: str isbn: str price: float quantity: int = 0 unique_id: str = field(default_factory=lambda: str(uuid.uuid4()), init=False) discount_rate: float = field(default=0.0, init=True, compare=False, repr=False) def __post_init__(self): discounted_price = self.price * (1 - self.discount_rate / 100) object.__setattr__(self, \'price\', discounted_price) @dataclass class BookInventory: inventory: List[Book] = field(default_factory=list) def add_book(self, book: Book): if any(b.isbn == book.isbn for b in self.inventory): raise ValueError(f\\"Book with ISBN {book.isbn} already exists in the inventory.\\") self.inventory.append(book) def remove_book(self, isbn: str): self.inventory = [book for book in self.inventory if book.isbn != isbn] def search_by_title(self, title: str): return [book for book in self.inventory if title.lower() in book.title.lower()] def search_by_author(self, author: str): return [book for book in self.inventory if author.lower() in book.author.lower()]"},{"question":"You are given a directory containing multiple Parquet files, each representing a year\'s worth of timestamped data. Your task is to perform a series of operations to aggregate data across these files efficiently using pandas. Specifically, you need to: 1. Load only specific columns from each Parquet file. 2. Optimize memory usage by converting data types where applicable. 3. Use efficient chunk-wise processing to compute the overall sum of a specified column across all files. # File Directory Structure The directory contains Parquet files named in the format `data/timeseries/ts-YY.parquet`, where `YY` ranges from 00 to 11. # Input - Directory path to the Parquet files: `str` - List of columns to read from each file: `List[str]` - The column name whose values you need to sum: `str` # Output - The total sum of the specified column across all files: `float` # Requirements 1. **Selective Column Loading:** Read only the necessary columns from each Parquet file. 2. **Efficient Memory Usage:** Convert text columns with low cardinality to `Categorical` type and downcast numeric columns to the smallest possible types. 3. **Chunk-wise Processing:** Implement chunk-wise processing to handle files individually and accumulate the sum. # Constraints - You should read the data in a memory-efficient manner. - Assume that each individual Parquet file can fit into memory but the entire dataset cannot. # Example ```python def process_timeseries_data(dir_path: str, columns: list, sum_column: str) -> float: import pandas as pd import pathlib total_sum = 0.0 files = pathlib.Path(dir_path).glob(\\"ts*.parquet\\") for path in files: df = pd.read_parquet(path, columns=columns) # Convert to more efficient data types for col in df.select_dtypes(include=[\'object\']).columns: df[col] = df[col].astype(\'category\') for col in df.select_dtypes(include=[\'int\', \'float\']).columns: df[col] = pd.to_numeric(df[col], downcast=\'float\') total_sum += df[sum_column].sum() return total_sum # Example usage: # Directory contains Parquet files \'ts-00.parquet\', \'ts-01.parquet\', ..., \'ts-11.parquet\' dir_path = \\"data/timeseries\\" columns = [\\"id_0\\", \\"name_0\\", \\"x_0\\", \\"y_0\\"] sum_column = \\"x_0\\" print(process_timeseries_data(dir_path, columns, sum_column)) # Should return the total sum of \'x_0\' column across all files. ``` Make sure your implementation follows the described requirements and handles large datasets efficiently using pandas.","solution":"def process_timeseries_data(dir_path: str, columns: list, sum_column: str) -> float: import pandas as pd from pathlib import Path total_sum = 0.0 files = Path(dir_path).glob(\\"ts*.parquet\\") for path in files: df = pd.read_parquet(path, columns=columns) # Convert to more efficient data types for col in df.select_dtypes(include=[\'object\']).columns: df[col] = df[col].astype(\'category\') for col in df.select_dtypes(include=[\'int\', \'float\']).columns: df[col] = pd.to_numeric(df[col], downcast=\'float\') total_sum += df[sum_column].sum() return total_sum"},{"question":"# Advanced Python Code Analysis with AST You are tasked with analyzing Python code using the `ast` (Abstract Syntax Trees) module. The goal is to assess the structure of a given Python function and extract certain details about its components. Problem Description: Write a function `analyze_function_code` that takes a string input representing the source code of a single Python function. The function should return a dictionary containing the following details: 1. **function_name**: The name of the function. 2. **params**: A list of parameter names used in the function. 3. **num_return_statements**: The number of `return` statements in the function. 4. **num_if_statements**: The number of `if` statements in the function. 5. **num_for_loops**: The number of `for` loops in the function. 6. **num_while_loops**: The number of `while` loops in the function. 7. **num_imports**: The number of import statements used within the function (including nested scopes). Input Format: - A single string containing the source code of a Python function. The function is guaranteed to be syntactically correct and self-contained. Output Format: - A dictionary with the structure outlined above. Example: ```python input_code = def example_function(x, y): import math import numpy as np if x > 0: return x else: for i in range(y): if i % 2 == 0: return i return 0 result = analyze_function_code(input_code) # Expected Output { \\"function_name\\": \\"example_function\\", \\"params\\": [\\"x\\", \\"y\\"], \\"num_return_statements\\": 3, \\"num_if_statements\\": 2, \\"num_for_loops\\": 1, \\"num_while_loops\\": 0, \\"num_imports\\": 2 } ``` Constraints: - The input function will not contain nested function definitions. Notes: - You may assume the use of the `ast` module for parsing the function\'s source code and analyzing its structure. - Your implementation should handle typical Python function components accurately. Implement the `analyze_function_code` function to carry out the required analysis.","solution":"import ast def analyze_function_code(function_code): Analyze the structure of a given Python function and extract specific details. Args: function_code (str): The source code of a single Python function. Returns: dict: A dictionary containing the analysis details of the function. tree = ast.parse(function_code) assert len(tree.body) == 1 and isinstance(tree.body[0], ast.FunctionDef), \\"Input must be a single function definition.\\" function_def = tree.body[0] result = { \\"function_name\\": function_def.name, \\"params\\": [arg.arg for arg in function_def.args.args], \\"num_return_statements\\": 0, \\"num_if_statements\\": 0, \\"num_for_loops\\": 0, \\"num_while_loops\\": 0, \\"num_imports\\": 0 } class FunctionAnalyzer(ast.NodeVisitor): def visit_Return(self, node): result[\\"num_return_statements\\"] += 1 self.generic_visit(node) def visit_If(self, node): result[\\"num_if_statements\\"] += 1 self.generic_visit(node) def visit_For(self, node): result[\\"num_for_loops\\"] += 1 self.generic_visit(node) def visit_While(self, node): result[\\"num_while_loops\\"] += 1 self.generic_visit(node) def visit_Import(self, node): result[\\"num_imports\\"] += 1 def visit_ImportFrom(self, node): result[\\"num_imports\\"] += 1 analyzer = FunctionAnalyzer() analyzer.visit(tree) return result"},{"question":"# Question: Advanced Function Caching and Dispatch Using `functools` You are tasked with creating a Python module for managing user-related computations. Your module should have a class `UserManager` that performs some computationally expensive operations, caches results efficiently, and utilizes single dispatch for handling different types of inputs. Requirements: 1. **Implement the `UserManager` class** which includes: - An `__init__` method that takes a list of users (represented as dictionaries with details such as `id`, `name`, and `age`) and stores it. - A method `calculate_average_age()` which computes the average age of all users. - A `@functools.lru_cache` optimized method `get_user_by_id(user_id)` that fetches user details by their ID. Store cached results to avoid recomputing previously fetched user details. - A `@functools.singledispatch` method `handle_input(arg)` that: - Prints how many users are in the list if `arg` is a `list`. - Prints an error message if `arg` is a `dict` or any other type. 2. **Use `functools.update_wrapper`** to create a decorator `logging_decorator` that logs the function\'s name before execution. Expected Input and Output - `UserManager(users: List[Dict[str, Union[str, int]]])` - **Input**: A list of dictionaries, where each dictionary contains user details. - `calculate_average_age() -> float` - **Output**: The average age of users as a float. - `get_user_by_id(user_id: int) -> Dict[str, Union[str, int]]` - **Input**: `user_id` as an integer. - **Output**: A dictionary with the user details or an appropriate message if the user is not found. - `handle_input(arg: Any) -> None` - **Input**: Any type (`list`, `dict`, etc.). - **Output**: An appropriate message based on the type of `arg`. Constraints: - Ensure that the caching mechanism is efficiently used to optimize performance for fetching user details by ID. - Follow best practices for coding and use appropriate `functools` utilities as required. Example Usage: ```python from functools import lru_cache, singledispatch, update_wrapper from typing import List, Dict, Union, Any class UserManager: def __init__(self, users: List[Dict[str, Union[str, int]]]) -> None: self.users = users @lru_cache(maxsize=32) def get_user_by_id(self, user_id: int) -> Dict[str, Union[str, int]]: for user in self.users: if user[\'id\'] == user_id: return user return {\'error\': \'User not found\'} def calculate_average_age(self) -> float: total_age = sum(user[\'age\'] for user in self.users) return total_age / len(self.users) if self.users else 0 @singledispatch def handle_input(self, arg: Any) -> None: print(\\"Unhandled input type\\") @handle_input.register def _(self, arg: list) -> None: print(f\\"Number of users: {len(arg)}\\") @handle_input.register def _(self, arg: dict) -> None: print(\\"Dict input is not supported\\") def logging_decorator(func): def wrapper(*args, **kwargs): print(f\\"Executing {func.__name__} function\\") return func(*args, **kwargs) return update_wrapper(wrapper, func) # Sample data users = [ {\'id\': 1, \'name\': \'Alice\', \'age\': 30}, {\'id\': 2, \'name\': \'Bob\', \'age\': 25}, {\'id\': 3, \'name\': \'Charlie\', \'age\': 35}, ] # Create UserManager instance user_manager = UserManager(users) # Test the methods print(user_manager.calculate_average_age()) # Output: 30.0 print(user_manager.get_user_by_id(2)) # Output: {\'id\': 2, \'name\': \'Bob\', \'age\': 25} user_manager.handle_input(users) # Output: Number of users: 3 user_manager.handle_input({\'test\': 123}) # Output: Dict input is not supported user_manager.handle_input(None) # Output: Unhandled input type ```","solution":"from functools import lru_cache, singledispatch, update_wrapper from typing import List, Dict, Union, Any class UserManager: def __init__(self, users: List[Dict[str, Union[str, int]]]) -> None: self.users = users @lru_cache(maxsize=32) def get_user_by_id(self, user_id: int) -> Dict[str, Union[str, int]]: for user in self.users: if user[\'id\'] == user_id: return user return {\'error\': \'User not found\'} def calculate_average_age(self) -> float: total_age = sum(user[\'age\'] for user in self.users) return total_age / len(self.users) if self.users else 0 @singledispatch def handle_input(self, arg: Any) -> None: print(\\"Unhandled input type\\") @handle_input.register def _(self, arg: list) -> None: print(f\\"Number of users: {len(arg)}\\") @handle_input.register def _(self, arg: dict) -> None: print(\\"Dict input is not supported\\") def logging_decorator(func): def wrapper(*args, **kwargs): print(f\\"Executing {func.__name__} function\\") return func(*args, **kwargs) return update_wrapper(wrapper, func)"},{"question":"You are required to implement a Python function that processes a file containing numeric data. The function should calculate the sum of all numbers in the file and ensure proper management of file resources, adhering to best practices along the lines suggested by the Python Development Mode guidelines. Function Signature: ```python def sum_numbers_in_file(file_path: str) -> int: pass ``` Input: - `file_path` (str): The path to the file containing numeric data. The file is assumed to contain one number per line. Output: - Returns an integer, which is the sum of all the numbers in the file. Constraints: - Ensure the file is properly closed after the operation to avoid resource warnings. - Handle potential exceptions that might occur during file operations, such as the file not existing or containing invalid data. - Do not use global variables; the function should be self-contained. - The function must ensure minimal memory usage, which implies processing the file line by line. Example: Assume `numbers.txt` contains: ```text 10 20 30 40 50 ``` ```python print(sum_numbers_in_file(\\"numbers.txt\\")) ``` Output: ```text 150 ``` Additional Information: - You can use context managers (i.e., `with` statements) to handle file operations safely. - Pay attention to any potential runtime warnings and assure the function adheres to Python Development Mode constraints. Hint: Perform exception handling for common file-related errors and ensure the function operates efficiently by reading the file line by line rather than loading the entire file into memory.","solution":"def sum_numbers_in_file(file_path: str) -> int: Calculates the sum of all numbers in the given file. :param file_path: Path to the file containing numeric data. :return: Sum of all numbers in the file. total = 0 try: with open(file_path, \'r\') as file: for line in file: try: number = int(line.strip()) total += number except ValueError: continue # Skip lines that do not contain valid integers except FileNotFoundError: print(f\\"Error: File \'{file_path}\' not found.\\") except IOError: print(f\\"Error: An IO error occurred while reading the file \'{file_path}\'.\\") return total"},{"question":"Objective Use seaborn to create a line plot with specified plot limits and customization. Problem Statement You are provided with a dataset of stock prices for a company over a 5-day period. You need to create a line plot showing the relationship between day and closing price on each day. The plot should follow these specifications: 1. Create a line plot of the stock prices using seaborn\'s `objects` API. 2. Set the x-axis limits from -1 to 6 and the y-axis limits from 95 to 105. 3. Reverse the y-axis such that the larger values are at the bottom. 4. Include custom y-axis limit settings where the upper limit retains the default setting. Here is the stock price data: - Days: [1, 2, 3, 4, 5] - Prices: [100, 101, 99, 103, 102] Function Signature ```python def create_stock_price_plot(): This function creates and returns a seaborn plot object for the stock prices. The plot is customized according to the specifications mentioned above. pass ``` Input and Output Format - **Input:** No input parameters. - **Output:** - Return a seaborn plot object with the specified customizations. Constraints - You must use the seaborn objects API (`seaborn.objects`) for this task. - The plot should use markers for the data points. Example Usage ```python # Correct usage would create and return a seaborn plot object plot = create_stock_price_plot() ``` Notes - Make sure to install the `seaborn` package if you haven\'t already. - Visual inspection of the generated plot is required to ensure the axis limits and customizations are correctly applied.","solution":"import seaborn as sns def create_stock_price_plot(): This function creates and returns a seaborn plot object for the stock prices. The plot is customized according to the specifications mentioned. import matplotlib.pyplot as plt days = [1, 2, 3, 4, 5] prices = [100, 101, 99, 103, 102] p = sns.lineplot(x=days, y=prices, marker=\'o\') p.set_xlim(-1, 6) p.set_ylim(105, 95) # Reverse y-axis plt.draw() # Draw the plot to apply customizations fig = plt.gcf() # Get the current figure return fig"},{"question":"# Coding Assessment Task: Advanced Base64 Data Encoding and Decoding Objective: Write a Python function named `process_base64_data` that demonstrates the functionality of the `base64` module. This function should read binary data from a file, encode it using different Base64 encodings, and then decode it back to ensure data integrity. The function should return a dictionary with the original data, encoded data using different methods, and decoded data to verify correctness. Requirements: 1. **Function Signature:** ```python def process_base64_data(file_path: str) -> dict: ``` 2. **Input:** - `file_path`: A string representing the path to a binary file to be processed. 3. **Output:** - A dictionary with the following key-value pairs: - `\'original_data\'`: The original binary data read from the file. - `\'standard_encoded\'`: Data encoded with the standard Base64 alphabet. - `\'standard_decoded\'`: Data decoded back from `standard_encoded`. - `\'urlsafe_encoded\'`: Data encoded with the URL- and filesystem-safe Base64 alphabet. - `\'urlsafe_decoded\'`: Data decoded back from `urlsafe_encoded`. - `\'is_data_intact\'`: A boolean value indicating whether the decoded data matches the original data for both standard and URL-safe methods. 4. **Constraints:** - Use only the modern interface of the `base64` module. - Ensure that the function handles potential exceptions (e.g., file not found, invalid data, etc.) Example: Assume there is a file `data.bin` containing the binary data `b\\"Hello, World!\\"`. ```python result = process_base64_data(\\"data.bin\\") print(result) ``` The above call should return a dictionary similar to: ```python { \'original_data\': b\'Hello, World!\', \'standard_encoded\': b\'SGVsbG8sIFdvcmxkIQ==\', \'standard_decoded\': b\'Hello, World!\', \'urlsafe_encoded\': b\'SGVsbG8sIFdvcmxkIQ==\', \'urlsafe_decoded\': b\'Hello, World!\', \'is_data_intact\': True } ``` Notes: - The `==\' suffix in Base64 encoded data represents padding. - For successful `urlsafe_b64decode`, ensure data integrity by using equivalent padding logic if necessary. - The aim is to validate the correct use of encoding and decoding functions while ensuring data integrity.","solution":"import base64 def process_base64_data(file_path: str) -> dict: result = {} try: with open(file_path, \'rb\') as file: original_data = file.read() # Standard Base64 Encoding standard_encoded = base64.b64encode(original_data) standard_decoded = base64.b64decode(standard_encoded) # URL-safe Base64 Encoding urlsafe_encoded = base64.urlsafe_b64encode(original_data) urlsafe_decoded = base64.urlsafe_b64decode(urlsafe_encoded) # Check data integrity is_data_intact = (original_data == standard_decoded == urlsafe_decoded) result = { \'original_data\': original_data, \'standard_encoded\': standard_encoded, \'standard_decoded\': standard_decoded, \'urlsafe_encoded\': urlsafe_encoded, \'urlsafe_decoded\': urlsafe_decoded, \'is_data_intact\': is_data_intact } except Exception as e: result[\'error\'] = str(e) return result"},{"question":"**Objective**: Evaluate the understanding of deterministic operations and tensor initialization behavior in PyTorch. # Problem Statement: Write a Python function using PyTorch to demonstrate the behavior of `fill_uninitialized_memory` when working with tensor initialization and resizing operations. Your function should perform the following tasks: 1. Set the deterministic mode by using `torch.use_deterministic_algorithms(True)`. 2. Enable the filling of uninitialized memory by setting `torch.utils.deterministic.fill_uninitialized_memory` to `True`. 3. Create an uninitialized floating-point tensor using `torch.empty` and print its values. 4. Resize the tensor using `torch.Tensor.resize_` and print its values. 5. Create another tensor using `torch.empty_like` and print its values. 6. Finally, compare the performance implications by disabling `fill_uninitialized_memory` and repeating the above steps, printing tensor values and the time taken for each operation. # Function Signature: ```python import torch import time def test_deterministic_memory_filling(): # Step 1: Set the deterministic mode to True torch.use_deterministic_algorithms(True) # Step 2: Enable fill_uninitialized_memory torch.utils.deterministic.fill_uninitialized_memory = True # Step 3: Create an unitialized tensor and print values init_tensor = torch.empty(5, 5) print(\\"Initialized Tensor with fill_uninitialized_memory=True:\\") print(init_tensor) # Step 4: Resize the tensor and print values resized_tensor = init_tensor.resize_(10, 10) print(\\"Resized Tensor with fill_uninitialized_memory=True:\\") print(resized_tensor) # Step 5: Create a tensor using empty_like and print values like_tensor = torch.empty_like(init_tensor) print(\\"Empty Like Tensor with fill_uninitialized_memory=True:\\") print(like_tensor) # Step 6: Disable fill_uninitialized_memory and repeat torch.utils.deterministic.fill_uninitialized_memory = False start_time = time.time() init_tensor_no_fill = torch.empty(5, 5) resized_tensor_no_fill = init_tensor_no_fill.resize_(10, 10) like_tensor_no_fill = torch.empty_like(init_tensor_no_fill) end_time = time.time() print(\\"Initialized Tensor with fill_uninitialized_memory=False:\\") print(init_tensor_no_fill) print(\\"Resized Tensor with fill_uninitialized_memory=False:\\") print(resized_tensor_no_fill) print(\\"Empty Like Tensor with fill_uninitialized_memory=False:\\") print(like_tensor_no_fill) print(f\\"Time taken with fill_uninitialized_memory=False: {end_time - start_time:.6f} seconds\\") # Call the function to test test_deterministic_memory_filling() ``` # Expected Output: - Tensor values after each creation/resize operation with `fill_uninitialized_memory=True`. - Tensor values after each creation/resize operation with `fill_uninitialized_memory=False`. - Time taken to perform operations without filling uninitialized memory. # Constraints: - Use the provided tensor creation and resizing functions. - Ensure the code runs correctly with the latest version of PyTorch. This question tests the student\'s ability to work with deterministic settings in PyTorch and understand the impact of filling uninitialized memory on tensor operations and performance.","solution":"import torch import time def test_deterministic_memory_filling(): # Step 1: Set the deterministic mode to True torch.use_deterministic_algorithms(True) # Step 2: Enable fill_uninitialized_memory torch.utils.deterministic.fill_uninitialized_memory = True # Step 3: Create an unitialized tensor and print values init_tensor = torch.empty(5, 5) print(\\"Initialized Tensor with fill_uninitialized_memory=True:\\") print(init_tensor) # Step 4: Resize the tensor and print values resized_tensor = init_tensor.resize_(10, 10) print(\\"Resized Tensor with fill_uninitialized_memory=True:\\") print(resized_tensor) # Step 5: Create a tensor using empty_like and print values like_tensor = torch.empty_like(init_tensor) print(\\"Empty Like Tensor with fill_uninitialized_memory=True:\\") print(like_tensor) # Step 6: Disable fill_uninitialized_memory and repeat torch.utils.deterministic.fill_uninitialized_memory = False start_time = time.time() init_tensor_no_fill = torch.empty(5, 5) resized_tensor_no_fill = init_tensor_no_fill.resize_(10, 10) like_tensor_no_fill = torch.empty_like(init_tensor_no_fill) end_time = time.time() print(\\"Initialized Tensor with fill_uninitialized_memory=False:\\") print(init_tensor_no_fill) print(\\"Resized Tensor with fill_uninitialized_memory=False:\\") print(resized_tensor_no_fill) print(\\"Empty Like Tensor with fill_uninitialized_memory=False:\\") print(like_tensor_no_fill) print(f\\"Time taken with fill_uninitialized_memory=False: {end_time - start_time:.6f} seconds\\") # Call the function to test test_deterministic_memory_filling()"},{"question":"Objective Create a visualization using seaborn that involves handling multiple subplots and customizing legend positions. Question Using the seaborn package, create a multi-plot visualization for the `penguins` dataset that meets the following requirements: 1. Create a series of histograms (`sns.histplot`) showing the distribution of `bill_length_mm` for different `species`. 2. Facet the plot by `island`, arranging the facets in a grid with two columns. 3. Move the legend for each subplot to the `lower center` of the subplot, outside of the axes. 4. Each subplot\'s legend should have no frame (`frameon=False`). **Input:** - No specific input. You should use the `penguins` dataset from seaborn. **Output:** - A seaborn `FacetGrid` object with histograms faceted by `island` and customized legend positions. **Constraints:** - The legends should not overlap with the plots. - The grid should be arranged in two columns regardless of the number of `island` values. **Performance Requirements:** - The plot should render within a reasonable time for a dataset of similar size (i.e., `penguins` dataset). Example The following steps outline a partial approach to solving the problem but do not provide the complete solution: ```python import seaborn as sns sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Create the FacetGrid g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, ) # Move legend for each subplot sns.move_legend(g, \\"lower center\\", bbox_to_anchor=(0.5, -0.1), frameon=False) # Note: The above code may need to be modified to precisely meet the requirements. ``` Provide the full implementation that meets all specified requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_facetgrid(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the FacetGrid g = sns.displot( data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=4, kind=\\"hist\\", multiple=\\"stack\\" ) # Customize the legend for each subplot for ax in g.axes.flatten(): handles, labels = ax.get_legend_handles_labels() ax.legend(handles=handles, labels=labels, loc=\'lower center\', bbox_to_anchor=(0.5, -0.1), frameon=False) # Adjust layout for better visualization plt.tight_layout() return g"},{"question":"**Task**: You need to implement a function `list_matching_files(root_dir, filename_pattern)` that uses the `glob` module to perform an advanced search for files. **Function Signature**: ```python def list_matching_files(root_dir: str, filename_pattern: str) -> list: pass ``` **Input**: - `root_dir` (str): A string representing the root directory where the search should start. - `filename_pattern` (str): A string containing the filename pattern to search for. This pattern can include wildcards such as \\"*\\", \\"?\\", and character ranges within \\"[ ]\\". **Output**: - Returns a list of paths to the files that match the given pattern. The list should include paths relative to `root_dir`. **Constraints**: - The function should handle both recursive and non-recursive patterns. - The results should include entries for hidden files and directories if their names match the pattern. - You should ensure the function is able to handle large directory trees efficiently. **Example**: ```python # Assuming the following directory structure: # # test_dir/ #  file1.txt #  file2.gif #  .hiddenfile #  subdir/ #  file3.txt list_matching_files(\\"test_dir\\", \\"*.txt\\") # Should give: [\'file1.txt\', \'subdir/file3.txt\'] list_matching_files(\\"test_dir\\", \\"**/*.gif\\") # Should give: [\'file2.gif\'] list_matching_files(\\"test_dir\\", \\".hidden*\\") # Should give: [\'.hiddenfile\'] ``` **Note**: - Ensure that your function efficiently handles large directories and diverse pathname patterns. - Use the `glob` module functions as per the documentation to achieve the required results.","solution":"import glob import os def list_matching_files(root_dir: str, filename_pattern: str) -> list: Returns a list of paths to the files that match the given pattern. The list should include paths relative to root_dir. Parameters: - root_dir: str - The directory to start searching from. - filename_pattern: str - The filename pattern to search for, can include glob patterns like *, ?, []. Returns: - list: List of paths that match the given pattern. search_pattern = os.path.join(root_dir, filename_pattern) matching_files = glob.glob(search_pattern, recursive=True) relative_paths = [os.path.relpath(path, root_dir) for path in matching_files] return relative_paths"},{"question":"**Question: Health Expense and Life Expectancy Analysis** You are provided with a dataset that contains information about health expenditure in USD and life expectancy across different countries over several years. The dataset has the following columns: - `Country`: Name of the country. - `Year`: Year of observation. - `Spending_USD`: Health expenditure in USD. - `Life_Expectancy`: Life expectancy in years. Your task is to design a function that creates a customized plot using seaborn to visualize the relationship between health expenditure and life expectancy for each country over the observed years. The function should meet the following requirements: 1. **Load the dataset**: Use the seaborn `load_dataset` function to load the dataset named `healthexp`. 2. **Sort the dataset**: Sort the data first by `Country` and then by `Year`. 3. **Create the plot**: Use `seaborn.objects.Plot` to create a plot with: - `Spending_USD` on the x-axis. - `Life_Expectancy` on the y-axis. - Different colors for different countries. 4. **Plot trajectories**: Add a `Path` mark to plot the trajectories of health expenditure vs. life expectancy over the years for each country. 5. **Customize the plot**: Customize the plot by: - Adding markers at each data point. - Setting the point size to 2. - Setting the line width to 0.75. - Filling the markers with white color. # Function Signature ```python import seaborn.objects as so def plot_health_expense_relationship(): # load dataset and sort it from seaborn import load_dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # create the plot p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) p.show() ``` # Input Format The function does not take any inputs. # Output Format The function should display the customized plot. # Example Calling the function: ```python plot_health_expense_relationship() ``` Should display a plot with: - `Spending_USD` on the x-axis. - `Life_Expectancy` on the y-axis. - Different colors for different countries. - Trajectories plotted for each country with markers at each data point. # Constraints - You must use the `healthexp` dataset from seaborn. - Ensure the plot is correctly sorted and customized as specified.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_health_expense_relationship(): Load the healthexp dataset, sort it by Country and Year, and create a customized plot to visualize the relationship between health expenditure and life expectancy for each country over the observed years. # load dataset and sort it healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # create the plot p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=0.75, fillcolor=\\"w\\")) p.show()"},{"question":"# Random Number Generation and Distribution Manipulation **Objective:** Write a Python program to simulate a simplified version of the Monty Hall problem using the random module. The Monty Hall problem is a famous probability puzzle based on a game show scenario. **Problem Description:** In this game, a contestant is presented with three doors. Behind one door is a car (the prize), and behind the other two doors are goats. The contestant picks one door. The host, who knows whats behind each door, then opens one of the other two doors, revealing a goat. The contestant is then given a choice to either stick with the original choice or switch to the other remaining door. Simulate this scenario 10,000 times and calculate the probability of winning if the contestant always switches doors and if the contestant never switches doors. **Function Signature:** ```python def monty_hall_simulation(num_simulations: int = 10000) -> (float, float): Simulate the Monty Hall problem and calculate probabilities of winning by always switching and by never switching. Parameters: num_simulations (int): The number of simulations to run. Default is 10,000. Returns: (float, float): A tuple containing the probability of winning by switching and the probability of winning by not switching. pass ``` **Constraints:** - Use the `random.randint` function to randomly place the car behind one of the doors. - Use the `random.choice` function to simulate the contestant\'s choice. - Use appropriate logic to simulate the host\'s action and the contestant\'s decision to switch or not switch. - Ensure performance is optimized to handle the default number of simulations efficiently. **Expected Output:** - A tuple of two floating-point numbers representing the probabilities of winning when always switching and when never switching. **Example Usage:** ```python prob_switch, prob_no_switch = monty_hall_simulation() print(f\\"Probability of winning when switching: {prob_switch}\\") print(f\\"Probability of winning when not switching: {prob_no_switch}\\") ``` **Notes:** 1. Ensure that your code handles edge cases and logically models the Monty Hall problem scenario. 2. Remember to seed the random number generator for reproducible results during testing. Good luck!","solution":"import random def monty_hall_simulation(num_simulations: int = 10000) -> (float, float): Simulate the Monty Hall problem and calculate probabilities of winning by always switching and by never switching. Parameters: num_simulations (int): The number of simulations to run. Default is 10,000. Returns: (float, float): A tuple containing the probability of winning by switching and the probability of winning by not switching. win_switch = 0 win_no_switch = 0 for _ in range(num_simulations): # Place the car behind one of the doors randomly (0, 1, 2) car_position = random.randint(0, 2) # Contestant makes a choice randomly (0, 1, 2) contestant_choice = random.randint(0, 2) # Host opens a door with a goat remaining_doors = [i for i in range(3) if i != contestant_choice and i != car_position] host_opens = random.choice(remaining_doors) # Determine the door that the contestant would switch to switch_choice = [i for i in range(3) if i != contestant_choice and i != host_opens][0] # Check if the contestant wins by switching if switch_choice == car_position: win_switch += 1 # Check if the contestant wins by not switching if contestant_choice == car_position: win_no_switch += 1 # Calculate the probabilities prob_switch = win_switch / num_simulations prob_no_switch = win_no_switch / num_simulations return prob_switch, prob_no_switch"},{"question":"Coding Assessment Question # Objective This task is designed to assess your understanding and proficiency in using scikit-learn\'s pipeline and composite estimators functionalities. # Problem Statement You are provided with a dataset containing information about cars, including features such as `make`, `model year`, `fuel type`, and various performance and safety ratings. Your task is to build a composite estimator using `Pipeline`, `FeatureUnion`, and `ColumnTransformer` to preprocess the data and train a regression model to predict the car\'s price. # Dataset Assume the dataset is available as a pandas DataFrame `df` with the following columns: - `make` (categorical) - `model_year` (numerical) - `fuel_type` (categorical) - `engine_power` (numerical, in HP) - `safety_rating` (numerical) - `price` (target, numerical) # Requirements 1. **Preprocess the dataset**: - Use `ColumnTransformer` to handle the heterogeneous data: - One-hot encode the `make` and `fuel_type` columns. - Standardize/normalize the `model_year`, `engine_power`, and `safety_rating` columns. - Concatenate the transformed features using `FeatureUnion`. 2. **Build the Pipeline**: - Use `Pipeline` to chain the preprocessing step and a regression model. - Utilize `GridSearchCV` to find the best hyperparameters for the regression model. 3. **Implement caching** in the pipeline to speed up the computation. # Constraints - You can only use the following regression models: `LinearRegression` and `Ridge`. - The hyperparameters to tune for the grid search are: - `alpha` for `Ridge` (values: [0.1, 1.0, 10.0]) - `fit_intercept` for `LinearRegression` and `Ridge` (values: [True, False]) # Expected Input and Output ```python def build_and_train_model(df: pd.DataFrame) -> Pipeline: This function builds a composite estimator for preprocessing data and training a regression model. Parameters: df (pd.DataFrame): DataFrame containing the car dataset with columns [\'make\', \'model_year\', \'fuel_type\', \'engine_power\', \'safety_rating\', \'price\']. Returns: Pipeline: The fitted Pipeline object after training. # Your implementation here return pipeline ``` # Example Usage ```python import pandas as pd # Sample DataFrame data = { \'make\': [\'Toyota\', \'BMW\', \'Ford\', \'Toyota\', \'Ford\'], \'model_year\': [2008, 2010, 2012, 2015, 2018], \'fuel_type\': [\'Petrol\', \'Diesel\', \'Petrol\', \'Diesel\', \'Petrol\'], \'engine_power\': [110, 130, 115, 120, 135], \'safety_rating\': [4, 5, 4, 4, 5], \'price\': [20000, 25000, 22000, 23000, 27000] } df = pd.DataFrame(data) # Build and train model pipeline = build_and_train_model(df) ``` # Key Points for the Solution - Ensure that the `ColumnTransformer` correctly handles the different columns based on their data types. - Properly configure the `Pipeline` to include the preprocessing step and the regression model. - Use `GridSearchCV` to find the optimal hyperparameters. - Implement caching in the `Pipeline`. Good luck!","solution":"import pandas as pd from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer, TransformedTargetRegressor from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import Ridge, LinearRegression from sklearn.model_selection import GridSearchCV from sklearn.impute import SimpleImputer def build_and_train_model(df: pd.DataFrame) -> Pipeline: This function builds a composite estimator for preprocessing data and training a regression model. Parameters: df (pd.DataFrame): DataFrame containing the car dataset with columns [\'make\', \'model_year\', \'fuel_type\', \'engine_power\', \'safety_rating\', \'price\']. Returns: Pipeline: The fitted Pipeline object after training. # Splitting features and target X = df.drop(\'price\', axis=1) y = df[\'price\'] # Preprocessing for categorical features categorical_features = [\'make\', \'fuel_type\'] categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Preprocessing for numerical features numerical_features = [\'model_year\', \'engine_power\', \'safety_rating\'] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()) ]) # Bundle preprocessing for categorical and numerical data preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Define the regression models model_candidates = { \'linear_regression\': LinearRegression(), \'ridge_regression\': Ridge() } # Create a pipeline that caches the transformations and model model_pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', model_candidates[\'ridge_regression\']) ], memory=\'cache_directory\') # Parameters grid param_grid = [ { \'regressor\': [LinearRegression()], \'regressor__fit_intercept\': [True, False] }, { \'regressor\': [Ridge()], \'regressor__alpha\': [0.1, 1.0, 10.0], \'regressor__fit_intercept\': [True, False] } ] # Using GridSearchCV to find the best model and parameters grid_search = GridSearchCV(estimator=model_pipeline, param_grid=param_grid, cv=5) # Fit the model grid_search.fit(X, y) return grid_search.best_estimator_"},{"question":"# Configuration File Management with `configparser` You have a set of configuration files provided by different teams in your organization. Each team has its format and preferences for storing configuration options. Your task is to write a Python script that reads these configuration files, performs some transformations, and merges them into a single configuration file while handling possible conflicts and customizing behaviors. Task Requirements: 1. **Reading Configuration Files:** - Read the given list of configuration files. - Ensure to handle any missing files gracefully. 2. **Specifying Default Values:** - Define default values for certain keys that will be used if the keys are not specified in the individual configuration files. 3. **Updating and Merging Configurations:** - Update the configuration data based on a given dictionary and a string, ensuring the most recent values are retained. - Merge these configurations into a single configuration object. 4. **Handling Different Data Types:** - Ensure integer, float, and boolean values are correctly recognized and converted. 5. **Interpolation:** - Use extended interpolation to allow values from different sections and keys to reference each other. 6. **Writing the Final Configuration:** - Write the merged configuration data to an output file. - Preserve the order of sections and keys. Input: - A list of filenames: `filenames` - A dictionary of updates: `update_dict` - A string containing additional configuration data: `update_string` Output: - A single configuration file named `merged_config.ini`. Constraints: - Ensure options without values are accepted. - Handle comments appropriately, including inline comments. Performance Requirements: - Efficient handling of multiple configuration files. Example: ```python import configparser from typing import List, Dict def merge_configurations(filenames: List[str], update_dict: Dict, update_string: str) -> None: # Your implementation here. # Example usage filenames = [\\"config1.ini\\", \\"config2.ini\\"] update_dict = {\\"extra.section\\": {\\"new_option\\": \\"new_value\\"}} update_string = [extra.section] added_option = added_value merge_configurations(filenames, update_dict, update_string) ``` `config1.ini` ```ini [DEFAULT] Compression = yes CompressionLevel = 9 [team.alpha] ServerAliveInterval = 45 ``` `config2.ini` ```ini [team.beta] Port = 50022 ForwardX11 = no ``` Expected output in `merged_config.ini` should handle: - Merging `team.alpha` and `team.beta`. - Including `extra.section` with the new and updated options. - Correctly applying boolean, integer, and extended interpolations if necessary. Notes: - Use `configparser.ConfigParser` with relevant options tailored to the task. - Pay attention to handling missing files and comments. - Ensure values like integers and booleans are correctly processed. Good luck!","solution":"import configparser import os from typing import List, Dict def merge_configurations(filenames: List[str], update_dict: Dict[str, Dict[str, str]], update_string: str) -> None: config = configparser.ConfigParser(inline_comment_prefixes=(\\";\\", \\"#\\"), interpolation=configparser.ExtendedInterpolation()) # Default values config[\'DEFAULT\'] = { \'Compression\': \'no\', \'CompressionLevel\': \'5\' } # Read existing configuration files for filename in filenames: if os.path.exists(filename): config.read(filename) # Update configuration with dictionary values for section, options in update_dict.items(): if section not in config: config.add_section(section) for key, value in options.items(): config[section][key] = value # Update configuration with string values config.read_string(update_string) # Write the merged configuration to a file with open(\'merged_config.ini\', \'w\') as configfile: config.write(configfile)"},{"question":"You are required to create a visual analysis of the provided diamonds dataset using the Seaborn `boxenplot` function. Your task is to write a function `create_diamond_plot` that will: 1. Load the `diamonds` dataset using Seaborn. 2. Create a `boxenplot` visualizing the distribution of diamond `price` across different `cut` categories. 3. Further group the data by the `color` of the diamonds, ensuring the boxes are colored by the `color` categories and are dodged to avoid overlap. 4. Ensure that the width of the boxes is chosen using a linear method. 5. Customize the plot by setting the following: - `linewidth` of the box outlines to 0.75. - `line_kws` to set the median line width to 1.5 and color to \\"blue\\". - `flier_kws` to set the facecolor of outliers to \\"red\\" and linewidth to 0.5. 6. Display the plot. Function Signature ```python def create_diamond_plot(): pass ``` Constraints - Use the Seaborn package for plotting. - Ensure all settings and customizations specified are applied correctly. - Ensure the plot is clear and readable with labels and titles where applicable. Expected Output A Seaborn `boxenplot` should be displayed according to the specifications above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_diamond_plot(): Load the diamonds dataset from Seaborn and create a boxenplot visualizing the price distribution across different cut categories, grouped by diamond color. Customizations include linewidths for box boundaries and median lines, coloring for outliers, and separate boxes for different colors to prevent overlap. # Load the dataset diamonds = sns.load_dataset(\'diamonds\') # Create the boxenplot sns.boxenplot( data=diamonds, x=\'cut\', y=\'price\', hue=\'color\', linewidth=0.75, k_depth=\'full\', width=0.8, dodge=True, palette=\'colorblind\' ) # Customize plot details plt.xlabel(\'Cut\') plt.ylabel(\'Price\') plt.title(\'Distribution of Diamond Prices by Cut and Color\') plt.legend(title=\'Color\') plt.show()"},{"question":"**Question Title:** Preprocessing Pipeline Implementation with scikit-learn **Question Description:** In this task, you are required to create a preprocessing pipeline for a given dataset using the `sklearn.preprocessing` module. The dataset contains both numerical and categorical features, and you need to perform the following preprocessing steps: 1. **Standardize** the numerical features to have zero mean and unit variance. 2. **Encode** the categorical features using One-Hot Encoding. 3. **Normalize** the entire dataset to have unit norm. You must implement this using scikit-learns `Pipeline` and `ColumnTransformer` classes. The input features will be given in a numpy array, and you need to return the transformed array. **Requirements:** 1. Use the `StandardScaler` for numerical features. 2. Use the `OneHotEncoder` for categorical features. 3. Use the `Normalizer` for the entire dataset. **Input:** - `X`: A 2D numpy array of shape (num_samples, num_features) containing both numerical and categorical features. The first half of the columns will be numerical and the second half will be categorical. ```python import numpy as np X = np.array([ [1.0, 2.0, \'male\', \'US\'], [3.0, 4.0, \'female\', \'Europe\'], [5.0, 6.0, \'female\', \'Asia\'], ... ]) ``` **Output:** - A 2D numpy array of shape (num_samples, transformed_num_features) containing the standardized, encoded, and normalized features. **Constraints:** - You can assume that the first half of the features are numerical and the second half are categorical. **Performance Requirements:** - The implementation should efficiently handle datasets with up to 10,000 samples and 50 features. **Example:** ```python import numpy as np from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer from sklearn.pipeline import Pipeline def preprocess_data(X): num_features = X.shape[1] // 2 cat_features = X.shape[1] - num_features numerical_features = list(range(num_features)) categorical_features = list(range(num_features, num_features + cat_features)) # Create transformers for numerical and categorical features preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numerical_features), (\'cat\', OneHotEncoder(), categorical_features) ] ) # Create a pipeline that standardizes and then normalizes the data pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'normalizer\', Normalizer()) ]) # Fit and transform the data X_transformed = pipeline.fit_transform(X) return X_transformed # Example usage X = np.array([ [1.0, 2.0, \'male\', \'US\'], [3.0, 4.0, \'female\', \'Europe\'], [5.0, 6.0, \'female\', \'Asia\'] ]) preprocess_data(X) ``` In this question, students need to demonstrate their understanding of scikit-learn preprocessing pipelines, handling both numerical and categorical data, and applying transformations in sequence.","solution":"import numpy as np from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer from sklearn.pipeline import Pipeline def preprocess_data(X): num_features = X.shape[1] // 2 cat_features = X.shape[1] - num_features numerical_features = list(range(num_features)) categorical_features = list(range(num_features, num_features + cat_features)) # Create transformers for numerical and categorical features preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numerical_features), (\'cat\', OneHotEncoder(), categorical_features) ] ) # Create a pipeline that standardizes and then normalizes the data pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'normalizer\', Normalizer()) ]) # Fit and transform the data X_transformed = pipeline.fit_transform(X) return X_transformed"},{"question":"**Objective:** Write a Python function that attempts to open a file and handles various potential errors using the `errno` module. **Function Signature:** ```python def open_file_with_error_handling(filepath: str) -> str: pass ``` **Description:** Your task is to implement the `open_file_with_error_handling` function that takes a file path as input and attempts to open the specified file for reading. If the file opens successfully, the function should return the first line of the file. If an error occurs, the function should return a meaningful error message corresponding to the type of error encountered. **Input:** - `filepath` (str): The path to the file you want to open. **Output:** - (str): The first line of the file, if opened successfully. If an error occurs, return an appropriate error message string. **Error Handling:** Your function should handle the following errors using the `errno` module and return respective messages: - `ENOENT` (No such file or directory): Return `\\"Error: No such file or directory\\"` - `EACCES` (Permission denied): Return `\\"Error: Permission denied\\"` - `EISDIR` (Is a directory): Return `\\"Error: Is a directory\\"` - `EMFILE` (Too many open files): Return `\\"Error: Too many open files\\"` - Any other error should return a default message including the errno code and description, formatted as `\\"Error [errno_code]: description\\"` **Constraints:** - Your solution should only use standard Python modules (`errno`, `os`, etc.). - Properly comment your code for readability. **Example Usage:** ```python # Assuming a file \\"example.txt\\" is present in the current directory print(open_file_with_error_handling(\\"example.txt\\")) # Output: The first line of the file # Trying to open a non-existent file print(open_file_with_error_handling(\\"non_existent.txt\\")) # Output: Error: No such file or directory # Trying to open a directory as a file print(open_file_with_error_handling(\\"/home/\\")) # Output: Error: Is a directory ``` **Hints:** - Use a try-except block to catch the IOError/OSError and retrieve the error number using `e.errno`. - Use `errno.errorcode.get()` to map the errno value to its string name for custom errors.","solution":"import errno def open_file_with_error_handling(filepath: str) -> str: Attempts to open a file and handles various potential errors using the errno module. Parameters: filepath (str): The path to the file you want to open. Returns: str: The first line of the file if opened successfully, or an appropriate error message. try: with open(filepath, \'r\') as file: return file.readline().strip() except OSError as e: if e.errno == errno.ENOENT: return \\"Error: No such file or directory\\" elif e.errno == errno.EACCES: return \\"Error: Permission denied\\" elif e.errno == errno.EISDIR: return \\"Error: Is a directory\\" elif e.errno == errno.EMFILE: return \\"Error: Too many open files\\" else: return f\\"Error [{e.errno}]: {e.strerror}\\""},{"question":"Email Processing with Iterators You are tasked with writing a Python function to process email messages using the iterators provided by the `email.iterators` module. The goal is to extract specific parts of the email as specified by MIME type and print a readable representation of the email\'s structure. Part 1: Extract Plain Text Parts Write a function `extract_plain_text_parts(msg)` that accepts an `email.message.Message` object `msg` and returns a list of strings. Each string in the list should be the payload of a part that has a MIME type of `text/plain`. Part 2: Print Email Structure Write a function `print_email_structure(msg)` that accepts an `email.message.Message` object `msg` and prints the structure of the email message similar to the output of `email.iterators._structure`. Function Signatures ```python import email from email.iterators import typed_subpart_iterator, _structure def extract_plain_text_parts(msg: email.message.Message) -> list: pass def print_email_structure(msg: email.message.Message) -> None: pass ``` Input and Output Formats - `msg`: An instance of `email.message.Message`. - `extract_plain_text_parts` should return a list of string payloads for parts with MIME type `text/plain`. - `print_email_structure` should print the email structure to standard output. Constraints - Do not assume the message is in any specific structure; it can have multiple nested parts and various MIME types. - You should use `typed_subpart_iterator` to handle the part extraction and `_structure` for structure printing. Example Usage ```python import email from email import policy from email.parser import BytesParser # Sample email data data = b\'\'\'Content-Type: multipart/mixed; boundary=\\"===============7330845974216740156==\\" MIME-Version: 1.0 --===============7330845974216740156== Content-Type: text/plain This is a plain text message. --===============7330845974216740156== Content-Type: text/html <html><body>This is an HTML message.</body></html> --===============7330845974216740156==--\'\'\' msg = BytesParser(policy=policy.default).parsebytes(data) print(extract_plain_text_parts(msg)) # Output: [\'This is a plain text message.\'] print_email_structure(msg) # Expected printed structure: # multipart/mixed # text/plain # text/html ``` Performance Requirements - Your solution should handle emails with nested structures efficiently. - Ensure that the functions do not consume excessive memory or processing time. Use the provided documentation and your Python skills to complete the above tasks. Happy coding!","solution":"import email from email.iterators import typed_subpart_iterator, _structure def extract_plain_text_parts(msg: email.message.Message) -> list: Extracts parts with MIME type \'text/plain\' from a given email message. Parameters: msg (email.message.Message): The email message object. Returns: list: A list of strings containing the payloads of \'text/plain\' parts. plain_text_parts = [] for part in typed_subpart_iterator(msg, \'text\', \'plain\'): plain_text_parts.append(part.get_payload(decode=True).decode(\'utf-8\')) return plain_text_parts def print_email_structure(msg: email.message.Message) -> None: Prints the structure of the given email message. Parameters: msg (email.message.Message): The email message object. _structure(msg)"},{"question":"# Question: Parallel File Processing with `concurrent.futures` You are provided with a list of text file paths. Your task is to analyze each file to count the number of words it contains. Given the large number of files and their sizes, you should implement this word count using parallel processing to speed up the task. You need to write a function `count_words_parallel(file_paths: List[str]) -> Dict[str, int]`. This function should use the `concurrent.futures` module to count words in each file in parallel and return a dictionary where the keys are the file paths and the values are the corresponding word counts. # Input - `file_paths`: A list of strings where each string is a path to a text file. # Output - A dictionary where each key is a file path from the input list and the corresponding value is the word count of that file. # Constraints - Each text file can be large, so performance optimization with parallel processing is crucial. - Assume all files are well-formed text files and can be read without encoding issues. - Use `ThreadPoolExecutor` if the task is I/O-bound (reading from files), and justify your choice if asked. # Example ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] # Assume file1.txt contains \\"Hello world\\" # Assume file2.txt contains \\"Hello again\\" # Assume file3.txt contains \\"This is a file with multiple words\\" result = count_words_parallel(file_paths) print(result) # Output: {\'file1.txt\': 2, \'file2.txt\': 2, \'file3.txt\': 6} ``` # Implementation Requirements - Use `concurrent.futures.ThreadPoolExecutor` for parallel processing. - Ensure proper handling of file I/O. - Ensure that the final dictionary contains accurate word counts for each file. # Performance Considerations - Use a suitable number of workers in the thread pool to handle the file processing efficiently. - Justify your choice of using `ThreadPoolExecutor` due to the I/O-bound nature of file reading tasks.","solution":"from typing import List, Dict import concurrent.futures def count_words_in_file(file_path: str) -> int: Helper function to count the number of words in a single file. try: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() words = content.split() return len(words) except Exception as e: print(f\\"Error reading file {file_path}: {e}\\") return 0 def count_words_parallel(file_paths: List[str]) -> Dict[str, int]: Count the number of words in each file in parallel. :param file_paths: List of strings where each string is a path to a text file. :return: Dictionary with file paths as keys and word counts as values. word_counts = {} with concurrent.futures.ThreadPoolExecutor() as executor: future_to_file = {executor.submit(count_words_in_file, file): file for file in file_paths} for future in concurrent.futures.as_completed(future_to_file): file = future_to_file[future] try: word_counts[file] = future.result() except Exception as e: print(f\\"Error processing file {file}: {e}\\") word_counts[file] = 0 return word_counts"},{"question":"# Custom Tab Completion and History Manager in Python Objective: Implement a custom tab completion and history management utility for Python. The goal is to create functions that mimic the tab completion and command history saving features as described in the documentation. You will use basic Python functionalities to implement this without relying on external packages like `readline`. Requirements: 1. **Tab Completion Function**: - **Input**: A string representing the current input. - **Output**: A list of possible completions based on the input string. - **Behavior**: The function should be able to auto-complete variable and function names based on the currently defined scope. 2. **History Management**: - Maintain a history of commands entered. - **Functions**: - `add_to_history(command: str)`: Add a command to the history. - `get_history()`: Retrieve the list of commands from history. - **Behavior**: The history should persist across different invocations within the same session. Constraints: - Do not use external libraries such as `readline`. - Ensure that the tab completion handles both global and local scope. - Implement the history management in such a way that it can work with a simple REPL (Read-Eval-Print Loop). Example: ```python def tab_complete(current_input: str) -> list: # Your implementation here pass command_history = [] def add_to_history(command: str): # Your implementation here pass def get_history() -> list: # Your implementation here pass # Example Usage add_to_history(\\"print(\'Hello World\')\\") add_to_history(\\"x = 42\\") print(get_history()) # Output: [\\"print(\'Hello World\')\\", \\"x = 42\\"] # Assuming \'cur\' is the only variable starting with \'cu\' in scope print(tab_complete(\\"cu\\")) # Output: [\'cur\'] ``` Tips: - You might want to use the `dir()` function to list current variables and functions in scope. - To simulate a basic REPL, you can use a while loop that accepts user input, executes it, and updates the history.","solution":"import builtins def tab_complete(current_input: str, scope: dict) -> list: Returns a list of possible completions based on the input string. possible_completions = [] for symbol in dir(builtins) + list(scope.keys()): if symbol.startswith(current_input): possible_completions.append(symbol) return possible_completions command_history = [] def add_to_history(command: str): Add a command to the history. global command_history command_history.append(command) def get_history() -> list: Retrieve the list of commands from history. global command_history return command_history"},{"question":"**Distributed Training with PyTorch** In this coding assessment, you are required to implement a distributed training setup for a PyTorch-based deep learning model using multiple GPUs. # Objective - Write a distributed training script using `torch.distributed` module to train a neural network on a given dataset. # Input Format 1. The script should be invoked using `torchrun` utility. 2. Expected input arguments to the script: - `--epochs`: Number of epochs for training (integer). - `--batch-size`: Batch size for training (integer). - `--backend`: The backend to use (either \'nccl\' or \'gloo\'). 3. Standard dataset for training (e.g., CIFAR-10) should be downloaded within the script. 4. Neural network architecture can be a simple CNN. # Output Format - The script should output training logs, showing loss and accuracy for every epoch. - Final trained model parameters should be saved to a file named `model.pth`. # Constraints and Requirements - The script must handle initializing and finalizing the distributed process groups. - Ensure proper data loading and distribution across available GPUs. - Use a learning rate scheduler for better convergence. - The script should be able to run multiple processes on multiple GPUs. - Maintain synchronization between processes to ensure consistent model training. # Performance Requirements - Ensure that the training process scales effectively with the number of GPUs. - Minimize inter-process communication overhead for better performance. # Example Usage ```bash torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=\\"localhost\\" --master_port=12345 train.py --epochs 10 --batch-size 64 --backend nccl ``` # Implementation Details: 1. **Initialization:** Use `torch.distributed.init_process_group` to initialize the distributed environment. 2. **Model Definition:** Define a simple Convolutional Neural Network (CNN). 3. **DataLoader:** Use `torch.utils.data.distributed.DistributedSampler` to distribute dataset across multiple GPUs. 4. **Training Loop:** Implement the training loop with synchronization using `torch.distributed`. 5. **Checkpointing:** Save the model weights synchronously after training. **BONUS:** Include a section in the script to measure and print the time taken for each epoch to monitor performance. ```python import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.utils.data import torch.utils.data.distributed from torchvision import datasets, transforms, models import time import argparse def main(): parser = argparse.ArgumentParser(description=\'PyTorch Distributed Training Example\') parser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\', help=\'number of epochs to train (default: 10)\') parser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\', help=\'input batch size for training (default: 64)\') parser.add_argument(\'--backend\', type=str, default=\'nccl\', help=\\"backend for distributed training (default: \'nccl\')\\") args = parser.parse_args() dist.init_process_group(backend=args.backend) torch.cuda.set_device(dist.get_rank() % torch.cuda.device_count()) transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) train_dataset = datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform) train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, sampler=train_sampler) model = models.resnet18() model = model.cuda() model = nn.parallel.DistributedDataParallel(model) criterion = nn.CrossEntropyLoss().cuda() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) for epoch in range(1, args.epochs + 1): model.train() epoch_loss, epoch_acc = 0.0, 0.0 start_time = time.time() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.cuda(), target.cuda() optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() _, predicted = torch.max(output, 1) correct = (predicted == target).sum().item() epoch_loss += loss.item() epoch_acc += correct scheduler.step() epoch_loss /= len(train_loader.dataset) epoch_acc /= len(train_loader.dataset) if dist.get_rank() == 0: print(f\'Epoch {epoch}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Time: {time.time() - start_time:.2f}s\') if dist.get_rank() == 0: torch.save(model.state_dict(), \'model.pth\') dist.destroy_process_group() if __name__ == \\"__main__\\": main() ```","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.utils.data import torch.utils.data.distributed from torchvision import datasets, transforms, models import time import argparse def main(): parser = argparse.ArgumentParser(description=\'PyTorch Distributed Training Example\') parser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\', help=\'number of epochs to train (default: 10)\') parser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\', help=\'input batch size for training (default: 64)\') parser.add_argument(\'--backend\', type=str, default=\'nccl\', help=\\"backend for distributed training (default: \'nccl\')\\") args = parser.parse_args() dist.init_process_group(backend=args.backend) torch.cuda.set_device(dist.get_rank() % torch.cuda.device_count()) transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) train_dataset = datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform) train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, sampler=train_sampler) model = models.resnet18() model = model.cuda() model = nn.parallel.DistributedDataParallel(model) criterion = nn.CrossEntropyLoss().cuda() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) for epoch in range(1, args.epochs + 1): model.train() epoch_loss, epoch_acc = 0.0, 0.0 start_time = time.time() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.cuda(), target.cuda() optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() _, predicted = torch.max(output, 1) correct = (predicted == target).sum().item() epoch_loss += loss.item() epoch_acc += correct scheduler.step() epoch_loss /= len(train_loader.dataset) epoch_acc /= len(train_loader.dataset) if dist.get_rank() == 0: print(f\'Epoch {epoch}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Time: {time.time() - start_time:.2f}s\') if dist.get_rank() == 0: torch.save(model.state_dict(), \'model.pth\') dist.destroy_process_group() if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch MPS Device and Profiling You are tasked with creating an enhanced matrix multiplication function that utilizes PyTorch\'s MPS acceleration on Apple devices and profiles the execution using the MPS Profiler. # Requirements: 1. **Function `enhanced_matrix_multiplication`**: - **Input**: - `matrix_a` (Tensor): A PyTorch tensor of shape `(m, n)`. - `matrix_b` (Tensor): A PyTorch tensor of shape `(n, p)`. - **Output**: - A PyTorch tensor of shape `(m, p)` resulting from the matrix multiplication of `matrix_a` and `matrix_b`. 2. Verify if MPS is available on the device. If not, raise an appropriate error. 3. Utilize the MPS Profiler to profile the matrix multiplication operation. 4. Print the time taken for the matrix multiplication when profiled. 5. Ensure any memory utilized during the operation is properly managed by freeing up the cache after the operation using `torch.mps.empty_cache()`. # Constraints: - You must ensure that the inputs are valid matrices for multiplication (i.e., `matrix_a`\'s number of columns must equal `matrix_b`\'s number of rows). # Example Usage ```python import torch matrix_a = torch.randn(2, 3, device=\'mps\') matrix_b = torch.randn(3, 4, device=\'mps\') result = enhanced_matrix_multiplication(matrix_a, matrix_b) print(result) ``` # Function Signature ```python def enhanced_matrix_multiplication(matrix_a: torch.Tensor, matrix_b: torch.Tensor) -> torch.Tensor: # Your code here ``` # Additional Information - You can find more details about how to use the MPS Profiler and device management tools in the `torch.mps` module documentation.","solution":"import torch def enhanced_matrix_multiplication(matrix_a: torch.Tensor, matrix_b: torch.Tensor) -> torch.Tensor: if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS device is not available. Please ensure you are running this on an Apple device with MPS support.\\") if matrix_a.size(1) != matrix_b.size(0): raise ValueError(\\"matrix_a\'s number of columns must equal matrix_b\'s number of rows for multiplication.\\") with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.MPS]) as prof: result = torch.matmul(matrix_a, matrix_b) print(prof.key_averages().table(sort_by=\\"cpu_time_total\\", row_limit=10)) torch.mps.empty_cache() return result"},{"question":"# Question: Implement a Custom Generator in Python Python generator objects are a powerful tool to create iterators. Your task is to implement a custom generator function that simulates a simplified version of the generator behavior, demonstrating the use of key principles of generators, such as yielding values and creating generator frames. Task Description 1. **Create a custom generator function `custom_generator`**: - It should accept an integer `n`. - It should yield the first `n` Fibonacci numbers iteratively. 2. **Implement a function `is_generator`**: - It should accept a single argument and return `True` if the argument is a generator object, otherwise `False`. - Use the principles provided in the documentation to perform this check. Input - An integer `n` for the `custom_generator` function. - An object for the `is_generator` function. Output - A generator producing the first `n` Fibonacci numbers for `custom_generator`. - A boolean value for `is_generator`. Constraints - `n` will be a positive integer less than or equal to 100. Examples ```python def custom_generator(n): # Your code here. def is_generator(obj): # Your code here. # Example Usage: gen = custom_generator(5) print(next(gen)) # Output: 0 print(next(gen)) # Output: 1 print(list(gen)) # Output: [1, 2, 3] print(is_generator(gen)) # Output: True print(is_generator([1, 2, 3])) # Output: False ``` Notes - Ensure your implementation handles edge cases such as yielding no values if `n` is 0. - Do not use external libraries to check the type of the object. Good luck!","solution":"def custom_generator(n): A custom generator that yields the first n Fibonacci numbers. a, b = 0, 1 for _ in range(n): yield a a, b = b, a + b def is_generator(obj): Checks if the object is a generator. return (hasattr(obj, \'send\') and callable(obj.send) and hasattr(obj, \'throw\') and callable(obj.throw) and hasattr(obj, \'close\') and callable(obj.close) and hasattr(obj, \'__iter__\') and hasattr(obj, \'__next__\'))"},{"question":"Objective: Create a complex visualization using Seaborn\'s `FacetGrid` class to analyze the \\"tips\\" dataset. Problem Statement: You are provided with the \\"tips\\" dataset from Seaborn. Your task is to create a facet grid that visualizes the relationship between the total bill amount and the tip amount, segmented by time of day (Lunch or Dinner) and gender (Male or Female). Additionally, you need to include a third segmentation based on the size of the dining party. Instructions: 1. Load the \\"tips\\" dataset from Seaborn. 2. Create a `FacetGrid` with: - Columns for `time` - Rows for `sex` - Colors (hue) for `smoker` status 3. Map the `scatterplot` function to visualize `total_bill` on the x-axis and `tip` on the y-axis. 4. Include a separate `histplot` showing the distribution of `total_bill` for each facet. 5. Add a reference line for the median `tip` value within each facet. 6. Customize the layout: - Set the size of each facet to `4x4`. - Adjust axis labels to \\"Total Bill\\" and \\"Tip\\". - Add an overall legend for the `hue` parameter. 7. Be sure to annotate each plot with the count of observations in that facet using a custom function. Constraints: - Use the provided dataset as is; do not modify the data. - Ensure that all plots on each facet have the same appearance for consistency. Expected Output: Produce a plot that matches the described specifications, within a single visual output using Seaborn and Matplotlib. Example Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create a custom annotation function def annotate(data, **kws): n = len(data) ax = plt.gca() ax.text(.1, .6, f\\"N = {n}\\", transform=ax.transAxes) # Create the FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\", hue=\\"smoker\\", height=4, aspect=1) # Map the plots g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") g.map_dataframe(sns.histplot, x=\\"total_bill\\") # Add reference lines for the median tip g.refline(y=tips[\\"tip\\"].median()) # Add annotations g.map_dataframe(annotate) # Customize the layout and labels g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") g.add_legend() # Save or show the plot plt.show() ``` Complete this task in a Jupyter Notebook or Python script.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_facet_grid(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create a custom annotation function def annotate(data, **kws): n = len(data) ax = plt.gca() ax.text(.1, .6, f\\"N = {n}\\", transform=ax.transAxes) # Create the FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\", hue=\\"smoker\\", height=4, aspect=1) # Map the scatterplot g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Map the histplot g.map_dataframe(sns.histplot, x=\\"total_bill\\") # Add reference lines for the median tip g.refline(y=tips[\\"tip\\"].median()) # Add annotations g.map_dataframe(annotate) # Customize the layout and labels g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") g.add_legend() # Save or show the plot plt.show() # Call the function to generate the plot create_facet_grid()"},{"question":"<|Analysis Begin|> The provided documentation is extensive and detailed, covering the `codecs` module in Python, which handles various encoding and decoding tasks, including text encodings, byte encodings, error handling, and more. Key aspects of the `codecs` module: 1. **Basic Functions**: Includes `codecs.encode()`, `codecs.decode()`, and `codecs.lookup()`. 2. **CodecInfo Class**: Describes a codec\'s properties. 3. **Error Handlers**: Various error strategies for handling encoding/decoding errors. 4. **Stateless Encoding/Decoding**: `Codec.encode()` and `Codec.decode()`. 5. **Incremental Encoding/Decoding**: `IncrementalEncoder` and `IncrementalDecoder` classes. 6. **Stream Encoding/Decoding**: `StreamWriter` and `StreamReader` classes. 7. **Text Transformations**: Specific codecs like `rot13` for text-to-text transformations. 8. **Binary Transforms**: Mapping rules for binary data encoding. Considering these aspects, we can design a question that tests the student\'s understanding of encoding/decoding, error handling, and custom codec implementation. <|Analysis End|> <|Question Begin|> # Advanced Python Coding Assessment **Objective**: To assess your understanding of the `codecs` module in Python, specifically focusing on implementing custom codecs and handling encoding/decoding with various error strategies. # Problem Description You are required to create a custom codec for a fictional encoding named `custom-hex`. This encoding should translate text to its hexadecimal ASCII representation and vice versa. In addition, you are to handle specific custom errors during the encoding and decoding processes. Encoding (Text to custom-hex) 1. Each character in the input string should be converted to its hexadecimal ASCII value. 2. Use `-` (dash) as a separator between character codes. Decoding (custom-hex to Text) 1. Convert each hexadecimal ASCII value back to its original character. 2. Ignore malformed data using the `\'ignore\'` error handling strategy. 3. Replace malformed data with `?` using the `\'replace\'` error handling strategy. # Input and Output Formats Function `encode_to_custom_hex` - **Input**: A string `text`. - **Output**: A string encoded in `custom-hex`. Function `decode_from_custom_hex` - **Input**: A string `custom_hex` and an optional error handling strategy `errors` which defaults to `\'strict\'`. - **Output**: A decoded string, following specified error strategies. # Constraints 1. The `errors` parameter for decoding can take only three values: `\'strict\'`, `\'ignore\'`, or `\'replace\'`. # Implementation Define the following functions: ```python def encode_to_custom_hex(text: str) -> str: # Convert text to custom-hex encoded string pass def decode_from_custom_hex(custom_hex: str, errors: str = \'strict\') -> str: # Convert custom-hex encoded string back to text # Handle errors based on the \'errors\' parameter pass ``` # Example ```python # Encoding Example input_text = \\"Hello\\" encoded = encode_to_custom_hex(input_text) print(encoded) # Should print: \'48-65-6c-6c-6f\' # Decoding Example - with \'ignore\' encoded_text = \\"48-65-6c-6c-6f-zz-20\\" decoded = decode_from_custom_hex(encoded_text, errors=\'ignore\') print(decoded) # Should print: \'Hello\' # Decoding Example - with \'replace\' decoded = decode_from_custom_hex(encoded_text, errors=\'replace\') print(decoded) # Should print: \'Hello? \' ``` # Notes - Use `codecs.register_error()` to handle the custom error strategies. - Ensure that your code gracefully handles other invalid inputs. Implement the functions `encode_to_custom_hex` and `decode_from_custom_hex` accordingly.","solution":"import codecs def encode_to_custom_hex(text: str) -> str: Encodes a given text to a custom-hex encoded string. return \'-\'.join(f\'{ord(char):02x}\' for char in text) def custom_ignore_error(exception: UnicodeError): Custom ignore error handler. return (\'\', exception.end) def custom_replace_error(exception: UnicodeError): Custom replace error handler. return (\'?\', exception.end) def decode_from_custom_hex(custom_hex: str, errors: str = \'strict\') -> str: Decodes a custom-hex encoded string back to text. Handles errors based on the \'errors\' parameter. codecs.register_error(\'custom_ignore\', custom_ignore_error) codecs.register_error(\'custom_replace\', custom_replace_error) hex_values = custom_hex.split(\'-\') decoded_chars = [] for hex_value in hex_values: try: decoded_char = chr(int(hex_value, 16)) decoded_chars.append(decoded_char) except ValueError: if errors == \'ignore\': continue elif errors == \'replace\': decoded_chars.append(\'?\') else: raise UnicodeDecodeError(\\"custom-hex\\", b\'\', 0, len(hex_value), \\"Value out of range\\") return \'\'.join(decoded_chars)"},{"question":"# Custom Exception Handling and Chaining in Python Problem Statement You are tasked with creating a robust event logging system for processing data files. The system should be able to handle various types of errors gracefully, log useful error messages, and maintain context for debugging purposes. Requirements 1. Define a custom exception hierarchy for the logging system: - `LoggingError`: The base class for all logging-related exceptions (inherits from `Exception`). - `FileError`: Raised for issues related to file operations (inherits from `LoggingError`). - `DataError`: Raised for errors related to data processing (inherits from `LoggingError`). 2. Implement the following behaviors: - Attempt to read a specified file for processing. - If the file does not exist, raise and catch a `FileNotFoundError`, and then raise a `FileError` while preserving the context. - If the data within the file is invalid (e.g., contains syntax errors), raise a custom `InvalidDataError` (inherits from `DataError`), and chain it from the original `SyntaxError`. - Implement proper logging of each exception raised, including the full traceback and any contextual information. Constraints and Performance Requirements - The file reading operation should be efficient and handle large files gracefully. - The logging system should clearly indicate the type of each error and provide useful debugging information. Use the built-in `logging` module for logging the errors. Input and Output Formats - *Input*: A file path (string) to a data file. - *Output*: No direct output. Errors should be logged using the Python logging module. Example Given a file path `data.txt`: - If `data.txt` does not exist, the system should log a `FileError` with context from the `FileNotFoundError`. - If `data.txt` contains invalid data, the system should log an `InvalidDataError` with context from the `SyntaxError`. Implementation Details - Create a main function `process_file(file_path: str) -> None` which: - Tries to read the file and handles exceptions as described above. - Define the custom exceptions as described. - Use the Python `logging` module to log the exceptions. ```python import logging import traceback # Set up logging logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s - %(levelname)s - %(message)s\') class LoggingError(Exception): Base class for all logging-related exceptions pass class FileError(LoggingError): Exception raised for file-related errors pass class DataError(LoggingError): Exception raised for data processing errors pass class InvalidDataError(DataError): Exception raised for invalid data in file pass def process_file(file_path: str) -> None: try: # Attempt to open the file with open(file_path, \'r\') as file: # Simulate reading and processing data data = file.read() # Simulate a syntax error in data processing if \'invalid\' in data: raise SyntaxError(\\"Invalid syntax in data\\") except FileNotFoundError as fnf_error: exc = FileError(\\"File could not be found\\") exc.__cause__ = fnf_error logging.error(\\"FileError raised\\", exc_info=exc) raise exc except SyntaxError as sy_error: exc = InvalidDataError(\\"Data processing error due to invalid syntax\\") exc.__cause__ = sy_error logging.error(\\"InvalidDataError raised\\", exc_info=exc) raise exc # Test cases (uncomment to run) # process_file(\'nonexistent_file.txt\') # process_file(\'invalid_data.txt\') ``` Ensure that you test your implementation with different scenarios to validate the exception handling and traceability.","solution":"import logging # Set up logging logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s - %(levelname)s - %(message)s\') class LoggingError(Exception): Base class for all logging-related exceptions pass class FileError(LoggingError): Exception raised for file-related errors pass class DataError(LoggingError): Exception raised for data processing errors pass class InvalidDataError(DataError): Exception raised for invalid data in file pass def process_file(file_path: str) -> None: try: # Attempt to open the file with open(file_path, \'r\') as file: # Simulate reading and processing data data = file.read() # Simulate a syntax error in data processing if \'invalid\' in data: raise SyntaxError(\\"Invalid syntax in data\\") except FileNotFoundError as fnf_error: exc = FileError(\\"File could not be found\\") exc.__cause__ = fnf_error logging.error(\\"FileError raised\\", exc_info=exc) raise exc except SyntaxError as sy_error: exc = InvalidDataError(\\"Data processing error due to invalid syntax\\") exc.__cause__ = sy_error logging.error(\\"InvalidDataError raised\\", exc_info=exc) raise exc"},{"question":"Objective In this exercise, you will demonstrate your understanding of networking and inter-process communication concepts by implementing a simple chat server and client using the `socket` module in Python 3.10. Your solution should handle multiple clients and ensure secure communication using the `ssl` module. # Task 1. **Chat Server Implementation:** - Write a Python function `start_chat_server(port: int, certfile: str, keyfile: str) -> None` that starts a secure chat server listening on the specified port. - The server should accept multiple clients and broadcast messages to all connected clients. - Use `ssl` to wrap the socket for secure communication. - Handle necessary exceptions to avoid server crashes. 2. **Chat Client Implementation:** - Write a Python function `start_chat_client(server_ip: str, server_port: int, cafile: str) -> None` that connects to the server using the given IP and port. - The client should be able to send messages to the server and receive messages from other clients through the server. - Use `ssl` to wrap the client socket for secure communication. - Handle necessary exceptions to ensure robust client operations. # Function Signatures ```python def start_chat_server(port: int, certfile: str, keyfile: str) -> None: # Your implementation here pass def start_chat_client(server_ip: str, server_port: int, cafile: str) -> None: # Your implementation here pass ``` # Input 1. **For `start_chat_server` function:** - `port` (int): The port number on which the server will listen for incoming connections. - `certfile` (str): Path to the certificate file to be used for SSL. - `keyfile` (str): Path to the private key file to be used for SSL. 2. **For `start_chat_client` function:** - `server_ip` (str): The IP address of the chat server. - `server_port` (int): The port number of the chat server. - `cafile` (str): Path to the CA certificate file to validate the server\'s SSL certificate. # Output - The functions do not need to return any value. Instead, they should be able to handle network communication as described. # Constraints and Considerations - Ensure that the server and client can handle multiple connections and messages asynchronously. - Use proper synchronization mechanisms to handle concurrent operations. - The server and client should handle keyboard interrupts gracefully to allow clean shutdown. # Example (Pseudo-code) Server ```python def start_chat_server(port: int, certfile: str, keyfile: str) -> None: # Create a socket and wrap it with SSL # Bind to the specified port and listen for connections # Accept multiple client connections and broadcast incoming messages pass ``` Client ```python def start_chat_client(server_ip: str, server_port: int, cafile: str) -> None: # Connect to the server using SSL # Send messages to the server and display messages from other clients pass ``` Implement these functions to create a secure chat server and client system.","solution":"import socket import ssl import threading def start_chat_server(port: int, certfile: str, keyfile: str) -> None: def handle_client_connection(conn, addr): print(f\\"Connection established with {addr}\\") clients.add(conn) try: while True: data = conn.recv(1024) if not data: break broadcast(data, conn) except: pass finally: clients.remove(conn) conn.close() print(f\\"Connection closed with {addr}\\") def broadcast(data, sender_conn): for client in clients: if client != sender_conn: try: client.send(data) except: pass server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\\"\\", port)) server_socket.listen(5) print(f\\"Server listening on port {port}...\\") context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) context.load_cert_chain(certfile=certfile, keyfile=keyfile) clients = set() # Set to store client connections try: while True: client_socket, addr = server_socket.accept() conn = context.wrap_socket(client_socket, server_side=True) threading.Thread(target=handle_client_connection, args=(conn, addr)).start() except Exception as e: print(f\\"Server error: {e}\\") finally: server_socket.close() def start_chat_client(server_ip: str, server_port: int, cafile: str) -> None: client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH, cafile=cafile) conn = context.wrap_socket(client_socket, server_hostname=server_ip) try: conn.connect((server_ip, server_port)) print(f\\"Connected to chat server at {server_ip}:{server_port}\\") # Function to receive messages def receive_messages(): while True: try: data = conn.recv(1024) if not data: break print(f\\"Received message: {data.decode()}\\") except: break threading.Thread(target=receive_messages).start() while True: message = input() if message.lower() == \'exit\': break conn.send(message.encode()) except Exception as e: print(f\\"Client error: {e}\\") finally: conn.close() print(\\"Connection closed\\")"},{"question":"# Hashlib Coding Assessment Question Objective: You are tasked with creating a Python utility that uses the `hashlib` module to hash files and verify their integrity. This exercise will assess your ability to work with different hashing algorithms and methods provided by the `hashlib` module. Problem Statement: You need to implement a function `hash_file` to hash the contents of a file using the specified hashing algorithm. Also, you need to implement a function `verify_hash` to verify the file\'s integrity by comparing its hash to a previously calculated hash. Functions to Implement: 1. **hash_file(file_path: str, algorithm: str = \'sha256\') -> str** - **Input:** - `file_path` (str): The path to the file that needs to be hashed. - `algorithm` (str): The name of the hashing algorithm to use (default is \'sha256\'). - **Output:** - Returns a hexadecimal string representing the hash of the file contents. - **Constraints:** - The `algorithm` must be one of the algorithms available in `hashlib.algorithms_guaranteed`. 2. **verify_hash(file_path: str, expected_hash: str, algorithm: str = \'sha256\') -> bool** - **Input:** - `file_path` (str): The path to the file that needs its hash verified. - `expected_hash` (str): The expected hash value the file should produce. - `algorithm` (str): The name of the hashing algorithm to use (default is \'sha256\'). - **Output:** - Returns `True` if the hash of the file matches the `expected_hash`, `False` otherwise. - **Constraints:** - The `algorithm` must be one of the algorithms available in `hashlib.algorithms_guaranteed`. Example Usage: ```python # Example file path and algorithm file_path = \\"example.txt\\" algorithm = \\"sha256\\" # Create hash of the file file_hash = hash_file(file_path, algorithm) print(f\\"File Hash: {file_hash}\\") # Verify the file hash is_verified = verify_hash(file_path, file_hash, algorithm) print(f\\"Verification Status: {is_verified}\\") ``` Additional Notes: - Be sure to handle possible exceptions such as file not found or unsupported hash algorithms. - The function should read the file in chunks to support hashing large files without overloading memory.","solution":"import hashlib def hash_file(file_path: str, algorithm: str = \'sha256\') -> str: Hash the contents of a file using the specified hashing algorithm. Parameters: - file_path (str): The path to the file that needs to be hashed. - algorithm (str): The name of the hashing algorithm to use (default is \'sha256\'). Returns: - str: A hexadecimal string representing the hash of the file contents. if algorithm not in hashlib.algorithms_guaranteed: raise ValueError(f\\"Unsupported hash algorithm: {algorithm}\\") hasher = hashlib.new(algorithm) try: with open(file_path, \'rb\') as f: while chunk := f.read(8192): hasher.update(chunk) return hasher.hexdigest() except FileNotFoundError: raise FileNotFoundError(f\\"File not found: {file_path}\\") def verify_hash(file_path: str, expected_hash: str, algorithm: str = \'sha256\') -> bool: Verify the file\'s integrity by comparing its hash to a previously calculated hash. Parameters: - file_path (str): The path to the file that needs its hash verified. - expected_hash (str): The expected hash value the file should produce. - algorithm (str): The name of the hashing algorithm to use (default is \'sha256\'). Returns: - bool: True if the hash of the file matches the expected_hash, False otherwise. calculated_hash = hash_file(file_path, algorithm) return calculated_hash == expected_hash"},{"question":"# Python 310 Coding Assessment Question: GUI Application with tkinter Problem Statement You are required to create a Python application using the `tkinter` package. The application will be a simple \\"Task Manager\\" that allows users to add, view, update, and delete tasks. Each task will have a title, description, priority level, and completion status. Requirements 1. **Main Window**: - Create the main window with the title \\"Task Manager\\". - The window should have a `Listbox` to display the list of tasks. - Add buttons for adding, updating, and deleting tasks. 2. **Add Task**: - Implement a dialog for adding a new task. Use `tkinter.simpledialog` for input fields such as Title, Description, and Priority Level (High, Medium, Low). - Add a checkbox to mark the task as completed. - Ensure that the new task is added to the `Listbox`. 3. **Update Task**: - Implement functionality to update a selected task. - Prepopulate the update dialog with existing details using `tkinter.simpledialog`. - Allow users to modify the task details and update the information in the `Listbox`. 4. **Delete Task**: - Implement functionality to delete a selected task from the `Listbox`. 5. **View Task Details**: - Display the details of a selected task in a separate `Toplevel` window. - Include fields to show Title, Description, Priority Level, and Completion Status. 6. **User Interface Enhancements**: - Use `tkinter.ttk` for a modern look and feel. - Apply padding, margins, and layout management for better usability. 7. **Data Persistence**: - Implement basic data persistence by saving tasks to a file when the application is closed and loading them when the application starts. Constraints - Assume the application will have at most 100 tasks. - Ensure that the UI is responsive and handles user actions efficiently. Example 1. **Add Task Dialog**: ``` +------------------ Add Task ------------------+ | Title: [_________________________] | | Description: | | [______________________________] | | Priority Level: ( ) High ( ) Medium ( ) Low | | Completed: [ ] | | [ OK ] [ Cancel ] | +------------------------------------------------+ ``` 2. **Task List**: ``` | Task 1: Title (High, Completed) | | Task 2: Another Title (Medium, Incomplete) | ``` 3. **Task Details Window**: ``` +-------------- Task Details ---------------+ | Title: Task 1 | | Description: Task Description | | Priority Level: High | | Completed: Yes | | | | [ Close ] | +--------------------------------------------+ ``` Performance Requirements - The application should load and display tasks within 2 seconds. - Adding, updating, or deleting tasks should reflect immediately in the `Listbox`. Evaluation Criteria - Correctness: The application meets all requirements. - Code Quality: The code is clean, well-organized, and adheres to Python best practices. - User Interface: The UI is user-friendly and visually appealing. - Robustness: The application handles edge cases gracefully, such as empty input fields and invalid entries. Submission - Submit the Python script file containing the complete implementation of the Task Manager application.","solution":"import tkinter as tk from tkinter import simpledialog, messagebox, ttk class Task: def __init__(self, title, description, priority, completed=False): self.title = title self.description = description self.priority = priority self.completed = completed def __repr__(self): return f\\"{self.title} ({self.priority}, {\'Completed\' if self.completed else \'Incomplete\'})\\" class TaskManagerApp: def __init__(self, root): self.root = root self.root.title(\\"Task Manager\\") self.tasks = [] self.load_tasks() self.main_frame = ttk.Frame(self.root, padding=\\"10 10 10 10\\") self.main_frame.grid(column=0, row=0, sticky=(tk.W, tk.E, tk.N, tk.S)) self.task_listbox = tk.Listbox(self.main_frame, height=10, width=50) self.task_listbox.grid(column=0, row=0, columnspan=4, sticky=(tk.W, tk.E)) self.task_listbox.bind(\'<<ListboxSelect>>\', self.view_task_details) self.add_button = ttk.Button(self.main_frame, text=\\"Add Task\\", command=self.add_task) self.add_button.grid(column=1, row=1, sticky=tk.W) self.update_button = ttk.Button(self.main_frame, text=\\"Update Task\\", command=self.update_task) self.update_button.grid(column=2, row=1, sticky=tk.W) self.delete_button = ttk.Button(self.main_frame, text=\\"Delete Task\\", command=self.delete_task) self.delete_button.grid(column=3, row=1, sticky=tk.W) self.populate_listbox() self.root.protocol(\\"WM_DELETE_WINDOW\\", self.save_tasks) def add_task(self): title = simpledialog.askstring(\\"Task Title\\", \\"Enter the task title:\\") if not title: return description = simpledialog.askstring(\\"Task Description\\", \\"Enter the task description:\\") if not description: return priority = simpledialog.askstring(\\"Task Priority\\", \\"Enter the task priority (High, Medium, Low):\\") if priority not in [\'High\', \'Medium\', \'Low\']: messagebox.showerror(\\"Invalid Input\\", \\"Priority must be one of High, Medium, Low.\\") return completed = messagebox.askyesno(\\"Task Completed\\", \\"Is the task completed?\\") new_task = Task(title, description, priority, completed) self.tasks.append(new_task) self.populate_listbox() def update_task(self): try: selected_task_index = self.task_listbox.curselection()[0] selected_task = self.tasks[selected_task_index] except IndexError: messagebox.showerror(\\"No Task Selected\\", \\"Please select a task to update.\\") return title = simpledialog.askstring(\\"Task Title\\", \\"Enter the task title:\\", initialvalue=selected_task.title) description = simpledialog.askstring(\\"Task Description\\", \\"Enter the task description:\\", initialvalue=selected_task.description) priority = simpledialog.askstring(\\"Task Priority\\", \\"Enter the task priority (High, Medium, Low):\\", initialvalue=selected_task.priority) completed = messagebox.askyesno(\\"Task Completed\\", \\"Is the task completed?\\", initialvalue=selected_task.completed) selected_task.title = title selected_task.description = description selected_task.priority = priority selected_task.completed = completed self.populate_listbox() def delete_task(self): try: selected_task_index = self.task_listbox.curselection()[0] del self.tasks[selected_task_index] self.populate_listbox() except IndexError: messagebox.showerror(\\"No Task Selected\\", \\"Please select a task to delete.\\") def view_task_details(self, event): try: selected_task_index = self.task_listbox.curselection()[0] selected_task = self.tasks[selected_task_index] except IndexError: return detail_window = tk.Toplevel(self.root) detail_window.title(\\"Task Details\\") title_label = ttk.Label(detail_window, text=\\"Title: \\" + selected_task.title) title_label.grid(column=0, row=0, sticky=tk.W) description_label = ttk.Label(detail_window, text=\\"Description: \\" + selected_task.description) description_label.grid(column=0, row=1, sticky=tk.W) priority_label = ttk.Label(detail_window, text=\\"Priority: \\" + selected_task.priority) priority_label.grid(column=0, row=2, sticky=tk.W) completed_label = ttk.Label(detail_window, text=\\"Completed: \\" + (\\"Yes\\" if selected_task.completed else \\"No\\")) completed_label.grid(column=0, row=3, sticky=tk.W) close_button = ttk.Button(detail_window, text=\\"Close\\", command=detail_window.destroy) close_button.grid(column=0, row=4, sticky=tk.W) def populate_listbox(self): self.task_listbox.delete(0, tk.END) for task in self.tasks: self.task_listbox.insert(tk.END, task) def save_tasks(self): with open(\\"tasks.txt\\", \\"w\\") as file: for task in self.tasks: file.write(f\\"{task.title}|{task.description}|{task.priority}|{task.completed}n\\") self.root.destroy() def load_tasks(self): try: with open(\\"tasks.txt\\", \\"r\\") as file: for line in file: title, description, priority, completed = line.strip().split(\'|\') task = Task(title, description, priority, completed == \'True\') self.tasks.append(task) except FileNotFoundError: pass if __name__ == \\"__main__\\": root = tk.Tk() app = TaskManagerApp(root) root.mainloop()"},{"question":"# Question: Accurate Summation of Floating-Point Numbers In this task, you will implement a function that accurately computes the sum of a list of floating-point numbers, handling the inaccuracies that arise from their binary representation. # Objective Implement a function `accurate_sum(numbers: list[float]) -> float` that takes a list of floating-point numbers and returns their sum as accurately as possible. You should use the `math.fsum` function which is specialized for minimizing the effect of accumulated rounding errors during summation. # Input - `numbers`: A list of floating-point numbers. The list length can vary from 1 to 10^6. # Output - A floating-point number representing the accurate summation of the input numbers. # Constraints - Each element in the list can be any floating-point number (positive, negative, or zero). - The input list may contain duplicate values. # Example ```python def accurate_sum(numbers: list[float]) -> float: import math return math.fsum(numbers) # Example usage: numbers = [0.1, 0.1, 0.1] print(accurate_sum(numbers)) # Output should be as close to 0.3 as possible. ``` # Explanation In this example, summing the values `0.1 + 0.1 + 0.1` using regular float summation might not yield exactly `0.3` due to floating-point representation errors. The use of `math.fsum` ensures that these errors are minimized, providing a more accurate result. # Additional Notes - Make sure to handle edge cases such as very large or very small numbers. - Consider the performance implications given the potential size of the input list. Good luck, and happy coding!","solution":"import math def accurate_sum(numbers: list[float]) -> float: Returns the accurate summation of a list of floating-point numbers using math.fsum. return math.fsum(numbers)"},{"question":"Objective You are required to demonstrate your understanding of the \\"nis\\" module by implementing a function that queries NIS for a set of keys and handles potential errors effectively. Problem Description Write a function `query_nis_keys(mapname, keys, domain=None)` that takes the following parameters: 1. `mapname`: A string representing the NIS map to query. 2. `keys`: A list of strings representing the keys to look up in the specified map. 3. `domain` (optional): A string representing the NIS domain to use for the query. If not specified, use the default NIS domain. The function should: 1. Return a dictionary where the keys are the input keys and the values are the corresponding values obtained from the NIS map. If a key does not exist, the value should be `None`. 2. Handle any `nis.error` exceptions and print an appropriate error message, returning an empty dictionary in such cases. Constraints - The function should only work on Unix systems where NIS is configured and available. - The `key` and `value` pairs in the result should be byte arrays if retrieved successfully. - The function must use the `nis.match` function to perform individual key lookups. Example ```python import nis def query_nis_keys(mapname, keys, domain=None): result = {} try: for key in keys: try: value = nis.match(key, mapname, domain) result[key] = value except nis.error: result[key] = None except nis.error as e: print(f\\"An error occurred: {e}\\") return {} return result # Example usage: # Assuming NIS is properly configured and \'hosts.byname\' is a valid mapname print(query_nis_keys(\'hosts.byname\', [\'localhost\', \'example.com\'])) ``` Output: ```python {\'localhost\': b\'127.0.0.1\', \'example.com\': None} ``` Notes - Ensure you handle exceptions carefully and provide meaningful error messages. - Efficiently perform the lookup for each key even if one of the lookups results in an error.","solution":"import nis def query_nis_keys(mapname, keys, domain=None): result = {} try: for key in keys: try: value = nis.match(key, mapname, domain) result[key] = value except nis.error: result[key] = None except nis.error as e: print(f\\"An error occurred: {e}\\") return {} return result"},{"question":"Objective Design a PyTorch module and demonstrate advanced features such as hooks, custom initialization, and training using optimizers. Your task is to create a composite module that first applies a custom linear transformation followed by a non-linear activation function, and train it to fit a simple function. Task Details 1. **Create a Custom Linear Module** - Implement a class `CustomLinear` that inherits from `nn.Module`. - This module should initialize its own `weight` and `bias` using custom initialization techniques. Use Xavier normal initialization for weights and zero initialization for biases. - Define a `forward` method that applies the linear transformation using @ for matrix multiplication and includes bias. 2. **Create a Composite Module** - Implement a class `CompositeNet` that inherits from `nn.Module`. - This module should contain: - Two instances of `CustomLinear`. - One instance of `nn.ReLU` activation function between the two linear layers. - Define a `forward` method that applies the transformations in sequence. 3. **Training the Module** - Create an instance of `CompositeNet`. - Define a simple synthetic dataset where the input feature is of size 4 and the target output is a linear combination of inputs passed through ReLU. - Train the module on the dataset for 10,000 iterations using Vanilla Gradient Descent (SGD optimizer) with a learning rate of 0.001. - The loss function should be Mean Squared Error (MSE). 4. **Implementing Hooks** - Register a forward hook to `CompositeNet` that scales the output of the first linear layer by 0.5 before passing it to the ReLU activation. - Register a backward hook to `CompositeNet` to inspect and print gradients after every backward pass. 5. **Saving and Loading the Model** - Save the trained model state to a file. - Load it back and verify that the model parameters are correctly restored. Input and Output Formats - **Input**: There is no direct input as the module should internally define the dataset. - **Output**: Print model parameters before and after training. Print gradients inspected in backward hook during training. Constraints - Use only the provided classes and functions from the PyTorch library. - Your code should run efficiently and complete the training within a reasonable time frame. Skeleton Code ```python import torch from torch import nn import torch.nn.functional as F from torch import optim # 1. Custom Linear Module class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() # Initialize parameters with custom techniques self.weight = nn.Parameter(torch.empty(in_features, out_features)) self.bias = nn.Parameter(torch.empty(out_features)) self.init_parameters() def init_parameters(self): nn.init.xavier_normal_(self.weight) nn.init.zeros_(self.bias) def forward(self, input): return input @ self.weight + self.bias # 2. Composite Module with hooks class CompositeNet(nn.Module): def __init__(self): super(CompositeNet, self).__init__() self.linear1 = CustomLinear(4, 3) self.relu = nn.ReLU() self.linear2 = CustomLinear(3, 1) def forward(self, x): x = self.linear1(x) x = self.relu(x) x = self.linear2(x) return x # Hook Functions def forward_hook(module, inputs, outputs): # Scale the output by 0.5 return outputs * 0.5 def backward_hook(module, grad_input, grad_output): # Print the gradients print(\'Gradients in backward hook:\', grad_output) return grad_input # 3. Training Section def train_model(): # Instantiate the model model = CompositeNet() # Register hooks model.linear1.register_forward_hook(forward_hook) model.linear1.register_full_backward_hook(backward_hook) # Define dataset torch.manual_seed(0) data = torch.randn(100, 4) target = 2 * data.sum(dim=1, keepdim=True) # Define optimizer and loss function optimizer = optim.SGD(model.parameters(), lr=0.001) criterion = nn.MSELoss() for epoch in range(10000): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() if (epoch + 1) % 1000 == 0: print(f\'Epoch [{epoch + 1}/10000], Loss: {loss.item():.4f}\') # Save and load the model torch.save(model.state_dict(), \'composite_net.pth\') new_model = CompositeNet() new_model.load_state_dict(torch.load(\'composite_net.pth\')) return model, new_model if __name__ == \\"__main__\\": model_before, model_after = train_model() print(\\"Trained model parameters before saving:\\") for param in model_before.named_parameters(): print(param) print(\\"nLoaded model parameters after loading:\\") for param in model_after.named_parameters(): print(param) ```","solution":"import torch from torch import nn import torch.nn.functional as F from torch import optim # 1. Custom Linear Module class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() # Initialize parameters with custom techniques self.weight = nn.Parameter(torch.empty(in_features, out_features)) self.bias = nn.Parameter(torch.empty(out_features)) self.init_parameters() def init_parameters(self): nn.init.xavier_normal_(self.weight) nn.init.zeros_(self.bias) def forward(self, input): return input @ self.weight + self.bias # 2. Composite Module with hooks class CompositeNet(nn.Module): def __init__(self): super(CompositeNet, self).__init__() self.linear1 = CustomLinear(4, 3) self.relu = nn.ReLU() self.linear2 = CustomLinear(3, 1) def forward(self, x): x = self.linear1(x) x = self.relu(x) x = self.linear2(x) return x # Hook Functions def forward_hook(module, inputs, outputs): # Scale the output by 0.5 return outputs * 0.5 def backward_hook(module, grad_input, grad_output): # Print the gradients print(\'Gradients in backward hook:\', grad_output) return grad_input # 3. Training Section def train_model(): # Instantiate the model model = CompositeNet() # Register hooks model.linear1.register_forward_hook(forward_hook) model.linear1.register_full_backward_hook(backward_hook) # Define dataset torch.manual_seed(0) data = torch.randn(100, 4) target = 2 * data.sum(dim=1, keepdim=True) # Define optimizer and loss function optimizer = optim.SGD(model.parameters(), lr=0.001) criterion = nn.MSELoss() for epoch in range(10000): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() if (epoch + 1) % 1000 == 0: print(f\'Epoch [{epoch + 1}/10000], Loss: {loss.item():.4f}\') # Save and load the model torch.save(model.state_dict(), \'composite_net.pth\') new_model = CompositeNet() new_model.load_state_dict(torch.load(\'composite_net.pth\')) return model, new_model if __name__ == \\"__main__\\": model_before, model_after = train_model() print(\\"Trained model parameters before saving:\\") for param in model_before.named_parameters(): print(param) print(\\"nLoaded model parameters after loading:\\") for param in model_after.named_parameters(): print(param)"},{"question":"**Advanced Coding Assessment Question: Mailbox Management** **Objective:** Implement a program that performs the following tasks using the `mailbox` module: 1. Reads and lists all messages from a given mailbox (Maildir format) directory and their metadata (subject, sender, flags). 2. Filters messages based on a specific flag (e.g., \\"Flagged\\") and prints their subjects. 3. Moves these filtered messages to another folder within the same Maildir structure. **Requirements:** 1. Implement a function `list_messages(maildir_path: str) -> None` that: - Lists all messages in the Maildir directory located at `maildir_path`. - Prints each message\'s subject, sender, and flags, if available. 2. Implement a function `filter_messages_by_flag(maildir_path: str, flag: str) -> list` that: - Filters messages in the Maildir directory located at `maildir_path` that have the specified `flag`. - Returns a list of keys of filtered messages. 3. Implement a function `move_messages_to_folder(maildir_path: str, message_keys: list, target_folder: str) -> None` that: - Moves messages identified by `message_keys` from their current folder to the `target_folder` within the same Maildir structure. 4. Ensure your implementation handles potential errors such as missing folders or malformed messages gracefully. **Constraints:** - The mailbox should be in Maildir format. - Use proper locking mechanisms to handle concurrent modifications. - Follow best practices to avoid data loss during operations. **Function Signatures:** ```python import mailbox def list_messages(maildir_path: str) -> None: # Your implementation here def filter_messages_by_flag(maildir_path: str, flag: str) -> list: # Your implementation here def move_messages_to_folder(maildir_path: str, message_keys: list, target_folder: str) -> None: # Your implementation here ``` **Example Usage:** ```python # Define Maildir path and target flag maildir_path = \'~/Maildir\' flag = \'F\' # Example flag to filter flagged messages # List all messages list_messages(maildir_path) # Filter messages by flag flagged_message_keys = filter_messages_by_flag(maildir_path, flag) # Move flagged messages to a folder named \\"Flagged\\" move_messages_to_folder(maildir_path, flagged_message_keys, \'Flagged\') ``` **Expected Output Format:** 1. **list_messages**: ``` Subject: Test Email 1, Sender: user@example.com, Flags: FS Subject: Test Email 2, Sender: admin@example.com, Flags: F ... ``` 2. **filter_messages_by_flag**: ``` [\'message_key1\', \'message_key2\', ...] ``` 3. **move_messages_to_folder**: ``` Moving messages to \'Flagged\' folder... Success: Moved 2 messages. ``` **Notes:** - Ensure that your functions are modular, well-documented, and handle edge cases appropriately. - You may use dummy data or mock objects during development and testing. Good luck!","solution":"import mailbox import os import shutil from typing import List def list_messages(maildir_path: str) -> None: Lists all messages in the Maildir directory located at `maildir_path`. Prints each message\'s subject, sender, and flags, if available. try: maildir = mailbox.Maildir(maildir_path, factory=None) for key, msg in maildir.iteritems(): subject = msg.get(\'subject\', \'[No Subject]\') sender = msg.get(\'from\', \'[Unknown Sender]\') flags = msg.get_flags() print(f\\"Subject: {subject}, Sender: {sender}, Flags: {flags}\\") except Exception as e: print(f\\"Error reading messages: {e}\\") def filter_messages_by_flag(maildir_path: str, flag: str) -> List[str]: Filters messages in the Maildir directory located at `maildir_path` that have the specified `flag`. Returns a list of keys of filtered messages. filtered_keys = [] try: maildir = mailbox.Maildir(maildir_path, factory=None) for key, msg in maildir.iteritems(): if flag in msg.get_flags(): filtered_keys.append(key) except Exception as e: print(f\\"Error filtering messages: {e}\\") return filtered_keys def move_messages_to_folder(maildir_path: str, message_keys: List[str], target_folder: str) -> None: Moves messages identified by `message_keys` from their current folder to the `target_folder` within the same Maildir structure. try: maildir = mailbox.Maildir(maildir_path, factory=None) target_path = os.path.join(maildir_path, target_folder) os.makedirs(target_path, exist_ok=True) for key in message_keys: msg_file = maildir.get_file(key) with open(os.path.join(target_path, key), \'wb\') as target_file: shutil.copyfileobj(msg_file, target_file) maildir.remove(key) print(f\\"Success: Moved {len(message_keys)} messages to \'{target_folder}\'.\\") except Exception as e: print(f\\"Error moving messages: {e}\\")"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],O={key:0},M={key:1};function N(i,e,l,m,o,s){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"prompts chat")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>o.searchQuery=r),placeholder:"Search..."},null,512),[[y,o.searchQuery]]),o.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>o.searchQuery="")},"  ")):d("",!0)]),t("div",F,[(a(!0),n(b,null,v(s.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),s.displayedPoems.length===0?(a(),n("div",R,' No results found for "'+c(o.searchQuery)+'". ',1)):d("",!0)]),s.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:o.isLoading,onClick:e[2]||(e[2]=(...r)=>s.loadMore&&s.loadMore(...r))},[o.isLoading?(a(),n("span",M,"Loading...")):(a(),n("span",O,"See more"))],8,q)):d("",!0)])}const j=p(z,[["render",N],["__scopeId","data-v-0b7fddc6"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/57.md","filePath":"deepseek/57.md"}'),L={name:"deepseek/57.md"},B=Object.assign(L,{setup(i){return(e,l)=>(a(),n("div",null,[x(j)]))}});export{Y as __pageData,B as default};
