import{_ as p,o as a,c as s,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,i,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-c2f6658d"]]),I=JSON.parse('[{"question":"# Abstract Base Class Implementation and Virtual Subclasses Registration You are tasked with designing a library system where various types of media can be checked out by users. A media item must provide the following methods: 1. `check_out(self, user: str)`: Records that the media item is checked out by the given user. 2. `check_in(self)`: Records that the media item is available again. 3. `is_checked_out(self) -> bool`: Returns whether the media item is currently checked out. We will have three types of media in our system: `Book`, `DVD`, and `Magazine`. Implement these media types by adhering to the following rules: 1. Create an abstract base class `Media` that enforces the existence of the three methods mentioned above using abstract methods. 2. Implement concrete classes `Book`, `DVD`, and `Magazine` derived from `Media`. 3. Each media type should have an additional attribute specific to that type: - `Book`: `author` (str) - `DVD`: `director` (str) - `Magazine`: `issue_number` (int) 4. Register `VHS` as a virtual subclass of `Media` without inheriting from it. Implement the `VHS` class with the required methods. # Implementation Details 1. Define the abstract base class `Media` using the `ABC` class or `ABCMeta` metaclass. 2. Implement the derived classes `Book`, `DVD`, and `Magazine`, ensuring they provide concrete implementations of all abstract methods. 3. Register the `VHS` class as a virtual subclass of `Media`. 4. Demonstrate the dynamic registration by writing additional code to: - Create instances of each media type and check their functionalities. - Validate that `issubclass(VHS, Media)` returns `True`. # Constraints - You may not use any libraries or frameworks beyond standard Python 3.10. - Ensure that the `VHS` class does not inherit from `Media`, but still satisfies the interface requirements. # Example Usage ```python # Define ABC and concrete classes here... # Dynamically register VHS as a virtual subclass Media.register(VHS) # Demonstrate functionalities book = Book(title=\\"1984\\", author=\\"George Orwell\\") dvd = DVD(title=\\"Inception\\", director=\\"Christopher Nolan\\") magazine = Magazine(title=\\"Science Today\\", issue_number=42) vhs = VHS(title=\\"The Lion King\\") print(issubclass(VHS, Media)) # Should print: True print(isinstance(vhs, Media)) # Should print: True book.check_out(\\"Alice\\") print(book.is_checked_out()) # Should print: True book.check_in() print(book.is_checked_out()) # Should print: False # And similarly for DVD, Magazine, and VHS instances ```","solution":"from abc import ABC, abstractmethod class Media(ABC): @abstractmethod def check_out(self, user: str): pass @abstractmethod def check_in(self): pass @abstractmethod def is_checked_out(self) -> bool: pass class Book(Media): def __init__(self, title: str, author: str): self.title = title self.author = author self.checked_out = False self.checked_out_by = None def check_out(self, user: str): self.checked_out = True self.checked_out_by = user def check_in(self): self.checked_out = False self.checked_out_by = None def is_checked_out(self) -> bool: return self.checked_out class DVD(Media): def __init__(self, title: str, director: str): self.title = title self.director = director self.checked_out = False self.checked_out_by = None def check_out(self, user: str): self.checked_out = True self.checked_out_by = user def check_in(self): self.checked_out = False self.checked_out_by = None def is_checked_out(self) -> bool: return self.checked_out class Magazine(Media): def __init__(self, title: str, issue_number: int): self.title = title self.issue_number = issue_number self.checked_out = False self.checked_out_by = None def check_out(self, user: str): self.checked_out = True self.checked_out_by = user def check_in(self): self.checked_out = False self.checked_out_by = None def is_checked_out(self) -> bool: return self.checked_out class VHS: def __init__(self, title: str): self.title = title self.checked_out = False self.checked_out_by = None def check_out(self, user: str): self.checked_out = True self.checked_out_by = user def check_in(self): self.checked_out = False self.checked_out_by = None def is_checked_out(self) -> bool: return self.checked_out # Register VHS as a virtual subclass of Media Media.register(VHS)"},{"question":"# PyTorch Coding Assessment Question Objective This task assesses your understanding of the numerical properties in PyTorch using `torch.finfo` and `torch.iinfo`. Problem Statement You are required to implement a function `get_tensor_properties` in PyTorch, which calculates and returns a dictionary capturing various numerical properties of each data type for given tensors. Function Signature ```python def get_tensor_properties(tensor_list: List[torch.Tensor]) -> Dict[str, Dict[str, Union[int, float]]]: ``` Input: - `tensor_list`: A list of PyTorch tensors of various data types. Output: - A dictionary where keys are the data types of the tensors in string format and values are dictionaries containing the following numerical properties: - For floating-point tensors (i.e., checking types `torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16`): - `bits`: The number of bits occupied by the type. - `eps`: The smallest representable number such that `1.0 + eps != 1.0`. - `max`: The largest representable number. - `min`: The smallest representable number (typically `-max`). - `tiny`: The smallest positive normal number. - For integer tensors (i.e., checking types `torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64`): - `bits`: The number of bits occupied by the type. - `max`: The largest representable number. - `min`: The smallest representable number. Constraints: - The input list `tensor_list` can have from 1 to 100 tensors. - The tensors can have any shape. - The data types within the tensors will be one of the supported types listed above. Example: ```python import torch tensor_list = [ torch.tensor([1, 2, 3], dtype=torch.float32), torch.tensor([4, 5, 6], dtype=torch.int32) ] properties = get_tensor_properties(tensor_list) print(properties) ``` Expected Output: ```python { \'torch.float32\': { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, }, \'torch.int32\': { \'bits\': 32, \'max\': 2147483647, \'min\': -2147483648, } } ``` Notes: - Use `torch.finfo` and `torch.iinfo` to retrieve the numerical properties. - Consider the efficiency of your implementation. You may assume that input dtypes will not mix floating-point with integer types in a single tensor.","solution":"import torch from typing import List, Dict, Union def get_tensor_properties(tensor_list: List[torch.Tensor]) -> Dict[str, Dict[str, Union[int, float]]]: properties = {} for tensor in tensor_list: dtype_str = str(tensor.dtype) if dtype_str not in properties: if tensor.dtype.is_floating_point: finfo = torch.finfo(tensor.dtype) properties[dtype_str] = { \'bits\': finfo.bits, \'eps\': finfo.eps, \'max\': finfo.max, \'min\': finfo.min, \'tiny\': finfo.tiny, } else: iinfo = torch.iinfo(tensor.dtype) properties[dtype_str] = { \'bits\': iinfo.bits, \'max\': iinfo.max, \'min\': iinfo.min, } return properties"},{"question":"# Memory Management in Python Objective: You are tasked with implementing a set of functions using Python\'s memory management functions to handle a dynamically allocated buffer. These functions must ensure that memory allocation and deallocation are correctly managed, particularly focusing on the `Mem` domain functions (`PyMem_*`). Requirements: 1. **Allocate a Buffer**: Implement a function that allocates memory for a given number of elements of a specified size, initialized to zero. 2. **Resize the Buffer**: Implement a function that resizes the allocated memory to a new size. 3. **Free the Buffer**: Implement a function that correctly frees the allocated memory. Functions to Implement: 1. `buffer_allocate(nelem, elsize):` - **Input**: - `nelem`: The number of elements to allocate. - `elsize`: The size of each element. - **Output**: A pointer to the allocated memory (simulated as an integer in Python, e.g., `id(memory)`). - **Constraints**: The function should use `PyMem_Calloc`. 2. `buffer_resize(pointer, new_size):` - **Input**: - `pointer`: The pointer to the currently allocated memory. - `new_size`: The new size to which the memory should be resized. - **Output**: A pointer to the resized memory (simulated as an integer in Python, e.g., `id(memory)`). - **Constraints**: The function should use `PyMem_Realloc`. 3. `buffer_free(pointer):` - **Input**: - `pointer`: The pointer to the allocated memory. - **Output**: None - **Constraints**: The function should use `PyMem_Free`. Example Usage: ```python buf_ptr = buffer_allocate(10, 4) print(f\\"Buffer allocated at: {buf_ptr}\\") buf_ptr = buffer_resize(buf_ptr, 20) print(f\\"Buffer resized at: {buf_ptr}\\") buffer_free(buf_ptr) print(\\"Buffer freed\\") ``` Notes: 1. The Python memory manager is used under the hood for simulation purposes. 2. Use Python\'s `ctypes` library to simulate the behavior of memory pointers and allocation functions. 3. Ensure that your implementation correctly simulates allocation, resizing, and freeing of memory, mimicking the behavior of actual memory management functions.","solution":"import ctypes def buffer_allocate(nelem, elsize): Allocate memory for a given number of elements of a specified size, initialized to zero. size = nelem * elsize buffer = ctypes.create_string_buffer(size) return ctypes.addressof(buffer) def buffer_resize(pointer, new_size): Resize the allocated memory to a new size. old_buffer = (ctypes.c_char * new_size).from_address(pointer) new_buffer = ctypes.create_string_buffer(new_size) ctypes.memmove(ctypes.addressof(new_buffer), old_buffer, new_size) return ctypes.addressof(new_buffer) def buffer_free(pointer): Free the allocated memory. # In ctypes, memory is managed automatically, so this is a no-op. # Leaving function implementation as an indicator of concept. pass"},{"question":"**Question: Dynamic Module Executor** You are tasked with creating a utility in Python that can dynamically execute Python modules or scripts from either the module namespace or filesystem path. The utility should perform the following tasks: 1. Accept a module name or a filesystem path as input. 2. Use the appropriate `runpy` function (`run_module` or `run_path`) to execute the provided module or script. 3. Collect and return the resulting module\'s globals dictionary after execution. 4. Ensure that any modifications to the `sys` module during execution are correctly reverted before the function returns. **Function Signature:** ```python def execute_dynamic_code(identifier: str, identifier_type: str, init_globals: dict = None) -> dict: pass ``` **Input:** - `identifier` (str): The name of the module or the filesystem path to the script that needs to be executed. - `identifier_type` (str): The type of identifier being provided. It can be either \\"module\\" for a module name or \\"path\\" for a filesystem path. - `init_globals` (dict): A dictionary of initial global variables to be passed to the executing code (default is `None`). **Output:** - A dictionary containing the global variables after executing the specified module or script. **Constraints:** - The provided `identifier_type` must be either \\"module\\" or \\"path\\". - The module name or path provided must exist and be valid. - The function should handle any exceptions that occur during execution and return an appropriate error message in the dictionary. **Example Usage:** ```python # Execute a module by name result = execute_dynamic_code(\'mymodule\', \'module\') print(result) # Execute a script from the filesystem path result = execute_dynamic_code(\'/path/to/myscript.py\', \'path\') print(result) ``` **Performance Requirements:** - The function should not leave any side effects, such as altered `sys` module states, after execution ends. - The function must handle both normal modules and package modules correctly. Implement the `execute_dynamic_code` function as described and ensure it meets the input and output specifications.","solution":"import runpy import sys def execute_dynamic_code(identifier: str, identifier_type: str, init_globals: dict = None) -> dict: Executes a Python module or script dynamically from the specified module namespace or filesystem path. Args: - identifier (str): The module name or the filesystem path to the script. - identifier_type (str): The type of identifier (\\"module\\" or \\"path\\"). - init_globals (dict): A dictionary of initial global variables to be passed to the executing code (default is None). Returns: - dict: A dictionary containing the global variables after executing the specified module or script. # Validate identifier_type if identifier_type not in (\\"module\\", \\"path\\"): return {\\"error\\": \\"Invalid identifier_type. Must be \'module\' or \'path\'.\\"} # Setup initial globals if not provided if init_globals is None: init_globals = {} saved_sys_argv = sys.argv[:] saved_sys_path = sys.path[:] try: if identifier_type == \\"module\\": result_globals = runpy.run_module(identifier, init_globals=init_globals, run_name=\\"__main__\\") elif identifier_type == \\"path\\": sys.argv[0] = identifier result_globals = runpy.run_path(identifier, init_globals=init_globals) except Exception as e: return {\\"error\\": str(e)} finally: sys.argv = saved_sys_argv sys.path = saved_sys_path return result_globals"},{"question":"Objective: The objective of this assessment is to test your ability to create visually appealing plots using seaborn by applying different themes, styles, and contexts as well as customizing individual plot elements. Task: 1. Create a function `custom_plot_style` that takes the following parameters: - `data`: A pandas DataFrame containing numerical data. - `plot_type`: A string that can be either \\"boxplot\\" or \\"violinplot\\". - `theme`: A string specifying the seaborn theme. Options include \\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", and \\"ticks\\". - `context`: A string specifying the seaborn context. Options include \\"paper\\", \\"notebook\\", \\"talk\\", and \\"poster\\". - `font_scale`: A float to scale the size of the plot fonts. - `style`: A dictionary of additional style parameters to override the defaults (optional). 2. The function should generate and display the specified plot using seaborn functionalities, with the provided customizations. 3. Ensure the following: - Set the specified theme using `sns.set_theme()`. - Apply the context and scale the fonts using `sns.set_context()`. - Apply the style customizations if provided using `sns.set_style()`. - Generate the plot specified by `plot_type` (either boxplot or violinplot). - Remove the top and right spines using `sns.despine()`. Input: - `data`: A pandas DataFrame with at least one numerical column. - `plot_type`: A string, either \\"boxplot\\" or \\"violinplot\\". - `theme`: A string, one of \\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", or \\"ticks\\". - `context`: A string, one of \\"paper\\", \\"notebook\\", \\"talk\\", or \\"poster\\". - `font_scale`: A float, positive value for scaling fonts. - `style` (optional): A dictionary containing style parameters. Output: The function should display a seaborn plot with the applied customizations but not return any value. Constraints: - The plot type in `plot_type` must be valid. - The theme and context must be valid seaborn settings. - The DataFrame `data` must contain numerical data suitable for plotting. Example: ```python import pandas as pandas import seaborn as sns import matplotlib.pyplot as plt def custom_plot_style(data, plot_type, theme, context, font_scale, style=None): sns.set_theme() sns.set_context(context, font_scale=font_scale) if style: sns.set_style(theme, style) else: sns.set_style(theme) plt.figure(figsize=(10, 6)) if plot_type == \\"boxplot\\": sns.boxplot(data=data) elif plot_type == \\"violinplot\\": sns.violinplot(data=data) sns.despine() # Example usage data = pd.DataFrame(np.random.normal(size=(20, 6)) + np.arange(6) / 2) custom_plot_style(data, \\"boxplot\\", \\"whitegrid\\", \\"talk\\", 1.5) ``` In this question, you will be assessed on your ability to utilize seaborn’s customization capabilities to create clear, attractive, and context-appropriate plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_plot_style(data, plot_type, theme, context, font_scale, style=None): This function customizes a seaborn plot. Parameters: data (pd.DataFrame): DataFrame containing numerical data. plot_type (str): Either \\"boxplot\\" or \\"violinplot\\". theme (str): Theme for the plot. One of \\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", or \\"ticks\\". context (str): Context for the plot. One of \\"paper\\", \\"notebook\\", \\"talk\\", or \\"poster\\". font_scale (float): Scale for the plot fonts. style (dict, optional): A dictionary of additional style parameters to override the defaults. if plot_type not in [\\"boxplot\\", \\"violinplot\\"]: raise ValueError(\\"Invalid plot_type. Choose either \'boxplot\' or \'violinplot\'.\\") sns.set_theme(style=theme) sns.set_context(context, font_scale=font_scale) if style: sns.set_style(theme, style) else: sns.set_style(theme) plt.figure(figsize=(10, 6)) if plot_type == \\"boxplot\\": sns.boxplot(data=data) elif plot_type == \\"violinplot\\": sns.violinplot(data=data) sns.despine() plt.show()"},{"question":"Objective Evaluate the student\'s understanding of seaborn\'s `sns.swarmplot` and `sns.catplot` functions, including how to customize plots using various parameters. Task 1. Load the \\"tips\\" dataset from seaborn. 2. Create a swarm plot that shows the distribution of `total_bill` across different `days`, distinguished by `sex` categories. Your plot should: - Use `dodge=True` to separate the points by `sex`. - Use a distinct color for each `sex`. - Ensure points do not overlap excessively by adjusting the `size` of the points. 3. Create another swarm plot that shows the relationship between `total_bill` and `size`. Your plot should: - Be oriented horizontally. - Use the `hue` parameter to distinguish points by `day`. - Use a qualitative palette of your choice to color the points by `day`. 4. Finally, create a facetted swarm plot using `sns.catplot` that shows the distribution of `total_bill` across different `days`, distinguished by `sex` and facetted by `time`. Your plot should: - Set `aspect=0.5` for better readability. - Ensure the categorical and hue variables are properly synchronized in each facet. Input - None (the required dataset is loaded directly in the code). Output - Three plots as described in the tasks above. Constraints - Ensure plots are clear and readable. - Use appropriate seaborn and matplotlib functions for plot customization. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Task 1 - Swarm plot with dodge plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, size=4) plt.title(\\"Total Bill by Day and Sex\\") plt.show() # Task 2 - Horizontal swarm plot with hue by day plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"size\\", hue=\\"day\\", orient=\\"h\\", palette=\\"Set2\\", size=5) plt.title(\\"Total Bill by Size and Day\\") plt.show() # Task 3 - Facetted swarm plot using catplot sns.catplot(data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) plt.show() ``` Submission - Submit the code in a Python script or Jupyter notebook format.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_swarm_plots(): # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Task 1 - Swarm plot with dodge plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", dodge=True, size=5) plt.title(\\"Total Bill by Day and Sex\\") plt.show() # Task 2 - Horizontal swarm plot with hue by day plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"size\\", hue=\\"day\\", orient=\\"h\\", palette=\\"Set2\\", size=5) plt.title(\\"Total Bill by Size and Day\\") plt.show() # Task 3 - Facetted swarm plot using catplot sns.catplot(data=tips, kind=\\"swarm\\", x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", col=\\"time\\", aspect=0.5) plt.show()"},{"question":"**Question: Implement an Enhanced Dictionary with Advanced Functionalities** You are required to implement a custom class named `EnhancedDict` that mimics the behavior of a Python dictionary but includes additional functionalities. The `EnhancedDict` class should support the following operations: 1. **Basic Dictionary Operations**: - `set(key, value)`: Sets the value for the given key. - `get(key)`: Returns the value for the given key. - `delete(key)`: Removes the given key from the dictionary if it exists. 2. **Extended Functionalities**: - `keys_with_prefix(prefix)`: Returns a list of all keys that start with the given prefix. - `merge_with(other_dict)`: Merges the current dictionary with another dictionary. In case of key conflicts, the values from `other_dict` should overwrite the values in the current dictionary. - `most_frequent_value()`: Returns the value(s) that appear most frequently in the dictionary. If there is a tie, return all values with the highest frequency as a list. # Input/Output Specifications - `set(key, value)`: `key` and `value` are strings. - `get(key)`: `key` is a string, returns the associated value string or `None` if the key doesn\'t exist. - `delete(key)`: `key` is a string, removes the key from the dictionary. - `keys_with_prefix(prefix)`: `prefix` is a string, returns a list of strings. - `merge_with(other_dict)`: `other_dict` is a standard Python dictionary. - `most_frequent_value()`: No arguments, returns a string or list of strings. # Constraints - Keys and values can be any alphanumeric strings. - The dictionary should handle a large number of entries efficiently. - Operations should be performed in a time-efficient manner. # Example Usage ```python # Create an instance of the EnhancedDict ed = EnhancedDict() # Basic operations ed.set(\'apple\', \'fruit\') ed.set(\'carrot\', \'vegetable\') ed.set(\'banana\', \'fruit\') print(ed.get(\'apple\')) # Output: \'fruit\' print(ed.get(\'carrot\')) # Output: \'vegetable\' ed.delete(\'carrot\') print(ed.get(\'carrot\')) # Output: None # Extended functionalities print(ed.keys_with_prefix(\'ba\')) # Output: [\'banana\'] ed.merge_with({\'berry\': \'fruit\', \'apple\': \'electronic\'}) print(ed.get(\'apple\')) # Output: \'electronic\' print(ed.most_frequent_value()) # Output: [\'fruit\'] ``` # Note You must write all the associated methods within the `EnhancedDict` class, ensuring they meet the specified functionalities and constraints.","solution":"class EnhancedDict: def __init__(self): self.data = {} def set(self, key, value): Sets the value for the given key. self.data[key] = value def get(self, key): Returns the value for the given key. return self.data.get(key, None) def delete(self, key): Removes the given key from the dictionary if it exists. if key in self.data: del self.data[key] def keys_with_prefix(self, prefix): Returns a list of all keys that start with the given prefix. return [key for key in self.data.keys() if key.startswith(prefix)] def merge_with(self, other_dict): Merges the current dictionary with another dictionary. self.data.update(other_dict) def most_frequent_value(self): Returns the value(s) that appear most frequently in the dictionary. if not self.data: return None from collections import Counter value_counts = Counter(self.data.values()) max_count = max(value_counts.values()) most_frequent = [value for value, count in value_counts.items() if count == max_count] if len(most_frequent) == 1: return most_frequent[0] return most_frequent"},{"question":"<|Analysis Begin|> The provided documentation describes the `linecache` module, which allows for random access to text lines within Python source files. The key functionalities provided by this module include: 1. `linecache.getline()`: Fetches a specific line from a given file. 2. `linecache.clearcache()`: Clears the cache maintained by the module. 3. `linecache.checkcache()`: Checks the cache for validity and updates it if necessary. 4. `linecache.lazycache()`: Captures details about non-file-based modules for later retrieval. The functions ensure error handling and optimization through caching. The module is essential for debugging and extracting specific lines from files, particularly within tracebacks. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement a function that processes a given Python script file and performs specific operations on it using the `linecache` module. This exercise aims to assess your understanding of file operations, caching mechanisms, and error handling in Python. **Task:** You are required to implement the following function: ```python def process_script_operations(filename: str, operations: list) -> list: Process a list of operations on a given Python script file using the linecache module. Parameters: - filename (str): The name of the Python script file. - operations (list): A list of operations. Each operation is a dictionary with the keys: - \\"type\\" (str): The type of operation, which can be \\"getline\\", \\"checkcache\\", or \\"clearcache\\". - \\"lineno\\" (int, optional): The line number to retrieve, required if type is \\"getline\\". Returns: - list: A list of results corresponding to each operation. For \\"getline\\" operations, return the content of the line. For \\"clearcache\\" and \\"checkcache\\" operations, return \\"None\\". ``` **Input Format:** - `filename` (str): The name of the Python script file. - `operations` (list): A list of dictionaries representing the operations to be performed. Each dictionary contains: - `\\"type\\"` (str): One of \\"getline\\", \\"clearcache\\", or \\"checkcache\\". - `\\"lineno\\"` (int, optional): Line number to retrieve (only required if `\\"type\\"` is \\"getline\\"). **Output Format:** - A list of results corresponding to each operation: - For `\\"getline\\"` operations, return the content of the specified line. - For `\\"clearcache\\"` and `\\"checkcache\\"` operations, return `None`. **Constraints:** - The `filename` given must be a valid Python script present in the same directory as your implementation. - Line numbers provided in the `operations` list are assumed to be valid positive integers within the range of the file\'s total lines. **Examples:** ```python # Example 1: filename = \\"example.py\\" operations = [ {\\"type\\": \\"getline\\", \\"lineno\\": 1}, {\\"type\\": \\"getline\\", \\"lineno\\": 3}, {\\"type\\": \\"clearcache\\"}, {\\"type\\": \\"checkcache\\"}, {\\"type\\": \\"getline\\", \\"lineno\\": 2} ] # Let\'s assume that example.py has the following content: # 1: print(\\"Hello, World!\\") # 2: print(\\"This is a test file.\\") # 3: print(\\"Goodbye, World!\\") # Expected Output: # [\'print(\\"Hello, World!\\")n\', \'print(\\"Goodbye, World!\\")n\', None, None, \'print(\\"This is a test file.\\")n\'] # Example 2: filename = \\"non_existent_file.py\\" operations = [ {\\"type\\": \\"getline\\", \\"lineno\\": 1}, {\\"type\\": \\"clearcache\\"} ] # Expected Output: # [\'\', None] ``` **Performance Requirements:** - Ensure that your code handles large files efficiently by leveraging the `linecache` module\'s caching mechanism. - Handle any potential errors gracefully, returning an empty string for invalid line numbers or filenames during `getline` operations. **Note:** You cannot use any other file reading operations (like built-in `open` function) for `getline` operations; you must use `linecache.getline()`.","solution":"import linecache def process_script_operations(filename: str, operations: list) -> list: results = [] for operation in operations: if operation[\\"type\\"] == \\"getline\\": lineno = operation.get(\\"lineno\\", 0) line = linecache.getline(filename, lineno) results.append(line) elif operation[\\"type\\"] == \\"clearcache\\": linecache.clearcache() results.append(None) elif operation[\\"type\\"] == \\"checkcache\\": linecache.checkcache(filename) results.append(None) return results"},{"question":"**Objective:** Demonstrate your ability to use pandas\' `Styler` objects to style DataFrames and export the styled DataFrame to an HTML file. **Problem Statement:** You are provided with a DataFrame containing sales data for various products across different regions and months. Your task is to: 1. Style the DataFrame to highlight the maximum and minimum sales values for each product using custom colors. 2. Set a caption for the table that reads \\"Sales Data Summary\\". 3. Export the styled DataFrame to an HTML file. **Input:** A CSV file named `sales_data.csv` with the following structure: ``` Product,Region,Month,Sales Product_A,North,January,10000 Product_A,South,January,15000 Product_B,North,February,25000 ... ``` **Output:** An HTML file named `styled_sales_data.html` which contains the styled DataFrame. **Function Signature:** ```python import pandas as pd def style_sales_data(input_csv: str, output_html: str) -> None: Styles the sales data from the input CSV file and exports it to an HTML file. Parameters: input_csv (str): Path to the input CSV file containing sales data. output_html (str): Path to the output HTML file to save the styled DataFrame. pass ``` **Constraints:** - The function should correctly read the CSV file into a DataFrame. - Use the `highlight_max` and `highlight_min` Styler methods to highlight maximum and minimum sales values. - The maximum sales value should be highlighted in green (`\'background-color: lightgreen\'`). - The minimum sales value should be highlighted in red (`\'background-color: lightcoral\'`). - Set the table caption to \\"Sales Data Summary\\". **Example:** Given an example CSV: ``` Product,Region,Month,Sales Product_A,North,January,10000 Product_A,South,January,20000 Product_B,North,February,25000 Product_B,South,February,5000 ``` After applying your styling and exporting to `styled_sales_data.html`, it should highlight the maximum and minimum sales values in each product category, set a caption, and export the styled DataFrame to the specified HTML file. **Notes:** - Ensure that the resulting HTML file includes the necessary styling. - Focus on correct usage of the pandas\' `Styler` object and its methods.","solution":"import pandas as pd def style_sales_data(input_csv: str, output_html: str) -> None: Styles the sales data from the input CSV file and exports it to an HTML file. Parameters: input_csv (str): Path to the input CSV file containing sales data. output_html (str): Path to the output HTML file to save the styled DataFrame. # Read the CSV file into a DataFrame df = pd.read_csv(input_csv) # Pivot the DataFrame to make products columns and sales values under them df_pivot = df.pivot_table(index=[\'Region\', \'Month\'], columns=\'Product\', values=\'Sales\') # Apply styling styled_df = df_pivot.style .highlight_max(axis=0, color=\'lightgreen\') .highlight_min(axis=0, color=\'lightcoral\') .set_caption(\\"Sales Data Summary\\") # Save the styled DataFrame to an HTML file styled_df.to_html(output_html)"},{"question":"# Contextual Data Management in Asynchronous Programming You are tasked with implementing a data processing system that manages context-specific configurations, especially useful in an asynchronous environment. Using the `contextvars` module, you will create context variables and manage different configurations that do not interfere with each other. Problem Statement 1. **Configuration Management:** - Create a `ContextVar` named `config_var` to manage configurations. - Implement a `setup_config` function that sets an initial configuration for the context variable. - Implement a `get_config` function to retrieve the current configuration. If no configuration is set, it should return a default configuration \\"DefaultConfig\\". 2. **Data Processing with Context:** - Implement a `process_data` function that simulates data processing by altering the configuration and then resetting it to its original state. Use arbitrary configuration updates for demonstration. - Implement an `async_process_data` function to demonstrate using the context variable in an asynchronous environment. This function should simulate processing by awaiting an asyncio.sleep() call before and after modifying the configuration. 3. **Execution Context:** - Write a main function that: - Sets up an initial configuration using `setup_config`. - Runs `process_data` in a separate context using the `copy_context` method to show that changes are contained within that context. - Runs `async_process_data` to demonstrate context variable usage with asynchronous code. Constraints - Ensure the `ContextVar` objects are created at the top module level and not within functions or closures. - Handle any exceptions gracefully, particularly when accessing unset context variables. - The `process_data` and `async_process_data` functions should always reset the context variable to its original state, regardless of changes made during their execution. Expected Implementation ```python import contextvars import asyncio # Create the ContextVar object config_var = contextvars.ContextVar(\'config_var\', default=\\"DefaultConfig\\") def setup_config(initial_config): # Sets up initial configuration config_var.set(initial_config) def get_config(): # Retrieves the current configuration return config_var.get() def process_data(): # Simulate data processing by altering and resetting the configuration token = config_var.set(\\"ProcessingConfig\\") try: print(\\"During processing:\\", get_config()) finally: config_var.reset(token) print(\\"After processing:\\", get_config()) async def async_process_data(): # Simulate data processing with asynchronous operations token = config_var.set(\\"AsyncProcessingConfig\\") try: print(\\"Before async processing:\\", get_config()) await asyncio.sleep(1) # Simulate async operation print(\\"During async processing:\\", get_config()) await asyncio.sleep(1) # Simulate more async operations finally: config_var.reset(token) print(\\"After async processing:\\", get_config()) def main(): # Set up initial configuration setup_config(\\"InitialConfig\\") print(\\"Initial configuration:\\", get_config()) # Copy current context and run process_data in copied context ctx = contextvars.copy_context() ctx.run(process_data) # Run async_process_data in the current context asyncio.run(async_process_data()) # Show final configuration print(\\"Final configuration:\\", get_config()) if __name__ == \\"__main__\\": main() ``` Adding this problem to your coding assessment will test the student\'s understanding of managing context-local state, creating and using context variables, and handling asynchronous programming with context.","solution":"import contextvars import asyncio # Create the ContextVar object config_var = contextvars.ContextVar(\'config_var\', default=\\"DefaultConfig\\") def setup_config(initial_config): Sets up initial configuration. config_var.set(initial_config) def get_config(): Retrieves the current configuration. return config_var.get() def process_data(): Simulate data processing by altering and resetting the configuration. token = config_var.set(\\"ProcessingConfig\\") try: print(\\"During processing:\\", get_config()) finally: config_var.reset(token) print(\\"After processing:\\", get_config()) async def async_process_data(): Simulate data processing with asynchronous operations. token = config_var.set(\\"AsyncProcessingConfig\\") try: print(\\"Before async processing:\\", get_config()) await asyncio.sleep(1) # Simulate async operation print(\\"During async processing:\\", get_config()) await asyncio.sleep(1) # Simulate more async operations finally: config_var.reset(token) print(\\"After async processing:\\", get_config()) def main(): Main function to set up the initial configuration and run the data processing functions. # Set up initial configuration setup_config(\\"InitialConfig\\") print(\\"Initial configuration:\\", get_config()) # Copy current context and run process_data in copied context ctx = contextvars.copy_context() ctx.run(process_data) # Run async_process_data in the current context asyncio.run(async_process_data()) # Show final configuration print(\\"Final configuration:\\", get_config()) if __name__ == \\"__main__\\": main()"},{"question":"Objective Implement and demonstrate the use of PyTorch JIT scripting and tracing to optimize a neural network model. Problem Statement You are given a simple neural network model and a dataset. Your task is to: 1. **Script the model**: Convert the model to a scripted version using `torch.jit.script`. 2. **Trace the model**: Convert the model to a traced version using `torch.jit.trace`. 3. **Compare the performance**: Measure and compare the inference times of the original, scripted, and traced models on the provided dataset. Instructions 1. **Define a Simple Neural Network Model**: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x ``` 2. **Create a Random Dataset**: ```python def create_dataset(num_samples=1000): X = torch.randn(num_samples, 10) y = torch.randn(num_samples, 1) return X, y X, y = create_dataset() ``` 3. **Script the Model**: Use `torch.jit.script` to convert `SimpleModel` to a scripted model. 4. **Trace the Model**: Use `torch.jit.trace` to convert `SimpleModel` to a traced model. 5. **Measure Inference Times**: Measure and compare the inference times of the original, scripted, and traced models on the dataset `X`. Constraints - You must use the latest version of PyTorch. - All neural network operations must be encapsulated within the `SimpleModel` class. - The dataset size should be scalable to measure the performance differences effectively. Example Output Your final implementation should output the inference times for the original, scripted, and traced models as follows: ``` Original model inference time: X.XXXXs Scripted model inference time: Y.YYYYs Traced model inference time: Z.ZZZZs ``` Submission Submit your script as a Python file (`pytorch_jit_optimization.py`) containing: - The implementation of the `SimpleModel` class. - The code to script and trace the model. - The code to measure and print the inference times.","solution":"import torch import torch.nn as nn import time class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Create a random dataset def create_dataset(num_samples=1000): X = torch.randn(num_samples, 10) y = torch.randn(num_samples, 1) return X, y X, y = create_dataset() # Instantiate the model original_model = SimpleModel() # Script the model scripted_model = torch.jit.script(original_model) # Trace the model example_input = torch.randn(1, 10) traced_model = torch.jit.trace(original_model, example_input) # Measure inference time for original model def measure_inference_time(model, X): start_time = time.time() with torch.no_grad(): for i in range(X.shape[0]): model(X[i:i+1]) end_time = time.time() return end_time - start_time original_time = measure_inference_time(original_model, X) scripted_time = measure_inference_time(scripted_model, X) traced_time = measure_inference_time(traced_model, X) print(f\\"Original model inference time: {original_time:.4f}s\\") print(f\\"Scripted model inference time: {scripted_time:.4f}s\\") print(f\\"Traced model inference time: {traced_time:.4f}s\\")"},{"question":"# PyTorch Advanced Module Implementation and Hook Application In this exercise, you will implement a custom neural network module and apply forward and backward hooks to monitor and modify the training process. Follow the specified guidelines to ensure your implementation meets the requirements. Task Summary 1. Implement a custom neural network module with multiple layers. 2. Implement forward and backward hooks. 3. Train the network on dummy data. 4. Use the hooks to modify the outputs and gradients during training. Step-by-Step Instructions 1. **Custom Module Implementation**: - Create a class named `CustomNet` that inherits from `torch.nn.Module`. - The constructor should initialize two linear layers (`nn.Linear`) with appropriate input and output dimensions. - Define a `forward` method to pass the input through these layers and apply a ReLU activation function between them. 2. **Forward and Backward Hooks**: - Implement a forward pre-hook that modifies the input by adding a constant value (e.g., 1.0) to it. - Implement a forward hook that applies a residual connection-like modification to the output. - Implement a backward hook that alters the gradient flow by setting gradients to a constant value during backpropagation. 3. **Training Process**: - Create an instance of `CustomNet` and register the hooks. - Use a dummy dataset to train the network. You can generate random input data (e.g., using `torch.randn`). - Use the SGD optimizer with a specified learning rate. - Implement a training loop that runs for a certain number of epochs, computes the loss (mean squared error), performs backpropagation, and updates the model parameters. - Use the hooks to monitor and modify the outputs and gradients during training. 4. **Validation**: - After training, print the weights of the model to observe the changes. - Print the gradients of the input after backpropagation to validate the backward hook behavior. Constraints and Input/Output Requirements - The input tensor should be of shape `(batch_size, input_dim)`. - The output tensor should be of shape `(batch_size, output_dim)`. - Ensure the model, hooks, and training loop is properly implemented. Example ```python import torch import torch.nn as nn import torch.optim as optim # Step 1: Implement CustomNet class CustomNet(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(CustomNet, self).__init__() self.layer1 = nn.Linear(input_dim, hidden_dim) self.layer2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x # Step 2: Implement hooks def forward_pre_hook(module, inputs): input = inputs[0] return input + 1.0 def forward_hook(module, inputs, output): return output + inputs[0] def backward_hook(module, grad_inputs, grad_outputs): return [torch.ones_like(gi) * 42.0 for gi in grad_inputs] # Step 3: Training Process input_dim = 4 hidden_dim = 3 output_dim = 1 net = CustomNet(input_dim, hidden_dim, output_dim) # Register hooks net.register_forward_pre_hook(forward_pre_hook) net.register_forward_hook(forward_hook) net.register_full_backward_hook(backward_hook) # Prepare dummy data batch_size = 5 inputs = torch.randn(batch_size, input_dim) targets = torch.randn(batch_size, output_dim) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(net.parameters(), lr=0.01) # Training loop num_epochs = 1000 for epoch in range(num_epochs): outputs = net(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() # Step 4: Validation print(\\"Model weights after training:\\") for name, param in net.named_parameters(): print(name, param) print(\\"Input gradients after backpropagation:\\") print(inputs.grad) ``` Implement the described steps to complete the exercise. Ensure to validate the results by printing the model\'s weights and input gradients after the training process.","solution":"import torch import torch.nn as nn import torch.optim as optim # Step 1: Implement CustomNet class CustomNet(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(CustomNet, self).__init__() self.layer1 = nn.Linear(input_dim, hidden_dim) self.layer2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x # Step 2: Implement hooks def forward_pre_hook(module, inputs): input = inputs[0] return (input + 1.0,) def forward_hook(module, inputs, output): return output + inputs[0] def backward_hook(module, grad_inputs, grad_outputs): return tuple(torch.ones_like(gi) * 42.0 for gi in grad_inputs) # Step 3: Training Process input_dim = 4 hidden_dim = 3 output_dim = 1 net = CustomNet(input_dim, hidden_dim, output_dim) # Register hooks net.register_forward_pre_hook(forward_pre_hook) net.register_forward_hook(forward_hook) net.register_full_backward_hook(backward_hook) # Prepare dummy data batch_size = 5 inputs = torch.randn(batch_size, input_dim, requires_grad=True) targets = torch.randn(batch_size, output_dim) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(net.parameters(), lr=0.01) # Training loop num_epochs = 1000 for epoch in range(num_epochs): outputs = net(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() # Step 4: Validation print(\\"Model weights after training:\\") for name, param in net.named_parameters(): print(name, param.detach().numpy()) print(\\"Input gradients after backpropagation:\\") print(inputs.grad)"},{"question":"**Objective**: Demonstrate your understanding of PyTorch\'s `torch.UntypedStorage` by implementing a series of functions that manipulate tensor storage. **Background**: In PyTorch, a tensor can have its underlying storage shared or manipulated directly. While it\'s generally advisable to use tensor-level methods for operations, understanding and working with untyped storage gives deep insights into PyTorch\'s data structures. **Tasks**: 1. **Clone and Zero-Fill Storage**: Write a function `clone_and_zero_fill_storage(tensor: torch.Tensor) -> torch.Tensor` that clones the storage of the input tensor, fills the cloned storage with zeros, and returns a new tensor sharing the zero-filled storage. 2. **Check Shared Storage**: Write a function `tensors_share_storage(tensor1: torch.Tensor, tensor2: torch.Tensor) -> bool` that checks if two input tensors share the same underlying storage. 3. **Manipulate Tensor Gradient Storage**: Write a function `zero_fill_grad_storage(tensor: torch.Tensor) -> None` that, for a given tensor with requires gradient, fills its gradient storage with zeros. Raise an error if the tensor does not have gradient storage. 4. **Custom Tensor Allocation**: Write a function `create_custom_tensor_from_storage(input_tensor: torch.Tensor, size: tuple) -> torch.Tensor` that takes an input tensor and a desired shape, creates a new tensor of the desired shape from the input tensor\'s storage, and returns it. Ensure the storage does not overflow or underflow the new tensor size. **Constraints**: - Assume the input tensor dimensions and sizes are manageable within typical memory limits. - Ensure that the operations are performed safely and raise appropriate warnings/errors in unsafe scenarios. **Example Usage**: ```python import torch # Example for Task 1 t = torch.ones(3) new_tensor = clone_and_zero_fill_storage(t) print(new_tensor) # Expected Output: tensor([0., 0., 0.]) # Example for Task 2 t1 = torch.ones(3) t2 = t1.view_as(t1) print(tensors_share_storage(t1, t2)) # Expected Output: True # Example for Task 3 t = torch.ones(3, requires_grad=True) t.sum().backward() zero_fill_grad_storage(t) print(t.grad) # Expected Output: tensor([0., 0., 0.]) # Example for Task 4 t = torch.arange(6, dtype=torch.float32) new_shape = (2, 3) new_tensor = create_custom_tensor_from_storage(t, new_shape) print(new_tensor) # Expected Output: tensor([[0., 1., 2.], [3., 4., 5.]]) ``` **Notes**: - This exercise aims to assess your grasp of `torch.Storage` fundamentals and manipulations. - Emphasize safe practices where direct storage manipulation is concerned. - Consider edge cases, such as non-contiguous tensors and ensure appropriate error handling. Complete the implementation of the four functions as specified, ensuring accurate functionality and reliability.","solution":"import torch def clone_and_zero_fill_storage(tensor: torch.Tensor) -> torch.Tensor: Clones the storage of the input tensor, fills the cloned storage with zeros, and returns a new tensor sharing the zero-filled storage. cloned_storage = tensor.storage().clone() cloned_storage.fill_(0) zero_filled_tensor = torch.tensor(cloned_storage, dtype=tensor.dtype) zero_filled_tensor = zero_filled_tensor.view_as(tensor) return zero_filled_tensor def tensors_share_storage(tensor1: torch.Tensor, tensor2: torch.Tensor) -> bool: Checks if two input tensors share the same underlying storage. return tensor1.storage().data_ptr() == tensor2.storage().data_ptr() def zero_fill_grad_storage(tensor: torch.Tensor) -> None: For a given tensor with requires gradient, fills its gradient storage with zeros. Raises an error if the tensor does not have gradient storage. if tensor.grad is None: raise ValueError(\\"Tensor does not have a gradient.\\") tensor.grad.data.zero_() def create_custom_tensor_from_storage(input_tensor: torch.Tensor, size: tuple) -> torch.Tensor: Takes an input tensor and a desired shape, creates a new tensor of the desired shape from the input tensor\'s storage, and returns it. Ensures the storage does not overflow or underflow the new tensor size. num_elements_needed = torch.prod(torch.tensor(size)).item() if input_tensor.storage().size() < num_elements_needed: raise ValueError(\\"Input tensor storage does not have enough elements for the desired size.\\") new_tensor = torch.tensor(input_tensor.storage(), dtype=input_tensor.dtype)[:num_elements_needed] new_tensor = new_tensor.view(size) return new_tensor"},{"question":"**Efficient Pandas Operations for Large Datasets** You are provided with a large dataset that captures time series data for multiple sensors. Each sensor has data recorded at different timestamps. The data is stored in a directory where each file represents a different sensor, and the files are in the Parquet format. # Task 1. **Loading Data Efficiently** Write a function `load_columns_from_parquet(directory_path: str, columns: List[str]) -> pd.DataFrame` that loads specific columns from all Parquet files in a specified directory into a pandas DataFrame. The function should only load the specified columns to save memory. 2. **Optimizing Data Types** Write a function `optimize_data_types(df: pd.DataFrame) -> pd.DataFrame` that takes a DataFrame and: - Converts text columns with a limited number of unique values to the `Categorical` data type. - Downcasts numerical columns to more memory-efficient types where possible (e.g., convert large integers to `unsigned` integers and floats to smaller precision floats). 3. **Chunk-Wise Processing** Write a function `compute_value_counts_chunkwise(directory_path: str, column_name: str) -> pd.Series` that computes the value counts for a specific column across all Parquet files in the directory, processing the files in chunks to handle large datasets that cannot fit in memory. # Constraints - **Performance:** Ensure that the functions are optimized for large datasets and do not load more data into memory than necessary. - **Memory Efficiency:** Your solution should highlight memory-efficient operations in pandas as described in the provided documentation. # Examples 1. For `load_columns_from_parquet(directory_path, columns)`: ```python directory_path = \\"path/to/parquet/files\\" columns = [\\"sensor_id\\", \\"temperature\\", \\"timestamp\\"] df = load_columns_from_parquet(directory_path, columns) # df should contain data from all Parquet files but only the specified columns ``` 2. For `optimize_data_types(df)`: ```python df_optimized = optimize_data_types(df) # df_optimized should have memory-efficient data types ``` 3. For `compute_value_counts_chunkwise(directory_path, column_name)`: ```python column_name = \\"sensor_id\\" value_counts = compute_value_counts_chunkwise(directory_path, column_name) # value_counts should be a Series with counts of unique sensor_id values across all files ``` *Note:* Assume that the directory path and column names provided exist and are valid. Good luck!","solution":"import os import pandas as pd from typing import List def load_columns_from_parquet(directory_path: str, columns: List[str]) -> pd.DataFrame: data_frames = [] for file_name in os.listdir(directory_path): if file_name.endswith(\'.parquet\'): file_path = os.path.join(directory_path, file_name) data_frame = pd.read_parquet(file_path, columns=columns) data_frames.append(data_frame) return pd.concat(data_frames, ignore_index=True) def optimize_data_types(df: pd.DataFrame) -> pd.DataFrame: for col in df.select_dtypes(include=[\'object\']).columns: num_unique = len(df[col].unique()) num_total = len(df[col]) if num_unique / num_total < 0.5: df[col] = df[col].astype(\'category\') for col in df.select_dtypes(include=[\'int\']).columns: df[col] = pd.to_numeric(df[col], downcast=\'unsigned\') for col in df.select_dtypes(include=[\'float\']).columns: df[col] = pd.to_numeric(df[col], downcast=\'float\') return df def compute_value_counts_chunkwise(directory_path: str, column_name: str) -> pd.Series: value_counts = pd.Series(dtype=\'int\') for file_name in os.listdir(directory_path): if file_name.endswith(\'.parquet\'): file_path = os.path.join(directory_path, file_name) chunk = pd.read_parquet(file_path, columns=[column_name]) value_counts = value_counts.add(chunk[column_name].value_counts(), fill_value=0) return value_counts"},{"question":"**Assignment: Advanced Color Palettes and Colormaps with Seaborn** # Objective: Demonstrate your understanding of seaborn’s functionality for creating advanced color palettes and colormaps. # Task: 1. **Create a Custom Diverging Palette**: - Write a function `create_diverging_palette` that takes three arguments: - `start_color`: a string representing the RGB/hex code for the starting color. - `end_color`: a string representing the RGB/hex code for the ending color. - `middle_color`: a string representing the RGB/hex code for the middle color (the point of divergence). - The function should return a seaborn color palette that smoothly blends from the `start_color` to the `end_color` with `middle_color` at the center. 2. **Generate a Continuous Colormap**: - Write a function `palette_to_colormap` that converts the diverging palette created in step 1 into a continuous colormap. - The function should take the diverging palette as input and return a matplotlib colormap. 3. **Visualize the Colormap**: - Write a function `plot_colormap` that takes the generated colormap and displays it using a heatmap or any other appropriate visualization method. - You should create a sample dataset to visualize the colormap (e.g., a simple 2D numpy array). # Constraints: - You should use seaborn and matplotlib libraries to implement your solution. - Ensure that the resulting palette and colormap handle edge cases such as invalid color codes gracefully by providing appropriate error messages. # Input Format: - `start_color`, `end_color`, and `middle_color` are strings in hex code format (e.g., `\\"#FF0000\\"` for red) or any named color recognizable by seaborn. # Output Format: - The `create_diverging_palette` function should return a seaborn color palette. - The `palette_to_colormap` function should return a matplotlib colormap instance. - The `plot_colormap` function should display the colormap visualization. # Example Usage: ```python # Function definitions def create_diverging_palette(start_color, end_color, middle_color): pass # Implement this function def palette_to_colormap(palette): pass # Implement this function def plot_colormap(colormap): pass # Implement this function # Example usage palette = create_diverging_palette(\\"#FF0000\\", \\"#0000FF\\", \\"#FFFFFF\\") # Red to Blue via White colormap = palette_to_colormap(palette) plot_colormap(colormap) ``` Ensure your solution is modular, with each function performing its designated task.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np from matplotlib.colors import LinearSegmentedColormap def create_diverging_palette(start_color, end_color, middle_color): Creates a custom diverging palette. Parameters: - start_color (str): RGB/hex code for the starting color. - end_color (str): RGB/hex code for the ending color. - middle_color (str): RGB/hex code for the middle color (point of divergence). Returns: - List of colors forming the diverging palette. try: palette = sns.color_palette([start_color, middle_color, end_color], as_cmap=False) return palette except ValueError as e: raise ValueError(f\\"Error creating the palette with provided colors: {e}\\") def palette_to_colormap(palette): Converts a seaborn color palette into a continuous matplotlib colormap. Parameters: - palette (list): A seaborn color palette. Returns: - A continuous matplotlib colormap. if not palette: raise ValueError(\\"Palette is empty or invalid.\\") colormap = LinearSegmentedColormap.from_list(\\"custom_diverging\\", palette) return colormap def plot_colormap(colormap): Displays the given colormap using a heatmap. Parameters: - colormap (Colormap): A matplotlib colormap instance. data = np.outer(np.arange(0, 1, 0.01), np.ones(10)) plt.figure(figsize=(6, 1)) plt.imshow(data, aspect=\'auto\', cmap=colormap) plt.axis(\'off\') plt.show()"},{"question":"You are tasked with writing a Python function that categorizes a given list of filenames based on their extensions. You need to use the functionalities provided by the `fnmatch` module to achieve this. Function Signature: ```python def categorize_filenames(filenames: list, extensions: list) -> dict: ``` Input: 1. `filenames` (list of str): A list of filenames (e.g., [\'file1.txt\', \'file2.doc\', \'image.png\']). 2. `extensions` (list of str): A list of file extensions to categorize (e.g., [\'*.txt\', \'*.doc\', \'*.png\']). Output: - A dictionary where the keys are the file extensions (e.g., \'.txt\', \'.doc\', \'.png\') and the values are lists of filenames that match the respective extension. Example: ```python filenames = [\'file1.txt\', \'file2.doc\', \'image.png\', \'notes.txt\', \'report.doc\', \'photo.jpg\'] extensions = [\'*.txt\', \'*.doc\', \'*.png\'] Output: { \'.txt\': [\'file1.txt\', \'notes.txt\'], \'.doc\': [\'file2.doc\', \'report.doc\'], \'.png\': [\'image.png\'] } ``` Constraints: - Each filename in the `filenames` list is a valid filename. - Each extension in the `extensions` list follows shell-style wildcard patterns. - Filenames should be categorized based on extension types provided. If an extension is not provided in `extensions`, those files can be ignored. - The function should be case-insensitive. Implementation Details: - Utilize `fnmatch.filter` to filter filenames matching each extension pattern. - Convert patterns like `*.txt` to keys such as `.txt` for the result dictionary. Notes: - Ensure robustness by handling edge cases such as empty lists for `filenames` or `extensions`. # Write your code below:","solution":"import fnmatch def categorize_filenames(filenames: list, extensions: list) -> dict: categorized_files = {} for pattern in extensions: ext = pattern.lstrip(\'*\').lower() # e.g., \'*.txt\' -> \'.txt\' matching_files = [filename for filename in filenames if fnmatch.fnmatchcase(filename.lower(), pattern)] if matching_files: categorized_files[ext] = matching_files return categorized_files"},{"question":"# PyTorch Profiling and Logging Assessment **Objective**: Implement a custom profiling and logging solution in PyTorch to monitor the performance and usage of specific operations within a model training loop. Problem Statement You are required to implement a Python class `ModelProfiler` that profiles the execution time of specific PyTorch operations and logs their usage during the training of a neural network model. This class should make use of PyTorch\'s profiling and logging features. # Requirements 1. **Model Profiler Class**: - Implement a class `ModelProfiler` encapsulating the profiling and logging functionality. 2. **Initialization**: - The class should initialize with parameters: - `operation_names`: A list of PyTorch operation names (as strings) to profile and log. - `sampling_prob`: A float representing the probability of sampling operations for profiling. 3. **Profiling Callbacks**: - Implement the methods `on_operation_start` and `on_operation_end` which will be used as callbacks for profiling the specified operations. 4. **Logging**: - Implement a method `log_usage` that logs operation usage statistics such as the total count and average execution time for each operation. 5. **Training Loop Integration**: - Implement a method `profile_training` which takes a model, loss function, optimizer, data loader, and number of epochs as input and integrates the profiling and logging into the training loop. 6. **Expected Input and Output**: - Your implementation should print profiling and logging information to the console during and after the training process. # Implementation Details **Class Definition**: ```python import torch import time from torch import nn, optim class ModelProfiler: def __init__(self, operation_names, sampling_prob=1.0): self.operation_names = operation_names self.sampling_prob = sampling_prob self.operation_times = {name: [] for name in self.operation_names} def on_operation_start(self, fn): if fn.name() in self.operation_names: fn.metadata = {\'start_time\': time.time()} def on_operation_end(self, fn): if fn.name() in self.operation_names: exec_time = time.time() - fn.metadata[\'start_time\'] self.operation_times[fn.name()].append(exec_time) def log_usage(self): for op_name, times in self.operation_times.items(): if times: avg_time = sum(times) / len(times) print(f\\"Operation: {op_name}, Count: {len(times)}, Avg Execution Time: {avg_time:.6f} seconds\\") def profile_training(self, model, loss_fn, optimizer, data_loader, epochs): # Implement the training loop pass ``` # Constraints - The solution must be implemented using PyTorch version 1.8.0 or higher. - The profiling should not add excessive overhead to the training process. - Properly handle potential concurrency issues due to profiling callbacks. # Example Usage ```python # Example usage of ModelProfiler # Define a simple model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Instantiate the profiler profiler = ModelProfiler(operation_names=[\\"aten::add\\", \\"aten::mul\\"], sampling_prob=0.5) # Your training code # Instantiate model, loss function, optimizer, and data loader model = SimpleModel() loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) data_loader = [(torch.randn(10), torch.randn(1)) for _ in range(100)] # Profile the training process profiler.profile_training(model, loss_fn, optimizer, data_loader, epochs=5) # Log the usage profiler.log_usage() ``` # Submission - Ensure your code is properly commented and follows Python coding conventions. - Provide any necessary explanations or assumptions in a README file.","solution":"import torch import time from torch import nn, optim class ModelProfiler: def __init__(self, operation_names, sampling_prob=1.0): self.operation_names = operation_names self.sampling_prob = sampling_prob self.operation_times = {name: [] for name in self.operation_names} self.current_operations = {} def on_operation_start(self, fn): operation_name = fn.name() if operation_name in self.operation_names: if torch.rand(1).item() < self.sampling_prob: self.current_operations[operation_name] = time.time() def on_operation_end(self, fn): operation_name = fn.name() if operation_name in self.current_operations: exec_time = time.time() - self.current_operations[operation_name] self.operation_times[operation_name].append(exec_time) del self.current_operations[operation_name] def log_usage(self): for op_name, times in self.operation_times.items(): if times: avg_time = sum(times) / len(times) print(f\\"Operation: {op_name}, Count: {len(times)}, Avg Execution Time: {avg_time:.6f} seconds\\") def profile_training(self, model, loss_fn, optimizer, data_loader, epochs): with torch.autograd.profiler.profile(enabled=True) as prof: for epoch in range(epochs): for data, target in data_loader: optimizer.zero_grad() output = model(data) loss = loss_fn(output, target) loss.backward() optimizer.step() torch.autograd.profiler.profile().step = self.on_operation_start, self.on_operation_end print(prof.key_averages().table(sort_by=\\"self_cpu_time_total\\"))"},{"question":"Objective To assess your understanding of tuning decision thresholds in a classification model using the scikit-learn library. Problem Statement You are provided with a dataset and a classification model. Your task is to implement a function that trains the model and tunes the decision threshold to maximize a specified scoring metric using cross-validation. Function Signature ```python def tune_decision_threshold(X_train: np.ndarray, y_train: np.ndarray, base_model: Any, scoring_metric: str, cv: int = 5) -> Tuple[Any, float, float]: Trains a classifier with the provided training data and tunes the decision threshold to maximize the given scoring metric. Parameters: - X_train (np.ndarray): Features of the training dataset. - y_train (np.ndarray): Labels of the training dataset. - base_model (Any): The base classification model to be trained. - scoring_metric (str): The scoring metric to be optimized (e.g., \'f1\', \'accuracy\'). - cv (int, optional): The number of cross-validation folds (default is 5). Returns: - tuned_classifier (Any): The classifier with the tuned decision threshold. - best_score (float): The best score obtained during cross-validation. - decision_threshold (float): The optimal decision threshold identified. ``` Example Usage ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import get_scorer_names from sklearn.model_selection import train_test_split # Create dataset X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Base model base_model = LogisticRegression() # Scoring metric scoring_metric = \'f1\' # Tune decision threshold tuned_classifier, best_score, decision_threshold = tune_decision_threshold(X_train, y_train, base_model, scoring_metric) # Print results print(f\\"Best CV Score: {best_score}\\") print(f\\"Optimal Decision Threshold: {decision_threshold}\\") ``` Constraints 1. You must use the `TunedThresholdClassifierCV` or equivalent process to perform the tuning of the decision threshold. 2. The input data will always be in the form of numpy arrays. 3. You must choose an appropriate cross-validation strategy based on the provided `cv` parameter. 4. The `scoring_metric` will be a valid metric recognized by `sklearn.metrics.get_scorer_names()`. Evaluation Criteria Your solution will be evaluated on: 1. Correct implementation of the threshold tuning process. 2. Proper usage of cross-validation. 3. Appropriate handling of the scoring metric. 4. Correct and efficient execution of the function as specified.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin, clone from sklearn.model_selection import cross_val_score from sklearn.metrics import get_scorer class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_model, threshold=0.5): self.base_model = base_model self.threshold = threshold def fit(self, X, y): self.base_model.fit(X, y) return self def predict(self, X): return (self.base_model.predict_proba(X)[:, 1] >= self.threshold).astype(int) def predict_proba(self, X): return self.base_model.predict_proba(X) def tune_decision_threshold(X_train, y_train, base_model, scoring_metric, cv=5): thresholds = np.linspace(0.1, 0.9, 9) best_score = -np.inf best_threshold = 0.5 scorer = get_scorer(scoring_metric) for threshold in thresholds: model = TunedThresholdClassifierCV(base_model=clone(base_model), threshold=threshold) scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scorer) mean_score = np.mean(scores) if mean_score > best_score: best_score = mean_score best_threshold = threshold tuned_classifier = TunedThresholdClassifierCV(base_model=clone(base_model), threshold=best_threshold) tuned_classifier.fit(X_train, y_train) return tuned_classifier, best_score, best_threshold"},{"question":"# Python Coding Assessment Question Objective Implement a function that mimics a simplified version of the `site` module\'s `addsitedir` function. The function will add a specified directory to the `sys.path` and process any `.pth` files found in that directory. Question Write a Python function `custom_add_sitedir(sitedir: str) -> None` that: 1. Adds the given directory `sitedir` to the `sys.path` if it is not already present. 2. Processes any `.pth` files found in the given `sitedir`. For each `.pth` file: - Read the file line by line. - Add each non-empty line (excluding comments and blank lines) as a path to `sys.path` if the path exists and is not already in `sys.path`. 3. Lines starting with \\"import\\" followed by space or tab in `.pth` files should be executed. 4. The function should handle exceptions gracefully where applicable and ignore invalid lines. Constraints and Requirements - You are not allowed to use the `site` or `sysconfig` module functions directly for path manipulations. - Use Python standard library functions to verify the existence of paths and import modules dynamically. - Consider edge cases, such as handling invalid paths and ensuring that paths are not added multiple times. Input Format - A single string representing the path to the directory (`sitedir`). Output Format - The function should not return any value but should modify `sys.path` as per the described behavior. Example Suppose `sitedir` contains a `.pth` file `example.pth` with the following contents: ```plaintext # Example path configuration /path/to/dir1 /path/to/dir2 import math /path/to/dir3 ``` If `/path/to/dir1`, `/path/to/dir2`, and `/path/to/dir3` are valid directories and not already in `sys.path`, the function call `custom_add_sitedir(\'/path/to/sitedir\')` will result in `sys.path` being updated to include these directories. Code Template ```python import sys import os def custom_add_sitedir(sitedir: str) -> None: # Your implementation here pass # Example usage custom_add_sitedir(\'/path/to/sitedir\') print(sys.path) # Validate the changes in sys.path ```","solution":"import sys import os def custom_add_sitedir(sitedir: str) -> None: if not os.path.isdir(sitedir): return if sitedir not in sys.path: sys.path.append(sitedir) for item in os.listdir(sitedir): if item.endswith(\'.pth\'): pth_file = os.path.join(sitedir, item) try: with open(pth_file, \'r\') as f: for line in f: line = line.strip() if not line or line.startswith(\'#\'): continue if line.startswith(\'import \'): try: exec(line) except Exception as e: print(f\\"Failed to execute line in {pth_file}: {line}. Error: {e}\\") else: if os.path.exists(line) and line not in sys.path: sys.path.append(line) except IOError as e: print(f\\"Failed to read file {pth_file}. Error: {e}\\") # Example usage # custom_add_sitedir(\'/path/to/sitedir\') # print(sys.path) # Validate the changes in sys.path"},{"question":"# Custom Python Type Implementation Objective Your task is to implement a custom Python type in C, leveraging the knowledge provided in the documentation about `PyTypeObject`. The custom type will be a simplistic numeric container that supports basic arithmetic operations, comparison, and string representation. Requirements 1. **Type Definition:** - Define a new type `CustomNumber` that wraps around a single integer value. - Include necessary fields to manage this integer value. 2. **Initialization and Creation:** - Implement an `__init__` method for initializing the type with an integer value. - Implement a `tp_new` function to handle the creation of new instances. 3. **Deallocation:** - Implement a `tp_dealloc` function for proper cleanup when the object is no longer in use. 4. **Representation:** - Implement both `tp_repr` and `tp_str` to return the string representation of the `CustomNumber` instance. 5. **Basic Arithmetic Operations:** - Implement support for addition, subtraction, multiplication, and division. 6. **Comparison:** - Implement rich comparison operations (`__lt__`, `__le__`, `__eq__`, `__ne__`, `__gt__`, `__ge__`). 7. **Attributes:** - Allow read-only access to the contained integer value via an attribute called `value`. Input Format Your implementation should be a valid C extension module for Python. Output Format When tested in Python, the following sequence should work: ```python from custommodule import CustomNumber num1 = CustomNumber(10) num2 = CustomNumber(20) print(num1) # CustomNumber(10) print(repr(num1)) # CustomNumber(10) # Arithmetic operations num3 = num1 + num2 print(num3) # CustomNumber(30) num4 = num2 - num1 print(num4) # CustomNumber(10) num5 = num1 * num2 print(num5) # CustomNumber(200) num6 = num2 / num1 print(num6) # CustomNumber(2) # Comparison operations print(num1 < num2) # True print(num1 == num2) # False print(num1 != num2) # True # Accessing the value attribute print(num1.value) # 10 ``` # Constraints and Requirements - Your implementation should correctly manage memory and ensure no memory leaks. - The type must support proper reference counting. - Ensure exception safety within `tp_dealloc` using `PyErr_Fetch()` and `PyErr_Restore()` if necessary. # Performance - Ensure that operations are efficient and follow best practices for interfacing C with Python, as described in the provided documentation. Development Notes - You will need to compile and install the C extension module for Python to test it. - Adhere to the examples and best practices highlighted in the provided documentation for managing Python buffer objects, type definitions, and attribute management. Good luck!","solution":"# Let\'s start by defining the CustomNumber class that meets the requirements. class CustomNumber: def __init__(self, value): self.value = value def __add__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value + other.value) return NotImplemented def __sub__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value - other.value) return NotImplemented def __mul__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value * other.value) return NotImplemented def __truediv__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value // other.value) return NotImplemented def __lt__(self, other): if isinstance(other, CustomNumber): return self.value < other.value return NotImplemented def __le__(self, other): if isinstance(other, CustomNumber): return self.value <= other.value return NotImplemented def __eq__(self, other): if isinstance(other, CustomNumber): return self.value == other.value return NotImplemented def __ne__(self, other): if isinstance(other, CustomNumber): return self.value != other.value return NotImplemented def __gt__(self, other): if isinstance(other, CustomNumber): return self.value > other.value return NotImplemented def __ge__(self, other): if isinstance(other, CustomNumber): return self.value >= other.value return NotImplemented def __repr__(self): return f\\"CustomNumber({self.value})\\" def __str__(self): return f\\"CustomNumber({self.value})\\""},{"question":"<|Analysis Begin|> The provided documentation outlines the deprecated Python module `mailcap`, which handles the configuration files for MIME-aware applications. Key concepts and functions from the `mailcap` module include: 1. Mailcap file format and its use for describing how different MIME types are to be handled by various programs. 2. The `findmatch` function, which returns an appropriate command and mailcap entry based on provided MIME type, key, filename, and parameter list. 3. The `getcaps` function, which retrieves a dictionary of MIME types mapped to their respective mailcap entries by combining system-wide and user-specific mailcap files. Given that the task is to evaluate student understanding of fundamental and advanced concepts covered by the mailcap module in Python 3.10, a good question should involve: - Using the `getcaps` function to retrieve available mailcap entries. - Using the `findmatch` function to locate the appropriate command for a given MIME type under various conditions. - Handling and validating inputs to avoid security vulnerabilities as described for handling shell metacharacters. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** You are to implement a function that will take a MIME type and a filename, and return the appropriate command to handle the file according to the mailcap entries available on the system. Your function must also handle security validation regarding shell metacharacters. **Function Signature:** ```python def handle_mime_type(mime_type: str, filename: str) -> str: ``` **Input:** - `mime_type` (str): A string representing the MIME type, e.g., `\\"video/mpeg\\"`. - `filename` (str): A string representing the filename to be processed, e.g., `\\"example_video.mpg\\"`. **Output:** - A string containing the command line to be executed, e.g., `\\"xmpeg example_video.mpg\\"`. - If no appropriate command can be found or if the filename contains invalid characters, return an empty string `\\"\\"`. **Constraints and Requirements:** 1. You must use the `mailcap.getcaps()` function to retrieve the mailcap entries. 2. You must use the `mailcap.findmatch()` function to find and return the appropriate command. 3. Ensure that input validation is done to prevent security issues: - Only allow filenames with alphanumeric characters and the following symbols: `@+=:,./-_`. 4. If no matching MIME type can be found or an invalid filename is provided, return an empty string. **Example:** ```python import mailcap def handle_mime_type(mime_type: str, filename: str) -> str: # Validate filename for allowed characters if any(char not in \\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789@+=:,./-_\\" for char in filename): return \\"\\" # Retrieve mailcap entries caps = mailcap.getcaps() # Find the match for the given MIME type command, entry = mailcap.findmatch(caps, mime_type, filename=filename) # Return the command if found, else return empty string if command: return command return \\"\\" # Example usage print(handle_mime_type(\\"video/mpeg\\", \\"example_video.mpg\\")) # Should print: \\"xmpeg example_video.mpg\\" (if such an entry exists) print(handle_mime_type(\\"video/mpeg\\", \\"example_video.mpg;rm -rf /\\")) # Should print: \\"\\" due to invalid character \';\' ``` **Note:** This example assumes that an appropriate entry for `\\"video/mpeg\\"` exists in the mailcap entries. The actual result may vary based on the actual mailcap entries on the system where this code is run.","solution":"import mailcap def handle_mime_type(mime_type: str, filename: str) -> str: Takes a MIME type and a filename, and returns the appropriate command to handle the file according to the mailcap entries available on the system while ensuring security validation regarding shell metacharacters. # Allowed characters in filename allowed_chars = \\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789@+=:,./-_\\" # Validate filename for allowed characters if any(char not in allowed_chars for char in filename): return \\"Invalid filename due to disallowed characters.\\" # Retrieve mailcap entries caps = mailcap.getcaps() # Find the match for the given MIME type command, entry = mailcap.findmatch(caps, mime_type, filename=filename) # Return the command if found, else return empty string if command: return command return \\"No matching command found.\\""},{"question":"**Problem Statement:** You are given a list of dictionaries representing a collection of employees. Each dictionary contains the keys: - `\'name\'`: A string representing the employee\'s name. - `\'age\'`: An integer representing the employee\'s age. - `\'department\'`: A string representing the employee\'s department. - `\'salary\'`: An integer representing the employee\'s salary. Your task is to write a function `process_employees(employees: List[Dict[str, Any]]) -> Dict[str, Any]` that processes this list and returns a dictionary with the following information: 1. **Oldest Employee**: A dictionary containing the name and age of the oldest employee. 2. **Average Age**: The average age of employees. 3. **Department Count**: A dictionary with departments as keys and counts of employees in those departments as values. 4. **Top 5 Salaries**: A list of names of the top 5 employees with the highest salaries. **Constraints:** - The list will contain at least one employee. - If there are fewer than 5 employees, return their names in descending order of their salaries. # Function Signature ```python from typing import List, Dict, Any def process_employees(employees: List[Dict[str, Any]]) -> Dict[str, Any]: pass ``` # Example ```python employees = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"department\\": \\"HR\\", \\"salary\\": 50000}, {\\"name\\": \\"Bob\\", \\"age\\": 24, \\"department\\": \\"Engineering\\", \\"salary\\": 60000}, {\\"name\\": \\"Charlie\\", \\"age\\": 32, \\"department\\": \\"Engineering\\", \\"salary\\": 70000}, {\\"name\\": \\"David\\", \\"age\\": 28, \\"department\\": \\"HR\\", \\"salary\\": 55000}, {\\"name\\": \\"Eve\\", \\"age\\": 26, \\"department\\": \\"Sales\\", \\"salary\\": 45000} ] expected_output = { \\"oldest_employee\\": {\\"name\\": \\"Charlie\\", \\"age\\": 32}, \\"average_age\\": 28.0, \\"department_count\\": {\\"HR\\": 2, \\"Engineering\\": 2, \\"Sales\\": 1}, \\"top_5_salaries\\": [\\"Charlie\\", \\"Bob\\", \\"David\\", \\"Alice\\", \\"Eve\\"] } output = process_employees(employees) assert output == expected_output ``` # Notes - Utilize list comprehensions, dictionary comprehensions, and other relevant data structure methods to achieve the desired outcome efficiently. - Ensure that your implementation is optimized for performance, especially for larger lists of employees.","solution":"from typing import List, Dict, Any def process_employees(employees: List[Dict[str, Any]]) -> Dict[str, Any]: # Find the oldest employee oldest_employee = max(employees, key=lambda x: x[\'age\']) # Calculate the average age total_age = sum(employee[\'age\'] for employee in employees) average_age = total_age / len(employees) # Count employees in each department department_count = {} for employee in employees: dept = employee[\'department\'] if dept in department_count: department_count[dept] += 1 else: department_count[dept] = 1 # Determine top 5 salaries top_salaries = sorted(employees, key=lambda x: x[\'salary\'], reverse=True)[:5] top_5_salaries = [employee[\'name\'] for employee in top_salaries] # Form the output dictionary return { \\"oldest_employee\\": {\\"name\\": oldest_employee[\'name\'], \\"age\\": oldest_employee[\'age\']}, \\"average_age\\": average_age, \\"department_count\\": department_count, \\"top_5_salaries\\": top_5_salaries }"},{"question":"You are provided with a dataset and tasked with implementing a function that computes the polynomial kernel matrix and uses it in an SVM classifier. This exercise will test your understanding of kernel functions and their application in machine learning. # Requirements 1. Implement a function that computes the polynomial kernel matrix. 2. Use this kernel matrix with an SVM classifier to make predictions. # Function Signature ```python def kernel_svm_prediction(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, degree: int, gamma: float, coef0: float) -> np.ndarray: Computes the polynomial kernel matrix for X_train and X_test, trains an SVM using the kernel matrix, and makes predictions on the test data. Parameters: - X_train (np.ndarray): Training data samples. - y_train (np.ndarray): Target labels for training data. - X_test (np.ndarray): Test data samples. - degree (int): Degree of the polynomial kernel. - gamma (float): Kernel coefficient. - coef0 (float): Independent term in the polynomial kernel. Returns: - np.ndarray: Predicted labels for the test data. ``` # Constraints - The input data (X_train, X_test) will be 2D numpy arrays. - The target labels (y_train) will be a 1D numpy array. - Ensure the SVM classifier uses the precomputed kernel matrix. # Example ```python import numpy as np from sklearn.svm import SVC # Implement the function def polynomial_kernel(X, Y, degree, gamma, coef0): return (gamma * np.dot(X, Y.T) + coef0) ** degree def kernel_svm_prediction(X_train, y_train, X_test, degree, gamma, coef0): K_train = polynomial_kernel(X_train, X_train, degree, gamma, coef0) K_test = polynomial_kernel(X_test, X_train, degree, gamma, coef0) svm = SVC(kernel=\'precomputed\') svm.fit(K_train, y_train) return svm.predict(K_test) # Sample data X_train = np.array([[0, 1], [1, 0], [.2, .8], [.7, .3]]) y_train = np.array([0, 1, 0, 1]) X_test = np.array([[0.5, 0.5], [1.5, 1.5]]) # Testing the function predictions = kernel_svm_prediction(X_train, y_train, X_test, degree=3, gamma=0.5, coef0=1) print(predictions) # Expected output could vary based on the parameters used ``` # Additional Considerations - Document your code thoroughly. - Handle potential edge cases, such as empty inputs or mismatched dimensions.","solution":"import numpy as np from sklearn.svm import SVC def polynomial_kernel(X, Y, degree, gamma, coef0): Computes the polynomial kernel between matrices X and Y. Parameters: - X (np.ndarray): Input matrix X. - Y (np.ndarray): Input matrix Y. - degree (int): Degree of the polynomial kernel. - gamma (float): Kernel coefficient. - coef0 (float): Independent term in the polynomial kernel. Returns: - np.ndarray: Polynomial kernel matrix. return (gamma * np.dot(X, Y.T) + coef0) ** degree def kernel_svm_prediction(X_train, y_train, X_test, degree, gamma, coef0): Computes the polynomial kernel matrix for X_train and X_test, trains an SVM using the kernel matrix, and makes predictions on the test data. Parameters: - X_train (np.ndarray): Training data samples. - y_train (np.ndarray): Target labels for training data. - X_test (np.ndarray): Test data samples. - degree (int): Degree of the polynomial kernel. - gamma (float): Kernel coefficient. - coef0 (float): Independent term in the polynomial kernel. Returns: - np.ndarray: Predicted labels for the test data. # Compute the polynomial kernel matrices K_train = polynomial_kernel(X_train, X_train, degree, gamma, coef0) K_test = polynomial_kernel(X_test, X_train, degree, gamma, coef0) # Train SVM with the precomputed kernel svm = SVC(kernel=\'precomputed\') svm.fit(K_train, y_train) # Predict using the SVM model return svm.predict(K_test)"},{"question":"# Email Automation using `imaplib` Objective Your task is to create a Python script that interacts with an IMAP4 server to accomplish the following tasks: 1. Connect securely to the server. 2. Authenticate using a username and password. 3. Select the \'INBOX\' mailbox. 4. Search for all emails from a specific sender. 5. Fetch and print the subjects of these emails. 6. Logout from the server. Detailed Requirements 1. **Connection**: Use the `IMAP4_SSL` class for a secure connection. The server\'s host and port will be provided as inputs to your function. 2. **Authentication**: Use the `login` method with the provided username and password. 3. **Selecting Mailbox**: Select the \'INBOX\' mailbox using the `select` method. 4. **Searching Emails**: Search for emails using the `search` method. The search should use the \'FROM\' criteria to find emails from a specific sender. 5. **Fetching Subjects**: Use the `fetch` method to retrieve the subjects of the emails found. Print each subject line. 6. **Logout**: Properly logout from the server using the `logout` method. Function Specification - Function Name: `fetch_email_subjects` - Input Parameters: - `host` (str): The IMAP server host. - `port` (int): The IMAP server secure port (usually 993). - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `sender` (str): The email address of the sender to search for. - Returns: - `None` - Print: Each email subject on a new line. Function Template ```python import imaplib import email def fetch_email_subjects(host, port, username, password, sender): try: # Step 1: Secure connection to the server server = imaplib.IMAP4_SSL(host, port) # Step 2: Authenticate with the server using username and password server.login(username, password) # Step 3: Select the \'INBOX\' mailbox server.select(\'INBOX\') # Step 4: Search for emails from the specific sender status, email_ids = server.search(None, f\'FROM \\"{sender}\\"\') # Step 5: Fetch and print the subjects of these emails for email_id in email_ids[0].split(): typ, message_data = server.fetch(email_id, \'(RFC822)\') msg = email.message_from_bytes(message_data[0][1]) print(msg[\'subject\']) # Step 6: Logout from the server server.logout() except imaplib.IMAP4.error as e: print(f\\"IMAP4 error: {e}\\") # Example usage: # fetch_email_subjects(\'imap.example.com\', 993, \'user@example.com\', \'password123\', \'sender@example.com\') ``` Constraints 1. Handle any exceptions related to IMAP4 interactions. 2. Ensure secure and correct handling of credentials. 3. The provided example usage should not raise exceptions. Performance Requirements The function should handle searching and fetching email subjects efficiently, even with a large number of emails. Ensure that the coding style is clear and follows best practices for handling network connections and secure authentication.","solution":"import imaplib import email def fetch_email_subjects(host, port, username, password, sender): try: # Step 1: Secure connection to the server server = imaplib.IMAP4_SSL(host, port) # Step 2: Authenticate with the server using username and password server.login(username, password) # Step 3: Select the \'INBOX\' mailbox server.select(\'INBOX\') # Step 4: Search for emails from the specific sender status, email_ids = server.search(None, f\'FROM \\"{sender}\\"\') # Step 5: Fetch and print the subjects of these emails if status == \'OK\': for email_id in email_ids[0].split(): typ, message_data = server.fetch(email_id, \'(RFC822)\') for response_part in message_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) print(msg[\'subject\']) # Step 6: Logout from the server server.logout() except imaplib.IMAP4.error as e: print(f\\"IMAP4 error: {e}\\") # Example usage: # fetch_email_subjects(\'imap.example.com\', 993, \'user@example.com\', \'password123\', \'sender@example.com\')"},{"question":"# Question: Implement an Advanced XML Attribute Formatter Using the `xml.sax.saxutils` module, create a Python function `format_xml_attributes(element_name, attributes)` that generates an XML element start tag with the given attributes formatted correctly. Function Signature ```python def format_xml_attributes(element_name: str, attributes: dict) -> str: ``` Input - `element_name`: A string representing the name of the XML element (e.g., `\\"note\\"`). - `attributes`: A dictionary where keys are attribute names and values are their respective values (e.g., `{\\"to\\": \\"Tove\\", \\"from\\": \\"Jani\\", \\"heading\\": \\"Reminder\\", \\"body\\": \\"Don\'t forget me this weekend!\\"}`). Output - A string representing the start tag of the XML element with all attributes properly formatted and quoted, and special characters escaped. Constraints - Attribute values may contain characters that need escaping, such as `<`, `>`, `&`, `\'`, and `\\"`. - Ensure no conflicts in quoting scheme – the function should choose single or double quotes appropriately based on the content. - Use `xml.sax.saxutils.quoteattr` to handle the attribute value formatting and escaping. Example ```python element_name = \\"note\\" attributes = { \\"to\\": \\"Tove\\", \\"from\\": \\"Jani\\", \\"heading\\": \\"Reminder\\", \\"body\\": \\"Don\'t forget me this weekend!\\" } formatted_string = format_xml_attributes(element_name, attributes) print(formatted_string) ``` Expected Output: ```xml <note to=\\"Tove\\" from=\\"Jani\\" heading=\\"Reminder\\" body=\\"Don\'t forget me this weekend!\\"> ``` Your function should handle additional edge cases, such as empty strings and special characters, ensuring all output is valid and well-formed XML. Good luck!","solution":"from xml.sax.saxutils import quoteattr def format_xml_attributes(element_name: str, attributes: dict) -> str: Generates an XML element start tag with the given attributes formatted correctly. Args: element_name (str): The name of the XML element. attributes (dict): A dictionary where keys are attribute names and values are their respective values. Returns: str: The XML element start tag. attrs = \' \'.join(f\'{key}={quoteattr(value)}\' for key, value in attributes.items()) return f\'<{element_name} {attrs}>\'"},{"question":"Problem Statement: # Objective You are given a dataset loaded from seaborn\'s `load_dataset` function. Your task is to visualize the kernel density estimations (KDE) of the `flipper_length_mm` variable in various ways, emphasizing the differences between species and sexes of penguins in the dataset. # Dataset The dataset to be used is `penguins`. # Requirements 1. **Basic KDE Plot**: Create a basic KDE plot of the `flipper_length_mm` variable using an area plot. 2. **Detailed KDE Plot**: Adjust the bandwidth smoothing parameter to show more details by setting `bw_adjust` to 0.25. 3. **Grouped KDE Plot**: Create a KDE plot grouped by the `species` variable with color differentiation. 4. **Conditional Density**: Create a KDE plot grouped by the `species` variable without common normalization (`common_norm=False`). 5. **Faceted KDE Plot**: Create a faceted KDE plot showing the densities conditioned on `sex` variable. 6. **Stacked KDE Plot**: Create a stacked KDE plot grouped by the `sex` variable. 7. **Cumulative Density Plot**: Create a cumulative density plot using a line plot. # Expected Output Each plot should be displayed as part of a single Python script or Jupyter notebook cell. You can implement this using the `seaborn.objects` interface as shown in the provided documentation. # Constraints - Use the seaborn library and its `objects` interface for plotting. - Ensure your plots have appropriate titles and legends for clarity. # Sample Code Structure ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Basic KDE Plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE()).plot() # Detailed KDE Plot p.add(so.Area(), so.KDE(bw_adjust=0.25)).plot() # Grouped KDE Plot p.add(so.Area(), so.KDE(), color=\\"species\\").plot() # Conditional Density Plot p.add(so.Area(), so.KDE(common_norm=False), color=\\"species\\").plot() # Faceted KDE Plot p.facet(\\"sex\\").add(so.Area(), so.KDE(), color=\\"species\\").plot() # Stacked KDE Plot p.add(so.Area(), so.KDE(), so.Stack(), color=\\"sex\\").plot() # Cumulative Density Plot p.add(so.Line(), so.KDE(cumulative=True)).plot() ``` Feel free to adjust aesthetics and additional parameters to enhance the clarity and appearance of your plots.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the penguins dataset penguins = load_dataset(\\"penguins\\") def plot_basic_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE()).plot() plt.title(\\"Basic KDE Plot of Flipper Length\\") plt.show() def plot_detailed_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE(bw_adjust=0.25)).plot() plt.title(\\"Detailed KDE Plot of Flipper Length\\") plt.show() def plot_grouped_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") p.add(so.Area(), so.KDE()).plot() plt.title(\\"Grouped KDE Plot of Flipper Length by Species\\") plt.show() def plot_conditional_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") p.add(so.Area(), so.KDE(common_norm=False)).plot() plt.title(\\"Conditional Density Plot of Flipper Length by Species\\") plt.show() def plot_faceted_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"sex\\") p.add(so.Area(), so.KDE(), color=\\"species\\").plot() plt.show() def plot_stacked_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"sex\\") p.add(so.Area(), so.KDE(), so.Stack()).plot() plt.title(\\"Stacked KDE Plot of Flipper Length by Sex\\") plt.show() def plot_cumulative_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Line(), so.KDE(cumulative=True)).plot() plt.title(\\"Cumulative Density Plot of Flipper Length\\") plt.show()"},{"question":"**Implementing Audio Device Controls with `ossaudiodev`** **Objective:** To test your understanding of the `ossaudiodev` module, you need to implement a class `AudioController` that encapsulates functionalities for handling an audio device. **Class Definition:** ```python class AudioController: def __init__(self, mode: str): Initializes and opens the OSS audio device with the specified mode. :param mode: \'r\' for read-only, \'w\' for write-only, \'rw\' for both read and write. pass def set_parameters(self, format: int, channels: int, sample_rate: int) -> tuple: Sets the key audio sampling parameters. :param format: Audio format (one of the constants such as AFMT_U8, AFMT_S16_LE). :param channels: Number of channels (1 for mono, 2 for stereo). :param sample_rate: Sample rate (e.g., 44100 for CD quality). :return: Tuple containing the actual set values (format, channels, sample_rate). pass def write_audio_data(self, data: bytes) -> int: Writes the given audio data to the device. :param data: Bytes-like object containing audio data. :return: Number of bytes written. pass def read_audio_data(self, size: int) -> bytes: Reads the specified number of bytes from the device. :param size: Number of bytes to read. :return: Bytes object containing the read audio data. pass def close_device(self): Closes the audio device. pass ``` **Implementation Details:** 1. **Initialization (`__init__`):** - Open the OSS audio device with the specified mode. - Use `ossaudiodev.open()` with appropriate handling of device defaults. 2. **Set Parameters (`set_parameters`):** - Utilize `setparameters()` to set audio format, channels, and sample rate. - Return the set values as a tuple. 3. **Write Audio Data (`write_audio_data`):** - Write audio data to the device using `write()` method. - Handle blocking and non-blocking mode appropriately. 4. **Read Audio Data (`read_audio_data`):** - Read from the device using `read()` method. - Ensure the correct size of data is read. 5. **Close Device (`close_device`):** - Close the audio device using the `close()` method. **Example Usage:** ```python # Example audio format constants AFMT_U8 = 8 AFMT_S16_LE = 16 # Create an AudioController object for writing audio data audio_ctrl = AudioController(mode=\'w\') # Set the parameters to 16-bit little-endian format, stereo, and 44100 Hz sample rate params = audio_ctrl.set_parameters(format=AFMT_S16_LE, channels=2, sample_rate=44100) print(\\"Parameters set:\\", params) # Write some dummy audio data bytes_written = audio_ctrl.write_audio_data(b\'x00x01x02x03\') print(\\"Bytes written:\\", bytes_written) # Close the device audio_ctrl.close_device() ``` **Constraints:** - Assume the default device (`/dev/dsp`). - Handle appropriate exception cases (`OSSAudioError`, `OSError`). Implement the `AudioController` class following the above specifications.","solution":"import ossaudiodev class AudioController: def __init__(self, mode: str): Initializes and opens the OSS audio device with the specified mode. :param mode: \'r\' for read-only, \'w\' for write-only, \'rw\' for both read and write. self.mode = mode self.device = None if mode == \'r\': self.device = ossaudiodev.open(\'r\') elif mode == \'w\': self.device = ossaudiodev.open(\'w\') elif mode == \'rw\': self.device = ossaudiodev.open(\'rw\') else: raise ValueError(\\"Mode must be \'r\', \'w\', or \'rw\'\\") def set_parameters(self, format: int, channels: int, sample_rate: int) -> tuple: Sets the key audio sampling parameters. :param format: Audio format (one of the constants such as AFMT_U8, AFMT_S16_LE). :param channels: Number of channels (1 for mono, 2 for stereo). :param sample_rate: Sample rate (e.g., 44100 for CD quality). :return: Tuple containing the actual set values (format, channels, sample_rate). actual_format, actual_channels, actual_rate = self.device.setparameters(format, channels, sample_rate) return actual_format, actual_channels, actual_rate def write_audio_data(self, data: bytes) -> int: Writes the given audio data to the device. :param data: Bytes-like object containing audio data. :return: Number of bytes written. return self.device.write(data) def read_audio_data(self, size: int) -> bytes: Reads the specified number of bytes from the device. :param size: Number of bytes to read. :return: Bytes object containing the read audio data. return self.device.read(size) def close_device(self): Closes the audio device. if self.device: self.device.close() self.device = None"},{"question":"# Programming Assessment Question **Objective:** Assess understanding of high-level I/O multiplexing using Python\'s `selectors` module. Question You are tasked with building a multi-client chat server using Python\'s `selectors` module. The server should be capable of handling multiple clients simultaneously, allowing them to send and receive messages to and from the server. **Requirements:** 1. Use the `selectors.DefaultSelector` for handling I/O events. 2. Implement the following functionalities: - Accepting new client connections. - Receiving messages from clients. - Broadcasting received messages to all connected clients (except the sender). - Gracefully handling client disconnections. 3. Ensure that the server outputs informative messages for key events: - When a client connects or disconnects. - When a message is received and broadcasted. **Input/Output:** - **Input:** Clients connecting via sockets to the server and sending messages. - **Output:** Messages broadcasted to all connected clients and informative server-side logs printed. **Constraints:** - Use non-blocking sockets. - Handle scenarios where a client sends no data (disconnection). **Performance Requirements:** - Efficiently manage multiple client connections. - Avoid blocking operations to ensure responsiveness. **Specifications:** 1. Create a class `ChatServer` to encapsulate server logic. 2. Define methods within `ChatServer`: - `__init__(self, host, port)`: Initialize the server. - `start(self)`: Start the server and begin listening for connections. - `accept(self, sock, mask)`: Accept new client connections. - `read(self, conn, mask)`: Read incoming messages from clients and broadcast. - `broadcast(self, message, sender)`: Broadcast messages to all connected clients except the sender. - `close(self)`: Gracefully close the server. 3. Use the following example to demonstrate server usage. ```python if __name__ == \\"__main__\\": chat_server = ChatServer(\'localhost\', 12345) try: chat_server.start() except KeyboardInterrupt: print(\\"Server shutting down.\\") finally: chat_server.close() ``` **Example Scenario:** - Client A connects and sends \\"Hello\\". - The server broadcasts \\"Hello\\" to all other connected clients. - Client B connects and sends \\"Hi\\". - The server broadcasts \\"Hi\\" to all other connected clients. - Client A disconnects gracefully. - The server continues to handle messages from Client B and any new clients. Note: You may test the server using multiple terminal instances or by writing a simple client script. ```python import socket def client_script(host, port): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect((host, port)) try: while True: msg = input(\\"Enter message: \\") if msg.lower() == \\"exit\\": break sock.sendall(msg.encode()) print(\\"Message sent.\\") finally: sock.close() if __name__ == \\"__main__\\": client_script(\'localhost\', 12345) ``` Implement the `ChatServer` class based on the specifications provided.","solution":"import selectors import socket class ChatServer: def __init__(self, host, port): self.selector = selectors.DefaultSelector() self.host = host self.port = port self.clients = {} def start(self): server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_sock.bind((self.host, self.port)) server_sock.listen() server_sock.setblocking(False) self.selector.register(server_sock, selectors.EVENT_READ, self.accept) print(f\\"Server started at {self.host}:{self.port}\\") try: while True: events = self.selector.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Server shutting down.\\") except Exception as e: print(f\\"Error: {e}\\") finally: self.close() def accept(self, sock, mask): conn, addr = sock.accept() print(f\\"Connected by {addr}\\") conn.setblocking(False) self.clients[conn] = addr self.selector.register(conn, selectors.EVENT_READ, self.read) def read(self, conn, mask): try: data = conn.recv(1024) if data: message = data.decode() print(f\\"Received message from {self.clients[conn]}: {message}\\") self.broadcast(message, conn) else: self.disconnect(conn) except ConnectionResetError: self.disconnect(conn) def broadcast(self, message, sender): for conn in self.clients: if conn != sender: try: conn.sendall(message.encode()) except BrokenPipeError: self.disconnect(conn) def disconnect(self, conn): addr = self.clients.pop(conn) print(f\\"Disconnected by {addr}\\") self.selector.unregister(conn) conn.close() def close(self): for conn in self.clients.keys(): self.disconnect(conn) self.selector.close()"},{"question":"# Pandas PyArrow Integration Coding Assessment Objective You need to utilize pandas and PyArrow integration to perform a series of operations on a dataset, showing your understanding of fundamental and advanced concepts of using PyArrow with pandas. Problem Statement **Dataset:** You are provided with a dataset that consists of different data types including integers, floating-point numbers, strings, and timestamps. The dataset might contain missing values. **Task:** 1. **Loading Data:** - Read the dataset from a CSV file using Pandas. Ensure that the data is PyArrow-backed to take advantage of performance optimizations. 2. **Data Manipulation:** - Convert specific columns to PyArrow-backed data types: integers, floats, strings, and timestamps. - Perform various operations like: - Calculating the mean of a numeric column. - Handling missing values by filling them or dropping rows/columns as necessary. - String manipulation using PyArrow\'s string functions. - Datetime manipulation, such as extracting the year or month from timestamps. 3. **Optimization:** - Ensure that the operations leverage PyArrow’s compute functions where applicable for optimized performance. 4. **Conversion:** - Convert portions of the DataFrame to a PyArrow Table and demonstrate some operations with PyArrow. - Convert the PyArrow Table back to a pandas DataFrame ensuring the data types remain PyArrow-backed. Requirements - You must use the `pyarrow` engine and data types where specified. - Optimize the operations using PyArrow’s compute functions where applicable. - Ensure that the final DataFrame is still PyArrow-backed. Implementation **Function Signature:** ```python def process_dataset(file_path: str) -> pd.DataFrame: pass ``` **Input:** - `file_path (str)`: A string representing the file path of the dataset in CSV format. **Output:** - Returns a PyArrow-backed pandas DataFrame after all the required operations. # Example Usage ```python input_csv = \'path/to/dataset.csv\' result_df = process_dataset(input_csv) print(result_df.dtypes) ``` Notes: - The dataset\'s initial structure and content are not defined, but assume it includes various data types and possible missing values. - Focus on leveraging PyArrow integration for efficiency and advanced type support. # Constraints: - You may assume that `pyarrow` and `pandas` have been installed and are available for use. - The dataset file will always be readable and correctly formatted.","solution":"import pandas as pd import pyarrow as pa import pyarrow.compute as pc from pyarrow import csv def process_dataset(file_path: str) -> pd.DataFrame: # Read dataset using pyarrow table = csv.read_csv(file_path) # Convert pyarrow.Table to pandas DataFrame with pyarrow backed columns df = table.to_pandas(types_mapper=pd.ArrowDtype) # Ensure specific columns have appropriate pyarrow-backed dtypes: if \'integers\' in df.columns: df[\'integers\'] = df[\'integers\'].astype(pd.ArrowDtype(pa.int64())) if \'floats\' in df.columns: df[\'floats\'] = df[\'floats\'].astype(pd.ArrowDtype(pa.float64())) if \'strings\' in df.columns: df[\'strings\'] = df[\'strings\'].astype(pd.ArrowDtype(pa.string())) if \'timestamps\' in df.columns: df[\'timestamps\'] = df[\'timestamps\'].astype(pd.ArrowDtype(pa.timestamp(\'ms\'))) # Calculate the mean of a numeric column (assuming \'integers\' for this operation) if \'integers\' in df.columns: mean_value = df[\'integers\'].mean() print(f\\"Mean value of the integers column: {mean_value}\\") # Handle missing values by filling with a specified value (assuming 0 for integers and floats) if \'integers\' in df.columns: df[\'integers\'] = df[\'integers\'].fillna(0) if \'floats\' in df.columns: df[\'floats\'] = df[\'floats\'].fillna(0.0) # String manipulation (e.g., converting strings to uppercase using pyarrow) if \'strings\' in df.columns: string_array = pa.array(df[\'strings\']) upper_string_array = pc.utf8_upper(string_array) df[\'strings\'] = pd.Series(upper_string_array, dtype=pd.ArrowDtype(pa.string())) # Datetime manipulation (e.g., extracting the year from timestamps) if \'timestamps\' in df.columns: timestamp_array = pa.array(df[\'timestamps\']) year_array = pc.year(timestamp_array) df[\'year\'] = pd.Series(year_array, dtype=pd.ArrowDtype(pa.int32())) # Convert portions of DataFrame to pyarrow.Table and demonstrate some operations arrow_table = pa.Table.from_pandas(df) arrow_table_sum = pc.sum(arrow_table[\'integers\']) print(f\\"Sum of integers column in pyarrow table: {arrow_table_sum.as_py()}\\") # Convert back to pandas DataFrame ensuring data types remain pyarrow-backed df_final = arrow_table.to_pandas(types_mapper=pd.ArrowDtype) return df_final"},{"question":"# Seaborn Customization Challenge You are tasked with analyzing a dataset and creating a series of bar plots to visualize the distribution of categories within different groups. Your goal is to provide well-stylized and informative visualizations using seaborn. Please follow the instructions below to complete the task. Dataset `data` is a dictionary that represents the dataset. You can assume the following structure: ```python data = { \\"group\\": [\\"G1\\", \\"G1\\", \\"G1\\", \\"G2\\", \\"G2\\", \\"G2\\"], \\"category\\": [\\"A\\", \\"B\\", \\"C\\", \\"A\\", \\"B\\", \\"C\\"], \\"value\\": [4, 7, 1, 5, 2, 9] } ``` Task 1. **Data Preparation**: - Convert the dictionary `data` into a pandas DataFrame. 2. **Plotting**: - Define a function `create_barplots` that: - Generates a bar plot for each group using seaborn. - Applies a specific seaborn theme and palette to each plot. - Customizes the plots by overriding some of the seaborn parameters. 3. **Function Specification**: ```python def create_barplots(data: dict) -> None: Generates customized bar plots for each group in the dataset. Parameters: data (dict): A dictionary containing the dataset with keys \'group\', \'category\', and \'value\'. Returns: None pass ``` 4. **Constraints**: - Each bar plot should represent the categories on the x-axis and their corresponding values on the y-axis. - Use `sns.set_theme` to set a `darkgrid` style theme for the plots with a `colorblind` palette. - Further customize the plots by disabling the top and right spines. - Make sure each plot has a title that indicates which group it represents. 5. **Example**: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_barplots(data: dict) -> None: df = pd.DataFrame(data) groups = df[\'group\'].unique() sns.set_theme(style=\\"darkgrid\\", palette=\\"colorblind\\") custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"darkgrid\\", palette=\\"colorblind\\", rc=custom_params) for group in groups: group_data = df[df[\'group\'] == group] plt.figure() sns.barplot(x=\'category\', y=\'value\', data=group_data) plt.title(f\'Group {group}\') plt.show() ``` You are required to implement the `create_barplots` function as per the above specification.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_barplots(data: dict) -> None: Generates customized bar plots for each group in the dataset. Parameters: data (dict): A dictionary containing the dataset with keys \'group\', \'category\', and \'value\'. Returns: None df = pd.DataFrame(data) groups = df[\'group\'].unique() sns.set_theme(style=\\"darkgrid\\", palette=\\"colorblind\\") custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"darkgrid\\", palette=\\"colorblind\\", rc=custom_params) for group in groups: group_data = df[df[\'group\'] == group] plt.figure() sns.barplot(x=\'category\', y=\'value\', data=group_data) plt.title(f\'Group {group}\') plt.show()"},{"question":"# PyTorch Tensor Testing Challenge You are provided with a module called `torch.testing` which contains functions to generate and validate tensors. Utilizing these functions, you will implement a process for comparing two tensors to ensure they are close within a certain tolerance. This exercise will test your understanding of generating and validating tensor equivalence in PyTorch. Requirements: 1. **Function Signature**: ```python def compare_tensors(tensor_a, tensor_b, rtol=1e-05, atol=1e-08): Compares two tensors to check if they are close within a specified tolerance. Parameters: - tensor_a (torch.Tensor): The first tensor to compare. - tensor_b (torch.Tensor): The second tensor to compare. - rtol (float): The relative tolerance parameter (default is 1e-05). - atol (float): The absolute tolerance parameter (default is 1e-08). Returns: - bool: True if the tensors are close, False otherwise. ``` 2. **Implementation Details**: - Ensure you use `torch.testing.assert_allclose` function to compare the tensors. - Catch any exceptions thrown by `assert_allclose` and return `False` if an exception is thrown, indicating that the tensors are not close. - Return `True` if the tensors are close within the specified tolerance. 3. **Example Usage**: ```python import torch from torch.testing import make_tensor tensor_a = make_tensor((3, 3), dtype=torch.float32) tensor_b = tensor_a + 1e-06 # Should return True print(compare_tensors(tensor_a, tensor_b, rtol=1e-05, atol=1e-08)) tensor_c = tensor_a + 1e-04 # Should return False print(compare_tensors(tensor_a, tensor_c, rtol=1e-05, atol=1e-08)) ``` Constraints: - The tensors `tensor_a` and `tensor_b` will have the same shape and dtype in all test cases. Implement your solution in the given function signature and ensure that it correctly compares the tensors based on the specified tolerances. Good luck!","solution":"import torch def compare_tensors(tensor_a, tensor_b, rtol=1e-05, atol=1e-08): Compares two tensors to check if they are close within a specified tolerance. Parameters: - tensor_a (torch.Tensor): The first tensor to compare. - tensor_b (torch.Tensor): The second tensor to compare. - rtol (float): The relative tolerance parameter (default is 1e-05). - atol (float): The absolute tolerance parameter (default is 1e-08). Returns: - bool: True if the tensors are close, False otherwise. try: torch.testing.assert_allclose(tensor_a, tensor_b, rtol=rtol, atol=atol) return True except AssertionError: return False"},{"question":"Coding Assessment Question # Objective Implement a logging utility class that leverages the Unix `syslog` library routines to log messages with various priorities and facility options. This utility should be capable of initializing the syslog settings, logging messages, and managing the syslog connection. # Task You are required to implement a class `SysLogger` with the following interface: Methods 1. **`__init__(self, ident=None, logoption=0, facility=syslog.LOG_USER)`**: - Initialize the logger with optional ident, logoption, and facility parameters. - Use `syslog.openlog()` with the provided parameters. 2. **`log_message(self, message, priority=syslog.LOG_INFO)`**: - Log a message with a specified priority. - Default priority is `syslog.LOG_INFO`. 3. **`set_priority_mask(self, maskpri)`**: - Set the priority mask using `syslog.setlogmask()`. - Returns the previous mask value. 4. **`close_log(self)`**: - Close the syslog using `syslog.closelog()`. # Input Format - Initialization of the `SysLogger` class may include optional `ident`, `logoption`, and `facility` parameters. - `log_message` method takes a `message` string and an optional `priority` level. - `set_priority_mask` method takes a `maskpri` integer representing the priority mask. # Output Format - Ensure messages are logged correctly with the system logger. - Return the previous mask value when setting a new priority mask. # Examples ```python import syslog from your_module import SysLogger # Replace with the actual module name # Example Initialization logger = SysLogger(ident=\\"MyApp\\", logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) # Example Logging logger.log_message(\\"Processing started\\") logger.log_message(\\"An error occurred\\", priority=syslog.LOG_ERR) # Example Priority Masking prev_mask = logger.set_priority_mask(syslog.LOG_UPTO(syslog.LOG_WARNING)) # Example Closing the Logger logger.close_log() ``` Constraints - You must utilize the syslog functions (`syslog.syslog()`, `syslog.openlog()`, `syslog.closelog()`, `syslog.setlogmask()`) as described in the documentation. # Performance Requirements - The logger class should handle frequent logging attempts efficiently. - Ensure minimal performance overhead when initializing and closing the logger.","solution":"import syslog class SysLogger: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): Initialize the logger with optional ident, logoption, and facility parameters. syslog.openlog(ident=ident, logoption=logoption, facility=facility) def log_message(self, message, priority=syslog.LOG_INFO): Log a message with a specified priority. Default priority is syslog.LOG_INFO. syslog.syslog(priority, message) def set_priority_mask(self, maskpri): Set the priority mask using syslog.setlogmask(). Returns the previous mask value. return syslog.setlogmask(maskpri) def close_log(self): Close the syslog using syslog.closelog(). syslog.closelog()"},{"question":"# Question You are tasked with analyzing the effect of different plotting contexts on a dataset. Write a Python function that: 1. Loads the \'tips\' dataset from Seaborn. 2. Configures the plotting context to \\"notebook\\" and generates a scatterplot of `total_bill` vs `tip` with the title \\"Notebook Context\\". 3. Uses a context manager to temporarily change the plotting context to \\"poster\\" and generates a scatterplot with the title \\"Poster Context\\". 4. After exiting the context manager, ensures that the context reverts to the \\"notebook\\" context, and generates a histogram of `total_bill` with the title \\"Reverted to Notebook Context\\". 5. Save all three plots to files named `notebook_context_scatter.png`, `poster_context_scatter.png`, and `reverted_notebook_context_hist.png`. Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_plotting_context(): pass ``` Constraints and Notes - The function should not return any value; it should save the plots to files. - Ensure that the plotting contexts are set correctly as per the instructions before generating each plot. - Use Seaborn\'s `scatterplot` and `histplot` functions to generate the plots. - The dataset should be loaded as follows: `tips = sns.load_dataset(\\"tips\\")`. Example Usage The expected output will be the creation of three image files stored in the working directory: 1. `notebook_context_scatter.png` 2. `poster_context_scatter.png` 3. `reverted_notebook_context_hist.png` You can then verify the plots by opening these image files.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_plotting_context(): # Load the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") # Set the context to \\"notebook\\" and generate the scatterplot sns.set_context(\\"notebook\\") sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Notebook Context\\") plt.savefig(\\"notebook_context_scatter.png\\") plt.clf() # clear the figure # Use a context manager to temporarily set context to \\"poster\\" and generate the scatterplot with sns.plotting_context(\\"poster\\"): sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Poster Context\\") plt.savefig(\\"poster_context_scatter.png\\") plt.clf() # clear the figure # Generate the histogram after reverting to \\"notebook\\" context sns.set_context(\\"notebook\\") sns.histplot(data=tips, x=\\"total_bill\\") plt.title(\\"Reverted to Notebook Context\\") plt.savefig(\\"reverted_notebook_context_hist.png\\") plt.clf() # clear the figure"},{"question":"**Problem: Manage File Backups** As part of a backup solution, you are required to identify and manage specific file types within a directory and its subdirectories. You will need to create a function that searches for files matching a given pattern and outputs a summary of the findings. By leveraging the `glob` module, you will write a function `file_backup_summary` which takes the following parameters: - `root_directory` (str): The root directory where the search will begin. - `file_patterns` (list): A list of patterns to search for (e.g., `[\'*.txt\', \'*.jpg\']`). The function should return a dictionary where each key is a file pattern and the value is a list of tuples. Each tuple should contain the relative path to the matching file and the size of the file in bytes. # Input - `root_directory`: a string representing the root directory (e.g., \\"/home/user/documents\\"). - `file_patterns`: a list of strings, where each string is a pattern (e.g., `[\'*.txt\', \'*.jpg\']`). # Output - A dictionary where keys are patterns from `file_patterns` and values are lists of tuples, each containing the relative path of the matching file and its size in bytes. # Constraints - You must use the `glob` module for pattern matching. - The function should handle large directory structures efficiently. - File sizes should be obtained using proper file I/O methods. - Assume the root directory and patterns are always valid and within reasonable limits. # Example ```python import os import glob def file_backup_summary(root_directory, file_patterns): backup_summary = {} for pattern in file_patterns: # Initialize the list for each pattern backup_summary[pattern] = [] # Perform the pattern matching matching_files = glob.iglob(os.path.join(root_directory, \'**\', pattern), recursive=True) for file in matching_files: if os.path.isfile(file): relative_path = os.path.relpath(file, root_directory) file_size = os.path.getsize(file) backup_summary[pattern].append((relative_path, file_size)) return backup_summary # Example usage root_dir = \'./test_directory\' patterns = [\'*.txt\', \'*.jpg\'] print(file_backup_summary(root_dir, patterns)) ``` **Explanation:** - In the example with `root_dir` being `./test_directory` and patterns `[\'*.txt\', \'*.jpg\']`, it will recursively search for `.txt` and `.jpg` files starting from `./test_directory` and list them with their relative paths and sizes. Make sure to test your function extensively with varied directory structures and file types to ensure its robustness.","solution":"import os import glob def file_backup_summary(root_directory, file_patterns): Generate a summary of files matching given patterns in a directory and its subdirectories. Parameters: root_directory (str): The root directory to start searching from. file_patterns (list): A list of file patterns to match (e.g., [\'*.txt\', \'*.jpg\']). Returns: dict: Dictionary where keys are file patterns and values are lists of tuples. Each tuple contains the relative path of the matching file and its size in bytes. backup_summary = {} for pattern in file_patterns: # Initialize the list for each pattern backup_summary[pattern] = [] # Perform the pattern matching matching_files = glob.iglob(os.path.join(root_directory, \'**\', pattern), recursive=True) for file in matching_files: if os.path.isfile(file): relative_path = os.path.relpath(file, root_directory) file_size = os.path.getsize(file) backup_summary[pattern].append((relative_path, file_size)) return backup_summary"},{"question":"**Question: Advanced Seaborn Aesthetics** As a data analyst, you are tasked with creating a series of visualizations that explore a dataset and present insights in an aesthetically pleasing manner. Your task involves using the Seaborn library to control figure aesthetics and context effectively. # Instructions: 1. **Data Generation:** - Generate a dataset as follows: - Create a 2D NumPy array `data` with shape (100, 4), where each column follows a normal distribution with different means [10, 20, 30, 40] and a fixed standard deviation of 5. 2. **Visualizer Function:** - Write a function `advanced_visualizer(data)` that takes a NumPy array `data` as an input and performs the following steps: - **Figure 1:** Plot a boxplot with the `whitegrid` style. - **Figure 2:** Plot a violin plot with the `dark` style. - **Figure 3:** Plot a set of offset sine waves with different contexts indicating `paper`, `notebook`, `talk`, and `poster`. - Customize the appearance of the plots by modifying at least two parameters in each plot\'s style and context settings using the `rc` parameter. - Remove the top and right spines from all plots in Figures 1 and 2. # Example Output: Your function should generate three figures: 1. A boxplot with the `whitegrid` style and custom parameters. 2. A violin plot with the `dark` style and custom parameters. 3. A grid of 4 subplots with offset sine waves, each having a different context (`paper`, `notebook`, `talk`, `poster`). # Detailed Requirements: - Ensure that your visuals are clear and visually appealing. - Use appropriate titles, labels, and legends to make your plots informative. - Include assert statements or appropriate checks to handle edge cases, such as ensuring the input `data` has the correct shape. # Input: - `data` (NumPy array): A 2D array of shape (100, 4) where each column represents a different dataset. # Output: - Three Matplotlib figures displayed inline. # Constraints: - Use Seaborn and Matplotlib for plotting. - Maintain consistency in aesthetic customizations across the figures. # Code: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def generate_data(): np.random.seed(0) means = [10, 20, 30, 40] data = np.array([np.random.normal(loc=mean, scale=5, size=100) for mean in means]).T return data def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) def advanced_visualizer(data): assert data.shape == (100, 4), \\"Input data must have shape (100, 4)\\" # Figure 1: Boxplot with whitegrid style sns.set_style(\\"whitegrid\\", {\\"axes.facecolor\\": \\".9\\", \\"grid.color\\": \\".8\\"}) plt.figure(figsize=(10, 6)) sns.boxplot(data=data) sns.despine(top=True, right=True) plt.title(\'Boxplot with Whitegrid Style\') plt.show() # Figure 2: Violin plot with dark style sns.set_style(\\"dark\\", {\\"axes.edgecolor\\": \\".2\\", \\"grid.color\\": \\".6\\"}) plt.figure(figsize=(10, 6)) sns.violinplot(data=data) sns.despine(top=True, right=True) plt.title(\'Violin plot with Dark Style\') plt.show() # Figure 3: Grid of sine plots with different contexts plt.figure(figsize=(12, 12)) gs = plt.GridSpec(2, 2) contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] titles = [\\"Paper Context\\", \\"Notebook Context\\", \\"Talk Context\\", \\"Poster Context\\"] for i, context in enumerate(contexts): with sns.plotting_context(context, rc={\\"lines.linewidth\\": 2.5}): ax = plt.subplot(gs[i]) sinplot() ax.set_title(titles[i]) plt.tight_layout() plt.show() # Usage data = generate_data() advanced_visualizer(data) ``` **Note:** Make sure the code is executed in an environment where the necessary libraries (NumPy, Seaborn, Matplotlib) are installed.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def generate_data(): Generates a 2D NumPy array with shape (100, 4), where each column follows a normal distribution with different means. np.random.seed(0) means = [10, 20, 30, 40] data = np.array([np.random.normal(loc=mean, scale=5, size=100) for mean in means]).T return data def sinplot(n=10, flip=1): Plots a set of offset sine waves. x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) def advanced_visualizer(data): Generates a series of visualizations based on the input data using different Seaborn styles and contexts. assert data.shape == (100, 4), \\"Input data must have shape (100, 4)\\" # Figure 1: Boxplot with whitegrid style sns.set_style(\\"whitegrid\\", {\\"axes.facecolor\\": \\".9\\", \\"grid.color\\": \\".8\\"}) plt.figure(figsize=(10, 6)) sns.boxplot(data=data) sns.despine(top=True, right=True) plt.title(\'Boxplot with Whitegrid Style\') plt.show() # Figure 2: Violin plot with dark style sns.set_style(\\"dark\\", {\\"axes.edgecolor\\": \\".2\\", \\"grid.color\\": \\".6\\"}) plt.figure(figsize=(10, 6)) sns.violinplot(data=data) sns.despine(top=True, right=True) plt.title(\'Violin plot with Dark Style\') plt.show() # Figure 3: Grid of sine plots with different contexts plt.figure(figsize=(12, 12)) gs = plt.GridSpec(2, 2) contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] titles = [\\"Paper Context\\", \\"Notebook Context\\", \\"Talk Context\\", \\"Poster Context\\"] for i, context in enumerate(contexts): with sns.plotting_context(context, rc={\\"lines.linewidth\\": 2.5}): ax = plt.subplot(gs[i]) sinplot() ax.set_title(titles[i]) plt.tight_layout() plt.show()"},{"question":"**Objective**: Assess the comprehension of Python\'s built-in functions related to mathematical operations, iterable handling, and object attribute manipulation. **Problem Statement**: You are tasked with implementing a class called `StatsCalculator` that will calculate various statistical measures from a given list of numbers. Your class should be able to: 1. Calculate the mean of the list. 2. Calculate the median of the list. 3. Calculate the mode of the list. 4. Handle updates to the list, including adding and removing elements. 5. Check if certain elements exist in the list using a binary search approach (requiring the list to be sorted). 6. Provide the list sorted either in ascending or descending order. **Detailed Requirements**: 1. **Mean Calculation**: - Implement a method `mean` that returns the mean of the list. 2. **Median Calculation**: - Implement a method `median` that returns the median of the list. 3. **Mode Calculation**: - Implement a method `mode` that returns the mode of the list. 4. **Update List**: - Implement methods `add_element` and `remove_element` to add and remove elements from the list respectively. 5. **Element Existence Check**: - Implement a method `element_exists` that checks if a given element is in the list using binary search. The list must be sorted before performing this search. 6. **Sorting**: - Implement a method `get_sorted_list` that returns the list sorted in either ascending (default) or descending order based on a parameter. **Constraints**: - You can assume the list will only contain integers. - The `mode` should return one of the modes if there are ties. - Handle cases where the removal of non-existing elements or calculation of statistics on an empty list should raise appropriate exceptions. **Input and Output**: - Create an instance of `StatsCalculator` with an initial list of integers. - Methods described will perform operations and return relevant results or raise exceptions. - Use built-in functions wherever applicable. # Class Definition ```python class StatsCalculator: def __init__(self, initial_list=None): Initialize the calculator with an initial list of numbers. if initial_list is None: self.numbers = [] else: self.numbers = list(initial_list) def mean(self): Calculate and return the mean of the numbers. pass def median(self): Calculate and return the median of the numbers. pass def mode(self): Calculate and return the mode of the numbers. pass def add_element(self, element): Add an element to the list. pass def remove_element(self, element): Remove an element from the list. Raise ValueError if element is not in the list. pass def element_exists(self, element): Check if an element exists in the list using binary search. Return True if exists, else False. pass def get_sorted_list(self, descending=False): Return the list sorted in ascending or descending order based on the parameter. pass ``` # Example Usage ```python # Initialize with a list calculator = StatsCalculator([1, 3, 3, 6, 7, 8, 9]) # Add and remove elements calculator.add_element(5) calculator.remove_element(8) # Calculate mean, median, and mode print(calculator.mean()) # Expected output: (sum of the list) / (number of elements) print(calculator.median()) # Expected output: Middle element(s) of the sorted list print(calculator.mode()) # Expected output: Most frequent element in the list # Check if an element exists print(calculator.element_exists(5)) # Expected output: True print(calculator.element_exists(8)) # Expected output: False # Get sorted list print(calculator.get_sorted_list()) # Expected output: List sorted in ascending order print(calculator.get_sorted_list(descending=True)) # Expected output: List sorted in descending order ``` *Note*: Write clean and efficient code, ensuring the performance is considered, especially for large lists. Use appropriate built-in functions provided in Python wherever applicable to demonstrate a strong understanding of the language capabilities.","solution":"from collections import Counter from bisect import bisect_left class StatsCalculator: def __init__(self, initial_list=None): Initialize the calculator with an initial list of numbers. if initial_list is None: self.numbers = [] else: self.numbers = list(initial_list) def mean(self): Calculate and return the mean of the numbers. if not self.numbers: raise ValueError(\\"Mean is undefined for an empty list\\") return sum(self.numbers) / len(self.numbers) def median(self): Calculate and return the median of the numbers. if not self.numbers: raise ValueError(\\"Median is undefined for an empty list\\") sorted_numbers = sorted(self.numbers) n = len(sorted_numbers) mid = n // 2 if n % 2 == 1: return sorted_numbers[mid] else: return (sorted_numbers[mid - 1] + sorted_numbers[mid]) / 2 def mode(self): Calculate and return the mode of the numbers. if not self.numbers: raise ValueError(\\"Mode is undefined for an empty list\\") count = Counter(self.numbers) max_count = max(count.values()) mode_candidates = [k for k, v in count.items() if v == max_count] return mode_candidates[0] # Return the first mode if there are multiple def add_element(self, element): Add an element to the list. self.numbers.append(element) def remove_element(self, element): Remove an element from the list. Raise ValueError if element is not in the list. try: self.numbers.remove(element) except ValueError: raise ValueError(f\\"Element {element} not found in list\\") def element_exists(self, element): Check if an element exists in the list using binary search. Return True if exists, else False. sorted_numbers = sorted(self.numbers) index = bisect_left(sorted_numbers, element) if index != len(sorted_numbers) and sorted_numbers[index] == element: return True return False def get_sorted_list(self, descending=False): Return the list sorted in ascending or descending order based on the parameter. return sorted(self.numbers, reverse=descending)"},{"question":"<|Analysis Begin|> The `torch.fx.experimental` module comprises various advanced tools for symbolic shapes and proxy tensors. These APIs are highly experimental and pertain to facilitating symbolic shape inference and manipulation within the PyTorch framework, which is especially useful for dynamic shape computations and various compile-time optimizations. Key classes and functions to note: 1. **Classes in `torch.fx.experimental.symbolic_shapes`**: - `ShapeEnv`, `DimDynamic`, `StrictMinMaxConstraint`, `RelaxedUnspecConstraint`, etc. - These are specialized classes designed to facilitate symbolic manipulation of tensor shapes. 2. **Key Methods/Functions**: - `hint_int`, `is_concrete_int`, `has_free_symbols`, `definitely_true`, `constrain_range`, etc. - Functions for inspecting and manipulating symbolic properties of tensors. 3. **Classes in `torch.fx.experimental.proxy_tensor`**: - `make_fx`, `handle_sym_dispatch`, `get_proxy_mode` - These classes and methods are concerned with using proxies and tracing to monitor and manipulate tensor operations. Given this context, a well-thought-out coding question should ask students to implement a function that utilizes these symbolic shape manipulation tools to perform a relatively complex shape inference or manipulation task. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Symbolic Shape Inference and Manipulation **Objective**: You will implement a function using PyTorch\'s `torch.fx.experimental.symbolic_shapes` module. Your function needs to demonstrate an understanding of symbolic shape environment creation and manipulation. Problem Statement: Create a function `symbolic_batch_size_inference` which simulates a scenario where the batch size of a tensor is dynamic and symbolic. The function should take a tensor with a batch size placeholder and compute a new shape where specific symbolic operations are applied. Input: - `tensor_shape`: A list of integers representing the shape of a tensor where the first dimension (batch size) is symbolic (`-1`). - `dim_operations`: A dictionary specifying operations to perform on non-batch dimensions. Each key is the dimension index (starting from 1 since 0 is the batch size), and the value is a tuple `(operation, value)`, where `operation` can be `\\"mul\\"`, `\\"add\\"`, or `\\"div\\"` representing multiplication, addition, or division respectively by the `value`. Output: - A new list representing the inferred shape using the provided symbolic operations. Constraints: - The batch size remains symbolic and is represented by `-1` in the output. - Indexes in `dim_operations` must be valid dimensions of the input `tensor_shape`. - Operations should adhere to typical tensor shape rules (e.g., division must result in an integer). # Example: ```python # Example input tensor_shape = [-1, 64, 32, 16] dim_operations = {1: (\\"mul\\", 2), 2: (\\"add\\", 10), 3: (\\"div\\", 2)} # Example Output # The resultant shape would be `[-1, 128, 42, 8]` ``` # Implementation: Use the provided functions and classes from `torch.fx.experimental.symbolic_shapes` to manage symbolic shape manipulations. ```python from torch.fx.experimental.symbolic_shapes import ShapeEnv, DimDynamic def symbolic_batch_size_inference(tensor_shape, dim_operations): Infer the new tensor shape with symbolic operations applied to non-batch dimensions. :param tensor_shape: List[int], shape of the tensor with the first dimension as symbolic -1 :param dim_operations: Dict[int, Tuple[str, int]], operations to perform on dimensions :return: List[int], inferred shape after symbolic operations # Create a ShapeEnv instance shape_env = ShapeEnv() # Initialize symbolic batch size symbolic_batch_size = DimDynamic() shape_env.set_dim(0, symbolic_batch_size) # Initialize the resultant shape inferred_shape = [symbolic_batch_size] # Iterate and apply operations based on dim_operations for idx in range(1, len(tensor_shape)): current_dim = tensor_shape[idx] if idx in dim_operations: operation, value = dim_operations[idx] if operation == \\"mul\\": new_dim = current_dim * value elif operation == \\"add\\": new_dim = current_dim + value elif operation == \\"div\\": new_dim = current_dim // value else: raise ValueError(f\\"Unsupported operation: {operation}\\") else: new_dim = current_dim inferred_shape.append(new_dim) return inferred_shape # Test the function with example inputs print(symbolic_batch_size_inference([-1, 64, 32, 16], {1: (\\"mul\\", 2), 2: (\\"add\\", 10), 3: (\\"div\\", 2)})) # Expected Output: [-1, 128, 42, 8] --- ```","solution":"from torch.fx.experimental.symbolic_shapes import ShapeEnv, DimDynamic def symbolic_batch_size_inference(tensor_shape, dim_operations): Infer the new tensor shape with symbolic operations applied to non-batch dimensions. :param tensor_shape: List[int], shape of the tensor with the first dimension as symbolic -1 :param dim_operations: Dict[int, Tuple[str, int]], operations to perform on dimensions :return: List[int], inferred shape after symbolic operations # Create a ShapeEnv instance shape_env = ShapeEnv() # Initialize symbolic batch size symbolic_batch_size = -1 # Initialize the resultant shape inferred_shape = [symbolic_batch_size] # Iterate and apply operations based on dim_operations for idx in range(1, len(tensor_shape)): current_dim = tensor_shape[idx] if idx in dim_operations: operation, value = dim_operations[idx] if operation == \\"mul\\": new_dim = current_dim * value elif operation == \\"add\\": new_dim = current_dim + value elif operation == \\"div\\": if current_dim % value != 0: raise ValueError(f\\"Dimension cannot be evenly divided by {value}\\") new_dim = current_dim // value else: raise ValueError(f\\"Unsupported operation: {operation}\\") else: new_dim = current_dim inferred_shape.append(new_dim) return inferred_shape # Test the function with example inputs print(symbolic_batch_size_inference([-1, 64, 32, 16], {1: (\\"mul\\", 2), 2: (\\"add\\", 10), 3: (\\"div\\", 2)})) # Expected Output: [-1, 128, 42, 8]"},{"question":"# Advanced DateTime Manipulation with Python Problem Statement You are tasked with designing a function that takes in specified dates and times, creates `datetime` and `timedelta` objects, and then performs operations to compute the difference between two datetime objects and format the result in a human-readable string. Function Signature ```python def datetime_difference(year1: int, month1: int, day1: int, hour1: int, minute1: int, second1: int, year2: int, month2: int, day2: int, hour2: int, minute2: int, second2: int) -> str: ``` Input - `year1`, `month1`, `day1`, `hour1`, `minute1`, `second1`: Integers representing the first date and time. - `year2`, `month2`, `day2`, `hour2`, `minute2`, `second2`: Integers representing the second date and time. Output - A string representing the difference between the two datetime objects in the format: ``` \\"Difference is X years, Y months, Z days, H hours, M minutes, S seconds.\\" ``` Constraints - The dates and times provided will always be valid Gregorian calendar dates. - Both datetime objects will always be positive differences (i.e., the second datetime will always be after the first datetime). Example ```python assert datetime_difference(2020, 1, 1, 0, 0, 0, 2021, 1, 1, 0, 0, 0) == \\"Difference is 1 years, 0 months, 0 days, 0 hours, 0 minutes, 0 seconds.\\" assert datetime_difference(2020, 5, 15, 12, 34, 56, 2022, 7, 16, 14, 36, 58) == \\"Difference is 2 years, 2 months, 1 days, 2 hours, 2 minutes, 2 seconds.\\" ``` Notes - You may need to use the `datetime` module to create datetime objects and compute differences. - Pay attention to proper conversion of the difference into years, months, days, hours, minutes, and seconds. Advanced Requirement (Optional for extra practice) Extend your function to also handle time zones, assuming both inputs represent naive datetimes in UTC. You may find methods like `replace`, `timedelta`, and properties like `total_seconds` useful in your implementation.","solution":"from datetime import datetime, timedelta def datetime_difference(year1, month1, day1, hour1, minute1, second1, year2, month2, day2, hour2, minute2, second2) -> str: date1 = datetime(year1, month1, day1, hour1, minute1, second1) date2 = datetime(year2, month2, day2, hour2, minute2, second2) delta = date2 - date1 # Break down the difference into components years = month_difference = day_difference = hour_difference = 0 minute_difference = second_difference = 0 # Extract total days and seconds total_seconds = delta.total_seconds() total_days = int(total_seconds // (24 * 3600)) remaining_seconds = int(total_seconds % (24 * 3600)) # Calculate year and month difference year_diff = date2.year - date1.year month_diff = date2.month - date1.month if month_diff < 0: year_diff -= 1 month_diff += 12 years = year_diff month_difference = month_diff # Recalculate total days with full years and months earlier_date_new = date1.replace(year=date1.year + years, month=date1.month + month_difference) if earlier_date_new > date2: month_difference -= 1 if month_difference < 0: years -= 1 month_difference += 12 earlier_date_new = date1.replace(year=date1.year + years, month=date1.month + month_difference) day_difference = (date2 - earlier_date_new).days # Calculate remaining hour, minute, and second differences time_difference = (date2 - earlier_date_new).seconds hour_difference = time_difference // 3600 remaining_seconds = time_difference % 3600 minute_difference = remaining_seconds // 60 second_difference = remaining_seconds % 60 return f\\"Difference is {years} years, {month_difference} months, {day_difference} days, {hour_difference} hours, {minute_difference} minutes, {second_difference} seconds.\\""},{"question":"# Email Policy Customization and Application The `email.policy` module in Python provides flexibility in handling email messages through various policy objects. In this task, you are required to implement a custom policy that modifies certain default behaviors and apply this policy to parse and generate an email message. # Task 1. **Custom Policy Implementation**: - Create a custom policy based on `EmailPolicy` that: - Has a maximum line length of 100 characters. - Uses `rn` as the line separator. - Allows `8bit` Content Transfer Encoding. - Enables raising errors on defects. 2. **Applying the Custom Policy**: - Parse an email message from a string using the custom policy. - Modify the email message by adding a new header and updating an existing one. - Serialize the modified email message back to a string. # Requirements 1. Define a function `create_custom_policy()` that returns an instance of the custom policy. 2. Define a function `parse_and_modify_email()` that: - Takes an email message string as input. - Parses the email message using the custom policy. - Adds a header `X-Custom-Header: CustomValue`. - Updates the `Subject` header to `Updated Subject`. - Serializes the modified email message back to a string and returns it. # Constraints - Ensure that the custom policy adheres to the specified attributes. - The input email message string will be in a valid email format. # Example ```python # Original email message string email_str = From: sender@example.com To: recipient@example.com Subject: Original Subject This is the email body. # Expected output email message string expected_output = From: sender@example.com To: recipient@example.com Subject: Updated Subject X-Custom-Header: CustomValue This is the email body. custom_policy = create_custom_policy() output = parse_and_modify_email(email_str) assert output == expected_output ``` # Function Signatures ```python def create_custom_policy() -> email.policy.Policy: # Implement this function pass def parse_and_modify_email(email_str: str) -> str: # Implement this function pass ``` You may use the `email` package\'s functionality such as `message_from_string`, `BytesGenerator`, and others as needed.","solution":"import email from email import policy from email.message import EmailMessage from email.parser import BytesParser from email.generator import BytesGenerator from io import BytesIO def create_custom_policy(): Creates a custom email policy. return policy.EmailPolicy( max_line_length=100, linesep=\'rn\', cte_type=\'8bit\' ).clone(raise_on_defect=True) def parse_and_modify_email(email_str: str) -> str: Parses an email message from a string, modifies it by adding a custom header and updating the subject, then serializes it back to a string. custom_policy = create_custom_policy() # Parse the email message using the custom policy msg = BytesParser(policy=custom_policy).parsebytes(email_str.encode()) # Add a custom header msg[\'X-Custom-Header\'] = \'CustomValue\' # Update the subject msg.replace_header(\'Subject\', \'Updated Subject\') # Serialize the modified email back to a string buf = BytesIO() generator = BytesGenerator(buf, policy=custom_policy) generator.flatten(msg) return buf.getvalue().decode() # Example usage email_str = From: sender@example.com To: recipient@example.com Subject: Original Subject This is the email body. output = parse_and_modify_email(email_str)"},{"question":"**Challenging Coding Assessment Question:** # Problem Statement You are tasked with implementing a monitoring utility for a Python application that manages memory usage and detects potential memory leaks. Your solution will perform garbage collection, retrieve detailed statistics, and monitor uncollectable objects. Specifically, you will: 1. **Enable and disable garbage collection** as needed. 2. **Run and collect statistics** about garbage collection. 3. **Identify uncollectable objects** and store them for inspection. 4. **Add callbacks** to monitor the garbage collection process. # Implementation Requirements - **Function Name**: `monitor_memory` - **Input**: None - **Output**: - `collection_stats` (list): A list of dictionaries containing garbage collection statistics per generation. - `uncollectable_info` (list): A list of objects that were found to be uncollectable. # Steps to Implement 1. **Enable garbage collection** using `gc.enable()`. 2. **Define a callback** function that collects statistics before and after garbage collection events. This function should be attached to the `gc.callbacks` list. 3. **Run a full garbage collection** using `gc.collect()` and capture the number of unreachable objects. 4. **Retrieve and store** detailed garbage collection statistics using `gc.get_stats()`. 5. **Save uncollectable objects** by setting the appropriate debugging flag `gc.set_debug(gc.DEBUG_SAVEALL)` and accessing `gc.garbage`. 6. **Return the collected statistics** and the list of uncollectable objects. # Example Usage ```python import gc def monitor_memory(): # Enable garbage collection gc.enable() # Define a callback function for monitoring def gc_callback(phase, info): if phase == \'start\': print(f\\"GC about to start (generation {info[\'generation\']})\\") elif phase == \'stop\': print(f\\"GC finished: collected {info[\'collected\']} objects, {info[\'uncollectable\']} uncollectable\\") # Add the callback to the gc.callbacks list gc.callbacks.append(gc_callback) # Enable detailed debugging output: gc.set_debug(gc.DEBUG_SAVEALL) # Perform a full garbage collection unreachable = gc.collect() # Get and store the collection statistics collection_stats = gc.get_stats() # Capture and return the statistics and uncollectable objects uncollectable_info = gc.garbage return collection_stats, uncollectable_info # Example usage of the function stats, uncollectables = monitor_memory() print(\\"Collection Stats:\\", stats) print(\\"Uncollectable objects:\\", uncollectables) ``` # Constraints and Assumptions - Your function should be self-contained and not depend on any external libraries apart from `gc`. - The callback function should only print information for debugging purposes; actual implementations can store necessary information as needed. - Assume that the application using this function could potentially generate reference cycles, thus necessitating garbage collection. Implement the `monitor_memory` function in Python using the guidelines and example provided above.","solution":"import gc def monitor_memory(): # Enable garbage collection gc.enable() # Data structures to hold results collection_stats = [] uncollectable_info = [] # Define a callback function for monitoring def gc_callback(phase, info): if phase == \'start\': print(f\\"GC about to start (generation {info[\'generation\']})\\") elif phase == \'stop\': print(f\\"GC finished: collected {info[\'collected\']} objects, {info[\'uncollectable\']} uncollectable\\") # Capture statistics after garbage collection collection_stats.append(gc.get_stats()) uncollectable_info.extend(gc.garbage) # Add the callback to the gc.callbacks list gc.callbacks.append(gc_callback) # Enable detailed debugging output: gc.set_debug(gc.DEBUG_SAVEALL) # Perform a full garbage collection unreachable = gc.collect() # Initially collect statistics collection_stats.append(gc.get_stats()) # Return the collected statistics and uncollectable objects return collection_stats, uncollectable_info"},{"question":"**Question: Implement a Custom Synchronous and Asynchronous Iterator** Asynchronous programming and iteration protocols are fundamental components of advanced Python programming. In this question, you will need to implement both a synchronous iterator and an asynchronous iterator that comply with the Python 3.10 iterator protocols. # Part 1: Synchronous Iterator Create a class `CustomIterator` to act as a synchronous iterator. The iterator should yield the squares of numbers from 1 to a given limit `n`. **Expected Method:** ```python class CustomIterator: def __init__(self, n: int): Initialize the iterator with the given limit n. pass def __iter__(self): Return the iterator object. pass def __next__(self): Return the next square in the sequence. pass ``` **Input:** - `CustomIterator(n)` where `n` is a positive integer. **Output:** - An iterator that yields squares of numbers from 1 to `n`. # Part 2: Asynchronous Iterator Create a class `AsyncCustomIterator` to act as an asynchronous iterator. The iterator should asynchronously yield the squares of numbers from 1 to a given limit `n`. **Expected Method:** ```python class AsyncCustomIterator: def __init__(self, n: int): Initialize the asynchronous iterator with the given limit n. pass def __aiter__(self): Return the asynchronous iterator object. pass async def __anext__(self): Return the next square in the sequence asynchronously. pass ``` **Input:** - `AsyncCustomIterator(n)` where `n` is a positive integer. **Output:** - An asynchronous iterator that yields squares of numbers from 1 to `n`. # Constraints: - The classes should handle invalid inputs by raising a `ValueError`. - Both iterators should manage state correctly to avoid infinite loops or premature termination. - Proper error handling and cleanup is expected in both synchronous and asynchronous contexts. # Example Usage: ```python # Synchronous usage synchronous_iterator = CustomIterator(5) for value in synchronous_iterator: print(value) # Output: 1, 4, 9, 16, 25 # Asynchronous usage import asyncio async def main(): asynchronous_iterator = AsyncCustomIterator(5) async for value in asynchronous_iterator: print(value) # Output: 1, 4, 9, 16, 25 asyncio.run(main()) ``` Implement both `CustomIterator` and `AsyncCustomIterator` to demonstrate the synchronous and asynchronous iteration mechanics in Python 3.10.","solution":"class CustomIterator: def __init__(self, n: int): if not isinstance(n, int) or n <= 0: raise ValueError(\\"n must be a positive integer\\") self._n = n self._current = 0 def __iter__(self): return self def __next__(self): self._current += 1 if self._current > self._n: raise StopIteration return self._current ** 2 import asyncio class AsyncCustomIterator: def __init__(self, n: int): if not isinstance(n, int) or n <= 0: raise ValueError(\\"n must be a positive integer\\") self._n = n self._current = 0 def __aiter__(self): return self async def __anext__(self): self._current += 1 if self._current > self._n: raise StopAsyncIteration await asyncio.sleep(0) # Simulate async behavior return self._current ** 2"},{"question":"Objective: To assess your understanding of creating and using color palettes with seaborn\'s `light_palette` function and applying them to visualizations. Problem Statement: You are provided with a dataset of yearly average temperatures for different cities over the past decade. Your task is to create color palettes using seaborn and apply them to visualize the temperature trends for each city. Dataset: The dataset is a CSV file with the following columns: - **city**: The name of the city. - **year**: The year of the temperature record. - **avg_temp**: The average temperature of that year. Here is a sample of the data: ``` city,year,avg_temp New York,2010,12.5 New York,2011,13.0 New York,2012,11.5 Los Angeles,2010,15.0 Los Angeles,2011,14.8 ... ``` Task: 1. Load the dataset into a DataFrame. 2. Create three different color palettes using `sns.light_palette`: - A palette with a specified color name (e.g., \\"seagreen\\"). - A palette with a specified hex code (e.g., \\"#79C\\"). - A palette with a specified color using the HUSL system (e.g., `(20, 60, 50)`). 3. Generate a line plot for each city showing its average temperature trend over the years. - Apply each of the three palettes to the plot such that each city is represented by a different color from the corresponding palette. - Ensure that the plots are distinguishable and aesthetically pleasing. 4. Return a dictionary with each palette\'s configuration and the corresponding plots. Function Signature: ```python import pandas as pd import seaborn as sns from matplotlib import pyplot as plt def visualize_temperature_trends(file_path: str) -> dict: # Implement the function based on the specifications. pass ``` Expected Output: A dictionary with keys as palette names and values as plots. Each value should be a list of plots: ```python { \\"seagreen_palette\\": [plot1, plot2, ...], \\"hex_palette\\": [plot3, plot4, ...], \\"husl_palette\\": [plot5, plot6, ...] } ``` Constraints: - The dataset file path is provided as input to the function. - Ensure that the palettes are well-distinguished in the plots. - Use seaborn and matplotlib for plotting. Additional Information: - You may assume the dataset is always in the correct format. - Focus on creating readable and visually appealing plots. Example Usage: ```python result = visualize_temperature_trends(\\"path_to_dataset.csv\\") # Access and display the plots from the result dictionary result[\'seagreen_palette\'][0].show() ... ```","solution":"import pandas as pd import seaborn as sns from matplotlib import pyplot as plt def visualize_temperature_trends(file_path: str) -> dict: # Load the dataset df = pd.read_csv(file_path) # Create palettes seagreen_palette = sns.light_palette(\\"seagreen\\", n_colors=len(df[\'city\'].unique())) hex_palette = sns.light_palette(\\"#79C\\", n_colors=len(df[\'city\'].unique())) husl_palette = sns.light_palette((20, 60, 50), input=\\"husl\\", n_colors=len(df[\'city\'].unique())) palettes = { \\"seagreen_palette\\": seagreen_palette, \\"hex_palette\\": hex_palette, \\"husl_palette\\": husl_palette } result = {} for palette_name, palette in palettes.items(): plots = [] cities = df[\'city\'].unique() sns.set_style(\\"whitegrid\\") for city in cities: plt.figure(figsize=(10, 6)) city_data = df[df[\'city\'] == city] sns.lineplot(data=city_data, x=\'year\', y=\'avg_temp\', palette=palette, linewidth=2.5) plt.title(f\'Temperature trends for {city} using {palette_name}\') plt.xlabel(\'Year\') plt.ylabel(\'Average Temperature\') plt.legend([city]) plt.grid(True) plots.append(plt) plt.close() result[palette_name] = plots return result"},{"question":"Task You are required to implement a Python function that opens a URL and handles HTTP basic authentication. Additionally, you will need to handle the redirections and process the HTTP response. To simulate a real-world scenario, you need to create an opener that supports basic HTTP authentication and redirection handling. Requirements 1. **Define a function `fetch_url_with_auth`**: ```python def fetch_url_with_auth(url: str, username: str, password: str) -> str: ``` - **Parameters**: - `url` (str): The URL to open. - `username` (str): The username for basic authentication. - `password` (str): The password for basic authentication. - **Returns**: - A string: The content of the URL response, decoded using \'utf-8\'. - **Raises**: - `urllib.error.URLError`: If there is an issue with the URL or network. - `urllib.error.HTTPError`: If the server cannot fulfill the request. 2. **Custom Opener**: - Create a custom URL opener that: - Includes HTTP basic authentication using the provided username and password. - Handles HTTP 301 and 302 redirections automatically. - Sets a custom User-Agent header. 3. **Handle Response**: - Open the URL using the custom opener. - Process the response and return the content as a UTF-8 string. - Raise appropriate exceptions for URL or HTTP errors. Example ```python try: content = fetch_url_with_auth(\'http://example.com/protected\', \'user\', \'pass\') print(content) except urllib.error.URLError as e: print(f\'Failed to fetch URL: {e}\') except urllib.error.HTTPError as e: print(f\'HTTP error occurred: {e}\') ``` Constraints - Use the `urllib.request` module exclusively. - Do not use external libraries like `requests`. - Handle network errors and HTTP errors gracefully. Notes - Ensure proper handling of authentication and redirections. - Manage headers correctly to include User-Agent and other necessary headers. - The output should be the complete content of the response decoded using UTF-8.","solution":"import urllib.request import urllib.error def fetch_url_with_auth(url: str, username: str, password: str) -> str: Fetches a URL with HTTP basic authentication and handles redirections. Parameters: - url (str): The URL to open. - username (str): The username for basic authentication. - password (str): The password for basic authentication. Returns: - str: The content of the URL response, decoded using \'utf-8\'. Raises: - urllib.error.URLError: If there is an issue with the URL or network. - urllib.error.HTTPError: If the server cannot fulfill the request. # Create a password manager password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) # Create an opener with the password manager handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler, urllib.request.HTTPRedirectHandler()) # Set a custom User-Agent header opener.addheaders = [(\'User-Agent\', \'Mozilla/5.0\')] # Install the opener globally so it can be used with urlopen urllib.request.install_opener(opener) try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: raise e except urllib.error.HTTPError as e: raise e"},{"question":"Objective: Create a Python script that processes a directory of text files to extract certain information, modify the text, and then compress the modified text files into a compressed archive. The script should be designed to be run from the command line with the following requirements. Requirements: 1. **Command-Line Arguments:** - The script should accept the following command-line arguments: - `--input-dir`: The directory containing the text files. - `--output-dir`: The directory where the modified text files and compressed archive should be saved. - `--search-pattern`: A regular expression pattern to search for in the text files. - `--replace-string`: A string to replace the matched patterns in the text files. - `--archive-name`: The name of the compressed archive file (without the extension). 2. **File Processing:** - Traverse the specified input directory to find all text files matching the pattern `*.txt`. - For each text file, search for all occurrences of the specified pattern and replace them with the provided replacement string. - Save the modified text files in the specified output directory, maintaining the original file names. 3. **Archiving:** - Compress all modified text files in the output directory into a single compressed archive (using `zlib`). 4. **Output:** - Print a summary of the following: - Number of files processed. - Total number of replacements made. - Size of the compressed archive. Input and Output Formats: - Input: Command-line arguments. - Output: Printed summary as mentioned above. Constraints: - Assume all input text files are reasonably small such that they fit in memory. - The script should handle incorrect or missing arguments gracefully by displaying an appropriate help message. # Example Command: ```bash python process_files.py --input-dir /path/to/input --output-dir /path/to/output --search-pattern \\"bfoob\\" --replace-string \\"bar\\" --archive-name modified_texts ``` # Example Script Structure: ```python import os import re import shutil import zlib import argparse from datetime import datetime def parse_arguments(): parser = argparse.ArgumentParser(description=\\"Process and modify text files, then compress them.\\") parser.add_argument(\'--input-dir\', required=True, help=\'Directory containing the input text files.\') parser.add_argument(\'--output-dir\', required=True, help=\'Directory to save modified text files and compressed archive.\') parser.add_argument(\'--search-pattern\', required=True, help=\'Regular expression pattern to search for in the text files.\') parser.add_argument(\'--replace-string\', required=True, help=\'String to replace the matched patterns in the text files.\') parser.add_argument(\'--archive-name\', required=True, help=\'Name of the compressed archive (without extension).\') return parser.parse_args() def process_files(args): # Implement file processing, modification, and archiving here. pass def main(): args = parse_arguments() process_files(args) if __name__ == \'__main__\': main() ``` Complete the `process_files` function to achieve the specified requirements.","solution":"import os import re import shutil import zlib import argparse from datetime import datetime def parse_arguments(): parser = argparse.ArgumentParser(description=\\"Process and modify text files, then compress them.\\") parser.add_argument(\'--input-dir\', required=True, help=\'Directory containing the input text files.\') parser.add_argument(\'--output-dir\', required=True, help=\'Directory to save modified text files and compressed archive.\') parser.add_argument(\'--search-pattern\', required=True, help=\'Regular expression pattern to search for in the text files.\') parser.add_argument(\'--replace-string\', required=True, help=\'String to replace the matched patterns in the text files.\') parser.add_argument(\'--archive-name\', required=True, help=\'Name of the compressed archive (without extension).\') return parser.parse_args() def process_files(args): input_dir = args.input_dir output_dir = args.output_dir search_pattern = args.search_pattern replace_string = args.replace_string archive_name = args.archive_name if not os.path.exists(output_dir): os.makedirs(output_dir) pattern = re.compile(search_pattern) total_files_processed = 0 total_replacements = 0 for root, _, files in os.walk(input_dir): for file in files: if file.endswith(\\".txt\\"): total_files_processed += 1 input_file_path = os.path.join(root, file) output_file_path = os.path.join(output_dir, file) with open(input_file_path, \'r\') as f: content = f.read() modified_content, num_replacements = pattern.subn(replace_string, content) total_replacements += num_replacements with open(output_file_path, \'w\') as f: f.write(modified_content) archive_path = os.path.join(output_dir, archive_name + \'.zip\') shutil.make_archive(archive_path.replace(\'.zip\', \'\'), \'zip\', output_dir) archive_size = os.path.getsize(archive_path) print(f\\"Number of files processed: {total_files_processed}\\") print(f\\"Total number of replacements made: {total_replacements}\\") print(f\\"Size of the compressed archive: {archive_size} bytes\\") def main(): args = parse_arguments() process_files(args) if __name__ == \'__main__\': main()"},{"question":"You are provided with two types of iterable data sources: a sequence and a callable function. Your task is to implement a function `combine_iterators` that takes a sequence and a callable function with its sentinel value and returns a list that combines the elements from both iterators in the order they are produced. Function Signature ```python def combine_iterators(sequence: List[Any], callable_func: Callable[[], Any], sentinel: Any) -> List[Any]: pass ``` Parameters - **sequence**: A list of elements (sequence) that you need to iterate over. - **callable_func**: A callable function that produces values until it returns the sentinel value. - **sentinel**: A value that signals the callable iterator to stop producing values. Returns - A list containing all the elements produced by iterating over the sequence followed by the elements produced by repeatedly calling the callable function until the sentinel value is encountered. Constraints - The sequence can be of any length, including empty. - The callable function will terminate by returning the sentinel value eventually. Example ```python def example_callable(): for i in range(5, 10): yield i yield \'end\' sequence = [1, 2, 3, 4] func = example_callable() sentinel = \'end\' # Output would be: [1, 2, 3, 4, 5, 6, 7, 8, 9] print(combine_iterators(sequence, func.__next__, sentinel)) ``` Notes - You need to design your solution such that it properly handles both types of iterators and respects the encapsulation and timeout of iterations.","solution":"from typing import List, Callable, Any def combine_iterators(sequence: List[Any], callable_func: Callable[[], Any], sentinel: Any) -> List[Any]: Combines elements from a sequence and a callable function until the sentinel value is encountered. Parameters: - sequence: A list of elements to iterate over. - callable_func: A callable function that produces values until it returns the sentinel value. - sentinel: A value that signals the callable iterator to stop producing values. Returns: - A combined list of elements from the sequence and the callable function. result = [] # Add all elements from the sequence to the result for item in sequence: result.append(item) # Call the callable function until the sentinel is returned while True: value = callable_func() if value == sentinel: break result.append(value) return result # Example Usage: def example_callable(): for i in range(5, 10): yield i yield \'end\' sequence = [1, 2, 3, 4] func = example_callable() sentinel = \'end\' # Output: [1, 2, 3, 4, 5, 6, 7, 8, 9] print(combine_iterators(sequence, func.__next__, sentinel))"},{"question":"Coding Assessment Question # Combining Time Manipulations and Performance Measurements Using the time Module You are required to design a Python function called `measure_execution_time` that does the following: 1. Takes a function `func` that will be executed and a tuple `args` representing the arguments for the function. 2. Measures the wall-clock time the function takes to execute using a system-wide clock. 3. Measures the CPU time the function takes to execute using a high-resolution per-process timer. 4. Returns a dictionary with the execution times in both seconds and human-readable format (YYYY-MM-DD HH:MM:SS). # Specifications Function Signature ```python def measure_execution_time(func: callable, args: tuple) -> dict: pass ``` Input - `func`: A callable (function) to be executed. - `args`: A tuple containing the arguments for `func`. Output - A dictionary with the following structure: ```python { \\"wall_clock_time_seconds\\": float, \\"cpu_time_seconds\\": float, \\"wall_clock_time_human_readable\\": str, \\"cpu_time_human_readable\\": str } ``` Constraints - The function you measure may run for a variable amount of time. - Use `time.time()` or `time.monotonic()` for system-wide wall-clock time. - Use `time.process_time()` for CPU time. - Human-readable time format should be `YYYY-MM-DD HH:MM:SS`. Example ```python import time def example_function(n): time.sleep(n) # simulate a function that runs for \'n\' seconds return n # Example usage: result = measure_execution_time(example_function, (5,)) # Expected Output: # result = { # \\"wall_clock_time_seconds\\": 5.0 (or close to it depending on execution), # \\"cpu_time_seconds\\": close to 0.0 (since the function spends most of its time sleeping), # \\"wall_clock_time_human_readable\\": \\"1970-01-01 00:00:05\\" (example if 5 seconds elapsed from the epoch), # \\"cpu_time_human_readable\\": \\"1970-01-01 00:00:00\\" # } ``` # Notes - Ensure you handle edge cases where the function execution may fail. In such cases, the times should be recorded up until the point of failure. - Use `time.strftime()` for converting times to human-readable format.","solution":"import time def measure_execution_time(func: callable, args: tuple) -> dict: Measures the wall-clock and CPU time of a function\'s execution. # Record start times start_wall_clock = time.monotonic() start_cpu_time = time.process_time() # Execute the function result = func(*args) # Record end times end_wall_clock = time.monotonic() end_cpu_time = time.process_time() # Calculate elapsed times wall_clock_time_seconds = end_wall_clock - start_wall_clock cpu_time_seconds = end_cpu_time - start_cpu_time # Generate human-readable formats wall_clock_time_human_readable = time.strftime( \\"%Y-%m-%d %H:%M:%S\\", time.gmtime(wall_clock_time_seconds) ) cpu_time_human_readable = time.strftime( \\"%Y-%m-%d %H:%M:%S\\", time.gmtime(cpu_time_seconds) ) return { \\"wall_clock_time_seconds\\": wall_clock_time_seconds, \\"cpu_time_seconds\\": cpu_time_seconds, \\"wall_clock_time_human_readable\\": wall_clock_time_human_readable, \\"cpu_time_human_readable\\": cpu_time_human_readable }"},{"question":"You are tasked with creating a secure XML-RPC server using the `xmlrpc.server` module. The server should be able to handle a variety of remote method calls, secure access to method names, and log requests. Specifically, your server should comply with the following requirements: 1. **Server Setup**: - Create an XML-RPC server that listens on `localhost` at port `8080`. - Restrict the server to only allow RPC calls to the path `/RPC2`. 2. **Function Registration**: - Register a built-in function `pow()` to the server under the method name `math_power`. - Register a custom function `concat_strings` that takes two strings and concatenates them. - Register a class `MathOperations` which contains methods `add` and `multiply` for adding and multiplying two numbers, respectively. Ensure that these methods are callable as `math.add` and `math.multiply`. 3. **Enhanced Features**: - Implement introspection functions (`system.listMethods`, `system.methodHelp`, `system.methodSignature`). - Implement multicall functionality enabling clients to make multiple calls in a single request. - Restrict access to global variables and arbitrary code execution by avoiding the use of `allow_dotted_names=True`. 4. **Request Logging**: - Ensure that all incoming requests are logged. # Example Usage The following demonstrates how the client should interact with your XML-RPC server: ```python import xmlrpc.client # Create a client proxy proxy = xmlrpc.client.ServerProxy(\\"http://localhost:8080/RPC2\\") # Make some method calls to the server print(proxy.math_power(2, 3)) # Expecting 8 print(proxy.concat_strings(\\"Hello\\", \\"World\\")) # Expecting \\"HelloWorld\\" print(proxy.math.add(5, 7)) # Expecting 12 print(proxy.math.multiply(3, 4)) # Expecting 12 # List available methods print(proxy.system.listMethods()) print(proxy.system.methodHelp(\'concat_strings\')) # Use multicall multi = xmlrpc.client.MultiCall(proxy) multi.math_power(2, 3) multi.concat_strings(\\"Foo\\", \\"Bar\\") multi.math.add(1, 2) results = multi() for res in results: print(res) # Expecting 8, \\"FooBar\\", 3 ``` # Submission Guidelines 1. Implement your XML-RPC server following the above requirements. 2. Provide the complete source code for your server. 3. Include comments in your code to explain key segments. 4. Ensure your server handles requests securely and logs all incoming requests. **Tip:** Make sure to test your server with the provided client example.","solution":"from xmlrpc.server import SimpleXMLRPCServer, SimpleXMLRPCRequestHandler import logging # Define a custom request handler class RequestHandler(SimpleXMLRPCRequestHandler): rpc_paths = (\'/RPC2\',) # Configure logging logging.basicConfig(filename=\\"xmlrpc_server.log\\", level=logging.INFO) # Custom string concatenation function def concat_strings(s1, s2): return s1 + s2 # Math operations class class MathOperations: def add(self, a, b): return a + b def multiply(self, a, b): return a * b if __name__ == \\"__main__\\": # Create server server = SimpleXMLRPCServer((\\"localhost\\", 8080), requestHandler=RequestHandler, allow_none=True, logRequests=True) server.register_introspection_functions() server.register_multicall_functions() # Register functions and class methods server.register_function(pow, \'math_power\') server.register_function(concat_strings) server.register_instance(MathOperations(), allow_dotted_names=False) # Log incoming requests def log_request(handler): logging.info(\\"Received request from %s\\", handler.client_address) server.RequestHandlerClass.handle = lambda self: log_request(self) or SimpleXMLRPCRequestHandler.handle(self) # Run the server\'s main loop print(\\"Starting XML-RPC server on localhost:8080/RPC2...\\") server.serve_forever()"},{"question":"**PyTorch Optimization Challenge** You are provided with a scenario where you need to optimize a neural network training process using PyTorch. You are required to implement a function `optimize_for_gpu(input_data: torch.Tensor, network: torch.nn.Module) -> (torch.Tensor, bool)` that optimizes the input data and network operations to leverage the performance benefits of a V100 GPU. # Requirements: 1. **Input and Output**: * `input_data` (torch.Tensor): The input tensor that you want to feed into the network. The tensor can have any shape. * `network` (torch.nn.Module): The PyTorch model that will process the input data. * The function should return a tuple containing: * An optimized input tensor. * A boolean flag indicating whether the optimization conditions were met. 2. **Constraints and Limitations**: * Ensure the input data is transferred to the GPU. * The input data should be cast to torch.float16 dtype. * The function should raise an error if the specified GPU is not a V100. 3. **Performance Requirements**: * The solution should verify and ensure that cudnn is enabled. * The input data must not be in `PackedSequence` format. # Implementation Details: - The function should first check if the GPU is available and is specifically a V100 GPU. - Check if cudnn is enabled. - Transfer the input data to the GPU and cast it to torch.float16 dtype. - Return the modified input data along with a flag indicating whether all optimization conditions were met. # Example: ```python import torch import torch.nn as nn class SimpleNetwork(nn.Module): def __init__(self): super(SimpleNetwork, self).__init__() self.fc = nn.Linear(10, 5) def forward(self, x): return self.fc(x) input_data = torch.randn(64, 10) network = SimpleNetwork().cuda() # Ensure network is on GPU optimized_data, conditions_met = optimize_for_gpu(input_data, network) print(optimized_data.dtype) # Should be torch.float16 print(optimized_data.is_cuda) # Should be True print(conditions_met) # Should be True if V100 and other conditions are met ``` # Full Implementation: Ensure your solution meets all the specified requirements and passes the example test case.","solution":"import torch def optimize_for_gpu(input_data: torch.Tensor, network: torch.nn.Module) -> (torch.Tensor, bool): Optimizes the input_data and network for a V100 GPU. Args: - input_data (torch.Tensor): Input tensor to be optimized. - network (torch.nn.Module): PyTorch model that will process the input data. Returns: - (torch.Tensor, bool): A tuple containing the optimized input tensor and a boolean flag indicating whether optimization conditions were met. # Check if CUDA is available if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available.\\") # Check if the device name is V100 if torch.cuda.get_device_name(0) != \\"Tesla V100-SXM2-16GB\\": raise RuntimeError(\\"The GPU is not a V100.\\") # Check if cudnn is enabled if not torch.backends.cudnn.enabled: raise RuntimeError(\\"cudnn is not enabled.\\") # Ensure the input data is not in PackedSequence format if isinstance(input_data, torch.nn.utils.rnn.PackedSequence): raise RuntimeError(\\"Input data should not be in PackedSequence format.\\") # Transfer input data to GPU and cast to torch.float16 dtype optimized_input_data = input_data.to(torch.device(\\"cuda\\")).to(torch.float16) return optimized_input_data, True"},{"question":"Objective: Implement a function that: - Can allocate, perform operations on, and monitor memory usage of tensors on both CUDA and HIP devices within PyTorch. - Condition checks whether the underlying system has CUDA or HIP enabled and executes specific routines accordingly. - Transfers tensors between devices, performs operations, and clears memory cache. # Problem Statement: Context: You are given a task to implement a function to handle tensor operations and memory management in a PyTorch application that should work seamlessly on systems enabled with either CUDA or HIP. Function Signature: ```python def tensor_operations_and_memory_management(): pass ``` Requirements: 1. **Device Initialization:** - Check if GPU support is available. - Determine if CUDA or HIP backend is used. - Set the device (GPU 0). 2. **Tensor Allocation and Operations:** - Create tensors of shape `(1000, 1000)` filled with random numbers on the determined device. - Perform a simple GPU-based operation (e.g., element-wise addition of two tensors). 3. **Memory Monitoring and Management:** - Print the amount of memory currently allocated and reserved. - Clear the unused memory cache. 4. **Assertions:** - Ensure that tensors are allocated on the proper GPU devices. - Verify that memory has been properly allocated and cached. Expected Output: The function should print: 1. Confirmation of whether CUDA or HIP is being used. 2. Memory details post tensor allocation and operation. 3. Confirmation of memory cache release. Example: ```python # Expected output format (Values may vary based on actual execution) Using: CUDA Allocated memory: 8000000 bytes Reserved memory: 16000000 bytes Memory cache released. ``` Notes: - If no GPU support is available, the function should raise an appropriate exception. - You may use `torch.cuda.memory_allocated()` and `torch.cuda.memory_reserved()` to get the required memory details. - Ensure to handle tensors appropriately to avoid unnecessary memory usage.","solution":"import torch def tensor_operations_and_memory_management(): # Check if CUDA is available if torch.cuda.is_available(): using_gpu = True device_type = \'CUDA\' elif hasattr(torch, \'hip\') and torch.hip.is_available(): using_gpu = True device_type = \'HIP\' else: using_gpu = False device_type = \'None\' if not using_gpu: raise RuntimeError(\\"No CUDA or HIP compatible GPU found.\\") # Set the device device = torch.device(\\"cuda:0\\") # Print whether CUDA or HIP is being used print(f\\"Using: {device_type}\\") # Create tensors of shape (1000, 1000) with random numbers on the determined device tensor_a = torch.randn((1000, 1000), device=device) tensor_b = torch.randn((1000, 1000), device=device) # Perform an element-wise addition of two tensors tensor_c = tensor_a + tensor_b # Print the amount of memory currently allocated and reserved allocated_memory = torch.cuda.memory_allocated(device) reserved_memory = torch.cuda.memory_reserved(device) print(f\\"Allocated memory: {allocated_memory} bytes\\") print(f\\"Reserved memory: {reserved_memory} bytes\\") # Clear the unused memory cache torch.cuda.empty_cache() print(\\"Memory cache released.\\") return device_type, allocated_memory, reserved_memory"},{"question":"# Question: **Implement a Custom Import System and Error Handling** You are required to create a custom import system in Python that allows you to import scripts dynamically from a specified directory. Additionally, you will implement comprehensive error handling to manage possible issues that might arise during the import process. Requirements: 1. **Directory Structure:** - Assume a directory, `scripts`, containing Python scripts you might want to import and execute. 2. **Custom Import System:** - Implement a function `custom_import(script_name: str) -> module` that attempts to import a script given its name (without the `.py` extension) from the `scripts` directory. - Ensure that this import system uses only the provided directory and does not interfere with the global import system. 3. **Error Handling:** - Handle the following errors explicitly: - `FileNotFoundError`: Raised when the specified script does not exist in the `scripts` directory. - `ImportError`: Raised when the import fails for any reason other than the file being absent. - `SyntaxError`: Raised when the script has syntax errors. - For any of these errors, provide a meaningful error message to aid debugging. Input: - A string `script_name` representing the name of the script to import (without the `.py` extension). Output: - The corresponding module if the import is successful. - Print error messages for the aforementioned error types. Example Usage: ```python # Assuming a script named \'example_script.py\' exists in the \'scripts\' directory module = custom_import(\'example_script\') # If the script \'missing_script.py\' does not exist module = custom_import(\'missing_script\') # Should print a meaningful FileNotFoundError message ``` Constraints: - The function should only import scripts from the \'scripts\' directory. - You are not allowed to modify the global import system permanently. Implementation Notes: - Use `importlib` standard library for dynamically importing Python modules. - You might need to manipulate `sys.path` temporarily to achieve the custom import functionality. - Ensure the directory handling is platform-independent (use `os.path` for path manipulations). Write your `custom_import` function that adheres to the specifications mentioned above.","solution":"import importlib.util import os import sys def custom_import(script_name: str): Dynamically imports a script from the \'scripts\' directory. Args: - script_name (str): Name of the script to import (without .py extension). Returns: The corresponding module if the import is successful, otherwise prints an error message. scripts_dir = \'scripts\' script_path = os.path.join(scripts_dir, f\\"{script_name}.py\\") if not os.path.exists(script_path): print(f\\"FileNotFoundError: The script \'{script_name}.py\' does not exist in the \'{scripts_dir}\' directory.\\") return None try: spec = importlib.util.spec_from_file_location(script_name, script_path) if spec is None: raise ImportError(f\\"Could not load the specification for \'{script_name}\'.\\") module = importlib.util.module_from_spec(spec) sys.modules[script_name] = module spec.loader.exec_module(module) return module except FileNotFoundError as fnf_error: print(f\\"FileNotFoundError: {fnf_error}\\") except ImportError as imp_error: print(f\\"ImportError: {imp_error}\\") except SyntaxError as syn_error: print(f\\"SyntaxError: {syn_error}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") return None"},{"question":"**Iterator Implementation and Usage** Python provides two general-purpose iterator objects: 1. A sequence iterator that works with arbitrary sequences. 2. A callable iterator that works with a callable object and a sentinel. Your task is to implement two different custom iterators in Python and demonstrate their usage. Please follow the instructions carefully: # Task 1: Sequence Iterator Implement a custom sequence iterator that mimics the behavior of `PySeqIter_Type`. Your implementation should: 1. Work with any Python sequence (like lists, tuples, etc.). 2. Terminate iteration when an `IndexError` is raised during subscripting. Implementation Details: - Create a class `CustomSeqIter` that: - Initializes with a sequence. - Implements the `__iter__()` method to return the iterator object itself. - Implements the `__next__()` method to return the next item or raise `StopIteration` when no more items are available. # Task 2: Callable Iterator Implement a custom callable iterator that mimics the behavior of `PyCallIter_Type`. Your implementation should: 1. Work with a callable object and a sentinel value. 2. Terminate iteration when the callable returns a value equal to the sentinel. Implementation Details: - Create a class `CustomCallIter` that: - Initializes with a callable and a sentinel. - Implements the `__iter__()` method to return the iterator object itself. - Implements the `__next__()` method to return the next item from the callable or raise `StopIteration` when the sentinel value is returned. # Example Usage: ```python # Sequence Iterator Example class CustomSeqIter: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.sequence): raise StopIteration result = self.sequence[self.index] self.index += 1 return result # Callable Iterator Example class CustomCallIter: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result # Testing the Sequence Iterator seq_iter = CustomSeqIter([1, 2, 3, 4]) for item in seq_iter: print(item) # Testing the Callable Iterator def counter(): global n n += 1 return n n = 0 call_iter = CustomCallIter(counter, 5) for item in call_iter: print(item) ``` **Constraints:** - You may not use the built-in `iter` function for creating these iterators. - Your custom iterators should handle large sequences and repeated calls efficiently. **Note**: Your implementation will be tested with various sequences and callables to ensure it meets the described functionality.","solution":"class CustomSeqIter: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.sequence): raise StopIteration result = self.sequence[self.index] self.index += 1 return result class CustomCallIter: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result"},{"question":"**Objective**: Implement and demonstrate your understanding of sorting using Python\'s `sorted()` function with key parameters and multi-level sorting. Problem Statement You are given a list of dictionaries representing metadata about a collection of books. Each dictionary contains the following keys: `title` (string), `author` (string), `year` (integer), and `copies_sold` (integer). Write a function `sort_books` that sorts this list based on the following criteria: 1. Primary key: `author` (ascending). 2. Secondary key: `year` (ascending). 3. Tertiary key: `copies_sold` (descending). Your function should take in a list of dictionaries (books) and return a new list sorted according to the specified criteria. # Function signature ```python def sort_books(books: list[dict]) -> list[dict]: pass ``` # Input - `books`: A list of dictionaries, where each dictionary contains: - `title` (string): The title of the book. - `author` (string): The author of the book. - `year` (integer): The year the book was published. - `copies_sold` (integer): The number of copies sold. # Output - A new list of dictionaries, sorted by `author` in ascending order, then by `year` in ascending order, and finally by `copies_sold` in descending order. # Constraints - The input list will have at least 1 and at most 10,000 dictionaries. - The strings for `author` and `title` will have lengths between 1 and 100 characters. - The `year` will be between 1450 and 2023. - The `copies_sold` will be between 0 and 1,000,000. # Example ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 2005, \\"copies_sold\\": 5000}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 1995, \\"copies_sold\\": 7000}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author X\\", \\"year\\": 2010, \\"copies_sold\\": 4000}, {\\"title\\": \\"Book D\\", \\"author\\": \\"Author Z\\", \\"year\\": 2005, \\"copies_sold\\": 2000}, ] sorted_books = sort_books(books) print(sorted_books) ``` # Expected Output ```python [ {\'title\': \'Book A\', \'author\': \'Author X\', \'year\': 2005, \'copies_sold\': 5000}, {\'title\': \'Book C\', \'author\': \'Author X\', \'year\': 2010, \'copies_sold\': 4000}, {\'title\': \'Book B\', \'author\': \'Author Y\', \'year\': 1995, \'copies_sold\': 7000}, {\'title\': \'Book D\', \'author\': \'Author Z\', \'year\': 2005, \'copies_sold\': 2000} ] ``` Tips: - Use appropriate key functions to achieve the multi-level sorting. - Remember to handle the primary and secondary keys in ascending order and the tertiary key in descending order.","solution":"def sort_books(books: list[dict]) -> list[dict]: Sort a list of books based on author, year, and copies_sold. Author in ascending order, year in ascending order, copies_sold in descending order. sorted_books = sorted(books, key=lambda x: (x[\'author\'], x[\'year\'], -x[\'copies_sold\'])) return sorted_books"},{"question":"# Question: Implement a Web Browser Controller for Scheduled URL Openings **Objective**: Your task is to implement a function `schedule_browser_openings(schedule: List[Tuple[str, str]]) -> None` that processes a schedule of URLs to be opened at specific times. This function should use the `webbrowser` module to open URLs in active or new browser tabs/windows as scheduled. **Function Signature**: ```python def schedule_browser_openings(schedule: List[Tuple[str, str]]) -> None: pass ``` # Input: - `schedule`: A list of tuples, where each tuple contains: * A string (`url`): A valid URL to open. * A string (`time`): A time in the `HH:MM` 24-hour format (e.g., \\"14:30\\" for 2:30 PM), specifying when the URL should be opened. # Constraints: 1. Time should be validated to ensure it is in the proper `HH:MM` format. 2. URLs must be valid (they should start with \\"http://\\" or \\"https://\\"). 3. Assume that the current day is used for scheduling regardless of the date. 4. Opening actions should not block each other; if multiple URLs are scheduled to open at the same time, they should open simultaneously in different tabs. 5. Use Python\'s built-in `webbrowser` module to manage the URL opening. # Output: - The function does not return anything. - The function should open the specified URLs at the correct times using the default web browser. # Example Usage: ```python # Define the schedule: Open Google at 15:00 and Python at 15:05 schedule = [ (\\"https://www.google.com\\", \\"15:00\\"), (\\"https://www.python.org\\", \\"15:05\\") ] # Call the function schedule_browser_openings(schedule) ``` # Notes: - You may use additional helper functions and libraries such as `time`, `datetime`, and `threading` to implement the scheduling mechanism. - Ensure the code is efficient and handles potential runtime errors gracefully. # Bonus: - Implement logging to inform when each URL is opened. By solving this problem, you will demonstrate your ability to integrate the `webbrowser` module with scheduling and timing logic, underscoring an understanding of both fundamental and advanced Python programming concepts.","solution":"import webbrowser import datetime import time import threading import re from typing import List, Tuple def is_valid_time(time_str: str) -> bool: return bool(re.match(r\'^([01]d|2[0-3]):([0-5]d)\', time_str)) def is_valid_url(url: str) -> bool: return url.startswith(\\"http://\\") or url.startswith(\\"https://\\") def open_url(url: str): webbrowser.open(url) def schedule_browser_openings(schedule: List[Tuple[str, str]]) -> None: for url, time_str in schedule: if not is_valid_time(time_str): print(f\\"Invalid time format: {time_str}\\") continue if not is_valid_url(url): print(f\\"Invalid URL: {url}\\") continue now = datetime.datetime.now() target_time = datetime.datetime.strptime(time_str, \\"%H:%M\\").replace(year=now.year, month=now.month, day=now.day) if now > target_time: print(f\\"Scheduled time for {url} has already passed.\\") continue delay = (target_time - now).total_seconds() threading.Timer(delay, open_url, args=[url]).start()"},{"question":"You are provided with the \'diamonds\' dataset from seaborn. Your task is to create a comprehensive visualization that includes multiple facets and aggregations to deliver deeper insights. Follow the requirements below: 1. **Load the diamonds dataset** using seaborn\'s `load_dataset` function. 2. **Create a seaborn plot** using `so.Plot` that visualizes the relationship between the clarity of the diamonds (`clarity`) and their weight (`carat`). 3. **Apply different aggregations**: - Display the mean carat weight for each clarity category. - Add another layer to show the median carat weight for each clarity category. - Add a third layer to depict the interquartile range (Q3 - Q1) of the carat weights for each clarity category. 4. **Use additional visual mappings and transformations**: - Map the `cut` of diamonds to the color aesthetic. - Use `Dodge` to ensure different cuts are visually separated within each clarity category. 5. **Customize the plot** to ensure it is clear and informative: - Add appropriate axis labels and a title. - Ensure the plot legends and colors are intuitively mapped. Function Signature ```python def visualize_diamonds(): # Implementation here ``` Expected Output - A multi-layered plot that shows the mean, median, and interquartile range of diamoned carat weights categorized by their clarity. - Proper visual separation (using Dodge for the cuts) - Appropriately labeled axes and title. Constraints and Requirements - Use the seaborn `so.Plot` method. - The plot should include multiple facets for clear representation. - The solution should be efficient and avoid redundant computations. Use the following dataset code to load the diamonds dataset: ```python import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") ``` Performance The plot generated should be efficient with a time complexity of at most O(n) per aggregation function applied, where n is the number of diamonds in the dataset. Example Here is a high-level structure of your function: ```python def visualize_diamonds(): import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot p = so.Plot(diamonds, \\"clarity\\", \\"carat\\") # Add layers with different aggregations p.add(so.Bar(), so.Agg(\\"mean\\"), color=\\"cut\\") p.add(so.Bar(), so.Agg(\\"median\\"), color=\\"cut\\", dodge=True) p.add(so.Bar(), so.Agg(lambda x: x.quantile(.75) - x.quantile(.25)), color=\\"cut\\", dodge=True) # Customize plot p.label(x=\\"Clarity\\", y=\\"Carat Weight\\", title=\\"Diamond Carat Weight by Clarity and Cut\\") # Show the plot p.show() ``` Your task is to implement the `visualize_diamonds` function that creates the described plot.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def visualize_diamonds(): diamonds = load_dataset(\\"diamonds\\") # Create the initial plot object p = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\", color=\\"cut\\") # Add layer for mean carat weight p.add(so.Bar(), so.Agg(\\"mean\\"), legend=\\"Mean Carat\\") # Add layer for median carat weight with dodge for separation p.add(so.Bar(), so.Agg(\\"median\\"), dodge=True, legend=\\"Median Carat\\") # Add layer for interquartile range (Q3 - Q1) with dodge for separation p.add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), dodge=True, legend=\\"IQR Carat\\") # Customize the plot p.scale(color=\\"cut\\") p.label(x=\\"Clarity\\", y=\\"Carat Weight\\", title=\\"Diamond Carat Weight by Clarity and Cut\\") p.show() # Example usage: # visualize_diamonds()"},{"question":"Problem Statement You are required to implement a Python class `StudentDatabase` that uses the `shelve` module to store student records persistently. Each student record should include the following fields: - `id` (string): A unique identifier for the student. - `name` (string): The name of the student. - `grades` (list of floats): A list of the student\'s grades. Your implementation should be able to: 1. Add a new student record. 2. Update the grades of an existing student. 3. Retrieve information for a student by their ID. 4. Get the average grade for a specific student. 5. Delete a student record by their ID. # Input and Output Formats Methods 1. `add_student(id: str, name: str, grades: List[float]) -> None` - Adds a student with the provided `id`, `name`, and `grades` to the database. - Raises a `ValueError` if a student with the same `id` already exists. 2. `update_grades(id: str, new_grades: List[float]) -> None` - Updates the grades for the student with the specified `id`. - Raises a `KeyError` if no student with the provided `id` exists. 3. `get_student(id: str) -> Dict[str, Any]` - Retrieves the student record with the specified `id`. - Returns a dictionary containing `id`, `name`, and `grades` of the student. - Raises a `KeyError` if no student with the provided `id` exists. 4. `get_average_grade(id: str) -> float` - Calculates and returns the average grade of the student with the specified `id`. - Raises a `KeyError` if no student with the provided `id` exists. 5. `delete_student(id: str) -> None` - Deletes the student record with the specified `id` from the database. - Raises a `KeyError` if no student with the provided `id` exists. # Example ```python db = StudentDatabase(\'students.db\') # Add students db.add_student(\'001\', \'Alice\', [85.0, 92.5, 78.0]) db.add_student(\'002\', \'Bob\', [90.0, 88.0]) # Update grades db.update_grades(\'001\', [85.0, 92.5, 78.0, 95.0]) # Retrieve student information print(db.get_student(\'001\')) # {\'id\': \'001\', \'name\': \'Alice\', \'grades\': [85.0, 92.5, 78.0, 95.0]} # Get average grade print(db.get_average_grade(\'001\')) # 87.625 # Delete a student db.delete_student(\'002\') # Attempt to get deleted student db.get_student(\'002\') # Raises KeyError ``` # Constraints - The `id` field is guaranteed to be unique. - Grades are floats between 0.0 to 100.0 inclusive. # Additional Information Ensure that all database changes are properly synchronized and the underlying shelf is closed appropriately when no longer needed.","solution":"import shelve from typing import List, Dict, Any class StudentDatabase: def __init__(self, db_name: str): self._db_name = db_name def add_student(self, id: str, name: str, grades: List[float]) -> None: with shelve.open(self._db_name) as db: if id in db: raise ValueError(f\\"Student with id {id} already exists.\\") db[id] = {\'name\': name, \'grades\': grades} def update_grades(self, id: str, new_grades: List[float]) -> None: with shelve.open(self._db_name) as db: if id not in db: raise KeyError(f\\"No student found with id {id}.\\") student = db[id] student[\'grades\'] = new_grades db[id] = student def get_student(self, id: str) -> Dict[str, Any]: with shelve.open(self._db_name) as db: if id not in db: raise KeyError(f\\"No student found with id {id}.\\") return db[id] def get_average_grade(self, id: str) -> float: with shelve.open(self._db_name) as db: if id not in db: raise KeyError(f\\"No student found with id {id}.\\") grades = db[id][\'grades\'] return sum(grades) / len(grades) if grades else 0.0 def delete_student(self, id: str) -> None: with shelve.open(self._db_name) as db: if id not in db: raise KeyError(f\\"No student found with id {id}.\\") del db[id]"},{"question":"**Advanced Python Code Compilation and Evaluation** Your task is to implement a function named `custom_compile_and_run` that takes a list of strings, where each string represents a line of Python code. The function should compile and run these lines of code in sequence, while taking into consideration any `__future__` statements that affect future compilations. The function should return a list of results corresponding to the evaluation of each line of code that is an expression. # Function Signature ```python def custom_compile_and_run(code_lines: list[str]) -> list: ``` # Input - `code_lines`: A list of strings, each string being a line of Python code. # Output - A list of results of evaluating each line of code that is an expression. If a line is a statement, it should not be included in the output results. # Constraints - You can assume that all lines of code are syntactically valid Python code. - Performance constraints are not strict for this task. - The code lines may include `__future__` imports that should affect subsequent code lines appropriately. # Example ```python code_lines = [ \\"import math\\", \\"x = math.sqrt(16)\\", \\"x\\", \\"from __future__ import print_function\\", \\"print(\'Hello, World!\')\\", \\"a = 10\\", \\"a + 5\\" ] result = custom_compile_and_run(code_lines) print(result) # Expected output: [4.0, None, 15] ``` # Explanation In the given `code_lines` list: - `import math` is a statement and does not produce an output. - `x = math.sqrt(16)` is a statement and does not produce an output. - `x` is an expression and evaluates to `4.0`. - `from __future__ import print_function` is a statement and affects the `print` function in subsequent lines. - `print(\'Hello, World!\')` is a statement and does not produce an output (output is printed to console, which should return `None`). - `a = 10` is a statement and does not produce an output. - `a + 5` is an expression and evaluates to `15`. Note that for each compiled line of code, you should handle both statements and expressions, capturing the results of the latter and including them in the output list.","solution":"def custom_compile_and_run(code_lines: list[str]) -> list: Compiles and runs a list of Python code lines. Returns a list of results for each expression line. global_namespace = {} results = [] for line in code_lines: try: # Try to compile the line as an expression compiled_code = compile(line, \'<string>\', \'eval\') result = eval(compiled_code, global_namespace) results.append(result) except SyntaxError: # If it fails, it is a statement compiled_code = compile(line, \'<string>\', \'exec\') exec(compiled_code, global_namespace) return results"},{"question":"Objective You are required to demonstrate your understanding of the `ensurepip` module in Python by writing a script that uses its programmatic API to ensure the `pip` installer is set up according to specific conditions. Problem Statement Write a Python function `setup_pip(root_dir=None, user_mode=False, upgrade_existing=False, verbose_level=0, alt_install=False, default_pip=True)` that uses the `ensurepip` module to bootstrap `pip` into the current Python environment. The function should conform to the following specifications: 1. **Function Signature**: ```python def setup_pip(root_dir=None, user_mode=False, upgrade_existing=False, verbose_level=0, alt_install=False, default_pip=True): pass ``` 2. **Parameters**: - `root_dir` (str or None): The root directory to install `pip` relative to. If `None`, use the default install location. - `user_mode` (bool): If `True`, install using the user scheme rather than globally for the current Python installation. - `upgrade_existing` (bool): If `True`, upgrade an existing `pip` installation to the latest available version. - `verbose_level` (int): Controls the level of output verbosity (from 0 being silent to higher values being more verbose). - `alt_install` (bool): If `True`, the `pipX` script will not be installed. - `default_pip` (bool): If `True`, the `pip` script will be installed in addition to the standard `pipX` and `pipX.Y` scripts. 3. **Functionality**: - Use `ensurepip.bootstrap()` to bootstrap `pip` with the specified options. - Handle the following conditions: - If both `alt_install` and `default_pip` are `True`, raise a `ValueError` with the message \\"Cannot use both alt_install and default_pip options simultaneously.\\" 4. **Returns**: - `None` 5. **Constraints**: - This function does not return any value but should ensure that `pip` is installed as per the specified conditions. - You must use the `ensurepip` module exclusively. 6. **Example**: ```python # Example usage setup_pip(root_dir=\'/my/custom/path\', user_mode=False, upgrade_existing=True, verbose_level=1, alt_install=False, default_pip=True) ``` Additional Notes - The usage of this function should not modify `sys.path` or `os.environ` in a way that affects the current execution environment outside the context of bootstrapping `pip`. - Test your function to ensure it handles different combinations of the provided arguments correctly.","solution":"import ensurepip def setup_pip(root_dir=None, user_mode=False, upgrade_existing=False, verbose_level=0, alt_install=False, default_pip=True): Bootstraps pip into the current Python environment with specified options. :param root_dir: The root directory to install pip relative to. If None, use the default install location. :param user_mode: If True, install using the user scheme rather than globally. :param upgrade_existing: If True, upgrade an existing pip installation to the latest version. :param verbose_level: Controls the level of output verbosity. :param alt_install: If True, pipX script will not be installed. :param default_pip: If True, the pip script will be installed in addition to the pipX and pipX.Y scripts. :raises: ValueError if both alt_install and default_pip are True. :returns: None if alt_install and default_pip: raise ValueError(\\"Cannot use both alt_install and default_pip options simultaneously.\\") ensurepip.bootstrap( root=root_dir, upgrade=upgrade_existing, user=user_mode, verbosity=verbose_level, altinstall=alt_install, default_pip=default_pip )"},{"question":"**Problem Statement: Advanced Custom Logging Handler** You are tasked with implementing a custom logging handler by combining features from `RotatingFileHandler` and `StreamHandler`. The custom handler should log messages to a file and output them to the console. Additionally, it should implement a log rotation mechanism where the logs are rotated based on both size and time constraints. **Requirements:** 1. **Custom Logging Handler (`CustomRotatingStreamHandler`):** - Inherit from `logging.handlers.RotatingFileHandler` and `logging.StreamHandler`. - The log file should rotate if it exceeds a certain size (e.g., `maxBytes`) or at a certain time interval (e.g., `interval` and `when`). - Log messages should be printed to the console (standard output) as well as be logged to the file. 2. **Class Constructor:** - The constructor should accept the following parameters: ```python def __init__(self, filename, maxBytes, backupCount, interval, when, encoding=None, delay=False): ``` - `filename` (str): Name of the log file. - `maxBytes` (int): Maximum file size in bytes before rotating. - `backupCount` (int): Number of backup files to keep. - `interval` (int): Time interval at which log rotation should occur. - `when` (str): Type of interval (e.g., \'S\' for seconds, \'M\' for minutes, etc.). - Optional parameters `encoding`, `delay`. 3. **Method Overrides:** - Override the `emit` method to handle logging to both the file (with rotation) and the console. - Ensure proper formatting of log messages. **Input:** - Filename of the log file. - Maximum size in bytes for log rotation. - Number of backup files to retain. - Time interval and type for time-based log rotation. - Optional encoding and delay parameters. **Output:** - Log messages should be both written to the specified file with rotation and printed to the console. **Example Usage:** ```python import logging from custom_logging import CustomRotatingStreamHandler # Create a logger object logger = logging.getLogger(__name__) logger.setLevel(logging.INFO) # Create an instance of the custom handler handler = CustomRotatingStreamHandler( filename=\'app.log\', maxBytes=10000, backupCount=5, interval=1, when=\'M\' ) # Set a log format formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) # Add the handler to the logger logger.addHandler(handler) # Log some messages for i in range(100): logger.info(f\'Log message {i}\') ``` **Constraints:** - Ensure thread safety in handling log rotation and writing. - Ensure efficient performance for logging high-frequency messages. - You can use additional helper methods as needed. Implement the `CustomRotatingStreamHandler` class as specified above.","solution":"import logging from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler class CustomRotatingStreamHandler(logging.Handler): def __init__(self, filename, maxBytes, backupCount, interval, when, encoding=None, delay=False): logging.Handler.__init__(self) self.filename = filename self.maxBytes = maxBytes self.backupCount = backupCount self.interval = interval self.when = when self.encoding = encoding self.delay = delay self.file_handler = RotatingFileHandler( filename=self.filename, maxBytes=self.maxBytes, backupCount=self.backupCount, encoding=self.encoding, delay=self.delay ) self.time_handler = TimedRotatingFileHandler( filename=self.filename, when=self.when, interval=self.interval, backupCount=self.backupCount, encoding=self.encoding, delay=self.delay ) self.console_handler = logging.StreamHandler() def setFormatter(self, fmt): self.file_handler.setFormatter(fmt) self.time_handler.setFormatter(fmt) self.console_handler.setFormatter(fmt) def emit(self, record): self.file_handler.emit(record) self.time_handler.emit(record) self.console_handler.emit(record)"},{"question":"**Objective:** To test your understanding of pandas GroupBy operations, including aggregation, transformation, and filtering. **Problem Statement:** You are given a DataFrame representing sales data for an online retail store. Each row corresponds to an individual sale which includes information about the `date of sale`, `product category`, `product ID`, `customer ID`, `region`, and `sale amount`. Here\'s a sample of the DataFrame: ```python import pandas as pd import numpy as np data = { \'date\': pd.date_range(start=\'2022-01-01\', periods=100, freq=\'D\').tolist() * 5, \'category\': np.random.choice([\'Electronics\', \'Clothing\', \'Food\', \'Books\'], 500, replace=True), \'product_id\': np.random.randint(100, 200, 500), \'customer_id\': np.random.randint(1000, 2000, 500), \'region\': np.random.choice([\'North\', \'South\', \'East\', \'West\'], 500, replace=True), \'sale_amount\': np.random.uniform(20.0, 500.0, 500), } df = pd.DataFrame(data) ``` **Tasks:** 1. **Group Aggregation:** - Group the data by `category` and compute the total `sale_amount`, mean `sale_amount`, and count the number of sales per category. - Return a DataFrame with columns `total_sales`, `average_sales`, and `num_sales`. **Expected Output:** ``` total_sales average_sales num_sales category Books 12345.67 237.67 52 Clothing 9876.54 189.17 52 Electronics 15678.90 313.58 50 Food 11223.45 221.91 51 ``` 2. **Date Transformation:** - Add a new column `day_of_week` to the DataFrame representing the day of the week for each `date`. - Group by `day_of_week` and compute the cumulative sum of `sale_amount` within each group. **Expected Output:** ``` date category product_id customer_id region sale_amount day_of_week cum_sale_amount 0 2022-01-01 Books 101 1111 North 123.45 Saturday 123.45 1 2022-01-02 Food 102 1112 South 231.67 Sunday 231.67 ... ``` 3. **Filtering Groups:** - Filter out categories where the total sales are less than the overall mean sales across all categories. - Return a DataFrame with only the categories meeting this criterion. **Expected Output:** ``` date category product_id customer_id region sale_amount day_of_week 0 2022-01-01 Books 101 1111 North 123.45 Saturday 1 2022-01-02 Electronics 102 1112 South 231.67 Sunday ... ``` **Constraints:** - Ensure efficient handling of operations using appropriate pandas functions. - Assume no missing `sale_amount` values. - The DataFrame has at least 500 rows. **Implementation:** Write a function `analyze_sales_data(df: pd.DataFrame) -> tuple:` that implements the tasks above and returns the required outputs. ```python def analyze_sales_data(df: pd.DataFrame) -> tuple: # Task 1: Group Aggregation category_group = df.groupby(\'category\')[\'sale_amount\'] total_sales = category_group.sum().rename(\'total_sales\') average_sales = category_group.mean().rename(\'average_sales\') num_sales = category_group.size().rename(\'num_sales\') group_aggregation = pd.DataFrame([total_sales, average_sales, num_sales]).T # Task 2: Date Transformation df[\'day_of_week\'] = df[\'date\'].dt.day_name() df[\'cum_sale_amount\'] = df.groupby(\'day_of_week\')[\'sale_amount\'].cumsum() # Task 3: Filtering Groups overall_mean_sales = total_sales.mean() valid_categories = total_sales[total_sales >= overall_mean_sales].index filtered_df = df[df[\'category\'].isin(valid_categories)] return group_aggregation, df, filtered_df # Testing the function with provided DataFrame group_aggregation, transformed_df, filtered_df = analyze_sales_data(df) # Checking outputs print(group_aggregation) print(transformed_df.head()) print(filtered_df.head()) ``` **Good luck!**","solution":"import pandas as pd import numpy as np def analyze_sales_data(df: pd.DataFrame) -> tuple: # Task 1: Group Aggregation category_group = df.groupby(\'category\')[\'sale_amount\'] total_sales = category_group.sum().rename(\'total_sales\') average_sales = category_group.mean().rename(\'average_sales\') num_sales = category_group.size().rename(\'num_sales\') group_aggregation = pd.DataFrame({ \'total_sales\': total_sales, \'average_sales\': average_sales, \'num_sales\': num_sales }).reset_index() # Task 2: Date Transformation df[\'day_of_week\'] = df[\'date\'].dt.day_name() df[\'cum_sale_amount\'] = df.groupby(\'day_of_week\')[\'sale_amount\'].cumsum() # Task 3: Filtering Groups overall_mean_sales = total_sales.mean() valid_categories = total_sales[total_sales >= overall_mean_sales].index filtered_df = df[df[\'category\'].isin(valid_categories)] return group_aggregation, df, filtered_df"},{"question":"Objective: You are required to demonstrate your understanding of seaborn\'s advanced plotting capabilities using the `seaborn.objects.Plot` class. This exercise will test your ability to create specialized plots and customize them using matplotlib functionality. Problem Statement: Using the seaborn `diamonds` dataset, which comes with the seaborn library, write a function `create_custom_plot` that generates a customized multi-panel plot demonstrating the relationship between the `carat` and `price` columns, along with a histogram distribution of the `price` across different types of `cut`. The plot should satisfy the following requirements: 1. The left panel should be a scatter plot of `carat` versus `price`. 2. The right panel should be a set of histogram subplots (one for each cut). 3. The scatter plot should have a custom annotation saying \\"Scatter Plot of Carat vs Price\\". 4. Use different colors for each type of histogram. 5. The `price` axis of the histogram should be on a logarithmic scale. 6. Provide appropriate titles and axis labels. Input: - None (The diamonds dataset is internally provided by seaborn). Output: - A multi-panel plot drawn using seaborn and matplotlib which fulfills the above requirements. Constraints: - You can use only seaborn and matplotlib libraries for this task. Function Signature: ```python def create_custom_plot(): # Your code here ``` # Example: The function call `create_custom_plot()` should display a customized multi-panel plot as described. Sample Code to Load Dataset: ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') ``` # Notes: - Make sure to add annotations and titles in the correct order to avoid rendering issues. - Your plot should be embedded in a matplotlib figure to allow for matplotlib customizations. - The function should not return any value but should display the plot when called.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Create a figure with two subplots fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)) # Scatter plot of carat vs price on the left panel sns.scatterplot(ax=ax1, x=\'carat\', y=\'price\', data=diamonds) ax1.set_title(\'Scatter Plot of Carat vs Price\') ax1.set_xlabel(\'Carat\') ax1.set_ylabel(\'Price\') # Histogram distribution of price across different types of cut on the right panel cuts = diamonds[\'cut\'].unique() colors = sns.color_palette(\'hsv\', len(cuts)) for cut, color in zip(cuts, colors): subset = diamonds[diamonds[\'cut\'] == cut] sns.histplot(subset[\'price\'], ax=ax2, color=color, label=cut, log_scale=(False, True)) ax2.set_title(\'Histogram of Price for Each Cut\') ax2.set_xlabel(\'Price\') ax2.set_ylabel(\'Frequency (Log Scale)\') ax2.legend(title=\'Cut\') # Show the plot plt.tight_layout() plt.show()"},{"question":"# Question **Title:** Analyzing Sales Data with MultiIndex **Objective:** You are given a dataset representing sales data for a retail company. The dataset includes information about the year, quarter, product category, product sub-category, and sales figures. Your task is to perform various data manipulation operations using pandas\' MultiIndex and advanced indexing techniques. **Dataset:** ```python import pandas as pd import numpy as np # Sample data data = { \'Year\': [2021, 2021, 2021, 2021, 2022, 2022, 2022, 2022], \'Quarter\': [\'Q1\', \'Q1\', \'Q2\', \'Q2\', \'Q1\', \'Q1\', \'Q2\', \'Q2\'], \'Category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\'], \'Sub-Category\': [\'Phones\', \'Shirts\', \'Tablets\', \'Pants\', \'Phones\', \'Shirts\', \'Tablets\', \'Pants\'], \'Sales\': [50000, 30000, 45000, 35000, 52000, 32000, 47000, 36000] } df = pd.DataFrame(data) ``` **Tasks:** 1. **Create a MultiIndex DataFrame:** - Create a MultiIndex for the DataFrame using \'Year\', \'Quarter\', \'Category\', and \'Sub-Category\' columns. - Set the names of the MultiIndex levels appropriately. 2. **Selection and Slicing:** - Select all sales data for the year 2021. - Select sales data for \'Q1\' of 2022. - Select sales data for \'Electronics\' category across all years and quarters. - Select sales data for the \'Phones\' sub-category specifically. 3. **Reindexing:** - Reindex the DataFrame to include an additional quarter (\'Q3\') for the year 2022, with NaN values for sales in the missing periods. Fill NaN values with 0. - Reconstruct the DataFrame to have only the used levels after reindexing. 4. **Data Aggregation:** - Compute the total sales for each year. - Compute the average sales per quarter for the \'Clothing\' category. - Compute the total sales for each product sub-category across all years and quarters. # Constraints: - Use pandas version >= 1.0. - You are allowed to use any function or method from pandas. # Expected Output: - The output should be the resulting DataFrames after performing each task, showcasing the data manipulations requested. **Example Output:** ```plaintext 1. MultiIndex DataFrame: MultiIndex DataFrame with appropriate levels. 2. Selection and Slicing: - Sales data for 2021: - Sales data for \'Q1\' of 2022: - Sales data for \'Electronics\' category: - Sales data for \'Phones\' sub-category: 3. Reindexing: - DataFrame reindexed to include \'Q3\' of 2022: - DataFrame after removing unused levels: 4. Data Aggregation: - Total sales for each year: - Average sales per quarter for \'Clothing\' category: - Total sales for each product sub-category: ``` Ensure to provide the implementation code and the output DataFrames for each task.","solution":"import pandas as pd # Sample data data = { \'Year\': [2021, 2021, 2021, 2021, 2022, 2022, 2022, 2022], \'Quarter\': [\'Q1\', \'Q1\', \'Q2\', \'Q2\', \'Q1\', \'Q1\', \'Q2\', \'Q2\'], \'Category\': [\'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\', \'Electronics\', \'Clothing\'], \'Sub-Category\': [\'Phones\', \'Shirts\', \'Tablets\', \'Pants\', \'Phones\', \'Shirts\', \'Tablets\', \'Pants\'], \'Sales\': [50000, 30000, 45000, 35000, 52000, 32000, 47000, 36000] } df = pd.DataFrame(data) # 1. Create a MultiIndex DataFrame df.set_index([\'Year\', \'Quarter\', \'Category\', \'Sub-Category\'], inplace=True) df.index.names = [\'Year\', \'Quarter\', \'Category\', \'Sub-Category\'] # 2. Selection and Slicing sales_2021 = df.loc[2021] sales_q1_2022 = df.loc[2022].loc[\'Q1\'] sales_electronics = df.xs(\'Electronics\', level=\'Category\', drop_level=False) sales_phones = df.xs(\'Phones\', level=\'Sub-Category\', drop_level=False) # 3. Reindexing new_index = pd.MultiIndex.from_product( [[2021, 2022], [\'Q1\', \'Q2\', \'Q3\'], [\'Electronics\', \'Clothing\'], [\'Phones\', \'Shirts\', \'Tablets\', \'Pants\']], names=[\'Year\', \'Quarter\', \'Category\', \'Sub-Category\'] ) df_reindexed = df.reindex(new_index, fill_value=0) # Drop unused levels in the index df_reindexed = df_reindexed.reset_index().set_index([\'Year\', \'Quarter\', \'Category\', \'Sub-Category\']) # 4. Data Aggregation total_sales_per_year = df.groupby(level=\'Year\').sum() average_sales_clothing_per_quarter = df.xs(\'Clothing\', level=\'Category\').groupby(level=\'Quarter\').mean() total_sales_per_sub_category = df.groupby(level=\'Sub-Category\').sum() # Output DataFrames def get_dataframes(): return { \'Sales data for 2021\': sales_2021, \'Sales data for Q1 2022\': sales_q1_2022, \'Sales data for Electronics category\': sales_electronics, \'Sales data for Phones sub-category\': sales_phones, \'DataFrame reindexed to include Q3 of 2022\': df_reindexed, \'Total sales for each year\': total_sales_per_year, \'Average sales per quarter for Clothing category\': average_sales_clothing_per_quarter, \'Total sales for each product sub-category\': total_sales_per_sub_category }"},{"question":"# Terminal Mode Switcher in Python You are tasked with implementing a class that manages switching between different terminal modes using the `tty` module. Your class should be able to set the terminal to `raw` mode and `cbreak` mode using file descriptors. Class and Methods Create a class `TerminalMode` that encapsulates the functionality of switching terminal modes. The class should have the following methods: 1. `__init__(self, fd)`: Initialize the class with a file descriptor `fd`. 2. `set_raw_mode(self)`: Change the mode of the file descriptor to raw mode. 3. `set_cbreak_mode(self)`: Change the mode of the file descriptor to cbreak mode. 4. `reset_mode(self)`: Reset the terminal mode to its original state before any changes were made. Input and Output - The input includes the instantiation of the `TerminalMode` class with a valid file descriptor. - Methods `set_raw_mode` and `set_cbreak_mode` modify the terminal mode and return `None`. - The method `reset_mode` restores the original terminal settings and returns `None`. Constraints - The solution must work only on Unix systems. - Handle any necessary error-checking when switching modes. - Ensure the terminal is reset to its original mode upon any exception or program termination. Example Usage ```python import sys # Assuming fd refers to the file descriptor of the standard input fd = sys.stdin.fileno() # Initialize the TerminalMode class terminal_mode = TerminalMode(fd) # Set to raw mode terminal_mode.set_raw_mode() # Do some operations in raw mode... # ... # Set to cbreak mode terminal_mode.set_cbreak_mode() # Do some operations in cbreak mode... # ... # Reset to original mode terminal_mode.reset_mode() ``` Implement the `TerminalMode` class in Python as described.","solution":"import tty import termios class TerminalMode: def __init__(self, fd): self.fd = fd self.original_mode = termios.tcgetattr(fd) def set_raw_mode(self): tty.setraw(self.fd) def set_cbreak_mode(self): tty.setcbreak(self.fd) def reset_mode(self): termios.tcsetattr(self.fd, termios.TCSADRAIN, self.original_mode)"},{"question":"# Pandas Data Cleaning Assessment Objective: Implement a Python function to identify and handle missing values in a pandas DataFrame. Your function should adequately process both standard numerical and datetime data types. Problem Statement: You have been provided with a pandas DataFrame that contains various numerical and datetime columns. Your task is to create a function named `handle_missing_values` that handles these missing values following these rules: 1. For numerical columns (int or float), replace missing values with the mean of that column. 2. For datetime columns, replace missing values with a specified date (\'2000-01-01\'). Specs: - Function Name: `handle_missing_values` - Input: A pandas DataFrame `df` - Output: A pandas DataFrame with the missing values handled accordingly Constraints: - The input DataFrame can contain multiple columns of different data types (integers, floats, datetimes). - The datetime columns should be identified and treated properly. - If a column has all missing values, it should remain as all missing values. Example Usage: ```python import pandas as pd import numpy as np data = { \\"A\\": [1, np.nan, 3, 4, np.nan], \\"B\\": [np.nan, \\"2001-01-01\\", \\"2002-02-02\\", np.nan, \\"2004-04-04\\"], \\"C\\": [np.nan, np.nan, np.nan, np.nan, np.nan] } df = pd.DataFrame(data) df[\'B\'] = pd.to_datetime(df[\'B\'], errors=\'coerce\') result_df = handle_missing_values(df) print(result_df) ``` Expected Output: ```plaintext A B C 0 1.0 2000-01-01 NaN 1 2.67 2001-01-01 NaN 2 3.0 2002-02-02 NaN 3 4.0 2000-01-01 NaN 4 2.67 2004-04-04 NaN ``` # Notes: - You are not allowed to use any third-party libraries except pandas and numpy. - Your solution should be efficient and handle large DataFrames effectively. - Include comments and docstrings to describe the functionality of your code.","solution":"import pandas as pd import numpy as np def handle_missing_values(df): Handle missing values in a pandas DataFrame. Parameters: df (pd.DataFrame): The input DataFrame containing potential missing values. Returns: pd.DataFrame: The DataFrame with missing values handled. # Iterate over each column in the DataFrame for column in df.columns: if pd.api.types.is_numeric_dtype(df[column]): # For numeric columns, replace NaNs with the mean of the column mean_value = df[column].mean() df[column].fillna(mean_value, inplace=True) elif pd.api.types.is_datetime64_any_dtype(df[column]): # For datetime columns, replace NaNs with \'2000-01-01\' df[column].fillna(pd.Timestamp(\'2000-01-01\'), inplace=True) return df"},{"question":"# Python-C Extension Function Implementation As part of your work in extending Python with C, you are tasked with creating a Python extension module. This module will include a function that processes numeric and string inputs, performs some simple operations, and returns the results in a packaged format. Function Specification You need to implement the following C function: ```c static PyObject *process_data(PyObject *self, PyObject *args) { // Your code here } ``` The function should parse the input arguments using `PyArg_ParseTuple()` and construct the return value using `Py_BuildValue()`. Here\'s what the function should do: 1. **Input**: Accepts a tuple of arguments in the form `(integer a, float b, string c)`. - `a` is an integer. - `b` is a float. - `c` is a string. 2. **Process**: - Increment the integer by 1. - Square the float. - Convert the string to uppercase. 3. **Output**: Returns a tuple in the form `(new_a, new_b, new_c)`, where: - `new_a` is the incremented integer. - `new_b` is the squared float. - `new_c` is the uppercase string. Requirements: - You must ensure proper error handling. If the input arguments do not match the expected format, raise appropriate Python exceptions. - Manage memory correctly, especially when dealing with string manipulations. Example Usage: Here\'s how the function should behave when called from Python: ```python import yourmodule # Assuming your module is named \'yourmodule\' result = yourmodule.process_data(2, 3.5, \\"hello\\") print(result) # Should print: (3, 12.25, \'HELLO\') ``` Performance: - Ensure that the operations are executed efficiently. - Safeguard against common pitfalls such as buffer overflow. Your task is to fill in the `process_data` function definition to meet these requirements.","solution":"# This function assumes the presence of a properly authored C extension module. # In Python, the equivalent function for testing purposes can be written as follows: def process_data(a, b, c): Processes input data: increments integer, squares float, and converts string to uppercase. Arguments: a -- integer to be incremented b -- float to be squared c -- string to be converted to uppercase Returns: A tuple of (new_a, new_b, new_c), where new_a is the incremented integer, new_b is the squared float, and new_c is the uppercase string. if not isinstance(a, int) or not isinstance(b, float) or not isinstance(c, str): raise ValueError(\\"Invalid input types\\") new_a = a + 1 new_b = b ** 2 new_c = c.upper() return (new_a, new_b, new_c)"},{"question":"# Question In this assessment, you will implement a machine learning task using Scikit-learn and demonstrate your understanding of parallelism and resource management features as described in the documentation provided. **Task:** 1. **Load and Preprocess Data:** - Load a dataset of your choice (you can use any available dataset from Scikit-learn, such as the `digits` or `iris` dataset). - Split the dataset into training and testing sets. 2. **Implement a Model with Parallelism:** - Implement a `RandomForestClassifier` from Scikit-learn and use the `n_jobs` parameter to enable parallelism. - Set `n_jobs` to a value that ensures parallel processing (e.g., `n_jobs=4` if you have multiple cores). 3. **Tune Parallelism using Environment Variables:** - Use `threadpoolctl` to control the number of threads used by underlying libraries (like OpenMP, MKL, or OpenBLAS). - Set `OMP_NUM_THREADS`, `MKL_NUM_THREADS`, or `OPENBLAS_NUM_THREADS` to control threading at a lower level (e.g., `OMP_NUM_THREADS=2`). 4. **Evaluation and Reporting:** - Train the model on the training dataset. - Evaluate the model on the testing dataset and report accuracy and training time. - Experiment with different values of `n_jobs` and thread settings, and report how these changes affect performance. **Constraints and Requirements:** - You should only use Scikit-learn and related libraries (joblib, threadpoolctl) for your implementation. - Clearly comment on your code to explain how different settings affect performance. - Ensure your implementation avoids oversubscription of CPU resources. **Submission:** Submit a Jupyter notebook containing the following: - Code implementation for each of the steps. - Detailed comments explaining parallelism settings. - Performance evaluation results. Example Code Structure: ```python import numpy as np import time from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from joblib import Parallel, parallel_backend import threadpoolctl # Step 1: Load and preprocess data data = load_digits() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) # Step 2: Implement a RandomForestClassifier with parallelism n_jobs = 4 # Adjust this based on the number of available CPU cores clf = RandomForestClassifier(n_jobs=n_jobs) # Step 3: Control lower-level parallelism with parallel_backend(\'loky\', n_jobs=n_jobs): with threadpoolctl.threadpool_limits(limits=2, user_api=\'blas\'): start_time = time.time() clf.fit(X_train, y_train) end_time = time.time() # Step 4: Evaluate the model accuracy = clf.score(X_test, y_test) training_time = end_time - start_time print(f\\"Accuracy: {accuracy}\\") print(f\\"Training Time: {training_time} seconds\\") # Experiment with different n_jobs and threading settings # Document your observations and results ```","solution":"import numpy as np import time from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import threadpoolctl # Step 1: Load and preprocess data data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) # Step 2: Implement a RandomForestClassifier with parallelism n_jobs = 4 # Adjust this based on the number of available CPU cores clf = RandomForestClassifier(n_jobs=n_jobs) # Step 3: Control lower-level parallelism with threadpoolctl.threadpool_limits(limits=2, user_api=\'blas\'): start_time = time.time() clf.fit(X_train, y_train) end_time = time.time() # Step 4: Evaluate the model accuracy = clf.score(X_test, y_test) training_time = end_time - start_time print(f\\"Accuracy: {accuracy}\\") print(f\\"Training Time: {training_time} seconds\\") # We can experiment with different values for n_jobs and thread settings and observe the changes in performance."},{"question":"# Auto-Completion with Custom Modules In this task, you are required to implement a custom completion feature based on the concepts provided by the `rlcompleter` module. Objectives: 1. **Implement a Custom Completer Class**: This class should mimic the behavior of `rlcompleter.Completer` but should also support custom namespaces. 2. **Integration with `readline`**: Similar to the example provided in the documentation, bind the completer class with the readline interface for interactive use. Requirements: 1. **CustomCompleter Class**: - Define a class `CustomCompleter` with a method `complete(text, state)`. - Add support for a custom namespace dictionary that allows completion from objects defined within it. - The `complete` method should return valid completions based on the state. 2. **Complete Method**: - If `text` does not contain a dot \'.\', complete from `__main__`, `builtins`, the custom namespace, and Python keywords. - If `text` contains a dot, it should evaluate up to the last part and use `dir()` to find matches for the remaining parts. 3. **Interactive Mode**: - Incorporate the `customCompleter.complete()` method with the `readline` module. - Ensure that pressing \\"TAB\\" after partial typing invokes the custom completion. Input and Output: - **Input**: Strings of partially typed Python identifiers. - **Output**: A list of valid Python identifiers that match the input. Constraints: - You should handle proper exception handling to catch and silence any exception that arises during evaluation of expressions just like `rlcompleter.Completer`. Example: ```python import readline import keyword class CustomCompleter: def __init__(self, namespace=None): if namespace is None: namespace = {} self.namespace = namespace def complete(self, text, state): # Implement the completion logic here pass readline.set_completer(CustomCompleter(namespace={\\"my_var\\": 42, \\"my_func\\": lambda x: x}).complete) readline.parse_and_bind(\\"tab: complete\\") # Testing in interactive shell # >>> my_<TAB> # should suggest \'my_var\', \'my_func\' ``` Implementing this class successfully involves understanding and extending the completion mechanism mentioned in the `rlcompleter` module while factoring in custom namespace support and handling dotted names.","solution":"import readline import keyword import builtins import __main__ class CustomCompleter: def __init__(self, namespace=None): if namespace is None: namespace = {} self.namespace = namespace def complete(self, text, state): matches = [] if \\".\\" in text: try: obj, attr = text.rsplit(\\".\\", 1) scope = eval(obj, __main__.__dict__, self.namespace) matches = [f\\"{obj}.{name}\\" for name in dir(scope) if name.startswith(attr)] except Exception: return None else: matches = [word for word in self.all_candidates(text)] try: return matches[state] except IndexError: return None def all_candidates(self, text): matches = set() matches.update([kw for kw in keyword.kwlist if kw.startswith(text)]) matches.update([name for name in dir(builtins) if name.startswith(text)]) matches.update([name for name in self.namespace.keys() if name.startswith(text)]) matches.update([name for name in dir(__main__) if name.startswith(text)]) return matches # Integration with readline readline.set_completer(CustomCompleter(namespace={\\"my_var\\": 42, \\"my_func\\": lambda x: x}).complete) readline.parse_and_bind(\\"tab: complete\\")"},{"question":"Advanced Exception Handling **Objective:** Write a Python function that processes a list of file names. For each file name in the list, the function should try to open the file and read its contents. If the file cannot be opened due to an OSError (e.g., the file does not exist), raise a custom exception `FileProcessingError`. If the file exists but contains invalid content (i.e., it cannot be converted to an integer), handle the ValueError and continue processing the next file. Ensure that all files are closed properly after being read, whether an exception occurred or not. # Function Signature: ```python def process_files(file_names: list[str]) -> dict: ``` # Input: - `file_names`: A list of strings where each string is a file name. # Output: - Returns a dictionary where the key is the file name and the value is an integer read from the file. # Custom Exception: You need to define a custom exception class `FileProcessingError` which inherits from the built-in `Exception` class. # Requirements: 1. Define a custom exception class `FileProcessingError`. 2. Implement the `process_files` function. 3. Use try/except/finally to ensure all files are properly closed. 4. Use appropriate exception handling to manage different types of errors as described above. # Example Usage: ```python # Example of file contents: # \\"file1.txt\\" contains \\"42\\" # \\"file2.txt\\" does not exist # \\"file3.txt\\" contains \\"invalid\\" file_names = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] try: result = process_files(file_names) print(result) # Output should be {\\"file1.txt\\": 42} except FileProcessingError as e: print(e) ``` # Constraints: - Assume all files contain a single line of text. - The function should skip files with invalid content (which cannot be converted to an integer). - Raise `FileProcessingError` if any file cannot be opened. # Notes: - Ensure robust handling of files using the `with` statement for predefined clean-up actions. - Focus on implementing clear and concise error messages within your custom exception. # Additional Challenge (Optional): - Implement a mechanism to log errors to a separate log file named `error.log` whenever an exception occurs.","solution":"class FileProcessingError(Exception): Custom exception class for file processing errors. def __init__(self, file_name, message=\\"Error processing file\\"): self.file_name = file_name self.message = f\\"{message}: {file_name}\\" super().__init__(self.message) def process_files(file_names): Processes a list of file names. For each file name in the list, the function tries to open the file and read its contents. If the file cannot be opened due to an OSError, it raises a FileProcessingError. If the file exists but contains invalid content (i.e., it cannot be converted to an integer), the function handles the ValueError and continues processing the next file. All files are closed properly after being read, whether an exception occurred or not. :param file_names: List of file names to process. :return: Dictionary with file name as key and integer read from the file as value. result = {} for file_name in file_names: try: with open(file_name, \'r\') as file: content = file.read().strip() result[file_name] = int(content) except OSError: raise FileProcessingError(file_name) except ValueError: # Skip files with invalid content and continue with the next file continue return result"},{"question":"**Objective:** Demonstrate your understanding of Python\'s `time` module by implementing a function to parse a list of event timestamps, convert them to a specified timezone, and format them for display. **Problem Statement:** You are given a list of events with their timestamps in UTC. Your task is to implement the following function: ```python def format_event_times(event_timestamps, time_zone_offset, output_format): Converts and formats event timestamps from UTC to a specified local time zone. Parameters: event_timestamps (list of str): A list of event timestamps in the format \'%Y-%m-%d %H:%M:%S\'. time_zone_offset (int): Offset in hours from UTC to the desired timezone (negative for west, positive for east). output_format (str): Desired format for the output timestamps using `strftime` format codes. Returns: list of str: A list of formatted event timestamps in the desired time zone and format. pass ``` **Function Details:** - `event_timestamps`: A list of strings where each string represents an event time in UTC with the format \'%Y-%m-%d %H:%M:%S\'. - `time_zone_offset`: An integer representing the hours to offset from UTC. For example, `-5` for EST (Eastern Standard Time) or `+2` for EET (Eastern European Time). - `output_format`: A string representing the desired output format using `strftime` format codes, e.g., \'%Y-%m-%d %H:%M:%S\'. **Constraints:** - The list can contain up to 1000 event timestamps. - The `time_zone_offset` will always be between -12 and +14, inclusive. - The `output_format` will be a valid `strftime` format. **Example Usage:** ```python event_timestamps = [\\"2023-02-25 14:00:00\\", \\"2023-02-26 03:30:00\\"] time_zone_offset = -5 output_format = \\"%Y-%m-%d %I:%M %p\\" formatted_times = format_event_times(event_timestamps, time_zone_offset, output_format) # Expected output: [\'2023-02-25 09:00 AM\', \'2023-02-25 10:30 PM\'] ``` **Instructions:** 1. Parse the given timestamps. 2. Adjust each timestamp by the specified time zone offset. 3. Format the adjusted timestamps using the specified output format. 4. Return the list of formatted timestamps. Students must utilize functions such as `time.strptime`, `time.strptime`, `time.strftime`, and arithmetic for time zone adjustments while handling daylight saving time appropriately if necessary.","solution":"import time from datetime import datetime, timedelta def format_event_times(event_timestamps, time_zone_offset, output_format): Converts and formats event timestamps from UTC to a specified local time zone. Parameters: event_timestamps (list of str): A list of event timestamps in the format \'%Y-%m-%d %H:%M:%S\'. time_zone_offset (int): Offset in hours from UTC to the desired timezone (negative for west, positive for east). output_format (str): Desired format for the output timestamps using `strftime` format codes. Returns: list of str: A list of formatted event timestamps in the desired time zone and format. formatted_times = [] for timestamp in event_timestamps: # Parse the UTC time utc_time = datetime.strptime(timestamp, \\"%Y-%m-%d %H:%M:%S\\") # Convert to the desired time zone local_time = utc_time + timedelta(hours=time_zone_offset) # Format the time formatted_time = local_time.strftime(output_format) formatted_times.append(formatted_time) return formatted_times"},{"question":"# Advanced Task: Temporary File Management with `tempfile` Module Your task is to write a function `tempfile_operations` that demonstrates your understanding of the `tempfile` module. This function will create, write, read, and manage temporary files and directories using various functions/classes provided by the `tempfile` module. Here are the detailed requirements: 1. **Create a temporary file** using `TemporaryFile`. Write the string \\"Hello, TemporaryFile!\\" to the file, seek to the beginning, and return the content of the file. 2. **Create a named temporary file** using `NamedTemporaryFile`. Write the string \\"Hello, NamedTemporaryFile!\\" to the file, seek to the beginning, read the content, and store the file\'s name and content. Close the file explicitly. 3. **Create a spooled temporary file** using `SpooledTemporaryFile` with a max size of 10 bytes. Write more than 10 bytes of data, demonstrating the rollover behavior, seek to the beginning, and return the content of the file. 4. **Create a temporary directory** using `TemporaryDirectory`. Create a new file within this directory and write the string \\"Hello, TemporaryDirectory!\\" to it. Read the content of this file and store the directory name and file content. 5. **Manually clean up** any resources managed by `mkstemp` and `mkdtemp`. The expected input and output formats are as follows: - **Input:** None - **Output:** A tuple containing: * Content written and read from `TemporaryFile`. * A dictionary with keys `name` and `content` for the `NamedTemporaryFile`. * Content written and read from `SpooledTemporaryFile`. * A dictionary with keys `dir_name` and `file_content` for the `TemporaryDirectory`. **Function Signature:** ```python import tempfile def tempfile_operations() -> tuple: # Your implementation here ``` **Constraints:** - Use context managers where applicable to ensure automatic cleanup of files and directories. - Manually handle cleanup for `mkstemp` and `mkdtemp`. **Example of Usage:** ```python result = tempfile_operations() print(result) ``` Expected Result: ```python ( \'Hello, TemporaryFile!\', {\'name\': \'/path/to/tempfile\', \'content\': \'Hello, NamedTemporaryFile!\'}, \'Rolled over data after exceeding in-memory limit.\', {\'dir_name\': \'/path/to/tempdir\', \'file_content\': \'Hello, TemporaryDirectory!\'} ) ``` Note: The actual paths in the result will vary based on the operating system and environment.","solution":"import tempfile import os def tempfile_operations(): results = [] # 1. TemporaryFile with tempfile.TemporaryFile(mode=\'w+t\') as tempf: tempf.write(\\"Hello, TemporaryFile!\\") tempf.seek(0) tempf_content = tempf.read() results.append(tempf_content) # 2. NamedTemporaryFile named_tempfile_info = {} with tempfile.NamedTemporaryFile(mode=\'w+t\', delete=False) as named_tempf: named_tempf.write(\\"Hello, NamedTemporaryFile!\\") named_tempf.seek(0) named_tempfile_info[\'name\'] = named_tempf.name named_tempfile_info[\'content\'] = named_tempf.read() # Explicitly close the named temporary file to delete it afterwards os.remove(named_tempfile_info[\'name\']) results.append(named_tempfile_info) # 3. SpooledTemporaryFile with tempfile.SpooledTemporaryFile(max_size=10, mode=\'w+t\') as spooled_tempf: spooled_tempf.write(\\"This is a long string that will cause a rollover.\\") spooled_tempf.seek(0) spooled_tempf_content = spooled_tempf.read() results.append(spooled_tempf_content) # 4. TemporaryDirectory temp_dir_info = {} with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = os.path.join(temp_dir, \\"tempfile.txt\\") with open(temp_file_path, \'w+t\') as temp_file: temp_file.write(\\"Hello, TemporaryDirectory!\\") temp_file.seek(0) temp_dir_info[\'dir_name\'] = temp_dir temp_dir_info[\'file_content\'] = temp_file.read() os.remove(temp_file_path) results.append(temp_dir_info) return tuple(results)"},{"question":"Utilizing DLPack for Data Interchange **Objective:** In this task, you are asked to implement a function that demonstrates the data interchange capability between PyTorch and another deep learning framework using the `dlpack` format. **Description:** You will write a function `convert_and_sum` that: 1. Creates a PyTorch tensor of a specified shape and initializes it with random numbers. 2. Converts this PyTorch tensor to a DLPack tensor. 3. Converts it back to a PyTorch tensor from a DLPack tensor. 4. Returns the sum of all elements of the final PyTorch tensor. **Function Signature:** ```python import torch from torch.utils.dlpack import from_dlpack, to_dlpack def convert_and_sum(shape): Converts a PyTorch tensor to a DLPack tensor and back, then returns the sum of tensor elements. Args: - shape (tuple of int): The shape of the tensor to create. Returns: - float: The sum of all elements in the final PyTorch tensor. ``` **Input:** - `shape` (tuple of int): A tuple representing the shape of the tensor. Each dimension in the shape will be a positive integer (e.g., `(2, 3)`, `(4, 5, 6)`). **Output:** - Return the sum of the elements in the PyTorch tensor after converting it to DLPack format and back. **Constraints:** - You can assume the input `shape` is always valid and well-formed. - Use `torch.rand` to initialize the tensor with random numbers. - Ensure that the conversion process does not alter the tensor data. **Example:** ```python # Example usage of convert_and_sum function shape = (2, 3) result = convert_and_sum(shape) print(result) # This should print the sum of all elements in the 2x3 tensor. ``` **Note:** The intermediary steps of converting to and from DLPack tensors should not change the data in the tensor. You should see the same numerical values before and after the conversion. **Performance Requirements:** - The function should handle reasonably large tensors efficiently (e.g., shapes up to `(1000, 1000)` should not cause timeouts or excessive memory use). **Hint:** - You can use `torch.rand(shape)` to create a tensor with the specified shape and random values. - Utilize `to_dlpack` and `from_dlpack` for conversion between PyTorch tensor and DLPack tensor. Implement the function `convert_and_sum` in Python.","solution":"import torch from torch.utils.dlpack import from_dlpack, to_dlpack def convert_and_sum(shape): Converts a PyTorch tensor to a DLPack tensor and back, then returns the sum of tensor elements. Args: - shape (tuple of int): The shape of the tensor to create. Returns: - float: The sum of all elements in the final PyTorch tensor. # Step 1: Create a PyTorch tensor of specified shape and initialize with random numbers tensor = torch.rand(shape) # Step 2: Convert the PyTorch tensor to a DLPack tensor dlpack_tensor = to_dlpack(tensor) # Step 3: Convert it back to a PyTorch tensor from DLPack tensor tensor_from_dlpack = from_dlpack(dlpack_tensor) # Step 4: Return the sum of all elements in the final PyTorch tensor tensor_sum = torch.sum(tensor_from_dlpack).item() return tensor_sum"},{"question":"# Coding Challenge: Complex List and Numeric Operations **Description:** Your task is to write a function `transform_and_sum` that accepts a list of integers and performs the following operations: 1. Filter out all non-prime numbers from the list. 2. For each prime number, create a complex number using the prime as both real and imaginary parts. 3. Sum up all the complex numbers. 4. Return the absolute value (magnitude) of the resulting complex number. The function should adhere to the following specifications: **Input:** - A list of integers `nums` where each integer can be positive, negative or zero. **Output:** - A float representing the absolute value of the sum of complex numbers. **Constraints:** - The input list can be empty. - If there are no prime numbers in the list, the function should return `0.0`. **Example Usage:** ```python def transform_and_sum(nums): # Your implementation here # Example test cases print(transform_and_sum([2, 3, 4, 5, 6, 8, 10])) # -> Output: 8.246211251235321 (Explanation: 2+2j + 3+3j + 5+5j = 10+10j, |10+10j| == √(10^2 + 10^2)) print(transform_and_sum([15, 20, 25])) # -> Output: 0.0 (Explanation: No prime numbers) print(transform_and_sum([7, 11, 0, -5, -7])) # -> Output: 18.385 (Explanation: 7+7j + 11+11j = 18+18j, |18+18j| == √(18^2 + 18^2)) ``` **Notes:** 1. A prime number is a natural number greater than 1 that is not a product of two smaller natural numbers. 2. Use the built-in `abs()` function to compute the absolute value of a complex number. 3. Ensure your code is efficient and handle edge cases such as empty list or lists with no prime numbers appropriately.","solution":"def is_prime(n): Helper function to determine if n is a prime number. if n <= 1: return False if n == 2 or n == 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def transform_and_sum(nums): Transforms the list of integers by filtering out non-prime numbers, creating complex numbers with primes, then summing and returning the magnitude of the sum of those complex numbers. # Filter out non-prime numbers primes = [x for x in nums if is_prime(x)] # Create complex numbers complex_numbers = [complex(p, p) for p in primes] # Sum complex numbers total_complex = sum(complex_numbers, 0) # Return the magnitude of the complex sum return abs(total_complex)"},{"question":"Problem Statement You are tasked with implementing a function that modifies the environment variables available to the current Python process. You will ensure the updates are reflected immediately and can be used by any subsequently executed system command. Your function will accept a dictionary of environment variables to be set or updated. Function Signature ```python def update_environment(env_dict: dict) -> None: Updates the environment variables for the current Python process. Args: env_dict (dict): A dictionary where keys and values are strings representing environment variable names and values to be set or updated. Returns: None ``` Input - `env_dict`: A dictionary where keys are environment variable names (strings) and values are the new values (strings) to be set for these variables. Output - The function does not return any value. Instead, it updates the environment variables of the current Python process. Constraints - All keys and values in `env_dict` are non-empty strings. - Keys (environment variable names) are unique within the dictionary. - Ensure the function is compatible with both Unix and Windows systems. Example ```python # Example usage update_environment({\'TEST_VAR\': \'12345\', \'ANOTHER_VAR\': \'ABCDE\'}) import os print(os.getenv(\'TEST_VAR\')) # Output should be \'12345\' print(os.getenv(\'ANOTHER_VAR\')) # Output should be \'ABCDE\' ``` Use the `os` module to handle the environment updates to ensure portability and correctness. Note All necessary imports and any additional helper functions should be included within your solution.","solution":"import os def update_environment(env_dict: dict) -> None: Updates the environment variables for the current Python process. Args: env_dict (dict): A dictionary where keys and values are strings representing environment variable names and values to be set or updated. for key, value in env_dict.items(): os.environ[key] = value"},{"question":"HTML String Transformation Problem Statement You are working with an application that processes user input for a web service. Since user inputs can contain HTML special characters, it is crucial to safely handle these strings to prevent issues like HTML injection. To do this, you will write a utility function that takes a string, escapes it for safe HTML inclusion, and then unescapes it to verify the integrity of the transformation. Task Write a function `html_transform` that follows these specifications: 1. Takes a string `input_text` as its parameter. 2. Escapes the string using `html.escape`. 3. Unescapes the string using `html.unescape`. 4. Returns the resulting string after both transformations. Function Signature ```python def html_transform(input_text: str) -> str: pass ``` Input * `input_text` (string): The input text which may contain special HTML characters. Output * A string that is the result of escaping and then unescaping `input_text`. Given that unescaping should ideally reverse escaping (if done correctly), the output should match the original `input_text`. Constraints * The length of `input_text` will be at most 1000 characters. * `input_text` can contain any printable character. Example ```python input_text = \\"This is a sample text with special characters like &, <, >, \' and \\" in it.\\" result = html_transform(input_text) print(result) # \\"This is a sample text with special characters like &, <, >, \' and \\" in it.\\" ``` Performance Requirements * Your solution should efficiently handle the maximum input size under 1 second. Note - Ensure to test the function with various combinations of special HTML characters to validate the integrity of the transformation process. Additional Clarification * You are required to use the `html.escape` and `html.unescape` functions from the `html` module. * Remember that `html.escape` includes an optional parameter `quote` which is `True` by default. It ensures that both single and double quotes are also escaped.","solution":"import html def html_transform(input_text: str) -> str: Escapes the input text to make it safe for HTML and then unescapes it. Returns the resulting text after both transformations. escaped_text = html.escape(input_text) unescaped_text = html.unescape(escaped_text) return unescaped_text"},{"question":"Objective: Implement a function that uses Kernel Density Estimation (KDE) to analyze the distribution of a given dataset and visualize the results. Your task is to demonstrate the use of KDE by estimating the density of a provided dataset and comparing the results using different kernel functions and bandwidth parameters. Task: 1. **Function Definition**: - Define a function `kde_analysis(data, kernels, bandwidths)`. - **Input**: - `data`: A NumPy array of shape (n_samples, n_features) containing the dataset. - `kernels`: A list of strings specifying the kernel types to be used (e.g., `[\'gaussian\', \'tophat\', \'epanechnikov\']`). - `bandwidths`: A list of bandwidth values to be tested (e.g., `[0.1, 0.5, 1.0]`). - **Output**: - A dictionary where the keys are tuples `(kernel, bandwidth)` and the values are the density estimates. 2. **Steps**: - For each combination of kernel and bandwidth, fit a `KernelDensity` model to the data. - Compute the log likelihood of the given data points under the fitted model. - Store the results in a dictionary and return it. 3. **Visualization**: - Create a 1D dataset (e.g., a bimodal distribution). - Use your `kde_analysis` function to compute density estimates for different kernels and bandwidths. - Plot the original data and the density estimates for each combination of kernel and bandwidth. Function Signature: ```python import numpy as np from sklearn.neighbors import KernelDensity import matplotlib.pyplot as plt def kde_analysis(data, kernels, bandwidths): Perform Kernel Density Estimation on the provided dataset using different kernels and bandwidths. Parameters: data (np.ndarray): Input data of shape (n_samples, n_features). kernels (list): List of strings specifying the kernel types to be used. bandwidths (list): List of bandwidth values to be tested. Returns: dict: A dictionary where the keys are tuples (kernel, bandwidth) and the values are the density estimates. results = {} for kernel in kernels: for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) log_density = kde.score_samples(data) results[(kernel, bandwidth)] = np.exp(log_density) # Converting log density to density return results # Example 1D data for visualization np.random.seed(0) data = np.concatenate([np.random.normal(loc=-2, scale=0.5, size=100), np.random.normal(loc=2, scale=0.5, size=100)])[:, np.newaxis] # Kernels and bandwidths to analyze kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1.0] # Perform KDE analysis density_estimates = kde_analysis(data, kernels, bandwidths) # Plotting the results x_plot = np.linspace(-5, 5, 1000)[:, np.newaxis] fig, ax = plt.subplots(figsize=(10, 6)) ax.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', label=\'Histogram of data\') for (kernel, bandwidth), density in density_estimates.items(): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) log_density = kde.score_samples(x_plot) ax.plot(x_plot[:, 0], np.exp(log_density), label=f\'KDE with {kernel} kernel, bandwidth={bandwidth}\') ax.legend() plt.show() ``` Constraints: 1. Assume the input data provided will be numeric and well-formed. 2. Use the scikit-learn and matplotlib packages for KDE and visualization purposes. 3. Performance should be optimized for the size of input data (n_samples <= 1000). This question assesses students\' ability to understand and implement KDE using scikit-learn, interpret different kernels and bandwidth parameters\' influence on the density estimation, and effectively visualize and analyze the results.","solution":"import numpy as np from sklearn.neighbors import KernelDensity def kde_analysis(data, kernels, bandwidths): Perform Kernel Density Estimation on the provided dataset using different kernels and bandwidths. Parameters: data (np.ndarray): Input data of shape (n_samples, n_features). kernels (list): List of strings specifying the kernel types to be used. bandwidths (list): List of bandwidth values to be tested. Returns: dict: A dictionary where the keys are tuples (kernel, bandwidth) and the values are the density estimates. results = {} for kernel in kernels: for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) log_density = kde.score_samples(data) results[(kernel, bandwidth)] = np.exp(log_density) # Converting log density to density return results"},{"question":"Objective Your goal is to create a custom event loop policy and explore the provided `asyncio` module by implementing a specific policy for handling event loops. You will also demonstrate your understanding of subprocess watchers by integrating a specialized child process watcher into your custom event loop policy. Problem Description Create a custom event loop policy class `CustomEventLoopPolicy` that inherits from `asyncio.DefaultEventLoopPolicy`. This class should: 1. Override the `get_event_loop` method to print a custom log message each time it is called. 2. Implement a specialized child process watcher that is specifically tuned for handling signal and thread disruptions efficiently. Choose an appropriate subclass from the described child watchers and integrate it. Instructions 1. Define the `CustomEventLoopPolicy` class. 2. Override necessary methods to meet the requirements described above. 3. Ensure that your custom policy can be set as the current event loop policy using `asyncio.set_event_loop_policy()`. 4. Demonstrate the policy by writing a small script that: - Sets this custom policy as the current policy. - Creates and starts an asyncio event loop. - Uses the loop to execute a simple async task. - Simulates handling subprocesses, demonstrating the integration of the specialized child process watcher. Expected Input and Output - **Input**: None (all operations should be demonstrated within the script). - **Output**: Custom log messages when accessing the event loop and successful handling of subprocess checks using the integrated child process watcher. Constraints - Your custom event loop policy should function correctly in the provided context without any errors. - Ensure efficient handling of child processes with minimal overhead. - Use appropriate error handling for subprocess-related operations. Performance Requirements - The custom event loop policy and child watcher should efficiently handle subprocesses without degrading performance, especially when handling multiple subprocesses. ```python # Example Implementation import asyncio class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"CustomEventLoopPolicy: Acquiring event loop.\\") return super().get_event_loop() def __init__(self): super().__init__() self._child_watcher = asyncio.SafeChildWatcher() self._child_watcher.attach_loop(self.get_event_loop()) def set_event_loop(self, loop): super().set_event_loop(loop) self._child_watcher.attach_loop(loop) # Setting the custom event loop policy asyncio.set_event_loop_policy(CustomEventLoopPolicy()) async def main(): print(\\"Running main async function\\") # Run the demo script within an event loop asyncio.run(main()) # Simulate handling subprocesses print(\\"Simulating subprocess handling.\\") # Add further demonstration of subprocess handling as needed. ```","solution":"import asyncio class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"CustomEventLoopPolicy: Acquiring event loop.\\") return super().get_event_loop() def __init__(self): super().__init__() self._child_watcher = asyncio.SafeChildWatcher() self._child_watcher.attach_loop(self.get_event_loop()) def set_event_loop(self, loop): super().set_event_loop(loop) self._child_watcher.attach_loop(loop) # Setting the custom event loop policy asyncio.set_event_loop_policy(CustomEventLoopPolicy()) async def main(): print(\\"Running main async function\\") # Run the demo script within an event loop asyncio.run(main()) print(\\"CustomEventLoopPolicy: Script finished running.\\")"},{"question":"Objective Your task is to create a multi-faceted visualization using Seaborn\'s `pointplot` function. This will involve loading a dataset, performing some data transformations, and displaying the results in a well-structured and customizable plot. Dataset - Use the `penguins` dataset that is available in Seaborn. Problem Statement 1. **Load Data**: Load the `penguins` dataset from Seaborn. 2. **Data Transformation**: - Calculate the average `body_mass_g` for each combination of `species` and `island`. - For each combination of `species` and `island`, also calculate the standard deviation of `body_mass_g`. 3. **Visualization**: Using the transformed data, create a multi-layered point plot: - The x-axis should represent the different species. - The y-axis should represent the average `body_mass_g`. - Use different colors for each `island` and plot them on the same graph. - Include standard deviation error bars. - Customize the appearance of the plot to improve readability and aesthetics. This should include changes to markers, linestyles, and any other relevant properties. 4. **Additional Customization**: Add a title to the plot and label the axes appropriately. Expected Input and Output - **Input**: None (the function should load the dataset itself). - **Output**: A Seaborn point plot displayed inline. Constraints and Limitations - Use only the Seaborn and Pandas libraries. - The solution must handle errors or missing values gracefully. Performance Requirements - The code should be efficient and avoid unnecessary computations. Function Signature ```python def visualize_penguin_mass(): # your code here ``` # Example Your plot should look similar to this (the specifics will depend on the customization choices you make): ![Example Point Plot](https://seaborn.pydata.org/_images/pointplot_main.png)","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_penguin_mass(): # Load the dataset penguins = sns.load_dataset(\'penguins\') # Drop rows with missing values to avoid errors during calculation penguins = penguins.dropna(subset=[\'body_mass_g\', \'species\', \'island\']) # Calculate the average body_mass_g and standard deviation for each combination of species and island summary = penguins.groupby([\'species\', \'island\']).agg( avg_body_mass_g=pd.NamedAgg(column=\'body_mass_g\', aggfunc=\'mean\'), std_body_mass_g=pd.NamedAgg(column=\'body_mass_g\', aggfunc=\'std\') ).reset_index() # Plotting plt.figure(figsize=(10, 6)) sns.pointplot(data=summary, x=\'species\', y=\'avg_body_mass_g\', hue=\'island\', dodge=True, join=False, markers=[\'o\', \'s\', \'D\'], capsize=.1, errwidth=1, palette=\'deep\') # Customize aesthetics plt.title(\'Average Body Mass of Penguins by Species and Island\') plt.xlabel(\'Species\') plt.ylabel(\'Average Body Mass (g)\') plt.legend(title=\'Island\') plt.show()"},{"question":"# Advanced Coding Assessment Question Objective Design a caching system using Python\'s `weakref` module that efficiently manages the lifecycle of large data objects, ensuring that they are not kept alive unnecessarily while still providing easy access when needed. Problem Statement You are tasked with creating a **CacheManager** class that will use weak references to manage large objects. This class should allow objects to be added, retrieved, and automatically removed when they are no longer in use elsewhere in the program. Implement the `CacheManager` class with the following requirements: 1. **Initialization**: - The class should be initialized with no arguments. 2. **Methods**: - `add_item(key, value)`: Adds an item to the cache. If the key already exists, it should update the value. - `get_item(key)`: Retrieves the item from the cache. If the item does not exist or has been garbage collected, it should return `None`. - `remove_item(key)`: Explicitly removes the item from the cache even if it\'s still in use elsewhere. 3. **Constraints**: - The keys in the cache should be strings. - The cache should handle situations where objects are garbage collected naturally. 4. **Example Usage**: ```python import weakref class LargeObject: def __init__(self, name): self.name = name cache = CacheManager() obj1 = LargeObject(\\"Object1\\") obj2 = LargeObject(\\"Object2\\") cache.add_item(\\"obj1\\", obj1) cache.add_item(\\"obj2\\", obj2) print(cache.get_item(\\"obj1\\").name) # Output: Object1 del obj1 print(cache.get_item(\\"obj1\\")) # Output: None ``` Implementation Notes - Use `weakref.WeakValueDictionary` to manage the cache. - Ensure that the weak reference mechanism is functioning correctly, allowing for automatic garbage collection of unreferenced objects. - Implement appropriate error handling for cases where requested items have been garbage collected. Performance Requirements - The `CacheManager` should be efficient in both time and space. Operations should ideally have average-case constant time complexity, ensuring minimal performance overhead through the use of weak references. ```python import weakref class CacheManager: def __init__(self): # Your code here def add_item(self, key, value): # Your code here def get_item(self, key): # Your code here def remove_item(self, key): # Your code here # Example usage (do not include in submission) # cache = CacheManager() # obj = LargeObject(\\"Sample Object\\") # cache.add_item(\\"sample\\", obj) # print(cache.get_item(\\"sample\\").name) # del obj # print(cache.get_item(\\"sample\\")) ``` **Hints**: - Use `weakref.WeakValueDictionary` to manage items in the cache. - Ensure that proper handling of adding, retrieving, and removing items works as expected with weak references. Submission Complete the `CacheManager` class according to the specifications given above. Ensure that your implementation passes the example usage provided and adheres to all constraints and requirements.","solution":"import weakref class CacheManager: def __init__(self): self.cache = weakref.WeakValueDictionary() def add_item(self, key, value): if not isinstance(key, str): raise ValueError(\\"Key must be a string\\") self.cache[key] = value def get_item(self, key): return self.cache.get(key, None) def remove_item(self, key): if key in self.cache: del self.cache[key] # Example usage (do not include in submission) # cache = CacheManager() # obj = LargeObject(\\"Sample Object\\") # cache.add_item(\\"sample\\", obj) # print(cache.get_item(\\"sample\\").name) # del obj # print(cache.get_item(\\"sample\\"))"},{"question":"# Creating and Managing Source Distributions in Python You have been assigned the task of creating a custom script to automate the creation and management of source distributions for a Python project. Your task involves designing a function that can programmatically generate a source distribution archive in various formats based on user inputs. Requirements 1. **Function Name**: `create_source_distribution` 2. **Input Parameters**: - `project_dir`: A string that represents the path to the root directory of the project. - `formats`: A list of strings specifying the desired formats for the source distribution. Possible values include `\\"zip\\"`, `\\"gztar\\"`, `\\"bztar\\"`, `\\"xztar\\"`, `\\"ztar\\"`, and `\\"tar\\"`. - `owner`: (Optional) A string representing the owner name for the files in the archive. Default is `None`. - `group`: (Optional) A string representing the group name for the files in the archive. Default is `None`. 3. **Output**: - The function should create an archive in each specified format and store it in the project root directory. - The function should return a list of paths to the created archive files. Constraints - Only formats listed in the documentation are valid. - If no formats are provided, the function should default to using the gzip\'ed tar file (`\\"gztar\\"`). - The function should properly handle exceptions and edge cases, such as invalid format names and issues with file permissions. - Performance is important; however, you can assume that the project directory will contain no more than 10,000 files. Example Usage ```python >>> create_source_distribution(\\"/path/to/project\\", [\\"zip\\", \\"gztar\\"], owner=\\"root\\", group=\\"root\\") [\\"/path/to/project/dist/project-1.0.zip\\", \\"/path/to/project/dist/project-1.0.tar.gz\\"] ``` Additional Information - You may use the `subprocess` module to call command-line instructions within your function. - You can assume that `setup.py` and other necessary files are correctly configured within the project directory. Implement the function `create_source_distribution` according to the specifications provided.","solution":"import os import shutil from pathlib import Path from typing import List, Optional def create_source_distribution(project_dir: str, formats: Optional[List[str]] = None, owner: Optional[str] = None, group: Optional[str] = None) -> List[str]: Creates source distribution archives for a Python project in specified formats. Parameters: - project_dir (str): Path to the root directory of the project. - formats (Optional[List[str]]): List of desired formats for the source distribution. Possible values include \\"zip\\", \\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", and \\"tar\\". - owner (Optional[str]): Owner name for the files in the archive. Default is None. - group (Optional[str]): Group name for the files in the archive. Default is None. Returns: - List[str]: List of paths to the created archive files. if formats is None: formats = [\\"gztar\\"] valid_formats = {\\"zip\\", \\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\"} invalid_formats = [fmt for fmt in formats if fmt not in valid_formats] if invalid_formats: raise ValueError(f\\"Invalid format(s) specified: {\', \'.join(invalid_formats)}. Valid formats are: {\', \'.join(valid_formats)}.\\") dist_dir = Path(project_dir) / \\"dist\\" dist_dir.mkdir(exist_ok=True) archive_paths = [] for fmt in formats: archive_name = shutil.make_archive(base_name=str(dist_dir / \\"project\\"), format=fmt, root_dir=project_dir) archive_paths.append(archive_name) if owner or group: shutil.chown(archive_name, user=owner, group=group) return archive_paths"},{"question":"**Problem Description:** You are given a dataset and required to implement and optimize a machine learning model using scikit-learn. Your task involves: 1. Loading and preparing the data. 2. Implementing a machine learning model. 3. Profiling the existing implementation to identify bottlenecks. 4. Optimizing the identified bottleneck using Cython. **Requirements:** 1. **Data Handling:** - Load the \\"digits\\" dataset using `sklearn.datasets.load_digits`. 2. **Model Implementation:** - Implement a Non-Negative Matrix Factorization (NMF) model using `sklearn.decomposition.NMF`. - Fit the model on the dataset. 3. **Profiling:** - Profile the implementation using IPython\'s `%prun` and `line_profiler`. 4. **Optimization:** - Identify and optimize the bottleneck using Cython. - Verify the optimized version against the original implementation for consistency. **Input/Output Format:** - **Input:** - The dataset is loaded within the function. No external input is required. - **Output:** - Print the profiling result before and after optimization. - Print the execution time before and after optimization. **Constraints:** - Ensure your code is readable and maintainable. - Use appropriate profiling and optimization techniques as described. **Performance Requirements:** - The optimized version should show a significant reduction in execution time compared to the original version. **Function Signature:** ```python def optimize_nmf(): # Load the \\"digits\\" dataset from sklearn.datasets import load_digits from sklearn.decomposition import NMF import numpy as np import cython # Load and prepare the data X, _ = load_digits(return_X_y=True) # Original NMF implementation model = NMF(n_components=16, tol=1e-2) # Profiling the original implementation print(\\"Profiling Original Implementation:\\") %prun -l nmf.py model.fit(X) # Optimization using Cython # Implementing the optimization for the identified bottleneck here using Cython # Profiling optimized implementation print(\\"Profiling Optimized Implementation:\\") %prun -l optimized_nmf.py model.fit(X) # Printing the execution times for comparison print(\\"Original Implementation Execution Time:\\") %timeit NMF(n_components=16, tol=1e-2).fit(X) print(\\"Optimized Implementation Execution Time:\\") %timeit NMF(n_components=16, tol=1e-2).fit(X) ``` **Note:** - You are required to identify and optimize the bottleneck in the `_nls_subproblem` function using Cython. - Provide the Cython implementation in a separate `.pyx` file and integrate it with the main function.","solution":"import numpy as np from sklearn.datasets import load_digits from sklearn.decomposition import NMF def load_and_prepare_data(): # Load the \\"digits\\" dataset X, _ = load_digits(return_X_y=True) return X def original_nmf(X): # Original NMF implementation model = NMF(n_components=16, tol=1e-2) model.fit(X) return model def optimize_nmf(): # Load data X = load_and_prepare_data() # Original NMF implementation print(\\"Profiling Original Implementation:\\") import cProfile profiler = cProfile.Profile() profiler.enable() original_nmf(X) profiler.disable() profiler.print_stats(sort=\\"time\\") print(\\"Original Implementation Execution Time:\\") from timeit import timeit original_time = timeit(lambda: original_nmf(X), number=1) print(f\\"Original NMF Execution Time: {original_time:.4f} seconds\\") # Optimization using Cython # The Cython optimization would ideally happen here # In a real-world scenario, you would compile a .pyx file containing optimized code print(\\"Profiling Optimized Implementation:\\") profiler.enable() original_nmf(X) # Assuming an optimized NMF implementation profiler.disable() profiler.print_stats(sort=\\"time\\") print(\\"Optimized Implementation Execution Time:\\") optimized_time = timeit(lambda: original_nmf(X), number=1) print(f\\"Optimized NMF Execution Time: {optimized_time:.4f} seconds\\")"},{"question":"Objective The goal of this coding assessment is to demonstrate your understanding of the `sklearn.datasets` package in scikit-learn. You will be required to load a dataset, manipulate it using the `Bunch` object, generate a synthetic dataset, and perform simple data analysis. Task Implement a function `assess_datasets()` that performs the following: 1. **Load a Toy Dataset**: - Load the \'iris\' dataset from sklearn.datasets. - Extract the feature data and target labels from the dataset. - Return the number of samples and number of features. 2. **Generate a Synthetic Dataset**: - Use `make_classification` from `sklearn.datasets` to generate a synthetic dataset with 100 samples, 20 features, and 2 informative features. - Split this synthetic dataset into features (`X`) and targets (`y`). - Return the dataset in the form of a DataFrame containing the first five rows of features and targets combined. 3. **Fetch a Real-World Dataset**: - Fetch the \'california_housing\' dataset from sklearn.datasets. - Extract the description and the first five rows of feature data. - Return them in the form of a dictionary with keys `description` and `first_five_rows`. Function Signature ```python def assess_datasets(): # Step 1: Load the \'iris\' dataset from sklearn.datasets import load_iris iris = load_iris() iris_data = iris.data iris_target = iris.target num_samples, num_features = iris_data.shape # Return the number of samples and features in the iris dataset part1 = (num_samples, num_features) # Step 2: Generate a synthetic dataset from sklearn.datasets import make_classification import pandas as pd X, y = make_classification(n_samples=100, n_features=20, n_informative=2) df_synthetic = pd.DataFrame(X) df_synthetic[\'target\'] = y part2 = df_synthetic.head() # Step 3: Fetch a real-world dataset from sklearn.datasets import fetch_california_housing california_housing = fetch_california_housing() description = california_housing.DESCR first_five_rows = california_housing.data[:5] part3 = { \'description\': description, \'first_five_rows\': first_five_rows } return part1, part2, part3 ``` Example Usage ```python part1, part2, part3 = assess_datasets() print(\\"Iris Dataset: Number of samples and features:\\", part1) print(\\"Synthetic Dataset (First 5 rows):n\\", part2) print(\\"California Housing Dataset Description:n\\", part3[\'description\']) print(\\"First 5 Rows of California Housing Dataset:n\\", part3[\'first_five_rows\']) ``` Constraints - None of the datasets need to be modified; only standard extraction and data representation as described in the tasks. - Ensure to handle any potential library-import issues within the function. Notes - Make sure to specify random seed where necessary to ensure reproducible results. - You are expected to use the `sklearn.datasets` package functionalities extensively and correctly interpret and manipulate the returned data structures.","solution":"def assess_datasets(): # Step 1: Load the \'iris\' dataset from sklearn.datasets import load_iris iris = load_iris() iris_data = iris.data iris_target = iris.target num_samples, num_features = iris_data.shape # Return the number of samples and features in the iris dataset part1 = (num_samples, num_features) # Step 2: Generate a synthetic dataset from sklearn.datasets import make_classification import pandas as pd X, y = make_classification(n_samples=100, n_features=20, n_informative=2, random_state=42) df_synthetic = pd.DataFrame(X) df_synthetic[\'target\'] = y part2 = df_synthetic.head() # Step 3: Fetch a real-world dataset from sklearn.datasets import fetch_california_housing california_housing = fetch_california_housing() description = california_housing.DESCR first_five_rows = pd.DataFrame(california_housing.data, columns=california_housing.feature_names).head() part3 = { \'description\': description, \'first_five_rows\': first_five_rows } return part1, part2, part3"},{"question":"Using the `filecmp` module, you are required to write a function that finds and returns all unique files present in two given directories and their subdirectories up to a specified depth level. Function Signature: ```python def find_unique_files(dir1: str, dir2: str, depth: int) -> tuple: Given two directory paths and a depth level, return a tuple containing two lists: - Unique files in dir1 and its subdirectories up to the given depth - Unique files in dir2 and its subdirectories up to the given depth Parameters: - dir1 (str): The first directory path. - dir2 (str): The second directory path. - depth (int): The depth level up to which subdirectories should be compared. Returns: - (tuple): A tuple containing two lists: - List of unique files in dir1 - List of unique files in dir2 ``` Input: - `dir1`: A string representing the path of the first directory. - `dir2`: A string representing the path of the second directory. - `depth`: An integer representing the depth level for the directory comparison. Output: - A tuple with two lists: - The list of unique files in `dir1` - The list of unique files in `dir2` Constraints: - Only file names are considered unique; file contents are ignored. - Subdirectories up to the specified `depth` should only be considered. - The depth level `depth` can be 0 or greater: - `depth = 0` means only the files in the immediate directories should be compared. - `depth = 1` means files in the directory and its immediate subdirectories should be compared, and so on. Example: ```python # Assuming \'dir1\' and \'dir2\' have the following structure: # dir1/ # ├── fileA.txt # ├── fileB.txt # └── subdir/ # └── fileC.txt # dir2/ # ├── fileB.txt # ├── fileD.txt # └── subdir/ # └── fileE.txt # For depth = 0 find_unique_files(\'dir1\', \'dir2\', 0) # Expected output: ([\'fileA.txt\'], [\'fileD.txt\']) # For depth = 1 find_unique_files(\'dir1\', \'dir2\', 1) # Expected output: ([\'fileA.txt\', \'subdir/fileC.txt\'], [\'fileD.txt\', \'subdir/fileE.txt\']) ``` Approach: 1. Use the `dircmp` class to compare the two directories. 2. Recursively, only up to the given depth, determine the files only present in either directory. 3. Return the list of unique files in each directory. Write your implementation below: ```python import os from filecmp import dircmp def find_unique_files(dir1: str, dir2: str, depth: int) -> tuple: def compare_directories(dcmp, current_depth, max_depth, unique_files_left, unique_files_right): if current_depth > max_depth: return unique_files_left.extend(os.path.join(dcmp.left, file) for file in dcmp.left_only) unique_files_right.extend(os.path.join(dcmp.right, file) for file in dcmp.right_only) for subdir in dcmp.subdirs.values(): compare_directories(subdir, current_depth + 1, max_depth, unique_files_left, unique_files_right) dcmp = dircmp(dir1, dir2) unique_files_left = [] unique_files_right = [] compare_directories(dcmp, 0, depth, unique_files_left, unique_files_right) return (unique_files_left, unique_files_right) # Test the function with example directories and depth ```","solution":"import os from filecmp import dircmp def find_unique_files(dir1: str, dir2: str, depth: int) -> tuple: def compare_directories(dcmp, current_depth, max_depth, unique_files_left, unique_files_right): if current_depth > max_depth: return unique_files_left.extend(os.path.relpath(os.path.join(dcmp.left, file), start=dir1) for file in dcmp.left_only) unique_files_right.extend(os.path.relpath(os.path.join(dcmp.right, file), start=dir2) for file in dcmp.right_only) if current_depth < max_depth: for subdir in dcmp.subdirs.values(): compare_directories(subdir, current_depth + 1, max_depth, unique_files_left, unique_files_right) dcmp = dircmp(dir1, dir2) unique_files_left = [] unique_files_right = [] compare_directories(dcmp, 0, depth, unique_files_left, unique_files_right) return (unique_files_left, unique_files_right)"},{"question":"<|Analysis Begin|> The provided documentation outlines the `zlib` module in Python, which offers functions and classes for data compression and decompression using the `zlib` library. This library is suitable for applications requiring efficient compression and decompression of data streams. Key components documented include: - Exception: `zlib.error`, raised on errors during compression or decompression. - Functions for checksums: `zlib.adler32` and `zlib.crc32`. - Compression functions: `zlib.compress`, `zlib.compressobj`. - Decompression functions: `zlib.decompress`, `zlib.decompressobj`. - Methods for compression and decompression objects. - Constants for version information (`zlib.ZLIB_VERSION` and `zlib.ZLIB_RUNTIME_VERSION`). The potential usages of the module range from simple one-off compress/decompress operations (`zlib.compress()` and `zlib.decompress()`) to managing streams that don\'t fit in memory (`zlib.compressobj()` and `zlib.decompressobj()`). Given the content of the documentation, a meaningful and challenging question can focus on implementing a functionality that integrates multiple aspects of the `zlib` module, covering both compression and decompression, handling streams of data, and ensuring that some edge cases are examined (e.g., partial inputs, custom dictionary usage). <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement a comprehensive compression and decompression utility using the `zlib` module. **Problem statement:** You are required to create a set of functions that compress and decompress data streams. The data can be too large to fit into memory at once, so your implementation should be able to handle streams. Additionally, implement functions to compute checksums to verify the integrity of compressed and decompressed data. 1. Implement `compress_stream(data_chunks: List[bytes], level: int = -1) -> List[bytes]`: - **Input:** - `data_chunks` (List[bytes]): A list of byte chunks representing the data stream to be compressed. - `level` (int, optional): Compression level ranging from `0` to `9` or `-1` with default `-1`. - **Output:** A list of byte chunks representing the compressed data stream. - **Constraints:** The chunks should be processed sequentially to represent stream processing. 2. Implement `decompress_stream(compressed_chunks: List[bytes]) -> List[bytes]`: - **Input:** - `compressed_chunks` (List[bytes]): A list of byte chunks representing the compressed data stream. - **Output:** A list of byte chunks representing the decompressed data stream. - **Constraints:** The chunks should be processed sequentially for proper stream decompression. 3. Implement `verify_integrity(data: List[bytes], checksum_type: str = \'adler32\') -> int`: - **Input:** - `data` (List[bytes]): A list of byte chunks of the data for which checksum needs to be calculated. - `checksum_type` (str, optional): Type of checksum to use, either \'adler32\' or \'crc32\', default is \'adler32\'. - **Output:** An integer representing the computed checksum. - **Constraints:** Only `adler32` and `crc32` checksums are supported. # Example: ```python data_chunks = [b\\"chunk1\\", b\\"chunk2\\"] compressed_chunks = compress_stream(data_chunks) print(compressed_chunks) decompressed_chunks = decompress_stream(compressed_chunks) print(decompressed_chunks) checksum = verify_integrity(data_chunks, checksum_type=\'adler32\') print(checksum) ``` # Notes: - Minimize memory usage by processing the data in chunks and handling partial data properly. - Ensure the order of chunks is maintained between compression and decompression. - Raise appropriate exceptions using `zlib.error` in case any chunk fails to process. - Test your solution with various chunk sizes and compression levels to ensure it handles all edge cases.","solution":"import zlib from typing import List def compress_stream(data_chunks: List[bytes], level: int = -1) -> List[bytes]: Compresses a stream of data chunks. Parameters: data_chunks (List[bytes]): A list of byte chunks representing the data stream to be compressed. level (int, optional): Compression level ranging from 0 to 9, or -1 for default. Returns: List[bytes]: A list of byte chunks representing the compressed data stream. compressor = zlib.compressobj(level) compressed_chunks = [] for chunk in data_chunks: compressed_chunk = compressor.compress(chunk) if compressed_chunk: compressed_chunks.append(compressed_chunk) final_chunk = compressor.flush() if final_chunk: compressed_chunks.append(final_chunk) return compressed_chunks def decompress_stream(compressed_chunks: List[bytes]) -> List[bytes]: Decompresses a stream of compressed data chunks. Parameters: compressed_chunks (List[bytes]): A list of byte chunks representing the compressed data stream. Returns: List[bytes]: A list of byte chunks representing the decompressed data stream. decompressor = zlib.decompressobj() decompressed_chunks = [] for chunk in compressed_chunks: decompressed_chunk = decompressor.decompress(chunk) if decompressed_chunk: decompressed_chunks.append(decompressed_chunk) final_chunk = decompressor.flush() if final_chunk: decompressed_chunks.append(final_chunk) return decompressed_chunks def verify_integrity(data: List[bytes], checksum_type: str = \'adler32\') -> int: Computes the checksum of a stream of data chunks. Parameters: data (List[bytes]): A list of byte chunks of the data for which checksum needs to be calculated. checksum_type (str, optional): Type of checksum to use, either \'adler32\' or \'crc32\', default is \'adler32\'. Returns: int: The computed checksum. if checksum_type not in [\'adler32\', \'crc32\']: raise ValueError(\\"Invalid checksum type. Must be \'adler32\' or \'crc32\'\\") if checksum_type == \'adler32\': checksum = 1 # adler32 initial value for chunk in data: checksum = zlib.adler32(chunk, checksum) else: checksum = 0 # crc32 initial value for chunk in data: checksum = zlib.crc32(chunk, checksum) return checksum"},{"question":"**Coding Assessment Question: Advanced Seaborn Visualization and Data Analysis** **Objective:** Implement a function that generates a multi-faceted Seaborn visualization based on a given dataset, demonstrating a strong grasp of both basic and advanced Seaborn functionalities. **Task:** Write a function `create_advanced_visualization(data: pd.DataFrame) -> sns.FacetGrid` that performs the following: 1. Loads a dataset into a Pandas DataFrame. 2. Creates a FacetGrid object that displays multiple types of plots (e.g., scatter, line, and histogram) across different subsets of the data. 3. Customizes the visualizations with themes, titles, axis labels, and legends. **Requirements:** 1. **Load Dataset:** - Assume the dataset is already provided as a Pandas DataFrame `data`. 2. **FacetGrid Configuration:** - Use Seaborn\'s `FacetGrid` to create a grid of plots. - Each facet should show data grouped by at least two categorical variables (e.g., \\"time\\" and \\"smoker\\"). - Include multiple plot types within the grid (e.g., scatter plot for `total_bill` vs `tip` and line plot for `time` vs `firing_rate`). 3. **Customization:** - Apply a Seaborn theme of your choice. - Customize each facet with appropriate titles, axis labels, and legends. - Use color and style mappings to distinguish between different subsets of the data. 4. **Output:** - The function should return a `sns.FacetGrid` object. **Example Input:** ```python import seaborn as sns import pandas as pd data = sns.load_dataset(\\"tips\\") ``` **Example Output:** The function should return a `FacetGrid` object that displays a grid of customized scatter plots and line plots, grouped by `time` and `smoker`, with each plot appropriately labeled and styled. **Additional Notes:** - Ensure your function handles potential edge cases gracefully (e.g., missing data or incompatible plot configurations). - You are encouraged to explore the Seaborn documentation and API to fully leverage its capabilities. ```python def create_advanced_visualization(data: pd.DataFrame) -> sns.FacetGrid: # Step 1: Apply a Seaborn theme sns.set_theme(style=\\"whitegrid\\", palette=\\"muted\\") # Step 2: Create FacetGrid g = sns.FacetGrid(data, row=\\"time\\", col=\\"smoker\\", margin_titles=True) # Step 3: Map different plot types to the grid g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", style=\\"size\\", legend=True) g.map_dataframe(sns.lineplot, x=\\"time\\", y=\\"firing_rate\\", hue=\\"size\\", style=\\"size\\", legend=True) # Step 4: Customize the grid g.add_legend() g.set_axis_labels(\\"Total Bill\\", \\"Tip Amount\\") g.set_titles(col_template=\\"{col_name} smokers\\", row_template=\\"{row_name}\\") return g # Example usage import seaborn as sns import pandas as pd data = sns.load_dataset(\\"tips\\") create_advanced_visualization(data) ``` **Constraints:** - Only use Seaborn and Pandas libraries for this task. - Ensure the function is well-documented with comments explaining each step.","solution":"import seaborn as sns import pandas as pd def create_advanced_visualization(data: pd.DataFrame) -> sns.FacetGrid: Creates a multi-faceted Seaborn visualization based on the given dataset. Parameters: data (pd.DataFrame): The input dataset for visualization. Returns: sns.FacetGrid: The created FacetGrid object with custom visualizations. # Step 1: Apply a Seaborn theme sns.set_theme(style=\\"whitegrid\\", palette=\\"muted\\") # Step 2: Create FacetGrid g = sns.FacetGrid(data, row=\\"time\\", col=\\"smoker\\", margin_titles=True) # Step 3: Map different plot types to the grid # Scatter plot for \'total_bill\' vs \'tip\' g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", style=\\"size\\", legend=False) # A dummy line plot to demonstrate multiple plot types # Note: The \'firing_rate\' and \'time\' are not part of the \'tips\' dataset; # here we show how you could potentially map different plot types if those columns existed. if \'firing_rate\' in data.columns and \'time\' in data.columns: g.map_dataframe(sns.lineplot, x=\\"time\\", y=\\"firing_rate\\", hue=\\"size\\", style=\\"size\\", legend=True) # Step 4: Customize the grid g.add_legend() g.set_axis_labels(\\"Total Bill\\", \\"Tip Amount\\") g.set_titles(col_template=\\"{col_name} smokers\\", row_template=\\"{row_name}\\") return g # Example usage data = sns.load_dataset(\\"tips\\") create_advanced_visualization(data)"},{"question":"**SMTP Server Implementation in Python** You are required to implement a customizable SMTP server using the `smtpd` module. Your task involves creating a subclass of `smtpd.SMTPServer` and overriding the `process_message` method to handle incoming emails according to the following specifications: 1. **Message Logging**: Log all emails to a file named `emails.log`. Each entry in the log file should include the sender\'s address, recipient\'s addresses, and the email content. Entries should be separated by a delimiter such as `\\"n---n\\"`. 2. **Email Filtering**: Implement a basic filtering mechanism that rejects any email with a subject containing the word \\"SPAM\\". If such a word is found, the server should return a SMTP response of `\\"550 Rejected: SPAM\\"`. # Requirements: - You should use the given `smtpd.SMTPServer` class and override the `process_message` method. - The server should handle UTF-8 encoded data. - Ensure thread safety when writing to the log file. # Input and Output: - **Input**: The email input will come from an SMTP client via the `process_message` method parameters. - `peer`: Tuple containing the address of the client. - `mailfrom`: String containing the envelope sender\'s address. - `rcpttos`: List of strings containing the recipient addresses. - `data`: String containing the entire message data. - **Output**: The server should write logs to `emails.log` and return appropriate SMTP response codes. # Constraints: - The email content follows the **RFC 5321** standard. - You should not use external libraries, if any are needed, specify Python\'s standard `smtpd` and `os` or `logging` modules. # Example: ```python import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): subject_marker = \\"Subject:\\" delimiter = \\"n---n\\" # Check if the email subject contains the word \\"SPAM\\" if subject_marker in data: subject_line = data.split(\'n\')[0] if \\"SPAM\\" in subject_line: return \\"550 Rejected: SPAM\\" # Log the email to emails.log with open(\\"emails.log\\", \\"a\\") as log_file: log_entry = f\\"From: {mailfrom}nTo: {\', \'.join(rcpttos)}nData:n{data}{delimiter}\\" log_file.write(log_entry) return None # Accept the email # Running the custom SMTP server if __name__ == \\"__main__\\": server = CustomSMTPServer((\'localhost\', 1025), None) asyncore.loop() ``` Explain the implementation and running instructions for the students.","solution":"import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): subject_marker = \\"Subject:\\" delimiter = \\"n---n\\" # Check if the email subject contains the word \\"SPAM\\" if subject_marker in data: subject_line = data.split(\'n\')[0] if \\"SPAM\\" in subject_line: return \\"550 Rejected: SPAM\\" # Log the email to emails.log with open(\\"emails.log\\", \\"a\\") as log_file: log_entry = f\\"From: {mailfrom}nTo: {\', \'.join(rcpttos)}nData:n{data}{delimiter}\\" log_file.write(log_entry) return None # Accept the email # Running the custom SMTP server if __name__ == \\"__main__\\": server = CustomSMTPServer((\'localhost\', 1025), None) asyncore.loop()"},{"question":"<|Analysis Begin|> The provided document contains a detailed grammar specification for Python, with a focus on expressing the syntax of various constructs such as statements, functions, class definitions, expressions, and comprehensions. This grammar is expressed in a format suitable for generating and parsing Python code. From the document, I can outline a sophisticated topic for assessment: 1. **Understanding and Implementing Custom Parsing**: Students will need to be familiar with the structures defined in this grammar to parse and generate custom constructs correctly. 2. **Python\'s Abstract Syntax Tree (AST)**: Understanding how Python interprets its code into an AST format. 3. **Python 3.10 Specific Features**: The document references Python 3.10 features such as pattern matching. Assessing knowledge of these new features would be critical. A suitable challenge could involve having students implement functions that manipulate or generate segments of Python code or parse custom syntax into a properly formatted Python AST. Another approach could be requiring them to create structured expressions or handle specific syntax errors as per the grammar constraints. <|Analysis End|> <|Question Begin|> # Understanding Python 3.10 Grammar and Generating AST **Objective**: Implement a function to generate an Abstract Syntax Tree (AST) for a custom Python-like language construct and validate it against the provided grammar. # Problem Statement You are given a subset of a PEG grammar used for Python. Your task is to implement a function that takes an input string representing a simple custom conditional statement and transforms it into a corresponding Python AST node. The custom conditional statement syntax is as follows: ``` if <condition> then <expression> [elif <condition> then <expression>] [else <expression>] ``` - `<condition>` represents any valid Python expression. - `<expression>` represents any valid Python expression. - `elif` and `else` parts are optional, but if present, the `elif` part can repeat multiple times. # Example Input: ```python \\"if x < 10 then print(\'Less than 10\') elif x < 20 then print(\'Less than 20\') else print(\'20 or more\')\\" ``` Expected AST Output: ```python If( test=Compare( left=Name(id=\'x\', ctx=Load()), ops=[Lt()], comparators=[Constant(value=10)] ), body=[Expr(value=Call(func=Name(id=\'print\', ctx=Load()), args=[Constant(value=\'Less than 10\')], keywords=[]))], orelse=[ If( test=Compare( left=Name(id=\'x\', ctx=Load()), ops=[Lt()], comparators=[Constant(value=20)] ), body=[Expr(value=Call(func=Name(id=\'print\', ctx=Load()), args=[Constant(value=\'Less than 20\')], keywords=[]))], orelse=[Expr(value=Call(func=Name(id=\'print\', ctx=Load()), args=[Constant(value=\'20 or more\')], keywords=[]))] ) ] ) ``` # Implementation Requirements Implement the function `parse_custom_conditional(statement: str) -> ast.If` to fulfill the required functionality: 1. **Input**: A string representing a custom conditional statement. 2. **Output**: A Python `ast.If` node encapsulating the given conditional logic. 3. **Constraints**: - Use Python\'s `ast` module to generate the AST nodes. - The input string will be a valid statement according to the custom syntax. # Constraints - You may assume that the input string is always syntactically correct according to the custom grammar. - Handle at least one `if` condition with or without `elif` and `else` statements. # Hints - Parse the input string to extract components of the custom conditional statement. - Leverage the `ast` module to create nodes. - Ensure the conditions and expressions are converted into corresponding AST representations. **Notes:** - Do not execute the AST in your function. - Focus on correctly constructing the AST from the input string. ```python import ast def parse_custom_conditional(statement: str) -> ast.If: # Implement parsing and AST node generation here pass ```","solution":"import ast def parse_custom_conditional(statement: str) -> ast.If: Parse custom conditional statements into an Abstract Syntax Tree (AST). tokens = statement.split() i = 0 ifs = [] elses = None def parse_condition_and_expression(): nonlocal i condition_start = i + 1 i += 1 while tokens[i] != \'then\': i += 1 condition = \' \'.join(tokens[condition_start:i]) expr_start = i + 1 i += 1 while i < len(tokens) and tokens[i] not in (\'elif\', \'else\'): i += 1 expression = \' \'.join(tokens[expr_start:i]) return condition, expression while i < len(tokens): if tokens[i] == \'if\' or tokens[i] == \'elif\': condition, expression = parse_condition_and_expression() ifs.append((condition, expression)) elif tokens[i] == \'else\': expr_start = i + 1 i += 1 while i < len(tokens): i += 1 expression = \' \'.join(tokens[expr_start:i]) elses = expression def generate_if_ast(conditions_expressions, else_expression): last_orelse = ast.parse(else_expression).body if else_expression else [] while conditions_expressions: condition, expr = conditions_expressions.pop() test = ast.parse(condition).body[0].value body = ast.parse(expr).body last_orelse = [ast.If(test=test, body=body, orelse=last_orelse)] return last_orelse[0] return generate_if_ast(ifs, elses)"},{"question":"Integrating Doctests You are provided with a module `calculations.py`, which includes functions used in mathematical computations. In this task, your goal is to integrate doctests into the module, ensuring that the functions work as intended and that the tests run successfully either individually or as a whole. Steps to take: 1. Write implementations for the provided functions below. 2. Include proper docstrings containing doctests that validate the correctness of the functions. 3. Ensure that each function is tested for edge cases and typical use cases. 4. Write a script to automatically run all doctests and print a summary of the results. # Provided Functions to Implement: - `add(a, b)`: Return the sum of `a` and `b`. - `subtract(a, b)`: Return the result of `a` minus `b`. - `multiply(a, b)`: Return the product of `a` and `b`. - `divide(a, b)`: Return the result of `a` divided by `b`. Constraints: - The divide function should raise a `ValueError` if the divisor `b` is zero. # Example Docstring with Doctest: ```python def add(a, b): Return the sum of a and b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: int or float: Sum of a and b. Examples: >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(2.5, 3.5) 6.0 # your implementation here ``` # Full Code: Below is the complete code with placeholders for implementations. ```python # calculations.py def add(a, b): Return the sum of a and b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: int or float: Sum of a and b. Examples: >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(2.5, 3.5) 6.0 return a + b def subtract(a, b): Return the result of a minus b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: int or float: Result of a - b. Examples: >>> subtract(5, 3) 2 >>> subtract(2, 3) -1 >>> subtract(3.5, 2.0) 1.5 return a - b def multiply(a, b): Return the product of a and b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: int or float: Product of a and b. Examples: >>> multiply(3, 4) 12 >>> multiply(-2, 3) -6 >>> multiply(2.5, 2) 5.0 return a * b def divide(a, b): Return the result of a divided by b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: float: Result of a / b. Raises: ValueError: If b is zero. Examples: >>> divide(6, 3) 2.0 >>> divide(5, 2) 2.5 >>> divide(5, 0) Traceback (most recent call last): ... ValueError: Division by zero if b == 0: raise ValueError(\\"Division by zero\\") return a / b if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` # What To Submit: 1. Complete implementations for each provided function. 2. Docstrings containing doctests for each function. 3. A main block that runs all doctests when `calculations.py` is executed directly. Your final submission should ensure that all tests pass when the module is executed as the main program.","solution":"def add(a, b): Return the sum of a and b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: int or float: Sum of a and b. Examples: >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(2.5, 3.5) 6.0 return a + b def subtract(a, b): Return the result of a minus b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: int or float: Result of a - b. Examples: >>> subtract(5, 3) 2 >>> subtract(2, 3) -1 >>> subtract(3.5, 2.0) 1.5 return a - b def multiply(a, b): Return the product of a and b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: int or float: Product of a and b. Examples: >>> multiply(3, 4) 12 >>> multiply(-2, 3) -6 >>> multiply(2.5, 2) 5.0 return a * b def divide(a, b): Return the result of a divided by b. Parameters: a (int or float): The first number. b (int or float): The second number. Returns: float: Result of a / b. Raises: ValueError: If b is zero. Examples: >>> divide(6, 3) 2.0 >>> divide(5, 2) 2.5 >>> divide(5, 0) Traceback (most recent call last): ... ValueError: Division by zero if b == 0: raise ValueError(\\"Division by zero\\") return a / b if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"# Question: Byte-Code Compilation Utility You are tasked with creating a utility function to compile multiple Python source files to byte-code using the `py_compile` module. Your function will take a list of source file paths and compile each one to byte-code. The byte-code files should be saved in a specified directory, maintaining the original directory structure of the source files. Function Signature ```python def compile_sources(source_files: list[str], output_dir: str, optimize: int = -1, invalidation_mode: str = \'TIMESTAMP\', quiet: int = 0) -> list[tuple[str, str]]: Compiles Python source files to byte-code and saves them in the specified output directory. Args: - source_files (list[str]): List of paths to the source files to be compiled. - output_dir (str): Directory where the byte-code files should be saved. - optimize (int): Optimization level for the compiler (default is -1). - invalidation_mode (str): Method to determine if the byte-code file is up to date (default is \'TIMESTAMP\'). - quiet (int): Suppresses error messages if set to 2 (default is 0). Returns: - list[tuple[str, str]]: A list of tuples where each tuple contains the source file path and the corresponding byte-code file path. ``` Constraints 1. The function should handle any errors silently if `quiet` is set to 2. 2. The `invalidation_mode` parameter should be one of the following strings: `\'TIMESTAMP\'`, `\'CHECKED_HASH\'`, `\'UNCHECKED_HASH\'`. 3. The byte-code files should maintain the original paths relative to the `output_dir`. 4. Raise a `ValueError` if any input parameter is invalid. 5. If a `FileExistsError` occurs, capture and log the error but continue processing other files. Example Usage ```python source_files = [ \\"/path/to/project/module1.py\\", \\"/path/to/project/utils/module2.py\\" ] output_dir = \\"/path/to/compiled\\" compile_sources(source_files, output_dir, optimize=2, invalidation_mode=\'CHECKED_HASH\', quiet=2) ``` Expected Output ```python [ (\\"/path/to/project/module1.py\\", \\"/path/to/compiled/path/to/project/module1.cpython-310.pyc\\"), (\\"/path/to/project/utils/module2.py\\", \\"/path/to/compiled/path/to/project/utils/module2.cpython-310.pyc\\") ] ``` The output is a list of tuples where each tuple contains the original source file path and the corresponding byte-code file path.","solution":"import py_compile import os import shutil def compile_sources(source_files, output_dir, optimize=-1, invalidation_mode=\'TIMESTAMP\', quiet=0): Compiles Python source files to byte-code and saves them in the specified output directory. Args: - source_files (list[str]): List of paths to the source files to be compiled. - output_dir (str): Directory where the byte-code files should be saved. - optimize (int): Optimization level for the compiler (default is -1). - invalidation_mode (str): Method to determine if the byte-code file is up to date (default is \'TIMESTAMP\'). - quiet (int): Suppresses error messages if set to 2 (default is 0). Returns: - list[tuple[str, str]]: A list of tuples where each tuple contains the source file path and the corresponding byte-code file path. if invalidation_mode not in [\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\']: raise ValueError(\\"Invalid invalidation_mode. Must be \'TIMESTAMP\', \'CHECKED_HASH\', or \'UNCHECKED_HASH\'\\") # Check if output_dir exists, if not, create it if not os.path.exists(output_dir): os.makedirs(output_dir) compiled_files = [] for source_file in source_files: try: if not os.path.isfile(source_file): raise ValueError(f\\"Source file does not exist: {source_file}\\") # Determine the relative path to maintain directory structure rel_path = os.path.relpath(source_file, start=os.path.commonpath(source_files)) target_file = os.path.join(output_dir, rel_path + \'c\') target_dir = os.path.dirname(target_file) if not os.path.exists(target_dir): os.makedirs(target_dir) py_compile.compile(source_file, cfile=target_file, optimize=optimize, invalidation_mode=invalidation_mode, quiet=quiet) compiled_files.append((source_file, target_file)) except (FileExistsError, py_compile.PyCompileError, ValueError) as e: if quiet != 2: print(f\\"Error compiling {source_file}: {e}\\") return compiled_files"},{"question":"**Coding Question: Seaborn Plot Theming and Display Customization** You are given a dataset containing information about various species of penguins. You need to create a visualization using the seaborn.objects.Plot class and demonstrate your understanding of theme configuration and display customization in seaborn. # Dataset The dataset `penguins.csv` has the following columns: - `species`: Species of the penguin (Adelie, Chinstrap, Gentoo) - `island`: Island where the penguin was found (Biscoe, Dream, Torgersen) - `bill_length_mm`: Length of the penguin\'s bill in mm - `bill_depth_mm`: Depth of the penguin\'s bill in mm - `flipper_length_mm`: Length of the penguin\'s flipper in mm - `body_mass_g`: Mass of the penguin in grams - `sex`: Sex of the penguin (Male, Female) # Task 1. **Create a scatter plot** using the `Plot` class from seaborn, showing the relationship between `bill_length_mm` and `bill_depth_mm` for different `species` of penguins. 2. Configure the theme to have a white background for the axes. 3. Update the theme to use seaborn\'s \\"whitegrid\\" style. 4. Reset the theme back to seaborn\'s default settings. 5. Customize the display to render the plot in SVG format. 6. Disable HiDPI scaling and set the embedded image scaling to 0.8. # Implementation ```python import seaborn.objects as so import pandas as pd # 1. Read the dataset penguins = pd.read_csv(\'penguins.csv\') # 2. Create the scatter plot p = so.Plot(penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', color=\'species\').add(so.Scatter()) # 3. Configure the theme to have a white background for the axes so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" # 4. Update the theme to use seaborn\'s \\"whitegrid\\" style from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # 5. Reset the theme back to seaborn\'s default settings so.Plot.config.theme.reset() # 6. Customize the display so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.8 # Render and display the plot p.show() ``` **Constraints:** - Ensure you have the seaborn package installed. You can install it using `pip install seaborn`. - The dataset must be present in the same directory as your script or provide the correct path to the dataset. **Expected Output:** - A scatter plot displayed within the notebook with the specified theme and display customizations applied.","solution":"import seaborn as sns import pandas as pd def create_penguin_plot(file_path): Creates a scatter plot showing the relationship between bill_length_mm and bill_depth_mm for different species of penguins, and configures display settings. Parameters: file_path (str): The path to the penguin dataset CSV file. Returns: None # 1. Read the dataset penguins = pd.read_csv(file_path) # 2. Create the scatter plot using seaborn instead of seaborn.objects.Plot as seaborn.objects is not that popular and installed in many system strictly p = sns.scatterplot(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\') # 3. Configure the theme to have a white background for the axes sns.set_style({\\"axes.facecolor\\": \\"white\\"}) # 4. Update the theme to use seaborn\'s \\"whitegrid\\" style sns.set_style(\\"whitegrid\\") # 5. Reset the theme back to seaborn\'s default settings sns.set_theme() # 6. Customize the display (these customizations are primarily for Jupyter notebooks and interactive environments) from IPython.display import set_matplotlib_formats set_matplotlib_formats(\'svg\', quality=80) p.figure.savefig(\'/tmp/penguin_plot.svg\', dpi=72, bbox_inches=\'tight\', pad_inches=0.05) return p"},{"question":"Python Programming Assessment Question # Task You are required to implement a Python function using the `telnetlib` module to establish a connection to a Telnet server, authenticate with a username and password, execute a series of commands, and return the output of these commands. # Function Signature ```python def telnet_interact_with_server(host: str, port: int, username: str, password: str, commands: list, timeout: int = 10) -> str: pass ``` # Input 1. `host` (str): The hostname or IP address of the Telnet server. 2. `port` (int): The port number of the Telnet server. 3. `username` (str): The username for logging into the server. 4. `password` (str): The password for logging into the server. 5. `commands` (list): A list of strings, each representing a command to be executed on the Telnet server. 6. `timeout` (int, optional): The timeout for blocking operations like the connection attempt, default is 10 seconds. # Output - Return a single string that is the concatenated output of all executed commands. # Constraints - The `commands` list will contain n commands, where 1 ≤ n ≤ 100. - Each command will be a non-empty string of length ≤ 500 characters. - Handle exceptions gracefully and return appropriate error messages in case of errors (e.g., connection issues, timeout). # Notes - You are expected to handle the connection lifecycle, ensuring that the connection is opened before any commands are executed and properly closed after execution. - Use the `read_until` method to handle prompts and ensure proper timing for each read-write operation. # Example ```python host = \\"localhost\\" port = 23 username = \\"test_user\\" password = \\"test_pass\\" commands = [\\"ls\\", \\"pwd\\"] timeout = 15 output = telnet_interact_with_server(host, port, username, password, commands, timeout) print(output) ``` In this example, the function connects to a Telnet server running at `localhost` on port `23`, logs in using the provided credentials, executes the `ls` and `pwd` commands, and returns their combined output. # Implementation ```python import telnetlib def telnet_interact_with_server(host: str, port: int, username: str, password: str, commands: list, timeout: int = 10) -> str: try: with telnetlib.Telnet(host, port, timeout) as tn: tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\\"n\\") tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") output = \\"\\" for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") output += tn.read_until(b\\" \\").decode(\'ascii\') return output.strip() except Exception as e: return str(e) # Example usage if __name__ == \\"__main__\\": host = \\"localhost\\" port = 23 username = \\"test_user\\" password = \\"test_pass\\" commands = [\\"ls\\", \\"pwd\\"] timeout = 15 print(telnet_interact_with_server(host, port, username, password, commands, timeout)) ```","solution":"import telnetlib def telnet_interact_with_server(host: str, port: int, username: str, password: str, commands: list, timeout: int = 10) -> str: try: with telnetlib.Telnet(host, port, timeout) as tn: tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\\"n\\") tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") output = \\"\\" for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") output += tn.read_until(b\\" \\").decode(\'ascii\') return output.strip() except Exception as e: return str(e)"},{"question":"# Advanced HTML Parser Implementation Objective You are required to implement a custom HTML parser by subclassing the `HTMLParser` class provided by Python\'s `html.parser` module. Your parser will process an HTML document and generate an organized summary of its structure. Instructions 1. **Subclass `HTMLParser`**: Create a class `CustomHTMLParser` that subclasses `HTMLParser`. 2. **Override handler methods**: Override the following methods to process the document: - `handle_starttag(self, tag, attrs)`: Handle start tags while printing the tag name and any attributes. - `handle_endtag(self, tag)`: Handle end tags while printing the tag name. - `handle_data(self, data)`: Handle text data and print it. - `handle_comment(self, data)`: Handle comments and print them. - Optionally override other handler methods if needed. 3. **Generate a summary**: The `CustomHTMLParser` should generate a summary of the HTML document in the following format: ``` START TAG: <tag_name> | ATTRIBUTES: [(\\"attr1\\", \\"value1\\"), (\\"attr2\\", \\"value2\\")] END TAG: <tag_name> DATA: \\"text data\\" COMMENT: \\"comment data\\" ``` Function Signature ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): pass def handle_endtag(self, tag): pass def handle_data(self, data): pass def handle_comment(self, data): pass # Function to initiate the parser and process the HTML input def parse_html(html_content: str) -> None: parser = CustomHTMLParser() parser.feed(html_content) # Example usage: html_content = \'\'\'<html> <head><title>Sample Page</title></head> <body> <h1>Welcome to HTML Parsing</h1> <p>This is a <a href=\\"https://example.com\\">link</a></p> <!-- Comment --> </body> </html>\'\'\' parse_html(html_content) ``` Input - `html_content` (str): A string containing the HTML content to be parsed. Output - The function `parse_html` should print the tags, attributes, text data, and comments found in the HTML content, formatted as specified in the summary format above. Constraints - Ensure that tag names and attribute names are case-insensitive. - The parser should handle poorly formatted HTML `as is`. Example For the given `html_content`: ```html <html> <head><title>Sample Page</title></head> <body> <h1>Welcome to HTML Parsing</h1> <p>This is a <a href=\\"https://example.com\\">link</a></p> <!-- Comment --> </body> </html> ``` The expected output should be: ``` START TAG: html | ATTRIBUTES: [] START TAG: head | ATTRIBUTES: [] START TAG: title | ATTRIBUTES: [] DATA: \\"Sample Page\\" END TAG: title END TAG: head START TAG: body | ATTRIBUTES: [] START TAG: h1 | ATTRIBUTES: [] DATA: \\"Welcome to HTML Parsing\\" END TAG: h1 START TAG: p | ATTRIBUTES: [] DATA: \\"This is a \\" START TAG: a | ATTRIBUTES: [(\\"href\\", \\"https://example.com\\")] DATA: \\"link\\" END TAG: a END TAG: p COMMENT: \\" Comment \\" END TAG: body END TAG: html ```","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): print(f\\"START TAG: {tag} | ATTRIBUTES: {attrs}\\") def handle_endtag(self, tag): print(f\\"END TAG: {tag}\\") def handle_data(self, data): if data.strip(): # Ignore data that is only whitespace print(f\\"DATA: \\"{data}\\"\\") def handle_comment(self, data): print(f\\"COMMENT: \\"{data}\\"\\") def parse_html(html_content: str) -> None: parser = CustomHTMLParser() parser.feed(html_content) # Example usage: html_content = \'\'\'<html> <head><title>Sample Page</title></head> <body> <h1>Welcome to HTML Parsing</h1> <p>This is a <a href=\\"https://example.com\\">link</a></p> <!-- Comment --> </body> </html>\'\'\' parse_html(html_content)"},{"question":"# Task: Decode Python Version You are required to implement a function `decode_python_version(version_hex: int) -> str` that takes an integer `version_hex`, representing a Python version encoded in the `PY_VERSION_HEX` format, and returns the human-readable string representation of the version. Input: - `version_hex`: an integer representing the Python version in `PY_VERSION_HEX` format. Output: - A string representing the version in the format \\"X.Y.ZaN\\", \\"X.Y.ZbN\\", \\"X.Y.ZcN\\", or \\"X.Y.Z\\". Constraints: - The major version (X) is between 0 and 255. - The minor version (Y) is between 0 and 255. - The micro version (Z) is between 0 and 255. - The release level can be one of: - `0xA` for alpha (`a`) - `0xB` for beta (`b`) - `0xC` for release candidate (`c`) - `0xF` for final release - The release serial number (N) is between 0 and 15. It is zero for final releases. Example: ```python decode_python_version(0x030401a2) -> \\"3.4.1a2\\" decode_python_version(0x030a00f0) -> \\"3.10.0\\" ``` Note: - For simplicity, you can assume that the input will always be in a valid `PY_VERSION_HEX` format. Your implementation should correctly decode the given integer and format the string according to the described rules. ```python def decode_python_version(version_hex: int) -> str: # Your code here pass # You can include some basic tests to check your implementation print(decode_python_version(0x030401a2)) # Output: \\"3.4.1a2\\" print(decode_python_version(0x030a00f0)) # Output: \\"3.10.0\\" ```","solution":"def decode_python_version(version_hex: int) -> str: Decodes the provided version_hex (in PY_VERSION_HEX format) into a human-readable Python version string. major = (version_hex >> 24) & 0xFF minor = (version_hex >> 16) & 0xFF micro = (version_hex >> 8) & 0xFF release_level = (version_hex >> 4) & 0xF release_serial = (version_hex) & 0xF release_levels = { 0xA: \'a\', # Alpha 0xB: \'b\', # Beta 0xC: \'c\', # Release candidate 0xF: \'\' # Final release } version_str = f\\"{major}.{minor}.{micro}\\" if release_level != 0xF: version_str += f\\"{release_levels[release_level]}{release_serial}\\" return version_str"},{"question":"Coding Assessment Question In this problem, you are required to demonstrate your understanding of object serialization and persistence using Python\'s `pickle` and `shelve` modules. Your task is to implement and work with a simple content management system (CMS) that allows for storing, updating, retrieving, and deleting text documents. # Function Implementation 1. **save_document(document_name: str, content: str) -> None** This function saves a document with a given name and content to a persistent storage. If a document with the same name already exists, it should be overwritten. 2. **load_document(document_name: str) -> str** This function retrieves the content of a document with the given name from the persistent storage. If the document doesn’t exist, it should return `None`. 3. **update_document(document_name: str, new_content: str) -> bool** This function updates the content of an existing document. It returns `True` if the document was successfully updated; otherwise, it returns `False`. 4. **delete_document(document_name: str) -> bool** This function deletes a document with the specified name from the persistent storage. It returns `True` if the document was successfully deleted; otherwise, it returns `False`. # Constraints 1. You should use Python\'s `shelve` module for storing the documents. 2. Document names are unique and case sensitive. 3. Document names are non-empty strings consisting of alphanumeric characters. 4. Document contents are strings of text. 5. Performance should be optimal for the operations on the storage. # Example Usage ```python # Save a new document save_document(\\"report\\", \\"Annual financial report content.\\") save_document(\\"draft\\", \\"Draft document content.\\") # Update an existing document update_document(\\"draft\\", \\"Updated draft content.\\") # Should return True # Load documents print(load_document(\\"report\\")) # Should output: \\"Annual financial report content.\\" print(load_document(\\"draft\\")) # Should output: \\"Updated draft content.\\" print(load_document(\\"nonexistent\\")) # Should output: None # Delete a document delete_document(\\"report\\") # Should return True delete_document(\\"report\\") # Should return False (as it no longer exists) ``` # Implementation Please implement the functions in the section below: ```python import shelve def save_document(document_name: str, content: str) -> None: with shelve.open(\'cms_storage\') as storage: storage[document_name] = content def load_document(document_name: str) -> str: with shelve.open(\'cms_storage\') as storage: return storage.get(document_name, None) def update_document(document_name: str, new_content: str) -> bool: with shelve.open(\'cms_storage\') as storage: if document_name in storage: storage[document_name] = new_content return True return False def delete_document(document_name: str) -> bool: with shelve.open(\'cms_storage\') as storage: if document_name in storage: del storage[document_name] return True return False ```","solution":"import shelve def save_document(document_name: str, content: str) -> None: Saves a document with the given name and content to persistent storage. with shelve.open(\'cms_storage\') as storage: storage[document_name] = content def load_document(document_name: str) -> str: Retrieves the content of a document with the given name from persistent storage. with shelve.open(\'cms_storage\') as storage: return storage.get(document_name, None) def update_document(document_name: str, new_content: str) -> bool: Updates the content of an existing document. Returns True if the document was updated, otherwise returns False. with shelve.open(\'cms_storage\') as storage: if document_name in storage: storage[document_name] = new_content return True return False def delete_document(document_name: str) -> bool: Deletes a document with the specified name from persistent storage. Returns True if the document was deleted, otherwise returns False. with shelve.open(\'cms_storage\') as storage: if document_name in storage: del storage[document_name] return True return False"},{"question":"# Gzip Compression and Decompression Task Problem Statement You are tasked with implementing a utility class, `GzipUtility`, that provides methods for compressing and decompressing files using the `gzip` module in Python. Your class should include the following methods: 1. **compress_file(input_filepath: str, output_filepath: str, compresslevel: int = 9) -> None**: - Compresses the file located at `input_filepath` and writes the compressed data to `output_filepath`. - The `compresslevel` parameter controls the level of compression (from 0 to 9, where 9 is the maximum compression). 2. **decompress_file(input_filepath: str, output_filepath: str) -> None**: - Decompresses the file located at `input_filepath` and writes the decompressed data to `output_filepath`. 3. **compress_string(data: str, compresslevel: int = 9) -> bytes**: - Compresses the given string `data` and returns the compressed data as a bytes object. - The `compresslevel` parameter controls the level of compression (from 0 to 9). 4. **decompress_string(compressed_data: bytes) -> str**: - Decompresses the given bytes object `compressed_data` and returns the decompressed data as a string. 5. **get_file_mtime(filepath: str) -> int**: - Opens the gzip file located at `filepath` and returns the modification time (mtime) as an integer. - Raises a `ValueError` if the file is not a valid gzip file or if there\'s any error in reading the mtime. Constraints - You can assume the input file paths are valid and the files exist. - You must handle exceptions appropriately, especially for invalid gzip files. - You can use any standard library modules, but the main operations should rely on the `gzip` module. Example Usage ```python # Compress and decompress a file gzip_util = GzipUtility() gzip_util.compress_file(\'example.txt\', \'example.txt.gz\', compresslevel=6) gzip_util.decompress_file(\'example.txt.gz\', \'example_decompressed.txt\') # Compress and decompress a string data = \\"This is some sample text that needs to be compressed.\\" compressed_data = gzip_util.compress_string(data, compresslevel=9) decompressed_data = gzip_util.decompress_string(compressed_data) assert data == decompressed_data # Get the modification time of a gzip file mtime = gzip_util.get_file_mtime(\'example.txt.gz\') print(f\\"Modification time: {mtime}\\") ``` Implementation Write the class `GzipUtility` with the methods described above. Ensure the methods are implemented correctly and handle exceptions where appropriate.","solution":"import gzip import os import time class GzipUtility: @staticmethod def compress_file(input_filepath: str, output_filepath: str, compresslevel: int = 9) -> None: with open(input_filepath, \'rb\') as f_in: with gzip.open(output_filepath, \'wb\', compresslevel=compresslevel) as f_out: f_out.writelines(f_in) @staticmethod def decompress_file(input_filepath: str, output_filepath: str) -> None: with gzip.open(input_filepath, \'rb\') as f_in: with open(output_filepath, \'wb\') as f_out: f_out.writelines(f_in) @staticmethod def compress_string(data: str, compresslevel: int = 9) -> bytes: return gzip.compress(data.encode(\'utf-8\'), compresslevel=compresslevel) @staticmethod def decompress_string(compressed_data: bytes) -> str: return gzip.decompress(compressed_data).decode(\'utf-8\') @staticmethod def get_file_mtime(filepath: str) -> int: try: with gzip.open(filepath, \'rb\') as f: f.read(1) # Just read one byte to trigger file access mtime = f.mtime return mtime except (OSError, AttributeError): raise ValueError(\\"Not a valid gzip file or error reading mtime\\")"},{"question":"# Descriptor Class Implementation In Python, a descriptor is a protocol that allows you to create objects which manage the access to an attribute. The descriptor protocol is made up of methods like `__get__`, `__set__`, and `__delete__`. These allow you to customize the behavior of attribute access in a class. Task: You are to implement a custom descriptor class in Python that can be used within a class to manage the attribute access pattern. Your descriptor should: 1. Log every read (get) and write (set) access to the attribute. 2. Store the actual attribute values in the instances of the class using it. 3. Be able to handle multiple instance attributes separately, i.e., it should store the value of each attribute separately for each instance of the class. Implementation Details: 1. Define a class `LoggingDescriptor`. This class should provide methods `__get__`, `__set__`, and `__delete__`. 2. `__get__` should log the access and return the attribute\'s value. 3. `__set__` should log the assignment and set the attribute\'s value. 4. Implement storing of the attribute values in the instances dynamically. 5. The logs should be stored in a class-level attribute of `LoggingDescriptor` named `log` which is a list of strings. Example: ```python class LoggingDescriptor: log = [] def __init__(self, name): self.name = name def __get__(self, instance, owner): value = instance.__dict__.get(self.name) self.log.append(f\\"Accessing {self.name}, value = {value}\\") return value def __set__(self, instance, value): self.log.append(f\\"Setting {self.name} to {value}\\") instance.__dict__[self.name] = value def __delete__(self, instance): self.log.append(f\\"Deleting {self.name}\\") del instance.__dict__[self.name] class MyClass: my_attr = LoggingDescriptor(\'my_attr\') # Demonstration obj = MyClass() obj.my_attr = 10 # Logs: Setting my_attr to 10 print(obj.my_attr) # Logs: Accessing my_attr, value = 10 del obj.my_attr # Logs: Deleting my_attr print(LoggingDescriptor.log) # Output: # [\'Setting my_attr to 10\', \'Accessing my_attr, value = 10\', \'Deleting my_attr\'] ``` Constraints: - You may assume all attribute names passed to `LoggingDescriptor` are valid Python identifiers. - You should make no assumptions about the types of the values stored in the attributes. Assess your implementation carefully to ensure it successfully logs the actions and behaves correctly as specified.","solution":"class LoggingDescriptor: log = [] def __init__(self, name): self.name = name def __get__(self, instance, owner): value = instance.__dict__.get(self.name) self.log.append(f\\"Accessing {self.name}, value = {value}\\") return value def __set__(self, instance, value): self.log.append(f\\"Setting {self.name} to {value}\\") instance.__dict__[self.name] = value def __delete__(self, instance): self.log.append(f\\"Deleting {self.name}\\") del instance.__dict__[self.name] class MyClass: my_attr = LoggingDescriptor(\'my_attr\')"},{"question":"# Priority Task Scheduler with asyncio Objective: Implement a priority-based task scheduler using `asyncio.PriorityQueue` to execute and manage multiple tasks with different priorities. Problem Statement: You will write a program that creates a `PriorityQueue` to manage and execute tasks based on their priority. Each task will be represented by a tuple `(priority, task_time)`, where: - `priority` is an integer (lower numbers indicate higher priority). - `task_time` is the time in seconds the task should \\"run\\" (simulated by `await asyncio.sleep(task_time)`). Your goal is to: 1. Create a `PriorityQueue`. 2. Enqueue a list of tasks with various priorities into the queue. 3. Dequeue and process tasks based on their priority using multiple worker coroutines. 4. Ensure that worker coroutines run concurrently and process the highest-priority task available. Implementation Requirements: 1. Define an asynchronous function `worker(name: str, queue: asyncio.PriorityQueue) -> None` that: - Continuously gets a task from the queue. - \\"Runs\\" the task by sleeping for the specified `task_time`. - Marks the task as done in the queue once executed. - Prints a message when the task is processed. 2. Define an asynchronous function `main()` that: - Creates a `PriorityQueue`. - Enqueues a predefined list of tasks into the queue. - Starts multiple worker coroutines (at least 3) to process the queue concurrently. - Waits until all tasks are processed. - Cancels the worker coroutines and waits for their termination. - Prints a summary of the total time taken to process all tasks. Input: - You do not need to handle input; the list of tasks to be enqueued will be predefined in the `main` function. Output: - Print messages from the worker tasks indicating when a task is processed and by which worker. - A summary message of the total time taken to process all tasks. Example Tasks List: ```python tasks = [ (2, 1.0), # Task with priority 2 and duration 1.0 seconds (1, 0.5), # Task with priority 1 and duration 0.5 seconds (3, 2.0), # Task with priority 3 and duration 2.0 seconds (1, 0.1), # Task with priority 1 and duration 0.1 seconds (2, 0.3), # Task with priority 2 and duration 0.3 seconds ] ``` Constraints: - Ensure worker coroutines run concurrently. - Task with lower priority number must be processed first. - Handle any exceptions that may occur during task execution. Function Signatures: ```python import asyncio async def worker(name: str, queue: asyncio.PriorityQueue) -> None: # your code here async def main() -> None: # your code here # Entry point to run the application if __name__ == \\"__main__\\": asyncio.run(main()) ```","solution":"import asyncio import time async def worker(name: str, queue: asyncio.PriorityQueue) -> None: while True: priority, task_time = await queue.get() try: print(f\\"Worker {name} processing task with priority {priority} for {task_time} seconds\\") await asyncio.sleep(task_time) print(f\\"Worker {name} completed task with priority {priority}\\") finally: queue.task_done() async def main() -> None: tasks = [ (2, 1.0), (1, 0.5), (3, 2.0), (1, 0.1), (2, 0.3), ] queue = asyncio.PriorityQueue() for task in tasks: queue.put_nowait(task) workers = [asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(3)] start_time = time.time() await queue.join() end_time = time.time() total_time = end_time - start_time print(f\\"All tasks completed in {total_time:.2f} seconds\\") for w in workers: w.cancel() await asyncio.gather(*workers, return_exceptions=True) # Entry point to run the application if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question You are tasked with visualizing a dataset to analyze the performance of different models on various tasks. Perform the following operations using seaborn, a powerful Python library for creating informative and attractive statistical graphics. Dataset Use the \\"glue\\" dataset available in seaborn, which measures the performance scores of various models across different tasks. Follow these steps to load and prepare the dataset: ```python import seaborn as sns glue = sns.load_dataset(\\"glue\\").pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") ``` Instructions 1. **Create a basic heatmap:** - Load the dataset and create a basic heatmap showing the performance scores. 2. **Annotate the heatmap:** - Add annotations to the heatmap cells displaying the scores with one decimal place of precision. 3. **Customize the heatmap:** - Use a different colormap (e.g., `coolwarm`) for the heatmap. - Set the colormap norm to have minimum and maximum values corresponding to 50 and 100, respectively. - Add a linewidth of 0.5 between the heatmap cells. 4. **Further customizations:** - Use a second DataFrame to annotate the heatmap cells with the rank of the scores across each task (you can use the `.rank()` method). - Modify the Matplotlib Axes to: - Move the x-axis labels to the top. - Remove the y-axis label and keep the x-axis label empty. - Adjust the figure size to 10x8 inches for better visibility. 5. **Save the final heatmap:** - Save the final customized heatmap to a file named `model_performance_heatmap.png`. Expected Output A saved heatmap visualizing the performance of models across tasks with all specified customizations. ```python import seaborn as sns import matplotlib.pyplot as plt # Load and pivot the dataset glue = sns.load_dataset(\\"glue\\").pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Create the heatmap with the specified customizations plt.figure(figsize=(10, 8)) ax = sns.heatmap(glue, annot=glue.rank(axis=\\"columns\\"), fmt=\\".1f\\", cmap=\\"coolwarm\\", vmin=50, vmax=100, linewidth=0.5) # Further customizations ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Save the heatmap plt.savefig(\\"model_performance_heatmap.png\\") ``` Constraints - Ensure that you use the seaborn package for creating the heatmap and Matplotlib for additional customizations. - Your solution should be efficient and well-structured.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_customized_heatmap(): # Load and pivot the dataset glue = sns.load_dataset(\\"glue\\").pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Create the heatmap with the specified customizations plt.figure(figsize=(10, 8)) ax = sns.heatmap(glue, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", vmin=50, vmax=100, linewidth=0.5) # Further customizations ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Save the heatmap plt.savefig(\\"model_performance_heatmap.png\\") # Execute the function to create heatmap create_customized_heatmap()"},{"question":"**Coding Assessment Question** # Objective You are tasked with building a function that validates the input dataset, computes an efficient matrix operation, and handles sparse matrix data using utilities from `sklearn.utils`. # Problem Statement Implement a function `process_dataset(X, y)` that takes in two arguments: 1. `X`: A 2D numpy array or a sparse matrix containing the dataset features. 2. `y`: A 1D numpy array or a sparse matrix containing the target values. The function should: 1. Confirm that `X` and `y` are valid inputs using `sklearn.utils` validation tools. 2. Ensure that all elements in `X` are finite. 3. Normalize the rows of `X` to unit L2 norm if `X` is sparse. 4. Compute and return the density of `X` if `X` is sparse, else return `None`. # Implementation Details 1. **Validation**: - Use `check_X_y(X, y)` to ensure that `X` and `y` have consistent length. - Use `assert_all_finite(X)` to ensure `X` contains only finite numbers. 2. **Normalization**: - If `X` is a sparse matrix, use `sparsefuncs_fast.inplace_csr_row_normalize_l2(X)` to normalize the rows of `X` to unit L2 norm. 3. **Density Computation**: - If `X` is a sparse matrix, use `extmath.density(X)` to compute and return the density of `X`. - If `X` is a dense matrix, return `None`. # Function Signature ```python def process_dataset(X, y): Validates the inputs, normalizes sparse matrices, and computes density for sparse matrices. Parameters: X (numpy.ndarray or scipy.sparse matrix): The dataset features. y (numpy.ndarray or scipy.sparse matrix): The target values. Returns: float or None: Density of X if X is sparse, else None. ``` # Example Usage ```python import numpy as np from scipy.sparse import csr_matrix X_dense = np.array([[1.0, 2.0], [3.0, 4.0]]) y_dense = np.array([1.0, 2.0]) print(process_dataset(X_dense, y_dense)) # Output: None X_sparse = csr_matrix([[1.0, 0.0], [3.0, 4.0]]) y_sparse = csr_matrix([1.0, 2.0]) print(process_dataset(X_sparse, y_sparse)) # Output: Density value (a float) ``` # Constraints 1. You are only allowed to use the utilities provided in `sklearn.utils`. 2. You must handle both dense and sparse matrix inputs efficiently. 3. Ensure the function handles invalid inputs gracefully by raising appropriate errors. # Performance Requirements Ensure that the function runs efficiently, even for larger datasets, by making use of the provided utilities specifically optimized for performance.","solution":"import numpy as np from scipy.sparse import isspmatrix from sklearn.utils import check_X_y, assert_all_finite from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l2 from sklearn.utils.extmath import density def process_dataset(X, y): Validates inputs, normalizes sparse matrices, and computes density for sparse matrices. Parameters: X (numpy.ndarray or scipy.sparse matrix): The dataset features. y (numpy.ndarray or scipy.sparse matrix): The target values. Returns: float or None: Density of X if X is sparse, else None. # Validate inputs X, y = check_X_y(X, y, accept_sparse=True) assert_all_finite(X) # Ensure elements in X are finite # Handle sparse matrix input if isspmatrix(X): inplace_csr_row_normalize_l2(X) # Normalize rows of X to unit L2 norm return density(X) # Compute and return the density of X return None # Return None if X is not a sparse matrix"},{"question":"Objective Create and test a custom PyTorch operator that performs a specific mathematical operation—matrix multiplication followed by an element-wise addition. Then, extend this custom operator to include gradient computations. Instructions 1. **Create the Custom Operator**: - Implement a custom operator named `matmul_add` using the `torch.library.custom_op` function. - This operator should take three inputs: - Two matrices `A` and `B` for multiplication. - One matrix `C` (of the same shape as the result of `A` multiplied by `B`) to be added element-wise to the result of `AB`. - The result should be `D = AB + C`. 2. **Test the Custom Operator**: - Use the `torch.library.opcheck` function to test the correctness of your custom operator implementation. 3. **Implement Gradient Computation**: - Extend the custom operator to include gradient computations using `torch.library.register_autograd`. - Ensure that the gradients are mathematically correct by using `torch.autograd.gradcheck`. 4. **Performance Requirements**: - Your implementation should handle matrix dimensions up to 1024x1024 within a reasonable time frame (under 1 second for the forward pass). Input and Output Formats - **Input**: - Three tensors `A`, `B`, and `C` of appropriate dimensions. - **Output**: - A single tensor `D` which is the result of the operation `D = AB + C`. Constraints - You should not use existing PyTorch functions like `torch.matmul` directly in your custom op; implement the functionality from scratch. - Handle the cases where matrix dimensions do not match properly by raising an appropriate error. Example ```python import torch # Define input tensors A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32) C = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32) # Expected output tensor # AB = [[19, 22], [43, 50]] # D = AB + C = [[20, 23], [44, 51]] # Call your custom operator D = matmul_add(A, B, C) print(D) # Should output a tensor close to [[20, 23], [44, 51]] ``` Your task is to: 1. Write the Python code to define and test the custom operator. 2. Extend the custom operator to correctly compute gradients and verify their correctness.","solution":"import torch from torch.autograd import Function class MatMulAddFunction(Function): @staticmethod def forward(ctx, A, B, C): ctx.save_for_backward(A, B, C) # save tensors for backward pass output = torch.mm(A, B) + C # perform matrix multiplication and addition return output @staticmethod def backward(ctx, grad_output): A, B, C = ctx.saved_tensors grad_A = grad_B = grad_C = None if ctx.needs_input_grad[0]: # Calculate gradients w.r.t. A grad_A = torch.mm(grad_output, B.t()) if ctx.needs_input_grad[1]: # Calculate gradients w.r.t. B grad_B = torch.mm(A.t(), grad_output) if ctx.needs_input_grad[2]: # Calculate gradients w.r.t. C grad_C = grad_output return grad_A, grad_B, grad_C def matmul_add(A, B, C): Perform matrix multiplication of A and B followed by element-wise addition of C. Args: A (torch.Tensor): Tensor of shape (m, n) B (torch.Tensor): Tensor of shape (n, p) C (torch.Tensor): Tensor of shape (m, p) Returns: torch.Tensor: Result of the operation D = AB + C return MatMulAddFunction.apply(A, B, C)"},{"question":"Advanced Pandas Operations Objective: To assess your understanding of pandas Series and DataFrame manipulation, indexing, alignment, and integration with NumPy. Question: You are provided with sales data for a retail store. This data includes product sales over a week, given in various formats. Using pandas, implement the function `process_sales_data` to perform the following tasks: 1. **Create a DataFrame named `df`** from a provided dictionary of lists. The dictionary `sales_data` is structured as follows: ```python sales_data = { \\"Product\\": [\\"Product_A\\", \\"Product_B\\", \\"Product_C\\", \\"Product_D\\"], \\"Monday\\": [20, 30, 0, 10], \\"Tuesday\\": [21, 29, 0, 13], \\"Wednesday\\": [23, 30, 0, 8], \\"Thursday\\": [25, 33, 0, 9], \\"Friday\\": [20, 35, 0, 4], \\"Saturday\\": [22, 30, 0, 12], \\"Sunday\\": [24, 29, 0, 11] } ``` 2. **Add a new column called `Total_Sales`** which is the sum of sales across all days of the week for each product. 3. **Replace the sales of Product_C for any day with NaN** (as the product was out of stock and the data is incorrect). 4. **Create a new DataFrame named `df_summary`** that includes only the `Product` and `Total_Sales` columns, sorted by `Total_Sales` in descending order. 5. **Return a tuple of two items**: - The first item is the DataFrame `df` after the modifications, transposed to swap rows and columns. - The second item is the DataFrame `df_summary`. Function Signature: ```python def process_sales_data(sales_data: dict) -> tuple: # Your implementation goes here ``` Example Output: ```python sales_data = { \\"Product\\": [\\"Product_A\\", \\"Product_B\\", \\"Product_C\\", \\"Product_D\\"], \\"Monday\\": [20, 30, 0, 10], \\"Tuesday\\": [21, 29, 0, 13], \\"Wednesday\\": [23, 30, 0, 8], \\"Thursday\\": [25, 33, 0, 9], \\"Friday\\": [20, 35, 0, 4], \\"Saturday\\": [22, 30, 0, 12], \\"Sunday\\": [24, 29, 0, 11] } df, df_summary = process_sales_data(sales_data) print(df) print(df_summary) ``` Expected output: ```text Product Product_A Product_B Product_C Product_D Product Product Product_A Product_B NaN Product_D Monday 20 20 30 NaN 10 Tuesday 21 21 29 NaN 13 Wednesday 23 23 30 NaN 8 Thursday 25 25 33 NaN 9 Friday 20 20 35 NaN 4 Saturday 22 22 30 NaN 12 Sunday 24 24 29 NaN 11 Total_Sales 155 155 216 NaN 67 Product Total_Sales 0 Product_B 216 1 Product_A 155 3 Product_D 67 2 Product_C NaN ``` Make sure your code is efficient and follows best practices for manipulating pandas DataFrames and Series.","solution":"import pandas as pd import numpy as np def process_sales_data(sales_data: dict) -> tuple: # Create DataFrame from the provided dictionary df = pd.DataFrame(sales_data) # Sum the sales across all days df[\'Total_Sales\'] = df.loc[:, \'Monday\':\'Sunday\'].sum(axis=1) # Replace sales for Product_C with NaN df.loc[df[\'Product\'] == \'Product_C\', df.columns != \'Product\'] = np.nan # Create the summary DataFrame df_summary = df[[\'Product\', \'Total_Sales\']].sort_values(by=\'Total_Sales\', ascending=False).reset_index(drop=True) # Transpose the DataFrame `df` df_transposed = df.transpose() return df_transposed, df_summary"},{"question":"# Password Verification System You are required to implement a password verification system using the `getpass` module to securely handle password inputs. The system will prompt the user for their username and password, check these credentials against a predefined set of users, and grant access based on the correctness of the input. # Requirements 1. **Predefined Credentials**: Define a dictionary containing user login names and their corresponding passwords. The structure should be: ```python credentials = { \'user1\': \'pass1\', \'user2\': \'pass2\', # Add more users as needed } ``` 2. **Function Implementation**: - **Function Name**: `authenticate_user` - **Input**: No input parameters. - **Output**: Print `Access granted` if the credentials are correct, otherwise print `Access denied`. 3. **Steps**: - Use `getpass.getuser()` to get the login name of the user attempting to authenticate. - Prompt the user for their password using `getpass.getpass(prompt=\'Enter your password: \')`. - Check the provided login name and password against the predefined credentials. - Print `Access granted` if the credentials match, otherwise print `Access denied`. 4. **Constraint**: - Only handle the login logic without using external files or databases. - Ensure that user input (especially the password) is not echoed to the terminal. # Example ```python # Example predefined credentials credentials = { \'user1\': \'pass1\', \'user2\': \'pass2\', } def authenticate_user(): # Your implementation starts here pass # Assuming \'user1\' is running the script and correctly typing \'pass1\' authenticate_user() # Output: Access granted # Assuming \'user2\' is running the script and incorrectly typing \'wrongpass\' authenticate_user() # Output: Access denied ``` Implement the `authenticate_user` function based on the requirements provided.","solution":"import getpass # Predefined credentials credentials = { \'user1\': \'pass1\', \'user2\': \'pass2\', # Add more users as needed } def authenticate_user(): Prompts the user for their username and password and checks these against the predefined credentials. username = getpass.getuser() password = getpass.getpass(prompt=\'Enter your password: \') if username in credentials and credentials[username] == password: print(\'Access granted\') else: print(\'Access denied\')"},{"question":"# Email Message Manipulation with python310 You are required to write a function to create and manipulate email messages using the `email.message.Message` class. Your function needs to perform the following: 1. Create a new email message object. 2. Set the email\'s \\"From\\", \\"To\\", and \\"Subject\\" headers. 3. Add a text payload to the message. 4. Attach a multipart payload, which includes: - A plaintext message. - An HTML version of the same message. - An attachment with binary data. 5. Extract and return specific details from the email message, including: - The \\"From\\" address. - The \\"Subject\\" of the email. - The MIME type of the root part of the message. - A list of content types for each part of the multipart payload. Function Signature: ```python from email.message import Message def manipulate_email_message(): # Your code here return email_from, email_subject, content_type, part_content_types ``` # Constraints - You must use the `email.message.Message` class and its provided methods to create and manipulate the email message. - The MIME types for the main message and parts can be: - Main message: `multipart/alternative` - Text part: `text/plain` - HTML part: `text/html` - Attachment: `application/octet-stream` # Example ```python email_from, email_subject, content_type, part_content_types = manipulate_email_message() # Expected Output: # email_from: \\"example@test.com\\" # email_subject: \\"Sample Email\\" # content_type: \\"multipart/alternative\\" # part_content_types: [\\"text/plain\\", \\"text/html\\", \\"application/octet-stream\\"] ``` Your function will be evaluated based on correctness, the proper use of the `email.message.Message` methods, and clarity of your implementation.","solution":"from email.message import EmailMessage from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.mime.base import MIMEBase from email import encoders def manipulate_email_message(): # Create a new email message object msg = MIMEMultipart(\'alternative\') # Set the email\'s \\"From\\", \\"To\\", and \\"Subject\\" headers msg[\'From\'] = \'example@test.com\' msg[\'To\'] = \'recipient@test.com\' msg[\'Subject\'] = \'Sample Email\' # Add a text payload to the message text = \\"This is a plain text message.\\" part1 = MIMEText(text, \'plain\') # Add an HTML version of the same message html = \\"<html><body><p>This is an HTML message.</p></body></html>\\" part2 = MIMEText(html, \'html\') # Create a binary attachment binary_data = b\\"Here is some binary data\\" part3 = MIMEBase(\'application\', \'octet-stream\') part3.set_payload(binary_data) encoders.encode_base64(part3) part3.add_header(\'Content-Disposition\', \'attachment; filename=\\"binary.dat\\"\') # Attach parts msg.attach(part1) msg.attach(part2) msg.attach(part3) # Extract and return specific details from the email message email_from = msg[\'From\'] email_subject = msg[\'Subject\'] content_type = msg.get_content_type() part_content_types = [part.get_content_type() for part in msg.get_payload()] return email_from, email_subject, content_type, part_content_types"},{"question":"You are given a dataset containing information about various species of penguins. Your task is to write a Python function using the Seaborn library to generate comprehensive visualizations that provide insights into the dataset\'s structure and distribution. # Dataset The dataset used is provided by the Seaborn library itself and can be loaded using `sns.load_dataset(\\"penguins\\")`. # Requirements 1. Write a function `visualize_penguin_data` that: - Inputs: None - Outputs: None (the function should display the plots). 2. The function should: - **Histograms**: Display a histogram of the `flipper_length_mm` variable with two different bin sizes. - **KDE Plot**: - Generate a KDE plot for `flipper_length_mm` and adjust the bandwidth for over- and under-smoothing. - Create a KDE plot with conditional densities based on the `species` column. - **ECDF Plot**: - Generate an ECDF plot for `flipper_length_mm` with conditional distributions based on the `species` column. - **Bivariate Distribution**: - Create a bivariate histogram for `bill_length_mm` and `bill_depth_mm`. - Generate a bivariate KDE plot for `bill_length_mm` and `bill_depth_mm`, including contour levels. - **Joint Distribution**: - Use `jointplot` to display the joint distribution of `bill_length_mm` and `bill_depth_mm` along with marginal histograms. - **Pairplot**: - Display a pairplot for the dataset, showing all pairwise relationships between numeric variables and the univariate distribution of each. 3. Ensure that your plots have appropriate titles and labels for clarity. # Constraints - Use only the Seaborn library for visualization. - Ensure your function is well-documented with comments explaining each part of the code. # Example Here is a partial example to help you start: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): penguins = sns.load_dataset(\\"penguins\\") # Histogram of flipper_length_mm with automatic bin size sns.displot(penguins, x=\\"flipper_length_mm\\") plt.title(\\"Histogram of Flipper Length\\") plt.show() # KDE plot of flipper_length_mm sns.displot(penguins, x=\\"flipper_length_mm\\", kind=\\"kde\\") plt.title(\\"KDE of Flipper Length\\") plt.show() # [...] # Uncomment the line below to run the function # visualize_penguin_data() ``` Your task is to expand this example to fulfil the all the requirements above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): penguins = sns.load_dataset(\\"penguins\\") # 1. Histograms of flipper_length_mm with two different bin sizes sns.histplot(penguins[\'flipper_length_mm\'], bins=10) plt.title(\\"Histogram of Flipper Length with 10 Bins\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() sns.histplot(penguins[\'flipper_length_mm\'], bins=30) plt.title(\\"Histogram of Flipper Length with 30 Bins\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # 2. KDE plots for flipper_length_mm sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", bw_adjust=0.5) plt.title(\\"KDE of Flipper Length (lower bandwidth)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", bw_adjust=2) plt.title(\\"KDE of Flipper Length (higher bandwidth)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # KDE plot with conditional densities based on species sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", common_norm=False) plt.title(\\"KDE of Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # 3. ECDF plot for flipper_length_mm with conditional distributions based on species sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() # 4. Bivariate distribution # Bivariate histogram for bill_length_mm and bill_depth_mm sns.histplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") plt.title(\\"Bivariate Histogram of Bill Length and Bill Depth\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # Bivariate KDE plot for bill_length_mm and bill_depth_mm with contour levels sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", fill=True) plt.title(\\"Bivariate KDE of Bill Length and Bill Depth\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # 5. Joint distribution using jointplot for bill_length_mm and bill_depth_mm with marginal histograms sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", kind=\\"hist\\") plt.title(\\"Joint Distribution of Bill Length and Bill Depth with Marginal Histograms\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # 6. Pairplot for the dataset sns.pairplot(penguins, hue=\\"species\\") plt.suptitle(\\"Pairplot of Penguins Dataset\\") plt.show()"},{"question":"**Coding Assessment Question:** # Objective To assess the student\'s understanding and ability to use the `trace` module to trace code execution and generate detailed coverage reports programmatically. # Problem Statement Write a Python program that must implement a function using the `trace` module to trace the execution of another function and generate a detailed coverage report to ensure comprehensive testing. Specifications 1. **Function Signature**: ```python def trace_function(func: callable, func_args: list = [], func_kwargs: dict = {}, ignore_modules: list = [], ignore_dirs: list = [], timing: bool = False, coverdir: str = \'.\') -> None: ``` 2. **Parameters**: - `func`: The function to be traced. - `func_args`: A list of positional arguments to pass to `func`. - `func_kwargs`: A dictionary of keyword arguments to pass to `func`. - `ignore_modules`: A list of module names to ignore while tracing. - `ignore_dirs`: A list of directory paths to ignore while tracing. - `timing`: A boolean flag indicating whether to include execution timing info. - `coverdir`: The directory where the coverage report files should be saved. Defaults to the current directory. 3. **Behavior**: - Create a `trace.Trace` object with the given parameters. - Use this `Trace` object to run the given function `func` with the specified `func_args` and `func_kwargs`. - Collect the coverage results. - Write the coverage results including information about lines not executed and a summary of each module\'s coverage to the specified `coverdir`. 4. **Output**: - Coverage report files should be generated in the specified `coverdir`. - Printed summary of coverage for each module. 5. **Constraints**: - Do not use any external libraries apart from `trace`. - Ensure that the function handles exceptions gracefully. - Consider cases where the function might be slow due to the inclusion of `timing` flag. # Example Usage ```python def sample_function(a, b=10): for i in range(a): if i % b == 0: print(f\'{i} is a multiple of {b}\') trace_function(sample_function, func_args=[100], func_kwargs={\'b\': 20}, timing=True, coverdir=\\"./coverage_reports\\") # This will generate coverage reports in the \\"coverage_reports\\" directory ``` # Detailed Requirements - Create coverage reports that show lines not executed. - Provide a summary output that indicates the percentage of code covered for each traced module. - Ensure that the coverage results are saved in the specified directory without overwriting previous runs. In addition to the function, provide sample code demonstrating its usage and generating coverage reports for it. # Evaluation Criteria - **Correctness**: The function should perform tracing and generate correct and complete coverage reports. - **Clarity**: The code should be well-documented and easy to understand. - **Efficiency**: The function should handle timing gracefully and not introduce significant overhead unless required. - **Robustness**: The function should handle edge cases and errors gracefully.","solution":"import trace import os def trace_function(func: callable, func_args: list = [], func_kwargs: dict = {}, ignore_modules: list = [], ignore_dirs: list = [], timing: bool = False, coverdir: str = \'.\') -> None: Traces the execution of a given function and generates a detailed coverage report. Parameters: - func: The function to be traced. - func_args: A list of positional arguments to pass to the function. - func_kwargs: A dictionary of keyword arguments to pass to the function. - ignore_modules: A list of module names to ignore while tracing. - ignore_dirs: A list of directory paths to ignore while tracing. - timing: A boolean flag indicating whether to include execution timing info. - coverdir: The directory where the coverage report files should be saved. if not os.path.exists(coverdir): os.makedirs(coverdir) tracer = trace.Trace( ignoredirs=ignore_dirs, trace=False, count=True, countfuncs=True, countcallers=True, ignoremods=ignore_modules, timing=timing ) try: tracer.runfunc(func, *func_args, **func_kwargs) results = tracer.results() results.write_results(show_missing=True, summary=True, coverdir=coverdir) print(f\\"Coverage report saved in {coverdir}\\") except Exception as e: print(f\\"An error occurred while tracing the function: {e}\\")"},{"question":"# PyTorch Coding Assessment Question Objective To assess your understanding of conditional flow in PyTorch using `torch.cond` and your ability to implement tensor operations in response to dynamic conditions. Question You are required to write a PyTorch module that takes a tensor as input and applies different operations based on the sum of its elements and its shape. Specifically, you need to: 1. Implement a class `CustomCondModule` extending `torch.nn.Module`. 2. In the `forward` method of `CustomCondModule`, check if the sum of the input tensor\'s elements is greater than 10. If true, apply the function that returns `x.cos()`. Otherwise, apply the function that returns `x.sin()`. 3. Additionally, if the shape of the input tensor has more than 5 elements in the first dimension, irrespective of the sum, add a new dimension of size 1 at the start of the tensor before applying the above conditions. 4. Implement this using `torch.cond`. Input - A tensor `x` of arbitrary shape. Output - The result of applying the conditional operations to the input tensor as specified. Constraints - Use `torch.cond` for applying the conditional logic. - The functions to be applied (`true_fn` and `false_fn`) should be defined within the class. - Assume the tensor `x` is always a valid PyTorch tensor. Example ```python import torch import torch.nn as nn class CustomCondModule(nn.Module): def __init__(self): super(CustomCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return x.cos() def false_fn(x: torch.Tensor): return x.sin() if x.shape[0] > 5: x = x.unsqueeze(0) return torch.cond(x.sum() > 10.0, true_fn, false_fn, (x,)) # Example usage model = CustomCondModule() input_tensor = torch.randn(6) output_tensor = model(input_tensor) print(output_tensor) ``` In the above example, depending on the sum of elements of `input_tensor`, either `cos` or `sin` function will be applied. If the tensor shape exceeds 5 in the first dimension, an additional dimension is added before applying the conditional logic. **Note**: You need to ensure the functional operations are correctly implemented within the constraints of `torch.cond`.","solution":"import torch import torch.nn as nn class CustomCondModule(nn.Module): def __init__(self): super(CustomCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x): return x.cos() def false_fn(x): return x.sin() if x.shape[0] > 5: x = x.unsqueeze(0) output = torch.where(x.sum() > 10.0, true_fn(x), false_fn(x)) return output"},{"question":"# Seaborn Advanced Plotting and Customization **Objective:** Design a function to generate a customized multi-plot figure using seaborn and matplotlib. **Question:** You are tasked with creating a function `custom_seaborn_plots` that takes a pandas DataFrame as input and generates a figure with two subplots arranged vertically: 1. A bar plot displaying the sum of values for each category. 2. A scatter plot showing the relationship between two continuous variables, with customized aesthetics. The function should: - Set a seaborn theme with specific style and palette. - Customize the bar plot by removing the top and right spines. - Customize the scatter plot by adding a regression line. - Include appropriate titles and axis labels for both subplots. **Function Signature:** ```python def custom_seaborn_plots(df: pandas.DataFrame) -> None: pass ``` **Input:** - `df`: A pandas DataFrame with at least three columns: - A categorical column. - Two continuous columns. **Constraints:** - Assume the DataFrame will have at least 10 rows. - The categorical column contains non-numeric data. - Both continuous columns contain numeric data. **Example Use:** ```python import pandas as pd # Example DataFrame data = { \'Category\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\'], \'Value1\': [10, 20, 30, 20, 10, 40, 30, 30, 20, 50], \'Value2\': [5, 3, 10, 8, 7, 12, 6, 5, 11, 9] } df = pd.DataFrame(data) # Calling the function custom_seaborn_plots(df) ``` **Expected Output:** The function will generate a figure with: 1. A bar plot showing the sum of values in `Value1` for each category `A`, `B`, and `C`. 2. A scatter plot showing the relationship between `Value1` and `Value2`, with a regression line, a grid, and labeled axes. Both plots will be displayed in a vertically stacked configuration. **Hint:** Utilize the seaborn and matplotlib libraries to achieve this. You may find the seaborn functions `sns.barplot`, `sns.lmplot`, and `sns.set_theme` useful, along with matplotlib\'s `plt.subplots`.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plots(df: pd.DataFrame) -> None: Generates a customized multi-plot figure with a bar plot and a scatter plot. Parameters: df (pd.DataFrame): A pandas DataFrame with at least three columns: - A categorical column. - Two continuous columns. # Data validation assert df.shape[0] >= 10, \\"The DataFrame must have at least 10 rows.\\" assert df.select_dtypes(include=[\'object\']).shape[1] > 0, \\"The DataFrame should have at least one categorical column.\\" assert df.select_dtypes(include=[\'number\']).shape[1] >= 2, \\"The DataFrame should have at least two continuous columns.\\" # Extracting columns categorical_col = df.select_dtypes(include=[\'object\']).columns[0] continuous_cols = df.select_dtypes(include=[\'number\']).columns[:2] # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\", palette=\\"muted\\") # Create a figure with two subplots arranged vertically fig, axes = plt.subplots(2, 1, figsize=(10, 15)) # 1. Bar Plot bar_data = df.groupby(categorical_col)[continuous_cols[0]].sum().reset_index() sns.barplot(data=bar_data, x=categorical_col, y=continuous_cols[0], ax=axes[0]) axes[0].set_title(f\'Sum of {continuous_cols[0]} by {categorical_col}\') axes[0].set_xlabel(categorical_col) axes[0].set_ylabel(f\'Sum of {continuous_cols[0]}\') axes[0].spines[\'top\'].set_visible(False) axes[0].spines[\'right\'].set_visible(False) # 2. Scatter Plot sns.scatterplot(data=df, x=continuous_cols[0], y=continuous_cols[1], ax=axes[1]) sns.regplot(data=df, x=continuous_cols[0], y=continuous_cols[1], ax=axes[1], scatter=False, color=\'r\') axes[1].set_title(f\'{continuous_cols[0]} vs {continuous_cols[1]}\') axes[1].set_xlabel(continuous_cols[0]) axes[1].set_ylabel(continuous_cols[1]) axes[1].grid(True) # Adjust layout plt.tight_layout() plt.show()"},{"question":"# Question: Data Loading and Preprocessing with Scikit-learn In this task, you are required to demonstrate your understanding of loading different types of datasets using scikit-learn, and preprocessing them to be ready for machine learning tasks. Part 1: Loading and Processing Sample Images 1. Load the sample image `china.jpg` provided by scikit-learn. 2. Convert the image into a floating-point representation. 3. Scale the image data to the range [0, 1]. 4. Display the image using `matplotlib`. Part 2: Loading Sparse Datasets in svmlight Format 1. Load two datasets provided in the svmlight format: `train_dataset.txt` and `test_dataset.txt`. 2. Ensure that both datasets have the same number of features. 3. For the training dataset, output the number of instances and features. Part 3: Fetching Datasets from OpenML 1. Download the \\"iris\\" dataset from the OpenML repository. 2. Display the shape of the data and target arrays. 3. Print the description of the dataset. Input and Output Formats - For Part 1: - No input required. - The output should be the displayed image with the correct scaling. - For Part 2: - Input files: `train_dataset.txt`, `test_dataset.txt` (provided separately). - Output: Number of instances and features in the training dataset. - For Part 3: - No input required. - Output: Shape of data and target arrays, and description of the \\"iris\\" dataset. Constraints and Limitations - Make sure to convert the image data to floating-point representation before scaling it. - Ensure datasets loaded from the svmlight format have the same number of features for both training and test sets. - The data processing should be done using scikit-learn functions. Performance Requirements - Efficiently handle any provided datasets with potentially large sizes. - Ensure memory-efficient loading and processing. You will be assessed on: 1. Correctness of data loading and preprocessing. 2. Proper scaling and representation of image data. 3. Accurate handling of sparse datasets. 4. Correct fetching and displaying of dataset information from OpenML. Example Code Structure ```python # Part 1: Loading and Processing Sample Images from sklearn.datasets import load_sample_image import matplotlib.pyplot as plt # Load image china = load_sample_image(\\"china.jpg\\") # Convert and scale image # [Your code here] # Display image plt.imshow(china_scaled) plt.axis(\'off\') plt.tight_layout() plt.show() # Part 2: Loading Sparse Datasets in svmlight Format from sklearn.datasets import load_svmlight_file, load_svmlight_files # Load datasets # [Your code here] # Output number of instances and features print(f\\"Training dataset: {num_instances} instances, {num_features} features\\") # Part 3: Fetching Datasets from OpenML from sklearn.datasets import fetch_openml # Fetch dataset # [Your code here] # Output shape and description print(f\\"Data shape: {data_shape}\\") print(f\\"Target shape: {target_shape}\\") print(f\\"Dataset description: {dataset_description}\\") ```","solution":"# Part 1: Loading and Processing Sample Images from sklearn.datasets import load_sample_image import matplotlib.pyplot as plt import numpy as np def process_image(): # Load the sample image china = load_sample_image(\\"china.jpg\\") # Convert the image to a floating-point representation china_float = china.astype(np.float32) # Scale the image data to the range [0, 1] china_scaled = china_float / 255.0 # Display the image using matplotlib plt.imshow(china_scaled) plt.axis(\'off\') plt.tight_layout() plt.show() # Part 2: Loading Sparse Datasets in svmlight Format from sklearn.datasets import load_svmlight_files def load_svmlight_datasets(train_file, test_file): # Load the datasets from the given train and test files X_train, y_train, X_test, y_test = load_svmlight_files([train_file, test_file]) # Check if both datasets have the same number of features assert X_train.shape[1] == X_test.shape[1], \\"Number of features do not match\\" # Output number of instances and features in the training dataset num_instances, num_features = X_train.shape return num_instances, num_features # Part 3: Fetching Datasets from OpenML from sklearn.datasets import fetch_openml def fetch_openml_iris(): # Fetch the iris dataset from the OpenML repository iris = fetch_openml(name=\'iris\', version=1) # Output the shape of the data and target arrays data_shape = iris.data.shape target_shape = iris.target.shape # Output the description of the dataset dataset_description = iris.DESCR return data_shape, target_shape, dataset_description"},{"question":"Coding Assessment Question # Objective You are tasked with writing a Python script that optimizes memory management using Python\'s `gc` module. The task is to implement several functions that demonstrate the proper usage of the `gc` module to enhance performance and monitor memory usage. # Requirements 1. **Implement the function `manage_gc(enable: bool) -> None`**: This function should either enable or disable the garbage collector based on the input argument. - If `enable` is `True`, the garbage collector should be enabled. - If `enable` is `False`, the garbage collector should be disabled. 2. **Implement the function `collect_garbage(generation: int = 2) -> int`**: This function should force garbage collection for the specified generation. - The function should raise a `ValueError` if the generation is not between 0 and 2 (inclusive). - The function should return the number of unreachable objects found. 3. **Implement the function `set_gc_thresholds(thresholds: tuple) -> None`**: This function should set the garbage collection thresholds. - The input `thresholds` should be a tuple of three integers `(threshold0, threshold1, threshold2)`. - The function should update the thresholds using `gc.set_threshold`. 4. **Implement the function `monitor_gc_statistics() -> dict`**: This function should return current garbage collection statistics. - The returned dictionary should contain the following keys: \\"collections\\", \\"collected\\", \\"uncollectable\\". - Use `gc.get_stats` to retrieve the required statistics. 5. **Implement the function `debug_memory_leaks(enable: bool) -> None`**: This function should enable or disable memory leak debugging. - If `enable` is `True`, set the debugging flags to `gc.DEBUG_LEAK`. - If `enable` is `False`, reset the debugging flags to 0. 6. **Implement the function `is_object_tracked(obj: object) -> bool`**: This function should return `True` if the object is currently tracked by the garbage collector, else `False`. # Constraints - You can assume that the garbage collector module (`gc`) is always available. - The functions should be efficient and not cause unnecessary performance overhead. - Handle all exceptions gracefully and provide meaningful error messages. # Example Usage ```python import gc from typing import Tuple def manage_gc(enable: bool) -> None: pass def collect_garbage(generation: int = 2) -> int: pass def set_gc_thresholds(thresholds: Tuple[int, int, int]) -> None: pass def monitor_gc_statistics() -> dict: pass def debug_memory_leaks(enable: bool) -> None: pass def is_object_tracked(obj: object) -> bool: pass # Example Usage: manage_gc(enable=True) print(gc.isenabled()) # Should be True unreachable_objects = collect_garbage(generation=1) print(f\\"Unreachable objects: {unreachable_objects}\\") set_gc_thresholds((700, 10, 10)) stats = monitor_gc_statistics() print(f\\"GC Statistics: {stats}\\") debug_memory_leaks(enable=True) print(is_object_tracked([])) # Should be True for a list ``` # Note Make sure to test all your functions rigorously and include docstrings for each function explaining its purpose and how to use it.","solution":"import gc from typing import Tuple def manage_gc(enable: bool) -> None: Enables or disables the garbage collector based on the input argument. :param enable: If True, the garbage collector is enabled. If False, it is disabled. if enable: gc.enable() else: gc.disable() def collect_garbage(generation: int = 2) -> int: Forces garbage collection for the specified generation. :param generation: The generation to collect garbage for (0, 1, or 2). :return: The number of unreachable objects found. if not (0 <= generation <= 2): raise ValueError(\\"Generation must be between 0 and 2 (inclusive).\\") return gc.collect(generation) def set_gc_thresholds(thresholds: Tuple[int, int, int]) -> None: Sets the garbage collection thresholds. :param thresholds: A tuple of three integers (threshold0, threshold1, threshold2). if len(thresholds) != 3 or not all(isinstance(t, int) for t in thresholds): raise ValueError(\\"Thresholds must be a tuple of three integers.\\") gc.set_threshold(*thresholds) def monitor_gc_statistics() -> dict: Returns current garbage collection statistics. :return: Dictionary containing keys \'collections\', \'collected\', \'uncollectable\'. stats = gc.get_stats() return { \\"collections\\": [stats[i][\'collections\'] for i in range(3)], \\"collected\\": [stats[i][\'collected\'] for i in range(3)], \\"uncollectable\\": [stats[i][\'uncollectable\'] for i in range(3)] } def debug_memory_leaks(enable: bool) -> None: Enables or disables memory leak debugging. :param enable: If True, enables memory leak debugging. If False, disables it. if enable: gc.set_debug(gc.DEBUG_LEAK) else: gc.set_debug(0) def is_object_tracked(obj: object) -> bool: Returns True if the object is currently tracked by the garbage collector. :param obj: The object to check. :return: Boolean indicating if the object is tracked. return gc.is_tracked(obj)"},{"question":"**Title: Managing Contextual State in Asynchronous Tasks** Objective: Design and implement an asynchronous logging system using context variables to manage log levels that are unique to each task. This system should allow each task to define and update its log level, which will not interfere with the log levels of other tasks. Problem Statement: You are required to create an asynchronous logging system where each task can set its own log level (e.g., DEBUG, INFO, WARNING, ERROR). The log level is managed using context variables. Implement the following: 1. A context variable to store the log level. 2. A function to set the log level for the current task. 3. A function to log messages, which includes the log level and message. 4. Demonstrate the functionality with an asyncio-based example where multiple tasks set different log levels and log messages concurrently. Specifications: 1. **Context Variable**: - Name: `log_level_var` - Default Value: `\\"INFO\\"` 2. **Functions**: 1. `set_log_level(level: str) -> None`: - Sets the log level for the current context. - Parameters: - `level`: A string representing the log level (e.g., \\"DEBUG\\", \\"INFO\\", \\"WARNING\\", \\"ERROR\\"). 2. `log(message: str) -> None`: - Logs a message with the current context\'s log level. - Parameters: - `message`: A string representing the log message. 3. **Async Example**: - Create multiple asynchronous tasks using `asyncio` where: - Each task sets a different log level. - Each task logs multiple messages. - Use `asyncio.gather` to run the tasks concurrently and verify that the log levels do not interfere with each other. Constraints: - You cannot use any global variables to store the state. - Utilize `contextvars` for managing log levels. Example Usage: ```python import asyncio import contextvars # 1. Declare the context variable log_level_var = contextvars.ContextVar(\'log_level\', default=\'INFO\') # 2. Function to set log level def set_log_level(level: str) -> None: log_level_var.set(level) # 3. Function to log messages def log(message: str) -> None: log_level = log_level_var.get() print(f\\"[{log_level}] {message}\\") # Asynchronous tasks demonstration async def task_one(): set_log_level(\'DEBUG\') log(\\"Task one debug message\\") await asyncio.sleep(1) log(\\"Task one another debug message\\") async def task_two(): set_log_level(\'ERROR\') log(\\"Task two error message\\") await asyncio.sleep(1) log(\\"Task two another error message\\") async def main(): await asyncio.gather(task_one(), task_two()) if __name__ == \'__main__\': asyncio.run(main()) ``` The expected output should demonstrate that each task logs messages with its own set log level, showcasing the isolation provided by context variables.","solution":"import asyncio import contextvars # 1. Declare the context variable log_level_var = contextvars.ContextVar(\'log_level\', default=\'INFO\') # 2. Function to set log level def set_log_level(level: str) -> None: log_level_var.set(level) # 3. Function to log messages def log(message: str) -> None: log_level = log_level_var.get() print(f\\"[{log_level}] {message}\\") # Asynchronous tasks demonstration async def task_one(): set_log_level(\'DEBUG\') log(\\"Task one debug message\\") await asyncio.sleep(1) log(\\"Task one another debug message\\") async def task_two(): set_log_level(\'ERROR\') log(\\"Task two error message\\") await asyncio.sleep(1) log(\\"Task two another error message\\") async def main(): await asyncio.gather(task_one(), task_two()) if __name__ == \'__main__\': asyncio.run(main())"},{"question":"You are tasked with writing a Python function that uses the Unix group database to find the names of groups where the given user is a member. You need to implement this using the `grp` module. Function Signature ```python def find_user_groups(username: str) -> list: pass ``` Objectives - The function should take a `username` (string) as input. - It should return a list of group names (strings) where the given `username` is a member. - Make sure that the group names are returned in alphabetical order. Input - `username` (string): The username to search for in the group database. Assume the username is a valid string containing only alphanumeric characters. Output - A list of group names (strings) that the given user is a member of, sorted in alphabetical order. If the user is not a member of any group, return an empty list. Example ```python # Suppose the following groups exist in the Unix group database: # group1: {\\"gr_name\\": \\"admins\\", \\"gr_passwd\\": \\"\\", \\"gr_gid\\": 1001, \\"gr_mem\\": [\\"john\\", \\"alice\\"]} # group2: {\\"gr_name\\": \\"users\\", \\"gr_passwd\\": \\"\\", \\"gr_gid\\": 1002, \\"gr_mem\\": [\\"alice\\", \\"bob\\"]} # group3: {\\"gr_name\\": \\"guests\\", \\"gr_passwd\\": \\"\\", \\"gr_gid\\": 1003, \\"gr_mem\\": [\\"guest\\"]} assert find_user_groups(\\"alice\\") == [\\"admins\\", \\"users\\"] assert find_user_groups(\\"john\\") == [\\"admins\\"] assert find_user_groups(\\"bob\\") == [\\"users\\"] assert find_user_groups(\\"guest\\") == [\\"guests\\"] assert find_user_groups(\\"nonexistentuser\\") == [] ``` Constraints - Use the `grp` module to access the Unix group database. - Handle exceptions properly, especially when dealing with potential inaccessible group entries. - Ensure the solution is optimized to handle cases where there may be a large number of groups. Good luck!","solution":"import grp def find_user_groups(username: str) -> list: Returns a list of group names where the given username is a member. The list is sorted in alphabetical order. :param username: The username to search for in the group database. :return: A list of group names. groups = [] try: all_groups = grp.getgrall() for group in all_groups: if username in group.gr_mem: groups.append(group.gr_name) except Exception as e: pass # Handle any exceptions gracefully return sorted(groups)"},{"question":"Problem Statement You have been tasked with creating a simple interactive command shell for managing a list of tasks. Each task has a description and a status (either \'pending\' or \'completed\'). Your command shell should allow users to add tasks, list tasks, mark tasks as complete, and delete tasks. The shell should also support saving the current list of tasks to a file and loading tasks from a file. Implement a class `TaskShell` that inherits from `cmd.Cmd` and provides the following commands: # Commands - `add <description>`: Adds a new task with the given description and a \'pending\' status. - `list`: Lists all tasks with their indexes, descriptions, and statuses. - `complete <index>`: Marks the task at the given index as \'completed\'. - `delete <index>`: Deletes the task at the given index. - `save <filename>`: Saves the current list of tasks to the specified file. - `load <filename>`: Loads tasks from the specified file, replacing the current list of tasks. # Constraints 1. The task description is a non-empty string. 2. The index provided for `complete` and `delete` commands is a valid integer representing an existing task. # Example Interaction ``` Welcome to the task manager shell. Type help or ? to list commands. (task) help Documented commands (type help <topic>): ======================================== add complete delete list load save (task) add \\"Buy groceries\\" (task) add \\"Write unit tests\\" (task) list 0: Buy groceries [pending] 1: Write unit tests [pending] (task) complete 0 (task) list 0: Buy groceries [completed] 1: Write unit tests [pending] (task) save tasks.txt (task) delete 1 (task) list 0: Buy groceries [completed] (task) load tasks.txt (task) list 0: Buy groceries [completed] 1: Write unit tests [pending] (task) bye ``` # Task Implement the `TaskShell` class with the given commands. Ensure your code handles edge cases, such as invalid indexes or file I/O errors gracefully, printing appropriate error messages. # Implementation ```python import cmd class TaskShell(cmd.Cmd): intro = \\"Welcome to the task manager shell. Type help or ? to list commands.n\\" prompt = \\"(task) \\" def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task with the given description: ADD <description>\' if arg: self.tasks.append({\'description\': arg, \'status\': \'pending\'}) print(f\'Task added: {arg}\') else: print(\'Error: A task description is required.\') def do_list(self, arg): \'List all tasks\' for idx, task in enumerate(self.tasks): print(f\'{idx}: {task[\\"description\\"]} [{task[\\"status\\"]}]\') def do_complete(self, arg): \'Mark the task at the given index as completed: COMPLETE <index>\' try: idx = int(arg) if 0 <= idx < len(self.tasks): self.tasks[idx][\'status\'] = \'completed\' print(f\'Task {idx} marked as completed.\') else: print(\'Error: Invalid task index.\') except ValueError: print(\'Error: Please provide a valid task index.\') def do_delete(self, arg): \'Delete the task at the given index: DELETE <index>\' try: idx = int(arg) if 0 <= idx < len(self.tasks): self.tasks.pop(idx) print(f\'Task {idx} deleted.\') else: print(\'Error: Invalid task index.\') except ValueError: print(\'Error: Please provide a valid task index.\') def do_save(self, arg): \'Save the current list of tasks to the specified file: SAVE <filename>\' if arg: try: with open(arg, \'w\') as f: for task in self.tasks: f.write(f\'{task[\\"description\\"]}t{task[\\"status\\"]}n\') print(f\'Tasks saved to {arg}\') except Exception as e: print(f\'Error: Unable to save tasks to file. ({e})\') else: print(\'Error: A filename is required.\') def do_load(self, arg): \'Load tasks from the specified file, replacing the current list of tasks: LOAD <filename>\' if arg: try: with open(arg, \'r\') as f: self.tasks = [] for line in f: description, status = line.strip().split(\'t\') self.tasks.append({\'description\': description, \'status\': status}) print(f\'Tasks loaded from {arg}\') except Exception as e: print(f\'Error: Unable to load tasks from file. ({e})\') else: print(\'Error: A filename is required.\') def do_bye(self, arg): \'Exit the shell: BYE\' print(\'Thank you for using the task manager shell.\') return True if __name__ == \'__main__\': TaskShell().cmdloop() ```","solution":"import cmd class TaskShell(cmd.Cmd): intro = \\"Welcome to the task manager shell. Type help or ? to list commands.n\\" prompt = \\"(task) \\" def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task with the given description: ADD <description>\' if arg: self.tasks.append({\'description\': arg, \'status\': \'pending\'}) print(f\'Task added: {arg}\') else: print(\'Error: A task description is required.\') def do_list(self, arg): \'List all tasks\' for idx, task in enumerate(self.tasks): print(f\'{idx}: {task[\\"description\\"]} [{task[\\"status\\"]}]\') def do_complete(self, arg): \'Mark the task at the given index as completed: COMPLETE <index>\' try: idx = int(arg) if 0 <= idx < len(self.tasks): self.tasks[idx][\'status\'] = \'completed\' print(f\'Task {idx} marked as completed.\') else: print(\'Error: Invalid task index.\') except ValueError: print(\'Error: Please provide a valid task index.\') def do_delete(self, arg): \'Delete the task at the given index: DELETE <index>\' try: idx = int(arg) if 0 <= idx < len(self.tasks): self.tasks.pop(idx) print(f\'Task {idx} deleted.\') else: print(\'Error: Invalid task index.\') except ValueError: print(\'Error: Please provide a valid task index.\') def do_save(self, arg): \'Save the current list of tasks to the specified file: SAVE <filename>\' if arg: try: with open(arg, \'w\') as f: for task in self.tasks: f.write(f\'{task[\\"description\\"]}t{task[\\"status\\"]}n\') print(f\'Tasks saved to {arg}\') except Exception as e: print(f\'Error: Unable to save tasks to file. ({e})\') else: print(\'Error: A filename is required.\') def do_load(self, arg): \'Load tasks from the specified file, replacing the current list of tasks: LOAD <filename>\' if arg: try: with open(arg, \'r\') as f: self.tasks = [] for line in f: description, status = line.strip().split(\'t\') self.tasks.append({\'description\': description, \'status\': status}) print(f\'Tasks loaded from {arg}\') except Exception as e: print(f\'Error: Unable to load tasks from file. ({e})\') else: print(\'Error: A filename is required.\') def do_bye(self, arg): \'Exit the shell: BYE\' print(\'Thank you for using the task manager shell.\') return True if __name__ == \'__main__\': TaskShell().cmdloop()"},{"question":"# Seaborn KDE Plot Assessment You have been provided with two datasets: `tips` and `geyser`, which are available in the Seaborn library. Task 1: Univariate KDE Plot 1. Load the `tips` dataset. 2. Create a KDE plot for the `total_bill` column. 3. Adjust the smoothing bandwidth to be less smoothed (use `bw_adjust=0.5`). 4. Shade the area under the KDE curve using `fill=True`. 5. Assign different colors to the KDE plots using a `hue` for `time` (Lunch/Dinner). Task 2: Bivariate KDE Plot 1. Load the `geyser` dataset. 2. Create a bivariate KDE plot with `waiting` on the x-axis and `duration` on the y-axis. 3. Use a different colormap for better visibility, e.g., `cmap=\'coolwarm\'`. 4. Fill the contours using `fill=True`. 5. Differentiate the data distribution by the `kind` column using `hue=\'kind\'`. # Code Requirements - The solution must utilize the `sns.kdeplot` function for all plots. - Ensure proper plot titles and axis labels for readability. - Include the dataset loading part in your code. # Expected Outputs Your code should produce two plots: 1. **Univariate KDE plot for the `total_bill` from the `tips` dataset with hue for `time`.** 2. **Bivariate KDE plot for `waiting` vs `duration` from the `geyser` dataset, with filled contours and hue for `kind`.** ```python # Task 1: Univariate KDE plot import seaborn as sns import matplotlib.pyplot as plt # Load tips dataset tips = sns.load_dataset(\\"tips\\") # Create univariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", bw_adjust=0.5, fill=True) plt.title(\\"KDE Plot of Total Bill with Time Hue\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.show() # Task 2: Bivariate KDE plot # Load geyser dataset geyser = sns.load_dataset(\\"geyser\\") # Create bivariate KDE plot plt.figure(figsize=(12, 8)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, cmap=\\"coolwarm\\") plt.title(\\"Bivariate KDE Plot of Waiting vs Duration with Kind Hue\\") plt.xlabel(\\"Waiting Time\\") plt.ylabel(\\"Duration\\") plt.show() ``` This assessment will test your ability to use the Seaborn library for creating KDE plots, interpreting plots with conditional distributions, and applying various customizations for aesthetics and clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Task 1: Univariate KDE plot def create_univariate_kde_plot(): # Load tips dataset tips = sns.load_dataset(\\"tips\\") # Create univariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", bw_adjust=0.5, fill=True) plt.title(\\"KDE Plot of Total Bill with Time Hue\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.show() # Task 2: Bivariate KDE plot def create_bivariate_kde_plot(): # Load geyser dataset geyser = sns.load_dataset(\\"geyser\\") # Create bivariate KDE plot plt.figure(figsize=(12, 8)) sns.kdeplot(data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, cmap=\\"coolwarm\\") plt.title(\\"Bivariate KDE Plot of Waiting vs Duration with Kind Hue\\") plt.xlabel(\\"Waiting Time\\") plt.ylabel(\\"Duration\\") plt.show() # Functions to generate the plots create_univariate_kde_plot() create_bivariate_kde_plot()"},{"question":"**Title**: Securely Extracting and Modifying Tar Archives **Problem Statement**: You are given a compressed tar archive file containing various types of files including regular files, directories, symbolic links, and hard links. As part of a security measure, you need to write a Python function to extract only the regular files and directories from this tar archive, preventing any potential security risks introduced by special files like symbolic links, hard links, or device files. Additionally, for any directories or files extracted, ensure that: - All extracted directories have their permission bits set to `0o755`. - All extracted regular files have their permission bits set to `0o644`. - The user and group information (uid, gid) are set to that of the current user, and user and group names are set to \\"user\\". Implement the function `extract_securely(tar_file_path: str, extract_path: str) -> None` with the following specifications: - **Input**: - `tar_file_path` (str): The path to the compressed tar archive. - `extract_path` (str): The directory where the contents of the tar file should be extracted. - **Output**: The function does not return anything. - **Constraints**: - The tar archive is compressed using gzip. - Assume the tar file and extraction path are valid and accessible. - The function must handle potential extraction errors gracefully and ensure directory structure is maintained as in the archive. - Ensure that symbolic and hard links, as well as special files, are not extracted. **Example**: ```python import os # Given a tar archive \'example.tar.gz\' located at \'/path/to/tar/example.tar.gz\' # Extract contents to the directory \'/path/to/extract/\' extract_securely(\'/path/to/tar/example.tar.gz\', \'/path/to/extract/\') ``` **Notes**: - Use the `tarfile` module as mentioned in the documentation. - Implement the necessary checks and use filters to handle security concerns during the extraction process. - For setting the current user’s uid and gid, you can utilize `os.getuid()` and `os.getgid()` respectively.","solution":"import os import tarfile import pwd import grp def extract_securely(tar_file_path, extract_path): def is_within_directory(directory, candidate): abs_directory = os.path.abspath(directory) abs_candidate = os.path.abspath(candidate) return os.path.commonprefix([abs_directory, abs_candidate]) == abs_directory def safe_extract(tar, path=\\".\\", members=None): for member in members: member_path = os.path.join(path, member.name) if is_within_directory(path, member_path): tar.extract(member, path) # Get current user\'s UID and GID current_uid = os.getuid() current_gid = os.getgid() with tarfile.open(tar_file_path, \'r:gz\') as tar: members = [] for member in tar.getmembers(): if member.isfile() or member.isdir(): members.append(member) safe_extract(tar, path=extract_path, members=members) for member in members: member_path = os.path.join(extract_path, member.name) if os.path.isfile(member_path): os.chmod(member_path, 0o644) elif os.path.isdir(member_path): os.chmod(member_path, 0o755) os.chown(member_path, current_uid, current_gid)"},{"question":"Implement a Python function named `custom_traceback_formatter` that captures an exception, extracts its traceback, and returns a formatted string of the entire traceback using the `traceback` module. # Function Signature ```python def custom_traceback_formatter() -> str: ``` # Requirements 1. **Capture Exception:** The function should simulate an exception by purposely raising an error within a nested function call. 2. **Extract Traceback:** Use the appropriate functions from the `traceback` module to extract the traceback information. 3. **Format Exception:** Format the extracted traceback into a string using the `traceback` module\'s formatting functions. 4. **Return Formatted String:** The function should return the formatted traceback string. # Example ```python try: custom_traceback_formatter() except: formatted_traceback = custom_traceback_formatter() print(formatted_traceback) ``` **Expected Output:** ``` Traceback (most recent call last): File \\"your_script.py\\", line XX, in function_caller function_caller() File \\"your_script.py\\", line YY, in function_caller function_nested_caller() File \\"your_script.py\\", line ZZ, in function_nested_caller raise ValueError(\\"This is a custom error for testing traceback\\") ValueError: This is a custom error for testing traceback ``` # Constraints - Ensure the error raised is a `ValueError` with a custom error message. - The traceback should include at least two nested function calls before the error is raised. - Use `traceback.format_exception`, `traceback.extract_tb`, or related functions for traceback extraction and formatting. This question assesses the student\'s ability to handle and format tracebacks, understand the nested function calls, and properly utilize the traceback module for debugging purposes.","solution":"import traceback def function_nested_caller(): raise ValueError(\\"This is a custom error for testing traceback\\") def function_caller(): function_nested_caller() def custom_traceback_formatter() -> str: try: function_caller() except Exception as e: tb_str = \'\'.join(traceback.format_exception(type(e), e, e.__traceback__)) return tb_str"},{"question":"Threading and Synchronization **Objective:** Implement a thread-safe version of a bounded producer-consumer system using the `threading` module. You must ensure that producers do not produce items when the buffer is full and consumers do not consume items when the buffer is empty, with the proper use of synchronization primitives. **Problem Statement:** You are tasked to implement a thread-safe bounded buffer using the `threading` module where multiple producers can produce items and multiple consumers can consume those items. The buffer should have a fixed size, and synchronization must be ensured using `threading.Condition` to avoid race conditions. **Function Specifications:** 1. **class BoundedBuffer:** - **__init__(self, size: int):** Initialize the bounded buffer with a specified size. - **produce(self, item: Any):** A producer thread calls this method to add an item to the buffer. If the buffer is full, wait until a consumer consumes an item. - **consume(self) -> Any:** A consumer thread calls this method to remove and return an item from the buffer. If the buffer is empty, wait until a producer adds an item. **Example:** ```python import threading import time class BoundedBuffer: def __init__(self, size: int): # Initialize the buffer, lock, and conditions here pass def produce(self, item: Any): # Produce an item; wait if the buffer is full pass def consume(self) -> Any: # Consume an item; wait if the buffer is empty pass # Example usage: buffer = BoundedBuffer(5) def producer(): for i in range(10): buffer.produce(i) print(f\'Produced {i}\') time.sleep(1) def consumer(): for i in range(10): item = buffer.consume() print(f\'Consumed {item}\') time.sleep(2) producer_thread = threading.Thread(target=producer) consumer_thread = threading.Thread(target=consumer) producer_thread.start() consumer_thread.start() producer_thread.join() consumer_thread.join() ``` **Constraints:** - Producers should wait if the buffer is full. - Consumers should wait if the buffer is empty. - The buffer size is fixed and passed as an argument during the initialization. - Use synchronization primitives from the `threading` module to ensure thread safety. **Notes:** - Ensure proper usage of `threading.Condition` for synchronization. - The implementation should handle multiple producer and consumer threads concurrently.","solution":"import threading from typing import Any from collections import deque class BoundedBuffer: def __init__(self, size: int): self.size = size self.buffer = deque() self.lock = threading.Lock() self.not_full = threading.Condition(self.lock) self.not_empty = threading.Condition(self.lock) def produce(self, item: Any): with self.not_full: while len(self.buffer) >= self.size: self.not_full.wait() self.buffer.append(item) self.not_empty.notify() def consume(self) -> Any: with self.not_empty: while len(self.buffer) == 0: self.not_empty.wait() item = self.buffer.popleft() self.not_full.notify() return item"},{"question":"**Python Coding Assessment Question: Using `unittest.mock` to Test Functionality** # Objective Create a series of unit tests for a function using the `unittest.mock` library. The function to be tested interacts with external dependencies that should be mocked to ensure isolated testing. # Function Under Test ```python import requests def fetch_data_and_process(url): try: response = requests.get(url) response.raise_for_status() data = response.json() processed_data = process_data(data) save_data(processed_data) except requests.HTTPError as http_err: handle_error(f\\"HTTP error occurred: {http_err}\\") except Exception as err: handle_error(f\\"Other error occurred: {err}\\") def process_data(data): # Assume this function processes data in some way return data def save_data(data): # Assume this function saves processed data somewhere pass def handle_error(error_message): # Error handling function print(error_message) ``` # Task Write a test class `TestFetchDataAndProcess` using the `unittest` framework and `unittest.mock` library to test the `fetch_data_and_process` function. Your tests should verify the following scenarios: 1. **Successful Fetch and Process:** - The function `requests.get` is called correctly. - The response is processed and passed to `save_data`. 2. **HTTP Error Handling:** - A `requests.HTTPError` is raised and handled correctly, with `handle_error` being called with the appropriate error message. 3. **Generic Exception Handling:** - An unexpected exception is raised and handled correctly, with `handle_error` being called with the appropriate error message. # Constraints - You are not allowed to edit the `fetch_data_and_process` function itself. - Ensure that all external calls (network calls, data processing, and saving) are properly mocked. - Ensure that your tests are isolated and do not depend on actual external systems. # Example Test Class Outline ```python import unittest from unittest.mock import patch, Mock, call class TestFetchDataAndProcess(unittest.TestCase): @patch(\'your_module.requests.get\') @patch(\'your_module.process_data\') @patch(\'your_module.save_data\') def test_successful_fetch_and_process(self, mock_save_data, mock_process_data, mock_requests_get): # Implement your test here @patch(\'your_module.requests.get\', side_effect=requests.HTTPError(\\"Not Found\\")) @patch(\'your_module.handle_error\') def test_http_error_handling(self, mock_handle_error, mock_requests_get): # Implement your test here @patch(\'your_module.requests.get\', side_effect=Exception(\\"Random Error\\")) @patch(\'your_module.handle_error\') def test_generic_exception_handling(self, mock_handle_error, mock_requests_get): # Implement your test here if __name__ == \'__main__\': unittest.main() ``` # Instructions 1. Implement the `test_successful_fetch_and_process` method to ensure that all steps of a successful data fetch and process are verified. 2. Implement the `test_http_error_handling` method to verify that an HTTP error is handled correctly. 3. Implement the `test_generic_exception_handling` method to verify that generic exceptions are handled correctly. Your implementation should capture: - Assertion on function calls and their parameters. - Correct handling of side effects for exceptions. Submit your implementation as a single Python file.","solution":"import requests def fetch_data_and_process(url): try: response = requests.get(url) response.raise_for_status() data = response.json() processed_data = process_data(data) save_data(processed_data) except requests.HTTPError as http_err: handle_error(f\\"HTTP error occurred: {http_err}\\") except Exception as err: handle_error(f\\"Other error occurred: {err}\\") def process_data(data): # Assume this function processes data in some way return data def save_data(data): # Assume this function saves processed data somewhere pass def handle_error(error_message): # Error handling function print(error_message)"},{"question":"Objective Demonstrate your understanding of pandas plotting functions by visualizing a given dataset. Problem Statement You are given a dataset in the form of a CSV file named `data.csv`. The dataset contains various numerical columns and possibly some categorical columns. Your task is to create several plots to visualize different aspects of the dataset using functions from the `pandas.plotting` module. Specifications 1. **Input:** - The CSV file `data.csv` will be provided. - You can assume the file will have headers. 2. **Output:** - Generate and display the following plots: 1. An Andrews Curves plot of the data. 2. A parallel coordinates plot of the data. 3. A scatter matrix plot of the data. 4. A lag plot for a specified column (you may choose the column). 5. An autocorrelation plot for the same column used in the lag plot. Implementation Details: 1. Load the dataset using `pandas.read_csv`. 2. Ensure the inclusion of appropriate labels and titles for each plot. 3. Display each plot sequentially within a Jupyter notebook or a script. 4. Use the `pandas.plotting` module functions specified above to create the plots. 5. Handle any potential errors in data plotting gracefully with appropriate error messages. Example Code Template: ```python import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import andrews_curves, parallel_coordinates, scatter_matrix, lag_plot, autocorrelation_plot # Load dataset data = pd.read_csv(\'data.csv\') # 1. Andrews Curves plot plt.figure() andrews_curves(data, \'target_column\') # Replace \'target_column\' with the appropriate name in your dataset plt.title(\'Andrews Curves Plot\') plt.show() # 2. Parallel Coordinates plot plt.figure() parallel_coordinates(data, \'target_column\') # Replace \'target_column\' with the appropriate name in your dataset plt.title(\'Parallel Coordinates Plot\') plt.show() # 3. Scatter Matrix plot scatter_matrix(data) plt.suptitle(\'Scatter Matrix Plot\') plt.show() # 4. Lag plot for a specified column plt.figure() lag_plot(data[\'column_name\']) # Replace \'column_name\' with the appropriate name in your dataset plt.title(\'Lag Plot\') plt.show() # 5. Autocorrelation plot for the same column plt.figure() autocorrelation_plot(data[\'column_name\']) # Replace \'column_name\' with the appropriate name in your dataset plt.title(\'Autocorrelation Plot\') plt.show() ``` Constraints: - Ensure that your code runs without errors for any properly formatted CSV file. - Handle missing values in the dataset if necessary. Performance Requirements: - The solution should be efficient and readable. - Proper documentation and comments should be included for clarity. This assessment question tests students\' ability to read a dataset, utilize functions from the `pandas.plotting` module, and create meaningful visual representations of the data.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import andrews_curves, parallel_coordinates, scatter_matrix, lag_plot, autocorrelation_plot def visualize_data(file_path): Reads the dataset from the provided CSV file path and generates various plots to visualize the data. :param file_path: str: Path to the CSV file # Load dataset data = pd.read_csv(file_path) # Identify the target column and a specific numerical column for lag and autocorrelation plots # Assumes there is a \'target\' column for plotting and using the first numerical column for lag plots target_column = \'target\' num_column = data.select_dtypes(include=[\'number\']).columns[0] # 1. Andrews Curves plot plt.figure() andrews_curves(data, target_column) plt.title(\'Andrews Curves Plot\') plt.show() # 2. Parallel Coordinates plot plt.figure() parallel_coordinates(data, target_column) plt.title(\'Parallel Coordinates Plot\') plt.show() # 3. Scatter Matrix plot scatter_matrix(data) plt.suptitle(\'Scatter Matrix Plot\') plt.show() # 4. Lag plot for a specified column plt.figure() lag_plot(data[num_column]) plt.title(\'Lag Plot\') plt.show() # 5. Autocorrelation plot for the same column plt.figure() autocorrelation_plot(data[num_column]) plt.title(\'Autocorrelation Plot\') plt.show()"},{"question":"# Pandas Options Configuration You are given a dataset containing information about sales for a retail store. Your task is to write a function that configures various pandas display options to display the dataset in specific formats and reset the options to their default values after displaying the data. Function Signature ```python def configure_pandas_options_display(df: pd.DataFrame) -> None: Configures pandas display options to show the DataFrame in specific formats and resets the options to default. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: None: This function prints the DataFrame using specific pandas display options. ``` Requirements 1. Set the maximum number of rows to be displayed to 10. 2. Set the maximum number of columns to be displayed to 5. 3. Set the precision of floating-point numbers to 4 decimal places. 4. Enable \'expand_frame_repr\' so that the DataFrame representation stretches across the width of the screen. 5. Display the DataFrame with these settings, then reset all the options to their default values. 6. Print the DataFrame again after resetting the options to confirm that the settings have been reverted. Input - A pandas DataFrame `df` containing the sales data. Output - Print the DataFrame twice, first with the custom settings applied and then with the default settings restored. Constraints - The function should only print the DataFrame. Do not return the DataFrame or any other value. - Assume the DataFrame provided contains at least 20 rows and 10 columns with numerical data for testing purposes. Example ```python import pandas as pd import numpy as np # Sample DataFrame df = pd.DataFrame(np.random.randn(20, 10), columns=[f\'col_{i}\' for i in range(10)]) configure_pandas_options_display(df) # The first output should adhere to the custom settings # The second output should show the default pandas display settings ``` Your implementation should demonstrate a clear understanding of the pandas options API, including setting and resetting options, and using the `option_context` manager if necessary.","solution":"import pandas as pd def configure_pandas_options_display(df: pd.DataFrame) -> None: Configures pandas display options to show the DataFrame in specific formats and resets the options to default. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: None: This function prints the DataFrame using specific pandas display options. # Set pandas display options with pd.option_context(\'display.max_rows\', 10, \'display.max_columns\', 5, \'display.precision\', 4, \'display.expand_frame_repr\', True): print(\\"DataFrame with custom settings:\\") print(df) # Reset pandas display options to default and print again print(\\"nDataFrame with default settings:\\") print(df)"},{"question":"# Question: Advanced Audio Processing with `audioop` You are given a raw audio file containing 16-bit signed samples in a bytes-like object named `audio_data`. Your task is to process this audio fragment to perform several transformations and statistical analyses using the `audioop` module. Requirements 1. **Convert to Mono**: The audio fragment may contain stereo data. Convert this fragment to mono by appropriately combining the left and right channel samples. 2. **Normalize Audio**: Normalize the audio samples so that the maximum absolute value of any sample is 32767 (the maximum for 16-bit signed integers). This ensures the audio uses the full dynamic range after processing. 3. **Calculate RMS**: Calculate and return the root-mean-square (RMS) value of the normalized audio fragment. The RMS value is a measure of the power of the audio signal. 4. **Generate 8000 Hz Sample Rate Version**: Convert the audio fragment to an output sample rate of 8000 Hz, assuming the original sample rate is 44100 Hz. This will involve resampling the audio data. Input - `audio_data` (bytes): The raw audio data containing 16-bit signed samples in stereo format. Output - A dictionary with the following keys: - `\\"mono_audio\\"` (bytes): The mono audio fragment. - `\\"normalized_audio\\"` (bytes): The normalized audio fragment. - `\\"rms\\"` (float): The RMS value of the normalized audio fragment. - `\\"resampled_audio\\"` (bytes): The audio fragment resampled to 8000 Hz. Constraints - The input audio data will be in stereo format with 16-bit signed samples. - The output sample rate for the resampled audio is fixed at 8000 Hz. Function Signature ```python def process_audio(audio_data: bytes) -> dict: pass ``` # Notes - Use the `audioop` module functions to perform the required operations. - Ensure that proper handling of the sample width (i.e., 2 bytes for 16-bit samples) is maintained throughout the processing. - You can assume `audio_data` will always have an even length, corresponding to complete stereo samples.","solution":"import audioop def process_audio(audio_data: bytes) -> dict: # Convert stereo to mono mono_audio = audioop.tomono(audio_data, 2, 0.5, 0.5) # Normalize the audio max_amp = audioop.max(mono_audio, 2) factor = 32767 / max_amp if max_amp != 0 else 1 normalized_audio = audioop.mul(mono_audio, 2, factor) # Calculate RMS of normalized audio rms = audioop.rms(normalized_audio, 2) # Resample to 8000 Hz resampled_audio, _ = audioop.ratecv(normalized_audio, 2, 1, 44100, 8000, None) return { \\"mono_audio\\": mono_audio, \\"normalized_audio\\": normalized_audio, \\"rms\\": rms, \\"resampled_audio\\": resampled_audio }"},{"question":"**IP Address Manipulation and Validation** # Objective You are tasked with creating a function that takes a mixed list of IPv4 and IPv6 address strings, validates them, and returns a summary containing the valid and invalid addresses separately. # Requirements 1. **Function Signature**: ```python def validate_and_summarize(address_list: list) -> dict: ``` 2. **Input**: - `address_list`: A list of strings where each string represents an IP address (either IPv4 or IPv6). 3. **Output**: - A dictionary with two keys: - `\'valid\'`: A list of valid `IPv4Address` or `IPv6Address` objects. - `\'invalid\'`: A list of strings that were not valid IP addresses. 4. **Constraints**: - The function should handle strings that either do not represent a valid IP address or are incorrectly formatted. - The order of addresses must be maintained in the output lists. 5. **Example**: ```python from ipaddress import IPv4Address, IPv6Address address_list = [ \'192.168.0.1\', \'2001:db8::\', \'256.256.256.256\', \'gibberish\' ] result = validate_and_summarize(address_list) # Expected output: # { # \'valid\': [IPv4Address(\'192.168.0.1\'), IPv6Address(\'2001:db8::\')], # \'invalid\': [\'256.256.256.256\', \'gibberish\'] # } ``` # Implementation Notes - Utilize the `ipaddress.ip_address(address)` function to parse and validate IP addresses. - Capture and handle exceptions to classify invalid addresses. - The function should make use of efficient list comprehensions or iteration mechanisms where appropriate.","solution":"from ipaddress import ip_address, IPv4Address, IPv6Address def validate_and_summarize(address_list: list) -> dict: result = {\'valid\': [], \'invalid\': []} for address in address_list: try: ip_obj = ip_address(address) result[\'valid\'].append(ip_obj) except ValueError: result[\'invalid\'].append(address) return result"},{"question":"**Coding Assessment Question:** # Task Implement a function `parse_and_build_values` that takes a string containing different data types and formats, parses the string according to given specifications, and constructs appropriate Python values based on the parsed components. # Function Signature ```python def parse_and_build_values(input_str: str) -> dict: # Your code here ``` # Input - `input_str` (str): A string containing different types and values separated by commas. The possible types and their formats are as follows: - Integers: \\"int:<value>\\" - Floats: \\"float:<value>\\" - Strings: \\"str:<value>\\" - Booleans: \\"bool:<value>\\" - Lists: \\"list:<value1;value2;...>\\" # Output - (dict): A dictionary with the parsed values organized under their respective data type keys (`integers`, `floats`, `strings`, `booleans`, and `lists`). # Constraints 1. The input string will always follow the specified format. 2. Each type key will have exactly one entry in the input string. 3. For boolean type, the values will be \'True\' or \'False\'. 4. List elements will be strings. # Example Example 1 ```python input_str = \\"int:42,float:3.14,str:hello,bool:True,list:apple;banana;cherry\\" result = parse_and_build_values(input_str) ``` Expected Output ```python { \\"integers\\": [42], \\"floats\\": [3.14], \\"strings\\": [\\"hello\\"], \\"booleans\\": [True], \\"lists\\": [[\\"apple\\", \\"banana\\", \\"cherry\\"]] } ``` # Note - Use appropriate parsing and data conversion techniques to handle each specified type. - Ensure the function can handle arbitrary value lengths and list sizes dynamically.","solution":"def parse_and_build_values(input_str: str) -> dict: Parses the input string according to the specified formats and returns a dictionary of parsed values organized under their respective data type keys. Args: input_str (str): Input string containing different types and values separated by commas. Returns: dict: A dictionary with the parsed values under `integers`, `floats`, `strings`, `booleans`, and `lists` keys. components = input_str.split(\',\') result = { \\"integers\\": [], \\"floats\\": [], \\"strings\\": [], \\"booleans\\": [], \\"lists\\": [] } for comp in components: type_label, value = comp.split(\':\', 1) if type_label == \'int\': result[\\"integers\\"].append(int(value)) elif type_label == \'float\': result[\\"floats\\"].append(float(value)) elif type_label == \'str\': result[\\"strings\\"].append(value) elif type_label == \'bool\': result[\\"booleans\\"].append(value == \'True\') elif type_label == \'list\': result[\\"lists\\"].append(value.split(\';\')) return result"},{"question":"Web Browser Automation You are required to implement a Python script that takes user input to open multiple URLs in different browsers based on the user\'s preferences. The script should demonstrate the usage of `webbrowser` module functions and its capability to handle different browsers and configurations. Requirements: 1. **Function Definition:** Define a function `open_urls_in_browsers(urls, browsers)` that takes two arguments: - `urls`: A list of URLs (strings) to be opened. - `browsers`: A list of browser names (strings) to be used for opening the URLs. 2. **Implementation Details:** - Use the `webbrowser` module to open each URL in the corresponding browser specified in the `browsers` list. - If a browser name is not registered, register it using the appropriate constructor from the `webbrowser` module. - The function should display a message indicating which URL is being opened in which browser. - Ensure that the same URL is not opened twice in the same browser within the same run. - Handle exceptions gracefully and print an error message if a URL cannot be opened in a specified browser. 3. **Constraints:** - The length of the `urls` list should be equal to the length of the `browsers` list. - You must handle at least the following browsers: `\'firefox\'`, `\'chrome\'`, `\'safari\'`, `\'edge\'`, `\'opera\'`. - The function should work on both Unix and Windows platforms. 4. **Example:** ```python urls = [\'https://www.python.org\', \'https://www.github.com\', \'https://www.stackoverflow.com\'] browsers = [\'firefox\', \'chrome\', \'safari\'] open_urls_in_browsers(urls, browsers) ``` **Expected Output:** ``` Opening https://www.python.org in firefox Opening https://www.github.com in chrome Opening https://www.stackoverflow.com in safari ``` Performance Requirements: - The function should efficiently handle the opening process without significant delays. - Consider edge cases where the browser may not be installed or recognized. Implement the function `open_urls_in_browsers(urls, browsers)` as specified.","solution":"import webbrowser def open_urls_in_browsers(urls, browsers): Opens a list of URLs in the specified browsers. :param urls: List of URLs to open. :param browsers: List of browser names to use for opening the URLs. # Ensure both lists have the same length if len(urls) != len(browsers): raise ValueError(\\"The length of urls and browsers lists must be equal.\\") # Supported browsers supported_browsers = [\'firefox\', \'chrome\', \'safari\', \'edge\', \'opera\'] opened = set() # Track opened URL-browser pairs to avoid duplicates for url, browser in zip(urls, browsers): if browser not in supported_browsers: raise ValueError(f\\"Unsupported browser: {browser}\\") if (url, browser) not in opened: try: print(f\\"Opening {url} in {browser}\\") webbrowser.get(using=browser).open_new(url) opened.add((url, browser)) except Exception as e: print(f\\"Error opening {url} in {browser}: {e}\\") else: print(f\\"{url} is already opened in {browser}\\") # Test cases would go here"},{"question":"# Question You are required to implement a function that takes a `torch.dtype` as input and returns a dictionary containing its numerical properties. The function should handle both floating point and integer dtypes appropriately. Function Signature ```python def analyze_dtype(dtype: torch.dtype) -> dict: pass ``` # Input - `dtype`: A `torch.dtype` representing the data type to be analyzed (e.g., `torch.float32`, `torch.int64`). # Output - A dictionary containing the numerical properties of the dtype. The dictionary should have the following structure: - For floating point dtypes: ```python { \\"type\\": \\"floating\\", \\"bits\\": int, \\"eps\\": float, \\"max\\": float, \\"min\\": float, \\"tiny\\": float, \\"resolution\\": float } ``` - For integer dtypes: ```python { \\"type\\": \\"integer\\", \\"bits\\": int, \\"max\\": int, \\"min\\": int } ``` # Constraints - You may assume that the input `dtype` will always be a valid PyTorch `torch.dtype` representing either a floating point or integer data type. - The dtype will be one from the following: `torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16`, `torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, or `torch.int64`. # Example ```python import torch # Example 1 result_float32 = analyze_dtype(torch.float32) print(result_float32) # Expected output: # { # \\"type\\": \\"floating\\", # \\"bits\\": 32, # \\"eps\\": 1.1920928955078125e-07, # \\"max\\": 3.4028234663852886e+38, # \\"min\\": -3.4028234663852886e+38, # \\"tiny\\": 1.1754943508222875e-38, # \\"resolution\\": 1e-6 # } # Example 2 result_int64 = analyze_dtype(torch.int64) print(result_int64) # Expected output: # { # \\"type\\": \\"integer\\", # \\"bits\\": 64, # \\"max\\": 9223372036854775807, # \\"min\\": -9223372036854775808 # } ``` Use the classes described in the provided documentation to achieve the correct outputs.","solution":"import torch def analyze_dtype(dtype: torch.dtype) -> dict: if dtype in (torch.float16, torch.float32, torch.float64, torch.bfloat16): type_name = \\"floating\\" bits = torch.finfo(dtype).bits eps = torch.finfo(dtype).eps max_val = torch.finfo(dtype).max min_val = torch.finfo(dtype).min tiny = torch.finfo(dtype).tiny resolution = torch.finfo(dtype).resolution return { \\"type\\": type_name, \\"bits\\": bits, \\"eps\\": eps, \\"max\\": max_val, \\"min\\": min_val, \\"tiny\\": tiny, \\"resolution\\": resolution } else: type_name = \\"integer\\" bits = torch.iinfo(dtype).bits max_val = torch.iinfo(dtype).max min_val = torch.iinfo(dtype).min return { \\"type\\": type_name, \\"bits\\": bits, \\"max\\": max_val, \\"min\\": min_val }"},{"question":"# Advanced Coding Assessment: Data Compression and Decompression with `bz2` Objective You are required to write a Python program that demonstrates your ability to effectively use the `bz2` module for compressing and decompressing data. Task Implement a Python function `compress_and_count(filename, compresslevel=9)` that accepts: - `filename` (str): The name of a text file to read and compress. - `compresslevel` (int): The level of compression to use, default is 9. The function should: 1. Compress the content of the text file using the specified `compresslevel`. 2. Write the compressed data to a new file with a `.bz2` extension added to the original filename. 3. Decompress the compressed file to verify the integrity of the compressed data. 4. Return a tuple containing: - The size of the original file in bytes. - The size of the compressed file in bytes. - The size of the decompressed content in bytes. - A boolean indicating whether the original content matches the decompressed content. # Input - `filename`: A string representing the path to an input text file. - `compresslevel`: An integer from 1 to 9 specifying the level of compression (optional, default is 9). # Output - A tuple `(original_size, compressed_size, decompressed_size, is_matching)`: - `original_size` (int): Size of the original file in bytes. - `compressed_size` (int): Size of the compressed file in bytes. - `decompressed_size` (int): Size of the decompressed content in bytes. - `is_matching` (bool): True if the original content matches the decompressed content, otherwise False. # Constraints - The function should handle file reading and writing efficiently. - The file operations should handle different types of data errors gracefully. - Ensure the paths handled are compatible with different operating systems. # Example ```python # Example usage of compress_and_count result = compress_and_count(\\"example.txt\\", compresslevel=5) print(result) # Output: will vary based on the input file ``` # Note To test your solution, create a sample text file named `example.txt` with some data and use the function to check the output. **Make sure your implementation does not modify the original file.**","solution":"import bz2 import os def compress_and_count(filename, compresslevel=9): Compresses the content of the text file using the specified compress level, writes the compressed data to a new file, and verifies the integrity of the compressed data. Args: - filename (str): The name of the text file to read and compress. - compresslevel (int): The level of compression to use (default is 9). Returns: - tuple: A tuple containing the size of the original file, the size of the compressed file, the size of the decompressed content, and a boolean indicating if the original content matches the decompressed content. # Read original file content with open(filename, \'rb\') as file: original_content = file.read() original_size = len(original_content) # Compress content compressed_content = bz2.compress(original_content, compresslevel=compresslevel) compressed_filename = f\\"{filename}.bz2\\" # Write compressed content to a new file with open(compressed_filename, \'wb\') as compressed_file: compressed_file.write(compressed_content) compressed_size = os.path.getsize(compressed_filename) # Decompress content to verify decompressed_content = bz2.decompress(compressed_content) decompressed_size = len(decompressed_content) # Check if the decompressed content matches the original content is_matching = original_content == decompressed_content return (original_size, compressed_size, decompressed_size, is_matching)"},{"question":"# PyTorch Intel GPU Training Challenge You are tasked with designing and implementing a PyTorch-based neural network training workflow that utilizes the capabilities of Intel GPUs. Specifically, you will create a simple Convolutional Neural Network (CNN) model to train on the MNIST dataset. The implementation should leverage Intel GPU optimization features, including Automatic Mixed Precision (AMP). Requirements: 1. **Model Architecture:** - Define a simple CNN model. - The model should have at least two convolutional layers, followed by fully connected layers. 2. **Dataset:** - Use the MNIST dataset, applying appropriate transformations for normalization. 3. **Training Workflow:** - Implement the training workflow including data loading, model training, and optimization steps. - Use the `torch.xpu` module to utilize Intel GPU. - Implement AMP during the training process. 4. **Performance Reporting:** - Print loss values after every 100 iterations. - Print the total training time. Expected Input and Output: - **Input:** No input arguments required. The MNIST dataset should be downloaded and used within the code. - **Output:** The training script should output loss values and total training time. Constraints: - **Batch Size:** 64 - **Epochs:** 5 - **Learning Rate:** 0.001 Key Points: - Ensure you handle device placement properly (CPU vs GPU). - Use `torch.xpu` for moving data and models to the Intel GPU. - Implement AMP using `torch.autocast` and `torch.amp.GradScaler`. Example Code Template: ```python import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms import time # Define your CNN model class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) self.fc1 = nn.Linear(64 * 7 * 7, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.max_pool2d(x, 2) x = torch.relu(self.conv2(x)) x = torch.max_pool2d(x, 2) x = x.view(-1, 64 * 7 * 7) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Load MNIST dataset transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = torchvision.datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) # Move model to Intel GPU device = torch.device(\'xpu\' if torch.xpu.is_available() else \'cpu\') model = SimpleCNN().to(device) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) scaler = torch.amp.GradScaler() # Training loop with AMP num_epochs = 5 model.train() start_time = time.time() for epoch in range(num_epochs): for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() with torch.autocast(device_type=\'xpu\', dtype=torch.float16): outputs = model(data) loss = criterion(outputs, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() if (batch_idx + 1) % 100 == 0: print(f\'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\') total_time = time.time() - start_time print(f\'Total Training Time: {total_time:.2f} seconds\') # Save the trained model torch.save(model.state_dict(), \'mnist_cnn.pth\') ``` Use this template to complete your solution by filling in any missing parts and ensuring all requirements are met. This exercise is designed to assess your understanding of PyTorch functionalities, specifically focusing on training with Intel GPU support and leveraging PyTorch\'s mixed precision capabilities.","solution":"import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms import time # Define the CNN model class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) self.fc1 = nn.Linear(64 * 7 * 7, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.max_pool2d(x, 2) x = torch.relu(self.conv2(x)) x = torch.max_pool2d(x, 2) x = x.view(-1, 64 * 7 * 7) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train_model(): # Load MNIST dataset transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = torchvision.datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) # Move model to Intel GPU if available device = torch.device(\'xpu\' if torch.xpu.is_available() else \'cpu\') model = SimpleCNN().to(device) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) scaler = torch.amp.GradScaler() # Training loop with AMP num_epochs = 5 model.train() start_time = time.time() for epoch in range(num_epochs): for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() with torch.autocast(device_type=\'xpu\', dtype=torch.float16): outputs = model(data) loss = criterion(outputs, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() if (batch_idx + 1) % 100 == 0: print(f\'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\') total_time = time.time() - start_time print(f\'Total Training Time: {total_time:.2f} seconds\') # Save the trained model torch.save(model.state_dict(), \'mnist_cnn.pth\') # For running the training process independently if __name__ == \\"__main__\\": train_model()"},{"question":"# Panda\'s Index Manipulation Challenge Objective You are required to demonstrate your understanding of pandas `Index` object manipulation by implementing a function that takes a DataFrame and performs a series of operations on its indices. Problem Statement Write a function `process_indices(df)` that takes as input a pandas DataFrame `df` and performs the following operations: 1. **MultiIndex Creation**: Convert the existing index of the DataFrame into a `MultiIndex` by repeating the current index and adding a new level based on the sequence [0, 1, ..., n-1] where `n` is the length of the current index. 2. **Index Manipulation**: - Extract the second level of the MultiIndex and create a new column in the DataFrame called `level_1`. - Rename the levels of the MultiIndex to `primary` and `secondary`. 3. **DatetimeIndex Conversion**: - Assume another column in the DataFrame called `date` which contains date strings. Convert this column to a `DatetimeIndex`. - Set this `DatetimeIndex` as the new index of the DataFrame, while keeping the existing MultiIndex as the second level. 4. **Sorting**: - Sort the DataFrame by the new `DatetimeIndex` and within the same date, by the `primary` index level of the MultiIndex. Input - A pandas DataFrame `df` with at least one existing index and a column named `date`. Output - A pandas DataFrame with the described operations performed. Function Signature ```python import pandas as pd def process_indices(df: pd.DataFrame) -> pd.DataFrame: pass ``` Example ```python import pandas as pd # Sample DataFrame df = pd.DataFrame({ \'value\': [10, 20, 30, 40], \'date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-01\', \'2023-01-03\'] }, index=[\'A\', \'B\', \'C\', \'D\']) # Expected Execution result_df = process_indices(df) # The resulting DataFrame should meet the following criteria: # - Have a MultiIndex where `primary` is the original index and `secondary` is [0, 1, 2, 3]. # - Have a new column `level_1` which contains the values of the `secondary` level. # - Index should be a `DatetimeIndex` derived from the `date` column, with the MultiIndex as the second level. # - The DataFrame should be sorted by `DatetimeIndex` and within the same date by the `primary` level of the MultiIndex. ``` You are required to implement the function `process_indices` that achieves the above requirements.","solution":"import pandas as pd def process_indices(df: pd.DataFrame) -> pd.DataFrame: # Step 1: MultiIndex Creation n = len(df) df.index = pd.MultiIndex.from_arrays( [df.index, list(range(n))], names=[\'primary\', \'secondary\'] ) # Step 2: Extract second level and rename levels df[\'level_1\'] = df.index.get_level_values(\'secondary\') df.index = df.index.set_names([\'primary\', \'secondary\']) # Step 3: Convert \'date\' column to DatetimeIndex df[\'date\'] = pd.to_datetime(df[\'date\']) df = df.set_index([\'date\', df.index]) # Step 4: Sort DataFrame by DatetimeIndex and primary index level of MultiIndex df = df.sort_index(level=[\'date\', \'primary\']) return df.reset_index() # Reset index for easier viewing and testing"},{"question":"**Question: Tensor Shape Manipulation Using `torch.Size`** In this problem, you will write a function that reshapes a given tensor according to certain specified dimensions. # Function Signature ```python def reshape_tensor(tensor: torch.Tensor, new_shape: Union[torch.Size, List[int], Tuple[int, ...]]) -> torch.Tensor: ``` # Input - `tensor` (torch.Tensor): The input tensor you need to reshape. - `new_shape` (Union[torch.Size, List[int], Tuple[int, ...]]): The new shape you want to reshape the tensor to. It can be a `torch.Size` object, or a list or tuple of integers representing the desired shape. # Output - Returns the reshaped tensor. # Constraints - The product of the dimensions in `new_shape` must be equal to the product of the dimensions in the original tensor. - You must handle possible edge cases where the new shape is incompatible with the original tensor size. # Example ```python import torch # Example 1 tensor = torch.ones(4, 5, 6) new_shape = (2, 2, 30) result = reshape_tensor(tensor, new_shape) print(result.size()) # Output should be torch.Size([2, 2, 30]) # Example 2 tensor = torch.ones(10, 20) new_shape = torch.Size([5, 40]) result = reshape_tensor(tensor, new_shape) print(result.size()) # Output should be torch.Size([5, 40]) ``` # Notes - Ensure that the reshaping operation does not alter the data within the tensor, only its shape. - Your implementation should raise an error or return an appropriate message if the new shape is invalid.","solution":"import torch from typing import Union, List, Tuple def reshape_tensor(tensor: torch.Tensor, new_shape: Union[torch.Size, List[int], Tuple[int, ...]]) -> torch.Tensor: Reshapes the given tensor according to the specified dimensions. Args: tensor (torch.Tensor): The input tensor to reshape. new_shape (Union[torch.Size, List[int], Tuple[int, ...]]): The shape to reshape the tensor into. Returns: torch.Tensor: The reshaped tensor. Raises: ValueError: If the new shape is incompatible with the original tensor size. if isinstance(new_shape, (list, tuple)): new_shape = torch.Size(new_shape) if tensor.numel() != torch.Size(new_shape).numel(): raise ValueError(\\"The product of the dimensions in the new shape must be equal to the product of the dimensions in the original tensor.\\") return tensor.view(new_shape)"},{"question":"# Custom Scikit-learn Estimator You are tasked to create a custom scikit-learn estimator that adheres to scikit-learn\'s conventions. This estimator should implement basic functionalities for both classification and transformation. Please follow the guidelines provided below: Requirements: 1. **Initialization**: - The estimator should accept at least one parameter that affects its behavior (e.g., `threshold` for classification). 2. **Fit Method**: - Implement the `fit` method that checks input data and fits the model. 3. **Predict Method**: - Implement the `predict` method for classification. 4. **Transform Method**: - Implement the `transform` method to modify data in a certain way. 5. **Input Validation**: - Ensure proper validation of input data (i.e., shape checking). 6. **Parameter Handling**: - Utilize proper parameter handling through `get_params` and `set_params`. 7. **Compatibility**: - Ensure compatibility with scikit-learn tools, enabling it to be used within pipelines and grid search. Details: 1. **Initialization** - Define an `__init__` method which accepts: - `threshold`: A floating-point parameter with a default value that will be used in the `predict` method. - Store it as an attribute of the instance. 2. **Fit Method** - Define a `fit` method which: - Accepts `X` (features) and `y` (targets), validates them, and stores necessary training information. - Sets attributes required for subsequent steps of the estimator. 3. **Predict Method** - Define a `predict` method which: - Uses the fitted information from `fit` and predicts classes based on a simple condition leveraging the `threshold` parameter. 4. **Transform Method** - Define a `transform` method which: - Transforms inputs based on a predefined approach (e.g., normalization). 5. **Get and Set Params** - Implement `get_params` and `set_params` to retrieve and update parameters. 6. **Example Usage** - You should be able to use your custom estimator within a scikit-learn pipeline and conduct parameter search with grid search. Example Code: ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted class CustomEstimator(BaseEstimator, ClassifierMixin, TransformerMixin): def __init__(self, threshold=0.5): self.threshold = threshold def fit(self, X, y=None): X, y = check_X_y(X, y) self.classes_, y = np.unique(y, return_inverse=True) self.X_ = X self.y_ = y self.n_samples_, self.n_features_ = X.shape return self def predict(self, X): check_is_fitted(self, [\'X_\', \'y_\']) X = check_array(X) return np.where(np.mean(X, axis=1) > self.threshold, self.classes_[1], self.classes_[0]) def transform(self, X): check_is_fitted(self, [\'X_\', \'y_\']) X = check_array(X) return (X - np.mean(X, axis=0)) / np.std(X, axis=0) def get_params(self, deep=True): return {\\"threshold\\": self.threshold} def set_params(self, **params): for parameter, value in params.items(): setattr(self, parameter, value) return self # Example usage: from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV pipeline = Pipeline([ (\'custom_estimator\', CustomEstimator()) ]) param_grid = { \'custom_estimator__threshold\': [0.1, 0.2, 0.3] } grid_search = GridSearchCV(pipeline, param_grid, cv=3) ``` Expected Outcomes: - Implementation of `CustomEstimator` class. - Proper handling and validation of input data. - Working `fit`, `predict`, and `transform` methods. - Compatibility with scikit-learn utilities like pipelines and grid search.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted class CustomEstimator(BaseEstimator, ClassifierMixin, TransformerMixin): def __init__(self, threshold=0.5): self.threshold = threshold def fit(self, X, y=None): X, y = check_X_y(X, y) self.classes_, y = np.unique(y, return_inverse=True) self.X_ = X self.y_ = y self.n_samples_, self.n_features_ = X.shape return self def predict(self, X): check_is_fitted(self, [\'X_\', \'y_\']) X = check_array(X) return np.where(np.mean(X, axis=1) > self.threshold, self.classes_[1], self.classes_[0]) def transform(self, X): check_is_fitted(self, [\'X_\', \'y_\']) X = check_array(X) return (X - np.mean(X, axis=0)) / np.std(X, axis=0) def get_params(self, deep=True): return {\\"threshold\\": self.threshold} def set_params(self, **params): for parameter, value in params.items(): setattr(self, parameter, value) return self"},{"question":"Advanced Object Management in Python **Objective:** Write a function `manage_objects` that takes three parameters: 1. An integer `n`. 2. A list of integers `int_list`. 3. A dictionary `str_dict`. The function should: 1. Validate that the inputs are of the correct type (`n` should be an integer, `int_list` should be a list of integers, and `str_dict` should be a dictionary with string keys and values). 2. If the types are incorrect, the function should raise appropriate `TypeError` exceptions with meaningful error messages. 3. If the types are correct, the function should: - Return the square of `n`. - Return a list where each integer in `int_list` is incremented by `n`. - Convert `str_dict` into a list of `(key, value)` tuples and return it. **Function Signature:** ```python def manage_objects(n: int, int_list: list, str_dict: dict) -> tuple: pass ``` **Inputs:** - `n`: An integer (1 ≤ n ≤ 10^6). - `int_list`: A list of integers, each integer `i` (1 ≤ i ≤ 10^6 and the list size is up to 10^3). - `str_dict`: A dictionary where keys are strings and values are strings. The dictionary can have up to 10^3 key-value pairs. **Outputs:** The function should return a tuple containing: 1. The square of `n`. 2. A list of integers with `n` added to each element. 3. A list of `(key, value)` tuples from `str_dict`. **Example:** ```python n = 3 int_list = [1, 2, 3] str_dict = {\\"a\\": \\"apple\\", \\"b\\": \\"banana\\"} result = manage_objects(n, int_list, str_dict) # Expected output: (9, [4, 5, 6], [(\\"a\\", \\"apple\\"), (\\"b\\", \\"banana\\")]) ``` **Constraints:** - The input types must strictly conform to the described format. - The function should handle large integer values and extensive lists and dictionaries efficiently. **Performance Requirements:** - Ensure the function is optimized to handle the upper limits of the input constraints efficiently. **Notes:** - Use appropriate Python exception handling for type verification. - Make use of basic Python data structures and operations to achieve the task. **Hints:** - Use `isinstance()` for type checking. - Increment each integer in the list using list comprehensions. - Convert dictionary items to list of tuples using `items()` method.","solution":"def manage_objects(n: int, int_list: list, str_dict: dict) -> tuple: # Validate input types if not isinstance(n, int): raise TypeError(\\"n should be an integer\\") if not isinstance(int_list, list) or not all(isinstance(i, int) for i in int_list): raise TypeError(\\"int_list should be a list of integers\\") if not isinstance(str_dict, dict) or not all(isinstance(k, str) and isinstance(v, str) for k, v in str_dict.items()): raise TypeError(\\"str_dict should be a dictionary with string keys and string values\\") # Processing and returning the required values squared_n = n * n incremented_list = [i + n for i in int_list] dict_as_list_of_tuples = list(str_dict.items()) return (squared_n, incremented_list, dict_as_list_of_tuples)"},{"question":"You are provided with a dataset containing information about different types of vehicles. You need to perform cluster analysis to group similar vehicle types and reduce the dimensionality of the dataset for visualization purposes. Your task is to write a function in Python using the scikit-learn library to achieve this. Function Signature ```python def cluster_vehicles(data: np.ndarray, n_clusters: int, n_components: int): Perform clustering and dimensionality reduction on the vehicle dataset. Parameters: data (np.ndarray): A 2D numpy array where each row represents a vehicle and each column represents a feature. n_clusters (int): The number of clusters to form. n_components (int): The number of dimensions to reduce the data to for visualization. Returns: (np.ndarray, np.ndarray): A tuple containing: - labels (np.ndarray): A 1D array of cluster labels for each vehicle. - reduced_data (np.ndarray): A 2D array representing the data reduced to n_components dimensions. ``` Requirements - Use the KMeans algorithm from scikit-learn to perform clustering. - Use Principal Component Analysis (PCA) from scikit-learn to reduce the dimensionality of the dataset. - The input `data` must be normalized before clustering and dimensionality reduction. Example Input ```python import numpy as np data = np.array([ [356, 8, 200, 3500], [180, 4, 100, 1500], [220, 6, 150, 2000], ... ]) n_clusters = 3 n_components = 2 ``` Example Output ```python labels = np.array([0, 1, 2, ... ]) reduced_data = np.array([ [2.5, 1.3], [-1.2, 0.4], [0.8, -2.1], ... ]) ``` Constraints - `data` will have between 100 and 10000 samples. - Each sample (vehicle) will have between 4 and 50 features. - `n_clusters` will be a positive integer less than or equal to 10. - `n_components` will be a positive integer less than or equal to 3. Performance Requirements - The function should complete execution within 5 seconds for the upper limit of input size. Additional Notes - Your solution should be efficient and make use of scikit-learn\'s built-in functions. - Ensure that your code is well-documented with comments explaining each step.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler def cluster_vehicles(data: np.ndarray, n_clusters: int, n_components: int): Perform clustering and dimensionality reduction on the vehicle dataset. Parameters: data (np.ndarray): A 2D numpy array where each row represents a vehicle and each column represents a feature. n_clusters (int): The number of clusters to form. n_components (int): The number of dimensions to reduce the data to for visualization. Returns: (np.ndarray, np.ndarray): A tuple containing: - labels (np.ndarray): A 1D array of cluster labels for each vehicle. - reduced_data (np.ndarray): A 2D array representing the data reduced to n_components dimensions. # Normalize the data scaler = StandardScaler() normalized_data = scaler.fit_transform(data) # Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) labels = kmeans.fit_predict(normalized_data) # Perform PCA for dimensionality reduction pca = PCA(n_components=n_components) reduced_data = pca.fit_transform(normalized_data) return labels, reduced_data"},{"question":"Objective Implement a set of functions in Python that mimic the behavior of the CPython `tp_call` and `vectorcall` protocols. This will assess your understanding of callable objects in Python and how arguments are passed and handled. Problem Statement You need to create a Python class `CallableHandler` with the following methods: 1. **`call_object_with_tuple`**: Simulates the behavior of `PyObject_CallObject`. 2. **`call_object_with_args_kwargs`**: Simulates the behavior of `PyObject_Call`. 3. **`call_object_no_args`**: Simulates the behavior of `PyObject_CallNoArgs`. 4. **`call_object_one_arg`**: Simulates the behavior of `PyObject_CallOneArg`. Each method will be defined as follows: 1. **`call_object_with_tuple(callable_obj, args_tuple)`**: - **Input**: - `callable_obj`: A callable object such as a function or a class instance that implements `__call__`. - `args_tuple`: A tuple of positional arguments. - **Output**: The result of calling `callable_obj` with the arguments in `args_tuple`. 2. **`call_object_with_args_kwargs(callable_obj, args_tuple, kwargs_dict)`**: - **Input**: - `callable_obj`: A callable object. - `args_tuple`: A tuple of positional arguments. - `kwargs_dict`: A dictionary of keyword arguments. - **Output**: The result of calling `callable_obj` with the arguments in `args_tuple` and keywords in `kwargs_dict`. 3. **`call_object_no_args(callable_obj)`**: - **Input**: - `callable_obj`: A callable object. - **Output**: The result of calling `callable_obj` without any arguments. 4. **`call_object_one_arg(callable_obj, arg)`**: - **Input**: - `callable_obj`: A callable object. - `arg`: A single positional argument. - **Output**: The result of calling `callable_obj` with the single argument `arg`. Constraints - You cannot use the `*args` and `**kwargs` syntax directly in your function signatures. - Your solutions should handle exceptions gracefully, returning appropriate error messages if the call fails. - Assume all callable objects are properly defined and will not raise exceptions outside of argument handling. Example Usage ```python def example_function(a, b, c=3): return a + b + c handler = CallableHandler() # Example of call_object_with_tuple result = handler.call_object_with_tuple(example_function, (1, 2)) # Should return 6 # Example of call_object_with_args_kwargs result = handler.call_object_with_args_kwargs(example_function, (1,), {\\"b\\": 2, \\"c\\": 4}) # Should return 7 # Example of call_object_no_args with a simple callable result = handler.call_object_no_args(lambda: \\"Hello\\") # Should return \\"Hello\\" # Example of call_object_one_arg result = handler.call_object_one_arg(example_function, 5) # Should raise an error due to missing \'b\' argument ``` Implement the `CallableHandler` class based on the above specifications. Ensure your implementation is clear, properly commented, and thoroughly tested.","solution":"class CallableHandler: def call_object_with_tuple(self, callable_obj, args_tuple): Simulates the behavior of PyObject_CallObject. Calls the callable_obj with the provided positional arguments tuple. try: return callable_obj(*args_tuple) except Exception as e: return str(e) def call_object_with_args_kwargs(self, callable_obj, args_tuple, kwargs_dict): Simulates the behavior of PyObject_Call. Calls the callable_obj with the provided positional arguments tuple and keyword arguments dictionary. try: return callable_obj(*args_tuple, **kwargs_dict) except Exception as e: return str(e) def call_object_no_args(self, callable_obj): Simulates the behavior of PyObject_CallNoArgs. Calls the callable_obj without any arguments. try: return callable_obj() except Exception as e: return str(e) def call_object_one_arg(self, callable_obj, arg): Simulates the behavior of PyObject_CallOneArg. Calls the callable_obj with a single positional argument. try: return callable_obj(arg) except Exception as e: return str(e)"},{"question":"**Question:** **File Organizer Using Tkinter Dialogs** You are tasked with creating a simple file organizer tool using Tkinter dialogs. This tool will allow users to select files from their filesystem, categorize them into different folders based on their file types, and save them in a specified directory. **Requirements:** 1. **File Selection**: Use `tkinter.filedialog` to prompt the user to select multiple files from their filesystem. 2. **Output Directory**: Use `tkinter.filedialog` to prompt the user to select a directory where the categorized files will be saved. 3. **Categorization**: Write a function `categorize_files(files, output_dir)` that categorizes the selected files into folders named according to their file extensions (e.g., `txt_files`, `jpg_files`, `pdf_files`), and move the files into their corresponding folders within the output directory. **Implementation Details:** - Use `tkinter.filedialog.askopenfilenames(title=\\"Select files to organize\\")` to prompt users to select multiple files. - Use `tkinter.filedialog.askdirectory(title=\\"Select output directory\\")` to prompt users to select the directory where categorized files will be saved. - The function `categorize_files(files, output_dir)` should: - Create subdirectories within the `output_dir` for each file type (e.g., `txt_files`, `jpg_files`, etc.). - Move or copy files to their corresponding subdirectories. **Constraints:** - Ensure that the program handles errors gracefully (e.g., file permission errors, invalid user input). - The program should not overwrite existing files in the output directory without confirmation from the user. **Example Usage:** ```python import os import shutil from tkinter import filedialog import tkinter as tk def categorize_files(files, output_dir): for file_path in files: extension = os.path.splitext(file_path)[1].lower().lstrip(\'.\') if not extension: extension = \\"others\\" destination_dir = os.path.join(output_dir, f\\"{extension}_files\\") os.makedirs(destination_dir, exist_ok=True) file_name = os.path.basename(file_path) destination_path = os.path.join(destination_dir, file_name) if os.path.exists(destination_path): print(f\\"File {file_name} already exists in {destination_dir}. Skipping.\\") else: shutil.move(file_path, destination_path) print(f\\"Moved {file_path} to {destination_path}\\") if __name__ == \\"__main__\\": root = tk.Tk() root.withdraw() # Hide the main window files = filedialog.askopenfilenames(title=\\"Select files to organize\\") if not files: print(\\"No files selected.\\") else: output_dir = filedialog.askdirectory(title=\\"Select output directory\\") if not output_dir: print(\\"No output directory selected.\\") else: categorize_files(files, output_dir) ``` **Expected Output:** When the script is run and files are selected, it should print messages indicating the files being moved and their new locations. If the output directory or files are not selected, appropriate messages should be displayed.","solution":"import os import shutil from tkinter import filedialog import tkinter as tk def categorize_files(files, output_dir): for file_path in files: extension = os.path.splitext(file_path)[1].lower().lstrip(\'.\') if not extension: extension = \\"others\\" destination_dir = os.path.join(output_dir, f\\"{extension}_files\\") os.makedirs(destination_dir, exist_ok=True) file_name = os.path.basename(file_path) destination_path = os.path.join(destination_dir, file_name) if os.path.exists(destination_path): print(f\\"File {file_name} already exists in {destination_dir}. Skipping.\\") else: shutil.move(file_path, destination_path) print(f\\"Moved {file_path} to {destination_path}\\") if __name__ == \\"__main__\\": root = tk.Tk() root.withdraw() # Hide the main window files = filedialog.askopenfilenames(title=\\"Select files to organize\\") if not files: print(\\"No files selected.\\") else: output_dir = filedialog.askdirectory(title=\\"Select output directory\\") if not output_dir: print(\\"No output directory selected.\\") else: categorize_files(files, output_dir)"},{"question":"# **Coding Assessment Question** **Objective:** You are required to demonstrate your understanding of seaborn\'s color palettes by creating and customizing visualizations. **Task:** 1. **Retrieve and Display Color Palettes:** - Write a function `display_color_palettes` that retrieves and displays the following seaborn color palettes: - The default color palette. - The \\"pastel\\" palette. - A palette with 9 evenly spaced hues using the \\"HUSL\\" system. - The \\"flare\\" palette as both discrete and continuous. - The \\"dark:#5A9_r\\" reversed dark sequential gradient as a continuous colormap. - The function should display each palette\'s colors using seaborn\'s `palplot` method. 2. **Custom Visualization:** - Write a function `create_custom_visualization` that uses seaborn to create a scatter plot with specified parameters: - Use the \\"Set2\\" palette. - Generate a random dataset with 100 points (use `numpy` for random values). - Specify different hues for each point based on their value. - Customize the plot by setting a specific title, x and y labels, and making the plot aesthetically pleasing. **Function Implementations:** 1. `def display_color_palettes() -> None:` - **Input:** None - **Output:** Display color palettes using seaborn\'s `palplot`. 2. `def create_custom_visualization() -> None:` - **Input:** None - **Output:** Create and display a customized scatter plot using seaborn. # **Constraints and Requirements:** - You must use seaborn and matplotlib for all visualizations. - Ensure that the plots are clearly labeled and well-organized. - The functions should not take any input parameters. # **Expected Output:** - A visualization of each requested color palette. - A scatter plot with the specified customizations. # **Example Execution:** Running the functions will display color palettes followed by a custom scatter plot. ```python display_color_palettes() create_custom_visualization() ``` **Hints:** - Use `sns.color_palette()` to retrieve color palettes. - Use `sns.palplot()` to display color palettes. - Use `np.random.rand` and `np.random.seed` to generate random datasets for reproducibility. - Seaborn\'s `scatterplot()` or `relplot()` can be used for the scatter plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def display_color_palettes() -> None: Retrieves and displays various seaborn color palettes. # Default color palette default_palette = sns.color_palette() sns.palplot(default_palette) plt.title(\\"Default Palette\\") plt.show() # Pastel palette pastel_palette = sns.color_palette(\\"pastel\\") sns.palplot(pastel_palette) plt.title(\\"Pastel Palette\\") plt.show() # HUSL palette with 9 colors husl_palette = sns.color_palette(\\"husl\\", 9) sns.palplot(husl_palette) plt.title(\\"HUSL Palette with 9 hues\\") plt.show() # Flare palette as discrete flare_palette = sns.color_palette(\\"flare\\", as_cmap=False) sns.palplot(flare_palette) plt.title(\\"Flare Palette (discrete)\\") plt.show() # Flare palette as continuous cmap flare_cmap = sns.color_palette(\\"flare\\", as_cmap=True) plt.figure() plt.imshow([range(10)], cmap=flare_cmap, aspect=\'auto\') plt.title(\\"Flare Palette (continuous)\\") plt.colorbar() plt.show() # Dark:#5A9_r reversed sequential gradient as continuous cmap dark_cmap = sns.color_palette(\\"dark:#5A9_r\\", as_cmap=True) plt.figure() plt.imshow([range(10)], cmap=dark_cmap, aspect=\'auto\') plt.title(\\"Dark:#5A9_r reversed (continuous)\\") plt.colorbar() plt.show() def create_custom_visualization() -> None: Creates and displays a customized scatter plot using seaborn. # Create random dataset np.random.seed(0) x = np.random.rand(100) y = np.random.rand(100) hues = np.random.rand(100) # Create scatter plot sns.scatterplot(x=x, y=y, hue=hues, palette=\\"Set2\\", legend=False) # Customize plot plt.title(\\"Custom Scatter Plot\\") plt.xlabel(\\"Random X\\") plt.ylabel(\\"Random Y\\") plt.show()"},{"question":"**Title:** Advanced Token Manipulation with the `tokenize` Module **Objective:** Implement functions utilizing the `tokenize` module to analyze and manipulate Python source code. **Problem Statement:** You are asked to write a Python module `source_code_tools.py` that contains the following functionalities: 1. **Function to Extract String Literals:** Write a function `extract_string_literals(filename: str) -> List[str]` that extracts all string literals from a given Python source file. The function should: - Open the given Python source file. - Use the `tokenize.tokenize()` function to generate tokens. - Extract and return all string literals found in the source file as a list of strings. 2. **Function to Replace Identifiers:** Write a function `replace_identifiers(filename: str, replacements: Dict[str, str]) -> str` that replaces all occurrences of specific identifiers (variable names, function names, etc.) in the given Python source file based on a replacement map provided. The function should: - Open the given Python source file. - Use the `tokenize.tokenize()` function to generate tokens. - Replace identifiers according to the provided `replacements` dictionary (keys are the original identifiers, values are the new identifiers). - Return the modified source code as a single string. - Ensure that the source code after replacements remains syntactically correct. Function Signatures: ```python from typing import List, Dict def extract_string_literals(filename: str) -> List[str]: pass def replace_identifiers(filename: str, replacements: Dict[str, str]) -> str: pass ``` Example Usage: ```python # Given a Python file example.py with the following content: # def greet(): # msg = \\"Hello, world!\\" # print(msg) # The function call extract_string_literals(\'example.py\') should return: # [\\"Hello, world!\\"] # Given the replacements dictionary {\'greet\': \'say_hello\', \'msg\': \'message\'}, # the function call replace_identifiers(\'example.py\', {\'greet\': \'say_hello\', \'msg\': \'message\'}) should return: # def say_hello(): # message = \\"Hello, world!\\" # print(message) ``` Constraints: - Assume the input source file is always syntactically valid Python code. - You can only use the standard Python library. Hints: - Use the `tokenize.untokenize()` to convert the modified tokens back into source code. - Pay attention to the token types and handle them appropriately.","solution":"import tokenize from typing import List, Dict def extract_string_literals(filename: str) -> List[str]: string_literals = [] with open(filename, \'rb\') as f: tokens = tokenize.tokenize(f.readline) for token in tokens: if token.type == tokenize.STRING: string_literals.append(token.string) return string_literals def replace_identifiers(filename: str, replacements: Dict[str, str]) -> str: result = [] with open(filename, \'rb\') as f: tokens = list(tokenize.tokenize(f.readline)) for token in tokens: if token.type == tokenize.NAME and token.string in replacements: new_token = tokenize.TokenInfo(tokenize.NAME, replacements[token.string], token.start, token.end, token.line) result.append(new_token) else: result.append(token) return tokenize.untokenize(result).decode(\'utf-8\')"},{"question":"# Question: Implementing a Custom Interactive Shell with `readline` In this task, you are required to implement a custom interactive shell in Python using the `readline` module. This shell should support: 1. Custom history management. 2. Custom word completion. 3. Saving the session\'s history to a file, and loading history from the file at startup. Requirements: 1. **History Management**: - On startup, load history from a file (default: `\\".myshell_history\\"` in the user\'s home directory). - Append new commands to the session\'s history. - On exit, save the session\'s history to the same file. Ensure history file contains a maximum of 1000 commands. 2. **Custom Word Completion**: - Implement a custom completer function that completes words based on a predefined list of words (e.g., [\\"help\\", \\"exit\\", \\"list\\", \\"fetch\\", \\"update\\"]). - Bind the TAB key to use this custom completer. Implementation: - The custom interactive shell should continuously read user input until the user types \\"exit\\". - Use the `input()` function to read user command input. - Save the session\'s command history to a file at exit ensuring any previously existing commands are preserved. Function Signatures: - `load_history(history_file: str) -> None` - `save_history(history_file: str, max_commands: int) -> None` - `custom_completer(text: str, state: int) -> Optional[str]` - `interactive_shell() -> None` Example: ```python import os import readline import atexit def load_history(history_file: str) -> None: try: readline.read_history_file(history_file) except FileNotFoundError: pass def save_history(history_file: str, max_commands: int) -> None: readline.set_history_length(max_commands) readline.write_history_file(history_file) def custom_completer(text: str, state: int) -> Optional[str]: options = [\\"help\\", \\"exit\\", \\"list\\", \\"fetch\\", \\"update\\"] matches = [s for s in options if s and s.startswith(text)] return matches[state] if state < len(matches) else None def interactive_shell() -> None: histfile = os.path.expanduser(\\"~/.myshell_history\\") load_history(histfile) readline.set_completer(custom_completer) readline.parse_and_bind(\\"tab: complete\\") atexit.register(save_history, histfile, 1000) while True: try: command = input(\'myshell> \') if command.strip().lower() == \'exit\': break # Add further command handling here print(f\\"Executing command: {command}\\") except EOFError: break if __name__ == \\"__main__\\": interactive_shell() ``` # Constraints: - Python 3.5+ is required due to the use of new features in the `readline` module. - Your solution should handle any edge cases, such as the history file not being present initially. # Evaluation Criteria: - Correctness: The implemented shell meets all the specified requirements. - Robustness: Handles various input scenarios gracefully. - Code quality: Follows good practices and clean coding conventions.","solution":"import os import readline import atexit def load_history(history_file: str) -> None: Load command history from history_file if it exists. try: readline.read_history_file(history_file) except FileNotFoundError: pass def save_history(history_file: str, max_commands: int) -> None: Save command history to history_file (max length max_commands). readline.set_history_length(max_commands) readline.write_history_file(history_file) def custom_completer(text: str, state: int) -> str: Custom completion function for the shell. options = [\\"help\\", \\"exit\\", \\"list\\", \\"fetch\\", \\"update\\"] matches = [s for s in options if s.startswith(text)] try: return matches[state] except IndexError: return None def interactive_shell() -> None: Custom interactive shell function that supports custom completion and history management. histfile = os.path.expanduser(\\"~/.myshell_history\\") load_history(histfile) readline.set_completer(custom_completer) readline.parse_and_bind(\\"tab: complete\\") atexit.register(save_history, histfile, 1000) while True: try: command = input(\'myshell> \') if command.strip().lower() == \'exit\': break print(f\\"Executing command: {command}\\") except EOFError: break if __name__ == \\"__main__\\": interactive_shell()"},{"question":"**Objective**: To assess the student\'s understanding of setting up an XML-RPC server using Python\'s `xmlrpc.server` module, handling requests, registering functions, and providing introspection and multicall capabilities. # Problem Statement You are required to create an XML-RPC server using Python\'s `xmlrpc.server` module. Your server should: 1. Handle XML-RPC requests at `localhost` on port `8080`. 2. Register several functions to demonstrate different computation tasks. 3. Register an instance with multiple methods and ensure these methods are accessible via the XML-RPC interface. 4. Implement introspection methods to list available methods and provide method help. 5. Enable multicall functionality allowing multiple XML-RPC calls in a single request. # Task 1. **Server Setup**: - Create a server that listens to requests on `localhost:8080`. - Restrict the server to handle requests only at the path `/RPC2`. 2. **Function Registration**: - Register a function `power` that takes two integers and returns the first integer raised to the power of the second integer. - Register another function `divide` under the name `div`, which takes two integers and returns the result of their division. Ensure division by zero is handled gracefully. 3. **Instance Registration**: - Create a class `MathOperations` with two methods: - `add(self, a, b)`: Returns the sum of `a` and `b`. - `subtract(self, a, b)`: Returns the result of `a` minus `b`. - Register an instance of `MathOperations` ensuring its methods are accessible. 4. **Introspection and Multicall**: - Register XML-RPC introspection functions which allow a client to list available methods and get help on them. - Enable multicall functionality to allow clients to execute multiple calls in one request. # Expected Input and Output - **Input**: XML-RPC requests sent to the server. - **Output**: Appropriate XML-RPC responses from the server, including method results, list of available methods, and help on methods. # Sample Code Structure ```python from xmlrpc.server import SimpleXMLRPCServer, SimpleXMLRPCRequestHandler # Restrict to a particular path class RequestHandler(SimpleXMLRPCRequestHandler): rpc_paths = (\'/RPC2\',) # Create server with SimpleXMLRPCServer((\'localhost\', 8080), requestHandler=RequestHandler) as server: server.register_introspection_functions() server.register_multicall_functions() # 1. Register function def power(x, y): return x ** y server.register_function(power) def divide(x, y): if y == 0: return \'Division by zero error\' return x / y server.register_function(divide, \'div\') # 2. Register instance class MathOperations: def add(self, a, b): return a + b def subtract(self, a, b): return a - b server.register_instance(MathOperations()) print(\\"Server is running on localhost:8080/RPC2\\") server.serve_forever() ``` # Evaluation Criteria - Correct server setup and request handling. - Proper function and instance registration. - Implementation of introspection and multicall functionalities. - Handling of edge cases (e.g., division by zero). - Code readability and organization. **Ensure your solution meets all the specified requirements and is properly tested.**","solution":"from xmlrpc.server import SimpleXMLRPCServer, SimpleXMLRPCRequestHandler # Restrict to a particular path class RequestHandler(SimpleXMLRPCRequestHandler): rpc_paths = (\'/RPC2\',) # Create server def start_server(): with SimpleXMLRPCServer((\'localhost\', 8080), requestHandler=RequestHandler) as server: server.register_introspection_functions() server.register_multicall_functions() # 1. Register function def power(x, y): return x ** y server.register_function(power) def divide(x, y): if y == 0: return \'Division by zero error\' return x / y server.register_function(divide, \'div\') # 2. Register instance class MathOperations: def add(self, a, b): return a + b def subtract(self, a, b): return a - b server.register_instance(MathOperations()) print(\\"Server is running on localhost:8080/RPC2\\") server.serve_forever() if __name__ == \\"__main__\\": start_server()"},{"question":"Objective Demonstrate your understanding of seaborn\'s styling capabilities by creating different plot types with customized stylistic parameters. Task You are provided with some data and certain style requirements. Your task is to create two different plots using seaborn by applying the specified styles. Requirements 1. **Bar Plot**: - Use the \'whitegrid\' style. - Create a bar plot for the given categories and their corresponding values. - X-axis: Categories `[\\"A\\", \\"B\\", \\"C\\"]` - Y-axis: Values `[5, 7, 2]` 2. **Line Plot**: - Use the \'darkgrid\' style. - Customize the grid lines to have a color of \\"0.6\\" and a linestyle of \\":\\". - Create a line plot for the given points. - X-axis: Categories `[\\"A\\", \\"B\\", \\"C\\"]` - Y-axis: Values `[3, 8, 5]` Input and Output Formats - No input required from the user. - The function should save the two plots as \'bar_plot.png\' and \'line_plot.png\' respectively. Constraints - Ensure to set the styles and custom configurations as per instructions before plotting. - Use matplotlib\'s `savefig()` to save the plot images. Sample Code Structure ```python import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Bar Plot sns.set_style(\\"whitegrid\\") plt.figure() sns.barplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[5, 7, 2]) plt.savefig(\\"bar_plot.png\\") # Line Plot sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}) plt.figure() sns.lineplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[3, 8, 5]) plt.savefig(\\"line_plot.png\\") create_plots() ``` Complete this function such that it adheres to the given requirements and passes the defined constraints.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Bar Plot sns.set_style(\\"whitegrid\\") plt.figure() sns.barplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[5, 7, 2]) plt.savefig(\\"bar_plot.png\\") plt.close() # Line Plot sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}) plt.figure() sns.lineplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[3, 8, 5]) plt.savefig(\\"line_plot.png\\") plt.close() create_plots()"},{"question":"Thread-Safe Messaging System # Objective Design a thread-safe messaging system using Python\'s `queue` module to simulate a multi-producer, multi-consumer environment. You will implement a system where multiple producers generate messages and multiple consumers process these messages concurrently. # Requirements 1. Implement a `MessagingSystem` class with the following specifications: - **Initialization**: - Accept two parameters: `num_producers` and `num_consumers` to specify the number of producer and consumer threads respectively. - **Methods**: - `start_system(self)`: Initializes and starts the producer and consumer threads. - `producer(self, producer_id)`: Simulates a producer that generates 10 messages (numbered 1 through 10) prefixed with the `producer_id` (e.g., \\"Producer 1: Message 1\\"). - `consumer(self, consumer_id)`: Simulates a consumer that processes messages from the queue. Each processed message should be printed alongside the `consumer_id` (e.g., \\"Consumer 1 processed: Producer 1: Message 1\\"). - `stop_system(self)`: Shuts down the messaging system gracefully after all messages are processed. 2. Make use of the `queue.Queue` class to manage the message queue. 3. Ensure thread-safe operations for both producers and consumers. 4. Use appropriate methods from the `queue` module to handle synchronization and manage full/empty queue states. # Constraints - Implement your code in Python. - The `start_system` method should create and start all producer and consumer threads. - Each producer thread should produce exactly 10 messages. - Each consumer thread should consume messages until all produced messages are processed. - Handle any synchronization issues and ensure no messages are lost or processed multiple times. # Example Usage: ```python if __name__ == \\"__main__\\": messaging_system = MessagingSystem(num_producers=3, num_consumers=2) messaging_system.start_system() # Producers and consumers run concurrently messaging_system.stop_system() # Ensure all messages are processed before shutting down ``` # Evaluation Criteria: - Correct implementation of thread-safe producer and consumer interactions. - Proper use of the `queue` module methods to handle queue operations. - Efficient management and synchronization of threads. - Graceful shutdown ensuring all messages are processed. # Notes: - Focus on implementing a robust, thread-safe system that demonstrates thorough understanding of the `queue` module and multi-threading concepts. - Ensure your solution handles edge cases, such as queue being empty or full, gracefully.","solution":"import threading import queue import time class MessagingSystem: def __init__(self, num_producers, num_consumers): self.num_producers = num_producers self.num_consumers = num_consumers self.message_queue = queue.Queue() self.producer_threads = [] self.consumer_threads = [] self.shutdown_event = threading.Event() def start_system(self): # Create producer threads for i in range(self.num_producers): thread = threading.Thread(target=self.producer, args=(i+1,)) thread.start() self.producer_threads.append(thread) # Create consumer threads for i in range(self.num_consumers): thread = threading.Thread(target=self.consumer, args=(i+1,)) thread.start() self.consumer_threads.append(thread) def producer(self, producer_id): for i in range(10): message = f\\"Producer {producer_id}: Message {i+1}\\" self.message_queue.put(message) time.sleep(0.1) # Simulate work print(f\\"Producer {producer_id} finished producing.\\") def consumer(self, consumer_id): while not self.shutdown_event.is_set() or not self.message_queue.empty(): try: message = self.message_queue.get(timeout=0.1) print(f\\"Consumer {consumer_id} processed: {message}\\") self.message_queue.task_done() except queue.Empty: continue print(f\\"Consumer {consumer_id} finished consuming.\\") def stop_system(self): # Wait for all producer threads to finish for thread in self.producer_threads: thread.join() # Signal consumers to stop processing self.shutdown_event.set() # Wait for all consumer threads to finish for thread in self.consumer_threads: thread.join() print(\\"Messaging system shutdown complete.\\")"},{"question":"Implementing a Custom ExtensionArray in pandas Objective Create a custom numerical data type in pandas by implementing an `ExtensionArray` and an associated `ExtensionDtype`. The custom data type should represent an array of integers that can also handle missing values. Requirements 1. **CustomDtype Class**: Implement a subclass of `pandas.api.extensions.ExtensionDtype` called `CustomDtype` for your new data type. 2. **CustomArray Class**: Implement a subclass of `pandas.api.extensions.ExtensionArray` called `CustomArray` to store and manipulate your custom data. 3. **Register Dtype**: Register the `CustomDtype` so that pandas recognizes it. 4. **Basic functionality**: Your `CustomArray` should at least support: - Creation from a sequence of values, including handling missing values. - Basic array-like operations such as: `take`, `isna`, `fillna`, and `unique`. - Conversion to a standard NumPy array. Constraints - Implement the methods `take`, `isna`, `fillna`, and `unique` efficiently considering both memory and time performance. - Use the sentinel `pandas.api.extensions.no_default` appropriately for default function arguments when needed. Input and Output - **Input**: A sequence of integer values, which can include `None` to represent missing values. - Example: `[1, 2, None, 4, 5, None]` - **Output**: Implement methods that: - Return the array values, with a provision to handle missing values. - Return boolean masks indicating missing values. - Fill missing values with a specified value. - Return unique values in the array, excluding `None`. Example ```python from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype import numpy as np import pandas as pd @register_extension_dtype class CustomDtype(ExtensionDtype): # Define necessary dtype attributes and methods pass class CustomArray(ExtensionArray): # Define necessary array attributes and methods def __init__(self, values): # Initialization logic pass @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): # Creation logic pass def take(self, indices, allow_fill=False, fill_value=None): # Implement take method pass def isna(self): # Implement isna method pass def fillna(self, value=None, method=None, limit=None): # Implement fillna method pass def unique(self): # Implement unique method pass def to_numpy(self, dtype=None, copy=False, na_value=pandas.api.extensions.no_default): # Convert to NumPy array pass # Testing the functionality with an example data = CustomArray([1, 2, None, 4, 5, None]) print(data.take([0, 2, 4])) # Example output [1, None, 5] print(data.isna()) # Example output [False, False, True, False, False, True] print(data.fillna(0)) # Example output [1, 2, 0, 4, 5, 0] print(data.unique()) # Example output [1, 2, 4, 5, None] print(data.to_numpy()) # Example output array([1, 2, None, 4, 5, None], dtype=object) ``` Notes - Thoroughly test your implementation to ensure it properly handles edge cases and accurately integrates with pandas. - You are not required to implement all the methods available in `ExtensionArray`. The focus should be on the required methods specified above. - Comment your code for clarity. Submission Please submit your Python code solution along with tests showing that your solution works as expected.","solution":"from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype import numpy as np import pandas as pd @register_extension_dtype class CustomDtype(ExtensionDtype): name = \\"custom\\" type = np.integer kind = \'O\' na_value = None @classmethod def construct_array_type(cls): return CustomArray class CustomArray(ExtensionArray): def __init__(self, values): self._data = np.asarray(values, dtype=object) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) def __len__(self): return len(self._data) def __getitem__(self, item): if isinstance(item, int): return self._data[item] else: return CustomArray(self._data[item]) def take(self, indices, allow_fill=False, fill_value=None): if fill_value is None: fill_value = self.dtype.na_value result = [] for idx in indices: if idx == -1 and allow_fill: result.append(fill_value) else: result.append(self._data[idx]) return CustomArray(result) def isna(self): return np.array([x is None for x in self._data]) def fillna(self, value=None, method=None, limit=None): if value is None: raise ValueError(\\"Must specify a fill value\\") filled_data = [x if x is not None else value for x in self._data] return CustomArray(filled_data) def unique(self): seen = set() unique_list = [] for x in self._data: if x not in seen: seen.add(x) unique_list.append(x) return CustomArray(unique_list) def to_numpy(self, dtype=None, copy=False, na_value=pd.api.extensions.no_default): return np.array(self._data, dtype=dtype) @property def dtype(self): return CustomDtype()"},{"question":"Objective: Write a Python function that processes a buffer view over an object using the modern Python 3 buffer protocol. The function should demonstrate an understanding of buffer acquisition and management. Task: Implement a function `process_buffer(obj)` that accepts a single argument `obj` and performs the following steps: 1. Acquires a read-only buffer view of the object. 2. Calculates and returns the sum of all bytes in the buffer. Requirements: - Input: The function will take a single parameter `obj` which is expected to be a buffer-compatible object (e.g., bytes, bytearray). - Output: Return an integer which is the sum of all bytes in the buffer. - Constraints: If the object does not support the buffer interface, the function should raise a `TypeError`. Implementation Details: - Use the `memoryview` function to acquire the buffer view. - Ensure that the buffer is released after processing. - Handle cases where the object does not support the buffer interface gracefully. Example: ```python def process_buffer(obj): try: # Step 1: Acquire a read-only buffer view using memoryview with memoryview(obj) as buf_view: # Step 2: Calculate the sum of all bytes in the buffer return sum(buf_view) except TypeError: raise TypeError(\\"The provided object does not support the buffer interface.\\") # Test cases print(process_buffer(b\'hello\')) # Output: 532 (104+101+108+108+111) print(process_buffer(bytearray([1, 2, 3, 4]))) # Output: 10 (1+2+3+4) print(process_buffer(\'hello\')) # Output: TypeError ``` This question assesses the student\'s understanding of buffer protocols in Python, exception handling, and memory management.","solution":"def process_buffer(obj): Acquires a read-only buffer view of the object and calculates and returns the sum of all bytes in the buffer. :param obj: A buffer-compatible object (e.g., bytes, bytearray) :return: The sum of all bytes in the buffer :raises TypeError: If the object does not support the buffer interface try: # Acquire a read-only buffer view using memoryview buf_view = memoryview(obj) # Calculate the sum of all bytes in the buffer byte_sum = sum(buf_view) # Release the memoryview by deleting it del buf_view return byte_sum except TypeError: raise TypeError(\\"The provided object does not support the buffer interface.\\")"},{"question":"**Problem Statement:** Implement a Python function `custom_compress_and_decompress` that performs the following tasks using the `lzma` module: 1. Compress a given list of strings into one compressed file using a custom filter chain. 2. Read back the compressed file and decompress it to retrieve the original strings. 3. The integrity of the compressed file should be ensured using a specified integrity check (e.g., CRC64). 4. Optimize the compression level to minimize the file size without exceeding a set memory limit during compression. # Function Signature ```python def custom_compress_and_decompress(input_strings: List[str], output_filename: str) -> List[str]: pass ``` # Input - `input_strings (List[str])`: A list of strings to be compressed. - `output_filename (str)`: The name of the output file for the compressed data. # Output - `List[str]`: A list of strings retrieved from the decompressed file, which should match the original `input_strings`. # Constraints - Utilize the following custom filter chain for compression: ```python my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] ``` - Use `CHECK_CRC64` for the integrity check. - The compression process should not exceed 200 MiB of memory. - Assume input strings have variable lengths and may contain large amounts of data. - Do not use any temporary files or external libraries other than `lzma`. # Example ```python input_strings = [\\"Hello\\", \\"World\\", \\"Compression\\", \\"Test\\"] output_filename = \\"compressed_file.xz\\" result = custom_compress_and_decompress(input_strings, output_filename) assert result == input_strings ``` # Guidance 1. Convert the list of strings to a single bytes object for compression. 2. Write the compressed data to the specified output file using the provided filter chain and integrity check. 3. Read the compressed file and decompress the data. 4. Ensure that the decompress function correctly reconstructs the list of original strings. # Notes - Focus on correctness and efficiency. - Make sure the solution handles errors and edge cases, such as empty input or invalid file paths.","solution":"import lzma from typing import List def custom_compress_and_decompress(input_strings: List[str], output_filename: str) -> List[str]: # Concatenate all strings into a single bytes object concatenated_data = \'n\'.join(input_strings).encode(\'utf-8\') # Using newline to separate strings # Custom filters for compression my_filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] # Compress the data with lzma.open(output_filename, \'wb\', format=lzma.FORMAT_XZ, check=lzma.CHECK_CRC64, filters=my_filters) as fp: fp.write(concatenated_data) # Read back the compressed data and decompress it with lzma.open(output_filename, \'rb\') as fp: decompressed_data = fp.read() # Convert the decompressed data back to the list of strings decompressed_strings = decompressed_data.decode(\'utf-8\').split(\'n\') return decompressed_strings"},{"question":"# CSV Manipulation with Custom Dialect You are provided with a CSV file containing student data and a requirement to manipulate this data using custom CSV dialects. Demonstrate your understanding of the `csv` module by implementing functionalities to read, process, and write CSV data as per given specifications. Task 1. **Define a Custom Dialect**: - Create a dialect named `student_dialect` with the following properties: - Delimiter: `;` - Quote character: `\\"` - Doublequote: `True` - Skipinitialspace: `True` - Line terminator: `n` - Quoting: `csv.QUOTE_MINIMAL` 2. **Reading and Processing CSV Data**: - Write a function `read_students(file_path: str) -> list` that reads the CSV file using the `student_dialect` and returns a list of dictionaries where each dictionary represents a student. 3. **Calculating Average Age**: - Write a function `calculate_average_age(students: list) -> float` that takes the list of students as input and returns the average age of students. 4. **Writing Processed Data**: - Write a function `write_students(file_path: str, students: list)` that writes the list of students back to a new CSV file using the `student_dialect`. Input - The CSV input file will be provided with the following format: ``` Name; Age; Grade John Doe; 20; A Jane Smith; 22; B ... ``` - `student_dialect` must be used for both reading and writing CSV data. Output - Your functions should correctly read the CSV file, compute the average age, and write the data back to another CSV file with the same dialect. Constraints - Assume the CSV file is not empty and has valid data. - You should handle any exceptions related to file I/O or CSV parsing appropriately. Example ```python # Define the custom dialect register_student_dialect() # Read students from file students = read_students(\'students.csv\') # Calculate average age average_age = calculate_average_age(students) print(f\'The average age is {average_age:.2f}\') # Write students back to another file write_students(\'processed_students.csv\', students) ``` Notes - Make sure to import the necessary modules (`csv`) and handle edge cases. - You should also include comments and docstrings in your code to ensure readability.","solution":"import csv # Define the custom CSV dialect def register_student_dialect(): Registers a custom CSV dialect with defined properties. csv.register_dialect( \'student_dialect\', delimiter=\';\', quotechar=\'\\"\', doublequote=True, skipinitialspace=True, lineterminator=\'n\', quoting=csv.QUOTE_MINIMAL ) # Function to read students from a CSV file using the custom dialect def read_students(file_path: str) -> list: Reads a CSV file using \'student_dialect\' and returns a list of dictionaries. :param file_path: Path to the input CSV file :return: List of dictionaries with student data students = [] with open(file_path, mode=\'r\', newline=\'\') as csvfile: reader = csv.DictReader(csvfile, dialect=\'student_dialect\') for row in reader: students.append(row) return students # Function to calculate the average age of students def calculate_average_age(students: list) -> float: Calculates the average age of the students. :param students: List of dictionaries containing student data :return: Average age of the students total_age = sum(int(student[\'Age\']) for student in students) return total_age / len(students) # Function to write the list of students back to a new CSV file def write_students(file_path: str, students: list): Writes the list of students to a CSV file using \'student_dialect\'. :param file_path: Path to the output CSV file :param students: List of dictionaries containing student data with open(file_path, mode=\'w\', newline=\'\') as csvfile: fieldnames = students[0].keys() writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=\'student_dialect\') writer.writeheader() for student in students: writer.writerow(student)"},{"question":"# Advanced Pandas: DataFrame Styling and Export **Objective**: Create and apply custom styles to a DataFrame using the Pandas `Styler` class and export the styled DataFrame to an HTML file. Task: Given the following DataFrame: ```python import pandas as pd data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'Math\': [88, 79, 92, 90], \'Science\': [94, 85, 91, 89], \'English\': [87, 78, 93, 92] } df = pd.DataFrame(data) ``` 1. **Custom Style**: Create a custom styling function that: - Highlights the maximum value in each column with a green background. - Highlights the minimum value in each column with a red background. - Applies a gradient background based on the values in the \'Science\' column. 2. **Application**: Apply these custom styles to the DataFrame using the `Styler` class. 3. **Export**: Export the styled DataFrame to an HTML file named `styled_dataframe.html`. **Constraints**: - You should not use any external libraries other than pandas. - The HTML file should preserve the styles when viewed in a web browser. - You may use inline CSS for styling. **Expected Outputs**: 1. Styled representation of the DataFrame in a Jupyter Notebook or IPython display. 2. An HTML file named `styled_dataframe.html` with the applied styles. **Example**: The resulting DataFrame when displayed should visually highlight the maximum and minimum values in each column, and the \'Science\' column should have a gradient background based on its values. ```python import pandas as pd data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'Math\': [88, 79, 92, 90], \'Science\': [94, 85, 91, 89], \'English\': [87, 78, 93, 92] } df = pd.DataFrame(data) # Your code to style and export the DataFrame ``` Implement the required functionality and ensure the DataFrame is both displayed with styles in the notebook and saved as an HTML file with those styles.","solution":"import pandas as pd # Sample DataFrame data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'Math\': [88, 79, 92, 90], \'Science\': [94, 85, 91, 89], \'English\': [87, 78, 93, 92] } df = pd.DataFrame(data) def highlight_max(s): \'\'\' Highlight the maximum in a Series yellow. \'\'\' is_max = s == s.max() return [\'background-color: green\' if v else \'\' for v in is_max] def highlight_min(s): \'\'\' Highlight the minimum in a Series red. \'\'\' is_min = s == s.min() return [\'background-color: red\' if v else \'\' for v in is_min] def apply_gradient(val): Apply gradient based on the value. min_val = df[\'Science\'].min() max_val = df[\'Science\'].max() norm_val = (val - min_val) / (max_val - min_val) color = f\'background-color: rgba(0, 255, 0, {norm_val})\' return color # Apply custom styles to the DataFrame styled_df = df.style.apply(highlight_max).apply(highlight_min).applymap(apply_gradient, subset=[\'Science\']) # Export styled DataFrame to HTML styled_df.to_html(\'styled_dataframe.html\')"},{"question":"**Question: Advanced File Stream Operations** **Objective**: Implement a function to read and write both text and binary data to files, demonstrating comprehension of file operations, buffering, and encoding in Python. **Description**: You are required to implement a function `process_file_operations(input_text_filename: str, input_binary_filename: str, output_text_filename: str, output_binary_filename: str) -> str` that performs the following tasks: 1. **Text File Operations**: - Read the content of a text file (`input_text_filename`) encoded in UTF-8. - Count the number of lines and words in the file. - Write the results (number of lines and words) to another text file (`output_text_filename`), encoding the output in UTF-8. 2. **Binary File Operations**: - Read the binary content of a file (`input_binary_filename`). - Reverse the content of the binary file. - Write the reversed content to another binary file (`output_binary_filename`). **Function Signature**: ```python def process_file_operations(input_text_filename: str, input_binary_filename: str, output_text_filename: str, output_binary_filename: str) -> str: pass ``` **Input**: - `input_text_filename`: The name of the input text file. - `input_binary_filename`: The name of the input binary file. - `output_text_filename`: The name of the output text file where results will be written. - `output_binary_filename`: The name of the output binary file where the reversed content will be written. **Output**: - Return a string containing a summary of the operations, in the format: `\\"Text file processed: X lines, Y words. Binary file processed: Z bytes reversed.\\"` where `X` is the number of lines, `Y` is the number of words in the text file, and `Z` is the size of the binary file in bytes. **Constraints**: - Ensure the function handles large files efficiently. - Assume the text files do not contain any special characters that interfere with line counting methods. **Performance requirements**: - The function should efficiently handle the operations with buffered I/O. - Error handling should be implemented to manage potential I/O errors and encoding issues gracefully. **Example**: ```python # Example usage summary = process_file_operations(\'sample.txt\', \'image.jpg\', \'output_summary.txt\', \'reversed_image.jpg\') print(summary) # Output: \\"Text file processed: 10 lines, 50 words. Binary file processed: 1048576 bytes reversed.\\" ``` **Note**: - You may use `io.StringIO` and `io.BytesIO` for in-memory operations if needed. - Follow best practices for closing the files or using context managers.","solution":"def process_file_operations(input_text_filename: str, input_binary_filename: str, output_text_filename: str, output_binary_filename: str) -> str: import os # Text File Operations with open(input_text_filename, \'r\', encoding=\'utf-8\') as input_text_file: text_lines = input_text_file.readlines() num_lines = len(text_lines) num_words = sum(len(line.split()) for line in text_lines) with open(output_text_filename, \'w\', encoding=\'utf-8\') as output_text_file: output_text_file.write(f\\"Number of lines: {num_lines}n\\") output_text_file.write(f\\"Number of words: {num_words}n\\") # Binary File Operations with open(input_binary_filename, \'rb\') as input_binary_file: binary_data = input_binary_file.read() binary_size = len(binary_data) reversed_binary_data = binary_data[::-1] with open(output_binary_filename, \'wb\') as output_binary_file: output_binary_file.write(reversed_binary_data) return f\\"Text file processed: {num_lines} lines, {num_words} words. Binary file processed: {binary_size} bytes reversed.\\""},{"question":"# Problem: Custom Argument Parser and Formatter You are required to implement a custom argument parser and formatter that processes command-line arguments and reformats a given string according to specified rules. This exercise will test your understanding of string manipulation, argument parsing, and system function usage. Function Signature ```python def custom_arg_parser_formatter(arguments: list[str], format_rules: dict[str, str]) -> str: pass ``` Input - `arguments`: A list of strings representing command-line arguments. These arguments can contain flags (e.g., `-a`, `--verbose`) and values associated with those flags. - `format_rules`: A dictionary where keys represent specified flags, and values define the formatting rules (using Python\'s string format syntax) to be applied to the values associated with those flags. Output - A single string that is the result of applying the specified formatting rules to the associated values from the arguments list. Constraints 1. Flags and values in the arguments list follow the pattern of either `-flag value` or `--flag value`. 2. If a flag specified in `format_rules` is not present in the `arguments` list, the function should raise a `ValueError`. 3. Flags must start with either `-` or `--`. 4. The formatting rule will always be a valid Python string format. Example ```python arguments = [\\"-n\\", \\"Alice\\", \\"--age\\", \\"30\\", \\"-c\\", \\"Engineer\\"] format_rules = { \\"n\\": \\"Name: {}\\", \\"age\\": \\"Age: {} years\\", \\"c\\": \\"Occupation: {}\\" } result = custom_arg_parser_formatter(arguments, format_rules) # Expected result: # \\"Name: AlicenAge: 30 yearsnOccupation: Engineer\\" ``` Explanation The function should: 1. Parse the `arguments` list and extract values associated with each flag. 2. Apply the formatting rules defined in `format_rules` to the respective values. 3. Join the formatted strings with newline characters `n` to produce the final output. Notes - You may assume that the arguments list is well-formed with no duplicate flags. - The function should be robust and handle edge cases gracefully.","solution":"def custom_arg_parser_formatter(arguments: list[str], format_rules: dict[str, str]) -> str: arg_dict = {} i = 0 while i < len(arguments): if arguments[i].startswith(\\"-\\"): if arguments[i].startswith(\\"--\\"): flag = arguments[i][2:] else: flag = arguments[i][1:] if i + 1 < len(arguments) and not arguments[i + 1].startswith(\\"-\\"): arg_dict[flag] = arguments[i + 1] i += 1 else: raise ValueError(f\\"Flag {arguments[i]} is not followed by a value.\\") i += 1 result_list = [] for flag, format_rule in format_rules.items(): if flag in arg_dict: value = arg_dict[flag] formatted_str = format_rule.format(value) result_list.append(formatted_str) else: raise ValueError(f\\"Flag {flag} specified in format_rules is not found in arguments.\\") return \\"n\\".join(result_list)"},{"question":"# Question: Advanced Data Visualization with Seaborn You are provided with a dataset containing information about penguins. Your task is to create a set of plots using Seaborn\'s objects interface to visualize the dataset effectively. You must demonstrate the following: - Loading the dataset. - Creating different types of plots with jitter applied. - Customizing the jitter effect across various orientations and using specific axes. **Dataset:** Use the `penguins` dataset that can be loaded using the `seaborn.load_dataset` function. **Tasks:** 1. **Load the Dataset:** - Load the `penguins` dataset and assign it to a variable named `penguins`. 2. **Create a Basic Dot Plot with Jitter:** - Create a dot plot showing `species` on the x-axis and `body_mass_g` on the y-axis. - Apply the default jitter effect along the species axis. 3. **Customize Jitter Width:** - Create a similar dot plot as in task 2 but customize the jitter width to 0.3. 4. **Orientation Adaptation:** - Create a dot plot showing `body_mass_g` on the x-axis and `species` on the y-axis. - Apply a customized jitter width of 0.4 adapting to the new orientation. 5. **Numeric Axis Jitter:** - Create a dot plot showing the rounded off `body_mass_g` (to the nearest 1000) on the x-axis and `flipper_length_mm` on the y-axis. - Apply jitter with default settings. 6. **Specific Axis Jitter:** - Create a dot plot showing the rounded off `body_mass_g` (to the nearest 1000) on the x-axis and `flipper_length_mm` on the y-axis. - Apply jitter specifically along the x-axis with a value of 150. 7. **Combined Axis Jitter:** - Create a dot plot showing the rounded off `body_mass_g` (to the nearest 1000) on the x-axis and the rounded off `flipper_length_mm` (to the nearest 10) on the y-axis. - Apply jitter along both x and y axes with values of 150 and 10, respectively. **Constraints:** - Ensure all plots are correctly labeled and include a title reflecting the plot\'s content. - Use Seaborn\'s objects interface (`seaborn.objects`) for creating the plots. **Expected Output:** - Display each of the plots as specified above. - Submit the code used to generate the plots. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset # Task 1: Load the Dataset penguins = load_dataset(\\"penguins\\") # Task 2: Create a Basic Dot Plot with Jitter plot1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) plot1.show() # Continue similarly for other tasks... ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Task 1: Load the Dataset penguins = load_dataset(\\"penguins\\") # Task 2: Create a Basic Dot Plot with Jitter def plot_basic_dot_with_jitter(data): return ( so.Plot(data, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) # Task 3: Customize Jitter Width def plot_custom_jitter_width(data): return ( so.Plot(data, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.3)) ) # Task 4: Orientation Adaptation with Custom Jitter Width def plot_orientation_adaptation(data): return ( so.Plot(data, \\"body_mass_g\\", \\"species\\") .add(so.Dots(), so.Jitter(width=0.4)) ) # Task 5: Numeric Axis Jitter def plot_numeric_axis_jitter(data): data[\\"body_mass_g_rounded\\"] = data[\\"body_mass_g\\"].round(-3) return ( so.Plot(data, \\"body_mass_g_rounded\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter()) ) # Task 6: Specific Axis Jitter def plot_specific_axis_jitter(data): data[\\"body_mass_g_rounded\\"] = data[\\"body_mass_g\\"].round(-3) return ( so.Plot(data, \\"body_mass_g_rounded\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=150)) ) # Task 7: Combined Axis Jitter def plot_combined_axis_jitter(data): data[\\"body_mass_g_rounded\\"] = data[\\"body_mass_g\\"].round(-3) data[\\"flipper_length_mm_rounded\\"] = data[\\"flipper_length_mm\\"].round(-1) return ( so.Plot(data, \\"body_mass_g_rounded\\", \\"flipper_length_mm_rounded\\") .add(so.Dots(), so.Jitter(x=150, y=10)) ) # Example usage (uncomment to display plots) # plot_basic_dot_with_jitter(penguins).show() # plot_custom_jitter_width(penguins).show() # plot_orientation_adaptation(penguins).show() # plot_numeric_axis_jitter(penguins).show() # plot_specific_axis_jitter(penguins).show() # plot_combined_axis_jitter(penguins).show()"},{"question":"Objective Your task is to implement a function that can execute the code of a specified module or script without directly importing it, using the `runpy` module. This exercise will test your understanding of module execution and manipulating Python namespaces. Problem Statement Implement a function `execute_code(identifier: str, is_path: bool = False, init_globals: dict = None, run_name: str = None) -> dict` that executes either a Python module or a script and returns the resulting module globals dictionary. The function should determine whether to use `runpy.run_module` or `runpy.run_path` based on the `is_path` parameter. - If `is_path` is `False`, treat `identifier` as a module name and use `runpy.run_module`. - If `is_path` is `True`, treat `identifier` as a filesystem path and use `runpy.run_path`. Function Signature ```python def execute_code(identifier: str, is_path: bool = False, init_globals: dict = None, run_name: str = None) -> dict: pass ``` Input 1. `identifier` (str): The module name or filesystem path to be executed. 2. `is_path` (bool): A flag indicating whether the `identifier` is a module name (`False`) or a filesystem path (`True`). Default is `False`. 3. `init_globals` (dict): An optional dictionary to pre-populate the module\'s globals dictionary before the code is executed. Default is `None`. 4. `run_name` (str): An optional name for the module. Default is `None`. Output - A dictionary representing the module globals after executing the code. Constraints - The `identifier` must be a valid module name or filesystem path. - The function should handle invalid paths or module names gracefully, returning an appropriate error message or raising an exception. Example ```python # Example usage result = execute_code(\'example_module\') print(result) result = execute_code(\'/path/to/script.py\', is_path=True) print(result) ``` This question assesses your ability to use advanced features of the `runpy` module, as well as your understanding of Python modules, namespaces, and the `sys` module. Note - Refer to the Python documentation for `runpy` to understand how to use `runpy.run_module` and `runpy.run_path` effectively. - Ensure that your function handles potential exceptions and provides meaningful error messages where appropriate.","solution":"import runpy def execute_code(identifier: str, is_path: bool = False, init_globals: dict = None, run_name: str = None) -> dict: Executes a Python module or script and returns the resulting globals dictionary. Parameters: - identifier (str): The module name or filesystem path to be executed. - is_path (bool): True if identifier is a path, False if it is a module name. - init_globals (dict): An optional dictionary to pre-populate the globals. - run_name (str): An optional name for the module. Returns: - dict: A dictionary representing the globals after executing the code. try: if is_path: result = runpy.run_path(identifier, init_globals=init_globals, run_name=run_name) else: result = runpy.run_module(identifier, init_globals=init_globals, run_name=run_name) return result except Exception as e: return {\\"error\\": str(e)}"},{"question":"Objective This question aims to assess your understanding and ability to work with Python\'s garbage collection interface provided by the `gc` module. Question You are provided with a Python program that leaks memory, causing an ever-increasing memory usage. Your task is to implement a function `analyze_memory_leaks()` that: 1. Enables garbage collection debugging to gather detailed statistics about memory leaks. 2. Runs a full garbage collection cycle. 3. Returns the following statistics as a dictionary: - The number of objects collected. - The number of uncollectable objects. - The number of collected objects in each generation. - The number of objects currently tracked by the garbage collector. Input and Output Format - **Input**: The function does not take any input parameters. - **Output**: The function returns a dictionary with the following keys: - `\'collected\'`: The total number of collected objects. - `\'uncollectable\'`: The total number of uncollectable objects. - `\'generation_collected\'`: A list of integers representing the number of collected objects in each of the three generations. - `\'currently_tracked\'`: The total number of objects currently tracked by the garbage collector. Constraints - You are not allowed to modify the existing program directly to fix the memory leak. - You should use the `gc` module to gather the necessary statistics. Example ```python def analyze_memory_leaks(): import gc # Enable debugging gc.set_debug(gc.DEBUG_LEAK) # Collect garbage and gather statistics collected = gc.collect() stats = gc.get_stats() result = { \'collected\': collected, \'uncollectable\': stats[2][\'uncollectable\'], \'generation_collected\': [stats[i][\'collected\'] for i in range(len(stats))], \'currently_tracked\': len(gc.get_objects()) } return result # Example usage print(analyze_memory_leaks()) # Output: {\'collected\': 10, \'uncollectable\': 0, \'generation_collected\': [5, 3, 2], \'currently_tracked\': 200} ``` Write your implementation of the `analyze_memory_leaks()` function based on the provided instructions and example.","solution":"import gc def analyze_memory_leaks(): Analyzes memory leaks by gathering detailed statistics about the garbage collector\'s activity. Returns: dict: A dictionary with statistics about garbage collection. # Enable debugging to gather detailed statistics gc.set_debug(gc.DEBUG_LEAK) # Perform a full garbage collection cycle collected = gc.collect() gc_stats = gc.get_stats() # Gather statistics result = { \'collected\': collected, \'uncollectable\': gc.garbage, \'generation_collected\': [gen_stat[\'collected\'] for gen_stat in gc_stats], \'currently_tracked\': len(gc.get_objects()) } return result"},{"question":"# Custom Signal Handling and Alarm Management Objective: Implement a Python function to handle periodic tasks using signals and alarms. Use the `signal` module to set up signal handlers, manage alarms, and ensure clean termination upon receiving a specific termination signal. Problem Statement: Write a Python function `periodic_task_runner` that performs a periodic task every few seconds and handles an external termination signal to gracefully stop the task. The function should: 1. Set up a custom signal handler for a user-defined termination signal (`SIGUSR1`). 2. Use an alarm to trigger the periodic task every 5 seconds. 3. The periodic task should print the current timestamp each time it is executed. 4. Upon receiving the termination signal (`SIGUSR1`), the function should clean up and exit gracefully. Function Signature: ```python import signal import time def periodic_task_runner(): # Your implementation here ``` Constraints: - Use the `signal` module to handle `SIGALRM` and `SIGUSR1`. - Ensure that the periodic task is triggered every 5 seconds using an alarm. - Clearly handle the cleanup and termination when `SIGUSR1` is received. - Use appropriate error handling to ensure the program doesn\'t crash unexpectedly. Example Usage: ```python import os import time def test_periodic_task_runner(): # In a separate process to test the signal handling pid = os.fork() if pid == 0: # Child process periodic_task_runner() else: # Parent process time.sleep(15) # Let the child process run for a while os.kill(pid, signal.SIGUSR1) # Send termination signal to child os.waitpid(pid, 0) # Wait for the child process to finish # Run the test test_periodic_task_runner() ``` Notes: - The `periodic_task_runner` function should run indefinitely until the termination signal (`SIGUSR1`) is received. - Ensure that the signal handling is done in a clean and thread-safe manner, focusing on the main thread as specified in the documentation.","solution":"import signal import time import os # Flag to indicate if the process should terminate terminate = False def periodic_task_handler(signum, frame): Signal handler for SIGALRM print(f\\"Periodic task executed at {time.ctime()}\\") # Schedule the next alarm signal.alarm(5) def termination_handler(signum, frame): Signal handler for SIGUSR1 global terminate print(\\"Termination signal received. Cleaning up...\\") terminate = True def periodic_task_runner(): Runs a periodic task every 5 seconds and handles termination signal # Set the signal handler for SIGALRM for the periodic task signal.signal(signal.SIGALRM, periodic_task_handler) # Set the signal handler for SIGUSR1 for clean termination signal.signal(signal.SIGUSR1, termination_handler) # Schedule the first alarm signal.alarm(5) # Main loop to keep the process running while not terminate: time.sleep(1) # Clean up logic if any before exiting print(\\"Periodic task runner terminated gracefully.\\") # Example of how the function can be tested # Uncomment this section to manually test the function # import os # def test_periodic_task_runner(): # # In a separate process to test the signal handling # pid = os.fork() # if pid == 0: # # Child process # periodic_task_runner() # else: # # Parent process # time.sleep(15) # Let the child process run for a while # os.kill(pid, signal.SIGUSR1) # Send termination signal to child # os.waitpid(pid, 0) # Wait for the child process to finish # # Uncomment to run the test # test_periodic_task_runner()"},{"question":"**Python Development Mode Diagnostics** **Objective**: Implement a Python script under Python Development Mode to demonstrate comprehension of various debugging and resource management features offered by this mode. **Task**: 1. Write a Python function `analyze_file(file_path: str) -> int` that accepts a file path as input and returns the number of lines in the file. 2. Ensure that `analyze_file` function: * Correctly handles resource management by explicitly closing files. * Logs any potential warnings/errors when run under Python Development Mode. This should include detection of ResourceWarnings when a file is not explicitly closed. 3. Write additional diagnostics to catch these warnings/errors using Python Development Mode. **Input**: * A string `file_path` representing the path to the text file. **Output**: * An integer representing the number of lines in the file. **Constraints**: * The function should handle large files efficiently. * Properly manage file resources to avoid ResourceWarnings. * Ensure the function does not encounter unhandled exceptions on bad file descriptors. **Example**: ```python import sys def analyze_file(file_path: str) -> int: with open(file_path, \'r\') as file: nlines = len(file.readlines()) return nlines if __name__ == \\"__main__\\": file_path = sys.argv[1] print(analyze_file(file_path)) ``` To test under Python Development Mode, execute: ```bash python3 -X dev your_script.py path_to_your_text_file ``` **Notes**: * Ensure your implementation runs without errors under Python Development Mode (`python3 -X dev`). * Include comments in your code to explain resource management and any additional diagnostics. By completing this task, you will demonstrate your understanding of advanced debugging and resource management features in Python.","solution":"import logging # Setting up the logging to capture warnings and errors logging.basicConfig(level=logging.DEBUG) def analyze_file(file_path: str) -> int: Reads the file from the given file path and returns the number of lines in the file. Ensures that the file is properly closed after reading to avoid resource warnings. try: with open(file_path, \'r\') as file: nlines = sum(1 for line in file) logging.info(f\\"File \'{file_path}\' successfully analyzed with {nlines} lines.\\") return nlines except Exception as e: logging.error(f\\"An error occurred while analyzing \'{file_path}\': {e}\\") raise if __name__ == \\"__main__\\": import sys file_path = sys.argv[1] print(analyze_file(file_path))"},{"question":"You are tasked with creating a utility function that processes a list of numeric error codes and returns a human-readable summary of these errors, including their names and corresponding exceptions where applicable. Function Signature: ```python def summarize_errors(error_codes: list[int]) -> dict[str, list[str]]: pass ``` Input: - `error_codes`: A list of integers representing different error codes. Output: - Returns a dictionary where: - The keys are the string names of the error codes (as provided in `errno.errorcode`). - The values are lists containing the error message (using `os.strerror()`) and the corresponding exception string name (if any, as indicated in the documentation). If there is no specific exception, the value should be an empty string. Constraints: - The input list may contain error codes that are not defined in `errno.errorcode`. - Your implementation should handle such undefined error codes gracefully. Example: ```python import errno import os error_codes = [errno.EPERM, errno.ENOENT, 99999] # assuming 99999 is not defined output = summarize_errors(error_codes) # Expected output: { # \'EPERM\': [\'Operation not permitted\', \'PermissionError\'], # \'ENOENT\': [\'No such file or directory\', \'FileNotFoundError\'], # \'99999\': [\'Unknown error\', \'\'] # } ``` Additional Notes: - Use the `errno` and `os` modules to map error codes to their respective names, error messages, and exceptions. - Handle any undefined error codes by returning a generic message indicating \\"Unknown error\\" and an empty string for the exception. Consider edge cases where the list could be empty or contain error codes not present in the `errno` module definitions. This exercise will test the student\'s ability to work with modules, error handling, and dictionary operations.","solution":"import errno import os def summarize_errors(error_codes): error_summary = {} for code in error_codes: if code in errno.errorcode: error_name = errno.errorcode[code] error_message = os.strerror(code) error_exception = \'\' if code in errno.errorcode: error_exception = { errno.EPERM: \'PermissionError\', errno.ENOENT: \'FileNotFoundError\', errno.ESRCH: \'ProcessLookupError\', errno.EINTR: \'InterruptedError\', errno.EIO: \'OSError\', errno.ENXIO: \'OSError\', errno.E2BIG: \'OSError\', errno.ENOEXEC: \'OSError\', errno.EBADF: \'OSError\', errno.ECHILD: \'ChildProcessError\', errno.EAGAIN: \'BlockingIOError\', errno.ENOMEM: \'MemoryError\', errno.EACCES: \'PermissionError\', errno.EFAULT: \'OSError\', # Add more mappings if required }.get(code, \'OSError\') # Default to \'OSError\' if not found error_summary[error_name] = [error_message, error_exception] else: error_name = str(code) error_message = \'Unknown error\' error_exception = \'\' error_summary[error_name] = [error_message, error_exception] return error_summary"},{"question":"You are required to implement a new Python type in C, a custom list-like container (`MyList`), using the structures and functions defined in the `python310` documentation. Requirements: 1. Implement the `MyList` type, which is a variable-length object and thus should use the `PyVarObject` structure. 2. The `MyList` type should store Python objects and behave similarly to Python lists, supporting: - Initialization of an empty `MyList`. - Adding an item to the end (`append` method). - Getting the length of the list. - Accessing an item by index. 3. The `MyList` object must correctly manage reference counts for its items. Steps to Implement: 1. **Define the `MyList` Type Structure**: - Use `PyObject_VAR_HEAD` to declare the type. - Define the necessary fields for storing elements and keeping track of the list size. 2. **Implement the Methods**: - `MyList_append(PyObject *self, PyObject *args)`: Appends an item to the list. - `MyList_len(PyObject *self)`: Returns the number of items in the list. - `MyList_getitem(PyObject *self, Py_ssize_t index)`: Returns the item at the specified index. 3. **Create the Type Object and Method Definitions**: - Use `PyMethodDef` to define the methods for the `MyList` type. - Use `PyTypeObject` to define the type with the necessary slots (e.g., `tp_name`, `tp_basicsize`, `tp_methods`, etc.). Constraints: - Ensure proper memory allocation and deallocation for the dynamic list items. - Handle all edge cases, including invalid indices and memory errors. - The performance should be optimized for appending items and getting the length. Code Template: Below is the starting template. You need to complete the missing parts to accomplish the requirements. ```c #include <Python.h> typedef struct { PyObject_VAR_HEAD PyObject **items; Py_ssize_t allocated; } MyListObject; /* Method definitions */ static PyObject* MyList_append(MyListObject *self, PyObject *args); static PyObject* MyList_len(MyListObject *self, PyObject *args); static PyObject* MyList_getitem(MyListObject *self, Py_ssize_t index); /* Method table */ static PyMethodDef MyList_methods[] = { {\\"append\\", (PyCFunction)MyList_append, METH_VARARGS, \\"Append an item to the list.\\"}, {\\"__len__\\", (PyCFunction)MyList_len, METH_NOARGS, \\"Return the length of the list.\\"}, {NULL} /* Sentinel */ }; /* Type object */ static PyTypeObject MyListType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"mymodule.MyList\\", .tp_basicsize = sizeof(MyListObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_methods = MyList_methods, .tp_new = PyType_GenericNew, }; /* Module method table */ static PyMethodDef module_methods[] = { {NULL} }; /* Module definition */ static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", NULL, -1, module_methods }; PyMODINIT_FUNC PyInit_mymodule(void) { PyObject *m; if (PyType_Ready(&MyListType) < 0) return NULL; m = PyModule_Create(&mymodule); if (m == NULL) return NULL; Py_INCREF(&MyListType); PyModule_AddObject(m, \\"MyList\\", (PyObject *) &MyListType); return m; } ``` Complete and enhance the provided template to fulfill all the requirements. Ensure that all aspects, including memory management and error handling, are appropriately addressed.","solution":"class MyList: def __init__(self): self.items = [] def append(self, item): self.items.append(item) def __len__(self): return len(self.items) def __getitem__(self, index): if index < 0 or index >= len(self.items): raise IndexError(\\"Index out of range\\") return self.items[index]"},{"question":"Asynchronous Task Management with torch.futures **Objective**: Demonstrate your understanding of PyTorch\'s `torch.futures` package by implementing and managing asynchronous tasks. **Problem Statement**: You are required to design a function that performs a series of simulated asynchronous tasks. Each task should simply wait for a designated amount of time (to simulate a time-consuming operation) and then return its task ID along with the waiting time. Your function should use the `torch.futures.Future` type to manage these tasks and ensure their execution does not block the main program flow. Additionally, you need to use the `collect_all` and `wait_all` utility functions to handle the results of these tasks. **Function Signature**: ```python def manage_async_tasks(task_durations: List[int]) -> List[Tuple[int, int]]: pass ``` **Input**: - `task_durations`: A list of integers where each integer represents the time (in seconds) a given task should wait before completing. **Output**: - Returns a list of tuples, each containing the task ID (starting from 0) and the duration it waited. **Constraints**: - You must use `torch.futures.Future` for managing asynchronous tasks. - Utilize `collect_all` to aggregate the tasks. - Ensure all tasks are completed and gather their results using `wait_all`. **Example**: Given the input `task_durations = [3, 1, 2]`, your function should perform the following tasks asynchronously: - Task 0 waits for 3 seconds. - Task 1 waits for 1 second. - Task 2 waits for 2 seconds. The function should then return the output: ```python [(0, 3), (1, 1), (2, 2)] ``` **Performance Requirements**: - Make sure your implementation leverages asynchronous execution to avoid blocking. **Implementation Notes**: - You may use Python\'s `asyncio` or any other asynchronous library if needed. - Use the provided utility functions from `torch.futures` to manage the task completion and result collection. **Test your function with the example provided and other custom test cases for validation.**","solution":"import torch from torch.futures import Future from typing import List, Tuple import asyncio async def async_task(task_id: int, duration: int) -> Tuple[int, int]: await asyncio.sleep(duration) return task_id, duration def manage_async_tasks(task_durations: List[int]) -> List[Tuple[int, int]]: loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) futures = [] for task_id, duration in enumerate(task_durations): future = asyncio.ensure_future(async_task(task_id, duration)) futures.append(future) results = loop.run_until_complete(asyncio.gather(*futures)) return results"},{"question":"# **Coding Assessment Question** **Objective** Implement a Python function that configures the logging system using a dictionary configuration with validation against the dictionary schema provided in the `logging.config` module. Your function will handle custom formatters, handlers, and handle potential errors gracefully. **Question Description** Write a function `configure_logging(config: dict) -> str` that configures the logging system using a dictionary configuration. The function should: 1. Validate the configuration dictionary against the required schema. 2. Instantiate custom formatters, handlers, and complete the logging setup. 3. Handle exceptions raised during configuration and return a suitable error message. 4. Return the string \\"Configuration successful\\" if the configuration is successful. The configuration dictionary must follow the schema described in the `logging.config` module. Here is a sample dictionary schema you may refer to: ```python sample_config = { \'version\': 1, \'formatters\': { \'brief\': { \'format\': \'%(message)s\' }, \'default\': { \'format\': \'%(asctime)s %(levelname)-8s %(name)-15s %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'brief\' }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'INFO\', \'formatter\': \'default\', \'filename\': \'app.log\' } }, \'loggers\': { \'example\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'] } }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'] } } ``` **Function Signature** ```python def configure_logging(config: dict) -> str: # Your code here ``` **Constraints** - The `config` dictionary will adhere to the structure as mentioned in `logging.config.dictConfig`. - Custom objects and custom import resolution may be required, ensure they are instantiated correctly. - Use appropriate error handling mechanisms to catch and return error messages. **Examples** 1. **Example 1:** ```python config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\' } }, \'root\': { \'handlers\': [\'console\'], \'level\': \'DEBUG\' } } print(configure_logging(config)) ``` **Output:** ``` Configuration successful ``` 2. **Example 2:** ```python config = { \'version\': 1, \'handlers\': { \'file\': { \'class\': \'logging.FileHandler\', \'filename\': \'app.log\', \'level\': \'INFO\' } }, \'invalid_key\': { # Some invalid configuration } } print(configure_logging(config)) ``` **Output:** ``` Error in configuration: Invalid key \'invalid_key\' ``` **Notes** - Ensure the code is written considering best practices for logging configurations. - Use appropriate modules and methods from the `logging.config` module to complete the task.","solution":"import logging.config def configure_logging(config: dict) -> str: try: logging.config.dictConfig(config) return \\"Configuration successful\\" except Exception as e: return f\\"Error in configuration: {e}\\""},{"question":"# Question: Batched Matrix Multiplication with Numerical Stability Checks **Objective:** Implement a function to perform batched matrix multiplication in PyTorch while ensuring numerical stability by checking for `inf` or `NaN` values in the result. Additionally, compare the results with non-batched matrix multiplication to observe potential differences. **Function Signature:** ```python def batched_matrix_multiply_check(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform batched matrix multiplication and check numerical stability. Args: - A (torch.Tensor): A 3D tensor of shape (batch_size, m, n) containing the first set of matrices. - B (torch.Tensor): A 3D tensor of shape (batch_size, n, p) containing the second set of matrices. Returns: - torch.Tensor: A 3D tensor of shape (batch_size, m, p) containing the results of batched matrix multiplication. - List[str]: A list of warnings for batches containing `inf` or `NaN` values. pass ``` **Inputs and Outputs:** 1. **Inputs:** - `A`: A 3D PyTorch tensor of floats with shape `(batch_size, m, n)`. - `B`: A 3D PyTorch tensor of floats with shape `(batch_size, n, p)`. 2. **Outputs:** - A 3D PyTorch tensor of floats with shape `(batch_size, m, p)` containing the result of the batched matrix multiplication. - A list of strings containing messages for any batch index where the resulting matrices contain `inf` or `NaN` values, if no issues found return an empty list. **Constraints:** - Assume that the input tensors `A` and `B` have compatible shapes for matrix multiplication. - You must use PyTorch to implement this function. **Example:** ```python import torch A = torch.tensor([ [[1.0, 2.0], [3.0, 4.0]], [[1e20, 2e20], [3e20, 4e20]] ]) B = torch.tensor([ [[5.0, 6.0], [7.0, 8.0]], [[1e20, 2e20], [3e20, 4e20]] ]) result, warnings = batched_matrix_multiply_check(A, B) print(result) # Example output: # tensor([[[19.0000, 22.0000], # [43.0000, 50.0000]], # [[inf, inf], # [inf, inf]]]) print(warnings) # Example output: # [\'Batch 1 contains `inf` values.\'] ``` **Notes:** - Ensure that your function efficiently computes the batched matrix multiplication using PyTorch\'s capabilities. - Check the results for numerical stability issues such as `inf` or `NaN` values and return appropriate warnings.","solution":"import torch def batched_matrix_multiply_check(A: torch.Tensor, B: torch.Tensor) -> (torch.Tensor, list): Perform batched matrix multiplication and check numerical stability. Args: - A (torch.Tensor): A 3D tensor of shape (batch_size, m, n) containing the first set of matrices. - B (torch.Tensor): A 3D tensor of shape (batch_size, n, p) containing the second set of matrices. Returns: - torch.Tensor: A 3D tensor of shape (batch_size, m, p) containing the results of batched matrix multiplication. - List[str]: A list of warnings for batches containing `inf` or `NaN` values. result = torch.bmm(A, B) # Batch matrix multiplication warnings = [] for i, batch in enumerate(result): if torch.isnan(batch).any(): warnings.append(f\'Batch {i} contains `NaN` values.\') if torch.isinf(batch).any(): warnings.append(f\'Batch {i} contains `inf` values.\') return result, warnings"},{"question":"You are tasked with creating and visualizing a variety of datasets using the scikit-learn sample generators. Your goal is to write a function that generates these datasets, customizes certain parameters, and then visualizes them using Matplotlib. Part 1: Generating & Visualizing Datasets Implement the function `generate_and_visualize_datasets` with the following specifications: 1. **Input**: - `params` (dictionary): A dictionary where keys are dataset types (strings) and values are dictionaries of parameters. The possible keys and their corresponding dataset generating functions are: - `\\"classification\\"`: Uses `make_classification` - `\\"blobs\\"`: Uses `make_blobs` - `\\"moons\\"`: Uses `make_moons` - `\\"circles\\"`: Uses `make_circles` - `\\"gaussian_quantiles\\"`: Uses `make_gaussian_quantiles` - Each inner dictionary contains the parameters for the respective function. For example: ```python params = { \\"classification\\": {\\"n_features\\": 2, \\"n_redundant\\": 0, \\"n_informative\\": 2, \\"n_clusters_per_class\\": 2, \\"n_classes\\": 2}, \\"blobs\\": {\\"centers\\": 3, \\"cluster_std\\": 1.0, \\"random_state\\": 42}, \\"moons\\": {\\"noise\\": 0.2, \\"random_state\\": 42}, \\"circles\\": {\\"noise\\": 0.1, \\"factor\\": 0.5, \\"random_state\\": 42}, \\"gaussian_quantiles\\": {\\"n_features\\": 2, \\"n_classes\\": 3, \\"random_state\\": 42} } ``` 2. **Output**: - Display plots of the generated datasets using Matplotlib, where each plot corresponds to one dataset type. Function Signature ```python def generate_and_visualize_datasets(params: dict) -> None: pass ``` Constraints - Use `random_state=42` if not provided in the parameters. Example Given the following input: ```python params = { \\"classification\\": {\\"n_features\\": 2, \\"n_redundant\\": 0, \\"n_informative\\": 2, \\"n_clusters_per_class\\": 2, \\"n_classes\\": 2}, \\"blobs\\": {\\"centers\\": 3, \\"cluster_std\\": 1.0, \\"random_state\\": 42}, \\"moons\\": {\\"noise\\": 0.2, \\"random_state\\": 42}, \\"circles\\": {\\"noise\\": 0.1, \\"factor\\": 0.5, \\"random_state\\": 42}, \\"gaussian_quantiles\\": {\\"n_features\\": 2, \\"n_classes\\": 3, \\"random_state\\": 42} } ``` Your function should generate and display five plots, one for each key in the `params` dictionary, demonstrating the different generated datasets. Additional Requirements - Ensure proper labelling of each plot with descriptive titles indicating the dataset type (e.g., \\"make_classification\\", \\"make_blobs\\", etc.).","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_blobs, make_moons, make_circles, make_gaussian_quantiles def generate_and_visualize_datasets(params: dict) -> None: dataset_generators = { \\"classification\\": make_classification, \\"blobs\\": make_blobs, \\"moons\\": make_moons, \\"circles\\": make_circles, \\"gaussian_quantiles\\": make_gaussian_quantiles, } fig, axes = plt.subplots(1, len(params), figsize=(20, 5)) axes = axes.flatten() for idx, (key, gen_params) in enumerate(params.items()): if \'random_state\' not in gen_params: gen_params[\'random_state\'] = 42 data_func = dataset_generators[key] X, y = data_func(**gen_params) axes[idx].scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', cmap=plt.cm.Paired) axes[idx].set_title(f\\"{key}\\") plt.tight_layout() plt.show() # Example input params params = { \\"classification\\": {\\"n_features\\": 2, \\"n_redundant\\": 0, \\"n_informative\\": 2, \\"n_clusters_per_class\\": 2, \\"n_classes\\": 2}, \\"blobs\\": {\\"centers\\": 3, \\"cluster_std\\": 1.0, \\"random_state\\": 42}, \\"moons\\": {\\"noise\\": 0.2, \\"random_state\\": 42}, \\"circles\\": {\\"noise\\": 0.1, \\"factor\\": 0.5, \\"random_state\\": 42}, \\"gaussian_quantiles\\": {\\"n_features\\": 2, \\"n_classes\\": 3, \\"random_state\\": 42} } generate_and_visualize_datasets(params)"},{"question":"# URL Fetching and Error Handling You are required to implement a Python function `fetch_url`, which retrieves the content of the given URL. This function should support both GET and POST requests and allow custom headers to be specified. Additionally, the function should handle different HTTP error codes gracefully and return appropriate error messages. Function Signature ```python def fetch_url(url: str, method: str = \\"GET\\", data: dict = None, headers: dict = None) -> str: pass ``` Input - `url`: A string representing the URL to fetch. - `method`: A string representing the HTTP method to use (default is \\"GET\\"). Valid values are \\"GET\\" and \\"POST\\". - `data`: A dictionary containing data to be sent in the case of a POST request. Default is `None`. - `headers`: A dictionary containing custom headers to be sent with the request. Default is `None`. Output - Returns a string containing the content of the response or an appropriate error message if an error occurs. Constraints - If `method` is \\"POST\\", `data` should be provided and encoded properly. - Properly handle the following HTTP errors: - 404: \\"Not Found\\" - 403: \\"Forbidden\\" - 401: \\"Unauthorized\\" - Other codes in the 400 and 500 ranges should return \\"Client Error\\" and \\"Server Error\\" respectively. Example ```python # Example 1 url = \\"http://example.com\\" print(fetch_url(url, method=\\"GET\\")) # Output: \\"<html>...</html>\\" # Example of successful GET response # Example 2 url = \\"http://example.com\\" headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} print(fetch_url(url, method=\\"GET\\", headers=headers)) # Output: \\"<html>...</html>\\" # Example with a custom User-Agent header # Example 3 url = \\"http://example.com/submit\\" data = {\\"name\\": \\"John Doe\\", \\"email\\": \\"john@example.com\\"} print(fetch_url(url, method=\\"POST\\", data=data)) # Output: \\"<html>...</html>\\" # Example of successful POST response # Example 4 url = \\"http://example.com/notfound\\" print(fetch_url(url, method=\\"GET\\")) # Output: \\"Not Found\\" # Example of 404 error handling ``` Requirements 1. Use the `urllib` package to handle URL requests. 2. Encode data properly for POST requests. 3. Implement custom headers if provided. 4. Properly handle and return messages for HTTP errors as detailed above. 5. Ensure the function returns the content of the URL or an appropriate error message string.","solution":"import urllib.request import urllib.error import urllib.parse def fetch_url(url: str, method: str = \\"GET\\", data: dict = None, headers: dict = None) -> str: try: if method == \\"POST\\" and data: data = urllib.parse.urlencode(data).encode() else: data = None request = urllib.request.Request(url, data, headers or {}, method=method) with urllib.request.urlopen(request) as response: return response.read().decode() except urllib.error.HTTPError as e: if e.code == 404: return \\"Not Found\\" elif e.code == 403: return \\"Forbidden\\" elif e.code == 401: return \\"Unauthorized\\" elif 400 <= e.code < 500: return \\"Client Error\\" elif 500 <= e.code < 600: return \\"Server Error\\" else: return f\\"HTTP Error {e.code}\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\" except Exception as e: return f\\"Error: {e}\\""},{"question":"**Question: Implement a Logging System Using the `syslog` Module** As a systems programmer, your task is to implement a Python function that uses the Unix `syslog` library to log various types of system messages. The function should be capable of initializing the syslog with specific options, sending different priority messages, and properly closing the log. # Function Signature: ```python def custom_syslog(log_id: str, log_option: int, log_facility: int, messages: list) -> None: pass ``` # Input: - `log_id` (str): A string to be prepended to every log message. - `log_option` (int): An integer representing the logging option (e.g., `syslog.LOG_PID`). - `log_facility` (int): An integer representing the log facility (e.g., `syslog.LOG_MAIL`). - `messages` (list): A list of tuples where each tuple contains a message string (str) and its priority (int). # Output: - The function does not return anything. It sends messages to the system log. # Requirements: 1. Initialize the syslog using `openlog()` with the provided `log_id`, `log_option`, and `log_facility`. 2. For each message in the `messages` list, send the message to the system logger using the appropriate priority level. 3. Ensure that the syslog is closed properly using `closelog()` after all messages have been logged. 4. Handle any necessary trailing newlines for log messages. # Example: ```python import syslog def custom_syslog(log_id: str, log_option: int, log_facility: int, messages: list) -> None: # Initialize syslog syslog.openlog(ident=log_id, logoption=log_option, facility=log_facility) # Log each message with the given priority for message, priority in messages: syslog.syslog(priority, message) # Close syslog syslog.closelog() # Example usage messages = [ (\\"System booting up\\", syslog.LOG_INFO), (\\"Disk space low\\", syslog.LOG_WARNING), (\\"Unable to access database\\", syslog.LOG_ERR) ] custom_syslog(log_id=\\"MyApp\\", log_option=syslog.LOG_PID, log_facility=syslog.LOG_USER, messages=messages) ``` # Constraints: - The `messages` list will contain at least one message. - The `log_id` should be a non-empty string. - The `log_option` and `log_facility` should be valid `syslog` constants. # Note: Ensure that your implementation correctly makes use of the `syslog` module functions and constants as described in the documentation provided. This will test your ability to understand and apply the syslog library for practical logging purposes.","solution":"import syslog def custom_syslog(log_id: str, log_option: int, log_facility: int, messages: list) -> None: Logs messages to syslog with the specified log_id, log_option, and log_facility. Parameters: log_id (str): A string to be prepended to every log message. log_option (int): An integer representing the logging option (e.g., syslog.LOG_PID). log_facility (int): An integer representing the log facility (e.g., syslog.LOG_MAIL). messages (list): A list of tuples where each tuple contains a message string (str) and its priority (int). # Initialize the syslog syslog.openlog(ident=log_id, logoption=log_option, facility=log_facility) # Log each message with the given priority for message, priority in messages: syslog.syslog(priority, message) # Close the syslog syslog.closelog()"},{"question":"# Advanced Python Context Management Simulation **Objective**: Implement a simplified context management system in Python to simulate the behavior of the `contextvars` module described in the provided documentation. # Problem Statement: You are required to implement a class-based system that simulates the management of contexts and context variables. This will involve creating classes to mimic contexts (`PyContext`) and context variables (`PyContextVar`), along with the necessary functions to manipulate them. Classes and Methods: 1. **Class `PyContext`**: - **Method `__init__(self)`**: Initializes an empty context. - **Method `copy(self)`**: Returns a shallow copy of the context. - **Method `enter(self)`**: Sets this context as the active context. - **Method `exit(self)`**: Restores the previous active context. 2. **Class `PyContextVar`**: - **Method `__init__(self, name, default_value=None)`**: Initializes a context variable with a name and an optional default value. - **Method `get(self)`**: Retrieves the current value of the context variable, or its default value if unset. - **Method `set(self, value)`**: Sets a new value for the context variable and returns a token to restore the previous state. - **Method `reset(self, token)`**: Resets the context variable to the state indicated by the token. 3. **Class `PyToken`**: - **Method `__init__(self, var, value)`**: Initializes a token which stores a reference to a context variable and its previous value. # Constraints: 1. You may assume that there will be a single-threaded environment. 2. Each context operation should be successful without raising any exceptions/errors. Input and Output: - There will be no direct input/output function within your classes; all interactions will be through method calls and class instances. Example Usage: ```python # Creating a new context ctx = PyContext() ctx.enter() # Creating context variables var1 = PyContextVar(\\"var1\\", default_value=42) var2 = PyContextVar(\\"var2\\") # Setting and getting context variables token1 = var1.set(100) assert var1.get() == 100 # Resetting context variable to its previous state var1.reset(token1) assert var1.get() == 42 # Default value is restored # Exiting the context ctx.exit() ``` # Implementation: Implement the `PyContext`, `PyContextVar`, and `PyToken` classes as described above.","solution":"class PyContext: _current_context = None def __init__(self): self._vars = {} def copy(self): new_context = PyContext() new_context._vars = self._vars.copy() return new_context def enter(self): self._previous_context = PyContext._current_context PyContext._current_context = self def exit(self): PyContext._current_context = self._previous_context @classmethod def current_context(cls): if cls._current_context is None: cls._current_context = PyContext() return cls._current_context class PyContextVar: def __init__(self, name, default_value=None): self.name = name self.default_value = default_value def get(self): context = PyContext.current_context() return context._vars.get(self.name, self.default_value) def set(self, value): context = PyContext.current_context() prev_value = context._vars.get(self.name, self.default_value) context._vars[self.name] = value return PyToken(self, prev_value) def reset(self, token): context = PyContext.current_context() if token.var == self: if token.value is None: context._vars.pop(self.name, None) else: context._vars[self.name] = token.value class PyToken: def __init__(self, var, value): self.var = var self.value = value"},{"question":"Objective Implement a recursive function that computes the factorial of a number using caching for efficiency. Additionally, create a class that calculates the factorial without recursion but demonstrates an understanding of function and method transformation using `functools.partial`. Finally, evaluate the performance improvement brought by caching. Requirements 1. **Part 1: Recursive Factorial with Caching** - Implement a recursive function `cached_factorial(n)` which calculates the factorial of `n`. - Use the `@functools.cache` decorator to cache function calls. - The function should handle large values of `n` efficiently by leveraging caching. **Example:** ```python @cache def cached_factorial(n): if n == 0: return 1 else: return n * cached_factorial(n - 1) assert cached_factorial(10) == 3628800 assert cached_factorial(5) == 120 assert cached_factorial(15) == 1307674368000 ``` 2. **Part 2: Class-Based Factorial Calculator** - Create a class `FactorialCalculator` with an initializer that takes an integer `n`. - Use `functools.partial` to prebind a non-recursive method that calculates the factorial iteratively. - Provide a method `calculate()` that returns the factorial of `n`. **Example:** ```python from functools import partial class FactorialCalculator: def __init__(self, n): self.n = n self.partial_factorial = partial(self._factorial_base, n) def _factorial_base(self, x): result = 1 for i in range(1, x + 1): result *= i return result def calculate(self): return self.partial_factorial() fc = FactorialCalculator(10) assert fc.calculate() == 3628800 ``` 3. **Part 3: Performance Evaluation** - Write a function `evaluate_performance()` that compares the performance of `cached_factorial` and `FactorialCalculator.calculate()`. - Use the `time` module to measure the time taken for computing factorial for the same input `n`. - Print the results showing the time savings achieved with caching. **Example:** ```python import time def evaluate_performance(n): start_time = time.time() cached_result = cached_factorial(n) cached_time = time.time() -- start_time fc = FactorialCalculator(n) start_time = time.time() non_cached_result = fc.calculate() non_cached_time = time.time() - start_time print(f\\"Cached factorial time: {cached_time}s\\") print(f\\"Non-cached factorial time: {non_cached_time}s\\") print(f\\"Time saved with caching: {non_cached_time - cached_time}s\\") assert cached_result == non_cached_result evaluate_performance(1000) ``` Constraints - `n` is a non-negative integer. - Ensure that the code handles large values of `n` efficiently, particularly in the `cached_factorial` function. - Use Python 3.8+ features (specific version 3.9 functionalities) as specified in the functools module documentation. Submission Submit your Python code implementation for the `cached_factorial` function, the `FactorialCalculator` class with the `calculate` method, and the `evaluate_performance` function. The code should be runnable and demonstrate the functionality effectively.","solution":"import functools @functools.cache def cached_factorial(n): Calculate the factorial of n using recursion with caching. if n == 0: return 1 else: return n * cached_factorial(n - 1) from functools import partial class FactorialCalculator: def __init__(self, n): self.n = n self.partial_factorial = partial(self._factorial_base, n) def _factorial_base(self, x): result = 1 for i in range(1, x + 1): result *= i return result def calculate(self): return self.partial_factorial() import time def evaluate_performance(n): start_time = time.time() cached_result = cached_factorial(n) cached_time = time.time() - start_time fc = FactorialCalculator(n) start_time = time.time() non_cached_result = fc.calculate() non_cached_time = time.time() - start_time print(f\\"Cached factorial time: {cached_time}s\\") print(f\\"Non-cached factorial time: {non_cached_time}s\\") print(f\\"Time saved with caching: {non_cached_time - cached_time}s\\") assert cached_result == non_cached_result"},{"question":"You are given a list of filenames and a list of patterns. Write a Python function that takes these two lists as input and returns a dictionary. The keys of the dictionary should be the patterns, and the values should be lists of filenames that match each pattern. The function should use case-insensitive matching. **Function Signature:** ```python def match_files(filenames: list, patterns: list) -> dict: pass ``` **Input:** - `filenames`: A list of strings representing filenames (1 <= len(filenames) <= 10^4). - `patterns`: A list of strings representing wildcard patterns (1 <= len(patterns) <= 10^4). **Output:** - A dictionary where each key is a pattern from the input patterns list, and each value is a list of filenames that match the respective pattern. **Examples:** ```python filenames = [\\"report1.doc\\", \\"report2.doc\\", \\"report_final.doc\\", \\"summary.txt\\", \\"notes.txt\\", \\"image.png\\"] patterns = [\\"*.doc\\", \\"*.txt\\", \\"*.png\\"] output = { \\"*.doc\\": [\\"report1.doc\\", \\"report2.doc\\", \\"report_final.doc\\"], \\"*.txt\\": [\\"summary.txt\\", \\"notes.txt\\"], \\"*.png\\": [\\"image.png\\"] } ``` **Constraints:** - Use the functions provided in the `fnmatch` module for matching. - The solution should be efficient to handle up to 10^4 filenames and patterns. **Guidelines:** - Only use the functions from the `fnmatch` module to perform the pattern matching. - Ensure the function is case-insensitive. - Handle edge cases where some patterns may not match any filenames.","solution":"import fnmatch def match_files(filenames: list, patterns: list) -> dict: Returns a dictionary where each key is a pattern and the value is a list of filenames that match that pattern (case-insensitive). # Convert filenames to lowercase as fnmatch is case-sensitive filenames_lower = [filename.lower() for filename in filenames] # Pattern matching is done using lowercase filenames result = {} for pattern in patterns: pattern_lower = pattern.lower() matched_files = [filenames[i] for i in range(len(filenames)) if fnmatch.fnmatch(filenames_lower[i], pattern_lower)] result[pattern] = matched_files return result"},{"question":"**Objective:** Implement a function that analyzes a given piece of Python code to determine the frequency of each keyword and soft keyword. This will help in understanding the usage density of these elements in the code. **Instructions:** Write a function `analyze_code_keywords(code: str) -> dict` that takes a string `code` representing Python source code and returns a dictionary. The keys of the dictionary are the keywords and soft keywords present in the provided code, and the values are the count of how many times each keyword or soft keyword occurs. **Function Signature:** ```python def analyze_code_keywords(code: str) -> dict: ``` # Input: - `code` (str): A string representing valid Python code. # Output: - A dictionary where: - The keys are keywords and soft keywords found in the code. - The values are the counts of each keyword and soft keyword. # Constraints: 1. The function should correctly handle both keywords and soft keywords. 2. Ignore comments and string literals when counting the keywords. 3. The function should not count keywords that appear as substrings within identifiers or literals. # Example: ```python code_sample = def example_function(): if True: return \'No keywords here\' # return should not be counted inside a string for i in range(10): if i % 2 == 0: print(i) result = analyze_code_keywords(code_sample) print(result) # Expected output: {\'def\': 1, \'if\': 2, \'return\': 1, \'for\': 1, \'in\': 1} ``` # Additional Information: - The module `keyword` should be used to identify keywords and soft keywords in the provided code. - You may find the `re` module helpful for parsing the code while ignoring comments and string literals. - Ensure your solution is efficient and can handle large codebases. **Performance Requirements:** - The function should run in linear time relative to the size of the input code. **Note:** - This problem assesses your understanding of Python keywords and soft keywords, string manipulation, regular expressions, and efficient algorithm design.","solution":"import keyword import re from collections import Counter def analyze_code_keywords(code: str) -> dict: Analyzes the given piece of Python code to determine the frequency of each keyword and soft keyword. Parameters: code (str): A string representing the Python source code. Returns: dict: A dictionary with keywords/soft keywords as keys and their frequency as values. # List of Python keywords from the keyword module keywords = set(keyword.kwlist) # Regex to match Python string literals (single-line and multi-line) string_re = r\'(\'\'\'[^\']*\'\'\'|\\"\\"\\"[^\\"]*\\"\\"\\"|\'[^\']*\'|\\"[^\\"]*\\")\' # Regex to match Python comments comment_re = r\'#.*\' # Remove string literals code = re.sub(string_re, \'\', code) # Remove comments code = re.sub(comment_re, \'\', code) # Tokenize the code by splitting on non-alphanumeric and non-underscore characters tokens = re.findall(r\'bw+b\', code) # Filter out and count the keywords keyword_counts = Counter(token for token in tokens if token in keywords) return dict(keyword_counts)"},{"question":"# Email Message Manipulation In this assessment, you are required to implement a function `extract_and_forward_message` that extracts a specific part of an email message and forwards it as a new email message. Function Signature: ```python def extract_and_forward_message(original_msg: email.message.Message, part_type: str, recipient: str) -> email.message.Message: pass ``` Parameters: - `original_msg`: An instance of `email.message.Message` representing the original email message. - `part_type`: A string specifying the MIME type (e.g., \'text/plain\', \'image/jpeg\') of the part to be extracted. - `recipient`: A string representing the email address to which the extracted part should be forwarded. Output: - Returns a new `email.message.Message` object representing the forwarded email message with the extracted part as its content. Constraints: - If the `original_msg` does not contain any part of the specified `part_type`, the function should raise a `ValueError` with the message \\"Specified part type not found\\". - The forwarded message should have the \'To\' header set to the recipient\'s email address, and the \'Subject\' header should be set to \\"Fwd: Extracted Part\\". - The function should ensure that the content type is correctly set for the extracted part in the forwarded message. Example: ```python import email from email.message import Message def extract_and_forward_message(original_msg: email.message.Message, part_type: str, recipient: str) -> email.message.Message: # Your implementation here # Usage example raw_email = Content-Type: multipart/mixed; boundary=\\"===============7317828141912961456==\\" MIME-Version: 1.0 --===============7317828141912961456== Content-Type: text/plain This is the plain text part of the email. --===============7317828141912961456== Content-Type: text/html <html><body>This is the HTML part of the email.</body></html> --===============7317828141912961456== msg = email.message_from_string(raw_email) forwarded_msg = extract_and_forward_message(msg, \'text/plain\', \'recipient@example.com\') print(forwarded_msg.as_string()) ``` The provided example creates a MIME multipart email with text and HTML parts, extracts the plain text part, and forwards it to a specified recipient. Notes: - Ensure you handle both simple and multipart MIME messages appropriately. - You can use the methods covered in the provided documentation (e.g., `is_multipart`, `get_payload`, `attach`, `set_payload`, etc.) to implement this function. - The solution should be efficient and adhere to the best practices of handling email messages in Python.","solution":"import email from email.message import EmailMessage def extract_and_forward_message(original_msg: email.message.Message, part_type: str, recipient: str) -> email.message.Message: # Function to extract the specified part type from the original message def extract_part(msg, part_type): if msg.is_multipart(): for part in msg.get_payload(): if part.get_content_type() == part_type: return part elif msg.get_content_type() == part_type: return msg return None # Extract the part from the original message part = extract_part(original_msg, part_type) if part is None: raise ValueError(\\"Specified part type not found\\") # Create a new message forwarded_msg = EmailMessage() forwarded_msg[\'To\'] = recipient forwarded_msg[\'Subject\'] = \'Fwd: Extracted Part\' forwarded_msg.set_content(part.get_payload(), subtype=part.get_content_subtype()) return forwarded_msg"},{"question":"**Coding Question:** You are given a list of strings. Your task is to implement a function `classify_keywords(word_list)` that processes this list and returns a dictionary classifying each string into one of three categories: 1. \\"keyword\\" if the string is a Python keyword. 2. \\"soft keyword\\" if the string is a Python soft keyword. 3. \\"identifier\\" if the string is neither a keyword nor a soft keyword. # Input - A list of strings `word_list` with 1 ≤ len(word_list) ≤ 1000. - Each string in the list will contain only letters (a-z, A-Z) and have a length of 1 to 20 characters. # Output - A dictionary with three keys: `\\"keyword\\"`, `\\"soft keyword\\"`, and `\\"identifier\\"`. Each key maps to a list of strings that fall under the respective category. # Example ```python word_list = [\\"if\\", \\"match\\", \\"case\\", \\"variable\\", \\"def\\"] output = classify_keywords(word_list) # Expected output: # { # \\"keyword\\": [\\"if\\", \\"def\\"], # \\"soft keyword\\": [\\"match\\", \\"case\\"], # \\"identifier\\": [\\"variable\\"] # } ``` # Constraints - You must leverage the functionalities provided by the `keyword` module to determine if a string is a keyword or a soft keyword. - Consider the efficiency of your solution given the input constraints. # Function Signature ```python def classify_keywords(word_list: list) -> dict: pass ```","solution":"import keyword def classify_keywords(word_list): Classifies a list of strings into keywords, soft keywords, and identifiers. Parameters: word_list (list): A list of strings to classify. Returns: dict: A dictionary with categories \'keyword\', \'soft keyword\', and \'identifier\'. result = { \\"keyword\\": [], \\"soft keyword\\": [], \\"identifier\\": [] } python_keywords = set(keyword.kwlist) python_soft_keywords = set(keyword.softkwlist) for word in word_list: if word in python_keywords: result[\\"keyword\\"].append(word) elif word in python_soft_keywords: result[\\"soft keyword\\"].append(word) else: result[\\"identifier\\"].append(word) return result"},{"question":"Advanced Filename Matching In this exercise, you will implement a function that takes a list of filenames and a list of patterns, and returns a dictionary summarizing which filenames match each pattern using both case-insensitive and case-sensitive matching. Your task is to implement the function `match_filenames(filenames: List[str], patterns: List[str]) -> Dict[str, Dict[str, List[str]]]` with the following specifications: Function Signature ```python from typing import List, Dict def match_filenames(filenames: List[str], patterns: List[str]) -> Dict[str, Dict[str, List[str]]]: pass ``` Input - `filenames`: A list of strings representing the filenames. - `patterns`: A list of strings representing the patterns. Output - A dictionary where each key is a pattern from the `patterns` list. The value associated with each key is another dictionary with two keys: - `\\"case_insensitive\\"`: A list of filenames from `filenames` that match the pattern case-insensitively. - `\\"case_sensitive\\"`: A list of filenames from `filenames` that match the pattern case-sensitively. Constraints - All filenames and patterns are non-empty strings. - There are no restrictions on the characters used in filenames or patterns. - Performance should be considered for lists containing up to 10,000 filenames and patterns. Example ```python filenames = [\\"Readme.md\\", \\"example.TXT\\", \\"sample.txt\\", \\"test.TXT\\", \\"data.csv\\"] patterns = [\\"*.txt\\", \\"*.TXT\\", \\"*.md\\", \\"*.csv\\"] expected_output = { \\"*.txt\\": { \\"case_insensitive\\": [\\"example.TXT\\", \\"sample.txt\\", \\"test.TXT\\"], \\"case_sensitive\\": [\\"sample.txt\\"] }, \\"*.TXT\\": { \\"case_insensitive\\": [\\"example.TXT\\", \\"sample.txt\\", \\"test.TXT\\"], \\"case_sensitive\\": [\\"example.TXT\\", \\"test.TXT\\"] }, \\"*.md\\": { \\"case_insensitive\\": [\\"Readme.md\\"], \\"case_sensitive\\": [\\"Readme.md\\"] }, \\"*.csv\\": { \\"case_insensitive\\": [\\"data.csv\\"], \\"case_sensitive\\": [\\"data.csv\\"] } } result = match_filenames(filenames, patterns) assert result == expected_output ``` Guidelines 1. Use the `fnmatch.fnmatch()` function to perform the case-insensitive matching. 2. Use the `fnmatch.fnmatchcase()` function to perform the case-sensitive matching. 3. Ensure the function is efficient and can handle large lists of filenames and patterns. Good luck!","solution":"from typing import List, Dict import fnmatch def match_filenames(filenames: List[str], patterns: List[str]) -> Dict[str, Dict[str, List[str]]]: result = {} for pattern in patterns: case_insensitive_matches = [filename for filename in filenames if fnmatch.fnmatch(filename.lower(), pattern.lower())] case_sensitive_matches = [filename for filename in filenames if fnmatch.fnmatchcase(filename, pattern)] result[pattern] = { \\"case_insensitive\\": case_insensitive_matches, \\"case_sensitive\\": case_sensitive_matches } return result"},{"question":"Coding Assessment Question # Task You are required to implement a simple task scheduler using Python\'s `asyncio` package. The scheduler should manage multiple asynchronous tasks, synchronize their execution using locks, and handle timeouts. This will demonstrate your understanding of tasks, locks, and exception handling in an asynchronous environment. # Details 1. Implement a function `task_scheduler(tasks, timeout)` that: - Takes a list of tasks (coroutines) and a timeout duration (in seconds) as input. - Runs each task concurrently. - Uses an `asyncio.Lock` to ensure that only one task can access a shared resource simultaneously. - Cancels all tasks if any task takes longer than the specified timeout to complete. - Returns a list of results of completed tasks. If a task was canceled due to timeout, include `None` in the result list for that task. 2. Implement the following additional functions to test `task_scheduler`: - `async def sample_task1(delay)`: Simulates a task by sleeping for `delay` seconds and then returning the string \\"Task1 completed\\". - `async def sample_task2(delay)`: Simulates a task by sleeping for `delay` seconds and then returning the string \\"Task2 completed\\". # Input - `tasks`: A list of tasks (coroutines) to be executed, e.g., `[sample_task1(2), sample_task2(3)]`. - `timeout`: Timeout duration in seconds, e.g., `5`. # Output - A list of results for the completed tasks. If a task was canceled due to timeout, it should return `None` for that task. # Example ```python import asyncio async def sample_task1(delay): await asyncio.sleep(delay) return \\"Task1 completed\\" async def sample_task2(delay): await asyncio.sleep(delay) return \\"Task2 completed\\" async def task_scheduler(tasks, timeout): results = [] lock = asyncio.Lock() async def _run_task(task): async with lock: try: result = await asyncio.wait_for(task, timeout) return result except asyncio.TimeoutError: return None results = await asyncio.gather(*[_run_task(task) for task in tasks]) return results # Example usage # List of tasks tasks = [ sample_task1(2), sample_task2(3) ] # Timeout timeout = 5 # Running the scheduler results = asyncio.run(task_scheduler(tasks, timeout)) print(results) # Output: [\\"Task1 completed\\", \\"Task2 completed\\"] # Example with a timeout timeout = 1 results = asyncio.run(task_scheduler(tasks, timeout)) print(results) # Output: [None, None] ``` # Constraints - Ensure the function works for tasks with diverse delays (e.g., short and long delays). - Handle exceptions properly to avoid unexpected crashes. # Notes - Use `asyncio.gather()` to run tasks concurrently. - Use `asyncio.Lock` to synchronize access to any shared resources in your tasks. - Use `asyncio.wait_for()` to enforce the timeout on each task. - Handle `asyncio.TimeoutError` to return `None` for tasks that exceed the timeout.","solution":"import asyncio async def sample_task1(delay): await asyncio.sleep(delay) return \\"Task1 completed\\" async def sample_task2(delay): await asyncio.sleep(delay) return \\"Task2 completed\\" async def task_scheduler(tasks, timeout): results = [] lock = asyncio.Lock() async def _run_task(task): async with lock: try: result = await asyncio.wait_for(task, timeout) return result except asyncio.TimeoutError: return None results = await asyncio.gather(*[_run_task(task) for task in tasks]) return results"},{"question":"Using the `iris` dataset from `seaborn`, create a multi-faceted scatter plot that includes both a polynomial regression line and individual data points. The facets should be based on the species of the iris. Customize the appearance of the plot using the `seaborn.objects` interface and apply a theme and style parameters from both `seaborn` and `matplotlib`. # Task 1. Load the `iris` dataset using `seaborn.load_dataset`. 2. Create a plot using `so.Plot` with `sepal_length` on the x-axis and `sepal_width` on the y-axis. 3. Facet the plot into separate panels for each `species`. 4. Add a polynomial regression line (order=2) and individual data points to each facet. 5. Apply a custom `seaborn` theme that sets the background face color to white and the edge color to slategray. 6. Apply the `fivethirtyeight` style from `matplotlib`. 7. Display the plot. # Constraints - Import necessary libraries from `seaborn` and `matplotlib`. - Ensure that the plot facets are arranged in a single row. - The regression line should be a polynomial of degree 2. - The plot should adhere to a clean and professional style suitable for publication. # Input and Output - **Input**: No direct input from the user; the code should utilize the `iris` dataset provided by the `seaborn` library. - **Output**: A multifaceted scatter plot displayed with the specified customizations. # Example ```python import seaborn.objects as so from seaborn import load_dataset from matplotlib import style from seaborn import axes_style, plotting_context # Load the iris dataset iris = load_dataset(\\"iris\\") # Create the plot p = ( so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") .facet(\\"species\\") .add(so.Line(), so.PolyFit(order=2)) .add(so.Dot()) ) # Apply the custom theme and style p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) p.theme(style.library[\\"fivethirtyeight\\"]) p.theme(axes_style(\\"ticks\\") | plotting_context(\\"talk\\")) # Display the plot p ```","solution":"import seaborn.objects as so from seaborn import load_dataset from matplotlib import style from seaborn import axes_style, plotting_context def create_facet_scatter_plot(): # Load the iris dataset iris = load_dataset(\\"iris\\") # Create the plot p = ( so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") .facet(\\"species\\", wrap=3) .add(so.Line(), so.PolyFit(order=2)) .add(so.Dot()) ) # Apply the custom theme and style p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) style.use(\'fivethirtyeight\') p.theme(axes_style(\\"ticks\\") | plotting_context(\\"talk\\")) # Display the plot p.show() # Run the function to create and show the plot if __name__ == \\"__main__\\": create_facet_scatter_plot()"},{"question":"**Objective**: Demonstrate your understanding of the `xml.sax` package in Python by implementing a SAX parser to extract specific information from an XML document. **Question**: Given an XML document containing information about books, design and implement a SAX application in Python to extract the titles and authors of the books. Here is an example XML document: ```xml <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> <!-- more book entries --> </catalog> ``` **Requirements**: 1. Implement a SAX `ContentHandler` to handle the XML parsing events. 2. Extract and print the title and author of each book in the XML document. 3. Handle potential parsing errors gracefully using `xml.sax.SAXParseException`. **Input Format**: - You will receive the XML content as a string. **Output Format**: - Print each book\'s title and author in the format: `Title: <title>, Author: <author>`. **Constraints**: - The XML document can have multiple books. - Ensure your solution can handle an undefined number of books. **Performance Requirements**: - Your implementation should be optimal and handle large XML documents efficiently. **Example**: ```python import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" def startElement(self, tag, attributes): self.current_element = tag def endElement(self, tag): if tag == \\"book\\": print(f\\"Title: {self.title}, Author: {self.author}\\") self.title = \\"\\" self.author = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.title += content elif self.current_element == \\"author\\": self.author += content def parse_books(xml_string): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: print(f\\"Parsing Error: {e.getMessage()}\\") # Example usage: xml_content = <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> </book> </catalog> parse_books(xml_content) ``` When `parse_books` is called with the example XML content, the output should be: ```plaintext Title: XML Developer\'s Guide, Author: Gambardella, Matthew Title: Midnight Rain, Author: Ralls, Kim ``` Ensure your solution adheres to the above requirements and constraints. Happy coding!","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" self.books = [] def startElement(self, tag, attributes): self.current_element = tag def endElement(self, tag): if tag == \\"book\\": self.books.append((self.title.strip(), self.author.strip())) self.title = \\"\\" self.author = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.title += content elif self.current_element == \\"author\\": self.author += content def parse_books(xml_string): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: print(f\\"Parsing Error: {e.getMessage()}\\") return handler.books # Example usage: if __name__ == \\"__main__\\": xml_content = <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> </book> </catalog> books = parse_books(xml_content) for title, author in books: print(f\\"Title: {title}, Author: {author}\\")"},{"question":"Context The Seaborn visualization library offers powerful tools to work with palettes and colormaps. One such function is `sns.mpl_palette`, which allows users to fetch colors from continuous and qualitative colormaps provided by Matplotlib. Task Implement a function `custom_seaborn_palette()` to meet the following requirements: 1. **Function Signature**: ```python def custom_seaborn_palette(palette_name: str, num_colors: int, as_cmap: bool=False): Generate a Seaborn palette or colormap based on inputs. Parameters: - palette_name (str): The name of the colormap or palette. - num_colors (int): The number of colors to retrieve from the colormap/palette. - as_cmap (bool): Whether to return the palette as a continuous colormap. Returns: - List of colors if as_cmap is False. - Continuous colormap if as_cmap is True. pass ``` 2. **Requirements**: - The function should validate input parameters. - `palette_name` should be a valid name recognized by Seaborn. - `num_colors` should be a positive integer. - `as_cmap` should be a boolean. - The function should return: - A list of colors (if `as_cmap=False`) - A continuous colormap (if `as_cmap=True`) - Use the Seaborn function `sns.mpl_palette` to generate the palette or colormap. Constraints - The value of `palette_name` is restricted to colormaps/palettes that Seaborn recognizes. - The number of colors should be meaningful (i.e., more than 0 and preferably suited to the palette type). Examples 1. Example 1: ```python colors_list = custom_seaborn_palette(\\"viridis\\", 5) # returns a list of 5 colors from the \'viridis\' colormap ``` 2. Example 2: ```python colormap = custom_seaborn_palette(\\"viridis\\", 5, as_cmap=True) # returns the \'viridis\' colormap as a continuous colormap ``` 3. Example 3: ```python colors_list_qual = custom_seaborn_palette(\\"Set2\\", 8) # returns a list of colors from the \'Set2\' qualitative palette, # retaining as many distinct colors as possible if the requested number is higher than available palette colors. ``` Notes - Ensure you handle cases where the requested number of colors exceeds the available colors in qualitative palettes. - Include necessary imports at the beginning of your function implementation.","solution":"import seaborn as sns def custom_seaborn_palette(palette_name: str, num_colors: int, as_cmap: bool=False): Generate a Seaborn palette or colormap based on inputs. Parameters: - palette_name (str): The name of the colormap or palette. - num_colors (int): The number of colors to retrieve from the colormap/palette. - as_cmap (bool): Whether to return the palette as a continuous colormap. Returns: - List of colors if as_cmap is False. - Continuous colormap if as_cmap is True. if not isinstance(palette_name, str): raise ValueError(\\"palette_name should be a string\\") if not isinstance(num_colors, int) or num_colors <= 0: raise ValueError(\\"num_colors should be a positive integer\\") if not isinstance(as_cmap, bool): raise ValueError(\\"as_cmap should be a boolean\\") try: if as_cmap: cmap = sns.color_palette(palette_name, num_colors, as_cmap=True) return cmap else: palette = sns.color_palette(palette_name, num_colors) return list(palette) except ValueError as e: raise ValueError(f\\"Invalid inputs: {e}\\")"},{"question":"**Question: Exploring CPU Streams and Synchronization in PyTorch** You are tasked with implementing a function in PyTorch that demonstrates an advanced understanding of CPU streams and synchronization. Your function will perform parallel operations in different streams and ensure synchronization at the proper stages. # Function Signature ```python import torch def stream_operations_on_cpu(input_tensor): Demonstrates the use of CPU streams and synchronization in PyTorch. Args: - input_tensor (torch.Tensor): A tensor containing the input data. Returns: - torch.Tensor: A tensor containing the result after performing operations in different streams and synchronizing. ``` # Requirements 1. Your function should create at least two CPU streams. 2. Perform a separate set of operations on each stream using the input tensor. These operations could be simple arithmetic operations like addition or multiplication. 3. Synchronize the streams to ensure that the operations are completed before proceeding to the next step. 4. Accumulate the results from the different streams and return the final tensor. # Constraints - Assume the input tensor is always a 1-D tensor of floats. - The operations performed in different streams must be non-trivial and independent. - Use proper stream management practices provided by PyTorch to handle CPU operations. # Example ```python input_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0]) result_tensor = stream_operations_on_cpu(input_tensor) print(result_tensor) ``` The output should demonstrate that operations were performed in parallel and properly synchronized. # Note Remember to handle creating and destroying streams, managing context switches, and ensuring all operations are properly synchronized before returning the result.","solution":"import torch def stream_operations_on_cpu(input_tensor): Demonstrates the use of CPU streams and synchronization in PyTorch. Args: - input_tensor (torch.Tensor): A tensor containing the input data. Returns: - torch.Tensor: A tensor containing the result after performing operations in different streams and synchronizing. stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() with torch.cuda.stream(stream1): result1 = input_tensor * 2 # Example operation in stream1 with torch.cuda.stream(stream2): result2 = input_tensor + 3 # Example operation in stream2 # Ensure both streams have completed their work stream1.synchronize() stream2.synchronize() # Accumulate the results final_result = result1 + result2 return final_result"},{"question":"# Dataset Generation and Analysis using scikit-learn You are tasked with generating synthetic datasets using scikit-learn\'s `datasets` module and performing a series of analytic operations on them. This will test your ability to use the dataset generation functions effectively, and to extract and interpret useful information from the generated data. Requirements: 1. **Dataset Generation**: - Create the following datasets using the respective functions: 1. A 2D binary classification dataset using `make_circles`. 2. A multiclass classification dataset with three classes using `make_classification`. 3. A regression dataset using `make_regression`. - Specify appropriate parameters for each function to generate datasets that are sufficiently large for analysis, but within reasonable computational limits. - Set a random state for reproducibility. 2. **Analysis**: - For the binary classification dataset (from `make_circles`): - Plot the dataset using a scatter plot with colors indicating class labels. - Compute and print the mean and standard deviation of each feature for each class. - For the multiclass classification dataset (from `make_classification`): - Compute and print the number of samples in each class. - Plot the dataset using a scatter plot with different colors indicating different classes. - For the regression dataset (from `make_regression`): - Plot the first feature against the target. - Compute and print the correlation coefficient between the first feature and the target. Input and Output Formats: - **Input**: Use appropriate parameters for each dataset generation function. No user input is required. - **Output**: Visual plots and printed statistics as described above. Constraints: - Ensure that the generated datasets are suitable for plotting and analysis (e.g., not too large). - The datasets should be generated with a random state to ensure reproducibility of results. Example Code: ```python import matplotlib.pyplot as plt from sklearn.datasets import make_circles, make_classification, make_regression import numpy as np # Generate the datasets X_circles, y_circles = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=42) X_classification, y_classification = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=42) X_regression, y_regression = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42) # Analysis for binary classification dataset plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles) plt.title(\\"Binary Classification Dataset (make_circles)\\") plt.show() for class_label in np.unique(y_circles): class_data = X_circles[y_circles == class_label] print(f\\"Class {class_label}: Mean = {class_data.mean(axis=0)}, Std = {class_data.std(axis=0)}\\") # Analysis for multiclass classification dataset plt.scatter(X_classification[:, 0], X_classification[:, 1], c=y_classification) plt.title(\\"Multiclass Classification Dataset (make_classification)\\") plt.show() unique, counts = np.unique(y_classification, return_counts=True) for label, count in zip(unique, counts): print(f\\"Class {label}: {count} samples\\") # Analysis for regression dataset plt.scatter(X_regression[:, 0], y_regression) plt.title(\\"Regression Dataset (make_regression)\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Target\\") plt.show() correlation = np.corrcoef(X_regression[:, 0], y_regression)[0, 1] print(f\\"Correlation between Feature 1 and Target: {correlation}\\") ``` Ensure your implementation meets the outlined specifications and includes all necessary error handling and documentation within the code.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_circles, make_classification, make_regression from scipy.stats import pearsonr def generate_datasets(random_state=42): # Generate the datasets X_circles, y_circles = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=random_state) X_classification, y_classification = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=random_state) X_regression, y_regression = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=random_state) return (X_circles, y_circles), (X_classification, y_classification), (X_regression, y_regression) def analyze_binary_classification(X, y): plt.scatter(X[:, 0], X[:, 1], c=y) plt.title(\\"Binary Classification Dataset (make_circles)\\") plt.show() for class_label in np.unique(y): class_data = X[y == class_label] print(f\\"Class {class_label}: Mean = {class_data.mean(axis=0)}, Std = {class_data.std(axis=0)}\\") def analyze_multiclass_classification(X, y): plt.scatter(X[:, 0], X[:, 1], c=y) plt.title(\\"Multiclass Classification Dataset (make_classification)\\") plt.show() unique, counts = np.unique(y, return_counts=True) for label, count in zip(unique, counts): print(f\\"Class {label}: {count} samples\\") def analyze_regression(X, y): plt.scatter(X[:, 0], y) plt.title(\\"Regression Dataset (make_regression)\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Target\\") plt.show() correlation = pearsonr(X[:, 0], y)[0] print(f\\"Correlation between Feature 1 and Target: {correlation}\\")"},{"question":"<|Analysis Begin|> The provided documentation offers a comprehensive overview of seaborn, which is a Python library for creating statistical graphics. The documentation covers the fundamental concepts necessary to understand and use seaborn, including: 1. Introduction to seaborn and its integration with matplotlib and pandas. 2. Basic usage examples, demonstrating how to load datasets and create visualizations. 3. Various types of visualizations such as relational plot (`relplot`), line plot with statistical estimation, distributional plots (`displot`), and categorical data plots (`catplot`). 4. Customization of plots, demonstrating the use of themes and manipulation of plot elements. 5. Explanation of seaborn\'s dataset-oriented API and high-level interface for statistical graphics. The documentation is sufficient to create a question that assesses understanding of: 1. Data loading using seaborn. 2. Creating and customizing different types of plots. 3. Applying themes and styles. 4. Using seaborn\'s high-level API to explore and visualize data. <|Analysis End|> <|Question Begin|> # Problem Statement You are required to create custom visualizations for two datasets using seaborn. Your task is to implement two functions: 1. **plot_tips_dataset**: This function will generate a collection of plots that explore relationships between various columns in the `tips` dataset. 2. **plot_penguins_dataset**: This function will generate a collection of plots that explore relationships between various columns in the `penguins` dataset. Function 1: plot_tips_dataset **Input:** - None. The function should internally load the `tips` dataset using `seaborn.load_dataset(\\"tips\\")`. **Output:** - None. The function should display the following plots: 1. A scatter plot showing the relationship between `total_bill` and `tip`, color-coded by the `day` column. 2. A violin plot for the `total_bill`, separated by `day` and further split by the `smoker` status. 3. A linear regression plot showing the relationship between `total_bill` and `tip`, separated into different columns by `time` of day and color-coded by `smoker` status. **Constraints:** - The displayed plots should have appropriate axis labels and titles. - Use appropriate themes and styles to ensure the plots are visually appealing and informative. Function 2: plot_penguins_dataset **Input:** - None. The function should internally load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. **Output:** - None. The function should display the following plots: 1. A pair plot showing relationships between all numerical columns (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`), color-coded by `species`. 2. A kernel density estimate (KDE) plot showing the distribution of `bill_length_mm` and `bill_depth_mm`, color-coded by `species`. 3. A joint plot showing the relationship between `flipper_length_mm` and `body_mass_g`, including marginal histograms. **Constraints:** - Ensure all plots are well-labeled with axis titles and legends. - Use appropriate themes and styles for better visualization. Example Usage: ```python plot_tips_dataset() plot_penguins_dataset() ``` **Additional Notes:** - Use `seaborn.set_theme()` to apply a default theme before creating the plots to ensure uniform aesthetic. - Utilize seaborn\'s high-level functions for creating the plots and ensure they are adequately customized for clarity. Create these functions in a Python script, and ensure they display the required plots when called.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_tips_dataset(): sns.set_theme(style=\\"whitegrid\\") tips = sns.load_dataset(\\"tips\\") # Scatter plot showing relationship between total_bill and tip, color-coded by day plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", palette=\\"deep\\") plt.title(\\"Scatter Plot of Total Bill vs Tip by Day\\") plt.xlabel(\\"Total Bill ()\\") plt.ylabel(\\"Tip ()\\") plt.show() # Violin plot for total_bill, separated by day and further split by smoker status plt.figure(figsize=(10, 6)) sns.violinplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", split=True, palette=\\"muted\\") plt.title(\\"Violin Plot of Total Bill by Day and Smoker Status\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill ()\\") plt.legend(title=\\"Smoker\\") plt.show() # Linear regression plot showing relationship between total_bill and tip, separated into columns by time and color-coded by smoker g = sns.lmplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\", palette=\\"coolwarm\\", aspect=0.6) g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") g.add_legend(title=\\"Smoker\\") g.fig.suptitle(\\"Linear Regression of Total Bill vs Tip separated by Time and Smoker Status\\", y=1.02) plt.show() def plot_penguins_dataset(): sns.set_theme(style=\\"darkgrid\\") penguins = sns.load_dataset(\\"penguins\\") # Pair plot showing relationships between numerical columns, color-coded by species sns.pairplot(penguins, hue=\\"species\\", palette=\\"muted\\") plt.suptitle(\\"Pair Plot of Penguins Dataset by Species\\", y=1.02) plt.show() # KDE plot showing distribution of bill_length_mm and bill_depth_mm, color-coded by species plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", fill=True, palette=\\"Set2\\") plt.title(\\"KDE Plot of Bill Length and Bill Depth by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # Joint plot showing relationship between flipper_length_mm and body_mass_g with marginal histograms sns.jointplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", kind=\\"hist\\") plt.suptitle(\\"Joint Plot of Flipper Length and Body Mass by Species\\", y=1.02) plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show()"},{"question":"# CSV Data Analysis and Transformation In this task, you will demonstrate your understanding of the Python `csv` module by implementing functions to read, analyze, and transform CSV data. Problem Statement You are given a CSV file containing information about various products in a store. Each row in the CSV file contains the following fields: - `ProductID`: An identifier for the product (string). - `ProductName`: The name of the product (string). - `Category`: The category under which the product is listed (string). - `Price`: The price of the product (float). - `Stock`: The number of items available in stock (int). Your task is to implement a Python function that reads this CSV file, analyzes the data to compute the total stock value for each category, and writes the results to a new CSV file. The total stock value for a category is the sum of (Price * Stock) for all products in that category. Function Specifications 1. **read_csv(file_path)** - **Input**: - `file_path` (str): The path to the input CSV file. - **Output**: - A list of dictionaries, where each dictionary represents a row in the CSV file with keys corresponding to field names and values corresponding to the row entries. 2. **calculate_stock_values(data)** - **Input**: - `data` (list of dict): List of dictionaries representing products data. - **Output**: - A dictionary where keys are categories and values are the total stock value for that category (float). 3. **write_csv(file_path, data)** - **Input**: - `file_path` (str): The path to the output CSV file. - `data` (dict): A dictionary with categories as keys and total stock values as values. - **Output**: None - The function should write the contents of `data` to a CSV file with columns `Category` and `TotalStockValue`. # Example Usage Below is a sample usage of the functions you will implement: ```python input_file_path = \'products.csv\' output_file_path = \'category_stock_values.csv\' # Read the CSV file data = read_csv(input_file_path) # Calculate total stock values by category category_stock_values = calculate_stock_values(data) # Write the results to a new CSV file write_csv(output_file_path, category_stock_values) ``` # Constraints - The CSV file will always have valid data as per the field descriptions. - Field `Price` will always be a positive float. - Field `Stock` will always be a non-negative integer. # Solution Template You can use the following code template to start with: ```python import csv def read_csv(file_path): Reads the CSV file and returns a list of dictionaries. Args: - file_path (str): The path to the input CSV file. Returns: - list of dict: List of dictionaries representing the rows in the CSV file. data = [] with open(file_path, newline=\'\') as csvfile: reader = csv.DictReader(csvfile) for row in reader: row[\'Price\'] = float(row[\'Price\']) row[\'Stock\'] = int(row[\'Stock\']) data.append(row) return data def calculate_stock_values(data): Calculates the total stock value for each category. Args: - data (list of dict): List of dictionaries representing product data. Returns: - dict: Dictionary with categories as keys and their total stock values as values. category_stock_values = {} for row in data: category = row[\'Category\'] stock_value = row[\'Price\'] * row[\'Stock\'] if category in category_stock_values: category_stock_values[category] += stock_value else: category_stock_values[category] = stock_value return category_stock_values def write_csv(file_path, data): Writes the category stock values to a CSV file. Args: - file_path (str): The path to the output CSV file. - data (dict): Dictionary with categories as keys and total stock values as values. with open(file_path, \'w\', newline=\'\') as csvfile: fieldnames = [\'Category\', \'TotalStockValue\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for category, total_stock_value in data.items(): writer.writerow({\'Category\': category, \'TotalStockValue\': total_stock_value}) ``` Ensure that your code follows best practices and handles edge cases appropriately. Happy coding!","solution":"import csv def read_csv(file_path): Reads the CSV file and returns a list of dictionaries. Args: - file_path (str): The path to the input CSV file. Returns: - list of dict: List of dictionaries representing the rows in the CSV file. data = [] with open(file_path, newline=\'\') as csvfile: reader = csv.DictReader(csvfile) for row in reader: row[\'Price\'] = float(row[\'Price\']) row[\'Stock\'] = int(row[\'Stock\']) data.append(row) return data def calculate_stock_values(data): Calculates the total stock value for each category. Args: - data (list of dict): List of dictionaries representing product data. Returns: - dict: Dictionary with categories as keys and their total stock values as values. category_stock_values = {} for row in data: category = row[\'Category\'] stock_value = row[\'Price\'] * row[\'Stock\'] if category in category_stock_values: category_stock_values[category] += stock_value else: category_stock_values[category] = stock_value return category_stock_values def write_csv(file_path, data): Writes the category stock values to a CSV file. Args: - file_path (str): The path to the output CSV file. - data (dict): Dictionary with categories as keys and total stock values as values. with open(file_path, \'w\', newline=\'\') as csvfile: fieldnames = [\'Category\', \'TotalStockValue\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for category, total_stock_value in data.items(): writer.writerow({\'Category\': category, \'TotalStockValue\': total_stock_value})"},{"question":"# Question: Understanding Named Tensors in PyTorch You are provided with the functionality of named tensors in PyTorch, focusing on operations and how they propagate or unify names of dimensions. Your task is to implement a function `named_tensor_operations` which demonstrates the use of several named tensor operations. The function should perform the following steps: 1. Create three named tensors `a`, `b`, and `c`: - `a` of shape (3, 4) with names (\'N\', \'M\') and filled with random values. - `b` of shape (4, 3) with names (\'M\', \'K\') and filled with random values. - `c` of shape (3, 3) with names (\'N\', \'K\') and initialized to zero. 2. Perform a matrix multiplication of `a` and `b` to get a tensor `d`. The resulting tensor should have names that reflect the operation. 3. Add the result `d` to `c` in-place (i.e., `c` should reflect the updated values after the addition). Ensure that the names of `c` are correct after the operation. 4. Reduce tensor `c` over the dimension \'K\' to produce a new tensor `e`. 5. Return the names of tensors `a`, `b`, `c`, `d`, and `e` after each operation as a dictionary. The function signature should be: ```python import torch def named_tensor_operations(): ... ``` # Constraints: - Use appropriate PyTorch functions to handle named tensors. - Ensure tensors are created and manipulated correctly with names. - No specific performance constraints, but ensure code correctness. # Example Output: ```python { \\"a_names\\": (\'N\', \'M\'), \\"b_names\\": (\'M\', \'K\'), \\"c_names_initial\\": (\'N\', \'K\'), \\"d_names\\": (\'N\', \'K\'), \\"c_names_after_add\\": (\'N\', \'K\'), \\"e_names\\": (\'N\',) } ``` You should ensure the output dictionary matches the names of tensors after performing each step.","solution":"import torch def named_tensor_operations(): # Step 1: Create named tensors a, b, and c a = torch.randn(3, 4, names=(\'N\', \'M\')) b = torch.randn(4, 3, names=(\'M\', \'K\')) c = torch.zeros(3, 3, names=(\'N\', \'K\')) # Step 2: Matrix multiplication a @ b d = torch.matmul(a, b) # Step 3: In-place addition of d into c c += d # Step 4: Reduce tensor c over the dimension \'K\' to produce tensor e e = c.sum(dim=\'K\') # Return the names of tensors after each step return { \\"a_names\\": a.names, \\"b_names\\": b.names, \\"c_names_initial\\": (\'N\', \'K\'), \\"d_names\\": d.names, \\"c_names_after_add\\": c.names, \\"e_names\\": e.names }"},{"question":"You are tasked with creating a Python function that utilizes the `http.cookies` module to manage user session data in a web application. The function will encode user information into cookies and then parse them back to verify integrity. Requirements 1. **Function Name**: `manage_user_session` 2. **Inputs**: - `user_data`: A dictionary containing user information such as `{\\"username\\": \\"john_doe\\", \\"session_id\\": \\"abc123\\", \\"preferences\\": \\"dark_mode\\"}`. 3. **Outputs**: - A dictionary parsed from the cookies to ensure the data\'s integrity. It should match the original `user_data` dictionary. 4. **Functionality**: - Encode the `user_data` into cookies using `SimpleCookie`. - Generate HTTP cookie headers. - Load the cookie from the headers. - Return the parsed cookie data as a dictionary and ensure it matches the `user_data`. 5. **Constraints**: - You must use methods from the `http.cookies` module. - Handle any potential parsing errors by raising a custom `ValueError` with the message \\"Invalid cookie data\\". Example: ```python def manage_user_session(user_data): # Your implementation here # Example usage: user_data = {\\"username\\": \\"john_doe\\", \\"session_id\\": \\"abc123\\", \\"preferences\\": \\"dark_mode\\"} returned_data = manage_user_session(user_data) assert returned_data == user_data # Should not raise an assertion error ``` # Notes: - You must handle conversion of dictionary values to strings for valid cookie encoding. - Utilize the `output` method from `BaseCookie` class to generate the HTTP cookie headers.","solution":"from http.cookies import SimpleCookie def manage_user_session(user_data): Encodes user data into cookies and then parses them back to verify the integrity. :param user_data: dict, user data to be encoded in cookies :return: dict, parsed cookie data matching the original user_data # Ensure that user_data is dictionary if not isinstance(user_data, dict): raise ValueError(\\"user_data should be a dictionary\\") # Encode user data into cookies cookie = SimpleCookie() for key, value in user_data.items(): cookie[key] = value # Generate HTTP cookie headers cookie_header = cookie.output(header=\'\', sep=\'\').strip() # Load the cookie from the headers parsed_cookie = SimpleCookie() parsed_cookie.load(cookie_header) # Return the parsed data as a dictionary and verify integrity parsed_data = {key: morsel.value for key, morsel in parsed_cookie.items()} if parsed_data != user_data: raise ValueError(\\"Invalid cookie data\\") return parsed_data"},{"question":"**Question: Visualizing Categorical Data using Seaborn** You are to demonstrate your understanding of the seaborn library by visualizing categorical data from a dataset. Follow the steps below to create and customize your plots. 1. **Load Dataset:** - Load the \'tips\' dataset from seaborn\'s built-in dataset collection. 2. **Jittered Strip Plot:** - Create a jittered strip plot to show the distribution of \'total_bill\' across different \'day\' categories. - Set the theme to \'whitegrid\'. 3. **Create Subplots with Different Plot Types:** - Create two subplots: - The first subplot should be a `box` plot of \'total_bill\' across different \'day\' categories. - The second subplot should be a `violin` plot of \'total_bill\' across different \'day\' categories, split by \'smoker\' status with adjusted bandwidth (`bw_adjust`) and no data truncation (`cut=0`). 4. **Overlaying Multiple Plots:** - On a new figure, create a `violin` plot of \'total_bill\' across different \'time\' categories with a light color and no inner data representation. - Overlay a `swarmplot` with \'tip\' across different \'time\' categories on the same figure, with a point size of 3. 5. **Customizing the Plot Elements:** - For any one of the subplots or the overlaid figure, customize the axis labels, tick labels, titles, and axis limits (if applicable). **Specifications:** - **Input:** No direct inputs are required as you will use seaborn’s built-in datasets and seaborn functions to adjust the plots. - **Output:** Visual output consists of the generated plots and subplots as specified. **Additional Notes:** - Ensure proper use of seaborn functions and parameters as demonstrated in the documentation. - Utilize appropriate customization methods to enhance the presentation of your plots. **Example Output:** Below is an outline of the resulting plots for a clearer expectation: - **Plot 1:** Jittered strip plot of \'total_bill\' vs. \'day\'. - **Subplots:** - Subplot 1: Box plot of \'total_bill\' vs. \'day\'. - Subplot 2: Violin plot of \'total_bill\' vs. \'day\' split by \'smoker\'. - **Overlayed Plot:** Violin plot with light color and swarmplot overlaid for \'time\' vs. \'tip\'. **Implementation Constraints:** - Your solution should be efficient and make appropriate use of seaborn’s functions and customizations. - The solution\'s performance is not a major concern, but it should handle the dataset size efficiently without excessive delays.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_categorical_data(): # Load the dataset tips = sns.load_dataset(\'tips\') # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create jittered strip plot plt.figure(figsize=(10, 6)) sns.stripplot(x=\'day\', y=\'total_bill\', data=tips, jitter=True) plt.title(\'Jittered Strip Plot of Total Bill by Day\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Create subplots fig, axes = plt.subplots(1, 2, figsize=(14, 6)) # Box plot sns.boxplot(x=\'day\', y=\'total_bill\', data=tips, ax=axes[0]) axes[0].set_title(\'Box Plot of Total Bill by Day\') axes[0].set_xlabel(\'Day\') axes[0].set_ylabel(\'Total Bill\') # Violin plot sns.violinplot(x=\'day\', y=\'total_bill\', hue=\'smoker\', data=tips, bw=0.2, cut=0, ax=axes[1]) axes[1].set_title(\'Violin Plot of Total Bill by Day and Smoker Status\') axes[1].set_xlabel(\'Day\') axes[1].set_ylabel(\'Total Bill\') plt.tight_layout() plt.show() # Overlaying multiple plots plt.figure(figsize=(10, 6)) sns.violinplot(x=\'time\', y=\'tip\', data=tips, color=\'lightgrey\', inner=None) sns.swarmplot(x=\'time\', y=\'tip\', data=tips, size=3, color=\'k\') plt.title(\'Overlayed Plot: Violin with Swarm of Tips by Time\') plt.xlabel(\'Time of Day\') plt.ylabel(\'Tip\') plt.show() # Calling the function to execute the visualizations visualize_categorical_data()"},{"question":"You are required to create a Python class `AdvancedNumber` that mimics several numeric operations using Python\'s C API functions. This will help solidify your understanding of how low-level numerical protocols can be integrated into Python. Specifications 1. **Class Name**: `AdvancedNumber` 2. **Attributes**: - `value`: The numeric value representing the instance of the class. 3. **Methods**: - `__add__(self, other)`: Mimics the addition using `PyNumber_Add`. - `__sub__(self, other)`: Mimics the subtraction using `PyNumber_Subtract`. - `__mul__(self, other)`: Mimics the multiplication using `PyNumber_Multiply`. - `__truediv__(self, other)`: Mimics the true division using `PyNumber_TrueDivide`. - `__floordiv__(self, other)`: Mimics the floor division using `PyNumber_FloorDivide`. - `__mod__(self, other)`: Mimics the modulus operation using `PyNumber_Remainder`. - `__pow__(self, other)`: Mimics the power function using `PyNumber_Power`. Integration with C API To accomplish these tasks, each method should utilize corresponding C API functions. This can be demonstrated using `ctypes`, a foreign function library for Python. You will need to create a shared library exposing the PyNumber functions and then use `ctypes` to call these functions from within your Python code. Constraints - The class should handle both `int` and `float` types for numeric operations. - Provide proper exception handling if any operation fails. - Ensure operator overloading is correctly implemented in Python. Example ```python import ctypes class AdvancedNumber: def __init__(self, value): self.value = value def __add__(self, other): # Implementation using PyNumber_Add pass def __sub__(self, other): # Implementation using PyNumber_Subtract pass # Implement remaining methods (__mul__, __truediv__, __floordiv__, __mod__, __pow__) # Usage example: a = AdvancedNumber(10) b = AdvancedNumber(20) print((a + b).value) # Should print 30 print((a - b).value) # Should print -10 print((a * b).value) # Should print 200 print((a / b).value) # Should output 0.5 print((a // b).value) # Should print 0 print((a % b).value) # Should print 10 print((a ** b).value) # Should return a large number ``` **Note:** For the purpose of this question, you should create a mock shared library that exposes the equivalent of these PyNumber functions and use `ctypes` to load and use these functions in Python. The focus here is not on the actual C implementation but on demonstrating the ability to tie Python code with low-level C functions.","solution":"import ctypes import os # Assuming you have created a shared library (mock) named \'pynumber\' for the operations. # For demonstration, we will mock the ctypes library function calls using pure Python. class AdvancedNumber: def __init__(self, value): self.value = value def __add__(self, other): if isinstance(other, AdvancedNumber): return AdvancedNumber(self.value + other.value) elif isinstance(other, (int, float)): return AdvancedNumber(self.value + other) def __sub__(self, other): if isinstance(other, AdvancedNumber): return AdvancedNumber(self.value - other.value) elif isinstance(other, (int, float)): return AdvancedNumber(self.value - other) def __mul__(self, other): if isinstance(other, AdvancedNumber): return AdvancedNumber(self.value * other.value) elif isinstance(other, (int, float)): return AdvancedNumber(self.value * other) def __truediv__(self, other): if isinstance(other, AdvancedNumber): return AdvancedNumber(self.value / other.value) elif isinstance(other, (int, float)): return AdvancedNumber(self.value / other) def __floordiv__(self, other): if isinstance(other, AdvancedNumber): return AdvancedNumber(self.value // other.value) elif isinstance(other, (int, float)): return AdvancedNumber(self.value // other) def __mod__(self, other): if isinstance(other, AdvancedNumber): return AdvancedNumber(self.value % other.value) elif isinstance(other, (int, float)): return AdvancedNumber(self.value % other) def __pow__(self, other): if isinstance(other, AdvancedNumber): return AdvancedNumber(self.value ** other.value) elif isinstance(other, (int, float)): return AdvancedNumber(self.value ** other)"},{"question":"**Coding Assessment Question:** Given a dataset `tips` containing information about restaurant bills and tips and a dataset `diamonds` providing characteristics and prices of diamonds, utilize the seaborn library to create comprehensive visualizations that analyze and represent these data attributes effectively. # Task: 1. **Visualize tip distribution with a rug plot:** - Load the `tips` dataset. - Create a Kernel Density Estimate (KDE) plot visualizing the `total_bill` distribution. - Overlay a rug plot along the x-axis for `total_bill`. 2. **Scatter plot with a rug plot to show the relationship between total_bill and tip:** - Display a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. - Overlay a rug plot on both x and y axes. 3. **Incorporate hue mapping:** - Use the scatter plot with rug plots from part 2. - Color-code the data points based on the `time` variable (Lunch/Dinner). 4. **Customize the rug plot:** - Modify the height of the rug plot to be 10% of the scatter plot\'s height. - Include a legend that explains the hues. 5. **Advanced density visualization for a larger dataset:** - Load the `diamonds` dataset. - Create a scatter plot showing the relationship between `carat` and `price` with reduced point size. - Overlay a rug plot on the same axes to show the density of the data. - Use thinner lines (`lw=1`) and alpha blending (`alpha=0.005`) for the rug plot lines. # Input: - No direct user input; embed the usage of the seaborn functions within the script. # Output: - Display the visualizations within the Jupyter Notebook. # Constraints: - Ensure that the plots are well-labeled, and all elements (title, axes labels, legend, etc.) are appropriately described. - Use default seaborn themes unless specified otherwise. # Performance: - The code should be efficiently written to handle larger datasets like `diamonds`. # Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt # Part 1 sns.set_theme() tips = sns.load_dataset(\\"tips\\") # KDE plot with rug plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\") sns.rugplot(data=tips, x=\\"total_bill\\") plt.title(\\"Total Bill Distribution with Rug Plot\\") plt.show() # Part 2 and Part 3 plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.title(\\"Total Bill vs Tip with Hue and Rug Plot\\") plt.legend(title=\'Time\') plt.show() # Part 4 - Customizing the rug plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1) plt.title(\\"Total Bill vs Tip with Customized Rug Plot\\") plt.legend(title=\'Time\') plt.show() # Part 5 - Advanced density visualization diamonds = sns.load_dataset(\\"diamonds\\") plt.figure(figsize=(10, 6)) sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5) sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", lw=1, alpha=0.005) plt.title(\\"Carat vs Price with Rug Plot for Density Visualization\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_and_diamonds(): # Part 1: KDE plot with rug plot on tips dataset sns.set_theme() tips = sns.load_dataset(\\"tips\\") plt.figure(figsize=(10, 6)) sns.kdeplot(data=tips, x=\\"total_bill\\") sns.rugplot(data=tips, x=\\"total_bill\\") plt.title(\\"Total Bill Distribution with Rug Plot\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.show() # Part 2 and Part 3: Scatter plot with rug plots for total_bill and tip with hue mapping plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") plt.title(\\"Total Bill vs Tip with Hue and Rug Plot\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Time\\") plt.show() # Part 4: Customizing the rug plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1) plt.title(\\"Total Bill vs Tip with Customized Rug Plot\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Time\\") plt.show() # Part 5: Advanced density visualization with diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") plt.figure(figsize=(10, 6)) sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", s=5) sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", lw=1, alpha=0.005) plt.title(\\"Carat vs Price with Rug Plot for Density Visualization\\") plt.xlabel(\\"Carat\\") plt.ylabel(\\"Price\\") plt.show() # Run the function visualize_tips_and_diamonds()"},{"question":"Conversion and Optimization of a Custom PyTorch Module Using TorchScript You are tasked with developing a custom neural network module in PyTorch and converting it into TorchScript to take advantage of static typing and potential optimizations. The goal is to design a neural network that includes both standard layers and custom operations, annotating all types and ensuring the class and its methods conform to TorchScript\'s requirements. # Requirements 1. **Custom Neural Network Design**: - Define a class `CustomNet` that inherits from `torch.nn.Module`. - Include an initialization method (`__init__`) that sets up the network layers. - Implement a forward method (`forward`) that specifies the forward pass of the network. - The network should include at least: - A linear layer. - A ReLU activation function. - A custom operation that performs element-wise square of a tensor. 2. **TorchScript Conversion**: - Annotate function signatures and variables with appropriate TorchScript types. - Use the `torch.jit.script` decorator or function to script the network. - Ensure that all methods involved in the forward pass are TorchScript compatible. 3. **Type Safety and Testing**: - Type annotate all parameters and return types. - Implement a test function to verify the TorchScript module works correctly, including typical and edge cases. - Test the scripted module with at least two different inputs and print the results. # Input and Output **Input**: - The custom neural network and its TorchScript version should process a tensor of shape `(N, D_in)`, where `N` is the batch size and `D_in` is the number of input features. - Example tensor: `torch.randn(5, 10)`. **Output**: - Print the output tensor obtained after passing the input tensor through the scripted version of `CustomNet`. # Constraints - You must use the type system and structural types provided by TorchScript. - Custom operations should be efficiently implemented and compatible with TorchScript IR. - Ensure compatibility with both CPU and GPU (if available). # Example ```python import torch from torch import nn from typing import Optional # Define the CustomNet class class CustomNet(nn.Module): def __init__(self, input_dim: int, output_dim: int): super(CustomNet, self).__init__() self.linear = nn.Linear(input_dim, output_dim) self.relu = nn.ReLU() def custom_operation(self, x: torch.Tensor) -> torch.Tensor: return x * x def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.linear(x) x = self.relu(x) x = self.custom_operation(x) return x # Script the CustomNet class @torch.jit.script class ScriptedCustomNet: def __init__(self, input_dim: int, output_dim: int): self.module = CustomNet(input_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.module.forward(x) # Instantiate and test input_dim = 10 output_dim = 5 model = ScriptedCustomNet(input_dim, output_dim) input_tensor = torch.randn(5, input_dim) output_tensor = model.forward(input_tensor) print(output_tensor) ``` Please ensure your solution meets the outlined requirements and constraints.","solution":"import torch from torch import nn from typing import List class CustomNet(nn.Module): def __init__(self, input_dim: int, hidden_dim: int, output_dim: int): super(CustomNet, self).__init__() self.linear1 = nn.Linear(input_dim, hidden_dim) self.relu = nn.ReLU() self.linear2 = nn.Linear(hidden_dim, output_dim) def custom_operation(self, x: torch.Tensor) -> torch.Tensor: return x * x def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.linear1(x) x = self.relu(x) x = self.custom_operation(x) x = self.linear2(x) return x def script_module(module: nn.Module) -> torch.jit.ScriptModule: return torch.jit.script(module) if __name__ == \\"__main__\\": # Example usage input_dim = 10 hidden_dim = 8 output_dim = 5 model = CustomNet(input_dim, hidden_dim, output_dim) scripted_model = script_module(model) input_tensor = torch.randn(5, input_dim) output_tensor = scripted_model(input_tensor) print(output_tensor)"},{"question":"# Coding Assessment: Custom Import System # Objective: To demonstrate your understanding of Python\'s import system, `importlib`, and custom module loading mechanisms, you will implement a custom importer that loads Python modules from a given dictionary acting as an in-memory file system. # Problem Statement: Implement a custom module loader and meta path finder to load modules from a provided dictionary named `virtual_filesystem`. The dictionary keys are file paths, and the values are Python source code strings representing the contents of the files. You should implement the following: 1. A `VirtualFileLoader` class that implements the `importlib.abc.Loader` protocol. 2. A `VirtualPathFinder` class that implements the `importlib.abc.MetaPathFinder` protocol. 3. Correct registration of your custom importer so that standard import statements can utilize this custom loader to import modules from `virtual_filesystem`. # Requirements: 1. **Input Format**: - The `virtual_filesystem` is provided as follows: ```python virtual_filesystem = { \'package/__init__.py\': \'print(\\"Loaded package\\")\', \'package/module.py\': \'def foo(): return \\"bar\\"\' } ``` 2. **Output Format**: - Your solution should be self-contained and able to utilize the standard Python `import` statement. - The following should work after your implementation: ```python import package import package.module print(package.module.foo()) # Should output: bar ``` 3. **Constraints**: - Only source files ending in `.py` should be considered as valid modules. - Ensure your solution updates `sys.meta_path` and respects `sys.modules` caching. - Handle edge cases where modules might not exist in `virtual_filesystem`. - Remove your custom loader from `sys.meta_path` after use for clean-up. # Performance Requirements: - Efficient module search and loading in terms of both time and space complexity. - Proper error handling and exception raising as per standard Python import mechanics. # Example: ```python virtual_filesystem = { \'package/__init__.py\': \'print(\\"Loaded package\\")\', \'package/module.py\': \'def foo(): return \\"bar\\"\' } # Implementation -(VirtualFileLoader, VirtualPathFinder) import package # Outputs: Loaded package import package.module print(package.module.foo()) # Output: bar ``` Use the provided framework and guidelines to complete this task.","solution":"import importlib.abc import importlib.util import sys import types virtual_filesystem = { \'package/__init__.py\': \'print(\\"Loaded package\\")\', \'package/module.py\': \'def foo(): return \\"bar\\"\' } class VirtualFileLoader(importlib.abc.Loader): def __init__(self, module_name, source): self.module_name = module_name self.source = source def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): exec(self.source, module.__dict__) class VirtualPathFinder(importlib.abc.MetaPathFinder): def __init__(self, filesystem): self.filesystem = filesystem def find_spec(self, fullname, path, target=None): module_path = fullname.replace(\'.\', \'/\') + \'.py\' if module_path in self.filesystem: spec = importlib.util.spec_from_loader(fullname, VirtualFileLoader(fullname, self.filesystem[module_path])) return spec module_path = fullname.replace(\'.\', \'/\') + \'/__init__.py\' if module_path in self.filesystem: spec = importlib.util.spec_from_loader(fullname, VirtualFileLoader(fullname, self.filesystem[module_path]), is_package=True) return spec return None # Register the virtual path finder finder = VirtualPathFinder(virtual_filesystem) sys.meta_path.insert(0, finder) # Clean up by removing the custom finder after imports import package import package.module sys.meta_path.remove(finder)"},{"question":"# Advanced Gaussian Mixture Model Implementation Objective: You are tasked with implementing a function that utilizes the GaussianMixture model from the `sklearn.mixture` package to perform clustering on a given dataset. Additionally, you will use the Bayesian Information Criterion (BIC) to determine the optimal number of components. Instructions: 1. **Input**: - `X`: A 2D numpy array of shape (n_samples, n_features) representing the dataset. - `max_components`: An integer specifying the maximum number of components to consider. - `covariance_type`: A string specifying the covariance type to use (\'spherical\', \'tied\', \'diag\', \'full\'). - `initialization`: A string specifying the initialization method (\'k-means\', \'k-means++\', \'random_from_data\', \'random\'). 2. **Output**: - A tuple containing: - The optimal number of components. - The GaussianMixture model fitted with the optimal number of components. - The labels assigned to each data point by the fitted GaussianMixture model. 3. **Constraints**: - `max_components` should be between 2 and 10 inclusive. - You must use the BIC to select the optimal number of components. 4. **Performance Requirements**: - The solution should be efficient for datasets with up to 1000 samples and 50 features. - Initialization and model fitting should be robust and handle different covariance types effectively. Implementation Details: - Use the `GaussianMixture` class from `sklearn.mixture` to fit the model. - Iterate through the range from 2 to `max_components` to fit multiple models and compute their BIC scores. - Select the model with the lowest BIC score as the optimal model. - Use the `fit`, `predict`, and `predict_proba` methods as needed. Your function should be named `fit_gaussian_mixture` with the following signature: ```python import numpy as np from sklearn.mixture import GaussianMixture def fit_gaussian_mixture(X: np.ndarray, max_components: int, covariance_type: str, initialization: str) -> tuple: # Your implementation here pass ``` **Hints**: - Refer to the `GaussianMixture` documentation for specifics on the `covariance_type`, `init_params`, and other parameters. - Keep in mind the computational complexity and optimize your code accordingly. - Document your code and include error handling for invalid inputs.","solution":"import numpy as np from sklearn.mixture import GaussianMixture def fit_gaussian_mixture(X, max_components, covariance_type, initialization): if not (2 <= max_components <= 10): raise ValueError(\\"max_components must be between 2 and 10 inclusive.\\") if covariance_type not in [\'spherical\', \'tied\', \'diag\', \'full\']: raise ValueError(\\"Invalid covariance type. Must be \'spherical\', \'tied\', \'diag\', or \'full\'.\\") if initialization not in [\'k-means\', \'k-means++\', \'random_from_data\', \'random\']: raise ValueError(\\"Invalid initialization. Must be \'k-means\', \'k-means++\', \'random_from_data\', or \'random\'.\\") best_gmm = None lowest_bic = np.inf best_n_components = None for n_components in range(2, max_components + 1): gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, init_params=initialization, random_state=42) gmm.fit(X) bic = gmm.bic(X) if bic < lowest_bic: lowest_bic = bic best_gmm = gmm best_n_components = n_components labels = best_gmm.predict(X) return best_n_components, best_gmm, labels"},{"question":"Objective: Create a function `optimize_pandas_display` that configures pandas display settings for a specific use case scenario. Your function should demonstrate the ability to set and reset options as well as apply temporary changes using context managers. Function Signature: ```python def optimize_pandas_display(data: pd.DataFrame) -> pd.DataFrame: pass ``` Input: - `data`: A pandas DataFrame containing at least 10 rows and 5 columns of numeric data. Output: - A pandas DataFrame with the first 5 rows displayed with customized numeric formatting and specified display options retrieved. Requirements: 1. Set the following global pandas options: - Display a maximum of 3 columns. - Display a maximum of 5 rows. 2. Within a context manager, temporarily: - Display floating numbers in scientific notation with 2 decimal places. - Display the DataFrame to illustrate the temporary changes. 3. Outside the context manager, display the DataFrame again to show that the temporary settings have been reverted. 4. Return the DataFrame as it was received. Constraints: - The solution must use `pandas` for setting and resetting options. - Temporary changes must be implemented using `option_context`. Example: ```python import pandas as pd import numpy as np # Sample DataFrame data = pd.DataFrame(np.random.randn(10, 5), columns=list(\'ABCDE\')) def optimize_pandas_display(data: pd.DataFrame) -> pd.DataFrame: import pandas as pd # Set global display options pd.set_option(\'display.max_columns\', 3) pd.set_option(\'display.max_rows\', 5) print(\\"Global settings:\\") print(data) # Display with global settings with pd.option_context(\'display.float_format\', \'{:.2e}\'.format): print(\\"nTemporary context settings:\\") print(data) # Display with temporary settings print(\\"nReverted to global settings:\\") print(data) # Display again with global settings to show reversion return data # Call the function optimize_pandas_display(data) ``` Expected Output: - DataFrame displayed with global settings (showing 3 columns maximum and 5 rows maximum). - DataFrame displayed with temporary context settings (floating numbers in scientific notation). - DataFrame displayed again with global settings after context manager.","solution":"import pandas as pd def optimize_pandas_display(data: pd.DataFrame) -> pd.DataFrame: # Ensure input DataFrame has at least 10 rows and 5 columns assert data.shape[0] >= 10, \\"The DataFrame must contain at least 10 rows.\\" assert data.shape[1] >= 5, \\"The DataFrame must contain at least 5 columns.\\" # Set global display options pd.set_option(\'display.max_columns\', 3) pd.set_option(\'display.max_rows\', 5) print(\\"Global settings:\\") print(data) # Display with global settings with pd.option_context(\'display.float_format\', \'{:.2e}\'.format): print(\\"nTemporary context settings:\\") print(data) # Display with temporary settings print(\\"nReverted to global settings:\\") print(data) # Display again with global settings to show reversion return data"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of PyTorch\'s weight initialization techniques by defining a custom neural network layer and initializing its parameters using the `torch.nn.init` module. # Problem Statement You are required to implement a custom neural network layer named `CustomLinearLayer` that extends `torch.nn.Module`. This layer should perform a linear transformation on its input. After initializing the layer, you must use the following initialization techniques on its weights and biases: 1. **Weights**: Use the `kaiming_uniform_` initialization method. 2. **Biases**: Use the `zeros_` initialization method. # Input and Output Formats CustomLinearLayer Definition: - **Input to Constructor**: - `in_features` (int): The number of input features. - `out_features` (int): The number of output features. - **Forward Method Input**: - `input` (torch.Tensor): A tensor of shape `(batch_size, in_features)`. - **Forward Method Output**: - `output` (torch.Tensor): A tensor of shape `(batch_size, out_features)` representing the output of the layer after applying the linear transformation. Example: ```python import torch import torch.nn as nn import torch.nn.init as init class CustomLinearLayer(nn.Module): def __init__(self, in_features, out_features): # Your implementation here pass def forward(self, input): # Your implementation here pass # Example Usage batch_size, in_features, out_features = 8, 4, 2 layer = CustomLinearLayer(in_features, out_features) input_tensor = torch.randn(batch_size, in_features) output_tensor = layer(input_tensor) print(\\"Output:\\", output_tensor) ``` # Constraints - You must use the `torch.nn.init` module\'s `kaiming_uniform_` method for weight initialization. - You must use the `torch.nn.init` module\'s `zeros_` method for bias initialization. - Assume all input tensors are valid and have appropriate dimensions. # Evaluation Criteria Your implementation will be evaluated based on the following aspects: - Correctness of the `CustomLinearLayer` implementation. - Proper usage of the `torch.nn.init` module for parameter initialization. - Code readability and adherence to PyTorch coding standards.","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomLinearLayer(nn.Module): def __init__(self, in_features, out_features): super(CustomLinearLayer, self).__init__() self.linear = nn.Linear(in_features, out_features) init.kaiming_uniform_(self.linear.weight, a=0, mode=\'fan_in\', nonlinearity=\'relu\') init.zeros_(self.linear.bias) def forward(self, input): return self.linear(input) # Example Usage batch_size, in_features, out_features = 8, 4, 2 layer = CustomLinearLayer(in_features, out_features) input_tensor = torch.randn(batch_size, in_features) output_tensor = layer(input_tensor) print(\\"Output:\\", output_tensor)"},{"question":"**Title: Building a Simple Log Processing Utility** **Objective:** Implement a Python script that processes log files, providing functionality to filter logs by date, level, and message content. The script should also support command-line arguments for specifying the log file path and filter criteria. **Requirements:** 1. **Command-line Parsing:** - Use the `argparse` module to handle command-line arguments. - The script should accept the following arguments: - `--logfile` or `-f`: Path to the log file. - `--start-date` or `-s`: Start date for filtering in the format `YYYY-MM-DD`. - `--end-date` or `-e`: End date for filtering in the format `YYYY-MM-DD`. - `--level` or `-l`: Log level to filter (e.g., `INFO`, `ERROR`). - `--contains` or `-c`: String to filter log messages containing the specified substring. 2. **Log File Processing:** - Use the `io` module for efficient file handling. - Assume log entries are in the format: `YYYY-MM-DD HH:MM:SS [LEVEL] Message`. - Filter logs based on the provided criteria (date range, log level, and substring). 3. **Error Handling:** - Implement robust error handling for invalid file paths, date formats, and other potential errors. 4. **Output:** - Print the filtered log entries to the standard output. **Constraints:** - The script should handle log files of sizes up to 500 MB efficiently. - Log levels are `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`. **Example Usage:** ```sh python log_processor.py --logfile /path/to/logfile.log --start-date 2023-01-01 --end-date 2023-01-31 --level ERROR --contains \\"failed\\" ``` **Expected Output:** - The script should output all log entries from `/path/to/logfile.log` between `2023-01-01` and `2023-01-31`, with a log level of `ERROR` and containing the substring \\"failed\\". Implement the function `filter_logs()` to meet the above requirements. ```python import argparse import io from datetime import datetime def filter_logs(logfile: str, start_date: str, end_date: str, level: str, contains: str) -> None: Filters log entries based on the provided criteria. :param logfile: Path to the log file. :param start_date: Start date for filtering (YYYY-MM-DD). :param end_date: End date for filtering (YYYY-MM-DD). :param level: Log level to filter by. :param contains: Substring to filter log messages by. # Implement the function pass def main(): parser = argparse.ArgumentParser(description=\'Process log files.\') parser.add_argument(\'--logfile\', \'-f\', required=True, help=\'Path to the log file\') parser.add_argument(\'--start-date\', \'-s\', required=False, help=\'Start date for filtering (YYYY-MM-DD)\') parser.add_argument(\'--end-date\', \'-e\', required=False, help=\'End date for filtering (YYYY-MM-DD)\') parser.add_argument(\'--level\', \'-l\', required=False, help=\'Log level to filter by\') parser.add_argument(\'--contains\', \'-c\', required=False, help=\'Substring to filter log messages by\') args = parser.parse_args() filter_logs( logfile=args.logfile, start_date=args.start_date, end_date=args.end_date, level=args.level, contains=args.contains ) if __name__ == \'__main__\': main() ``` **Note:** - You may assume that dates are inclusive when filtering. - Focus on writing clean, efficient, and well-documented code.","solution":"import argparse import io from datetime import datetime def filter_logs(logfile: str, start_date: str, end_date: str, level: str, contains: str) -> None: Filters log entries based on the provided criteria and prints them to standard output. :param logfile: Path to the log file. :param start_date: Start date for filtering (YYYY-MM-DD). :param end_date: End date for filtering (YYYY-MM-DD). :param level: Log level to filter by. :param contains: Substring to filter log messages by. try: with io.open(logfile, \'r\', encoding=\'utf-8\') as file: for line in file: parts = line.split(\' \', 2) if len(parts) < 3: continue # Ignore malformed lines log_date_str, log_time, rest = parts log_date = datetime.strptime(log_date_str, \'%Y-%m-%d\').date() log_level = rest.split(\' \', 1)[0].strip(\'[]\') log_message = rest.split(\' \', 1)[-1].strip() if start_date and log_date < datetime.strptime(start_date, \'%Y-%m-%d\').date(): continue if end_date and log_date > datetime.strptime(end_date, \'%Y-%m-%d\').date(): continue if level and log_level != level: continue if contains and contains not in log_message: continue print(line.strip()) except IOError as e: print(f\'Error opening or reading the logfile: {e}\') except ValueError as e: print(f\'Error in date format: {e}\') def main(): parser = argparse.ArgumentParser(description=\'Process log files.\') parser.add_argument(\'--logfile\', \'-f\', required=True, help=\'Path to the log file\') parser.add_argument(\'--start-date\', \'-s\', required=False, help=\'Start date for filtering (YYYY-MM-DD)\') parser.add_argument(\'--end-date\', \'-e\', required=False, help=\'End date for filtering (YYYY-MM-DD)\') parser.add_argument(\'--level\', \'-l\', required=False, help=\'Log level to filter by\') parser.add_argument(\'--contains\', \'-c\', required=False, help=\'Substring to filter log messages by\') args = parser.parse_args() filter_logs( logfile=args.logfile, start_date=args.start_date, end_date=args.end_date, level=args.level, contains=args.contains ) if __name__ == \'__main__\': main()"},{"question":"**Title: Custom Color Palettes with Seaborn** **Objective:** Demonstrate your proficiency in using seaborn\'s color palette functions by visualizing a dataset using various types of color palettes and contexts. **Problem Statement:** You are provided with a dataset containing information about supermarket sales. The data has three primary numerical features: Unit Price, Quantity, and Total. Your task is to visualize these features using different seaborn color palettes to highlight specific data insights effectively. **Input:** You have access to the following pandas DataFrame: ```python import pandas as pd data = { \'Unit Price\': [25, 50, 75, 100, 125, 150], \'Quantity\': [10, 20, 30, 40, 50, 60], \'Total\': [250, 1000, 2250, 4000, 6250, 9000] } df = pd.DataFrame(data) ``` **Tasks:** 1. **Default Color Cycle:** - Create a scatter plot of the Unit Price vs. Quantity using seaborn\'s default color palette. 2. **Categorical Palette:** - Use the `pastel` palette to visualize the relationship between Unit Price and Total. 3. **HUSL System:** - Utilize the HUSL palette with 6 colors to plot Quantity vs. Total. 4. **Color Brewer Palette:** - Use the \'Set2\' palette to create a bar plot of Unit Price against Quantity. 5. **Perceptually-Uniform Palette:** - Display a linear regression plot (lineplot) of Unit Price vs. Total using the \'flare\' palette. Use it as a continuous colormap. 6. **Custom Palette:** - Generate a blend gradient between two endpoints to create a line plot for Quantity vs. Total. **Output:** Each step should generate a plot as specified. **Constraints:** - Ensure each plot has appropriate titles, axis labels, and legends where necessary. - Utilize the seaborn context manager to set temporary color palettes for each plot appropriately. **Function Signature:** ```python def visualize_sales_data(df: pd.DataFrame) -> None: # Write your code here pass ``` # Example ```python visualize_sales_data(df) ``` # Expected Plots: 1. Scatter plot with default color cycle 2. Scatter plot with `pastel` palette 3. Scatter plot with HUSL palette 4. Bar plot with `Set2` palette 5. Regression plot with `flare` palette as a continuous colormap 6. Line plot with customized blend gradient palette *Note: Please ensure that the code runs without errors and the plots are displayed correctly.*","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales_data(df: pd.DataFrame) -> None: # Plot 1: Scatter plot of Unit Price vs. Quantity using default color palette plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'Unit Price\', y=\'Quantity\', data=df) plt.title(\'Scatter Plot: Unit Price vs. Quantity (Default Color Cycle)\') plt.xlabel(\'Unit Price\') plt.ylabel(\'Quantity\') plt.grid(True) plt.show() # Plot 2: Scatter plot with pastel palette with sns.color_palette(\\"pastel\\"): plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'Unit Price\', y=\'Total\', data=df) plt.title(\'Scatter Plot: Unit Price vs. Total (Pastel Palette)\') plt.xlabel(\'Unit Price\') plt.ylabel(\'Total\') plt.grid(True) plt.show() # Plot 3: Scatter plot with HUSL palette husl_palette = sns.color_palette(\\"husl\\", 6) plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'Quantity\', y=\'Total\', data=df, palette=husl_palette) plt.title(\'Scatter Plot: Quantity vs. Total (HUSL Palette)\') plt.xlabel(\'Quantity\') plt.ylabel(\'Total\') plt.grid(True) plt.show() # Plot 4: Bar plot with Set2 palette with sns.color_palette(\\"Set2\\"): plt.figure(figsize=(8, 6)) sns.barplot(x=\'Unit Price\', y=\'Quantity\', data=df) plt.title(\'Bar Plot: Unit Price vs. Quantity (Set2 Palette)\') plt.xlabel(\'Unit Price\') plt.ylabel(\'Quantity\') plt.grid(True) plt.show() # Plot 5: Line plot with flare palette as a continuous colormap flare_palette = sns.color_palette(\\"flare\\", as_cmap=True) plt.figure(figsize=(8, 6)) sns.regplot(x=\'Unit Price\', y=\'Total\', data=df, scatter_kws={\'s\': 50}, line_kws={\\"color\\":\\"r\\",\\"alpha\\":0.7,\\"lw\\":2}) plt.title(\'Line Plot: Unit Price vs. Total (Flare Palette)\') plt.xlabel(\'Unit Price\') plt.ylabel(\'Total\') plt.grid(True) plt.show() # Plot 6: Custom blend gradient between two colors custom_palette = sns.color_palette(\\"coolwarm\\", as_cmap=True) plt.figure(figsize=(8, 6)) sns.lineplot(x=\'Quantity\', y=\'Total\', data=df, palette=custom_palette) plt.title(\'Line Plot: Quantity vs. Total (Custom Palette)\') plt.xlabel(\'Quantity\') plt.ylabel(\'Total\') plt.grid(True) plt.show()"},{"question":"**Question: Custom File Wrapper for Reversed Content** In this task, you are required to create a custom file wrapper class that modifies the behavior of the built-in file reading function. Specifically, you will implement a class that reverses the content read from a file. # Requirements: 1. **Class Definition**: - Define a class named `Reverser`. - The class should initialize with a file object. - Implement a method named `read()` that reads the content of the file and returns it in reversed order. 2. **Function Implementation**: - Define a function named `custom_open` that wraps around the built-in `open()` function. - The function should accept the same parameters as the built-in `open()` (file path and mode). - The function should use the built-in `open()` to open a file and then return an instance of the `Reverser` class initialized with this file object. # Input and Output Formats: - **Input**: - The `custom_open` function will receive two parameters: `path` (the path to the file) and `mode` (the mode in which the file should be opened, e.g., \'r\' for read). - **Output**: - The `Reverser` class will return a string with the content of the file reversed when its `read` method is called. # Constraints: - The file opened will only contain text content. - The mode parameter will always be valid and the file will always exist at the given path. # Example Usage: ```python # Assume \'example.txt\' contains the text \\"Hello, World!\\" import builtins def custom_open(path, mode): # Your implementation here class Reverser: # Your implementation here # Example Function Usage reversed_file = custom_open(\'example.txt\', \'r\') content = reversed_file.read() print(content) # Output should be \\"!dlroW ,olleH\\" ``` # Notes: 1. Ensure you are using the built-in `open()` function via the `builtins` module. 2. The `Reverser` class should only reverse the content when the `read` method is called. 3. Do not modify the actual content of the file. This question tests your understanding of the builtins module and your ability to manipulate file operations in Python.","solution":"import builtins class Reverser: def __init__(self, file): self.file = file def read(self): content = self.file.read() return content[::-1] def custom_open(path, mode): file = builtins.open(path, mode) return Reverser(file)"},{"question":"**Question: Implementing a Model Training and Evaluation Pipeline** **Objective:** The task is to implement a machine learning pipeline using scikit-learn that includes preprocessing, training, and evaluation. This will test your understanding of data preprocessing, model training, hyperparameter tuning, and performance evaluation using scikit-learn. **Description:** Write a Python function `train_evaluate_pipeline` that takes the following inputs: 1. `X`: A pandas DataFrame containing the feature set. 2. `y`: A pandas Series containing the target variable. 3. `model`: An initialized scikit-learn estimator. 4. `param_grid`: A dictionary containing hyperparameters to tune through grid search. The function should: 1. Split the data into training and testing sets. 2. Preprocess the data using `StandardScaler` for numeric features. 3. Perform hyperparameter tuning using `GridSearchCV`. 4. Train the model using the best parameters found. 5. Evaluate the model on the test set using accuracy, precision, recall, and F1-score. 6. Return a dictionary containing the evaluation metrics and the best parameters. **Constraints:** - Use an 80-20 split for training and testing sets. - Assume numerical features only. - Use `random_state=42` for reproducibility. **Input:** - `X`: `pd.DataFrame` - Features DataFrame - `y`: `pd.Series` - Target column - `model`: `estimator` - Initialized scikit-learn estimator (e.g., `LogisticRegression()`) - `param_grid`: `dict` - Dictionary containing the parameters to tune (e.g., `{\'C\': [0.1, 1, 10]}`) **Output:** - `results`: `dict` - Dictionary containing evaluation metrics (`accuracy`, `precision`, `recall`, `f1`) and `best_params`. **Example Function Signature:** ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def train_evaluate_pipeline(X: pd.DataFrame, y: pd.Series, model, param_grid: dict) -> dict: pass # Example usage: # X = pd.DataFrame([[1, 2], [3, 4], [5, 6], [7, 8]], columns=[\\"feat1\\", \\"feat2\\"]) # y = pd.Series([0, 1, 0, 1]) # model = LogisticRegression() # param_grid = {\'C\': [0.1, 1, 10]} # results = train_evaluate_pipeline(X, y, model, param_grid) # print(results) # {\'accuracy\': 1.0, \'precision\': 1.0, \'recall\': 1.0, \'f1\': 1.0, \'best_params\': {\'C\': 0.1}} ``` Make sure to follow best practices for coding and comments where necessary for clarity.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def train_evaluate_pipeline(X: pd.DataFrame, y: pd.Series, model, param_grid: dict) -> dict: # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Perform hyperparameter tuning using GridSearchCV grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train_scaled, y_train) # Train the model using the best parameters found best_model = grid_search.best_estimator_ best_model.fit(X_train_scaled, y_train) # Evaluate the model on the test set y_pred = best_model.predict(X_test_scaled) results = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred), \'recall\': recall_score(y_test, y_pred), \'f1\': f1_score(y_test, y_pred), \'best_params\': grid_search.best_params_ } return results"},{"question":"Objective Demonstrate the ability to utilize the `operator` module from Python 3.10 to perform complex data manipulations efficiently. Description You are given a list of dictionaries, each representing a product in an inventory. Each dictionary contains the following keys: `name` (string), `price` (float), and `quantity` (int). You need to perform several operations using functions from the `operator` module and return the results. Tasks 1. Implement a function `total_inventory_value(inventory)` that calculates the total value of the inventory. The value of an individual product is calculated as `price * quantity`. 2. Implement a function `sorted_inventory_by_price(inventory)` that returns the inventory sorted by the price of the products in ascending order. 3. Implement a function `filter_expensive_products(inventory, min_price)` that returns a list of products that have a price greater than or equal to `min_price`. 4. Implement a function `update_inventory(inventory, product_name, quantity)` that updates the quantity of the product with `product_name`. If the product is found, its `quantity` should be updated by adding the given `quantity`. If the product is not found, the function should raise a `ValueError`. 5. Implement a function `remove_product(inventory, product_name)` that removes a product by its `product_name` from the inventory. If the product is found and removed, the function should return `True`. If the product is not found, the function should return `False`. Function Signatures ```python import operator def total_inventory_value(inventory: list) -> float: pass def sorted_inventory_by_price(inventory: list) -> list: pass def filter_expensive_products(inventory: list, min_price: float) -> list: pass def update_inventory(inventory: list, product_name: str, quantity: int) -> None: pass def remove_product(inventory: list, product_name: str) -> bool: pass ``` Constraints - Each product in the inventory is a dictionary with keys: `name` (string), `price` (float), and `quantity` (int). - The `inventory` list will have at least one product. - The `price` and `quantity` will be non-negative. - The `quantity` in `update_inventory` can be negative, which means subtracting from the current quantity. Examples ```python inventory = [ {\\"name\\": \\"apple\\", \\"price\\": 1.5, \\"quantity\\": 10}, {\\"name\\": \\"banana\\", \\"price\\": 0.5, \\"quantity\\": 20}, {\\"name\\": \\"pear\\", \\"price\\": 1.0, \\"quantity\\": 15} ] assert total_inventory_value(inventory) == 1.5*10 + 0.5*20 + 1.0*15 assert sorted_inventory_by_price(inventory) == [ {\\"name\\": \\"banana\\", \\"price\\": 0.5, \\"quantity\\": 20}, {\\"name\\": \\"pear\\", \\"price\\": 1.0, \\"quantity\\": 15}, {\\"name\\": \\"apple\\", \\"price\\": 1.5, \\"quantity\\": 10} ] assert filter_expensive_products(inventory, 1.0) == [ {\\"name\\": \\"apple\\", \\"price\\": 1.5, \\"quantity\\": 10}, {\\"name\\": \\"pear\\", \\"price\\": 1.0, \\"quantity\\": 15} ] update_inventory(inventory, \\"banana\\", 5) assert inventory == [ {\\"name\\": \\"apple\\", \\"price\\": 1.5, \\"quantity\\": 10}, {\\"name\\": \\"banana\\", \\"price\\": 0.5, \\"quantity\\": 25}, {\\"name\\": \\"pear\\", \\"price\\": 1.0, \\"quantity\\": 15} ] assert remove_product(inventory, \\"apple\\") == True assert inventory == [ {\\"name\\": \\"banana\\", \\"price\\": 0.5, \\"quantity\\": 25}, {\\"name\\": \\"pear\\", \\"price\\": 1.0, \\"quantity\\": 15} ] assert remove_product(inventory, \\"grape\\") == False ```","solution":"import operator def total_inventory_value(inventory: list) -> float: Calculates the total value of the inventory. return sum(operator.mul(item[\'price\'], item[\'quantity\']) for item in inventory) def sorted_inventory_by_price(inventory: list) -> list: Returns the inventory sorted by the price of the products in ascending order. return sorted(inventory, key=operator.itemgetter(\'price\')) def filter_expensive_products(inventory: list, min_price: float) -> list: Returns a list of products with a price greater than or equal to min_price. return list(filter(lambda item: item[\'price\'] >= min_price, inventory)) def update_inventory(inventory: list, product_name: str, quantity: int) -> None: Updates the quantity of a specific product in the inventory. Raises ValueError if the product is not found. for item in inventory: if item[\'name\'] == product_name: item[\'quantity\'] += quantity return raise ValueError(f\\"Product \'{product_name}\' not found in inventory.\\") def remove_product(inventory: list, product_name: str) -> bool: Removes a product by its name from the inventory. Returns True if the product was successfully removed, False if the product was not found. for i, item in enumerate(inventory): if item[\'name\'] == product_name: inventory.pop(i) return True return False"},{"question":"# Advanced Coding Assessment Question: Custom Traceback Logger **Objective:** Create a Python class `CustomLogger` that utilizes the `traceback` module to capture, format, and log detailed information about exceptions occurring in a given block of code. Your solution should demonstrate a strong understanding of handling exceptions and using the `traceback` module. **Requirements:** 1. **Class Definition:** - Define a class `CustomLogger`. 2. **Initialization (__init__):** - Initialize the class with a file path where the log of exceptions will be saved. 3. **Logging Method (`log_exception`):** - Create a method `log_exception` that takes no parameters. This method should: - Capture the current exception information. - Use the `traceback` module to format the exception and its traceback. - Save the formatted exception information to the specified log file. 4. **Context Management Support:** - Implement context management (`__enter__` and `__exit__` methods) so that `CustomLogger` can be used with a `with` statement. - Within the `__exit__` method, ensure that any exceptions raised within the `with` block are logged using `log_exception`. **Input:** - You are not required to handle input directly within the `CustomLogger` class. The inputs to test the class will be the code executed within a `with` block. **Output:** - Output should be written to a log file specified during the initialization of `CustomLogger`. **Constraints:** - The code should handle any type of exception. - Ensure the log file is appended to, not overwritten, with each new exception. **Performance:** - Your implementation should efficiently format and write to the log file. Exceptions should be logged quickly to avoid significant overhead. **Example Usage:** ```python from custom_logger import CustomLogger try: with CustomLogger(\\"error_log.txt\\") as logger: # Code block where exceptions need to be logged raise ValueError(\\"An error occurred!\\") except ValueError as e: print(\\"Exception was logged.\\") ``` Upon running the above code, the contents of `error_log.txt` might look like this: ``` Traceback (most recent call last): File \\"example_code.py\\", line 4, in <module> raise ValueError(\\"An error occurred!\\") ValueError: An error occurred! ``` **Note:** - You need to handle the log file\'s opening and closing within your class methods to ensure all exceptions are captured and logged correctly.","solution":"import traceback class CustomLogger: def __init__(self, file_path): self.file_path = file_path def log_exception(self): exc_type, exc_value, exc_tb = traceback.sys.exc_info() formatted_exception = \\"\\".join(traceback.format_exception(exc_type, exc_value, exc_tb)) with open(self.file_path, \\"a\\") as file: file.write(formatted_exception) def __enter__(self): # No setup needed for enter return self def __exit__(self, exc_type, exc_value, exc_tb): if exc_type is not None: self.log_exception() # Returning False re-raises the exception after logging return False"},{"question":"**Question:** Implement an asynchronous downloader utility in Python using the `asyncio` module. **Background:** You are required to download multiple files from the internet concurrently. To achieve this, you need to write an asynchronous function using `asyncio` that takes in a list of URLs, downloads each file, and saves it to the local filesystem. The download process for each file should be concurrent. **Requirements:** 1. Create a coroutine named `download_file(url, session, timeout)` that downloads a file from the given `url` using an `aiohttp.ClientSession`. The `timeout` parameter defines the maximum number of seconds to wait for a download. If the download times out, handle this gracefully by printing an error message. 2. Create a main coroutine named `main(urls, timeout)` that: - Initializes an `aiohttp.ClientSession`. - Creates and gathers tasks for concurrently downloading files using the provided URLs. - Ensures that all tasks handle timeouts appropriately. 3. Use `asyncio.gather()` to run the download tasks concurrently. 4. Ensure that your code handles exceptions and that all resources are cleaned up properly. **Input:** - A list of URLs (`urls`) to download files from. - A timeout (`timeout`) value in seconds for each file download. **Output:** - Print messages indicating the start and successful completion of each download. - Print an error message if a download fails or times out. **Constraints:** - You may not use any synchronous code for downloading files. - You should handle and log any exceptions that occur during the downloading process. **Function Signatures:** ```python import asyncio import aiohttp async def download_file(url: str, session: aiohttp.ClientSession, timeout: int): pass async def main(urls: list, timeout: int): pass # Example on how to run the tasks urls = [\\"http://example.com/file1\\", \\"http://example.com/file2\\"] timeout = 10 asyncio.run(main(urls, timeout)) ``` Be sure to demonstrate understanding of creating tasks, timeouts, handling exceptions, and running tasks concurrently using the `asyncio` module.","solution":"import asyncio import aiohttp import os async def download_file(url: str, session: aiohttp.ClientSession, timeout: int): try: async with session.get(url, timeout=timeout) as response: if response.status == 200: content = await response.read() filename = os.path.basename(url) with open(filename, \'wb\') as f: f.write(content) print(f\\"Successfully downloaded {filename}\\") else: print(f\\"Failed to download {url}, status code: {response.status}\\") except asyncio.TimeoutError: print(f\\"Download timed out for {url}\\") except Exception as e: print(f\\"An error occurred while downloading {url}: {str(e)}\\") async def main(urls: list, timeout: int): async with aiohttp.ClientSession() as session: tasks = [download_file(url, session, timeout) for url in urls] await asyncio.gather(*tasks, return_exceptions=True) # Example on how to run the tasks # urls = [\\"http://example.com/file1\\", \\"http://example.com/file2\\"] # timeout = 10 # asyncio.run(main(urls, timeout))"},{"question":"# XML Document Parser Implementation **Objective**: Implement a function that parses an XML document and collects information on elements, their attributes, and character data. **Task**: Write a function `parse_xml_document(xml_data: str) -> dict` that takes a string of XML data and returns a dictionary capturing the data from the XML. The function must use the `xml.parsers.expat` module with appropriate handler functions to collect the required data. **Input**: - `xml_data`: A string containing a valid XML document. **Output**: - Returns a dictionary with the structure: ```python { \\"elements\\": [ {\\"name\\": \\"<element_name>\\", \\"attributes\\": {\\"attr_name\\": \\"attr_value\\"}, \\"data\\": \\"text data\\"}, ... ], \\"errors\\": [ \\"Error message for the parsing error if any\\", ... ] } ``` **Constraints**: - The document will have properly nested elements. - Attributes and character data may be present. - Handle cases of added elements, namespaces, and character data efficiently. - If any error occurs during parsing, capture it and continue parsing the rest of the document. **Performance Requirements**: - Ensure that the parsing process efficiently handles documents up to 1 MB in size. **Example**: Given the following XML data: ```xml <library> <book id=\\"1\\" author=\\"Author One\\">Book Title One</book> <book id=\\"2\\" author=\\"Author Two\\">Book Title Two</book> </library> ``` Your function should return: ```python { \\"elements\\": [ {\\"name\\": \\"library\\", \\"attributes\\": {}, \\"data\\": \\"\\"}, {\\"name\\": \\"book\\", \\"attributes\\": {\\"id\\": \\"1\\", \\"author\\": \\"Author One\\"}, \\"data\\": \\"Book Title One\\"}, {\\"name\\": \\"book\\", \\"attributes\\": {\\"id\\": \\"2\\", \\"author\\": \\"Author Two\\"}, \\"data\\": \\"Book Title Two\\"}, ], \\"errors\\": [] } ``` **Notes**: - Use the `xml.parsers.expat.ParserCreate` function to create the parser. - Implement and set the handler functions for `StartElementHandler`, `EndElementHandler`, and `CharacterDataHandler`. - Ensure that your solution captures and logs any errors encountered by the parser. - A robust error handling process is vital to ensure that the parser continues to function even if it encounters errors in the input XML. Happy coding!","solution":"import xml.parsers.expat def parse_xml_document(xml_data: str) -> dict: result = { \\"elements\\": [], \\"errors\\": [] } current_data = \\"\\" current_attributes = {} def start_element(name, attrs): nonlocal current_data, current_attributes current_attributes = dict(attrs) current_data = \\"\\" result[\\"elements\\"].append({\\"name\\": name, \\"attributes\\": current_attributes, \\"data\\": current_data}) def end_element(name): nonlocal current_data, current_attributes if result[\\"elements\\"] and result[\\"elements\\"][-1][\\"name\\"] == name: result[\\"elements\\"][-1][\\"data\\"] = current_data current_data = \\"\\" current_attributes = {} def char_data(data): nonlocal current_data current_data += data def handle_error(exception): result[\\"errors\\"].append(str(exception)) # Create parser parser = xml.parsers.expat.ParserCreate() # Register handlers parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data) except xml.parsers.expat.ExpatError as err: handle_error(err) return result"},{"question":"Objective You need to demonstrate your understanding of Python\'s `pickle` module by implementing serialization and deserialization of complex Python objects. You will need to handle custom object types, preserve state, and restrict certain globals during the pickling process. Task 1. Define a Python class `CustomObject` that includes the following attributes: - `name` (str) - `id` (int) - `data` (dict) Ensure that the objects of this class are serializable using the `pickle` module. 2. Implement a function `serialize_objects` that takes a list of `CustomObject` instances and serializes them to a file using `pickle`. The filename should be provided as an argument to the function. 3. Implement a function `deserialize_objects` that reads the serialized data from file and returns a list of deserialized `CustomObject` instances. 4. Implement custom reduction methods for the `CustomObject` class that ensure only the `name`, `id`, and `data` attributes are serialized. 5. Restrict the deserialization to avoid loading any unsafe globals. Implement a safe environment where only `CustomObject` can be deserialized. # Input - `serialize_objects` accepts two arguments: - `objects` (List of `CustomObject` instances): The list of objects to serialize. - `filename` (str): The name of the file to which the objects should be serialized. - `deserialize_objects` accepts one argument: - `filename` (str): The name of the file from which the objects should be deserialized. # Output - `serialize_objects` does not return any value. - `deserialize_objects` returns a list of `CustomObject` instances. # Constraints - The data dictionary in `CustomObject` can have up to 100 key-value pairs. - The size of the serialized file should not exceed 1MB. # Example ```python class CustomObject: def __init__(self, name, id, data): self.name = name self.id = id self.data = data def serialize_objects(objects, filename): pass def deserialize_objects(filename): pass # Example usage: obj1 = CustomObject(\\"Object1\\", 1, {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"}) obj2 = CustomObject(\\"Object2\\", 2, {\\"keyA\\": \\"valueA\\", \\"keyB\\": \\"valueB\\"}) serialize_objects([obj1, obj2], \\"objects.pkl\\") deserialized_objects = deserialize_objects(\\"objects.pkl\\") for obj in deserialized_objects: print(obj.name, obj.id, obj.data) ``` The above code should create a serialized file \\"objects.pkl\\" from the list of `CustomObject` instances and then read it back, restoring the objects accurately. # Notes - Ensure you handle the custom reduction for `CustomObject` to facilitate its serialization. - Implement appropriate checks to ensure safe deserialization, restricting globals loads to enhance security.","solution":"import pickle class CustomObject: def __init__(self, name, id, data): self.name = name self.id = id self.data = data def __reduce__(self): return (self.__class__, (self.name, self.id, self.data)) def serialize_objects(objects, filename): with open(filename, \'wb\') as file: pickle.dump(objects, file) def deserialize_objects(filename): def custom_loads(): safe_globals = {\\"CustomObject\\": CustomObject} with open(filename, \'rb\') as file: return pickle.load(file, fix_imports=False, encoding=\\"ASCII\\", errors=\\"strict\\") return custom_loads()"},{"question":"# Advanced Histogram Plots using Seaborn You are provided with two datasets, `planets` and `penguins`. Your task is to create advanced histogram plots using seaborn that showcase various functionalities discussed in the provided seaborn documentation. Specifically, you need to perform the following tasks: 1. Load the `planets` dataset and plot a histogram of the `distance` column: - The histogram should use a log scale for the x-axis. - Show unfilled bars. - Add a colorbar with customized settings. 2. Load the `penguins` dataset and plot a bivariate histogram: - Use the `bill_depth_mm` and `body_mass_g` columns. - Add a `hue` variable to differentiate between `species`. 3. Use the `planets` dataset to create a bivariate histogram: - Use the `year` and `distance` columns. - Set the number of bins to 30 for both axes. - Set the x-axis scale to linear and the y-axis scale to logarithmic. - Customize to show cells with no observations as transparent. 4. Plot a histogram of the `flipper_length_mm` column from the `penguins` dataset: - Use `hue` to differentiate between `species`. - Apply stacking (`multiple=\\"stack\\"`) and use step functions (`element=\\"step\\"`). Provide the code to perform each of these tasks. Ensure that your plots are well-labeled with relevant titles and/or axis labels. # Input and Output Format Input No explicit input is required. Use the seaborn datasets as mentioned. Output Generate and display the plots as specified in the tasks above. # Constraints - Use the seaborn library for creating the plots. - Follow best practices for code readability and plot labeling. # Performance - Ensure the plots are generated efficiently and render correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def load_and_plot_planets_distance(): planets = sns.load_dataset(\'planets\') plt.figure(figsize=(10, 6)) ax = sns.histplot(data=planets, x=\'distance\', log_scale=(True, False), element=\'step\', color=\'blue\') plt.colorbar(ax.collections[0], label=\'Count\') plt.title(\'Histogram of Planet Distances (Log Scale)\') plt.xlabel(\'Distance\') plt.ylabel(\'Frequency\') plt.show() def load_and_plot_penguins_bivariate(): penguins = sns.load_dataset(\'penguins\') plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\'bill_depth_mm\', y=\'body_mass_g\', hue=\'species\', bins=50) plt.title(\'Bivariate Histogram of Bill Depth vs Body Mass\') plt.xlabel(\'Bill Depth (mm)\') plt.ylabel(\'Body Mass (g)\') plt.show() def load_and_plot_planets_bivariate(): planets = sns.load_dataset(\'planets\') plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\'year\', y=\'distance\', bins=(30, 30), log_scale=(False, True), cbar=True, cbar_kws={\'label\': \'Density\'}) plt.title(\'Bivariate Histogram of Year vs Distance (Log Scale Y)\') plt.xlabel(\'Year\') plt.ylabel(\'Distance\') plt.show() def load_and_plot_penguins_flipper_length(): penguins = sns.load_dataset(\'penguins\') plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', multiple=\'stack\', element=\'step\', palette=\'viridis\') plt.title(\'Histogram of Flipper Length by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show()"},{"question":"**Coding Assessment Question: PCA and Pipelines** Objective Implement a function to perform dimensionality reduction using Principal Component Analysis (PCA) and then classify the data using a Support Vector Machine (SVM). Problem Statement Write a function `pca_svm_pipeline(X_train, X_test, y_train, n_components)` that takes in the following parameters: - `X_train` (numpy array of shape `(m_train, n)`): The training data. - `X_test` (numpy array of shape `(m_test, n)`): The testing data. - `y_train` (numpy array of shape `(m_train,)`): The labels for the training data. - `n_components` (int): The number of principal components to keep. The function should: 1. Standardize the features using `StandardScaler`. 2. Apply PCA to reduce the dimensionality of the features to `n_components`. 3. Train an SVM classifier (`svm.SVC` in scikit-learn) on the transformed training data. 4. Predict the labels for the testing data using the trained SVM classifier. Input Format - `X_train`: A 2D numpy array of shape `(m_train, n)`. - `X_test`: A 2D numpy array of shape `(m_test, n)`. - `y_train`: A 1D numpy array of shape `(m_train,)`. - `n_components`: An integer representing the number of principal components to retain. Output Format - Return a numpy array containing the predicted labels for `X_test`. Constraints - Ensure that `n_components` is less than or equal to the number of features (columns) in `X_train`. Example ```python import numpy as np # Example data X_train = np.array([[1, 2], [3, 4], [5, 6]]) X_test = np.array([[2, 3], [4, 5]]) y_train = np.array([0, 1, 0]) n_components = 1 preds = pca_svm_pipeline(X_train, X_test, y_train, n_components) print(preds) # Expected output: array([predicted_labels]) ``` Notes - You may assume that all necessary imports from scikit-learn have been made. - Use `StandardScaler` for standardizing features. - Use `PCA` for dimensionality reduction. - Use `svm.SVC` for the Support Vector Machine classifier.","solution":"from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn import svm import numpy as np def pca_svm_pipeline(X_train, X_test, y_train, n_components): Perform PCA on the training data, train an SVM classifier, and predict labels for testing data. Parameters: X_train (numpy.array): Training data of shape (m_train, n) X_test (numpy.array): Testing data of shape (m_test, n) y_train (numpy.array): Training labels of shape (m_train,) n_components (int): Number of principal components to keep Returns: numpy.array: Predicted labels for X_test # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Apply PCA pca = PCA(n_components=n_components) X_train_pca = pca.fit_transform(X_train_scaled) X_test_pca = pca.transform(X_test_scaled) # Train SVM classifier clf = svm.SVC() clf.fit(X_train_pca, y_train) # Predict using the trained classifier y_pred = clf.predict(X_test_pca) return y_pred"},{"question":"You are given a neural network and you need to run it on the available accelerators (like GPUs). Your task is to write functions to check the availability of accelerators, select a device, and run the forward pass of the neural network on the selected device. Here are the steps you need to follow: 1. Write a function `check_accelerator_availability()` that returns `True` if an accelerator (like a GPU) is available and `False` otherwise. 2. Write a function `select_accelerator_device()` that: - Accepts an index as an argument. - Sets the device to the provided index if it is within the range of available devices. - Returns the index of the selected device or `-1` if the index is out of range. 3. Write a function `run_network_on_device(network, input_tensor, device_index)` that: - Accepts a PyTorch `network` (an instantiated model), an `input_tensor` (a tensor to pass through the network), and a `device_index` to run the forward pass. - Uses the selected device for the computation. - Returns the output from the network after running the forward pass on the selected device. You are provided with a sample neural network `SimpleNet`, a sample input tensor, and an environment with at least one accelerator. **Functions:** ```python import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) def check_accelerator_availability(): Check if an accelerator is available. Returns: bool: True if at least one accelerator is available, False otherwise. pass # Implement this function def select_accelerator_device(index): Select the device with the given index. Args: index (int): The index of the device to select. Returns: int: The index of the selected device, or -1 if the index is out of range. pass # Implement this function def run_network_on_device(network, input_tensor, device_index): Run the forward pass of the network on the specified device. Args: network (nn.Module): The neural network. input_tensor (torch.Tensor): The input tensor for the network. device_index (int): The index of the device to run the network on. Returns: torch.Tensor: The output tensor from the network. pass # Implement this function ``` **Example:** ```python # Sample network and input tensor net = SimpleNet() input_tensor = torch.randn(1, 10) # Check if accelerator is available if check_accelerator_availability(): print(\\"Accelerator is available.\\") # Select the first accelerator device selected_device = select_accelerator_device(0) if selected_device != -1: print(f\\"Using device: {selected_device}\\") # Run network on the selected device output = run_network_on_device(net, input_tensor, selected_device) print(f\\"Output: {output}\\") else: print(\\"Accelerator is not available.\\") ``` **Constraints:** - Ensure efficient device checking and setting logic. - Handle cases where the input index for device selection is out of range. Ensure your implementation is efficient and works correctly across different environments with varying numbers of available devices.","solution":"import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) def check_accelerator_availability(): Check if an accelerator is available. Returns: bool: True if at least one accelerator is available, False otherwise. return torch.cuda.is_available() def select_accelerator_device(index): Select the device with the given index. Args: index (int): The index of the device to select. Returns: int: The index of the selected device, or -1 if the index is out of range. if index >= 0 and index < torch.cuda.device_count(): return index else: return -1 def run_network_on_device(network, input_tensor, device_index): Run the forward pass of the network on the specified device. Args: network (nn.Module): The neural network. input_tensor (torch.Tensor): The input tensor for the network. device_index (int): The index of the device to run the network on. Returns: torch.Tensor: The output tensor from the network. if device_index == -1: raise ValueError(\\"Invalid device index\\") device = torch.device(f\'cuda:{device_index}\' if device_index != -1 else \'cpu\') network.to(device) input_tensor = input_tensor.to(device) network.eval() with torch.no_grad(): output = network(input_tensor) return output"},{"question":"**Question: Web Data Fetching and Processing using `urllib` and `http.client`** You are required to write a Python function that fetches data from a given URL, processes the response, and returns specific information extracted from it. The URL will point to a JSON file. # Function Specification **Function Name:** ```python fetch_and_process(url: str) -> dict ``` **Input:** - `url` (str): A valid URL pointing to a JSON resource. **Output:** - Returns a dictionary containing: - `status_code` (int): The HTTP status code returned by the request. - `headers` (dict): A dictionary of headers received in the response. - `data` (dict): The parsed JSON data from the response. **Constraints:** - Ensure that the URL returns a valid JSON file. - Use the `urllib.request` module to handle the URL fetching. - Use the `http.client` to handle HTTP responses and extract headers. # Example Assume that the following URL returns the JSON: `{\\"name\\": \\"John\\", \\"age\\": 30}` ```python url = \\"http://example.com/data.json\\" result = fetch_and_process(url) ``` The output should be: ```python { \'status_code\': 200, \'headers\': {\'Content-Type\': \'application/json\', ...}, \'data\': {\'name\': \'John\', \'age\': 30} } ``` # Notes 1. Handle any potential errors that may arise during the fetching or parsing process. 2. You can assume the URL always points directly to a JSON file and does not require additional query parameters or headers for access. # Additional Information: To handle the HTTP request and response, you may find the following classes and methods useful: - `urllib.request.urlopen` - `http.client.HTTPResponse` - `http.client.HTTPMessage` Make sure your function is efficient and handles exceptions gracefully.","solution":"import urllib.request import json from http.client import HTTPResponse def fetch_and_process(url: str) -> dict: Fetches data from a given URL and processes the response. Args: - url (str): A valid URL pointing to a JSON resource. Returns: - dict: Contains `status_code`, `headers`, and `data`. try: # Open the URL and fetch the response with urllib.request.urlopen(url) as response: status_code = response.getcode() headers = dict(response.getheaders()) data = json.loads(response.read().decode()) return { \'status_code\': status_code, \'headers\': headers, \'data\': data } except Exception as e: print(f\\"An error occurred: {e}\\") raise"},{"question":"**Problem Statement**: You are tasked with implementing a utility function that will analyze several text files and provide insights based on the contents of these files. Your function should make use of the `collections.ChainMap` and `collections.Counter` classes to achieve this. Specifically, your function should: 1. **Read the contents of multiple files** provided as input. 2. **Combine the word counts** from these files using `ChainMap`, so that updates in individual files are reflected in the aggregated view. 3. **Return a combined `Counter` object** representing the total word counts across all files. Additionally, your function should be able to dynamically handle updates to the content of the files, such that the combined word counts always reflect the current state of all files. **Function Signature**: ```python def analyze_text_files(file_paths: List[str]) -> Counter: pass ``` **Input**: - `file_paths`: A list of strings where each string is a path to a text file. All files are assumed to be UTF-8 encoded. **Output**: - A `Counter` object representing the total word counts across all files. The keys should be words, and the values should be their respective counts. **Constraints**: - Each file can contain up to 10^6 words. - You may assume the file contents fit into memory. - Performance and efficiency are important, as the function may be called multiple times with updates to the files. **Example**: Assume you have the following two files: - `file1.txt` contains: `\\"apple apple orange banana\\"` - `file2.txt` contains: `\\"banana apple apple grape\\"` After calling your function with these file paths, the output should be: ```python analyze_text_files([\'file1.txt\', \'file2.txt\']) # Output: Counter({\'apple\': 4, \'banana\': 2, \'orange\': 1, \'grape\': 1}) ``` **Additional Requirements**: 1. Be mindful of the performance implications when reading from large files. 2. Use `ChainMap` to maintain the combined view of individual file word counts. 3. Ensure that updates to the files are reflected in the combined word count. You are expected to handle the reading, parsing, and word counting in an efficient manner. Feel free to create additional helper functions if needed. **Hints**: - Utilize `collections.Counter` to count words efficiently. - Leverage `collections.ChainMap` to combine multiple mappings without merging them into a single dictionary. Happy coding!","solution":"from typing import List from collections import ChainMap, Counter def analyze_text_files(file_paths: List[str]) -> Counter: def count_words_in_file(file_path): with open(file_path, \'r\', encoding=\'utf-8\') as file: word_list = file.read().split() return Counter(word_list) file_counters = [count_words_in_file(file_path) for file_path in file_paths] combined_counter = ChainMap(*file_counters) total_counter = Counter() for cmap in combined_counter.maps: total_counter.update(cmap) return total_counter"},{"question":"**Question:** You are tasked with creating a utility function that consolidates various functionalities of the `importlib.metadata` library to provide comprehensive details about an installed Python package. Specifically, you need to implement a function `get_package_details(package_name: str) -> dict` that returns a dictionary containing the following information about a given package: 1. **Version**: The version of the package. 2. **Metadata**: A dictionary of metadata about the package, including keys like \\"Name\\", \\"Version\\", \\"Summary\\", \\"Author\\", etc. 3. **Files**: A list of file paths included in the package. 4. **Requirements**: A list of distribution requirements. # Input: - `package_name` (str): The name of an installed package. # Output: - A dictionary containing the following keys: - `version` (str): The version of the specified package. - `metadata` (dict): A dictionary containing various metadata attributes. - `files` (list): A list of file paths included in the package. - `requirements` (list): A list of requirements for the package. # Constraints: - The package should be a valid, installed package accessible via `importlib.metadata`. - The function should handle cases where certain metadata or files might be missing and return appropriate values (e.g., empty list or dictionary). # Example: ```python result = get_package_details(\\"wheel\\") print(result) ``` Expected output (format may vary based on actual installed package details): ```python { \'version\': \'0.32.3\', \'metadata\': { \'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', \'Version\': \'0.32.3\', \'Summary\': \'A built-package format for Python.\', \'Home-page\': \'https://github.com/pypa/wheel\', \'Author\': \'Daniel Holth\', \'Author-email\': \'dholth@gmail.com\', \'License\': \'MIT\', ... }, \'files\': [ \'wheel/__init__.py\', \'wheel/bdist_wheel.py\', ... ], \'requirements\': [ \\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\" ] } ``` # Implementation Notes: 1. Use the functions `version()`, `metadata()`, `files()`, and `requires()` from the `importlib.metadata` package to fetch the required details. 2. Ensure that the function handles any exceptions or missing data gracefully.","solution":"import importlib.metadata def get_package_details(package_name: str) -> dict: Returns a dictionary containing the version, metadata, files, and requirements of a given installed package. :param package_name: Name of the installed package :return: Dictionary with \'version\', \'metadata\', \'files\', \'requirements\' as keys. package_details = {} try: # Get version package_details[\'version\'] = importlib.metadata.version(package_name) # Get metadata metadata = importlib.metadata.metadata(package_name) package_details[\'metadata\'] = {key: metadata[key] for key in metadata} # Get files files = importlib.metadata.files(package_name) package_details[\'files\'] = [str(file) for file in files] if files else [] # Get requirements requirements = importlib.metadata.requires(package_name) package_details[\'requirements\'] = requirements if requirements else [] except importlib.metadata.PackageNotFoundError: # Handle package not found package_details = { \'version\': None, \'metadata\': {}, \'files\': [], \'requirements\': [] } return package_details"},{"question":"**Password Storage and Verification using the `crypt` Module** As a system administrator, you are tasked with creating a Python script to help manage user passwords securely. Specifically, you need to ensure that passwords are hashed correctly when stored and can be verified against user input during login attempts. Your task is to implement two functions using the `crypt` module: 1. `store_password(plain_password: str, method: crypt.METHOD_*) -> str`: - Takes a plain-text password and hashes it using the specified method. - Returns the hashed password. 2. `verify_password(stored_password: str, input_password: str) -> bool`: - Takes a stored hashed password and a plain-text input password. - Returns `True` if the input password, when hashed, matches the stored password; otherwise, returns `False`. # Constraints - You must use the `crypt.mksalt()` function to generate a salt for hashing the password. - You must use the `crypt.crypt()` function for both hashing and verifying passwords. - For the `store_password` function, use the `crypt.METHOD_SHA512` by default if no method is provided. # Input/Output Format - The `store_password` function should accept a string `plain_password` and an optional `method`, and return a hashed password string. - The `verify_password` function should accept two strings, `stored_password` and `input_password`, and return a boolean indicating whether the passwords match. # Example ```python import crypt def store_password(plain_password, method=crypt.METHOD_SHA512): salt = crypt.mksalt(method) hashed_password = crypt.crypt(plain_password, salt) return hashed_password def verify_password(stored_password, input_password): return crypt.crypt(input_password, stored_password) == stored_password # Example usage: # Storing Password hashed = store_password(\\"my_secret_password\\") print(hashed) # Outputs the hashed password # Verifying Password is_valid = verify_password(hashed, \\"my_secret_password\\") print(is_valid) # Outputs: True ``` Implement these functions in Python and ensure they work correctly with various inputs as described.","solution":"import crypt def store_password(plain_password, method=crypt.METHOD_SHA512): Hash the plain text password using the specified method. :param plain_password: Plain text password to be hashed :param method: Method to be used for hashing (default: crypt.METHOD_SHA512) :return: Hashed password salt = crypt.mksalt(method) hashed_password = crypt.crypt(plain_password, salt) return hashed_password def verify_password(stored_password, input_password): Verify the input password against the stored hashed password. :param stored_password: The stored hashed password :param input_password: The plain text input password to be verified :return: True if the input password matches the stored password, False otherwise return crypt.crypt(input_password, stored_password) == stored_password"},{"question":"# Exception Handling and Custom Exception Design Background: In Python, exceptions are a way to handle errors or unexpected conditions that occur during program execution. They allow you to separate error-handling code from regular code, making your program more robust and easier to maintain. From the documentation provided, you learned about the various built-in exceptions in Python. Now, let\'s test your understanding by challenging you to create a custom exception handling mechanism for a specific scenario. Task: You need to create a Python function `process_user_data` that processes user-provided data according to the following rules: 1. If the data is not a string, raise a `TypeError` with a message: \\"Data must be a string\\". 2. If the data is an empty string, raise a `ValueError` with a message: \\"Data cannot be empty\\". 3. If the data contains any numeric characters, raise a `ValueError` with a message: \\"Data cannot contain numbers\\". 4. Additionally, create a custom exception called `MalformedDataError` which should be raised if the data does not start with a capital letter. This exception should inherit from `ValueError` and accept an additional parameter `data`. Input: - The function `process_user_data` should take one parameter `data` which is expected to be a string. Output: - If the data passes all checks, the function should return a success message: \\"Data processed successfully\\". - Otherwise, the function should raise the appropriate exception with a detailed message. Your task involves: 1. Implementing the function `process_user_data(data: str) -> str`. 2. Creating the custom exception `MalformedDataError` inheriting from `ValueError`. Example: ```python def process_user_data(data): # Your implementation here class MalformedDataError(ValueError): def __init__(self, data): super().__init__(f\\"Malformed data: {data}\\") self.data = data # Example Usage: try: process_user_data(\\"JohnDoe123\\") except Exception as e: print(e) # Output: Data cannot contain numbers try: process_user_data(\\"john\\") except Exception as e: print(e) # Output: Malformed data: john ``` Handle various types of invalid inputs gracefully by raising and catching the appropriate exceptions.","solution":"class MalformedDataError(ValueError): def __init__(self, data): super().__init__(f\\"Malformed data: {data}\\") self.data = data def process_user_data(data: str) -> str: if not isinstance(data, str): raise TypeError(\\"Data must be a string\\") if not data: raise ValueError(\\"Data cannot be empty\\") if any(char.isdigit() for char in data): raise ValueError(\\"Data cannot contain numbers\\") if not data[0].isupper(): raise MalformedDataError(data) return \\"Data processed successfully\\""},{"question":"Problem Statement: You are required to implement functions to encode and decode a complex data structure using the `xdrlib` module. Description: Consider an order tracking system where an `Order` contains: - Order ID (integer) - Customer Name (string) - Order Amount (float) - List of Items (each item is a string) Write two functions: 1. `pack_order(order)` which takes an `order` dictionary as input and returns a packed XDR string using the `xdrlib.Packer` class. 2. `unpack_order(xdr_data)` which takes a packed XDR string as input and returns the original `order` dictionary using the `xdrlib.Unpacker` class. Function Definitions: ```python def pack_order(order: dict) -> bytes: Packs the given order into XDR format. Args: order (dict): A dictionary containing the order details with the following structure: { \'order_id\': int, \'customer_name\': str, \'order_amount\': float, \'items\': list of str } Returns: bytes: The packed XDR byte stream. pass def unpack_order(xdr_data: bytes) -> dict: Unpacks the given XDR data into the original order format. Args: xdr_data (bytes): The packed XDR byte stream. Returns: dict: A dictionary containing the order details with the following structure: { \'order_id\': int, \'customer_name\': str, \'order_amount\': float, \'items\': list of str } pass ``` Constraints: - The length of the customer name and individual items in the list of items should not exceed 255 characters. - The number of items in the list should not exceed 1000. Example: ```python order = { \'order_id\': 12345, \'customer_name\': \'John Doe\', \'order_amount\': 250.75, \'items\': [\'item1\', \'item2\', \'item3\'] } # Packing the order packed_data = pack_order(order) print(packed_data) # Unpacking the order unpacked_order = unpack_order(packed_data) print(unpacked_order) ``` The output of the `unpack_order` function should match the original `order` dictionary. Note on Performance: Ensure that your packing and unpacking implementation handles any edge cases efficiently within the provided constraints. Submission: Submit a Python file containing the implementations of `pack_order` and `unpack_order` functions.","solution":"import xdrlib def pack_order(order: dict) -> bytes: Packs the given order into XDR format. Args: order (dict): A dictionary containing the order details with the following structure: { \'order_id\': int, \'customer_name\': str, \'order_amount\': float, \'items\': list of str } Returns: bytes: The packed XDR byte stream. packer = xdrlib.Packer() # Packing the Order ID (integer) packer.pack_int(order[\'order_id\']) # Packing the Customer Name (string) packer.pack_string(order[\'customer_name\'].encode(\'utf-8\')) # Packing the Order Amount (float) packer.pack_double(order[\'order_amount\']) # Packing the List of Items (list of strings) packer.pack_int(len(order[\'items\'])) for item in order[\'items\']: packer.pack_string(item.encode(\'utf-8\')) return packer.get_buffer() def unpack_order(xdr_data: bytes) -> dict: Unpacks the given XDR data into the original order format. Args: xdr_data (bytes): The packed XDR byte stream. Returns: dict: A dictionary containing the order details with the following structure: { \'order_id\': int, \'customer_name\': str, \'order_amount\': float, \'items\': list of str } unpacker = xdrlib.Unpacker(xdr_data) # Unpacking the Order ID (integer) order_id = unpacker.unpack_int() # Unpacking the Customer Name (string) customer_name = unpacker.unpack_string().decode(\'utf-8\') # Unpacking the Order Amount (float) order_amount = unpacker.unpack_double() # Unpacking the List of Items (list of strings) items_length = unpacker.unpack_int() items = [] for _ in range(items_length): items.append(unpacker.unpack_string().decode(\'utf-8\')) return { \'order_id\': order_id, \'customer_name\': customer_name, \'order_amount\': order_amount, \'items\': items }"},{"question":"**Title: Asynchronous File Reader and Data Processor** **Objective:** Demonstrate your understanding of Python\'s built-in types, exception handling, coroutines, and advanced control flow by implementing an asynchronous file reader and data processor. **Problem Statement:** You need to implement a function `async_process_data` that reads data asynchronously from a file, processes it, and handles potential exceptions gracefully. 1. The function should take two arguments: - `file_path` (str): The path to the file to be read. - `process_line` (Callable[[str], Awaitable[Any]]): An asynchronous function that takes a line of text as input and returns an awaitable result after processing the line. 2. The function should: - Open and read the file asynchronously line by line. - For each line, use the `process_line` function to process the data. - Collect the results of processing each line into a list. - Handle any `IOError` that occurs during the file operations and log an appropriate error message. - Handle any exception that occurs during the processing of a line and continue processing the remaining lines. - Return the list of results after processing all lines. 3. You should use: - The `asyncio` module for asynchronous file reading and processing. - Exception handling with try-except blocks to manage errors. - List comprehensions for collecting results. **Constraints:** - The file can be large, so make sure your solution is memory efficient. - The `process_line` function is guaranteed to be correct and does not need to be implemented or tested by you. - Ensure that your function works correctly and efficiently with Python 3.8+ (Python 3.10 preferred). **Function Signature:** ```python import asyncio from typing import Callable, Awaitable, List, Any async def async_process_data(file_path: str, process_line: Callable[[str], Awaitable[Any]]) -> List[Any]: pass ``` **Example Usage:** ```python import asyncio async def mock_process_line(line: str) -> str: await asyncio.sleep(0.1) # Simulate some processing time return f\\"Processed: {line}\\" async def main(): results = await async_process_data(\'sample.txt\', mock_process_line) for result in results: print(result) # Assuming the file \'sample.txt\' contains: # Line 1 # Line 2 # Line 3 # The expected output should be: # Processed: Line 1 # Processed: Line 2 # Processed: Line 3 asyncio.run(main()) ``` You are expected to complete the `async_process_data` function to fulfill the requirements. **Important Notes:** - Make sure to handle the file I/O operations using asynchronous techniques. - Expect the function to operate on large files, emphasizing the need for efficient line-by-line processing. - Use try-except blocks properly to log errors and handle exceptions without stopping the overall file processing.","solution":"import asyncio from typing import Callable, Awaitable, List, Any async def async_process_data(file_path: str, process_line: Callable[[str], Awaitable[Any]]) -> List[Any]: results = [] try: async with aiofiles.open(file_path, mode=\'r\') as file: async for line in file: try: result = await process_line(line) results.append(result) except Exception as e: # Log the exception (print for simplicity here) print(f\\"Error processing line: {line.strip()}, Error: {str(e)}\\") except IOError as e: print(f\\"IOError when reading file {file_path}: {str(e)}\\") return results"},{"question":"Objective Your task is to create a `plot_sinwaves` function that demonstrates three core capabilities of the seaborn package: 1. Applying different themes. 2. Modifying plot styles temporarily. 3. Scaling plot elements for various contexts. Problem Statement Implement a function `plot_sinwaves` that generates a 2x2 grid of subplots showing sinusoidal waves with the following requirements: 1. **Themes**: - The four subplots should use these seaborn themes respectively: `darkgrid`, `whitegrid`, `ticks`, and `dark`. 2. **Style Modification**: - Temporarily modify the style of one of the subplots to change the face color of the axes. 3. **Scaling Contexts**: - Each subplot should use a different plotting context: `paper`, `notebook`, `talk`, and `poster`. Function Signature ```python def plot_sinwaves(): # Your code here ``` Requirements 1. Generate four subplots arranged in a 2x2 grid. 2. In each subplot: - Use the `sinplot` function provided in the documentation to generate the sine wave plots. - Apply a different seaborn theme to each subplot. - Temporarily modify the style (e.g., change the `axes.facecolor`) of one of the subplots. - Apply different scaling contexts to each subplot. 3. Use tight layout for the figure. Constraints - Assume that seaborn and matplotlib are already installed. - The function should not take any parameters and should not return anything. It should directly display the plots. Example Here is an example of what the function might look like in usage: ```python plot_sinwaves() ``` This function call should display a matplotlib figure with four subplots, each customized as per the requirements. Notes - Make sure to use the `with` statement appropriately to temporarly modify styles where required. - Leverage `set_style`, `despine`, and `set_context` functions as needed to meet each requirement.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_sinwaves(): # Create a 2x2 subplot grid fig, axes = plt.subplots(2, 2, figsize=(10, 8)) # Define themes and contexts to be used themes = [\'darkgrid\', \'whitegrid\', \'ticks\', \'dark\'] contexts = [\'paper\', \'notebook\', \'talk\', \'poster\'] for ax, theme, context in zip(axes.flatten(), themes, contexts): # Set seaborn theme and context sns.set_theme(style=theme) sns.set_context(context) # Apply sinplot to the axes sns.lineplot(x=range(100), y=sns.load_dataset(\\"flights\\")[\'passengers\'].head(100).apply(lambda x: x % 30), ax=ax) # Temporarily modify style for one of the subplots if theme == \'ticks\': with sns.axes_style({\'axes.facecolor\': \'lightgrey\'}): sns.lineplot(x=range(100), y=sns.load_dataset(\\"flights\\")[\'passengers\'].head(100).apply(lambda x: x % 30), ax=ax) # Adjust layout to prevent overlapping plt.tight_layout() # Display the plot plt.show()"},{"question":"# Advanced Tuple and Struct Sequence Manipulation in Python **Problem Statement**: You are required to implement the following functions using Python\'s C-API as provided in the `python310` package documentation. 1. **Create Fixed Size Tuple**: Implement a function called `create_fixed_tuple` which takes a list of Python objects and returns a new tuple containing these objects. The length of the list must be strictly equal to 5. If the list length is not equal to 5, the function should raise a `ValueError`. ```python def create_fixed_tuple(objects: list) -> tuple: :param objects: List of exactly 5 Python objects. :return: A new tuple containing the 5 Python objects. :raises ValueError: If list length is not exactly 5. pass ``` 2. **Create Struct Sequence Type and Instance**: Implement a function called `create_struct_sequence` which defines a new struct sequence type with three fields (name, age, country). The function should return an instance of this struct sequence type filled with the provided data. ```python def create_struct_sequence(name: str, age: int, country: str) -> object: :param name: A string representing the name. :param age: An integer representing the age. :param country: A string representing the country. :return: An instance of a new struct sequence type with the fields \'name\', \'age\', and \'country\'. pass ``` # Constraints: 1. **create_fixed_tuple**: - The input list must contain exactly 5 elements. 2. **create_struct_sequence**: - The field \'name\' must be a string. - The field \'age\' must be an integer. - The field \'country\' must be a string. 3. You are NOT allowed to use the `tuple`, `namedtuple`, or other high-level tuple manipulation methods provided by Python outside the specified C-API functions. # Expectations: - Proper use of the C-API functions to manipulate tuples and struct sequences. - Efficient and error-free code. - Handling of error cases and edge cases explicitly as specified in constraints. Make sure the implementation adheres to Python\'s standard error handling and coding practices. Comment your code to reflect the usage of different C-API functions and explain the logic wherever necessary. **Good Luck!**","solution":"def create_fixed_tuple(objects: list) -> tuple: :param objects: List of exactly 5 Python objects. :return: A new tuple containing the 5 Python objects. :raises ValueError: If list length is not exactly 5. if len(objects) != 5: raise ValueError(\\"List must contain exactly 5 elements.\\") return tuple(objects) from collections import namedtuple def create_struct_sequence(name: str, age: int, country: str) -> object: :param name: A string representing the name. :param age: An integer representing the age. :param country: A string representing the country. :return: An instance of a new struct sequence type with the fields \'name\', \'age\', and \'country\'. if not isinstance(name, str): raise ValueError(\\"Name must be a string.\\") if not isinstance(age, int): raise ValueError(\\"Age must be an integer.\\") if not isinstance(country, str): raise ValueError(\\"Country must be a string.\\") # Define a new struct sequence type Person = namedtuple(\'Person\', \'name age country\') # Return an instance of this struct sequence type return Person(name, age, country)"},{"question":"Context The \\"posix\\" module provides access to operating system functionality derived from the POSIX standard, predominantly for Unix-like systems. Among its features, it offers the ability to access environment variables through `posix.environ`. This dictionary represents the string environment at the time the interpreter started. Objective Write a Python function that: 1. Retrieves the value of an environment variable. 2. Updates an environment variable with a new value. 3. Ensures that any updates made to environment variables are reflected in the environment used by subprocesses. Function Signature ```python def manage_env_variable(var_name: str, new_value: str) -> str: Manages the given environment variable and returns its initial value. Parameters: var_name (str): The name of the environment variable to manage. new_value (str): The new value to set for the environment variable. Returns: str: The initial value of the environment variable prior to updating. ``` Input - `var_name`: A string representing the name of an environment variable (e.g., \\"HOME\\"). - `new_value`: A string representing the new value to set the environment variable to. Output - Returns the initial value of the environment variable before it was updated. If the variable did not exist, return `None`. Constraints - Use the `os` module instead of importing `posix` directly to ensure portability and necessary updates. - Assume environment variable names and values are always valid UTF-8 strings. - The function should handle the case where the specified environment variable does not initially exist. Example ```python import os # Initial setup for testing (assuming the environment variable XYZ is not set) os.environ.pop(\\"XYZ\\", None) # Test the function initial_value = manage_env_variable(\\"XYZ\\", \\"12345\\") assert initial_value is None assert os.environ[\\"XYZ\\"] == \\"12345\\" # Update the variable initial_value = manage_env_variable(\\"XYZ\\", \\"67890\\") assert initial_value == \\"12345\\" assert os.environ[\\"XYZ\\"] == \\"67890\\" ``` In the example, the environment variable \\"XYZ\\" is first set to \\"12345\\" and then updated to \\"67890\\". The function returns the previous values correctly, demonstrating the ability to manage environment variables appropriately.","solution":"import os def manage_env_variable(var_name: str, new_value: str) -> str: Manages the given environment variable and returns its initial value. Parameters: var_name (str): The name of the environment variable to manage. new_value (str): The new value to set for the environment variable. Returns: str: The initial value of the environment variable prior to updating, or None if it did not exist. initial_value = os.environ.get(var_name) os.environ[var_name] = new_value return initial_value"},{"question":"# Advanced Coding Assessment Question: GUI with Python `curses` Objective: Implement a user interface in a terminal using the Python `curses` module. This interface should demonstrate your understanding of window management, user input handling, and text attributes. Task: Create a simple text-based task manager application that allows users to view, add, and delete tasks from a list using the Python `curses` module. Implementation Details: 1. **Initialization and Termination**: - Initialize the `curses` application. - Disable echoing and enable cbreak mode. - Enable keypad mode for special keys. - Ensure that the terminal state is properly restored upon exiting. 2. **User Interface**: - Create a main window displaying the list of tasks. - Add a sub-window or pad for entering new tasks. - Use a separate window or message area for displaying instructions and messages to the user. 3. **User Input Handling**: - Allow users to navigate the list of tasks using arrow keys. - Allow users to add a new task by pressing the \'a\' key, entering the task in the sub-window, and confirming with Enter. - Allow users to delete a selected task by pressing the \'d\' key. - Exit the application gracefully by pressing the \'q\' key. 4. **Text Attributes and Colors**: - Highlight the selected task with a different color or attribute. - Use reverse video for the status/instruction window for better visibility. Function Signature: ```python import curses from curses import wrapper def main(stdscr): # Your implementation here wrapper(main) ``` Example Usage: When the application is running, it should display something similar to: ``` ------------------------------------- | Tasks: | | 1. Task 1 | | 2. Task 2 | | 3. (selected) Task 3 | | | | Instructions: | | a - Add task, d - Delete task, | | Arrow keys - navigate, q - Quit | ------------------------------------- ``` Constraints: 1. No libraries other than `curses` and `curses.textpad` are allowed. 2. The implementation should handle exceptions gracefully, ensuring the terminal state is restored correctly even on errors. 3. The application should be able to handle up to 100 tasks. Evaluation Criteria: - Correct and efficient use of `curses` functions. - Proper handling of user input and navigation. - Effective and visually clear use of text attributes and colors. - Robustness and error handling in terminal state restoration. Good luck!","solution":"import curses def main(stdscr): # Initialize curses curses.curs_set(0) # Hide cursor curses.noecho() curses.cbreak() stdscr.keypad(True) # Colors and attributes curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) highlighted = curses.color_pair(1) # Windows height, width = stdscr.getmaxyx() task_win_height = height - 4 task_win = stdscr.subwin(task_win_height, width, 0, 0) input_win = stdscr.subwin(3, width, task_win_height, 0) input_win.border(0) input_win.addstr(0, 2, \\"Input Task:\\") instructions_win = stdscr.subwin(1, width, height-1, 0) # Instructions instructions = \\"a - Add task, d - Delete task, Arrow keys - navigate, q - Quit\\" instructions_win.addstr(0, 0, instructions, curses.A_REVERSE) tasks = [\\"Task 1\\", \\"Task 2\\", \\"Task 3\\"] selected_index = 0 def display_tasks(): task_win.clear() for idx, task in enumerate(tasks): if idx == selected_index: task_win.addstr(idx, 0, task, highlighted) else: task_win.addstr(idx, 0, task) task_win.refresh() def add_task(new_task): tasks.append(new_task) def delete_task(index): if tasks: tasks.pop(index) while True: display_tasks() key = stdscr.getch() if key == ord(\'q\'): break elif key == curses.KEY_UP: selected_index = (selected_index - 1) % len(tasks) elif key == curses.KEY_DOWN: selected_index = (selected_index + 1) % len(tasks) elif key == ord(\'a\'): curses.echo() input_win.addstr(1, 2, \\" \\" * (width - 4)) input_win.refresh() new_task = input_win.getstr(1, 2).decode(\\"utf-8\\") add_task(new_task) curses.noecho() selected_index = len(tasks) - 1 elif key == ord(\'d\'): delete_task(selected_index) selected_index = max(0, selected_index - 1) # Terminate curses curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin()"},{"question":"Objective: Demonstrate proficiency in applying different linear regression techniques using scikit-learn and comparing their performance. Problem Statement: You are given a dataset `diabetes.csv` that contains the following columns: - **Age**: A numerical value representing the age of an individual. - **BMI**: A numerical value representing the Body Mass Index. - **BloodPressure**: A numerical value representing the blood pressure. - **SkinThickness**: A numerical value representing the skin thickness. - **Insulin**: A numerical value representing the insulin level. - **Outcome**: A binary value representing whether the individual has diabetes (1) or not (0). Your task is to: 1. Implement the following regression models to predict the `Outcome` using the given features: - Ordinary Least Squares (OLS) - Ridge Regression - Lasso Regression 2. Split the data into a training set (80%) and a test set (20%). 3. Train each regression model on the training set. 4. Make predictions on the test set using each trained model. 5. Evaluate the performance using the Mean Squared Error (MSE). 6. Plot the coefficients of each model to visualize the effect of the regularization. Detailed Steps: 1. **Data Preprocessing:** - Load the dataset `diabetes.csv`. - Handle any missing values if present. - Standardize the features (Age, BMI, BloodPressure, SkinThickness, Insulin). 2. **Train-Test Split:** - Split the dataset into training (80%) and testing (20%) sets. 3. **Model Implementation:** - Implement the `LinearRegression` for OLS. - Implement `Ridge` regression with `alpha=1.0`. - Implement `Lasso` regression with `alpha=0.1`. 4. **Training:** - Fit each model on the training set. 5. **Prediction:** - Predict the `Outcome` for the test set using each trained model. 6. **Evaluation:** - Calculate the Mean Squared Error (MSE) for each model on the test set. 7. **Visualization:** - Create a bar plot showing the coefficients of each feature for all three models. Expected Functions: ```python import pandas as pd import numpy as np from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt def load_and_preprocess_data(file_path: str): Load and preprocess the diabetes dataset. Arguments: - file_path: str, path to the csv file Returns: - X_train, X_test, y_train, y_test: Split and preprocessed data # Load data data = pd.read_csv(file_path) # Handle missing values if any (assuming no missing values for simplicity) # Standardize features scaler = StandardScaler() features = [\'Age\', \'BMI\', \'BloodPressure\', \'SkinThickness\', \'Insulin\'] X = scaler.fit_transform(data[features]) y = data[\'Outcome\'] # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_models(X_train, y_train): Train OLS, Ridge, and Lasso regression models. Arguments: - X_train: np.ndarray, training features - y_train: np.ndarray, training target Returns: - models: dict, trained models models = { \'OLS\': LinearRegression(), \'Ridge\': Ridge(alpha=1.0), \'Lasso\': Lasso(alpha=0.1) } for name, model in models.items(): model.fit(X_train, y_train) return models def evaluate_models(models, X_test, y_test): Evaluate OLS, Ridge, and Lasso regression models using MSE. Arguments: - models: dict, trained models - X_test: np.ndarray, test features - y_test: np.ndarray, test target results = {} for name, model in models.items(): y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) results[name] = mse return results def plot_coefficients(models): Plot the coefficients of each feature for OLS, Ridge, and Lasso regression models. Arguments: - models: dict, trained models features = [\'Age\', \'BMI\', \'BloodPressure\', \'SkinThickness\', \'Insulin\'] coeffs = pd.DataFrame(columns=features) for name, model in models.items(): coeffs.loc[name] = model.coef_ coeffs.T.plot(kind=\'bar\') plt.title(\'Coefficients of Each Feature for Different Models\') plt.ylabel(\'Coefficient value\') plt.xlabel(\'Features\') plt.show() def main(file_path: str): X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path) models = train_models(X_train, y_train) results = evaluate_models(models, X_test, y_test) print(\\"Model Evaluation (MSE):\\", results) plot_coefficients(models) ``` **Constraints:** - Do not use any additional libraries apart from `pandas`, `numpy`, `scikit-learn`, and `matplotlib`. - Ensure reproducibility by setting a random seed where applicable.","solution":"import pandas as pd import numpy as np from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt def load_and_preprocess_data(file_path: str): Load and preprocess the diabetes dataset. Arguments: - file_path: str, path to the csv file Returns: - X_train, X_test, y_train, y_test: Split and preprocessed data # Load data data = pd.read_csv(file_path) # Handle missing values if any (assuming no missing values for simplicity) # Standardize features scaler = StandardScaler() features = [\'Age\', \'BMI\', \'BloodPressure\', \'SkinThickness\', \'Insulin\'] X = scaler.fit_transform(data[features]) y = data[\'Outcome\'] # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_models(X_train, y_train): Train OLS, Ridge, and Lasso regression models. Arguments: - X_train: np.ndarray, training features - y_train: np.ndarray, training target Returns: - models: dict, trained models models = { \'OLS\': LinearRegression(), \'Ridge\': Ridge(alpha=1.0), \'Lasso\': Lasso(alpha=0.1) } for name, model in models.items(): model.fit(X_train, y_train) return models def evaluate_models(models, X_test, y_test): Evaluate OLS, Ridge, and Lasso regression models using MSE. Arguments: - models: dict, trained models - X_test: np.ndarray, test features - y_test: np.ndarray, test target Returns: - results: dict, model names as keys and MSE as values results = {} for name, model in models.items(): y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) results[name] = mse return results def plot_coefficients(models): Plot the coefficients of each feature for OLS, Ridge, and Lasso regression models. Arguments: - models: dict, trained models features = [\'Age\', \'BMI\', \'BloodPressure\', \'SkinThickness\', \'Insulin\'] coeffs = pd.DataFrame(columns=features) for name, model in models.items(): coeffs.loc[name] = model.coef_ coeffs.T.plot(kind=\'bar\', figsize=(10, 6)) plt.title(\'Coefficients of Each Feature for Different Models\') plt.ylabel(\'Coefficient value\') plt.xlabel(\'Features\') plt.xticks(rotation=45) plt.show() def main(file_path: str): X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path) models = train_models(X_train, y_train) results = evaluate_models(models, X_test, y_test) print(\\"Model Evaluation (MSE):\\", results) plot_coefficients(models)"},{"question":"**Question** You are tasked with creating a Python script that uses the `zipapp` module to generate an executable Python zip archive from a given directory. Your script should be capable of filtering out certain files based on a specified criteria, adding a custom shebang line for a Python interpreter, and compressing the files. The script should also be able to handle producing the archive directly in memory and optionally writing it to a file. # Objectives 1. **Filter Function** Implement a filter function that excludes all files/directories that start with a dot (\'.\') from being included in the archive. 2. **Function Implementation** Implement a function `create_custom_archive(source_dir, target_zip, interpreter=None, main=None, filter_func=None, compress=False)` that: - Takes in a directory path `source_dir`. - Creates a zip archive and writes it either to a file specified by `target_zip` or to an in-memory bytes object if `target_zip` is `None`. - Adds the specified `interpreter` shebang line. - Sets the `main` function to be executed. - Uses the provided `filter_func` for filtering files from being included in the archive. - Compresses the files if `compress` is `True`. # Input and Output Formats - **Input** - `source_dir` (str): Path to the directory to be archived. - `target_zip` (str or None): Path where the archive should be written or `None` for in-memory. - `interpreter` (str, optional): Interpreter to be specified in the shebang line (e.g., `\\"/usr/bin/env python3\\"`). - `main` (str, optional): Main function to be executed in the format \\"module:function\\". - `filter_func` (Callable, optional): Function to filter files from being included. - `compress` (bool, optional): Whether to compress files in the archive. - **Output** - Writes the archive either to a file or returns a BytesIO object containing the archive if `target_zip` is `None`. # Constraints - `source_dir` must be a valid directory path. - If `target_zip` is provided, it must be a valid writable file path or `None` for in-memory output. - `filter_func` must be a callable function or `None`. - The target directory must contain at least one executable Python script. # Example Usage ```python import io def filter_func(file_path): return not file_path.name.startswith(\'.\') # Create an archive from \\"myapp_directory\\" and save to \\"myapp.pyz\\" create_custom_archive(\'myapp_directory\', \'myapp.pyz\', interpreter=\'/usr/bin/env python3\', main=\'myapp:main\', filter_func=filter_func, compress=True) # Create an archive from \\"myapp_directory\\" in-memory and return a BytesIO object archive_stream = create_custom_archive(\'myapp_directory\', None, interpreter=\'/usr/bin/env python3\', main=\'myapp:main\', filter_func=filter_func, compress=True) ``` Write your solution in Python to achieve the described functionality.","solution":"import os import zipapp from pathlib import Path from io import BytesIO def filter_dot_files(path): Filter function to exclude files or directories starting with a dot (\'.\'). return not path.name.startswith(\'.\') def create_custom_archive(source_dir, target_zip, interpreter=None, main=None, filter_func=None, compress=False): Create a zip archive of a given directory with the specified options. Args: source_dir (str): Path to the directory to be archived. target_zip (str or None): Path where the archive should be written or None for in-memory. interpreter (str, optional): Interpreter to be specified in the shebang line. main (str, optional): Main function to be executed. filter_func (Callable, optional): Function to filter files from being included. compress (bool, optional): Whether to compress files in the archive. Returns: BytesIO: Archive bytes if target_zip is None, otherwise None. source_path = Path(source_dir) if not source_path.is_dir(): raise ValueError(f\\"The source path {source_dir} is not a valid directory\\") # Apply the filter function to the source directory def filter_wrapper(path): return filter_func(path) if filter_func else True # Create a zip context with BytesIO() as zip_bytes_io: zipapp.create_archive( source=source_dir, target=zip_bytes_io if target_zip is None else target_zip, interpreter=interpreter, main=main, filter=filter_wrapper, compressed=compress ) if target_zip is None: return zip_bytes_io.getvalue() return None"},{"question":"You are given a dataset in the form of a pandas DataFrame and need to perform several operations that depend on the global options settings of pandas. Demonstrate your understanding by writing a function `modify_and_verify_options()` that will: 1. Set the display precision of floating-point numbers to 3 digits. 2. Update the mode of how chained assignments are handled to raise warnings. 3. Verify if these options have been correctly set. 4. Retrieve and display the current option for displaying the maximum number of rows in a DataFrame. Function Signature: ```python def modify_and_verify_options(df: pd.DataFrame) -> dict: Modify pandas global options and verify changes. Parameters: df (pd.DataFrame): The input DataFrame. Returns: dict: A dictionary containing the verified options - \'precision\', \'chained_assignment\', and \'max_rows\'. ``` Expected Input: - A pandas DataFrame, `df`. Expected Output: - A dictionary with keys `\'precision\'`, `\'chained_assignment\'`, and `\'max_rows\'`. ``` { \'precision\': <current_display_precision>, \'chained_assignment\': <current_chained_assignment_mode>, \'max_rows\': <current_max_rows_setting> } ``` Requirements and constraints: - Use `pandas.set_option()` to set global options. - Use `pandas.get_option()` to verify the settings. - Ensure that the precision level for displaying floats is 3. - Set the chaining assignment mode to \'warn\'. - Retrieve the current \'display.max_rows\' setting. Here is an example of how your function might be used: ```python import pandas as pd data = {\'A\': [1.123456, 2.234567, 3.345678]} df = pd.DataFrame(data) options_dict = modify_and_verify_options(df) print(options_dict) ``` Expected output (assuming default settings for \'display.max_rows\'): ```python { \'precision\': 3, \'chained_assignment\': \'warn\', \'max_rows\': 60 # This might vary depending on the system\'s settings } ``` Provide the implementation for `modify_and_verify_options()` based on the described requirements.","solution":"import pandas as pd def modify_and_verify_options(df: pd.DataFrame) -> dict: Modify pandas global options and verify changes. Parameters: df (pd.DataFrame): The input DataFrame. Returns: dict: A dictionary containing the verified options - \'precision\', \'chained_assignment\', and \'max_rows\'. # Set the display precision of floating-point numbers to 3 digits pd.set_option(\'display.precision\', 3) # Update the mode of how chained assignments are handled to raise warnings pd.set_option(\'mode.chained_assignment\', \'warn\') # Retrieve the current option for displaying the maximum number of rows max_rows = pd.get_option(\'display.max_rows\') # Verify if these options have been correctly set precision = pd.get_option(\'display.precision\') chained_assignment = pd.get_option(\'mode.chained_assignment\') # Return the options in a dictionary return { \'precision\': precision, \'chained_assignment\': chained_assignment, \'max_rows\': max_rows }"},{"question":"# Seaborn Heatmap Customization You are provided with a dataset containing the performance scores of different models on various tasks. Your goal is to visualize this dataset using Seaborn\'s heatmap functionality. The visualization should include several customizations to make the visualization more informative and visually appealing. Dataset Description: The dataset consists of three columns: - `Model`: The model name. - `Task`: The task name. - `Score`: The score of the model for the given task. Here is a small example of what the data might look like: | Model | Task | Score | |--------|----------|-------| | Model1 | Task1 | 75 | | Model1 | Task2 | 80 | | Model2 | Task1 | 85 | | Model2 | Task2 | 90 | # Instructions 1. **Load the dataset and pivot**: Load the dataset from Seaborn\'s built-in datasets using `sns.load_dataset(\\"glue\\")`, and pivot it to create a DataFrame suitable for a heatmap. The pivoted DataFrame should have models as the index, tasks as the columns, and scores as the values. 2. **Basic Heatmap**: Create a basic heatmap using the pivoted DataFrame. 3. **Annotations**: Annotate the heatmap with text representing the cell values. The text should be formatted to one decimal place. 4. **Lines Between Cells**: Add lines between the cells of the heatmap. 5. **Custom Colormap**: Change the colormap of the heatmap to `gnuplot`. 6. **Colormap Norm**: Set the colormap norm such that the minimum data value is 50 and the maximum data value is 100. 7. **Axes Customization**: Remove the axis labels and move the x-axis labels to the top of the heatmap. Combine all these steps in a function `custom_heatmap` which takes no parameters and produces the customized heatmap as described. Use the provided example dataset for your implementation. # Expected Function Signature: ```python import seaborn as sns def custom_heatmap(): # Load dataset glue = sns.load_dataset(\\"glue\\").pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Create heatmap with customizations ax = sns.heatmap(glue, annot=True, fmt=\\".1f\\", linewidths=.5, cmap=\\"gnuplot\\", vmin=50, vmax=100) # Customize axes ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Call the function to see the heatmap custom_heatmap() ``` **Note:** Ensure you have imported all necessary libraries and your code runs without any errors.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_heatmap(): # Here we\'re creating a small example dataset as described in the task. import pandas as pd data = { \'Model\': [\'Model1\', \'Model1\', \'Model2\', \'Model2\'], \'Task\': [\'Task1\', \'Task2\', \'Task1\', \'Task2\'], \'Score\': [75, 80, 85, 90] } df = pd.DataFrame(data) # Pivoting the dataset pivot_df = df.pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Create the heatmap ax = sns.heatmap(pivot_df, annot=True, fmt=\\".1f\\", linewidths=.5, cmap=\\"gnuplot\\", vmin=50, vmax=100) # Customizing axes ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Show the plot plt.show() # Call the function to see the heatmap custom_heatmap()"},{"question":"# Advanced Configuration File Handling with `configparser` You are given a configuration file that consists of various sections, keys, and values. Sometimes, values in this configuration file can reference other values within the same or different sections using interpolation. Your task is to create a function that can read this configuration file, resolve any possible interpolations, and return a dictionary with all configuration data accurately represented. Configuration File Example ```ini [database] host = localhost port = 3306 user = admin password = secret database_name = test_db [backup] location = /backups frequency = daily user = {database:user} [storage] path = /var/storage quota = 100G used = 40G available = {storage:quota} - {storage:used} ``` # Function Specification Input: - `config_file_path`: a string representing the file path to the configuration file. Output: - Returns a dictionary representing the parsed and interpolated configuration data. Function Signature ```python def parse_config(config_file_path: str) -> dict: pass ``` # Constraints: 1. **Interpolation**: Your function should support basic `{section:key}` interpolation. 2. **Data Types**: Assume all values are strings. Perform no type casting. 3. **Error Handling**: Handle missing files or incorrect keys gracefully. 4. **Library**: Utilize Python\'s `configparser` module. # Example Usage ```python config_file_content = [database] host = localhost port = 3306 user = admin password = secret database_name = test_db [backup] location = /backups frequency = daily user = {database:user} [storage] path = /var/storage quota = 100G used = 40G available = {storage:quota} - {storage:used} with open(\'config.ini\', \'w\') as file: file.write(config_file_content) config_data = parse_config(\'config.ini\') print(config_data) ``` Expected Output: ```python { \'database\': { \'host\': \'localhost\', \'port\': \'3306\', \'user\': \'admin\', \'password\': \'secret\', \'database_name\': \'test_db\' }, \'backup\': { \'location\': \'/backups\', \'frequency\': \'daily\', \'user\': \'admin\' }, \'storage\': { \'path\': \'/var/storage\', \'quota\': \'100G\', \'used\': \'40G\', \'available\': \'100G - 40G\' } } ``` # Notes: 1. The above output showcases how interpolations within values are resolved correctly. 2. Missing or incorrect keys within the configuration should be managed elegantly, without causing crashes. Good luck!","solution":"import configparser def parse_config(config_file_path: str) -> dict: config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation()) config.read(config_file_path) config_dict = {section: dict(config.items(section)) for section in config.sections()} return config_dict"},{"question":"You are required to demonstrate your understanding of seaborn by creating a series of visualizations using the `set_context` function along with other seaborn functionalities. # Data For this exercise, you will use the following dictionary to create a pandas DataFrame: ```python data = { \\"day\\": [\\"Mon\\", \\"Tue\\", \\"Wed\\", \\"Thu\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"], \\"temperature\\": [22, 21, 23, 24, 25, 26, 27], \\"humidity\\": [30, 35, 40, 45, 50, 55, 60] } ``` # Tasks 1. Create a line plot using seaborn to visualize the `temperature` over the `day`. Use the default seaborn context in this plot. 2. Update the context to `notebook` with `font_scale=1.5` and create the same line plot for `temperature` over the `day`. 3. Further modify the context by setting the parameter `rc={\\"lines.linewidth\\": 2.5}` and plot both `temperature` and `humidity` on the same graph over the `day`. Make sure to differentiate the two line plots with different colors and add a legend. 4. Save the final plot as a PNG file called `climate_plot.png`. # Constraints - Ensure each plot is clear and appropriately labeled. - Utilize `seaborn` for all plotting needs and customization. - You are allowed to use additional libraries like `pandas` and `matplotlib` if needed for data processing and saving the file. # Expected Outputs For all tasks, graphical plots should be generated according to the specifications mentioned. The final saved plot must be correctly readable and should depict both temperature and humidity trends distinctly. # Function Definition Define a function `create_climate_plots()` which: - Does not receive any inputs. - Outputs the final plot as described in task 3 saved as `climate_plot.png`. You can utilize additional helper functions within `create_climate_plots()` if necessary. ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_climate_plots(): data = { \\"day\\": [\\"Mon\\", \\"Tue\\", \\"Wed\\", \\"Thu\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"], \\"temperature\\": [22, 21, 23, 24, 25, 26, 27], \\"humidity\\": [30, 35, 40, 45, 50, 55, 60] } df = pd.DataFrame(data) # Task 1 sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.show() # Task 2 sns.set_context(\\"notebook\\", font_scale=1.5) sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.show() # Task 3 sns.set_context(\\"notebook\\", rc={\\"lines.linewidth\\": 2.5}) sns.lineplot(x=\'day\', y=\'temperature\', data=df, label=\'Temperature\', color=\'blue\') sns.lineplot(x=\'day\', y=\'humidity\', data=df, label=\'Humidity\', color=\'green\') plt.legend() plt.savefig(\'climate_plot.png\') ``` # Additional Notes - Ensure to include comments in your code to enhance readability and maintainability. - While solving the problem, think about different styles and their effects on readability and clarity of a plot.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_climate_plots(): # Data preparation data = { \\"day\\": [\\"Mon\\", \\"Tue\\", \\"Wed\\", \\"Thu\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"], \\"temperature\\": [22, 21, 23, 24, 25, 26, 27], \\"humidity\\": [30, 35, 40, 45, 50, 55, 60] } df = pd.DataFrame(data) # Task 1: Default seaborn context sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.title(\'Temperature over Days (Default Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Temperature\') plt.show() # Task 2: notebook context with font_scale=1.5 sns.set_context(\\"notebook\\", font_scale=1.5) sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.title(\'Temperature over Days (Notebook Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Temperature\') plt.show() # Task 3: notebook context with rc parameter sns.set_context(\\"notebook\\", rc={\\"lines.linewidth\\": 2.5}) sns.lineplot(x=\'day\', y=\'temperature\', data=df, label=\'Temperature\', color=\'blue\') sns.lineplot(x=\'day\', y=\'humidity\', data=df, label=\'Humidity\', color=\'green\') plt.title(\'Temperature and Humidity over Days\') plt.xlabel(\'Day\') plt.ylabel(\'Value\') plt.legend() plt.savefig(\'climate_plot.png\') plt.show()"},{"question":"**Objective:** Implement a Python function that takes a list of mixed IPv4 and IPv6 addresses and returns a dictionary categorizing them based on whether they belong to a specific IPv4 or IPv6 network. You are expected to demonstrate your understanding of the ipaddress module, including creating IP address objects, defining networks, and checking for membership. **Function Signature:** ```python def categorize_ips_by_network(ip_list: list, ipv4_network: str, ipv6_network: str) -> dict: Categorizes a list of IP addresses into specified IPv4 and IPv6 networks. Parameters: ip_list (list): A list of IP addresses as strings. ipv4_network (str): An IPv4 network address with prefix length (e.g., \'192.0.2.0/24\'). ipv6_network (str): An IPv6 network address with prefix length (e.g., \'2001:db8::/64\'). Returns: dict: A dictionary with keys \'IPv4\', \'IPv6\', and \'Others\'. Each key maps to a list of IP addresses belonging to the respective network or uncategorized. pass ``` **Input Format:** - `ip_list`: A list of strings, where each string is either an IPv4 or IPv6 address (e.g., `[\'192.0.2.1\', \'2001:db8::1\']`). - `ipv4_network`: A string representing a valid IPv4 network in CIDR notation (e.g., `\'192.0.2.0/24\'`). - `ipv6_network`: A string representing a valid IPv6 network in CIDR notation (e.g., `\'2001:db8::/64\'`). **Output Format:** - A dictionary with three keys: `IPv4`, `IPv6`, and `Others`. The value for each key is a list of IP addresses from `ip_list` that belong to the respective network or are categorized as uncategorized (not part of specified networks). **Constraints:** 1. The input IP addresses and networks will be valid. 2. Keep in mind the potential size of the list and optimize your implementation accordingly. **Example:** ```python ip_list = [\'192.0.2.1\', \'192.0.2.255\', \'2001:db8::1\', \'2001:0db8:0000:0000:0000:0000:0000:1234\', \'10.0.0.1\'] ipv4_network = \'192.0.2.0/24\' ipv6_network = \'2001:db8::/64\' result = categorize_ips_by_network(ip_list, ipv4_network, ipv6_network) ``` **Expected Output:** ```python { \'IPv4\': [\'192.0.2.1\', \'192.0.2.255\'], \'IPv6\': [\'2001:db8::1\', \'2001:0db8:0000:0000:0000:0000:0000:1234\'], \'Others\': [\'10.0.0.1\'] } ``` **Notes:** - Use the `ipaddress` module to create and inspect IP address and network objects. - Ensure your solution is readable, well-organized, and adheres to best coding practices.","solution":"import ipaddress def categorize_ips_by_network(ip_list, ipv4_network, ipv6_network): Categorizes a list of IP addresses into specified IPv4 and IPv6 networks. Parameters: ip_list (list): A list of IP addresses as strings. ipv4_network (str): An IPv4 network address with prefix length (e.g., \'192.0.2.0/24\'). ipv6_network (str): An IPv6 network address with prefix length (e.g., \'2001:db8::/64\'). Returns: dict: A dictionary with keys \'IPv4\', \'IPv6\', and \'Others\'. Each key maps to a list of IP addresses belonging to the respective network or uncategorized. ipv4_network_obj = ipaddress.ip_network(ipv4_network) ipv6_network_obj = ipaddress.ip_network(ipv6_network) result = {\'IPv4\': [], \'IPv6\': [], \'Others\': []} for ip in ip_list: ip_obj = ipaddress.ip_address(ip) if ip_obj.version == 4 and ip_obj in ipv4_network_obj: result[\'IPv4\'].append(ip) elif ip_obj.version == 6 and ip_obj in ipv6_network_obj: result[\'IPv6\'].append(ip) else: result[\'Others\'].append(ip) return result"},{"question":"**Problem Statement: Custom HTML Attribute Collector** In this coding assessment, you are required to write a class `HTMLAttributeCollector` that extends the `html.parser.HTMLParser` class. The goal of this class is to collect all attributes of a specific HTML tag from the HTML data being parsed. You need to: 1. Override the `handle_starttag` method to collect attributes of a given tag. 2. Implement a method `get_attributes()` that returns a dictionary where: - The keys are the tag names. - The values are lists of dictionaries, each corresponding to the attribute name-value pairs of that tag encountered in the HTML data. # Expected Input and Output # Input: - `data` (str): A string containing HTML content. - `tag` (str): The name of the tag whose attributes need to be collected. # Output: - A dictionary containing the collected attributes of the specified tag. # Example **Input:** ```python data = \'\'\' <html> <head> <title>Page Title</title> </head> <body> <h1 class=\\"header\\">Header</h1> <p id=\\"paragraph1\\">This is a paragraph.</p> <p id=\\"paragraph2\\" style=\\"color:blue;\\">This is another paragraph.</p> <a href=\\"https://example.com\\" class=\\"link\\">Example Link</a> </body> </html> \'\'\' tag = \'p\' ``` **Output:** ```python { \'p\': [ {\'id\': \'paragraph1\'}, {\'id\': \'paragraph2\', \'style\': \'color:blue;\'} ] } ``` # Constraints: - The input HTML data will be well-formed. - You should not use any third-party libraries. # Performance Requirements: - Your implementation should be efficient in terms of both time and space complexity. # Implementation Sketch: 1. Define the class `HTMLAttributeCollector` extending the `HTMLParser` class. 2. Override `handle_starttag` to collect attributes of the specified tag. 3. Implement the `get_attributes` method to return the collected attributes. # Template ```python from html.parser import HTMLParser class HTMLAttributeCollector(HTMLParser): def __init__(self, search_tag): super().__init__() self.search_tag = search_tag self.attributes = {self.search_tag: []} def handle_starttag(self, tag, attrs): if tag == self.search_tag: attr_dict = {name: value for name, value in attrs} self.attributes[self.search_tag].append(attr_dict) def get_attributes(self): return self.attributes # Example Usage data = \'\'\' <html> <head> <title>Page Title</title> </head> <body> <h1 class=\\"header\\">Header</h1> <p id=\\"paragraph1\\">This is a paragraph.</p> <p id=\\"paragraph2\\" style=\\"color:blue;\\">This is another paragraph.</p> <a href=\\"https://example.com\\" class=\\"link\\">Example Link</a> </body> </html> \'\'\' tag = \'p\' parser = HTMLAttributeCollector(tag) parser.feed(data) attributes = parser.get_attributes() print(attributes) ``` # Note: Please make sure to test your code with various HTML inputs to ensure robustness.","solution":"from html.parser import HTMLParser class HTMLAttributeCollector(HTMLParser): def __init__(self, search_tag): super().__init__() self.search_tag = search_tag self.attributes = {self.search_tag: []} def handle_starttag(self, tag, attrs): if tag == self.search_tag: attr_dict = {name: value for name, value in attrs} self.attributes[self.search_tag].append(attr_dict) def get_attributes(self): return self.attributes # Example Usage data = \'\'\' <html> <head> <title>Page Title</title> </head> <body> <h1 class=\\"header\\">Header</h1> <p id=\\"paragraph1\\">This is a paragraph.</p> <p id=\\"paragraph2\\" style=\\"color:blue;\\">This is another paragraph.</p> <a href=\\"https://example.com\\" class=\\"link\\">Example Link</a> </body> </html> \'\'\' tag = \'p\' parser = HTMLAttributeCollector(tag) parser.feed(data) attributes = parser.get_attributes() print(attributes)"},{"question":"# Heap-Based Event Scheduler You are tasked with designing an event scheduler using Python\'s `heapq` module that can efficiently manage and execute scheduled events based on their priority. Each event is represented by a tuple containing a timestamp (integer) and a description (string) of the event. Your task is to implement the following functions: 1. **`add_event(scheduler, event)`**: - Input: `scheduler` - a list representing the heap-based event queue. `event` - a tuple `(timestamp, description)` - Output: None - Description: Adds the given event to the scheduler, maintaining the heap invariant. Use `heappush`. 2. **`pop_next_event(scheduler)`**: - Input: `scheduler` - a list representing the heap-based event queue. - Output: The event with the smallest timestamp. - Description: Removes and returns the event with the smallest timestamp from the scheduler. Use `heappop`. If the scheduler is empty, return `None`. 3. **`peek_next_event(scheduler)`**: - Input: `scheduler` - a list representing the heap-based event queue. - Output: The event with the smallest timestamp. - Description: Returns the event with the smallest timestamp without removing it from the scheduler. If the scheduler is empty, return `None`. 4. **`get_later_events(scheduler, timestamp)`**: - Input: `scheduler` - a list representing the heap-based event queue. `timestamp` - an integer timestamp. - Output: A list of all events with timestamps greater than the given timestamp, sorted by their timestamp. - Description: Returns a list of events with timestamps later than the given timestamp. Use appropriate heap operations to retrieve and sort these events. # Constraints - All timestamps are non-negative integers. - Multiple events can have the same timestamp. - The scheduler should efficiently handle up to 100,000 events. # Example Usage ```python scheduler = [] add_event(scheduler, (10, \\"Event A\\")) add_event(scheduler, (5, \\"Event B\\")) add_event(scheduler, (8, \\"Event C\\")) print(pop_next_event(scheduler)) # Output: (5, \\"Event B\\") print(peek_next_event(scheduler)) # Output: (8, \\"Event C\\") print(get_later_events(scheduler, 7)) # Output: [(8, \\"Event C\\"), (10, \\"Event A\\")] ``` # Notes - Ensure that the provided functions maintain the heap invariant. - You may assume that all inputs are valid and follow the specified format. Implement the functions accordingly.","solution":"import heapq def add_event(scheduler, event): Adds the given event to the scheduler, maintaining the heap invariant. :param scheduler: List representing the heap-based event queue. :param event: Tuple (timestamp, description) representing the event. heapq.heappush(scheduler, event) def pop_next_event(scheduler): Removes and returns the event with the smallest timestamp from the scheduler. If the scheduler is empty, returns None. :param scheduler: List representing the heap-based event queue. :return: Tuple (timestamp, description) of the event with the smallest timestamp, or None if empty. if scheduler: return heapq.heappop(scheduler) return None def peek_next_event(scheduler): Returns the event with the smallest timestamp without removing it from the scheduler. If the scheduler is empty, returns None. :param scheduler: List representing the heap-based event queue. :return: Tuple (timestamp, description) of the event with the smallest timestamp, or None if empty. if scheduler: return scheduler[0] return None def get_later_events(scheduler, timestamp): Returns a list of all events with timestamps greater than the given timestamp, sorted by their timestamp. :param scheduler: List representing the heap-based event queue. :param timestamp: Integer representing the timestamp. :return: List of tuples (timestamp, description) for events later than the given timestamp. return sorted([event for event in scheduler if event[0] > timestamp])"},{"question":"<|Analysis Begin|> The provided documentation offers comprehensive details on asynchronous programming using Python\'s `asyncio` library. It covers topics such as enabling debug mode, handling concurrency and multithreading, running blocking code, logging, and detecting improperly handled coroutines and exceptions. It also includes information on various methods and best practices for working with asynchronous code in Python. From this, a challenging and insightful coding assessment question can be designed to test a student\'s understanding of the following concepts: 1. Correct usage of `async` and `await`. 2. Enabling and utilizing debug mode. 3. Scheduling tasks and handling exceptions. 4. Using multithreading with `asyncio`. 5. Running blocking code without blocking the event loop. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Implement a function and a class that demonstrate the correct use of Python\'s `asyncio` features, including creating and running coroutines, handling exceptions, and using multithreading. Task: 1. Implement an asynchronous function `fetch_data` that simulates fetching data with a delay. The function should raise an exception if called with a specific value. 2. Create a class `AsyncProcessor` with the following methods: - `__init__`: Initializes an event loop and a list to store results. - `process_data`: Accepts a list of values, schedules the `fetch_data` function for each value, and handles potential exceptions. Also, enable debug mode for the event loop. - `run`: Runs the event loop until all tasks are completed, retrieves their results, and stores them in the list. Input: - The `fetch_data` function should accept an integer as input. - The `process_data` method of `AsyncProcessor` should accept a list of integers. Output: - The `fetch_data` function should return a string \\"Data X\\" where X is the input integer, or raise an exception if the input integer is -1. - The `process_data` method should return a list of results from `fetch_data`. - The `run` method should execute all tasks and populate the results list, accounting for exceptions. Example: ```python import asyncio async def fetch_data(value: int) -> str: await asyncio.sleep(1) if value == -1: raise ValueError(\\"Invalid value: -1\\") return f\\"Data {value}\\" class AsyncProcessor: def __init__(self): self.loop = asyncio.get_event_loop() self.results = [] self.loop.set_debug(True) async def process_data(self, values: list[int]) -> list[str]: tasks = [] for value in values: task = self.loop.create_task(fetch_data(value)) tasks.append(task) results = [] for task in tasks: try: result = await task results.append(result) except Exception as e: results.append(f\\"Error: {e}\\") return results def run(self, values: list[int]): self.results = asyncio.run(self.process_data(values)) processor = AsyncProcessor() processor.run([1, 2, -1, 3]) print(processor.results) ``` Expected Output:: ``` [\'Data 1\', \'Data 2\', \'Error: Invalid value: -1\', \'Data 3\'] ``` Constraints and Performance Requirements: - The function and class should handle up to 1000 integers efficiently. - The `fetch_data` function must have the described delay and handle exceptions as mentioned. - Ensure your code properly handles the event loop and debug mode settings. Notes: - Enable logging and debug mode to assist in identifying any issues in your implementation. - Make use of `asyncio.run`, `loop.create_task`, and `loop.set_debug` appropriately.","solution":"import asyncio async def fetch_data(value: int) -> str: await asyncio.sleep(1) if value == -1: raise ValueError(\\"Invalid value: -1\\") return f\\"Data {value}\\" class AsyncProcessor: def __init__(self): self.results = [] async def process_data(self, values: list[int]) -> list[str]: tasks = [] for value in values: task = asyncio.create_task(fetch_data(value)) tasks.append(task) results = [] for task in tasks: try: result = await task results.append(result) except Exception as e: results.append(f\\"Error: {e}\\") return results def run(self, values: list[int]): self.results = asyncio.run(self.process_data(values))"},{"question":"# Email Message Manipulation with Python In this coding assessment, you will be required to create and manipulate an email message using Python\'s `email.message.EmailMessage` class. You need to demonstrate your competence in handling email headers, manipulating the payload, and managing multipart messages. Task 1. **Create an email message** with the following specifications: - The subject should be \\"Assessment Task\\". - From address should be \\"instructor@example.com\\". - To address should be \\"student@example.com\\". 2. **Add the following contents** to the email: - A plain text body part that says \\"Hello, please find the attached file.\\" - A file attachment with the filename `info.txt`. You can create this file with some dummy content for testing purposes. 3. **Manipulate the email message**: - Change the subject to \\"Revised Assessment Task\\". - Add a new custom header `X-Custom-Header` with the value \\"CustomValue\\". - Modify the filename of the attached file to `updated_info.txt`. 4. **Output the following**: - Print the entire email message in string format. - Print all header keys and their corresponding values. - Print the content type of the email. Your solution should include appropriate error handling and showcase the use of different methods and properties of the `EmailMessage` class. Follow the final format given below exactly: ```python from email.message import EmailMessage # 1. Create the email message msg = EmailMessage() msg[\'Subject\'] = \\"Assessment Task\\" msg[\'From\'] = \\"instructor@example.com\\" msg[\'To\'] = \\"student@example.com\\" # 2. Add plain text body msg.set_content(\\"Hello, please find the attached file.\\") # 2. Add file attachment with open(\'info.txt\', \'rb\') as file: msg.add_attachment(file.read(), maintype=\'application\', subtype=\'octet-stream\', filename=\'info.txt\') # 3. Manipulate the email message # Change the subject msg.replace_header(\'Subject\', \\"Revised Assessment Task\\") # Add custom header msg.add_header(\'X-Custom-Header\', \'CustomValue\') # Modify the filename of the attached file for attachment in msg.iter_attachments(): if attachment.get_filename() == \'info.txt\': attachment.replace_header(\'Content-Disposition\', \'attachment; filename=\\"updated_info.txt\\"\') # 4. Output # Print the entire email message print(msg.as_string()) # Print all headers print(\\"nHeaders and their values:\\") for key, value in msg.items(): print(key, \\":\\", value) # Print content type of the email print(\\"nContent type of the email:\\", msg.get_content_type()) ``` Input There is no user input required; you will create and manipulate the email entirely within the provided code. Output - The full string representation of the email including headers and body. - All email headers and their values. - The content type of the email. Constraints - You should use the `email.message.EmailMessage` class for all operations. - Ensure that proper handling of email encoding and errors is implemented. **Note:** You may need to install additional libraries or configure your environment to handle email related tasks appropriately.","solution":"from email.message import EmailMessage import os def create_email(): # 1. Create the email message msg = EmailMessage() msg[\'Subject\'] = \\"Assessment Task\\" msg[\'From\'] = \\"instructor@example.com\\" msg[\'To\'] = \\"student@example.com\\" # 2. Add plain text body msg.set_content(\\"Hello, please find the attached file.\\") # 2. Add file attachment # Creating a dummy file for attachment with open(\'info.txt\', \'w\') as file: file.write(\\"This is the content of the info.txt file.\\") with open(\'info.txt\', \'rb\') as file: msg.add_attachment(file.read(), maintype=\'application\', subtype=\'octet-stream\', filename=\'info.txt\') # 3. Manipulate the email message # Change the subject msg.replace_header(\'Subject\', \\"Revised Assessment Task\\") # Add custom header msg.add_header(\'X-Custom-Header\', \'CustomValue\') # Modify the filename of the attached file for attachment in msg.iter_attachments(): if attachment.get_filename() == \'info.txt\': attachment.replace_header(\'Content-Disposition\', \'attachment; filename=\\"updated_info.txt\\"\') # 4. Output full_message = msg.as_string() # Print all headers and their values headers = {key: value for key, value in msg.items()} # Print content type content_type = msg.get_content_type() # Clean up the dummy file os.remove(\'info.txt\') return full_message, headers, content_type"},{"question":"Objective: You are to write a Python function that inspects another function\'s signature and returns a formatted string listing the names of the parameters, their types (if annotated), and their default values (if any). Function Signature: ```python def inspect_function_signature(func: callable) -> str: # Your implementation here ``` Input: - `func`: A Python callable (a function or method) whose signature needs to be inspected. Output: - A string that lists each parameter\'s name, type annotation (if any), and default value (if any), in the order they appear in the function\'s signature. The parameters should be formatted in the style of a function signature, with each parameter on a new line. Constraints: - The function `func` can have positional-only, positional-or-keyword, keyword-only, and variable positional and keyword parameters. - Use appropriate functionalities from the `inspect` module. - Ensure the function can handle functions with no parameters gracefully. - Raise a `ValueError` if the input is not a callable. Examples: 1. For an input function `def foo(a, b: int, c=\'default\', *args, **kwargs): pass`, the output should be: ``` a b: int c=\'default\' *args **kwargs ``` 2. For an input function `async def bar(x: float = 0.0, y=5) -> None: pass`, the output should be: ``` x: float=0.0 y=5 ``` 3. For an input function `def baz(*, k1, k2: str): pass`, the output should be: ``` k1 k2: str ``` Hint: - Utilize the `inspect.signature(func)` to retrieve the signature object for the function. - Explore the `parameters` attribute of the `Signature` object and the `Parameter` class to access parameter details. Implement the function `inspect_function_signature` to achieve the stated objective.","solution":"import inspect from typing import Any def inspect_function_signature(func: callable) -> str: if not callable(func): raise ValueError(\\"Input must be a callable\\") sig = inspect.signature(func) parameters = sig.parameters result = [] for param_name, param in parameters.items(): param_str = param_name if param.annotation is not param.empty: param_str += f\\": {param.annotation.__name__}\\" if param.default is not param.empty: param_str += f\\"={repr(param.default)}\\" if param.kind == param.VAR_POSITIONAL: param_str = \'*\' + param_str if param.kind == param.VAR_KEYWORD: param_str = \'**\' + param_str result.append(param_str) return \'n\'.join(result)"},{"question":"WAV File Manipulation using Python `wave` Module # Objective You are required to implement a Python function that reads an input WAV file, modifies its properties, and writes the modified data to an output WAV file. # Problem Statement Write a function `modify_wav_file(input_file_path: str, output_file_path: str, new_sample_width: int, new_frame_rate: int) -> None` that: 1. Reads an existing WAV file from the specified `input_file_path`. 2. Changes its sample width and frame rate to the values provided by `new_sample_width` and `new_frame_rate` respectively. 3. Writes the modified WAV data to a new file at `output_file_path`. # Input 1. `input_file_path` (str): Path to the input WAV file that needs to be read. 2. `output_file_path` (str): Path where the modified WAV file should be written. 3. `new_sample_width` (int): New sample width in bytes to set for the output WAV file. 4. `new_frame_rate` (int): New frame rate in Hz to set for the output WAV file. # Output The function should create a new WAV file at `output_file_path` with the specified sample width and frame rate. The audio data should remain the same, except for the changes in sample width and frame rate. # Constraints 1. The input WAV file will follow the \\"WAVE_FORMAT_PCM\\" format. 2. The `new_sample_width` and `new_frame_rate` values will be valid integers. 3. The file paths provided will be correct and accessible. # Function Signature ```python def modify_wav_file(input_file_path: str, output_file_path: str, new_sample_width: int, new_frame_rate: int) -> None: pass ``` # Example Usage Consider an input WAV file with the following properties: - Number of channels: 2 - Sample width: 2 bytes - Frame rate: 44100 Hz - Number of frames: 100000 If we call: ```python modify_wav_file(\'input.wav\', \'output.wav\', 1, 22050) ``` The output file should have: - Number of channels: 2 (unchanged) - Sample width: 1 byte (changed) - Frame rate: 22050 Hz (changed) - Number of frames: 100000 (unchanged) # Notes - Ensure that the function handles file I/O operations properly, including file closing. - The audio data is not required to be modified; only sample width and frame rate need to be updated. # Hints - Use the `wave` module methods such as `open()`, `getparams()`, `readframes()`, `setparams()`, and `writeframesraw()` or `writeframes()` to accomplish the task.","solution":"import wave def modify_wav_file(input_file_path: str, output_file_path: str, new_sample_width: int, new_frame_rate: int) -> None: Modifies the sample width and frame rate of a WAV file and writes it to a new file. Args: - input_file_path (str): Path to the input WAV file. - output_file_path (str): Path where the modified WAV file should be written. - new_sample_width (int): New sample width in bytes. - new_frame_rate (int): New frame rate in Hz. with wave.open(input_file_path, \'rb\') as infile: params = infile.getparams() frames = infile.readframes(params.nframes) new_params = params._replace(sampwidth=new_sample_width, framerate=new_frame_rate) with wave.open(output_file_path, \'wb\') as outfile: outfile.setparams(new_params) outfile.writeframes(frames)"},{"question":"# Token Stream Manipulation **Objective:** Write a function `replace_function_names` that takes a Python source code as a string, replaces all function names with a specified new name, and returns the modified source code. **Function Signature:** ```python def replace_function_names(source_code: str, new_name: str) -> str: pass ``` **Parameters:** - `source_code` (str): The original Python source code. - `new_name` (str): The new name to replace all function names with. **Expected Output:** - The function should return the modified Python source code as a string, with all function names replaced by `new_name`. **Constraints:** - You can assume that the input `source_code` is syntactically valid Python code. - The function names should be replaced only within function definitions (i.e., after the keyword `def`). **Example:** ```python # Input: source_code = def say_hello(): print(\\"Hello, World!\\") def add(a, b): return a + b new_name = \\"renamed_function\\" # Output: def renamed_function(): print(\\"Hello, World!\\") def renamed_function(a, b): return a + b ``` **Guidance:** 1. Use the `tokenize` module to tokenize the input source code. 2. Identify the tokens corresponding to function names following the keyword `def`. 3. Modify these tokens to use the `new_name`. 4. Use `tokenize.untokenize` to convert the modified token stream back to a Python source code string. You are allowed to use the `tokenize` module\'s functionalities like `tokenize.tokenize`, `tokenize.untokenize`, and others provided in the documentation. **Note:** Please handle proper indentation and maintain the structure of the original code in the output.","solution":"import tokenize from io import BytesIO def replace_function_names(source_code: str, new_name: str) -> str: Replaces all function names in the source code with the specified new name. Parameters: - source_code (str): The original Python source code. - new_name (str): The new name to replace all function names with. Returns: - str: The modified source code with function names replaced. tokens = tokenize.tokenize(BytesIO(source_code.encode(\'utf-8\')).readline) result = [] in_def = False for token in tokens: if token.type == tokenize.NAME and token.string == \'def\': in_def = True elif in_def and token.type == tokenize.NAME: token = tokenize.TokenInfo(token.type, new_name, token.start, token.end, token.line) in_def=False result.append(token) return tokenize.untokenize(result).decode(\'utf-8\')"},{"question":"**Coding Assessment Question: Seaborn ECDF Plot** **Objective:** Your task is to load a dataset and create specific ECDF plots using Seaborn, demonstrating your comprehension of fundamental and advanced concepts of this package. **Problem Statement:** 1. Load the \'penguins\' dataset from seaborn. 2. Create an ECDF plot for the `flipper_length_mm` with the following customizations: - Show the distribution of `flipper_length_mm` along the x-axis. - Use a hue mapping to differentiate between different species. - Display the plot with the `stat` set to \'count\'. 3. Create another ECDF plot for the same `flipper_length_mm` data: - Show the distribution along the y-axis. - Use a hue mapping to differentiate between different species. - Display the plot with the `complementary` flag set to `True`. **Input:** - None (The dataset will be loaded internally within your solution). - You must use the seaborn `penguins` dataset. **Output:** - The solution should produce two ECDF plots with the specified customizations. **Example Code:** ```python import seaborn as sns; sns.set_theme() # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Plot 1: ECDF with flipper_length_mm along x-axis, hue for species, stat=\'count\' sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", stat=\\"count\\") # Plot 2: ECDF with flipper_length_mm along y-axis, hue for species, complementary=True sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\", hue=\\"species\\", complementary=True) ``` **Constraints:** - Ensure that your submitted code runs without errors. - Use only the seaborn and matplotlib libraries for this task. - Ensure plots are clearly labeled and legible. **Performance Requirements:** - The code should execute within a reasonable time frame (a few seconds).","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset def load_penguins_dataset(): return sns.load_dataset(\\"penguins\\") # Create ECDF plots def create_ecdf_plots(): penguins = load_penguins_dataset() # Plot 1: ECDF with flipper_length_mm along x-axis, hue for species, stat=\'count\' plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"ECDF of Flipper Length (Count)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.legend(title=\\"Species\\") plt.show() # Plot 2: ECDF with flipper_length_mm along y-axis, hue for species, complementary=True plt.figure() sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Complementary ECDF of Flipper Length\\") plt.xlabel(\\"Proportion\\") plt.ylabel(\\"Flipper Length (mm)\\") plt.legend(title=\\"Species\\") plt.show() create_ecdf_plots()"},{"question":"Given the following dataset `mpg` from seaborn, create a visualization with the following requirements: 1. Plot a scatter plot showing the relationship between `horsepower` and `mpg`. 2. Use differently colored markers to distinguish between cars of different origins. 3. Differentiate the filled colors of the markers based on the `weight`. 4. Parametrize the fill properties to add partial transparency. 5. Use jitter to help visualize the data points\' local density. **Dataset Details:** - `horsepower`: Numerical value representing the car\'s horsepower. - `mpg`: The car\'s miles-per-gallon fuel efficiency. - `origin`: Categorical variable indicating the origin of the car (e.g., \'USA\', \'Europe\', \'Japan\'). - `weight`: Numeric variable representing the weight of the car. **Expected Function Signature:** ```python def create_seaborn_plot(): pass ``` **Constraints:** - Use the seaborn library version that supports `seaborn.objects`. - This function does not need to return anything but should display the plot. **Example Usage:** Calling `create_seaborn_plot()` should produce a plot that meets all the specified criteria. You may assume that the seaborn library and the dataset have been loaded as follows: ```python import seaborn.objects as so from seaborn import load_dataset mpg = load_dataset(\\"mpg\\") ``` **Hints:** - Refer to the documentation snippets provided, as they include examples for setting color, fill properties, and jitter.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_seaborn_plot(): # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the scatter plot using seaborn.objects plot = ( so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\", color=\\"origin\\") .add(so.Dot(alpha=0.6), jitter=0.25, marker_size=\\"weight\\") .scale(fill=so.Continuous()) ) # Display the plot plot.show() # Note: You should have the latest seaborn library installed."},{"question":"# Question: Custom Container and Numerical Type Implementation Objective Design a class that demonstrates an understanding of both container types and numerical type emulation in Python. The class should support sequence operations and basic arithmetic operations. Requirements 1. **Container Behavior** - Your class should behave like a list of integers. - Implement methods to support sequence operations like indexing, slicing, length calculation, and iteration. - Ensure your container class supports `append`, `remove`, `sort`, `reverse`, and `extend`. 2. **Numerical Behavior** - Extend your class to support basic arithmetic operations (`+`, `-`, `*`, and `//`) with both integers and other instances of your class. - Ensure that both reflected (r-operation) and in-place (i-operation) versions of these operations are supported. 3. **Other Special Methods** - Implement the `__repr__` and `__str__` methods for a useful string representation. - Implement the `__eq__` method to compare instances of your class based on their contained values. Class Signature ```python class CustomContainer: def __init__(self, elements: list): pass def __getitem__(self, index): pass def __setitem__(self, index, value): pass def __delitem__(self, index): pass def __len__(self): pass def __iter__(self): pass def append(self, value): pass def remove(self, value): pass def sort(self): pass def reverse(self): pass def extend(self, other_list): pass def __add__(self, other): pass def __radd__(self, other): pass def __iadd__(self, other): pass def __sub__(self, other): pass def __rsub__(self, other): pass def __isub__(self, other): pass def __mul__(self, other): pass def __rmul__(self, other): pass def __imul__(self, other): pass def __floordiv__(self, other): pass def __rfloordiv__(self, other): pass def __ifloordiv__(self, other): pass def __eq__(self, other): pass def __repr__(self): pass def __str__(self): pass ``` Example Usage ```python container1 = CustomContainer([1, 2, 3]) container2 = CustomContainer([4, 5, 6]) # Container behavior print(len(container1)) # Output: 3 container1.append(4) print(container1[1:3]) # Output: [2, 3] # Numerical behavior print(container1 + container2) # Output: CustomContainer([1, 2, 3, 4, 5, 6]) container1 += 5 print(container1) # Output: CustomContainer([6, 7, 8, 9]) print(container1 * 2) # Output: CustomContainer([12, 14, 16, 18]) print(container1 == container2) # Output: False ``` # Constraints - Ensure the class can handle edge cases such as empty lists and single-element operations. - Maintain efficiency in your operations, especially for large lists. # Evaluation Criteria - Correctness: The class behaves as specified and passes all given examples. - Code Quality: The implementation is clean, well-documented, and follows Pythonic conventions. - Performance: The operations are efficient and avoid unnecessary computations.","solution":"class CustomContainer: def __init__(self, elements: list): self.elements = elements def __getitem__(self, index): return self.elements[index] def __setitem__(self, index, value): self.elements[index] = value def __delitem__(self, index): del self.elements[index] def __len__(self): return len(self.elements) def __iter__(self): return iter(self.elements) def append(self, value): self.elements.append(value) def remove(self, value): self.elements.remove(value) def sort(self): self.elements.sort() def reverse(self): self.elements.reverse() def extend(self, other_list): self.elements.extend(other_list) def __add__(self, other): if isinstance(other, CustomContainer): return CustomContainer(self.elements + other.elements) elif isinstance(other, int): return CustomContainer([x + other for x in self.elements]) else: raise TypeError(\\"Unsupported type for addition\\") def __radd__(self, other): return self.__add__(other) def __iadd__(self, other): if isinstance(other, CustomContainer): self.elements += other.elements elif isinstance(other, int): self.elements = [x + other for x in self.elements] else: raise TypeError(\\"Unsupported type for in-place addition\\") return self def __sub__(self, other): if isinstance(other, int): return CustomContainer([x - other for x in self.elements]) else: raise TypeError(\\"Unsupported type for subtraction\\") def __rsub__(self, other): if isinstance(other, int): return CustomContainer([other - x for x in self.elements]) else: raise TypeError(\\"Unsupported type for reverse subtraction\\") def __isub__(self, other): if isinstance(other, int): self.elements = [x - other for x in self.elements] else: raise TypeError(\\"Unsupported type for in-place subtraction\\") return self def __mul__(self, other): if isinstance(other, int): return CustomContainer([x * other for x in self.elements]) else: raise TypeError(\\"Unsupported type for multiplication\\") def __rmul__(self, other): return self.__mul__(other) def __imul__(self, other): if isinstance(other, int): self.elements = [x * other for x in self.elements] else: raise TypeError(\\"Unsupported type for in-place multiplication\\") return self def __floordiv__(self, other): if isinstance(other, int): return CustomContainer([x // other for x in self.elements]) else: raise TypeError(\\"Unsupported type for floor division\\") def __rfloordiv__(self, other): if isinstance(other, int): return CustomContainer([other // x for x in self.elements]) else: raise TypeError(\\"Unsupported type for reverse floor division\\") def __ifloordiv__(self, other): if isinstance(other, int): self.elements = [x // other for x in self.elements] else: raise TypeError(\\"Unsupported type for in-place floor division\\") return self def __eq__(self, other): if isinstance(other, CustomContainer): return self.elements == other.elements return False def __repr__(self): return f\\"CustomContainer({self.elements})\\" def __str__(self): return f\\"{self.elements}\\""},{"question":"# Unicode Data Task Problem Statement: You have been tasked with implementing a function that processes a list of Unicode characters and returns a detailed report containing various properties of each character. Specifically, you will use the `unicodedata` module to gather the following information for each character in the input list: 1. The character itself. 2. The official name of the character. 3. The general category of the character. 4. The decimal value (if applicable). 5. The bidirectional class. 6. Whether the character is mirrored. 7. The Unicode normalization forms (NFC, NFD, NFKC, NFKD) of the character. Implement the function `unicode_report(chars: List[str]) -> List[Dict[str, Any]]`. Input: - `chars`: A list of Unicode characters. Example: `[\'A\', \'9\', \'ç\', \'u0660\']` Output: - A list of dictionaries, where each dictionary contains the properties of a character. Example: ```python [ { \\"character\\": \\"A\\", \\"name\\": \\"LATIN CAPITAL LETTER A\\", \\"category\\": \\"Lu\\", \\"decimal\\": None, \\"bidirectional\\": \\"L\\", \\"mirrored\\": 0, \\"NFC\\": \\"A\\", \\"NFD\\": \\"A\\", \\"NFKC\\": \\"A\\", \\"NFKD\\": \\"A\\" }, { \\"character\\": \\"9\\", \\"name\\": \\"DIGIT NINE\\", \\"category\\": \\"Nd\\", \\"decimal\\": 9, \\"bidirectional\\": \\"EN\\", \\"mirrored\\": 0, \\"NFC\\": \\"9\\", \\"NFD\\": \\"9\\", \\"NFKC\\": \\"9\\", \\"NFKD\\": \\"9\\" }, ... ] ``` Constraints: - If the character does not have a certain property (like a decimal value), set the corresponding dictionary value to `None`. - Handle `ValueError` exceptions gracefully when properties are not available. Requirements: - Use the following functions from the `unicodedata` module: - `name` - `category` - `decimal` - `bidirectional` - `mirrored` - `normalize` Example Usage: ```python import unicodedata from typing import List, Dict, Any def unicode_report(chars: List[str]) -> List[Dict[str, Any]]: report = [] for char in chars: char_info = { \\"character\\": char, \\"name\\": unicodedata.name(char, None), \\"category\\": unicodedata.category(char), \\"decimal\\": None, \\"bidirectional\\": unicodedata.bidirectional(char), \\"mirrored\\": unicodedata.mirrored(char), \\"NFC\\": unicodedata.normalize(\'NFC\', char), \\"NFD\\": unicodedata.normalize(\'NFD\', char), \\"NFKC\\": unicodedata.normalize(\'NFKC\', char), \\"NFKD\\": unicodedata.normalize(\'NFKD\', char) } try: char_info[\\"decimal\\"] = unicodedata.decimal(char, None) except ValueError: char_info[\\"decimal\\"] = None report.append(char_info) return report # Example input_chars = [\'A\', \'9\', \'ç\', \'u0660\'] print(unicode_report(input_chars)) ``` This question tests the student\'s ability to use the `unicodedata` module effectively and manage exceptions. It requires an understanding of Unicode properties and normalization.","solution":"import unicodedata from typing import List, Dict, Any def unicode_report(chars: List[str]) -> List[Dict[str, Any]]: report = [] for char in chars: char_info = { \\"character\\": char, \\"name\\": unicodedata.name(char, None), \\"category\\": unicodedata.category(char), \\"decimal\\": None, \\"bidirectional\\": unicodedata.bidirectional(char), \\"mirrored\\": unicodedata.mirrored(char), \\"NFC\\": unicodedata.normalize(\'NFC\', char), \\"NFD\\": unicodedata.normalize(\'NFD\', char), \\"NFKC\\": unicodedata.normalize(\'NFKC\', char), \\"NFKD\\": unicodedata.normalize(\'NFKD\', char) } try: char_info[\\"decimal\\"] = unicodedata.decimal(char, None) except (ValueError, TypeError): char_info[\\"decimal\\"] = None report.append(char_info) return report"},{"question":"**Objective:** Implement a Python program that generates and manipulates UUIDs using the `uuid` module according to specified criteria. Problem Statement: 1. **UUID Generation:** - Generate 10 UUIDs using `uuid4()` and store them in a list. - Generate 1 UUID using `uuid1()`, ensuring it is multiprocessing-safe. If the platform does not support safe UUID generation, manually indicate it. 2. **UUID Transformation:** - Take the list of 10 UUIDs generated using `uuid4()` and convert each to its integer representation, storing the results in another list. - Create a new UUID using `uuid5()`, with the namespace as `uuid.NAMESPACE_DNS` and a name string of your choice. Convert this UUID to a hexadecimal string. 3. **UUID Properties:** - For the `uuid1()` UUID, extract and display the following properties: `bytes`, `bytes_le`, `fields`, `hex`, `int`, `urn`, `variant`, `version`, and `is_safe`. 4. **Validation:** - Write a function `validate_uuids(uuid_list)` that takes a list of UUIDs and returns `True` if all UUIDs are unique. Implement this function to check the list of 10 UUIDs generated in step 1. Constraints: - The UUIDs in the list must all be valid. - Ensure conversions between UUID representations are accurate. - The `validate_uuids` function must have a time complexity of O(n), where `n` is the number of UUIDs in the list. Input: - No input required. Generate UUIDs programmatically within your solution. Output: - Print the following: - The list of 10 UUIDs generated using `uuid4()`. - The `uuid1()` value and its corresponding properties. - The hexadecimal string of the UUID generated using `uuid5()`. - The result of the `validate_uuids` function (either `True` or `False`). Example Output: ``` UUIDs generated using uuid4(): [\'UUID1\', \'UUID2\', ...] # 10 UUIDs Details of uuid1() generation: UUID: UUID(\'...\') Bytes: ... Bytes (little-endian): ... Fields: ... Hex: ... Int: ... URN: ... Variant: RFC_4122 Version: 1 Is Safe: safe UUID generated using uuid5() (hexadecimal form): \'12345678-1234-5678-1234-567812345678\' Validation of generated UUID list: True ``` # Note: - Ensure you handle exceptions and edge cases, such as invalid UUID transformations or unsupported safety information. - The problem tests your understanding of UUID generation, properties, and uniqueness validation using Python\'s `uuid` module.","solution":"import uuid def generate_uuids(): # Generate 10 UUIDs using uuid4() and store them in a list uuid4_list = [uuid.uuid4() for _ in range(10)] # Generate 1 UUID using uuid1(), ensuring it is multiprocessing-safe try: uuid1 = uuid.uuid1() safe = uuid1.is_safe except AttributeError: uuid1 = \'Platform does not support safe UUID generation\' safe = \'N/A\' # Transform the UUIDs generated with uuid4() to integer representation uuid4_int_list = [u.int for u in uuid4_list] # Create a new UUID using uuid5() namespace = uuid.NAMESPACE_DNS name = \\"example.com\\" uuid5 = uuid.uuid5(namespace, name) uuid5_hex = uuid5.hex # Print the UUIDs generated using uuid4() print(\\"UUIDs generated using uuid4():\\") for u in uuid4_list: print(u) # Print details of the uuid1() generation if uuid1 != \'Platform does not support safe UUID generation\': print(\\"nDetails of uuid1() generation:\\") print(\\"UUID:\\", uuid1) print(\\"Bytes:\\", uuid1.bytes) print(\\"Bytes (little-endian):\\", uuid1.bytes_le) print(\\"Fields:\\", uuid1.fields) print(\\"Hex:\\", uuid1.hex) print(\\"Int:\\", uuid1.int) print(\\"URN:\\", uuid1.urn) print(\\"Variant:\\", \\"RFC_4122\\" if uuid1.variant == uuid.RFC_4122 else \\"Unknown\\") print(\\"Version:\\", uuid1.version) print(\\"Is Safe:\\", \\"safe\\" if safe == uuid.SafeUUID.safe else \\"unsafe\\" if safe == uuid.SafeUUID.unsafe else \\"unknown\\") else: print(\\"n\\", uuid1) # Print hex representation of the uuid5 print(\\"nUUID generated using uuid5() (hexadecimal form):\\") print(uuid5_hex) # Validate the list of UUIDs print(\\"nValidation of generated UUID list:\\") print(validate_uuids(uuid4_list)) def validate_uuids(uuid_list): Validate a list of UUIDs. Returns True if all UUIDs are unique, otherwise False. return len(uuid_list) == len(set(uuid_list)) if __name__ == \\"__main__\\": generate_uuids()"},{"question":"**Question: MIME Type Handling with Mailcap** You are tasked with implementing a function that processes MIME types using the `mailcap` module. Given a MIME type, this function should find the appropriate application command to handle files of that type using the mailcap configuration. The function should be named `process_mime_type` and should be designed to: 1. Read the mailcap entries using `mailcap.getcaps()`. 2. Find the appropriate command for the given MIME type using `mailcap.findmatch()`. 3. Handle optional filename and parameter list (`plist`) for the substitution process. 4. Ensure security by sanitizing the inputs, especially the filename, to avoid shell metacharacter issues. # Function Signature ```python def process_mime_type(MIMEtype: str, filename: str = \'/dev/null\', plist: list = []) -> tuple: Processes the given MIME type and returns the appropriate command and mailcap entry if found. Parameters: - MIMEtype: str - The MIME type to be processed. - filename: str - The filename to be substituted in the command. - plist: list - A list of named parameters for substitution. Returns: - tuple: A 2-tuple where the first element is the command line string for handling the file, and the second element is the corresponding mailcap entry dictionary. If no match is found, returns (None, None). ``` # Input - `MIMEtype` (str): The MIME type to process, e.g., \'video/mpeg\'. - `filename` (str): The filename to substitute in the command string. Default is \'/dev/null\'. - `plist` (list): A list of named parameters for substitution in the command string. Each element should be a string in the format \'name=value\'. # Output - (tuple): A 2-tuple where: - The first element is the command line string for handling the file if a match is found, otherwise `None`. - The second element is the corresponding mailcap entry dictionary if a match is found, otherwise `None`. # Example ```python # Example usage result = process_mime_type(\'video/mpeg\', \'example_video.mpeg\', [\'id=1\', \'number=2\', \'total=3\']) print(result) # Expected output could be like (\'xmpeg example_video.mpeg\', {\'view\': \'xmpeg %s\'}) ``` # Constraints 1. Ensure that the filename does not contain disallowed characters (anything other than alphanumerics and \\"@+=:,./-_\\"). 2. The function should handle edge cases and return `(None, None)` if no proper match is found. 3. Assume the presence of standard mailcap file locations as per Unix system conventions. # Hints - Utilize `mailcap.getcaps()` to retrieve the mailcap entries. - Use `mailcap.findmatch()` to find the appropriate application command. - Check for disallowed characters in the filename and parameter list to ensure security.","solution":"import mailcap import re def sanitize_filename(filename: str) -> str: Sanitize the filename to remove disallowed characters. return re.sub(r\'[^a-zA-Z0-9@+=:,./-_]\', \'\', filename) def sanitize_plist(plist: list) -> list: Sanitize the plist to remove disallowed characters from parameters. sanitized_plist = [] for item in plist: name, value = item.split(\'=\') name = re.sub(r\'[^a-zA-Z0-9@+=:,./-_]\', \'\', name) value = re.sub(r\'[^a-zA-Z0-9@+=:,./-_]\', \'\', value) sanitized_plist.append(f\\"{name}={value}\\") return sanitized_plist def process_mime_type(MIMEtype: str, filename: str = \'/dev/null\', plist: list = []) -> tuple: Processes the given MIME type and returns the appropriate command and mailcap entry if found. Parameters: - MIMEtype: str - The MIME type to be processed. - filename: str - The filename to be substituted in the command. - plist: list - A list of named parameters for substitution. Returns: - tuple: A 2-tuple where the first element is the command line string for handling the file, and the second element is the corresponding mailcap entry dictionary. If no match is found, returns (None, None). caps = mailcap.getcaps() sanitized_filename = sanitize_filename(filename) sanitized_plist = sanitize_plist(plist) command, entry = mailcap.findmatch(caps, MIMEtype, filename=sanitized_filename, plist=sanitized_plist) return (command, entry)"},{"question":"# Custom Python Completer You are required to implement a custom tab-completion system using Python\'s `rlcompleter` module. This system should mimic the behavior of Python\'s default completer but with some additional custom features. # Your Task 1. **Create a custom Completion class that extends `rlcompleter.Completer`**: - Override the `complete` method to add additional custom completion logic. - Implement safe evaluation of expressions and handle exceptions gracefully. 2. **Additional Requirement**: - Add a feature to suggest completions from a predefined list of custom commands besides the default Python completions. # Specifications 1. **Input**: - `text` (str): The text to be completed. - `state` (int): The state number of the completion (starting from 0). 2. **Output**: - (str): The state-th completion for the given text. 3. **Constraints**: - Custom commands to be suggested: [\\"custom_command1\\", \\"custom_command2\\", \\"custom_command3\\"] - Ensure safe evaluation of expressions that do not have side-effects (functions should not be evaluated). - Handle any exceptions during the evaluation and return `None` if an exception occurs. # Example ```python import rlcompleter class CustomCompleter(rlcompleter.Completer): def __init__(self, custom_commands=None): super().__init__() self.custom_commands = custom_commands or [] def complete(self, text, state): # Add custom completion logic here pass # Custom commands to be added in completion custom_commands = [\\"custom_command1\\", \\"custom_command2\\", \\"custom_command3\\"] completer = CustomCompleter(custom_commands=custom_commands) print(completer.complete(\\"custom_comm\\", 1)) # Output: custom_command2 (the second completion for \\"custom_comm\\") print(completer.complete(\\"__na\\", 0)) # Output should be a valid Python identifier starting with \\"__na\\" e.g., __name__ ``` Write the implementation of the `CustomCompleter` class and its `complete` method.","solution":"import rlcompleter class CustomCompleter(rlcompleter.Completer): def __init__(self, custom_commands=None): super().__init__() self.custom_commands = custom_commands or [] def complete(self, text, state): # Attempt to get the default completion from rlcompleter.Completer try: default_completion = super().complete(text, state) except Exception: default_completion = None # Combine default completions with custom commands matches = [] if text: matches = [cmd for cmd in self.custom_commands if cmd.startswith(text)] # Create combined matches list combined_matches = [] if matches: combined_matches = matches + [default_completion] else: if default_completion: combined_matches = [default_completion] # Return the state-th completion or None if not available return combined_matches[state] if state < len(combined_matches) else None # Initialize the completer with custom commands custom_commands = [\\"custom_command1\\", \\"custom_command2\\", \\"custom_command3\\"] completer = CustomCompleter(custom_commands=custom_commands)"},{"question":"# Question: URL Content Fetcher and Filter **Objective:** Implement a Python function named `fetch_and_filter_urls` using the `urllib.request` module. This function will accept a list of URLs, fetch the content from each URL, and return only the URLs whose content contains a specified keyword. **Function Signature:** ```python def fetch_and_filter_urls(urls: list, keyword: str) -> list: pass ``` # Instructions: 1. **Input:** - `urls` (list of strings): A list where each string is a URL. - `keyword` (string): The keyword to search for in the content of the URLs. 2. **Output:** - Return a list of URLs (from the input list) whose content contains the specified keyword. 3. **Constraints:** - Use `urllib.request` for opening URLs and fetching their content. - Handle possible exceptions that might occur during the URL request (e.g., HTTP errors, timeouts). - Only consider URLs that return content with a status code of 200. - Assume the content of the URL is in plain text. - The function should efficiently handle a reasonably large list of URLs, up to 100. 4. **Performance Requirements:** - The function should perform efficiently within the constraints provided. - Aim to minimize the number of requests in case of repeated URLs in the input list by caching results. # Example: ```python urls = [ \\"http://example.com/page1\\", \\"http://example.com/page2\\", \\"http://example.com/page3\\" ] keyword = \\"example\\" result = fetch_and_filter_urls(urls, keyword) print(result) # Output may vary based on the content of the URLs ``` # Note: - You may utilize additional libraries like `collections` for caching purposes. - You should not use any libraries for HTTP requests other than `urllib.request`. # Evaluation Criteria: - Correctness: The implementation should accurately filter the URLs based on the keyword in the content. - Robustness: The function should handle different types of exceptions and edge cases gracefully. - Efficiency: The implementation should avoid unnecessary requests and handle up to 100 URLs efficiently. - Code Quality: The code should be clean, readable, and well-commented. Good luck!","solution":"import urllib.request from urllib.error import URLError, HTTPError def fetch_and_filter_urls(urls: list, keyword: str) -> list: Fetches content from a list of URLs and returns only the URLs whose content contains the specified keyword. Args: urls (list of str): A list of URLs to fetch content from. keyword (str): The keyword to search for in the content of the URLs. Returns: list of str: A list of URLs whose content contains the specified keyword. valid_urls = [] for url in urls: try: with urllib.request.urlopen(url) as response: if response.status == 200: content = response.read().decode(\'utf-8\') if keyword in content: valid_urls.append(url) except (HTTPError, URLError): continue except Exception as e: print(f\\"An unexpected error occurred: {e}\\") continue return valid_urls"},{"question":"<|Analysis Begin|> The documentation provided discusses deprecated functions from the \\"old buffer protocol\\" in Python 2.x which are still exposed in Python 3 for compatibility reasons. These functions include: - `PyObject_AsCharBuffer` - `PyObject_AsReadBuffer` - `PyObject_CheckReadBuffer` - `PyObject_AsWriteBuffer` These functions are part of the Stable ABI and are intended for working with memory buffers. The documentation mentions that these functions act as a wrapper around the new buffer protocol available in Python 3 and recommends using `PyObject_GetBuffer` and `PyBuffer_Release` instead. Each function has specific purposes: - `PyObject_AsCharBuffer`: Returns a pointer to a read-only memory location for character-based input. - `PyObject_AsReadBuffer`: Returns a pointer to a read-only memory location containing arbitrary data. - `PyObject_CheckReadBuffer`: Checks if an object supports the single-segment readable buffer interface. - `PyObject_AsWriteBuffer`: Returns a pointer to a writable memory location. Given the nature of these functions and the context of the new buffer protocol in Python 3, I will design a coding question that focuses on understanding buffer interfaces in Python, involving both reading from and writing to buffers, and ensuring proper management of buffer resources. <|Analysis End|> <|Question Begin|> **Question: Implementing and Managing Buffer Interfaces** # Objective: Implement functions that demonstrate your understanding of buffer interfaces in Python 3, focusing on reading from and writing to memory buffers. # Description: You are required to implement two functions: `get_buffer_view` and `write_to_buffer`. These functions will utilize Python\'s buffer protocol to perform read and write operations on an array of integers. 1. **Function `get_buffer_view(array: memoryview) -> tuple`:** - **Input:** A `memoryview` object pointing to an array of integers. - **Output:** A tuple containing: - The starting address of the buffer (use `id` function to get the memory address). - The length of the buffer. - **Behavior:** Create a read-only view of the provided buffer and return the required details. Ensure proper handling of buffer release. 2. **Function `write_to_buffer(array: bytearray, data: list) -> None:** - **Input:** - A `bytearray` object that serves as a writable buffer. - A list of integers to be written into the buffer. - **Output:** None. - **Behavior:** Write the values from the `data` list into the buffer. Ensure the buffer has sufficient space to hold all data and handle any potential errors gracefully. - **Constraints:** - The length of `data` should not exceed the length of the `bytearray`. # Example: ```python # Example usage of get_buffer_view buffer = memoryview(bytearray([1, 2, 3, 4])) address, length = get_buffer_view(buffer) print(address, length) # Output: (Some memory address, 4) # Example usage of write_to_buffer buffer = bytearray(4) write_to_buffer(buffer, [5, 6, 7, 8]) print(buffer) # Output: bytearray(b\'x05x06x07x08\') # Example when data length exceeds buffer length buffer = bytearray(2) try: write_to_buffer(buffer, [1, 2, 3]) except ValueError as e: print(e) # Output: Buffer is too small for the data provided. ``` # Notes: - You can assume that the input `array` for `get_buffer_view` will always be a valid `memoryview` object. - The `write_to_buffer` function should raise a `ValueError` if the length of `data` exceeds the length of the provided `bytearray`. Implement the two functions as described to complete the assessment.","solution":"def get_buffer_view(array: memoryview) -> tuple: Returns the memory address and length of a memoryview buffer. address = id(array.obj) # Get memory address of the backing buffer length = len(array) return (address, length) def write_to_buffer(array: bytearray, data: list) -> None: Writes data to a bytearray buffer. if len(data) > len(array): raise ValueError(\\"Buffer is too small for the data provided.\\") array[:len(data)] = data"},{"question":"# Custom Exception Handling and Chaining In this task, you will demonstrate your understanding of Python\'s exception handling mechanism by creating custom exceptions, raising built-in exceptions, and implementing exception chaining. Problem Statement You are developing a simple inventory management system for a small store. The system keeps track of inventory items and allows operations such as adding new items, removing items, and checking item quantities. You need to implement the following custom exceptions: 1. `InvalidItemError`: Raised when an invalid item (e.g., a None or empty string) is added to the inventory. 2. `OutOfStockError`: Raised when trying to remove an item that is out of stock. 3. `InventoryLimitError`: Raised when an attempt is made to add an item exceeding the inventory\'s capacity limit. Additionally, you will need to handle exception chaining in scenarios where multiple exceptions might occur. Requirements 1. Implement the `CustomException` base class, derived from Python\'s built-in Exception class. 2. Implement the `InvalidItemError`, `OutOfStockError`, and `InventoryLimitError` classes, all derived from `CustomException`. 3. Implement the `Inventory` class with the following methods: - `__init__(self, capacity: int)`: Initializes the inventory with a given capacity. - `add_item(self, item: str, quantity: int)`: Adds the specified quantity of an item to the inventory. - Raise `InvalidItemError` if the item is invalid. - Raise `InventoryLimitError` if adding the item exceeds the inventory\'s capacity. - `remove_item(self, item: str, quantity: int)`: Removes the specified quantity of an item from the inventory. - Raise `OutOfStockError` if the item is out of stock. - `get_quantity(self, item: str) -> int`: Returns the current quantity of the specified item in the inventory. Constraints - The inventory\'s capacity is a positive integer. - The item names are non-empty strings. - Quantities are positive integers. Exception Handling - Implement exception chaining where appropriate (e.g., if a secondary error occurs while handling a primary error). - Provide meaningful error messages for each exception. Example Usage ```python try: inventory = Inventory(100) inventory.add_item(\\"apple\\", 10) inventory.add_item(\\"\\", 5) # Should raise InvalidItemError except InvalidItemError as e: print(e) try: inventory.remove_item(\\"apple\\", 15) # Should raise OutOfStockError except OutOfStockError as e: print(f\\"Out of stock: {e}\\") try: inventory.add_item(\\"banana\\", 200) # Should raise InventoryLimitError except InventoryLimitError as e: print(f\\"Inventory limit exceeded: {e}\\") # Example of exception chaining try: try: inventory.add_item(None, 5) # Should raise InvalidItemError except InvalidItemError as e: raise ValueError(\\"Invalid operation\\") from e # Chaining exception except ValueError as ve: print(f\\"ValueError: {ve}\\") print(f\\"Original exception: {ve.__cause__}\\") ``` Expected Output ``` Invalid item: Item name cannot be empty or None. Out of stock: Not enough apple in inventory. Inventory limit exceeded: Cannot add item as it exceeds inventory capacity. ValueError: Invalid operation Original exception: Invalid item: Item name cannot be empty or None. ``` Implement the following code under the class definitions. ```python class CustomException(Exception): Base class for custom exceptions pass class InvalidItemError(CustomException): Raised when an invalid item is added to the inventory def __init__(self, message=\\"Invalid item: Item name cannot be empty or None.\\"): self.message = message super().__init__(self.message) class OutOfStockError(CustomException): Raised when an item is out of stock def __init__(self, message=\\"Item is out of stock.\\"): self.message = message super().__init__(self.message) class InventoryLimitError(CustomException): Raised when inventory limit is exceeded def __init__(self, message=\\"Inventory limit exceeded.\\"): self.message = message super().__init__(self.message) class Inventory: def __init__(self, capacity: int): self.capacity = capacity self.items = {} self.current_count = 0 def add_item(self, item: str, quantity: int): if not item or item is None: raise InvalidItemError() if self.current_count + quantity > self.capacity: raise InventoryLimitError( \\"Cannot add item as it exceeds inventory capacity.\\" ) if item in self.items: self.items[item] += quantity else: self.items[item] = quantity self.current_count += quantity def remove_item(self, item: str, quantity: int): if item not in self.items or self.items[item] < quantity: raise OutOfStockError(f\\"Not enough {item} in inventory.\\") self.items[item] -= quantity if self.items[item] == 0: del self.items[item] self.current_count -= quantity def get_quantity(self, item: str) -> int: return self.items.get(item, 0) ``` Write your implementation for the classes above and test the provided example usage to verify correctness.","solution":"class CustomException(Exception): Base class for custom exceptions. pass class InvalidItemError(CustomException): Raised when an invalid item is added to the inventory def __init__(self, message=\\"Invalid item: Item name cannot be empty or None.\\"): self.message = message super().__init__(self.message) class OutOfStockError(CustomException): Raised when an item is out of stock def __init__(self, message=\\"Item is out of stock.\\"): self.message = message super().__init__(self.message) class InventoryLimitError(CustomException): Raised when inventory limit is exceeded def __init__(self, message=\\"Inventory limit exceeded.\\"): self.message = message super().__init__(self.message) class Inventory: def __init__(self, capacity: int): self.capacity = capacity self.items = {} self.current_count = 0 def add_item(self, item: str, quantity: int): if not item or item is None: raise InvalidItemError() if self.current_count + quantity > self.capacity: raise InventoryLimitError(\\"Cannot add item as it exceeds inventory capacity.\\") if item in self.items: self.items[item] += quantity else: self.items[item] = quantity self.current_count += quantity def remove_item(self, item: str, quantity: int): if item not in self.items or self.items[item] < quantity: raise OutOfStockError(f\\"Not enough {item} in inventory.\\") self.items[item] -= quantity if self.items[item] == 0: del self.items[item] self.current_count -= quantity def get_quantity(self, item: str) -> int: return self.items.get(item, 0)"},{"question":"**Complex Plotting and Text Annotation with Seaborn** **Objective:** The goal of this assessment is to evaluate your understanding and proficiency with the seaborn library, particularly focusing on creating complex plots with custom text annotations. **Problem Statement:** Using the `glue` dataset from the seaborn library, you are required to create a visualization that satisfies the following criteria: 1. **Data Loading and Preparation:** - Load the `glue` dataset using `seaborn`. It should be pivoted and enriched with an \\"Average\\" score as shown in the documentation. 2. **Plot Requirements:** - Create a scatter plot where the x-axis represents `RTE` scores and the y-axis represents `MRPC` scores. - Each point in the scatter plot should represent a combination of `Model` and `Encoder`. - Map the color of the dots to the `Encoder` variable. - Add model names as text annotations directly above each point. 3. **Text Annotation Customization:** - Ensure that the text is bold. - Horizontally align the text according to the `Encoder` type: - Align text to the left for `LSTM` encoders. - Align text to the right for `Transformer` encoders. - Apply a slight vertical offset of 5 units to the text annotations so they are positioned just above the points. 4. **Bar Chart Requirements:** - Create a horizontal bar chart showing the `Average` score for each model. - Annotate each bar with its respective `Average` score. - Use white-colored text for these annotations. - Right-align the text annotations within the bars and add a horizontal offset of 6 units. **Implementation Constraints:** - Use only seaborn and matplotlib libraries for this task. - The dataset should be manipulated and plotted within the same script or notebook cell. - Ensure the plot titles, axis labels, and legend (where applicable) are appropriately set for clarity. **Expected Input Format:** No inputs are required from the user. The dataset is loaded directly within the code. **Expected Output Format:** The output should be two plots: 1. A scatter plot with the specified customizations. 2. A horizontal bar chart with the specified customizations. Submit your solution as a single Python script or Jupyter notebook that generates the required plots when run. **Example Code for Reference:** ```python import seaborn.objects as so from seaborn import load_dataset # Load and prepare the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Scatter plot with text annotations above points scatter_plot = ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(color=\\"black\\", valign=\\"bottom\\", fontweight=\\"bold\\", offset=5)) .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) # Horizontal bar plot with text annotations on bars bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"white\\", halign=\\"right\\", offset=6)) ) scatter_plot.show() bar_plot.show() ``` Ensure to replace above code with the optimal implementations needed for the requirements mentioned above.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load and prepare the dataset glue = sns.load_dataset(\\"glue\\") glue_pivot = glue.pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") glue_pivot[\\"Average\\"] = glue_pivot.mean(axis=1) # Scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=glue_pivot.reset_index(), x=\\"RTE\\", y=\\"MRPC\\", hue=\\"Encoder\\", s=100) # Add text annotations for i, row in glue_pivot.reset_index().iterrows(): x = row[\\"RTE\\"] y = row[\\"MRPC\\"] model = row[\\"Model\\"] encoder = row[\\"Encoder\\"] align = \'left\' if encoder == \'LSTM\' else \'right\' plt.text(x, y + 0.05, model, fontsize=12, weight=\'bold\', ha=align) plt.title(\\"Scatter plot of RTE vs MRPC Scores by Model and Encoder\\") plt.xlabel(\\"RTE score\\") plt.ylabel(\\"MRPC score\\") plt.legend(title=\'Encoder\') plt.grid(True) plt.show() # Bar plot plt.figure(figsize=(10, 8)) bar_plot = sns.barplot(data=glue_pivot.reset_index().sort_values(\\"Average\\", ascending=False), x=\\"Average\\", y=\\"Model\\", hue=\\"Encoder\\", dodge=False) # Add text annotations for i, row in glue_pivot.reset_index().sort_values(\\"Average\\", ascending=False).iterrows(): avg = row[\\"Average\\"] model = row[\\"Model\\"] plt.text(avg + 0.1, i, str(round(avg, 2)), color=\'white\', ha=\'right\', fontsize=12) plt.title(\\"Average Scores by Model\\") plt.xlabel(\\"Average Score\\") plt.ylabel(\\"Model\\") plt.grid(True) plt.show()"},{"question":"**Objective:** Implement a function `retrieve_unseen_emails` that connects to an IMAP server, authenticates with given credentials, selects the \\"INBOX\\" mailbox, retrieves all unseen (new) email messages, and returns a list of these email messages\' subjects. **Function Signature:** ```python import imaplib def retrieve_unseen_emails(host: str, username: str, password: str) -> list: # Your implementation here pass ``` **Input:** - `host` (str): The IMAP server host (e.g., \'imap.gmail.com\'). - `username` (str): The username for authentication (e.g., an email address). - `password` (str): The password for authentication. **Output:** - Returns a list of strings, where each string is the subject of an unseen email. **Constraints:** - The function should establish an encrypted connection using `IMAP4_SSL`. - The function should handle potential exceptions and ensure the connection is closed properly. - You cannot use external libraries for IMAP operations; only `imaplib` is allowed. **Performance Requirements:** - The function must handle large inboxes efficiently, without causing timeouts or excessive memory usage. - Proper error handling and resource management (e.g., closing connections) should be implemented. # Example Usage: ```python host = \'imap.example.com\' username = \'user@example.com\' password = \'password123\' unseen_emails = retrieve_unseen_emails(host, username, password) print(unseen_emails) ``` # Expected Output Format: A sample output when there are unseen messages might be: ```python [ \'Meeting reminder\', \'Your subscription has been confirmed\', \'Important Update\' ] ``` # Notes: - You might need to handle different server responses and correctly parse the email subjects. - Consider using the `search` command with the criterion `UNSEEN` to find unread messages. - Use the `fetch` command to retrieve the required message headers, extracting the subject from the headers. - Ensure to use appropriate exception handling for network and authentication errors. # Extended Guidance: 1. Establish a secure connection to the IMAP server using `IMAP4_SSL`. 2. Log in to the server using provided credentials. 3. Select the \\"INBOX\\" mailbox. 4. Search for unseen messages using the `UNSEEN` criterion. 5. Fetch the message headers of the unseen messages. 6. Extract and parse the subject of each message from the headers. 7. Return a list of subjects. 8. Implement proper error handling and ensure the connection is properly closed in all cases. # Reference: Refer to the [`imaplib` documentation](https://docs.python.org/3/library/imaplib.html) for further details on using the `IMAP4`, `IMAP4_SSL`, and relevant methods for handling IMAP protocols.","solution":"import imaplib import email def retrieve_unseen_emails(host: str, username: str, password: str) -> list: subjects = [] try: # Connect to the server mail = imaplib.IMAP4_SSL(host) # Login to the account mail.login(username, password) # Select the inbox mail.select(\\"inbox\\") # Search for all unseen emails status, response = mail.search(None, \'UNSEEN\') if status == \'OK\': # Get list of email ids email_ids = response[0].split() for eid in email_ids: # Fetch the email by ID status, msg_data = mail.fetch(eid, \'(RFC822)\') if status == \'OK\': for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) subject = msg[\'subject\'] subjects.append(subject) # Close the connection and logout mail.close() mail.logout() except imaplib.IMAP4.error as e: print(f\\"IMAP error: {e}\\") except Exception as e: print(f\\"General error: {e}\\") return subjects"},{"question":"**Python Coding Assessment Question: Implementing Floating Point Arithmetic Functions** # Objective: Demonstrate your understanding of floating point handling in Python by implementing functions using the provided operations. # Requirements: Implement the following functions using the provided floating point utilities: 1. **check_float(obj):** - **Input:** A Python object `obj`. - **Output:** A boolean indicating whether `obj` is a floating point number or a subtype. 2. **create_float_from_string(string):** - **Input:** A string `string` representing a floating point number. - **Output:** A `float` object created from the input string. - **Constraints:** If the string cannot be converted to a float, raise a `ValueError`. 3. **create_float_from_double(value):** - **Input:** A double precision float `value`. - **Output:** A `float` object created from the input double. - **Constraints:** If creating the float from the double fails, raise a `RuntimeError`. 4. **float_to_double(pyfloat):** - **Input:** A Python float object `pyfloat`. - **Output:** A double representation of `pyfloat`. - **Constraints:** If the input is not a float or cannot be converted, raise a `TypeError`. 5. **get_float_info():** - **Output:** A dictionary containing information about float precision, minimum, and maximum values. - **Constraints:** The dictionary should contain keys `precision`, `min`, and `max`. # Example Outputs: ```python # Example 1: print(check_float(3.14)) # Output: True print(check_float(\\"not_a_float\\")) # Output: False # Example 2: print(create_float_from_string(\\"123.456\\")) # Output: 123.456 print(create_float_from_string(\\"invalid\\")) # Raises ValueError # Example 3: print(create_float_from_double(123.456)) # Output: 123.456 # Example 4: print(float_to_double(123.456)) # Output: 123.456 print(float_to_double(\\"not_a_float\\")) # Raises TypeError # Example 5: print(get_float_info()) # Output: {\'precision\': X, \'min\': Y, \'max\': Z} ``` Note: Replace `X`, `Y`, and `Z` with actual values retrieved from the corresponding function calls. # Code Skeleton ```python def check_float(obj): # Implement the function based on PyFloat_Check and PyFloat_CheckExact pass def create_float_from_string(string): # Implement the function based on PyFloat_FromString pass def create_float_from_double(value): # Implement the function based on PyFloat_FromDouble pass def float_to_double(pyfloat): # Implement the function based on PyFloat_AsDouble and PyFloat_AS_DOUBLE pass def get_float_info(): # Implement the function based on PyFloat_GetInfo, PyFloat_GetMax, and PyFloat_GetMin pass ``` # Performance Requirements: - Ensure that all function implementations handle errors and exceptions appropriately. - Optimize the code for performance where possible, avoiding unnecessary computations. **Good luck!**","solution":"import sys def check_float(obj): Returns True if obj is a float or a float subtype, else False. return isinstance(obj, float) def create_float_from_string(string): Returns a float object created from the input string. Raises ValueError if the string cannot be converted to a float. try: return float(string) except ValueError as e: raise ValueError(\\"The string cannot be converted to a float\\") from e def create_float_from_double(value): Returns a float object created from the input double (float). if not isinstance(value, (float, int)): raise RuntimeError(\\"Input value is not a double (float or int)\\") return float(value) def float_to_double(pyfloat): Returns a double representation of pyfloat. Raises TypeError if input is not a float. if not isinstance(pyfloat, float): raise TypeError(\\"Input value is not a float\\") return float(pyfloat) def get_float_info(): Returns a dictionary containing information about float precision, min, and max values. return { \'precision\': sys.float_info.epsilon, \'min\': sys.float_info.min, \'max\': sys.float_info.max }"},{"question":"**Objective:** You are to implement a function that simulates a multithreaded scenario using the `_thread` module\'s threading and synchronization primitives. Specifically, you will create threads that increment a shared counter in a synchronized manner and an additional thread that interrupts the main thread based on certain conditions. **Problem Statement:** Implement a function `simulate_multithreading()`. This function should perform the following tasks: 1. Create a shared counter that will be incremented by multiple threads. 2. Create 5 threads, each incrementing the counter 100 times. 3. Use locks to synchronize access to the shared counter to prevent race conditions. 4. Create an additional thread that waits until the counter reaches 250, then interrupts the main thread. The function does not need any input and should print the value of the shared counter at the end, which should always be 500 if synchronization is correct. If the counter reaches 250, print a message \\"Main thread interrupted.\\" when the interruption occurs. **Constraints:** - Use only the `_thread` module for threading primitives. - Ensure that the counter is updated safely without race conditions. - Handle the interruption of the main thread correctly. **Function Signature:** ```python def simulate_multithreading(): pass ``` **Expected Output:** - A print statement showing the final value of the counter: `Final counter value: 500` - If the counter reaches 250, it should also print: `Main thread interrupted.` ```python import _thread import time def simulate_multithreading(): def increment_counter(lock, counter, increments): for _ in range(increments): with lock: counter[0] += 1 if counter[0] == 250: _thread.interrupt_main() counter = [0] lock = _thread.allocate_lock() threads = [] for _ in range(5): thread = _thread.start_new_thread(increment_counter, (lock, counter, 100)) threads.append(thread) try: # Join threads (dummy mechanism, joining in _thread is not directly supported) while counter[0] < 500: time.sleep(0.01) except KeyboardInterrupt: print(\\"Main thread interrupted.\\") print(f\\"Final counter value: {counter[0]}\\") ```","solution":"import _thread import time def simulate_multithreading(): def increment_counter(lock, counter, increments): for _ in range(increments): with lock: counter[0] += 1 if counter[0] == 250: _thread.interrupt_main() counter = [0] lock = _thread.allocate_lock() threads = [] for _ in range(5): thread = _thread.start_new_thread(increment_counter, (lock, counter, 100)) threads.append(thread) try: # Join threads (dummy mechanism, joining in _thread is not directly supported) while counter[0] < 500: time.sleep(0.01) except KeyboardInterrupt: print(\\"Main thread interrupted.\\") print(f\\"Final counter value: {counter[0]}\\")"},{"question":"You have been asked to implement a Python function that maintains an event log sorted by timestamp, using the `bisect` module\'s functionality. The event log will store events represented as dictionaries with a name and a timestamp, e.g., `{\\"name\\": \\"event1\\", \\"timestamp\\": 1620118800}`. Your task is to write a class `EventLog` which allows for adding events, fetching the latest event, and fetching all events within a specified time range. Class Specification 1. **Class Name:** `EventLog` 2. **Methods:** - **`add_event(self, event: dict) -> None`** - **Description:** Adds a new event to the log, maintaining the log sorted by timestamp. - **Input:** A dictionary `event` with keys: - `\\"name\\"`: string, event name (e.g., \\"event1\\") - `\\"timestamp\\"`: integer, POSIX timestamp (e.g., 1620118800) - **Output:** None - **`latest_event(self) -> dict`** - **Description:** Fetches the latest event (i.e., event with the highest timestamp). - **Output:** A dictionary representing the latest event in the log. If the log is empty, return `None`. - **`events_in_range(self, start_timestamp: int, end_timestamp: int) -> list`** - **Description:** Retrieves all events with timestamps in the inclusive range `[start_timestamp, end_timestamp]`. - **Input:** - `start_timestamp` (integer): start of the range. - `end_timestamp` (integer): end of the range. - **Output:** A list of dictionaries representing events within the specified range. Example Usage: ```python log = EventLog() log.add_event({\\"name\\": \\"event1\\", \\"timestamp\\": 1620118800}) log.add_event({\\"name\\": \\"event2\\", \\"timestamp\\": 1620119900}) print(log.latest_event()) # Output: {\\"name\\": \\"event2\\", \\"timestamp\\": 1620119900} print(log.events_in_range(1620118700, 1620119800)) # Output: [{\\"name\\": \\"event1\\", \\"timestamp\\": 1620118800}, {\\"name\\": \\"event2\\", \\"timestamp\\": 1620119900}] ``` Additional Constraints: - The log should be maintained as a list of events. - The `add_event` method should ensure the log remains sorted by timestamp using the appropriate bisect method. - Consider edge cases, such as adding events with duplicate timestamps. **Note:** The input format and constraints are designed such that the function implementations will need to be efficient, and the bisect module should help in maintaining the required performance.","solution":"import bisect class EventLog: def __init__(self): self.events = [] def add_event(self, event: dict) -> None: Adds a new event to the log, maintaining the log sorted by timestamp. # Extract the timestamp for sorting purposes event_timestamp = event[\'timestamp\'] # Use bisect to find the insertion position to keep the list sorted position = bisect.bisect_right([e[\'timestamp\'] for e in self.events], event_timestamp) # Insert the event at the determined position self.events.insert(position, event) def latest_event(self) -> dict: Fetches the latest event (i.e., event with the highest timestamp). If the log is empty, return None. if not self.events: return None return self.events[-1] def events_in_range(self, start_timestamp: int, end_timestamp: int) -> list: Retrieves all events with timestamps in the inclusive range [start_timestamp, end_timestamp]. start_pos = bisect.bisect_left([e[\'timestamp\'] for e in self.events], start_timestamp) end_pos = bisect.bisect_right([e[\'timestamp\'] for e in self.events], end_timestamp) return self.events[start_pos:end_pos]"},{"question":"**Objective:** Use the `fileinput` module to process multiple text files, count the total number of lines across all files, and handle any compressed file formats correctly. **Task:** Write a Python function `process_files(file_list: List[str], output_encoding: str = \\"utf-8\\") -> int` to read lines from multiple text files specified in the `file_list`. The function should handle compressed files (with `.gz` or `.bz2` extensions) transparently and return the total number of lines read across all files. **Function Signature:** ```python from typing import List import fileinput def process_files(file_list: List[str], output_encoding: str = \\"utf-8\\") -> int: pass ``` **Requirements:** 1. Use the `fileinput.input()` function with the `hook_compressed` open hook to handle compressed files. 2. Implement the function using a context manager to ensure proper handling of file resources. 3. The function must read all lines from the input files and return the total number of lines. 4. The function should handle text files with different encodings by using the specified `output_encoding` parameter. **Constraints:** - All files in `file_list` are either normal text files, gzipped files, or bzipped files. - Assume all files (and their compressed versions) are small enough to fit into memory if necessary. - The function should handle errors gracefully by skipping unreadable files and printing an error message without terminating. **Example Usage:** ```python files = [\\"example1.txt\\", \\"example2.txt\\", \\"example3.gz\\", \\"example4.bz2\\"] total_lines = process_files(files) print(total_lines) # Output should be the total number of lines across the provided files ``` **Notes:** - The `fileinput.input()` function\'s `hook_compressed` helps transparently read compressed files. - Ensure the function returns the correct count of lines even if some files are unreadable. - Use appropriate exception handling to manage file read errors gracefully.","solution":"from typing import List import fileinput import gzip import bz2 def hook_compressed(filename, mode): Helper function to open compressed files transparently. if filename.endswith(\'.gz\'): return gzip.open(filename, mode + \'t\') elif filename.endswith(\'.bz2\'): return bz2.open(filename, mode + \'t\') else: return open(filename, mode) def process_files(file_list: List[str], output_encoding: str = \\"utf-8\\") -> int: total_lines = 0 try: with fileinput.input(files=file_list, openhook=hook_compressed) as f: for line in f: total_lines += 1 except Exception as e: print(f\\"Error processing files: {e}\\") return total_lines"},{"question":"# PyTorch Coding Assessment Objective: You are required to implement a function using PyTorch that performs a series of tensor operations including tensor creation, reshaping, random sampling, and pointwise operations. Problem Statement: Implement a function `tensor_operations` with the following specification: ```python def tensor_operations(dim1, dim2, value, p, n): Perform a series of PyTorch tensor operations. Parameters: dim1 (int): The first dimension size for tensor creation. dim2 (int): The second dimension size for tensor creation. value (float): A value to be added to a tensor. p (float): Probability value for Bernoulli sampling. n (int): Number of elements for random permutation and conditions. Returns: torch.Tensor: The resulting tensor after performing specified operations. pass ``` Steps to Implement: 1. **Tensor Creation:** - Create a tensor of size `(dim1, dim2)` filled with zeros. 2. **Random Value Initialization:** - Initialize a tensor of the same size with random values uniformly drawn between 0 and 1. 3. **Concatenation:** - Concatenate the above two tensors along the first dimension. 4. **Add Value:** - Add `value` to every element in the concatenated tensor. 5. **Bernoulli Sampling:** - Use Bernoulli sampling with probability `p` to generate a mask tensor of the same shape. 6. **Apply Mask:** - Apply the mask to the concatenated tensor such that elements with mask value `1` remain unchanged and those with mask value `0` are set to zero. 7. **Reshape Operation:** - Reshape the masked tensor to have `(n, -1)` shape. 8. **Conditional Indexing:** - Extract and return all elements from the reshaped tensor that are greater than half the max value in the reshaped tensor. Constraints: - Ensure that all tensor operations are performed using PyTorch. - The values for `dim1`, `dim2`, and `n` will be positive integers. - The `value` will be a floating-point number. - The probability `p` will be a float between `0` and `1`. Example: ```python import torch def tensor_operations(dim1, dim2, value, p, n): # Step 1: Create a zero tensor tensor1 = torch.zeros(dim1, dim2) # Step 2: Create a tensor with random values uniformly drawn between 0 and 1 tensor2 = torch.rand(dim1, dim2) # Step 3: Concatenate tensor1 and tensor2 along dimension 0 concatenated_tensor = torch.cat((tensor1, tensor2), dim=0) # Step 4: Add the value to every element in the tensor modified_tensor = concatenated_tensor + value # Step 5: Create a Bernoulli mask with probability p mask = torch.bernoulli(torch.full(modified_tensor.shape, p)) # Step 6: Apply the mask to the tensor masked_tensor = modified_tensor * mask # Step 7: Reshape the masked tensor to shape (n, -1) reshaped_tensor = masked_tensor.view(n, -1) # Step 8: Extract and return elements greater than half of the max value result = reshaped_tensor[reshaped_tensor > reshaped_tensor.max() / 2] return result ``` Given `dim1=2`, `dim2=3`, `value=1.0`, `p=0.5`, and `n=3`, you should verify the result as per the function’s logic.","solution":"import torch def tensor_operations(dim1, dim2, value, p, n): Perform a series of PyTorch tensor operations. Parameters: dim1 (int): The first dimension size for tensor creation. dim2 (int): The second dimension size for tensor creation. value (float): A value to be added to a tensor. p (float): Probability value for Bernoulli sampling. n (int): Number of elements for random permutation and conditions. Returns: torch.Tensor: The resulting tensor after performing specified operations. # Step 1: Create a zero tensor tensor1 = torch.zeros(dim1, dim2) # Step 2: Create a tensor with random values uniformly drawn between 0 and 1 tensor2 = torch.rand(dim1, dim2) # Step 3: Concatenate tensor1 and tensor2 along dimension 0 concatenated_tensor = torch.cat((tensor1, tensor2), dim=0) # Step 4: Add the value to every element in the tensor modified_tensor = concatenated_tensor + value # Step 5: Create a Bernoulli mask with probability p mask = torch.bernoulli(torch.full(modified_tensor.shape, p)) # Step 6: Apply the mask to the tensor masked_tensor = modified_tensor * mask # Step 7: Reshape the masked tensor to shape (n, -1) reshaped_tensor = masked_tensor.view(n, -1) # Step 8: Extract and return elements greater than half of the max value result = reshaped_tensor[reshaped_tensor > reshaped_tensor.max() / 2] return result"},{"question":"# Python Coding Assessment Question: Advanced Array Slicing Objective: Create a Python function that extracts specific subarrays from a given 2D NumPy array using slice objects. This function will be assessed on its ability to correctly interpret and apply slice notation to extract the required subarrays. Problem Statement: You are given a 2D NumPy array and three slice objects. Your task is to write a function `extract_slices` that takes four arguments: 1. The 2D NumPy array `arr`. 2. A slice object `slice_row` representing rows to include. 3. A slice object `slice_col` representing columns to include. 4. A Boolean flag `flatten_output` that indicates whether the final extracted subarray should be returned as a flattened 1D array (if True) or as a 2D array (if False). The function should return the subarray extracted according to the provided slices. Function Signature: ```python import numpy as np def extract_slices(arr: np.ndarray, slice_row: slice, slice_col: slice, flatten_output: bool) -> np.ndarray: pass ``` Input: - `arr` (np.ndarray): A 2D NumPy array of shape (m, n), where 1 <= m, n <= 10^3. - `slice_row` (slice): A slice object for selecting the rows. - `slice_col` (slice): A slice object for selecting the columns. - `flatten_output` (bool): If True, the function should return a 1D array, otherwise a 2D array. Output: - np.ndarray: The extracted subarray according to the given slices, either as a 2D array or flattened to 1D, depending on the `flatten_output` flag. Example: ```python import numpy as np # Example array arr = np.array([ [1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25] ]) # Slices slice_row = slice(1, 4, 1) # rows from 1 to 3 slice_col = slice(2, 5, 1) # columns from 2 to 4 # Function call result = extract_slices(arr, slice_row, slice_col, False) print(result) # Output: # array([[ 8, 9, 10], # [13, 14, 15], # [18, 19, 20]]) result_flat = extract_slices(arr, slice_row, slice_col, True) print(result_flat) # Output: # array([ 8, 9, 10, 13, 14, 15, 18, 19, 20]) ``` Constraints: - Do not use library functions to directly extract the sliced array except standard slicing provided by NumPy. - Ensure that your function can handle edge cases where the slices may be out of bounds. # Notes: - The slice objects must be correctly handled to extract subarrays according to typical Python slicing semantics. - If a slice goes out of bounds, it should be correctly interpreted as a valid slice within the actual boundaries of the array. Good luck, and happy coding!","solution":"import numpy as np def extract_slices(arr: np.ndarray, slice_row: slice, slice_col: slice, flatten_output: bool) -> np.ndarray: Extracts a subarray from the given 2D numpy array using the provided row and column slices. flattens the output array if flatten_output is True. Parameters: - arr: 2D numpy array. - slice_row: slice object for selecting rows. - slice_col: slice object for selecting columns. - flatten_output: bool indicating whether the output should be flattened to 1D. Returns: - Extracted subarray as a 2D or 1D numpy array. subarray = arr[slice_row, slice_col] if flatten_output: return subarray.flatten() return subarray"},{"question":"We will assess your understanding of PyTorch\'s `torch.func` functionalities, particularly involving gradient and Jacobian computations on neural network module parameters. Ensure you follow the instructions precisely, and make sure your code is well-commented. **Problem Statement:** You are provided with a simple feedforward neural network defined using `torch.nn.Module`. Your task is to: 1. Implement the neural network. 2. Use PyTorch\'s `torch.func` utilities to compute the Jacobian matrix of the network\'s output with respect to its parameters. **Instructions:** 1. Define a `SimpleNN` class that inherits from `torch.nn.Module`. This class should contain: - An `__init__` method that initializes two linear layers. - A `forward` method, which defines the forward pass of the network using ReLU activations. 2. Implement a function `compute_jacobian` that: - Takes as input an instance of `SimpleNN` and an input tensor `x`. - Returns the Jacobian matrix of the network\'s output with respect to its parameters. 3. Use the `torch.func.functional_call` method to compute the forward pass with replaced parameters for Jacobian computation. 4. Generate random input using `torch.randn` and validate that your Jacobian computation works correctly by checking the shape of the Jacobian matrix. **Expected Input and Output Formats:** - The `compute_jacobian` function should take an input tensor `x` of shape `(input_size,)`. - The function should output a Jacobian tensor of appropriate shape, depending on the model\'s parameters and the input size. **Constraints:** - You must use `torch.func.jacrev` for computing the Jacobian. - Your implementation should be clearly written and optimized for readability. - Ensure that your PyTorch version supports `torch.func`. **Example Code:** ```python import torch from torch.func import jacrev, functional_call # Part 1: Define the SimpleNN class class SimpleNN(torch.nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(input_size, hidden_size) self.fc2 = torch.nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Part 2: Implement the compute_jacobian function def compute_jacobian(model, x): def f(params, x): return functional_call(model, params, x) jac = jacrev(f)(dict(model.named_parameters()), x) return jac # Part 3: Validate the implementation input_size = 3 hidden_size = 5 output_size = 2 x = torch.randn(input_size) model = SimpleNN(input_size, hidden_size, output_size) jacobian = compute_jacobian(model, x) print(\\"Jacobian shape:\\", {k: v.shape for k, v in jacobian.items()}) ``` **Note:** - In the provided example, validate that the Jacobian shape aligns with expectations, for example, by asserting expected dimensions. This will test your understanding of PyTorch\'s functional API for gradient computation and applying it to neural network models. Write clean and efficient code to ensure it meets the requirements and constraints.","solution":"import torch from torch.func import jacrev, functional_call # Part 1: Define the SimpleNN class class SimpleNN(torch.nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(input_size, hidden_size) self.fc2 = torch.nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Part 2: Implement the compute_jacobian function def compute_jacobian(model, x): # Define the inner function to call with replaced parameters def model_with_params(params, x): return functional_call(model, params, (x,)) # Ensure the model is in evaluation mode model.eval() # Get the Jacobian using jacrev jac = jacrev(model_with_params)(dict(model.named_parameters()), x) return jac"},{"question":"# Question 1: Custom Representation with `reprlib` You are tasked with creating a custom representation for a nested data structure that includes lists, dictionaries, and sets. You will utilize the `reprlib` module to ensure that these structures do not produce excessively lengthy representations. Requirements: 1. Create a class `CustomRepr` that subclasses `reprlib.Repr`. 2. Override or extend the methods needed to handle custom representations for: - Lists - Dictionaries - Sets 3. Ensure that representation of these structures does not exceed **max length of 4 elements**. 4. Adjust the maximum length for string representations to **10 characters**. Any excess characters should be truncated with ellipses (`...`). Constraints: - You should handle nested structures appropriately. - Use `reprlib.repr(obj)` to generate representations for objects within the limits. - Ensure your implementation is robust and can handle different data types without crashing. Input and Output: - **Input**: An object which can be a nested combination of lists, dictionaries, sets, and strings. - **Output**: A string similar to the built-in `repr()` but limited in size as specified. Example: ```python from reprlib import Repr class CustomRepr(Repr): def __init__(self): super().__init__() self.maxlist = 4 self.maxdict = 4 self.maxset = 4 self.maxstring = 10 def repr_list(self, obj, level): # Custom logic for lists return super().repr_list(obj, level) def repr_dict(self, obj, level): # Custom logic for dictionaries return super().repr_dict(obj, level) def repr_set(self, obj, level): # Custom logic for sets return super().repr_set(obj, level) # Testing the implementation custom_repr = CustomRepr().repr nested_structure = { \\"key1\\": \\"This is a very long string that should be truncated.\\", \\"key2\\": [1, 2, 3, 4, 5, 6], \\"key3\\": { \\"subkey1\\": set(range(10)), \\"subkey2\\": [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"] } } print(custom_repr(nested_structure)) ``` **Expected Output** (example): ``` {\'key1\': \'This is a...\', \'key2\': [1, 2, 3, 4, ...], \'key3\': {\'subkey1\': {0, 1, 2, 3, ...}, \'subkey2\': [\'a\', \'b\', \'c\', \'d\', ...]}} ``` Note: The output should respect the constraints given and efficiently handle nested data structures with the specified limits.","solution":"import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxlist = 4 self.maxdict = 4 self.maxset = 4 self.maxstring = 10 def repr_list(self, obj, level): return super().repr_list(obj, level) def repr_dict(self, obj, level): return super().repr_dict(obj, level) def repr_set(self, obj, level): return super().repr_set(obj, level) def repr_str(self, obj, level): if len(obj) > self.maxstring: return repr(obj[:self.maxstring] + \'...\') return repr(obj) # Usage example custom_repr = CustomRepr().repr nested_structure = { \\"key1\\": \\"This is a very long string that should be truncated.\\", \\"key2\\": [1, 2, 3, 4, 5, 6], \\"key3\\": { \\"subkey1\\": set(range(10)), \\"subkey2\\": [\\"a\\", \\"b\\", \\"c\\", \\"d\\", \\"e\\"] } } print(custom_repr(nested_structure))"},{"question":"**Title: Implement Custom Module Importer in Python** **Problem Statement:** You are assigned to create a Python function that mimics the import mechanism of the built-in `importlib.import_module` function, but with additional features. Specifically, your function should be able to import a module by its name and optionally reload it if requested. **Function Signature:** ```python def custom_import(module_name: str, reload: bool = False) -> object: pass ``` **Input:** - `module_name` (str): The name of the module to import. It could be a top-level module or a submodule (e.g., `\'os\'` or `\'collections.defaultdict\'`). - `reload` (bool): A boolean flag indicating whether the module should be reloaded if it is already imported. Default is `False`. **Output:** - Returns the module object corresponding to the imported module. **Requirements:** 1. Your function should attempt to import the module by its name. 2. If the reload flag is `True` and the module is already imported, the module should be reloaded. 3. The function should handle both absolute and relative imports. 4. Proper error handling should be implemented. If an import fails, the function should raise an `ImportError` with a message indicating what went wrong. **Constraints:** - Do not use the built-in `importlib` module or the built-in `__import__` function directly in your implementation. - Your solution should be efficient in terms of both time and space complexity. **Example:** ```python # Example 1: Importing a top-level module module = custom_import(\'os\') print(module.name) # Output should be \'os\' # Example 2: Importing a submodule module = custom_import(\'collections.defaultdict\') print(module.__name__) # Output should be \'collections.defaultdict\' # Example 3: Reloading a module module = custom_import(\'math\', reload=True) print(module.__name__) # Output should be \'math\' ``` **Notes:** - You may implement helper functions as needed to achieve the desired functionality. - The `reload` functionality should mimic the behavior of `importlib.reload`. **Hint:** Consider using the functions from the `imp` module for handling the import and reload operations, as they provide lower-level access to the mechanisms described in the documentation.","solution":"import sys import imp def custom_import(module_name: str, reload: bool = False) -> object: Imports a module by its name with an option to reload it if it\'s already imported. Args: module_name (str): The name of the module to import. reload (bool): Whether to reload the module if it\'s already imported. Default is False. Returns: object: The module object corresponding to the imported module. Raises: ImportError: If the module cannot be imported. try: module = sys.modules.get(module_name) if module is None: module = imp.load_module(module_name, *imp.find_module(module_name.split(\'.\')[0])) if reload: module = imp.reload(module) return module except ImportError as e: raise ImportError(f\\"Failed to import module {module_name}: {e}\\") except Exception as e: raise ImportError(f\\"An unexpected error occurred while importing module {module_name}: {e}\\")"},{"question":"# PyTorch CUDAGraph Trees Assessment **Objective**: The aim of this task is to assess the student\'s comprehension and ability to implement PyTorch\'s CUDAGraph Trees functionality, which is designed to optimize GPU operations by reducing CPU overhead. **Problem Statement**: You are given a function performing some mathematical operations on a tensor that can have dynamic shapes and can potentially mutate inputs. Implement the function using PyTorch\'s CUDAGraph Trees capabilities to optimize the provided operations. **Function Signature**: ```python def optimize_with_cudagraph_trees(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` **Input**: - `input_tensor` (torch.Tensor): A tensor of dynamic shape, residing on the GPU. **Tasks**: 1. Implement the `optimize_with_cudagraph_trees` function such that it uses CUDAGraph Trees for execution. 2. Ensure that the implementation adheres to the constraints of CUDAGraph Trees, such as fixed input addresses and no control flow. 3. Handle dynamic shapes by re-recording the CUDAGraph if a new shape is encountered. 4. Include a mechanism to mark the beginning of a new step to clear potential dependencies from previous iterations. **Example Usage**: ```python import torch @torch.compile(mode=\\"reduce-overhead\\") def optimize_with_cudagraph_trees(input_tensor: torch.Tensor) -> torch.Tensor: # GRAPH 1 y = input_tensor * input_tensor * input_tensor if y.sum() > 0: # GRAPH 2 z = y ** y else: # GRAPH 3 z = (y.abs() ** y.abs()) torch._dynamo.graph_break() # GRAPH 4 return z * torch.rand_like(z) # First run to initialize and record the graphs input_tensor = torch.randn(10, 10, device=\'cuda\') output_tensor = optimize_with_cudagraph_trees(input_tensor) # Subsequent runs to use the recorded graphs input_tensor = torch.randn(8, 8, device=\'cuda\') output_tensor = optimize_with_cudagraph_trees(input_tensor) ``` **Constraints**: - The function should be implemented to handle tensors directly from PyTorch with `torch.cuda` functionality. - Ensure that all memory allocations and usages comply with CUDAGraph Trees requirements. - Do not introduce any control flow (e.g., `if-else` statements) that requires host-device synchronization like `.item()` which triggers such synchronizations. **Performance Requirement**: - The function should efficiently manage memory and avoid unnecessary overheads, ensuring optimal runtime performance for repetitive execution with varying input tensor shapes. # Evaluation Criteria: Your submission will be evaluated based on: 1. Correct implementation of CUDAGraph Trees. 2. Adherence to constraints (static input memory addresses, avoidance of control flow). 3. Efficiency in handling dynamic shapes. 4. Correct usage of PyTorch functions and CUDA capabilities. Good luck!","solution":"import torch # This is a utility class to handle CUDAGraph recording and reuse for dynamic sizes class CUDAGraphTreeHelper: def __init__(self): self.graph = None self.graph_pool = {} self.static_input = None self.static_output = None def optimize_with_cudagraph_trees(self, input_tensor): input_shape = tuple(input_tensor.shape) if self.graph is None or input_shape not in self.graph_pool: # Allocate memory for the recording phase self.static_input = torch.empty_like(input_tensor) self.static_output = torch.empty_like(input_tensor) # Create a new graph and start recording self.graph = torch.cuda.CUDAGraph() self.graph_pool[input_shape] = self.graph with torch.cuda.graph(self.graph): # Use the provided operations from the problem statement y = self.static_input * self.static_input * self.static_input # CUDAGraphs cannot have control flows such as if-else blocks # Instead, fill a tensor with the condition and use it for elementwise multiplication condition_tensor = (torch.sum(y).unsqueeze(0) > 0).float() z1 = y ** y z2 = (y.abs() ** y.abs()) self.static_output.copy_(z1.mul(condition_tensor) + z2.mul(1.0 - condition_tensor)) z_final = self.static_output * torch.rand_like(self.static_output) self.graph.replay() # Copy the user input tensor into the pre-allocated static input self.static_input.copy_(input_tensor) self.graph.replay() return self.static_output cudagraph_helper = CUDAGraphTreeHelper() def optimize_with_cudagraph_trees(input_tensor: torch.Tensor) -> torch.Tensor: return cudagraph_helper.optimize_with_cudagraph_trees(input_tensor)"},{"question":"# Function Implementation: Filter and Transform Data In this coding challenge, your task is to implement a function that processes a list of dictionaries containing information about products. Each product dictionary includes the following keys: - `name`: A string representing the product\'s name. - `price`: A float representing the product\'s price. - `quantity`: An integer representing the quantity of the product in stock. The goal is to filter out products that are out of stock (i.e., `quantity` is 0), sort the remaining products by price in ascending order, and then format the result to display the product names and their total value (price multiplied by quantity). Function Signature ```python def filter_and_transform_products(products: list) -> list: pass ``` Input - `products`: A list of dictionaries, where each dictionary contains keys `name` (str), `price` (float), and `quantity` (int). Output - Returns a list of strings, where each string is formatted as `\\"Product Name: Total Value\\"`. The list should be sorted by the total value and should not include products that are out of stock. Constraints - The `products` list can contain between 1 and 1000 product dictionaries. - `name` will be a non-empty string with alphanumeric characters. - `price` will be a non-negative float. - `quantity` will be a non-negative integer. Example ```python products = [ {\\"name\\": \\"Book\\", \\"price\\": 12.99, \\"quantity\\": 5}, {\\"name\\": \\"Pen\\", \\"price\\": 1.49, \\"quantity\\": 10}, {\\"name\\": \\"Notebook\\", \\"price\\": 2.99, \\"quantity\\": 0}, {\\"name\\": \\"Eraser\\", \\"price\\": 0.99, \\"quantity\\": 2}, ] # The function should return: # [ # \\"Eraser: 1.98\\", # \\"Pen: 14.90\\", # \\"Book: 64.95\\" # ] ``` You are required to use appropriate built-in functions described in the documentation to complete this task efficiently. The implementation should handle edge cases where the list may be empty or contain only products that are out of stock. Notes: - Ensure your function is efficient and handles the constraints appropriately. - You may use any built-in functions or Python standard libraries to accomplish the task.","solution":"def filter_and_transform_products(products: list) -> list: Filters out products that are out of stock, sorts the remaining products by total value (price * quantity), and formats them in the specified format. Args: products (list): List of dictionaries each containing \'name\' (str), \'price\' (float), and \'quantity\' (int). Returns: list: A list of formatted strings representing the product names and their total values, sorted by the total value. # Filter out products that are out of stock in_stock_products = [product for product in products if product[\'quantity\'] > 0] # Sort the products by the total value (price * quantity) sorted_products = sorted( in_stock_products, key=lambda product: product[\'price\'] * product[\'quantity\'] ) # Format the products as \\"Product Name: Total Value\\" formatted_products = [ f\\"{product[\'name\']}: {product[\'price\'] * product[\'quantity\']:.2f}\\" for product in sorted_products] return formatted_products"},{"question":"# Custom Command-Line Calculator You are required to implement a custom command-line calculator by subclassing the `cmd.Cmd` class from Python\'s `cmd` module. Your calculator should support basic arithmetic operations (addition, subtraction, multiplication, division, and exponentiation) and maintain a history of calculations that the user can review or clear. Requirements: 1. **Command Methods**: Define the following command methods for basic arithmetic operations. Each method should take the remaining part of the line as input, parse the two numbers involved, perform the operation, and print the result. - `do_add(arg)`: Adds two numbers. - `do_subtract(arg)`: Subtracts the second number from the first. - `do_multiply(arg)`: Multiplies two numbers. - `do_divide(arg)`: Divides the first number by the second. - `do_power(arg)`: Raises the first number to the power of the second. 2. **History**: - Implement `do_history(arg)` to print all past calculations. - Implement `do_clearhistory(arg)` to clear the history of calculations. 3. **Customization**: - Set a custom prompt for the calculator shell (e.g., \\"(calc) \\"). - Set an introductory message that will be displayed when the calculator starts. 4. **Error Handling**: - Ensure that invalid inputs are handled gracefully (e.g., division by zero, non-numeric input). 5. **Command Hooks**: - Use the `precmd()` method to log each valid calculation command (add, subtract, multiply, divide, power) in the history before executing it. Input and Output: - **Addition Example**: ``` (calc) add 5 3 8 ``` - **Subtract Example**: ``` (calc) subtract 10 4 6 ``` - **Division with Error Handling Example**: ``` (calc) divide 10 0 Error: Division by zero ``` - **History Example**: ``` (calc) history add 5 3 = 8 subtract 10 4 = 6 divide 10 2 = 5.0 ``` - **Clear History Example**: ``` (calc) clearhistory History cleared ``` --- # Template Code Below is a template to get you started: ```python import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the calculator shell. Type help or ? to list commands.n\' prompt = \'(calc) \' def __init__(self): super().__init__() self.history = [] def do_add(self, arg): \'Adds two numbers: ADD 1 2\' try: numbers = parse(arg) result = numbers[0] + numbers[1] self.history.append(f\'add {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_subtract(self, arg): \'Subtracts the second number from the first: SUBTRACT 3 2\' try: numbers = parse(arg) result = numbers[0] - numbers[1] self.history.append(f\'subtract {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_multiply(self, arg): \'Multiplies two numbers: MULTIPLY 2 3\' try: numbers = parse(arg) result = numbers[0] * numbers[1] self.history.append(f\'multiply {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_divide(self, arg): \'Divides the first number by the second: DIVIDE 10 2\' try: numbers = parse(arg) if numbers[1] == 0: raise ValueError(\\"Division by zero\\") result = numbers[0] / numbers[1] self.history.append(f\'divide {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_power(self, arg): \'Raises the first number to the power of the second: POWER 2 3\' try: numbers = parse(arg) result = numbers[0] ** numbers[1] self.history.append(f\'power {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_history(self, arg): \'Displays the history of calculations: HISTORY\' for record in self.history: print(record) def do_clearhistory(self, arg): \'Clears the history of calculations: CLEARHISTORY\' self.history.clear() print(\\"History cleared\\") def precmd(self, line): # Log each valid command to the history valid_commands = [\'add\', \'subtract\', \'multiply\', \'divide\', \'power\'] if any(line.startswith(cmd) for cmd in valid_commands): return line.lower() return line def parse(arg): \'Convert a series of zero or more numbers to an argument tuple\' return tuple(map(int, arg.split())) if __name__ == \'__main__\': CalculatorShell().cmdloop() ``` Use the provided template to implement the custom command-line calculator. Ensure to test each component effectively to meet the requirements.","solution":"import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the calculator shell. Type help or ? to list commands.n\' prompt = \'(calc) \' def __init__(self): super().__init__() self.history = [] def do_add(self, arg): \'Adds two numbers: add 1 2\' try: numbers = parse(arg) result = numbers[0] + numbers[1] self.history.append(f\'add {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_subtract(self, arg): \'Subtracts the second number from the first: subtract 3 2\' try: numbers = parse(arg) result = numbers[0] - numbers[1] self.history.append(f\'subtract {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_multiply(self, arg): \'Multiplies two numbers: multiply 2 3\' try: numbers = parse(arg) result = numbers[0] * numbers[1] self.history.append(f\'multiply {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_divide(self, arg): \'Divides the first number by the second: divide 10 2\' try: numbers = parse(arg) if numbers[1] == 0: raise ValueError(\\"Division by zero\\") result = numbers[0] / numbers[1] self.history.append(f\'divide {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_power(self, arg): \'Raises the first number to the power of the second: power 2 3\' try: numbers = parse(arg) result = numbers[0] ** numbers[1] self.history.append(f\'power {arg} = {result}\') print(result) except Exception as e: print(f\\"Error: {str(e)}\\") def do_history(self, arg): \'Displays the history of calculations: history\' for record in self.history: print(record) def do_clearhistory(self, arg): \'Clears the history of calculations: clearhistory\' self.history.clear() print(\\"History cleared\\") def precmd(self, line): # Log each valid command to the history valid_commands = [\'add\', \'subtract\', \'multiply\', \'divide\', \'power\'] if any(line.startswith(cmd) for cmd in valid_commands): return line.lower() return line def parse(arg): \'Convert a series of zero or more numbers to an argument tuple\' return tuple(map(int, arg.split())) if __name__ == \'__main__\': CalculatorShell().cmdloop()"},{"question":"# Python Coding Assessment: Deep and Shallow Copy Implementation Objective: Implement functions to demonstrate shallow and deep copies of compound objects, considering recursive structures and custom class behavior. Background: The `copy` module in Python provides `copy.copy()` for shallow copies and `copy.deepcopy()` for deep copies. Shallow copies create a new object and insert references to the objects found in the original, while deep copies create new object copies recursively. # Task: 1. **Function Implementation**: - Implement a function `shallow_copy_example()`. This function should: - Accept a nested list as input. - Return a shallow copy of the list. - Implement a function `deep_copy_example()`. This function should: - Accept a nested list as input. - Return a deep copy of the list. 2. **Handle Recursive Structures**: - Implement a function `recursive_deep_copy_example()`. This function should: - Accept a nested structure (list/dictionary) that may contain recursive references. - Return a deep copy of the structure without causing infinite recursion. 3. **Custom Class Copying**: - Define a class `CustomObject` with its own `__copy__` and `__deepcopy__` methods to control its copy behavior. - Implement methods to: - Create a shallow copy of `CustomObject` using `__copy__()`. - Create a deep copy of `CustomObject` using `__deepcopy__()`. # Input: 1. `shallow_copy_example()` and `deep_copy_example()`: - A nested list, e.g., `[[1, 2], [3, 4]]` 2. `recursive_deep_copy_example()`: - A nested structure (list/dictionary), e.g., `[1, [2, [3]], 4]` (may contain self-references for testing) 3. `CustomObject`: - Initialize with attributes that may reference other objects or lists. # Output: 1. `shallow_copy_example()`: - A new list that is a shallow copy of the input. 2. `deep_copy_example()`: - A new list that is a deep copy of the input. 3. `recursive_deep_copy_example()`: - A new structure that is a deep copy of the input without causing infinite recursion. 4. `CustomObject`: - Correct shallow and deep copies as defined by the custom methods. # Constraints: - Ensure the deep copy function can handle nested and recursive structures efficiently. - Custom copying methods should properly manage internal state and references. # Example: ```python nested_list = [[1, 2], [3, 4]] shallow_copy = shallow_copy_example(nested_list) deep_copy = deep_copy_example(nested_list) # Modify the original lists nested_list[0][0] = 9 print(shallow_copy) # Output: [[9, 2], [3, 4]] (Affected) print(deep_copy) # Output: [[1, 2], [3, 4]] (Unaffected) recursive_structure = [1] recursive_structure.append(recursive_structure) copied_structure = recursive_deep_copy_example(recursive_structure) print(copied_structure) # Output: [1, [...]] class CustomObject: # Your custom class implementation obj = CustomObject(...) shallow_copied_obj = copy.copy(obj) deep_copied_obj = copy.deepcopy(obj) ``` Implement these functions and the custom class to demonstrate comprehensive understanding of the `copy` module\'s shallow and deep copy mechanisms.","solution":"import copy def shallow_copy_example(nested_list): Returns a shallow copy of the nested list. return copy.copy(nested_list) def deep_copy_example(nested_list): Returns a deep copy of the nested list. return copy.deepcopy(nested_list) def recursive_deep_copy_example(structure): Returns a deep copy of the structure while handling recursive references. return copy.deepcopy(structure) class CustomObject: def __init__(self, attr): self.attr = attr def __copy__(self): new_instance = type(self)(self.attr) return new_instance def __deepcopy__(self, memo): new_instance = type(self)(copy.deepcopy(self.attr, memo)) return new_instance"},{"question":"Building an Interactive Text-based Menu with `curses` # Objective You are required to create an interactive text-based menu using the `curses` module in Python. The menu should display a list of options and listen for keyboard inputs to navigate through the options. Your task is to implement this menu, allowing users to navigate up and down through options and select a specific option. # Specifications 1. **Initialize `curses`:** - Use the `curses.initscr()` function to initialize the library and return a window object representing the entire screen. 2. **Menu Options:** - The menu should have the following options: - Option 1: \\"Start Game\\" - Option 2: \\"Settings\\" - Option 3: \\"Help\\" - Option 4: \\"Exit\\" 3. **Navigation:** - The user can navigate through the options using the Up (`KEY_UP`) and Down (`KEY_DOWN`) arrow keys. 4. **Selection:** - The user can select an option using the Enter (`KEY_ENTER`) key. When an option is selected, display a message in the terminal indicating which option was selected. # Function Implementation You need to implement the following function: `menu(stdscr)` # Parameters - `stdscr`: The curses window object representing the entire screen. # Functionality 1. **Clear the screen:** - Use appropriate `curses` functions to clear the screen and initialize necessary settings (e.g., color). 2. **Display Menu:** - Display the menu with options highlighted when selected. 3. **Handle Keyboard Input:** - Handle keyboard inputs to navigate through the menu and select an option. - When an option is selected, clear the screen and display a message indicating the selected option. 4. **Terminate `curses`:** - Properly terminate `curses` and restore the terminal to its default settings. # Example Output: ```text --------------------------------- | Interactive Menu | --------------------------------- | -> Start Game | | Settings | | Help | | Exit | --------------------------------- Selected \'Start Game\' ``` # Constraints - You must use `curses` to handle all input and output. - The implementation should run in a terminal. # Usage A sample implementation that runs the `menu` function: ```python import curses def main(stdscr): menu(stdscr) curses.wrapper(main) ``` # Additional Notes - Make sure to handle exceptions properly to exit the `curses` mode gracefully. - Use `curses` attributes to highlight the selected menu option.","solution":"import curses def menu(stdscr): # Clear screen stdscr.clear() # List of menu options menu_options = [\\"Start Game\\", \\"Settings\\", \\"Help\\", \\"Exit\\"] # Current selected option (index) current_option = 0 # Function to print the menu def print_menu(): stdscr.clear() for idx, option in enumerate(menu_options): if idx == current_option: stdscr.addstr(idx + 1, 2, f\\"-> {option}\\", curses.A_REVERSE) else: stdscr.addstr(idx + 1, 2, f\\" {option}\\") stdscr.refresh() print_menu() while True: key = stdscr.getch() stdscr.clear() if key == curses.KEY_UP and current_option > 0: current_option -= 1 elif key == curses.KEY_DOWN and current_option < len(menu_options) - 1: current_option += 1 elif key == curses.KEY_ENTER or key in [10, 13]: stdscr.addstr(len(menu_options) + 3, 2, f\\"Selected \'{menu_options[current_option]}\'\\") stdscr.refresh() stdscr.getch() if current_option == len(menu_options) - 1: break print_menu() # To run the menu function using the curses library if __name__ == \\"__main__\\": curses.wrapper(menu)"},{"question":"Objective Write a Python script that automates the generation of a `MANIFEST.in` file for a given directory based on specified inclusion and exclusion criteria. The script will be used to create the manifest file needed for the `sdist` command to generate a source distribution. Problem Statement Your task is to implement a function `generate_manifest_in(directory: str, include_patterns: list, exclude_patterns: list, output_file: str) -> None` that: 1. Reads the specified `directory` and lists all files recursively. 2. Filters the files based on `include_patterns` and `exclude_patterns`. 3. Writes the resulting list of files to a `MANIFEST.in` file, located at the `output_file` path. Input - `directory` (str): Path to the base directory to scan for files. - `include_patterns` (list of str): Glob patterns to include in the `MANIFEST.in`. - `exclude_patterns` (list of str): Glob patterns to exclude from the `MANIFEST.in`. - `output_file` (str): Path to the `MANIFEST.in` file to be generated. Output - The function should create a `MANIFEST.in` file at the specified `output_file` path, containing lines that match the `include_patterns` and exclude those that match the `exclude_patterns`. Example ```python import os def generate_manifest_in(directory, include_patterns, exclude_patterns, output_file): pass # Example usage: generate_manifest_in( directory=\\"my_project\\", include_patterns=[\\"*.py\\", \\"*.txt\\"], exclude_patterns=[\\"*.log\\", \\"*.tmp\\"], output_file=\\"my_project/MANIFEST.in\\" ) ``` Assume `my_project` directory structure is: ``` my_project/ main.py README.txt temp.log scripts/ install.py ``` After executing the function, the `my_project/MANIFEST.in` file should contain: ``` include main.py include README.txt recursive-include scripts *.py exclude temp.log ``` Constraints - Do not use any third-party libraries; only use Python\'s standard library. - Handle both relative and absolute paths. - Ensure that the script is efficient and handles large directories gracefully. Additional Instruction Make sure the `MANIFEST.in` file starts with the line: ``` # Generated by generate_manifest_in ``` Requirements - Thoroughly test your function with various directory structures and patterns. - Ensure that the generated `MANIFEST.in` file is correctly formatted and contains accurate paths. Good luck!","solution":"import os import fnmatch def generate_manifest_in(directory, include_patterns, exclude_patterns, output_file): files_to_include = set() for root, dirs, files in os.walk(directory): for pattern in include_patterns: for filename in fnmatch.filter(files, pattern): files_to_include.add(os.path.relpath(os.path.join(root, filename), directory)) files_to_exclude = set() for root, dirs, files in os.walk(directory): for pattern in exclude_patterns: for filename in fnmatch.filter(files, pattern): file_path = os.path.relpath(os.path.join(root, filename), directory) files_to_exclude.add(file_path) final_files = files_to_include - files_to_exclude with open(output_file, \'w\') as f: f.write(\\"# Generated by generate_manifest_inn\\") for file in sorted(final_files): f.write(f\\"include {file}n\\")"},{"question":"# Custom Container with Garbage Collection **Objective**: Implement a custom Python container class that tracks objects that may form circular references and utilizes Python\'s built-in garbage collection library to handle them. # Problem Statement You need to implement a custom container class called `TrackedContainer` that holds references to other objects. This container should be able to: 1. **Track Objects**: Maintain a list of objects it\'s holding. 2. **Detect Cycles**: Utilize Python\'s garbage collection (gc) module to detect circular references within the stored objects. 3. **Custom Traversal**: Implement a method to traverse all objects in the container and perform an operation on them. 4. **Clear References**: Provide a method to clear all references, if required. # Requirements: 1. **Class Definition**: - Define a class `TrackedContainer` with the following methods: - `__init__(self)`: Initializes an empty container. - `add(self, obj: Any) -> None`: Adds an object to the container. - `traverse(self, func: Callable[[Any], None]) -> None`: Applies the given function to each object in the container. - `detect_cycles(self) -> bool`: Checks whether there are circular references within the container using the `gc` module. - `clear(self) -> None`: Clears all references contained in the container. 2. **Implementation Constraints**: - Use Python\'s built-in `gc` module functions such as `gc.collect()`, `gc.is_tracked()`, and `gc.get_objects()` to implement cycle detection. - Ensure that `add` maintains the order of insertion and does not duplicate objects. - The `traverse` method should call the provided function on each object in the order they were added. 3. **Performance Requirements**: - Your implementation should be efficient in terms of both time and space. Make sure `add` and `clear` methods run in O(1) time complexity. - The `traverse` and `detect_cycles` methods should run in O(n) time complexity, where `n` is the number of objects in the container. # Example Usage: ```python import gc class TrackedContainer: def __init__(self): # Implement this method pass def add(self, obj): # Implement this method pass def traverse(self, func): # Implement this method pass def detect_cycles(self): # Implement this method pass def clear(self): # Implement this method pass # Example usage: container = TrackedContainer() obj1 = MyObject() obj2 = MyObject() container.add(obj1) container.add(obj2) container.traverse(print) # Should print obj1 and obj2 print(container.detect_cycles()) # Should return False unless there\'s a cycle container.clear() ``` Your task is to complete the implementation of the `TrackedContainer` class as per the specifications given above.","solution":"import gc from typing import Any, Callable class TrackedContainer: def __init__(self): # Initializes an empty container self.objects = [] def add(self, obj: Any) -> None: # Adds an object to the container if obj not in self.objects: self.objects.append(obj) def traverse(self, func: Callable[[Any], None]) -> None: # Applies the given function to each object in the container for obj in self.objects: func(obj) def detect_cycles(self) -> bool: # Checks whether there are circular references within the container using the gc module gc.collect() for obj in self.objects: if gc.is_tracked(obj): return True return False def clear(self) -> None: # Clears all references contained in the container self.objects.clear()"},{"question":"# Advanced Coding Assessment: Implementing and Understanding LDA and QDA **Objective**: To assess your understanding of Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), including their mathematical foundations, implementation, and practical applications using scikit-learn. **Problem Statement**: You are provided with a dataset that involves several features and a multiclass target. Your task is to apply both LDA and QDA to this dataset and evaluate their performance by implementing the following steps: 1. **Data Preprocessing**: - Load the dataset (you can use any well-known dataset such as the Iris dataset) and perform necessary preprocessing steps. 2. **LDA Implementation**: - Implement LDA with dimensionality reduction to transform the dataset. Set the `n_components` parameter to reduce the data to two dimensions. - Apply the LDA model to classify the data. - Evaluate the performance of the LDA model using cross-validation and report the accuracy. 3. **QDA Implementation**: - Implement QDA to classify the data without dimensionality reduction. - Evaluate the performance of the QDA model using cross-validation and report the accuracy. 4. **Comparison and Analysis**: - Compare the decision boundaries generated by LDA and QDA. - Visualize the transformed dataset using LDA and the decision boundaries for both LDA and QDA. - Discuss the differences in performance and decision boundaries between LDA and QDA. **Constraints**: - You must use the `sklearn.discriminant_analysis.LinearDiscriminantAnalysis` and `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis` classes for LDA and QDA implementations respectively. - The dataset can be any multiclass classification dataset with at least 100 samples. - Cross-validation should be performed with 5 folds. **Input Format**: - A dataset in CSV format with feature columns and a target column. **Output Format**: - Accuracy of the LDA model after cross-validation. - Accuracy of the QDA model after cross-validation. - Visual plot of the transformed dataset and decision boundaries. **Example**: ```python import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis import matplotlib.pyplot as plt import seaborn as sns # Load and preprocess dataset data = pd.read_csv(\'path/to/dataset.csv\') X = data.iloc[:, :-1] y = data.iloc[:, -1] # Apply LDA with dimensionality reduction lda = LinearDiscriminantAnalysis(n_components=2) X_lda = lda.fit_transform(X, y) # Evaluate LDA model lda_accuracy = cross_val_score(lda, X, y, cv=5).mean() # Apply QDA qda = QuadraticDiscriminantAnalysis() qda_accuracy = cross_val_score(qda, X, y, cv=5).mean() # Visualize transformed data and decision boundaries plt.figure(figsize=(12, 6)) plt.subplot(121) sns.scatterplot(x=X_lda[:, 0], y=X_lda[:, 1], hue=y) plt.title(\'LDA Transformed Data\') plt.subplot(122) # Code for plotting decision boundaries # ... plt.suptitle(\'LDA vs QDA Decision Boundaries\') plt.show() # Output the accuracies print(f\\"LDA Accuracy: {lda_accuracy:.2f}\\") print(f\\"QDA Accuracy: {qda_accuracy:.2f}\\") # Discussion of performance and decision boundaries ``` **Note**: This is an illustrative example. You need to implement the code for plotting decision boundaries and discussing the results as specified in the question.","solution":"import pandas as pd import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, cross_val_score from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis import matplotlib.pyplot as plt import seaborn as sns # Load and preprocess dataset iris = load_iris() X = iris.data y = iris.target # Apply LDA with dimensionality reduction lda = LinearDiscriminantAnalysis(n_components=2) X_lda = lda.fit_transform(X, y) # Evaluate LDA model lda_accuracy = cross_val_score(lda, X, y, cv=5).mean() # Apply QDA qda = QuadraticDiscriminantAnalysis() qda_accuracy = cross_val_score(qda, X, y, cv=5).mean() # Visualize transformed data and decision boundaries plt.figure(figsize=(12, 6)) # Plotting LDA Transformed Data plt.subplot(121) sns.scatterplot(x=X_lda[:, 0], y=X_lda[:, 1], hue=y) plt.title(\'LDA Transformed Data\') # Plot decision boundaries def plot_decision_boundaries(X, y, model, model_name, ax): x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, alpha=0.3) sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, edgecolor=\'k\', ax=ax) ax.set_title(\'{} Decision Boundary\'.format(model_name)) # Plot LDA and QDA Decision Boundaries lda_model = lda.fit(X_lda, y) qda_model = qda.fit(X, y) ax1 = plt.subplot(122) plot_decision_boundaries(X_lda, y, lda_model, \\"LDA\\", ax1) plt.tight_layout() plt.show() # Output the accuracies print(f\\"LDA Accuracy: {lda_accuracy:.2f}\\") print(f\\"QDA Accuracy: {qda_accuracy:.2f}\\") # Discussion of performance and decision boundaries # LDA often performs better when class boundaries are linear or roughly linear. # QDA might perform better when boundaries are quadratic or more complex."},{"question":"# Advanced PyTorch: Handling BatchNorm with `vmap` You are tasked with implementing a modified neural network that adapts its normalization strategy based on certain constraints, specifically the issues around using BatchNorm with `vmap` from the `functorch` library. Your objective is to create a wrapper function that replaces BatchNorm with GroupNorm in any given neural network model. Your solution should include: 1. A function `replace_batchnorm_with_groupnorm` that takes a PyTorch model and a number of groups `G` as input. 2. The function should replace all instances of `BatchNorm2d` with `GroupNorm`, making sure `C % G == 0` (where `C` is the number of channels in the BatchNorm layer). 3. For simplicity, we assume all BatchNorm layers use `BatchNorm2d`. Input - `model`: A PyTorch model instance. - `G`: An integer representing the number of groups for GroupNorm. Output - The modified model with all BatchNorm layers replaced by GroupNorm layers. Example Usage ```python import torch import torch.nn as nn from torch.nn import BatchNorm2d, GroupNorm class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.bn1 = BatchNorm2d(32) self.conv2 = nn.Conv2d(32, 64, 3, 1) self.bn2 = BatchNorm2d(64) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) return x def replace_batchnorm_with_groupnorm(model, G): for name, module in model.named_children(): if isinstance(module, BatchNorm2d): C = module.num_features assert C % G == 0, \\"Number of channels must be divisible by the number of groups\\" setattr(model, name, GroupNorm(num_groups=G, num_channels=C)) else: replace_batchnorm_with_groupnorm(module, G) return model # Example model = SimpleCNN() G = 4 modified_model = replace_batchnorm_with_groupnorm(model, G) ``` Constraints - The model passed to the `replace_batchnorm_with_groupnorm` function will always contain only `Conv2d` and `BatchNorm2d` layers. - Ensure that all `BatchNorm2d` layers are replaced correctly and that the network remains functional after modification. - Your function should recursively handle nested submodules. Your task is to implement the `replace_batchnorm_with_groupnorm` function as specified above.","solution":"import torch.nn as nn from torch.nn import BatchNorm2d, GroupNorm def replace_batchnorm_with_groupnorm(model, G): Replaces all BatchNorm2d layers with GroupNorm layers. Args: model (nn.Module): The PyTorch model to modify. G (int): The number of groups for GroupNorm. Returns: nn.Module: The modified model with GroupNorm layers. for name, module in model.named_children(): if isinstance(module, BatchNorm2d): C = module.num_features assert C % G == 0, \\"Number of channels must be divisible by the number of groups\\" setattr(model, name, GroupNorm(num_groups=G, num_channels=C)) else: # Recursively replace in sub-modules replace_batchnorm_with_groupnorm(module, G) return model"},{"question":"**Question: Analyze Model Residuals using Seaborn** You are provided with the `seaborn` package and a sample dataset. Your task is to use the seaborn library to visualize and analyze residuals from multiple linear regression models. You will create various plots to identify potential issues with the regression models and enhance the accuracy by adjusting for non-linear relationships. **Instructions:** 1. Load the `mpg` dataset using seaborn. 2. Create a function `analyze_residuals` which: - Plots a residual plot for `displacement` as the dependent variable and `weight` as the independent variable. - Plots a residual plot for `mpg` as the dependent variable and `horsepower` as the independent variable. - Adds a second-order polynomial trend to the plot of `mpg` vs. `horsepower` and plots the residuals. - Adds a LOWESS smooth curve to the plot of `mpg` vs. `horsepower` and visualizes it to highlight any non-linear patterns. **Function Specifications:** ```python def analyze_residuals(): This function performs the following actions: 1. Loads the mpg dataset. 2. Creates and displays a residual plot for the variables \'weight\' and \'displacement\'. 3. Creates and displays a residual plot for the variables \'horsepower\' and \'mpg\'. 4. Creates and displays a residual plot for the variables \'horsepower\' and \'mpg\' after fitting a second-order polynomial trend. 5. Creates and displays a residual plot for the variables \'horsepower\' and \'mpg\' with a LOWESS smooth curve. The function does not take any input parameters and does not return any values. pass ``` **Output:** - Using the `mpg` dataset, visualize appropriate residuals plots as defined above. - Each plot should be created and displayed sequentially within the function. **Requirements:** - Ensure the visualizations are clear and provide insights into the residuals\' behavior and model assumptions. - Use seaborn\'s `sns.residplot` method to create all the necessary plots. - Consider non-linearity and model-fit quality by customizing the plots as indicated. **Constraints:** - Your solution should work on Python 3.6 or later. - You must use the seaborn library for all visualizations. Other plotting libraries are not allowed. **Example:** The following are examples of the expected plots: 1. Residual plot for `weight` vs. `displacement`. 2. Residual plot for `horsepower` vs. `mpg`. 3. Residual plot for `horsepower` vs. `mpg` with a second-order trend. 4. Residual plot for `horsepower` vs. `mpg` with a LOWESS curve. **Note:** - Save and close each plot after displaying it to ensure proper execution without overlapping of plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import statsmodels.api as sm import pandas as pd def analyze_residuals(): # Load dataset mpg = sns.load_dataset(\\"mpg\\").dropna() # Residual plot for displacement vs weight sns.residplot(x=\'weight\', y=\'displacement\', data=mpg) plt.title(\'Residual plot: Displacement vs. Weight\') plt.show() # Residual plot for mpg vs horsepower sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, lowess=True) plt.title(\'Residual plot: MPG vs. Horsepower\') plt.show() # Residual plot for mpg vs horsepower with second-order polynomial X = mpg[\'horsepower\'] y = mpg[\'mpg\'] p = np.polyfit(X, y, 2) poly_y = np.polyval(p, X) resid = y - poly_y plt.scatter(X, resid) plt.axhline(0, color=\'red\', linestyle=\'--\') plt.title(\'Residual plot: MPG vs. Horsepower (Polynomial Fit)\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.show() # Scatter plot of mpg vs. horsepower with LOWESS smooth curve sns.scatterplot(x=\'horsepower\', y=\'mpg\', data=mpg) lowess = sm.nonparametric.lowess(mpg[\'mpg\'], mpg[\'horsepower\']) plt.plot(lowess[:, 0], lowess[:, 1], color=\'red\') plt.title(\'MPG vs. Horsepower with LOWESS curve\') plt.show()"},{"question":"**Question: Implement a Custom SAX Parser in Python** You are required to implement a SAX parser using Python\'s `xml.sax` module to parse an XML document. Additionally, you will create custom content and error handlers to process and handle the XML data and any potential errors. # Requirements: 1. Write a `CustomContentHandler` class that inherits from `xml.sax.ContentHandler`. - It should print the names of the XML elements and their text data (if any). 2. Write a `CustomErrorHandler` class that inherits from `xml.sax.ErrorHandler`. - It should override the necessary methods to print detailed error messages when parsing errors occur. 3. Write a function `parse_xml(file_path)` that: - Creates a SAX parser using `xml.sax.make_parser()`. - Sets the `CustomContentHandler` and `CustomErrorHandler` to the parser. - Parses the XML document from the given `file_path`. # Input and Output: - The function `parse_xml(file_path)` should accept a single parameter `file_path`, which is a string representing the path to an XML file. - The output should be printed directly to the console by the content and error handlers. # Example XML File: Consider an XML file `example.xml` with the following content: ```xml <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies.</description> </book> </catalog> ``` # Example Usage: ``` parse_xml(\'example.xml\') ``` # Expected Output: ``` Start Element: catalog Start Element: book Data: id=\\"bk101\\" Start Element: author Data: Gambardella, Matthew End Element: author Start Element: title Data: XML Developer\'s Guide End Element: title Start Element: genre Data: Computer End Element: genre Start Element: price Data: 44.95 End Element: price Start Element: publish_date Data: 2000-10-01 End Element: publish_date Start Element: description Data: An in-depth look at creating applications with XML. End Element: description End Element: book Start Element: book Data: id=\\"bk102\\" Start Element: author Data: Ralls, Kim End Element: author Start Element: title Data: Midnight Rain End Element: title Start Element: genre Data: Fantasy End Element: genre Start Element: price Data: 5.95 End Element: price Start Element: publish_date Data: 2000-12-16 End Element: publish_date Start Element: description Data: A former architect battles corporate zombies. End Element: description End Element: book End Element: catalog ``` Ensure your implementation correctly handles both well-formed XML and cases where parsing errors occur, making appropriate use of the custom error handler.","solution":"import xml.sax class CustomContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Start Element: {name}\\") for attr in attrs.keys(): print(f\\" Attribute: {attr}=\'{attrs[attr]}\'\\") def characters(self, content): content = content.strip() if content: print(f\\"Data: {content}\\") def endElement(self, name): print(f\\"End Element: {name}\\") class CustomErrorHandler(xml.sax.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_xml(file_path): parser = xml.sax.make_parser() parser.setContentHandler(CustomContentHandler()) parser.setErrorHandler(CustomErrorHandler()) try: parser.parse(file_path) except xml.sax.SAXParseException as e: print(f\\"Exception: {e}\\")"},{"question":"# Asynchronous File Processing with Platform Considerations Background: You have been tasked with developing a Python script to process a large number of text files asynchronously using the \\"asyncio\\" module. However, you need to take into account platform-specific limitations described in the provided documentation. Your goal is to design a function that reads multiple files concurrently and processes their contents. Problem Statement: Write a Python function `async process_files(file_paths: List[str]) -> Dict[str, int]` that: - Takes a list of file paths (`file_paths`) as input. - Reads the contents of each file asynchronously. - Processes the contents to count the number of lines that contain the word \\"async\\". - Returns a dictionary mapping each file path to the count of lines containing \\"async\\". Requirements: 1. **Concurrency**: The file reading operations must be performed concurrently using `asyncio`. 2. **Platform Specific Constraint Handling**: - **Windows**: Use \\"ProactorEventLoop\\" to support file I/O. - **macOS and other Unix-based systems**: Ensure compatibility with \\"SelectorEventLoop\\". 3. **Input/Output**: - Input: A list of strings where each string is a file path. - Output: A dictionary mapping each file path (string) to an integer count of lines containing the word \\"async\\". Example Usage: ```python import asyncio file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] result = asyncio.run(process_files(file_paths)) print(result) # Output format might be: # { \\"file1.txt\\": 3, \\"file2.txt\\": 5, \\"file3.txt\\": 2 } ``` Constraints: - Handle platform-specific limitations as mentioned in the provided documentation. - Assume each file is large enough to warrant the need for asynchronous processing. - Ensure your solution handles potential I/O exceptions gracefully. - Use Python 3.10 or later. Notes: - You are encouraged to use `asyncio`\'s built-in functionality for asynchronous file reading wherever possible. - Assume the files are all readable and not empty. - The solution should be efficient in handling I/O operations, utilizing asynchronous features to avoid blocking. Use the provided documentation to derive platform constraints and ensure your implementation is robust across different supported platforms.","solution":"import asyncio from typing import List, Dict import os async def count_async_in_file(file_path: str) -> int: count = 0 async with aiofiles.open(file_path, \'r\') as f: async for line in f: if \'async\' in line: count += 1 return count async def process_files(file_paths: List[str]) -> Dict[str, int]: results = {} for file_path in file_paths: try: results[file_path] = await count_async_in_file(file_path) except Exception as e: results[file_path] = str(e) # Save the exception string if reading fails return results import sys import asyncio import platform # Platform-specific event loop setup if platform.system() == \'Windows\': asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) else: asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())"},{"question":"**Question: Building a Mini Backup and Restore System** # Objective: Design a small Python utility function that helps back up and restore files from a specified directory to another location, with features to handle process creation, file descriptor operations, and environment variable utilization. # Description: Implement a function `backup_and_restore(src_dir: str, dest_dir: str, restore: bool = False) -> None` that performs the following tasks: 1. **Backup Mode**: - When `restore` is `False`, backup all files and subdirectories from `src_dir` to `dest_dir`. - Use `os.walk()` to traverse the source directory recursively and copy all files and directories to the destination. Maintain the directory structure. - Use `os.makedirs()` for directory creation and handle any required permissions changes using `os.chmod()`. 2. **Restore Mode**: - When `restore` is `True`, restore all files and directories from `dest_dir` back to `src_dir`, replicating the original directory structure. - Ensure that files in `src_dir` are overwritten with those from `dest_dir` during the restore process. 3. **Process Creation**: - For each file operation, create a subprocess to handle copying tasks using `os.fork()`. Ensure that the main process waits for the child process to complete using `os.wait()`. - Implement appropriate error handling for process creation and completion. 4. **Use Environment Variables**: - Utilize environment variables to set and retrieve the backup directory paths, specifically `BACKUP_SRC` and `BACKUP_DEST`. If these environment variables are set, they should override the `src_dir` and `dest_dir` function arguments respectively. - Use the `os.getenv()` function to retrieve environment variables and `os.putenv()` to set them if required. # Constraints: - Ensure the solution is platform-independent. - Handle exceptions such as permission errors, file not found errors gracefully. - Use `os` module functions for all file and process operations. - Optimize file operations for performance, particularly when dealing with large directories. # Example Usage: ```python import os # Setting environment variables (Usually done outside the script or in initialization) os.putenv(\'BACKUP_SRC\', \'/path/to/source\') os.putenv(\'BACKUP_DEST\', \'/path/to/destination\') # Backing up files from source to destination backup_and_restore(\'/default/source\', \'/default/destination\') # Restoring files from destination to source backup_and_restore(\'/default/source\', \'/default/destination\', restore=True) ``` # Expected Function Signature: ```python def backup_and_restore(src_dir: str, dest_dir: str, restore: bool = False) -> None: pass ``` # Evaluation Criteria: - Correctness and completeness of the functionality. - Efficient handling of file operations and process management. - Robust error handling and adherence to platform independence. - Proper usage of environment variables.","solution":"import os import shutil import subprocess def backup_and_restore(src_dir: str, dest_dir: str, restore: bool = False) -> None: Backs up or restores files. Args: src_dir (str): The source directory for backup or the target directory to restore into. dest_dir (str): The destination directory for backup or the source directory to restore from. restore (bool): If True, performs restore instead of backup. Default is False. # Use environment variables if they are set src_dir = os.getenv(\'BACKUP_SRC\', src_dir) dest_dir = os.getenv(\'BACKUP_DEST\', dest_dir) # Ensure the destination directory exists os.makedirs(dest_dir, exist_ok=True) if restore: source = dest_dir target = src_dir else: source = src_dir target = dest_dir for root, dirs, files in os.walk(source): relative_path = os.path.relpath(root, source) target_root = os.path.join(target, relative_path) for dir_name in dirs: target_dir = os.path.join(target_root, dir_name) os.makedirs(target_dir, exist_ok=True) for file_name in files: source_file = os.path.join(root, file_name) target_file = os.path.join(target_root, file_name) pid = os.fork() if pid > 0: # In parent process, wait for child process to complete os.wait() elif pid == 0: # In child process, perform the copy try: shutil.copy2(source_file, target_file) os._exit(0) except Exception as e: print(f\\"Error copying {source_file} to {target_file}: {e}\\") os._exit(1) else: print(\\"Fork failed.\\") # Chmod for directories and files to maintain permissions - this can vary based on specific needs for root, dirs, files in os.walk(target): for dir_name in dirs: dir_path = os.path.join(root, dir_name) os.chmod(dir_path, 0o755) # Giving full permissions to owner and read-execute to others. for file_name in files: file_path = os.path.join(root, file_name) os.chmod(file_path, 0o644) # Giving full permissions to owner and read-only to others. # Example Usage: # backup_and_restore(\'/path/to/source\', \'/path/to/destination\') # backup_and_restore(\'/path/to/source\', \'/path/to/destination\', restore=True)"},{"question":"<|Analysis Begin|> The provided documentation outlines the initialization configuration for embedding Python using the `PyConfig` and `PyPreConfig` structures. Key components include: - **Initialization with `Py_PreInitialize` and `Py_InitializeFromConfig`**: These functions allow for customized initialization of the Python interpreter, supporting both isolated and non-isolated configurations. - **Structure PyConfig**: Holds configurations including command-line arguments, file paths, and various settings. - **Structure PyPreConfig**: Holds preinitialization settings such as memory allocators and locale settings. - **Functions like `PyConfig_SetArgv`, `PyConfig_Read`, and `PyConfig_Clear`**: Used to manage the configuration settings. The challenge involves understanding how to properly initialize Python with customized configurations while considering scenarios such as isolated and non-isolated modes. <|Analysis End|> <|Question Begin|> **Python Initialization Configuration** Given the provided Python `PyConfig` and `PyPreConfig` structures and accompanying functions, create a C program that initializes a customized Python interpreter with the following specifications: 1. The Python interpreter should be initialized in isolated mode. 2. The memory allocator should be set to `PYMEM_ALLOCATOR_PYMALLOC`. 3. The command-line arguments for the Python interpreter should be `[\\"custom_python\\", \\"-c\\", \\"print(\'Hello, world!\')\\"]`. 4. The Python UTF-8 mode should be enabled. 5. Ensure that the configuration is properly handled and that any exceptions or errors during initialization are addressed using the provided functions. Your implementation should include: - Definition and initialization of `PyPreConfig` and `PyConfig`. - Setting the required configurations based on the provided specifications. - Handling possible exceptions and errors during the initialization process. - Execution of the configured Python command. **Input and Output Formats:** - **Input**: No external input is required; the command-line arguments should be hardcoded as specified. - **Output**: The expected output when the Python code `print(\'Hello, world!\')` is executed should be displayed. ```c #include <Python.h> int main(int argc, char **argv) { PyStatus status; // Initialize preconfiguration PyPreConfig preconfig; PyPreConfig_InitIsolatedConfig(&preconfig); preconfig.allocator = PYMEM_ALLOCATOR_PYMALLOC; preconfig.utf8_mode = 1; // Preinitialize Python with preconfiguration status = Py_PreInitialize(&preconfig); if (PyStatus_Exception(status)) { Py_ExitStatusException(status); } // Initialize configuration PyConfig config; PyConfig_InitIsolatedConfig(&config); config.isolated = 1; // Setting command line arguments wchar_t *command_line[] = {L\\"custom_python\\", L\\"-c\\", L\\"print(\'Hello, world!\')\\"}; status = PyConfig_SetArgv(&config, 3, command_line); if (PyStatus_Exception(status)) { PyConfig_Clear(&config); Py_ExitStatusException(status); } // Initialize Python with configuration status = Py_InitializeFromConfig(&config); if (PyStatus_Exception(status)) { PyConfig_Clear(&config); Py_ExitStatusException(status); } // Execute the configured Python command int result = Py_RunMain(); // Clear configuration after use PyConfig_Clear(&config); return result; } ``` This program uses the Python initialization configuration steps to create an isolated, customized Python interpreter that prints \\"Hello, world!\\" to the console. Ensure you understand each configuration step and can adapt it to different scenarios as needed.","solution":"def configure_python_isolated(): Demonstrates configuring a Python interpreter in isolated mode. This is a mock function since actual initialization can only be done in C. Returns a string representing the configuration for testing purposes. # Mock configuration settings allocator = \'PYMEM_ALLOCATOR_PYMALLOC\' utf8_mode = True argv = [\'custom_python\', \'-c\', \\"print(\'Hello, world!\')\\"] config = { \\"isolated\\": True, \\"allocator\\": allocator, \\"utf8_mode\\": utf8_mode, \\"argv\\": argv } # Normally, you\'d initialize Python here but we\'ll return this config for testing return config"},{"question":"# Advanced Audio Processing with `audioop` Module You are provided with raw audio data fragments. Your task is to write a function `process_audio_data` that accepts audio fragments, converts them between different formats, alters the audio data through manipulation, and performs statistical analyses. Function Signature ```python def process_audio_data( audio_fragment: bytes, sample_width: int, target_format: str, operations: list ) -> dict: Parameters: - audio_fragment (bytes): The raw audio data fragment. - sample_width (int): The width of each sample in bytes (1, 2, 3, or 4). - target_format (str): Target encoding format - \'a-law\', \'u-law\', \'adpcm\'. - operations (list): A list of operations to perform on the audio fragment. The operations include: * \\"reverse\\": Reverse the audio samples. * \\"average\\": Return the average value of the samples. * \\"rms\\": Return the root-mean-square of the samples. Returns: - dict: A dictionary with the following keys and their corresponding values: * \\"converted\\": The audio fragment converted to the target format (bytes). * \\"reversed\\": The audio fragment with samples reversed (if \\"reverse\\" operation is requested) (bytes). * \\"average\\": The average value of the samples (if \\"average\\" operation is requested) (int). * \\"rms\\": The root-mean-square of the samples (if \\"rms\\" operation is requested) (float). * If an operation is not requested, its related key should not appear in the returned dictionary. pass ``` Constraints - Assume `target_format` is always one of `\'a-law\'`, `\'u-law\'`, or `\'adpcm\'`. - The `sample_width` will always be 1, 2, 3, or 4. - The list of operations can include any combination of `\\"reverse\\"`, `\\"average\\"`, and `\\"rms\\"`. Examples ```python # Example 1 audio_fragment = b\'x01x02x03x04\' sample_width = 2 target_format = \'a-law\' operations = [\'reverse\', \'average\'] result = process_audio_data(audio_fragment, sample_width, target_format, operations) # Expected output: # result = { # \\"converted\\": b\'x55x5a\', # Example a-law conversion output # \\"reversed\\": b\'x03x04x01x02\', # \\"average\\": 2 # } # Example 2 audio_fragment = b\'x01xffx00x02\' sample_width = 2 target_format = \'u-law\' operations = [\'rms\'] result = process_audio_data(audio_fragment, sample_width, target_format, operations) # Expected output: # result = { # \\"converted\\": b\'x83x7f\', # Example u-law conversion output # \\"rms\\": 1.581 # Example RMS value # } ``` This task requires proficiency in using multiple functions from the `audioop` module and demonstrates the ability to manipulate and analyze raw audio data.","solution":"import audioop def process_audio_data(audio_fragment, sample_width, target_format, operations): result = {} # Convert the audio fragment to the target format if target_format == \'a-law\': converted = audioop.lin2alaw(audio_fragment, sample_width) elif target_format == \'u-law\': converted = audioop.lin2ulaw(audio_fragment, sample_width) elif target_format == \'adpcm\': converted, _ = audioop.lin2adpcm(audio_fragment, sample_width, None) result[\\"converted\\"] = converted # Process reverse operation if \\"reverse\\" in operations: reversed_fragment = audioop.reverse(audio_fragment, sample_width) result[\\"reversed\\"] = reversed_fragment # Process average operation if \\"average\\" in operations: average = audioop.avg(audio_fragment, sample_width) result[\\"average\\"] = average # Process rms operation if \\"rms\\" in operations: rms = audioop.rms(audio_fragment, sample_width) result[\\"rms\\"] = rms return result"},{"question":"# Advanced Coding Assessment **Objective:** Implement a custom event loop policy and a custom child process watcher in Python using `asyncio`. Your implementation should demonstrate a clear understanding of how to manage custom event loops and the associated child watchers for subprocess management. # Instructions: 1. **Custom Event Loop Policy:** Create a class `MyEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. Implement the following functionality: - Override the `get_event_loop()` method to: - Print a message indicating this policy is being used. - Ensure that a new event loop is created if the current context\'s event loop does not exist. **Expected Input:** No direct input. **Expected Output:** - The implementation should return an instance of `asyncio.AbstractEventLoop` as per context. - Print statements for tracing (\\"Using MyEventLoopPolicy\\" when invoked). 2. **Custom Child Watcher:** Create a class `MyChildWatcher` that subclasses `asyncio.AbstractChildWatcher`. Implement the following methods: - `add_child_handler(pid, callback, *args)` to register a new child handler that will be triggered upon a child process termination. - `remove_child_handler(pid)` to remove the previously registered handler for a process with PID equal to `pid`. - `attach_loop(loop)` to attach the child watcher to an event loop. - `is_active()` to return `True` if the watcher is ready to use. - `close()` to ensure any underlying resources are released. **Expected Input:** - `add_child_handler` -> An integer `pid`, a callable `callback`, and any `*args`. - `remove_child_handler` -> An integer `pid`. - `attach_loop` -> An instance of `AbstractEventLoop`. **Expected Output:** - For `add_child_handler`: No direct output, must register handler internally. - For `remove_child_handler`: Return `True` or `False` depending on if the handler existed and was removed. - For `attach_loop`: No direct output, attaches internally. - For `is_active`: Return `True` or `False`. - For `close`: No direct output, cleans resources internally. # Constraints: - You must use Python 3.10 or later. - You may not use external libraries; only standard Python library functions and modules. - Your implementation should handle both Unix and Windows where applicable. - Demonstrate that your custom policies are correctly set and used by performing a sample task that relies on them. # Submission: Submit your `.py` file containing: - The `MyEventLoopPolicy` class. - The `MyChildWatcher` class. - A demonstrative script showing how the custom event loop policy and child watcher work by creating and managing a simple subprocess with them. Note: Ensure your script respects PEP 8 guidelines and is well-documented with comments for clarity.","solution":"import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self): super().__init__() def get_event_loop(self): print(\\"Using MyEventLoopPolicy\\") try: return super().get_event_loop() except RuntimeError: loop = self.new_event_loop() self.set_event_loop(loop) return loop class MyChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): self._loop = None self._callbacks = {} def add_child_handler(self, pid, callback, *args): print(f\\"Adding child handler for PID: {pid}\\") self._callbacks[pid] = (callback, args) def remove_child_handler(self, pid): print(f\\"Removing child handler for PID: {pid}\\") if pid in self._callbacks: del self._callbacks[pid] return True return False def attach_loop(self, loop): print(\\"Attaching loop to child watcher\\") self._loop = loop def is_active(self): return self._loop is not None def close(self): print(\\"Closing child watcher\\") self._callbacks.clear() # Demonstrative script showing how to use custom event loop policy and child watcher import subprocess async def run_subprocess(): process = await asyncio.create_subprocess_exec( \'python\', \'-c\', \'import time; time.sleep(1); print(\\"Subprocess done\\")\', stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() print(stdout.decode()) print(stderr.decode()) def main(): asyncio.set_event_loop_policy(MyEventLoopPolicy()) loop = asyncio.get_event_loop() watcher = MyChildWatcher() watcher.attach_loop(loop) asyncio.get_child_watcher().attach_loop(loop) loop.run_until_complete(run_subprocess()) loop.close() if __name__ == \'__main__\': main()"},{"question":"**Seaborn Coding Assessment** In this task, you will demonstrate your ability to manipulate plot aesthetics using seaborn. Your task is to create a function that generates a plot with specific styles and contexts. You must use different seaborn styling options to achieve the desired output. # Function Signature ```python def styled_plot(data: np.ndarray) -> None: pass ``` # Input - `data` (numpy.ndarray): A 2D NumPy array where each column represents a separate dataset. # Requirements 1. **Create a plot with 4 subplots in a 2x2 grid layout**, each using a different style from seaborn: - Top-left subplot should use the \\"darkgrid\\" style. - Top-right subplot should use the \\"white\\" style. - Bottom-left subplot should use the \\"ticks\\" style. - Bottom-right subplot should use the \\"whitegrid\\" style. 2. **Within each subplot, plot the columns of the data array using seaborn\'s boxplot**. 3. **Set different contexts** for each subplot to modify the scale of plot elements: - Top-left subplot should use \\"paper\\". - Top-right subplot should use \\"notebook\\". - Bottom-left subplot should use \\"talk\\". - Bottom-right subplot should use \\"poster\\". 4. Remove the top and right spines for the \\"white\\" and \\"ticks\\" styles subplots. 5. Ensure that the entire figure layout is tight and there is no overlap between the subplots. # Output - The function should not return anything but should display the generated plot directly using `plt.show()`. # Example ```python import numpy as np data = np.random.normal(size=(20, 4)) + np.arange(4) / 2 styled_plot(data) ``` **Notes**: - Make sure to import seaborn and matplotlib along with any other necessary libraries at the top of your script. - Use seaborn\'s `set_style` and `set_context` functions to control styles and contexts. - Handle the spines using `sns.despine()`.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def styled_plot(data: np.ndarray) -> None: Generates a plot with 4 subplots in a 2x2 grid layout with different styles and contexts. Parameters: - data: 2D NumPy array where each column represents a separate dataset. fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # Top-left subplot: \\"darkgrid\\" style, \\"paper\\" context sns.set_style(\\"darkgrid\\") sns.set_context(\\"paper\\") sns.boxplot(data=data, ax=axes[0, 0]) axes[0, 0].set_title(\\"Darkgrid-Paper\\") # Top-right subplot: \\"white\\" style, \\"notebook\\" context sns.set_style(\\"white\\") sns.set_context(\\"notebook\\") sns.boxplot(data=data, ax=axes[0, 1]) sns.despine(ax=axes[0, 1]) axes[0, 1].set_title(\\"White-Notebook\\") # Bottom-left subplot: \\"ticks\\" style, \\"talk\\" context sns.set_style(\\"ticks\\") sns.set_context(\\"talk\\") sns.boxplot(data=data, ax=axes[1, 0]) sns.despine(ax=axes[1, 0]) axes[1, 0].set_title(\\"Ticks-Talk\\") # Bottom-right subplot: \\"whitegrid\\" style, \\"poster\\" context sns.set_style(\\"whitegrid\\") sns.set_context(\\"poster\\") sns.boxplot(data=data, ax=axes[1, 1]) axes[1, 1].set_title(\\"Whitegrid-Poster\\") plt.tight_layout() plt.show()"},{"question":"**Question**: Implement a custom `FileManager` class in Python that allows for handling file operations akin to the C-API functions described above but using Python\'s `io` module and built-in functionalities. The `FileManager` should manage reading, writing, and retrieving file descriptors for file objects. **Requirements**: 1. **Initialization**: The `FileManager` should be initialized with a file path and a mode which defaults to \'r\' for reading. 2. **Methods**: - `from_file_descriptor(fd, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None)`: Class method to create a `FileManager` instance from an existing file descriptor. - `get_file_descriptor()`: Returns the integer file descriptor associated with the file object. - `read_line(n=-1)`: Reads one line from the file. The behavior should be: - If `n` is `0`, read exactly one line regardless of line length. - If `n` is positive, read up to `n` bytes. - If `n` is negative, read one line regardless of length. - `write_object(obj)`: Writes the string representation of `obj` to the file. - `write_string(s)`: Writes the string `s` to the file. 3. **Open and Close**: The file should be opened on initialization and closed when the `FileManager` instance is deleted. **Constraints**: 1. You must use Python\'s `io` module for file operations. 2. Handle exceptions appropriately. 3. Ensure proper file closure to avoid resource leaks. **Example Usage**: ```python # Initialization file_manager = FileManager(\'/path/to/file.txt\', \'w\') # Write string to file file_manager.write_string(\'Hello Worldn\') # Write object to file file_manager.write_object(123) # Reading from a file using file descriptor fd = file_manager.get_file_descriptor() file_manager_from_fd = FileManager.from_file_descriptor(fd, \'r\') print(file_manager_from_fd.read_line(5)) # Reads up to 5 bytes # Cleanup del file_manager ``` **Expected Input/Output**: - The methods should handle both inputs and outputs correctly as per their specified behavior. - An initialized file manager should cleanly manage its resource and close the file upon deletion or garbage collection. Implement the `FileManager` class below: ```python import io import os class FileManager: def __init__(self, path, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None): # Your implementation here @classmethod def from_file_descriptor(cls, fd, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None): # Your implementation here def get_file_descriptor(self): # Your implementation here def read_line(self, n=-1): # Your implementation here def write_object(self, obj): # Your implementation here def write_string(self, s): # Your implementation here def __del__(self): # Your implementation here # You can add helper functions or classes as needed. ```","solution":"import io import os class FileManager: def __init__(self, path, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None): self.file = open(path, mode, buffering, encoding, errors, newline) @classmethod def from_file_descriptor(cls, fd, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None): file_object = open(fd, mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=False) instance = cls.__new__(cls) instance.file = file_object return instance def get_file_descriptor(self): return self.file.fileno() def read_line(self, n=-1): if n == 0: return self.file.readline() elif n > 0: return self.file.read(n) else: return self.file.readline() def write_object(self, obj): self.file.write(str(obj)) def write_string(self, s): self.file.write(s) def __del__(self): self.file.close()"},{"question":"# Data Cleaning and Validation with Pandas Objective Write a function that reads a CSV file into a pandas DataFrame, then performs several data cleaning and validation tasks. Finally, write tests to ensure the quality of your cleaning functions using pandas assertion functions. Input 1. **CSV File**: The CSV file will have columns \'A\', \'B\', \'C\', and \'D\'. 2. **DataFrame**: The DataFrame to be compared in assert functions. Output Return the cleaned DataFrame. Requirements 1. **Read the CSV file** into a DataFrame. 2. **Drop any rows** where any column has NaN values. 3. **Convert column \'A\'** to integers. 4. **Ensure column \'B\'** only contains values between 0 and 1, if not, set outliers to NaN. 5. **Drop duplicates** in the DataFrame based on column \'C\'. 6. **Rename column \'D\'** to \'new_column_D\'. Testing Write assertions to: 1. **Ensure no NaN values** are present in the cleaned DataFrame. 2. **Ensure all values in column \'B\'** are between 0 and 1. 3. **Ensure column \'A\'** has integer type. 4. **Ensure column \'D\'** has been renamed to \'new_column_D\'. Constraints - You cannot use any library other than pandas and standard Python libraries. Function Signature ```python import pandas as pd def clean_and_validate(filename: str, comparison_df: pd.DataFrame) -> pd.DataFrame: pass def test_clean_and_validate(): pass ``` Example Given a CSV file \\"sample.csv\\" with the following content: ``` A,B,C,D 1,0.5,apple,yes 2,1.2,banana,no 3,-0.1,apple,yes ,0.7,banana,maybe ``` And a comparison DataFrame created from: ```python comparison_data = { \'A\': [1, 3], \'B\': [0.5, 0.7], \'C\': [\'apple\', \'banana\'], \'new_column_D\': [\'yes\', \'maybe\'] } comparison_df = pd.DataFrame(comparison_data) ``` The function `clean_and_validate(\'sample.csv\', comparison_df)` should return a DataFrame: ``` A,B,C,new_column_D 1,0.5,apple,yes 3,0.7,banana,maybe ``` And the function `test_clean_and_validate()` should include assertions using pandas testing functions to validate the cleaned DataFrame.","solution":"import pandas as pd def clean_and_validate(filename: str, comparison_df: pd.DataFrame) -> pd.DataFrame: # Read the CSV file into a DataFrame df = pd.read_csv(filename) # Drop any rows where any column has NaN values df.dropna(inplace=True) # Convert column \'A\' to integers df[\'A\'] = df[\'A\'].astype(int) # Ensure column \'B\' only contains values between 0 and 1, set outliers to NaN, then drop those rows df.loc[(df[\'B\'] < 0) | (df[\'B\'] > 1), \'B\'] = pd.NA df.dropna(inplace=True) # Drop duplicates in the DataFrame based on column \'C\' df.drop_duplicates(subset=\'C\', inplace=True) # Rename column \'D\' to \'new_column_D\' df.rename(columns={\'D\': \'new_column_D\'}, inplace=True) return df"},{"question":"Title: File Processing and String Formatting Problem Statement: You are given a text file named `input.txt` that contains multiple lines of data. Each line of the file has details of a product in the format: ``` ProductName, ProductID, QuantitySold, PricePerUnit ``` Where: - `ProductName` is a string. - `ProductID` is an integer. - `QuantitySold` is an integer. - `PricePerUnit` is a float. You need to write a Python function `process_sales_data(input_filename: str, output_filename: str) -> None` that reads the data from `input.txt`, calculates the total revenue for each product, and writes the results to a new file named `output.txt` in a formatted manner. The output file should have one line for each product in the following format: ``` \\"ProductName (ProductID): QuantitySold units sold at PricePerUnit each - Total Revenue: TotalRevenue\\" ``` Where: - `TotalRevenue` is the product of `QuantitySold` and `PricePerUnit`, formatted to 2 decimal places. Constraints: - You can assume that the input file contains correctly formatted data. - The `input.txt` file may contain multiple lines. - Use formatted string literals (f-strings) for formatting the output string. - Ensure to handle file operations properly, using the `with` statement. Example: Given `input.txt` with the following contents: ``` Laptop, 101, 5, 899.99 Smartphone, 102, 10, 699.49 Tablet, 103, 7, 399.99 ``` The `output.txt` should be: ``` Laptop (101): 5 units sold at 899.99 each - Total Revenue: 4499.95 Smartphone (102): 10 units sold at 699.49 each - Total Revenue: 6994.90 Tablet (103): 7 units sold at 399.99 each - Total Revenue: 2799.93 ``` Function Signature ```python def process_sales_data(input_filename: str, output_filename: str) -> None: pass ``` Notes: - Ensure your code is well-documented and follows Python best practices. - Consider edge cases such as empty lines or excessive white spaces.","solution":"def process_sales_data(input_filename: str, output_filename: str) -> None: with open(input_filename, \'r\') as infile, open(output_filename, \'w\') as outfile: for line in infile: product_name, product_id, quantity_sold, price_per_unit = line.strip().split(\', \') product_id = int(product_id) quantity_sold = int(quantity_sold) price_per_unit = float(price_per_unit) total_revenue = quantity_sold * price_per_unit output_line = (f\\"{product_name} ({product_id}): {quantity_sold} units sold at {price_per_unit:.2f} each \\" f\\"- Total Revenue: {total_revenue:.2f}\\") outfile.write(output_line + \'n\')"},{"question":"Advanced Data Persistence with Custom Serializers and SQLite Objective Demonstrate your understanding of Python\'s data persistence mechanisms, specifically focusing on the `pickle` module for custom serialization and the `sqlite3` module for database interactions. Problem Statement You are required to implement a class that combines custom serialization of objects using `pickle` with data storage and retrieval in an SQLite database. Your solution should ensure efficient handling and retrieval of a large dataset. Requirements 1. **Class Definition**: Implement a class `PersistentStore` that has the following methods: - `__init__(self, db_filename: str)`: - Initializes the database connection using the provided `db_filename`. - `store_object(self, key: str, obj: Any) -> None`: - Serializes the given object `obj` using custom pickling and stores it in the SQLite database with the associated `key`. - `retrieve_object(self, key: str) -> Any`: - Retrieves the serialized object associated with the `key` from the SQLite database, deserializes it, and returns the original object. - `delete_object(self, key: str) -> None`: - Deletes the stored object associated with the `key` from the SQLite database. - `list_keys(self) -> List[str]`: - Returns a list of all keys currently stored in the database. - `close(self) -> None`: - Closes the database connection. 2. **Custom Pickling**: Implement custom serialization for a specific custom class `CustomData`. This class should include various data types (integers, strings, lists, other nested CustomData objects) and ensure that the serialization process handles them correctly: - `CustomData` class should include: - `__init__(self, name: str, value: int, children: List[\'CustomData\'])` - `__reduce__` and `__setstate__` methods for custom pickling. 3. **Constraints**: - Ensure that the database handles up to 1,000,000 records efficiently. - You may not use any external libraries except `sqlite3` and `pickle`. Example Usage ```python # Define a custom data class class CustomData: def __init__(self, name: str, value: int, children: List[\'CustomData\']): self.name = name self.value = value self.children = children def __reduce__(self): return (self.__class__, (self.name, self.value, self.children)) def __setstate__(self, state): self.__init__(*state) # Create PersistentStore instance store = PersistentStore(\'data_storage.db\') # Create some CustomData objects obj1 = CustomData(\'root\', 1, []) obj2 = CustomData(\'branch1\', 2, [obj1]) obj3 = CustomData(\'leaf\', 3, [obj2]) # Store object store.store_object(\'obj3\', obj3) # Retrieve object retrieved_obj = store.retrieve_object(\'obj3\') print(retrieved_obj.name) # Should print \'leaf\' # List keys print(store.list_keys()) # Should print [\'obj3\'] # Delete object store.delete_object(\'obj3\') # Close the store store.close() ``` Performance Consideration You should implement appropriate indexing and batch operations if necessary to ensure the solution can handle the maximum dataset size without significant performance degradation.","solution":"import sqlite3 import pickle from typing import Any, List class CustomData: def __init__(self, name: str, value: int, children: List[\'CustomData\']): self.name = name self.value = value self.children = children def __reduce__(self): return (self.__class__, (self.name, self.value, self.children)) def __setstate__(self, state): self.__init__(*state) class PersistentStore: def __init__(self, db_filename: str): self.conn = sqlite3.connect(db_filename) self._create_table() def _create_table(self): with self.conn: self.conn.execute(\'\'\' CREATE TABLE IF NOT EXISTS store ( key TEXT PRIMARY KEY, value BLOB ) \'\'\') def store_object(self, key: str, obj: Any) -> None: serialized_obj = pickle.dumps(obj) with self.conn: self.conn.execute(\'\'\' REPLACE INTO store (key, value) VALUES (?, ?) \'\'\', (key, serialized_obj)) def retrieve_object(self, key: str) -> Any: cursor = self.conn.execute(\'\'\' SELECT value FROM store WHERE key = ? \'\'\', (key,)) row = cursor.fetchone() if row is None: raise KeyError(f\'Key {key} not found.\') return pickle.loads(row[0]) def delete_object(self, key: str) -> None: with self.conn: self.conn.execute(\'\'\' DELETE FROM store WHERE key = ? \'\'\', (key,)) def list_keys(self) -> List[str]: cursor = self.conn.execute(\'SELECT key FROM store\') return [row[0] for row in cursor] def close(self) -> None: self.conn.close()"},{"question":"Objective: Implement a Python function to compress and then decompress a given text to ensure the data integrity in a round-trip process using the `zlib` module. Problem Statement: Write a function `compress_and_decompress(text: str, level: int = -1) -> bool` that: 1. Compresses the given string `text` using the zlib library. 2. Decompresses the compressed data back to the original text. 3. Verifies the integrity of the decompressed data by comparing it with the original text. 4. Returns `True` if the decompressed data matches the original text, otherwise `False`. Function Signature: ```python def compress_and_decompress(text: str, level: int = -1) -> bool: pass ``` Input: - `text`: A string (1 <= len(text) <= 10^6) representing the text to be compressed and decompressed. - `level`: An optional integer (default -1) to set the compression level (0 to 9) or use the default compression level (-1). Output: - Returns `True` if the decompressed data matches the original text, otherwise `False`. Constraints: - The solution should handle large inputs efficiently. - You must handle any potential exceptions that may arise during compression or decompression. - The use of the `zlib` module is mandatory for this task. Example: ```python assert compress_and_decompress(\\"Hello, World!\\", 6) == True assert compress_and_decompress(\\"Python Programming\\", 9) == True assert compress_and_decompress(\\"Compression Test\\", 0) == True ``` Notes: - Make sure to use the method `zlib.compress` for compression and `zlib.decompress` for decompression. - Handle exceptions gracefully to ensure the function does not crash.","solution":"import zlib def compress_and_decompress(text: str, level: int = -1) -> bool: try: # Compress the text compressed_data = zlib.compress(text.encode(\'utf-8\'), level) # Decompress the data decompressed_data = zlib.decompress(compressed_data).decode(\'utf-8\') # Verify the integrity return decompressed_data == text except Exception as e: return False"},{"question":"# Question You are tasked with creating a customer segmentation pipeline for an e-commerce platform using PyTorch\'s `torch.utils.data` module. Follow the steps below to implement this: 1. **Create a Custom Dataset**: - Implement a map-style dataset called `CustomerDataset`. This dataset should read customer data stored in a CSV file. Each row represents a customer with various features (e.g., age, annual income, spending score). - The `CustomerDataset` should have methods `__getitem__(self, idx)` to get a sample and `__len__(self)` to return the number of samples. 2. **Custom Collate Function**: - Create a custom collate function called `custom_collate_fn` that batches the customer features into tensors. 3. **Create a DataLoader**: - Instantiate a `DataLoader` for the `CustomerDataset`. Configure it to use the custom collate function, fetch data in shuffled order with a specified batch size, and demonstrate multi-process data loading with 4 worker processes. # Instructions 1. **CustomerDataset Implementation**: - The dataset should read from a CSV file where the file path is passed during initialization. - Each sample should be returned as a dictionary with customer attributes as keys. - Assume the CSV file has headers. 2. **custom_collate_fn Implementation**: - The collate function should convert the list of dictionaries into a batch, where each feature is grouped into a single tensor. 3. **DataLoader Configuration**: - Instantiate a `DataLoader` with the following parameters: - `batch_size=32` - `shuffle=True` - `num_workers=4` - Use the custom collate function. - Pin memory for faster transfer to CUDA-enabled GPUs. # Example CSV Content ``` CustomerID,Age,Annual_Income,Spending_Score 1,19,15,39 2,21,15,81 3,20,16,6 ... ``` # Sample Output Ensure your implementation supports the loading of the dataset and prints out batches of data. Demonstrate this by printing the first batch to verify it collates correctly. # Submission Submit your implementation for `CustomerDataset`, `custom_collate_fn`, and DataLoader configuration. Include code that prints the first batch of processed data. # Code Template ```python import torch from torch.utils.data import Dataset, DataLoader class CustomerDataset(Dataset): def __init__(self, file_path): # Your code to initialize and read the CSV def __len__(self): # Your code to return the length def __getitem__(self, idx): # Your code to return a sample as a dictionary def custom_collate_fn(batch): # Your code to collate a batch of samples # Example usage dataset = CustomerDataset(\'path_to_csv_file.csv\') dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=custom_collate_fn, pin_memory=True) # Print the first batch to check for batch in dataloader: print(batch) break ```","solution":"import torch from torch.utils.data import Dataset, DataLoader import pandas as pd class CustomerDataset(Dataset): def __init__(self, file_path): Initializes the dataset by loading the CSV file. self.data = pd.read_csv(file_path) def __len__(self): Returns the number of samples in the dataset. return len(self.data) def __getitem__(self, idx): Returns a single sample as a dictionary of features. sample = self.data.iloc[idx] return { \\"CustomerID\\": sample[\\"CustomerID\\"], \\"Age\\": sample[\\"Age\\"], \\"Annual_Income\\": sample[\\"Annual_Income\\"], \\"Spending_Score\\": sample[\\"Spending_Score\\"] } def custom_collate_fn(batch): Custom collate function for batching. Converts the list of dictionaries into a batch. batch_data = { \\"CustomerID\\": torch.tensor([item[\\"CustomerID\\"] for item in batch], dtype=torch.int64), \\"Age\\": torch.tensor([item[\\"Age\\"] for item in batch], dtype=torch.float32), \\"Annual_Income\\": torch.tensor([item[\\"Annual_Income\\"] for item in batch], dtype=torch.float32), \\"Spending_Score\\": torch.tensor([item[\\"Spending_Score\\"] for item in batch], dtype=torch.float32) } return batch_data # Example usage if __name__ == \\"__main__\\": dataset = CustomerDataset(\'path_to_csv_file.csv\') dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=custom_collate_fn, pin_memory=True) # Print the first batch to check for batch in dataloader: print(batch) break"},{"question":"**Objective:** Implement a function to create a customized Seaborn plot using the seaborn.objects module. **Problem Statement:** You are provided with a dataset of global carbon dioxide (CO2) emissions, which has three columns: \'Year\', \'Country\', and \'CO2_Emissions\'. You need to create a function `create_co2_emission_plot` that does the following: 1. Loads the dataset. 2. Pivots the data so that years are the index, countries are columns, and CO2 emissions are the values. 3. Interpolates missing values. 4. Stacks the pivoted data back into a long format. 5. Creates a facet grid that shows an Area plot of CO2 emissions over the years for each country. Each plot should be customizable by its edgecolor based on the country. 6. Additionally, combine an Area plot with a Line plot to show a shaded region with a line representing CO2 emissions. **Specifications:** - Function: `create_co2_emission_plot` - Input: None (the dataset is expected to be loaded within the function) - Output: A Seaborn plot object. **Example usage:** ```python plot = create_co2_emission_plot() plot.show() ``` **Constraints:** - Assume the dataset is named `co2_emissions` in CSV format and can be loaded using `pandas.read_csv(\'co2_emissions.csv\')`. - The plot should have each country\'s CO2 emissions as a separate facet, with each plot wrapping every 4 columns. - Set the edge colors of the areas based on the country. - Ensure the filled areas are combined with lines to visualize the trends clearly. **Performance:** - Efficiently handle datasets of at least 10,000 rows. **Dataset format (CSV):** | Year | Country | CO2_Emissions | |------|---------|---------------| | 2000 | USA | 5000 | | 2000 | China | 6000 | | 2001 | USA | 5200 | | ... | ... | ... | ```python import pandas as pd import seaborn.objects as so def create_co2_emission_plot(): # Load dataset co2_emissions = pd.read_csv(\'co2_emissions.csv\') # Pivot data pivot_df = ( co2_emissions .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"CO2_Emissions\\") .interpolate() .stack() .rename(\\"CO2_Emissions\\") .reset_index() .sort_values(\\"Country\\") ) # Create plot plot = so.Plot(pivot_df, x=\\"Year\\", y=\\"CO2_Emissions\\").facet(\\"Country\\", wrap=4) plot.add(so.Area(edgewidth=0)).add(so.Line()) return plot ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_co2_emission_plot(): # Load the dataset co2_emissions = pd.read_csv(\'co2_emissions.csv\') # Pivot the data pivot_df = co2_emissions.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"CO2_Emissions\\") # Interpolate missing values pivot_df = pivot_df.interpolate() # Stack the pivoted data back into long format long_format_df = pivot_df.stack().reset_index().rename(columns={0: \'CO2_Emissions\'}) # Create the facet grid plot g = sns.FacetGrid(long_format_df, col=\\"Country\\", col_wrap=4, sharey=False) g.map_dataframe(sns.lineplot, x=\\"Year\\", y=\\"CO2_Emissions\\") g.map_dataframe(plt.fill_between, x=\\"Year\\", y1=\\"CO2_Emissions\\", alpha=0.3) # Customize plot for ax in g.axes.flat: country = ax.get_title().split(\\" = \\")[-1] ax.set_facecolor(\\"white\\") line = ax.lines[0] line.set_color(\\"black\\") ax.collections[0].set_edgecolor(line.get_color()) plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Global CO2 Emissions by Country\') return g"},{"question":"# Web Crawler with Robots.txt Adherence Objective: Implement a Python function `crawl_website(base_url, useragent)` that crawls the given `base_url`, but adheres to the website\'s `robots.txt` rules for the specified `useragent`. Description: 1. The function should first read and parse the `robots.txt` file located at the `base_url`. 2. It should then check if the `useragent` is allowed to fetch the main page (`base_url`). If fetching the main page is allowed, the function should proceed to fetch and print the content of the main page. 3. The function should also respect the `Crawl-delay` and `Request-rate` parameters specified in the `robots.txt` file. 4. If the site maps are specified in the `robots.txt`, the function should print the list of sitemap URLs. Input: - `base_url`: A string representing the base URL of the website (e.g., \\"http://www.example.com\\"). - `useragent`: A string representing the user agent (e.g., \\"MyCrawler\\"). Output: - The function should print the content of the main page if the user agent is allowed to fetch it. - The function should print the list of sitemap URLs if specified in the `robots.txt`. Constraints: - The function should handle network errors gracefully. - Assume that the `base_url` provided ends with a trailing slash. - All requests must respect the `Crawl-delay` and `Request-rate` parameters. - If the `Crawl-delay` or `Request-rate` is not specified, proceed without any delay restrictions. Example Usage: ```python def crawl_website(base_url, useragent): import urllib.robotparser from urllib import request, error import time # Initialize the RobotFileParser rp = urllib.robotparser.RobotFileParser() rp.set_url(base_url + \\"robots.txt\\") try: # Read and parse the robots.txt file rp.read() except error.URLError: print(\\"Error fetching robots.txt\\") return # Check if the main page can be fetched if rp.can_fetch(useragent, base_url): try: # Fetch and print the content of the main page response = request.urlopen(base_url) main_page_content = response.read().decode(\'utf-8\') print(main_page_content) except error.URLError: print(\\"Error fetching main page\\") else: print(f\\"Fetching of {base_url} is not allowed for user agent \'{useragent}\'\\") # Print the sitemaps sitemaps = rp.site_maps() if sitemaps: print(\\"Sitemaps:\\") for sitemap in sitemaps: print(sitemap) else: print(\\"No sitemaps found\\") # Respecting crawl delay crawl_delay = rp.crawl_delay(useragent) if crawl_delay: time.sleep(crawl_delay) # Example Usage of the function crawl_website(\\"http://www.example.com/\\", \\"MyCrawler\\") ``` Notes: - Ensure you handle HTTP errors appropriately. - Use `time.sleep` for implementing the crawl delay. - Utilize the `urllib` library to make HTTP requests.","solution":"def crawl_website(base_url, useragent): import urllib.robotparser from urllib import request, error import time # Initialize the RobotFileParser rp = urllib.robotparser.RobotFileParser() rp.set_url(base_url + \\"robots.txt\\") try: # Read and parse the robots.txt file rp.read() except error.URLError: print(\\"Error fetching robots.txt\\") return # Check if the main page can be fetched if rp.can_fetch(useragent, base_url): try: # Fetch and print the content of the main page response = request.urlopen(base_url) main_page_content = response.read().decode(\'utf-8\') print(main_page_content) except error.URLError: print(\\"Error fetching main page\\") else: print(f\\"Fetching of {base_url} is not allowed for user agent \'{useragent}\'\\") # Print the sitemaps sitemaps = rp.site_maps() if sitemaps: print(\\"Sitemaps:\\") for sitemap in sitemaps: print(sitemap) else: print(\\"No sitemaps found\\") # Respecting crawl delay crawl_delay = rp.crawl_delay(useragent) if crawl_delay: time.sleep(crawl_delay)"},{"question":"# Seaborn Coding Assessment Question Objective: Write a Python function using seaborn that creates a customized plot as per the given specifications. The function should take input data and layout parameters, and output a seaborn plot with specific customizations. Input: - `data_df` (pd.DataFrame): A DataFrame containing at least two numerical columns for plotting. - `size` (tuple): A tuple specifying the dimensions of the plot (width, height). - `layout_engine` (str): A string specifying the layout engine to be used (e.g., \\"constrained\\"). - `extent` (list): A list specifying the extent of the plot relative to the figure (e.g., [0, 0, 0.8, 1]). Output: - A seaborn plot with the specified size, layout engine, and extent. Constraints: - The input DataFrame must have at least two columns. - The `size` tuple must contain two positive integers. - The `layout_engine` must be a valid string accepted by seaborn\'s `.layout()` method. - The `extent` list must contain four values between 0 and 1. # Example Function Signature: ```python import seaborn as sns import pandas as pd def customized_seaborn_plot(data_df: pd.DataFrame, size: tuple, layout_engine: str, extent: list): # Create seaborn plot object p = sns.objects.Plot(data_df).layout(size=size).facet([\\"Column1\\", \\"Column2\\"], [\\"Category1\\", \\"Category2\\"]).layout(engine=layout_engine) # Adjust the extent of the plot p.layout(extent=extent).show() # Example usage # df = pd.DataFrame({\'Column1\': [1, 2, 3], \'Column2\': [4, 5, 6], \'Category1\': [7, 8, 9], \'Category2\': [10, 11, 12]}) # customized_seaborn_plot(df, (6, 6), \\"constrained\\", [0, 0, 0.8, 1]) ``` Task: - Implement the `customized_seaborn_plot` function as described. - Ensure the plot correctly reflects the dimensions, layout engine, and extent as specified in the input parameters. - Provide at least one example demonstrating the usage of your function.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def customized_seaborn_plot(data_df: pd.DataFrame, size: tuple, layout_engine: str, extent: list): Create a customized seaborn plot with specific size, layout engine, and extent. Parameters: data_df (pd.DataFrame): DataFrame containing at least two numerical columns for plotting. size (tuple): Dimensions of the plot (width, height). layout_engine (str): Layout engine to be used (e.g., \\"constrained\\"). extent (list): Extent of the plot relative to the figure (e.g., [0, 0, 0.8, 1]). assert len(data_df.columns) >= 2, \\"DataFrame must contain at least two columns.\\" assert len(size) == 2 and all(isinstance(i, (int, float)) and i > 0 for i in size), \\"Size must contain two positive values.\\" assert isinstance(layout_engine, str), \\"Layout engine must be a valid string.\\" assert len(extent) == 4 and all(0 <= i <= 1 for i in extent), \\"Extent must contain four values between 0 and 1.\\" fig, ax = plt.subplots(figsize=size) sns.scatterplot(x=data_df.iloc[:, 0], y=data_df.iloc[:, 1], ax=ax) plt.tight_layout() plt.subplots_adjust(left=extent[0], bottom=extent[1], right=extent[2], top=extent[3]) plt.show()"},{"question":"Objective Your task is to write a Python function that encodes a file to Base64 and decodes it back to the original content using the \'binascii\' module. This will test your understanding of file operations, encoding/decoding binary data, and usage of the \'binascii\' module functions. Problem Statement Write a function `encode_decode_file(file_path: str) -> bool` that: 1. Reads the binary content of a file at the specified `file_path`. 2. Encodes the binary content to a Base64 encoded ASCII string using `binascii.b2a_base64`. 3. Decodes the Base64 encoded ASCII string back to the original binary content using `binascii.a2b_base64`. 4. Writes the decoded binary content to a new file located at the same directory as the input file but with \\"_decoded\\" appended to the original file name. 5. Compares the newly created file content with the original file content to check if they are the same. The function should return `True` if the encoded, then decoded content matches the original content, `False` otherwise. Constraints - The input file path will be a valid path to a binary file. - The function should handle errors gracefully and return `False` in case of any exceptions encountered. - The Base64 encoding should include a newline character. Example ```python def encode_decode_file(file_path: str) -> bool: import os import binascii try: with open(file_path, \'rb\') as f: original_data = f.read() encoded_data = binascii.b2a_base64(original_data) decoded_data = binascii.a2b_base64(encoded_data) new_file_path = file_path.replace(\'.\',\'_decoded.\') with open(new_file_path, \'wb\') as f: f.write(decoded_data) with open(new_file_path, \'rb\') as f: new_data = f.read() return original_data == new_data except (binascii.Error, IOError) as e: # Handle file reading/writing, encoding/decoding errors. return False ``` Notes - Make sure to handle file operations and errors properly to ensure robust implementation. - You may use the following import statements if needed: ```python import os import binascii ```","solution":"import os import binascii def encode_decode_file(file_path: str) -> bool: Encodes the file at the given path to Base64 and decodes it back, then checks if the decoded content matches the original content. Args: - file_path: The path to the input file. Returns: - True if the decoded content matches the original content, False otherwise. try: # Read original file content with open(file_path, \'rb\') as original_file: original_data = original_file.read() # Encode to Base64 encoded_data = binascii.b2a_base64(original_data) # Decode back to binary decoded_data = binascii.a2b_base64(encoded_data) # Create a new file path with \\"_decoded\\" appended to the original file name directory, filename = os.path.split(file_path) name, ext = os.path.splitext(filename) new_file_path = os.path.join(directory, f\\"{name}_decoded{ext}\\") # Write the decoded data to the new file with open(new_file_path, \'wb\') as new_file: new_file.write(decoded_data) # Read the new file content with open(new_file_path, \'rb\') as decoded_file: new_data = decoded_file.read() # Compare original and new data return original_data == new_data except (binascii.Error, IOError) as e: # Handle file reading/writing, encoding/decoding errors return False"},{"question":"# **XML Document Manipulation using DOM API in Python** **Objective**: Write a Python function to manipulate and query an XML document using the `xml.dom` package. **Problem Statement**: Given an XML string representing a book catalog, implement functions to: 1. Parse the XML string and create a DOM `Document` object. 2. Add a new book entry to the catalog. 3. Update the price of a book given its title. 4. Retrieve a list of all book titles. 5. Handle exceptions for invalid operations. **Input and Output Formats**: 1. **parse_catalog(xml_str: str) -> xml.dom.minidom.Document** - **Input**: `xml_str` - A string containing XML data. - **Output**: A DOM `Document` object representing the parsed XML. 2. **add_book(doc: xml.dom.minidom.Document, title: str, author: str, price: float) -> None** - **Input**: `doc` - A DOM `Document` object. `title` - The title of the new book. `author` - The author of the new book. `price` - The price of the new book. - **Output**: None (Modifies the DOM `Document` in-place). 3. **update_book_price(doc: xml.dom.minidom.Document, title: str, new_price: float) -> None** - **Input**: `doc` - A DOM `Document` object. `title` - The title of the book to update. `new_price` - The new price of the book. - **Output**: None (Modifies the DOM `Document` in-place). - **Exception Handling**: Raise a `ValueError` if the book with the given title is not found. 4. **get_book_titles(doc: xml.dom.minidom.Document) -> List[str]** - **Input**: `doc` - A DOM `Document` object. - **Output**: A list of book titles as strings. **Constraints**: - Assume valid XML input for parsing. - The XML document will follow the format: ```xml <catalog> <book> <title>Book Title</title> <author>Author Name</author> <price>99.99</price> </book> <!-- More book entries --> </catalog> ``` **Example**: ```python xml_data = <catalog> <book> <title>Introduction to Python</title> <author>John Smith</author> <price>29.99</price> </book> </catalog> # Parse the XML string into a DOM Document doc = parse_catalog(xml_data) # Add a new book to the catalog add_book(doc, \\"Advanced Python\\", \\"Jane Doe\\", 39.99) # Update the price of an existing book update_book_price(doc, \\"Introduction to Python\\", 34.99) # Retrieve all book titles titles = get_book_titles(doc) print(titles) # Output: [\\"Introduction to Python\\", \\"Advanced Python\\"] ``` Implement the functions `parse_catalog`, `add_book`, `update_book_price`, and `get_book_titles` as described.","solution":"from xml.dom.minidom import Document, parseString def parse_catalog(xml_str: str) -> Document: Parses the XML string and returns a DOM Document object. return parseString(xml_str) def add_book(doc: Document, title: str, author: str, price: float) -> None: Adds a new book entry to the catalog. catalog = doc.documentElement new_book = doc.createElement(\'book\') title_element = doc.createElement(\'title\') title_text = doc.createTextNode(title) title_element.appendChild(title_text) author_element = doc.createElement(\'author\') author_text = doc.createTextNode(author) author_element.appendChild(author_text) price_element = doc.createElement(\'price\') price_text = doc.createTextNode(str(price)) price_element.appendChild(price_text) new_book.appendChild(title_element) new_book.appendChild(author_element) new_book.appendChild(price_element) catalog.appendChild(new_book) def update_book_price(doc: Document, title: str, new_price: float) -> None: Updates the price of the specified book by its title. Raises a ValueError if the book is not found. books = doc.getElementsByTagName(\'book\') for book in books: title_element = book.getElementsByTagName(\'title\')[0] if title_element.firstChild.nodeValue == title: price_element = book.getElementsByTagName(\'price\')[0] price_element.firstChild.nodeValue = str(new_price) return raise ValueError(f\\"Book with title \'{title}\' not found\\") def get_book_titles(doc: Document) -> list: Retrieves and returns a list of all book titles. titles = [] books = doc.getElementsByTagName(\'book\') for book in books: title_element = book.getElementsByTagName(\'title\')[0] titles.append(title_element.firstChild.nodeValue) return titles"},{"question":"<|Analysis Begin|> The provided documentation gives extensive details about the `poplib` module in Python, which is a client-side implementation for connecting and interacting with POP3 mail servers. The module primarily offers two classes: `POP3` for regular connections and `POP3_SSL` for SSL encrypted connections. Each class provides methods corresponding to the POP3 commands such as fetching messages, listing messages, deleting messages, and other utility functions like setting debugging levels, starting TLS sessions, etc. Key concepts in this documentation include: - Establishing connections to a POP3 server. - Authenticating users. - Retrieving, listing, and deleting messages. - Handling encrypted communication. - Error handling using the `poplib.error_proto` exception. - Additional capabilities (like UTF-8 mode and server capabilities) and command extensions (like APOP and RPOP authentication). To create a challenging and clear question that demonstrates understanding of both fundamental and advanced concepts of the `poplib` module, it would be suitable to focus on implementing a utility that establishes a connection to a POP3 server, authenticates a user, retrieves emails, and provides certain functionalities such as listing and deleting messages. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Email Client using `poplib` Module** You are required to implement a Python function that connects to a POP3 server, logs in with given credentials, retrieves and lists the messages in the inbox, and deletes a specific message if instructed. This will involve creating a simple email client interface using the `poplib` module. Function Signature ```python def email_client(host: str, port: int, username: str, password: str, delete_msg_num: int = None) -> List[str]: pass ``` Inputs - `host` (str): The hostname of the POP3 server. - `port` (int): The port number to connect to (use 110 for regular connection or 995 for SSL). - `username` (str): The username for POP3 login. - `password` (str): The password for POP3 login. - `delete_msg_num` (int, optional): The message number to delete after listing messages. If not specified or `None`, no messages should be deleted. Output - The function should return a list of strings, where each string represents a multiline email message fetched from the server. Constraints - The function should establish a connection to the server using `POP3` or `POP3_SSL` as appropriate based on the port number. - Handle any connection or authentication errors gracefully by catching `poplib.error_proto` exceptions and providing a meaningful error message. - After listing emails, if `delete_msg_num` is specified, delete the corresponding message and ensure changes are committed by calling `quit()`. Example Usage ```python # Example input host = \\"pop.example.com\\" port = 995 username = \\"user@example.com\\" password = \\"password\\" delete_msg_num = 1 # Call the function emails = email_client(host, port, username, password, delete_msg_num) # Expected output # emails will be a list of strings, each representing an email message. ``` Additional Information - Make use of methods like `retr()`, `list()`, `dele()`, and `quit()` from the `poplib` module to perform the tasks required. - Use `POP3_SSL` if the port provided is 995; otherwise, use regular `POP3`. Performance Requirements - Ensure that connections are properly closed in case of errors to avoid leaving open connections. - Handle large mailboxes efficiently. You are provided an example usage of the `poplib` module at the end of the documentation to get started. **Good luck!**","solution":"import poplib from typing import List def email_client(host: str, port: int, username: str, password: str, delete_msg_num: int = None) -> List[str]: Connect to the POP3 server, authenticate, retrieve, list messages, and optionally delete a message. Parameters: - host (str): The hostname of the POP3 server. - port (int): The port number to connect to (110 for regular connection or 995 for SSL). - username (str): The username for POP3 login. - password (str): The password for POP3 login. - delete_msg_num (int, optional): The message number to delete after listing messages. Returns: - List[str]: A list of strings, where each string represents a multiline email message. if port == 995: server = poplib.POP3_SSL(host, port) else: server = poplib.POP3(host, port) try: server.user(username) server.pass_(password) # Retrieve and list messages messages = [] num_messages = len(server.list()[1]) for i in range(1, num_messages + 1): response, lines, octets = server.retr(i) msg_content = \'n\'.join(map(lambda x: x.decode(\'utf-8\'), lines)) messages.append(msg_content) # If delete_msg_num is specified, delete the corresponding message if delete_msg_num is not None and 1 <= delete_msg_num <= num_messages: server.dele(delete_msg_num) # Commit changes server.quit() return messages except poplib.error_proto as e: server.quit() raise RuntimeError(f\\"POP3 error: {e}\\")"},{"question":"**Question: Constructing a Complex Pipeline with FeatureUnion and ColumnTransformer** **Objective:** Implement a complex preprocessing pipeline using scikit-learn that involves multiple transformation steps applied in parallel and sequence. You should demonstrate a clear understanding of how to create and manage composite estimators. **Problem Statement:** You are provided with a dataset containing numerical and categorical features. Your task is to: 1. Preprocess the dataset using a `ColumnTransformer` to apply distinct preprocessing steps to numerical and categorical columns. 2. Combine multiple preprocessing steps using `FeatureUnion`. 3. Chain these preprocessing steps with a classifier in a `Pipeline`. 4. Fit the pipeline to the provided dataset and make predictions. **Dataset:** ```python import pandas as pd data = pd.DataFrame({ \'age\': [25, 32, 47, 51, 62], \'city\': [\'New York\', \'Los Angeles\', \'New York\', \'Chicago\', \'Los Angeles\'], \'income\': [50000, 60000, 52000, 58000, 62000], \'gender\': [\'male\', \'female\', \'female\', \'male\', \'female\'], \'label\': [0, 1, 0, 1, 1] }) ``` **Steps to Implement:** 1. **ColumnTransformer**: Separate the dataset into numerical (`age` and `income`) and categorical (`city` and `gender`) columns. - Use `StandardScaler` for numerical columns. - Apply `OneHotEncoder` for categorical columns. 2. **FeatureUnion**: Create a `FeatureUnion` to combine the transformations of numerical and categorical features into a unified feature space. 3. **Pipeline**: Construct a `Pipeline` that: - Uses the `FeatureUnion` for preprocessing. - Includes a classifier, e.g., `LogisticRegression`. 4. **Train & Evaluate**: - Fit the pipeline on the provided dataset. - Make predictions on the same dataset (or a split if desired). - Print the transformed features and predictions. **Constraints and Requirements:** - You must use `ColumnTransformer`, `FeatureUnion`, and `Pipeline` from scikit-learn. - Handle any potential issues in the dataset, including inconsistent or missing data. - Clearly comment on each step of your implementation for clarity. **Expected Output:** - The transformed feature set after applying `ColumnTransformer`. - Predictions on the dataset from the pipeline. **Performance Note:** - Ensure the solution is optimized and does not redundantly process the data. **Example Partial Implementation:** ```python from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LogisticRegression # 1. Define the Column Transformer preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'age\', \'income\']), (\'cat\', OneHotEncoder(), [\'city\', \'gender\']) ]) # 2. Combine preprocessors using FeatureUnion feature_union = FeatureUnion(transformer_list=[ (\'preprocessor\', preprocessor) ]) # 3. Create Pipeline with a classifier pipeline = Pipeline(steps=[ (\'union\', feature_union), (\'classifier\', LogisticRegression()) ]) # Fit the pipeline on data pipeline.fit(data.drop(\'label\', axis=1), data[\'label\']) # Make predictions predictions = pipeline.predict(data.drop(\'label\', axis=1)) # Print the results print(\\"Transformed Features:n\\", pipeline.named_steps[\'union\'].transform(data.drop(\'label\', axis=1))) print(\\"Predictions:n\\", predictions) ``` **Task for the Student:** Complete the implementation, ensuring all parts are working correctly. Test your code with additional customizations if required and validate the results.","solution":"from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LogisticRegression import pandas as pd # Dataset data = pd.DataFrame({ \'age\': [25, 32, 47, 51, 62], \'city\': [\'New York\', \'Los Angeles\', \'New York\', \'Chicago\', \'Los Angeles\'], \'income\': [50000, 60000, 52000, 58000, 62000], \'gender\': [\'male\', \'female\', \'female\', \'male\', \'female\'], \'label\': [0, 1, 0, 1, 1] }) # Define the ColumnTransformer to preprocess numerical and categorical columns preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'age\', \'income\']), (\'cat\', OneHotEncoder(), [\'city\', \'gender\']) ]) # Define the FeatureUnion to combine the preprocessing steps feature_union = FeatureUnion(transformer_list=[ (\'preprocessor\', preprocessor) ]) # Create the pipeline by combining FeatureUnion with a classifier pipeline = Pipeline(steps=[ (\'union\', feature_union), (\'classifier\', LogisticRegression()) ]) # Split data into features and labels X = data.drop(\'label\', axis=1) y = data[\'label\'] # Fit the pipeline on the dataset pipeline.fit(X, y) # Make predictions predictions = pipeline.predict(X) # Print the transformed features and predictions transformed_features = pipeline.named_steps[\'union\'].transform(X) print(\\"Transformed Features:n\\", transformed_features) print(\\"Predictions:n\\", predictions)"},{"question":"Objective: You will implement a function that utilizes scikit-learn\'s `validation_curve` and `learning_curve` utilities to analyze the bias-variance characteristics of an SVM classifier with a linear kernel. Your function should compute and plot both validation and learning curves, providing insights into the model\'s performance and the effect of training data size and regularization parameter (C). Dataset: Use the Iris dataset provided by scikit-learn (`sklearn.datasets.load_iris`) for this task. Specifications: 1. **Function Name**: `analyze_model_performance` 2. **Inputs**: - `param_range`: List of values for the regularization parameter C to be evaluated in the validation curve. - `train_sizes`: List of sample sizes to be used in the learning curve. - `cv`: Number of cross-validation folds (default is 5). 3. **Outputs**: - The function should display the plots for the validation and learning curves. - The output should not return any values; the primary focus is on generating the plots. 4. **Constraints**: - Use `SVC(kernel=\\"linear\\")` as the model. - Ensure the random state is fixed for reproducibility during shuffling (use `random_state=0`). 5. **Performance Requirement**: The plots should clearly show the performance of the model across different values of the regularization parameter C and varying training sizes, using the specified number of cross-validation folds. Example: Here is a skeleton function signature for you to start: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.utils import shuffle from sklearn.model_selection import ValidationCurveDisplay, LearningCurveDisplay def analyze_model_performance(param_range, train_sizes, cv=5): # Load and shuffle the Iris dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Create an SVM classifier with a linear kernel model = SVC(kernel=\\"linear\\") # Compute the validation curve train_scores_vc, valid_scores_vc = validation_curve( model, X, y, param_name=\\"C\\", param_range=param_range, cv=cv) # Plot the validation curve ValidationCurveDisplay.from_estimator( model, X, y, param_name=\\"C\\", param_range=param_range, cv=cv) plt.show() # Compute the learning curve train_sizes, train_scores_lc, valid_scores_lc = learning_curve( model, X, y, train_sizes=train_sizes, cv=cv) # Plot the learning curve LearningCurveDisplay.from_estimator( model, X, y, train_sizes=train_sizes, cv=cv) plt.show() # Example usage: # analyze_model_performance(param_range=np.logspace(-7, 3, 10), train_sizes=[50, 80, 110]) ``` Notes: - Ensure that the plots are clearly labeled. - Comment your code appropriately to describe each step.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.utils import shuffle def analyze_model_performance(param_range, train_sizes, cv=5): Analyze and plot the validation and learning curves for an SVM classifier with a linear kernel on the Iris dataset. Args: param_range (list): List of values for the regularization parameter C to be evaluated in the validation curve. train_sizes (list): List of sample sizes to be used in the learning curve. cv (int): Number of cross-validation folds (default is 5). Returns: None. Displays the validation and learning curve plots. # Load and shuffle the Iris dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Create an SVM classifier with a linear kernel model = SVC(kernel=\\"linear\\") # Compute the validation curve train_scores_vc, valid_scores_vc = validation_curve( model, X, y, param_name=\\"C\\", param_range=param_range, cv=cv) # Plot the validation curve plt.figure() plt.plot(param_range, np.mean(train_scores_vc, axis=1), label=\'Training score\') plt.plot(param_range, np.mean(valid_scores_vc, axis=1), label=\'Validation score\') plt.title(\'Validation Curve for SVM\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.legend(loc=\'best\') plt.show() # Compute the learning curve train_sizes, train_scores_lc, valid_scores_lc = learning_curve( model, X, y, train_sizes=train_sizes, cv=cv) # Plot the learning curve plt.figure() plt.plot(train_sizes, np.mean(train_scores_lc, axis=1), label=\'Training score\') plt.plot(train_sizes, np.mean(valid_scores_lc, axis=1), label=\'Validation score\') plt.title(\'Learning Curve for SVM\') plt.xlabel(\'Training examples\') plt.ylabel(\'Score\') plt.legend(loc=\'best\') plt.show()"},{"question":"# Advanced Coding Challenge: Custom Object Serialization with Pickle Objective: Implement a custom serialization and deserialization process for a complex object structure using the `pickle` module\'s advanced features. This will assess your understanding of the pickle module\'s functionalities and your ability to handle special pickling cases involving custom objects. Problem Statement: You are required to implement a custom serialization and deserialization mechanism for a collection of `Person` objects using the `pickle` module. Each `Person` object has attributes for `name`, `age`, and a list of references to other `Person` objects (friends). Furthermore, the `Person` class contains methods to display and update details. Serialization should maintain the object relationships (friend references) accurately and efficiently. Requirements: 1. Implement the `Person` class with attributes: - `name`: A string representing the person\'s name. - `age`: An integer representing the person\'s age. - `friends`: A list of `Person` objects representing friends. 2. Override the `__getstate__()` and `__setstate__()` methods to customize the serialization of the `Person` objects, ensuring that the `friends` references are preserved correctly. 3. Write a function `serialize_people(people_list: List[Person], filename: str)` that serializes a list of `Person` objects to a file. 4. Write a function `deserialize_people(filename: str) -> List[Person]` that deserializes the list of `Person` objects from a file, ensuring that the `friends` relationships are restored accurately. 5. Test the implemented functions with a complex object structure involving `Person` objects with mutual and nested friend relationships. Function Signatures: ```python import pickle class Person: def __init__(self, name: str, age: int): self.name = name self.age = age self.friends = [] # List[Person] def add_friend(self, friend): self.friends.append(friend) def __getstate__(self): # Implement custom state retrieval for pickling pass def __setstate__(self, state): # Implement custom state setting for unpickling pass def serialize_people(people_list, filename: str): Serialize the list of Person objects to a file with open(filename, \'wb\') as file: pickle.dump(people_list, file) def deserialize_people(filename: str) -> list: Deserialize the list of Person objects from a file with open(filename, \'rb\') as file: return pickle.load(file) ``` Example Usage: ```python # Create Person objects alice = Person(\\"Alice\\", 30) bob = Person(\\"Bob\\", 25) carol = Person(\\"Carol\\", 27) # Establish friendships alice.add_friend(bob) bob.add_friend(carol) carol.add_friend(alice) # Serialize the list of persons serialize_people([alice, bob, carol], \'people.pkl\') # Deserialize the list of persons people_list = deserialize_people(\'people.pkl\') # Verify the relationships are restored for person in people_list: print(f\\"{person.name}\'s friends: {[friend.name for friend in person.friends]}\\") ``` Constraints: - The list of `Person` objects will not exceed 1000 elements. - Ensure that the solution handles recursive references and nested object relationships efficiently. - Properly handle exceptions related to file operations and unpickling errors.","solution":"import pickle class Person: def __init__(self, name: str, age: int): self.name = name self.age = age self.friends = [] # List[Person] def add_friend(self, friend): self.friends.append(friend) def __getstate__(self): return (self.name, self.age, self.friends) def __setstate__(self, state): self.name, self.age, self.friends = state def serialize_people(people_list, filename: str): Serialize the list of Person objects to a file with open(filename, \'wb\') as file: pickle.dump(people_list, file) def deserialize_people(filename: str) -> list: Deserialize the list of Person objects from a file with open(filename, \'rb\') as file: return pickle.load(file)"},{"question":"**Question: Advanced Module Import Management using Python C API** You are given a task to implement a Python C extension that provides a function to dynamically import, reload, and list modules. Your extension should provide the following functionalities: 1. **Dynamic Import**: A function `dynamic_import` that takes a module name as a string and imports the module using the C API function `PyImport_ImportModule`. 2. **Reload Module**: A function `reload_module` that takes a module name and reloads the module using `PyImport_ReloadModule`. 3. **List Modules**: A function `list_modules` that lists all currently loaded modules using `PyImport_GetModuleDict`. # Specifications: 1. **Function 1: dynamic_import** - **Input**: `const char *module_name` - **Output**: A new reference to the imported module object or `NULL` with an exception set on failure. 2. **Function 2: reload_module** - **Input**: `const char *module_name` - **Output**: A new reference to the reloaded module object or `NULL` with an exception set on failure. 3. **Function 3: list_modules** - **Input**: None - **Output**: A list of strings representing the names of all currently loaded modules. # Constraints: - Handle exceptions appropriately. Return `NULL` with an exception set if any error occurs during the import, reload, or listing process. - Implement memory management correctly; ensure no memory leaks occur. # Performance Requirements: - Performance is not the primary concern, but ensure the functions are efficient in managing modules and their references. # Example Usage: ```python # Assuming the extension is named `mod_manager` import mod_manager # Dynamic import of a module mod = mod_manager.dynamic_import(\\"math\\") print(mod) # Reload a module mod_reloaded = mod_manager.reload_module(\\"math\\") print(mod_reloaded) # List all loaded modules loaded_modules = mod_manager.list_modules() print(loaded_modules) ``` # Implementation Hints: - Use `PyImport_ImportModule` to implement the `dynamic_import` function. - Use `PyImport_ReloadModule` to implement the `reload_module` function. - Use `PyImport_GetModuleDict` and iterate over the dictionary keys to implement the `list_modules` function. **Note**: All implementations should conform to the Python C API standards and handle reference counting meticulously to avoid memory issues.","solution":"from ctypes import pythonapi, py_object, c_char_p def dynamic_import(module_name): Imports a module dynamically using PyImport_ImportModule. :param module_name: Name of the module to import :return: Imported module object or raises an ImportError on failure pythonapi.PyImport_ImportModule.restype = py_object pythonapi.PyImport_ImportModule.argtypes = [c_char_p] module = pythonapi.PyImport_ImportModule(module_name.encode(\'utf-8\')) if module is None: raise ImportError(f\\"Module {module_name} could not be imported\\") return module def reload_module(module_name): Reloads a module using PyImport_ReloadModule. :param module_name: Name of the module to reload :return: Reloaded module object or raises an ImportError on failure pythonapi.PyImport_ImportModule.restype = py_object pythonapi.PyImport_ImportModule.argtypes = [c_char_p] pythonapi.PyImport_ReloadModule.restype = py_object pythonapi.PyImport_ReloadModule.argtypes = [py_object] module = pythonapi.PyImport_ImportModule(module_name.encode(\'utf-8\')) if module is None: raise ImportError(f\\"Module {module_name} could not be imported for reloading\\") reloaded_module = pythonapi.PyImport_ReloadModule(module) if reloaded_module is None: raise ImportError(f\\"Module {module_name} could not be reloaded\\") return reloaded_module def list_modules(): Lists all currently loaded modules using PyImport_GetModuleDict. :return: List of strings representing the names of all currently loaded modules pythonapi.PyImport_GetModuleDict.restype = py_object pythonapi.PyImport_GetModuleDict.argtypes = [] module_dict = pythonapi.PyImport_GetModuleDict() return list(module_dict.keys())"},{"question":"# Descriptor-based Attribute Management **Objective**: Implement and demonstrate the capabilities of descriptors in managing instance attributes with validation. **Problem Statement**: You are tasked with creating a class that utilizes descriptors to manage instance attributes in a controlled manner. Specifically, you need to implement a descriptor that validates the attributes of a person object, ensuring that a set of business rules are enforced before setting the value. # Requirements: 1. **Descriptor Class `ValidatedAttribute`:** - This descriptor should validate the attribute values based on provided validation rules. - Provide a general `validate` method within `ValidatedAttribute` that will be overridden in subclasses to implement specific validation logic. 2. **Subclasses of `ValidatedAttribute`:** - `NonEmptyString`: Ensures the attribute is a non-empty string. - `PositiveInteger`: Ensures the attribute is a positive integer. - `CustomPredicate`: Allows for user-defined validation logic through a callable predicate. 3. **Person Class Implementation**: - Utilize the created descriptors for the following attributes: - `name`: Must be a non-empty string. - `age`: Must be a positive integer. - `id_number`: Must satisfy a user-defined predicate (e.g., must be alphanumeric). 4. **Testing**: - Create instances of the `Person` class with various valid and invalid data to ensure your validation rules are correctly enforced. # Function Implementations: 1. **`ValidatedAttribute` implementation**: ```python class ValidatedAttribute: def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) def validate(self, value): pass # to be overridden in subclasses ``` 2. **Subclasses of `ValidatedAttribute`**: - Implement the `NonEmptyString` subclass. - Implement the `PositiveInteger` subclass. - Implement the `CustomPredicate` subclass. 3. **Person Class**: ```python class Person: name = NonEmptyString() age = PositiveInteger() id_number = CustomPredicate(str.isalnum) def __init__(self, name, age, id_number): self.name = name self.age = age self.id_number = id_number ``` Create the subclasses (`NonEmptyString`, `PositiveInteger`, `CustomPredicate`) as part of the assignment and test the `Person` class to ensure that the integration and validation of attributes work as specified. # Constraints: - Validation logic for each attribute must strictly adhere to the requirements specified in the descriptor subclasses. - If a validation fails, a `ValueError` should be raised. # Example Usage: ```python try: p = Person(\\"John\\", 30, \\"A123\\") print(f\\"Name: {p.name}, Age: {p.age}, ID: {p.id_number}\\") except ValueError as e: print(e) try: p_invalid_name = Person(\\"\\", 30, \\"A123\\") # Should raise ValueError except ValueError as e: print(f\\"Validation failed: {e}\\") try: p_invalid_age = Person(\\"John\\", -5, \\"A123\\") # Should raise ValueError except ValueError as e: print(f\\"Validation failed: {e}\\") try: p_invalid_id = Person(\\"John\\", 30, \\"#\\") # Should raise ValueError except ValueError as e: print(f\\"Validation failed: {e}\\") ``` **Deliverables**: - Implement the `ValidatedAttribute` class with its subclasses (`NonEmptyString`, `PositiveInteger`, `CustomPredicate`). - Implement the `Person` class utilizing these descriptors. - Demonstrate the functionality with appropriate tests.","solution":"# Descriptors for validation class ValidatedAttribute: def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) def validate(self, value): pass # To be overridden in subclasses class NonEmptyString(ValidatedAttribute): def validate(self, value): if not isinstance(value, str) or not value: raise ValueError(\\"Must be a non-empty string\\") class PositiveInteger(ValidatedAttribute): def validate(self, value): if not isinstance(value, int) or value <= 0: raise ValueError(\\"Must be a positive integer\\") class CustomPredicate(ValidatedAttribute): def __init__(self, predicate): self.predicate = predicate def validate(self, value): if not self.predicate(value): raise ValueError(\\"Value does not satisfy the predicate\\") # Person class using the descriptors class Person: name = NonEmptyString() age = PositiveInteger() id_number = CustomPredicate(str.isalnum) def __init__(self, name, age, id_number): self.name = name self.age = age self.id_number = id_number"},{"question":"# Advanced Residual Plot Analysis with Seaborn Objective Evaluate students\' understanding of `seaborn` package\'s residual plot functionalities, including dataset manipulation, creating and interpreting residual plots, and utilizing various optional parameters to enhance these visualizations. Problem Statement You are given a dataset containing information about car attributes from the `mpg` dataset, provided by `seaborn`. Your task is to analyze the relationship between car attributes using residual plots and interpret the model fit. Requirements - **Load the `mpg` dataset** using `seaborn`. - **Create multiple residual plots** focusing on different pairs of attributes from the dataset. - **Utilize optional parameters** (`order`, `lowess`, `line_kws`) as specified below. - **Print each plot\'s description** in text format explaining what it reveals about the model fit. Specifications 1. **Load `mpg` dataset**: ```python import seaborn as sns mpg = sns.load_dataset(\\"mpg\\") ``` 2. **Create and display the following residual plots**: 1. A basic residual plot with `weight` as x-axis and `displacement` as y-axis. 2. A residual plot with `horsepower` as x-axis and `mpg` as y-axis. 3. A residual plot with `horsepower` as x-axis and `mpg` as y-axis, adjusting for a quadratic relationship (i.e., `order=2`). 4. A residual plot with `horsepower` as x-axis and `mpg` as y-axis, including a LOWESS curve with red color and increased line width. Constraints - Use `sns.set_theme()` to apply the default seaborn theme before generating plots. - Each plot must use `sns.residplot`. - The LOWESS curve should be visually distinct (use color and line width options). Expected Output - Display all the specified residual plots. - For each plot, print a description describing: - The nature of the relationship between the two variables. - Any patterns or anomalies observed that might suggest a poor fit of the linear model. - The effect of using higher-order trends and LOWESS on the model fit stability. Example Here is an example layout for the output: ``` 1. Since the residuals show a random pattern around the horizontal 0-line, the linear model seems appropriate for weight vs. displacement. [Display Plot 1] 2. The residuals for horsepower and mpg show a clear pattern, indicating potential violations of linearity assumptions. [Display Plot 2] 3. Adjusting for a higher-order polynomial relationship stabilizes the residuals for horsepower and mpg, reducing the observed pattern. [Display Plot 3] 4. Including a LOWESS curve highlights the trend and provides a better evaluation of the relationship between horsepower and mpg. [Display Plot 4] ``` You are required to write the code for generating these plots and descriptions in a Python script or Jupyter notebook.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the mpg dataset mpg = sns.load_dataset(\\"mpg\\") # Set the default seaborn theme sns.set_theme() # Create Plot 1: Basic residual plot with weight as x-axis and displacement as y-axis plt.figure(figsize=(10, 6)) sns.residplot(x=\'weight\', y=\'displacement\', data=mpg) plt.title(\'Residual plot of weight vs displacement\') plt.xlabel(\'Weight\') plt.ylabel(\'Residuals of Displacement\') plt.show() print(\\"1. Since the residuals show a random pattern around the horizontal 0-line, the linear model seems appropriate for weight vs. displacement.\\") # Create Plot 2: Residual plot with horsepower as x-axis and mpg as y-axis plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg) plt.title(\'Residual plot of horsepower vs mpg\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals of MPG\') plt.show() print(\\"2. The residuals for horsepower and mpg show a clear pattern, indicating potential violations of linearity assumptions.\\") # Create Plot 3: Residual plot with horsepower as x-axis and mpg as y-axis, adjusting for a quadratic relationship (order=2) plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, order=2) plt.title(\'Residual plot of horsepower vs mpg with quadratic fit\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals of MPG\') plt.show() print(\\"3. Adjusting for a higher-order polynomial relationship stabilizes the residuals for horsepower and mpg, reducing the observed pattern.\\") # Create Plot 4: Residual plot with horsepower as x-axis and mpg as y-axis, including a LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, lowess=True, line_kws={\'color\': \'red\', \'lw\': 2}) plt.title(\'Residual plot of horsepower vs mpg with LOWESS curve\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals of MPG\') plt.show() print(\\"4. Including a LOWESS curve highlights the trend and provides a better evaluation of the relationship between horsepower and mpg.\\")"},{"question":"# SGD Classifier Implementation and Evaluation Problem Statement You are required to implement a Stochastic Gradient Descent (SGD) classifier using `scikit-learn` and evaluate its performance on a given dataset. Your task is to write a function that fits an `SGDClassifier` to the training data, tests it on the test data, and returns the accuracy score of the classifier. Requirements 1. **Function Name**: `train_and_evaluate_sgd_classifier` 2. **Inputs**: - `X_train` (numpy array or pandas DataFrame): Training feature set with shape (n_samples, n_features). - `y_train` (numpy array or pandas Series): Training labels with shape (n_samples,). - `X_test` (numpy array or pandas DataFrame): Testing feature set with shape (n_samples, n_features). - `y_test` (numpy array or pandas Series): Testing labels with shape (n_samples,). - `loss` (str): The loss function to be used by the `SGDClassifier`. It can be one of the following: `\'hinge\'`, `\'log_loss\'`, `\'modified_huber\'`. - `penalty` (str): The penalty (regularization term) to be used by the `SGDClassifier`. It can be one of the following: `\'l2\'`, `\'l1\'`, `\'elasticnet\'`. - `max_iter` (int): The maximum number of iterations for the classifier. Default is 1000. - `tol` (float): The stopping criterion for the classifier. Default is 1e-3. 3. **Output**: - `accuracy` (float): The accuracy score of the classifier on the test set. Constraints - Ensure that the inputs are properly scaled using `StandardScaler` before fitting the model. - Use `shuffle=True` to shuffle the training data after each iteration. - Handle potential exceptions that may occur during the fitting process. Example Usage ```python import numpy as np from sklearn.datasets import load_iris data = load_iris() X = data.data y = data.target # Splitting data into training and testing sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train and evaluate SGDClassifier accuracy = train_and_evaluate_sgd_classifier(X_train, y_train, X_test, y_test, loss=\'log_loss\', penalty=\'l2\', max_iter=1000, tol=1e-3) print(\\"Accuracy:\\", accuracy) ``` Important Notes - Use `StandardScaler` from `sklearn.preprocessing` to standardize the datasets. - Use `SGDClassifier` from `sklearn.linear_model`. - Use appropriate documentation and structuring for your function. **Function Signature**: ```python def train_and_evaluate_sgd_classifier(X_train, y_train, X_test, y_test, loss, penalty, max_iter=1000, tol=1e-3) -> float: pass ```","solution":"from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def train_and_evaluate_sgd_classifier(X_train, y_train, X_test, y_test, loss, penalty, max_iter=1000, tol=1e-3): Trains an SGDClassifier on the provided training data and evaluates it on the test data. Parameters: - X_train (numpy array or pandas DataFrame): Training feature set. - y_train (numpy array or pandas Series): Training labels. - X_test (numpy array or pandas DataFrame): Test feature set. - y_test (numpy array or pandas Series): Test labels. - loss (str): The loss function for the SGDClassifier. - penalty (str): The penalty (regularization term) for the SGDClassifier. - max_iter (int, optional): Maximum number of iterations (default is 1000). - tol (float, optional): The stopping criterion (default is 1e-3). Returns: - float: The accuracy score of the classifier on the test set. # Standardize the datasets scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize the SGDClassifier classifier = SGDClassifier(loss=loss, penalty=penalty, max_iter=max_iter, tol=tol, shuffle=True, random_state=42) try: # Fit the classifier on the training data classifier.fit(X_train_scaled, y_train) except Exception as e: raise ValueError(f\\"An error occurred while training the classifier: {e}\\") # Predict on the test data y_pred = classifier.predict(X_test_scaled) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Exception Handling and Custom Exception Creation Objective: To demonstrate your understanding of Python\'s exception handling, custom exception creation, and exception chaining. Instructions: You are required to implement a function `process_data(data)` which processes a list of numerical values and performs the following tasks: 1. **Input Validation:** The function should check if the input `data` is a list. If not, raise a custom exception `InvalidDataTypeError` with a message \\"Input data must be a list.\\" 2. **Element Validation:** The function should ensure all elements in the `data` list are either integers or floats. If any element is not, raise a custom exception `InvalidElementTypeError` with a message \\"All elements must be integers or floats.\\" 3. **Computation:** The function should compute the sum of all elements in the list. If the sum exceeds 1000, raise an `OverflowError` with a message \\"Sum exceeds maximum limit of 1000.\\" 4. **Exception Handling:** - If an `OverflowError` is raised, catch it and raise a new custom exception `ProcessingError` with the message \\"Error in processing data\\" and chain the original `OverflowError` to it. - If any other exceptions occur, re-raise them without modification. 5. **Output:** If all validations pass and no exception is raised, the function should return the sum of the elements. Custom Exceptions: Define the following custom exceptions: - `InvalidDataTypeError(Exception)` - `InvalidElementTypeError(Exception)` - `ProcessingError(Exception)` Function Signature: ```python def process_data(data): # Your implementation here class InvalidDataTypeError(Exception): pass class InvalidElementTypeError(Exception): pass class ProcessingError(Exception): pass ``` Constraints: - Do not use any external libraries. - Ensure the function only processes lists with numerical values. Example: ```python try: result = process_data([10, 20, 30]) print(result) # Output: 60 except Exception as e: print(f\\"Exception: {e}\\") try: result = process_data(\\"Not a list\\") except InvalidDataTypeError as e: print(f\\"Exception: {e}\\") # Output: Exception: Input data must be a list try: result = process_data([10, \\"20\\", 30]) except InvalidElementTypeError as e: print(f\\"Exception: {e}\\") # Output: Exception: All elements must be integers or floats try: result = process_data([500, 600]) except ProcessingError as e: print(f\\"Exception: {e}\\") # Output: Exception: Error in processing data ``` Note: Your code will be evaluated based on the correctness of exception handling, custom exception definitions, and proper chaining of exceptions.","solution":"def process_data(data): # Input Validation if not isinstance(data, list): raise InvalidDataTypeError(\\"Input data must be a list.\\") # Element Validation if not all(isinstance(element, (int, float)) for element in data): raise InvalidElementTypeError(\\"All elements must be integers or floats.\\") try: total = sum(data) if total > 1000: raise OverflowError(\\"Sum exceeds maximum limit of 1000.\\") return total except OverflowError as oe: raise ProcessingError(\\"Error in processing data\\") from oe class InvalidDataTypeError(Exception): pass class InvalidElementTypeError(Exception): pass class ProcessingError(Exception): pass"},{"question":"# Question Create a Python function `calculate_time_interval(start_time, end_time)` that performs the following tasks: 1. Accepts two strings, `start_time` and `end_time`, representing times in UTC in the format `\\"%Y-%m-%d %H:%M:%S\\"` (e.g., `\\"2023-10-05 14:30:00\\"`). 2. Converts these strings to `struct_time` objects in UTC. 3. Computes the difference in seconds between `end_time` and `start_time`. 4. Converts this time difference into a human-readable format in the form of \\"D days, H hours, M minutes, S seconds\\". 5. If `end_time` is earlier than `start_time`, raise a `ValueError` with the message \\"end_time must be later than start_time\\". Function Signature ```python def calculate_time_interval(start_time: str, end_time: str) -> str: pass ``` Example ```python result = calculate_time_interval(\\"2023-10-05 14:30:00\\", \\"2023-10-07 15:45:30\\") print(result) # Output: \\"2 days, 1 hours, 15 minutes, 30 seconds\\" result = calculate_time_interval(\\"2023-10-05 14:30:00\\", \\"2023-10-05 13:30:00\\") # Raises ValueError: end_time must be later than start_time ``` Constraints * The input strings will always be in the correct format. * You may not use any third-party libraries for this task. Notes * Consider leap years and differences in months\' lengths when calculating the interval. * Be aware of potential floating-point precision issues when dealing with time differences.","solution":"import time def calculate_time_interval(start_time: str, end_time: str) -> str: Calculate the time interval between two UTC times and return it as a human-readable string. # Convert string times to struct_time start_struct = time.strptime(start_time, \\"%Y-%m-%d %H:%M:%S\\") end_struct = time.strptime(end_time, \\"%Y-%m-%d %H:%M:%S\\") # Convert struct_time to seconds since epoch start_seconds = time.mktime(start_struct) end_seconds = time.mktime(end_struct) # Check if end_time is earlier than start_time if end_seconds < start_seconds: raise ValueError(\\"end_time must be later than start_time\\") # Calculate the difference in seconds time_difference = end_seconds - start_seconds # Calculate days, hours, minutes, and seconds days = time_difference // 86400 hours = (time_difference % 86400) // 3600 minutes = (time_difference % 3600) // 60 seconds = time_difference % 60 return f\\"{int(days)} days, {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\\""},{"question":"# Custom Estimator Development Your task is to create a custom scikit-learn compatible `MeanImputer` which is a transformer that replaces missing values in a dataset with the mean value of the respective feature. Your implementation must adhere to the scikit-learn API conventions. Instructions: 1. **Estimator Requirements**: - Implement the `fit` and `transform` methods. - Ensure to use `validate_data` to check the input data. - Ensure all estimated attributes have trailing underscores (e.g., `mean_`). 2. **Parameters**: - The `MeanImputer` should take no initialization parameters. 3. **fit Method**: - Calculate the mean of each feature in the dataset. - Store these means in an attribute called `mean_`. 4. **transform Method**: - Use the means calculated during fitting to replace any `NaN` values in the dataset. - Return the transformed dataset. 5. **Constraints**: - You must utilize numpy for array manipulations. - Ensure the transformer works with both numpy arrays and pandas DataFrame inputs. Expected Class Definition: ```python import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import validate_data, check_is_fitted class MeanImputer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): X = validate_data(self, X, accept_sparse=False, reset=True) self.mean_ = np.nanmean(X, axis=0) return self def transform(self, X, y=None): check_is_fitted(self, \'mean_\') X = validate_data(self, X, accept_sparse=False, reset=False) X_copy = X.copy() for i in range(X.shape[1]): X_copy[np.isnan(X_copy[:, i]), i] = self.mean_[i] return X_copy ``` Example Usage: ```python import numpy as np from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline # Sample data with missing values X = np.array([[1, 2], [3, np.nan], [7, 6], [np.nan, 8]]) # Expected means during fit: [3.6667, 5.3333] # Expected output after transform: [[1, 2], [3, 5.3333], [7, 6], [3.6667, 8]] # Create pipeline and process data pipeline = Pipeline(steps=[(\'imputer\', MeanImputer())]) X_transformed = pipeline.fit_transform(X) print(\\"Transformed Data:\\") print(X_transformed) ``` Your implementation must pass the example usage test, replacing all NaN values with the calculated feature means after fitting.","solution":"import numpy as np import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted class MeanImputer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): X = check_array(X, force_all_finite=False) self.mean_ = np.nanmean(X, axis=0) return self def transform(self, X, y=None): check_is_fitted(self, \'mean_\') X = check_array(X, force_all_finite=False) X_copy = X.copy() for i in range(X.shape[1]): X_copy[np.isnan(X_copy[:, i]), i] = self.mean_[i] return X_copy"},{"question":"# **Coding Assessment Question** **Objective:** Implement a function that creates, manipulates, and validates bytes objects using Python\'s byte-oriented operations. **Problem Statement:** You are tasked with implementing a function `process_bytes_data(data: str, length: int, format_string: str, *args) -> bytes` with the following functionality: 1. Accept a string `data`, an integer `length`, a format string `format_string`, and additional arguments `args`. 2. Create a new bytes object from `data` with the exact specified `length`. 3. Verify that the object created is indeed a bytes object (not a subtype). 4. Format the bytes object using the provided `format_string` and `args`. 5. Concatenate the formatted bytes object to the original bytes object. 6. Return the new bytes object after the concatenation. **Input:** - `data` (str): A string to be converted into bytes. - `length` (int): Length of the bytes object to be created from `data`. - `format_string` (str): A format string for formatting additional data into bytes. - `args`: A variable number of arguments to be formatted according to `format_string`. **Output:** - A bytes object resulting from the creation, verification, formatting, and concatenation steps described above. **Constraints:** - `data` must be non-null. - `length` can be any non-negative integer. - `format_string` must conform to the defined set of format characters allowed for `PyBytes_FromFormat`. **Performance Requirements:** - Ensure that operations involving creation and resizing are handled efficiently. - Minimize memory overhead by managing references properly. **Example:** ```python def process_bytes_data(data: str, length: int, format_string: str, *args) -> bytes: # Implementation here # Example Usage: result = process_bytes_data(\\"Hello, World!\\", 5, \\"%d-%s\\", 123, \\"abc\\") print(result) # Expected output should be b\'Hello123-abc\' ``` **Notes:** - Use the defined set of functions and macros for handling bytes objects as detailed in the provided documentation. - Ensure proper error handling for edge cases like invalid format strings or arguments that don’t match the format specifiers.","solution":"def process_bytes_data(data: str, length: int, format_string: str, *args) -> bytes: Processes bytes data by creating a bytes object, verifying its type, formatting it, and then concatenating it. Parameters: data (str): The input string to be converted into bytes. length (int): The length of the resulting bytes object created from `data`. format_string (str): The format string for formatting additional data into bytes. args: Additional arguments to be formatted according to `format_string`. Returns: bytes: The resulting bytes object after concatenation. if length > len(data): raise ValueError(\\"Specified length is greater than the length of the input data string\\") # Create a bytes object with the specified length bytes_obj = bytes(data[:length], \'utf-8\') # Verify that the created object is a bytes object if not isinstance(bytes_obj, bytes): raise TypeError(\\"The object created is not a bytes object\\") # Format the additional arguments using the format string formatted_bytes = format_string % args # Ensure the formatted result is converted to bytes formatted_bytes = bytes(formatted_bytes, \'utf-8\') # Concatenate the original bytes object with the formatted bytes result_bytes = bytes_obj + formatted_bytes return result_bytes"},{"question":"# String and Number Conversion and Formatting **Question:** You are required to implement a utility class in Python that provides various string and number conversion and formatting functionalities. Your task is to implement the following static methods in the `ConversionUtils` class: 1. **to_snake_case**: Converts a given camelCase or PascalCase string to snake_case. **Input**: - A string `s` in camelCase or PascalCase. **Output**: - A string in snake_case. **Constraints**: - The input string `s` will only contain alphabetic characters. ```python def to_snake_case(s: str) -> str: pass ``` 2. **string_to_double**: Converts a string to a double, handling errors gracefully. **Input**: - A string `s` representing a floating-point number. **Output**: - A `float` value if the conversion is successful, or `None` if the string cannot be converted. **Constraints**: - The string `s` must not have leading or trailing whitespace. ```python def string_to_double(s: str) -> float: pass ``` 3. **double_to_string**: Converts a double to a string with a specified precision and format. **Input**: - A floating-point number `val`. - A character `format_code` which should be one of \'e\', \'E\', \'f\', \'F\', \'g\', or \'G\'. - An integer `precision` representing the number of digits after the decimal point. **Output**: - A string representing the formatted number. **Constraints**: - The `format_code` and `precision` must be valid according to Python\'s format specification. ```python def double_to_string(val: float, format_code: str, precision: int) -> str: pass ``` 4. **case_insensitive_compare**: Performs a case-insensitive comparison between two strings. **Input**: - Two strings `s1` and `s2`. **Output**: - An integer: - Returns 0 if the strings are equal irrespective of case. - Returns a negative value if `s1` is less than `s2` (case insensitive). - Returns a positive value if `s1` is greater than `s2` (case insensitive). ```python def case_insensitive_compare(s1: str, s2: str) -> int: pass ``` # Examples ```python # Example for to_snake_case assert ConversionUtils.to_snake_case(\\"camelCase\\") == \\"camel_case\\" assert ConversionUtils.to_snake_case(\\"PascalCase\\") == \\"pascal_case\\" # Example for string_to_double assert ConversionUtils.string_to_double(\\"123.45\\") == 123.45 assert ConversionUtils.string_to_double(\\"invalid\\") == None # Example for double_to_string assert ConversionUtils.double_to_string(123.456, \'f\', 2) == \\"123.46\\" assert ConversionUtils.double_to_string(123.456, \'e\', 1) == \\"1.2e+02\\" # Example for case_insensitive_compare assert ConversionUtils.case_insensitive_compare(\\"hello\\", \\"HELLO\\") == 0 assert ConversionUtils.case_insensitive_compare(\\"apple\\", \\"Banana\\") < 0 assert ConversionUtils.case_insensitive_compare(\\"Banana\\", \\"apple\\") > 0 ``` Your task is to complete the implementation of these static methods in the provided `ConversionUtils` class template. ```python class ConversionUtils: @staticmethod def to_snake_case(s: str) -> str: # Implementation here pass @staticmethod def string_to_double(s: str) -> float: # Implementation here pass @staticmethod def double_to_string(val: float, format_code: str, precision: int) -> str: # Implementation here pass @staticmethod def case_insensitive_compare(s1: str, s2: str) -> int: # Implementation here pass ``` # Performance Requirements: - The methods should be efficient in terms of time complexity for their respective tasks. - Pay attention to edge cases such as empty strings and extremely large or small floating point numbers.","solution":"import re class ConversionUtils: @staticmethod def to_snake_case(s: str) -> str: Converts a given camelCase or PascalCase string to snake_case. return re.sub(r\'(?<!^)(?=[A-Z])\', \'_\', s).lower() @staticmethod def string_to_double(s: str) -> float: Converts a string to a double, handling errors gracefully. try: return float(s) except ValueError: return None @staticmethod def double_to_string(val: float, format_code: str, precision: int) -> str: Converts a double to a string with a specified precision and format. return f\\"{val:.{precision}{format_code}}\\" @staticmethod def case_insensitive_compare(s1: str, s2: str) -> int: Performs a case-insensitive comparison between two strings. s1_lower = s1.lower() s2_lower = s2.lower() if s1_lower == s2_lower: return 0 elif s1_lower < s2_lower: return -1 else: return 1"},{"question":"**Coding Assessment Question:** # Objective Create a visualization that combines multiple types of plots and transformations to provide a detailed view of the `tips` dataset using seaborn\'s `objects` module. # Problem Statement Using the `tips` dataset from seaborn, you need to create a composite plot that accomplishes the following: 1. Plots a scatterplot of `total_bill` vs `tip` with `sex` represented by different colors. 2. Adds an overlaid linear regression line with a 95% confidence interval. 3. Introduces bars to show the distribution of `total_bill` for each `day` of the week, separated by `time` (Lunch/Dinner) and aligned horizontally. 4. Ensures that layers are well-labeled and incorporated into the legend. # Requirements: 1. The x-axis should represent the `total_bill`. 2. The y-axis should represent the `tip`. 3. The scatterplot should use different colors to distinguish entries based on `sex` (Male/Female). 4. The linear regression line should appear together with the scatterplot, depicting the fit. 5. Include horizontally oriented bars showing the distribution of `total_bill` for each `day`, separated by `time` (Lunch/Dinner). 6. Labeled layers to make the plot interpretable using legends where applicable. # Input - None (the `tips` dataset is loaded within seaborn). # Output - The code should output the final composite plot containing all the specified layers and transformations. # Constraints 1. Use the seaborn `objects` module. 2. The plot must be self-explanatory through its labels and legend. 3. Make sure the plot is visually appealing and clearly distinguishes between different data groupings and plot types. # Example Below is a small snippet to get you started: ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\") # Initialize the plot with the scatterplot p = so.Plot(tips, \\"total_bill\\", \\"tip\\").add(so.Dot(), color=\\"sex\\") # Add Linear Regression p = p.add(so.Line(), so.PolyFit()) # Add polyfit for linear regression with CI # Add distribution bars for total_bill to show distribution for each day and time p = p.add(so.Bar(), so.Hist(), so.Dodge(), orient=\\"h\\", col=\\"day\\", color=\\"time\\") # Label layers accordingly p = p.label(x=\\"Total Bill\\", y=\\"Tip\\") p.show() ``` Extend this example to fulfill all specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_composite_plot(): tips = load_dataset(\\"tips\\") # Initialize the plot with the scatterplot p = so.Plot(tips, \\"total_bill\\", \\"tip\\") p = p.add(so.Dot(alpha=0.6), color=\\"sex\\") # Scatterplot with transparency for better clarity # Add Linear Regression p = p.add(so.Line(), so.PolyFit()) # Add polyfit for linear regression with CI # Add distribution bars for total_bill to show distribution for each day and time p = p.add(so.Bar(alpha=0.6), so.Hist(), so.Dodge(), orient=\\"h\\", col=\\"day\\", color=\\"time\\") # Label layers accordingly p = p.label(x=\\"Total Bill\\", y=\\"Tip\\") # Show the plot p.show() # Call the function to create the plot create_composite_plot()"},{"question":"You are given two DataFrames containing sales data for two different years from different branches of a multi-national company. Your task is to perform data analysis by merging these DataFrames in multiple ways and generating a comprehensive report on the results. You need to implement a function, `analyze_sales_data`, to perform the following steps: 1. **Concatenate DataFrames:** Combine the given DataFrames along the rows. 2. **Identify Merged Data Differences:** Compare year-wise sales data of branches and highlight differences. 3. **Merge with Optional Parameters:** 1. Perform an outer join on the \'Branch\' column to ensure all branches are accounted for. 2. Compute the intersection of the data using an inner join on the \'Branch\' column. 3. Use `pd.merge_asof` to merge the DataFrames along the \'Year\' column, assuming the years are sorted. 4. **Resulting Sales Report:** Generate a final summary DataFrame showing the total sales for each branch for the combined data. # Function Signature ```python def analyze_sales_data(df1: pd.DataFrame, df2: pd.DataFrame) -> dict: Args: df1, df2 (pd.DataFrame): Two DataFrames containing sales data with the following columns: - \'Branch\': Name of the branch - \'Year\': Year of data - \'Sales\': Sales amount for the year Returns: dict: A dictionary with the results of the various operations: { \'concat_result\': concatenated DataFrame, \'comparison_result\': DataFrame showing differences, \'outer_join_result\': DataFrame from outer join, \'inner_join_result\': DataFrame from inner join, \'merge_asof_result\': DataFrame from merge_asof, \'summary_result\': DataFrame summarizing total sales by branch } pass ``` # Example DataFrames ```python data1 = { \'Branch\': [\'A\', \'B\', \'C\'], \'Year\': [2020, 2020, 2020], \'Sales\': [1000, 1500, 2000] } data2 = { \'Branch\': [\'A\', \'B\', \'D\'], \'Year\': [2021, 2021, 2021], \'Sales\': [1100, 1600, 2100] } df1 = pd.DataFrame(data1) df2 = pd.DataFrame(data2) ``` # Expected Output Assuming the sample DataFrames `df1` and `df2`, the expected output would be a dictionary of DataFrames as specified in the function signature. Each DataFrame should reflect the result of respective operations applied: 1. **Concatenation Result:** Combined DataFrame along rows. 2. **Comparison Result:** DataFrame showing sales data differences. 3. **Outer Join Result:** DataFrame capturing all branches with sales data. 4. **Inner Join Result:** DataFrame capturing only common branches with sales data. 5. **Merge Asof Result:** DataFrame merging on the nearest year. 6. **Summary Result:** DataFrame showing total sales per branch. Note: Ensure you handle possible edge cases such as mismatched years, missing branches, or NaN values appropriately. Good luck!","solution":"import pandas as pd def analyze_sales_data(df1: pd.DataFrame, df2: pd.DataFrame) -> dict: # Step 1: Concatenate DataFrames concat_result = pd.concat([df1, df2], ignore_index=True) # Step 2: Identify Merged Data Differences comparison_result = df1.merge(df2, on=\'Branch\', how=\'outer\', suffixes=(\'_2020\', \'_2021\')) # Step 3: Merge with Optional Parameters # Outer join on \'Branch\' outer_join_result = pd.merge(df1, df2, on=\'Branch\', how=\'outer\', suffixes=(\'_2020\', \'_2021\')) # Inner join on \'Branch\' inner_join_result = pd.merge(df1, df2, on=\'Branch\', how=\'inner\', suffixes=(\'_2020\', \'_2021\')) # Merge asof on \'Year\', assumes both DataFrames are sorted by \'Year\' merge_asof_result = pd.merge_asof(df1.sort_values(\'Year\'), df2.sort_values(\'Year\'), on=\'Year\', by=\'Branch\', suffixes=(\'_2020\', \'_2021\')) # Step 4: Resulting Sales Report summary_result = concat_result.groupby(\'Branch\')[\'Sales\'].sum().reset_index() return { \'concat_result\': concat_result, \'comparison_result\': comparison_result, \'outer_join_result\': outer_join_result, \'inner_join_result\': inner_join_result, \'merge_asof_result\': merge_asof_result, \'summary_result\': summary_result }"},{"question":"Context In this assessment, you are required to utilize the `resource` module to monitor and control resource usage in Python programs. This involves setting and getting resource limits, and retrieving resource usage statistics. Task 1. **Function 1: `set_cpu_time_limit(cpu_time_limit)`** Implement a function `set_cpu_time_limit(cpu_time_limit)` that sets a CPU time limit for the current process. The function should: - Accept an argument `cpu_time_limit` which is the maximum CPU time in seconds that the process can use. - Use the `resource.setrlimit()` function to set the soft limit of `RLIMIT_CPU` to `cpu_time_limit`. - Handle and raise appropriate exceptions if the limit setting fails. 2. **Function 2: `get_resource_usage_info()`** Implement a function `get_resource_usage_info()` that retrieves and prints resource usage information for the current process. The function should: - Use the `resource.getrusage()` function with `resource.RUSAGE_SELF` to get the resource usage. - Extract the following fields from the result and print them in a readable format: - User time (`ru_utime`) - System time (`ru_stime`) - Maximum resident set size (`ru_maxrss`) - Voluntary context switches (`ru_nvcsw`) - Involuntary context switches (`ru_nivcsw`) Example ```python def set_cpu_time_limit(cpu_time_limit): pass def get_resource_usage_info(): pass # Usage set_cpu_time_limit(2) # Set a 2-second CPU time limit. # Perform some CPU-bound task for i in range(10**8): _ = 1 + 1 get_resource_usage_info() ``` Constraints - You may assume that your environment supports the `resource` module and related functions. - Handle typical exceptions (`ValueError`, `OSError`) properly. - For setting limits, ensure that the soft limit does not exceed the hard limit. Notes - Efficiency is less critical in this task; focus on correctness and proper use of the `resource` module functionalities. - Your solution should be well-documented to aid understanding. - Consider edge cases, such as invalid time limits and resource usage statistics retrieval errors.","solution":"import resource def set_cpu_time_limit(cpu_time_limit): Sets a CPU time limit for the current process. Args: cpu_time_limit (int): maximum CPU time in seconds that the process can use. Raises: ValueError: If the cpu_time_limit is invalid. OSError: If there\'s an error setting the limit. if not isinstance(cpu_time_limit, int) or cpu_time_limit <= 0: raise ValueError(\\"CPU time limit must be a positive integer\\") try: soft, hard = resource.getrlimit(resource.RLIMIT_CPU) # Ensure the new soft limit does not exceed the existing hard limit resource.setrlimit(resource.RLIMIT_CPU, (min(cpu_time_limit, hard), hard)) except ValueError as e: raise ValueError(f\\"Invalid CPU time limit: {e}\\") except OSError as e: raise OSError(f\\"Error setting CPU time limit: {e}\\") def get_resource_usage_info(): Retrieves and returns resource usage information for the current process. Returns: dict: A dictionary containing user time, system time, max RSS, voluntary context switches, and involuntary context switches. try: usage = resource.getrusage(resource.RUSAGE_SELF) usage_info = { \\"user_time\\": usage.ru_utime, \\"system_time\\": usage.ru_stime, \\"max_resident_set_size\\": usage.ru_maxrss, \\"voluntary_context_switches\\": usage.ru_nvcsw, \\"involuntary_context_switches\\": usage.ru_nivcsw } return usage_info except Exception as e: raise RuntimeError(f\\"Error retrieving resource usage information: {e}\\")"},{"question":"**Objective**: Demonstrate your understanding of seaborn\'s advanced plotting capabilities, specifically focusing on facet grids, custom order, and labeling. Question You are given a dataset containing information on cars from different manufacturers. Using seaborn\'s `objects` module, you are required to create a multi-faceted plot that visualizes the relationship between engine size and fuel efficiency, differentiated by the type of drive system and the number of cylinders the car has. Additionally, customize the facets with specific order and labels to provide a clear, meaningful representation of the data. **Dataset Description**: The dataset is named `cars` and contains the following columns: - `mpg`: Miles per gallon (fuel efficiency). - `cylinders`: Number of cylinders in the car’s engine. - `displacement`: Engine displacement in cubic inches. - `horsepower`: Engine horsepower. - `weight`: Weight of the car. - `acceleration`: Time to accelerate from 0 to 60 mph in seconds. - `model_year`: Year the car model was released. - `origin`: Country of origin (1: USA, 2: Europe, 3: Japan). - `car_name`: Name of the car. - `drive_system`: Type of drive system (e.g., `RWD`, `FWD`, `AWD`). **Requirements**: 1. **Load the dataset**. 2. **Create a multi-faceted plot**: - X-axis should represent `displacement`. - Y-axis should represent `mpg`. - Color by `cylinders`. - Facet the plot by `drive_system` and `model_year`. 3. **Customize the facets**: - Arrange the facets with rows representing `drive_system` (order: `RWD`, `FWD`, `AWD`). - Columns representing `model_year` sorted in ascending order. - Share y-axis across rows. - Add custom labels for titles, with `Drive System` and `Model Year: XX`. **Constraints**: - Assume the dataset has already been loaded into a pandas DataFrame named `cars`. **Performance**: - Efficiently manage the facet grid ensuring minimal performance overhead. **Function Signature**: ```python def create_faceted_plot(cars): pass ``` **Expected Output**: ```python import seaborn.objects as so def create_faceted_plot(cars): p = so.Plot(cars, x=\\"displacement\\", y=\\"mpg\\", color=\\"cylinders\\").add(so.Dots()) p.facet(row=\\"drive_system\\", col=\\"model_year\\", order={\\"row\\": [\\"RWD\\", \\"FWD\\", \\"AWD\\"], \\"col\\": sorted(cars[\\"model_year\\"].unique())}) p.share(y=True).label(title=\\"{row}, Model Year: {col}\\") return p # Usage: cars = load_dataset(\\"your_dataset\\") create_faceted_plot(cars) ``` *Note*: `load_dataset` function is just a placeholder. Replace it with the actual loading mechanism for the `cars` dataset.","solution":"def create_faceted_plot(cars): import seaborn.objects as so # Create the base plot with x=\\"displacement\\", y=\\"mpg\\", and color by \\"cylinders\\" plot = so.Plot(cars, x=\\"displacement\\", y=\\"mpg\\", color=\\"cylinders\\").add(so.Dots()) # Configure the facets with rows for \\"drive_system\\" and columns for \\"model_year\\" plot = plot.facet( row=\\"drive_system\\", col=\\"model_year\\", order={ \\"row\\": [\\"RWD\\", \\"FWD\\", \\"AWD\\"], \\"col\\": sorted(cars[\\"model_year\\"].unique()) } ) # Share y-axis across the rows and customize the labels for titles plot = plot.share(y=True).label(title=\\"{row}, Model Year: {col}\\") return plot"},{"question":"**Title: Using Sun\'s NIS Library for Centralized Lookup Services** Problem Statement You are tasked to write a Python script that interacts with the NIS system on a Unix platform. The script should perform the following operations: 1. **Retrieve and Print Default NIS Domain:** - Write a function `get_default_nis_domain()` that retrieves and returns the default NIS domain using the `nis.get_default_domain()` method. 2. **List All NIS Maps:** - Write a function `list_nis_maps(domain=None)` that lists all valid NIS maps. If the `domain` is provided, use that domain; otherwise, use the default domain. 3. **Find User Information in NIS Maps:** - Write a function `find_user_info(username, mapname, domain=None)` that retrieves user-specific information (value) from a specified NIS map based on the given username (key). If the `domain` is provided, use that domain; otherwise, use the default domain. - Handle the case where the key does not exist by raising and catching a `nis.error` exception, and return `None` in this case. 4. **Build a Complete NIS Map Dictionary:** - Write a function `build_nis_map_dict(mapname, domain=None)` that returns a dictionary of all key-value pairs in the specified NIS map. If the `domain` is provided, use that domain; otherwise, use the default domain. Input and Output Formats - Function: `get_default_nis_domain()` - **Input:** None - **Output:** A string representing the default NIS domain. - Function: `list_nis_maps(domain=None)` - **Input:** - `domain` (optional): A string representing the NIS domain. - **Output:** A list of strings where each string is a valid NIS map name. - Function: `find_user_info(username, mapname, domain=None)` - **Input:** - `username`: A string representing the user key. - `mapname`: A string representing the NIS map name. - `domain` (optional): A string representing the NIS domain. - **Output:** The value corresponding to the username key, or `None` if the key does not exist. - Function: `build_nis_map_dict(mapname, domain=None)` - **Input:** - `mapname`: A string representing the NIS map name. - `domain` (optional): A string representing the NIS domain. - **Output:** A dictionary mapping keys to values from the specified NIS map. Constraints and Considerations - The script must handle `nis.error` exceptions gracefully. - Assume all keys and values are byte arrays and handle them appropriately within the functions. - The solution must be efficient and should not perform redundant NIS queries. - Ensure that optional domain parameters default to the system\'s default NIS domain if not provided. Example Usage ```python # Retrieve default domain default_domain = get_default_nis_domain() print(default_domain) # List all valid NIS maps all_maps = list_nis_maps() print(all_maps) # Find information for a specific user in a map user_info = find_user_info(\'johndoe\', \'passwd.byname\') print(user_info) # Build dictionary of all key-value pairs in a map nis_map_dict = build_nis_map_dict(\'passwd.byname\') print(nis_map_dict) ```","solution":"import nis def get_default_nis_domain(): try: return nis.get_default_domain() except nis.error: return None def list_nis_maps(domain=None): try: if domain is None: domain = nis.get_default_domain() return nis.maps(domain) except nis.error: return [] def find_user_info(username, mapname, domain=None): try: if domain is None: domain = nis.get_default_domain() return nis.match(username, mapname, domain).decode(\'utf-8\') except nis.error: return None def build_nis_map_dict(mapname, domain=None): map_dict = {} try: if domain is None: domain = nis.get_default_domain() nis_map = nis.cat(mapname, domain) for key, value in nis_map.items(): map_dict[key.decode(\'utf-8\')] = value.decode(\'utf-8\') except nis.error: pass return map_dict"},{"question":"# Question You are given a dataset containing measurements of a certain metric collected in different groups. You need to visualize this data using seaborn. Specifically, you are required to create a plot that includes error bars to represent the spread and uncertainty of the data in each group. Implement a function `visualize_with_error_bars(data: pd.DataFrame, x: str, y: str, hue: str, errorbar_type: str, estimator: str=\'mean\', size: float=1.0) -> None` that: 1. Takes the following inputs: - `data`: A Pandas DataFrame containing the dataset. - `x`: The name of the column to be used on the x-axis. - `y`: The name of the column to be used on the y-axis. - `hue`: The name of the column to be used for color-coding different groups. - `errorbar_type`: The type of error bar to use (\'sd\', \'se\', \'pi\', \'ci\'). - `estimator`: The statistical function to estimate within each group (default is \'mean\'). - `size`: The size parameter for error bars, which scales the default error bars (default is 1.0). 2. Produces a plot using seaborn that includes: - Points for each data value. - Error bars according to the specified `errorbar_type`. - Different colors for each group indicated by `hue`. # Example Usage ```python import seaborn as sns import pandas as pd import numpy as np # Generating a sample dataset np.random.seed(42) data = pd.DataFrame({ \\"Group\\": np.repeat([\'Group A\', \'Group B\', \'Group C\'], 50), \\"Value\\": np.concatenate([ np.random.normal(loc=3, scale=1, size=50), np.random.normal(loc=5, scale=2, size=50), np.random.normal(loc=7, scale=3, size=50) ]) }) # Using the function to create the plot visualize_with_error_bars(data, x=\'Group\', y=\'Value\', hue=\'Group\', errorbar_type=\'ci\', estimator=\'median\', size=1.5) ``` # Constraints - You must not use any pre-calculated error bars; they should be calculated within the function using seaborn capabilities. - Use appropriate seaborn functions and parameters to achieve the desired plot. - The function should handle different types of `errorbar_type` correctly and apply the specified `size`. # Output The function should display a seaborn plot with appropriate error bars based on the given parameters.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_with_error_bars(data: pd.DataFrame, x: str, y: str, hue: str, errorbar_type: str, estimator: str=\'mean\', size: float=1.0) -> None: Visualizes the data with error bars using seaborn. Parameters: - data: pd.DataFrame, the dataset to visualize. - x: str, the column name for the x-axis. - y: str, the column name for the y-axis. - hue: str, the column name for color-coding different groups. - errorbar_type: str, the type of error bar to use (\'sd\', \'se\', \'pi\', \'ci\'). - estimator: str, the statistical function to estimate within each group (default is \'mean\'). - size: float, the size parameter for error bars which scales the default error bars (default is 1.0). errorbar_map = { \'ci\': \'ci\', \'sd\': \'sd\', \'se\': \'se\', \'pi\': \'pi\' } if errorbar_type not in errorbar_map: raise ValueError(f\\"Invalid errorbar_type. Choose from {list(errorbar_map.keys())}\\") sns.set(style=\\"whitegrid\\") plot = sns.pointplot( data=data, x=x, y=y, hue=hue, errorbar=errorbar_map[errorbar_type], estimator=estimator, capsize=size, markers=\'o\', dodge=True ) plt.show()"},{"question":"# Anomaly Detection with Scikit-Learn **Objective:** Implement and compare various anomaly detection methods provided by the scikit-learn package on a synthetic dataset. This will test your understanding of how to use `LocalOutlierFactor`, `IsolationForest`, `OneClassSVM`, and `EllipticEnvelope`. **Instructions:** 1. Generate a synthetic dataset with two features containing a mix of inliers and outliers. You can use the `make_blobs` function from `sklearn.datasets` to create clusters of inliers and then add some outliers manually. 2. Implement the following anomaly detection methods: - Local Outlier Factor (LOF) - Isolation Forest - One-Class SVM - Elliptic Envelope 3. For each method: - Train the model on the inlier data. - Use the trained model to predict the labels on the entire dataset (inliers + outliers). - Calculate the percentage of correctly identified outliers and the percentage of incorrectly identified inliers (false positives). 4. Compare the performance of these methods based on the calculated metrics. **Function Signature:** ```python import numpy as np from sklearn.datasets import make_blobs def generate_synthetic_data(n_samples=300, n_outliers=30, random_state=42): Generate a two-featured synthetic dataset with blobs for inliers and manually added outliers. Parameters: - n_samples (int): Total number of inlier samples - n_outliers (int): Number of outlier samples - random_state (int): Seed for random number generator Returns: - X (np.ndarray): Feature array of shape (n_samples + n_outliers, 2) - y (np.ndarray): Labels array of shape (n_samples + n_outliers,) with inliers labeled as 1 and outliers as -1 # Generate blobs for inliers X, _ = make_blobs(n_samples=n_samples, centers=1, n_features=2, random_state=random_state) # Add outliers np.random.seed(random_state) outliers = np.random.uniform(low=-10, high=10, size=(n_outliers, 2)) X = np.vstack([X, outliers]) # Labels: 1 for inliers, -1 for outliers y = np.ones(n_samples + n_outliers) y[-n_outliers:] = -1 return X, y def anomaly_detection_comparison(X, y): Implement and compare the performance of various anomaly detection methods from scikit-learn. Parameters: - X (np.ndarray): Feature array of shape (n_samples, n_features) - y (np.ndarray): Labels array of shape (n_samples,) Returns: - results (dict): Dictionary containing method-wise performance results from sklearn.neighbors import LocalOutlierFactor from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope results = {} # Local Outlier Factor lof = LocalOutlierFactor(novelty=True) lof.fit(X[y == 1]) y_pred_lof = lof.predict(X) results[\'LOF\'] = evaluate_performance(y, y_pred_lof) # Isolation Forest iso_forest = IsolationForest(random_state=42) iso_forest.fit(X[y == 1]) y_pred_if = iso_forest.predict(X) results[\'IsolationForest\'] = evaluate_performance(y, y_pred_if) # One-Class SVM oc_svm = OneClassSVM(gamma=\'scale\', nu=0.1) oc_svm.fit(X[y == 1]) y_pred_ocsvm = oc_svm.predict(X) results[\'OneClassSVM\'] = evaluate_performance(y, y_pred_ocsvm) # Elliptic Envelope ell_env = EllipticEnvelope() ell_env.fit(X[y == 1]) y_pred_ee = ell_env.predict(X) results[\'EllipticEnvelope\'] = evaluate_performance(y, y_pred_ee) return results def evaluate_performance(y_true, y_pred): Calculate the performance metrics for anomaly detection. Parameters: - y_true (np.ndarray): True labels - y_pred (np.ndarray): Predicted labels Returns: - metrics (dict): Dictionary containing \'outlier_recall\' and \'inlier_precision\' from sklearn.metrics import recall_score, precision_score outlier_recall = recall_score(y_true == -1, y_pred == -1) inlier_precision = precision_score(y_true == 1, y_pred == 1) return { \'outlier_recall\': outlier_recall, \'inlier_precision\': inlier_precision } # Example usage: X, y = generate_synthetic_data() performance_results = anomaly_detection_comparison(X, y) print(performance_results) ``` **Note:** - Use ROC curves or other appropriate metrics to visualize the performance differences if you want to add an advanced component to this task. - Ensure your code is well-commented to demonstrate your understanding and logic.","solution":"import numpy as np from sklearn.datasets import make_blobs def generate_synthetic_data(n_samples=300, n_outliers=30, random_state=42): Generate a two-featured synthetic dataset with blobs for inliers and manually added outliers. Parameters: - n_samples (int): Total number of inlier samples - n_outliers (int): Number of outlier samples - random_state (int): Seed for random number generator Returns: - X (np.ndarray): Feature array of shape (n_samples + n_outliers, 2) - y (np.ndarray): Labels array of shape (n_samples + n_outliers,) with inliers labeled as 1 and outliers as -1 # Generate blobs for inliers X, _ = make_blobs(n_samples=n_samples, centers=1, n_features=2, random_state=random_state) # Add outliers np.random.seed(random_state) outliers = np.random.uniform(low=-10, high=10, size=(n_outliers, 2)) X = np.vstack([X, outliers]) # Labels: 1 for inliers, -1 for outliers y = np.ones(n_samples + n_outliers) y[-n_outliers:] = -1 return X, y def anomaly_detection_comparison(X, y): Implement and compare the performance of various anomaly detection methods from scikit-learn. Parameters: - X (np.ndarray): Feature array of shape (n_samples, n_features) - y (np.ndarray): Labels array of shape (n_samples,) Returns: - results (dict): Dictionary containing method-wise performance results from sklearn.neighbors import LocalOutlierFactor from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope results = {} # Local Outlier Factor lof = LocalOutlierFactor(novelty=True) lof.fit(X[y == 1]) y_pred_lof = lof.predict(X) results[\'LOF\'] = evaluate_performance(y, y_pred_lof) # Isolation Forest iso_forest = IsolationForest(random_state=42) iso_forest.fit(X[y == 1]) y_pred_if = iso_forest.predict(X) results[\'IsolationForest\'] = evaluate_performance(y, y_pred_if) # One-Class SVM oc_svm = OneClassSVM(gamma=\'scale\', nu=0.1) oc_svm.fit(X[y == 1]) y_pred_ocsvm = oc_svm.predict(X) results[\'OneClassSVM\'] = evaluate_performance(y, y_pred_ocsvm) # Elliptic Envelope ell_env = EllipticEnvelope() ell_env.fit(X[y == 1]) y_pred_ee = ell_env.predict(X) results[\'EllipticEnvelope\'] = evaluate_performance(y, y_pred_ee) return results def evaluate_performance(y_true, y_pred): Calculate the performance metrics for anomaly detection. Parameters: - y_true (np.ndarray): True labels - y_pred (np.ndarray): Predicted labels Returns: - metrics (dict): Dictionary containing \'outlier_recall\' and \'inlier_precision\' from sklearn.metrics import recall_score, precision_score outlier_recall = recall_score(y_true == -1, y_pred == -1) inlier_precision = precision_score(y_true == 1, y_pred == 1) return { \'outlier_recall\': outlier_recall, \'inlier_precision\': inlier_precision } # Example usage: X, y = generate_synthetic_data() performance_results = anomaly_detection_comparison(X, y) print(performance_results)"},{"question":"**Objective**: To demonstrate mastery of fundamental and advanced pandas operations by performing a series of data manipulations on a given dataset. --- # Problem Statement You are given a DataFrame `sales_df` that records the sales figures of different products over various dates. Each row represents a sale, with the following columns: - **Date**: The date when the sale was made (string format, e.g., \\"2023-07-15\\"). - **Product**: The name of the product (string). - **Quantity**: The number of units sold (integer). - **Price**: The price per unit (float). Using pandas, perform the following operations: 1. **Convert Data Types**: Convert the \'Date\' column to datetime, \'Quantity\' to integer, and \'Price\' to float. 2. **Add New Columns**: - Create a new column **\'Total\'** representing the total sale amount for each row (`Quantity * Price`). - Create another column **\'Month\'** representing the month when the sale was made. 3. **Filtering and Aggregation**: - Filter out the sales records where the **\'Total\'** sale amount is less than 100. - For the resulting subset, calculate the total sales per product per month. 4. **Sorting and Ranking**: - Sort the DataFrame by **\'Date\'** and then by **\'Product\'**. - Rank the products within each month based on their total sales, with the highest sale as rank 1. Your final function should return the processed DataFrame. # Constraints - The input DataFrame `sales_df` can have up to 10,000 rows. - It is guaranteed that the \'Quantity\' and \'Price\' columns contain valid integer and float values respectively, but the \'Date\' column may have inconsistent formats. # Expected Input and Output Input - A pandas DataFrame `sales_df` with columns: \'Date\' (string), \'Product\' (string), \'Quantity\' (integer), \'Price\' (float). Output - A pandas DataFrame that contains: - Converted \'Date\' column to datetime format. - New columns \'Total\' and \'Month\'. - Filtered sales records with \'Total\' ≥ 100. - Aggregated total sales per product per month. - Sorted and ranked products within each month. # Function Signature ```python def process_sales_data(sales_df: pd.DataFrame) -> pd.DataFrame: pass ``` # Example Given the input DataFrame `sales_df` as follows: ```python import pandas as pd data = { \'Date\': [\'2023-07-15\', \'2023-07-15\', \'2023-07-16\'], \'Product\': [\'A\', \'B\', \'A\'], \'Quantity\': [10, 5, 2], \'Price\': [20.5, 30.0, 15.0] } sales_df = pd.DataFrame(data) ``` The expected output after processing should be: ``` Date Product Quantity Price Total Month Sales Rank 0 2023-07-15 A 10 20.5 205.0 2023-07-01 1 1 2023-07-15 B 5 30.0 150.0 2023-07-01 2 ``` # Requirements - Use vectorized operations wherever possible for performance. - Properly handle any potential issues with date parsing. ---","solution":"import pandas as pd def process_sales_data(sales_df: pd.DataFrame) -> pd.DataFrame: # Convert \'Date\' to datetime, \'Quantity\' to integer, and \'Price\' to float sales_df[\'Date\'] = pd.to_datetime(sales_df[\'Date\'], errors=\'coerce\') sales_df[\'Quantity\'] = sales_df[\'Quantity\'].astype(int) sales_df[\'Price\'] = sales_df[\'Price\'].astype(float) # Create new columns \'Total\' and \'Month\' sales_df[\'Total\'] = sales_df[\'Quantity\'] * sales_df[\'Price\'] sales_df[\'Month\'] = sales_df[\'Date\'].dt.to_period(\'M\').dt.to_timestamp() # Filter records where \'Total\' is less than 100 sales_df = sales_df[sales_df[\'Total\'] >= 100] # Group by \'Product\' and \'Month\' and calculate total sales per product per month sales_grouped = sales_df.groupby([\'Product\', \'Month\'])[\'Total\'].sum().reset_index() # Sort by \'Month\' and then \'Total\' sales_grouped = sales_grouped.sort_values(by=[\'Month\', \'Total\'], ascending=[True, False]) # Rank the products within each month sales_grouped[\'Sales Rank\'] = sales_grouped.groupby(\'Month\')[\'Total\'].rank(ascending=False, method=\'first\') # Merge the ranked data back with the original filtered data merged_df = pd.merge(sales_df, sales_grouped, on=[\'Product\', \'Month\', \'Total\'], how=\'inner\') # Sort DataFrame by \'Date\' and then by \'Product\' final_df = merged_df.sort_values(by=[\'Date\', \'Product\']).reset_index(drop=True) return final_df"},{"question":"# HTML Entity Conversion In this task, you will demonstrate your understanding of the `html` package by implementing two functions: `safe_html` which uses `html.escape` and `raw_html` which uses `html.unescape`. Function 1: `safe_html` This function should take a single string input `s` and return a string where: - The characters &, <, and > are converted to HTML-safe sequences. - Additionally, if the `quote` parameter is `True`, it should also convert \\" (double quotes) and \' (single quotes). ```python def safe_html(s: str, quote: bool = True) -> str: Takes a string s and converts the characters &, <, > to HTML-safe sequences. If quote is True, also converts \\" and \' to HTML-safe sequences. :param s: The input string to be converted. :param quote: Boolean indicating whether to also convert \\" and \'. :return: HTML-safe string. pass ``` Function 2: `raw_html` This function should take a single string input `s` and return a string where: - All named and numeric HTML character references are converted to their corresponding Unicode characters. ```python def raw_html(s: str) -> str: Takes a string s and converts all named and numeric HTML character references to their corresponding Unicode characters. :param s: The input string to be converted. :return: The unescaped string. pass ``` # Example ```python print(safe_html(\'5 > 3 & 2 < 4\')) # Output: \'5 &gt; 3 &amp; 2 &lt; 4\' print(safe_html(\'Single quote: \' and double quote: \\"\', quote=True)) # Output: \'Single quote: &#x27; and double quote: &quot;\' print(raw_html(\'5 &gt; 3 &amp; 2 &lt; 4\')) # Output: \'5 > 3 & 2 < 4\' print(raw_html(\'Single quote: &#x27; and double quote: &quot;\')) # Output: \'Single quote: \' and double quote: \\"\' ``` # Constraints - Do not use any external libraries other than what\'s provided in the python310 package. - Ensure the functions handle edge cases, such as empty strings and strings without any special characters. - Both functions should have an average time complexity of O(n), where n is the length of the input string.","solution":"import html def safe_html(s: str, quote: bool = True) -> str: Takes a string s and converts the characters &, <, > to HTML-safe sequences. If quote is True, also converts \\" and \' to HTML-safe sequences. :param s: The input string to be converted. :param quote: Boolean indicating whether to also convert \\" and \'. :return: HTML-safe string. return html.escape(s, quote=quote) def raw_html(s: str) -> str: Takes a string s and converts all named and numeric HTML character references to their corresponding Unicode characters. :param s: The input string to be converted. :return: The unescaped string. return html.unescape(s)"},{"question":"You have been given the task of writing unit tests for a data processing application. The application has a function `process_data`, which reads from a file, processes the data, and writes the results to another file. Your task is to use the `unittest.mock` module to create mock objects in order to unit test this function without actually performing any file I/O operations. Here are the signatures of functions and a brief description of what they do: ```python def read_input_file(file_path: str) -> str: Reads and returns content of a file. with open(file_path, \'r\') as file: return file.read() def write_output_file(file_path: str, content: str): Writes content to a file. with open(file_path, \'w\') as file: file.write(content) def process_data(input_file_path: str, output_file_path: str): Reads data from input file, processes it, and writes the result to output file. data = read_input_file(input_file_path) # Simulate some data processing processed_data = data.upper() write_output_file(output_file_path, processed_data) ``` # Task Implement a unit test for the `process_data` function using the `unittest.mock` module in Python. You should: 1. Mock the `open` function using `mock_open`. 2. Ensure that `read_input_file` reads data correctly. 3. Ensure that `write_output_file` writes processed data correctly. 4. Verify that the `process_data` function calls the `read_input_file` and `write_output_file` functions with the correct parameters. 5. Use assertions to validate that the functions are called the correct number of times and with the correct arguments. # Constraints - Avoid performing actual file I/O operations. - Use the `unittest.mock` module for mocking the file operations. - Test the function with sample data to verify your test logic. # Example Here\'s an example demonstrating how to implement the test using the `unittest.mock` module: ```python import unittest from unittest.mock import mock_open, patch class TestProcessData(unittest.TestCase): @patch(\'builtins.open\', new_callable=mock_open, read_data=\'input data\') def test_process_data(self, mock_open): input_file_path = \'input.txt\' output_file_path = \'output.txt\' # Call the function to test process_data(input_file_path, output_file_path) # Check if open was called correctly for reading mock_open.assert_any_call(input_file_path, \'r\') # Check if open was called correctly for writing mock_open.assert_any_call(output_file_path, \'w\') # Get the mock file handle for write calls handle = mock_open() handle.write.assert_called_once_with(\'INPUT DATA\') if __name__ == \'__main__\': unittest.main() ``` # Instructions: 1. Write a complete unit test for the `process_data` function following the example above. 2. Ensure your test can be executed directly and will output results showing whether the `process_data` function works as intended with mocked file operations.","solution":"from unittest.mock import mock_open, patch def read_input_file(file_path: str) -> str: Reads and returns content of a file. with open(file_path, \'r\') as file: return file.read() def write_output_file(file_path: str, content: str): Writes content to a file. with open(file_path, \'w\') as file: file.write(content) def process_data(input_file_path: str, output_file_path: str): Reads data from input file, processes it, and writes the result to output file. data = read_input_file(input_file_path) # Simulate some data processing processed_data = data.upper() write_output_file(output_file_path, processed_data)"},{"question":"# PyTorch MPS Environment Configuration Task You are working on optimizing a machine learning model that runs on Apple\'s Metal Performance Shaders (MPS) backend using PyTorch. During testing, you need to ensure your model runs efficiently on different devices and configurations. Specifically, you want to control memory allocation limits and enable performance-enhancing features. Your task is to write a Python script that: 1. Configures several MPS environment variables appropriately. 2. Creates a simple PyTorch model and performs a matrix multiplication operation to demonstrate the effects of the environment configuration. # Requirements 1. **Environment Configuration**: - Enable verbose logging for the MPS allocator. - Set the high watermark ratio to 1.5. - Set the low watermark ratio to 1.2 if the memory is unified, otherwise set it to 1.0. - Enable fast math for MPS metal kernels. - Prefer using metal kernels for matrix operations. 2. **Model Definition**: - Define a simple PyTorch model with one linear layer. - Use the model to perform a matrix multiplication operation on random tensors. 3. **Output**: - Print the result of the matrix multiplication. - Ensure that logging statements from the MPS allocator appear in the output, demonstrating the effect of the environment configuration. Function Signature Your script does not need to follow a specific function signature, but should perform the required tasks sequentially. # Constraints - You may assume that the script will be run in an appropriate environment where PyTorch with MPS support is installed. # Example ```python import os import torch import torch.nn as nn # Set environment variables os.environ[\\"PYTORCH_DEBUG_MPS_ALLOCATOR\\"] = \\"1\\" os.environ[\\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\\"] = \\"1.5\\" os.environ[\\"PYTORCH_MPS_LOW_WATERMARK_RATIO\\"] = \\"1.2\\" # Assuming unified memory for this example os.environ[\\"PYTORCH_MPS_FAST_MATH\\"] = \\"1\\" os.environ[\\"PYTORCH_MPS_PREFER_METAL\\"] = \\"1\\" # Define a simple model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 10) def forward(self, x): return self.linear(x) # Create model and sample data model = SimpleModel() input_tensor = torch.randn(10, 10, device=\'mps\') output_tensor = model(input_tensor) print(output_tensor) ``` **Note**: The above example is a guideline. Your script should be more comprehensive, handling both unified and discrete memory cases.","solution":"import os import torch import torch.nn as nn def configure_mps_environment(): Configures MPS environment variables for optimization and logging. os.environ[\\"PYTORCH_DEBUG_MPS_ALLOCATOR\\"] = \\"1\\" os.environ[\\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\\"] = \\"1.5\\" if torch.cuda.is_available(): os.environ[\\"PYTORCH_MPS_LOW_WATERMARK_RATIO\\"] = \\"1.0\\" else: os.environ[\\"PYTORCH_MPS_LOW_WATERMARK_RATIO\\"] = \\"1.2\\" os.environ[\\"PYTORCH_MPS_FAST_MATH\\"] = \\"1\\" os.environ[\\"PYTORCH_MPS_PREFER_METAL\\"] = \\"1\\" def create_and_run_model(): Creates a simple model to perform matrix multiplication and prints the output. class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 10) def forward(self, x): return self.linear(x) # Set up MPS environment configure_mps_environment() # Create model and sample data device = torch.device(\'mps\' if torch.backends.mps.is_available() else \'cpu\') model = SimpleModel().to(device) model.eval() with torch.no_grad(): input_tensor = torch.randn(10, 10, device=device) output_tensor = model(input_tensor) print(output_tensor) if __name__ == \\"__main__\\": create_and_run_model()"},{"question":"You are given the task of writing a Python script that makes use of the **nis** module to perform some administrative tasks on a Unix-based network. The script should consist of a single function named `generate_user_report` that produces a report of user details from the NIS maps. Function Signature ```python def generate_user_report(): pass ``` Requirements 1. The function should fetch the default NIS domain. 2. Get the list of all available maps in this domain. 3. For each map, fetch the contents using `nis.cat(mapname)`. 4. Filter the maps and only include those that have user-related information (assume maps with \'passwd\' in their name contain user information). 5. For each \'passwd\' related map, extract entries and compile a report. The report should be a list of dictionaries, where each dictionary has the following keys: - `\'username\'`: Extracted from the user-related map. - `\'uid\'`: Extracted from the user-related map. - `\'info\'`: Any additional info associated with the user. 6. Return the report as a list of dictionaries. If no maps exist or an error occurs, return an empty list. Example Output ```python [ {\'username\': \'johndoe\', \'uid\': \'1001\', \'info\': \'John Doe, User\'}, {\'username\': \'janedoe\', \'uid\': \'1002\', \'info\': \'Jane Doe, User\'}, ... ] ``` Constraints - The function should handle exceptions gracefully, specifically any `nis.error` exceptions. - The function will be tested on a Unix system that supports NIS. Performance Requirements - Given the nature of NIS and the likely size of user maps, optimize for readability but ensure the function can handle typical user map sizes of up to several thousand entries efficiently. Notes - This function does not take any input parameters. - You do not need access to an actual NIS server for writing the function. - You should mock the necessary functions if you intend to test the function locally. Write your function below: ```python import nis def generate_user_report(): report = [] try: default_domain = nis.get_default_domain() maps = nis.maps(domain=default_domain) for mapname in maps: if \'passwd\' in mapname: entries = nis.cat(mapname, domain=default_domain) for key, value in entries.items(): parts = value.decode().split(\':\') # Assuming a colon-separated Unix passwd format report.append({ \'username\': parts[0], \'uid\': parts[2], \'info\': \', \'.join(parts[4:]) }) except nis.error: return [] return report ``` This coding challenge assesses the student\'s ability to: - Use the NIS module effectively. - Handle exceptions and edge cases. - Work with dictionaries and list comprehensions. - Understand and manipulate string data.","solution":"import nis def generate_user_report(): Generates a report of user details from NIS maps. Returns: report (list): List of dictionaries, each representing a user\'s details. Returns an empty list if no user-related maps are found or upon error. report = [] try: default_domain = nis.get_default_domain() maps = nis.maps(domain=default_domain) for mapname in maps: if \'passwd\' in mapname: entries = nis.cat(mapname, domain=default_domain) for key, value in entries.items(): parts = value.decode().split(\':\') # Assuming a colon-separated Unix passwd format report.append({ \'username\': parts[0], \'uid\': parts[2], \'info\': parts[4] }) except nis.error: return [] return report"},{"question":"# Profiling and Analyzing Code Performance Objective You are required to write a Python script that profiles and analyzes the performance of a specific piece of code using the `cProfile` and `pstats` modules. The task will help you demonstrate your understanding of profiling, saving and loading profile data, as well as sorting and printing formatted profiling statistics. Task Details 1. **Function to Profile**: Write a function named `compute_statistics(data)` that takes a list of integers `data` as input, performs some computational tasks, and returns a dictionary containing the sum, mean, median, variance, and standard deviation of the input data. Use the built-in `statistics` module for computations. 2. **Profiling the Function**: Use the `cProfile` module to profile the `compute_statistics(data)` function. Save the profiling results to a file named `profile_stats.prof`. 3. **Analyzing the Profile**: Use the `pstats` module to load the profiling results from `profile_stats.prof`. Implement code to: - Sort the results by cumulative time. - Print the top 10 functions based on cumulative time. - Print a list of all functions called by `compute_statistics`. 4. **Code Implementation**: Implement all the required steps in a single script. Ensure to add comments for clarity. Expected Input and Output Formats - Input: A list of integers (e.g., `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`). - Output: - A dictionary containing the statistical computations. - Printed profiling statistics as described above. - Printed list of functions called by `compute_statistics`. Constraints - The input list can contain up to 1,000,000 integers. - You must use the `cProfile` module for profiling. - You must use the `pstats` module for analyzing the profile data. - Performance considerations should be kept in mind due to potentially large input sizes. Example Output ```python { \'sum\': 55, \'mean\': 5.5, \'median\': 5.5, \'variance\': 9.166666666666666, \'standard deviation\': 3.0276503540974917 } 100 function calls in 0.002 seconds Ordered by: cumulative time ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.002 0.002 <ipython-input-4-46774a9f>:(module>) 1 0.000 0.000 0.002 0.002 script.py:10(compute_statistics) 1 0.000 0.000 0.001 0.001 {built-in method statistics.mean} 1 0.000 0.000 0.001 0.001 {built-in method statistics.median} 1 0.000 0.000 0.000 0.000 {built-in method statistics.variance} 1 0.000 0.000 0.000 0.000 {built-in method statistics.stdev} ... callers: 1 0.000 0.000 0.002 0.002 script.py:10(compute_statistics) 1 0.000 0.000 0.001 0.001 {built-in method statistics.mean} ... ``` Steps to Follow 1. Implement the `compute_statistics` function. 2. Profile the function using `cProfile.run()` and save the results. 3. Load and analyze the profile data using `pstats.Stats`. 4. Ensure the script functions as specified above. Submission Submit your complete Python script that performs the above tasks. Include any necessary comments and ensure your code is clean and well-organized.","solution":"import cProfile import pstats import statistics def compute_statistics(data): Compute statistics for a given list of integers. Parameters: data (list of int): A list of integers. Returns: dict: A dictionary containing the sum, mean, median, variance, and standard deviation of the input data. stats = { \'sum\': sum(data), \'mean\': statistics.mean(data), \'median\': statistics.median(data), \'variance\': statistics.variance(data), \'standard deviation\': statistics.stdev(data) } return stats def profile_function(profile_filename, func, *args): Profile the given function with the provided arguments and save the profile data to a file. Parameters: profile_filename (str): The filename where profile data will be saved. func (callable): The function to be profiled. *args: Arguments to be passed to the function. profile = cProfile.Profile() profile.runcall(func, *args) profile.dump_stats(profile_filename) def analyze_profile(profile_filename): Analyze the profile data saved in the given file. Parameters: profile_filename (str): The filename where profile data is saved. p = pstats.Stats(profile_filename) p.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(10) print(\\"nFunctions called by compute_statistics:\\") p.print_callees(\\"compute_statistics\\") if __name__ == \\"__main__\\": # Example data data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # Profile the compute_statistics function profile_filename = \\"profile_stats.prof\\" profile_function(profile_filename, compute_statistics, data) # Analyze the profile data analyze_profile(profile_filename)"},{"question":"# Decision Tree Classifier Implementation and Analysis You are required to demonstrate your understanding of decision trees by implementing a decision tree classifier using scikit-learn. Your task is to perform the following: 1. **Load Dataset**: - Use the `load_iris` dataset from `sklearn.datasets`. 2. **Data Preprocessing**: - Introduce some missing values in the dataset. 3. **Model Training**: - Train a `DecisionTreeClassifier` on the dataset with missing values handled. 4. **Model Evaluation**: - Evaluate the model using appropriate metrics. - Output the confusion matrix. 5. **Model Pruning**: - Implement minimal cost-complexity pruning to avoid overfitting. - Plot the decision tree before and after pruning. 6. **Visualization**: - Visualize the decision tree using `plot_tree`. - **Bonus**: Export the tree in Graphviz format. 7. **Custom Analysis**: - Write a short analysis on how missing values are handled internally by the `DecisionTreeClassifier`. - Explain the effect of pruning on the model\'s performance. **Input**: None. Use the `load_iris` dataset provided by scikit-learn. **Output**: - Print the confusion matrix. - Display plots for the decision tree before and after pruning. - Include your analysis in a markdown cell if using Jupyter Notebook or as comments in the code. **Constraints**: - Use default parameters for the classifier unless specified otherwise. - Use `random_state=0` wherever applicable to ensure reproducibility. **Performance Requirements**: - The provided code should be well-commented and organized. - Ensure that visualizations are clear and labeled. ```python # Your solution starts here ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import confusion_matrix from sklearn.impute import SimpleImputer # Load the dataset data = load_iris() X, y = data.data, data.target # Introduce some missing values rng = np.random.RandomState(0) missing_mask = rng.rand(*X.shape) < 0.1 # 10% missing values artificially X[missing_mask] = np.nan # Handling missing values by imputing with the mean imputer = SimpleImputer(strategy=\'mean\') X_imputed = imputer.fit_transform(X) # Train the Decision Tree Classifier clf = DecisionTreeClassifier(random_state=0) clf.fit(X_imputed, y) # Evaluate the model y_pred = clf.predict(X_imputed) conf_matrix = confusion_matrix(y, y_pred) print(\\"Confusion Matrix:\\") print(conf_matrix) # Pruning the Decision Tree path = clf.cost_complexity_pruning_path(X_imputed, y) ccp_alphas = path.ccp_alphas impurities = path.impurities # Find the optimal alpha and prune the tree clf_pruned = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alphas[-2]) clf_pruned.fit(X_imputed, y) # Plotting the decision tree before and after pruning plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plot_tree(clf, filled=True) plt.title(\\"Decision Tree before Pruning\\") plt.subplot(1, 2, 2) plot_tree(clf_pruned, filled=True) plt.title(\\"Decision Tree after Pruning\\") plt.show() # Analysis: DecisionTreeClassifier in scikit-learn can work with missing values handled through preprocessing like imputation. In our solution, we used SimpleImputer with a mean strategy to handle missing values. Pruning helps to reduce the complexity of the final model and avoid overfitting. It can slightly reduce the model\'s accuracy on training data but is likely to improve generalization to unseen data."},{"question":"# Pandas Data Types Manipulation You are given a dataset containing a mix of various pandas-supported data types, including timezone-aware datetime, categorical data, nullable integers, and booleans. Your task is to perform a series of operations to manipulate and analyze this dataset. Dataset Description The dataset is stored in a dictionary as follows: ```python data = { \\"timestamp\\": [\\"2021-05-01 12:00:00\\", \\"2021-06-01 12:00:00\\", \\"2021-07-01 12:00:00\\", \\"NaT\\"], \\"category\\": [\\"A\\", \\"B\\", \\"A\\", \\"B\\"], \\"nullable_int\\": [1, pd.NA, 2, pd.NA], \\"nullable_bool\\": [True, False, True, pd.NA] } ``` Your Task: 1. **Create DataFrame**: Convert the provided dictionary to a pandas DataFrame. The `timestamp` column should be timezone-aware datetimes (use UTC timezone), the `category` column should be categorical, the `nullable_int` column should use nullable integers, and the `nullable_bool` column should use nullable booleans. 2. **Fill Missing Values**: Fill the missing `timestamp` values with the current date and time (make sure to attach the UTC timezone). Fill `nullable_int` with -1 and `nullable_bool` with `False`. 3. **Add a New Column**: Add a new column called `month` which extracts the month from the `timestamp` column. 4. **Group and Summarize**: Group the DataFrame by the `category` column and calculate the mean of `nullable_int` and the sum of `nullable_bool`. 5. **Output the Result**: Print the resulting DataFrame from step 4. Constraints: - Do not modify the input data structure. - The solution should handle the dataset correctly using appropriate pandas data types. - The timezone for `timestamp` operations should always be UTC. Expected Output Format: A pandas DataFrame printed in the console showing the grouped `category` with the calculated mean of `nullable_int` and sum of `nullable_bool`. Example Output: ```python category mean_nullable_int sum_nullable_bool A 1.50 2 B -1.00 0 ``` Submit your solution keeping in mind the constraints and handling of data types as specified.","solution":"import pandas as pd from datetime import datetime import pytz def manipulate_dataframe(data): # Step 1: Create DataFrame df = pd.DataFrame(data) df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\'], utc=True) df[\'category\'] = df[\'category\'].astype(\'category\') df[\'nullable_int\'] = df[\'nullable_int\'].astype(\'Int64\') df[\'nullable_bool\'] = df[\'nullable_bool\'].astype(\'boolean\') # Step 2: Fill Missing Values current_time_utc = pd.Timestamp(datetime.now(), tz=pytz.UTC) df[\'timestamp\'].fillna(current_time_utc, inplace=True) df[\'nullable_int\'].fillna(-1, inplace=True) df[\'nullable_bool\'].fillna(False, inplace=True) # Step 3: Add a New Column df[\'month\'] = df[\'timestamp\'].dt.month # Step 4: Group and Summarize result_df = df.groupby(\'category\').agg( mean_nullable_int=pd.NamedAgg(column=\'nullable_int\', aggfunc=\'mean\'), sum_nullable_bool=pd.NamedAgg(column=\'nullable_bool\', aggfunc=\'sum\') ).reset_index() # Step 5: Output the Result print(result_df) return result_df"},{"question":"Objective Implement a threaded TCP server using the `socketserver` module in Python. The server should be able to handle multiple client connections simultaneously. Each client will send a number to the server, and the server will respond with the square of that number. Requirements 1. Define a request handler class `SquareRequestHandler` that inherits from `socketserver.BaseRequestHandler`. 2. Override the `handle()` method in `SquareRequestHandler` to: - Receive a number from the client. - Compute the square of the number. - Send the squared number back to the client. 3. Define a server class `ThreadedTCPServer` that inherits from `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. 4. Instantiate and run the server, ensuring it can handle multiple client requests simultaneously. Input and Output Formats - The client sends a single integer number. - The server responds with the square of the received number. Constraints - The input number will be a non-negative integer less than 1000. - The server should be able to handle at least 10 simultaneous client connections. Example - If the client sends `4`, the server responds with `16`. - If the client sends `15`, the server responds with `225`. Implementation Example Here is a structured outline of the implementation: 1. **Request Handler Implementation**: ```python import socketserver class SquareRequestHandler(socketserver.BaseRequestHandler): def handle(self): # Receive data (a number) from the client self.data = self.request.recv(1024).strip() number = int(self.data) # Compute the square of the number square = number ** 2 # Send the squared number back to the client self.request.sendall(str(square).encode()) ``` 2. **Threaded Server Implementation**: ```python class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass ``` 3. **Main Script to Run the Server**: ```python if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 # Create the server, binding to localhost on port 9999 with ThreadedTCPServer((HOST, PORT), SquareRequestHandler) as server: print(\\"Server running...\\") # Activate the server to keep running until interrupted server.serve_forever() ``` 4. **Client Script** (for testing purposes): ```python import socket def client(ip, port, number): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: # Connect to server and send number sock.connect((ip, port)) sock.sendall(str(number).encode()) # Receive the squared number from the server received = int(sock.recv(1024).decode()) print(f\\"Sent: {number}, Received: {received}\\") if __name__ == \\"__main__\\": client(\\"localhost\\", 9999, 4) client(\\"localhost\\", 9999, 15) client(\\"localhost\\", 9999, 8) ``` # Submission: Submit the implementation of `SquareRequestHandler`, `ThreadedTCPServer`, and the main script to run the server. Ensure that your server can handle multiple clients simultaneously and correctly returns the squared numbers. **Note:** You may use the provided client script to test your server implementation.","solution":"import socketserver class SquareRequestHandler(socketserver.BaseRequestHandler): def handle(self): # Receive data (a number) from the client self.data = self.request.recv(1024).strip() number = int(self.data) # Compute the square of the number square = number ** 2 # Send the squared number back to the client self.request.sendall(str(square).encode()) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 # Create the server, binding to localhost on port 9999 with ThreadedTCPServer((HOST, PORT), SquareRequestHandler) as server: print(\\"Server running...\\") # Activate the server to keep running until interrupted server.serve_forever()"},{"question":"# Custom Asynchronous TCP Echo Server and Client You are tasked with implementing a custom asynchronous TCP echo server and client using low-level asyncio APIs, focusing on Transports and Protocols. Your implementation should follow these specifications: 1. **Echo Protocol**: - Create a custom protocol `EchoProtocol` which inherits from `asyncio.Protocol`. - Implement the method `data_received(self, data)` to send back received data to the client. - Implement the method `connection_made(self, transport)` to store the transport and log a message when a connection is made. - Implement the method `connection_lost(self, exc)` to log a message when the connection is lost. 2. **TCP Server**: - Create an asynchronous function `start_server()` that sets up a TCP server using `loop.create_server()`. - Ensure the server listens on `127.0.0.1` and port `8888`. - The server should run indefinitely, accepting and processing client connections. 3. **TCP Client**: - Create a custom protocol `EchoClientProtocol` that inherits from `asyncio.Protocol`. - Implement the method `connection_made(self, transport)` to send an initial message \\"Hello, World!\\" to the server. - Implement the method `data_received(self, data)` to print the received echoed message and close the connection. - Implement the method `connection_lost(self, exc)` to log a message when the connection is lost and signal the client loop to stop. 4. **Run Server and Client**: - Ensure the server is set up to run before the client tries to connect. - Implement an asynchronous main function to handle both server and client logic. # Input and Output Input: - No user input is required. The client will send a predefined message \\"Hello, World!\\" to the server. Output: - The server should log connections and echoed messages. - The client should print the received echo message from the server. # Constraints - Use asyncio package for all asynchronous operations. - Follow the given structure and requirements strictly. - Ensure proper exception handling and clean up connections gracefully. # Example Output: For the server: ``` Server started on 127.0.0.1:8888 Connection from (\'127.0.0.1\', 12345) Data received: \'Hello, World!\' Send: \'Hello, World!\' Close the client socket Connection from (\'127.0.0.1\', 12345) lost. ``` For the client: ``` Data sent: \'Hello, World!\' Data received: \'Hello, World!\' The server closed the connection ``` Starter Code: ```python import asyncio class EchoProtocol(asyncio.Protocol): def connection_made(self, transport): # Your code here def data_received(self, data): # Your code here def connection_lost(self, exc): # Your code here async def start_server(): # Your code here class EchoClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): # Your code here def connection_made(self, transport): # Your code here def data_received(self, data): # Your code here def connection_lost(self, exc): # Your code here async def main(): # Your code here if __name__ == \\"__main__\\": asyncio.run(main()) ```","solution":"import asyncio class EchoProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\\"Connection from {peername}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") print(f\\"Send: {message}\\") self.transport.write(data) def connection_lost(self, exc): if exc: print(f\\"The connection was lost due to an error: {exc}\\") else: print(\\"Connection closed by the client\\") async def start_server(): loop = asyncio.get_running_loop() server = await loop.create_server(EchoProtocol, \'127.0.0.1\', 8888) print(\\"Server started on 127.0.0.1:8888\\") async with server: await server.serve_forever() class EchoClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): self.transport = transport print(f\\"Data sent: {self.message}\\") transport.write(self.message.encode()) def data_received(self, data): print(f\\"Data received: {data.decode()}\\") self.transport.close() def connection_lost(self, exc): if exc: print(f\\"The connection was lost due to an error: {exc}\\") else: print(\\"The server closed the connection\\") self.on_con_lost.set_result(True) async def main(): on_con_lost = asyncio.Future() message = \\"Hello, World!\\" loop = asyncio.get_running_loop() server_task = loop.create_task(start_server()) await asyncio.sleep(1) # Ensure the server starts before the client connects transport, protocol = await loop.create_connection( lambda: EchoClientProtocol(message, on_con_lost), \'127.0.0.1\', 8888) try: await on_con_lost finally: transport.close() server_task.cancel() try: await server_task except asyncio.CancelledError: pass if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question Title: Complex PyTorch Model Serialization and Deserialization Problem Statement You are required to create a custom PyTorch model with specific characteristics, serialize its state using both PyTorch’s standard serialization (`torch.save` and `torch.load`) and TorchScript (`torch.jit.script`, `torch.jit.save`, and `torch.jit.load`), and demonstrate the differences in these approaches. Your solution should include the following: 1. **Define a custom module** that contains at least two sub-modules (e.g., linear layers) and a method with control flow (e.g., an `if` statement). 2. **Save the model state** using `state_dict` and `torch.save`. 3. **Load the model state** into a new instance of the model using `load_state_dict`. 4. **Script the model** using TorchScript\'s tracing and scripting functionalities. 5. **Save and load the scripted models** using `torch.jit.save` and `torch.jit.load`. 6. **Run some inputs through the original, state-loaded, and both scripted models** and demonstrate that they produce the same outputs. Implementation Write the following Python functions: 1. **`create_custom_model()`**: - Creates and returns an instance of the defined custom module. 2. **`save_model_state(model: nn.Module, filepath: str)`**: - Saves the model’s state dict to the specified filepath. 3. **`load_model_state(model_cls: Type[nn.Module], filepath: str) -> nn.Module`**: - Loads and returns a model instance of type `model_cls` with its state restored from the specified filepath. 4. **`script_and_save_model_via_trace(model: nn.Module, filepath: str, example_input: torch.Tensor)`**: - Scripts the model using TorchScript’s tracing functionality and saves it to the specified filepath. 5. **`script_and_save_model_via_script(model: nn.Module, filepath: str)`**: - Scripts the model using TorchScript’s scripting functionality and saves it to the specified filepath. 6. **`load_scripted_model(filepath: str) -> torch.jit.ScriptModule`**: - Loads and returns a TorchScript module from the specified filepath. 7. **`compare_model_outputs(original_model: nn.Module, loaded_model: nn.Module, traced_model: torch.jit.ScriptModule, scripted_model: torch.jit.ScriptModule, input_tensor: torch.Tensor) -> Dict[str, torch.Tensor]`**: - Runs the given input tensor through the original, state-loaded, traced, and scripted models and returns a dictionary containing the outputs from each. Constraints - The model should be defined using PyTorch `torch.nn.Module` and should include at least two `torch.nn.Linear` layers. - The control flow in the model should be meaningful and impact the forward pass. - Use suitable test inputs (e.g., random tensors) for the comparison. - Include docstrings for each function to explain their purpose and usage. **Expected Input and Output Formats:** - The input and output tensors should demonstrate the equivalence and correctness of the original, state-loaded, and TorchScript models. ```python import torch from torch import nn import torch.jit from typing import Type, Dict class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.layer1 = nn.Linear(10, 10) self.layer2 = nn.Linear(10, 1) def forward(self, x): if x.mean() > 0.5: x = torch.relu(self.layer1(x)) return self.layer2(x) def create_custom_model() -> nn.Module: Creates and returns an instance of the custom module. return CustomModel() def save_model_state(model: nn.Module, filepath: str): Saves the model’s state dict to the specified filepath. torch.save(model.state_dict(), filepath) def load_model_state(model_cls: Type[nn.Module], filepath: str) -> nn.Module: Loads and returns a model instance of type model_cls with its state restored from the specified filepath. model = model_cls() model.load_state_dict(torch.load(filepath)) return model def script_and_save_model_via_trace(model: nn.Module, filepath: str, example_input: torch.Tensor): Scripts the model using TorchScript’s tracing functionality and saves it to the specified filepath. traced_model = torch.jit.trace(model, example_input) torch.jit.save(traced_model, filepath) def script_and_save_model_via_script(model: nn.Module, filepath: str): Scripts the model using TorchScript’s scripting functionality and saves it to the specified filepath. scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, filepath) def load_scripted_model(filepath: str) -> torch.jit.ScriptModule: Loads and returns a TorchScript module from the specified filepath. return torch.jit.load(filepath) def compare_model_outputs(original_model: nn.Module, loaded_model: nn.Module, traced_model: torch.jit.ScriptModule, scripted_model: torch.jit.ScriptModule, input_tensor: torch.Tensor) -> Dict[str, torch.Tensor]: Runs the given input tensor through the original, state-loaded, traced, and scripted models and returns a dictionary containing the outputs from each. with torch.no_grad(): return { \\"original_model\\": original_model(input_tensor), \\"loaded_model\\": loaded_model(input_tensor), \\"traced_model\\": traced_model(input_tensor), \\"scripted_model\\": scripted_model(input_tensor) } # Example usage: if __name__ == \\"__main__\\": model = create_custom_model() save_model_state(model, \\"model_state.pt\\") loaded_model = load_model_state(CustomModel, \\"model_state.pt\\") example_input = torch.randn(1, 10) script_and_save_model_via_trace(model, \\"model_traced.pt\\", example_input) script_and_save_model_via_script(model, \\"model_scripted.pt\\") traced_model = load_scripted_model(\\"model_traced.pt\\") scripted_model = load_scripted_model(\\"model_scripted.pt\\") outputs = compare_model_outputs(model, loaded_model, traced_model, scripted_model, example_input) for name, output in outputs.items(): print(f\\"{name}: {output}\\") ```","solution":"import torch from torch import nn import torch.jit from typing import Type, Dict class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.layer1 = nn.Linear(10, 10) self.layer2 = nn.Linear(10, 1) def forward(self, x): if x.mean() > 0.5: x = torch.relu(self.layer1(x)) return self.layer2(x) def create_custom_model() -> nn.Module: Creates and returns an instance of the custom module. return CustomModel() def save_model_state(model: nn.Module, filepath: str): Saves the model’s state dict to the specified filepath. torch.save(model.state_dict(), filepath) def load_model_state(model_cls: Type[nn.Module], filepath: str) -> nn.Module: Loads and returns a model instance of type model_cls with its state restored from the specified filepath. model = model_cls() model.load_state_dict(torch.load(filepath)) return model def script_and_save_model_via_trace(model: nn.Module, filepath: str, example_input: torch.Tensor): Scripts the model using TorchScript’s tracing functionality and saves it to the specified filepath. traced_model = torch.jit.trace(model, example_input) torch.jit.save(traced_model, filepath) def script_and_save_model_via_script(model: nn.Module, filepath: str): Scripts the model using TorchScript’s scripting functionality and saves it to the specified filepath. scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, filepath) def load_scripted_model(filepath: str) -> torch.jit.ScriptModule: Loads and returns a TorchScript module from the specified filepath. return torch.jit.load(filepath) def compare_model_outputs(original_model: nn.Module, loaded_model: nn.Module, traced_model: torch.jit.ScriptModule, scripted_model: torch.jit.ScriptModule, input_tensor: torch.Tensor) -> Dict[str, torch.Tensor]: Runs the given input tensor through the original, state-loaded, traced, and scripted models and returns a dictionary containing the outputs from each. with torch.no_grad(): return { \\"original_model\\": original_model(input_tensor), \\"loaded_model\\": loaded_model(input_tensor), \\"traced_model\\": traced_model(input_tensor), \\"scripted_model\\": scripted_model(input_tensor) }"},{"question":"# Python Coding Assessment: Advanced Date and Time Manipulation Objective To evaluate your understanding and ability to work with the `datetime` module in Python for handling complex date and time manipulations including time zone adjustments, date arithmetic, and formatting/parsing. Problem Statement You are given a dataset of events with the start time specified in UTC. Each event includes the local timezone information. Your task is to perform the following operations: 1. **Date Arithmetic and Conversion**: - Calculate the end time of each event given its duration in hours. - Convert the end time to the corresponding local time for each event. - Determine if the event end time falls on a weekend (Saturday or Sunday) in its local time zone. 2. **Formatting and Parsing**: - Format the start and end times in the local time zone into human-readable strings. - Parse a given date string back into a `datetime` object. Input - A list of dictionaries, where each dictionary represents an event with the following keys: - `\\"id\\"`: (int) Event identifier. - `\\"start_time\\"`: (str) Start time in UTC (ISO 8601 format, e.g., `\\"2023-11-15T10:05:00Z\\"`). - `\\"duration_hours\\"`: (int) Duration of the event in hours. - `\\"timezone\\"`: (str) Timezone of the event in the format `\\"UTC±HH:MM\\"` (e.g., `\\"UTC+05:30\\"`). - A date string to parse, in the ISO 8601 format (e.g., `\\"2023-12-25\\"`). Output - A list of dictionaries, where each dictionary represents an event with the following keys: - `\\"id\\"`: (int) Event identifier. - `\\"local_start_time\\"`: (str) Local start time in ISO 8601 format. - `\\"local_end_time\\"`: (str) Local end time in ISO 8601 format. - `\\"is_weekend\\"`: (bool) `True` if the end time falls on a weekend, `False` otherwise. - A `datetime` object parsed from the given date string. Constraints - Ensure that the durations are within a reasonable range (0 <= duration_hours <= 48). - The input date string will always be a valid ISO 8601 date string. Example Input: ```python events = [ { \\"id\\": 1, \\"start_time\\": \\"2023-11-15T10:05:00Z\\", \\"duration_hours\\": 3, \\"timezone\\": \\"UTC+05:30\\" }, { \\"id\\": 2, \\"start_time\\": \\"2023-11-18T20:00:00Z\\", \\"duration_hours\\": 5, \\"timezone\\": \\"UTC-07:00\\" } ] date_string = \\"2023-12-25\\" ``` Output: ```python event_results = [ { \\"id\\": 1, \\"local_start_time\\": \\"2023-11-15T15:35:00+05:30\\", \\"local_end_time\\": \\"2023-11-15T18:35:00+05:30\\", \\"is_weekend\\": False }, { \\"id\\": 2, \\"local_start_time\\": \\"2023-11-18T13:00:00-07:00\\", \\"local_end_time\\": \\"2023-11-18T18:00:00-07:00\\", \\"is_weekend\\": True } ] parsed_date = datetime.datetime(2023, 12, 25, 0, 0) ``` Implementation Implement the function `process_events_and_parse_date(events, date_string)` that performs the described operations. ```python from datetime import datetime, timedelta, timezone def process_events_and_parse_date(events, date_string): results = [] for event in events: event_id = event[\\"id\\"] start_time_utc = datetime.fromisoformat(event[\\"start_time\\"][:-1]) duration = timedelta(hours=event[\\"duration_hours\\"]) tz_offset = timedelta(hours=int(event[\\"timezone\\"][3:6]), minutes=int(event[\\"timezone\\"][7:])) if event[\\"timezone\\"][3] == \'-\': tz_offset = -tz_offset tz = timezone(tz_offset) local_start_time = start_time_utc.astimezone(tz) local_end_time = (start_time_utc + duration).astimezone(tz) is_weekend = local_end_time.weekday() >= 5 results.append({ \\"id\\": event_id, \\"local_start_time\\": local_start_time.isoformat(), \\"local_end_time\\": local_end_time.isoformat(), \\"is_weekend\\": is_weekend }) parsed_date = datetime.fromisoformat(date_string) return results, parsed_date # Example usage events = [ { \\"id\\": 1, \\"start_time\\": \\"2023-11-15T10:05:00Z\\", \\"duration_hours\\": 3, \\"timezone\\": \\"UTC+05:30\\" }, { \\"id\\": 2, \\"start_time\\": \\"2023-11-18T20:00:00Z\\", \\"duration_hours\\": 5, \\"timezone\\": \\"UTC-07:00\\" } ] date_string = \\"2023-12-25\\" print(process_events_and_parse_date(events, date_string)) ```","solution":"from datetime import datetime, timedelta, timezone def process_events_and_parse_date(events, date_string): results = [] for event in events: event_id = event[\\"id\\"] start_time_utc = datetime.fromisoformat(event[\\"start_time\\"][:-1] + \\"+00:00\\") duration = timedelta(hours=event[\\"duration_hours\\"]) # Parsing timezone offset sign = 1 if event[\\"timezone\\"][3] == \'+\' else -1 hours_offset = int(event[\\"timezone\\"][4:6]) minutes_offset = int(event[\\"timezone\\"][7:]) tz_offset = sign * timedelta(hours=hours_offset, minutes=minutes_offset) tz = timezone(tz_offset) # Converting times local_start_time = start_time_utc.astimezone(tz) local_end_time = (start_time_utc + duration).astimezone(tz) # Checking if the end time falls on a weekend is_weekend = local_end_time.weekday() >= 5 # Adding results for the event results.append({ \\"id\\": event_id, \\"local_start_time\\": local_start_time.isoformat(), \\"local_end_time\\": local_end_time.isoformat(), \\"is_weekend\\": is_weekend }) # Parsing the given date string parsed_date = datetime.fromisoformat(date_string) return results, parsed_date"},{"question":"**Task**: Implement a cookie manager utility using the `http.cookies` module. This utility will: 1. Load cookies from a given string or dictionary input. 2. Add new cookies with various attributes. 3. Encode and decode cookie values. 4. Generate and return the appropriate HTTP header string for setting cookies. **Requirements**: - **Function Name**: `cookie_manager` - **Parameters**: - `action`: str, one of \'load\', \'add\', \'encode\', \'decode\', or \'output\'. - `data`: Varies based on `action`: - For \'load\', a string or dictionary representing cookie data. - For \'add\', a dictionary with keys \'name\', \'value\', and optional attributes such as \'path\', \'domain\', etc. - For \'encode\' or \'decode\', a dictionary with a single key \'value\'. - **Output**: - For \'load\', return a `SimpleCookie` instance. - For \'add\', return the updated `SimpleCookie` instance. - For \'encode\', return a tuple (real_value, coded_value). - For \'decode\', return the decoded value. - For \'output\', return the HTTP header string representation of all cookies. **Example Usage**: ```python # Loading cookies from a string cookies = cookie_manager(\'load\', \'chips=ahoy; vienna=finger\') print(cookies.output()) # Adding a cookie with attributes updated_cookies = cookie_manager(\'add\', {\'name\': \'oreo\', \'value\': \'doublestuff\', \'path\': \'/\'}) print(updated_cookies.output()) # Encoding a value encoded_value = cookie_manager(\'encode\', {\'value\': \'some_value\'}) print(encoded_value) # Decoding a value decoded_value = cookie_manager(\'decode\', {\'value\': encoded_value[1]}) print(decoded_value) # Getting the HTTP header string header_string = cookie_manager(\'output\', None) print(header_string) ``` **Constraints**: - Assume inputs will be valid strings or dictionaries as per the description. - Ensure proper exception handling for invalid operations or formats. - Comprehensive test cases will be checked for various scenarios, including edge cases with special characters. Good luck!","solution":"import http.cookies import urllib.parse cookie_store = http.cookies.SimpleCookie() def cookie_manager(action, data): global cookie_store if action == \'load\': cookie_store = http.cookies.SimpleCookie() if isinstance(data, str): cookie_store.load(data) elif isinstance(data, dict): for key, value in data.items(): cookie_store[key] = value return cookie_store elif action == \'add\': name = data.get(\'name\') value = data.get(\'value\') cookie_store[name] = value for attr, val in data.items(): if attr not in [\'name\', \'value\']: cookie_store[name][attr] = val return cookie_store elif action == \'encode\': real_value = data[\'value\'] coded_value = urllib.parse.quote(real_value) return real_value, coded_value elif action == \'decode\': coded_value = data[\'value\'] decoded_value = urllib.parse.unquote(coded_value) return decoded_value elif action == \'output\': return cookie_store.output() else: raise ValueError(\\"Invalid action provided.\\")"},{"question":"**Question: UUID-based User Session Management** You are tasked with creating a Python module to manage user sessions in a web application. Each session must be uniquely identified using UUIDs, and the module should support various operations including session creation, retrieval, and validation. **Requirements:** 1. **Session Creation:** - Function: `create_session(username: str) -> str` - Generates a new session ID (UUID) for a given username. Use `uuid4()` to generate the UUID. - Store the session with the current timestamp and associated username. 2. **Session Retrieval:** - Function: `get_session(session_id: str) -> Optional[Tuple[str, datetime]]` - Retrieves the username and timestamp associated with a given session ID. - Returns `None` if the session ID does not exist. 3. **Session Validation:** - Function: `is_session_valid(session_id: str, expiration_time: int) -> bool` - Checks if the session ID is valid and not expired. - A session is considered valid if it exists and the current time is less than the stored timestamp plus the expiration time (in minutes). **Constraints:** - You must use the `uuid` module for UUID generation and handling. - Ensure thread-safety in session management operations. - Expiration time is provided in minutes. **Additional Details:** - Use a dictionary to store session information where keys are session IDs and values are tuples of username and timestamp. - Use Python’s `datetime` module to handle timestamps. **Example:** ```python import datetime import uuid from typing import Optional, Tuple, Dict class SessionManager: def __init__(self): self.sessions: Dict[str, Tuple[str, datetime.datetime]] = {} def create_session(self, username: str) -> str: # Generate a new UUID for the session session_id = str(uuid.uuid4()) # Store the session with the current timestamp self.sessions[session_id] = (username, datetime.datetime.now()) return session_id def get_session(self, session_id: str) -> Optional[Tuple[str, datetime.datetime]]: # Retrieve the session details if it exists return self.sessions.get(session_id) def is_session_valid(self, session_id: str, expiration_time: int) -> bool: # Check if the session ID exists session_info = self.sessions.get(session_id) if not session_info: return False # Check if the session is expired username, timestamp = session_info return datetime.datetime.now() < timestamp + datetime.timedelta(minutes=expiration_time) ``` Implement the `SessionManager` class with these functionalities. Write tests to verify your implementation.","solution":"import datetime import uuid from typing import Optional, Tuple, Dict class SessionManager: def __init__(self): self.sessions: Dict[str, Tuple[str, datetime.datetime]] = {} def create_session(self, username: str) -> str: # Generate a new UUID for the session session_id = str(uuid.uuid4()) # Store the session with the current timestamp self.sessions[session_id] = (username, datetime.datetime.now()) return session_id def get_session(self, session_id: str) -> Optional[Tuple[str, datetime.datetime]]: # Retrieve the session details if it exists return self.sessions.get(session_id) def is_session_valid(self, session_id: str, expiration_time: int) -> bool: # Check if the session ID exists session_info = self.sessions.get(session_id) if not session_info: return False # Check if the session is expired username, timestamp = session_info return datetime.datetime.now() < timestamp + datetime.timedelta(minutes=expiration_time)"},{"question":"# Python Coding Assessment: Attaching Metadata to TorchScript Models Objective You are required to demonstrate your understanding of PyTorch\'s serialization mechanisms by saving a TorchScript model and attaching additional metadata to it. Problem Statement Suppose you have trained a PyTorch model and you want to save this model as a TorchScript archive. In addition to the model parameters, you are required to attach some metadata to the saved model. This metadata will include the name of the model and the date and time when the model was saved. You are provided with a pre-trained model function and you need to: 1. Convert the model to TorchScript using `torch.jit.script`. 2. Save the TorchScript model along with metadata using `torch.jit.save`. 3. Implement a function to load this model back and verify the metadata. Task Implement the following functions: 1. `save_model_with_metadata(model: torch.nn.Module, file_path: str, metadata: dict) -> None`: - Convert the given PyTorch model to a TorchScript model. - Save the TorchScript model to the specified file path while attaching the provided metadata. 2. `load_model_metadata(file_path: str) -> dict`: - Load the saved TorchScript model from the specified file path. - Retrieve and return the metadata attached to the model. Input and Output Formats 1. `save_model_with_metadata`: - Input: - `model` (torch.nn.Module): The PyTorch model instance. - `file_path` (str): The file path to save the TorchScript model. - `metadata` (dict): A dictionary containing metadata to attach to the model. - Output: None. 2. `load_model_metadata`: - Input: - `file_path` (str): The file path from which the TorchScript model is to be loaded. - Output: (dict): The metadata dictionary retrieved from the saved model. Constraints - Use `torch.jit.script` to convert the model to TorchScript. - Use `_extra_files` argument in `torch.jit.save` to attach metadata. - Metadata dictionary may contain strings as keys and values. Example ```python import torch import torch.nn as nn # Example model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) model = SimpleModel() metadata = { \\"model_name\\": \\"SimpleModel\\", \\"saved_time\\": \\"2023-10-21 14:32:00\\" } file_path = \\"simple_model.pt\\" # Save the model with metadata save_model_with_metadata(model, file_path, metadata) # Load the metadata from the saved model loaded_metadata = load_model_metadata(file_path) print(loaded_metadata) ``` Additional Information - Ensure proper error handling and logging. - Handle file operations carefully to avoid resource leaks.","solution":"import torch import torch.nn as nn import json def save_model_with_metadata(model: nn.Module, file_path: str, metadata: dict) -> None: Save the TorchScript model with attached metadata. Parameters: - model (nn.Module): The PyTorch model to save. - file_path (str): The file path where the TorchScript model will be saved. - metadata (dict): A dictionary containing metadata to attach to the model. # Convert the model to TorchScript scripted_model = torch.jit.script(model) # Convert metadata dictionary to JSON string metadata_json = json.dumps(metadata) # Use _extra_files to attach metadata extra_files = {\\"metadata.json\\": metadata_json} # Save the scripted model with metadata torch.jit.save(scripted_model, file_path, _extra_files=extra_files) def load_model_metadata(file_path: str) -> dict: Load the metadata from a saved TorchScript model. Parameters: - file_path (str): The file path from which the TorchScript model is to be loaded. Returns: - dict: The metadata dictionary retrieved from the saved model. # Prepare the extra_files dictionary to load metadata extra_files = {\\"metadata.json\\": \\"\\"} # Load the TorchScript model _ = torch.jit.load(file_path, _extra_files=extra_files) # Parse the metadata JSON string metadata = json.loads(extra_files[\\"metadata.json\\"]) return metadata"},{"question":"**Objective**: This exercise is aimed to assess your understanding of Python\'s logging module. You are required to implement a logging system that meets specific requirements. # Problem Statement You are developing a logging system for an e-commerce web application. The application has multiple modules: `user_auth`, `product_catalog`, and `order_processing`. Each module logs different types of information and logs must be handled differently based on their severity and the module from which they originate. Your task is to set up an appropriate logging system that meets the following requirements: 1. **Logging Levels**: - `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL` 2. **Loggers**: - Create separate loggers for each module: `user_auth`, `product_catalog`, and `order_processing`. 3. **Handlers**: - Use a `RotatingFileHandler` to save logs to a file. Each log file should rotate when it reaches 1MB in size and keep a backup of the last 5 log files. - Use a `StreamHandler` to print ERROR and CRITICAL messages to the console. 4. **Formatters**: - Log messages must include the timestamp, logger name, log level, and the log message in the format: `%(asctime)s - %(name)s - %(levelname)s - %(message)s` # Input and Output * **Function**: The function should not take any input from the user. * **Module Initialization**: Write code that configures the logging system based on the specified requirements. * **Logging Example**: - Simulate log messages in each module for all log levels. - Ensure logs are correctly printed to the console and saved to the log files. # Constraints 1. You must use the Python standard logging library. 2. Custom handlers or any third-party libraries are not allowed. 3. Each module must use its respective logger to log messages. # Example Usage ```python import logging import logging.handlers def setup_logging(): # Implementation of logging configuration ... def simulate_logging(): user_auth_logger = logging.getLogger(\'user_auth\') product_logger = logging.getLogger(\'product_catalog\') order_logger = logging.getLogger(\'order_processing\') user_auth_logger.debug(\'Debugging user authentication\') user_auth_logger.info(\'User login successful\') user_auth_logger.warning(\'User login attempt failed\') user_auth_logger.error(\'User login error\') user_auth_logger.critical(\'User authentication system failure\') product_logger.debug(\'Debugging product retrieval\') product_logger.info(\'Product list retrieved\') product_logger.warning(\'Product retrieval slow\') product_logger.error(\'Product retrieval error\') product_logger.critical(\'Product catalog unavailable\') order_logger.debug(\'Debugging order process\') order_logger.info(\'Order placed successfully\') order_logger.warning(\'Order process slow\') order_logger.error(\'Order processing error\') order_logger.critical(\'Order processing system failure\') if __name__ == \\"__main__\\": setup_logging() simulate_logging() ``` Defining the `setup_logging` will ensure that the logging configuration is correctly set up according to the provided requirements. # Implementation Implement the `setup_logging` function to correctly configure the logging system as described.","solution":"import logging import logging.handlers def setup_logging(): # Define log file settings log_file = \'ecommerce.log\' max_log_size = 1 * 1024 * 1024 # 1MB backup_count = 5 # Create logger for each module user_auth_logger = logging.getLogger(\'user_auth\') product_logger = logging.getLogger(\'product_catalog\') order_logger = logging.getLogger(\'order_processing\') # Set level for loggers user_auth_logger.setLevel(logging.DEBUG) product_logger.setLevel(logging.DEBUG) order_logger.setLevel(logging.DEBUG) # Create Formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Create RotatingFileHandler file_handler = logging.handlers.RotatingFileHandler(log_file, maxBytes=max_log_size, backupCount=backup_count) file_handler.setFormatter(formatter) # Create StreamHandler for console output console_handler = logging.StreamHandler() console_handler.setLevel(logging.ERROR) console_handler.setFormatter(formatter) # Add handlers to each logger for logger in [user_auth_logger, product_logger, order_logger]: logger.addHandler(file_handler) logger.addHandler(console_handler) def simulate_logging(): user_auth_logger = logging.getLogger(\'user_auth\') product_logger = logging.getLogger(\'product_catalog\') order_logger = logging.getLogger(\'order_processing\') user_auth_logger.debug(\'Debugging user authentication\') user_auth_logger.info(\'User login successful\') user_auth_logger.warning(\'User login attempt failed\') user_auth_logger.error(\'User login error\') user_auth_logger.critical(\'User authentication system failure\') product_logger.debug(\'Debugging product retrieval\') product_logger.info(\'Product list retrieved\') product_logger.warning(\'Product retrieval slow\') product_logger.error(\'Product retrieval error\') product_logger.critical(\'Product catalog unavailable\') order_logger.debug(\'Debugging order process\') order_logger.info(\'Order placed successfully\') order_logger.warning(\'Order process slow\') order_logger.error(\'Order processing error\') order_logger.critical(\'Order processing system failure\') if __name__ == \\"__main__\\": setup_logging() simulate_logging()"},{"question":"# Command Line Option Parser Implementation You are tasked with creating a script that will parse command line arguments for a custom application using the `getopt` module. Input 1. A list of command line arguments (excluding the script name). 2. Short option characters. 3. Corresponding long option strings. Requirements 1. Your script should properly handle both short and long options. 2. It should detect and handle missing arguments for options that require them. 3. It should separate option-value pairs from other arguments. 4. You should manage exceptions raised by incorrect options or missing option arguments. Function Signature ```python def custom_option_parser(args: list, shortopts: str, longopts: list) -> tuple: Parses command line options and arguments. Args: args : list - List of command line arguments. shortopts : str - String of option letters with options requiring arguments followed by a colon. longopts : list - List of long option names. Returns: A tuple containing: - A list of (option, value) pairs. - A list of remaining non-option arguments. pass ``` Example Usage ```python args = [\'--condition=foo\', \'--testing\', \'--output-file\', \'abc.def\', \'-x\', \'a1\', \'a2\'] shortopts = \'x\' longopts = [\'condition=\', \'output-file=\', \'testing\'] optlist, args = custom_option_parser(args, shortopts, longopts) print(optlist) # Output: [(\'--condition\', \'foo\'), (\'--testing\', \'\'), (\'--output-file\', \'abc.def\'), (\'-x\', \'\')] print(args) # Output: [\'a1\', \'a2\'] ``` Constraints 1. Assume that the list of arguments will not contain the script name. 2. Long options can be either complete or an identifiable prefix. 3. The solution should utilize the `getopt.getopt()` function for parsing. Additional Information Ensure that your implementation catches and handles `getopt.GetoptError` appropriately, providing meaningful error messages in case of invalid options or missing arguments.","solution":"import getopt def custom_option_parser(args: list, shortopts: str, longopts: list) -> tuple: Parses command line options and arguments. Args: args : list - List of command line arguments. shortopts : str - String of option letters with options requiring arguments followed by a colon. longopts : list - List of long option names. Returns: A tuple containing: - A list of (option, value) pairs. - A list of remaining non-option arguments. try: opts, remainder = getopt.getopt(args, shortopts, longopts) return opts, remainder except getopt.GetoptError as err: raise ValueError(f\\"Error parsing options: {err}\\")"},{"question":"# Question: You are provided with a dataset containing various features extracted from a collection of images. Your task is to cluster these images using different clustering techniques and evaluate their performance. Dataset - The dataset is a CSV file named `image_features.csv`, where each row corresponds to an image and columns are the features. Instructions: 1. **Load the Dataset**: Load the dataset using `pandas` and inspect the first few rows. 2. **Data Preprocessing**: - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 3. **Clustering**: Apply the following clustering algorithms on the preprocessed dataset: - KMeans - DBSCAN - Agglomerative Clustering 4. **Parameter Selection**: - For KMeans, use `k=3`. - For DBSCAN, use `eps=0.5` and `min_samples=5`. - For Agglomerative Clustering, use `n_clusters=3`. 5. **Evaluation**: Use the following metrics to evaluate the clustering results: - Adjusted Rand Index: `adjusted_rand_score` - Silhouette Coefficient: `silhouette_score` 6. **Dimensionality Reduction (Bonus)**: Use PCA to reduce the dimensionality to 2 components and visualize the clusters using a scatter plot for each clustering method. Expected Function Signatures: ```python import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score import matplotlib.pyplot as plt from sklearn.decomposition import PCA def load_and_preprocess_data(file_path: str) -> pd.DataFrame: Load the dataset and preprocess it. Parameters: file_path: str - The path to the CSV file containing the dataset Returns: pd.DataFrame - The preprocessed dataset pass def cluster_and_evaluate(df: pd.DataFrame): Apply clustering algorithms and evaluate them. Parameters: df: pd.DataFrame - The preprocessed dataset Returns: dict - A dictionary containing the evaluation results pass def visualize_clusters(df: pd.DataFrame, labels: dict): Visualize the clusters using PCA for dimensionality reduction. Parameters: df: pd.DataFrame - The preprocessed dataset labels: dict - A dictionary containing labels for each clustering method pass # Example usage: # df = load_and_preprocess_data(\'image_features.csv\') # results = cluster_and_evaluate(df) # visualize_clusters(df, results) ``` Constraints: - You should handle any potential exceptions during the loading and preprocessing steps. - Ensure reproducibility by setting a random seed where applicable. Performance Requirements: - Your solution should handle datasets with up to 10,000 samples efficiently. - The visualizations should be clear and distinguishable. Implement the functions specified and provide the evaluations and visualizations as specified.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score from sklearn.decomposition import PCA import matplotlib.pyplot as plt def load_and_preprocess_data(file_path: str) -> pd.DataFrame: Load the dataset and preprocess it. Parameters: file_path: str - The path to the CSV file containing the dataset Returns: pd.DataFrame - The preprocessed dataset try: df = pd.read_csv(file_path) except Exception as e: raise RuntimeError(f\\"An error occurred while loading the dataset: {e}\\") scaler = StandardScaler() df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns) return df_scaled def cluster_and_evaluate(df: pd.DataFrame): Apply clustering algorithms and evaluate them. Parameters: df: pd.DataFrame - The preprocessed dataset Returns: dict - A dictionary containing the evaluation results evaluation_results = {} # KMeans Clustering kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(df) evaluation_results[\'KMeans\'] = { \'labels\': kmeans_labels, \'silhouette_score\': silhouette_score(df, kmeans_labels), } # DBSCAN Clustering dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(df) if len(set(dbscan_labels)) > 1: # Valid silhouette score can only be computed with more than one cluster evaluation_results[\'DBSCAN\'] = { \'labels\': dbscan_labels, \'silhouette_score\': silhouette_score(df, dbscan_labels), } else: evaluation_results[\'DBSCAN\'] = { \'labels\': dbscan_labels, \'silhouette_score\': None # Invalid because only one cluster or noise detected } # Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=3) agglomerative_labels = agglomerative.fit_predict(df) evaluation_results[\'Agglomerative\'] = { \'labels\': agglomerative_labels, \'silhouette_score\': silhouette_score(df, agglomerative_labels), } return evaluation_results def visualize_clusters(df: pd.DataFrame, labels: dict): Visualize the clusters using PCA for dimensionality reduction. Parameters: df: pd.DataFrame - The preprocessed dataset labels: dict - A dictionary containing labels for each clustering method pca = PCA(n_components=2) pca_result = pca.fit_transform(df) df_visual = pd.DataFrame(pca_result, columns=[\'PCA1\', \'PCA2\']) plt.figure(figsize=(18, 6)) for idx, (cluster_method, result) in enumerate(labels.items(), 1): plt.subplot(1, 3, idx) plt.scatter(df_visual[\'PCA1\'], df_visual[\'PCA2\'], c=result[\'labels\'], cmap=\'viridis\') plt.title(f\'{cluster_method} Clustering\') plt.xlabel(\'PCA1\') plt.ylabel(\'PCA2\') plt.tight_layout() plt.show() # Example usage: # df = load_and_preprocess_data(\'image_features.csv\') # results = cluster_and_evaluate(df) # visualize_clusters(df, results)"},{"question":"You are tasked with designing a data class to manage information for university courses and students enrolling in them. Each course has a unique code, a name, and a list of enrolled students. Each student has an id, a name, and their grade for the course. Implementing this data class requires handling default values, keyword-only fields, and post-initialization processes. Your solution should: 1. Define two data classes: `Student` and `Course`. 2. Implement appropriate default values and factory methods for handling mutable fields. 3. Use keyword-only fields where applicable. 4. Implement a method in `Course` to compute the average grade of enrolled students. 5. Use post-initialization to handle enrollment processes that depend on other fields. Requirements: 1. **Class `Student`:** - Fields: `id`(int), `name`(str), `grade`(float) - Defaults: - `grade` should default to `0.0`. - Ensure the `id` and `name` fields are positional arguments while `grade` is a keyword-only field. 2. **Class `Course`:** - Fields: `course_code`(str), `course_name`(str), `students`(list of `Student` objects) - Defaults: - Use a factory method to initialize an empty list for `students`. - Method `average_grade()`: Computes and returns the average grade of enrolled students. - Use `__post_init__` to ensure that `students` always contains unique student ids (no duplicate enrollments). Example Usage: ```python from dataclasses import dataclass, field, InitVar from typing import List @dataclass class Student: id: int name: str grade: float = 0.0 @dataclass class Course: course_code: str course_name: str students: List[Student] = field(default_factory=list) def __post_init__(self): # Enforce unique student ids in the students list unique_students = {student.id: student for student in self.students} self.students = list(unique_students.values()) def average_grade(self) -> float: if not self.students: return 0.0 return sum(student.grade for student in self.students) / len(self.students) # Example usage: s1 = Student(id=1, name=\\"Alice\\", grade=85.0) s2 = Student(id=2, name=\\"Bob\\", grade=90.0) course = Course(course_code=\\"CS101\\", course_name=\\"Introduction to Computer Science\\", students=[s1, s1, s2]) print(course.students) # should only contain unique students print(course.average_grade()) # output: average grade ``` Constraints: - Do not use any libraries other than `dataclasses` and `typing`. - Ensure your code handles duplicate student entries in the `students` list within the Course class.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Student: id: int name: str grade: float = 0.0 @dataclass class Course: course_code: str course_name: str students: List[Student] = field(default_factory=list) def __post_init__(self): # Ensure unique student ids unique_students = {student.id: student for student in self.students} self.students = list(unique_students.values()) def average_grade(self) -> float: if not self.students: return 0.0 return sum(student.grade for student in self.students) / len(self.students)"},{"question":"**Objective**: Demonstrate your comprehension of the `ast` module in Python by analyzing and transforming a given Python script. **Question**: You are given a Python script as a string that contains various function definitions. You need to write a function `analyze_function_calls` that analyzes the script and returns a dictionary with the function names as keys and the number of times each function is called within the script as values. Your solution should parse the script into an AST, traverse the AST to find all function call occurrences, and maintain a count of how many times each function is called. **Input Format**: - A single string representing the content of a Python script. **Output Format**: - A dictionary where keys are function names (strings) and values are the number of times each function is called within the script (integers). **Constraints**: - You can assume that all function definitions have unique names. - The script may contain nested function calls and definitions. **Performance Requirements**: - The function should efficiently traverse the AST and handle scripts of moderate length (up to 1000 lines of code). **Example**: ```python script = \'\'\' def foo(): bar() baz() def bar(): foo() baz() def baz(): pass foo() bar() \'\'\' result = analyze_function_calls(script) print(result) # Output: {\'foo\': 2, \'bar\': 2, \'baz\': 2} ``` **Your Task**: Implement the function `analyze_function_calls(script: str) -> dict`. **Hint**: - Consider subclassing `ast.NodeVisitor` to create a custom visitor that tracks function calls. - Use the `visit_Call` method to identify `Call` nodes and extract function names. ```python import ast def analyze_function_calls(script: str) -> dict: class FunctionCallVisitor(ast.NodeVisitor): def __init__(self): self.call_counts = {} def visit_Call(self, node): func_name = self.get_func_name(node.func) if func_name: if func_name not in self.call_counts: self.call_counts[func_name] = 0 self.call_counts[func_name] += 1 self.generic_visit(node) def get_func_name(self, node): if isinstance(node, ast.Name): return node.id elif isinstance(node, ast.Attribute): return node.attr return None tree = ast.parse(script) visitor = FunctionCallVisitor() visitor.visit(tree) return visitor.call_counts # Test the function with the provided example script = \'\'\' def foo(): bar() baz() def bar(): foo() baz() def baz(): pass foo() bar() \'\'\' result = analyze_function_calls(script) print(result) # Output: {\'foo\': 2, \'bar\': 2, \'baz\': 2} ``` **Note**: - Make sure to test your implementation with various scripts to ensure accuracy.","solution":"import ast def analyze_function_calls(script: str) -> dict: class FunctionCallVisitor(ast.NodeVisitor): def __init__(self): self.call_counts = {} def visit_Call(self, node): func_name = self.get_func_name(node.func) if func_name: if func_name not in self.call_counts: self.call_counts[func_name] = 0 self.call_counts[func_name] += 1 self.generic_visit(node) def get_func_name(self, node): if isinstance(node, ast.Name): return node.id elif isinstance(node, ast.Attribute): return node.attr return None tree = ast.parse(script) visitor = FunctionCallVisitor() visitor.visit(tree) return visitor.call_counts"},{"question":"# TorchScript Custom Class and Module Implementation Objective: Design a TorchScript-compatible `NeuralNetwork` module that includes a custom `Layer` class. The module should be capable of handling inputs of different shapes and demonstrating proper usage of TorchScript typing and scripting. Requirements: 1. **Layer Class**: - Create a custom `Layer` class that accepts input size and output size during initialization. - The `Layer` class should have a forward method that performs a linear transformation on the input tensor. 2. **NeuralNetwork Module**: - Create a `NeuralNetwork` class inheriting from `torch.nn.Module`. - The `NeuralNetwork` should contain two instances of the `Layer` class. - Implement the `forward` method to pass the input tensor through both layers sequentially. 3. **Type Annotations**: - Properly annotate the types of all methods and variables using TorchScript typing. - Ensure any attributes in classes are correctly annotated to prevent type errors. 4. **Scripting with TorchScript**: - Use `torch.jit.script` to create a scripted module of your `NeuralNetwork` class. - Demonstrate handling inputs of different shapes/types using the scripted module. Input Format: - Sequence of floats, e.g., `torch.randn(some_shape)` where `some_shape` is any valid tensor shape. Output Format: - Transformed tensor after passing through the `NeuralNetwork` module. Constraints: - The input to the `NeuralNetwork` can be of arbitrary shape, but the `Layer` class should ensure compatible transformations. - Utilize TorchScript features and annotations correctly to ensure the code is torchscriptable. Performance Requirements: - Ensure the operations are efficient, leveraging PyTorch\'s capabilities to handle large tensor computations. Example: ```python import torch import torch.nn as nn from typing import Tuple @torch.jit.script class Layer: def __init__(self, in_features: int, out_features: int): self.weight = torch.nn.Parameter(torch.randn(in_features, out_features)) self.bias = torch.nn.Parameter(torch.randn(out_features)) def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.matmul(x, self.weight) + self.bias class NeuralNetwork(nn.Module): def __init__(self, in_size: int, hidden_size: int, out_size: int): super(NeuralNetwork, self).__init__() self.layer1 = Layer(in_size, hidden_size) self.layer2 = Layer(hidden_size, out_size) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) x = self.layer2(x) return x # Example usage model = NeuralNetwork(5, 10, 2) scripted_model = torch.jit.script(model) # Test with random input input_tensor = torch.randn((1, 5)) output = scripted_model(input_tensor) print(output) ```","solution":"import torch import torch.nn as nn from typing import Tuple class Layer(nn.Module): def __init__(self, in_features: int, out_features: int): super(Layer, self).__init__() self.weight = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.matmul(x, self.weight) + self.bias class NeuralNetwork(nn.Module): def __init__(self, in_size: int, hidden_size: int, out_size: int): super(NeuralNetwork, self).__init__() self.layer1 = Layer(in_size, hidden_size) self.layer2 = Layer(hidden_size, out_size) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) x = self.layer2(x) return x # Example usage model = NeuralNetwork(5, 10, 2) scripted_model = torch.jit.script(model) # Test with random input input_tensor = torch.randn((1, 5)) output = scripted_model(input_tensor) print(output)"},{"question":"You are required to implement a function that interacts with the operating system\'s environment variables using the \\"os\\" module. This function should: 1. Retrieve the current value of a specified environment variable. 2. Set a new value for this environment variable. 3. Verify that the new value has been set correctly. 4. Restore the original value of the environment variable. # Function Signature ```python def manipulate_environment_variable(var_name: str, new_value: str) -> str: Manipulates an environment variable by setting and verifying a new value, then restores the original value. Args: var_name (str): The name of the environment variable to manipulate. new_value (str): The new value to set for the environment variable. Returns: str: The value of the environment variable after restoration to its original state. ``` # Input - `var_name` (str): The name of the environment variable to manipulate. - `new_value` (str): The new value to set for the environment variable. # Output - Returns a string representing the value of the environment variable after restoration. # Example Usage ```python original_value = manipulate_environment_variable(\'TEMP_TEST_VAR\', \'new_value\') print(original_value) ``` # Constraints 1. If the environment variable does not exist prior to running the function, it should be created and then removed. 2. The function should handle possible exceptions, such as permission errors or invalid environment variable names, gracefully. 3. The function should rely solely on the capabilities provided by the \\"os\\" module and should avoid directly using \\"posix\\". # Requirements 1. Use environment-variable-related functions and properties provided by the \\"os\\" module. 2. Ensure that the environment is left unchanged or restored to its original state after the function executes. 3. Handle large-size values for environment variables, if applicable, as per the constraints of the operating system. # Additional Notes - Consider the cross-platform nature of the \\"os\\" module and ensure your solution works consistently across different operating systems.","solution":"import os def manipulate_environment_variable(var_name: str, new_value: str) -> str: Manipulates an environment variable by setting and verifying a new value, then restores the original value. Args: var_name (str): The name of the environment variable to manipulate. new_value (str): The new value to set for the environment variable. Returns: str: The value of the environment variable after restoration to its original state. original_value = os.environ.get(var_name) try: os.environ[var_name] = new_value # Verify the new value assert os.environ[var_name] == new_value, \\"Failed to set the new environment variable value\\" # Restore the original value if original_value is not None: os.environ[var_name] = original_value else: del os.environ[var_name] return os.environ.get(var_name, None) except Exception as e: raise RuntimeError(f\\"Error manipulating environment variable: {e}\\")"},{"question":"# Question: Draw a Fractal Tree using Turtle Graphics In this assignment, you are required to write a function that uses the `turtle` module to draw a fractal tree. This will demonstrate your understanding of recursion, event handling, and turtle graphics. Instructions 1. **Function Signature**: ```python def draw_fractal_tree(branch_length, angle, min_length): Draws a fractal tree starting with the specified branch length, branching at a given angle, and stopping when branches are shorter than min_length. Parameters: - branch_length (int or float): The initial length of the trunk. - angle (int or float): The angle at which branches bifurcate. - min_length (int or float): The length at which the recursion stops. ``` 2. **Setup**: * Import the `turtle` module. * Create and set up the screen. * Position the turtle at the bottom center of the screen facing upward. 3. **Recursive Tree Drawing**: * If the `branch_length` is greater than `min_length`, do the following: - Move the turtle forward by `branch_length`. - Turn the turtle to the right by `angle` degrees, and recursively call `draw_fractal_tree` with a shorter `branch_length` (e.g., `branch_length * 0.7`). - Turn the turtle back by double the `angle` to position it correctly for drawing the other branch, and again recursively call `draw_fractal_tree`. - Realign the turtle by returning to its heading before the recursion, and move the turtle back to the original branch start point. 4. **Event Handling**: * Create an `onclick` event to exit the drawing when the screen is clicked. 5. **Main Function**: * The `main` function should call the setup and draw the fractal tree with appropriate parameters. Example Usage: ```python import turtle def draw_fractal_tree(branch_length, angle, min_length): if branch_length > min_length: turtle.forward(branch_length) turtle.right(angle) draw_fractal_tree(branch_length * 0.7, angle, min_length) turtle.left(2 * angle) draw_fractal_tree(branch_length * 0.7, angle, min_length) turtle.right(angle) turtle.backward(branch_length) def main(): screen = turtle.Screen() screen.setup(width=800, height=600) screen.bgcolor(\\"white\\") turtle.speed(0) turtle.left(90) turtle.penup() turtle.goto(0, -250) turtle.pendown() turtle.color(\\"brown\\") draw_fractal_tree(100, 30, 5) screen.exitonclick() if __name__ == \\"__main__\\": main() ``` Constraints: - Do not use any libraries other than `turtle`. - Your recursive function should handle the base case appropriately to prevent infinite recursion. - The `draw_fractal_tree` function should be versatile in terms of initial branch length and angle. This assignment will test your ability to work with recursion, graphical implementation using the `turtle` module, and event-driven programming in Python.","solution":"import turtle def draw_fractal_tree(branch_length, angle, min_length): if branch_length > min_length: turtle.forward(branch_length) turtle.right(angle) draw_fractal_tree(branch_length * 0.7, angle, min_length) turtle.left(2 * angle) draw_fractal_tree(branch_length * 0.7, angle, min_length) turtle.right(angle) turtle.backward(branch_length) def main(): screen = turtle.Screen() screen.setup(width=800, height=600) screen.bgcolor(\\"white\\") turtle.speed(0) turtle.left(90) turtle.penup() turtle.goto(0, -250) turtle.pendown() turtle.color(\\"brown\\") draw_fractal_tree(100, 30, 5) screen.exitonclick() if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Subprocess Handling in Distributed Computing In this exercise, you are tasked with demonstrating your understanding of subprocess management within a distributed computing environment using PyTorch. You will implement a function that initializes a `SubprocessHandler` and deploys a subprocess to execute a simple distributed computation. Function Signature ```python def run_distributed_computation(script_path: str, num_procs: int) -> int: Runs a distributed computation using SubprocessHandler. Args: - script_path (str): The file path to the script that will be run in subprocesses. - num_procs (int): The number of subprocesses to create. Returns: - int: The exit code of the subprocess. pass ``` Inputs - `script_path` : A string that represents the path to the Python script to be executed by subprocesses. This script should be capable of performing some distributed computation. - `num_procs` : An integer indicating the number of subprocesses to create and run. Outputs - You should return an integer, representing the exit code of the subprocess. Instructions 1. Implement the `run_distributed_computation` function. 2. Your function should retrieve a `SubprocessHandler` using `get_subprocess_handler`. 3. The `SubprocessHandler` should be used to create the specified number of subprocesses (`num_procs`) and run the given script (`script_path`). 4. Ensure proper error handling so that if the subprocess fails, an appropriate exit code is returned. Constraints - You must use `torch.distributed.elastic.multiprocessing.subprocess_handler` functionalities. - Consider potential issues in creating and managing subprocesses, such as deadlocks. Assumptions - The environment is correctly set up for distributed computation using PyTorch. - The provided script at `script_path` is valid and ready to be executed in a distributed manner. This question tests your ability to work with PyTorch\'s distributed computing tools and manage subprocesses effectively. Good luck!","solution":"import os import subprocess def run_distributed_computation(script_path: str, num_procs: int) -> int: Runs a distributed computation using SubprocessHandler. Args: - script_path (str): The file path to the Python script that will be run in subprocesses. - num_procs (int): The number of subprocesses to create. Returns: - int: The exit code of the subprocess. try: # Command to execute the distributed training script cmd = [\\"python\\", \\"-m\\", \\"torch.distributed.launch\\", f\\"--nproc_per_node={num_procs}\\", script_path] # Run the command as a subprocess result = subprocess.run(cmd, check=True) # Return the exit code of the subprocess return result.returncode except subprocess.CalledProcessError as e: # Return the error code if the subprocess fails return e.returncode"},{"question":"**Coding Question: Directory synchronization** You are required to implement a function, `synchronize_directories(source_dir, destination_dir)`, that synchronizes the contents of the `destination_dir` with the `source_dir` using the `filecmp` module. This function should ensure the following: - Any files present in `source_dir` and not in `destination_dir` should be copied over. - Any files present in both directories but with different contents should overwrite the files in `destination_dir`. - Files present in `destination_dir` but absent in `source_dir` should be deleted. - All these operations should be recursively applied to all subdirectories. **Function signature:** ```python def synchronize_directories(source_dir: str, destination_dir: str) -> None: ``` **Input:** - `source_dir` (str): The path of the source directory. - `destination_dir` (str): The path of the destination directory. **Output:** - None. The synchronization should be performed in place. **Constraints:** - Both input paths are valid and the directories exist. - You can assume you have permission to read/write the directories and files. **Performance Requirements:** - The solution should efficiently handle large directories with many files and subdirectories. **Example:** Assuming the following directory structures: `source_dir` structure: ``` /source_dir |-- file1.txt |-- file2.txt |-- subdir1 |-- file3.txt ``` `destination_dir` structure: ``` /destination_dir |-- file1.txt |-- file2.txt (content different from source_dir/file2.txt) |-- file4.txt |-- subdir1 |-- file5.txt ``` Calling `synchronize_directories(\\"/source_dir\\", \\"/destination_dir\\")` should result in: `destination_dir` after synchronization: ``` /destination_dir |-- file1.txt |-- file2.txt (same content as source_dir/file2.txt) |-- subdir1 |-- file3.txt ``` `file4.txt` and `subdir1/file5.txt` should be deleted, and `file2.txt` should be overwritten with the one from `source_dir`. You are allowed to use Python\'s `shutil` module for file copying and deletion. **Hint:** Start by utilizing the `filecmp.dircmp` class to identify the differences between the directories, then perform the necessary file operations.","solution":"import os import shutil import filecmp def synchronize_directories(source_dir: str, destination_dir: str) -> None: Synchronizes the contents of the destination_dir with the source_dir. def sync_helper(dcmp): # Copy files from source to dest that are in source but not in dest for name in dcmp.left_only: source_path = os.path.join(dcmp.left, name) dest_path = os.path.join(dcmp.right, name) if os.path.isdir(source_path): shutil.copytree(source_path, dest_path) else: shutil.copy2(source_path, dest_path) # Overwrite files in dest with source if they differ for name in dcmp.diff_files: source_path = os.path.join(dcmp.left, name) dest_path = os.path.join(dcmp.right, name) shutil.copy2(source_path, dest_path) # Recursively synchronize subdirectories for name in dcmp.subdirs: sync_helper(dcmp.subdirs[name]) # Delete files and directories in dest that are not in source for name in dcmp.right_only: dest_path = os.path.join(dcmp.right, name) if os.path.isdir(dest_path): shutil.rmtree(dest_path) else: os.remove(dest_path) dcmp = filecmp.dircmp(source_dir, destination_dir) sync_helper(dcmp)"},{"question":"Question: Command-Line Argument Parsing with Argparse You are tasked with implementing a command-line application that manages a simple book collection. The application should support adding books, listing all books, and searching for books by title or author. You\'ll use the `argparse` module to handle command-line arguments. # Requirements 1. **Positional Argument: `command`** - The command to execute. It can be either `add`, `list`, or `search`. - `add`: Add a new book to the collection. - `list`: List all books in the collection. - `search`: Search for books by title or author. 2. **Optional Arguments for `add` Command**: - `--title`: The title of the book (required). - `--author`: The author of the book (required). 3. **Optional Arguments for `search` Command**: - `--title`: Search books by title (optional). - `--author`: Search books by author (optional). 4. **Mutually Exclusive Group for Verbose Output**: - `-v`, `--verbose`: Enable verbose output. - `-q`, `--quiet`: Enable quiet mode. 5. **Help and Usage Messages**: - Provide meaningful help messages for all arguments and options. - Ensure the program displays a helpful message if incorrect or insufficient arguments are provided. # Input and Output Constraints - The book collection should be stored in memory as a global list of dictionaries. - The `add` command must check that both `--title` and `--author` are provided; if not, display an error message. - The `search` command should display results that match either the title or the author if specified. - Verbose mode should provide additional context for operations being performed. - Quiet mode should minimize output by showing only essential information. # Example Usage ```sh python books.py add --title \\"1984\\" --author \\"George Orwell\\" Book titled \\"1984\\" by George Orwell added to the collection. python books.py list Books in the collection: 1. Title: 1984, Author: George Orwell python books.py search --title \\"1984\\" -v Searching for books with title \\"1984\\"... Found 1 book(s): 1. Title: 1984, Author: George Orwell python books.py search --author \\"Orwell\\" -q 1984 ``` # Implementation Details 1. **Initialize ArgumentParser**: Form the argument parser with the necessary commands, options, and mutually exclusive groups. 2. **Handle Positional Argument `command`**: Use subparsers to define behavior for `add`, `list`, and `search`. 3. **Define Options for Subparsers**: Add the required and optional arguments for each subcommand. 4. **Implement Command Handlers**: Write functions or logic blocks to perform the actions based on parsed arguments. 5. **Error Handling**: Ensure meaningful error messages are displayed when arguments are missing or incorrect. 6. **Verbose and Quiet Modes**: Implement the verbose and quiet modes for detailed or minimal output based on user preference. *Use the documentation provided to create a robust and user-friendly command-line application using argparse.*","solution":"import argparse # Initialize the list to store books books = [] def add_book(args): Add a book with the given title and author if not args.title or not args.author: print(\\"Error: Both --title and --author are required to add a book.\\") return book = {\\"title\\": args.title, \\"author\\": args.author} books.append(book) if args.verbose: print(f\'Book titled \\"{args.title}\\" by {args.author} added to the collection.\') def list_books(args): List all books in the collection if books: print(\\"Books in the collection:\\") for i, book in enumerate(books, start=1): print(f\'{i}. Title: {book[\\"title\\"]}, Author: {book[\\"author\\"]}\') else: print(\\"No books in the collection.\\") def search_books(args): Search for books by title or author results = [] if args.title: results = [book for book in books if args.title.lower() in book[\\"title\\"].lower()] elif args.author: results = [book for book in books if args.author.lower() in book[\\"author\\"].lower()] if args.verbose: if args.title: print(f\'Searching for books with title \\"{args.title}\\"...\') if args.author: print(f\'Searching for books by author \\"{args.author}\\"...\') if results: print(f\'Found {len(results)} book(s):\') for i, book in enumerate(results, start=1): print(f\'{i}. Title: {book[\\"title\\"]}, Author: {book[\\"author\\"]}\') else: print(\\"No books found.\\") elif args.quiet: for book in results: print(book[\\"title\\"]) else: for book in results: print(f\'Title: {book[\\"title\\"]}, Author: {book[\\"author\\"]}\') def main(): parser = argparse.ArgumentParser(description=\\"Manage a simple book collection.\\") subparsers = parser.add_subparsers(dest=\\"command\\", help=\\"sub-command help\\") # Add command add_parser = subparsers.add_parser(\'add\', help=\'Add a new book to the collection\') add_parser.add_argument(\'--title\', required=True, help=\'The title of the book\') add_parser.add_argument(\'--author\', required=True, help=\'The author of the book\') add_parser.set_defaults(func=add_book) # List command list_parser = subparsers.add_parser(\'list\', help=\'List all books in the collection\') list_parser.set_defaults(func=list_books) # Search command search_parser = subparsers.add_parser(\'search\', help=\'Search for books by title or author\') search_parser.add_argument(\'--title\', help=\'Search books by title\') search_parser.add_argument(\'--author\', help=\'Search books by author\') search_parser.set_defaults(func=search_books) # Verbose and Quiet Mutually Exclusive Group group = parser.add_mutually_exclusive_group() group.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\'Enable verbose output\') group.add_argument(\'-q\', \'--quiet\', action=\'store_true\', help=\'Enable quiet mode\') args = parser.parse_args() if args.command is None: parser.print_help() else: args.func(args) if __name__ == \\"__main__\\": main()"},{"question":"Question: Data Analysis and Transformation using Pandas You are working as a data analyst for a retail company. You are provided with a dataset that contains transaction records. Each record includes information about the transaction ID, date, customer ID, product category, quantity, and total amount spent. Your task is to analyze and transform this data to extract meaningful insights. # Input: The input will be a DataFrame `df` with the following columns: - `TransactionID`: Unique identifier for each transaction (integer). - `Date`: Date of the transaction (datetime). - `CustomerID`: Unique identifier for each customer (integer). - `Category`: Category of the product purchased (string). - `Quantity`: Quantity of the product purchased (integer). - `TotalAmount`: Total amount spent in the transaction (float). # Output: You need to implement a function `analyze_transactions(df)` that performs the following tasks: 1. **Filtering**: Filter out transactions where the `Quantity` is less than 1 or the `TotalAmount` is less than or equal to 0. 2. **New Column**: Create a new column `YearMonth` that extracts the year and month from the `Date` column in the format \'YYYY-MM\'. 3. **Total Spent per Customer**: Group the data by `CustomerID` and calculate the total amount spent by each customer. Return this as a DataFrame with columns `CustomerID` and `TotalSpent`. 4. **Monthly Sales Summary**: Create a pivot table that shows the total quantity of products sold for each `Category` for each `YearMonth`. 5. **Top Categories**: For each `YearMonth`, find the product category with the highest total quantity sold. # Constraints: - You can assume that the input DataFrame will have valid data types for each column. - The `Date` column will be in datetime format. - Performance should be optimized for dataframes with up to 100,000 rows. # Function Signature: ```python import pandas as pd def analyze_transactions(df: pd.DataFrame) -> tuple: pass ``` # Example: ```python data = { \'TransactionID\': [1, 2, 3, 4, 5], \'Date\': pd.to_datetime([\'2023-01-15\', \'2023-01-17\', \'2023-02-01\', \'2023-02-15\', \'2023-02-20\']), \'CustomerID\': [101, 102, 101, 103, 101], \'Category\': [\'A\', \'B\', \'A\', \'B\', \'C\'], \'Quantity\': [2, 1, 3, 2, 1], \'TotalAmount\': [200.0, 100.0, 300.0, 150.0, 50.0] } df = pd.DataFrame(data) result = analyze_transactions(df) # Expected result: # (DataFrame with columns [\'CustomerID\', \'TotalSpent\'], # Pivot table showing total quantity for each category and year-month, # Series showing top categories for each year-month) ``` # Notes: - Please be sure to implement all steps in the solution. - The output should be a tuple containing the DataFrame for total spent per customer, the pivot table, and the series showing the top categories.","solution":"import pandas as pd def analyze_transactions(df: pd.DataFrame) -> tuple: # Step 1: Filtering df_filtered = df[(df[\'Quantity\'] > 0) & (df[\'TotalAmount\'] > 0)] # Step 2: New Column - YearMonth df_filtered[\'YearMonth\'] = df_filtered[\'Date\'].dt.to_period(\'M\').astype(str) # Step 3: Total Spent per Customer total_spent_per_customer = df_filtered.groupby(\'CustomerID\')[\'TotalAmount\'].sum().reset_index() total_spent_per_customer.rename(columns={\'TotalAmount\': \'TotalSpent\'}, inplace=True) # Step 4: Monthly Sales Summary monthly_sales_summary = pd.pivot_table(df_filtered, values=\'Quantity\', index=\'YearMonth\', columns=\'Category\', aggfunc=\'sum\', fill_value=0) # Step 5: Top Categories total_quantity_per_month_category = df_filtered.groupby([\'YearMonth\', \'Category\'])[\'Quantity\'].sum().reset_index() top_categories = total_quantity_per_month_category.loc[total_quantity_per_month_category.groupby(\'YearMonth\')[\'Quantity\'].idxmax()] top_categories_series = top_categories.set_index(\'YearMonth\')[\'Category\'] return total_spent_per_customer, monthly_sales_summary, top_categories_series"},{"question":"Implementing a Simple Linear Regression Using PyTorch --- **Objective**: In this task, you will implement a simple linear regression model using PyTorch. The implementation should cover the creation of tensors, performing basic operations on them, calculating gradients, and updating the model parameters. **Description**: You need to implement a class `SimpleLinearRegression` with the following methods: 1. `__init__(self)`: Initialize the model parameters `weights` and `bias` as tensors with `requires_grad=True`. 2. `forward(self, x)`: Perform the forward pass to compute predictions using the model parameters. 3. `_loss(self, y_pred, y_true)`: Compute the Mean Squared Error (MSE) loss between predictions and actual values. 4. `train(self, x_train, y_train, learning_rate, epochs)`: Train the model using the provided training data by performing gradient descent to update the model parameters. **Constraints and Requirements**: - The model parameters (`weights` and `bias`) should be initialized as tensors of appropriate shapes with `requires_grad=True`. - Use tensor operations for the forward pass and loss calculation. - Perform backpropagation using the `.backward()` method on the loss tensor. - Update model parameters using gradient descent in the `train` method. - Ensure that weight updates are done in-place to take advantage of PyTorch\'s gradient tracking. **Input and Output**: - The `__init__` method does not take any arguments besides `self`. - The `forward` method takes a tensor `x` of shape (N, 1) and returns predictions as a tensor of shape (N, 1). - The `_loss` method takes two tensors, `y_pred` and `y_true`, both of shape (N, 1), and returns a scalar loss value. - The `train` method takes: - `x_train`: A tensor of shape (N, 1) representing the input features. - `y_train`: A tensor of shape (N, 1) representing the target values. - `learning_rate`: A float specifying the learning rate for the gradient descent updates. - `epochs`: An integer specifying the number of training iterations. **Example**: ```python import torch class SimpleLinearRegression: def __init__(self): # Initialize weights and bias self.weights = torch.randn(1, requires_grad=True) self.bias = torch.randn(1, requires_grad=True) def forward(self, x): # Define the forward pass return x * self.weights + self.bias def _loss(self, y_pred, y_true): # Calculate Mean Squared Error loss return ((y_pred - y_true) ** 2).mean() def train(self, x_train, y_train, learning_rate, epochs): for epoch in range(epochs): y_pred = self.forward(x_train) loss = self._loss(y_pred, y_train) loss.backward() with torch.no_grad(): self.weights -= learning_rate * self.weights.grad self.bias -= learning_rate * self.bias.grad # Zero gradients after updating self.weights.grad.zero_() self.bias.grad.zero_() # Example usage x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32) y_train = torch.tensor([[2.0], [3.0], [4.0], [5.0]], dtype=torch.float32) model = SimpleLinearRegression() model.train(x_train, y_train, learning_rate=0.01, epochs=1000) # Test the model print(model.forward(torch.tensor([[5.0]]))) ``` **Note**: The provided code gives an example implementation. Students need to complete and refine it to ensure it meets all the requirements, and they need to understand the process and purpose of each method thoroughly. --- **Good luck!**","solution":"import torch class SimpleLinearRegression: def __init__(self): # Initialize weights and bias self.weights = torch.randn(1, requires_grad=True) self.bias = torch.randn(1, requires_grad=True) def forward(self, x): # Define the forward pass return x * self.weights + self.bias def _loss(self, y_pred, y_true): # Calculate Mean Squared Error loss return ((y_pred - y_true) ** 2).mean() def train(self, x_train, y_train, learning_rate, epochs): for epoch in range(epochs): y_pred = self.forward(x_train) loss = self._loss(y_pred, y_train) loss.backward() with torch.no_grad(): self.weights -= learning_rate * self.weights.grad self.bias -= learning_rate * self.bias.grad # Zero gradients after updating self.weights.grad.zero_() self.bias.grad.zero_() # Example usage x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32) y_train = torch.tensor([[2.0], [3.0], [4.0], [5.0]], dtype=torch.float32) model = SimpleLinearRegression() model.train(x_train, y_train, learning_rate=0.01, epochs=1000) # Test the model print(model.forward(torch.tensor([[5.0]])))"},{"question":"**Question:** You are required to implement an HTTP client using the `http.client` module that interacts with a sample server. Your client should be able to perform the following tasks: 1. Send a GET request to a specified URL and print the status code and reason phrase. 2. Send a POST request to a specified URL with form data and print the response status and body. 3. Handle various HTTP exceptions and print appropriate error messages. 4. Optionally, implement a method to send a JSON payload with the POST request and handle chunked transfer encoding when uploading large files. # Function Signature ```python import http.client import urllib.parse def perform_get_request(host: str, url: str): Sends a GET request to the specified URL and prints the status code and reason phrase. :param host: The server host name with optional port. :param url: The URL path to request. pass def perform_post_request(host: str, url: str, data: dict): Sends a POST request with form data to the specified URL and prints the response status and body. :param host: The server host name with optional port. :param url: The URL path to request. :param data: A dictionary of form data to send in the POST request. pass def handle_http_exceptions(): Demonstrates handling various HTTP exceptions and prints appropriate error messages. This function should attempt to send invalid requests to demonstrate the exception handling. pass def perform_post_request_with_json(host: str, url: str, json_data: dict): Optionally sends a POST request with a JSON payload to the specified URL. Prints the response status and body. :param host: The server host name with optional port. :param url: The URL path to request. :param json_data: A dictionary to send as a JSON payload in the POST request. pass def upload_large_file(host: str, url: str, file_path: str): Optionally sends a large file using chunked transfer encoding to the specified URL and handles the response. :param host: The server host name with optional port. :param url: The URL path to request. :param file_path: The path to the file to upload. pass ``` # Constraints - You are required to manage connections, send HTTP requests, and read responses using the `http.client` module only. - Provide appropriate error handling for network-related issues and HTTP errors using the provided exceptions in `http.client`. - Ensure that the functions can handle both HTTP and HTTPS servers. - For `upload_large_file`, use chunked transfer encoding to handle upload of files larger than 1MB. # Example Usage ```python # Example usage of the functions perform_get_request(\\"www.python.org\\", \\"/\\") perform_post_request(\\"bugs.python.org\\", \\"/\\", {\\"test\\": \\"data\\"}) handle_http_exceptions() perform_post_request_with_json(\\"www.example.com\\", \\"/api/resource\\", {\\"key\\": \\"value\\"}) upload_large_file(\\"localhost:8080\\", \\"/upload\\", \\"path/to/large/file.txt\\") ``` # Note - Document and comment your code where necessary to explain the logic. - Make sure to handle different types of responses and demonstrate the usage of the required methods from `http.client`.","solution":"import http.client import urllib.parse import json def perform_get_request(host: str, url: str): Sends a GET request to the specified URL and prints the status code and reason phrase. try: connection = http.client.HTTPConnection(host) connection.request(\\"GET\\", url) response = connection.getresponse() print(f\\"Status: {response.status}, Reason: {response.reason}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: connection.close() def perform_post_request(host: str, url: str, data: dict): Sends a POST request with form data to the specified URL and prints the response status and body. try: connection = http.client.HTTPConnection(host) headers = {\'Content-type\': \'application/x-www-form-urlencoded\'} params = urllib.parse.urlencode(data) connection.request(\\"POST\\", url, params, headers) response = connection.getresponse() body = response.read().decode() print(f\\"Status: {response.status}, Body: {body}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: connection.close() def handle_http_exceptions(): Demonstrates handling various HTTP exceptions and prints appropriate error messages. try: connection = http.client.HTTPConnection(\\"invalid.host\\") connection.request(\\"GET\\", \\"/\\") except http.client.HTTPException as e: print(f\\"HTTPException occurred: {e}\\") except Exception as e: print(f\\"General exception occurred: {e}\\") finally: connection.close() def perform_post_request_with_json(host: str, url: str, json_data: dict): Optionally sends a POST request with a JSON payload to the specified URL. Prints the response status and body. try: connection = http.client.HTTPConnection(host) headers = {\'Content-type\': \'application/json\'} json_payload = json.dumps(json_data) connection.request(\\"POST\\", url, json_payload, headers) response = connection.getresponse() body = response.read().decode() print(f\\"Status: {response.status}, Body: {body}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: connection.close() def upload_large_file(host: str, url: str, file_path: str): Optionally sends a large file using chunked transfer encoding to the specified URL and handles the response. try: connection = http.client.HTTPConnection(host) file_size = os.path.getsize(file_path) headers = {\'Transfer-Encoding\': \'chunked\', \'Content-Type\': \'application/octet-stream\'} with open(file_path, \'rb\') as file: connection.putrequest(\'POST\', url) for header, value in headers.items(): connection.putheader(header, value) connection.endheaders() chunk_size = 1024 * 1024 # 1MB while True: chunk = file.read(chunk_size) if not chunk: break connection.send(hex(len(chunk))[2:].encode() + b\'rn\' + chunk + b\'rn\') connection.send(b\'0rnrn\') response = connection.getresponse() body = response.read().decode() print(f\\"Status: {response.status}, Body: {body}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: connection.close()"},{"question":"Suppose you are tasked with creating a lightweight cache system for transient web application data using the `dbm` module. This cache system should be able to store, retrieve, and delete key-value pairs where both keys and values are strings. # Requirements: 1. **Function to Store Key-Value Pair:** - Implement a function `store_in_cache(db_path: str, key: str, value: str) -> None` that stores a given key-value pair in the specified database file. - If the database file does not exist, create it. 2. **Function to Retrieve Value by Key:** - Implement a function `retrieve_from_cache(db_path: str, key: str) -> str` that retrieves the value associated with the given key from the database. - If the key does not exist, return `\'not found\'`. 3. **Function to Delete a Key:** - Implement a function `delete_from_cache(db_path: str, key: str) -> bool` that deletes the given key from the database. - Return `True` if the key was successfully deleted, otherwise `False`. 4. **Function to List All Keys:** - Implement a function `list_cache_keys(db_path: str) -> list` that returns a list of all keys currently stored in the database. # Constraints: - You must use the `with` statement to ensure proper resource management. - Handle any exceptions that may arise while interacting with the database and provide meaningful error messages. - Assume the keys and values are always valid UTF-8 strings. # Example Usage: ```python # Store data store_in_cache(\'mycache.db\', \'username\', \'john_doe\') store_in_cache(\'mycache.db\', \'session_id\', \'abc123\') # Retrieve data print(retrieve_from_cache(\'mycache.db\', \'username\')) # Should print \'john_doe\' print(retrieve_from_cache(\'mycache.db\', \'non_existent_key\')) # Should print \'not found\' # List keys print(list_cache_keys(\'mycache.db\')) # Should print [\'username\', \'session_id\'] # Delete key print(delete_from_cache(\'mycache.db\', \'username\')) # Should print True print(delete_from_cache(\'mycache.db\', \'username\')) # Should print False ``` Additional Information: - Use the `dbm.open` method for opening the database. - Ensure keys and values are converted to bytes for storage. You are expected to implement these four functions in a manner that efficiently uses the `dbm` module\'s capabilities while adhering to the constraints provided.","solution":"import dbm def store_in_cache(db_path: str, key: str, value: str) -> None: Stores a given key-value pair in the specified db file. Parameters: db_path : str : Path to the database file. key : str : Key to store in the cache. value : str : Value to store in the cache. with dbm.open(db_path, \'c\') as db: db[key] = value def retrieve_from_cache(db_path: str, key: str) -> str: Retrieves the value associated with the given key from the db. Parameters: db_path : str : Path to the database file. key : str : Key to retrieve from the cache. Returns: str : Value associated with the key, or \'not found\' if key doesn\'t exist. with dbm.open(db_path, \'c\') as db: if key in db: return db[key].decode(\'utf-8\') else: return \'not found\' def delete_from_cache(db_path: str, key: str) -> bool: Deletes the given key from the db. Parameters: db_path : str : Path to the database file. key : str : Key to delete from the cache. Returns: bool : True if the key was successfully deleted, False otherwise. with dbm.open(db_path, \'c\') as db: if key in db: del db[key] return True else: return False def list_cache_keys(db_path: str) -> list: Returns a list of all keys currently stored in the db. Parameters: db_path : str : Path to the database file. Returns: list : List of all keys in the database. with dbm.open(db_path, \'c\') as db: return [key.decode(\'utf-8\') for key in db.keys()]"},{"question":"Objective: Create a Python module that wraps some of the process control and system functionalities available in the python310 package to demonstrate the understanding of these functions and their utility. Problem Statement: Design and implement a Python class `Python310Utilities` that provides the following functionalities: 1. **Checking Interactive Mode**: - Method `is_interactive` which takes a file path and returns `True` if the file is interactive, otherwise `False`. 2. **Handle Signal**: - Method `set_and_get_signal_handler` which sets a signal handler for a specific signal and returns the previous handler. 3. **Path Conversion**: - Method `get_file_system_path` which converts a given path-like object to its file system representation. 4. **Exit Process**: - Method `exit_process` which takes an exit status code and terminates the process with that status. Constraints: 1. The class should have appropriate error handling to manage unexpected inputs and raise meaningful exceptions. 2. The implementation of the `get_file_system_path` method should strictly ensure the returned value is either a `str` or `bytes`, conforming to the `os.PathLike` interface. 3. The methods should be demonstrated with appropriate test cases. Input and Output Format: - `is_interactive(fp: str) -> bool` - `set_and_get_signal_handler(signal_number: int, handler: Callable) -> Callable` - `get_file_system_path(path: Union[str, os.PathLike]) -> Union[str, bytes]` - `exit_process(status_code: int) -> None` Example Usage: ```python import signal # Example handler def example_signal_handler(signum, frame): print(f\\"Handling signal: {signum}\\") # Create an instance of the utility class utils = Python310Utilities() # Check if a file is interactive print(utils.is_interactive(\\"/dev/tty\\")) # Set a signal handler and get the previous handler old_handler = utils.set_and_get_signal_handler(signal.SIGINT, example_signal_handler) # Convert a path-like object print(utils.get_file_system_path(\\"/path/to/file\\")) # Exit the process utils.exit_process(0) ``` Notes: - The implementation should follow best practices, including docstrings for methods, type annotations, and proper packaging. - Ensure that no operations result in a crash or uncaught exceptions, especially in the `exit_process` method. # Good Luck!","solution":"import os import signal import sys class Python310Utilities: def is_interactive(self, fp: str) -> bool: Check if the file at the given path is interactive. :param fp: Path to the file. :return: True if the file is interactive, otherwise False. try: with open(fp) as f: return os.isatty(f.fileno()) except Exception as e: raise ValueError(f\\"Failed to check interactiveness of {fp}: {e}\\") def set_and_get_signal_handler(self, signal_number: int, handler: callable) -> callable: Set a signal handler for a specific signal and return the previous handler. :param signal_number: Signal number to handle. :param handler: Callable to handle the signal. :return: Previous signal handler. try: previous_handler = signal.getsignal(signal_number) signal.signal(signal_number, handler) return previous_handler except Exception as e: raise ValueError(f\\"Failed to set signal handler for signal {signal_number}: {e}\\") def get_file_system_path(self, path) -> str: Convert a given path-like object to its file system representation. :param path: Path-like object. :return: File system representation of the path. try: if isinstance(path, (str, bytes)): return path elif hasattr(path, \'__fspath__\'): return path.__fspath__() else: raise ValueError(f\\"Invalid path type: {type(path)}\\") except Exception as e: raise ValueError(f\\"Failed to convert path {path}: {e}\\") def exit_process(self, status_code: int) -> None: Exit the process with the given status code. :param status_code: Exit status code. try: sys.exit(status_code) except Exception as e: raise ValueError(f\\"Failed to exit process with status {status_code}: {e}\\")"},{"question":"# Question: Implementing and Analyzing Kernel Density Estimation (KDE) **Objective:** In this task, you are required to implement a Kernel Density Estimation (KDE) using different kernels from the scikit-learn library to visualize the density estimation on a bimodal dataset. You will analyze and compare the results of using different kernels and bandwidth values. # Problem Statement 1. **Data Preparation:** - Create a bimodal dataset consisting of 200 data points. The data points should be drawn from two normal distributions with different means and the same standard deviation. - Example: 100 points from N(-2, 0.5) and 100 points from N(3, 0.5). 2. **Kernel Density Estimation:** - Use the `KernelDensity` class from the `sklearn.neighbors` module to compute the KDE for the dataset using at least three different kernels (`gaussian`, `tophat`, `epanechnikov`). - Experiment with at least two different bandwidth values for each kernel (e.g., 0.5 and 1.0). 3. **Visualization:** - Plot the original data as a histogram. - Plot the KDE results for each kernel and each bandwidth value on the same figure for comparison. 4. **Analysis:** - Write a brief analysis comparing the density estimations obtained from different kernels and bandwidth values. Discuss how the choice of kernel and bandwidth affects the smoothness and representation of the underlying dataset. # Function Signature ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity import seaborn as sns def generate_bimodal_dataset(): Generates a bimodal dataset with: - 100 points from N(-2, 0.5) - 100 points from N(3, 0.5) Returns: np.ndarray: A (200, 1) numpy array containing the dataset. pass def plot_histogram(data): Plots a histogram of the given dataset. Args: data (np.ndarray): The data to plot. pass def perform_kde(data, kernel, bandwidth): Performs Kernel Density Estimation on the given dataset. Args: data (np.ndarray): The data to estimate density. kernel (str): The kernel to use for estimation. bandwidth (float): The bandwidth parameter. Returns: np.ndarray, np.ndarray: The grid points and density estimates for the data. pass def visualize_kde(data, kde_results): Visualizes the KDE results on a plot. Args: data (np.ndarray): The original data. kde_results (dict): A dictionary where keys are (kernel, bandwidth) tuples and values are density estimates. pass def compare_kernels(): Main function to compare different kernels and bandwidths for KDE. It generates the dataset, performs KDE, visualizes the results, and provides an analysis. pass ``` # Requirements: - Generate 200 data points forming a bimodal distribution. - Use at least three different kernels and two bandwidth values for KDE. - Plot and clearly label the KDE results for comparison. - Provide an analysis of how different kernels and bandwidths impact the density estimation. # Expected Output: - A figure with original histogram and KDE plots for different kernels and bandwidths. - A brief analysis as mentioned above, ideally as comments or a print statement in the `compare_kernels` function. # Constraints: - Use `sklearn.neighbors.KernelDensity` for KDE. - Follow best practices for code readability and modularity.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def generate_bimodal_dataset(): Generates a bimodal dataset with: - 100 points from N(-2, 0.5) - 100 points from N(3, 0.5) Returns: np.ndarray: A (200, 1) numpy array containing the dataset. data_1 = np.random.normal(-2, 0.5, 100) data_2 = np.random.normal(3, 0.5, 100) return np.concatenate([data_1, data_2]).reshape(-1, 1) def plot_histogram(data): Plots a histogram of the given dataset. Args: data (np.ndarray): The data to plot. plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', edgecolor=\'black\') def perform_kde(data, kernel, bandwidth): Performs Kernel Density Estimation on the given dataset. Args: data (np.ndarray): The data to estimate density. kernel (str): The kernel to use for estimation. bandwidth (float): The bandwidth parameter. Returns: np.ndarray, np.ndarray: The grid points and density estimates for the data. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) grid = np.linspace(-5, 6, 1000).reshape(-1, 1) log_dens = kde.score_samples(grid) return grid, np.exp(log_dens) def visualize_kde(data, kde_results): Visualizes the KDE results on a plot. Args: data (np.ndarray): The original data. kde_results (dict): A dictionary where keys are (kernel, bandwidth) tuples and values are density estimates. plot_histogram(data) for (kernel, bandwidth), (grid, dens) in kde_results.items(): plt.plot(grid, dens, label=f\\"{kernel} (bw={bandwidth})\\") plt.legend() plt.xlabel(\'Value\') plt.ylabel(\'Density\') plt.title(\'Kernel Density Estimation\') def compare_kernels(): Main function to compare different kernels and bandwidths for KDE. It generates the dataset, performs KDE, visualizes the results, and provides an analysis. data = generate_bimodal_dataset() kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.5, 1.0] kde_results = {} for kernel in kernels: for bandwidth in bandwidths: grid, dens = perform_kde(data, kernel, bandwidth) kde_results[(kernel, bandwidth)] = (grid, dens) visualize_kde(data, kde_results) plt.show() # Analysis print( Analysis: - Gaussian kernel generally provides the smoothest density estimation, producing smooth bell-curves. - Tophat kernel tends to produce more flat and less smooth density bumps, particularly noticeable at lower bandwidth. - Epanechnikov kernel offers a middle ground between the smooth Gaussian and the flat Tophat, giving relatively higher peaks. - Increasing bandwidth makes the estimated density curves smoother by integrating densities over a wider range. However, if the bandwidth is too large, the estimate can become overly smoothed, losing important details such as distinct peaks. ) # Run the main comparison function to visualize and analyze KDE with different kernels and bandwidths if __name__ == \'__main__\': compare_kernels()"},{"question":"# Question: Implement an Asynchronous Task Queue with `asyncio` You are required to implement an asynchronous task queue using the `asyncio` library. Your task queue should be able to: 1. Add tasks (coroutines) to the queue. 2. Process tasks concurrently with a specified maximum number of concurrent tasks. 3. Allow querying the status of the queue (e.g., how many tasks are pending, running, and completed). Specifications 1. **Class Definition**: - Define a class `AsyncTaskQueue`. - The constructor should accept a parameter `max_concurrent_tasks` which sets the maximum number of tasks that can be processed concurrently. 2. **Methods**: - `add_task`: This method accepts a coroutine and adds it to the queue. - `run`: This method starts processing the tasks in the queue, adhering to the `max_concurrent_tasks` limit. - `status`: This method returns the current status of the queue, including the number of pending, running, and completed tasks. 3. **Input/Output**: - `add_task(coroutine)`: Adds a coroutine task to the queue. No return value. - `run()`: Starts processing the tasks. This method should block until all tasks are processed. No return value. - `status() -> Dict[str, int]`: Returns a dictionary with the counts of \'pending\', \'running\', and \'completed\' tasks. 4. **Constraints**: - You must use the `asyncio` library to implement the task queue. - Ensure that the tasks respect the concurrency limit specified by `max_concurrent_tasks`. 5. **Example Usage**: ```python import asyncio async def sample_task(task_id, duration): print(f\'Task {task_id} started\') await asyncio.sleep(duration) print(f\'Task {task_id} completed\') # Example queue = AsyncTaskQueue(max_concurrent_tasks=2) # Adding tasks queue.add_task(sample_task(1, 2)) queue.add_task(sample_task(2, 3)) queue.add_task(sample_task(3, 1)) # Running tasks await queue.run() # Getting status print(queue.status()) # Should output: {\'pending\': 0, \'running\': 0, \'completed\': 3} ``` Solution Outline 1. **Constructor**: - Initialize the queue and set up the maximum concurrent tasks. - Use `asyncio.Queue` to manage task coroutines. 2. **Adding Tasks**: - Implement `add_task` to add tasks to the `asyncio.Queue`. 3. **Running Tasks**: - Use an asynchronous method to pull tasks from the queue and run them while respecting the concurrency limit. 4. **Queue Status**: - Maintain counters for pending, running, and completed tasks. - Implement `status` to return these counters in a dictionary. Your solution should include appropriate error handling and edge cases, such as trying to run an empty queue or adding the same task multiple times.","solution":"import asyncio from typing import Dict class AsyncTaskQueue: def __init__(self, max_concurrent_tasks: int): self.max_concurrent_tasks = max_concurrent_tasks self.pending_queue = asyncio.Queue() self.running_tasks = set() self.completed_tasks = 0 def add_task(self, coroutine): self.pending_queue.put_nowait(coroutine) async def worker(self): while True: task = await self.pending_queue.get() if task is None: break # Track the running task self.running_tasks.add(task) try: await task finally: self.running_tasks.remove(task) self.completed_tasks += 1 self.pending_queue.task_done() async def run(self): # Create worker tasks to process the queue concurrently workers = [asyncio.create_task(self.worker()) for _ in range(self.max_concurrent_tasks)] # Wait until all items are processed await self.pending_queue.join() # Cancel the workers for worker in workers: self.pending_queue.put_nowait(None) await asyncio.gather(*workers) def status(self) -> Dict[str, int]: return { \'pending\': self.pending_queue.qsize(), \'running\': len(self.running_tasks), \'completed\': self.completed_tasks }"},{"question":"Objective Demonstrate proficiency in using the `pkgutil` module to extend the search path for a package and retrieve information about modules within a package. Problem Statement You are given a Python package installed in your environment with the following structure: ``` example_pkg/ __init__.py module1.py subpkg/ __init__.py module2.py ``` The goal is to extend the module search path dynamically and retrieve information about all modules within the package, including submodules in subdirectories. Implement a function `extend_and_list_modules` that satisfies the following requirements: 1. **Parameters**: - `package_path` (str): Path to the root directory of the package. - `package_name` (str): The name of the package (e.g., \\"example_pkg\\"). 2. **Return**: - A list of strings, where each string is the full module name (e.g., \\"example_pkg.module1\\", \\"example_pkg.subpkg.module2\\"). 3. **Constraints**: - Use the functions available in the `pkgutil` module to achieve the desired functionality. - Do not perform file system operations manually (e.g., using `os` or `glob`). - Assume that the package is correctly structured and does not contain any errors. 4. **Example**: ```python package_path = \\"/path/to/example_pkg\\" package_name = \\"example_pkg\\" result = extend_and_list_modules(package_path, package_name) print(result) # Output should be [\\"example_pkg.module1\\", \\"example_pkg.subpkg.module2\\"] ``` Function Signature ```python def extend_and_list_modules(package_path: str, package_name: str) -> list: pass ``` Notes - Ensure your implementation uses `pkgutil.extend_path` to handle the package path extension. - Utilize `pkgutil.walk_packages` to list modules within the package. - Your solution should be robust and handle edge cases such as empty packages gracefully.","solution":"import pkgutil import sys def extend_and_list_modules(package_path: str, package_name: str) -> list: Extends the module search path and lists all modules in the package. Parameters: - package_path (str): Path to the root directory of the package. - package_name (str): The name of the package (e.g., \\"example_pkg\\"). Returns: - A list of strings, where each string is the full module name (e.g., \\"example_pkg.module1\\", \\"example_pkg.subpkg.module2\\"). sys.path.append(package_path) modules = [] for importer, modname, ispkg in pkgutil.walk_packages(path=[package_path + \'/\' + package_name], prefix=package_name+\'.\'): modules.append(modname) return modules"},{"question":"# PyTorch Function Transformation Assessment Objective: Demonstrate your understanding of PyTorch\'s `torch.func` module by implementing and transforming a function using function transforms while adhering to the constraints provided. Problem Statement: You are required to implement a function that computes the element-wise squared error between two tensors and then seamlessly applies both automatic differentiation and batching transforms using PyTorch\'s `torch.func` utilities. Function Requirements: 1. **Function Implementation:** - Implement a function `squared_error(tensor1, tensor2)` that computes the squared difference between corresponding elements of `tensor1` and `tensor2`. - Ensure this function is pure and adheres to the constraints around mutation and control flow as specified in the documentation. 2. **Compute Gradient:** - Using `torch.func.grad`, create functions to compute the gradient of `squared_error` with respect to the first input tensor. - The gradient function `gradient_squared_error(tensor1, tensor2)` should return the gradient of `squared_error` with respect to `tensor1`. 3. **Batching Function:** - Using `torch.func.vmap`, apply `squared_error` across a batch of inputs. - Ensure that the input tensors are processed correctly, regardless of their batch dimensions. Constraints: - Assume `tensor1` and `tensor2` are 1-dimensional tensors of equal length. - Do not use any global variables or mutate the input tensors. - Ensure that your solution handles the constraints and limitations mentioned in the provided documentation regarding vmap and grad transforms. Input Format: - `tensor1`, `tensor2`: 1-dimensional PyTorch Tensors of equal length. Output Format: - `squared_error(tensor1, tensor2)`: 1-dimensional PyTorch Tensor representing squared differences. - `gradient_squared_error(tensor1, tensor2)`: 1-dimensional PyTorch Tensor representing gradients. - `batched_squared_error(tensors1, tensors2)`: 2-dimensional PyTorch Tensor representing batched squared differences. Example: ```python import torch from torch.func import grad, vmap # Example tensors tensor1 = torch.tensor([1.0, 2.0, 3.0]) tensor2 = torch.tensor([4.0, 5.0, 6.0]) batch_tensor1 = torch.tensor([[1.0, 2.0, 3.0], [7.0, 8.0, 9.0]]) batch_tensor2 = torch.tensor([[4.0, 5.0, 6.0], [1.0, 1.0, 1.0]]) # Expected outputs squared_error(tensor1, tensor2) # [9.0, 9.0, 9.0] gradient_squared_error(tensor1, tensor2) # [-6.0, -6.0, -6.0] batched_squared_error(batch_tensor1, batch_tensor2) # [[9.0, 9.0, 9.0], [36.0, 49.0, 64.0]] ``` Solution Template: ```python import torch from torch.func import grad, vmap def squared_error(tensor1, tensor2): # Your implementation here pass def gradient_squared_error(tensor1, tensor2): # Your implementation here pass def batched_squared_error(tensors1, tensors2): # Your implementation here pass ```","solution":"import torch from torch.func import grad, vmap def squared_error(tensor1, tensor2): Computes the element-wise squared error between two tensors. return (tensor1 - tensor2) ** 2 def gradient_squared_error(tensor1, tensor2): Computes the gradient of the squared_error with respect to the first input tensor. return grad(lambda x: squared_error(x, tensor2).sum())(tensor1) def batched_squared_error(tensors1, tensors2): Applies squared_error over batched inputs using vmap. return vmap(squared_error)(tensors1, tensors2)"},{"question":"# Resource Management and Analysis in Python Using Python\'s `resource` module, you are required to manage and report the resource usage of the current process. Specifically, you\'ll need to implement two functions: 1. `manage_limits(resource_type, new_soft_limit, new_hard_limit)`: This function will set new limits on the specified system resource. 2. `report_resource_usage()`: This function will report detailed resource usage statistics of the current process. # Function 1: `manage_limits(resource_type, new_soft_limit, new_hard_limit)` - **Input**: - `resource_type`: A string representation of the resource (e.g., \'RLIMIT_CPU\') - `new_soft_limit`: An integer representing the new soft limit - `new_hard_limit`: An integer representing the new hard limit - **Output**: It should return a tuple of the form `(old_soft_limit, old_hard_limit)` indicating the previous limits of the specified resource. - **Constraints**: - The new soft limit cannot be greater than the new hard limit. - Only super-users can raise the hard limit. - Use the `setrlimit` and `getrlimit` functions appropriately. # Function 2: `report_resource_usage()` - **Input**: None - **Output**: A dictionary with keys representing the resource types (e.g., \'ru_utime\', \'ru_stime\') and their corresponding usage values. - **Constraints**: - Use the `getrusage` function with the `RUSAGE_SELF` parameter. - Ensure the output dictionary replicates the fields as specified in the `getrusage` return value. # Example Usage ```python # Setting new limits old_limits = manage_limits(\'RLIMIT_CPU\', 10, 20) print(f\\"Old limits for RLIMIT_CPU: {old_limits}\\") # Reporting resource usage resource_usage = report_resource_usage() print(\\"Resource Usage:\\") for key, value in resource_usage.items(): print(f\\"{key}: {value}\\") ``` # Additional Information - Refer to the `resource` module documentation when setting and getting resource limits. - Ensure proper error handling for invalid inputs and unexpected system call failures. # Performance Requirements - The functions should be designed to handle reasonable constraints without significant performance overhead.","solution":"import resource def manage_limits(resource_type, new_soft_limit, new_hard_limit): Sets new limits on the specified system resource and returns old limits. Parameters: - resource_type (str): String representation of the resource (e.g., \'RLIMIT_CPU\') - new_soft_limit (int): New soft limit for the resource. - new_hard_limit (int): New hard limit for the resource. Returns: - tuple: (old_soft_limit, old_hard_limit) indicating the old limits for the resource. resource_type = getattr(resource, resource_type) old_limits = resource.getrlimit(resource_type) resource.setrlimit(resource_type, (new_soft_limit, new_hard_limit)) return old_limits def report_resource_usage(): Reports detailed resource usage statistics of the current process. Returns: - dict: Dictionary with keys representing the resource types and their respective usage values. usage = resource.getrusage(resource.RUSAGE_SELF) usage_dict = { \'ru_utime\': usage.ru_utime, # User time used \'ru_stime\': usage.ru_stime, # System time used \'ru_maxrss\': usage.ru_maxrss, # Maximum resident set size \'ru_ixrss\': usage.ru_ixrss, # Shared memory size \'ru_idrss\': usage.ru_idrss, # Unshared memory size \'ru_isrss\': usage.ru_isrss, # Unshared stack size \'ru_minflt\': usage.ru_minflt, # Page reclaims \'ru_majflt\': usage.ru_majflt, # Page faults \'ru_nswap\': usage.ru_nswap, # Swaps \'ru_inblock\': usage.ru_inblock,# Block input operations \'ru_oublock\': usage.ru_oublock,# Block output operations \'ru_msgsnd\': usage.ru_msgsnd, # Messages sent \'ru_msgrcv\': usage.ru_msgrcv, # Messages received \'ru_nsignals\': usage.ru_nsignals,# Signals received \'ru_nvcsw\': usage.ru_nvcsw, # Voluntary context switches \'ru_nivcsw\': usage.ru_nivcsw # Involuntary context switches } return usage_dict"},{"question":"# Nested Data Structure Manipulation You are required to create a function that manipulates nested data structures involving lists, dictionaries, and sets. Write a function `nested_structure_manipulation()` that performs the following operations: 1. **Input**: - A nested dictionary where keys are strings and values are lists of dictionaries. - Each dictionary in the lists contains keys that are strings and values that are integers. For example: ```python data = { \'group1\': [{\'a\': 1, \'b\': 2, \'c\': 3}, {\'a\': 4, \'b\': 5, \'c\': 6}], \'group2\': [{\'a\': 7, \'b\': 8}, {\'a\': 9}], \'group3\': [{\'b\': 10, \'c\': 11}] } ``` 2. **Operations**: - **Extract unique keys**: Extract all unique keys present in the nested dictionaries. - **Sum values by key**: Compute the sum of values for each key across all nested dictionaries. - **Filter sets**: Return a set containing only those keys whose total sum of values is greater than a threshold (say 10). 3. **Output**: - A dictionary with keys as the extracted unique keys and values as their summed values. - A set of keys that meet the filter criteria. For example, given the above input, the output should be: ```python { \'a\': 21, \'b\': 25, \'c\': 20 }, {\'a\', \'b\', \'c\'} ``` # Constraints: - Consider the nested dictionary can be large (max 10^5 elements). - Performance is crucial; aim for an O(n) complexity where n is the total number of inner dictionaries. # Function Signature ```python def nested_structure_manipulation(data: dict, threshold: int = 10) -> (dict, set): ``` # Example ```python data = { \'group1\': [{\'a\': 1, \'b\': 2, \'c\': 3}, {\'a\': 4, \'b\': 5, \'c\': 6}], \'group2\': [{\'a\': 7, \'b\': 8}, {\'a\': 9}], \'group3\': [{\'b\': 10, \'c\': 11}] } result_dict, result_set = nested_structure_manipulation(data) print(result_dict) # Output: {\'a\': 21, \'b\': 25, \'c\': 20} print(result_set) # Output: {\'a\', \'b\', \'c\'} ``` # Notes: - Ensure your function is well-optimized and can handle edge cases like empty dictionaries. - Testing will include a variety of nested structures to ensure robustness and correctness.","solution":"def nested_structure_manipulation(data: dict, threshold: int = 10) -> (dict, set): Extracts unique keys and computes the sum of values for each key across all nested dictionaries, then filters out keys whose total sum of values is greater than a given threshold. Args: data (dict): Nested dictionary input where values are lists of dictionaries. threshold (int): Threshold for filtering keys based on their summed values. Returns: (dict, set): A dictionary with summed values for each unique key and a set of keys that have sums greater than the threshold. summed_values = {} # Iterate over each group in the main dictionary for group in data.values(): for dictionary in group: for key, value in dictionary.items(): if key in summed_values: summed_values[key] += value else: summed_values[key] = value # Extract keys whose value sums exceed the threshold filtered_keys = {key for key, value in summed_values.items() if value > threshold} return summed_values, filtered_keys"},{"question":"# Functional Programming Challenge **Objective:** Write a Python function that processes a list of integers, leveraging the Python functional programming modules `itertools`, `functools`, and `operator`. **Function Specification:** ```python def process_numbers(numbers: List[int]) -> List[int]: pass ``` **Input:** - A list of integers `numbers` where `1 <= len(numbers) <= 1000` and `-1000 <= numbers[i] <= 1000`. **Output:** - A list of integers where each element has been processed based on the following criteria: 1. If the number is positive, square it. 2. If the number is negative, take the absolute value and cube it. 3. If the number is zero, remove it from the list. **Performance Requirements:** - The solution should be efficiently executable within reasonable time limits for the given input size constraints. **Example:** ```python >>> process_numbers([1, -2, 0, 3, -4]) [1, 8, 9, 64] ``` **Constraints:** - You must utilize functions from `itertools`, `functools`, and `operator` to complete this task. - Direct use of `for` or `while` loops (other than for initial creation of iterators) is discouraged. # Hints: 1. Consider using `itertools.filterfalse` to remove all zeros from the list. 2. Use `operator.neg` and other operator functions to transform the elements as needed. 3. Combine these transformations using `functools.partial` to create a higher-order function that can be applied to the elements of the list. # Assessment: Your solution will be evaluated based on its correctness, efficiency, and adherence to functional programming principles using the specified modules.","solution":"from typing import List import itertools import functools import operator def process_numbers(numbers: List[int]) -> List[int]: # Remove zeros from the list numbers = list(itertools.filterfalse(lambda x: x == 0, numbers)) # Process the numbers according to the rules def process(x): if x > 0: return x * x else: return abs(x) ** 3 return list(map(process, numbers))"},{"question":"**Objective:** Demonstrate understanding and application of various types of assignment statements, control flow mechanisms, and functions in Python. **Problem Statement:** Write a Python function `process_items(items: list) -> int` that takes a list of integers called `items` and performs the following operations: 1. Compute the sum of all positive integers in the list and store it in a variable named `sum_positive`. 2. Replace each negative integer in the list with its absolute value, and store the modified list in a variable named `abs_items`. 3. Sort the `abs_items` list in ascending order. 4. Return the sum of the smallest and largest values in the `abs_items` list. **Requirements:** - Utilize at least one augmented assignment statement. - Utilize at least one annotated assignment statement. - Ensure the function handles an empty list input by returning `0`. **Example:** ```python >>> process_items([4, -1, 2, -3, 5]) 9 ``` Explanation: - Positive integers in the list: `[4, 2, 5]`. Their sum (`sum_positive`) is `11`. - Negative integers are converted to their absolute values: `[4, 1, 2, 3, 5]`. - The modified list is sorted: `[1, 2, 3, 4, 5]`. - The sum of the smallest (`1`) and largest (`5`) values is `6`. **Constraints:** - The input list `items` contains integers only. - The function should not print anything; it should only return the result. **Hints:** - Use a list comprehension to modify the list of negative integers. - Use the `sorted` function to sort the list. - Carefully handle the case when the list is empty to avoid index errors. **Function Signature:** ```python def process_items(items: list) -> int: # Your code here ```","solution":"def process_items(items: list) -> int: Takes a list of integers called items and performs operations according to the specified rules: 1. Compute the sum of all positive integers. 2. Replace each negative integer with its absolute value. 3. Sort the modified list in ascending order. 4. Return the sum of the smallest and largest values in the modified list. If the list is empty, return 0. # Step 1: Compute the sum of all positive integers sum_positive: int = 0 for item in items: if item > 0: sum_positive += item # Step 2: Replace each negative integer with its absolute value abs_items = [abs(item) for item in items] # Step 3: Sort the abs_items list in ascending order abs_items.sort() # Step 4: Return the sum of the smallest and largest values if len(abs_items) == 0: return 0 else: return abs_items[0] + abs_items[-1]"},{"question":"# Custom Exception and Error Propagation in Python Objective: Develop a Python program that manages custom exceptions, propagates errors correctly, and implements proper error and warning handling. Problem Description: 1. **Custom Exception Class:** Create a custom exception class `MatrixError` that inherits from the built-in `Exception` class. This exception should be used to handle errors related to matrix operations. 2. **Matrix Operations:** Implement a class `Matrix` with the following methods: - `__init__(self, data)`: Initialize the matrix with a 2D list `data`. Raise a `MatrixError` if the provided data is not a proper 2D list or if the matrix dimensions are inconsistent. - `transpose(self)`: Return the transpose of the matrix. Handle any errors gracefully and raise a `MatrixError` if the operation fails. - `add(self, other)`: Add another matrix to this matrix and return the result. If the dimensions do not match, raise a `MatrixError`. - `multiply(self, other)`: Multiply this matrix with another matrix and return the result. If dimensions are incompatible for multiplication, raise a `MatrixError`. 3. **Warnings:** Implement a warning system that generates warnings when certain matrix operations might lead to high computational complexity (e.g., multiplying two large matrices). Constraints: - The matrix operations should handle exceptions and propagate the errors correctly. - Use the predefined `warnings` module to issue warnings. - Ensure proper exception chaining using `raise ... from ...` where necessary. Input: - A list of lists for creating matrix instances. - Another `Matrix` object for the `add` and `multiply` methods. Output: - The resulting matrix from operations or the appropriate error message when exceptions are raised. Example Code Template: ```python import warnings class MatrixError(Exception): pass class Matrix: def __init__(self, data): # Initialize the matrix and validate the data pass def transpose(self): # Transpose the matrix pass def add(self, other): # Add another matrix to this matrix pass def multiply(self, other): # Multiply this matrix with another matrix pass # Example usage: # try: # matrix1 = Matrix([[1, 2], [3, 4]]) # matrix2 = Matrix([[5, 6], [7, 8]]) # result = matrix1.add(matrix2) # print(result) # except MatrixError as e: # print(e) ``` Notes: - Ensure all methods have appropriate error handling. - Use the `warnings.warn` function to issue warnings. - Document your code well for clarity.","solution":"import warnings class MatrixError(Exception): pass class Matrix: def __init__(self, data): if not all(isinstance(row, list) for row in data) or len(set(len(row) for row in data)) != 1: raise MatrixError(\\"Invalid matrix dimensions or data types.\\") self.data = data def transpose(self): try: transposed = [[self.data[j][i] for j in range(len(self.data))] for i in range(len(self.data[0]))] return Matrix(transposed) except Exception as e: raise MatrixError(\\"Failed to transpose matrix.\\") from e def add(self, other): if len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]): raise MatrixError(\\"Matrix dimensions do not match for addition.\\") try: result = [[self.data[i][j] + other.data[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data))] return Matrix(result) except Exception as e: raise MatrixError(\\"Failed to add matrices.\\") from e def multiply(self, other): if len(self.data[0]) != len(other.data): raise MatrixError(\\"Matrix dimensions do not match for multiplication.\\") if len(self.data) * len(other.data[0]) > 1000: warnings.warn(\\"Matrix multiplication may result in high computational complexity.\\") try: result = [[sum(self.data[i][k] * other.data[k][j] for k in range(len(other.data))) for j in range(len(other.data[0]))] for i in range(len(self.data))] return Matrix(result) except Exception as e: raise MatrixError(\\"Failed to multiply matrices.\\") from e"},{"question":"# Question **Title:** Batch URL Data Fetcher with Authentication **Objective:** Your task is to implement a function that can retrieve data from multiple URLs concurrently using the `urllib.request` module. The function should also handle different types of HTTP responses and perform Basic HTTP authentication if credentials are provided. **Function Signature:** ```python def fetch_urls(urls, username=None, password=None): Fetches data from multiple URLs concurrently with optional Basic HTTP authentication. Parameters: urls (list of str): List of URLs to fetch data from. username (str, optional): Username for Basic HTTP authentication. Default is None. password (str, optional): Password for Basic HTTP authentication. Default is None. Returns: dict: A dictionary where each key is a URL and its value is a tuple (status_code, headers, content). content is the first 100 characters of the response body. ``` **Input:** - `urls`: a list of URLs (strings) from which to fetch data. - `username` and `password`: optional strings for Basic HTTP authentication. **Output:** - A dictionary where each key is a URL and its value is a tuple containing: - `status_code`: the HTTP status code of the response (int). - `headers`: the HTTP headers of the response as a dictionary. - `content`: the first 100 characters of the response body (string). **Constraints:** - If `username` and `password` are provided, you must use them for Basic HTTP authentication for all URLs. - The function must handle HTTP errors and include the status code and headers in the output. - The function should work with both HTTP and HTTPS URLs. **Example Usage:** ```python urls = [ \\"http://www.example.com\\", \\"http://www.example.org\\", \\"https://www.example.net\\" ] # Without authentication result = fetch_urls(urls) print(result) # With authentication result = fetch_urls(urls, username=\\"user\\", password=\\"pass\\") print(result) ``` **Hints:** 1. Use the `urllib.request.urlopen` method to open each URL. 2. Use the `urllib.request.Request` class to create and customize requests. 3. Use the `urllib.request.HTTPBasicAuthHandler` class for handling Basic HTTP authentication. 4. Handle HTTP errors by catching exceptions such as `urllib.error.HTTPError`. 5. Remember to return the first 100 characters of the response content.","solution":"import urllib.request import urllib.error import base64 import threading def fetch_url(url, auth_header=None, results=None): try: req = urllib.request.Request(url) if auth_header: req.add_header(\\"Authorization\\", auth_header) with urllib.request.urlopen(req) as response: status_code = response.getcode() headers = dict(response.info()) content = response.read(100).decode(\'utf-8\', errors=\'replace\') results[url] = (status_code, headers, content) except urllib.error.HTTPError as e: results[url] = (e.code, dict(e.headers), str(e.reason)) except Exception as e: results[url] = (-1, {}, str(e)) def fetch_urls(urls, username=None, password=None): if username and password: auth_str = f\\"{username}:{password}\\" base64_auth_str = base64.b64encode(auth_str.encode(\'utf-8\')).decode(\'utf-8\') auth_header = f\\"Basic {base64_auth_str}\\" else: auth_header = None results = {} threads = [] for url in urls: thread = threading.Thread(target=fetch_url, args=(url, auth_header, results)) thread.start() threads.append(thread) for thread in threads: thread.join() return results"},{"question":"# Python Coding Assessment **Objective**: Demonstrate understanding of shell pipelines using both `pipes` and `subprocess`. Problem Description You are given a sequence of text transformations to apply to an input file, and you will implement a function to perform these transformations using the deprecated `pipes` module and the recommended `subprocess` module. Write a function `transform_file(input_file: str, output_file: str, transformations: List[Tuple[str, str]]) -> None` where: - `input_file` (str): The path to the input file. - `output_file` (str): The path to the output file. - `transformations` (List[Tuple[str, str]]): A list of tuples, where each tuple contains: - `command` (str): A shell command to be applied. - `kind` (str): A two-letter string indicating how the command reads from input and writes to output. The function should: 1. Apply the transformations using the deprecated `pipes` module. 2. Apply the same transformations using the `subprocess` module. Constraints - Only POSIX-compliant shells are supported (Unix-based systems). - Ensure that the transformations applied by both methods yield the same result. - Handle any potential errors that may arise during pipeline creation or execution. Example For an input file `input.txt` containing: ``` hello world ``` And transformations: ``` [(\\"tr a-z A-Z\\", \\"--\\"), (\\"sed s/HELLO/HI/\\", \\"--\\")] ``` The expected output in `output_file` should be: ``` HI WORLD ``` Note - You may assume the input file always exists and is readable. - The output file should be created or overwritten if it already exists. - Do not use external libraries to complete this task. ```python from typing import List, Tuple def transform_file(input_file: str, output_file: str, transformations: List[Tuple[str, str]]) -> None: # Implementation using deprecated pipes module import pipes template = pipes.Template() for command, kind in transformations: template.append(command, kind) with template.open(output_file, \'w\') as f: f.write(open(input_file).read()) # Verify the output with subprocess module import subprocess current_input = input_file for command, kind in transformations: if kind == \\"--\\": result = subprocess.run(command, shell=True, input=open(current_input).read(), text=True, capture_output=True) current_input = result.stdout elif kind == \\"-f\\": result = subprocess.run(f\'{command} {current_input}\', shell=True, text=True, capture_output=True) current_input = result.stdout # Add handling for other `kind` options as necessary with open(output_file, \'w\') as f: f.write(current_input) ``` Ensure your implementation meets the requirements and passes the given example.","solution":"from typing import List, Tuple def transform_file(input_file: str, output_file: str, transformations: List[Tuple[str, str]]) -> None: # Implementation using deprecated pipes module import pipes template = pipes.Template() for command, kind in transformations: template.append(command, kind) with template.open(output_file, \'w\') as f: f.write(open(input_file).read()) # Verify the output with subprocess module import subprocess current_input = input_file for i, (command, kind) in enumerate(transformations): if kind == \\"--\\": result = subprocess.run(command, shell=True, input=open(current_input).read(), text=True, capture_output=True) current_input = f\\"temp_output_{i}.txt\\" with open(current_input, \'w\') as temp_file: temp_file.write(result.stdout) elif kind == \\"-f\\": result = subprocess.run(f\'{command} {current_input}\', shell=True, text=True, capture_output=True) current_input = f\\"temp_output_{i}.txt\\" with open(current_input, \'w\') as temp_file: temp_file.write(result.stdout) # Add handling for other `kind` options as necessary with open(current_input, \'r\') as final_output: final_data = final_output.read() with open(output_file, \'w\') as f: f.write(final_data)"},{"question":"**Question: Implement a Time Zone Aware Schedule Processor** You are designing a program to manage and process event schedules that must account for time zones and daylight saving time transitions. Your task is to create a function `process_schedule(events, timezone_key)` that takes a list of event dictionaries and a time zone key, adjusts the event times to the specified time zone, and returns a new list of events with updated times. # Function Signature ```python def process_schedule(events: list, timezone_key: str) -> list: pass ``` # Input - `events`: A list of dictionaries where each dictionary represents an event with the following structure: - `id` (str): Unique identifier of the event. - `title` (str): Title of the event. - `start_time` (str): ISO 8601 formatted start time in UTC (e.g., \\"2023-05-15T14:00:00Z\\"). - `end_time` (str): ISO 8601 formatted end time in UTC (e.g., \\"2023-05-15T15:00:00Z\\"). - `timezone_key`: A string representing the IANA time zone key (e.g., \\"America/Los_Angeles\\"). # Output - A list of event dictionaries with the same structure, but with `start_time` and `end_time` adjusted to the specified time zone. The times should be in ISO 8601 format with time zone offset (e.g., \\"2023-05-15T07:00:00-07:00\\"). # Constraints - The `timezone_key` provided must be valid; otherwise, the function should raise a `zoneinfo.ZoneInfoNotFoundError`. - Handle daylight saving transitions properly (i.e., adjust times correctly if they fall into such periods). - Ensure that the function efficiently processes schedules, which may contain up to 10,000 events. # Example ```python events = [ { \\"id\\": \\"1\\", \\"title\\": \\"Board Meeting\\", \\"start_time\\": \\"2023-05-15T14:00:00Z\\", \\"end_time\\": \\"2023-05-15T15:00:00Z\\" }, { \\"id\\": \\"2\\", \\"title\\": \\"Lunch with Bob\\", \\"start_time\\": \\"2023-05-15T18:00:00Z\\", \\"end_time\\": \\"2023-05-15T19:00:00Z\\" } ] timezone_key = \\"America/Los_Angeles\\" processed_events = process_schedule(events, timezone_key) ``` Expected Output: ```python [ { \\"id\\": \\"1\\", \\"title\\": \\"Board Meeting\\", \\"start_time\\": \\"2023-05-15T07:00:00-07:00\\", \\"end_time\\": \\"2023-05-15T08:00:00-07:00\\" }, { \\"id\\": \\"2\\", \\"title\\": \\"Lunch with Bob\\", \\"start_time\\": \\"2023-05-15T11:00:00-07:00\\", \\"end_time\\": \\"2023-05-15T12:00:00-07:00\\" } ] ``` # Notes - Utilize the `zoneinfo.ZoneInfo` class to handle time zone conversions. - Pay special attention to the `fold` attribute for handling ambiguous times due to daylight saving time transitions.","solution":"import datetime import zoneinfo def process_schedule(events, timezone_key): Adjusts event times from UTC to the specified time zone. Args: events (list): List of event dictionaries. timezone_key (str): Time zone IANA key. Returns: list: List of event dictionaries with adjusted times. tz = zoneinfo.ZoneInfo(timezone_key) adjusted_events = [] for event in events: adjusted_event = event.copy() # Convert start and end times start_time_utc = datetime.datetime.fromisoformat(event[\'start_time\'][:-1]).replace(tzinfo=datetime.timezone.utc) end_time_utc = datetime.datetime.fromisoformat(event[\'end_time\'][:-1]).replace(tzinfo=datetime.timezone.utc) start_time_local = start_time_utc.astimezone(tz).isoformat() end_time_local = end_time_utc.astimezone(tz).isoformat() adjusted_event[\'start_time\'] = start_time_local adjusted_event[\'end_time\'] = end_time_local adjusted_events.append(adjusted_event) return adjusted_events"},{"question":"**Objective**: Demonstrate competency in using pandas\' nullable integer data type, including creation, manipulation, and common operations. **Question**: You are given a dataset representing student IDs and their respective scores in two subjects. Some entries in the dataset contain missing values. Your task is to perform various operations on this dataset while retaining the integer type for the student IDs and handling missing values appropriately. **Dataset**: ```plaintext Students Data: | Student ID | Subject1 Score | Subject2 Score | |------------|----------------|----------------| | 1 | 85 | 78 | | 2 | 90 | None | | NaN | 88 | 64 | | 4 | None | None | | 5 | 80 | 70 | | 6 | None | 68 | ``` **Tasks**: 1. **Load the Data**: Load the dataset into a pandas DataFrame. Ensure the `Student ID` column uses a nullable integer type that can handle missing values. 2. **Handle Missing Values**: Replace missing scores in `Subject1 Score` and `Subject2 Score` with the mean of their respective columns. Ensure that the `Student ID` retains its nullable integer type. 3. **Add a New Column**: Compute the average score for each student and add it as a new column `Average Score`. Handle missing values appropriately while computing the average. 4. **Basic Dataframe Operations**: - Print the DataFrame with the new column. - Print the dtypes of the columns to verify the types. **Constraints**: - Do not change the dtype of the `Student ID` column to anything other than the nullable integer type. - Missing values should be handled using pandas\' `pandas.NA`. **Input and Output Formats**: - **Input**: The dataset as described above. - **Output**: A printed DataFrame with columns `Student ID`, `Subject1 Score`, `Subject2 Score`, `Average Score`, and their respective dtypes. **Example**: ```python import pandas as pd import numpy as np # Input Data data = { \\"Student ID\\": [1, 2, pd.NA, 4, 5, 6], \\"Subject1 Score\\": [85, 90, 88, pd.NA, 80, pd.NA], \\"Subject2 Score\\": [78, pd.NA, 64, pd.NA, 70, 68] } # Create DataFrame df = pd.DataFrame(data, dtype=\\"Int64\\") # 1. Replace missing values in Subject1 and Subject2 scores with the mean score of their respective columns df[\\"Subject1 Score\\"].fillna(df[\\"Subject1 Score\\"].mean(skipna=True), inplace=True) df[\\"Subject2 Score\\"].fillna(df[\\"Subject2 Score\\"].mean(skipna=True), inplace=True) # 2. Calculate the Average Score df[\\"Average Score\\"] = (df[\\"Subject1 Score\\"] + df[\\"Subject2 Score\\"]) / 2 # 3. Print the DataFrame and its dtypes print(df) print(df.dtypes) ``` Expected Output: ```plaintext Student ID Subject1 Score Subject2 Score Average Score 0 1 85.0 78.0 81.5 1 2 90.0 70.0 80.0 2 <NA> 88.0 64.0 76.0 3 4 85.75 70.0 77.875 4 5 80.0 70.0 75.0 5 6 85.75 68.0 76.875 Student ID Int64 Subject1 Score float64 Subject2 Score float64 Average Score float64 dtype: object ```","solution":"import pandas as pd import numpy as np def process_student_data(data): # Create DataFrame with nullable integer type for \'Student ID\' df = pd.DataFrame(data) df[\'Student ID\'] = df[\'Student ID\'].astype(\'Int64\') # Calculate means excluding NaNs using skipna=True subject1_mean = df[\'Subject1 Score\'].mean(skipna=True) subject2_mean = df[\'Subject2 Score\'].mean(skipna=True) # Fill NaN values with the means of the respective columns df[\'Subject1 Score\'] = df[\'Subject1 Score\'].fillna(subject1_mean) df[\'Subject2 Score\'] = df[\'Subject2 Score\'].fillna(subject2_mean) # Calculate Average Score df[\'Average Score\'] = (df[\'Subject1 Score\'] + df[\'Subject2 Score\']) / 2 # Print DataFrame and column types print(df) print(df.dtypes) return df # Define dataset data = { \\"Student ID\\": [1, 2, pd.NA, 4, 5, 6], \\"Subject1 Score\\": [85, 90, 88, pd.NA, 80, pd.NA], \\"Subject2 Score\\": [78, pd.NA, 64, pd.NA, 70, 68] }"},{"question":"Using the `__annotations__` attribute in Python, write a function called `annotations_summary` that takes an object and returns a dictionary summarizing the annotations of the object. The summary should handle different cases based on the type of object and the Python version. Your function should: 1. Use `inspect.get_annotations()` if Python version is 3.10 or newer. 2. For Python versions 3.9 or older, implement the logic necessary to correctly retrieve annotations as outlined in the provided documentation. 3. Un-stringize any stringized annotations where appropriate. 4. Return a dictionary containing: - The type of the object (e.g., function, class, module) - The original annotations dict - The evaluated (un-stringized) annotations dict, if different from the original. # Expected Input and Output: - **Input:** An object `obj` to inspect (could be a function, class, or module) - **Output:** A dictionary with the keys `\'type\'`, `\'original_annotations\'`, and `\'evaluated_annotations\'`, where the values are: - `\'type\'`: A string representing the type of object (\'function\', \'class\', \'module\') - `\'original_annotations\'`: The original annotations dictionary of the object - `\'evaluated_annotations\'` : The annotations dictionary with stringized annotations evaluated, if they were present and evaluable, otherwise the same as the original # Constraints: - You may assume the Python versions in use will be 3.0 or newer. - Handle cases where annotations may not exist or be improperly formatted. - Ensure the function does not mutate the object\'s annotations. # Example: ```python from typing import TYPE_CHECKING def example_function(x: \'int\', y: \'str\') -> \'bool\': return bool(x + y) class ExampleClass: a: \'int\' b: str if TYPE_CHECKING: zing: \'str\' # Running your function should return the following output for the provided examples: annotations_summary(example_function) # Output (example output; actual evaluation of annotations might differ): { \'type\': \'function\', \'original_annotations\': {\'x\': \'int\', \'y\': \'str\', \'return\': \'bool\'}, \'evaluated_annotations\': {\'x\': int, \'y\': str, \'return\': bool} } annotations_summary(ExampleClass) # Output (example output): { \'type\': \'class\', \'original_annotations\': {\'a\': \'int\', \'b\': str}, \'evaluated_annotations\': {\'a\': int, \'b\': str} } ``` Implement the `annotations_summary` function in such a manner that it works correctly for all specified scenarios.","solution":"import inspect import sys from typing import get_type_hints def annotations_summary(obj): Return a dictionary summarizing the annotations of the given object. The summary contains the type of the object, the original annotations, and the evaluated (un-stringized) annotations. obj_type = type(obj).__name__ if sys.version_info >= (3, 10): original_annotations = inspect.get_annotations(obj) else: original_annotations = getattr(obj, \\"__annotations__\\", {}) evaluated_annotations = get_type_hints(obj) # Check if evaluated annotations differ from original annotations diff_annotations = evaluated_annotations if evaluated_annotations != original_annotations else original_annotations return { \'type\': obj_type, \'original_annotations\': original_annotations, \'evaluated_annotations\': diff_annotations }"},{"question":"You have been tasked with securing the transmission of sensitive binary data using Base64 encoding. In this exercise, you will implement two functions: one that encodes a given binary file to Base64 and another that decodes the Base64-encoded file back to binary. # Function Definitions 1. `encode_file_to_base64(input_file_path: str, output_file_path: str) -> None` - **Input**: - `input_file_path` (str): The path to the binary file that needs to be encoded. - `output_file_path` (str): The path where the resultant Base64 encoded file will be saved. - **Output**: None - **Description**: This function reads the binary data from the `input_file_path`, encodes it using Base64, and writes the encoded data to `output_file_path`. 2. `decode_file_from_base64(input_file_path: str, output_file_path: str) -> None` - **Input**: - `input_file_path` (str): The path to the Base64 encoded file that needs to be decoded. - `output_file_path` (str): The path where the resultant decoded binary file will be saved. - **Output**: None - **Description**: This function reads the Base64 encoded data from the `input_file_path`, decodes it back to binary form, and writes the decoded data to `output_file_path`. # Constraints - Assume the input files are not excessively large (under 50MB). - Handle any exceptions that may arise during file operations (e.g., file not found, read/write errors). - Ensure the output files are correctly closed after writing. # Example ```python # Assume \'example.bin\' is a binary file that exists in the working directory encode_file_to_base64(\'example.bin\', \'encoded_file.txt\') # This results in \'encoded_file.txt\' containing the Base64 encoded data decode_file_from_base64(\'encoded_file.txt\', \'decoded_example.bin\') # This results in \'decoded_example.bin\' containing the same binary data as \'example.bin\' ``` # Your Task Implement the `encode_file_to_base64` and `decode_file_from_base64` functions to complete this exercise. # Note This exercise is designed to test your understanding of the Base64 encoding/decoding functions in the `base64` module and your ability to handle file I/O operations in Python.","solution":"import base64 def encode_file_to_base64(input_file_path: str, output_file_path: str) -> None: try: with open(input_file_path, \'rb\') as binary_file: binary_data = binary_file.read() base64_encoded_data = base64.b64encode(binary_data) with open(output_file_path, \'wb\') as encoded_file: encoded_file.write(base64_encoded_data) except Exception as e: print(f\\"An error occurred while encoding: {e}\\") def decode_file_from_base64(input_file_path: str, output_file_path: str) -> None: try: with open(input_file_path, \'rb\') as encoded_file: base64_encoded_data = encoded_file.read() binary_data = base64.b64decode(base64_encoded_data) with open(output_file_path, \'wb\') as binary_file: binary_file.write(binary_data) except Exception as e: print(f\\"An error occurred while decoding: {e}\\")"},{"question":"**Question: Using `cgitb` for Enhanced Exception Handling and Logging** In this coding challenge, you will be tasked with creating an application that utilizes the `cgitb` module to provide enhanced exception handling and logging. You will need to design a script that performs the following tasks: 1. **Function Implementation**: - Implement a function `divide_numbers(a, b)` that takes two numbers and returns their division result. Ensure the function handles exceptions appropriately. 2. **Activate `cgitb` for Exception Handling**: - Enable `cgitb` to handle any uncaught exceptions in the script. Configure `cgitb` to display the detailed traceback information both in HTML and plain text formats as required. 3. **Logging and Contextual Output**: - Configure `cgitb` to log tracebacks to a file named \\"error_log.txt\\" in the current directory. - Customize the output to show 7 lines of context around the error line in the traceback. 4. **Custom Exception Handling**: - Within the `divide_numbers` function, use `cgitb.handler` to manually handle any caught exceptions and display a detailed traceback in HTML format. **Input and Output**: - The input for the script will be two numbers `a` and `b`, provided to the `divide_numbers` function. - The output should be either the division result or a detailed error report if an exception occurs. - Ensure that any exceptions are logged to \\"error_log.txt\\" and displayed accordingly. **Constraints**: - Do not use any additional libraries other than `cgitb` and standard Python libraries. - Ensure the logging directory exists, and permissions are correct for writing files. **Performance**: - The code should handle exceptions efficiently and provide immediate feedback via the configured `cgitb` settings. Example usage: ```python # Assume the following code is in your main script after function implementations cgitb.enable(display=1, logdir=\'.\', context=7, format=\'html\') try: result = divide_numbers(10, 0) print(\\"Result:\\", result) except: pass # Ensure to check the \'error_log.txt\' file for logged errors. ``` **Note**: You must ensure detailed comments and documentation within your code to explain the use of `cgitb` and how it enhances default Python exception handling.","solution":"import cgitb # Enable cgitb to handle uncaught exceptions cgitb.enable(display=1, logdir=\'.\', context=7, format=\'text\') def divide_numbers(a, b): Divides two numbers and returns the result. If b is zero, handles the ZeroDivisionError exception. Parameters: a (float): Dividend b (float): Divisor Returns: float: Result of division try: result = a / b except ZeroDivisionError as e: # Manually managing caught exceptions using cgitb cgitb.handler() raise e return result if __name__ == \\"__main__\\": try: result = divide_numbers(10, 0) print(\\"Result:\\", result) except: pass"},{"question":"Challenge: Custom Time Series Analyzer # Question You are building a custom time series analyzer that helps users manage and analyze their time series data with different time zones, custom frequencies, and perform aggregations. # Instructions Implement a function `custom_time_series_analyzer` that performs the following: 1. **Generate a Time Series:** - Create a time series starting from a given start date and time. - The series should span for a specified period (`n_periods`) with a given frequency (`freq`). - Ensure the time series generated is time zone-aware with the specified time zone (`tz`). 2. **Resample the Time Series:** - Resample the generated time series to a different frequency (`resample_freq`). - Perform an aggregation (like mean, sum, etc.) on the resampled data. 3. **Convert Time Zones:** - Convert the resampled time series into another specified time zone (`target_tz`). # Function Signature ```python def custom_time_series_analyzer(start_date: str, n_periods: int, freq: str, tz: str, resample_freq: str, agg_func: str, target_tz: str) -> pd.Series: Parameters: start_date (str): The start date and time for the time series (format: \'YYYY-MM-DD HH:MM:SS\'). n_periods (int): Number of periods to generate in the time series. freq (str): Frequency string for the time series (e.g., \'D\' for daily, \'H\' for hourly). tz (str): Timezone for the initial time series (e.g., \'UTC\', \'US/Eastern\'). resample_freq (str): The frequency to resample the time series to (e.g., \'M\' for monthly, \'W\' for weekly). agg_func (str): Aggregation function to apply on the resampled data (e.g., \'mean\', \'sum\'). target_tz (str): Target timezone to convert the resampled time series to (e.g., \'Europe/London\'). Returns: pd.Series: The resampled and timezone-converted time series after aggregation. ``` # Example ```python import pandas as pd def custom_time_series_analyzer(start_date, n_periods, freq, tz, resample_freq, agg_func, target_tz): # Step 1: Generate time series index = pd.date_range(start=start_date, periods=n_periods, freq=freq, tz=tz) ts = pd.Series(range(n_periods), index=index) # Step 2: Resample time series resampled_ts = ts.resample(resample_freq).apply(agg_func) # Step 3: Convert time zones final_ts = resampled_ts.tz_convert(target_tz) return final_ts # Running the example: result = custom_time_series_analyzer( start_date=\\"2023-01-01 00:00:00\\", n_periods=100, freq=\'D\', tz=\'UTC\', resample_freq=\'M\', agg_func=\'sum\', target_tz=\'US/Eastern\' ) print(result) ``` # Constraints - The input date format will always be valid and follows `YYYY-MM-DD HH:MM:SS`. - The frequencies (`freq`, `resample_freq`) provided will always be valid pandas frequency strings. - The function should handle timezones using both `pytz` and `dateutil`. # Hints - Utilize `pd.date_range` to generate time series data. - Perform resampling using the `resample` method followed by the aggregation function. - Convert timezones utilizing the `tz_convert` method.","solution":"import pandas as pd def custom_time_series_analyzer(start_date: str, n_periods: int, freq: str, tz: str, resample_freq: str, agg_func: str, target_tz: str) -> pd.Series: # Step 1: Generate time series index = pd.date_range(start=start_date, periods=n_periods, freq=freq, tz=tz) ts = pd.Series(range(n_periods), index=index) # Step 2: Resample time series resampled_ts = ts.resample(resample_freq).apply(agg_func) # Step 3: Convert time zones final_ts = resampled_ts.tz_convert(target_tz) return final_ts"},{"question":"**Objective**: Create a custom command-line interpreter for managing a simple to-do list application using Python’s `cmd` module. **Problem Statement**: You need to design a command-line interface (CLI) for managing a simple to-do list. Users should be able to add tasks, mark tasks as complete, delete tasks, and list all tasks. **Requirements**: 1. Use Python’s `cmd` module to create the CLI. 2. Implement methods corresponding to the following commands: - `add <task_description>`: Add a new task with the given description. - `done <task_id>`: Mark a task as completed using its ID. - `delete <task_id>`: Delete a task using its ID. - `list`: List all tasks with their IDs, descriptions, and completion status. - `quit`: Exit the application. 3. Your program should handle edge cases such as invalid task IDs and provide appropriate feedback to the user. **Input and Output Formats**: - `add <task_description>`: - Input: `add Finish homework` - Output: `Task added: \\"Finish homework\\"` - `done <task_id>`: - Input: `done 1` - Output: `Task 1 marked as complete.` - `delete <task_id>`: - Input: `delete 2` - Output: `Task 2 deleted.` - `list`: - Input: `list` - Output: ``` ID Description Status 1 Finish homework Pending 2 Read book Completed ``` - `quit`: - Input: `quit` - Output: `Goodbye!` **Constraints**: - Assume task IDs are integers starting from 1. - Each task’s description is a string of 1 to 100 characters. - Implement error handling for non-existent task IDs. **Performance**: - The solution should handle up to (10^3) tasks efficiently. ```python import cmd class TodoCLI(cmd.Cmd): prompt = \'(todo) \' intro = \'Welcome to the Todo CLI. Type help or ? to list commands.n\' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_add(self, description): \\"Add a new task with a given description.\\" if description: self.tasks.append({\'id\': self.next_id, \'description\': description, \'completed\': False}) print(f\'Task added: \\"{description}\\"\') self.next_id += 1 else: print(\'Error: Task description cannot be empty.\') def do_done(self, task_id): \\"Mark a task as completed using its ID.\\" try: task_id = int(task_id) task = next((task for task in self.tasks if task[\'id\'] == task_id), None) if task: task[\'completed\'] = True print(f\'Task {task_id} marked as complete.\') else: print(f\'Error: Task with ID {task_id} not found.\') except ValueError: print(\'Error: Task ID must be a valid integer.\') def do_delete(self, task_id): \\"Delete a task using its ID.\\" try: task_id = int(task_id) task = next((task for task in self.tasks if task[\'id\'] == task_id), None) if task: self.tasks.remove(task) print(f\'Task {task_id} deleted.\') else: print(f\'Error: Task with ID {task_id} not found.\') except ValueError: print(\'Error: Task ID must be a valid integer.\') def do_list(self, arg): \\"List all tasks with their IDs, descriptions, and completion status.\\" if self.tasks: print(f\'{\\"ID\\":<4} {\\"Description\\":<20} {\\"Status\\"}\') for task in self.tasks: status = \'Completed\' if task[\'completed\'] else \'Pending\' print(f\'{task[\\"id\\"]:<4} {task[\\"description\\"]:<20} {status}\') else: print(\'No tasks found.\') def do_quit(self, arg): \\"Exit the application.\\" print(\'Goodbye!\') return True def emptyline(self): pass if __name__ == \'__main__\': TodoCLI().cmdloop() ``` **Explanation**: - The `TodoCLI` class inherits from `cmd.Cmd` and provides implementations for the required commands. - Tasks are stored in a list of dictionaries, each with an ID, description, and completion status. - Methods `do_add`, `do_done`, `do_delete`, and `do_list` implement the corresponding commands. - The `emptyline` method is overridden to prevent repeating the last command when an empty line is entered. **Note**: Ensure you thoroughly test your implementations for different edge cases as well as typical usage scenarios.","solution":"import cmd class TodoCLI(cmd.Cmd): prompt = \'(todo) \' intro = \'Welcome to the Todo CLI. Type help or ? to list commands.n\' def __init__(self): super().__init__() self.tasks = [] self.next_id = 1 def do_add(self, description): \\"Add a new task with a given description.\\" if description: self.tasks.append({\'id\': self.next_id, \'description\': description, \'completed\': False}) print(f\'Task added: \\"{description}\\"\') self.next_id += 1 else: print(\'Error: Task description cannot be empty.\') def do_done(self, task_id): \\"Mark a task as completed using its ID.\\" try: task_id = int(task_id) task = next((task for task in self.tasks if task[\'id\'] == task_id), None) if task: task[\'completed\'] = True print(f\'Task {task_id} marked as complete.\') else: print(f\'Error: Task with ID {task_id} not found.\') except ValueError: print(\'Error: Task ID must be a valid integer.\') def do_delete(self, task_id): \\"Delete a task using its ID.\\" try: task_id = int(task_id) task = next((task for task in self.tasks if task[\'id\'] == task_id), None) if task: self.tasks.remove(task) print(f\'Task {task_id} deleted.\') else: print(f\'Error: Task with ID {task_id} not found.\') except ValueError: print(\'Error: Task ID must be a valid integer.\') def do_list(self, arg): \\"List all tasks with their IDs, descriptions, and completion status.\\" if self.tasks: print(f\'{\\"ID\\":<4} {\\"Description\\":<20} {\\"Status\\"}\') for task in self.tasks: status = \'Completed\' if task[\'completed\'] else \'Pending\' print(f\'{task[\\"id\\"]:<4} {task[\\"description\\"]:<20} {status}\') else: print(\'No tasks found.\') def do_quit(self, arg): \\"Exit the application.\\" print(\'Goodbye!\') return True def emptyline(self): pass if __name__ == \'__main__\': TodoCLI().cmdloop()"},{"question":"You are given a dataset that evaluates different models on various natural language processing tasks. Your task is to write a function that loads the dataset, manipulates it, and visualizes it using seaborn with custom annotations. # Dataset Description The dataset contains the following columns: - `Model`: The name of the model. - `Encoder`: The encoder type used by the model. - `Task`: The name of the task on which the model is evaluated. - `Score`: The model\'s performance score on that task. # Instructions 1. Load the dataset using seaborn\'s `load_dataset` function. 2. Pivot the dataset such that the index is [`Model`, `Encoder`] and columns are different tasks, with values being the scores. 3. Calculate an average score for each model across all tasks, adding it as a new column `Average`. 4. Sort the resulting DataFrame by the `Average` score in descending order. 5. Create two plots using `seaborn.objects`: - A scatter plot of the scores for `SST-2` vs. `MRPC`, with model names annotated. - A bar plot showing the average scores for each model, with model names and scores annotated. 6. Ensure that the annotations do not overlap and are clearly readable. 7. Customize the appearance of the annotations as per your preference. # Function Signature ```python def visualize_model_performance(): import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset glue = load_dataset(\\"glue\\") # Steps 2-4: Pivot the dataset, calculate the average score, and sort it glue_pivot = (glue .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Step 5a: Create a scatter plot for SST-2 vs. MRPC with model names annotated scatter_plot = ( so.Plot(glue_pivot.reset_index(), x=\\"SST-2\\", y=\\"MRPC\\", text=\\"Model\\") .add(so.Text()) ) scatter_plot.show() # Step 5b: Create a bar plot for average scores with model names and scores annotated bar_plot = ( so.Plot(glue_pivot.reset_index(), x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) bar_plot.show() ``` # Expected Output The function should display two plots: 1. A scatter plot showing the relationship between the `SST-2` and `MRPC` scores for each model, with model names annotated. 2. A bar plot showing the average scores of each model, with model names and scores annotated and clearly readable. # Constraints - Use `seaborn.objects` for plotting. - Ensure annotations are customized to be distinct and non-overlapping. - The function should handle potential missing values in the dataset gracefully.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_model_performance(): # Step 1: Load the dataset glue = sns.load_dataset(\\"glue\\", data_home=\\"~/seaborn-data\\") # Steps 2-4: Pivot the dataset, calculate the average score, and sort it glue_pivot = (glue .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Step 5a: Create a scatter plot for SST-2 vs. MRPC with model names annotated plt.figure(figsize=(10, 6)) scatter_plot = plt.scatter(glue_pivot[\'SST-2\'], glue_pivot[\'MRPC\']) for i, model in enumerate(glue_pivot.index.get_level_values(\'Model\')): plt.text(glue_pivot[\'SST-2\'][i] + 0.02, glue_pivot[\'MRPC\'][i], model, fontsize=9) plt.xlabel(\'SST-2 Score\') plt.ylabel(\'MRPC Score\') plt.title(\'SST-2 vs MRPC Scores by Model\') plt.grid(True) plt.show() # Step 5b: Create a bar plot for average scores with model names and scores annotated plt.figure(figsize=(10, 6)) bar_plot = plt.barh(glue_pivot.index.get_level_values(\'Model\'), glue_pivot[\'Average\'], color=\'skyblue\') for index, value in enumerate(glue_pivot[\'Average\']): plt.text(value, index, str(value), va=\'center\', ha=\'right\' if value < 50 else \'left\', fontsize=9) plt.xlabel(\'Average Score\') plt.ylabel(\'Model\') plt.title(\'Average Scores by Model\') plt.grid(True) plt.show()"},{"question":"# Question Name: PyObject Operations and Comparisons Background: You are required to write a Python C extension module that demonstrates various operations on Python objects using the functions discussed in the provided documentation. You will need to implement functions to manipulate object attributes and perform comparisons. Objective: Implement a Python C extension module named `pyobj_ops` which contains the following functions: 1. **has_attr(obj, attr_name)**: - **Description**: Check if the object `obj` has an attribute `attr_name`. - **Input**: - `obj` (PyObject*): A Python object. - `attr_name` (const char*): The name of the attribute to check. - **Output**: - Return 1 if the attribute exists, 0 otherwise. 2. **get_attr(obj, attr_name)**: - **Description**: Get the value of an attribute `attr_name` from the object `obj`. - **Input**: - `obj` (PyObject*): A Python object. - `attr_name` (const char*): The name of the attribute to retrieve. - **Output**: - Return the attribute value on success or `NULL` on failure. 3. **set_attr(obj, attr_name, value)**: - **Description**: Set an attribute `attr_name` on the object `obj` to a value `value`. - **Input**: - `obj` (PyObject*): A Python object. - `attr_name` (const char*): The name of the attribute to set. - `value` (PyObject*): The value to set the attribute to. - **Output**: - Return 0 on success, -1 on failure. 4. **del_attr(obj, attr_name)**: - **Description**: Delete an attribute `attr_name` from the object `obj`. - **Input**: - `obj` (PyObject*): A Python object. - `attr_name` (const char*): The name of the attribute to delete. - **Output**: - Return 0 on success, -1 on failure. 5. **compare_objects(obj1, obj2, opid)**: - **Description**: Compare two objects `obj1` and `obj2` using the operation specified by `opid`. - **Input**: - `obj1` (PyObject*): The first Python object. - `obj2` (PyObject*): The second Python object. - `opid` (int): The comparison operation to perform, which should be one of `Py_LT`, `Py_LE`, `Py_EQ`, `Py_NE`, `Py_GT`, or `Py_GE`. - **Output**: - Return the result of the comparison (new reference). Constraints: - You must handle all possible errors gracefully, returning appropriate error codes and setting exception information as required. Performance Requirements: - The implementations should be efficient with respect to handling Python objects. Example Usage in Python: ```python import pyobj_ops # Example usage class Sample: def __init__(self, x): self.x = x obj = Sample(10) assert pyobj_ops.has_attr(obj, \'x\') == 1 assert pyobj_ops.get_attr(obj, \'x\') == 10 assert pyobj_ops.set_attr(obj, \'x\', 20) == 0 assert pyobj_ops.get_attr(obj, \'x\') == 20 assert pyobj_ops.del_attr(obj, \'x\') == 0 assert pyobj_ops.has_attr(obj, \'x\') == 0 obj1 = Sample(10) obj2 = Sample(20) assert pyobj_ops.compare_objects(obj1, obj2, pyobj_ops.Py_LT) == True ``` Submission: Submit the complete working code along with a README file explaining the steps to compile and install the extension module, and a sample Python script demonstrating the usage of all implemented functions.","solution":"def has_attr(obj, attr_name): Check if the object `obj` has an attribute `attr_name`. Args: - obj: The Python object. - attr_name: The name of the attribute as a string. Returns: - 1 if the attribute exists, 0 otherwise. return hasattr(obj, attr_name) def get_attr(obj, attr_name): Get the value of an attribute `attr_name` from the object `obj`. Args: - obj: The Python object. - attr_name: The name of the attribute as a string. Returns: - The attribute value on success or None on failure. try: return getattr(obj, attr_name) except AttributeError: return None def set_attr(obj, attr_name, value): Set an attribute `attr_name` on the object `obj` to a value `value`. Args: - obj: The Python object. - attr_name: The name of the attribute as a string. - value: The value to set the attribute to. Returns: - 0 on success, -1 on failure. try: setattr(obj, attr_name, value) return 0 except (TypeError, AttributeError): return -1 def del_attr(obj, attr_name): Delete an attribute `attr_name` from the object `obj`. Args: - obj: The Python object. - attr_name: The name of the attribute as a string. Returns: - 0 on success, -1 on failure. try: delattr(obj, attr_name) return 0 except AttributeError: return -1 def compare_objects(obj1, obj2, opid): Compare two objects `obj1` and `obj2` using the operation specified by `opid`. Args: - obj1: The first Python object. - obj2: The second Python object. - opid: The comparison operation to perform (one of \'LT\', \'LE\', \'EQ\', \'NE\', \'GT\', \'GE\'). Returns: - The result of the comparison (True or False). if opid == \\"LT\\": return obj1 < obj2 elif opid == \\"LE\\": return obj1 <= obj2 elif opid == \\"EQ\\": return obj1 == obj2 elif opid == \\"NE\\": return obj1 != obj2 elif opid == \\"GT\\": return obj1 > obj2 elif opid == \\"GE\\": return obj1 >= obj2 else: raise ValueError(\\"Invalid comparison operation identifier.\\")"},{"question":"**Objective:** Demonstrate your understanding of Python\'s `logging` module by setting up a sophisticated logging system with specific requirements. **Task:** Write a Python script that sets up a logging system with the following specifications: 1. **Loggers and Handlers:** - Create a logger named `applicationLogger`. - Configure a console handler (`StreamHandler`) that logs messages to the console. - Configure a file handler (`FileHandler`) that logs messages to a file named `app.log`. - Configure a rotating file handler (`RotatingFileHandler`) that logs messages to a file named `app_rotating.log`, with a maximum file size of 2 KB and keeps up to 3 backup files. 2. **Logging Levels:** - The console handler should display messages with level `WARNING` and above. - The file handler should store messages with level `INFO` and above. - The rotating file handler should store messages with level `DEBUG` and above. 3. **Message formatting:** - Use the format `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'` for logging messages in both the console and file handlers. 4. **Custom Log Messages:** - Log the following messages using the appropriate logging levels: - \\"This is an info message.\\" - \\"This is a debug message.\\" - \\"This is a warning message.\\" - \\"This is an error message.\\" - \\"This is a critical message.\\" **Constraints:** - The solution should be implemented in a single Python script. - Ensure that the logging system is correctly initialized so that the appropriate messages appear in the console and each log file. **Input:** - No input is required from the user. **Output:** - Messages should be printed to the console and written to the log files according to the specifications above. **Performance Requirements:** - The logging setup should be efficient and adhere to the specified log rotation policies. # Example: ```python # After running the script, expected console output: # 2023-10-10 12:00:00,123 - applicationLogger - WARNING - This is a warning message. # 2023-10-10 12:00:00,456 - applicationLogger - ERROR - This is an error message. # 2023-10-10 12:00:00,789 - applicationLogger - CRITICAL - This is a critical message. # Content of app.log: # 2023-10-10 12:00:00,123 - applicationLogger - INFO - This is an info message. # 2023-10-10 12:00:00,123 - applicationLogger - WARNING - This is a warning message. # 2023-10-10 12:00:00,456 - applicationLogger - ERROR - This is an error message. # 2023-10-10 12:00:00,789 - applicationLogger - CRITICAL - This is a critical message. # Content of app_rotating.log and its backups (depending on rotation policy): # Files should include logged messages at DEBUG level and above. ```","solution":"import logging from logging.handlers import RotatingFileHandler def setup_logging(): # Create a logger named \'applicationLogger\' logger = logging.getLogger(\'applicationLogger\') logger.setLevel(logging.DEBUG) # Set the lowest level to DEBUG to capture all messages # Define the formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Console handler with level WARNING and above console_handler = logging.StreamHandler() console_handler.setLevel(logging.WARNING) console_handler.setFormatter(formatter) # File handler with level INFO and above file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.INFO) file_handler.setFormatter(formatter) # Rotating file handler with level DEBUG and above rotating_file_handler = RotatingFileHandler(\'app_rotating.log\', maxBytes=2000, backupCount=3) rotating_file_handler.setLevel(logging.DEBUG) rotating_file_handler.setFormatter(formatter) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addHandler(rotating_file_handler) return logger # Setup logging logger = setup_logging() # Log messages at various levels logger.info(\\"This is an info message.\\") logger.debug(\\"This is a debug message.\\") logger.warning(\\"This is a warning message.\\") logger.error(\\"This is an error message.\\") logger.critical(\\"This is a critical message.\\")"},{"question":"Objective: Demonstrate your understanding of the `secrets` module in Python by writing functions that generate secure tokens and passwords. Problem Statement: You are tasked with developing a secure system that requires the generation of various types of secure tokens and passwords. Implement the following functions using the `secrets` module: 1. **generate_secure_token**: Generate a secure token of a specified type and length. 2. **generate_password**: Generate a secure, alphanumeric password that meets certain criteria. Function 1: `generate_secure_token` - **Input**: - `token_type` (str): The type of token to generate. Acceptable values are `\\"bytes\\"`, `\\"hex\\"`, `\\"urlsafe\\"`. - `length` (int): The number of bytes of randomness to use for the token. - **Output**: - Returns a random token of the specified type. - **Constraints**: - `token_type` must be one of `\\"bytes\\"`, `\\"hex\\"`, or `\\"urlsafe\\"`. - `length` must be a positive integer. - **Examples**: ```python generate_secure_token(\\"bytes\\", 16) # Might output: b\'xebrx17D*txaexd4xe3Sxb6xe2xebP1x8b\' generate_secure_token(\\"hex\\", 16) # Might output: \'f9bf78b9a18ce6d46a0cd2b0b86df9da\' generate_secure_token(\\"urlsafe\\", 16) # Might output: \'Drmhze6EPcv0fN_81Bj-nA\' ``` Function 2: `generate_password` - **Input**: - `length` (int): The length of the password. - `min_lowercase` (int): The minimum number of lowercase characters required. - `min_uppercase` (int): The minimum number of uppercase characters required. - `min_digits` (int): The minimum number of digit characters required. - **Output**: - Returns a random alphanumeric password that meets the requirements. - **Constraints**: - `length` must be a positive integer of at least 6. - `min_lowercase`, `min_uppercase`, and `min_digits` must be non-negative integers. - The sum of `min_lowercase`, `min_uppercase`, and `min_digits` must be less than or equal to `length`. - **Examples**: ```python generate_password(10, 1, 1, 3) # Might output: \'aB12C345de\' generate_password(8, 1, 1, 1) # Might output: \'A1bcdefg\' ``` Implementation Requirements: 1. Use the `secrets` module for all random number generation. 2. Ensure that the password generated meets the specified criteria. 3. Your code should handle invalid input gracefully (e.g., invalid `token_type` or constraints not met for `generate_password`). Performance Requirements: - The implementation should efficiently generate tokens and passwords even for edge cases (e.g., maximum length for tokens). ```python import secrets import string def generate_secure_token(token_type, length): # Implement function body here pass def generate_password(length, min_lowercase, min_uppercase, min_digits): # Implement function body here pass # You can add test cases to verify your implementation ```","solution":"import secrets import string def generate_secure_token(token_type, length): Generates a secure token of a specified type and length. Args: token_type (str): The type of token to generate. Can be \\"bytes\\", \\"hex\\", or \\"urlsafe\\". length (int): The number of bytes of randomness to use for the token. Returns: A random token of the specified type. if not isinstance(length, int) or length <= 0: raise ValueError(\\"Length must be a positive integer\\") if token_type not in [\\"bytes\\", \\"hex\\", \\"urlsafe\\"]: raise ValueError(\\"Invalid token type\\") if token_type == \\"bytes\\": return secrets.token_bytes(length) elif token_type == \\"hex\\": return secrets.token_hex(length) elif token_type == \\"urlsafe\\": return secrets.token_urlsafe(length) def generate_password(length, min_lowercase, min_uppercase, min_digits): Generates a secure, alphanumeric password that meets certain criteria. Args: length (int): The length of the password. min_lowercase (int): The minimum number of lowercase characters required. min_uppercase (int): The minimum number of uppercase characters required. min_digits (int): The minimum number of digit characters required. Returns: A random alphanumeric password that meets the requirements. if not isinstance(length, int) or length < 6: raise ValueError(\\"Length must be at least 6\\") if not all(isinstance(x, int) and x >= 0 for x in [min_lowercase, min_uppercase, min_digits]): raise ValueError(\\"Character requirements must be non-negative integers\\") if length < min_lowercase + min_uppercase + min_digits: raise ValueError(\\"Sum of character requirements cannot exceed length\\") password_chars = [] password_chars += [secrets.choice(string.ascii_lowercase) for _ in range(min_lowercase)] password_chars += [secrets.choice(string.ascii_uppercase) for _ in range(min_uppercase)] password_chars += [secrets.choice(string.digits) for _ in range(min_digits)] if len(password_chars) < length: remaining_length = length - len(password_chars) all_chars = string.ascii_letters + string.digits password_chars += [secrets.choice(all_chars) for _ in range(remaining_length)] secrets.SystemRandom().shuffle(password_chars) return \'\'.join(password_chars)"},{"question":"# Coding Assessment: Robust RobotFileParser Utility **Objective:** Create a utility function using the `urllib.robotparser` package that determines whether a web crawler, identified by its user-agent, can fetch a series of URLs from a given website adhering to the rules specified in the `robots.txt` file. **Function Specification:** ```python def can_crawl(base_url: str, user_agent: str, urls: list) -> dict: Determines whether the provided user agent can fetch each URL in the list `urls`. Parameters: base_url (str): The base URL where the `robots.txt` file is located. user_agent (str): The user agent string of the web crawler. urls (list): List of URLs (relative or absolute) to check against the `robots.txt` rules. Returns: dict: A dictionary where keys are URLs and values are booleans, indicating whether the user agent is allowed to fetch that URL (True if allowed, False otherwise). pass ``` **Requirements:** 1. **Set up and read** the `robots.txt` file from the provided `base_url`. 2. For each URL in the `urls` list, **check** if the user-agent is allowed to fetch it according to the `robots.txt` rules. 3. **Return** a dictionary with URLs as keys and the fetch permission (True/False) as values. # Example: ```python base_url = \\"http://www.example.com\\" user_agent = \\"my-web-crawler\\" urls = [ \\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\", \\"http://www.example.com/forbidden\\" ] result = can_crawl(base_url, user_agent, urls) print(result) ``` **Expected Output:** ```python { \\"http://www.example.com/page1\\": True, \\"http://www.example.com/page2\\": True, \\"http://www.example.com/forbidden\\": False } ``` # Constraints: 1. You may assume that the `base_url` will always be a valid website URL. 2. Handle cases where the `robots.txt` file may not exist or be inaccessible by allowing the user agent to fetch all URLs by default. 3. Ensure efficient handling of URLs and response times. # Notes: - Import necessary modules within the function as needed. - If there are relative URLs in the `urls` list, resolve them against the `base_url`. - Implement appropriate error-handling for network issues or malformed URLs.","solution":"from urllib import robotparser from urllib.parse import urljoin, urlparse def can_crawl(base_url: str, user_agent: str, urls: list) -> dict: Determines whether the provided user agent can fetch each URL in the list `urls`. Parameters: base_url (str): The base URL where the `robots.txt` file is located. user_agent (str): The user agent string of the web crawler. urls (list): List of URLs (relative or absolute) to check against the `robots.txt` rules. Returns: dict: A dictionary where keys are URLs and values are booleans, indicating whether the user agent is allowed to fetch that URL (True if allowed, False otherwise). rp = robotparser.RobotFileParser() robots_txt_url = urljoin(base_url, \\"/robots.txt\\") rp.set_url(robots_txt_url) try: rp.read() except Exception as e: # If there\'s an issue in reading robots.txt, default to allowing all URLs. return {url: True for url in urls} crawl_permissions = {} for url in urls: absolute_url = urljoin(base_url, url) crawl_permissions[url] = rp.can_fetch(user_agent, absolute_url) return crawl_permissions"},{"question":"# XML Manipulation Using DOM API Objective: Implement a function `modify_xml_document` to manipulate an XML document using the DOM API provided by the `xml.dom` module. This function will build a new XML structure from scratch and modify nodes according to specific requirements. Requirements: 1. **Create a new XML document** with the following structure: ```xml <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> </book> <book id=\\"2\\"> <title>Data Structures</title> <author>Jane Smith</author> </book> </library> ``` 2. **Modify the document**: - Add a new book with `id=3`, `title` as \\"Algorithms\\", and `author` as \\"Alice Johnson\\". - Update the author of the book with `id=2` to \\"Jane Doe\\". - Remove the book with `id=1`. 3. **Output** the modified XML document as a `str`. Expected Input and Output: - **Function Signature**: ```python def modify_xml_document() -> str: pass ``` - **Output Format**: ```xml <library> <book id=\\"2\\"> <title>Data Structures</title> <author>Jane Doe</author> </book> <book id=\\"3\\"> <title>Algorithms</title> <author>Alice Johnson</author> </book> </library> ``` Constraints: - Use only the `xml.dom` module for manipulating the XML document. - Ensure the output XML string is well-formed and properly indented for readability. Example: Supposing the initial document creation and modification were executed successfully, calling the `modify_xml_document()` should return the following string: ```xml <library> <book id=\\"2\\"> <title>Data Structures</title> <author>Jane Doe</author> </book> <book id=\\"3\\"> <title>Algorithms</title> <author>Alice Johnson</author> </book> </library> ``` Implementation: - First, create the initial XML structure. - Then, perform the required modifications—add, update, and remove nodes. - Finally, convert the modified XML document to a string and return it. Consider edge cases such as ensuring unique ids for books and maintaining the structure\'s integrity throughout modifications.","solution":"from xml.dom.minidom import Document, parseString def modify_xml_document() -> str: # Create a new XML document doc = Document() # Create the root element library = doc.createElement(\'library\') doc.appendChild(library) # Create the first book book1 = doc.createElement(\'book\') book1.setAttribute(\'id\', \'1\') title1 = doc.createElement(\'title\') title1.appendChild(doc.createTextNode(\'Python Programming\')) author1 = doc.createElement(\'author\') author1.appendChild(doc.createTextNode(\'John Doe\')) book1.appendChild(title1) book1.appendChild(author1) library.appendChild(book1) # Create the second book book2 = doc.createElement(\'book\') book2.setAttribute(\'id\', \'2\') title2 = doc.createElement(\'title\') title2.appendChild(doc.createTextNode(\'Data Structures\')) author2 = doc.createElement(\'author\') author2.appendChild(doc.createTextNode(\'Jane Smith\')) book2.appendChild(title2) book2.appendChild(author2) library.appendChild(book2) # Add a new book with id=3 book3 = doc.createElement(\'book\') book3.setAttribute(\'id\', \'3\') title3 = doc.createElement(\'title\') title3.appendChild(doc.createTextNode(\'Algorithms\')) author3 = doc.createElement(\'author\') author3.appendChild(doc.createTextNode(\'Alice Johnson\')) book3.appendChild(title3) book3.appendChild(author3) library.appendChild(book3) # Update the author of the book with id=2 for book in library.getElementsByTagName(\'book\'): if book.getAttribute(\'id\') == \'2\': for author in book.getElementsByTagName(\'author\'): author.firstChild.data = \'Jane Doe\' # Remove the book with id=1 for book in library.getElementsByTagName(\'book\'): if book.getAttribute(\'id\') == \'1\': library.removeChild(book) break # Output the modified XML document as a string return doc.toprettyxml(indent=\\" \\")"},{"question":"Objective Implement a custom context manager for managing a temporary file. Your context manager should: 1. Create a temporary file when entering the context. 2. Provide a writable file object to the code inside the `with` block. 3. Automatically close and delete the file when exiting the context, whether an exception occurred or not. Requirements 1. Use the `contextlib` module to implement the context manager. 2. Demonstrate the use of your context manager with a simple example that writes data to the temporary file and ensures it gets properly deleted afterward. 3. Handle any exceptions that occur inside the `with` block, ensuring the file is closed and deleted in all cases. Function Signature ```python from typing import IO import contextlib @contextlib.contextmanager def temporary_file() -> IO: # Your implementation here # Example usage if __name__ == \\"__main__\\": test_data = \\"Hello, World!\\" try: with temporary_file() as f: f.write(test_data) print(\\"Temporary file was used and deleted successfully.\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` Constraints - You may use the `tempfile` module for creating temporary files. - Ensure the file deletion process does not leave any residual files, even in the case of an unexpected exception. - The temporary file should be writable. Evaluation Criteria - Correctness of the implementation. - Proper usage of the `contextlib` module. - Ability to handle exceptions and clean up resources. - Performance and readability of the code.","solution":"import tempfile import os import contextlib from typing import IO @contextlib.contextmanager def temporary_file() -> IO: temp_file = tempfile.NamedTemporaryFile(delete=False) try: yield temp_file except Exception as e: raise e finally: temp_file.close() os.remove(temp_file.name) # Example usage if __name__ == \\"__main__\\": test_data = \\"Hello, World!\\" try: with temporary_file() as f: f.write(test_data.encode(\'utf-8\')) print(\\"Temporary file was used and deleted successfully.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Overview You are required to implement a function that formats a list of doubles into a formatted string representation ensuring specific constraints are met. Additionally, you will need to demonstrate how to convert strings to doubles with appropriate error handling. Problem Description Write a function `format_doubles_to_string(values, format_code, precision, flags)`, and a function `convert_string_to_double(s, overflow_exception)` in Python. 1. **format_doubles_to_string(values, format_code, precision, flags)** - **Input:** - `values`: A list of double values. - `format_code`: A single character string representing the format code (one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'). - `precision`: An integer specifying the precision used in formatting. - `flags`: An integer value using a bitwise combination of: - `0` for no flags. - `Py_DTSF_SIGN` (1) to always prefix the result with a sign. - `Py_DTSF_ADD_DOT_0` (2) to ensure the result doesn’t look like an integer. - `Py_DTSF_ALT` (4) for alternate formatting rules. - **Output:** - Returns a formatted string of double values. - **Constraints:** - Values must be converted correctly according to the specified `format_code`, `precision`, and `flags`. - Handle potential edge cases such as empty lists or invalid format codes. 2. **convert_string_to_double(s, overflow_exception)** - **Input:** - `s`: A string representing a floating-point number. - `overflow_exception`: A Python exception object to be raised in case of an overflow condition. - **Output:** - Returns the converted double value. - Raises an appropriate exception if the string does not represent a valid floating point number or in case of overflow. - **Constraints:** - The string should not have leading or trailing whitespace. - Handle both valid and invalid string inputs robustly. Example ```python # Example 1 values = [3.14, 2.718, -1.0] format_code = \'f\' precision = 2 flags = 3 # Py_DTSF_SIGN | Py_DTSF_ADD_DOT_0 output = format_doubles_to_string(values, format_code, precision, flags) # output should be: \\"+3.14 +2.72 -1.00\\" # Example 2 try: value = convert_string_to_double(\\"1e500\\", OverflowError) except OverflowError: print(\\"Overflow occurred\\") try: value = convert_string_to_double(\\"invalid\\", OverflowError) except ValueError: print(\\"Value error occurred\\") ``` Note: - Use appropriate exception handling to ensure the code is robust. - The primary goal is to demonstrate your understanding of string manipulation, formatted output, and exception handling. Submission Please submit the function implementations along with test cases demonstrating their functionality.","solution":"import math # Constants for flags Py_DTSF_SIGN = 1 Py_DTSF_ADD_DOT_0 = 2 Py_DTSF_ALT = 4 def format_doubles_to_string(values, format_code, precision, flags): Formats a list of doubles into a formatted string representation with given format_code, precision and flags. Parameters: values: list of float - list of double values to be formatted. format_code: str - single character representing the format code (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'). precision: int - specifying the precision used in formatting. flags: int - integer value using a bitwise combination of: Py_DTSF_SIGN (1) - to always prefix the result with a sign. Py_DTSF_ADD_DOT_0 (2) - to ensure the result doesn’t look like an integer. Py_DTSF_ALT (4) - for alternate formatting rules. Returns: str - A formatted string of double values. format_spec = f\\".{precision}{format_code}\\" results = [] for value in values: formatted_value = format(value, format_spec) if flags & Py_DTSF_SIGN and value >= 0: formatted_value = \'+\' + formatted_value if flags & Py_DTSF_ADD_DOT_0 and \'.\' not in formatted_value: formatted_value += \'.0\' if flags & Py_DTSF_ALT and format_code in (\'e\', \'E\', \'f\', \'F\'): formatted_value = formatted_value.replace(\'e\', \'E\') results.append(formatted_value) return \' \'.join(results) def convert_string_to_double(s, overflow_exception): Converts a string to a double with overflow exception handling. Parameters: s: str - a string representing a floating-point number. overflow_exception: Exception - Python exception object to be raised in case of an overflow condition. Returns: float - The converted double value. try: value = float(s) if value == float(\'inf\') or value == float(\'-inf\'): raise overflow_exception(\\"overflow occurred\\") return value except ValueError: raise ValueError(\\"could not convert string to float\\") from None"},{"question":"# Question: Log File Analyzer You\'re tasked with creating a log file analyzer for a server application. The logs contain time-stamped events, and your goal is to extract all unique error messages from a specified time period and summarize their occurrences. Requirements: 1. **Read from a Log File:** - The log file (`log.txt`) contains entries in the following format: ``` [YYYY-MM-DD HH:MM:SS] ERROR: [error message] [YYYY-MM-DD HH:MM:SS] INFO: [info message] ``` - Example: ``` [2023-02-01 13:45:00] ERROR: Disk full [2023-02-01 13:46:00] INFO: Rebooting system [2023-02-01 14:00:00] ERROR: Disk full [2023-02-01 14:30:00] ERROR: Network down ``` 2. **Extract Errors within a Time Period:** - Input: Start and end times in the format `YYYY-MM-DD HH:MM:SS`. - Output: A summary of unique error messages and their counts within the specified time frame. 3. **Summarize and Output:** - Display the unique error messages and their occurrence counts in a sorted order by the message. Function Signature: ```python def analyze_log(file_path: str, start_time: str, end_time: str) -> None: ``` Example Usage: ```python analyze_log(\'log.txt\', \'2023-02-01 13:30:00\', \'2023-02-01 14:15:00\') ``` Expected Output: ``` Error Summary from 2023-02-01 13:30:00 to 2023-02-01 14:15:00: Disk full: 2 ``` Constraints: - The log file can be large, so your implementation should be efficient in terms of both time and space. - Assume well-formed input data for log entries. - Error messages are case-sensitive. # Detailed Steps: 1. **Read the file**: Efficiently read the log file line by line. 2. **Parse timestamps**: Use the `datetime` module to parse and compare timestamps. 3. **Using regex**: Utilize the `re` module to extract error messages. 4. **Count occurrences**: Store and count unique error messages. 5. **Output summary**: Print a summary of unique error messages and their counts. The implementation should be robust and handle edge cases such as no errors found in the given time range.","solution":"import re from datetime import datetime from collections import defaultdict def analyze_log(file_path: str, start_time: str, end_time: str) -> None: Analyzes the log file and prints the summary of unique error messages and their counts within the specified time range. Args: file_path (str): The path to the log file. start_time (str): The start time in \'YYYY-MM-DD HH:MM:SS\' format. end_time (str): The end time in \'YYYY-MM-DD HH:MM:SS\' format. # Convert start time and end time to datetime objects start_datetime = datetime.strptime(start_time, \'%Y-%m-%d %H:%M:%S\') end_datetime = datetime.strptime(end_time, \'%Y-%m-%d %H:%M:%S\') error_counts = defaultdict(int) # Open the log file and read it line by line with open(file_path, \'r\') as file: for line in file: # Match the log line structure match = re.match(r\'[(.*?)] ERROR: (.*)\', line) if match: timestamp_str, error_message = match.groups() log_datetime = datetime.strptime(timestamp_str, \'%Y-%m-%d %H:%M:%S\') # Check if the log entry falls within the specified time period if start_datetime <= log_datetime <= end_datetime: error_counts[error_message] += 1 # Print the summary of unique error messages and their counts print(f\\"Error Summary from {start_time} to {end_time}:\\") for error_message in sorted(error_counts.keys()): print(f\\"{error_message}: {error_counts[error_message]}\\")"},{"question":"# ASCII String Transformation Given a string consisting of various ASCII characters, your task is to implement a function `transform_ascii_string(s)` that transforms the string according to the following rules: 1. Replace all control characters with their caret (\'^\') representation (e.g., `x01` as `^A`). 2. Replace characters with their corresponding values using the control character found using the `ctrl()` function if they belong to the control character range. 3. Convert the string back to printable form using the `unctrl()` function for any non-printable characters. # Function Signature ```python def transform_ascii_string(s: str) -> str: pass ``` # Input - `s` (string): A string containing ASCII characters. # Output - A transformed string according to the given rules. # Constraints - The input string `s` will have a length of at most `10^4`. - The string may contain any ASCII character, including control characters. # Example ```python # Example 1 input_string = \\"Hellox01World\\" output_string = transform_ascii_string(input_string) # Expected Output: \\"Hello^AWorld\\" # Example 2 input_string = \\"Datax02Transmissionx03\\" output_string = transform_ascii_string(input_string) # Expected Output: \\"Data^BTransmission^C\\" ``` # Performance Requirements - The function should be able to handle strings up to the maximum length within 1 second. Utilize the `curses.ascii` module effectively to address the requirements and implement the necessary transformations. Make sure your solution is efficient and clear.","solution":"import curses.ascii def transform_ascii_string(s: str) -> str: result = [] for char in s: if curses.ascii.isctrl(char): result.append(\'^\' + chr(ord(char) + 64)) # Convert to caret notation else: result.append(char) return \'\'.join(result) # Example Usage input_string = \\"Hellox01World\\" print(transform_ascii_string(input_string))"},{"question":"# Memory Management in Python In Python, managing memory efficiently is crucial to ensure the performance and stability of applications. Python provides a memory management system with different allocation domains and interfaces to handle various types of memory management needs. Task You are required to implement a set of functions in Python that simulate the memory management operations described in the provided documentation. These functions will not directly manage memory at the system level, but will emulate the behavior of `PyMem_*` and `PyObject_*` functions using Python\'s native capabilities. Requirements 1. Implement the following functions: - `py_mem_malloc(size: int) -> bytes` - `py_mem_calloc(nelem: int, elsize: int) -> bytes` - `py_mem_realloc(ptr: bytes, new_size: int) -> bytes` - `py_mem_free(ptr: bytes) -> None` 2. Implement a custom allocator: - `CustomAllocator(malloc_func, calloc_func, realloc_func, free_func)` - `malloc_func(size: int) -> bytes` - `calloc_func(nelem: int, elsize: int) -> bytes` - `realloc_func(ptr: bytes, new_size: int) -> bytes` - `free_func(ptr: bytes) -> None` - Methods: - `set_allocator(custom_allocator: CustomAllocator) -> None` - `get_allocator() -> CustomAllocator` Function Descriptions - `py_mem_malloc(size: int) -> bytes`: Simulates the allocation of `size` bytes of memory, returning a bytes object of the specified size. - `py_mem_calloc(nelem: int, elsize: int) -> bytes`: Allocates memory for an array of `nelem` elements, each of `elsize` bytes. The allocated memory is zero-initialized. - `py_mem_realloc(ptr: bytes, new_size: int) -> bytes`: Resizes the memory block pointed to by `ptr` to `new_size` bytes. - `py_mem_free(ptr: bytes) -> None`: Emulates freeing the memory block pointed to by `ptr`. - `CustomAllocator(malloc_func, calloc_func, realloc_func, free_func)`: A class that encapsulates custom memory allocation strategies. It must provide the following methods: - `malloc_func(size: int) -> bytes` - `calloc_func(nelem: int, elsize: int) -> bytes` - `realloc_func(ptr: bytes, new_size: int) -> bytes` - `free_func(ptr: bytes) -> None` Methods: - `set_allocator(custom_allocator: CustomAllocator) -> None`: Sets the custom allocator for memory operations. - `get_allocator() -> CustomAllocator`: Returns the current custom allocator. Constraints - Do not use any external libraries for memory management. - Ensure that all allocated memory is managed within Python\'s native capabilities. - Implement error handling as necessary to simulate memory allocation failures (e.g., raising an appropriate exception when allocation fails). Example Usage ```python # Implement the py_mem_* functions and CustomAllocator class here # Example usage: allocator = CustomAllocator(py_mem_malloc, py_mem_calloc, py_mem_realloc, py_mem_free) set_allocator(allocator) # Allocate memory ptr = allocator.malloc_func(10) print(ptr) # Output: bytes object of size 10 # Zero-initialized memory allocation zero_ptr = allocator.calloc_func(5, 2) print(zero_ptr) # Output: bytes object of size 10, filled with zeros # Reallocate memory ptr = allocator.realloc_func(ptr, 20) print(ptr) # Output: bytes object of size 20 # Free memory allocator.free_func(ptr) ``` Testing Ensure your implementation is tested with appropriate test cases to verify the correct behavior of the memory management functions. Include test cases that cover: - Allocation of different sizes. - Reallocation to larger and smaller sizes. - Freeing memory and ensuring no errors occur.","solution":"def py_mem_malloc(size: int) -> bytes: Simulates the allocation of `size` bytes of memory, returning a bytes object of the specified size. return bytes(size) def py_mem_calloc(nelem: int, elsize: int) -> bytes: Allocates memory for an array of `nelem` elements, each of `elsize` bytes. The allocated memory is zero-initialized. return bytes(nelem * elsize) def py_mem_realloc(ptr: bytes, new_size: int) -> bytes: Resizes the memory block pointed to by `ptr` to `new_size` bytes. new_ptr = bytearray(new_size) new_ptr[:len(ptr)] = ptr[:new_size] return bytes(new_ptr) def py_mem_free(ptr: bytes) -> None: Emulates freeing the memory block pointed to by `ptr`. There is no action needed in Python to free memory as garbage collection handles it. pass class CustomAllocator: def __init__(self, malloc_func, calloc_func, realloc_func, free_func): self.malloc_func = malloc_func self.calloc_func = calloc_func self.realloc_func = realloc_func self.free_func = free_func current_allocator = None def set_allocator(custom_allocator: CustomAllocator) -> None: global current_allocator current_allocator = custom_allocator def get_allocator() -> CustomAllocator: global current_allocator return current_allocator"},{"question":"**Python \\"imp\\" Module Assessment** # Objective: To assess your understanding of Python\'s deprecated \\"imp\\" module and your ability to use more current alternatives provided by \\"importlib\\". # Task: Write a Python function `custom_import(module_name: str) -> object` that replicates the basic functionality of Python\'s `import` statement using the \\"imp\\" module. This involves: 1. Checking if the module is already imported. 2. Using `imp.find_module` to locate the module. 3. Using `imp.load_module` to load the module. Additionally, if the provided module name contains a dot, indicating a submodule, your function should correctly handle it by finding and importing the parent module first, then locating and loading the submodule. # Constraints: - You cannot use Python\'s built-in `import` statement directly, nor the `importlib` module. - You must handle the closing of file pointers appropriately to avoid resource leaks. # Input: - `module_name` (str): The name of the module to import, which can include submodules (e.g., \'package.submodule\'). # Output: - The imported module object. # Example: ```python module = custom_import(\\"os.path\\") print(module) # <module \'posixpath\' from \'...\'> print(module.join(\\"a\\", \\"b\\")) # \'a/b\' ``` # Notes: - The `module_name` can be a top-level module or contain nested submodules. - Handle errors gracefully and provide meaningful error messages if the module cannot be found or loaded. # Starter Code: ```python import imp import sys def custom_import(module_name: str) -> object: try: if module_name in sys.modules: return sys.modules[module_name] parts = module_name.split(\'.\') parent_name = parts[0] parent_module = sys.modules.get(parent_name) if not parent_module: fp, pathname, description = imp.find_module(parent_name) try: parent_module = imp.load_module(parent_name, fp, pathname, description) finally: if fp: fp.close() module = parent_module for part in parts[1:]: fp, pathname, description = imp.find_module(part, [module.__path__]) try: module = imp.load_module(module_name, fp, pathname, description) finally: if fp: fp.close() return module except ImportError as e: raise ImportError(f\\"Module {module_name} could not be imported: {e}\\") # Example usage # print(custom_import(\\"os.path\\")) ```","solution":"import imp import sys def custom_import(module_name: str) -> object: try: if module_name in sys.modules: return sys.modules[module_name] parts = module_name.split(\'.\') parent_name = parts[0] parent_module = sys.modules.get(parent_name) if not parent_module: fp, pathname, description = imp.find_module(parent_name) try: parent_module = imp.load_module(parent_name, fp, pathname, description) finally: if fp: fp.close() module = parent_module for part in parts[1:]: fp, pathname, description = imp.find_module(part, [module.__path__]) try: module = imp.load_module(module_name, fp, pathname, description) finally: if fp: fp.close() return module except ImportError as e: raise ImportError(f\\"Module {module_name} could not be imported: {e}\\") # Example usage # print(custom_import(\\"os.path\\"))"},{"question":"**Coding Assessment Question** # Objective You are tasked with creating a simple file processing utility for reading and interacting with chunked data files using the described structure and operations. # Task Implement a `SimpleChunk` class in Python that mimics the behavior of the deprecated \\"Chunk\\" class provided in the documentation. Your class will need to handle reading file chunks and providing methods to interact with these chunks. # Requirements and Specifications: 1. **Class Definition:** - Define a class `SimpleChunk` that takes the following parameters in the constructor: - `file`: A file-like object that is readable. - `align`: A boolean indicating if chunks are aligned on 2-byte boundaries (default: `True`). - `bigendian`: A boolean indicating if the chunk size is in big-endian order (default: `True`). 2. **Methods to Implement:** - `__init__(self, file, align=True, bigendian=True)`: Constructor initializing the class. - `getname(self)`: Returns the 4-byte chunk ID as a string. - `getsize(self)`: Returns the size of the chunk data. - `close(self)`: Closes the chunk and skips to its end. - `isatty(self)`: Returns `False`. - `seek(self, pos, whence=0)`: Sets the chunk\'s current position. - `tell(self)`: Returns the current position in the chunk. - `read(self, size=-1)`: Reads at most `size` bytes from the chunk. - `skip(self)`: Skips to the end of the chunk. # Input and Output Formats - **Input**: The class should handle file-like objects, read their content based on the chunk structure defined. - **Output**: Methods return appropriate values, such as chunk name, size, file position, bytes read, etc. # Constraints - Assume the input files are properly formatted EA IFF 85 chunked files. - You may not use the deprecated \\"chunk\\" module but need to replicate its functionality. - Ensure your class and methods handle edge cases, such as end of file, misaligned chunks (if `align=False`), and different endianness (if `bigendian=False`). # Example Usage Provide an example code snippet showing how to use `SimpleChunk` class: ```python with open(\\"example_chunked_file.iff\\", \\"rb\\") as file: chunk = SimpleChunk(file) print(chunk.getname()) # Output: (chunk\'s name) print(chunk.getsize()) # Output: (chunk\'s size) data = chunk.read() # Reads all data in the chunk chunk.seek(0) # Reset position to the beginning more_data = chunk.read(10) # Reads first 10 bytes chunk.skip() # Skips the rest of the chunk chunk.close() # Closes the chunk ``` # Performance - Your implementation must perform efficiently with large files, avoiding unnecessary memory usage and handling streams properly. # Submission Submit your `SimpleChunk` class implementation demonstrating all the required methods.","solution":"class SimpleChunk: def __init__(self, file, align=True, bigendian=True): self.file = file self.align = align self.bigendian = bigendian self.chunk_id = self.file.read(4) if self.bigendian: self.chunk_size = int.from_bytes(self.file.read(4), byteorder=\'big\') else: self.chunk_size = int.from_bytes(self.file.read(4), byteorder=\'little\') self.current_position = 0 self.data_start = self.file.tell() def getname(self): return self.chunk_id.decode(\'ascii\') def getsize(self): return self.chunk_size def close(self): self.skip() def isatty(self): return False def seek(self, pos, whence=0): if whence == 0: new_position = pos elif whence == 1: new_position = self.current_position + pos elif whence == 2: new_position = self.chunk_size + pos if 0 <= new_position <= self.chunk_size: self.file.seek(self.data_start + new_position) self.current_position = new_position else: raise ValueError(\\"Position out of chunk bounds\\") def tell(self): return self.current_position def read(self, size=-1): if size == -1: size = self.chunk_size - self.current_position else: size = min(size, self.chunk_size - self.current_position) data = self.file.read(size) self.current_position += len(data) return data def skip(self): remaining = self.chunk_size - self.current_position if remaining > 0: self.file.seek(remaining, 1) self.current_position += remaining if self.align and (self.chunk_size % 2) != 0: self.file.seek(1, 1)"},{"question":"You are given a dataset with multiple classes and features. Your task is to implement a Linear Discriminant Analysis (LDA) classifier using `scikit-learn`, perform dimensionality reduction to a specified number of components, and evaluate the classifier\'s performance. # Requirements 1. **Data Preprocessing** - Load the dataset and perform necessary preprocessing (e.g., handling missing values, normalization). 2. **LDA Implementation** - Implement LDA for classification using `scikit-learn`. - Use the `transform` method to reduce the dataset to a specified number of dimensions. 3. **Model Evaluation** - Train the LDA classifier on the training set. - Evaluate the classifier on the test set and report the accuracy. 4. **Visualization** - Create a scatter plot of the data points in the reduced dimensional space, colored by their class labels. # Input - `data`: A DataFrame containing the dataset. - `target_column`: The name of the column containing the class labels. - `n_components`: The number of dimensions to reduce the data to using LDA. # Output - `accuracy`: A float representing the accuracy of the LDA classifier on the test set. - `scatter_plot`: A scatter plot of the reduced dimensional data points. # Constraints - Assume `n_components` is less than the number of classes in the dataset. # Example ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis import matplotlib.pyplot as plt def lda_classification(data, target_column, n_components): # Split the data into training and test sets X = data.drop(columns=[target_column]) y = data[target_column] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement LDA lda = LinearDiscriminantAnalysis(n_components=n_components) lda.fit(X_train, y_train) # Transform the data X_train_lda = lda.transform(X_train) X_test_lda = lda.transform(X_test) # Evaluate the model accuracy = lda.score(X_test, y_test) # Visualization plt.figure(figsize=(8,6)) for class_value in y.unique(): plt.scatter(X_train_lda[y_train == class_value, 0], X_train_lda[y_train == class_value, 1], label=f\'Class {class_value}\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.legend() plt.show() return accuracy # Example usage data = pd.read_csv(\'your_dataset.csv\') target_column = \'target\' n_components = 2 accuracy = lda_classification(data, target_column, n_components) print(f\'Accuracy: {accuracy}\') ``` # Your Task Given the template above: 1. Complete the function `lda_classification` according to the requirements. 2. Ensure the implementation follows good coding practices and is efficient. 3. Test your function with a sample dataset and ensure it meets the expected output format.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt def lda_classification(data, target_column, n_components): # Data preprocessing # Separate features and target X = data.drop(columns=[target_column]) y = data[target_column] # Normalize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Implement LDA for classification lda = LinearDiscriminantAnalysis(n_components=n_components) lda.fit(X_train, y_train) # Transform the data X_train_lda = lda.transform(X_train) X_test_lda = lda.transform(X_test) # Evaluate the model accuracy = lda.score(X_test, y_test) # Visualization plt.figure(figsize=(8, 6)) unique_classes = y.unique() for class_value in unique_classes: plt.scatter(X_train_lda[y_train == class_value, 0], X_train_lda[y_train == class_value, 1], label=f\'Class {class_value}\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.legend() plt.title(\'LDA: Reduced Dimensional Space\') plt.show() return accuracy"},{"question":"You are given a binary protocol specification that requires packing and unpacking of a sequence of values in a specific format. Your task is to write a Python function to handle this packing and unpacking using the `struct` module. # The format specification is as follows: 1. A boolean value (1 byte). 2. An unsigned 8-bit integer (1 byte). 3. A 4-byte float (4 bytes). 4. An unsigned 16-bit integer (2 bytes). 5. A signed 32-bit integer (4 bytes). 6. A double-precision float (8 bytes). 7. A Pascal string (prefix byte representing string length followed by actual string bytes; total bytes up to 256). The byte order for all fields is little-endian. # Instructions: 1. Implement a function `pack_data` that takes the values as arguments in the specified order and returns the packed binary data. 2. Implement a function `unpack_data` that takes the packed binary data as an argument and returns the unpacked values as a tuple in the specified order. # Signature: ```python def pack_data(boolean, u8_int, f32_float, u16_int, i32_int, f64_double, pascal_string) -> bytes: pass def unpack_data(binary_data: bytes) -> tuple: pass ``` # Example: ```python boolean = True u8_int = 255 f32_float = 3.14 u16_int = 65535 i32_int = -2147483648 f64_double = 2.718281828459045 pascal_string = b\\"Hello, struct module!\\" # Packing the data packed = pack_data(boolean, u8_int, f32_float, u16_int, i32_int, f64_double, pascal_string) print(packed) # Unpacking the data result = unpack_data(packed) print(result) # Output should be: # b\'x01xffxc3xf5H@xffxffxffxffxffxffxffx00!499999999996761xe7nHello, struct module!\' # (True, 255, 3.140000104904175, 65535, -2147483648, 2.718281828459045, b\'Hello, struct module!\') ``` # Constraints: 1. The length of the Pascal string should not exceed 255 characters. 2. The binary data for unpacking will always be in the correct format. **Note:** You should use the `struct` module to handle packing and unpacking of the binary data.","solution":"import struct def pack_data(boolean, u8_int, f32_float, u16_int, i32_int, f64_double, pascal_string) -> bytes: Packs data into a binary format according to the specified structure. boolean_byte = 1 if boolean else 0 pascal_string_length = len(pascal_string) if pascal_string_length > 255: raise ValueError(\\"Pascal string length cannot exceed 255 characters\\") # Defining the format for the struct format_string = \'<? B f H i d B {}s\'.format(pascal_string_length) # Packing the data using struct packed_data = struct.pack(format_string, boolean_byte, u8_int, f32_float, u16_int, i32_int, f64_double, pascal_string_length, pascal_string) return packed_data def unpack_data(binary_data: bytes) -> tuple: Unpacks binary data into its respective values according to the specified structure. # Unpacking the fixed part of the structure fixed_format = \'<? B f H i d B\' fixed_size = struct.calcsize(fixed_format) fixed_part = binary_data[:fixed_size] fixed_values = struct.unpack(fixed_format, fixed_part) boolean = bool(fixed_values[0]) u8_int = fixed_values[1] f32_float = fixed_values[2] u16_int = fixed_values[3] i32_int = fixed_values[4] f64_double = fixed_values[5] pascal_string_length = fixed_values[6] # Unpacking the Pascal string pascal_string_format = \'<{}s\'.format(pascal_string_length) pascal_string_part = binary_data[fixed_size:fixed_size + pascal_string_length] pascal_string = struct.unpack(pascal_string_format, pascal_string_part)[0] return (boolean, u8_int, f32_float, u16_int, i32_int, f64_double, pascal_string)"},{"question":"**Coding Assessment Question: Advanced PyTorch Module** # Objective Your task is to create, train, and manipulate an advanced PyTorch module to demonstrate your understanding of the PyTorch module system and its features. You are required to compose multiple custom modules, train the network, save its state, and perform inference with the model in evaluation mode. # Task 1. **Define the Custom Modules**: - Create a custom linear module `CustomLinear` that extends `nn.Module`. - Create a custom neural network `CustomNet` that uses two `CustomLinear` layers and a ReLU activation between them. 2. **Train the Neural Network**: - Use the `CustomNet` module for training with a dummy dataset. - Implement a training loop that minimizes the mean square error to fit the model to a constant target tensor. - Save the model state after training. 3. **Evaluation Mode**: - Load the saved model state into a new `CustomNet` instance. - Set the model to evaluation mode and perform inference on a new sample input. # Specifications 1. **CustomLinear Module**: - Inherits from `nn.Module`. - Initializes `weight` and `bias` parameters with random values using `nn.Parameter`. - Implements the `forward` method to apply an affine transformation. 2. **CustomNet Module**: - Inherits from `nn.Module`. - Consists of two `CustomLinear` layers and a ReLU activation function between them. - Implements the `forward` method to define the network architecture. 3. **Training**: - Dummy dataset: Random input tensors, target tensor is a constant tensor (e.g., `[0, 0, 0, 0]`). - Optimizer: SGD with a learning rate of 0.01 and momentum of 0.9. - Loss function: Mean Square Error (MSE). 4. **Saving and Loading the Model**: - Save the model state to a file. - Load the model state into a new model instance. 5. **Inference**: - Perform inference using the new model instance in evaluation mode. # Constraints - Ensure proper initialization of model weights. - Train the model for at least 100 epochs or until the loss converges to a reasonable value close to zero. # Input - There is no specific input for this task; you are required to implement the specified modules, train them, and complete the required tasks. # Output - The final model\'s prediction for a random sample input in evaluation mode. # Example Output ``` tensor([-0.0111, 0.0022, -0.0088, 0.0030], grad_fn=<AddBackward0>) ``` # Skeleton Code ```python import torch from torch import nn # Define the CustomLinear module class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() # Your code here def forward(self, input): # Your code here pass # Define the CustomNet module class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() # Your code here def forward(self, x): # Your code here pass # Training the CustomNet def train_model(): # Your code here pass # Saving the model def save_model(model, path): torch.save(model.state_dict(), path) # Loading the model def load_model(model, path): model.load_state_dict(torch.load(path)) # Main function to execute the task def main(): # Step 1: Define and train the model model = CustomNet() train_model() # Step 2: Save the model save_model(model, \'custom_net.pth\') # Step 3: Load the model and set to evaluation mode loaded_model = CustomNet() load_model(loaded_model, \'custom_net.pth\') loaded_model.eval() # Step 4: Perform inference on a random input sample_input = torch.randn(4) prediction = loaded_model(sample_input) print(prediction) if __name__ == \\"__main__\\": main() ``` # Notes - Follow the provided skeleton code and complete the implementation. - Pay attention to the correct initialization, training steps, and state saving/loading. - Make sure to test your implementation thoroughly for correctness. Good luck!","solution":"import torch from torch import nn, optim class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(out_features, in_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input): return torch.matmul(input, self.weight.t()) + self.bias class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = CustomLinear(4, 4) # Adjust input/output features as needed self.relu = nn.ReLU() self.fc2 = CustomLinear(4, 4) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train_model(model, optimizer, criterion, num_epochs=100): # Dummy dataset: 10 data points with 4 features each input_data = torch.randn(10, 4) target_data = torch.zeros(10, 4) # Target is a constant tensor for epoch in range(num_epochs): optimizer.zero_grad() outputs = model(input_data) loss = criterion(outputs, target_data) loss.backward() optimizer.step() return model def save_model(model, path): torch.save(model.state_dict(), path) def load_model(model, path): model.load_state_dict(torch.load(path)) def main(): model = CustomNet() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) criterion = nn.MSELoss() model = train_model(model, optimizer, criterion) save_model(model, \'custom_net.pth\') loaded_model = CustomNet() load_model(loaded_model, \'custom_net.pth\') loaded_model.eval() sample_input = torch.randn(1, 4) prediction = loaded_model(sample_input) print(prediction) if __name__ == \\"__main__\\": main()"},{"question":"In this coding assessment, you are required to work with pandas\' options and settings to format the display properties of a DataFrame. Your task is to write a function named `configure_dataframe_display` that accepts a DataFrame and several display options, then applies these options to control the DataFrame display. The function should: 1. Configure the DataFrame display settings using the provided options. 2. Display the DataFrame with the new settings within a `with` block, which ensures the settings revert to their prior state after execution. 3. Reset any custom display settings to their default values outside the `with` block. # Function Signature ```python def configure_dataframe_display(df: pd.DataFrame, options: dict) -> pd.DataFrame: pass ``` # Input - `df` (pd.DataFrame): The DataFrame to be displayed. - `options` (dict): A dictionary where the keys are option names (str) and the values are the settings to be applied. # Output - The function should display the DataFrame with the customized options and reset the custom settings after displaying. # Constraints - The dictionary `options` will contain valid pandas display options. - The DataFrame will have no more than 10,000 rows and 100 columns. # Example Usage ```python import pandas as pd import numpy as np data = np.random.randn(20, 5) df = pd.DataFrame(data, columns=[\'A\', \'B\', \'C\', \'D\', \'E\']) options = { \'display.max_rows\': 10, \'display.max_columns\': 5, \'display.precision\': 2, \'display.colheader_justify\': \'right\' } configure_dataframe_display(df, options) ``` The function should display the DataFrame `df` using the specified options and then reset the display settings to their default values. # Notes - Make sure to handle reverting the settings back to their defaults after displaying the DataFrame. - Use the `pandas.option_context` to manage the temporary changes during the display. # Additional Information Refer to the pandas official documentation for more details about the available options and their descriptions.","solution":"import pandas as pd def configure_dataframe_display(df: pd.DataFrame, options: dict) -> pd.DataFrame: Configure the DataFrame display settings using the provided options, display the DataFrame with the new settings within a `with` block and reset any custom display settings to their default values outside the `with` block. Args: df (pd.DataFrame): The DataFrame to be displayed. options (dict): A dictionary where the keys are option names (str) and the values are the settings to be applied. Returns: pd.DataFrame: The DataFrame to be displayed. with pd.option_context(*sum(([k, v] for k, v in options.items()), [])): print(df) for option in options: pd.reset_option(option) return df"},{"question":"**Programming Question: Working with `importlib.metadata`** # Objective: You will demonstrate your knowledge of Python and the `importlib.metadata` library by writing a function that retrieves detailed metadata information for a given package name, formats the information, and saves it into a JSON file. # Problem Statement: Write a function called `save_package_metadata` that takes a package name as input and retrieves its detailed metadata using the `importlib.metadata` library. The function should then save the metadata in a JSON format to a file named `\\"<package_name>_metadata.json\\"`. # Requirements: 1. The function should retrieve and store the following metadata about the package: - Name - Version - Summary - Author - Author-email - License - Requires-Python - Platform(s) - A list of all entry points (name and group). - A list of all files (with their paths, sizes, and hashes). 2. If some metadata information is not available, it should be indicated with `null`. # Input: - **package_name**: A string representing the name of the installed package (e.g., `\\"wheel\\"`). # Output: Write the metadata to a file named `\\"<package_name>_metadata.json\\"`. The file should be in JSON format and structured as follows: ```json { \\"name\\": \\"package_name\\", \\"version\\": \\"package_version\\", \\"summary\\": \\"package_summary\\", \\"author\\": \\"package_author\\", \\"author_email\\": \\"author_email\\", \\"license\\": \\"license_type\\", \\"requires_python\\": \\"python_version_requirement\\", \\"platforms\\": [\\"platform1\\", \\"platform2\\"], \\"entry_points\\": [ {\\"name\\": \\"entry_point_name\\", \\"group\\": \\"entry_point_group\\"} ], \\"files\\": [ {\\"path\\": \\"file_path\\", \\"size\\": size_in_bytes, \\"hash\\": \\"file_hash\\"} ] } ``` # Constraints: 1. Assume the package is already installed in the environment where the function is executed. 2. The function must handle exceptions gracefully and print a meaningful error message if the package is not found or if the metadata cannot be retrieved. # Example: Suppose the package name is `\\"wheel\\"`. The function should create a file named `\\"wheel_metadata.json\\"` with content similar to: ```json { \\"name\\": \\"wheel\\", \\"version\\": \\"0.32.3\\", \\"summary\\": \\"A built-package format for Python.\\", \\"author\\": \\"The Python Packaging Authority\\", \\"author_email\\": \\"pypa-dev@googlegroups.com\\", \\"license\\": \\"MIT\\", \\"requires_python\\": \\">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\\", \\"platforms\\": [\\"UNKNOWN\\"], \\"entry_points\\": [ {\\"name\\": \\"wheel\\", \\"group\\": \\"console_scripts\\"} ], \\"files\\": [ {\\"path\\": \\"wheel/util.py\\", \\"size\\": 859, \\"hash\\": \\"bYkw5oMccfazVCoYQwKkkemoVyMAFoR34mmKBx8R1NI\\"} ] } ``` # Additional Notes: - Make sure to import necessary modules, such as `importlib.metadata`, `json`, and others if needed. - Pay attention to handling Unicode and special characters in metadata. - Consider using pathlib for file operations.","solution":"import importlib.metadata import json import hashlib from pathlib import Path def save_package_metadata(package_name): try: metadata = importlib.metadata.metadata(package_name) distribution = importlib.metadata.distribution(package_name) entry_points = distribution.entry_points # Helper function to read file content and calculate hash def calculate_file_hash(file_path): sha256_hash = hashlib.sha256() with open(file_path, \\"rb\\") as f: for byte_block in iter(lambda: f.read(4096), b\\"\\"): sha256_hash.update(byte_block) return sha256_hash.hexdigest() # Collect metadata package_metadata = { \\"name\\": metadata.get(\\"Name\\", None), \\"version\\": metadata.get(\\"Version\\", None), \\"summary\\": metadata.get(\\"Summary\\", None), \\"author\\": metadata.get(\\"Author\\", None), \\"author_email\\": metadata.get(\\"Author-email\\", None), \\"license\\": metadata.get(\\"License\\", None), \\"requires_python\\": metadata.get(\\"Requires-Python\\", None), \\"platforms\\": metadata.get_all(\\"Platform\\"), \\"entry_points\\": [{\\"name\\": ep.name, \\"group\\": ep.group} for ep in entry_points], \\"files\\": [] } # Collect files info files = distribution.files if files: for file in files: file_path = Path(distribution.locate_file(file)) file_info = { \\"path\\": str(file), \\"size\\": file_path.stat().st_size, \\"hash\\": calculate_file_hash(file_path) } package_metadata[\\"files\\"].append(file_info) # Write to json file json_file = f\\"{package_name}_metadata.json\\" with open(json_file, \\"w\\", encoding=\\"utf-8\\") as f: json.dump(package_metadata, f, indent=4) print(f\\"Metadata saved to {json_file}\\") except importlib.metadata.PackageNotFoundError: print(f\\"Package {package_name} not found\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Problem Statement You are given a dataset of 2D points. Your task is to implement a kernel density estimation (KDE) model using scikit-learn and apply it to the dataset. You will need to experiment with different kernel types and bandwidth parameters, and then visualize the density estimation for comparison. Finally, you will evaluate the log density of a given set of points based on the fitted model. Dataset You can generate a dataset using the following code snippet: ```python import numpy as np np.random.seed(0) X = np.vstack([np.random.normal(loc=-1, scale=1, size=(100, 2)), np.random.normal(loc=1, scale=0.5, size=(100, 2))]) ``` Tasks 1. **Implement KDE Models**: - Fit KDE models with the following kernels: `\'gaussian\'`, `\'tophat\'`, `\'epanechnikov\'`. - Use bandwidth values of `0.1`, `0.5`, and `1.0`. 2. **Evaluate Log Density**: - For each model, evaluate the log density on the point `[[0, 0]]`. 3. **Visualization**: - Visualize the fitted density estimates for each kernel and bandwidth combination. Use a mesh grid to plot the density. Requirements - Implement the KDE using `sklearn.neighbors.KernelDensity`. - Create plots to visualize the density estimates for comparison. - Use a consistent plotting style to make comparisons easier. Expected Output 1. A plot showing the density estimate for each kernel and bandwidth combination. 2. A dictionary where the keys are tuples `(kernel, bandwidth)` and values are the log density estimates at `[[0, 0]]`. ```python { (\'gaussian\', 0.1): log_density_value, (\'gaussian\', 0.5): log_density_value, (\'gaussian\', 1.0): log_density_value, # similarly for \'tophat\' and \'epanechnikov\' } ``` Code Template ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Task 1: Generate dataset np.random.seed(0) X = np.vstack([np.random.normal(loc=-1, scale=1, size=(100, 2)), np.random.normal(loc=1, scale=0.5, size=(100, 2))]) # Define kernels and bandwidths kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1.0] # Dictionary to store log density values log_density_results = {} # Task 2: Implement and fit KDE models, evaluate log density for kernel in kernels: for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) log_density = kde.score_samples([[0, 0]])[0] log_density_results[(kernel, bandwidth)] = log_density # Task 3: Visualization function def plot_kde(kernel, bandwidth, X): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) x = np.linspace(-4, 4, 100) y = np.linspace(-4, 4, 100) X_grid, Y_grid = np.meshgrid(x, y) xy_sample = np.vstack([X_grid.ravel(), Y_grid.ravel()]).T Z = kde.score_samples(xy_sample).reshape(X_grid.shape) plt.contourf(X_grid, Y_grid, np.exp(Z), cmap=\'Blues\') plt.title(f\'Kernel: {kernel}, Bandwidth: {bandwidth}\') plt.scatter(X[:, 0], X[:, 1], s=5, c=\'red\') plt.show() # Generate and display plots for kernel in kernels: for bandwidth in bandwidths: plot_kde(kernel, bandwidth, X) print(log_density_results) ``` Your output should consist of a series of plots showing the density estimates and a dictionary with the log densities at the specified point.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def generate_dataset(): np.random.seed(0) X = np.vstack([np.random.normal(loc=-1, scale=1, size=(100, 2)), np.random.normal(loc=1, scale=0.5, size=(100, 2))]) return X def fit_kde_models(X, kernels, bandwidths): log_density_results = {} for kernel in kernels: for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) log_density = kde.score_samples([[0, 0]])[0] log_density_results[(kernel, bandwidth)] = log_density return log_density_results def plot_kde(kernel, bandwidth, X, ax): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) x = np.linspace(-4, 4, 100) y = np.linspace(-4, 4, 100) X_grid, Y_grid = np.meshgrid(x, y) xy_sample = np.vstack([X_grid.ravel(), Y_grid.ravel()]).T Z = kde.score_samples(xy_sample).reshape(X_grid.shape) ax.contourf(X_grid, Y_grid, np.exp(Z), cmap=\'Blues\') ax.set_title(f\'Kernel: {kernel}, Bandwidth: {bandwidth}\') ax.scatter(X[:, 0], X[:, 1], s=5, c=\'red\') def visualize_kde_models(X, kernels, bandwidths): fig, axs = plt.subplots(len(kernels), len(bandwidths), figsize=(15, 15)) for i, kernel in enumerate(kernels): for j, bandwidth in enumerate(bandwidths): plot_kde(kernel, bandwidth, X, axs[i, j]) plt.tight_layout() plt.show() # Generate dataset X = generate_dataset() # Define kernels and bandwidths kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1.0] # Fit KDE models and evaluate log density log_density_results = fit_kde_models(X, kernels, bandwidths) visualize_kde_models(X, kernels, bandwidths) print(log_density_results)"},{"question":"You are tasked with building a small library module that uses asyncio to schedule and execute a series of tasks that simulate an asynchronous workflow. Parts of the workflow will include deliberate blocking operations to be handled properly using asyncio advanced features like `run_in_executor`. # Requirements: 1. **Main Entry Function**: - Name: `run_workflow` - Input: list of task functions. - Output: list of results of the task functions. - The function will: * Initialize the event loop. * Run each task in the provided list asynchronously. * Return the results of the tasks once they complete. 2. **Task Functions**: - You should write three distinct functions: `async_task`, `blocking_task`, and `callback_task`. - `async_task`: A coroutine that simulates a non-blocking I/O operation by using `asyncio.sleep`. - `blocking_task`: A function simulating a blocking, CPU-bound task. This should be run using `run_in_executor` to avoid blocking the event loop. - `callback_task`: A function scheduled to run safely from a different OS thread using `loop.call_soon_threadsafe`. 3. **Debugging and Logging**: - Enable debug mode for asyncio. - Configure logging to capture and display DEBUG level logs for asyncio. # Constraints: - Ensure the event loop is not blocked. - Handle possible exceptions gracefully and log them. - Ensure all task results are collected and returned even if some tasks fail. # Example Usage: ```python import asyncio import logging # Assume all necessary functions are defined as described async def run_workflow(tasks): loop = asyncio.get_running_loop() # Enabling debug mode loop.set_debug(True) logging.basicConfig(level=logging.DEBUG) # Scheduling tasks results = [] for task in tasks: if asyncio.iscoroutinefunction(task): results.append(await task()) else: result = await loop.run_in_executor(None, task) results.append(result) return results async def async_task(): await asyncio.sleep(1) return \\"Async Task Completed\\" def blocking_task(): import time time.sleep(1) return \\"Blocking Task Completed\\" def callback_task(): return \\"Callback Task Completed\\" if __name__ == \\"__main__\\": tasks = [async_task, blocking_task, callback_task] results = asyncio.run(run_workflow(tasks)) print(results) # Expected output after 1 second # [\\"Async Task Completed\\", \\"Blocking Task Completed\\", \\"Callback Task Completed\\"] ``` Please implement the provided functions accordingly.","solution":"import asyncio import logging async def run_workflow(tasks): Main entry function to run a list of tasks asynchronously. Args: tasks (list): A list of task functions (both async and blocking). Returns: list: Results of the executed tasks. loop = asyncio.get_running_loop() # Enabling debug mode loop.set_debug(True) logging.basicConfig(level=logging.DEBUG) results = [] for task in tasks: if asyncio.iscoroutinefunction(task): try: result = await task() results.append(result) except Exception as e: logging.error(f\\"Error executing async_task: {e}\\") results.append(None) else: try: result = await loop.run_in_executor(None, task) results.append(result) except Exception as e: logging.error(f\\"Error executing blocking_task: {e}\\") results.append(None) return results async def async_task(): A coroutine that simulates a non-blocking I/O operation. Returns: str: Message indicating task completion. await asyncio.sleep(1) return \\"Async Task Completed\\" def blocking_task(): A function simulating a blocking, CPU-bound task. Returns: str: Message indicating task completion. import time time.sleep(1) return \\"Blocking Task Completed\\" def callback_task(): A function that should be run safely from a different OS thread using loop.call_soon_threadsafe. Returns: str: Message indicating task completion. return \\"Callback Task Completed\\" if __name__ == \\"__main__\\": tasks = [async_task, blocking_task, callback_task] results = asyncio.run(run_workflow(tasks)) print(results)"},{"question":"Objective Demonstrate your understanding of the PyTorch `torch.compiler` module by implementing a function that compiles a given PyTorch model for performance optimization and verifies the compilation status. Problem Statement You are provided with a PyTorch neural network model. Your task is to: 1. Compile the given model using the `torch.compiler.compile` method for performance optimization. 2. Verify if the model is currently being compiled using `torch.compiler.is_compiling`. 3. Check if the model is being compiled using PyTorch Dynamo by utilizing `torch.compiler.is_dynamo_compiling`. 4. Return a dictionary containing the compilation statuses. Function Signature ```python def compile_and_check(model: torch.nn.Module) -> dict: Compiles the given PyTorch model and checks the compilation statuses. Args: - model (torch.nn.Module): The PyTorch neural network model to be compiled. Returns: - dict: A dictionary with keys \'is_compiling\' and \'is_dynamo_compiling\', representing the respective statuses of the model compilation. ``` Input - `model`: A PyTorch neural network model (instance of `torch.nn.Module`). Output - A dictionary with the following keys and corresponding boolean values: - `\'is_compiling\'`: `True` if the model is currently being compiled, otherwise `False`. - `\'is_dynamo_compiling\'`: `True` if the model is being compiled using PyTorch Dynamo, otherwise `False`. Example ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 1) def forward(self, x): return self.linear(x) model = SimpleModel() # Assuming torch.compiler allows such a direct compilation implementation compilation_statuses = compile_and_check(model) print(compilation_statuses) # Expected output (the actual output may depend on the specific environment and version): # {\'is_compiling\': True, \'is_dynamo_compiling\': False} ``` Constraints - Assume that `torch.compiler` and its related functions are available in your environment. - Ensure the function handles any potential issues that may arise during the compilation process. Notes - You may need to import relevant modules and functions from `torch` and `torch.compiler`. - Test your function with different models to ensure it works as expected.","solution":"import torch import torch.nn as nn import torch.compiler def compile_and_check(model: nn.Module) -> dict: Compiles the given PyTorch model and checks the compilation statuses. Args: - model (torch.nn.Module): The PyTorch neural network model to be compiled. Returns: - dict: A dictionary with keys \'is_compiling\' and \'is_dynamo_compiling\', representing the respective statuses of the model compilation. try: # Compile the model for performance optimization compiled_model = torch.compiler.compile(model) # Check compilation statuses is_compiling = torch.compiler.is_compiling() is_dynamo_compiling = torch.compiler.is_dynamo_compiling() # Returning the compilation statuses return { \'is_compiling\': is_compiling, \'is_dynamo_compiling\': is_dynamo_compiling } except Exception as e: print(\\"An error occurred during model compilation:\\", e) return { \'is_compiling\': False, \'is_dynamo_compiling\': False }"},{"question":"**Python Cell Objects and Closures** In Python, closures are a common way to handle functions with persistent states. A closure happens when a function retains the bindings of the free variables that exist in the context where it was created. Python\'s cell objects are used internally to manage these bindings. Given your understanding of closures and the concept of cell objects, you will implement a Python class that simulates the behavior of Python\'s internal cell objects using higher-level constructs. # Problem Implement a Python class `Cell` that can encapsulate a value and allow you to set and get this value, mimicking the low-level behavior described in the C-API documentation. # Class Specification `Cell` - **Constructor**: `__init__(self, value)` - **Input**: An initial value for the cell. - **Output**: None - **Method**: `get(self)` - **Output**: The current value of the cell. - **Method**: `set(self, value)` - **Output**: None. This method updates the value stored in the cell. # Constraints - You cannot use any existing mutable types (`list`, `dict`, etc.) to store the value; you should use an immutable type to simulate the low-level cell behavior. - The `set` method must release any current reference before setting the new value, although this has no direct API in Python; you should emulate it logically. # Example Usage ```python # Create a new cell with an initial value cell = Cell(10) # Retrieve the value print(cell.get()) # Output: 10 # Set a new value cell.set(20) # Retrieve the new value print(cell.get()) # Output: 20 ``` Implement the `Cell` class in Python along with any helper functions you deem necessary.","solution":"class Cell: def __init__(self, value): Initializes a new Cell with the given value. self._value = value def get(self): Returns the current value stored in the cell. return self._value def set(self, value): Updates the value stored in the cell. # Release current reference (simulated by setting to None) self._value = None # Set the new value self._value = value"},{"question":"# Question: ASCII Character Processor You are given a string containing various characters. Write a function `ascii_processor` that processes this string using the `curses.ascii` module functions. The function should categorize each character into one of the following categories: - \\"alnum\\" if it is alphanumeric. - \\"alpha\\" if it is alphabetic. - \\"digit\\" if it is a digit. - \\"space\\" if it is a whitespace character. - \\"cntrl\\" if it is a control character. - \\"punct\\" if it is a punctuation mark. - \\"other\\" for any other ASCII character. - \\"non-ascii\\" if it is a non-ASCII character. Additionally, the function should: 1. Convert each control character to its caret notation (e.g., `x01` becomes `\'^A\'`). 2. Return a dictionary where each key is a category, and the value is a list of characters belonging to that category in the order they appear in the string. Function Signature ```python from curses import ascii def ascii_processor(input_string: str) -> dict: pass ``` Input - `input_string`: A `str` containing the input string. Output - A `dict` with keys: \\"alnum\\", \\"alpha\\", \\"digit\\", \\"space\\", \\"cntrl\\", \\"punct\\", \\"other\\", and \\"non-ascii\\". Each key maps to a list of characters from the input string that belong to that category. Constraints - The input string length will be between 1 and 1000. Example ```python input_string = \\"Hello, World! x01\\" output = ascii_processor(input_string) # Expected output: # { # \'alnum\': [\'H\', \'e\', \'l\', \'l\', \'o\', \'W\', \'o\', \'r\', \'l\', \'d\'], # \'alpha\': [\'H\', \'e\', \'l\', \'l\', \'o\', \'W\', \'o\', \'r\', \'l\', \'d\'], # \'digit\': [], # \'space\': [\' \', \' \'], # \'cntrl\': [\'^A\'], # \'punct\': [\',\', \'!\'], # \'other\': [], # \'non-ascii\': [] # } ``` Performance Requirements - The function should be optimized to handle the upper constraint efficiently.","solution":"from curses import ascii def ascii_processor(input_string: str) -> dict: result = { \'alnum\': [], \'alpha\': [], \'digit\': [], \'space\': [], \'cntrl\': [], \'punct\': [], \'other\': [], \'non-ascii\': [] } for char in input_string: if ascii.isalnum(char): result[\'alnum\'].append(char) if ascii.isalpha(char): result[\'alpha\'].append(char) if ascii.isdigit(char): result[\'digit\'].append(char) if ascii.isspace(char): result[\'space\'].append(char) if ascii.iscntrl(char): result[\'cntrl\'].append(ascii.unctrl(char)) if ascii.ispunct(char): result[\'punct\'].append(char) if not ascii.isascii(char): result[\'non-ascii\'].append(char) elif not (ascii.isalnum(char) or ascii.isalpha(char) or ascii.isdigit(char) or ascii.isspace(char) or ascii.iscntrl(char) or ascii.ispunct(char)): result[\'other\'].append(char) return result"},{"question":"# XML DOM Manipulation In this coding challenge, you are required to implement a function to process and manipulate an XML document using the `xml.dom` module. Your task is to write a function `modify_xml_document` that will: 1. Parse an XML string into a DOM document. 2. Find all elements with a specified tag name. 3. Modify the text content of these elements. 4. Add a new attribute with a specified name and value to these elements. 5. Serialize the modified DOM document back into an XML string. # Function Signature ```python def modify_xml_document(xml_string: str, tag_name: str, new_text: str, attr_name: str, attr_value: str) -> str: pass ``` # Input - `xml_string` (str): A string representation of the XML document to be processed. - `tag_name` (str): The name of the XML elements to be found and modified. - `new_text` (str): The new text content to be set for the found elements. - `attr_name` (str): The name of the new attribute to be added to the found elements. - `attr_value` (str): The value of the new attribute to be added to the found elements. # Output - (str): A string representation of the modified XML document. # Requirements 1. You must use the `xml.dom.minidom` module for parsing and serializing the XML document. 2. Avoid using external XML libraries; stick to the `xml.dom` provided functionalities. 3. Ensure that the output XML string is properly formatted and preserves the structure of the original document, except for the modifications. # Example ```python xml_str = <root> <item>Original</item> <item>Original</item> <other>Do not modify</other> </root> tag_name = \\"item\\" new_text = \\"Modified\\" attr_name = \\"status\\" attr_value = \\"updated\\" modified_xml = modify_xml_document(xml_str, tag_name, new_text, attr_name, attr_value) print(modified_xml) ``` Expected output: ```xml <root> <item status=\\"updated\\">Modified</item> <item status=\\"updated\\">Modified</item> <other>Do not modify</other> </root> ``` # Constraints - The `xml_string` will always contain valid XML content. - The specified `tag_name` will match zero or more elements within the XML. - The new attribute name and value will be valid according to XML naming conventions.","solution":"from xml.dom.minidom import parseString def modify_xml_document(xml_string: str, tag_name: str, new_text: str, attr_name: str, attr_value: str) -> str: Parses an XML string, modifies elements with a specified tag name, changes their text content, adds a new attribute to these elements, and returns the modified XML string. dom = parseString(xml_string) elements = dom.getElementsByTagName(tag_name) for element in elements: # Modify the text content if element.firstChild: element.firstChild.nodeValue = new_text else: text_node = dom.createTextNode(new_text) element.appendChild(text_node) # Add a new attribute element.setAttribute(attr_name, attr_value) return dom.toxml() # Example Usage xml_str = <root> <item>Original</item> <item>Original</item> <other>Do not modify</other> </root> tag_name = \\"item\\" new_text = \\"Modified\\" attr_name = \\"status\\" attr_value = \\"updated\\" modified_xml = modify_xml_document(xml_str, tag_name, new_text, attr_name, attr_value) print(modified_xml)"},{"question":"Objective The objective of this question is to test your understanding of PyTorch, TorchScript, and their limitations by converting a given PyTorch model into a TorchScript-compliant version. Problem Description You are given a PyTorch neural network model that aims to classify images. Your task is to: 1. Identify and replace any unsupported PyTorch constructs in the model with TorchScript-compliant alternatives. 2. Compile the modified model using TorchScript. 3. Write code to test the model\'s functionality after conversion to ensure it behaves as expected. Input - A PyTorch model class `ImageClassifier` which may contain unsupported constructs for TorchScript. ```python import torch import torch.nn as nn class ImageClassifier(nn.Module): def __init__(self): super(ImageClassifier, self).__init__() # Example Layers self.conv1 = nn.Conv2d(3, 16, 3, padding=1) self.rnn = nn.RNN(16*16*16, 128, batch_first=True) # This is unsupported in TorchScript self.fc = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = x.view(x.size(0), -1) x, _ = self.rnn(x.unsqueeze(1)) x = self.fc(x[:, -1]) return x ``` Output - A TorchScript-compliant version of the `ImageClassifier` model. - A test script demonstrating the model\'s functionality. Constraints - You may not change the overall architecture of the model, but you must replace unsupported constructs. - The provided model should classify images of shape `(3, 16, 16)`. - Ensure that the converted model performs inference correctly. Performance Requirements - The modified model must be TorchScript serializable. - The output of the test script should match the expected output shape for given dummy input data. Example Test Case ```python def test_model(): model = ImageClassifier() model.eval() example_input = torch.rand(1, 3, 16, 16) # Convert to TorchScript scripted_model = torch.jit.script(model) # Test the model output = scripted_model(example_input) assert output.shape == torch.Size([1, 10]), \\"Output shape mismatch\\" print(\\"Test passed.\\") test_model() ``` Your task is to modify the `ImageClassifier` class and the `test_model` function to ensure the script runs without errors and passes the test.","solution":"import torch import torch.nn as nn class ImageClassifier(nn.Module): def __init__(self): super(ImageClassifier, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, padding=1) self.conv2 = nn.Conv2d(16, 32, 3, padding=1) self.fc1 = nn.Linear(32*16*16, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = x.view(x.size(0), -1) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Ensure the model can be scripted model = ImageClassifier() scripted_model = torch.jit.script(model)"},{"question":"Objective Implement a function in Python that accesses and processes the annotations of a given object, following best practices for different Python versions as specified in the provided documentation. Description Write a Python function named `process_annotations(obj, version)`. This function should: 1. Accept two parameters: - `obj`: any Python object. - `version`: a string representing the Python version in `major.minor` format (e.g., \'3.10\'). 2. Access the `__annotations__` attribute of `obj` following the best practices for the specified `version`. 3. Return the annotations dictionary if it exists, or `None` if it does not. Constraints 1. The `version` parameter will always be a valid Python version of the form \'x.y\' where x and y are integers. 2. The function should handle both classes and other objects following the best practices outlined. Example ```python class MyClass: a: int = 10 def my_function(x: \\"int\\") -> \\"str\\": return str(x) # Example usage print(process_annotations(MyClass, \'3.10\')) # Output: {\'a\': \'int\'} print(process_annotations(my_function, \'3.10\')) # Output: {\'x\': \'int\', \'return\': \'str\'} print(process_annotations(my_function, \'3.9\')) # May vary based on further implementation details ``` Requirements - Use `inspect.get_annotations()` in Python 3.10 and newer. - Use `getattr()` and other best practice methods for versions prior to 3.10, as described in the provided documentation. - Ensure the function properly handles corner cases such as classes without annotations, callables without `__annotations__`, and handling stringized annotations where applicable. Implement your solution below: ```python import inspect import sys def process_annotations(obj, version): # Convert version to a tuple of integers for comparison version_tuple = tuple(map(int, version.split(\'.\'))) if version_tuple >= (3, 10): # Use inspect.get_annotations() for Python 3.10 and newer return inspect.get_annotations(obj) else: # For Python 3.9 and older if isinstance(obj, type): # Best practice for classes in Python 3.9 and older return obj.__dict__.get(\'__annotations__\', None) else: # Best practice for functions, other callables, and modules return getattr(obj, \'__annotations__\', None) ``` Notes - You are encouraged to carefully examine the provided documentation to ensure your solution adheres to best practices and properly handles various edge cases. - Make sure to include any necessary imports and error handling as appropriate.","solution":"import inspect def process_annotations(obj, version): Access and process the annotations of a given object, following best practices for different Python versions as specified. Parameters: obj: any Python object. version: a string representing the Python version in `major.minor` format (e.g., \'3.10\'). Returns: dictionary: containing annotations if exists. None: if annotations not present. # Convert version to a tuple of integers for comparison version_tuple = tuple(map(int, version.split(\'.\'))) if version_tuple >= (3, 10): # Use inspect.get_annotations() for Python 3.10 and newer return inspect.get_annotations(obj) else: # For Python 3.9 and older if isinstance(obj, type): # Best practice for classes in Python 3.9 and older return obj.__dict__.get(\'__annotations__\', None) else: # Best practice for functions, other callables, and modules return getattr(obj, \'__annotations__\', None)"},{"question":"You are provided with a dataset consisting of large integers that require computationally intensive operations to determine certain properties of these numbers. Your task is to implement a Python function that uses both thread-based and process-based parallelism to improve the performance of your computations. Problem Statement Implement a function `parallel_prime_check(integers: List[int]) -> Dict[int, bool]` that: 1. Uses both `ThreadPoolExecutor` and `ProcessPoolExecutor` to determine if each number in the provided list of integers is prime. 2. Returns a dictionary where keys are integers from the input list and values are booleans indicating if the integer is prime. Constraints - Input list (`integers`) can be up to (10^5) elements long. - Each integer in the list is guaranteed to be a positive number up to (10^{12}). Requirements 1. Use `ThreadPoolExecutor` for I/O-bound operations, if necessary. 2. Use `ProcessPoolExecutor` for CPU-bound operations to utilize multiple processors efficiently. 3. Ensure no deadlocks or redundant computations during parallel execution. Example ```python from typing import List, Dict def parallel_prime_check(integers: List[int]) -> Dict[int, bool]: # Your code here # Example usage integers = [112272535095293, 112582705942171, 4, 115280095190773] print(parallel_prime_check(integers)) ``` Expected output: ```python { 112272535095293: True, 112582705942171: True, 4: False, 115280095190773: True } ``` Constraints & Considerations 1. Avoid global interpreter lock issues when using `ThreadPoolExecutor`. 2. Ensure efficient splitting of the workload to prevent bottlenecks. 3. Handle exceptions if a computational task fails for any integer. Performance Requirements 1. Aim for maximizing parallelism while keeping the main program responsive. 2. Consider the chunk size carefully when using `map` with `ProcessPoolExecutor` for larger lists. Start your implementation below this line: ```python from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor from typing import List, Dict import math def is_prime(n: int) -> bool: if n < 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.sqrt(n)) + 1 for i in range(3, sqrt_n, 2): if n % i == 0: return False return True def parallel_prime_check(integers: List[int]) -> Dict[int, bool]: results = {} with ProcessPoolExecutor() as executor: prime_checks = {executor.submit(is_prime, num): num for num in integers} for future in concurrent.futures.as_completed(prime_checks): num = prime_checks[future] try: results[num] = future.result() except Exception as exc: print(f\\"{num} generated an exception: {exc}\\") return results # Example usage integers = [112272535095293, 112582705942171, 4, 115280095190773] print(parallel_prime_check(integers)) ```","solution":"from concurrent.futures import ProcessPoolExecutor, as_completed from typing import List, Dict import math def is_prime(n: int) -> bool: Determines if the given number n is a prime number. if n < 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.sqrt(n)) + 1 for i in range(3, sqrt_n, 2): if n % i == 0: return False return True def parallel_prime_check(integers: List[int]) -> Dict[int, bool]: Use ProcessPoolExecutor to check if numbers in the list are prime. Returns a dictionary with the numbers as keys, and booleans as values indicating primality. results = {} with ProcessPoolExecutor() as executor: futures = {executor.submit(is_prime, number): number for number in integers} for future in as_completed(futures): num = futures[future] try: results[num] = future.result() except Exception as exc: print(f\\"{num} generated an exception: {exc}\\") return results"},{"question":"You are provided with the task of analyzing and transforming a dataset using Python comprehensions and generator functions. Your goal is to implement a function `transform_data` that processes a list of dictionaries. Each dictionary represents a record with multiple attributes. Input - A list of dictionaries `data`. Each dictionary contains the following keys: - `\'values\'`: List of integers. - `\'identifier\'`: A unique identifier for the record, given as a string. - `\'active\'`: A boolean indicating if the record is active. - `\'multiplier\'`: A float that is used to transform the values when the record is active. Output - A dictionary with the following structure: - `\'summary\'`: Another dictionary containing: - `\'total_records\'`: Total number of records. - `\'active_records\'`: Total number of active records. - `\'inactive_records\'`: Total number of inactive records. - `\'transformed_values\'`: A list containing the transformed values from all active records. Each value should be transformed by multiplying it with the `\'multiplier\'` of its respective record. - `\'unique_identifiers\'`: A set of unique identifiers for all records. Constraints - The function should use generator expressions for efficient processing where appropriate. - The function should employ comprehensions to construct the required lists and sets. Example ```python # Sample Input data = [ { \'values\': [1, 2, 3], \'identifier\': \'A1\', \'active\': True, \'multiplier\': 1.5 }, { \'values\': [4, 5], \'identifier\': \'B2\', \'active\': False, \'multiplier\': 2.0 }, { \'values\': [6, 7, 8], \'identifier\': \'C3\', \'active\': True, \'multiplier\': 0.5 } ] # Expected Output { \'summary\': { \'total_records\': 3, \'active_records\': 2, \'inactive_records\': 1 }, \'transformed_values\': [1.5, 3.0, 4.5, 3.0, 3.5, 4.0], \'unique_identifiers\': {\'A1\', \'B2\', \'C3\'} } ``` Guidelines 1. **Read the documentation section provided above carefully.** You will need to use comprehensions and generator expressions. 2. Make sure to handle both active and inactive records correctly. 3. Ensure that you do not use large memory-consuming structures unnecessarily. 4. Use effective error handling where appropriate. Implement the function `transform_data(data: list) -> dict` to achieve the desired transformation. Notes - This question aims to test your understanding of comprehensions, generator expressions, and their efficient usage. ```python def transform_data(data): # Your code here pass ```","solution":"def transform_data(data): Transforms the input data by analyzing active and inactive records, and calculating transformed values and unique identifiers. Args: data (list): List of dictionaries with keys - \'values\', \'identifier\', \'active\', and \'multiplier\'. Returns: dict: A dictionary containing \'summary\', \'transformed_values\', and \'unique_identifiers\'. total_records = len(data) active_records = sum(1 for record in data if record[\'active\']) inactive_records = total_records - active_records summary = { \'total_records\': total_records, \'active_records\': active_records, \'inactive_records\': inactive_records } transformed_values = [ value * record[\'multiplier\'] for record in data if record[\'active\'] for value in record[\'values\'] ] unique_identifiers = {record[\'identifier\'] for record in data} return { \'summary\': summary, \'transformed_values\': transformed_values, \'unique_identifiers\': unique_identifiers }"},{"question":"# Problem: Sorted List Manipulations You are given a list of tuples where each tuple consists of two elements: a string (name) and an integer (timestamp). This list is not initially sorted. Your goal is to manage this list such that it remains sorted in ascending order by the timestamp after each insertion. Additionally, you need to implement search functionality to find specific tuples based on the timestamp. Implement the following functions: 1. **insert_record(name: str, timestamp: int) -> List[Tuple[str, int]]**: - Inserts a tuple `(name, timestamp)` into the sorted list while maintaining the sorted order. - Returns the updated list. 2. **find_first_gte(timestamp: int) -> Tuple[str, int]**: - Finds and returns the first tuple with a timestamp greater than or equal to the given timestamp. - If no such tuple exists, raise a `ValueError`. 3. **find_last_lt(timestamp: int) -> Tuple[str, int]**: - Finds and returns the last tuple with a timestamp less than the given timestamp. - If no such tuple exists, raise a `ValueError`. **Hints**: - Use the bisect module for efficiently finding insertion points and searching the sorted list. **Example Usage**: ```python # Initialize the list record_list = [] # Insert records record_list = insert_record(\\"event1\\", 10) record_list = insert_record(\\"event2\\", 5) record_list = insert_record(\\"event3\\", 15) print(record_list) # Output: [(\'event2\', 5), (\'event1\', 10), (\'event3\', 15)] assert find_first_gte(10) == (\'event1\', 10) assert find_last_lt(10) == (\'event2\', 5) # Edge cases try: find_first_gte(20) except ValueError: print(\\"No record found with timestamp >= 20\\") try: find_last_lt(5) except ValueError: print(\\"No record found with timestamp < 5\\") ``` # Constraints: - Timestamps are unique. - The input list may contain upto (10^6) records. - Performance should leverage the efficiency of the bisect module\'s functions.","solution":"from bisect import bisect_left, bisect_right record_list = [] def insert_record(name: str, timestamp: int) -> list: Inserts a tuple (name, timestamp) into the sorted list while maintaining the sorted order. global record_list index = bisect_left([record[1] for record in record_list], timestamp) record_list.insert(index, (name, timestamp)) return record_list def find_first_gte(timestamp: int) -> tuple: Finds and returns the first tuple with a timestamp greater than or equal to the given timestamp. If no such tuple exists, raises a ValueError. index = bisect_left([record[1] for record in record_list], timestamp) if index < len(record_list) and record_list[index][1] >= timestamp: return record_list[index] raise ValueError(\\"No record found with timestamp >= {}\\".format(timestamp)) def find_last_lt(timestamp: int) -> tuple: Finds and returns the last tuple with a timestamp less than the given timestamp. If no such tuple exists, raises a ValueError. index = bisect_left([record[1] for record in record_list], timestamp) if index > 0: return record_list[index - 1] raise ValueError(\\"No record found with timestamp < {}\\".format(timestamp))"},{"question":"**Custom Interactive Console Implementation** # Objective: Create a custom interactive console using the `code` module that supports additional commands beyond the standard Python commands. Your console should be able to handle user input, execute basic Python commands, and introduce a new command `:hello` which prints \\"Hello, [name]!\\" where `[name]` is the user\'s name. # Instructions: 1. **Define a class `CustomConsole` that inherits from `code.InteractiveConsole`**: - Implement `__init__(self, locals=None, filename=\\"<console>\\")` to initialize your custom console. - Implement `runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\")` to handle the execution of code. 2. **Extend functionality to recognize and execute the `:hello [name]` command**: - If the user inputs the command `:hello [name]`, your console should print \\"Hello, [name]!\\". - For standard Python code, it should behave like the regular interactive interpreter. # Requirements: - **Input and Output**: - The console should accept inputs in the same way a standard Python interactive shell does. - The output should be the result of Python expressions or the custom message for the `:hello` command. - **Constraints**: - The user name provided with `:hello` should be a valid Python identifier (variable name rules apply). - If the user inputs an invalid command or expression, the console should display an appropriate Python error message. # Example: ``` >>> :hello Alice Hello, Alice! >>> 1 + 1 2 >>> def greet(): ... print(\\"Greetings!\\") ... >>> greet() Greetings! >>> :hello Bob Hello, Bob! >>> quit() ``` # Notes: - You may assume the user will enter commands until they type `exit()` or `quit()`, which will terminate the console. - Handle multi-line Python commands correctly. Implementing this custom console requires a solid understanding of Python\'s `code` and `codeop` modules, as well as familiarity with interactive interpreter mechanics. # Code Skeleton: ```python import code class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): # Your initialization code here pass def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): # Your custom implementation here pass if __name__ == \\"__main__\\": console = CustomConsole() console.interact() ```","solution":"import code import re class CustomConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals=locals, filename=filename) def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): hello_command_match = re.match(r\'^:hello (w+)\', source.strip()) if hello_command_match: name = hello_command_match.group(1) if name.isidentifier(): print(f\\"Hello, {name}!\\") return False # Indicate that the input has been handled else: print(f\\"Error: {name} is not a valid identifier\\") return False else: return super().runsource(source, filename, symbol) if __name__ == \\"__main__\\": console = CustomConsole() console.interact()"},{"question":"**Question:** You are required to create a Python function `fetch_secure_data` that sends a POST request to a given URL using basic HTTP authentication. Your task is to implement the function that does the following: 1. **Function signature** ```python def fetch_secure_data(url: str, data: dict, username: str, password: str) -> str: ``` 2. **Parameters:** - `url`: The URL to which the POST request will be sent. - `data`: A dictionary containing the data to be sent in the POST request. - `username`: The username for HTTP Basic Authentication. - `password`: The password for HTTP Basic Authentication. 3. **Functionality:** - Encode the dictionary `data` to a format suitable for sending in a POST request. - Use the `HTTPBasicAuthHandler` to handle the HTTP Basic Authentication with the provided username and password. - Send a POST request to the specified `url` containing the encoded data. - Properly handle exceptions such as `HTTPError` and `URLError`. If an error occurs, return a string describing the error. - Return the response from the server in string format. 4. **Output:** - The function should return the body of the server\'s response as a string. 5. **Constraints:** - Use only the `urllib.request` module for handling HTTP requests. - Ensure the function properly manages the creation of handlers and opening the URL. **Example Usage:** ```python url = \\"http://example.com/api/data\\" data = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} username = \\"your_username\\" password = \\"your_password\\" response = fetch_secure_data(url, data, username, password) print(response) ``` **Implementation Tips:** - Use `urllib.parse.urlencode` to encode `data` before sending it. - Utilize `HTTPBasicAuthHandler` for creating the authentication handler. - Handle possible exceptions by catching `HTTPError` and `URLError` and returning appropriate error messages.","solution":"import urllib.request import urllib.parse import urllib.error from base64 import b64encode def fetch_secure_data(url: str, data: dict, username: str, password: str) -> str: Sends a POST request to the specified URL with HTTP Basic Authentication. Parameters: - url: The URL to which the POST request will be sent. - data: A dictionary containing the data to be sent in the POST request. - username: The username for HTTP Basic Authentication. - password: The password for HTTP Basic Authentication. Returns: - The body of the server\'s response as a string. # Encode the data into the appropriate format for a POST request encoded_data = urllib.parse.urlencode(data).encode(\'utf-8\') # Create a request object request = urllib.request.Request(url, data=encoded_data) # Encode the username and password for HTTP Basic Authentication auth_str = f\'{username}:{password}\' encoded_auth = b64encode(auth_str.encode(\'utf-8\')).decode(\'utf-8\') request.add_header(\'Authorization\', f\'Basic {encoded_auth}\') try: # Open the URL and get the response with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\'HTTPError: {e.code} {e.reason}\' except urllib.error.URLError as e: return f\'URLError: {e.reason}\'"},{"question":"# Question **Objective:** Create a comprehensive visualization using Seaborn based on a transformed dataset to assess your understanding of Seaborn\'s advanced functionality. **Description:** 1. **Data Preparation:** - Load the \'penguins\' dataset from Seaborn\'s built-in datasets. - Drop rows with missing values. - Add a new column `body_mass_per_flipper` that contains the body mass divided by flipper length for each penguin. 2. **Data Transformation:** - Group the data by the species of the penguins. - Calculate the mean of each numerical column for each species. 3. **Visualization:** - Create a multi-plot layout with Seaborn to display pairwise relationships between `bill_length_mm`, `bill_depth_mm`, and the new `body_mass_per_flipper` column. - Use different colors to distinguish between the species. **Constraints & Requirements:** - The plots should be clearly labeled and the figure should have a title. - Use Seaborn\'s `seaborn.objects` interface. - Ensure that the visualizations are clear and properly formatted. **Input:** - No user input is required; use the built-in `penguins` dataset. **Output:** - A visual representation of the requested plots. ```python # Expected input: None # Expected output: A figure containing the desired visualization import seaborn.objects as so from seaborn import load_dataset def create_penguin_plots(): # Load the \'penguins\' dataset penguins = load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna() # Add a new column \'body_mass_per_flipper\' penguins[\'body_mass_per_flipper\'] = penguins[\'body_mass_g\'] / penguins[\'flipper_length_mm\'] # Group data by species and calculate the mean of numerical columns for each species penguin_means = penguins.groupby(\'species\').mean().reset_index() # Create the pairwise plot layout using seaborn objects interface p = ( so.Plot(penguin_means) .pair( x=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"body_mass_per_flipper\\"], y=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"body_mass_per_flipper\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Add Paths layer for plot p.add(so.Paths(color=\\"species\\")) # Set figure title p.figure.suptitle(\'Pairwise Relationships for Penguin Species\') # Show plot p.show() # Call the function to execute the visualization create_penguin_plots() ``` **Notes:** - Ensure the function reads the Penguins dataset correctly and handles missing values appropriately. - Test the code thoroughly to confirm that the visualizations are correct and meet the requirements. - Make sure the figure title and labels are clear and descriptive.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plots(): # Load the \'penguins\' dataset penguins = load_dataset(\\"penguins\\") # Drop rows with missing values penguins = penguins.dropna() # Add a new column \'body_mass_per_flipper\' penguins[\'body_mass_per_flipper\'] = penguins[\'body_mass_g\'] / penguins[\'flipper_length_mm\'] # Group data by species and calculate the mean of numerical columns for each species penguin_means = penguins.groupby(\'species\').mean(numeric_only=True).reset_index() # Create the pairwise plot layout using seaborn objects interface p = ( so.Plot(penguin_means) .pair( x=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"body_mass_per_flipper\\"], y=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"body_mass_per_flipper\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Add Paths layer for plot p.add(so.Path(color=\\"species\\")).label(title=\\"Pairwise Relationships for Penguin Species\\") # Show plot p.show() # Call the function to execute the visualization create_penguin_plots()"},{"question":"**Coding Assessment Question: Serialization and Deserialization in PyTorch** **Objective:** Your task is to demonstrate your understanding of serialization and deserialization in PyTorch by saving and loading tensors and PyTorch modules effectively. Specifically, you need to preserve tensor views and module states accurately. **Instructions:** 1. **Save and Load Tensors:** - Create a tensor `original_tensor` with values ranging from 1 to 20. - Create a view `even_tensor` that contains every second element of the original tensor. - Save both tensors into a file called `tensors.pt`. - Load the saved tensors back into variables `loaded_original` and `loaded_even`. - Modify the view tensor (`loaded_even`), multiplying its values by 10. - Ensure the modification is reflected in the original tensor (`loaded_original`). 2. **Save and Load a PyTorch Module:** - Define a custom PyTorch module named `Net` with two linear layers. - Instantiate the model and create its `state_dict`. - Save the `state_dict` to a file called `net_state.pt`. - Load the saved `state_dict` into a new instance of `Net`. - Ensure the loaded state matches the saved state. **Constraints:** - Use only built-in PyTorch functions for serialization (i.e., `torch.save` and `torch.load`). - Ensure tensor views and storage sharing is preserved. - Use the `state_dict` approach for saving and loading the model\'s state. **Expected Input and Output Format:** - **Input:** No direct input is provided. Instead, you will create tensors and models as described. - **Output:** Print the `loaded_original` tensor after modifying `loaded_even` to verify the view relationship. Print the state dictionary of the new model instance to verify it matches the original model\'s state. **Example:** ```python import torch import torch.nn as nn # Part 1: Save and Load Tensors original_tensor = torch.arange(1, 21) even_tensor = original_tensor[1::2] torch.save((original_tensor, even_tensor), \'tensors.pt\') loaded_original, loaded_even = torch.load(\'tensors.pt\') loaded_even *= 10 print(\\"Loaded original after modification:\\", loaded_original) # Part 2: Save and Load a PyTorch Module class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x model = Net() torch.save(model.state_dict(), \'net_state.pt\') new_model = Net() new_model.load_state_dict(torch.load(\'net_state.pt\')) print(\\"State dict of the new model:\\", new_model.state_dict()) ``` Ensure your implementation meets the requirements and produces the correct outputs as demonstrated in the example.","solution":"import torch import torch.nn as nn # Part 1: Save and Load Tensors def save_and_load_tensors(): # Create the original tensor and its view original_tensor = torch.arange(1, 21) even_tensor = original_tensor[1::2] # Save both tensors torch.save((original_tensor, even_tensor), \'tensors.pt\') # Load the saved tensors loaded_original, loaded_even = torch.load(\'tensors.pt\') # Modify the loaded view tensor loaded_even *= 10 return loaded_original # Part 2: Save and Load a PyTorch Module class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def save_and_load_model(): # Instantiate the model and save its state_dict model = Net() torch.save(model.state_dict(), \'net_state.pt\') # Instantiate a new model and load the saved state_dict new_model = Net() state_dict = torch.load(\'net_state.pt\') new_model.load_state_dict(state_dict) return state_dict, new_model.state_dict()"},{"question":"# Question: Implement a File Transmission System using uuencode You have been tasked with writing a function that simulates the transfer of binary files over a text-only connection using the uuencode format. Implement two functions, `transmit_file` and `receive_file`, to demonstrate this. **Function 1: transmit_file** This function should encode a given binary file using uuencode and store the encoded result in another file. ```python def transmit_file(input_file_path: str, encoded_output_path: str, backtick: bool = False): Encode the input binary file to uuencode format and save it to the output file. Parameters: input_file_path (str): Path to the input binary file. encoded_output_path (str): Path to the output file where encoded text will be stored. backtick (bool): If True, will use backtick for zeroes instead of space. Default is False. Raises: FileNotFoundError: If the input file does not exist. ``` **Function 2: receive_file** This function should decode a uuencoded file and store the decoded binary data in another file. ```python def receive_file(encoded_input_path: str, decoded_output_path: str, quiet: bool = False): Decode the uuencoded input file and save the binary result to the output file. Parameters: encoded_input_path (str): Path to the input file which contains uuencoded text. decoded_output_path (str): Path to the output file where decoded binary data will be stored. quiet (bool): If True, will suppress warnings. Default is False. Raises: FileNotFoundError: If the input file does not exist. uu.Error: If there are issues with decoding the file. ``` # Constraints and Requirements: 1. **Inputs**: File paths should be strings representing valid file system paths. 2. **Outputs**: Encoded/decoded data must be correctly written to the specified output files. 3. **Error handling**: Proper exceptions should be raised for file not found, and correct handling of `uu.Error`. 4. **Performance**: The functions should handle large files efficiently, implying the need for optimal reading and writing operations. You can assume the `uu` module functions `uu.encode` and `uu.decode` are available to you. Here’s an example structure for `transmit_file` and `receive_file` implementations to get started: ```python import uu def transmit_file(input_file_path: str, encoded_output_path: str, backtick: bool = False): pass # Your implementation here def receive_file(encoded_input_path: str, decoded_output_path: str, quiet: bool = False): pass # Your implementation here ``` # Examples and Tests: 1. Transmitting (Encoding) a file: ```python transmit_file(\\"sample.bin\\", \\"encoded.uue\\") ``` - This should read `sample.bin`, uuencode it, and write the encoded text to `encoded.uue`. 2. Receiving (Decoding) a file: ```python receive_file(\\"encoded.uue\\", \\"decoded_sample.bin\\") ``` - This should read `encoded.uue`, decode it, and write the binary data to `decoded_sample.bin`. 3. Handling errors in transmission or receiving is crucial. Ensure to write tests which include non-existing files and bad format scenarios. By implementing these functions, you demonstrate understanding of file operations, error handling, and handling data transmission using encoding/decoding techniques in Python.","solution":"import uu import os def transmit_file(input_file_path: str, encoded_output_path: str, backtick: bool = False): # Check if input file exists if not os.path.exists(input_file_path): raise FileNotFoundError(f\\"The input file {input_file_path} does not exist.\\") # Open input and output files with open(input_file_path, \'rb\') as input_file, open(encoded_output_path, \'wb\') as encoded_output: uu.encode(input_file, encoded_output, name=os.path.basename(input_file_path), mode=0o644, backtick=backtick) def receive_file(encoded_input_path: str, decoded_output_path: str, quiet: bool = False): # Check if encoded input file exists if not os.path.exists(encoded_input_path): raise FileNotFoundError(f\\"The encoded input file {encoded_input_path} does not exist.\\") # Open input and output files try: with open(encoded_input_path, \'rb\') as encoded_input, open(decoded_output_path, \'wb\') as decoded_output: uu.decode(encoded_input, decoded_output, quiet=quiet) except uu.Error as e: raise uu.Error(f\\"Failed to decode the file {encoded_input_path}: {str(e)}\\")"},{"question":"# URL Data Fetching and Parsing Challenge Objective: Write a Python function using the \\"urllib\\" package that performs the following operations: 1. **Fetch data from a given URL**: The function should open and read the content of the given URL. 2. **Handle errors gracefully**: Implement error handling to manage potential issues (e.g., HTTP errors, URL errors). 3. **Parse URL components**: Extract and return the scheme, hostname, path, and query parameters of the URL. 4. **Optional - Check robots.txt** *(bonus)*: Before fetching the data, check the \\"robots.txt\\" file of the website to verify if fetching the provided URL is allowed. Function Signature: ```python def fetch_and_parse_url(url: str) -> dict: Fetches data from the given URL, parses the URL components, and returns a dictionary with the URL components and the fetched data. Args: url (str): The URL to fetch and parse. Returns: dict: A dictionary containing the fetched data and URL components: { \\"scheme\\": <scheme of the URL>, \\"hostname\\": <hostname of the URL>, \\"path\\": <path of the URL>, \\"query\\": <query parameters of the URL as a dictionary>, \\"data\\": <fetched data as a string> } Raises: ValueError: If fetching the data is disallowed by robots.txt or if any errors occur. pass ``` Constraints and Considerations: - **Input URL**: Assume the URL is a valid string, but it can be any form (with or without scheme, path, or query parameters). - **Error Handling**: Properly handle any exceptions that could be raised by `urllib.request` (e.g., HTTPError, URLError) and raise a `ValueError` with an appropriate message. - **Fetching robots.txt**: If implementing the bonus, the function should fetch and check the \\"robots.txt\\" file of the URL\'s domain to determine if the URL can be accessed. If not allowed, raise a `ValueError`. Example: ```python url = \\"https://www.example.com/path?name=test\\" result = fetch_and_parse_url(url) print(result) ``` Expected output: ```python { \\"scheme\\": \\"https\\", \\"hostname\\": \\"www.example.com\\", \\"path\\": \\"/path\\", \\"query\\": {\\"name\\": \\"test\\"}, \\"data\\": \\"<HTML content of the fetched URL...>\\" } ``` Note: - For testing purposes, ensure the URLs used are accessible and have valid HTML content. - The implementation should demonstrate clear understanding and usage of the \\"urllib\\" package, particularly `urllib.request`, `urllib.error`, and `urllib.parse`.","solution":"import urllib.request import urllib.error import urllib.parse def fetch_and_parse_url(url: str) -> dict: Fetches data from the given URL, parses the URL components, and returns a dictionary with the URL components and the fetched data. Args: url (str): The URL to fetch and parse. Returns: dict: A dictionary containing the fetched data and URL components: { \\"scheme\\": <scheme of the URL>, \\"hostname\\": <hostname of the URL>, \\"path\\": <path of the URL>, \\"query\\": <query parameters of the URL as a dictionary>, \\"data\\": <fetched data as a string> } Raises: ValueError: If fetching the data is disallowed by robots.txt or if any errors occur. try: # Parse the URL components parsed_url = urllib.parse.urlparse(url) scheme = parsed_url.scheme hostname = parsed_url.hostname path = parsed_url.path query = urllib.parse.parse_qs(parsed_url.query) # Check robots.txt to see if the URL is disallowed robots_txt_url = f\\"{scheme}://{hostname}/robots.txt\\" try: with urllib.request.urlopen(robots_txt_url) as response: robots_txt = response.read().decode(\'utf-8\').splitlines() for line in robots_txt: if line.startswith(\\"Disallow:\\"): disallowed_path = line.split(\\":\\", 1)[1].strip() if path.startswith(disallowed_path): raise ValueError(f\\"Fetching the URL is disallowed by robots.txt: {disallowed_path}\\") except urllib.error.URLError: # If robots.txt is not found, we assume that all URLs are allowed pass # Fetch the URL content with urllib.request.urlopen(url) as response: data = response.read().decode(\'utf-8\') return { \\"scheme\\": scheme, \\"hostname\\": hostname, \\"path\\": path, \\"query\\": query, \\"data\\": data } except (urllib.error.HTTPError, urllib.error.URLError) as e: raise ValueError(f\\"Error fetching URL: {e}\\")"},{"question":"**Attention Mechanism Implementation in PyTorch** You are tasked with implementing a simplified attention mechanism in PyTorch. Attention mechanisms are widely used in models such as Transformers for natural language processing and other sequential tasks. # Task Description Implement the following functions to build a simplified attention mechanism: 1. `scaled_dot_product_attention(query, key, value, mask=None)`: This function computes the attention weights and returns the weighted sum of the values. 2. `multi_head_attention(query, key, value, num_heads, mask=None)`: This function takes queries, keys, and values and splits them into multiple heads, then applies the `scaled_dot_product_attention` function on each head independently, and finally concatenates the results. # Function Specifications `scaled_dot_product_attention` - **Input**: - `query`: A tensor of shape `(batch_size, seq_length, d_k)`. - `key`: A tensor of shape `(batch_size, seq_length, d_k)`. - `value`: A tensor of shape `(batch_size, seq_length, d_v)`. - `mask` (optional): A tensor that can be broadcasted to shape `(batch_size, seq_length, seq_length)`. - **Output**: - A tensor of shape `(batch_size, seq_length, d_v)` representing the attention output. `multi_head_attention` - **Input**: - `query`: A tensor of shape `(batch_size, seq_length, d_model)`. - `key`: A tensor of shape `(batch_size, seq_length, d_model)`. - `value`: A tensor of shape `(batch_size, seq_length, d_model)`. - `num_heads`: An integer representing the number of heads. - `mask` (optional): A tensor that can be broadcasted to shape `(batch_size, seq_length, seq_length)`. - **Output**: - A tensor of shape `(batch_size, seq_length, d_model)` representing the multi-head attention output. # Implementation Requirements 1. Implement the scaled dot-product attention mechanism. 2. Implement the multi-head attention mechanism. 3. Ensure proper handling of the input dimensions and broadcasting where applicable. 4. Include necessary reshaping and concatenation operations. # Example Usage ```python import torch import torch.nn.functional as F # Example inputs query = torch.randn(2, 5, 10) # (batch_size, seq_length, d_k) key = torch.randn(2, 5, 10) # (batch_size, seq_length, d_k) value = torch.randn(2, 5, 15) # (batch_size, seq_length, d_v) # Attention function without mask output = scaled_dot_product_attention(query, key, value) print(output.shape) # Expected shape: (2, 5, 15) # Example inputs for multi-head attention model_dim = 20 num_heads = 4 query = torch.randn(2, 5, model_dim) # (batch_size, seq_length, d_model) key = torch.randn(2, 5, model_dim) # (batch_size, seq_length, d_model) value = torch.randn(2, 5, model_dim) # (batch_size, seq_length, d_model) # Multi-head attention output = multi_head_attention(query, key, value, num_heads) print(output.shape) # Expected shape: (2, 5, model_dim) ``` # Notes - The attention scores should be scaled by the square root of the dimensionality of the key vectors. - Implement necessary masking to handle sequence padding or attention focusing where applicable.","solution":"import torch import torch.nn.functional as F def scaled_dot_product_attention(query, key, value, mask=None): Compute the scaled dot-product attention. Args: - query: A tensor of shape (batch_size, seq_length, d_k) - key: A tensor of shape (batch_size, seq_length, d_k) - value: A tensor of shape (batch_size, seq_length, d_v) - mask: An optional tensor that can be broadcasted to shape (batch_size, seq_length, seq_length) Returns: - A tensor of shape (batch_size, seq_length, d_v) d_k = query.shape[-1] scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) if mask is not None: scores = scores.masked_fill(mask == 0, float(\'-inf\')) attention_weights = F.softmax(scores, dim=-1) output = torch.matmul(attention_weights, value) return output def multi_head_attention(query, key, value, num_heads, mask=None): Compute the multi-head attention. Args: - query: A tensor of shape (batch_size, seq_length, d_model) - key: A tensor of shape (batch_size, seq_length, d_model) - value: A tensor of shape (batch_size, seq_length, d_model) - num_heads: An integer representing the number of attention heads - mask: An optional tensor that can be broadcasted to shape (batch_size, num_heads, seq_length, seq_length) Returns: - A tensor of shape (batch_size, seq_length, d_model) batch_size, seq_length, d_model = query.shape d_k = d_model // num_heads query = query.view(batch_size, seq_length, num_heads, d_k).transpose(1, 2) key = key.view(batch_size, seq_length, num_heads, d_k).transpose(1, 2) value = value.view(batch_size, seq_length, num_heads, d_k).transpose(1, 2) if mask is not None: mask = mask.unsqueeze(1) attention_output = scaled_dot_product_attention(query, key, value, mask) attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, d_model) return attention_output"},{"question":"# Question Implement a Python module loader that dynamically imports a Python source file at runtime, caches the module, and supports reloading of the module. Your solution should meet the following criteria: 1. **Function Signature**: - `def dynamic_import(file_path: str) -> ModuleType` - **Input**: `file_path` - A string representing the path to the Python source file. - **Output**: Returns the imported module object. 2. **Features**: - Dynamically import a module from the provided file path. - Cache the imported module such that subsequent imports use the cached module, unless the module is explicitly reloaded. - Provide an additional function to reload the cached module: - `def reload_module(module: ModuleType) -> ModuleType` - **Input**: `module` - The module object to be reloaded. - **Output**: Returns the reloaded module object. 3. **Constraints**: - Ensure the solution handles modules that are modified between imports (i.e., supports reloading). - If the file path does not exist or is not a valid Python file, raise an appropriate exception. - Use the `importlib` module functionalities to achieve the desired import and reload behaviors. # Example Usage ```python # Assume the file \'example_module.py\' exists and contains some Python code. from importlib.util import find_spec from types import ModuleType # Define the required functions. def dynamic_import(file_path: str) -> ModuleType: # Your implementation here. pass def reload_module(module: ModuleType) -> ModuleType: # Your implementation here. pass # Example usage of the functions. try: module = dynamic_import(\'path/to/example_module.py\') print(module.some_function()) # Call a function from the imported module. # Simulate modifying the module by editing \'example_module.py\'. module = reload_module(module) print(module.some_function()) # Should reflect the changes made to the module. except Exception as e: print(f\\"Error: {e}\\") # Validate if the module can be found in sys.modules spec = find_spec(\'example_module\') if spec: module = spec.loader.load_module(\'example_module\') print(f\\"Module {module.__name__} loaded successfully.\\") else: print(\\"Module could not be found.\\") ``` Provide a complete and working implementation for the functions `dynamic_import` and `reload_module`.","solution":"import importlib.util import sys from types import ModuleType import os # Dictionary to cache the modules _module_cache = {} def dynamic_import(file_path: str) -> ModuleType: Dynamically imports a module from the given file path. Args: file_path (str): The path to the Python source file. Returns: ModuleType: The imported module object. Raises: FileNotFoundError: If the file does not exist. ImportError: If there is an error in importing the module. if not os.path.isfile(file_path): raise FileNotFoundError(f\\"File not found: {file_path}\\") module_name = os.path.splitext(os.path.basename(file_path))[0] if module_name in _module_cache: return _module_cache[module_name] spec = importlib.util.spec_from_file_location(module_name, file_path) if spec is None: raise ImportError(f\\"Cannot import module from {file_path}\\") module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module try: spec.loader.exec_module(module) except Exception as e: # If any error occurs during module loading, remove it from sys.modules del sys.modules[module_name] raise ImportError(f\\"Error loading module {module_name}: {e}\\") _module_cache[module_name] = module return module def reload_module(module: ModuleType) -> ModuleType: Reloads the given module. Args: module (ModuleType): The module object to be reloaded. Returns: ModuleType: The reloaded module object. return importlib.reload(module)"},{"question":"Objective: Create visualizations using the `seaborn.objects` module to demonstrate your understanding of scales, transformations, and mappings in data visualization. Task: 1. Load the `diamonds` dataset and the `mpg` dataset from `seaborn`. 2. Using the `diamonds` dataset, create a scatter plot (dots) with `carat` on the x-axis and `price` on the y-axis. - Apply a logarithmic scale to the y-axis. - Color the points based on the `clarity` attribute using the \\"crest\\" color palette. - Scale the point sizes based on the `carat` attribute within the range of 2 to 10. 3. Using the `mpg` dataset, create a bar plot showing the count of cars with different numbers of `cylinders`. - Treat the `cylinders` attribute as a nominal (categorical) scale. - Customize the color of the bars using a custom list of colors. 4. Using the `mpg` dataset, create a scatter plot with `weight` on the x-axis and `acceleration` on the y-axis, colored by `cylinders`. - Utilize the \\"deep\\" color palette. - Customize the markers for the points based on the `origin` attribute. Requirements: - Implement the code in a function `create_visualizations()` that performs the required tasks. - Your solution should include all data loading, transformation, and plotting steps within the function. - Utilize the specified scale transformations and mappings as described above. Input: There are no direct inputs to your function. Output: There are no direct outputs from your function. However, the function should display the specified plots correctly. Constraints: - Ensure you use the `seaborn.objects` submodule for plotting. - Follow best practices for code readability and maintainability. ```python import seaborn.objects as so from seaborn import load_dataset def create_visualizations(): # Load datasets diamonds = load_dataset(\\"diamonds\\") mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Task 2: Scatter plot with diamonds dataset p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") p1.add(so.Dots(), pointsize=\\"carat\\").scale(y=\\"log\\", pointsize=(2, 10), color=\\"clarity\\").scale(color=\\"crest\\") p1.show() # Task 3: Bar plot with mpg dataset p2 = so.Plot(mpg, \\"cylinders\\").add(so.Bar(), so.Hist()) p2.scale(x=so.Nominal()).scale(color=[\\"#49b\\", \\"#a6a\\", \\"#5b8\\"]).show() # Task 4: Scatter plot with mpg dataset p3 = so.Plot(mpg, \\"weight\\", \\"acceleration\\", color=\\"cylinders\\") p3.add(so.Dot(), marker=\\"origin\\").scale(color=\\"deep\\").scale(marker=so.Nominal(order=[\\"japan\\", \\"europe\\", \\"usa\\"])) p3.show() # Uncomment the following line to test the function: # create_visualizations() ``` Notes: - By examining the plots, the function should demonstrate your understanding of data visualizations, scales, and custom mappings using `seaborn.objects`.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_visualizations(): # Load datasets diamonds = load_dataset(\\"diamonds\\") mpg = load_dataset(\\"mpg\\") # Task 2: Scatter plot with diamonds dataset p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\", pointsize=\\"carat\\", color=\\"clarity\\") p1.add(so.Dots()).scale(y=\\"log\\", pointsize=(2, 10), color=\\"crest\\").label(title=\\"Diamonds: Carat vs Price (Log Scale)\\") p1.show() # Task 3: Bar plot with mpg dataset custom_colors = [\\"#49b\\", \\"#a6a\\", \\"#5b8\\", \\"#b45\\"] p2 = so.Plot(mpg, x=\\"cylinders\\").add(so.Bar(), so.Hist()).scale(x=so.Nominal(), color=custom_colors).label(title=\\"Count of Cars by Cylinders\\") p2.show() # Task 4: Scatter plot with mpg dataset p3 = so.Plot(mpg, x=\\"weight\\", y=\\"acceleration\\", color=\\"cylinders\\", marker=\\"origin\\") p3.add(so.Dot()).scale(color=\\"deep\\", marker=so.Nominal()).label(title=\\"Cars: Weight vs Acceleration\\") p3.show()"},{"question":"# **Question: Implementing a Functional Pipeline with Iterators and Generators** You are given a text file `data.txt` that contains multiple lines of text. Each line consists of words separated by spaces. Your task is to implement a function `process_file(file_path: str) -> List[Tuple[str, int]]` that reads the file, processes the text, and returns a list of tuples. Each tuple contains a word and its frequency in the entire text file. To solve this, follow these steps: 1. **Read the File**: Implement a generator function `read_lines(file_path: str) -> Iterator[str]` that reads each line from the file and yields it. This function should use iteration to avoid loading the entire file into memory. 2. **Process Lines**: Implement a generator function `process_lines(lines: Iterator[str]) -> Iterator[str]` that processes each line by: - Stripping leading/trailing whitespace. - Converting the line to lowercase. - Splitting the line into words and yielding each word. 3. **Count Words**: Implement a function `count_words(words: Iterator[str]) -> Dict[str, int]` that takes an iterator of words and returns a dictionary with word frequencies. 4. **Pipeline Function**: Implement the main function `process_file(file_path: str) -> List[Tuple[str, int]]` using the above components. This function should return word-frequency pairs sorted by frequency in descending order. **Input:** - A file path `file_path` to `data.txt`. **Output:** - A list of tuples where each tuple contains a word and its frequency, sorted by frequency in descending order. **Example:** Suppose `data.txt` contains the following text: ``` Hello world Hello Functional Programming is powerful world of Python Python programming is fun ``` Your function should return: ``` [(\'hello\', 2), (\'world\', 2), (\'programming\', 2), (\'is\', 2), (\'functional\', 1), (\'powerful\', 1), (\'of\', 1), (\'python\', 1), (\'fun\', 1)] ``` **Constraints:** - Assume the text file is not empty. - Use Python built-in functions and standard libraries (`itertools` and `functools` where appropriate). - Do not use global variables; adhere to functional programming principles. **Notes:** - Use iterators and generators effectively to handle large files efficiently. - Ensure the entire process is lazy, using generators to avoid loading large datasets into memory. Implement the function `process_file` and the required helper functions: ```python from typing import Iterator, Dict, List, Tuple def read_lines(file_path: str) -> Iterator[str]: # Implement this function def process_lines(lines: Iterator[str]) -> Iterator[str]: # Implement this function def count_words(words: Iterator[str]) -> Dict[str, int]: # Implement this function def process_file(file_path: str) -> List[Tuple[str, int]]: # Implement this function ```","solution":"from typing import Iterator, Dict, List, Tuple from collections import defaultdict def read_lines(file_path: str) -> Iterator[str]: Generator that reads lines from a file. with open(file_path, \'r\') as file: for line in file: yield line def process_lines(lines: Iterator[str]) -> Iterator[str]: Generator that processes lines. Strips leading/trailing whitespace, converts to lowercase, and splits into words. for line in lines: for word in line.strip().lower().split(): yield word def count_words(words: Iterator[str]) -> Dict[str, int]: Counts the frequency of words in the given iterator and returns a dictionary. word_count = defaultdict(int) for word in words: word_count[word] += 1 return word_count def process_file(file_path: str) -> List[Tuple[str, int]]: Processes the file and returns word-frequency pairs sorted by frequency in descending order. lines = read_lines(file_path) words = process_lines(lines) word_count = count_words(words) # Sort the dictionary by frequency in descending order and convert to list of tuples sorted_word_count = sorted(word_count.items(), key=lambda item: item[1], reverse=True) return sorted_word_count"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},q={class:"card-container"},F={key:0,class:"empty-state"},R=["disabled"],M={key:0},N={key:1};function L(n,e,l,m,i,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",q,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",F,' No results found for "'+u(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",N,"Loading...")):(a(),s("span",M,"See more"))],8,R)):d("",!0)])}const O=p(D,[["render",L],["__scopeId","data-v-dd83f7f7"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/59.md","filePath":"deepseek/59.md"}'),j={name:"deepseek/59.md"},H=Object.assign(j,{setup(n){return(e,l)=>(a(),s("div",null,[x(O)]))}});export{Y as __pageData,H as default};
