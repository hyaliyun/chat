import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(s,e,l,m,n,o){return a(),i("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-8e4561f5"]]),I=JSON.parse('[{"question":"# Persistent Storage with Shelve You are required to implement a program that uses the `shelve` module to manage a persistent dictionary. Your task is to create a class `PersistentDict` that: 1. Initializes a shelf with a given filename. 2. Provides methods to add, update, and delete key-value pairs. 3. Ensures that all changes are persistently saved. 4. Handles multiple types of values, including lists and nested dictionaries. 5. Ensures data consistency even when dealing with mutable objects. Requirements: - You must use `writeback=True` to handle mutable objects. - Provide a method to retrieve all keys as a list. - Implement a context manager to ensure the shelf is properly closed after use. - Implement error handling for common issues like trying to access or delete a non-existent key. Class Definition: ```python class PersistentDict: def __init__(self, filename: str): Initialize the PersistentDict with the specified filename. Args: filename (str): The filename for the shelve. pass def add(self, key: str, value): Add a key-value pair to the shelf. If the key already exists, update the value. Args: key (str): The key for the value. value: The value to be stored, can be any picklable Python object. pass def delete(self, key: str): Delete the key-value pair from the shelf. Args: key (str): The key to be deleted. Raises: KeyError: If the key does not exist. pass def get_all_keys(self) -> list: Retrieve a list of all keys in the shelf. Returns: list: A list of all keys stored in the shelf. pass def __enter__(self): Enter the runtime context related to this object. Returns self to allow the context manager to handle the resource. pass def __exit__(self, exc_type, exc_val, exc_tb): Exit the runtime context related to this object. Ensures that the shelf is properly closed. pass ``` Example Usage: ```python with PersistentDict(\'test_shelf\') as pd: pd.add(\'key1\', \'value1\') pd.add(\'key2\', [1, 2, 3]) pd.add(\'key3\', {\'nested_key\': \'nested_value\'}) print(pd.get_all_keys()) # Output: [\'key1\', \'key2\', \'key3\'] pd.delete(\'key2\') print(pd.get_all_keys()) # Output: [\'key1\', \'key3\'] ``` **Constraints:** - Your solution must handle large amounts of data efficiently. - Ensure the program operates correctly with concurrent access (multiple instantiations). - Implement appropriate error handling and resource management. **Performance Requirements:** - The program should handle at least 10,000 key-value pairs without significant performance degradation. - Minimize memory usage when storing and retrieving data.","solution":"import shelve class PersistentDict: def __init__(self, filename: str): Initialize the PersistentDict with the specified filename. self.filename = filename self.shelf = None def __enter__(self): Enter the runtime context related to this object. self.shelf = shelve.open(self.filename, writeback=True) return self def __exit__(self, exc_type, exc_val, exc_tb): Exit the runtime context related to this object. if self.shelf is not None: self.shelf.close() def add(self, key: str, value): Add a key-value pair to the shelf. If the key already exists, update the value. self.shelf[key] = value def delete(self, key: str): Delete the key-value pair from the shelf. try: del self.shelf[key] except KeyError: raise KeyError(f\\"Key \'{key}\' not found.\\") def get_all_keys(self) -> list: Retrieve a list of all keys in the shelf. return list(self.shelf.keys())"},{"question":"Implementing a Custom Transformer using PyTorch Special Functions Objective The goal of this assessment is to evaluate your ability to utilize the advanced special functions available in the `torch.special` module in the implementation of a custom transformer model. This model will involve applying mathematical transformations to a tensor using these functions. Problem Statement You are required to implement a custom tensor transformer in PyTorch that applies a series of mathematical operations to each element of an input tensor. Specifically, you will be implementing a function that takes in a tensor and applies the following sequence of transformations using `torch.special` functions: 1. Compute the `logit` of each element. 2. Apply the `expit` function to the result. 3. Compute the `logsumexp` across a specified dimension. 4. Normalize the resulting tensor using the `softmax` function. Requirements - The input tensor will be a 2D tensor of shape (N, M), where N is the number of samples and M is the number of features. - Your function should apply the above transformations and return the final transformed tensor. - You are allowed to use only the functions from `torch.special` for the mathematical transformations. - Ensure that the final tensor is normalized along the specified dimension using the `softmax` function. Function Signature ```python import torch def special_transformer(input_tensor: torch.Tensor, dim: int) -> torch.Tensor: Apply a custom series of transformations to the input tensor using PyTorch\'s special functions. Parameters: input_tensor (torch.Tensor): A 2D tensor of shape (N, M). dim (int): The dimension along which to perform the logsumexp and softmax operations. Returns: torch.Tensor: The transformed tensor after applying the sequence of special function transformations. pass ``` Example ```python import torch # Define a 2D input tensor input_tensor = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]) # Specify the dimension dim = 1 # Perform the transformation output_tensor = special_transformer(input_tensor, dim) print(output_tensor) ``` Constraints - Ensure the input tensor values remain in a valid range for all transformations (such as avoiding infinite values). Notes - Carefully handle numerical stability and possible edge cases, such as input values at the boundaries of the functions\' domains. - The use of torch.special functions is mandatory; using torch\'s standard equivalents will not be considered a valid solution. Happy coding!","solution":"import torch def special_transformer(input_tensor: torch.Tensor, dim: int) -> torch.Tensor: Apply a custom series of transformations to the input tensor using PyTorch\'s special functions. Parameters: input_tensor (torch.Tensor): A 2D tensor of shape (N, M). dim (int): The dimension along which to perform the logsumexp and softmax operations. Returns: torch.Tensor: The transformed tensor after applying the sequence of special function transformations. logit_values = torch.special.logit(input_tensor) expit_values = torch.special.expit(logit_values) logsumexp_values = torch.special.logsumexp(expit_values, dim=dim, keepdim=True) softmax_values = torch.softmax(logsumexp_values, dim=dim) return softmax_values"},{"question":"# Advanced Python Coding Assessment Objective To demonstrate your understanding of file manipulation, shell command execution, and object-oriented programming in Python, you will recreate a simplified version of the `pipes.Template` functionality using the `subprocess` module. Your task is to design and implement a class named `Pipeline` that allows users to chain shell commands and apply them to files or strings. Task 1. **Class Definition**: Define the `Pipeline` class. 2. **Method to Add Commands**: Implement methods to add commands to the pipeline. - `append(cmd, kind)`: Adds a command to the end of the pipeline. - `prepend(cmd, kind)`: Adds a command to the beginning of the pipeline. 3. **File Processing**: Implement a method to process files. - `open(file, mode)`: Takes a filename and mode (\'w\' or \'r\'), processes the file through the pipeline, and returns the result as a string. 4. **Copy Method**: Implement a method to copy from one file to another while applying the pipeline. - `copy(infile, outfile)`: Reads content from `infile`, processes it through the pipeline, and writes the result to `outfile`. Implementation Details - **Methods to be Implemented**: - `def __init__(self):` - `def append(self, cmd: str, kind: str):` - `def prepend(self, cmd: str, kind: str):` - `def open(self, file: str, mode: str) -> str:` - `def copy(self, infile: str, outfile: str):` - **Constraints**: - Only POSIX-compatible shell commands should be used. - The `subprocess` module should be used for command execution. Example Usage ```python pipeline = Pipeline() # Example: Convert text to uppercase then reverse the text. pipeline.append(\'tr a-z A-Z\', \'--\') pipeline.append(\'rev\', \'--\') # Write processed content to a file pipeline.open(\'pipefile\', \'w\') with open(\'pipefile\', \'r\') as f: content = f.read() print(content) # It should print: DLROW OLLEH ``` Notes - Make sure to handle edge cases such as invalid commands and empty pipelines gracefully. - Implement proper exception handling for file operations and shell command execution. - Ensure that your solution is efficient and avoids unnecessary resource usage. Good luck, and happy coding!","solution":"import subprocess class Pipeline: def __init__(self): self.commands = [] def append(self, cmd: str, kind: str): if kind == \'--\': self.commands.append(cmd) else: raise ValueError(\\"Unsupported kind. Only \'--\' is supported.\\") def prepend(self, cmd: str, kind: str): if kind == \'--\': self.commands.insert(0, cmd) else: raise ValueError(\\"Unsupported kind. Only \'--\' is supported.\\") def _process_pipe(self, input_text): if not self.commands: return input_text result = input_text for cmd in self.commands: process = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) result, _ = process.communicate(result) return result def open(self, file: str, mode: str): if mode == \'w\': raise ValueError(\\"Mode \'w\' not supported. Use the \'copy\' method to write to a file.\\") with open(file, \'r\') as f: content = f.read() return self._process_pipe(content) def copy(self, infile: str, outfile: str): with open(infile, \'r\') as f: content = f.read() processed_content = self._process_pipe(content) with open(outfile, \'w\') as f: f.write(processed_content)"},{"question":"Coding Assessment Question # Problem Statement You are provided with a dataset containing both numerical and categorical features. Your task is to preprocess the dataset and prepare it for fitting a machine learning model using scikit-learn\'s preprocessing utilities. You will need to handle missing values, scale numerical features, and encode categorical features. Finally, you will evaluate the impact of preprocessing on a simple classification model. # Input * A CSV file named `data.csv` with the following columns: - `feature_1`: numerical feature with missing values - `feature_2`: numerical feature - `feature_3`: categorical feature with values like \\"A\\", \\"B\\", \\"C\\" - `target`: binary target variable (0 or 1) # Output * A 2-item tuple containing: - Preprocessed feature matrix `X` (as a numpy array). - Accuracy score of the Logistic Regression model on a test set (as a float). # Constraints * Use `SimpleImputer` to handle missing values in numerical features. * Use `StandardScaler` to scale the numerical features. * Use `OneHotEncoder` to encode the categorical feature. * Split the dataset into training (80%) and testing (20%) sets using `train_test_split`. * Evaluate the model using `accuracy_score`. # Performance Requirements * Your solution should be efficient and make use of scikit-learn\'s fit-transform methods appropriately. Example ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.utils import shuffle # Define the function to preprocess the data and evaluate the model def preprocess_and_evaluate(data_path): # Load the dataset df = pd.read_csv(data_path) # Split the data into features and target X = df.drop(columns=\'target\') y = df[\'target\'] # Define the preprocessing pipelines for numerical and categorical data numerical_features = [\'feature_1\', \'feature_2\'] categorical_features = [\'feature_3\'] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer(transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Create and fit the full pipeline with logistic regression clf = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Shuffle the data to ensure randomness X, y = shuffle(X, y, random_state=42) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit the model clf.fit(X_train, y_train) # Make predictions on the test set y_pred = clf.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) # Obtain the preprocessed feature matrix X_processed = preprocessor.fit_transform(X) return (X_processed, accuracy) ``` # Notes * Ensure that the necessary libraries (pandas, numpy, scikit-learn) are installed. * You may need to create a sample `data.csv` file with appropriate values to test the function.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.utils import shuffle def preprocess_and_evaluate(data_path): # Load the dataset df = pd.read_csv(data_path) # Split the data into features and target X = df.drop(columns=\'target\') y = df[\'target\'] # Define the preprocessing pipelines for numerical and categorical data numerical_features = [\'feature_1\', \'feature_2\'] categorical_features = [\'feature_3\'] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer(transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Create and fit the full pipeline with logistic regression clf = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(max_iter=1000, random_state=42)) ]) # Shuffle the data to ensure randomness X, y = shuffle(X, y, random_state=42) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit the model clf.fit(X_train, y_train) # Make predictions on the test set y_pred = clf.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) # Obtain the preprocessed feature matrix X_processed = preprocessor.fit_transform(X) return (X_processed, accuracy)"},{"question":"# Question: Simulating Function Objects in Python You are tasked with simulating some behavior of Python function objects using Python classes. Implement a class `FunctionObject` that will allow you to create and manage function objects similar to the C functions described in the provided documentation. Requirements: 1. **Initialization**: - The constructor should take `func`, `globals`, and optionally `qualname`. - `func` is a Python callable. - `globals` is a dictionary with global variables accessible to the function. - `qualname` is an optional string for the qualified name of the function. 2. **Methods**: - `get_code()`: Returns the code object associated with the function. - `get_globals()`: Returns the globals dictionary. - `get_module()`: Returns the module name. - `get_defaults()`: Returns the default values of the functionâ€™s arguments. - `set_defaults(defaults)`: Sets the default values for the functionâ€™s arguments. - `get_annotations()`: Returns the annotations of the function. - `set_annotations(annotations)`: Sets the annotations for the function. Constraints: - The `defaults` in `set_defaults` and the return value of `get_defaults` should be a tuple. - The `annotations` in `set_annotations` and the return value of `get_annotations` should be a dictionary. You should also provide an example usage of the `FunctionObject` class demonstrating each requirement. Example: ```python def sample_function(a, b=2): Sample function return a + b # Example Usage globals_dict = {\\"__name__\\": \\"__main__\\"} func_obj = FunctionObject(sample_function, globals_dict, \\"example.sample_function\\") print(func_obj.get_code()) # <code object sample_function at 0x...> print(func_obj.get_globals()) # {\'__name__\': \'__main__\'} print(func_obj.get_module()) # \\"__main__\\" print(func_obj.get_defaults()) # (2,) func_obj.set_defaults((3,)) print(func_obj.get_defaults()) # (3,) print(func_obj.get_annotations()) # {} func_obj.set_annotations({\'a\': int, \'return\': int}) print(func_obj.get_annotations()) # {\'a\': <class \'int\'>, \'return\': <class \'int\'>} ``` Implement the `FunctionObject` class below: ```python class FunctionObject: def __init__(self, func, globals, qualname=None): pass def get_code(self): pass def get_globals(self): pass def get_module(self): pass def get_defaults(self): pass def set_defaults(self, defaults): pass def get_annotations(self): pass def set_annotations(self, annotations): pass ```","solution":"class FunctionObject: def __init__(self, func, globals, qualname=None): self.func = func self.globals = globals self.qualname = qualname or func.__qualname__ def get_code(self): return self.func.__code__ def get_globals(self): return self.globals def get_module(self): return self.func.__module__ def get_defaults(self): return self.func.__defaults__ def set_defaults(self, defaults): if not isinstance(defaults, tuple): raise TypeError(\\"Defaults should be a tuple.\\") self.func.__defaults__ = defaults def get_annotations(self): return self.func.__annotations__ def set_annotations(self, annotations): if not isinstance(annotations, dict): raise TypeError(\\"Annotations should be a dictionary.\\") self.func.__annotations__ = annotations"},{"question":"# Gaussian Process Regression with Custom Kernels In this task, you are required to implement a Gaussian Process Regressor (GPR) using scikit-learnâ€™s Gaussian Processes capabilities. You\'ll be using a custom kernel created by combining fundamental kernels. Your task involves data preparation, model training, prediction, and visualization of results with confidence intervals. Requirements: 1. **Data Preparation**: - Generate a synthetic dataset using the function ( f(x) = sin(x) ) with added Gaussian noise. - Split the data into training and test sets. 2. **Kernel Creation**: - Define a custom kernel by combining the Radial Basis Function (RBF) kernel with a WhiteKernel (for noise estimation). 3. **Model Implementation**: - Implement a GaussianProcessRegressor using the custom kernel created. - Train the model on the training data. - Make predictions on the test data. 4. **Visualization**: - Plot the training data points, true function, predicted mean, and confidence intervals. 5. **Evaluation**: - Calculate and display the mean squared error (MSE) on the test data. Function Signatures: ```python from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, WhiteKernel import numpy as np import matplotlib.pyplot as plt def generate_synthetic_data(): Generates a synthetic dataset using the function f(x) = sin(x) with added Gaussian noise. Returns: -------- X_train, y_train, X_test, y_test: np.ndarray Training and test data. pass def create_custom_kernel(): Creates a custom kernel by combining RBF and WhiteKernel. Returns: -------- kernel: Kernel The custom kernel created by combining RBF and WhiteKernel. pass def train_gpr_model(X_train, y_train, kernel): Trains a Gaussian Process Regressor model using the provided training data and kernel. Parameters: ----------- X_train: np.ndarray Training features. y_train: np.ndarray Training target values. kernel: Kernel Custom kernel to be used in the GaussianProcessRegressor. Returns: -------- gpr: GaussianProcessRegressor Trained Gaussian Process Regressor model. pass def predict_and_visualize(gpr, X_train, y_train, X_test, y_test): Makes predictions with the trained GPR model, and plots the results. Parameters: ----------- gpr: GaussianProcessRegressor Trained Gaussian Process Regressor model. X_train: np.ndarray Training features. y_train: np.ndarray Training target values. X_test: np.ndarray Test features. y_test: np.ndarray Test target values. pass def evaluate_model(gpr, X_test, y_test): Evaluates the trained GPR model on test data and computes the mean squared error (MSE). Parameters: ----------- gpr: GaussianProcessRegressor Trained Gaussian Process Regressor model. X_test: np.ndarray Test features. y_test: np.ndarray Test target values. Returns: -------- mse: float Mean squared error of the test predictions. pass # Example usage: # X_train, y_train, X_test, y_test = generate_synthetic_data() # kernel = create_custom_kernel() # gpr = train_gpr_model(X_train, y_train, kernel) # predict_and_visualize(gpr, X_train, y_train, X_test, y_test) # mse = evaluate_model(gpr, X_test, y_test) # print(f\\"Mean Squared Error: {mse}\\") ``` Constraints: - Use `sklearn.gaussian_process.kernels.RBF` and `sklearn.gaussian_process.kernels.WhiteKernel` for kernel creation. - Generate at least 100 data points for training and 100 data points for testing. - Set a fixed random seed (e.g., `np.random.seed(42)`) for reproducibility. Performance Requirements: - Ensure the code runs efficiently within reasonable time limits (~ few seconds).","solution":"from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, WhiteKernel import numpy as np import matplotlib.pyplot as plt def generate_synthetic_data(): np.random.seed(42) X = np.linspace(0, 10, 100) y = np.sin(X) + np.random.normal(0, 0.1, X.shape) X_train = X[:80].reshape(-1, 1) y_train = y[:80] X_test = X[80:].reshape(-1, 1) y_test = y[80:] return X_train, y_train, X_test, y_test def create_custom_kernel(): kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0) return kernel def train_gpr_model(X_train, y_train, kernel): gpr = GaussianProcessRegressor(kernel=kernel, random_state=42) gpr.fit(X_train, y_train) return gpr def predict_and_visualize(gpr, X_train, y_train, X_test, y_test): X = np.linspace(0, 10, 1000).reshape(-1, 1) y_true = np.sin(X).ravel() y_pred, y_std = gpr.predict(X, return_std=True) plt.figure(figsize=(10, 5)) plt.plot(X, y_true, \'r\', label=\'True function\') plt.scatter(X_train, y_train, c=\'b\', label=\'Training data\') plt.plot(X, y_pred, \'k\', label=\'Predicted mean\') plt.fill_between(X.ravel(), y_pred - 1.96 * y_std, y_pred + 1.96 * y_std, alpha=0.2, label=\'Confidence interval\') plt.title(\'Gaussian Process Regression\') plt.xlabel(\'X\') plt.ylabel(\'y\') plt.legend() plt.show() def evaluate_model(gpr, X_test, y_test): y_pred, _ = gpr.predict(X_test, return_std=True) mse = np.mean((y_pred - y_test) ** 2) return mse # Example usage: # X_train, y_train, X_test, y_test = generate_synthetic_data() # kernel = create_custom_kernel() # gpr = train_gpr_model(X_train, y_train, kernel) # predict_and_visualize(gpr, X_train, y_train, X_test, y_test) # mse = evaluate_model(gpr, X_test, y_test) # print(f\\"Mean Squared Error: {mse}\\")"},{"question":"<|Analysis Begin|> The \\"pipes\\" module in Python provides an abstraction for creating and managing shell pipelines. The central class in this module is `pipes.Template`, which represents a pipeline template and provides methods to manipulate and execute the pipeline. Here are some key aspects of this class: - **Template Methods**: - `reset()`: Restores the pipeline template to its initial state. - `clone()`: Returns a new, equivalent pipeline template. - `debug(flag)`: Enables or disables debugging based on the `flag`. - `append(cmd, kind)`: Appends a new action at the end of the pipeline. The `cmd` is a shell command and `kind` specifies the input/output behavior. - `prepend(cmd, kind)`: Adds a new action at the beginning of the pipeline. Parameters are similar to those for `append()`. - `open(file, mode)`: Opens a file and connects it to the pipeline. - `copy(infile, outfile)`: Copies data from `infile` to `outfile` through the pipeline. The module is deprecated as of Python 3.11 and is not available on all operating systems (only on Unix). This module\'s usage is tied to `os.system()` and `os.popen()` which require a POSIX-compliant shell. Given the function-oriented nature of the `pipes` module and its deprecation, it\'s essential to emphasize creating, managing, and utilizing pipelines effectively in Unix environments. The question should focus critically on understanding the mechanics of pipeline creation and manipulation using the provided methods. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Demonstrate your understanding of Unix shell pipeline creation and manipulation using Python\'s `pipes` module. Problem Statement: You need to create a Python script that reads a text file, performs a series of transformations using Unix commands, and writes the final output to another text file. Specifically, you will use the `pipes.Template` class to define, manage and execute this pipeline. The processing steps involve the following transformations: 1. Convert all text to uppercase. 2. Replace all spaces with underscores. 3. Sort the lines in alphabetical order. Instructions: Implement the function `transform_text_pipeline(input_file: str, output_file: str) -> None` which performs the following: 1. Create a pipeline template using the `pipes.Template` class. 2. Append the necessary Unix commands to the pipeline to: - Convert text to uppercase. - Replace spaces with underscores. - Sort the lines alphabetically. 3. Open `input_file` for reading and `output_file` for writing as part of the pipeline. 4. Ensure that the pipeline reads from `input_file` and writes the final output to `output_file`. Note: Assume that `input_file` and `output_file` are valid file paths and that the files are accessible. Constraints: - Only use Unix utility commands in the pipeline. - The solution should be compatible only with Unix-based systems. - Do not use any deprecated functionality or modules other than `pipes`. Example: Assume the content of `input_file` is: ``` hello world this is a test python is fun ``` After processing, the content of `output_file` should be: ``` PYTHON_IS_FUN THIS_IS_A_TEST HELLO_WORLD ``` Implementation: ```python import pipes def transform_text_pipeline(input_file: str, output_file: str) -> None: # Create a template object template = pipes.Template() # Step 1: Convert all text to uppercase template.append(\'tr a-z A-Z\', \'--\') # Step 2: Replace all spaces with underscores template.append(\'tr \\" \\" \\"_\\"\', \'--\') # Step 3: Sort lines alphabetically template.append(\'sort\', \'--\') # Open the input and output files and execute the pipeline with template.open(input_file, \'r\') as infile: with template.open(output_file, \'w\') as outfile: outf.write(infile.read()) print(f\\"Transformation complete. Output saved to {output_file}\\") # Example usage # transform_text_pipeline(\'input.txt\', \'output.txt\') ``` Ensure your solution works correctly by testing it with the provided example content.","solution":"import pipes def transform_text_pipeline(input_file: str, output_file: str) -> None: # Create a template object template = pipes.Template() # Step 1: Convert all text to uppercase template.append(\'tr a-z A-Z\', \'--\') # Step 2: Replace all spaces with underscores template.append(\'tr \\" \\" \\"_\\"\', \'--\') # Step 3: Sort lines alphabetically template.append(\'sort\', \'--\') # Open the input and output files and execute the pipeline with template.open(input_file, \'r\') as infile: with template.open(output_file, \'w\') as outfile: outfile.write(infile.read())"},{"question":"# Objective Implement the functionality provided by the `uu` module using the `base64` module instead. # Task You are asked to create python functions to encode and decode files using the `base64` module. Specifically, you need to replicate the behavior of `uu.encode` and `uu.decode` as closely as possible. # Requirements 1. **Function 1: `base64_encode(in_file, out_file, name=None, mode=None, *, backtick=False)`** - **Input:** - `in_file` (file-like object or string path): The file to be encoded. - `out_file` (file-like object or string path): The destination for the encoded data. - `name` (string, optional): The name to include in the encoded header. - `mode` (int, optional): The file mode to include in the encoded header. - `backtick` (bool, optional): If true, zero bytes are represented with a backtick (`) instead of a space. - **Output:** - An encoded file at `out_file` location. 2. **Function 2: `base64_decode(in_file, out_file=None, mode=None, quiet=False)`** - **Input:** - `in_file` (file-like object or string path): The file to be decoded. - `out_file` (file-like object or string path, optional): The destination for the decoded data. - `mode` (int, optional): Permission bits for the `out_file` if it needs to be created. - `quiet` (bool, optional): If true, suppress warning messages. - **Output:** - A decoded file at `out_file` location. # Constraints 1. You must handle the case where the `out_file` already exists by raising a custom exception. 2. The file permissions must be set correctly when creating `out_file`. 3. The implementation must handle reading and writing binary data accurately. # Example Usage ```python # Example of encoding a file base64_encode(\'input.txt\', \'encoded_output.txt\') # Example of decoding a file with detailed debug base64_decode(\'encoded_output.txt\', \'decoded_output.txt\') ``` # Notes - For your implementation, ignore the UUencode specific header (i.e., name and mode in headers), but ensure you raise an exception if `out_file` already exists. - Ensure your functions read and write binary data correctly, akin to the `uu` module\'s behavior. Implementing these functions will provide a practical understanding of handling file I/O operations, dealing with exceptions, and using the `base64` module in Python.","solution":"import base64 import os class FileExistsError(Exception): pass def base64_encode(in_file, out_file, name=None, mode=None, *, backtick=False): if os.path.exists(out_file): raise FileExistsError(f\\"The file \'{out_file}\' already exists.\\") with open(in_file, \'rb\') as fin, open(out_file, \'wb\') as fout: encoded_data = base64.b64encode(fin.read()) fout.write(encoded_data) def base64_decode(in_file, out_file=None, mode=None, quiet=False): if out_file is None: out_file = in_file + \'.decoded\' if os.path.exists(out_file): raise FileExistsError(f\\"The file \'{out_file}\' already exists.\\") with open(in_file, \'rb\') as fin, open(out_file, \'wb\') as fout: decoded_data = base64.b64decode(fin.read()) fout.write(decoded_data) if mode is not None: os.chmod(out_file, mode) if not quiet: print(f\\"Decoded file written to: {out_file}\\")"},{"question":"Custom Pytorch Function Implementation and Verification # Objective Implement a custom PyTorch function using the `torch.autograd.Function` class to evaluate forward and backward passes for a simple operation. Verify the correctness of your implementation by comparing the gradients computed by your backward method with the numerical gradients using `gradcheck`. # Problem Statement Implement a new PyTorch operation called `MyExp` that computes the exponential function element-wise on each input. You are required to implement both the forward and backward passes for this operation. 1. Subclass `torch.autograd.Function` to create `MyExpFunction`. 2. Implement the `forward(ctx, input)` method to compute the element-wise exponential of the input tensor. 3. Implement the `setup_context(ctx, inputs, output)` method to save any necessary tensors needed for the backward pass. 4. Implement the `backward(ctx, grad_output)` method to compute the gradient of the input tensor. 5. Wrap the custom function into a user-friendly wrapper function `my_exp(input)`. 6. Validate your implementation using `torch.autograd.gradcheck`. # Requirements * The input to the `forward` method will be a tensor `input` of any shape. * The `backward` method should correctly compute the gradient with respect to the input tensor. * Use `torch.exp()` for computing the forward pass. * Use `torch.autograd.gradcheck` to verify the correctness of your backward implementation. * Ensure your custom function works with higher-order derivatives. # Input * A tensor `input` of any shape with `requires_grad=True`. # Output * A tensor of the same shape as `input` containing the element-wise exponential of the input elements. # Template Code ```python import torch from torch.autograd import Function from torch.autograd import gradcheck class MyExpFunction(Function): @staticmethod def forward(ctx, input): # Compute the exponential of the input tensor exp_input = torch.exp(input) # Save any necessary tensors for the backward pass ctx.save_for_backward(exp_input) return exp_input @staticmethod def setup_context(ctx, inputs, output): ctx.save_for_backward(*inputs) @staticmethod def backward(ctx, grad_output): exp_input, = ctx.saved_tensors # Compute the gradient of the input tensor grad_input = grad_output * exp_input return grad_input def my_exp(input): # Apply the custom MyExpFunction return MyExpFunction.apply(input) # Validate your implementation using gradcheck if __name__ == \\"__main__\\": input = (torch.randn(10, dtype=torch.double, requires_grad=True),) test = gradcheck(my_exp, input, eps=1e-6, atol=1e-4) print(\\"Gradcheck passed:\\", test) ``` Your task is to complete the implementation of `MyExpFunction` and ensure that all requirements are met.","solution":"import torch from torch.autograd import Function from torch.autograd import gradcheck class MyExpFunction(Function): @staticmethod def forward(ctx, input): # Compute the exponential of the input tensor exp_input = torch.exp(input) # Save necessary tensors for the backward pass ctx.save_for_backward(exp_input) return exp_input @staticmethod def backward(ctx, grad_output): exp_input, = ctx.saved_tensors # Compute the gradient of the input tensor grad_input = grad_output * exp_input return grad_input def my_exp(input): # Apply the custom MyExpFunction return MyExpFunction.apply(input) # Gradcheck to validate the implementation def validate_gradcheck(): input = (torch.randn(10, dtype=torch.double, requires_grad=True),) test = gradcheck(my_exp, input, eps=1e-6, atol=1e-4) return test if __name__ == \\"__main__\\": print(\\"Gradcheck passed:\\", validate_gradcheck())"},{"question":"Objective You are required to demonstrate your understanding and usage of the `pwd` module in Python. Problem Statement Write a function `get_user_info(username_or_uid)` that takes a single input parameter: * `username_or_uid` (string or integer): The username (string) or user ID (integer) to query. The function should return a dictionary containing the following user details: * \\"Login Name\\" (`pw_name`) * \\"User ID\\" (`pw_uid`) * \\"Group ID\\" (`pw_gid`) * \\"User Name or Comment\\" (`pw_gecos`) * \\"Home Directory\\" (`pw_dir`) * \\"Shell\\" (`pw_shell`) The keys of the dictionary should be exactly as shown above. If the user does not exist, raise a `KeyError` with an appropriate error message. Constraints 1. Your code should be compatible with Python 3.10. 2. Ensure to handle the case where the input is neither a valid username nor a valid user ID. 3. You can assume the input will be either a valid string or an integer. Function Signature ```python def get_user_info(username_or_uid): pass ``` Example ```python # If the user exists print(get_user_info(\\"root\\")) # Output: # { # \\"Login Name\\": \\"root\\", # \\"User ID\\": 0, # \\"Group ID\\": 0, # \\"User Name or Comment\\": \\"root\\", # \\"Home Directory\\": \\"/root\\", # \\"Shell\\": \\"/bin/bash\\" # } print(get_user_info(0)) # Output: # { # \\"Login Name\\": \\"root\\", # \\"User ID\\": 0, # \\"Group ID\\": 0, # \\"User Name or Comment\\": \\"root\\", # \\"Home Directory\\": \\"/root\\", # \\"Shell\\": \\"/bin/bash\\" # } # If the user does not exist try: print(get_user_info(\\"nonexistent_user\\")) except KeyError as e: print(e) # Output: Key error: User \'nonexistent_user\' not found try: print(get_user_info(99999)) except KeyError as e: print(e) # Output: Key error: User ID \'99999\' not found ``` Make sure to handle both user names and user IDs properly and raise the appropriate errors when the user is not found. Notes - Utilize the `pwd` module functions `getpwuid` and `getpwnam` for fetching user details. - Use `isinstance` to check the type of `username_or_uid` and decide whether to call `getpwuid` or `getpwnam`.","solution":"import pwd def get_user_info(username_or_uid): Returns user information dictionary for a given username or user ID. :param username_or_uid: User name (string) or user ID (integer) :return: Dictionary containing user information :raises: KeyError if the user or user ID is not found try: if isinstance(username_or_uid, int): pw_record = pwd.getpwuid(username_or_uid) elif isinstance(username_or_uid, str): pw_record = pwd.getpwnam(username_or_uid) else: raise KeyError(f\\"Invalid type for username_or_uid: {username_or_uid}\\") except KeyError: raise KeyError(f\\"User \'{username_or_uid}\' not found\\") user_info = { \\"Login Name\\": pw_record.pw_name, \\"User ID\\": pw_record.pw_uid, \\"Group ID\\": pw_record.pw_gid, \\"User Name or Comment\\": pw_record.pw_gecos, \\"Home Directory\\": pw_record.pw_dir, \\"Shell\\": pw_record.pw_shell } return user_info"},{"question":"# IP Address Manipulation and Analysis Using the `ipaddress` module, write a function `analyze_networks(ip_ranges: List[str]) -> Dict[str, Any]` that takes a list of IP range strings in CIDR notation and returns a dictionary with the following information for each network: 1. **Total Number of Hosts**: The total number of usable IP addresses within the network. 2. **Network Mask**: The network mask of the network. 3. **Broadcast Address**: The broadcast address if it exists; otherwise, return \\"N/A\\". 4. **Network Addresses**: List of all IP addresses within the network. Input: - `ip_ranges` (List[str]): A list of IP range strings in CIDR notation (e.g., \'192.168.1.0/24\', \'2001:db8::/96\'). Output: - A dictionary where the keys are the input IP range strings and the values are dictionaries with the following structure: ```python { \\"total_hosts\\": int, # total number of usable IP addresses \\"network_mask\\": str, # network mask (e.g., \'255.255.255.0\') \\"broadcast_address\\": str, # broadcast address if it exists, otherwise \\"N/A\\" \\"network_addresses\\": List[str] # list of all IP addresses within the network } ``` Constraints: 1. Assume that all input strings are valid IP range strings in CIDR notation. 2. Focus on efficiency and readability in your implementation. Example: ```python ip_ranges = [\'192.168.1.0/24\', \'2001:db8::/96\'] print(analyze_networks(ip_ranges)) # Expected output (formatted for clarity): # { # \'192.168.1.0/24\': { # \'total_hosts\': 254, # \'network_mask\': \'255.255.255.0\', # \'broadcast_address\': \'192.168.1.255\', # \'network_addresses\': [\'192.168.1.1\', \'192.168.1.2\', ..., \'192.168.1.254\'] # }, # \'2001:db8::/96\': { # \'total_hosts\': 4294967294, # \'network_mask\': \'ffff:ffff:ffff:ffff:ffff:ffff::\', # \'broadcast_address\': \'N/A\', # \'network_addresses\': [\'2001:db8::1\', \'2001:db8::2\', ..., \'2001:db8::ff:ffff\'] # } # } ``` Note: - For performance reasons, it may be impractical to list all IP addresses in a large network (e.g., IPv6 networks), so you should handle large outputs appropriately. Implement the function `analyze_networks` to fulfill the requirements described.","solution":"import ipaddress from typing import List, Dict, Any def analyze_networks(ip_ranges: List[str]) -> Dict[str, Any]: result = {} for ip_range in ip_ranges: net = ipaddress.ip_network(ip_range, strict=False) network_info = { \\"total_hosts\\": net.num_addresses - 2 if not net.is_reserved else net.num_addresses, # Usable hosts in IPv4, all hosts in IPv6 \\"network_mask\\": str(net.netmask if isinstance(net, ipaddress.IPv4Network) else net.prefixlen), \\"broadcast_address\\": str(net.broadcast_address) if isinstance(net, ipaddress.IPv4Network) and not net.is_reserved else \\"N/A\\", \\"network_addresses\\": [str(ip) for ip in net.hosts()] if net.num_addresses <= 2**16 else [] # Limit large networks } result[ip_range] = network_info return result"},{"question":"# HTML Entity Decoder **Objective**: Your task is to implement a function that decodes HTML entities in a given string. You must use the `html.entities` module to aid in this process. **Function Signature**: ```python def decode_html_entities(input_str: str) -> str: pass ``` **Input**: - `input_str` (str): A string that may contain HTML entity references. The HTML entities can appear both with and without a trailing semicolon. **Output**: - (str): A string with all HTML entity references replaced by their corresponding Unicode characters. **Constraints**: 1. You must use the dictionaries provided in `html.entities` for your implementation. 2. Handle both named entities (e.g., `&amp;`, `&lt;`, `&gt;`) and numeric entities (e.g., `&#38;`, `&#x26;`). 3. The input string will only contain valid HTML entity references. **Example**: ```python assert decode_html_entities(\\"Hello &amp; welcome to StackOverflow!\\") == \\"Hello & welcome to StackOverflow!\\" assert decode_html_entities(\\"4 &lt; 5 &gt; 3\\") == \\"4 < 5 > 3\\" assert decode_html_entities(\\"Price is 100 &euro;\\") == \\"Price is 100 â‚¬\\" assert decode_html_entities(\\"Use hex &#x26; decimal &#38; for &#x1F600; emoji\\") == \\"Use hex & decimal & for ðŸ˜€ emoji\\" ``` **Notes**: - Use the `html5` dictionary for named character references. - Ensure to handle entity references both with `&name;` and `&name` formats. - For numeric entities, consider both decimal (`&#number;`) and hexadecimal (`&#xnumber;`). # Additional Information: - You can refer to the official documentation of the `html.entities` module for more details: https://docs.python.org/3/library/html.entities.html Implement the function `decode_html_entities` to pass the provided test cases.","solution":"import html def decode_html_entities(input_str: str) -> str: Decodes HTML entities in the given string using the html.entities module. Args: input_str (str): A string containing HTML entity references. Returns: str: A string with HTML entity references replaced by their corresponding Unicode characters. return html.unescape(input_str)"},{"question":"**Pandas Display Configuration and DataFrame Manipulation** You are tasked with analyzing a large dataset and need to configure the pandas display settings to optimize the process. Your task is to implement a function that performs the following: 1. Configures the pandas display settings based on specified requirements. 2. Generates a sample DataFrame. 3. Displays the DataFrame according to the configured settings. 4. Resets the settings to their default values after displaying the DataFrame. **Function Signature:** ```python def configure_and_display_dataframe(rows: int, cols: int, max_rows: int, max_columns: int, precision: int) -> None: pass ``` **Input:** - `rows` (int): the number of rows in the sample DataFrame. - `cols` (int): the number of columns in the sample DataFrame. - `max_rows` (int): the maximum number of rows to display. - `max_columns` (int): the maximum number of columns to display. - `precision` (int): the number of decimal places to display. **Output:** - The function doesn\'t return anything but prints the DataFrame with the configured display settings. **Constraints:** - `rows` and `cols` are positive integers greater than 0. - `max_rows`, `max_columns`, and `precision` are positive integers. **Performance Requirements:** - The function should efficiently handle DataFrames with up to 10,000 rows and 100 columns. **Example:** ```python # Configure and display a DataFrame with the specified settings configure_and_display_dataframe(rows=15, cols=10, max_rows=10, max_columns=5, precision=4) ``` **Behavior:** This will configure the pandas options to display a maximum of 10 rows and 5 columns, with values displayed to 4 decimal places. It will then create a 15x10 DataFrame of random numbers, print it according to these settings, and finally reset the display settings. **Additional Notes:** - Be sure to use the `option_context` to configure the settings temporarily. - Use `numpy` to generate random data for the DataFrame (`np.random.randn` can be useful). --- This question tests the student\'s ability to manipulate pandas settings and display options, as well as their understanding of DataFrame creation and management.","solution":"import pandas as pd import numpy as np def configure_and_display_dataframe(rows: int, cols: int, max_rows: int, max_columns: int, precision: int) -> None: Configure pandas display settings and display a sample DataFrame. Args: - rows (int): Number of rows in the sample DataFrame. - cols (int): Number of columns in the sample DataFrame. - max_rows (int): Maximum number of rows to display. - max_columns (int): Maximum number of columns to display. - precision (int): Number of decimal places to display. Returns: - None # Generate a sample DataFrame df = pd.DataFrame(np.random.randn(rows, cols)) # Set pandas display options using option_context to ensure they are temporary with pd.option_context(\'display.max_rows\', max_rows, \'display.max_columns\', max_columns, \'display.precision\', precision): print(df)"},{"question":"**Python Coding Assessment Question: Working with `pathlib`** **Objective:** Implement a Python function that utilizes the `pathlib` module to manage and analyze filesystem paths. **Task:** Write a function `organize_files` that takes a single argument: 1. `directory_path`: A string representing the path to a directory. The function should: 1. Ensure the provided path is a directory. If not, raise a `ValueError`. 2. List all the files (non-directory files) inside the given directory and its subdirectories. 3. Group these files based on their file extensions. 4. For each group of files sharing the same extension, create a subdirectory named after the extension (e.g., `.txt`, `.py`, etc.) within the given directory. 5. Move each file into the corresponding subdirectory. **Example:** ```python from pathlib import Path def organize_files(directory_path): # Your implementation here. pass # Example usage # Assume we have a directory \'test_dir\' with files: # \'test_dir/file1.txt\', \'test_dir/file2.jpg\', \'test_dir/subdir/file3.txt\' organize_files(\'test_dir\') # The \'test_dir\' structure after running the function should be: # test_dir # â”œâ”€â”€ .txt # â”‚ â”œâ”€â”€ file1.txt # â”‚ â””â”€â”€ file3.txt # â””â”€â”€ .jpg # â””â”€â”€ file2.jpg ``` **Constraints:** - Do not remove or overwrite existing files. - Handle any necessary errors, such as directories not existing or files being inaccessible. - It should be efficient for directories containing a large number of files. **Additional Notes:** - You may use any functionality provided by the `pathlib` module. - Consider edge cases, such as empty directories or files without extensions. **Evaluation Criteria:** - Correctness: The function should correctly group and move files as specified. - Robustness: Adequate error handling and edge case management. - Use of `pathlib`: Effective and appropriate use of the moduleâ€™s functionalities. - Efficiency: The function should handle large directories efficiently. **Input/Output:** ```python def organize_files(directory_path: str) -> None: ... ``` **Sample Test Case:** Given a directory structure before running the function: ``` test_dir â”œâ”€â”€ file1.txt â”œâ”€â”€ file2.jpg â””â”€â”€ subdir â””â”€â”€ file3.txt ``` After running `organize_files(\'test_dir\')`, the directory structure should be organized as: ``` test_dir â”œâ”€â”€ .txt â”‚ â”œâ”€â”€ file1.txt â”‚ â””â”€â”€ file3.txt â””â”€â”€ .jpg â””â”€â”€ file2.jpg ``` **Ensure your function is efficient and leverages the `pathlib` module properly.**","solution":"from pathlib import Path def organize_files(directory_path): Organize files in the given directory by their extensions. Parameters: directory_path (str): Path to the directory to organize. Raises: ValueError: If the provided path is not a directory. dir_path = Path(directory_path) # Ensure the provided path is a directory if not dir_path.is_dir(): raise ValueError(f\\"The provided path {directory_path} is not a directory.\\") # Dictionary to store files grouped by extension files_by_extension = {} # Iterate through all files in the directory and subdirectories for file_path in dir_path.rglob(\'*\'): if file_path.is_file(): ext = file_path.suffix if ext not in files_by_extension: files_by_extension[ext] = [] files_by_extension[ext].append(file_path) # Create subdirectories and move files for ext, files in files_by_extension.items(): # Get the subdirectory path named after the extension ext_dir = dir_path / ext ext_dir.mkdir(exist_ok=True) for file_path in files: destination = ext_dir / file_path.name # Move the file to the corresponding subdirectory file_path.rename(destination)"},{"question":"**Advanced PyTorch Coding Challenge: Profiling and Extensions** As an experienced software engineer at a company using PyTorch in production, your task is to instrument PyTorch code to gather performance data and attach metadata to models. You should demonstrate your understanding by solving the following tasks: 1. **Task 1: Profiling PyTorch Operators** You are required to write a Python script that leverages PyTorch\'s profiling capabilities to measure the execution time of PyTorch operations in a sample model. Implement a function `profile_model_operations` that: - Takes a PyTorch model and a dummy input tensor. - Uses `torch.autograd.profiler` to profile the model execution. - Prints out the times taken by each operation in the forward pass. ```python import torch import torch.nn as nn import torch.optim as optim def profile_model_operations(model: nn.Module, input_tensor: torch.Tensor): Profiles the execution time of operations in the given PyTorch model. Args: model (nn.Module): The PyTorch model to be profiled. input_tensor (torch.Tensor): A dummy input tensor appropriate for the model. Returns: None: Prints the operation times to stdout. # Your implementation here # Example usage class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.fc1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.fc2 = nn.Linear(10, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x model = SampleModel() dummy_input = torch.randn(1, 10) profile_model_operations(model, dummy_input) ``` 2. **Task 2: Attaching Metadata to TorchScript Models** Implement a function `save_model_with_metadata` that saves a given PyTorch model as a TorchScript module with additional metadata. The metadata should include a description and the creation timestamp of the model. ```python import torch import torch.jit as jit import json from datetime import datetime def save_model_with_metadata(model: nn.Module, file_path: str, description: str): Saves the given model as a TorchScript module with attached metadata. Args: model (nn.Module): The PyTorch model to be saved. file_path (str): The file path to save the TorchScript model. description (str): Description metadata to be attached to the saved model. Returns: None # Your implementation here # Example usage model = SampleModel() save_model_with_metadata(model, \'sample_model.pt\', \'This is a sample model for demo purposes.\') ``` **Constraints:** - The saved model should contain a file named `description.txt` with the provided description. - The saved model should contain a file named `timestamp.txt` with the current datetime. **Notes:** - Make sure your implementations are efficient and handle potential edge cases. - Document your code to explain the steps and logic used.","solution":"import torch import torch.nn as nn from torch.autograd import profiler import torch.jit as jit import json from datetime import datetime def profile_model_operations(model: nn.Module, input_tensor: torch.Tensor): Profiles the execution time of operations in the given PyTorch model. Args: model (nn.Module): The PyTorch model to be profiled. input_tensor (torch.Tensor): A dummy input tensor appropriate for the model. Returns: None: Prints the operation times to stdout. model.eval() with profiler.profile(record_shapes=True) as prof: with profiler.record_function(\\"model_inference\\"): model(input_tensor) print(prof.key_averages().table(sort_by=\\"cpu_time_total\\", row_limit=10)) def save_model_with_metadata(model: nn.Module, file_path: str, description: str): Saves the given model as a TorchScript module with attached metadata. Args: model (nn.Module): The PyTorch model to be saved. file_path (str): The file path to save the TorchScript model. description (str): Description metadata to be attached to the saved model. Returns: None scripted_model = jit.script(model) metadata = { \'description\': description, \'timestamp\': datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") } metadata_file_path = file_path + \\".json\\" with open(metadata_file_path, \'w\') as f: json.dump(metadata, f) torch.jit.save(scripted_model, file_path) # Example usage for profiling class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.fc1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.fc2 = nn.Linear(10, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x model = SampleModel() dummy_input = torch.randn(1, 10) profile_model_operations(model, dummy_input) # Example usage for saving model with metadata save_model_with_metadata(model, \'sample_model.pt\', \'This is a sample model for demo purposes.\')"},{"question":"**Objective:** Implement a Python-based chat server and client using TCP sockets where the server handles multiple clients simultaneously using non-blocking sockets and the `select` module. **Background:** - You need to set up a server that can accept multiple client connections. - The clients should be able to send messages to the server, which the server will broadcast to all connected clients. - The communication should be designed to handle messages of variable lengths. **Task:** **1. Server Implementation:** Implement a class `ChatServer` with the following methods: - `__init__(self, host, port)`: Initializes the server socket. - `start(self)`: Starts the server to accept connections and manage active clients. - `broadcast(self, message, sender_socket)`: Sends the message to all connected clients except the sender. - `shutdown(self)`: Gracefully shuts down the server. **2. Client Implementation:** Implement a class `ChatClient` with the following methods: - `__init__(self, host, port)`: Initializes the client socket and connects to the server. - `send_message(self, message)`: Sends a message to the server. - `receive_messages(self)`: Receives messages from the server. - `close(self)`: Closes the connection to the server. **Requirements:** - The server should handle new connections and incoming messages from multiple clients concurrently using non-blocking sockets. - Use the `select` module to handle multiple connections in the server. - Ensure communication handles and tests for complete messages considering partial transmissions. - Implement proper error handling for network failures. **Constraints:** - Use only Python\'s standard libraries (`socket`, `select`, `threading`). **Input/Output:** _Server (ChatServer):_ ```python server = ChatServer(host=\'localhost\', port=8080) server.start() ``` _Client (ChatClient):_ ```python client = ChatClient(host=\'localhost\', port=8080) client.send_message(\\"Hello Server!\\") received_messages = client.receive_messages() for msg in received_messages: print(msg) client.close() ``` **Example:** ```python # Start the server chat_server = ChatServer(host=\'localhost\', port=8080) chat_server.start() # Create two clients and connect them to the server client1 = ChatClient(host=\'localhost\', port=8080) client2 = ChatClient(host=\'localhost\', port=8080) # Client1 sends a message client1.send_message(\\"Hello from Client 1\\") # Client2 receives the message messages = client2.receive_messages() print(messages) # Should print: [\\"Hello from Client 1\\"] # Close the clients client1.close() client2.close() # Shutdown the server chat_server.shutdown() ``` Write clean, maintainable code and include comments to explain key parts of your implementation.","solution":"import socket import select import threading class ChatServer: def __init__(self, host, port): self.host = host self.port = port self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.server_socket.bind((self.host, self.port)) self.server_socket.listen(5) self.server_socket.setblocking(False) self.clients = [] self.running = True def start(self): print(f\'Server started on {self.host}:{self.port}\') self.manage_connections() def manage_connections(self): try: while self.running: read_sockets, _, exception_sockets = select.select([self.server_socket] + self.clients, [], self.clients) for notified_socket in read_sockets: if notified_socket == self.server_socket: client_socket, client_address = self.server_socket.accept() client_socket.setblocking(False) self.clients.append(client_socket) print(f\'Accepted new connection from {client_address}\') else: message = notified_socket.recv(1024) if message: self.broadcast(message, notified_socket) else: self.clients.remove(notified_socket) notified_socket.close() for notified_socket in exception_sockets: self.clients.remove(notified_socket) notified_socket.close() except Exception as e: print(f\\"Error: {e}\\") finally: self.shutdown() def broadcast(self, message, sender_socket): for client in self.clients: if client != sender_socket: client.send(message) def shutdown(self): self.running = False self.server_socket.close() for client in self.clients: client.close() print(f\'Server on {self.host}:{self.port} shutdown\') class ChatClient: def __init__(self, host, port): self.host = host self.port = port self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.client_socket.connect((self.host, self.port)) self.client_socket.setblocking(False) def send_message(self, message): self.client_socket.send(message.encode()) def receive_messages(self): messages = [] try: while True: message = self.client_socket.recv(1024) if not message: break messages.append(message.decode()) except BlockingIOError: pass return messages def close(self): self.client_socket.close()"},{"question":"**Challenging Coding Assessment Question: PyTorch Inference Threading Optimization** As a developer, you are tasked with optimizing a PyTorch model\'s inference performance by leveraging both inter-op and intra-op parallelism. You will implement a function that: 1. Loads a pretrained model. 2. Runs inference in parallel using multiple threads. 3. Tunes the number of inter-op and intra-op threads to optimize performance. Implement the function `optimize_inference` with the following specifications: # Function Signature ```python def optimize_inference(model_path: str, input_data: torch.Tensor, inter_threads: int, intra_threads: int) -> torch.Tensor: pass ``` # Parameters - `model_path` (`str`): Path to the pretrained PyTorch model in `TorchScript` format. - `input_data` (`torch.Tensor`): Input tensor on which to run the model\'s inference. - `inter_threads` (`int`): The number of inter-op threads to be set for the inference. - `intra_threads` (`int`): The number of intra-op threads to be set for the inference. # Returns - `torch.Tensor`: The output tensor resulting from the model inference. # Constraints - The function should load the model using `torch.jit.load`. - The function should set the specified number of inter-op and intra-op threads using `torch.set_num_interop_threads` and `torch.set_num_threads`, respectively. - The inference must be performed using the specified threading configurations. - Consider large tensor operations or computationally expensive models for effective multithreading. # Performance Requirements - The function should minimize the latency of model inference as much as possible by effectively using the given threading parameters. - Ensure no oversubscription occurs (i.e., the number of threads used should not exceed the number of physical CPU cores available). # Example ```python import torch # Example usage model_path = \'path_to_model.pt\' input_data = torch.randn(1024, 1024) inter_threads = 4 intra_threads = 8 output = optimize_inference(model_path, input_data, inter_threads, intra_threads) print(output) ``` # Note - You may use additional utility functions if necessary, but the main implementation should be within `optimize_inference`. - Ensure your code is well-commented and follows best practices for readability and optimization.","solution":"import torch def optimize_inference(model_path: str, input_data: torch.Tensor, inter_threads: int, intra_threads: int) -> torch.Tensor: Optimizes and runs a PyTorch model inference using specified inter-op and intra-op thread settings. Parameters: model_path (str): Path to the pretrained PyTorch model in TorchScript format. input_data (torch.Tensor): Input tensor on which to run the model\'s inference. inter_threads (int): The number of inter-op threads to be set for the inference. intra_threads (int): The number of intra-op threads to be set for the inference. Returns: torch.Tensor: The output tensor resulting from the model inference. # Load the pretrained model from the file path model = torch.jit.load(model_path) # Set the number of inter-op and intra-op threads for efficient parallel execution torch.set_num_interop_threads(inter_threads) torch.set_num_threads(intra_threads) # Perform the inference model.eval() # Set the model to evaluation mode with torch.no_grad(): # Disable gradient computation for inference output = model(input_data) return output"},{"question":"<|Analysis Begin|> The provided documentation describes the event loop in Python\'s asyncio package in detail. It explains how to obtain the event loop, run and stop it, schedule callbacks, create Futures and Tasks, manage network connections, servers, and subprocesses, and work with sockets. It also details the APIs for managing DNS queries, pipes, Unix signals, running code in thread or process pools, enabling debug mode, and error handling. The core functionalities that could be focused on in the assessment question include: - Creating and running an event loop. - Scheduling callbacks and handling them. - Working with Futures and creating tasks. - Managing network connections and servers. - Executing subprocesses and handling I/O. Given this, the question should assess the student\'s ability to use asyncio\'s event loop APIs to build an asynchronous application. The evaluation can involve multiple aspects, such as creating tasks, handling networking, scheduling timed events, and managing subprocesses. <|Analysis End|> <|Question Begin|> # Asynchronous Task Scheduler with Networking **Objective:** Implement an asynchronous task scheduler that schedules multiple tasks to be executed with different delays. Additionally, implement a simple TCP server to which clients can connect to fetch the status of scheduled tasks. This will demonstrate the fundamental and advanced concepts of the asyncio package, including event loops, task management, scheduling, and network I/O. **Requirements:** 1. **Task Scheduler:** - Implement a function `schedule_task(delay: int, name: str)` that schedules a task to be executed after a given delay. - The task should simply print a statement `Task {name} executed`. - Implement a list to store the statuses of tasks including their name, scheduled delay, and completed status. 2. **TCP Server:** - Implement a TCP server that clients can connect to query the status of all tasks. - The server should listen on `localhost` and port `8888`. - Upon client connection, send the list of tasks and their statuses. 3. **Main Function:** - Obtain the event loop. - Schedule multiple tasks with different delays. - Start the TCP server to listen for incoming client connections. **Constraints:** - Use high-level asyncio functions wherever possible. - Handle exceptions gracefully and ensure that the server can close cleanly on exit. **Input:** - List of tasks with names and delays. **Output:** - When a task is scheduled and executed, output `Task {name} executed` to the console. - When a client connects to the server, send the list of tasks and statuses as a formatted string. **Example:** 1. Scheduling tasks: ```python schedule_task(2, \\"Task1\\") schedule_task(5, \\"Task2\\") ``` Output: ``` Task Task1 executed Task Task2 executed ``` 2. Server output upon client connection: ``` [ {\\"name\\": \\"Task1\\", \\"delay\\": 2, \\"status\\": \\"Completed\\"}, {\\"name\\": \\"Task2\\", \\"delay\\": 5, \\"status\\": \\"Completed\\"} ] ``` **Code Implementation:** ```python import asyncio import json tasks = [] async def schedule_task(delay: int, name: str): await asyncio.sleep(delay) print(f\'Task {name} executed\') for task in tasks: if task[\\"name\\"] == name: task[\\"status\\"] = \\"Completed\\" async def handle_client(reader, writer): data = json.dumps(tasks).encode() writer.write(data) await writer.drain() writer.close() await writer.wait_closed() async def tcp_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() async def main(): task_list = [(\\"Task1\\", 2), (\\"Task2\\", 5)] for name, delay in task_list: tasks.append({\\"name\\": name, \\"delay\\": delay, \\"status\\": \\"Scheduled\\"}) asyncio.create_task(schedule_task(delay, name)) await tcp_server() if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server closed.\\") ``` **Submission:** Submit the fully implemented code fulfilling the above requirements. Ensure to handle cases where tasks may not execute due to errors and the server can shut down gracefully.","solution":"import asyncio import json tasks = [] async def schedule_task(delay: int, name: str): await asyncio.sleep(delay) print(f\'Task {name} executed\') for task in tasks: if task[\\"name\\"] == name: task[\\"status\\"] = \\"Completed\\" async def handle_client(reader, writer): data = json.dumps(tasks).encode() writer.write(data) await writer.drain() writer.close() await writer.wait_closed() async def tcp_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() async def main(): task_list = [(\\"Task1\\", 2), (\\"Task2\\", 5)] for name, delay in task_list: tasks.append({\\"name\\": name, \\"delay\\": delay, \\"status\\": \\"Scheduled\\"}) asyncio.create_task(schedule_task(delay, name)) await tcp_server() if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server closed.\\")"},{"question":"**Question:** # Visualization with Seaborn Objective: Utilize the seaborn library to create a complex visualization demonstrating your understanding of its functionality. This assessment will evaluate your ability to manipulate data, apply seaborn\'s plotting functions, and customize visual aspects of the plot. Task: Using the `fmri` dataset, create a combined visualization that provides insights into the brain activity signals measured over time. Your plot should include the following elements: 1. **Line Plot**: - Plot the `signal` against `timepoint`. - The dataset should be grouped by the `subject` to show individual lines for each subject. - Use different line markers and ensure the lines are distinguishable. 2. **Error Bands**: - Add an error band (interval) around the lines representing the standard error of the signal within each group (`region`). - Use different colors to differentiate `region`. 3. **Linestyles**: - Map the `event` to different linestyles within the line plot. 4. **Legends and Axis Labels**: - Ensure that the plot includes appropriate axis labels (`Timepoint` for x-axis and `Signal` for y-axis). - Add a legend indicating the region and event for clarity. 5. **Annotations**: - Annotate significant points using markers. **Constraints**: - Use only the seaborn package and matplotlib for plots. - The plot should be clearly readable, with proper labeling and a legend. - Ensure to handle any missing values in the dataset appropriately. Input: - No specific input required; use the provided `fmri` dataset from seaborn. Output: - A rendered complex plot as described. **Performance Requirements**: - The plot rendering should be done efficiently, ideally within a few seconds. **Dataset Loading**: ```python import seaborn as sns from seaborn import objects as so fmri = sns.load_dataset(\\"fmri\\") ``` **Expected Code Structure**: ```python import seaborn as sns from seaborn import objects as so import matplotlib.pyplot as plt # Load dataset fmri = sns.load_dataset(\\"fmri\\") # Filter or preprocess data if necessary # ... # Create the main plot p = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") # Add a line plot with groups p.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\", linewidth=1), group=\\"subject\\") # Add error bands p.add(so.Band(), so.Est(), group=\\"region\\") # Customize plot (axis labels, legend, etc.) p.label(x=\\"Timepoint\\", y=\\"Signal\\") # Render the plot p.show() ``` Ensure your final plot meets all the specified requirements and demonstrates an advanced understanding of seaborn\'s capabilities.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load dataset fmri = sns.load_dataset(\\"fmri\\") # Create the main plot plt.figure(figsize=(14, 8)) # Plot the line plot with error bands sns.lineplot(data=fmri, x=\'timepoint\', y=\'signal\', hue=\'region\', style=\'event\', markers=True, ci=\'sd\', estimator=np.mean, err_style=\'band\', palette=\'colorblind\', dashes=True) # Customize plot (axis labels, legend, etc.) plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.title(\'FMRI Signal Over Time by Region and Event\') # Adding annotations for some specific significant points # For demonstration, we annotate the peak of the signal for regions significant_points = fmri.groupby([\'region\', \'event\']).apply(lambda df: df[df[\'signal\'] == df[\'signal\'].max()]) for _, row in significant_points.iterrows(): plt.annotate(\'Peak\', (row[\'timepoint\'], row[\'signal\']), textcoords=\\"offset points\\", xytext=(5,10), ha=\'center\', arrowprops=dict(arrowstyle=\\"->\\", connectionstyle=\\"arc3,rad=.5\\")) # Show legend plt.legend(title=\'Region and Event\') # Show the plot plt.show()"},{"question":"Objective: Implement a Python function that reads an input text file, compresses its content using the LZMA compression algorithm, writes the compressed data to a new file, then reads the compressed file to decompress its content, and finally writes the decompressed data to another new file. Requirements: 1. The function should take three arguments: - `input_file_path`: A string that specifies the path of the input text file. - `compressed_file_path`: A string that specifies the path where the compressed file will be saved. - `decompressed_file_path`: A string that specifies the path where the decompressed file will be saved. 2. The function should: - Read the content of the `input_file_path` file. - Compress the content and write it to `compressed_file_path` using the LZMA algorithm. - Read the compressed content from `compressed_file_path`. - Decompress the content and write it to `decompressed_file_path`. 3. Handle any potential exceptions that might occur during file operations or compression/decompression processes. Implementation Details: - Use the `lzma` module for both compression and decompression. - Ensure that the original content and the decompressed content match exactly. - Add appropriate error handling to catch and handle `IOError`, `LZMAError`, and any other potential exceptions. Constraints: - You may assume that the input text file is not larger than 100MB. - You should ensure the operations are memory efficient and handle large files. Example Usage: ```python def compress_and_decompress(input_file_path: str, compressed_file_path: str, decompressed_file_path: str) -> None: # Implement the function here # Example Usage compress_and_decompress(\'uncompressed.txt\', \'compressed.xz\', \'decompressed.txt\') # Check that the content match with open(\'uncompressed.txt\', \'r\') as f1, open(\'decompressed.txt\', \'r\') as f2: assert f1.read() == f2.read(), \\"The decompressed content does not match the original!\\" ``` Notes: - Use appropriate read/write modes (`\'r\'`, `\'wb\'`, `\'rb\'`, etc.) for file operations. - Remember to close the file handlers properly to avoid potential resource leaks. - Make sure the function is well-documented and follows good coding practices. Good luck!","solution":"import lzma def compress_and_decompress(input_file_path: str, compressed_file_path: str, decompressed_file_path: str) -> None: try: # Read the original file content with open(input_file_path, \'rb\') as input_file: original_content = input_file.read() # Compress the content and write to compressed_file_path with lzma.open(compressed_file_path, \'wb\') as compressed_file: compressed_file.write(original_content) # Read the compressed content with lzma.open(compressed_file_path, \'rb\') as compressed_file: compressed_content = compressed_file.read() # Decompress the content and write to decompressed_file_path with open(decompressed_file_path, \'wb\') as decompressed_file: decompressed_file.write(compressed_content) except (IOError, lzma.LZMAError) as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Create a Seaborn plot that visualizes the trajectory of brain network activity over time. Context You are provided with a dataset that contains timepoint-specific brain network activity data. Your task is to load and preprocess this dataset and then visualize specific trajectories of brain network activities over time using Seaborn. Instructions 1. **Load the Dataset:** Use `seaborn.load_dataset` to load the dataset named `brain_networks` with headers in the first three rows and using the first column as the index. 2. **Data Transformation:** Transform the dataset using the following steps: - Rename the index to `timepoint`. - Stack, group by multiple columns, and compute the mean. - Unstack by the `network` column and reset the index. - Filter the dataset to include only rows where the `timepoint` is less than 100. 3. **Plot the Data:** Use `seaborn.objects.Plot` to create a pairwise plot with trajectories: - Set the x-axis variables to `[\'5\', \'8\', \'12\', \'15\']` and the y-axis variables to `[\'6\', \'13\', \'16\']`. - Customize the layout size to `(8, 5)` and ensure axes are shared. - Add the plot paths with a linewidth of 1 and transparency of 0.8, colored by the `hemi` column. Expected Function Implementation ```python import seaborn.objects as so from seaborn import load_dataset def plot_brain_networks(): # Load dataset networks = (load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\")) # Create plot p = (so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(8, 5)) .share(x=True, y=True)) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") return p # Call the function to display the plot plot_brain_networks() ``` Output Your function should output the plot visualizing brain network activity trajectories based on the given transformation steps and plot specifications. Constraints - Ensure the plot accurately reflects the transformations and customizations specified. - Use the `seaborn.objects` module only. Performance The function should run efficiently, handling the dataset within reasonable time frames without excessive memory usage.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_brain_networks(): # Load dataset with header and index configuration networks = (load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\")) # Create plot using seaborn.objects.Plot p = (so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(8, 5)) .share(x=True, y=True)) # Add trajectories with custom styling p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") return p"},{"question":"**Question: Implement a Unix User Report Generator** You are tasked with creating a Unix User Report Generator using the `pwd` module. The report generator should provide a summarized report of user details from the Unix password database. # Requirements 1. **Function Name**: `generate_user_report` 2. **Input**: None 3. **Output**: A dictionary where the keys are user identifiers (a combination of `pw_name` and `pw_uid`) and the values are dictionaries containing the following information: - `name`: The user\'s login name. - `uid`: The user\'s numeric user ID. - `gid`: The user\'s numeric group ID. - `home_dir`: The user\'s home directory. - `shell`: The user\'s command interpreter (shell). # Constraints: - The function should handle scenarios where the password database entries might not be found gracefully. - The system should have the `pwd` module available and be running on a Unix-based system. # Example Output: ```python { \\"user1:1001\\": { \\"name\\": \\"user1\\", \\"uid\\": 1001, \\"gid\\": 1002, \\"home_dir\\": \\"/home/user1\\", \\"shell\\": \\"/bin/bash\\" }, \\"user2:1002\\": { \\"name\\": \\"user2\\", \\"uid\\": 1002, \\"gid\\": 1003, \\"home_dir\\": \\"/home/user2\\", \\"shell\\": \\"/bin/zsh\\" }, # More users... } ``` # Performance Requirements: - The implementation should efficiently handle a large number of users, ideally in O(n) time complexity with respect to the number of users in the password database. # Usage: You can assume that your function will be called in a script designed to generate and print the user report. Here\'s a sample usage: ```python def main(): user_report = generate_user_report() for user_id, details in user_report.items(): print(f\\"{user_id}: {details}\\") if __name__ == \\"__main__\\": main() ``` # Implementation Notes: - Use the `pwd.getpwall()` function to retrieve all user entries. - Construct the identifier using the user\'s name and uid to ensure uniqueness. - Handle any potential exceptions that might arise, such as missing fields. Write the function `generate_user_report` below: ```python import pwd def generate_user_report(): # Your implementation here pass ```","solution":"import pwd def generate_user_report(): try: user_entries = pwd.getpwall() user_report = {} for entry in user_entries: user_id = f\\"{entry.pw_name}:{entry.pw_uid}\\" user_report[user_id] = { \\"name\\": entry.pw_name, \\"uid\\": entry.pw_uid, \\"gid\\": entry.pw_gid, \\"home_dir\\": entry.pw_dir, \\"shell\\": entry.pw_shell } return user_report except Exception as e: print(f\\"An error occurred: {e}\\") return {}"},{"question":"**Question:** You are given a `DataFrame` containing data about various cars, similar to the `mpg` dataset shown in the documentation. Your task is to: 1. Use Seaborn to load the provided `mpg` dataset. 2. Create three different residual plots that: - Plot the residuals of `horsepower` against `mpg`. - Remove higher-order trends with an order of 3. - Add a LOWESS curve to the plot with specific customizations. # Input and Output Formats: - **Input:** None - **Output:** None (but the function should display plots). # Constraints: - Use Seaborn for all visualizations. - Ensure the plots are displayed inline if using a Jupyter Notebook. # Hints: - Consider using `sns.residplot` for creating the residual plots. - Use keywords like `data`, `x`, `y`, `order`, `lowess`, and `line_kws` appropriately. # Performance Requirements: - The function must run efficiently without any significant delays, even for large datasets. ```python import seaborn as sns import matplotlib.pyplot as plt def create_residual_plots(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Plot 1: Basic residual plot of horsepower versus mpg plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\'Residual Plot: horsepower vs mpg\') # Plot 2: Residual plot with higher-order trend removal (order=3) plt.subplot(1, 3, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=3) plt.title(\'Residual Plot (Order 3): horsepower vs mpg\') # Plot 3: Residual plot with LOWESS smoothing plt.subplot(1, 3, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residual Plot with LOWESS: horsepower vs mpg\') plt.tight_layout() plt.show() # Call the function to generate the residual plots create_residual_plots() ``` This function generates three residual plots side by side to compare different analyses of the residuals from the linear regression model.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_residual_plots(): Creates and displays three residual plots for the \'mpg\' dataset: 1. A residual plot of horsepower versus mpg. 2. A residual plot removing higher-order trends (order=3). 3. A residual plot with a LOWESS smoothing curve. # Load the \'mpg\' dataset mpg = sns.load_dataset(\\"mpg\\") # Plot 1: Basic residual plot of horsepower versus mpg plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\'Residual Plot: horsepower vs mpg\') # Plot 2: Residual plot with higher-order trend removal (order=3) plt.subplot(1, 3, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=3) plt.title(\'Residual Plot (Order 3): horsepower vs mpg\') # Plot 3: Residual plot with LOWESS smoothing plt.subplot(1, 3, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residual Plot with LOWESS: horsepower vs mpg\') plt.tight_layout() plt.show() # Call the function to generate the residual plots create_residual_plots()"},{"question":"**Objective:** Implement a pair of functions `encode_data` and `decode_data` that use the `xdrlib` module to pack and unpack a specific data structure. **Data Structure:** The data structure to be encoded and decoded is a dictionary containing: - An integer `id` - A string `name` - A list of float values `scores` **Function Requirements:** - `encode_data(data: dict) -> bytes`: - Takes a dictionary with keys `\\"id\\"` (int), `\\"name\\"` (string), and `\\"scores\\"` (list of floats). - Encodes this dictionary into XDR format using the `xdrlib.Packer` class. - Returns the encoded data as a bytes object. - `decode_data(data: bytes) -> dict`: - Takes a bytes object encoded by `encode_data`. - Decodes this data back into the original dictionary format using the `xdrlib.Unpacker` class. - Returns the decoded dictionary. **Inputs and Outputs:** - For `encode_data(data: dict)`: - Input: A dictionary with the structure: `{\'id\': 1, \'name\': \'Alice\', \'scores\': [85.5, 90.0, 88.75]}` - Output: Bytes object representing the packed XDR format. - For `decode_data(data: bytes)`: - Input: A bytes object from `encode_data`. - Output: The original dictionary with the structure: `{\'id\': 1, \'name\': \'Alice\', \'scores\': [85.5, 90.0, 88.75]}` **Constraints:** - The `id` should be a valid integer. - The `name` should be a non-empty string. - The `scores` list should contain only float values. **Exception Handling:** - Proper error handling should be implemented to catch and handle any exceptions from the `xdrlib` module during the packing and unpacking process. # Example: ```python data = { \'id\': 1, \'name\': \'Alice\', \'scores\': [85.5, 90.0, 88.75] } encoded_data = encode_data(data) print(encoded_data) # Output: b\'...\' (XDR packed bytes data) decoded_data = decode_data(encoded_data) print(decoded_data) # Output: {\'id\': 1, \'name\': \'Alice\', \'scores\': [85.5, 90.0, 88.75]} ``` You are required to define the `encode_data` and `decode_data` functions, ensuring they meet the input-output specifications and handle any potential errors effectively.","solution":"import xdrlib def encode_data(data): Encodes a dictionary into XDR format using xdrlib.Packer. Args: data (dict): Dictionary with keys \\"id\\" (int), \\"name\\" (str), and \\"scores\\" (list of floats). Returns: bytes: Encoded data as a bytes object in XDR format. packer = xdrlib.Packer() try: packer.pack_int(data[\'id\']) packer.pack_string(data[\'name\'].encode()) packer.pack_int(len(data[\'scores\'])) for score in data[\'scores\']: packer.pack_float(score) except Exception as e: raise ValueError(f\\"Error packing data: {e}\\") return packer.get_buffer() def decode_data(data): Decodes a bytes object previously encoded by encode_data into its original dictionary form using xdrlib.Unpacker. Args: data (bytes): Bytes object in XDR format. Returns: dict: Decoded dictionary with keys \\"id\\", \\"name\\", and \\"scores\\". unpacker = xdrlib.Unpacker(data) try: id = unpacker.unpack_int() name = unpacker.unpack_string().decode() scores_length = unpacker.unpack_int() scores = [unpacker.unpack_float() for _ in range(scores_length)] except Exception as e: raise ValueError(f\\"Error unpacking data: {e}\\") return {\'id\': id, \'name\': name, \'scores\': scores}"},{"question":"Coding Assessment Question # Objective: You are given a series of Parquet files that form a large dataset on disk. The task is to load, preprocess, and analyze these datasets efficiently using pandas. # Problem Statement: Suppose you have a directory of Parquet files, where each file contains time series data for a specific year. Each file has the following columns: `\\"timestamp\\"`, `\\"name\\"`, `\\"id\\"`, `\\"x\\"`, and `\\"y\\"`. You need to implement a function that calculates and returns the total memory usage reduction achieved by: 1. Loading only the necessary columns. 2. Converting appropriate columns to more memory-efficient data types. 3. Calculating the value counts for the `\\"name\\"` column. Your function should process the files in chunks to ensure that it can handle datasets larger than available memory. # Function Signature: ```python import pandas as pd from typing import List def process_timeseries(directory: str, columns: List[str]) -> float: Args: - directory (str): Path to the directory containing Parquet files. - columns (List[str]): List of column names to be loaded (e.g., [\\"timestamp\\", \\"name\\"]). Returns: - float: The reduction in memory usage as a ratio (0 to 1). pass ``` # Input: - `directory`: A string representing the path to the directory containing Parquet files. - `columns`: A list of column names to load from each Parquet file. For this question, you will always be given `[\\"timestamp\\", \\"name\\", \\"id\\", \\"x\\", \\"y\\"]`. # Output: - Return the reduction in memory usage as a float ratio (from 0 to 1). # Constraints: 1. Each Parquet file fits individually into memory. 2. You should not load all files into memory at once. 3. Use the `Categorical` datatype for the `\\"name\\"` column. 4. Downcast numeric columns `\\"id\\"`, `\\"x\\"`, `\\"y\\"` to the smallest possible types. # Example: Given the directory structure with Parquet files: ``` data/ â”œâ”€â”€ ts_2000.parquet â”œâ”€â”€ ts_2001.parquet â”œâ”€â”€ ts_2002.parquet ... ``` Function call: ```python columns = [\\"timestamp\\", \\"name\\", \\"id\\", \\"x\\", \\"y\\"] reduction_ratio = process_timeseries(\\"data\\", columns) ``` The function should return a float value representing the reduction in memory usage. # Note: - Ensure to handle cases with large datasets using memory-efficient techniques as discussed. - Your implementation should reflect a significant understanding of pandas\' capabilities to manage large datasets.","solution":"import os import pandas as pd from typing import List def process_timeseries(directory: str, columns: List[str]) -> float: Calculate the reduction in memory usage for the specified columns by converting data types. Args: - directory (str): Path to the directory containing Parquet files. - columns (List[str]): List of column names to be loaded (e.g., [\\"timestamp\\", \\"name\\", \\"id\\", \\"x\\", \\"y\\"]). Returns: - float: The reduction in memory usage as a ratio (0 to 1). initial_memory_usage = 0 optimized_memory_usage = 0 for file in os.listdir(directory): if file.endswith(\\".parquet\\"): file_path = os.path.join(directory, file) df = pd.read_parquet(file_path, columns=columns) # Calculate initial memory usage initial_memory_usage += df.memory_usage(deep=True).sum() # Optimize the DataFrame df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"integer\\") df[\\"x\\"] = pd.to_numeric(df[\\"x\\"], downcast=\\"float\\") df[\\"y\\"] = pd.to_numeric(df[\\"y\\"], downcast=\\"float\\") # Calculate optimized memory usage optimized_memory_usage += df.memory_usage(deep=True).sum() # Calculate the reduction ratio reduction_ratio = (initial_memory_usage - optimized_memory_usage) / initial_memory_usage return reduction_ratio"},{"question":"**Problem Description:** You have been tasked with creating a small library management system. The system should allow adding new books, lending books to patrons, and keeping track of books currently lent out. Implement the following functionalities: 1. **Book Class:** - Create a class called `Book`. - Each book should have: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `isbn`: A unique string representing the ISBN number of the book. - The class should have a method `__str__` that returns a string in the format `\\"Title\\" by Author (ISBN: isbn)`. 2. **Library Class:** - Create a class called `Library`. - The `Library` class should maintain a list of books and a dictionary mapping ISBN numbers to the patron who has borrowed the book. - Implement the following methods: - `add_book(book: Book)`: Adds a new book to the library. - `lend_book(isbn: str, patron: str)`: Lends a book to a patron. If the book is already lent out, it should raise an Exception with the message `\\"Book already lent out\\"`. - `return_book(isbn: str)`: Returns a book to the library. If the book is not currently lent out, it should raise an Exception with the message `\\"Book was not lent out\\"`. - `available_books()`: Returns a list of books that are currently not lent out. - `lent_books()`: Returns a dictionary mapping ISBN numbers to the pattons who have borrowed them. **Input Constraints:** - Assume the `isbn` values in operations are always valid and correspond to books in the library. **Example Usage:** ```python # Initialize library and books library = Library() book1 = Book(\\"1984\\", \\"George Orwell\\", \\"1234567890\\") book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"0987654321\\") library.add_book(book1) library.add_book(book2) # Lending books library.lend_book(\\"1234567890\\", \\"Alice\\") try: library.lend_book(\\"1234567890\\", \\"Bob\\") except Exception as e: print(e) # Output: Book already lent out # Returning books library.return_book(\\"1234567890\\") try: library.return_book(\\"1234567890\\") except Exception as e: print(e) # Output: Book was not lent out # Checking available and lent books available = library.available_books() for book in available: print(book) # Output: \\"1984\\" by George Orwell (ISBN: 1234567890), \\"To Kill a Mockingbird\\" by Harper Lee (ISBN: 0987654321) lent = library.lent_books() print(lent) # Output: {} ``` **Solution Requirements:** - The solution should include proper class definitions, function implementations, and handle exceptions as specified. - Functions should demonstrate usage of lists, dictionaries, and string formatting. **Performance Constraints:** - Assume the number of books in the library `N` will not exceed 10,000. - Operations should perform in `O(1)` time on average for lending and returning books (assuming proper dictionary usage).","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn def __str__(self): return f\'\\"{self.title}\\" by {self.author} (ISBN: {self.isbn})\' class Library: def __init__(self): self.books = [] self.lent_out = {} def add_book(self, book): self.books.append(book) def lend_book(self, isbn, patron): if isbn in self.lent_out: raise Exception(\\"Book already lent out\\") for book in self.books: if book.isbn == isbn: self.lent_out[isbn] = patron return raise ValueError(\\"Book not found\\") def return_book(self, isbn): if isbn not in self.lent_out: raise Exception(\\"Book was not lent out\\") del self.lent_out[isbn] def available_books(self): return [book for book in self.books if book.isbn not in self.lent_out] def lent_books(self): return self.lent_out"},{"question":"Objective Demonstrate your comprehension of the `optparse` module (even though deprecated) by creating a Python script that processes a set of complex command-line options. Task Create a Python script that accomplishes the following: 1. **Define an OptionParser** with the following options: - `-f` or `--file`: a required string option to specify a filename. - `-v` or `--verbose`: an optional flag (boolean) to enable verbose mode. - `-n` or `--numbers`: an optional list of integers. The integers should be parsed from a comma-separated string. - `-m` or `--mode`: an optional choice from the set `[\'beginner\', \'intermediate\', \'advanced\']` with a default value of `intermediate`. 2. **Add Custom Option Types:** - Add a custom option `-c` or `--complex` that takes a single argument representing a complex number and stores it appropriately. 3. **Add Option Grouping:** - Add a group called \\"Advanced Options\\" that includes: - `-d` or `--debug`: an optional flag to print debug information. - `-e` or `--execute`: an option to specify a script to execute. 4. **Use Callback Function:** - Add a callback option `--check` that verifies if a specified condition (e.g., verbose mode and debug mode cannot be enabled simultaneously) is met. 5. **Handle Errors Gracefully:** - Use the `OptionParser.error()` method to handle improper usage or invalid values, providing helpful error messages. Expected Input and Output - Input: Command-line arguments. - Output: Processed results based on the provided options. - Example: ```sh python script.py -f sample.txt -v -n 1,2,3 -m advanced --complex 3+4j -d --execute run.py --check ``` - Output: ``` Filename: sample.txt Verbose mode: Enabled Numbers: [1, 2, 3] Mode: advanced Complex number: (3+4j) Debug info: Enabled Executing script: run.py Error: Verbose mode and Debug mode cannot be enabled simultaneously. ``` Constraints - The script should handle: - Proper argument validation. - Meaningful error messages for invalid inputs. - Graceful handling of conflicts between options. Implementation Write your implementation within the following structure: ```python from optparse import OptionParser, OptionGroup, OptionValueError # Custom type function def check_complex(option, opt, value, parser): try: return complex(value) except ValueError: raise OptionValueError(f\\"option {opt}: invalid complex value: {value}\\") # Callback function for --check def validate_modes(option, opt, value, parser): if parser.values.verbose and parser.values.debug: parser.error(\\"Verbose mode and Debug mode cannot be enabled simultaneously.\\") print(\\"Validation successful. All conditions are met.\\") def main(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) # Define file option parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"specify the filename\\", metavar=\\"FILE\\") # Define verbose option parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"enable verbose mode\\") # Define numbers option parser.add_option(\\"-n\\", \\"--numbers\\", dest=\\"numbers\\", type=\\"string\\", help=\\"comma-separated list of integers\\") # Define mode option parser.add_option(\\"-m\\", \\"--mode\\", dest=\\"mode\\", type=\\"choice\\", choices=[\\"beginner\\", \\"intermediate\\", \\"advanced\\"], default=\\"intermediate\\", help=\\"choose the mode: beginner, intermediate, advanced [default: %default]\\") # Define complex option parser.add_option(\\"-c\\", \\"--complex\\", dest=\\"complex\\", type=\\"string\\", help=\\"specify a complex number\\", action=\\"callback\\", callback=check_complex) # Advanced Options group advanced_group = OptionGroup(parser, \\"Advanced Options\\", \\"Caution: use these options at your own risk.\\") advanced_group.add_option(\\"-d\\", \\"--debug\\", action=\\"store_true\\", dest=\\"debug\\", help=\\"enable debug information\\") advanced_group.add_option(\\"-e\\", \\"--execute\\", dest=\\"script\\", help=\\"specify the script to execute\\") parser.add_option_group(advanced_group) # Adding --check option with a callback parser.add_option(\\"--check\\", action=\\"callback\\", callback=validate_modes) (options, args) = parser.parse_args() # Processed results (for demonstration) print(f\\"Filename: {options.filename}\\") print(f\\"Verbose mode: {\'Enabled\' if options.verbose else \'Disabled\'}\\") if options.numbers: print(f\\"Numbers: {list(map(int, options.numbers.split(\',\')))}\\") print(f\\"Mode: {options.mode}\\") print(f\\"Complex number: {options.complex}\\") print(f\\"Debug info: {\'Enabled\' if options.debug else \'Disabled\'}\\") print(f\\"Executing script: {options.script}\\") if __name__ == \\"__main__\\": main() ``` Deliverable A fully functional Python script that meets the requirements and handles both expected and erroneous inputs gracefully.","solution":"from optparse import OptionParser, OptionGroup, OptionValueError # Custom type function def check_complex(option, opt, value, parser): try: parser.values.complex_val = complex(value) except ValueError: raise OptionValueError(f\\"option {opt}: invalid complex value: {value}\\") # Callback function for --check def validate_modes(option, opt, value, parser): if parser.values.verbose and parser.values.debug: parser.error(\\"Verbose mode and Debug mode cannot be enabled simultaneously.\\") print(\\"Validation successful. All conditions are met.\\") def main(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) # Define file option parser.add_option(\\"-f\\", \\"--file\\", dest=\\"filename\\", help=\\"specify the filename\\", metavar=\\"FILE\\") # Define verbose option parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"enable verbose mode\\") # Define numbers option parser.add_option(\\"-n\\", \\"--numbers\\", dest=\\"numbers\\", type=\\"string\\", help=\\"comma-separated list of integers\\") # Define mode option parser.add_option(\\"-m\\", \\"--mode\\", dest=\\"mode\\", type=\\"choice\\", choices=[\\"beginner\\", \\"intermediate\\", \\"advanced\\"], default=\\"intermediate\\", help=\\"choose the mode: beginner, intermediate, advanced [default: %default]\\") # Define complex option parser.add_option(\\"-c\\", \\"--complex\\", dest=\\"complex_val\\", type=\\"string\\", help=\\"specify a complex number\\", action=\\"callback\\", callback=check_complex) # Advanced Options group advanced_group = OptionGroup(parser, \\"Advanced Options\\", \\"Caution: use these options at your own risk.\\") advanced_group.add_option(\\"-d\\", \\"--debug\\", action=\\"store_true\\", dest=\\"debug\\", help=\\"enable debug information\\") advanced_group.add_option(\\"-e\\", \\"--execute\\", dest=\\"script\\", help=\\"specify the script to execute\\") parser.add_option_group(advanced_group) # Adding --check option with a callback parser.add_option(\\"--check\\", action=\\"callback\\", callback=validate_modes) (options, args) = parser.parse_args() # Ensure the filename option is provided if options.filename is None: parser.error(\\"Filename is required\\") # Processed results (for demonstration) print(f\\"Filename: {options.filename}\\") print(f\\"Verbose mode: {\'Enabled\' if options.verbose else \'Disabled\'}\\") if options.numbers: try: numbers_list = list(map(int, options.numbers.split(\',\'))) print(f\\"Numbers: {numbers_list}\\") except ValueError: parser.error(\\"Invalid number format. Numbers should be a comma-separated list of integers.\\") print(f\\"Mode: {options.mode}\\") print(f\\"Complex number: {options.complex_val}\\") print(f\\"Debug info: {\'Enabled\' if options.debug else \'Disabled\'}\\") print(f\\"Executing script: {options.script}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question: Implementing a Robust HTTP Client** **Objective:** To test the student\'s understanding of advanced features of the `urllib.request` module by requiring them to implement a function that performs a complex HTTP operation, handles potential issues such as redirects and authentication, and processes the response appropriately. **Problem Statement:** Write a Python function `fetch_data_from_url` that performs the following operations: 1. Accepts a URL, optional data, and headers. 2. Checks if the URL uses HTTPS and ensures proper SSL handling. 3. Handles basic HTTP authentication if required. 4. Manages HTTP redirections, especially converting POST requests to GET on 301/302 responses. 5. Optionally allows the use of HTTP proxies. 6. Returns the status code, headers, and body of the response. **Function Signature:** ```python def fetch_data_from_url(url: str, data: bytes = None, headers: dict = None, auth: tuple = None, proxy: dict = None) -> tuple: Fetch data from the specified URL. Args: - url (str): The URL to fetch data from. - data (bytes, optional): Data to send with the request (for POST). - headers (dict, optional): Additional headers to include in the request. - auth (tuple, optional): A tuple containing username and password for basic HTTP authentication. - proxy (dict, optional): A dictionary containing proxy settings (e.g., {\'http\': \'http://proxy.example.com:8080/\'}). Returns: - tuple: A tuple containing the status code (int), headers (http.client.HTTPMessage), and body (bytes) of the response. pass ``` **Requirements:** 1. The function should handle HTTP and HTTPS URLs properly. 2. Implement HTTPS handling with optional SSL context. 3. If `auth` is provided, set up basic authentication. 4. Follow redirections (301 and 302) and convert POST requests to GET if necessary. 5. If `proxy` is provided, ensure that the request goes through the proxy. 6. Handle responses and return the status code, headers, and body appropriately. 7. Raise appropriate exceptions for errors like URLError, HTTPError, etc. **Example Usage:** ```python url = \\"https://example.com/api/data\\" data = b\\"key1=value1&key2=value2\\" headers = {\\"Content-Type\\": \\"application/x-www-form-urlencoded\\"} auth = (\\"username\\", \\"password\\") proxy = {\'http\': \'http://proxy.example.com:8080/\'} try: status_code, response_headers, response_body = fetch_data_from_url(url, data, headers, auth, proxy) print(f\\"Status Code: {status_code}\\") print(f\\"Response Headers: {response_headers}\\") print(f\\"Response Body: {response_body.decode(\'utf-8\')}\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` **Notes:** - Use the appropriate classes and methods from the `urllib.request` module. - Ensure proper error handling and resource cleanup. - Write clear and concise docstrings.","solution":"import urllib.request from urllib.error import URLError, HTTPError from urllib.parse import urlencode import ssl def fetch_data_from_url(url: str, data: bytes = None, headers: dict = None, auth: tuple = None, proxy: dict = None) -> tuple: Fetch data from the specified URL. Args: - url (str): The URL to fetch data from. - data (bytes, optional): Data to send with the request (for POST). - headers (dict, optional): Additional headers to include in the request. - auth (tuple, optional): A tuple containing username and password for basic HTTP authentication. - proxy (dict, optional): A dictionary containing proxy settings (e.g., {\'http\': \'http://proxy.example.com:8080/\'}). Returns: - tuple: A tuple containing the status code (int), headers (http.client.HTTPMessage), and body (bytes) of the response. if headers is None: headers = {} if auth: auth_encoded = f\\"{auth[0]}:{auth[1]}\\".encode(\'ascii\') headers[\'Authorization\'] = \'Basic \' + urllib.request.base64.b64encode(auth_encoded).decode(\'ascii\') req = urllib.request.Request(url, data=data, headers=headers) if data: req.method = \\"POST\\" else: req.method = \\"GET\\" ssl_context = None if url.lower().startswith(\'https\'): ssl_context = ssl.create_default_context() proxy_handler = None if proxy: proxy_handler = urllib.request.ProxyHandler(proxy) handlers = [] if ssl_context: handlers.append(urllib.request.HTTPSHandler(context=ssl_context)) if proxy_handler: handlers.append(proxy_handler) opener = urllib.request.build_opener(*handlers) opener.add_handler(urllib.request.HTTPRedirectHandler()) try: with opener.open(req) as response: status_code = response.getcode() response_headers = response.headers response_body = response.read() return (status_code, response_headers, response_body) except HTTPError as e: return (e.code, e.headers, e.read()) except URLError as e: raise e"},{"question":"# Problem Description In this coding assessment, you will implement a Python function that utilizes the `asyncio` library to perform asynchronous web scraping tasks. The primary task is to scrape multiple web pages concurrently and return the combined results. # Task Implement the function `async scrape_urls(urls: List[str], timeout: int) -> Dict[str, Any]:`. Input - `urls`: A list of URLs to be scraped (List[str]). - `timeout`: The timeout (in seconds) for each individual web scraping task (int). Output - Returns a dictionary where each key is a URL and the corresponding value is the content of the URL or an error message if the scraping failed. Specifications 1. You must fetch all URLs concurrently using `asyncio.gather`. 2. Each URL must be fetched within the provided timeout. Use `asyncio.wait_for` to enforce this. 3. Handle exceptions: - If fetching a URL raises an exception or times out, store the error message instead of the content. 4. Ensure graceful cancellation of tasks using `asyncio.shield` if one of the tasks takes too long. 5. Utilize `asyncio.create_task` for concurrent task creation. # Constraints 1. You are only allowed to use standard python libraries (`asyncio`, `aiohttp`). 2. You should handle up to 100 URLs simultaneously. # Example ```python import asyncio from typing import List, Dict, Any import aiohttp async def fetch(url: str, timeout: int) -> str: try: async with aiohttp.ClientSession() as session: async with session.get(url, timeout=timeout) as response: return await response.text() except Exception as e: return str(e) async def scrape_urls(urls: List[str], timeout: int) -> Dict[str, Any]: async def fetch_with_error_handling(url): try: return await asyncio.wait_for(fetch(url, timeout), timeout) except asyncio.TimeoutError: return \\"Timeout Error\\" except Exception as e: return str(e) tasks = [asyncio.create_task(fetch_with_error_handling(url)) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return dict(zip(urls, results)) # Example usage: urls = [\\"http://example.com\\", \\"http://example.org\\"] timeout = 5 asyncio.run(scrape_urls(urls, timeout)) ``` # Note This task evaluates understanding of `asyncio` functionalities such as task creation, timeouts, exception handling, concurrent execution, and the effective use of `async/await` syntax. Ensure you test the implementation locally before submission.","solution":"import asyncio from typing import List, Dict, Any import aiohttp async def fetch(url: str, timeout: int) -> str: Fetches the content of a URL within the defined timeout. Returns the content of the URL or an error message if failed. try: async with aiohttp.ClientSession() as session: async with session.get(url, timeout=timeout) as response: return await response.text() except Exception as e: return str(e) async def scrape_urls(urls: List[str], timeout: int) -> Dict[str, Any]: Scrapes multiple URLs concurrently with the provided timeout for each URL. Returns a dictionary with URLs as keys and their content or error messages as values. async def fetch_with_error_handling(url): try: return await asyncio.wait_for(fetch(url, timeout), timeout) except asyncio.TimeoutError: return \\"Timeout Error\\" except Exception as e: return str(e) tasks = [asyncio.create_task(fetch_with_error_handling(url)) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return dict(zip(urls, results))"},{"question":"**Coding Assessment Question:** # Objective: Design a Python function that demonstrates comprehensive usage of various dictionary manipulation techniques as described in the Python C API documentation. # Problem Description: Write a Python function `manage_dictionaries(dict1, dict2)` that takes two dictionaries as input and performs the following operations using the C API functions described: 1. **Create a New Dictionary**: Create a new empty dictionary. 2. **Add Key-Value Pairs**: Add all key-value pairs from `dict1` to this new dictionary. 3. **Check for Key Presence**: Check if any key from `dict2` is present in the new dictionary and print out the status for each key. 4. **Merge Dictionaries**: Merge the contents of `dict2` into the new dictionary. 5. **Copy Dictionary**: Create a copy of the merged dictionary. 6. **Retrieve Items**: Retrieve and print the value for a key `\'example_key\'` from the copied dictionary. If the key is not present, print an appropriate message. 7. **Delete Items**: Remove a key `\'remove_key\'` from the copied dictionary if it exists. 8. **Iterate and Modify**: Iterate over the copied dictionary and for each integer value, increment it by 1. 9. **Return Result**: Return the modified copied dictionary. # Function Signature: ```python def manage_dictionaries(dict1: dict, dict2: dict) -> dict: pass ``` # Input: - `dict1`: Dictionary containing initial key-value pairs. - `dict2`: Dictionary containing additional key-value pairs. # Output: - Return a dictionary that has gone through the described operations. # Constraints: - Dictionaries `dict1` and `dict2` can have any mix of hashable key types and corresponding values. - You may use the Python C API equivalents in Python standard library where applicable. # Example: ```python dict1 = {\\"a\\": 1, \\"b\\": 2} dict2 = {\\"b\\": 3, \\"c\\": 4, \\"example_key\\": 5} result = manage_dictionaries(dict1, dict2) print(result) ``` Expected output: ``` Key \'b\' found in the new dictionary. Key \'c\' not found in the new dictionary. {\'a\': 1, \'b\': 3, \'c\': 4, \'example_key\': 6} ``` # Notes: - Ensure that each operation mimics the described C API function as closely as possible, using Python\'s standard dictionary methods. - The function should handle potential errors gracefully and print relevant messages.","solution":"def manage_dictionaries(dict1: dict, dict2: dict) -> dict: This function demonstrates comprehensive usage of various dictionary manipulation techniques and performs a sequence of operations on the input dictionaries dict1 and dict2. # 1. Create a New Dictionary new_dict = {} # 2. Add Key-Value Pairs from dict1 to the new dictionary new_dict.update(dict1) # 3. Check for Key Presence in new_dict for keys in dict2 for key in dict2: if key in new_dict: print(f\\"Key \'{key}\' found in the new dictionary.\\") else: print(f\\"Key \'{key}\' not found in the new dictionary.\\") # 4. Merge dictionaries - add contents of dict2 to new_dict new_dict.update(dict2) # 5. Copy the merged dictionary copied_dict = new_dict.copy() # 6. Retrieve item for \'example_key\' if \'example_key\' in copied_dict: print(f\\"Value for \'example_key\': {copied_dict[\'example_key\']}\\") else: print(\\"Key \'example_key\' not found in the copied dictionary.\\") # 7. Delete \'remove_key\' if it exists if \'remove_key\' in copied_dict: del copied_dict[\'remove_key\'] # 8. Iterate and modify - incrementing integer values by 1 for key, value in copied_dict.items(): if isinstance(value, int): copied_dict[key] = value + 1 # 9. Return the modified copied dictionary return copied_dict"},{"question":"# Question: Implementing a Custom SAX ContentHandler Objective Create a custom XML content handler by subclassing `xml.sax.handler.ContentHandler` to process specific XML data and extract meaningful information. Task You are given an XML document containing a catalog of books. Your task is to implement a custom SAX content handler that extracts and prints information about each book, including the title, author, genre, price, and publish date. Input - An XML string representing a catalog of books. Output - Print the details of each book in the format: `Title: <title>, Author: <author>, Genre: <genre>, Price: <price>, Publish Date: <publish_date>` Constraints - You must use the `xml.sax` module and subclass `xml.sax.handler.ContentHandler`. - Handle all necessary SAX events to extract and print the required information. - Assume that the XML is well-formed. XML Example ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> </book> <!-- More book entries --> </catalog> ``` Implementation 1. Subclass `xml.sax.handler.ContentHandler`. 2. Override the necessary methods such as `startElement`, `endElement`, and `characters` to process the XML data. 3. Print the details of each book when the end of the `book` element is reached. Example Output ``` Title: XML Developer\'s Guide, Author: Gambardella, Matthew, Genre: Computer, Price: 44.95, Publish Date: 2000-10-01 Title: Midnight Rain, Author: Ralls, Kim, Genre: Fantasy, Price: 5.95, Publish Date: 2000-12-16 ``` Starter Code ```python import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \\"\\" self.current_data = {} self.book_details = [] def startElement(self, name, attrs): self.current_element = name if name == \\"book\\": self.current_data = {} def endElement(self, name): if name == \\"book\\": self.book_details.append(self.current_data) print(f\\"Title: {self.current_data.get(\'title\')}, Author: {self.current_data.get(\'author\')}, Genre: {self.current_data.get(\'genre\')}, Price: {self.current_data.get(\'price\')}, Publish Date: {self.current_data.get(\'publish_date\')}\\") self.current_element = \\"\\" def characters(self, content): if self.current_element in [\\"title\\", \\"author\\", \\"genre\\", \\"price\\", \\"publish_date\\"]: if self.current_element in self.current_data: self.current_data[self.current_element] += content else: self.current_data[self.current_element] = content xml_data = <put the XML string here> # Create a parser object parser = xml.sax.make_parser() # Override the default ContextHandler handler = BookContentHandler() parser.setContentHandler(handler) # Parse the XML string xml.sax.parseString(xml_data, handler) ``` Replace the placeholder `xml_data` with the provided XML string to test your implementation.","solution":"import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() self.current_element = \\"\\" self.current_data = {} self.book_details = [] def startElement(self, name, attrs): self.current_element = name if name == \\"book\\": self.current_data = {} def endElement(self, name): if name == \\"book\\": self.book_details.append(self.current_data) print(f\\"Title: {self.current_data.get(\'title\')}, Author: {self.current_data.get(\'author\')}, Genre: {self.current_data.get(\'genre\')}, Price: {self.current_data.get(\'price\')}, Publish Date: {self.current_data.get(\'publish_date\')}\\") self.current_element = \\"\\" def characters(self, content): if self.current_element in [\\"title\\", \\"author\\", \\"genre\\", \\"price\\", \\"publish_date\\"]: if self.current_element in self.current_data: self.current_data[self.current_element] += content else: self.current_data[self.current_element] = content xml_data = <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> </book> </catalog> # Create a parser object parser = xml.sax.make_parser() # Override the default ContextHandler handler = BookContentHandler() parser.setContentHandler(handler) # Parse the XML string xml.sax.parseString(xml_data, handler)"},{"question":"# CSV Data Processing You are given a CSV file named `employees.csv` that contains employee data with the following columns: `EmployeeID`, `FirstName`, `LastName`, `Department`, `Salary`. The task is to write a function `process_employee_data(input_csv: str, output_csv: str, department: str)` that reads the employee data from `employees.csv`, filters the employees that belong to the given `department`, and writes only the `EmployeeID`, `FirstName`, `LastName`, and `Salary` of these employees to a new CSV file specified by `output_csv`. Function Signature ```python def process_employee_data(input_csv: str, output_csv: str, department: str) -> None: ``` Input - `input_csv`: A string representing the file path to the input CSV file (`employees.csv`). - `output_csv`: A string representing the file path to the output CSV file where the filtered data should be written. - `department`: A string specifying the department to filter the employees by (e.g., \\"HR\\", \\"Engineering\\", etc.). Output - The function does not return anything but writes the filtered data to the `output_csv` file. Constraints - You can assume the input CSV file exists and is formatted correctly. - The output CSV file should have the columns: `EmployeeID`, `FirstName`, `LastName`, `Salary`. Example Usage ```python process_employee_data(\'employees.csv\', \'filtered_employees.csv\', \'Engineering\') ``` Example CSV Content Contents of `employees.csv`: ``` EmployeeID,FirstName,LastName,Department,Salary 1,John,Doe,Engineering,70000 2,Jane,Smith,HR,60000 3,Jim,Brown,Engineering,65000 4,Lisa,Johnson,Marketing,62000 ``` After calling `process_employee_data(\'employees.csv\', \'filtered_employees.csv\', \'Engineering\')`, the contents of `filtered_employees.csv` should be: ``` EmployeeID,FirstName,LastName,Salary 1,John,Doe,70000 3,Jim,Brown,65000 ``` Notes - Make sure to handle opening and closing of files properly to avoid resource leaks. - Utilize the `csv.DictReader` and `csv.DictWriter` classes for easier handling of CSV data as dictionaries.","solution":"import csv def process_employee_data(input_csv: str, output_csv: str, department: str) -> None: Reads employee data from input_csv, filters by department, and writes the filtered data to output_csv. Args: - input_csv (str): Path to the input CSV file. - output_csv (str): Path to the output CSV file. - department (str): Department to filter employees by. with open(input_csv, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) fieldnames = [\'EmployeeID\', \'FirstName\', \'LastName\', \'Salary\'] filtered_employees = [row for row in reader if row[\'Department\'] == department] with open(output_csv, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for employee in filtered_employees: writer.writerow({ \'EmployeeID\': employee[\'EmployeeID\'], \'FirstName\': employee[\'FirstName\'], \'LastName\': employee[\'LastName\'], \'Salary\': employee[\'Salary\'] })"},{"question":"Advanced PyTorch Coding Assessment # Objective The goal of this assessment is to verify the student\'s understanding of PyTorch\'s `gradcheck` and `gradgradcheck` mechanisms, including their implementation for both real and complex-valued functions. The student will also demonstrate their ability to apply these checks to custom neural network layers. # Problem Statement You are required to implement a custom PyTorch module and test its gradients using PyTorch\'s `gradcheck` and `gradgradcheck`. This assesses your understanding of both forward and backward mode AD as well as higher-order derivatives. # Task 1. **Implement a Custom PyTorch Module:** - Implement a custom PyTorch module called `CustomLayer`. - This module should accept a tensor `input` and perform the following operations: - Apply a linear transformation with weights `W` and bias `b`. - Apply the sine activation function. ```python import torch from torch.autograd import Function class CustomLayer(Function): @staticmethod def forward(ctx, input, weight, bias): # Save context for backward pass ctx.save_for_backward(input, weight, bias) # Linear transformation output = input.mm(weight.t()) + bias # Sine activation return torch.sin(output) @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias class CustomLayerModule(torch.nn.Module): def __init__(self, input_features, output_features): super(CustomLayerModule, self).__init__() self.weight = torch.nn.Parameter(torch.randn(output_features, input_features)) self.bias = torch.nn.Parameter(torch.randn(output_features)) def forward(self, input): return CustomLayer.apply(input, self.weight, self.bias) ``` 2. **Gradcheck and Gradgradcheck:** - Write a function `check_gradients` that: - Creates an instance of `CustomLayerModule`. - Generates random input and output data. - Checks the gradients using both `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck`. - Outputs whether the gradients passed the checks or not. ```python def check_gradients(): input_features = 3 output_features = 2 layer = CustomLayerModule(input_features, output_features) # Generate random input and output data input = torch.randn(4, input_features, dtype=torch.double, requires_grad=True) layer.weight.data = torch.randn(output_features, input_features, dtype=torch.double, requires_grad=True) layer.bias.data = torch.randn(output_features, dtype=torch.double, requires_grad=True) # Perform gradcheck gradcheck_result = torch.autograd.gradcheck(CustomLayer.apply, (input, layer.weight, layer.bias)) gradgradcheck_result = torch.autograd.gradgradcheck(CustomLayer.apply, (input, layer.weight, layer.bias)) return gradcheck_result, gradgradcheck_result # Example usage gradcheck_passed, gradgradcheck_passed = check_gradients() print(f\\"Gradcheck passed: {gradcheck_passed}\\") print(f\\"Gradgradcheck passed: {gradgradcheck_passed}\\") ``` # Input and Output Formats - **No direct input**: You are writing functions and classes which will be tested. - **Output**: Your implementation should print whether the gradcheck and gradgradcheck passed. # Constraints - Utilize PyTorch\'s automatic differentiation capabilities. - Ensure the custom layer works for both real and complex-valued inputs. - Implement both forward and backward methods for the custom autograd Function. # Performance Requirements - The implementation should be efficient and make use of PyTorch tensor operations. - The gradient checks should pass without errors for randomly generated input and parameters. Good luck!","solution":"import torch from torch.autograd import Function class CustomLayer(Function): @staticmethod def forward(ctx, input, weight, bias): # Save context for backward pass ctx.save_for_backward(input, weight, bias) # Linear transformation output = input.mm(weight.t()) + bias # Sine activation return torch.sin(output) @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None grad_output_sin = grad_output * torch.cos(input.mm(weight.t()) + bias) if ctx.needs_input_grad[0]: grad_input = grad_output_sin.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output_sin.t().mm(input) if ctx.needs_input_grad[2]: grad_bias = grad_output_sin.sum(0) return grad_input, grad_weight, grad_bias class CustomLayerModule(torch.nn.Module): def __init__(self, input_features, output_features): super(CustomLayerModule, self).__init__() self.weight = torch.nn.Parameter(torch.randn(output_features, input_features)) self.bias = torch.nn.Parameter(torch.randn(output_features)) def forward(self, input): return CustomLayer.apply(input, self.weight, self.bias) def check_gradients(): input_features = 3 output_features = 2 layer = CustomLayerModule(input_features, output_features) # Generate random input and output data input = torch.randn(4, input_features, dtype=torch.double, requires_grad=True) layer.weight.data = torch.randn(output_features, input_features, dtype=torch.double, requires_grad=True) layer.bias.data = torch.randn(output_features, dtype=torch.double, requires_grad=True) # Perform gradcheck and gradgradcheck gradcheck_result = torch.autograd.gradcheck(CustomLayer.apply, (input, layer.weight, layer.bias)) gradgradcheck_result = torch.autograd.gradgradcheck(CustomLayer.apply, (input, layer.weight, layer.bias)) return gradcheck_result, gradgradcheck_result # Example usage gradcheck_passed, gradgradcheck_passed = check_gradients() print(f\\"Gradcheck passed: {gradcheck_passed}\\") print(f\\"Gradgradcheck passed: {gradgradcheck_passed}\\")"},{"question":"# Custom Import Mechanism in Python Objective: To assess the student\'s understanding of Python\'s import system, including the usage of import hooks, finders, loaders, and package management. Task: Implement a custom module loader and finder for Python modules. The custom loader should load modules from a dictionary where keys are module names and values are the code, rather than from the file system. This simulates loading modules dynamically from a non-standard resource, such as a database or network source. Specifications: 1. **Module Source**: - Use a dictionary named `module_dict` where: ```python module_dict = { \'mymodule\': \'def hello():n print(\\"Hello from mymodule\\")\', \'anothermodule\': \'def greet():n print(\\"Greetings from anothermodule\\")\' } ``` - Each key is a module name and the value is a string representation of the module\'s code. 2. **Custom Finder and Loader**: - Implement a class `DictModuleLoader` which can load the module from `module_dict`. - Implement a class `DictModuleFinder` which can find modules in `module_dict`. 3. **Integration with Import System**: - Integrate `DictModuleFinder` into Python\'s import system using `sys.meta_path`. - Demonstrate importing and using the modules (`mymodule` and `anothermodule`) loaded by the custom loader. Requirements: 1. **DictModuleLoader**: - Make sure the loader follows the protocols expected of custom loaders (e.g., `exec_module`, `create_module` methods). - It should be able to execute the module code and populate the module\'s namespace appropriately. 2. **DictModuleFinder**: - Implement the necessary method (`find_spec`) to locate a module in `module_dict` and return a module spec. 3. **Usage Example**: ```python import sys # Example dictionary definition module_dict = { \'mymodule\': \'def hello():n print(\\"Hello from mymodule\\")\', \'anothermodule\': \'def greet():n print(\\"Greetings from anothermodule\\")\' } # Write the DictModuleLoader and DictModuleFinder classes here # Integrate finder into sys.meta_path sys.meta_path.insert(0, DictModuleFinder(module_dict)) # Test importing the modules import mymodule import anothermodule mymodule.hello() # Should print \\"Hello from mymodule\\" anothermodule.greet() # Should print \\"Greetings from anothermodule\\" ``` Input: - The dictionary `module_dict` containing module names and their corresponding code. - The classes for custom loader and finder. Output: - Successfully importing and using the modules defined in `module_dict`. - Printed outputs from the module functions. Constraints: 1. The implementation should conform to Python\'s import system requirements and should not use any deprecated practices. 2. Ensure the custom loader and finder classes are properly integrated without bypassing any standard import system checks. Performance Requirement: - The solution should handle module discovery and loading efficiently without unnecessary overhead.","solution":"import sys import importlib.util import importlib.abc import types module_dict = { \'mymodule\': \'def hello():n print(\\"Hello from mymodule\\")\', \'anothermodule\': \'def greet():n print(\\"Greetings from anothermodule\\")\' } class DictModuleLoader(importlib.abc.Loader): def __init__(self, module_dict): self.module_dict = module_dict def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = self.module_dict[module.__name__] exec(code, module.__dict__) class DictModuleFinder(importlib.abc.MetaPathFinder): def __init__(self, module_dict): self.module_dict = module_dict def find_spec(self, fullname, path, target=None): if fullname in self.module_dict: return importlib.util.spec_from_loader(fullname, DictModuleLoader(self.module_dict), origin=\'dict\') return None # Integrate finder into sys.meta_path sys.meta_path.insert(0, DictModuleFinder(module_dict))"},{"question":"# PyTorch Storage Operations **Objective**: Write a function that demonstrates the creation and manipulation of tensor storage in PyTorch. You should create two tensors that share the same underlying storage and modify one of them to show its effect on the other. **Task**: 1. Create a tensor `t1` of size `(4, 4)` initialized with ones (`torch.ones`) and obtain its `UntypedStorage`. 2. Create a tensor `t2` which shares the same storage as `t1` using the `.view()` method. 3. Modify certain elements of `t2` and show the changes reflected in `t1`. 4. Return both tensors. **Function Signature**: ```python def manipulate_storage(): # Your code here ``` **Expected Output**: - Return a tuple `(t1, t2)` where `t1` and `t2` are the tensors demonstrating shared storage manipulation. **Constraints**: - Use only PyTorch functions and methods. - Do not use direct manipulation of storage through `.data_ptr` or any similarly low-level operations. **Example**: ```python t1, t2 = manipulate_storage() # Expected output could be: # t1: tensor([[0., 0., 0., 0.], # [0., 1., 1., 0.], # [0., 1., 1., 0.], # [0., 0., 0., 0.]]) # t2: tensor([[0., 0., 0., 0.], # [0., 1., 1., 0.], # [0., 1., 1., 0.], # [0., 0., 0., 0.]]) ``` **Notes**: - Be careful about how views and slices affect the storage. - Ensure you\'ve properly demonstrated the sharing aspect between `t1` and `t2`. **Hint**: Consider modifying a sub-region of one tensor and see how it affects the other tensor due to shared storage.","solution":"import torch def manipulate_storage(): Demonstrates the creation and manipulation of tensor storage in PyTorch. Returns a tuple of two tensors (t1, t2) where t2 modifies elements that share the same underlying storage as t1. # Step 1: Create tensor t1 of size (4, 4) initialized with ones t1 = torch.ones(4, 4) # Step 2: Create tensor t2 which shares the same storage as t1 using the .view() method t2 = t1.view(4, 4) # Step 3: Modify certain elements of t2 t2[1:3, 1:3] = 0 # Return both tensors to show the changes reflected return t1, t2"},{"question":"# Email Iterator Implementation You are tasked with implementing a Python function that mimics the behavior of the `typed_subpart_iterator` function from the `email.iterators` module. Function Signature ```python def custom_typed_subpart_iterator(msg, maintype=\'text\', subtype=None): pass ``` Objective Write a function `custom_typed_subpart_iterator` which iterates over all the subparts of an email message object (`msg`), returning only those subparts that match the specified MIME type (`maintype`) and optional subtype (`subtype`). Input - `msg`: A `Message` object from the `email` library representing the root of the email message tree. - `maintype` (str): The main MIME type to match, defaults to `\'text\'`. - `subtype` (str): The specific MIME subtype to match, defaults to `None`. Output - A generator that yields message subparts matching the specified MIME type and subtype. Constraints - The function should handle nested messages. - If `subtype` is `None`, match only on the main type. - You can assume the message object (`msg`) will not be `None`. Example ```python from email.message import EmailMessage # Example Message Setup msg = EmailMessage() msg.set_content(\\"Hello, this is plain text.\\") msg.add_alternative(\\"<html><body><p>Hello, this is HTML.</p></body></html>\\", subtype=\\"html\\") for part in custom_typed_subpart_iterator(msg, maintype=\'text\'): print(part.get_content_type(), part.get_payload()) # Output should be: # text/plain # Hello, this is plain text. # text/html # <html><body><p>Hello, this is HTML.</p></body></html> ``` Notes - You are encouraged to use recursion to handle nested messages. - Utilize built-in methods where appropriate to navigate and filter the message subparts. Testing Your implementation will be tested against a variety of nested email messages with different MIME types and structures to ensure the correct subparts are iterated over and returned.","solution":"from email.message import Message def custom_typed_subpart_iterator(msg, maintype=\'text\', subtype=None): Yields subparts of the provided email message object that match the specified MIME type and subtype. :param msg: The email message object to iterate over :param maintype: The main MIME type to match, defaults to \'text\' :param subtype: The specific MIME subtype to match, defaults to None :returns: A generator that yields matching subparts if msg.get_content_maintype() == maintype and (subtype is None or msg.get_content_subtype() == subtype): yield msg if msg.is_multipart(): for part in msg.get_payload(): yield from custom_typed_subpart_iterator(part, maintype, subtype)"},{"question":"# URL Manipulation and Validation You are tasked with designing a URL management system for a web application. This system should be able to: 1. Parse an input URL and extract components. 2. Validate the hostname by ensuring it follows the standard URL structure. 3. Convert any relative URLs into absolute URLs using a given base URL. 4. Encode query parameters properly for safe use in URLs. 5. Provide functionalities to modify parsed URL components such as updating the scheme or fragment and reconstructing the URL. 6. Handle potential issues such as invalid URLs by raising appropriate exceptions when necessary. Requirements: 1. **function parse_and_validate_url(url: str) -> dict:** - **Input:** A URL string. - **Output:** A dictionary containing \'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\', \'hostname\', \'port\'. - **Raises:** A `ValueError` if the URL is invalid or the hostname does not follow the proper URL structure. 2. **function convert_to_absolute(base: str, url: str) -> str:** - **Input:** A base URL string and a relative URL string. - **Output:** An absolute URL string by combining the base and relative URL. 3. **function encode_query_params(params: dict) -> str:** - **Input:** A dictionary of query parameters. - **Output:** A properly encoded query string. 4. **function update_url_component(url: str, **components) -> str:** - **Input:** A URL string and keyword arguments for components to update. The keyword arguments can include \'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\'. - **Output:** A new URL string with the specified components updated. Constraints: - The URL string and components should adhere to the standard URL structures. - Ensure robustness by handling and raising appropriate exceptions for invalid inputs. - The functions should efficiently handle URLs of typical lengths and structures encountered in web applications. Here\'s the implementation template with function signatures: ```python from urllib.parse import urlparse, urljoin, urlencode, urlunparse, ParseResult def parse_and_validate_url(url: str) -> dict: # Your implementation here pass def convert_to_absolute(base: str, url: str) -> str: # Your implementation here pass def encode_query_params(params: dict) -> str: # Your implementation here pass def update_url_component(url: str, **components) -> str: # Your implementation here pass # Example usage: try: components = parse_and_validate_url(\\"http://docs.python.org:80/3/library/urllib.parse.html?highlight=params#url-parsing\\") print(components) absolute_url = convert_to_absolute(\\"http://www.example.com/path/\\", \\"subpath/resource.html\\") print(absolute_url) query_str = encode_query_params({\\"name\\": \\"John Doe\\", \\"age\\": \\"30\\"}) print(query_str) modified_url = update_url_component(\\"http://docs.python.org:80/3/library/urllib.parse.html\\", scheme=\\"https\\", fragment=\\"new-fragment\\") print(modified_url) except ValueError as e: print(f\\"Error: {e}\\") ``` Implementing these functions will demonstrate your understanding of URL parsing, manipulation, and validation as per the Python `urllib.parse` module. Ensure to write appropriate test cases to validate each function.","solution":"from urllib.parse import urlparse, urljoin, urlencode, urlunparse, ParseResult def parse_and_validate_url(url: str) -> dict: parsed = urlparse(url) # Check if URL is valid and has a hostname if not parsed.scheme or not parsed.hostname: raise ValueError(\\"Invalid URL structure.\\") return { \'scheme\': parsed.scheme, \'netloc\': parsed.netloc, \'path\': parsed.path, \'params\': parsed.params, \'query\': parsed.query, \'fragment\': parsed.fragment, \'hostname\': parsed.hostname, \'port\': parsed.port } def convert_to_absolute(base: str, url: str) -> str: return urljoin(base, url) def encode_query_params(params: dict) -> str: return urlencode(params) def update_url_component(url: str, **components) -> str: parsed = urlparse(url) updated = parsed._replace(**components) return urlunparse(updated)"},{"question":"You have been provided a sales dataset containing information on different products sold by a company. The dataset includes the following columns: - `product_id`: Unique identifier for each product. - `category`: Category to which the product belongs. - `units_sold`: Number of units sold. - `sale_date`: Date on which the sale was made. Here is a sample of what the dataset might look like: ```python import pandas as pd data = pd.DataFrame({ \'product_id\': [1, 2, 3, 4, 5], \'category\': [\'A\', \'B\', \'A\', \'C\', \'B\'], \'units_sold\': [10, 20, 15, 10, 30], \'sale_date\': [\'2023-01-10\', \'2023-01-11\', \'2023-01-12\', \'2023-01-13\', \'2023-01-14\'] }) ``` Your task is to write a function `plot_category_sales` that takes this dataset as input and produces a bar plot showing the total units sold per category. Furthermore, the plot should differentiate between sales made before and after a specified date. Function Signature ```python import seaborn.objects as so import pandas as pd def plot_category_sales(data: pd.DataFrame, split_date: str) -> None: pass ``` Input - `data (pd.DataFrame)`: The sales dataset. - `split_date (str)`: A date string in the format \'YYYY-MM-DD\'. This date is used to differentiate sales made before and after this date. Output - The function should display a bar plot. Constraints - The `split_date` is always a valid date string. - The `data` DataFrame always contains the columns: `product_id`, `category`, `units_sold`, and `sale_date`. Example ```python import pandas as pd from seaborn import load_dataset data = pd.DataFrame({ \'product_id\': [1, 2, 3, 4, 5], \'category\': [\'A\', \'B\', \'A\', \'C\', \'B\'], \'units_sold\': [10, 20, 15, 10, 30], \'sale_date\': [\'2023-01-10\', \'2023-01-11\', \'2023-01-12\', \'2023-01-13\', \'2023-01-14\'] }) split_date = \'2023-01-12\' plot_category_sales(data, split_date) ``` This function would produce a bar plot showing the total units sold in each category, and within each bar, there will be a differentiation between sales made before and after \'2023-01-12\'. Instructions 1. Load the dataset into a pandas DataFrame within the function. 2. Create a new column in the DataFrame called `period` which indicates whether the sale was made \'before\' or \'after\' the `split_date`. 3. Utilize seaborn to create a bar plot that displays the total `units_sold` per `category`, with an additional grouping by the `period` column. 4. Annotate the plot to make it clear and readable.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_category_sales(data: pd.DataFrame, split_date: str) -> None: Plots a bar plot showing the total units sold per category, with differentiation between sales made before and after the specified split_date. :param data: pd.DataFrame - The sales dataset. :param split_date: str - The date used to differentiate sales in \'YYYY-MM-DD\' format. # Convert sale_date column to datetime data[\'sale_date\'] = pd.to_datetime(data[\'sale_date\']) # Create period column based on split_date split_date_parsed = pd.to_datetime(split_date) data[\'period\'] = data[\'sale_date\'].apply(lambda x: \'before\' if x < split_date_parsed else \'after\') # Aggregate data agg_data = data.groupby([\'category\', \'period\'])[\'units_sold\'].sum().reset_index() # Plotting using seaborn sns.set_theme(style=\\"whitegrid\\") bar_plot = sns.barplot(x=\'category\', y=\'units_sold\', hue=\'period\', data=agg_data) # Set plot labels and title bar_plot.set(xlabel=\'Category\', ylabel=\'Total Units Sold\', title=\'Total Units Sold per Category\') plt.legend(title=\'Period\') plt.show()"},{"question":"Problem Statement You are tasked with implementing a Python function that compiles multiple Python source files into byte-code files using the `py_compile` module. The function should handle compilation errors gracefully and provide an option for detailed or silent error messages. Additionally, the function should allow control over the invalidation mode of the byte-code files. Function Signature ```python def compile_python_files(file_paths: List[str], output_dir: str, raise_on_error: bool = False, error_level: int = 0, optimization_level: int = -1, invalidation_mode: str = \'TIMESTAMP\') -> List[str]: Compiles a list of Python source files into byte-code files. Parameters: - file_paths (List[str]): List of paths to the source files to be compiled. - output_dir (str): Directory where the compiled byte-code files should be stored. - raise_on_error (bool): If True, raise an exception on compilation error; otherwise, write an error message or be silent based on error_level. - error_level (int): Level of error reporting (0: standard error messages, 1: minimal error messages, 2: silent). - optimization_level (int): Optimization level for the compilation (default is -1). - invalidation_mode (str): Method of byte-code invalidation (\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'). Returns: - List[str]: List of paths to the successfully compiled byte-code files. Raises: - ValueError: If invalid input is provided. - py_compile.PyCompileError: If raise_on_error is True and a compilation error occurs. Notes: - The function should ensure the output directory exists. - It should handle edge cases where file paths do not exist or are not valid Python files. ``` Constraints - Ensure that all provided source file paths exist and are valid Python files. - Create the output directory if it does not exist. - Map the `invalidation_mode` string to the corresponding `PycInvalidationMode` enum value. Examples ```python # Example 1 file_paths = [\'script1.py\', \'script2.py\'] output_dir = \'./compiled\' compiled_files = compile_python_files(file_paths, output_dir) print(compiled_files) # Output: [\'./compiled/script1.pyc\', \'./compiled/script2.pyc\'] # Example 2 file_paths = [\'nonexistent.py\'] output_dir = \'./compiled\' compiled_files = compile_python_files(file_paths, output_dir, error_level=1) print(compiled_files) # Output: [] # Example 3 file_paths = [\'script1.py\', \'script2.py\'] output_dir = \'./compiled\' try: compiled_files = compile_python_files(file_paths, output_dir, raise_on_error=True) except py_compile.PyCompileError as e: print(f\\"Compilation failed: {e}\\") ``` Hints - Use `os.path.exists` and `os.path.isfile` to check the validity of file paths. - Use `os.makedirs` to create the output directory if it does not exist. - Use `py_compile.compile` for the actual compilation, handling parameters accordingly.","solution":"import os import py_compile from typing import List from py_compile import PycInvalidationMode def compile_python_files(file_paths: List[str], output_dir: str, raise_on_error: bool = False, error_level: int = 0, optimization_level: int = -1, invalidation_mode: str = \'TIMESTAMP\') -> List[str]: Compiles a list of Python source files into byte-code files. Parameters: - file_paths (List[str]): List of paths to the source files to be compiled. - output_dir (str): Directory where the compiled byte-code files should be stored. - raise_on_error (bool): If True, raise an exception on compilation error; otherwise, write an error message or be silent based on error_level. - error_level (int): Level of error reporting (0: standard error messages, 1: minimal error messages, 2: silent). - optimization_level (int): Optimization level for the compilation (default is -1). - invalidation_mode (str): Method of byte-code invalidation (\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'). Returns: - List[str]: List of paths to the successfully compiled byte-code files. Raises: - ValueError: If invalid input is provided. - py_compile.PyCompileError: If raise_on_error is True and a compilation error occurs. Notes: - The function should ensure the output directory exists. - It should handle edge cases where file paths do not exist or are not valid Python files. if invalidation_mode not in (\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'): raise ValueError(\\"Invalid invalidation mode. Choose from \'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'.\\") mode_mapping = { \'TIMESTAMP\': PycInvalidationMode.TIMESTAMP, \'CHECKED_HASH\': PycInvalidationMode.CHECKED_HASH, \'UNCHECKED_HASH\': PycInvalidationMode.UNCHECKED_HASH } if not os.path.exists(output_dir): os.makedirs(output_dir) compiled_files = [] for file_path in file_paths: if not os.path.isfile(file_path): if error_level < 2: print(f\\"File {file_path} does not exist or is not a valid file.\\") if error_level == 0 else None if raise_on_error: raise ValueError(f\\"File {file_path} does not exist or is not a valid file.\\") continue try: compiled_file = py_compile.compile( file=file_path, cfile=None, dfile=None, doraise=raise_on_error, optimize=optimization_level, invalidation_mode=mode_mapping[invalidation_mode] ) compiled_files.append(compiled_file) except py_compile.PyCompileError as err: if error_level < 2: print(f\\"Compilation error in file {file_path}: {err.msg}\\") if error_level == 0 else None if raise_on_error: raise return compiled_files"},{"question":"**Objective:** Demonstrate the execution of a Python module using the `runpy` module while managing global variables and handling system alterations. Problem Statement: Create a function `execute_script(script_path: str, globals_dict: dict = None) -> dict:` that executes a given Python script located at `script_path` using the `runpy` module and returns the resultant global variables dictionary. The function should optionally allow pre-populating the module\'s global variables with the provided `globals_dict` dictionary. Requirements: 1. **Input:** - `script_path` (str): The file path to the Python script to execute. - `globals_dict` (dict): An optional dictionary to pre-populate the module\'s globals. 2. **Output:** - Returns a dictionary containing the global variables resulting from the script execution. 3. **Constraints:** - Ensure `globals_dict` is not modified during the function call. - Handle both successful execution and potential execution errors gracefully. 4. **Performance:** - The function should efficiently run scripts of moderate size, given typical constraints of runtime and memory for small to medium-sized scripts. Example Usage: ```python # example_script.py content: # def hello(): # return \\"Hello, world!\\" # # if __name__ == \\"__main__\\": # print(hello()) execute_script(\'example_script.py\') # Expected Output: # {\\"__name__\\": \\"__main__\\", ..., \\"hello\\": <function at 0x...>} ``` Implementation Notes: - Utilize `runpy.run_path()` to execute the script. - Handle setting and restoring alterations to `sys` attributes if needed. - Ensure the function does not modify the provided `globals_dict` and returns a fresh dictionary with the results.","solution":"import runpy import copy def execute_script(script_path, globals_dict=None): Executes a Python script at the given path with optional pre-populated globals. Parameters: - script_path (str): The file path to the Python script to execute. - globals_dict (dict): An optional dictionary to pre-populate the module\'s globals. Returns: - A dictionary containing the global variables after script execution. if globals_dict is None: globals_dict = {} # Ensure globals_dict is not modified by working with a copy provided_globals = copy.deepcopy(globals_dict) try: # Use runpy to execute the script with provided globals and capture the result result_globals = runpy.run_path(script_path, init_globals=provided_globals) return result_globals except Exception as e: print(f\\"Error executing script: {e}\\") return {}"},{"question":"**Challenging Python Buffer Protocol Task** In this task, you are required to create a Python function to manipulate memory buffers using the buffer protocol. Your task is to implement a function that processes elements within a given mutable buffer-like object. # Problem Statement Implement a function `process_buffer(buf_obj)` that takes a mutable buffer-like object, traverses it, and for each byte, applies a transformation function. The transformation function will invert the bits of a byte. The function does not return any value but modifies the buffer in place. # Function Signature ```python def process_buffer(buf_obj: bytearray) -> None: ``` # Input - `buf_obj`: A mutable buffer-like object. For simplicity, you can assume that it will always be a `bytearray`. # Output - The function modifies `buf_obj` in place. # Transformation Details - For each byte `b` in the buffer, transform it to `~b & 0xFF` (which inverts the bits of the byte). # Constraints 1. The length of `buf_obj` will be between 1 and 10^6. 2. The function should efficiently handle large buffers. # Example ```python buf = bytearray([0b00001111, 0b11110000]) process_buffer(buf) print(list(buf)) # Output: [240, 15] ``` # Notes 1. You should use the buffer protocol to access and work with the buffer\'s memory directly. 2. Pay attention to resource management to avoid memory leaks. Use `memoryview` as necessary. 3. Ensure that your implementation is efficient and can handle the upper limit of the constraints in a reasonable time. Implement this function ensuring you work within the bounds of Python\'s buffer protocol: ```python def process_buffer(buf_obj: bytearray) -> None: # Your code here to modify buf_obj in place ``` # Evaluation - Correctness: The function should correctly apply the transformation to each byte in the buffer. - Efficiency: The function should perform efficiently for large buffers within the given constraint.","solution":"def process_buffer(buf_obj: bytearray) -> None: Modifies the buffer object in place by inverting the bits of each byte. :param buf_obj: A mutable buffer-like object to be processed. with memoryview(buf_obj) as view: for i in range(len(view)): view[i] = ~view[i] & 0xFF"},{"question":"# PyTorch DataLoader Customization Problem **Objective:** Implement a custom PyTorch `DataLoader` for a map-style dataset with advanced features: multi-process data loading, custom batching, and memory pinning. **Problem Statement:** You are given a dataset comprising images and their corresponding labels. The dataset is a map-style dataset such that each index corresponds to a specific image-label pair. 1. **Dataset Preparation:** - Create a class `CustomImageDataset` that inherits from `torch.utils.data.Dataset`. - Implement `__init__`, `__len__`, and `__getitem__` methods to initialize the dataset, return the length of the dataset, and retrieve an image-label pair, respectively. 2. **DataLoader Customization:** - Write a function `create_data_loader` that takes the following parameters: * `dataset`: A dataset object from the `CustomImageDataset` class. * `batch_size`: An integer for the batch size. * `num_workers`: An integer for the number of worker processes. * `pin_memory`: A boolean to indicate whether to use pinned memory. * `collate_fn`: A custom collation function which pads sequences to the maximum length within the batch. - The function should return a `DataLoader` object that applies the specified batching and multi-processing features. **Constraints:** - Each image is represented as a tensor of size `(3, H, W)`, where `H` and `W` vary but `3` is constant (RGB channels). - Labels are integers representing the class index. - You should include a custom collation function that pads sequences to the maximum length within the batch. # Expected Input and Output Formats Input - A dataset object from `CustomImageDataset`. - An integer `batch_size`. - An integer `num_workers`. - A boolean `pin_memory`. - A custom `collate_fn`. Output - A `DataLoader` object properly configured. # Example ```python import torch from torch.utils.data import Dataset, DataLoader class CustomImageDataset(Dataset): # Use provided dataset initialization def __init__(self, image_list, label_list): self.image_list = image_list self.label_list = label_list # Get the length of the dataset def __len__(self): return len(self.image_list) # Retrieve an image-label pair def __getitem__(self, idx): image = self.image_list[idx] label = self.label_list[idx] return image, label # Custom collate function for padding sequences def custom_collate_fn(batch): # Logic for padding sequences to the maximum length within the batch images, labels = zip(*batch) max_len = max(image.shape[1] for image in images) # assuming channel-first format padded_images = [torch.nn.functional.pad(image, (0, 0, 0, max_len - image.shape[1])) for image in images] return torch.stack(padded_images), torch.tensor(labels) # DataLoader creation function def create_data_loader(dataset, batch_size, num_workers, pin_memory, collate_fn): return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn) # Example usage image_list = [torch.randn(3, h, w) for h, w in [(32, 32), (28, 28), (30, 30)]] label_list = [0, 1, 2] dataset = CustomImageDataset(image_list, label_list) loader = create_data_loader(dataset, batch_size=2, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn) for batch in loader: print(batch) ``` # Additional Notes - Ensure that your implementation handles variable image sizes correctly and uses efficient techniques suitable for multi-process data loading. - You may test the implementation with randomized image tensors and labels.","solution":"import torch from torch.utils.data import Dataset, DataLoader class CustomImageDataset(Dataset): Custom PyTorch Dataset for loading images and their corresponding labels. def __init__(self, image_list, label_list): self.image_list = image_list self.label_list = label_list def __len__(self): return len(self.image_list) def __getitem__(self, idx): image = self.image_list[idx] label = self.label_list[idx] return image, label def custom_collate_fn(batch): Custom collation function to pad images in the batch to the same size. images, labels = zip(*batch) max_height = max(image.shape[1] for image in images) max_width = max(image.shape[2] for image in images) padded_images = [] for image in images: padding = torch.nn.functional.pad(image, (0, max_width - image.shape[2], 0, max_height - image.shape[1])) padded_images.append(padding) return torch.stack(padded_images), torch.tensor(labels) def create_data_loader(dataset, batch_size, num_workers, pin_memory, collate_fn): Function to create a DataLoader with customized settings. return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn)"},{"question":"**Seaborn Boxplot Customization and Grouping Challenge** You are provided with the Titanic dataset, which contains various details about the passengers. Your task is to create a series of visualizations using Seaborn\'s `boxplot` function to analyze age and fare distribution across different classes and survival statuses. The goal is to demonstrate your ability to customize and manipulate boxplots using Seaborn. # Requirements: 1. **Load the Titanic Dataset**: - Use the Seaborn library to load the Titanic dataset. 2. **Basic Boxplot**: - Create a basic horizontal boxplot of the `age` variable. 3. **Grouped Boxplot**: - Create a vertical boxplot showing the age distribution across different `class` categories. 4. **Nested Grouping and Customization**: - Create a vertical boxplot of `age` grouped by `class` and `alive` status (nested grouping). Customize the plot to: - Draw the boxes as line art. - Add a gap between nested boxes. - Set the whiskers to cover the full range of the data (0 to 100 percentile). 5. **Fully Customized Plot**: - Create a vertical boxplot of `age` across different `deck` levels. Apply the following customizations: - Make the boxes narrower. - Change the color and width of all the line artists. - Customize the median line to be red and thicker. 6. **Numeric Grouping**: - Create a boxplot to show the distribution of `fare` grouped by rounded `age` (to the nearest decade). Ensure that the grouping preserves the native scale and add a vertical line at age 25. # Expectations: - Your solution should include well-commented code explaining each step. - The plots must be easy to read and should display meaningful insights. - All customizations and groupings must be implemented correctly according to the specifications. # Constraints: - Import only necessary libraries (ensure minimal library usage for loading datasets and plotting). - Do NOT manually modify any part of the dataset; use Seaborn and Pandas functionalities for data handling. **Submission:** - Submit a Python script or Jupyter notebook containing your solution. - Ensure that your code runs correctly and generates the required plots when executed. **Example Code Structure**: ```python # Importing necessary libraries import seaborn as sns import matplotlib.pyplot as plt # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Step-by-step solutions for the requirements # 1. Basic Boxplot sns.boxplot(x=titanic[\\"age\\"]) plt.show() # 2. Grouped Boxplot sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\") plt.show() # And so forth... ``` This question is designed to test your ability to handle data visualization tasks that require a combination of basic and advanced customizations using the Seaborn library. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Basic Horizontal Boxplot of the `age` variable def basic_age_boxplot(data): sns.boxplot(x=data[\\"age\\"]) plt.title(\\"Basic Horizontal Boxplot of Age\\") plt.xlabel(\\"Age\\") plt.show() # 2. Vertical Boxplot showing the `age` distribution across different `class` categories def age_distributed_by_class(data): sns.boxplot(data=data, x=\\"class\\", y=\\"age\\") plt.title(\\"Vertical Boxplot of Age by Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # 3. Vertical Boxplot of `age` grouped by `class` and `alive` status (nested grouping) def age_distributed_by_class_and_alive(data): sns.boxplot(data=data, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", palette=\\"Set2\\", dodge=True, boxprops=dict(alpha=0.5)) plt.title(\\"Vertical Boxplot of Age by Class and Alive Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # 4. Vertical Boxplot of `age` across different `deck` levels with customizations def age_by_deck_customized(data): sns.boxplot(data=data, x=\\"deck\\", y=\\"age\\", width=0.5, linewidth=2.5, palette=\\"coolwarm\\", boxprops=dict(edgecolor=\'b\', linewidth=3)) plt.title(\\"Vertical Boxplot of Age by Deck\\") plt.xlabel(\\"Deck\\") plt.ylabel(\\"Age\\") plt.axhline(25, color=\'red\', linestyle=\'-\', linewidth=2) plt.show() # 5. Boxplot of `fare` grouped by rounded `age` (nearest decade) with a vertical line at age 25 def fare_by_rounded_age(data): data[\'age_rounded\'] = (data[\'age\'] // 10) * 10 sns.boxplot(data=data, x=\\"age_rounded\\", y=\\"fare\\", linewidth=2.5, palette=\\"viridis\\") plt.title(\\"Boxplot of Fare by Decade-rounded Age\\") plt.xlabel(\\"Age (Rounded to Decade)\\") plt.ylabel(\\"Fare\\") plt.axvline(2.5, color=\'red\', linestyle=\'-\', linewidth=2) # vertical line approximation for age 25 (places between 2nd and 3rd tick) plt.show()"},{"question":"# PyTorch Meta Tensors **Objective:** Assess your understanding of working with meta tensors in PyTorch. By the end of this task, you should be able to demonstrate proficiency in managing model parameters abstractly using meta tensors and converting them into actual data for practical computations. **Task:** 1. **Create a meta tensor from a saved model state**: - Save a random tensor to a file. - Load the tensor using a `meta` device and print it. 2. **Perform operations in meta device context**: - Create a sample linear layer in the meta device context and inspect its representation metadata. 3. **Convert the meta tensors to actual tensors**: - Initialize the previously created meta tensor with actual data. - Move the initialized tensor to CPU and print it. **Specifications:** 1. **Save and Load a Tensor**: - **Input**: None - **Output**: Print the meta tensor loaded from a file. ```python # Save a random tensor torch.save(torch.randn(4, 4), \'tensor.pt\') # Load the tensor with meta device meta_tensor = torch.load(\'tensor.pt\', map_location=\'meta\') print(meta_tensor) ``` 2. **Create and Inspect a Linear Layer**: - **Input**: None - **Output**: Print the metadata of the Linear layer. ```python from torch.nn import Linear with torch.device(\'meta\'): layer = Linear(10, 5) print(layer) ``` 3. **Convert and Initialize Meta Tensors**: - **Input**: `meta_tensor` from Step 1 - **Output**: Print the tensor initialized with zeros and moved to CPU. ```python # Use a factory function to initialize data for the meta tensor initialized_tensor = torch.zeros_like(meta_tensor, device=\'cpu\') print(initialized_tensor) ``` **Constraints:** - Ensure you do not directly move meta tensors to CPU/CUDA without initializing their data. - Use appropriate factory functions for the initializations. **Performance Requirements:** - Efficiently utilize memory by leveraging meta tensors. - Ensure operations respect the constraints of meta tensors and device conversions. Good luck!","solution":"import torch from torch.nn import Linear def save_and_load_tensor(): # Save a random tensor torch.save(torch.randn(4, 4), \'tensor.pt\') # Load the tensor with meta device meta_tensor = torch.load(\'tensor.pt\', map_location=\'meta\') return meta_tensor def create_linear_layer(): with torch.device(\'meta\'): layer = Linear(10, 5) return layer def initialize_tensor(meta_tensor): # Use a factory function to initialize data for the meta tensor initialized_tensor = torch.zeros_like(meta_tensor, device=\'cpu\') return initialized_tensor"},{"question":"# Memory and Data Type Analysis with pandas DataFrames You are provided with a dataset containing various types of data, represented in a dictionary format. Your task is to create a pandas DataFrame, analyze its memory usage, handle missing values, and ensure that the DataFrame is optimized for memory usage. Input You will be given a dictionary `data_dict` with the following structure: ```python data_dict = { \\"integers\\": [1, 2, None, 4, 5], \\"floats\\": [1.0, 2.5, None, 4.5, 5.0], \\"dates\\": [\\"2021-01-01\\", None, \\"2021-03-01\\", \\"2021-04-01\\", \\"2021-05-01\\"], \\"strings\\": [\\"apple\\", None, \\"banana\\", \\"cherry\\", \\"date\\"] } ``` Task 1. Create a DataFrame from `data_dict`. 2. Handle missing values by filling them with appropriate values: - For integer values, use a special sentinel value like `-1`. - For float values, use the mean of the column. - For date values, use the first date in the column. - For string values, fill with an empty string `\\"\\"`. 3. Convert data types where appropriate to ensure optimal memory usage: - Integers should use `Int32Dtype`. - Dates should be converted to datetime64. - Strings should remain as object type (since categorical conversion may not always save memory). 4. Calculate the deep memory usage of the DataFrame and return it. 5. Identify and return which column consumes the most memory. Output A dictionary containing: - The total memory usage of the DataFrame. - The memory usage of each column. - The name of the column that consumes the most memory. Example ```python def analyze_memory_usage(data_dict): # Your implementation here # Example usage: data_dict = { \\"integers\\": [1, 2, None, 4, 5], \\"floats\\": [1.0, 2.5, None, 4.5, 5.0], \\"dates\\": [\\"2021-01-01\\", None, \\"2021-03-01\\", \\"2021-04-01\\", \\"2021-05-01\\"], \\"strings\\": [\\"apple\\", None, \\"banana\\", \\"cherry\\", \\"date\\"] } result = analyze_memory_usage(data_dict) print(result) ``` Constraints - Use pandas and numpy packages only. - Ensure optimized performance for processing large DataFrames. This question assesses the student\'s understanding of key pandas concepts including DataFrame creation, memory usage analysis, handling missing values, and data type conversion for optimal memory usage.","solution":"import pandas as pd import numpy as np def analyze_memory_usage(data_dict): # Create DataFrame df = pd.DataFrame(data_dict) # Handle missing values df[\'integers\'].fillna(-1, inplace=True) df[\'floats\'].fillna(df[\'floats\'].mean(), inplace=True) df[\'dates\'].fillna(df[\'dates\'].dropna().iloc[0], inplace=True) df[\'strings\'].fillna(\\"\\", inplace=True) # Convert data types df[\'integers\'] = df[\'integers\'].astype(\'Int32\') df[\'dates\'] = pd.to_datetime(df[\'dates\']) # Calculate memory usage memory_usage = df.memory_usage(deep=True) total_memory_usage = memory_usage.sum() max_memory_usage_column = memory_usage.idxmax() return { \\"total_memory_usage\\": total_memory_usage, \\"memory_usage_by_column\\": memory_usage.to_dict(), \\"max_memory_usage_column\\": max_memory_usage_column }"},{"question":"# Question You are provided with a dataset and your task is to preprocess this data using various preprocessing techniques provided by scikit-learn. You will implement a function `preprocess_data` that takes a 2D NumPy array `X` and a list of preprocessing steps, and returns the preprocessed data. Each preprocessing step is represented as a dictionary containing the `step` name and corresponding parameters: - `standardize`: Standardize features by removing the mean and scaling to unit variance. - `normalize`: Scale individual samples to have unit norm. - `minmax_scale`: Scale features to a given range [0, 1]. - `binarize`: Threshold numerical features to get boolean values. - `onehot_encode`: Encode categorical features using one-hot encoding. - `polynomial_features`: Generate polynomial and interaction features. Input Format - `X`: A 2D NumPy array of shape (n_samples, n_features) - `preprocessing_steps`: A list of dictionaries where each dictionary contains: - `step`: A string representing the preprocessing step. Possible values are: \'standardize\', \'normalize\', \'minmax_scale\', \'binarize\', \'onehot_encode\', \'polynomial_features\'. - `params`: A dictionary of parameters required for the specified preprocessing step. If no parameters are needed, it will be an empty dictionary. Output Format - Returns a 2D NumPy array of the preprocessed data. Constraints - `X` can have a mix of numerical and categorical features. - Categorical features should be represented as strings. - Assume all categorical features are the last columns of `X`. Example ```python X = np.array([ [1.5, 2, \'male\'], [3.1, 0, \'female\'], [4.2, 1, \'male\'] ]) preprocessing_steps = [ {\'step\': \'standardize\', \'params\': {}}, {\'step\': \'onehot_encode\', \'params\': {}} ] preprocessed_data = preprocess_data(X, preprocessing_steps) print(preprocessed_data) ``` Implement the function `preprocess_data(X, preprocessing_steps)` using the instructions provided.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, Binarizer from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures def preprocess_data(X, preprocessing_steps): Preprocess the given data based on the specified preprocessing steps. Args: - X (np.ndarray): 2D array of shape (n_samples, n_features) - preprocessing_steps (list): List of dictionaries containing preprocessing steps Returns: - np.ndarray: Preprocessed data for step in preprocessing_steps: if step[\'step\'] == \'standardize\': scaler = StandardScaler() X = scaler.fit_transform(X) elif step[\'step\'] == \'normalize\': normalizer = Normalizer() X = normalizer.fit_transform(X) elif step[\'step\'] == \'minmax_scale\': scaler = MinMaxScaler() X = scaler.fit_transform(X) elif step[\'step\'] == \'binarize\': binarizer = Binarizer(**step[\'params\']) X = binarizer.fit_transform(X) elif step[\'step\'] == \'onehot_encode\': categorical_features = np.array(X[:,-1]) numerical_features = np.array(X[:,:-1], dtype=float) encoder = OneHotEncoder() categorical_encoded = encoder.fit_transform(categorical_features.reshape(-1, 1)).toarray() X = np.hstack((numerical_features, categorical_encoded)) elif step[\'step\'] == \'polynomial_features\': poly = PolynomialFeatures(**step[\'params\']) X = poly.fit_transform(X) return X"},{"question":"**Context:** You are provided with a time series dataset representing some measurable quantity over time (e.g., stock prices). You are required to implement a solution to analyze this time series data using various window functions from pandas to extract meaningful statistical insights. **Objective:** 1. Compute a series of statistics using rolling windows with a specified window size. 2. Compute a similar series of statistics using expanding windows. 3. Compute exponentially-weighted moving averages. 4. Implement a custom window indexer to apply a non-standard windowing approach. **Input Format:** 1. A pandas DataFrame `df` with a time series indexed by datetime and containing at least one numerical column, `value`. 2. An integer `window_size` representing the size of the rolling window. 3. A float `alpha` representing the smoothing factor for exponentially weighted window functions. **Tasks:** 1. Implement a function `compute_rolling_statistics(df, window_size)` that computes the rolling mean, standard deviation, and sum of the column `value` using a rolling window with the given `window_size`. 2. Implement a function `compute_expanding_statistics(df)` that computes the expanding mean, standard deviation, and sum of the column `value`. 3. Implement a function `compute_ewm_statistics(df, alpha)` that computes the exponentially weighted mean and standard deviation of the column `value` using the given `alpha`. 4. Implement a function `apply_custom_window_indexer(df, window_size)` that uses a custom window indexer to compute the rolling mean and standard deviation. Use `FixedForwardWindowIndexer` to define a window which includes previous `window_size` elements plus the current element. **Constraints:** - The input DataFrame `df` is guaranteed to have a DateTimeIndex. - The `window_size` will be a positive integer. # Implementation ```python import pandas as pd import numpy as np from pandas.api.indexers import FixedForwardWindowIndexer def compute_rolling_statistics(df, window_size): rolling_df = df[\'value\'].rolling(window=window_size) return { \'rolling_mean\': rolling_df.mean(), \'rolling_std\': rolling_df.std(), \'rolling_sum\': rolling_df.sum() } def compute_expanding_statistics(df): expanding_df = df[\'value\'].expanding() return { \'expanding_mean\': expanding_df.mean(), \'expanding_std\': expanding_df.std(), \'expanding_sum\': expanding_df.sum() } def compute_ewm_statistics(df, alpha): ewm_df = df[\'value\'].ewm(alpha=alpha) return { \'ewm_mean\': ewm_df.mean(), \'ewm_std\': ewm_df.std() } def apply_custom_window_indexer(df, window_size): indexer = FixedForwardWindowIndexer(window_size=window_size) rolling_df = df[\'value\'].rolling(window=indexer) return { \'custom_rolling_mean\': rolling_df.mean(), \'custom_rolling_std\': rolling_df.std() } # Example usage: # df = pd.DataFrame({\'value\': np.random.randn(100)}, index=pd.date_range(start=\'2020-01-01\', periods=100)) # result1 = compute_rolling_statistics(df, window_size=5) # result2 = compute_expanding_statistics(df) # result3 = compute_ewm_statistics(df, alpha=0.5) # result4 = apply_custom_window_indexer(df, window_size=5) ``` # Performance and Edge Cases: - Ensure that the implementation handles edge cases such as the window size being larger than the DataFrame length. - The functions should be efficient enough to handle large time series data with tens of thousands of rows.","solution":"import pandas as pd import numpy as np from pandas.api.indexers import FixedForwardWindowIndexer def compute_rolling_statistics(df, window_size): rolling_df = df[\'value\'].rolling(window=window_size) return { \'rolling_mean\': rolling_df.mean(), \'rolling_std\': rolling_df.std(), \'rolling_sum\': rolling_df.sum() } def compute_expanding_statistics(df): expanding_df = df[\'value\'].expanding() return { \'expanding_mean\': expanding_df.mean(), \'expanding_std\': expanding_df.std(), \'expanding_sum\': expanding_df.sum() } def compute_ewm_statistics(df, alpha): ewm_df = df[\'value\'].ewm(alpha=alpha) return { \'ewm_mean\': ewm_df.mean(), \'ewm_std\': ewm_df.std() } def apply_custom_window_indexer(df, window_size): indexer = FixedForwardWindowIndexer(window_size=window_size) rolling_df = df[\'value\'].rolling(window=indexer) return { \'custom_rolling_mean\': rolling_df.mean(), \'custom_rolling_std\': rolling_df.std() }"},{"question":"# Question: Implement a Custom Distutils Command Your task is to create a custom Distutils command that integrates with the existing Distutils system. Specifically, you will create a command called `list_py_modules`, which will list all Python modules present in the specified package directory. # Requirements: 1. **Command Name**: - The command should be named `list_py_modules`. 2. **Functionality**: - The command should take a single argument `package_dir`, which is the directory of the package where Python modules should be listed from. - The command should list all `.py` files present in the provided directory and its subdirectories, but exclude test files (i.e., files ending with `_test.py`). 3. **Implementation Details**: - Your command should be implemented as a subclass of `distutils.cmd.Command`. - Define the `initialize_options`, `finalize_options`, and `run` methods as required by Distutils commands. - Use suitable Distutils and Python standard library functions and classes for file operations. # Example Usage: A `setup.py` file using this custom command might look like this: ```python from distutils.core import setup from list_py_modules import ListPyModules setup( name=\'MyPackage\', version=\'0.1\', cmdclass={ \'list_py_modules\': ListPyModules, }, package_dir={\'\': \'src\'}, packages=[\'mypackage\'], ) ``` And the command could be run from the command line as: ```sh python setup.py list_py_modules --package-dir=src/mypackage ``` # Expected Output: When run, the command should print a list of all `.py` files in the specified package directory, excluding test files. # Constraints: - Your implementation must handle errors gracefully, such as providing meaningful error messages if the directory does not exist. - Ensure that your implementation follows the standard practices of creating and running Distutils commands. # Performance Requirements: - The solution should efficiently handle directories with a large number of files and nested subdirectories. # Example Code Template: ```python import os from distutils.cmd import Command class ListPyModules(Command): description = \'List all Python modules in the specified package directory, excluding test files.\' user_options = [ (\'package-dir=\', \'p\', \'Directory of the package where Python modules should be listed from\') ] def initialize_options(self): self.package_dir = None def finalize_options(self): if self.package_dir is None: raise Exception(\'package-dir is required\') def run(self): # Logic to list all .py files excluding test files pass ``` Complete the implementation of the `run` method to accomplish the task described.","solution":"import os from distutils.cmd import Command class ListPyModules(Command): description = \'List all Python modules in the specified package directory, excluding test files.\' user_options = [ (\'package-dir=\', \'p\', \'Directory of the package where Python modules should be listed from\') ] def initialize_options(self): self.package_dir = None def finalize_options(self): if self.package_dir is None: raise Exception(\'package-dir is required\') if not os.path.isdir(self.package_dir): raise Exception(f\'Directory {self.package_dir} does not exist\') def run(self): for root, _, files in os.walk(self.package_dir): for file in files: if file.endswith(\'.py\') and not file.endswith(\'_test.py\'): print(os.path.join(root, file))"},{"question":"**Question: Manipulating and Performing Operations on Complex Tensors in PyTorch** You are required to write a function that performs a series of operations on complex tensors using PyTorch. The function should take as input a real tensor representing complex numbers and output a transformed tensor after a series of operations. # Input - A real tensor `input_tensor` of shape `(..., 2)` where the last dimension contains the real and imaginary parts of the complex numbers. # Output - A complex tensor that has undergone the following transformations: 1. Convert the input tensor to a complex tensor. 2. Double the real part of each complex number in the tensor. 3. Compute the magnitude (`abs`) and angle (`angle`) of each complex number. 4. Create a new complex tensor using the magnitude as the real part and the angle as the imaginary part. # Function Signature ```python def transform_complex_tensor(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` # Constraints - You can assume the input tensor has floating point numbers. - You can use any function provided by PyTorch for complex number operations. # Example ```python import torch input_tensor = torch.tensor([[3.0, 4.0], [1.0, 1.0]], dtype=torch.float32) result_tensor = transform_complex_tensor(input_tensor) print(result_tensor) # Output should be a complex tensor with the specified transformations applied. ``` # Your Task Implement the function `transform_complex_tensor` to perform the series of operations specified.","solution":"import torch def transform_complex_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Transforms the input real tensor representing complex numbers. Parameters: - input_tensor (torch.Tensor): A tensor of shape (..., 2) where the last dimension contains the real and imaginary parts of the complex numbers. Returns: - torch.Tensor: A complex tensor after the specified transformations. # Step 1: Convert the input tensor to a complex tensor complex_tensor = torch.view_as_complex(input_tensor) # Step 2: Double the real part of each complex number in the tensor complex_tensor = 2 * complex_tensor.real + 1j * complex_tensor.imag # Step 3: Compute magnitude and angle of each complex number magnitude = torch.abs(complex_tensor) angle = torch.angle(complex_tensor) # Step 4: Create a new complex tensor using magnitude as the real part and angle as the imaginary part transformed_tensor = magnitude + 1j * angle return transformed_tensor"},{"question":"**Objective**: Write a custom Python module using C extension which demonstrates the creation of a module with specific attributes, methods, and proper handling of initialization phases. # Requirements: 1. **Module Creation and Attributes**: - Create a module named `mymodule`. - Set the following attributes for the module: - `__name__`: The name of the module (`mymodule`). - `__doc__`: A simple docstring, e.g., `\\"This is a custom module called mymodule.\\"`. - `__version__`: Version of your module, e.g., `\\"1.0\\"`. - Add a state attribute that can store an integer value which starts at 0 and can be updated by the methods in the module. 2. **Module Methods**: - Implement a method `set_state` that takes an integer argument and sets the module\'s state to this value. - Implement a method `get_state` that returns the current state value. - Implement a method `increment_state` that increments the state by 1 each time itâ€™s called. 3. **Initialization Phase**: - Ensure that your module can be initialized using both single-phase and multi-phase initialization, with appropriate use of the `PyModuleDef` structure. - Ensure that the state attribute is managed correctly in both types of initialization. # Constraints: - Your code should compile and run on Python 3.10. - You should handle reference counting and memory management according to best practices. # Input and Output: Input: You do not need to handle user inputs directly. Your functions should behave correctly with internal tests. Output: No explicit output. The correctness will be validated by importing the module and calling the methods to ensure they work as expected. # Example Usage: ```python import mymodule mymodule.set_state(10) print(mymodule.get_state()) # Output: 10 mymodule.increment_state() print(mymodule.get_state()) # Output: 11 ``` # Implementation Tips: - Use the documentation provided to understand the functions and structures. - Pay close attention to reference counts when adding objects/methods to the module. - Ensure proper error handling by setting appropriate exceptions where needed. - Comment your code thoroughly to explain the process and decision-making. Note: This question does not require you to implement the actual integration with C or provide the complete setup for a C extension. However, demonstrating the knowledge of handling modules using the provided functions in the documentation is crucial.","solution":"# This will be a mock Python implementation since the actual creation of C extension modules involves # compiling C code and interacting with Python\'s C API, which cannot be run in this environment. class MyModule: This is a custom module called mymodule. __name__ = \\"mymodule\\" __doc__ = \\"This is a custom module called mymodule.\\" __version__ = \\"1.0\\" def __init__(self): self.state = 0 def set_state(self, value): Sets the module\'s state to the provided value. if isinstance(value, int): self.state = value else: raise ValueError(\\"State must be an integer.\\") def get_state(self): Returns the current state value. return self.state def increment_state(self): Increments the current state value by 1. self.state += 1 # Create an instance of the module to emulate the actual module behavior mymodule = MyModule()"},{"question":"# PyTorch Special Functions Assessment You are given a dataset which resembles a signal captured over time. Your task is to apply a series of special mathematical transformations to this dataset using the `torch.special` module in PyTorch. # Task Description 1. **Generate a Synthetic Signal**: - Create a synthetic signal `x` using a combination of sinusoidal functions. This signal should be represented as a 1D tensor with 1000 points over the interval `[0, 10]`. 2. **Apply Special Functions**: - Use at least five different functions from the `torch.special` module to transform your signal. - Create a new signal which combines these transformations in a meaningful way (e.g., summing, multiplying, or creating a composite function). 3. **Plotting**: - Use a plotting library of your choice (e.g., Matplotlib) to visualize the original and transformed signals on the same plot for comparison. # Requirements 1. **Input Format**: - No direct input needed. The signal is generated within the script. 2. **Output Format**: - A plot that shows the original and transformed signals. 3. **Constraints**: - Use PyTorch tensors and functions from `torch.special` module exclusively for transformations. - Appropriately comment your code to explain the transformations being applied. # Example Here\'s an outline of how your code might look (it\'s not a complete solution, just a sketch): ```python import torch import numpy as np import matplotlib.pyplot as plt # Generate synthetic signal t = torch.linspace(0, 10, 1000) x = torch.sin(t) + 0.5 * torch.sin(3 * t) # Apply special functions x_transformed = torch.special.erf(x) + torch.special.sinc(x) + torch.special.logit(x + 1) # and two more functions # Plotting plt.plot(t.numpy(), x.numpy(), label=\'Original Signal\') plt.plot(t.numpy(), x_transformed.numpy(), label=\'Transformed Signal\') plt.legend() plt.show() ``` # Notes - Ensure your code is efficient and leverages the capabilities of PyTorch properly. - Think critically about which transformations to use and explain your reasoning in the comments. Good luck and happy coding!","solution":"import torch import numpy as np import matplotlib.pyplot as plt # Generate synthetic signal t = torch.linspace(0, 10, 1000) x = torch.sin(t) + 0.5 * torch.sin(3 * t) # Apply special functions x_erf = torch.special.erf(x) # Error function x_sinc = torch.special.sinc(x) # Sinc function x_logit = torch.special.logit(x + 1) # Logit function x_i0 = torch.special.i0(x) # Modified Bessel function of the first kind x_gammaln = torch.special.gammaln(x + 1) # Logarithm of the absolute value of the gamma function # Combine the transformations in a meaningful way x_transformed = x_erf + x_sinc * x_logit + x_i0 - x_gammaln # Plotting plt.plot(t.numpy(), x.numpy(), label=\'Original Signal\') plt.plot(t.numpy(), x_transformed.numpy(), label=\'Transformed Signal\') plt.legend() plt.xlabel(\'Time\') plt.ylabel(\'Signal\') plt.title(\'Original and Transformed Signal\') plt.show()"},{"question":"**Objective:** Develop a command-line calendar event manager using the `argparse` module. Your tool should allow users to create, list, update, and delete events. The tool must demonstrate the following `argparse` functionalities: 1. Handling sub-commands (e.g., add, list, update, delete). 2. Adding both positional and optional arguments. 3. Customizing help messages. 4. Ensuring required arguments are provided. **Specifications:** 1. Implement a command-line tool using `argparse` that supports the following commands: - **add**: Add a new event with the event name and date. - **list**: List all events. - **update**: Update an existing event based on the event name. - **delete**: Delete an event based on the event name. 2. Each command should have its own arguments as follows: - **add**: - Required positional argument: `name` (event name) - Required positional argument: `date` (event date in `YYYY-MM-DD` format) - **list**: No additional arguments. - **update**: - Required positional argument: `name` (event name) - Optional positional argument: `new_name` (new event name) - Optional positional argument: `new_date` (new event date in `YYYY-MM-DD` format) - **delete**: - Required positional argument: `name` (event name) 3. Custom help messages should be provided for each command. 4. The program must validate the date format and ensure mandatory arguments are provided. **Example Usage:** ```sh # Add an event python event_manager.py add \\"Meeting\\" \\"2023-12-01\\" # List all events python event_manager.py list # Update an event python event_manager.py update \\"Meeting\\" \\"Conference\\" \\"2023-12-02\\" # Delete an event python event_manager.py delete \\"Conference\\" ``` **Constraints:** - The Python code should only use standard libraries. - The events can be stored in memory (a list or dictionary) for simplicity. **Submission Requirements:** 1. Python script implementing the command-line tool. 2. Example usage of the tool along with expected outputs. ```python import argparse from datetime import datetime events = {} def add_event(name, date): try: datetime.strptime(date, \'%Y-%m-%d\') except ValueError: print(\\"Error: Incorrect date format. Expected YYYY-MM-DD.\\") return events[name] = date print(f\\"Event \'{name}\' added on {date}\\") def list_events(): if not events: print(\\"No events found.\\") return for name, date in events.items(): print(f\\"Event: {name}, Date: {date}\\") def update_event(name, new_name, new_date): if name not in events: print(f\\"Error: Event \'{name}\' not found.\\") return if new_date: try: datetime.strptime(new_date, \'%Y-%m-%d\') except ValueError: print(\\"Error: Incorrect date format. Expected YYYY-MM-DD.\\") return events[name] = new_date if new_name: events[new_name] = events.pop(name) print(f\\"Event \'{name}\' updated.\\") def delete_event(name): if name not in events: print(f\\"Error: Event \'{name}\' not found.\\") return del events[name] print(f\\"Event \'{name}\' deleted.\\") def main(): parser = argparse.ArgumentParser(description=\\"Manage Your Calendar Events\\") subparsers = parser.add_subparsers(dest=\'command\', required=True, help=\'Sub-command help\') add_parser = subparsers.add_parser(\'add\', help=\'Add a new event\') add_parser.add_argument(\'name\', type=str, help=\'Event name\') add_parser.add_argument(\'date\', type=str, help=\'Event date (YYYY-MM-DD)\') list_parser = subparsers.add_parser(\'list\', help=\'List all events\') update_parser = subparsers.add_parser(\'update\', help=\'Update an existing event\') update_parser.add_argument(\'name\', type=str, help=\'Event name\') update_parser.add_argument(\'new_name\', type=str, nargs=\'?\', help=\'New event name\') update_parser.add_argument(\'new_date\', type=str, nargs=\'?\', help=\'New event date (YYYY-MM-DD)\') delete_parser = subparsers.add_parser(\'delete\', help=\'Delete an event\') delete_parser.add_argument(\'name\', type=str, help=\'Event name\') args = parser.parse_args() if args.command == \'add\': add_event(args.name, args.date) elif args.command == \'list\': list_events() elif args.command == \'update\': update_event(args.name, args.new_name, args.new_date) elif args.command == \'delete\': delete_event(args.name) if __name__ == \'__main__\': main() ```","solution":"import argparse from datetime import datetime events = {} def add_event(name, date): try: datetime.strptime(date, \'%Y-%m-%d\') except ValueError: print(\\"Error: Incorrect date format. Expected YYYY-MM-DD.\\") return events[name] = date print(f\\"Event \'{name}\' added on {date}\\") def list_events(): if not events: print(\\"No events found.\\") return for name, date in events.items(): print(f\\"Event: {name}, Date: {date}\\") def update_event(name, new_name, new_date): if name not in events: print(f\\"Error: Event \'{name}\' not found.\\") return if new_date: try: datetime.strptime(new_date, \'%Y-%m-%d\') except ValueError: print(\\"Error: Incorrect date format. Expected YYYY-MM-DD.\\") return events[name] = new_date if new_name: events[new_name] = events.pop(name) print(f\\"Event \'{name}\' updated.\\") def delete_event(name): if name not in events: print(f\\"Error: Event \'{name}\' not found.\\") return del events[name] print(f\\"Event \'{name}\' deleted.\\") def main(): parser = argparse.ArgumentParser(description=\\"Manage Your Calendar Events\\") subparsers = parser.add_subparsers(dest=\'command\', required=True, help=\'Sub-command help\') add_parser = subparsers.add_parser(\'add\', help=\'Add a new event\') add_parser.add_argument(\'name\', type=str, help=\'Event name\') add_parser.add_argument(\'date\', type=str, help=\'Event date (YYYY-MM-DD)\') list_parser = subparsers.add_parser(\'list\', help=\'List all events\') update_parser = subparsers.add_parser(\'update\', help=\'Update an existing event\') update_parser.add_argument(\'name\', type=str, help=\'Event name\') update_parser.add_argument(\'new_name\', type=str, nargs=\'?\', help=\'New event name\') update_parser.add_argument(\'new_date\', type=str, nargs=\'?\', help=\'New event date (YYYY-MM-DD)\') delete_parser = subparsers.add_parser(\'delete\', help=\'Delete an event\') delete_parser.add_argument(\'name\', type=str, help=\'Event name\') args = parser.parse_args() if args.command == \'add\': add_event(args.name, args.date) elif args.command == \'list\': list_events() elif args.command == \'update\': update_event(args.name, args.new_name, args.new_date) elif args.command == \'delete\': delete_event(args.name) if __name__ == \'__main__\': main()"},{"question":"**Secure URL Generator with Token Validation** You are tasked with implementing an API to generate secure URLs with embedded tokens for password recovery, and to validate those tokens upon user request for resetting their password. # Function Requirements 1. **Function: `generate_reset_url(base_url: str, token_length: int) -> str`** - **Input:** - `base_url`: A string representing the base URL (e.g., \\"https://example.com/reset\\"). - `token_length`: An integer representing the number of random bytes to include in the token. - **Output:** - A string representing the full URL with an embedded secure token. - **Constraints:** - `token_length` should be a positive integer. - **Description:** - This function should generate a secure URL by appending a URL-safe token to the given `base_url`. Use the `secrets.token_urlsafe` function to generate the token. 2. **Function: `validate_token(provided_token: str, valid_token: str) -> bool`** - **Input:** - `provided_token`: A string representing the token extracted from a user\'s request URL. - `valid_token`: A string representing the original valid token that was generated when the URL was created. - **Output:** - A boolean that is `True` if the provided token matches the valid token, otherwise `False`. - **Description:** - This function should compare the two tokens using the `secrets.compare_digest` function to prevent timing attacks. # Implementation 1. Implement the `generate_reset_url` function to generate a secure URL containing a URL-safe token of the specified length. 2. Implement the `validate_token` function to securely compare the provided token with the valid token. # Example Usage ```python # Example of using the functions: base_url = \\"https://example.com/reset\\" token_length = 16 # Generate a secure reset URL reset_url = generate_reset_url(base_url, token_length) print(f\\"Generated URL: {reset_url}\\") # Simulate extracting the token from the generated URL import urllib.parse as urlparse parsed_url = urlparse.urlparse(reset_url) query_components = urlparse.parse_qs(parsed_url.query) provided_token = query_components.get(\'token\', [None])[0] # Validate the extracted token (assuming we know the original token) valid_token = reset_url.split(\'=\')[-1] # Extract the token from the generated URL is_valid = validate_token(provided_token, valid_token) print(f\\"Token is valid: {is_valid}\\") ``` # Constraints - Both functions should handle edge cases such as invalid input types and values properly, with appropriate error handling. - Ensure the implementation is efficient and adheres to the cryptographic security practices indicated in the `secrets` module documentation.","solution":"import secrets import urllib.parse def generate_reset_url(base_url: str, token_length: int) -> str: Generates a secure URL with an embedded token for password reset. Parameters: base_url (str): The base URL to which the token will be appended. token_length (int): The number of random bytes to include in the token. Returns: str: The generated secure URL with the token. if not isinstance(base_url, str) or not isinstance(token_length, int): raise ValueError(\\"Invalid input types. \'base_url\' must be a string and \'token_length\' must be an integer.\\") if token_length <= 0: raise ValueError(\\"token_length must be a positive integer.\\") token = secrets.token_urlsafe(token_length) parsed_url = urllib.parse.urljoin(base_url, f\\"?token={token}\\") return parsed_url def validate_token(provided_token: str, valid_token: str) -> bool: Validates if the provided token matches the valid token. Parameters: provided_token (str): The token extracted from the user\'s request URL. valid_token (str): The original valid token that was generated. Returns: bool: True if the provided token matches the valid token, otherwise False. if not isinstance(provided_token, str) or not isinstance(valid_token, str): raise ValueError(\\"Both tokens must be strings.\\") return secrets.compare_digest(provided_token, valid_token)"},{"question":"# Python Coding Assessment: Configuration File Manipulation **Objective**: Demonstrate your understanding of the `configparser` module by creating a function to read, modify, and write configuration files. **Problem Statement**: You are given a configuration (INI) file with various sections, each containing key-value pairs. Your task is to implement a function that reads this file, modifies specific key-value pairs as specified by the user, and writes the updated configuration back to the file. **Function Signature**: ```python def modify_config_file(file_path: str, section: str, key: str, new_value: str) -> None: ``` **Input**: 1. `file_path` (str): The path to the configuration (INI) file. 2. `section` (str): The section in the configuration file that contains the key to be modified. 3. `key` (str): The key within the section whose value is to be modified. 4. `new_value` (str): The new value to be assigned to the key. **Output**: - The function should not return anything. It should modify the file in place. **Constraints**: - You may assume that the file at `file_path` exists and is a valid INI file. - The specified `section` and `key` may or may not exist in the INI file. If the `section` does not exist, it should be created. If the `key` does not exist within the section, it should be created. - The value associated with the specified `key` should be updated to `new_value`. **Example**: Assume you have a configuration file `config.ini` with the following content: ```ini [General] version=1.0 author=John Doe [Settings] theme=dark auto_save=true ``` If you call the function: ```python modify_config_file(\'config.ini\', \'General\', \'author\', \'Jane Smith\') ``` The content of `config.ini` after modification should be: ```ini [General] version=1.0 author=Jane Smith [Settings] theme=dark auto_save=true ``` **Requirements**: - Use the `configparser` module for parsing, modifying, and writing back the configuration file. - Ensure that the modified configuration file maintains the correct structure and formatting. **Performance**: - The function should be efficient in terms of reading and writing the file. - Handle potential errors gracefully, such as file access issues or invalid file formats. Implement the function `modify_config_file` with the above specifications.","solution":"import configparser def modify_config_file(file_path: str, section: str, key: str, new_value: str) -> None: config = configparser.ConfigParser() # Read the existing configuration file config.read(file_path) # If the section does not exist, add it if not config.has_section(section): config.add_section(section) # Set the new value for the specified key in the section config.set(section, key, new_value) # Write the changes back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# FTP Automation Script As an experienced software developer, you have been tasked with writing a Python script to automate the interaction with an FTP server. Specifically, your script should perform the following tasks: 1. Connect to an FTP server using the provided host, username, and password. 2. Navigate to a specific directory on the server. 3. List the contents of the directory and save this listing to a local file named `directory_listing.txt`. 4. Download a specific file from the server to the local machine. 5. Upload a given file from the local machine to the server directory. 6. Handle any potential exceptions that may be raised during these operations. Implement the function `ftp_automation` which performs these tasks. **Function Signature:** ```python def ftp_automation(host: str, username: str, password: str, directory: str, download_filename: str, upload_filename: str) -> bool: pass ``` # Parameters: - `host (str)`: The FTP server address. - `username (str)`: The username for the FTP login. - `password (str)`: The password for the FTP login. - `directory (str)`: The directory on the server to navigate to. - `download_filename (str)`: The name of the file to download from the server. - `upload_filename (str)`: The name of the file to upload to the server. # Returns: - `bool`: Returns `True` if all operations are successful, otherwise `False`. # Constraints: - Assume the provided filenames (for downloading and uploading) are valid and accessible. - Handle exceptions using the relevant exception classes provided in the documentation (`ftplib.error_reply`, `ftplib.error_temp`, `ftplib.error_perm`, and `ftplib.error_proto`). - The script should work for servers adhering to standard FTP protocols. # Example Usage: ```python # Example usage host = \\"ftp.us.debian.org\\" username = \\"anonymous\\" password = \\"anonymous@\\" directory = \\"debian\\" download_filename = \\"README\\" upload_filename = \\"local_file.txt\\" success = ftp_automation(host, username, password, directory, download_filename, upload_filename) print(success) # Expected output: True if all operations are successful ``` You will need to write the code for the `ftp_automation` function to handle these tasks accordingly using the `ftplib` module. **Notes:** - Use the `ftplib.FTP` methods to perform the necessary FTP commands. - Make sure to handle the file operations (reading and writing) correctly. - Ensure robust exception handling to cover different potential issues during the FTP operations.","solution":"import ftplib def ftp_automation(host: str, username: str, password: str, directory: str, download_filename: str, upload_filename: str) -> bool: try: with ftplib.FTP(host) as ftp: ftp.login(user=username, passwd=password) ftp.cwd(directory) # List directory contents and save to local file with open(\'directory_listing.txt\', \'w\') as f: ftp.retrlines(\'LIST\', f.write) # Download a specific file from the server with open(download_filename, \'wb\') as f: ftp.retrbinary(f\'RETR {download_filename}\', f.write) # Upload a given file to the server with open(upload_filename, \'rb\') as f: ftp.storbinary(f\'STOR {upload_filename}\', f) return True except (ftplib.error_reply, ftplib.error_temp, ftplib.error_perm, ftplib.error_proto) as e: print(f\\"FTP error: {e}\\") return False except Exception as e: print(f\\"General error: {e}\\") return False"},{"question":"**Objective:** As part of this coding assessment, you are required to demonstrate your ability to utilize the `seaborn.objects` module to create informative visualizations. Specifically, you will work with the \\"diamonds\\" dataset to create a layered bar plot with different aggregation methods and transformations. **Task:** Write a Python function `create_diamond_barplot(diamond_dataset)` that generates a bar plot using the Seaborn `objects` interface. Your function should perform the following tasks: 1. Load the Seaborn `diamonds` dataset if not provided. 2. Create a plot where: a. The x-axis represents the \\"cut\\" category. b. The y-axis represents the carat weight. 3. Add a bar layer where each bar represents the mean carat weight for each cut category. 4. Overlay another bar layer that shows the 75th percentile carat weight reduced by the 25th percentile carat weight for each cut category. 5. Use `so.Dodge()` to distinguish bars from the different aggregation methods. 6. Color the bars based on the \\"clarity\\" category. **Input:** - `diamond_dataset`: A Pandas DataFrame containing diamond data. If not provided, use the Seaborn `load_dataset(\'diamonds\')`. **Output:** - A Seaborn plot object. **Constraints:** - You must use the `seaborn.objects` interface to complete this task. - You should handle cases where the dataset may not be provided by the user. **Example Usage:** ```python import seaborn as sns diamonds = sns.load_dataset(\\"diamonds\\") plot = create_diamond_barplot(diamonds) plot.show() ``` **Solution Template:** ```python import seaborn.objects as so import seaborn as sns def create_diamond_barplot(diamond_dataset=None): # Load the diamonds dataset if not provided if diamond_dataset is None: diamond_dataset = sns.load_dataset(\\"diamonds\\") # Create the plot p = so.Plot(diamond_dataset, x=\\"cut\\", y=\\"carat\\") # Add the first bar layer for the mean carat weight p.add(so.Bar(), so.Agg()) # Add the second bar layer for the 75th percentile - 25th percentile carat weight p.add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), so.Dodge(), color=\\"clarity\\") # Return the plot object return p ```","solution":"import seaborn.objects as so import seaborn as sns def create_diamond_barplot(diamond_dataset=None): Generates a bar plot using the Seaborn `objects` interface. Parameters: diamond_dataset (pd.DataFrame): DataFrame containing diamond data. If not provided, the Seaborn `diamonds` dataset will be used. Returns: seaborn.objects.Plot: A Seaborn plot object. # Load the diamonds dataset if not provided if diamond_dataset is None: diamond_dataset = sns.load_dataset(\\"diamonds\\") # Create the plot p = so.Plot(diamond_dataset, x=\\"cut\\", y=\\"carat\\") # Add the first bar layer for the mean carat weight p = p.add(so.Bar(), so.Agg(\\"mean\\"), so.Dodge(), color=\\"clarity\\") # Add the second bar layer for the 75th percentile - 25th percentile carat weight p = p.add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), so.Dodge(), color=\\"clarity\\") # Return the plot object return p"},{"question":"**Objective:** Demonstrate your understanding of Python\'s high-level data types, file I/O, and modular programming by implementing a complex data processing task. Problem Statement You are given a text file `library_database.txt` that contains library records. Each record is on a new line, and each field within a record is separated by a semicolon (`;`). The fields are as follows: - Book Title - Author - Year of Publication - Genre - Shelf Number Example content of `library_database.txt`: ``` The Great Gatsby;F. Scott Fitzgerald;1925;Fiction;A1 To Kill a Mockingbird;Harper Lee;1960;Fiction;A2 A Brief History of Time;Stephen Hawking;1988;Science;B1 The Art of Computer Programming;Donald E. Knuth;1968;Science;B2 Python Crash Course;Eric Matthes;2015;Programming;C1 ``` Your task is to implement a Python module that performs the following operations: 1. **Read Library Records**: Read the library records from `library_database.txt` and store them in an appropriate data structure. 2. **Display Books by Genre**: Implement a function `display_books_by_genre(genre)` that lists all books in the specified genre, sorted by year of publication. 3. **Search Book by Title**: Implement a function `search_book_by_title(title_keyword)` that searches for books by title keyword (case insensitive) and returns all matching records. 4. **Add New Record**: Implement a function `add_new_record(book_title, author, year, genre, shelf)` that adds a new record to the library and updates `library_database.txt`. Ensure that writing the file is done without disrupting existing data. 5. **Error Handling**: Ensure your functions handle possible errors gracefully, such as file not found, incorrect file format, or invalid input data. Constraints: - All fields are non-empty strings except for the Year of Publication, which is a positive integer. - The text file name `library_database.txt` is fixed and will be in the current working directory of the script. - You must use built-in Python functions and libraries only. Input & Output Format: 1. **display_books_by_genre(genre)** - Input: A string representing the genre (e.g., \\"Fiction\\"). - Output: A list of strings representing books in the specified genre, sorted by year in ascending order. Example: ``` display_books_by_genre(\\"Fiction\\") ``` Output: ``` [\'The Great Gatsby by F. Scott Fitzgerald (1925)\', \'To Kill a Mockingbird by Harper Lee (1960)\'] ``` 2. **search_book_by_title(title_keyword)** - Input: A string representing the keyword to search for in the book title (e.g., \\"Python\\"). - Output: A list of strings representing matching books. Example: ``` search_book_by_title(\\"Python\\") ``` Output: ``` [\'Python Crash Course by Eric Matthes (2015)\'] ``` 3. **add_new_record(book_title, author, year, genre, shelf)** - Input: Strings for book title, author, genre, and shelf; an integer for the year of publication. - Output: Boolean `True` if the record is added successfully or `False` if an error occurs. Example: ``` add_new_record(\\"Clean Code\\", \\"Robert C. Martin\\", 2008, \\"Programming\\", \\"C2\\") ``` Output: ``` True ``` All necessary information for solving this problem is provided. Ensure you write clean, modular, and well-documented code.","solution":"import os def read_library_database(file_name=\'library_database.txt\'): Reads the library records from a text file and returns them as a list of dictionaries. Each dictionary represents a book\'s record with the following keys: \'title\', \'author\', \'year\', \'genre\', \'shelf\'. if not os.path.exists(file_name): raise FileNotFoundError(f\\"File {file_name} not found.\\") library_records = [] try: with open(file_name, \'r\') as file: for line in file: parts = line.strip().split(\';\') if len(parts) != 5: raise ValueError(f\\"Incorrect format in line: {line}\\") record = { \'title\': parts[0], \'author\': parts[1], \'year\': int(parts[2]), \'genre\': parts[3], \'shelf\': parts[4] } library_records.append(record) except Exception as e: print(f\\"Error reading file: {e}\\") return [] return library_records def display_books_by_genre(genre): Lists all books in the specified genre, sorted by year of publication. records = read_library_database() filtered_records = [f\\"{rec[\'title\']} by {rec[\'author\']} ({rec[\'year\']})\\" for rec in records if rec[\'genre\'].lower() == genre.lower()] filtered_records.sort(key=lambda x: int(x.split(\'(\')[-1].strip(\')\'))) return filtered_records def search_book_by_title(title_keyword): Searches for books by title keyword (case insensitive) and returns all matching records. records = read_library_database() matching_records = [f\\"{rec[\'title\']} by {rec[\'author\']} ({rec[\'year\']})\\" for rec in records if title_keyword.lower() in rec[\'title\'].lower()] return matching_records def add_new_record(book_title, author, year, genre, shelf): Adds a new record to the library and updates the text file. try: # Validating inputs if not all([book_title, author, genre, shelf]): raise ValueError(\\"All fields except \'year\' must be non-empty strings.\\") if not isinstance(year, int) or year <= 0: raise ValueError(\\"Year must be a positive integer.\\") # Appending the new record to the file with open(\'library_database.txt\', \'a\') as file: file.write(f\\"n{book_title};{author};{year};{genre};{shelf}\\") return True except Exception as e: print(f\\"Error adding new record: {e}\\") return False"},{"question":"**Question: Implement Custom Import Mechanism** Implement a custom Python function to import a module using low-level mechanisms inspired by the functionality described in the documentation. The custom function should cover: 1. **Module Importing**: Simulating the behavior of `PyImport_ImportModule` to load modules by name. 2. **Submodule Handling**: Ensuring submodules can be imported correctly. 3. **Module Reloading**: Providing functionality similar to `PyImport_ReloadModule` to refresh existing modules. # Function Signature: ```python def custom_import(module_name: str) -> None: Custom import function that imports a module and its submodules. Parameters: module_name (str): Module name to be imported. Returns: None pass def custom_reload(module_name: str) -> None: Custom reload function that reloads an existing module. Parameters: module_name (str): Module name to be reloaded. Returns: None pass ``` # Constraints: 1. Do not use the in-built `importlib` or the `import` statement directly. 2. Handle exceptions gracefully and provide informative error messages. 3. Ensure that the function can dynamically handle both absolute module names (like `os`) and submodule names (like `os.path`). # Example: 1. Importing a module: ```python custom_import(\'math\') ``` 2. Importing a submodule: ```python custom_import(\'os.path\') ``` 3. Reloading a module: ```python custom_reload(\'math\') ``` Implement these functions by mimicking the behavior and constraints specified in the provided documentation. Ensure to write comprehensive test cases to demonstrate that your implementation works as expected.","solution":"import sys import types def custom_import(module_name: str) -> None: Custom import function that imports a module and its submodules. Parameters: module_name (str): Module name to be imported. Returns: None parts = module_name.split(\'.\') if not parts: raise ValueError(\\"Empty module name provided\\") current_module = None base_module = None try: for i in range(len(parts)): sub_name = \'.\'.join(parts[:i+1]) if sub_name in sys.modules: current_module = sys.modules[sub_name] else: current_module = __import__(sub_name) sys.modules[sub_name] = current_module if i == 0: base_module = current_module return base_module except ImportError as e: raise ImportError(f\\"Failed to import module {module_name}\\") from e def custom_reload(module_name: str) -> None: Custom reload function that reloads an existing module. Parameters: module_name (str): Module name to be reloaded. Returns: None if module_name not in sys.modules: raise ImportError(f\\"Module {module_name} is not loaded, so it cannot be reloaded\\") try: module = sys.modules[module_name] parent_module = module if \'.\' in module_name: parent_module_name, child_name = module_name.rsplit(\'.\', 1) parent_module = sys.modules[parent_module_name] module = getattr(parent_module, child_name) reloaded_module = __import__(module_name) if hasattr(module, \'__spec__\') and hasattr(module.__spec__, \'_initializing\'): module.__spec__._initializing = False sys.modules[module_name] = reloaded_module except ImportError as e: raise ImportError(f\\"Failed to reload module {module_name}\\") from e"},{"question":"You are tasked with writing a Python function that calculates the difference between two date strings and returns the result in a human-readable format, specifically showing the difference in years, months, and days. # Requirements: 1. Implement a function `calculate_date_difference(date1: str, date2: str) -> str` where: - `date1` and `date2` are strings in the format `YYYY-MM-DD`. - The function should parse these strings into `datetime.date` objects and calculate the difference between them. - The result should be returned as a string in the format \\"X years, Y months, Z days\\". 2. Your solution should handle cases where `date1` is earlier than `date2` and vice versa. The calculated difference should always be positive. 3. Assume that all dates are valid and fall within the range [datetime.MINYEAR, datetime.MAXYEAR]. # Constraints: - You are not allowed to use any external libraries; only the built-in `datetime` module is permitted. - You must handle leap years correctly. - Focus on clear and efficient implementation. # Function Signature: ```python def calculate_date_difference(date1: str, date2: str) -> str: # Your code here ``` # Example: ```python assert calculate_date_difference(\\"2020-01-15\\", \\"2022-04-18\\") == \\"2 years, 3 months, 3 days\\" assert calculate_date_difference(\\"2022-04-18\\", \\"2020-01-15\\") == \\"2 years, 3 months, 3 days\\" assert calculate_date_difference(\\"2000-02-29\\", \\"2004-02-29\\") == \\"4 years, 0 months, 0 days\\" ``` # Note: - Take note of the rules for leap years and different month lengths. - Handle negative and positive differences correctly. Provide your implementation in the following cell.","solution":"from datetime import datetime, date def calculate_date_difference(date1: str, date2: str) -> str: def months_between_dates(d1: date, d2: date): return (d2.year - d1.year) * 12 + d2.month - d1.month # Parse input date strings into date objects d1 = datetime.strptime(date1, \\"%Y-%m-%d\\").date() d2 = datetime.strptime(date2, \\"%Y-%m-%d\\").date() # Ensure that d1 is the earlier date if d1 > d2: d1, d2 = d2, d1 # Calculate the difference in full years and months between dates total_months = months_between_dates(d1, d2) full_years = total_months // 12 remaining_months = total_months % 12 # Calculate the difference in days day_diff = (d2 - d1.replace(year=d2.year, month=d2.month)).days if day_diff < 0: remaining_months -= 1 day_diff += (d2 - d2.replace(month=d2.month-1 if d2.month > 1 else 12, year=d2.year if d2.month > 1 else d2.year-1)).days return f\\"{full_years} years, {remaining_months} months, {day_diff} days\\""},{"question":"# Context Management with ContextVars **Objective:** Write a Python program to demonstrate the usage of `contextvars` for managing state and context within concurrent code. # Problem Statement: You are required to implement a function `manage_context` that creates a new context variable, sets its value, retrieves the value, and finally resets it back to the original state. The function should simulate a series of operations where contexts might be altered and need to be restored to their initial state. # Function Signature: ```python def manage_context() -> dict: pass ``` # Expected Operations: 1. Create a new context variable with a default value of `None`. 2. Set the context variable to a value. 3. Retrieve the current value of the context variable. 4. Set a new value to the context variable. 5. Retrieve the new value of the context variable. 6. Reset the context variable to its previous value. 7. Retrieve the value of the context variable after reset. # Return: - A dictionary containing: - \'initial_value\': The initial set value. - \'after_first_set\': The value after the first set. - \'after_second_set\': The value after changing the value. - \'after_reset\': The value after resetting the variable. # Example: ```python context_dict = manage_context() # Example return value of context_dict: # { # \'initial_value\': \'First Value\', # \'after_first_set\': \'First Value\', # \'after_second_set\': \'Second Value\', # \'after_reset\': \'First Value\' # } ``` # Constraints: - Use the `contextvars` module to create and manage context variables. - Simulate these operations within a single thread. - Ensure proper handling of context states with appropriate resets. # Notes: - This problem assesses the ability to manage context-specific state and handle context restoration. - It emphasizes understanding the context management functionalities provided by Python\'s `contextvars` module. # References: Refer to Python\'s official documentation for more details on the `contextvars` module. - [contextvars Documentation](https://docs.python.org/3/library/contextvars.html)","solution":"import contextvars def manage_context() -> dict: # Creating a new context variable with a default value of None ctx_var = contextvars.ContextVar(\'ctx_var\', default=None) # Set the context variable to an initial value token1 = ctx_var.set(\'First Value\') initial_value = ctx_var.get() # Change the value of the context variable token2 = ctx_var.set(\'Second Value\') after_second_set = ctx_var.get() # Reset to the previous value ctx_var.reset(token2) after_reset = ctx_var.get() return { \'initial_value\': initial_value, \'after_first_set\': initial_value, # \'initial_value\' after first set \'after_second_set\': after_second_set, \'after_reset\': after_reset }"},{"question":"In this task, you will demonstrate your understanding of seaborn\'s object-oriented interface to generate visualizations for the `diamonds` dataset. Your goal is to create a histogram that effectively conveys the distribution of a given variable, while implementing advanced customization and handling potential visual overlaps. Requirements 1. **Create a histogram** of the `carat` variable from the `diamonds` dataset, using a logarithmic scale on the x-axis. 2. **Apply a color scheme** to differentiate between the different types of `cut`. Ensure that the bars do not overlap by using an appropriate transformation. 3. **Customize the appearance** by setting edge properties, transparency, and adjust bar widths to improve clarity. Detailed Instructions 1. Load the `diamonds` dataset using `seaborn.load_dataset(\\"diamonds\\")`. 2. Use the `so.Plot` class to initiate the plot with the `carat` variable. 3. Apply a logarithmic scale to the x-axis using `.scale(x=\\"log\\")`. 4. Add a histogram with bars representing the distribution of `carat` values, differentiated by the `cut` variable. Use the appropriate transformation to resolve overlapping. 5. Set the bar edges to have a width of 0 and adjust transparency based on the `clarity` variable. 6. Ensure the bars are unfilled but still visible by setting edge color and width. Expected Output - A seaborn plot showing a customized histogram based on the instructions above. - Properly colored and separated bars avoiding visual overlap. - Proper application of logarithmic scaling for the x-axis. Constraints - **Input**: You are only allowed to use seaborn\'s object-oriented interface (`seaborn.objects`) for creating the plot. - **Output**: Display the generated plot. ```python # Import the necessary libraries and load the diamonds dataset import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the Plot object p = so.Plot(diamonds, \\"carat\\").scale(x=\\"log\\") # Add customized bars to the plot p.add(so.Bars(edgewidth=0), so.Hist(), so.Dodge(), color=\\"cut\\", alpha=\\"clarity\\") # Display the plot p.show() ``` Ensure the code correctly implements the requirements and produces a clear and insightful visual representation of the `diamonds` dataset.","solution":"# Import the necessary libraries and load the diamonds dataset import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the Plot object p = so.Plot(diamonds, \\"carat\\").scale(x=\\"log\\") # Add customized bars to the plot p.add(so.Bars(edgewidth=0, edgecolor=\\"black\\", alpha=0.6, width=0.7), so.Hist(), so.Dodge(), color=\\"cut\\") # Display the plot p.show()"},{"question":"Objective: Demonstrate your ability to leverage the `lzma` module in Python for compressing and decompressing data, handling both files and in-memory data with custom filter chains. Problem Statement: You are given a plaintext file named `input.txt` that contains a large amount of text data. Your task is to compress this file using the LZMA algorithm with specific filter configurations and then decompress the resulting file back to its original content. Additionally, you need to compress a byte string in memory and decompress it, ensuring the functionality works correctly. Requirements: 1. **File Compression and Decompression**: - Read the data from `input.txt`. - Compress the data using the LZMA algorithm with a custom filter chain: - Delta filter with a distance of 4. - LZMA2 filter with a preset level of 9. - Write the compressed data to `compressed_file.xz`. - Read the compressed data from `compressed_file.xz`. - Decompress the data back to its original form and write it to a file named `output.txt`. 2. **In-Memory Data Compression and Decompression**: - Compress a given byte string `data_in` using the LZMA algorithm with the default settings. - Decompress the resulting data, ensuring it matches the original `data_in`. 3. **Error Handling**: - Properly handle any `lzma.LZMAError` exceptions that may arise during the compression and decompression processes. Input and Output Formats: - **Input**: - A plaintext file named `input.txt`. - A byte string `data_in` containing arbitrary data. - **Output**: - A compressed file named `compressed_file.xz`. - A decompressed file named `output.txt` that should match `input.txt`. - In-memory decompressed data that should match the original `data_in`. Constraints: - You must use the `lzma` module. - Ensure that the compressed data size is less than or equal to 50% of the original data size for the file `input.txt`. - Handle all edge cases, including empty input data and large files. Function Signature: ```python def compress_and_decompress_files(input_filepath: str, compressed_filepath: str, output_filepath: str) -> None: pass def compress_and_decompress_memory(data_in: bytes) -> bytes: pass ``` Example Usage: ```python data_in = b\\"Example data to be compressed and decompressed\\" compress_and_decompress_files(\'input.txt\', \'compressed_file.xz\', \'output.txt\') decompressed_data = compress_and_decompress_memory(data_in) assert decompressed_data == data_in ``` Implement these functionalities to prove your understanding and capability in handling data compression and decompression using the `lzma` module in Python.","solution":"import lzma def compress_and_decompress_files(input_filepath: str, compressed_filepath: str, output_filepath: str) -> None: # Define the filters for compression filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9} ] # Read the input data from the file with open(input_filepath, \'rb\') as f: input_data = f.read() # Compress the data with lzma.open(compressed_filepath, \'wb\', filters=filters) as f: f.write(input_data) # Decompress the data with lzma.open(compressed_filepath, \'rb\') as f: decompressed_data = f.read() # Write the decompressed data to the output file with open(output_filepath, \'wb\') as f: f.write(decompressed_data) def compress_and_decompress_memory(data_in: bytes) -> bytes: # Compress the in-memory data compressed_data = lzma.compress(data_in) # Decompress the in-memory data try: decompressed_data = lzma.decompress(compressed_data) except lzma.LZMAError as e: raise RuntimeError(f\\"Decompression failed: {e}\\") return decompressed_data"},{"question":"**Seaborn `regplot` Assessment** # Objective In this task, you will demonstrate your understanding of the Seaborn library\'s `regplot` function by implementing a function to create different types of regression plots with a variety of customizations. # Problem Statement You are given a dataset containing records of various attributes of cars, including \'weight\', \'horsepower\', \'mpg\', \'cylinders\', and \'origin\'. Your task is to implement a function `create_reg_plots` that generates and saves four distinct regression plots demonstrating different functionalities and customization options of Seaborn\'s `regplot`. # Function Signature ```python def create_reg_plots(data: pd.DataFrame) -> None: pass ``` # Input - `data`: A pandas DataFrame with the following columns: - \'weight\': Numerical values representing the weight of the car. - \'horsepower\': Numerical values representing the horsepower of the car. - \'mpg\': Numerical values representing the miles per gallon of the car. - \'cylinders\': Numerical values representing the number of cylinders in the car. - \'origin\': Categorical values representing the origin of the car, one of \\"usa\\", \\"europe\\", or \\"japan\\". # Output The function should output four saved plots with the following specifications: 1. **Polynomial Regression Plot**: - Plot the relationship between \'weight\' and \'mpg\'. - Fit a second-order polynomial regression line. - Save the plot as `polynomial_regression.png`. 2. **Log-Linear Regression Plot**: - Plot the relationship between \'displacement\' and \'mpg\' (assuming \'displacement\' is a column in the provided dataset). - Fit a log-linear regression line (log-transform the x-axis). - Save the plot as `log_linear_regression.png`. 3. **LOWESS Smoother Plot**: - Plot the relationship between \'horsepower\' and \'mpg\'. - Use a locally-weighted (LOWESS) smoother. - Save the plot as `lowess_smoother.png`. 4. **Customized Scatter Plot with Regression**: - Plot the relationship between \'weight\' and \'horsepower\'. - Customize the scatter plot with: - A blue cross (\'x\') as the marker. - Red regression line. - Confidence interval of 99%. - Save the plot as `customized_regression.png`. # Constraints - You may assume the input DataFrame will always contain the specified required columns. - You need to ensure the plots are saved with the correct filenames. # Example Usage ```python import pandas as pd # Example DataFrame data = pd.DataFrame({ \'weight\': [3000, 3200, 2800, 3600, 2100], \'horsepower\': [120, 150, 130, 160, 110], \'mpg\': [35, 30, 40, 25, 45], \'cylinders\': [4, 6, 4, 8, 4], \'origin\': [\'usa\', \'japan\', \'europe\', \'japan\', \'usa\'] }) create_reg_plots(data) ``` - This will generate and save four plots named `polynomial_regression.png`, `log_linear_regression.png`, `lowess_smoother.png`, and `customized_regression.png`. # Notes - Make sure to handle any necessary imports within your function or script. - Add your own checks and validations as needed.","solution":"import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt def create_reg_plots(data: pd.DataFrame) -> None: # Polynomial Regression Plot plt.figure(figsize=(10, 6)) sns.regplot(x=\'weight\', y=\'mpg\', data=data, order=2) plt.title(\'Polynomial Regression: Weight vs MPG\') plt.savefig(\'polynomial_regression.png\') plt.close() # Log-Linear Regression Plot plt.figure(figsize=(10, 6)) sns.regplot(x=np.log(data[\'displacement\']), y=data[\'mpg\']) plt.title(\'Log-Linear Regression: Log(Displacement) vs MPG\') plt.savefig(\'log_linear_regression.png\') plt.close() # LOWESS Smoother Plot plt.figure(figsize=(10, 6)) sns.regplot(x=\'horsepower\', y=\'mpg\', data=data, lowess=True) plt.title(\'LOWESS Smoother: Horsepower vs MPG\') plt.savefig(\'lowess_smoother.png\') plt.close() # Customized Scatter Plot with Regression plt.figure(figsize=(10, 6)) sns.regplot(x=\'weight\', y=\'horsepower\', data=data, marker=\'x\', color=\'blue\', line_kws={\'color\': \'red\'}, ci=99) plt.title(\'Customized Regression: Weight vs Horsepower\') plt.savefig(\'customized_regression.png\') plt.close()"},{"question":"# Question: Implementing a Custom GenericAlias Context In Python, advanced type hinting mechanisms allow for the creation of generic types which can be parameterized. The `GenericAlias` is an example of such a type introduced in Python 3.9. You are tasked with implementing a simplified version of this feature using fundamental Python concepts. Objective Create a Python class `CustomGenericAlias` that emulates some of the behaviors described in the documentation for the `GenericAlias` type. Specifically, your class should: 1. Store the `origin` and `args` types. 2. Allow retrieval of these types. 3. Provide a string representation that emulates the `GenericAlias` format. Requirements 1. **Class Definition**: Define a class `CustomGenericAlias` with the following constructor signature: ```python class CustomGenericAlias: def __init__(self, origin: type, args: tuple): pass ``` 2. **Attributes**: - `origin`: should be stored as the initial type. - `args`: should be stored as a tuple of types. If a single type is passed, automatically convert it into a tuple with one element. 3. **Methods**: - `__repr__(self) -> str`: Return a string representation of the generic alias in the format: `\\"{origin}[{args}]\\"`, where `origin` is the name of the origin type, and `args` are the names of the types joined by commas. Example Usage The following example demonstrates the desired behavior of the `CustomGenericAlias` class: ```python # Example types for usage from typing import List, Dict # Creating instances of CustomGenericAlias example1 = CustomGenericAlias(list, (int,)) example2 = CustomGenericAlias(dict, (str, int)) # Accessing attributes print(example1.origin) # Output: <class \'list\'> print(example1.args) # Output: (<class \'int\'>,) # String representation print(repr(example1)) # Output: \\"list[int]\\" print(repr(example2)) # Output: \\"dict[str, int]\\" ``` Constraints - Types passed to `args` can be any built-in type or user-defined classes. - Your implementation should handle edge cases like empty tuples. - The solution should leverage fundamental Python constructs and avoid using advanced metaclass manipulations. Performance Requirements - Your implementation should be efficient in storing and retrieving type information. Implement the `CustomGenericAlias` class in the following cell: ```python class CustomGenericAlias: def __init__(self, origin: type, args: tuple): # Your implementation here pass def __repr__(self) -> str: # Your implementation here pass # Example test cases: example1 = CustomGenericAlias(list, (int,)) example2 = CustomGenericAlias(dict, (str, int)) print(example1.origin) # Output: <class \'list\'> print(example1.args) # Output: (<class \'int\'>,) print(repr(example1)) # Output: \\"list[int]\\" print(repr(example2)) # Output: \\"dict[str, int]\\" ``` Ensure your implementation passes the provided example test cases. You can add additional test cases to verify the correctness of your implementation.","solution":"class CustomGenericAlias: def __init__(self, origin: type, args: tuple): self.origin = origin if not isinstance(args, tuple): self.args = (args,) else: self.args = args def __repr__(self) -> str: args_str = \\", \\".join(arg.__name__ for arg in self.args) return f\\"{self.origin.__name__}[{args_str}]\\""},{"question":"Objective The goal of this assessment is to test your understanding of model evaluation using validation curves and learning curves, as well as your ability to implement and interpret these curves to diagnose bias and variance issues. Problem Statement You are provided a dataset and need to perform the following tasks: 1. Load and preprocess the dataset. 2. Implement and plot a validation curve for a given estimator and hyperparameter. 3. Implement and plot a learning curve for the same estimator. 4. Analyze and interpret the plots to determine if the model suffers from high bias, high variance, or neither. Dataset Use the Iris dataset provided by `sklearn.datasets`. Tasks 1. **Load the dataset**: - Use `load_iris` from `sklearn.datasets` to load the Iris dataset. 2. **Shuffle the dataset**: - Shuffle the dataset using `shuffle` from `sklearn.utils` with `random_state=0`. 3. **Validation Curve**: - Implement the `validation_curve` function to evaluate the effect of the `C` hyperparameter for an SVM classifier with a linear kernel. - Use `param_range=np.logspace(-3, 3, 7)` for the values of `C`. - Split the data into training and validation sets using 5-fold cross-validation. - Plot the validation curve using `ValidationCurveDisplay.from_estimator`. - Save and display the plot. 4. **Learning Curve**: - Implement the `learning_curve` function to evaluate the effect of varying training sample sizes for an SVM classifier with a linear kernel. - Use `train_sizes=np.linspace(0.1, 1.0, 10)` for the training sizes. - Split the data into training and validation sets using 5-fold cross-validation. - Plot the learning curve using `LearningCurveDisplay.from_estimator`. - Save and display the plot. 5. **Analysis**: - Based on the validation and learning curves, analyze and interpret if the model suffers from high bias, high variance, or neither. Input - No input arguments are required as the Iris dataset is used directly. Output - Two plots: Validation Curve and Learning Curve. - A text output indicating whether the model suffers from high bias, high variance, or neither, based on your analysis of the curves. Example of Expected Code Structure ```python import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle from sklearn.model_selection import validation_curve, learning_curve from sklearn.model_selection import ValidationCurveDisplay, LearningCurveDisplay import matplotlib.pyplot as plt # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Validation Curve param_range = np.logspace(-3, 3, 7) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) ValidationCurveDisplay.from_estimator( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range ) plt.title(\\"Validation Curve with SVM\\") plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Score\\") plt.show() # Learning Curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5 ) LearningCurveDisplay.from_estimator( SVC(kernel=\\"linear\\"), X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5 ) plt.title(\\"Learning Curve with SVM\\") plt.xlabel(\\"Training Examples\\") plt.ylabel(\\"Score\\") plt.show() # Analysis # ... (explain whether the model suffers from high bias, high variance, or neither) ``` Constraints - Ensure you use only the techniques and libraries mentioned in the provided code structure. - Set `random_state` to 0 where applicable to ensure reproducibility. Performance Requirements - The solution should efficiently plot the validation and learning curves and provide a clear analysis.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle from sklearn.model_selection import validation_curve, learning_curve from sklearn.model_selection import ValidationCurveDisplay, LearningCurveDisplay import matplotlib.pyplot as plt # Load and shuffle dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Validation Curve param_range = np.logspace(-3, 3, 7) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) # Plot the validation curve plt.figure() ValidationCurveDisplay.from_estimator( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) plt.title(\\"Validation Curve with SVM\\") plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Score\\") plt.show() # Learning Curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5 ) # Plot the learning curve plt.figure() LearningCurveDisplay.from_estimator( SVC(kernel=\\"linear\\"), X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5 ) plt.title(\\"Learning Curve with SVM\\") plt.xlabel(\\"Training Examples\\") plt.ylabel(\\"Score\\") plt.show() # Analysis analysis = From the validation curve: - If the training score is much higher than the validation score, it indicates high variance. - If both scores are low, it indicates high bias. - If both scores are high and close to each other, the model is well-generalized. From the learning curve: - If the training score decreases and the validation score increases with more training data, it suggests a reduction in variance. - If both scores converge to a low value, it points to high bias. - If both scores are high and close to each other, it shows neither high bias nor high variance. Based on these points, we need to analyze the exact numbers from the curves to draw specific conclusions. print(analysis)"},{"question":"Problem Statement: You are tasked with creating a feature-rich task management tool that helps users to manage tasks with deadlines. The tool should be able to handle tasks with different priorities and travel needs across different time zones. # Requirements: 1. **Task Representation**: Each task should contain the following attributes: - `description`: A string that describes the task. - `deadline`: A `datetime` object representing the deadline of the task. - `priority`: An integer indicating the priority of the task. (Lower numbers indicate higher priority). - `location`: A string representing the IANA time zone location (e.g., \'America/New_York\'). 2. **Task Management**: - The tool should support adding tasks. - The tool should support removing tasks by description. - The tool should support sorting and retrieving tasks by their deadlines or priority. 3. **Time-zone Handling**: - The deadlines should be converted and stored in UTC. - When displaying deadlines, they should be converted back to the local time zone of the task\'s location. # Input/Output Format: - Functions should be core components of the task management tool. - You are required to implement three main methods for this task management tool. # Constraints: - Ensure proper handling of edge cases such as invalid dates, priorities and time zones. - The operations should be efficient in terms of time complexity. # Performance: - Tasks should be managed efficiently without unnecessary time complexity, focusing particularly on the sorting of deadlines and priorities. # Example Usage: ```python from datetime import datetime from zoneinfo import ZoneInfo class TaskManager: def __init__(self): # initialize an empty list to store tasks self.tasks = [] def add_task(self, description: str, deadline: datetime, priority: int, location: str): Add a task to the task manager, ensuring the deadline is converted to UTC. # implementation here def remove_task(self, description: str): Remove a task by its description. # implementation here def get_sorted_tasks(self, by: str = \'deadline\'): Retrieve tasks sorted by deadline (default) or priority. # implementation here # Example usage: task_manager = TaskManager() task_manager.add_task(\\"Complete assignment\\", datetime(2023, 10, 5, 12, 0, tzinfo=ZoneInfo(\\"America/New_York\\")), 1, \\"America/New_York\\") task_manager.add_task(\\"Meeting with team\\", datetime(2023, 10, 4, 15, 30, tzinfo=ZoneInfo(\\"Europe/London\\")), 2, \\"Europe/London\\") print(task_manager.get_sorted_tasks(by=\'priority\')) # Expected output should list the tasks sorted by their priority. ``` You are required to implement the class and methods described above with the given features. # Notes: - You may use the `datetime` and `zoneinfo` modules outlined in the documentation. - Handle edge cases such as invalid data input gracefully, providing meaningful error messages where necessary.","solution":"from datetime import datetime from zoneinfo import ZoneInfo class TaskManager: def __init__(self): self.tasks = [] def add_task(self, description: str, deadline: datetime, priority: int, location: str): deadline_utc = deadline.astimezone(ZoneInfo(\\"UTC\\")) task = { \'description\': description, \'deadline\': deadline_utc, \'priority\': priority, \'location\': location } self.tasks.append(task) def remove_task(self, description: str): self.tasks = [task for task in self.tasks if task[\'description\'] != description] def get_sorted_tasks(self, by: str = \'deadline\'): if by == \'deadline\': return sorted(self.tasks, key=lambda x: x[\'deadline\']) elif by == \'priority\': return sorted(self.tasks, key=lambda x: x[\'priority\']) else: raise ValueError(\\"Sort key must be either \'deadline\' or \'priority\'\\") def display_task(self, task): local_time = task[\'deadline\'].astimezone(ZoneInfo(task[\'location\'])) return { \'description\': task[\'description\'], \'deadline\': local_time, \'priority\': task[\'priority\'], \'location\': task[\'location\'] }"},{"question":"# POP3 Mailbox Management You are tasked with implementing a Python class that uses the `poplib` module to interact with a POP3 mail server and perform mailbox management. The class should provide methods to connect to the server, authenticate, retrieve emails, delete emails, and disconnect from the server. The class should be named `MailboxManager` and have the following methods: 1. `__init__(self, host: str, port: int = 110, use_ssl: bool = False)`: Initializes the mailbox manager. If `use_ssl` is `True`, a secure connection using `POP3_SSL` should be established. Otherwise, an unsecured `POP3` connection should be used. 2. `connect(self, username: str, password: str)`: Connects to the POP3 server and authenticates using the provided username and password. 3. `get_email_count(self) -> int`: Returns the number of emails in the mailbox. 4. `retrieve_email(self, index: int) -> str`: Retrieves the email at the specified index. The email should be returned as a string. 5. `delete_email(self, index: int)`: Flags the email at the specified index for deletion. 6. `disconnect(self)`: Disconnects from the POP3 server, committing any changes (such as deletions). # Constraints - The `__init__` method should ensure that a connection is created when the instance is initialized. - The `connect` method should raise an appropriate exception if the connection or authentication fails. - Methods involving index should handle valid index ranges (e.g., `index` should be a positive integer less than or equal to the number of emails). # Example Usage ```python # Sample usage: manager = MailboxManager(\'pop.example.com\', 995, use_ssl=True) manager.connect(\'username\', \'password\') print(\\"Number of emails:\\", manager.get_email_count()) print(\\"First email content:\\", manager.retrieve_email(1)) manager.delete_email(1) manager.disconnect() ``` # Performance Requirements - The solution should efficiently handle mailboxes with a large number of emails, ensuring that operations like retrieving and deleting emails respond promptly. Implement the `MailboxManager` class to meet the above requirements.","solution":"import poplib from email.parser import BytesParser class MailboxManager: def __init__(self, host: str, port: int = 110, use_ssl: bool = False): self.host = host self.port = port self.use_ssl = use_ssl self.connection = None self.is_connected = False def connect(self, username: str, password: str): try: if self.use_ssl: self.connection = poplib.POP3_SSL(self.host, self.port) else: self.connection = poplib.POP3(self.host, self.port) self.connection.user(username) self.connection.pass_(password) self.is_connected = True except Exception as e: raise ConnectionError(\\"Failed to connect or authenticate with the POP3 server\\") from e def get_email_count(self) -> int: if not self.is_connected: raise ConnectionError(\\"Not connected to the server\\") return len(self.connection.list()[1]) def retrieve_email(self, index: int) -> str: if not self.is_connected: raise ConnectionError(\\"Not connected to the server\\") if index < 1 or index > self.get_email_count(): raise IndexError(\\"Email index out of range\\") response, lines, octets = self.connection.retr(index) msg_content = b\'n\'.join(lines) msg = BytesParser().parsebytes(msg_content) return msg.as_string() def delete_email(self, index: int): if not self.is_connected: raise ConnectionError(\\"Not connected to the server\\") if index < 1 or index > self.get_email_count(): raise IndexError(\\"Email index out of range\\") self.connection.dele(index) def disconnect(self): if self.is_connected: self.connection.quit() self.is_connected = False"},{"question":"**Objective:** Implement a function that demonstrates your understanding of the `asyncio.Future` class and related concepts. **Task:** Write an asynchronous function named `fetch_data_with_timeout` that accepts two parameters: 1. `fetch_data` - an asynchronous function that fetches data. 2. `timeout` - an integer representing the maximum time in seconds to wait for the `fetch_data` function to complete. Your function should return the result of the `fetch_data` function if it completes within the given timeout. Otherwise, it should raise a `TimeoutError` exception. The function should handle the following cases: - If `fetch_data` raises an exception, your function should propagate that exception. - Use `asyncio.Future` objects to manage the asynchronous operation and implement timeout handling. **Input:** - `fetch_data`: Callable[[], Awaitable[Any]] - An asynchronous function that gets some data. - `timeout`: int - Maximum time in seconds to wait for the data fetch operation. **Output:** - Any - The result of the `fetch_data` function if it completes within the timeout period. **Constraints:** - You cannot change the interface of the `fetch_data` function. - Your solution must use `asyncio.Future` to manage the asynchronous operation. **Example:** ```python import asyncio import random async def example_fetch(): await asyncio.sleep(random.uniform(0.5, 2.5)) # Simulate a delay in fetching data return \\"data received\\" async def main(): try: result = await fetch_data_with_timeout(example_fetch, 2) print(result) except TimeoutError: print(\\"Operation timed out\\") asyncio.run(main()) ``` In this example, the `example_fetch` function may complete successfully within 2 seconds, or it may raise a `TimeoutError` if it takes longer. **Hints:** - Utilize asyncio features such as `loop.create_future()` and `future.set_result()`. - Consider using `asyncio.wait_for` or similar timeout-handling mechanisms in combination with `Future` objects. You need to write the implementation for the `fetch_data_with_timeout` function. Ensure your function adheres to the described behavior and handles all specified scenarios appropriately.","solution":"import asyncio async def fetch_data_with_timeout(fetch_data, timeout): Fetch data with a timeout. Raise TimeoutError if fetch_data does not complete within the given timeout. :param fetch_data: Callable[[], Awaitable[Any]] - An asynchronous function that fetches data. :param timeout: int - Maximum time in seconds to wait for the fetch_data function to complete. :return: Any - The result of the fetch_data function if it completes within the timeout period. :raises: TimeoutError - If the fetch_data function does not complete within the timeout period. loop = asyncio.get_event_loop() future = loop.create_future() try: result = await asyncio.wait_for(fetch_data(), timeout) future.set_result(result) except asyncio.TimeoutError: future.set_exception(TimeoutError) raise TimeoutError(\\"Operation timed out\\") except Exception as e: future.set_exception(e) raise e return await future"},{"question":"Objective: To test your understanding of system functions and process control using the python310 utility package, you will write a function that interacts with the operating system to execute and manage system commands. Problem Statement: Implement a Python function `execute_command(cmd: str) -> dict` that takes a single string argument `cmd` representing a system command. The function should execute this command in a subprocess, wait for it to complete, and return a dictionary containing the following information: - `output`: A string containing the standard output of the command. - `error`: A string containing the standard error output of the command. - `exit_code`: An integer representing the command\'s exit status code. Function Signature: ```python def execute_command(cmd: str) -> dict: pass ``` Input: - `cmd`: A string (1 â‰¤ len(cmd) â‰¤ 1024) containing the system command to be executed. Output: - A dictionary with the keys: - `output`: A string containing the standard output of the command (can be empty). - `error`: A string containing the standard error output of the command (can be empty). - `exit_code`: An integer representing the command\'s exit status code. Constraints: - You must handle commands that may produce both standard output and standard error. - The function should be able to execute any valid command available on the system. Example: ```python result = execute_command(\\"echo Hello World!\\") print(result) # Expected Output: # { # \'output\': \'Hello World!n\', # \'error\': \'\', # \'exit_code\': 0 # } result = execute_command(\\"cat non_existing_file.txt\\") print(result) # Expected Output: # { # \'output\': \'\', # \'error\': \'cat: non_existing_file.txt: No such file or directoryn\', # \'exit_code\': 1 # } ``` Notes: - Use the `subprocess` module to execute the command and capture the output and error streams. - Ensure that your function waits for the command to complete and correctly captures its exit status.","solution":"import subprocess def execute_command(cmd: str) -> dict: Executes a system command and captures the output, error, and exit code. :param cmd: A string representing the system command to execute. :return: A dictionary with \'output\', \'error\', and \'exit_code\' keys. result = subprocess.run(cmd, shell=True, text=True, capture_output=True) return { \'output\': result.stdout, \'error\': result.stderr, \'exit_code\': result.returncode }"},{"question":"# Advanced Python Memory Management Background: In Python, memory management is generally handled automatically through the garbage collector. However, understanding how objects are managed at a lower level can offer significant insights into optimizing Python programs and dealing with performance-critical applications. This problem explores the concept of memory management by simulating some of these low-level operations in pure Python. Task: You are required to create a Python class `PyObjectManager` that simulates a simplified version of the object management mechanism detailed in the documentation above. The class should manage the creation, initialization, and deletion of custom objects. Implement the following methods within the `PyObjectManager` class: 1. `new_object(type_name: str) -> dict`: Simulates the allocation of a new object with only type information. 2. `init_object(obj: dict, fields: dict) -> dict`: Simulates initializing an object with additional fields. 3. `del_object(obj: dict) -> None`: Simulates the deletion of an object. 4. `none_object() -> dict`: Returns a singleton object representing `None`. Ensure that your class respects the following considerations: - An \\"object\\" is represented as a dictionary with at least two keys: `\\"type\\"` and `\\"ref_count\\"`. - The `new_object` method increases the `ref_count` to 1 upon creation. - The `init_object` method adds initial fields to the object while maintaining its `type` and `ref_count`. - The `del_object` method should simulate decreasing the reference count and print a message indicating the object is being deleted if the reference count drops to zero. - The `none_object` method should return the same object (singleton) representing `None` each time it is called. Example: ```python manager = PyObjectManager() # Create a new object obj = manager.new_object(\'CustomType\') print(obj) # Output example: {\'type\': \'CustomType\', \'ref_count\': 1} # Initialize object with fields obj = manager.init_object(obj, {\'field1\': \'value1\'}) print(obj) # Output example: {\'type\': \'CustomType\', \'ref_count\': 1, \'field1\': \'value1\'} # Delete the object manager.del_object(obj) # Prints: Object of type \'CustomType\' is being deleted. # Get the singleton None object none_obj = manager.none_object() print(none_obj) # Output example: {\'type\': \'NoneType\', \'ref_count\': 1} ``` Constraints: - Ensure that memory management operations simulate a realistic low-level environment where possible. - Do not use Python\'s in-built garbage collector methods (`gc` library) directly. - Your solution should be written entirely in Python. Note: This task does not require actual memory management as performed by Python\'s C-API but is intended to provide insight into how such mechanisms work internally.","solution":"class PyObjectManager: def __init__(self): self.none_singleton = {\'type\': \'NoneType\', \'ref_count\': 1} def new_object(self, type_name: str) -> dict: return {\'type\': type_name, \'ref_count\': 1} def init_object(self, obj: dict, fields: dict) -> dict: obj.update(fields) return obj def del_object(self, obj: dict) -> None: obj[\'ref_count\'] -= 1 if obj[\'ref_count\'] == 0: print(f\\"Object of type \'{obj[\'type\']}\' is being deleted.\\") def none_object(self) -> dict: return self.none_singleton"},{"question":"**Objective:** Implement a Python class that can serialize and deserialize its instances using the \\"pickle\\" module. This should include custom handling for pickling and unpickling certain attributes to manage state and external references. **Question:** You are required to implement a class called `SerializableTask` that represents a task with an ID, description, and metadata. The `SerializableTask` class should: 1. Serialize its instances using the `pickle` module. 2. De-serialize its instances ensuring that instances are correctly restored with all their attributes. 3. Implement custom methods to control the serialization of certain attributes. 4. Demonstrate how an external object can be referenced and restored during deserialization. **Specifications:** - Your class should have the following attributes: - `task_id` (int): The unique identifier for the task. - `description` (str): A brief description of the task. - `metadata` (dict): Additional information related to the task. - Implement the `__getstate__()` and `__setstate__()` methods to customize the serialization of the `metadata` attribute. - Use persistent IDs to handle external references for a `TaskDatabase` (a hypothetical database class that handles task records). ```python import pickle class SerializableTask: def __init__(self, task_id, description, metadata=None): self.task_id = task_id self.description = description self.metadata = metadata or {} def __getstate__(self): # Customizing state: the metadata dictionary must be stringified during pickling state = self.__dict__.copy() state[\'metadata\'] = repr(state[\'metadata\']) return state def __setstate__(self, state): # Restoring state: convert the stringified metadata back to a dictionary state[\'metadata\'] = eval(state[\'metadata\']) self.__dict__.update(state) @staticmethod def save_to_file(task, filename): with open(filename, \'wb\') as file: pickle.dump(task, file) @staticmethod def load_from_file(filename): with open(filename, \'rb\') as file: return pickle.load(file) # Hypothetical TaskDatabase class to simulate an external object reference during serialization. class TaskDatabase: def __init__(self, db): self.db = db def get_task_by_id(self, task_id): # Simulating a method that retrieves the task by its id. return self.db.get(task_id, None) def main(): # Create a sample task and serialize it to a file task = SerializableTask(task_id=1, description=\\"Complete the coding assessment\\", metadata={\\"priority\\": \\"high\\", \\"due_date\\": \\"tomorrow\\"}) SerializableTask.save_to_file(task, \'task.pkl\') print(\\"Task saved to file.\\") # Load the task from the file and display its attributes loaded_task = SerializableTask.load_from_file(\'task.pkl\') print(\\"Task loaded from file with ID:\\", loaded_task.task_id) print(\\"Description:\\", loaded_task.description) print(\\"Metadata:\\", loaded_task.metadata) if __name__ == \\"__main__\\": main() ``` **Inputs** - Create an instance of `SerializableTask` with `task_id=1`, `description=\\"Complete the coding assessment\\"`, and `metadata={\\"priority\\": \\"high\\", \\"due_date\\": \\"tomorrow\\"}`. - Save this instance to the file `task.pkl`. **Outputs** - On loading from `task.pkl`, the restored `SerializableTask` instance should have the same attributes as the original one. **Constraints** - Ensure all necessary methods (`__getstate__`, `__setstate__`, `save_to_file`, `load_from_file`) are implemented. - Use proper handling for the `metadata` attribute to demonstrate customized serialization. - Demonstrate reference handling for the hypothetical `TaskDatabase` class.","solution":"import pickle class SerializableTask: def __init__(self, task_id, description, metadata=None): self.task_id = task_id self.description = description self.metadata = metadata or {} def __getstate__(self): # Customizing state: the metadata dictionary must be stringified during pickling state = self.__dict__.copy() state[\'metadata\'] = repr(state[\'metadata\']) return state def __setstate__(self, state): # Restoring state: convert the stringified metadata back to a dictionary state[\'metadata\'] = eval(state[\'metadata\']) self.__dict__.update(state) @staticmethod def save_to_file(task, filename): with open(filename, \'wb\') as file: pickle.dump(task, file) @staticmethod def load_from_file(filename): with open(filename, \'rb\') as file: return pickle.load(file) # Hypothetical TaskDatabase class to simulate an external object reference during serialization. class TaskDatabase: def __init__(self, db): self.db = db def get_task_by_id(self, task_id): # Simulating a method that retrieves the task by its id. return self.db.get(task_id, None)"},{"question":"You are tasked with creating a comprehensive visual analysis of trends in health expenditure over time using the seaborn library. # Requirements: 1. Load the `healthexp` dataset using the `seaborn.load_dataset` function. 2. Manipulate the dataset: - Pivot the dataset such that `Year` becomes the index. - The columns should be `Country` and the values should be `Spending_USD`. - Use `interpolate` to fill any missing values. - Stack the dataset and reset the index to create a tidy dataframe. - Sort the dataframe by `Country`. 3. Create a seaborn plot using `seaborn.objects.Plot` with: - The `healthexp` dataframe. - `Year` as the x-axis and `Spending_USD` as the y-axis. - Facet the plot by `Country`, wrapping the facets to have 3 columns. 4. Add an area plot for each country. 5. Customize the plot: - Map the area plot `color` to `Country`. - Set the `edgewidth` of the area plot to 2. - To enhance visualization, add a line plot on top of the area plots. 6. Additionally, create a stacked area plot to show cumulative spending over time: - Use the same dataset. - Create a plot with `Country` as the categorical variable for the color. - Use stacking in the area plot to visualize part-whole relationships. # Constraints and Limitations: - Assume that there are no missing values after interpolation. - Ensure that the code provided is efficient and runs within a reasonable time frame for large datasets. # Expected Input and Output Input: No direct input from user; use the seaborn `healthexp` dataset. Output: Two plots: 1. A faceted area plot of health expenditure over time by country. 2. A stacked area plot to visualize cumulative health expenditures of all countries over time. # Sample Code The code structure for loading and manipulating the dataset is partially provided. You need to complete the rest following the requirements. ```python import seaborn.objects as so from seaborn import load_dataset # Load and manipulate the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create and customize the faceted area plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\", edgewidth=2).add(so.Line()) # Create and customize the stacked area plot p_stacked = ( so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") .add(so.Area(alpha=.7), so.Stack()) ) # Display the plots p.show() p_stacked.show() ``` Ensure the solution is complete, tested, and meets all the requirements specified.","solution":"import seaborn.objects as so import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_and_prepare_data(): # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Pivot the dataframe healthexp_pivot = healthexp.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") # Interpolate missing values healthexp_interpolated = healthexp_pivot.interpolate() # Stack the dataframe and reset index healthexp_tidy = healthexp_interpolated.stack().rename(\\"Spending_USD\\").reset_index() # Sort the dataframe by Country healthexp_sorted = healthexp_tidy.sort_values(by=\\"Country\\") return healthexp_sorted def create_facet_area_plot(healthexp_sorted): # Create the faceted area plot p = so.Plot(healthexp_sorted, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\", edgewidth=2).add(so.Line()) return p def create_stacked_area_plot(healthexp_sorted): # Create the stacked area plot p_stacked = so.Plot(healthexp_sorted, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\").add(so.Area(alpha=0.7), so.Stack()) return p_stacked # Prepare the dataset healthexp_sorted = load_and_prepare_data() # Create plots facet_plot = create_facet_area_plot(healthexp_sorted) stacked_plot = create_stacked_area_plot(healthexp_sorted) # Display the plots facet_plot.show() stacked_plot.show()"},{"question":"# Asyncio and Threading in Python **Problem Statement:** You are tasked with processing a large dataset that is retrieved from a web service. This dataset is processed using a CPU-intensive operation, after which it needs to be stored in a database. Due to the CPU-intensive nature of the processing step, you are required to use concurrency to maximize performance without blocking the main event loop. Write a Python program using the asyncio module that: 1. Retrieves data asynchronously from a mock web service. 2. Processes the data in a CPU-bound operation using a thread pool executor to avoid blocking the event loop. 3. Store the processed data in a database (you may mock this step). 4. Uses asyncio\'s debug mode to catch common mistakes such as never-awaited coroutines and never-retrieved exceptions. 5. Ensures that all tasks are properly awaited and exceptions are handled. **Requirements:** - The retrieval function should simulate an asynchronous fetch with a delay. - The processing function should simulate a CPU-bound task of significant duration. - The storing function should be asynchronous but can be mocked with a print statement. - Enable asyncio\'s debug mode and log the output. **Input and Output:** - There are no specific inputs and outputs for this function. You are required to write a complete script that simulates the task outlined and prints statements to indicate the status. **Constraints:** - Use the `asyncio` module to handle asynchronous tasks. - Use `concurrent.futures.ThreadPoolExecutor` to handle CPU-bound processing. - Enable and demonstrate the use of asyncio\'s debug mode. - Handle any exceptions that occur and ensure that all coroutines are properly awaited. ```python import asyncio import logging from concurrent.futures import ThreadPoolExecutor import time # Mock function to fetch data asynchronously async def fetch_data(): await asyncio.sleep(2) # Simulate network delay print(\\"Data retrieved\\") return \\"raw_data\\" # CPU-bound function to process data def process_data(data): time.sleep(5) # Simulate CPU-bound task print(\\"Data processed\\") return f\\"processed_{data}\\" # Mock function to store data asynchronously async def store_data(data): await asyncio.sleep(1) # Simulate database operation delay print(f\\"Data stored: {data}\\") async def main(): logging.basicConfig(level=logging.DEBUG) asyncio.get_running_loop().set_debug(True) # Fetch data raw_data = await fetch_data() # Process data using a thread pool executor with ThreadPoolExecutor() as executor: processed_data = await asyncio.get_event_loop().run_in_executor(executor, process_data, raw_data) # Store data await store_data(processed_data) # Running the main function with asyncio if __name__ == \\"__main__\\": asyncio.run(main()) ``` In this task, you will demonstrate your ability to use asyncio for concurrent programming, utilize thread pools for handling CPU-bound tasks, and effectively handle and log exceptions and common mistakes using asyncio\'s debug mode.","solution":"import asyncio import logging from concurrent.futures import ThreadPoolExecutor import time # Mock function to fetch data asynchronously async def fetch_data(): await asyncio.sleep(2) # Simulate network delay print(\\"Data retrieved\\") return \\"raw_data\\" # CPU-bound function to process data def process_data(data): time.sleep(5) # Simulate CPU-bound task print(\\"Data processed\\") return f\\"processed_{data}\\" # Mock function to store data asynchronously async def store_data(data): await asyncio.sleep(1) # Simulate database operation delay print(f\\"Data stored: {data}\\") async def main(): logging.basicConfig(level=logging.DEBUG) asyncio.get_running_loop().set_debug(True) # Fetch data raw_data = await fetch_data() # Process data using a thread pool executor with ThreadPoolExecutor() as executor: processed_data = await asyncio.get_event_loop().run_in_executor(executor, process_data, raw_data) # Store data await store_data(processed_data) # Running the main function with asyncio if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question: Automate \\"pip\\" Installation Checker You are required to design a Python function to automate the process of checking and installing the latest version of \\"pip\\" using the `ensurepip` package. This function should ensure that \\"pip\\" is installed and, if not, install it using the `ensurepip` package. Additionally, the function should support an optional argument to upgrade the existing \\"pip\\" installation if it is outdated. # Function Signature ```python def ensure_latest_pip(upgrade: bool = False) -> None: ... ``` # Input - `upgrade` (bool): A flag indicating whether to upgrade the existing installation of \\"pip\\" if it already exists (default is `False`). # Output - The function does not return anything. It should perform the task of ensuring \\"pip\\" is installed and it is the latest version if the `upgrade` flag is set. # Constraints - The solution must utilize the `ensurepip` package and its functions. - The function should handle and report any exceptional cases using appropriate exception handling. - The function should output relevant messages to the console for: - Whether \\"pip\\" is already installed and its current version. - Whether \\"pip\\" needs to be installed or upgraded. # Example ```python import ensurepip import subprocess def ensure_latest_pip(upgrade: bool = False) -> None: try: # Check the current version of pip if it exists try: current_version = subprocess.check_output( [\\"pip\\", \\"--version\\"], encoding=\\"utf-8\\" ).split()[1] print(f\\"Current pip version: {current_version}\\") except subprocess.CalledProcessError: current_version = None print(\\"pip is not currently installed.\\") available_version = ensurepip.version() print(f\\"Available pip version: {available_version}\\") if current_version is None or upgrade: print(\\"Installing/upgrading pip...\\") ensurepip.bootstrap(upgrade=upgrade) print(\\"pip has been installed/upgraded successfully.\\") else: print(\\"pip is already up to date.\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\") ``` In this problem, you need to: 1. Check if \\"pip\\" is currently installed and its version. 2. Compare it with the available version from `ensurepip`. 3. Install or upgrade \\"pip\\" as required using `ensurepip.bootstrap`.","solution":"import ensurepip import subprocess def ensure_latest_pip(upgrade: bool = False) -> None: try: # Check the current version of pip if it exists try: current_version = subprocess.check_output( [\\"pip\\", \\"--version\\"], encoding=\\"utf-8\\" ).split()[1] print(f\\"Current pip version: {current_version}\\") except subprocess.CalledProcessError: current_version = None print(\\"pip is not currently installed.\\") available_version = ensurepip.version() print(f\\"Available pip version: {available_version}\\") if current_version is None or upgrade: print(\\"Installing/upgrading pip...\\") ensurepip.bootstrap(upgrade=upgrade) print(\\"pip has been installed/upgraded successfully.\\") else: print(\\"pip is already up to date.\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\")"},{"question":"# Multi-Threaded Counter with Safe Increment and Decrement Problem Statement You are tasked with implementing a thread-safe counter using the `_thread` module in Python. The counter should support concurrent increments and decrements, ensuring that the internal value does not get corrupted by race conditions. Requirements 1. The counter must be initialized with a starting value. 2. The counter should support the following methods: - `increment(amount=1)`: Increment the counter by the specified amount. - `decrement(amount=1)`: Decrement the counter by the specified amount. - `value()`: Return the current value of the counter. 3. Implement thread safety using locks from the `_thread` module to ensure that increments and decrements are atomic operations. 4. Demonstrate the use of this counter in a multi-threaded environment by creating multiple threads that concurrently increment and decrement the counter. 5. Handle any potential exceptions gracefully. Input and Output - **Initialize Counter**: `Counter(initial_value)`: Initializes the counter with the starting value `initial_value`. - **Increment Value**: `counter.increment(amount)`: Increments the counter by `amount`. - **Decrement Value**: `counter.decrement(amount)`: Decrements the counter by `amount`. - **Get Value**: `counter.value()`: Returns the current value of the counter. Constraints - The starting value of the counter is a non-negative integer. - The increment and decrement amounts are positive integers. Example Usage ```python from _thread import allocate_lock, start_new_thread class Counter: def __init__(self, initial_value): self._value = initial_value self._lock = allocate_lock() def increment(self, amount=1): with self._lock: self._value += amount def decrement(self, amount=1): with self._lock: self._value -= amount def value(self): with self._lock: return self._value # Example multi-threaded usage def thread_increment(counter, increments): for _ in range(increments): counter.increment() def thread_decrement(counter, decrements): for _ in range(decrements): counter.decrement() counter = Counter(0) thread_count = 10 increments_per_thread = 100 decrements_per_thread = 50 threads = [] # Start increment threads for _ in range(thread_count): t = start_new_thread(thread_increment, (counter, increments_per_thread)) threads.append(t) # Start decrement threads for _ in range(thread_count): t = start_new_thread(thread_decrement, (counter, decrements_per_thread)) threads.append(t) # Wait for threads to complete (not the best practice) import time time.sleep(1) print(\\"Final counter value:\\", counter.value()) ``` The above example initializes the counter to 0, then creates 10 threads to increment it by 100, and 10 threads to decrement it by 50. Finally, it prints the counter\'s final value.","solution":"from _thread import allocate_lock, start_new_thread class Counter: def __init__(self, initial_value): self._value = initial_value self._lock = allocate_lock() def increment(self, amount=1): with self._lock: self._value += amount def decrement(self, amount=1): with self._lock: self._value -= amount def value(self): with self._lock: return self._value"},{"question":"# Question: Implement an Advanced SMTP Client with Error Handling **Objective**: Implement a Python function using the `smtplib` module that sends an email and handles various exceptions, ensuring robust functionality and security. **Function Signature**: ```python def send_advanced_email( smtp_server: str, port: int, use_ssl: bool, username: str, password: str, from_addr: str, to_addrs: list, subject: str, body: str ) -> dict: ``` # Task 1. **Connect and Authenticate**: - Connect to the specified SMTP server. - If `use_ssl` is `True`, use `SMTP_SSL` class to create the connection, otherwise use `SMTP` and optionally upgrade to TLS using `starttls()`. - Authenticate with the provided `username` and `password`. 2. **Prepare the Email**: - Construct the email with the specified `from_addr`, `to_addrs`, `subject`, and `body`. Ensure proper email headers. 3. **Send the Email**: - Send the email using `sendmail()` method. 4. **Error Handling**: - Handle exceptions such as `SMTPAuthenticationError`, `SMTPSenderRefused`, `SMTPRecipientsRefused`, `SMTPDataError`, `SMTPConnectError`, and `SMTPHeloError`. - Return a dictionary with the status of the email sending process: - `status`: \\"success\\" or \\"failure\\" - `message`: Relevant message if there was an error - `failed_recipients`: List of recipient addresses that failed, if any # Input - `smtp_server (str)`: The address of the SMTP server (e.g., \'smtp.gmail.com\'). - `port (int)`: The port number to use for the SMTP connection. - `use_ssl (bool)`: Whether to use SSL directly or upgrade to TLS. - `username (str)`: The username for authenticating with the SMTP server. - `password (str)`: The password for authenticating with the SMTP server. - `from_addr (str)`: The email address of the sender. - `to_addrs (list)`: A list of recipient email addresses. - `subject (str)`: The subject line of the email. - `body (str)`: The body content of the email. # Output - A dictionary containing: - `status (str)`: \\"success\\" if the email was sent successfully, \\"failure\\" otherwise. - `message (str)`: Additional information or error message. - `failed_recipients (list)`: List of recipient addresses that failed (non-empty only in case of partial failure). # Constraints - All email addresses are valid RFC 821 format strings. - `username` and `password` are assumed to be correct and valid. # Performance Requirements - Ensure that the function efficiently handles connections and email sending without unnecessary delays. # Example ```python result = send_advanced_email( smtp_server=\'smtp.example.com\', port=587, use_ssl=False, username=\'user@example.com\', password=\'password\', from_addr=\'user@example.com\', to_addrs=[\'recipient1@example.com\', \'recipient2@example.com\'], subject=\'Test Email\', body=\'This is a test email.\' ) # Example Result # { # \'status\': \'success\', # \'message\': \'Email sent successfully\', # \'failed_recipients\': [] # } ``` # Notes - Use `set_debuglevel(1)` while developing and testing to understand the flow of commands and responses. - Make sure to call `quit()` method to close the connection properly after sending the email.","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def send_advanced_email( smtp_server: str, port: int, use_ssl: bool, username: str, password: str, from_addr: str, to_addrs: list, subject: str, body: str ) -> dict: try: # Connect to the server if use_ssl: server = smtplib.SMTP_SSL(smtp_server, port) else: server = smtplib.SMTP(smtp_server, port) server.starttls() # Upgrade the connection to a secure encrypted SSL/TLS connection server.login(username, password) # Create the email msg = MIMEMultipart() msg[\'From\'] = from_addr msg[\'To\'] = \\", \\".join(to_addrs) msg[\'Subject\'] = subject msg.attach(MIMEText(body, \'plain\')) # Send the email failed_recipients = server.sendmail(from_addr, to_addrs, msg.as_string()) # Quit the server connection server.quit() if failed_recipients: return { \'status\': \'failure\', \'message\': \'Some recipients failed to receive the email\', \'failed_recipients\': list(failed_recipients.keys()) } return { \'status\': \'success\', \'message\': \'Email sent successfully\', \'failed_recipients\': [] } except smtplib.SMTPAuthenticationError as e: return {\'status\': \'failure\', \'message\': \'Authentication error: \' + str(e), \'failed_recipients\': []} except smtplib.SMTPSenderRefused as e: return {\'status\': \'failure\', \'message\': \'Sender address refused: \' + str(e), \'failed_recipients\': []} except smtplib.SMTPRecipientsRefused as e: return {\'status\': \'failure\', \'message\': \'Recipient addresses refused: \' + str(e), \'failed_recipients\': list(e.recipients.keys())} except smtplib.SMTPDataError as e: return {\'status\': \'failure\', \'message\': \'SMTP data error: \' + str(e), \'failed_recipients\': []} except smtplib.SMTPConnectError as e: return {\'status\': \'failure\', \'message\': \'SMTP connect error: \' + str(e), \'failed_recipients\': []} except smtplib.SMTPHeloError as e: return {\'status\': \'failure\', \'message\': \'SMTP HELO error: \' + str(e), \'failed_recipients\': []} except Exception as e: return {\'status\': \'failure\', \'message\': \'General error: \' + str(e), \'failed_recipients\': []}"},{"question":"Problem Statement You are required to implement a function that performs multiple operations on a dictionary. Given a dictionary, a sequence of key-value pairs, and a key, your function should accomplish the following: 1. Check if the given key is present in the dictionary. 2. If the key is present, clear the dictionary. 3. If the key is absent, merge the sequence of key-value pairs into the dictionary. If any key from the sequence is already present in the dictionary, and the condition `override` is true, it should update the dictionary with the new value from the sequence. If `override` is false, existing values should not be overridden. 4. Return the modified dictionary and the result of step 1 (boolean). # Function Signature ```python def process_dictionary( dictionary: dict, seq2: list, key: str, override: bool ) -> (dict, bool): pass ``` # Input - `dictionary`: A Python dictionary object. - `seq2`: A list of key-value pairs represented as tuples (key, value). - `key`: A string representing the key to search in the dictionary. - `override`: A boolean value, if `True` existing keys in the dictionary should be overridden with values from `seq2`. # Output - A tuple containing the modified dictionary and a boolean value indicating the presence of the key in the original dictionary. # Constraints - The keys in `dictionary` and `seq2` are hashable. - The function should not modify the input `seq2`. # Example ```python # Example 1 dictionary = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3} seq2 = [(\\"a\\", 10), (\\"d\\", 4)] key = \\"b\\" override = False # Since \\"b\\" is present in the dictionary, it should clear the dictionary completely. assert process_dictionary(dictionary, seq2, key, override) == ({}, True) # Example 2 dictionary = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3} seq2 = [(\\"a\\", 10), (\\"d\\", 4)] key = \\"x\\" override = True # Since \\"x\\" is not present in the dictionary, merge seq2 into the dictionary with override. assert process_dictionary(dictionary, seq2, key, override) == ({\\"a\\": 10, \\"b\\": 2, \\"c\\": 3, \\"d\\": 4}, False) # Example 3 dictionary = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3} seq2 = [(\\"a\\", 10), (\\"d\\", 4)] key = \\"x\\" override = False # Since \\"x\\" is not present in the dictionary, merge seq2 into the dictionary without override. assert process_dictionary(dictionary, seq2, key, override) == ({\\"a\\": 1, \\"b\\": 2, \\"c\\": 3, \\"d\\": 4}, False) ``` # Note You can utilize various dictionary methods such as: - `key in dictionary` to check key presence. - `dictionary.clear()` to clear the dictionary. - `dictionary.update(new_dict)` to merge another dictionary. - Loop through `seq2` to add or update key-value pairs based on the `override` flag.","solution":"def process_dictionary(dictionary: dict, seq2: list, key: str, override: bool) -> (dict, bool): Processes the dictionary based on the specified key presence and merge condition. Parameters: dictionary (dict): The dictionary to operate on. seq2 (list): A list of key-value pairs to merge into the dictionary. key (str): The key to check in the dictionary. override (bool): If True, overrides existing keys with values from `seq2`. Returns: (dict, bool): A tuple containing the modified dictionary and a boolean indicating the presence of the given key in the original dictionary. key_present = key in dictionary if key_present: dictionary.clear() else: for k, v in seq2: if override or k not in dictionary: dictionary[k] = v return dictionary, key_present"},{"question":"**Task:** Write an asynchronous Python program that simulates a restaurant kitchen management system using `asyncio.Queue`. The kitchen should manage orders from customers and process them one at a time while maintaining the order of arrival (FIFO). You are also required to handle given constraints and outputs. # Requirements: 1. **Order Queue:** - Create an `asyncio.Queue` named `order_queue` with a maximum size of 5. - If the queue reaches the max size, new orders should wait until there is space available in the queue. 2. **Order Placement:** - Define an asynchronous function `place_order(order_id)` which accepts an order identifier and puts it into the `order_queue`. - If the queue is full, the function should wait until a slot is available. 3. **Order Processing:** - Define an asynchronous function `process_order()` which pulls orders from the `order_queue` and processes them one by one. - Simulate the order processing by sleeping for a random time between 1 and 3 seconds. - Use the `task_done()` method after processing each order. 4. **Handling Overflows:** - Utilize `put_nowait()` to demonstrate what happens if you try to put an order into the queue when it is full. This should raise a `QueueFull` exception. 5. **Running the Simulation:** - Implement a `main` function to simulate placing and processing of 10 orders. - Make sure that orders are processed concurrently and in the order they were placed. - Print output statements indicating the state of the queue and the order processing status. # Example Output: ``` Order 1 placed in queue. Order 2 placed in queue. Order 3 placed in queue. ... Queue is full. Waiting to place order 6. Order 4 processed. Order 5 processed. ... Order 10 processed. ``` # Constraints: - You must use the `asyncio.Queue` for order management. - Ensure that the program demonstrates the use of asyncio concepts and efficient handling of the queue. ```python import asyncio import random # Implement the required functions here async def place_order(order_id): # Your code goes here async def process_order(): # Your code goes here async def main(): # Your code goes here # Run the main function asyncio.run(main()) ``` **Note:** Ensure your program handles the queue operations correctly in an asynchronous manner, demonstrating proper use of functions such as `await`, `asyncio.sleep`, `put`, `get`, `task_done`, and exception handling.","solution":"import asyncio import random # Define the global order queue with a maximum size of 5 order_queue = asyncio.Queue(maxsize=5) async def place_order(order_id): Places an order into the order_queue. If the queue is full, wait until a slot is available. print(f\\"Placing order {order_id}\\") await order_queue.put(order_id) print(f\\"Order {order_id} placed in queue.\\") async def process_order(): Processes orders in the order_queue. Simulates processing time by sleeping for a random time between 1 and 3 seconds. while True: order_id = await order_queue.get() print(f\\"Processing order {order_id}\\") await asyncio.sleep(random.randint(1, 3)) order_queue.task_done() print(f\\"Order {order_id} processed.\\") async def main(): Simulates placing and processing of 10 orders. # Start the order processing coroutine asyncio.create_task(process_order()) # Place 10 orders for i in range(1, 11): await place_order(i) await asyncio.sleep(random.uniform(0.1, 0.5)) # Simulate time taken to place next order # Wait until all orders are processed await order_queue.join() print(\\"All orders processed.\\") # Run the main function asyncio.run(main())"},{"question":"# Coding Assessment: Implement a Custom Numerical Operations Class Objective Implement a class `CustomNumber` that provides an interface for performing various numerical operations using the provided functions. This class should encapsulate a basic numerical value and support operations such as addition, subtraction, multiplication, division, bitwise operations, and conversions. Requirements 1. Create a class `CustomNumber` that initializes with a single numerical value. 2. Implement the following methods in the class: - `add(self, other)` - `subtract(self, other)` - `multiply(self, other)` - `divide(self, other, floor=False)` - `modulus(self, other)` - `power(self, exponent, modulo=None)` - `negate(self)` - `absolute(self)` - `bitwise_and(self, other)` - `bitwise_or(self, other)` - `bitwise_xor(self, other)` - `bitwise_invert(self)` - `left_shift(self, other)` - `right_shift(self, other)` - `to_int(self)` - `to_float(self)` - `to_base(self, base)` 3. Methods should make use of the functions described in the documentation and handle errors appropriately by returning a custom error message or raising an appropriate exception. 4. The class should be immutable, meaning each operation returns a new instance of `CustomNumber`. Input and Output Formats - Each `CustomNumber` instance should be initialized with an integer or float. - Each operation method accepts one or more parameters as needed and returns a `CustomNumber` instance with the result. - If an invalid operation is attempted, the method should raise a `ValueError` with an appropriate message. Constraints - The class should only accept integers and floats for operations. - Ensure to handle exceptional cases like division by zero, invalid base for conversions, and invalid types for operations gracefully. Performance Requirements - The operations should be implemented efficiently, leveraging the provided numerical functions. - Ensure operations do not create unnecessary copies of data. Example Usage: ```python num1 = CustomNumber(10) num2 = CustomNumber(3) # Basic arithmetic operations result = num1.add(num2) # CustomNumber(13) result = num1.subtract(num2) # CustomNumber(7) result = num1.multiply(num2) # CustomNumber(30) result = num1.divide(num2) # CustomNumber(3.3333333333333335) # Bitwise operations result = num1.bitwise_and(num2) # CustomNumber(2) result = num1.left_shift(CustomNumber(2)) # CustomNumber(40) # Conversions result = num1.to_base(2) # CustomNumber(\'0b1010\') result = num1.to_int() # CustomNumber(10) ``` Implement the `CustomNumber` class below: ```python class CustomNumber: def __init__(self, value): # Initialize with integer or float pass def add(self, other): pass def subtract(self, other): pass def multiply(self, other): pass def divide(self, other, floor=False): pass def modulus(self, other): pass def power(self, exponent, modulo=None): pass def negate(self): pass def absolute(self): pass def bitwise_and(self, other): pass def bitwise_or(self, other): pass def bitwise_xor(self, other): pass def bitwise_invert(self): pass def left_shift(self, other): pass def right_shift(self, other): pass def to_int(self): pass def to_float(self): pass def to_base(self, base): pass ```","solution":"class CustomNumber: def __init__(self, value): if not isinstance(value, (int, float)): raise ValueError(\\"Value must be an integer or float\\") self.value = value def add(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(self.value + other) def subtract(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(self.value - other) def multiply(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(self.value * other) def divide(self, other, floor=False): if isinstance(other, CustomNumber): other = other.value if other == 0: raise ValueError(\\"Cannot divide by zero\\") if floor: return CustomNumber(self.value // other) return CustomNumber(self.value / other) def modulus(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(self.value % other) def power(self, exponent, modulo=None): if isinstance(exponent, CustomNumber): exponent = exponent.value if modulo: return CustomNumber(pow(self.value, exponent, modulo)) return CustomNumber(self.value ** exponent) def negate(self): return CustomNumber(-self.value) def absolute(self): return CustomNumber(abs(self.value)) def bitwise_and(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(int(self.value) & int(other)) def bitwise_or(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(int(self.value) | int(other)) def bitwise_xor(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(int(self.value) ^ int(other)) def bitwise_invert(self): return CustomNumber(~int(self.value)) def left_shift(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(int(self.value) << int(other)) def right_shift(self, other): if isinstance(other, CustomNumber): other = other.value return CustomNumber(int(self.value) >> int(other)) def to_int(self): return int(self.value) def to_float(self): return float(self.value) def to_base(self, base): if not isinstance(base, int) or base < 2 or base > 36: raise ValueError(\\"Base must be an integer between 2 and 36\\") digits = \\"0123456789abcdefghijklmnopqrstuvwxyz\\" is_negative = self.value < 0 num = abs(int(self.value)) result = \\"\\" while num: result = digits[num % base] + result num //= base if is_negative: result = \'-\' + result return result if result else \\"0\\""},{"question":"In this assessment, you are required to demonstrate your understanding of using the `unittest.mock` module in Python. The task is to mock specific parts of a class and assert that these parts are being called with the correct arguments. Additionally, you must handle different scenarios, such as setting return values and using side effects. # Problem Statement Given the following class: ```python class Calculator: def add(self, x, y): return x + y def subtract(self, x, y): return x - y def multiply(self, x, y): return x * y def divide(self, x, y): if y == 0: raise ValueError(\\"Cannot divide by zero\\") return x / y class AdvancedCalculator(Calculator): def __init__(self): self.log = [] def log_operation(self, operation, x, y, result): log_entry = f\\"{operation}: {x} {y} = {result}\\" self.log.append(log_entry) def add_and_log(self, x, y): result = self.add(x, y) self.log_operation(\\"add\\", x, y, result) return result def complex_calculation(self, x, y, z): result_add = self.add(x, y) result_sub = self.subtract(result_add, z) result_mul = self.multiply(result_sub, x) self.log_operation(\\"complex_calculation\\", x, y, result_mul) return result_mul ``` # Tasks 1. **Mock Method Calls:** Write test functions using the `unittest` framework to: - Mock the `add` method of the `Calculator` class in the `add_and_log` method and assert that it was called with the correct arguments. - Mock the `log_operation` method of the `AdvancedCalculator` class and assert that it logs the correct operation message. 2. **Ensure Correct Return Values and Side Effects:** - Mock the `add` method to return a specific value when called within the `add_and_log` method. - Use the `side_effect` attribute to simulate a scenario where the `log_operation` method raises an exception. Ensure that your test captures this scenario correctly. 3. **Verify Complex Method Interactions:** - Test the `complex_calculation` method by mocking the `add`, `subtract`, and `multiply` methods to return predefined values. Ensure these methods are called in the correct order with the correct arguments. **Input Format:** - You do not need to create input functions. Focus on writing the `test` functions. **Output Format:** - Under normal execution, your tests should pass without any assertion errors. - If a method is not called with the correct arguments, or if an incorrect value is returned, an assertion error should be raised. **Constraints:** - You should use the `unittest.mock` module. - Ensure that all patches are correctly applied using the appropriate decorators or context managers. - Do not modify the `Calculator` or `AdvancedCalculator` classes. **Example Test Cases:** ```python import unittest from unittest.mock import patch, MagicMock class TestAdvancedCalculator(unittest.TestCase): @patch(\'path.to.Calculator.add\') @patch(\'path.to.AdvancedCalculator.log_operation\') def test_add_and_log(self, mock_log_operation, mock_add): mock_add.return_value = 5 calc = AdvancedCalculator() result = calc.add_and_log(2, 3) mock_add.assert_called_once_with(2, 3) mock_log_operation.assert_called_once_with(\\"add\\", 2, 3, 5) self.assertEqual(result, 5) @patch(\'path.to.AdvancedCalculator.log_operation\') def test_log_operation_side_effect(self, mock_log_operation): mock_log_operation.side_effect = Exception(\\"Logging Error\\") calc = AdvancedCalculator() with self.assertRaises(Exception) as context: calc.add_and_log(2, 3) self.assertTrue(\\"Logging Error\\" in str(context.exception)) @patch(\'path.to.Calculator.add\', return_value=5) @patch(\'path.to.Calculator.subtract\', return_value=2) @patch(\'path.to.Calculator.multiply\', return_value=10) def test_complex_calculation(self, mock_multiply, mock_subtract, mock_add): calc = AdvancedCalculator() result = calc.complex_calculation(3, 2, 1) mock_add.assert_called_once_with(3, 2) mock_subtract.assert_called_once_with(5, 1) mock_multiply.assert_called_once_with(2, 3) self.assertEqual(result, 10) if __name__ == \'__main__\': unittest.main() ``` Your task is to ensure that all tests are written comprehensively to cover the behaviors as specified.","solution":"import unittest from unittest.mock import patch, MagicMock class Calculator: def add(self, x, y): return x + y def subtract(self, x, y): return x - y def multiply(self, x, y): return x * y def divide(self, x, y): if y == 0: raise ValueError(\\"Cannot divide by zero\\") return x / y class AdvancedCalculator(Calculator): def __init__(self): self.log = [] def log_operation(self, operation, x, y, result): log_entry = f\\"{operation}: {x} {y} = {result}\\" self.log.append(log_entry) def add_and_log(self, x, y): result = self.add(x, y) self.log_operation(\\"add\\", x, y, result) return result def complex_calculation(self, x, y, z): result_add = self.add(x, y) result_sub = self.subtract(result_add, z) result_mul = self.multiply(result_sub, x) self.log_operation(\\"complex_calculation\\", x, y, result_mul) return result_mul"},{"question":"# Question: Testing a File System Interaction with Mocks Objective: Write a function `process_file(file_path)` that reads a file, processes its contents by reversing each line, and writes the reversed lines to a new file. The new file should be named by appending `_reversed` to the original file\'s name before the extension. Using the `unittest.mock` library, you need to write tests to: 1. Mock the file reading and writing operations. 2. Ensure that the file reading and writing operations are called correctly. 3. Check that the processed content is as expected. Function Signature: ```python def process_file(file_path: str) -> None: pass ``` Example: If the input file contains: ``` hello world python is great mocks are useful ``` The output file should contain: ``` dlrow olleh taerg si nohtyp lufesu era skcom ``` Constraints: - You can assume that the input file exists and contains text data. - The function should not use any actual file I/O operations. Requirements: 1. Implement the `process_file` function. 2. Implement a test case using the `unittest.mock` library to mock file I/O operations and validate the function\'s correctness. Hints: - Use `mock_open` to mock file operations. - Use `patch` to replace the built-in `open` function with `mock_open`. - Use `assert_called_once_with` to ensure the file operations are called with the correct parameters. Example Test Case: ```python import unittest from unittest.mock import mock_open, patch, call class TestProcessFile(unittest.TestCase): @patch(\'builtins.open\', new_callable=mock_open, read_data=\'hello worldnpython is greatnmocks are useful\') def test_process_file(self, mock_file): file_path = \'test.txt\' process_file(file_path) mock_file.assert_called_with(file_path, \'r\') mock_file().write.assert_called_once_with(\'dlrow ollehntaerg si nohtypnlufesu era skcomn\') if __name__ == \'__main__\': unittest.main() ``` This question assesses the student\'s understanding of mocking techniques in Python, especially focusing on file handling and the use of `assert_called_once_with` for asserting correct behavior.","solution":"def process_file(file_path: str) -> None: Reads a file, processes its contents by reversing each line, and writes the reversed lines to a new file. import os # Read file content with open(file_path, \'r\') as file: lines = file.readlines() # Process lines reversed_lines = [line.strip()[::-1] for line in lines] base, ext = os.path.splitext(file_path) new_file_path = f\\"{base}_reversed{ext}\\" # Write new file content with open(new_file_path, \'w\') as new_file: new_file.write(\\"n\\".join(reversed_lines) + \\"n\\")"},{"question":"**Objective:** Your task is to implement a function using scikit-learn\'s `DictVectorizer` and `TfidfVectorizer` to process a dataset of text documents and their associated metadata. The goal is to create a combined feature matrix that includes both text features and metadata features. **Problem Description:** You are provided with a dataset of documents, where each document is represented as a dictionary containing: - A `text` field, which contains the document\'s content as a string. - Metadata fields such as `author`, `year`, and `categories`. 1. Use `TfidfVectorizer` to transform the `text` field into a TF-IDF weighted document-term matrix. 2. Use `DictVectorizer` to transform the metadata fields into a feature matrix. 3. Combine both feature matrices into a single feature matrix. **Function Signature:** ```python from typing import List, Dict import numpy as np def combine_features(documents: List[Dict]) -> np.ndarray: pass ``` **Input:** - `documents`: A list of dictionaries, where each dictionary represents a document. - Each dictionary will contain: - `text` (str): The content of the document. - `author` (str): The author of the document. - `year` (int): The publication year of the document. - `categories` (List[str]): A list of categories associated with the document. **Output:** - Returns a numpy 2D array combining both the TF-IDF weighted term-document matrix and the metadata feature matrix. **Constraints:** - Assume the number of documents will be at most 1000. - The `text` field in each document will contain less than 10000 characters. - The combination of all unique authors and categories will not exceed 500. **Example:** Input: ```python documents = [ { \'text\': \'This is a sample document.\', \'author\': \'John Doe\', \'year\': 2020, \'categories\': [\'sample\', \'document\'] }, { \'text\': \'Another example document for testing.\', \'author\': \'Jane Doe\', \'year\': 2019, \'categories\': [\'example\', \'test\'] } ] ``` Output: ``` A numpy 2D array with combined TF-IDF features and metadata features (shape (2, <combined_features>)). ``` **Implementation Notes:** 1. First, use `TfidfVectorizer` to convert the `text` content into TF-IDF scores matrix. 2. Then, use `DictVectorizer` to convert `author`, `year`, and `categories` into a feature matrix. Note that the \'categories\' field should be expanded into multiple binary indicator features. 3. Finally, concatenate both matrices horizontally to form the combined feature matrix. **Hints:** - Ensure that the `text` is processed with `TfidfVectorizer`. - The metadata fields should be structured correctly for `DictVectorizer`. - Make sure to handle the different data types (string, integer, list) appropriately for metadata.","solution":"from typing import List, Dict import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import DictVectorizer def combine_features(documents: List[Dict]) -> np.ndarray: # Extract texts and metadata texts = [doc[\'text\'] for doc in documents] metadata = [ { \\"author\\": doc[\'author\'], \\"year\\": doc[\'year\'] } for doc in documents ] for doc, met in zip(documents, metadata): for category in doc[\'categories\']: met[f\\"category_{category}\\"] = 1 # Vectorize the text field using TfidfVectorizer tfidf_vectorizer = TfidfVectorizer() text_features = tfidf_vectorizer.fit_transform(texts) # Vectorize the metadata fields using DictVectorizer dict_vectorizer = DictVectorizer() metadata_features = dict_vectorizer.fit_transform(metadata) # Combine the text and metadata features combined_features = np.hstack([text_features.toarray(), metadata_features.toarray()]) return combined_features"},{"question":"# Coding Assessment: Working with `importlib.metadata` **Objective:** Demonstrate your understanding of the `importlib.metadata` module in Python 3.10 by implementing a set of functions to interact with package metadata and entry points. **Problem Statement:** You are provided with several tasks that require you to fetch and manipulate metadata information for installed packages using the `importlib.metadata` module. Your goal is to implement the following functions: 1. **get_package_version(package_name: str) -> str:** - This function should return the version of the specified package as a string. - If the package is not found, return the string `\\"Package not found\\"`. 2. **get_entry_points_by_group(group: str) -> dict:** - This function should return a dictionary where keys are entry point names and values are the entry points\' values for the specified group. - If no entry points are found for the group, return an empty dictionary. 3. **get_package_files(package_name: str) -> list:** - This function should return a list of strings representing the paths of all files installed by the specified package. - If the package is not found or there are no files, return an empty list. 4. **get_distribution_metadata(package_name: str) -> dict:** - This function should return a dictionary containing all metadata key-value pairs for the specified package. - If the package is not found, return an empty dictionary. 5. **get_requirements(package_name: str) -> list:** - This function should return a list of strings listing all the requirements of the specified package. - If the package is not found or there are no requirements, return an empty list. **Function Signatures:** ```python def get_package_version(package_name: str) -> str: pass def get_entry_points_by_group(group: str) -> dict: pass def get_package_files(package_name: str) -> list: pass def get_distribution_metadata(package_name: str) -> dict: pass def get_requirements(package_name: str) -> list: pass ``` **Example:** ```python # Assuming \'wheel\' is installed print(get_package_version(\\"wheel\\")) # Output: \'0.32.3\' or the version of \'wheel\' currently installed print(get_entry_points_by_group(\\"console_scripts\\")) # Output: {\'some_entry_point\': \'some_value\', ... } print(get_package_files(\\"wheel\\")) # Output: [\'path/to/file1\', \'path/to/file2\', ...] print(get_distribution_metadata(\\"wheel\\")) # Output: {\'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', ... } print(get_requirements(\\"wheel\\")) # Output: [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] ``` **Constraints:** - Assume the functions will be tested with package names that exist in the environment where the tests are executed. - Return values should handle cases where data might not be available as specified in the function descriptions. Use the information in the `importlib.metadata` module documentation to implement these functions.","solution":"import importlib.metadata def get_package_version(package_name: str) -> str: Returns the version of the specified package as a string. If the package is not found, return the string \\"Package not found\\". try: return importlib.metadata.version(package_name) except importlib.metadata.PackageNotFoundError: return \\"Package not found\\" def get_entry_points_by_group(group: str) -> dict: Returns a dictionary where keys are entry point names and values are the entry points\' values for the specified group. If no entry points are found for the group, return an empty dictionary. entry_points = importlib.metadata.entry_points().get(group, []) return {ep.name: ep.value for ep in entry_points} def get_package_files(package_name: str) -> list: Returns a list of strings representing the paths of all files installed by the specified package. If the package is not found or there are no files, return an empty list. try: distribution = importlib.metadata.distribution(package_name) return [str(file) for file in distribution.files or []] except importlib.metadata.PackageNotFoundError: return [] def get_distribution_metadata(package_name: str) -> dict: Returns a dictionary containing all metadata key-value pairs for the specified package. If the package is not found, return an empty dictionary. try: distribution = importlib.metadata.distribution(package_name) return dict(distribution.metadata) except importlib.metadata.PackageNotFoundError: return {} def get_requirements(package_name: str) -> list: Returns a list of strings listing all the requirements of the specified package. If the package is not found or there are no requirements, return an empty list. try: distribution = importlib.metadata.distribution(package_name) return distribution.requires or [] except importlib.metadata.PackageNotFoundError: return []"},{"question":"Implement Custom Kernel Computations Objective Your task is to implement custom kernel functions using the scikit-learn library\'s `pairwise_kernels` utility. You will provide a kernel computation for a given dataset and verify its correctness. Problem Statement Create a function `compute_kernels` that calculates various kernels between two sets of samples, ( X ) and ( Y ): # Input: 1. `X`: A numpy array of shape (n_samples_X, n_features), representing the first set of samples. 2. `Y`: A numpy array of shape (n_samples_Y, n_features), representing the second set of samples. 3. `kernel_type`: A string representing the type of kernel to compute. It can be one of the following values: `\'linear\'`, `\'polynomial\'`, `\'rbf\'`, `\'sigmoid\'`. # Output: - A numpy array with the computed kernel values between the samples in ( X ) and ( Y ). # Constraints: - The input arrays ( X ) and ( Y ) can have up to 1000 samples and up to 100 features each. - The kernel type will always be one of the specified values. - Use default parameters for kernel computations where not specified. # Function Signature: ```python def compute_kernels(X: np.ndarray, Y: np.ndarray, kernel_type: str) -> np.ndarray: ``` Example: ```python import numpy as np from sklearn.metrics.pairwise import pairwise_kernels def compute_kernels(X, Y, kernel_type): if kernel_type not in [\'linear\', \'polynomial\', \'rbf\', \'sigmoid\']: raise ValueError(f\\"Unsupported kernel type: {kernel_type}\\") return pairwise_kernels(X, Y, metric=kernel_type) # Example usage: X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) kernel_type = \'linear\' result = compute_kernels(X, Y, kernel_type) print(result) ``` Expected output: ```plaintext [[ 2. 7.] [ 3. 11.] [ 5. 18.]] ``` Evaluate your implementation by creating additional test cases with different values of `kernel_type`. Notes: - Ensure the function raises appropriate exceptions or errors for invalid inputs. - Document your code to explain the logic behind your implementation steps.","solution":"import numpy as np from sklearn.metrics.pairwise import pairwise_kernels def compute_kernels(X, Y, kernel_type): Compute the specified kernel between two sets of samples. Parameters: X (np.ndarray): First set of samples with shape (n_samples_X, n_features). Y (np.ndarray): Second set of samples with shape (n_samples_Y, n_features). kernel_type (str): Type of kernel to compute. It can be \'linear\', \'polynomial\', \'rbf\', or \'sigmoid\'. Returns: np.ndarray: Computed kernel values between the samples in X and Y. if kernel_type not in [\'linear\', \'polynomial\', \'rbf\', \'sigmoid\']: raise ValueError(f\\"Unsupported kernel type: {kernel_type}\\") return pairwise_kernels(X, Y, metric=kernel_type)"},{"question":"Unicode Handling in Python # Background In software systems that handle text data from various languages, proper management of Unicode characters is crucial for ensuring data integrity and correctness. Python provides extensive support for Unicode, including encoding conversions, string normalization, and detailed character properties. # Problem Statement You are tasked with implementing a `UnicodeManager` class that provides various utilities for handling Unicode text. The functionalities of this class should include reading and writing Unicode text files, normalizing Unicode strings, converting between different encodings, and comparing Unicode strings correctly. # Requirements 1. **Class Definition**: Define a class named `UnicodeManager`. 2. **Reading a Unicode File**: - Implement a method `read_unicode_file(file_path: str, encoding: str = \'utf-8\') -> str` that reads and returns the content of a Unicode text file. 3. **Writing a Unicode File**: - Implement a method `write_unicode_file(file_path: str, text: str, encoding: str = \'utf-8\') -> None` that writes a given Unicode text to a file. 4. **Normalization**: - Implement a method `normalize_text(text: str, form: str = \'NFC\') -> str` that normalizes a given text using the specified normalization form (one of `\'NFC\'`, `\'NFKC\'`, `\'NFD\'`, `\'NFKD\'`). 5. **Encoding Conversion**: - Implement a method `convert_encoding(text: str, to_encoding: str, from_encoding: str = \'utf-8\') -> bytes` that converts a Unicode string from one encoding to another. 6. **Comparing Unicode Strings**: - Implement a method `compare_unicode_strings(s1: str, s2: str) -> bool` that compares two Unicode strings for equality after normalizing them to form `\'NFC\'`. # Input and Output Formats - `read_unicode_file(file_path: str, encoding: str = \'utf-8\') -> str`: Reads a file with the specified encoding and returns its content as a string. - `write_unicode_file(file_path: str, text: str, encoding: str = \'utf-8\') -> None`: Writes the given text to a file with the specified encoding. - `normalize_text(text: str, form: str = \'NFC\') -> str`: Returns the normalized version of the given text. - `convert_encoding(text: str, to_encoding: str, from_encoding: str = \'utf-8\') -> bytes`: Returns a byte sequence of the text converted to the specified encoding. - `compare_unicode_strings(s1: str, s2: str) -> bool`: Returns `True` if the normalized forms of the two strings are equal, otherwise `False`. # Constraints 1. You can assume the input paths will always be valid and accessible. 2. The text for normalization and encoding conversion can contain any arbitrary Unicode characters. 3. Normalization forms input will be one of `\'NFC\'`, `\'NFKC\'`, `\'NFD\'`, `\'NFKD\'`. # Example ```python # Creating an instance of the UnicodeManager manager = UnicodeManager() # Reading from a Unicode file content = manager.read_unicode_file(\'example.txt\', \'utf-8\') # Writing to a Unicode file manager.write_unicode_file(\'output.txt\', content, \'utf-16\') # Normalizing text normalized_text = manager.normalize_text(\'cafÃ©\', \'NFD\') # Converting encoding encoded_text = manager.convert_encoding(\'ã“ã‚“ã«ã¡ã¯\', \'utf-16\', \'utf-8\') # Comparing Unicode strings are_equal = manager.compare_unicode_strings(\'Ãª\', \'ex0302\') print(are_equal) # Output: True ``` # Submission - Implement the `UnicodeManager` class with the required methods. - Ensure your code is well-documented and follows best practices for handling Unicode in Python. - Write test cases demonstrating the use and correctness of each method in the `UnicodeManager` class.","solution":"import os import unicodedata class UnicodeManager: def read_unicode_file(self, file_path: str, encoding: str = \'utf-8\') -> str: with open(file_path, \'r\', encoding=encoding) as file: return file.read() def write_unicode_file(self, file_path: str, text: str, encoding: str = \'utf-8\') -> None: with open(file_path, \'w\', encoding=encoding) as file: file.write(text) def normalize_text(self, text: str, form: str = \'NFC\') -> str: return unicodedata.normalize(form, text) def convert_encoding(self, text: str, to_encoding: str, from_encoding: str = \'utf-8\') -> bytes: return text.encode(from_encoding).decode(from_encoding).encode(to_encoding) def compare_unicode_strings(self, s1: str, s2: str) -> bool: return unicodedata.normalize(\'NFC\', s1) == unicodedata.normalize(\'NFC\', s2)"},{"question":"**Objective**: The goal of this question is to demonstrate your understanding of configuring and querying PyTorch backend settings, particularly focusing on the CUDA and cuDNN backends. Problem Statement You are tasked with implementing a function that configures and prints out specific backend settings in PyTorch. The function `configure_and_query_backends()` should perform the following: 1. Enable the use of TensorFloat-32 and reduced precision reductions for the CUDA backend. 2. Set the `max_size` of the `cufft_plan_cache` for the current CUDA device to a specified value. 3. Enable cuDNN, set it to benchmark mode, and set a specified `benchmark_limit`. 4. Query and print out whether TensorFloat-32 and reduced precision reductions are allowed, the size of the `cufft_plan_cache`, and the cuDNN benchmark status. Function Signature ```python def configure_and_query_backends(cufft_cache_max_size: int, cudnn_benchmark_limit: int) -> dict: # Your implementation here ``` Input - `cufft_cache_max_size` (int): The maximum size to set for the cuFFT plan cache for the current CUDA device. - `cudnn_benchmark_limit` (int): The benchmark limit to set for cuDNN. Output - A dictionary with the following keys and values: - `\'tf32_allowed\'`: bool, whether TensorFloat-32 is allowed in CUDA. - `\'fp16_reduced_precision_allowed\'`: bool, whether reduced precision reductions are allowed in CUDA. - `\'cufft_plan_cache_size\'`: int, the current size of the cuFFT plan cache for the current CUDA device. - `\'cudnn_enabled\'`: bool, whether cuDNN is enabled. - `\'cudnn_benchmark\'`: bool, whether cuDNN benchmark mode is set. - `\'cudnn_benchmark_limit\'`: int, the current benchmark limit for cuDNN. Constraints - Ensure that your function runs efficiently. - Handle cases where specific backends or features might not be available gracefully. Example ```python result = configure_and_query_backends(100, 10) print(result) # Expected Output # { # \'tf32_allowed\': True, # \'fp16_reduced_precision_allowed\': True, # \'cufft_plan_cache_size\': 0, # Assuming it starts empty # \'cudnn_enabled\': True, # \'cudnn_benchmark\': True, # \'cudnn_benchmark_limit\': 10 # } ``` Notes - You should handle potential exceptions gracefully. For example, check if CUDA is available before attempting to configure CUDA-specific settings. - Assume that the necessary package imports are already handled. Good luck, and demonstrate your understanding of PyTorch backend configuration!","solution":"import torch def configure_and_query_backends(cufft_cache_max_size: int, cudnn_benchmark_limit: int) -> dict: if not torch.cuda.is_available(): return { \'tf32_allowed\': False, \'fp16_reduced_precision_allowed\': False, \'cufft_plan_cache_size\': None, \'cudnn_enabled\': False, \'cudnn_benchmark\': False, \'cudnn_benchmark_limit\': None } torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True torch.backends.cuda.cufft_plan_cache.set_max_size(cufft_cache_max_size) torch.backends.cudnn.enabled = True torch.backends.cudnn.benchmark = True torch.backends.cudnn.benchmark_limit = cudnn_benchmark_limit return { \'tf32_allowed\': torch.backends.cuda.matmul.allow_tf32, \'fp16_reduced_precision_allowed\': torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction, \'cufft_plan_cache_size\': torch.backends.cuda.cufft_plan_cache.size(), \'cudnn_enabled\': torch.backends.cudnn.enabled, \'cudnn_benchmark\': torch.backends.cudnn.benchmark, \'cudnn_benchmark_limit\': cudnn_benchmark_limit }"},{"question":"**Seaborn Coding Assessment** # Objective Demonstrate understanding of seaborn\'s `husl_palette` function and its application in creating color palettes for data visualizations. # Problem Statement You are given a dataset `iris` which consists of the following columns: `\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\', \'species\'`. You are required to create and visualize a scatterplot for this dataset, using seaborn\'s `husl_palette` to customize the colors of the plots. # Requirements: 1. **Load the Dataset**: Use seaborn\'s inbuilt `iris` dataset. 2. **Scatter Plot**: Create a scatter plot using seaborn to visualize `sepal_length` vs. `sepal_width`, differentiated by `species` using different colors. 3. **Customization using `husl_palette`**: - Generate a color palette of 3 colors with `sns.husl_palette()` representing the three species. - Customize the plot by adjusting the lightness and saturation of the generated colors. - Use the generated custom palette in your seaborn scatter plot. # Expected Input and Output Format - **Input**: None (you will use seaborn\'s inbuilt `iris` dataset). - **Output**: A customized scatter plot visualizing `sepal_length` vs. `sepal_width` with colors differentiated by species using a customized `husl_palette`. # Constraints - You must use seaborn for the plotting. - You should appropriately adjust the lightness and saturation to clearly distinguish between the species. # Performance Requirements - The plot should be clear and well-differentiated using the color palette generated from `husl_palette`. # Implementation ```python import seaborn as sns import matplotlib.pyplot as plt # Load seaborn\'s inbuilt \'iris\' dataset iris = sns.load_dataset(\'iris\') # Generate a customized HUSL palette of 3 colors palette = sns.husl_palette(3, l=0.6, s=0.9) # Create a scatter plot for \'sepal_length\' vs. \'sepal_width\' differentiated by \'species\' sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette) # Display the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(): Loads the iris dataset and creates a scatter plot of \'sepal_length\' vs \'sepal_width\', differentiated by \'species\' using a customized husl_palette. # Load seaborn\'s inbuilt \'iris\' dataset iris = sns.load_dataset(\'iris\') # Generate a customized HUSL palette of 3 colors palette = sns.husl_palette(3, l=0.6, s=0.9) # Create a scatter plot for \'sepal_length\' vs. \'sepal_width\' differentiated by \'species\' sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette) # Display the plot plt.show()"},{"question":"`.netrc` File Parser You are required to implement a function to parse a `.netrc` file and return useful information about the hosts defined in it. Your task is to implement a function named `parse_netrc_file` which does the following: 1. Accepts a file path to a `.netrc` file as an input. 2. Returns a dictionary where: - The keys are hostnames defined in the `.netrc` file. - The values are 3-tuples containing (login, account, password) for each hostname. The function should also handle errors gracefully by catching and returning error messages for the following scenarios: - If the file path does not exist, return `\\"FileNotFoundError: The specified netrc file was not found.\\"` - If there is a parsing error in the file, return `\\"NetrcParseError: {message}\\"` where `{message}` is the error message from the exception. Function Signature ```python def parse_netrc_file(file_path: str) -> Union[Dict[str, Tuple[str, str, str]], str]: pass ``` Input - `file_path` : str : The file path of the `.netrc` file. Output - Returns a dictionary mapping each hostname to a tuple `(login, account, password)`. - Returns a string with an appropriate error message if the file could not be accessed or parsed. Example ```python # Assume the content of \'sample.netrc\' is: # machine host1 # login user1 # password pass1 # # machine host2 # login user2 # password pass2 result = parse_netrc_file(\'sample.netrc\') # Expected output: # { # \'host1\': (\'user1\', None, \'pass1\'), # \'host2\': (\'user2\', None, \'pass2\') # } ``` # Constraints: - The `.netrc` file is guaranteed to be in a valid format, but may potentially contain commented lines. - The `account` field may be absent for some hosts. Performance Requirements - The function should be efficient and able to handle `.netrc` files up to 1MB in size seamlessly. - Use exception handling judiciously to manage and report errors as specified.","solution":"import os from typing import Union, Dict, Tuple import netrc def parse_netrc_file(file_path: str) -> Union[Dict[str, Tuple[str, str, str]], str]: if not os.path.exists(file_path): return \\"FileNotFoundError: The specified netrc file was not found.\\" try: netrc_data = netrc.netrc(file_path) result = {} for host, auth in netrc_data.hosts.items(): login, account, password = auth result[host] = (login, account, password) return result except netrc.NetrcParseError as e: return f\\"NetrcParseError: {e}\\""},{"question":"# Seaborn Context Customization and Comparison You are required to write a Python function that creates and saves seaborn line plots with different contexts and customizations. Demonstrate your understanding of setting contexts, adjusting font scales, and overriding specific parameters within the seaborn plotting library. Function Signature ```python def create_custom_plots(): pass ``` Requirements 1. **Generate Line Plots:** Create a line plot with x values `[0, 1, 2, 3, 4]` and y values `[0, 1, 4, 9, 16]`. 2. **Context Variations:** - Default context. - \\"notebook\\" context with default settings. - \\"notebook\\" context with `font_scale=1.5`. - \\"notebook\\" context with `rc={\\"lines.linewidth\\": 2.5}`. 3. **Save the Plots:** - Save each generated plot as a PNG image with appropriate filenames indicating the context and customizations. For example, `default_context.png`, `notebook_context.png`, `notebook_context_font_scale.png`, `notebook_context_linewidth.png`. Detailed Steps: 1. Use the `sns.lineplot` function to create line plots. 2. Set the appropriate context using `sns.set_context`. 3. Implement additional customizations for font scale and specific parameters where required. 4. Save each plot using a unique filename reflecting the context and customization as specified above. Constraints: - Use only the x and y values provided. - Ensure that each plot file has the correct filename format and context-related customizations. Example: ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(): x = [0, 1, 2, 3, 4] y = [0, 1, 4, 9, 16] # Default context sns.lineplot(x=x, y=y) plt.savefig(\\"default_context.png\\") plt.clf() # Clear the current figure # \\"notebook\\" context sns.set_context(\\"notebook\\") sns.lineplot(x=x, y=y) plt.savefig(\\"notebook_context.png\\") plt.clf() # \\"notebook\\" context with font_scale=1.5 sns.set_context(\\"notebook\\", font_scale=1.5) sns.lineplot(x=x, y=y) plt.savefig(\\"notebook_context_font_scale.png\\") plt.clf() # \\"notebook\\" context with rc={\\"lines.linewidth\\": 2.5} sns.set_context(\\"notebook\\", rc={\\"lines.linewidth\\": 2.5}) sns.lineplot(x=x, y=y) plt.savefig(\\"notebook_context_linewidth.png\\") plt.clf() create_custom_plots() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(): x = [0, 1, 2, 3, 4] y = [0, 1, 4, 9, 16] # Default context sns.set_context(\\"paper\\") sns.lineplot(x=x, y=y) plt.savefig(\\"default_context.png\\") plt.clf() # Clear the current figure # \\"notebook\\" context sns.set_context(\\"notebook\\") sns.lineplot(x=x, y=y) plt.savefig(\\"notebook_context.png\\") plt.clf() # \\"notebook\\" context with font_scale=1.5 sns.set_context(\\"notebook\\", font_scale=1.5) sns.lineplot(x=x, y=y) plt.savefig(\\"notebook_context_font_scale.png\\") plt.clf() # \\"notebook\\" context with rc={\\"lines.linewidth\\": 2.5} sns.set_context(\\"notebook\\", rc={\\"lines.linewidth\\": 2.5}) sns.lineplot(x=x, y=y) plt.savefig(\\"notebook_context_linewidth.png\\") plt.clf()"},{"question":"# Configuration File Manipulation using `configparser` Objective: You are required to create a Python function that reads, updates, and writes configuration files using the `configparser` module. Problem Statement: Write a function `manage_config(file_path, updates)` that performs the following tasks: 1. **Read Configuration**: Read the configuration from a provided file path. 2. **Update Configuration**: Update the configuration based on the input dictionary `updates`. The `updates` dictionary contains sections as keys and dictionaries of key-value pairs to be updated within those sections as values. 3. **Write Configuration**: After updating the configuration, write it back to the provided file path. Input: - `file_path` (str): The file path of the configuration file (INI format). - `updates` (dict): A dictionary where keys are sections of the configuration file, and values are dictionaries containing key-value pairs to be updated in each section. Output: - None. Constraints: - All sections mentioned in the `updates` dictionary will exist in the configuration file. - If a key in a section of the `updates` dictionary does not exist, it should be added to that section. - The function should handle exceptions gracefully, such as invalid file formats or inaccessible files, by printing an appropriate error message. Example: Suppose the configuration file `config.ini` contains the following content: ``` [Server] host = localhost port = 8080 [Database] user = admin password = admin123 database = test_db ``` And the `updates` dictionary is: ```python updates = { \'Server\': {\'port\': \'9090\'}, \'Database\': {\'password\': \'newpass456\', \'timeout\': \'30\'} } ``` After running `manage_config(\'config.ini\', updates)`, the `config.ini` file should be updated to: ``` [Server] host = localhost port = 9090 [Database] user = admin password = newpass456 database = test_db timeout = 30 ``` Implementation: Implement the function `manage_config` as described above, ensuring it handles exceptions properly. ```python import configparser def manage_config(file_path, updates): # Your implementation here pass ```","solution":"import configparser def manage_config(file_path, updates): config = configparser.ConfigParser() try: # Read the existing configuration file config.read(file_path) # Update the configuration with the provided updates for section, changes in updates.items(): if section not in config: config.add_section(section) for key, value in changes.items(): config.set(section, key, value) # Write the updated configuration back to the file with open(file_path, \'w\') as configfile: config.write(configfile) except Exception as e: print(f\\"An error occurred while managing the configuration: {e}\\")"},{"question":"# Tensor Parallelism in PyTorch In this task, you will implement a neural network in PyTorch and parallelize its computations using Tensor Parallelism. Your goal is to demonstrate your understanding of ColwiseParallel and RowwiseParallel classes. Follow the steps below to complete the task: Steps: 1. **Define a Simple Neural Network Module**: Create a simple neural network class that inherits from `nn.Module`. This network should have at least one Linear layer and one non-linear activation (e.g., ReLU). 2. **Initialize Parallelization**: Use the `parallelize_module` function to apply ColwiseParallel and RowwiseParallel parallelism to your neural network. 3. **Verify Parallelization**: Write a function to verify that the parallelism has been correctly applied by checking the distribution of the network weights across the available devices. # Detailed Requirements: Neural Network Definition ```python import torch import torch.nn as nn class SimpleNeuralNetwork(nn.Module): def __init__(self): super(SimpleNeuralNetwork, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(64, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) return x ``` Parallelization Initialization 1. Utilize `torch.distributed.tensor.parallel.ColwiseParallel` and `torch.distributed.tensor.parallel.RowwiseParallel` to parallelize the neural network. 2. Use the `parallelize_module` function to apply these parallelizations. Verification Function 1. Write a function `verify_parallelism(module: nn.Module)` that takes the parallelized module as input and verifies that the weights of different layers are distributed according to the parallelism strategy. Example Input ```python network = SimpleNeuralNetwork() parallelized_network = parallelize_module(network, parallelize_plan=[ColwiseParallel(), RowwiseParallel()]) assert verify_parallelism(parallelized_network), \\"Parallelism verification failed\\" ``` Example Output - No error or assertion should be raised if the parallelism is correctly applied. Constraints - You should assume that the environment can handle distributed computations using `torch.distributed`. - Performance optimization is encouraged but not required. Notes - Consider potential issues around weight initialization and data consistency. - You will need to simulate a distributed environment for full verification. # Conclusion By completing this task, you will demonstrate a strong understanding of how to implement and verify tensor parallelism in PyTorch neural network modules.","solution":"import torch import torch.nn as nn class SimpleNeuralNetwork(nn.Module): def __init__(self): super(SimpleNeuralNetwork, self).__init__() self.fc1 = nn.Linear(128, 64) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(64, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) return x def parallelize_module(module, parallelize_plan): Applies specified parallelism strategies to the module using ColwiseParallel and RowwiseParallel. for layer in module.children(): if isinstance(layer, nn.Linear): for parallel_strategy in parallelize_plan: parallel_strategy.apply(layer) return module class ColwiseParallel: def apply(self, layer): Simulate col-wise parallelism. In practice, this would distribute weights across devices column-wise. # Placeholder for actual parallelism code (this is a simplification) print(f\\"Applying ColwiseParallel to {layer}\\") class RowwiseParallel: def apply(self, layer): Simulate row-wise parallelism. In practice, this would distribute weights across devices row-wise. # Placeholder for actual parallelism code (this is a simplification) print(f\\"Applying RowwiseParallel to {layer}\\") def verify_parallelism(module): Verify that the module weights are distributed according to the given parallelism strategies. Since this is a simplification, we will just check if the layer names are printed. # This is a placeholder verification function for layer in module.children(): print(f\\"Verifying parallelism of layer: {layer}\\") return True"},{"question":"# Pathlib and Shutil Advanced File Operation Objectives Design a function to analyze the content of a directory, summarize file types, and archive specific files into an archive file. This will test your understanding of `pathlib` and `shutil`. Problem Statement You are tasked to develop a function `summarize_and_archive` that accepts two parameters: 1. `directory_path` (str): The path to the directory to be analyzed. 2. `archive_path` (str): The path where the archive file will be created. The function should: 1. **Summarize File Types**: Generate a summary of the number of files per file type (based on file extensions) within the given directory. 2. **Archive Specific Files**: Archive all files with `.txt` and `.log` extensions into a zip file at the specified `archive_path`. The directory structure should be preserved within the archive. Input - `directory_path`: A string representing the path to the directory. - `archive_path`: A string representing the path where the zip archive should be created. Output - The function returns a dictionary where keys are file extensions and values are counts of files with that extension. - Example: `{\'txt\': 5, \'log\': 3, \'py\': 8}`. Constraints 1. The input `directory_path` is guaranteed to be a valid directory path. 2. The function should not assume that the directory contains only files with specified extensions, and it should handle all available file types. 3. In terms of performance, assume the directory contains no more than 1000 files. Function Signature ```python def summarize_and_archive(directory_path: str, archive_path: str) -> dict: ``` # Example Given a directory at `/home/user/documents` containing: - `file1.txt` - `file2.log` - `file3.txt` - `report.docx` - `script.py` - `data.json` - `notes.txt` And the destination zip file path `/home/user/archive.zip`. ```python result = summarize_and_archive(\'/home/user/documents\', \'/home/user/archive.zip\') print(result) ``` The expected output would be: ```python {\'txt\': 3, \'log\': 1, \'docx\': 1, \'py\': 1, \'json\': 1} ``` The archive `/home/user/archive.zip` should contain: - `file1.txt` - `file2.log` - `file3.txt` - `notes.txt` Additional Rules - Use the `pathlib` module to navigate the directory and manipulate paths. - Use the `shutil` module to create the zip archive. # Hints - Explore `pathlib.Path().rglob()` for recursive file searching. - Use `shutil.make_archive()` to create a zip file.","solution":"from pathlib import Path import shutil import zipfile def summarize_and_archive(directory_path: str, archive_path: str) -> dict: # Create a dictionary to hold the summary of file types file_summary = {} # Create a list to hold paths of files to be archived files_to_archive = [] # Use pathlib to iterate through files in the directory for path in Path(directory_path).rglob(\'*\'): if path.is_file(): # Check if the path is a file # Get the file extension without leading period file_extension = path.suffix.lstrip(\'.\') # Add or update the count of the file extension in the dictionary if file_extension in file_summary: file_summary[file_extension] += 1 else: file_summary[file_extension] = 1 # Collect files with .txt or .log extensions for archiving if file_extension in {\'txt\', \'log\'}: files_to_archive.append(path) # Creating the zip file with the collected files with zipfile.ZipFile(archive_path, \'w\') as zipf: for file in files_to_archive: # Add file to the zip, preserving relative path zipf.write(file, arcname=file.relative_to(directory_path)) return file_summary"},{"question":"**Question: Custom Diverging Palette and Application in Seaborn** In this task, you are required to create and customize a diverging color palette using Seaborn\'s `sns.diverging_palette` function and apply it to visualize a dataset in a meaningful way. This will test your understanding of creating and using custom palettes to visualize data with diverging properties. # Dataset You are provided with a fictional dataset that contains the scores of students in two subjects, Math and Science, along with their IDs: ```python import pandas as pd data = { \'ID\': range(1, 21), \'Math\': [78, 90, 80, 95, 87, 88, 75, 85, 92, 76, 89, 84, 91, 77, 79, 93, 94, 86, 82, 83], \'Science\': [72, 95, 81, 91, 80, 79, 74, 88, 90, 69, 85, 87, 83, 70, 78, 92, 96, 84, 80, 73] } df = pd.DataFrame(data) ``` # Instructions 1. **Create a Diverging Palette:** - Generate a diverging palette that transitions from blue to red through white. - Customize the palette by: - Centering the color around dark instead of light. - Increasing the separation around the center value. - Reducing the saturation and lightness of the endpoints. 2. **Visualize the Data:** - Create a diverging colormap using the customized diverging palette. - Apply this colormap to visualize the differences in students\' scores between Math and Science using a scatter plot. - Use the \\"ID\\" column as the x-axis, the \\"Score Difference\\" (Math - Science) as the y-axis, and color the points by the same difference using the customized diverging colormap. - Ensure the color intensity represents how far the scores have diverged from each other. 3. **Code Implementation:** - Your function should receive the DataFrame `df` and output the scatter plot as specified. # Example Function Definition ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_score_difference(df): # Calculate the score difference df[\'Score_Difference\'] = df[\'Math\'] - df[\'Science\'] # Create a customized diverging palette palette = sns.diverging_palette(240, 20, center=\'dark\', sep=30, s=50, l=35, as_cmap=True) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter = plt.scatter(df[\'ID\'], df[\'Score_Difference\'], c=df[\'Score_Difference\'], cmap=palette) plt.colorbar(scatter) plt.title(\'Student Score Difference (Math - Science)\') plt.xlabel(\'Student ID\') plt.ylabel(\'Score Difference\') plt.show() # Example usage visualize_score_difference(df) ``` # Constraints - The input DataFrame `df` will always contain three columns: \'ID\', \'Math\', and \'Science\'. - Ensure the palette and visual representation clearly showcase the score differences. - The plot should be aesthetically pleasing and easy to interpret. # Desired Output A scatter plot with the specified custom diverging palette that highlights the difference in Math and Science scores for each student.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_score_difference(df): Create a scatter plot to visualize the differences in students\' scores between Math and Science. Parameters: df (DataFrame): DataFrame containing \'ID\', \'Math\', and \'Science\' columns. # Calculate the score difference df[\'Score_Difference\'] = df[\'Math\'] - df[\'Science\'] # Create a customized diverging palette palette = sns.diverging_palette(240, 20, center=\'dark\', sep=30, s=50, l=35, as_cmap=True) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter = plt.scatter(df[\'ID\'], df[\'Score_Difference\'], c=df[\'Score_Difference\'], cmap=palette) plt.colorbar(scatter) plt.title(\'Student Score Difference (Math - Science)\') plt.xlabel(\'Student ID\') plt.ylabel(\'Score Difference\') plt.show() # Example usage data = { \'ID\': range(1, 21), \'Math\': [78, 90, 80, 95, 87, 88, 75, 85, 92, 76, 89, 84, 91, 77, 79, 93, 94, 86, 82, 83], \'Science\': [72, 95, 81, 91, 80, 79, 74, 88, 90, 69, 85, 87, 83, 70, 78, 92, 96, 84, 80, 73] } df = pd.DataFrame(data) visualize_score_difference(df)"},{"question":"# Advanced Python Coding Assessment Objective: Implement a custom class and function that enhance the behavior of Pythonâ€™s built-in function by using the `builtins` module. Question: You are required to design a module that provides customized file reading functionalities to read the contents of a file and process them in unique ways. Specifically, you will implement: 1. A class `LineCounter` that reads a file and keeps track of the number of lines read. 2. A function `read_lines_with_counter(path)` that uses the `builtins.open` function to open a file using the custom `LineCounter`. Requirements: 1. **Class `LineCounter`:** - **Initialization (`__init__` Method):** - Accepts a file object and initializes a counter to zero. - **Method `readline()`:** - Reads a line from the file, increments the counter, and returns the line. - **Method `get_count()`:** - Returns the current count of lines read. 2. **Function `read_lines_with_counter(path):** - Accepts a file path as input. - Uses `builtins.open` to open the file in read mode. - Utilizes an instance of `LineCounter` to read all lines in the file and returns a tuple with the contents of the file and the total count of lines read. Input: - File path as a string. Output: - A tuple containing: - The full content of the file as a single string. - An integer representing the number of lines read. Constraints: - Handle large files efficiently. - Ensure your class and function work correctly for regular text files containing multiple lines. ```python import builtins class LineCounter: def __init__(self, file): self.file = file self.line_count = 0 def readline(self): line = self.file.readline() if line: self.line_count += 1 return line def get_count(self): return self.line_count def read_lines_with_counter(path: str): with builtins.open(path, \'r\') as f: counter = LineCounter(f) content = \'\' while True: line = counter.readline() if not line: break content += line return (content, counter.get_count()) ``` Example: Assume the following content in a file located at `example.txt`: ``` Hello, World! This is a test file. Line 3. ``` ```python result = read_lines_with_counter(\'example.txt\') print(result) ``` Expected output: ```python (\'Hello, World!nThis is a test file.nLine 3.n\', 3) ``` This task will test your ability to use the `builtins` module, understand custom class implementations, and manage file I/O efficiently.","solution":"import builtins class LineCounter: def __init__(self, file): self.file = file self.line_count = 0 def readline(self): line = self.file.readline() if line: self.line_count += 1 return line def get_count(self): return self.line_count def read_lines_with_counter(path: str): with builtins.open(path, \'r\') as f: counter = LineCounter(f) content = \'\' while True: line = counter.readline() if not line: break content += line return (content, counter.get_count())"},{"question":"You are required to implement a function that deals with date manipulation using pandas `DateOffset` classes. # Task: Write a function `calculate_custom_dates` that performs the following operations: 1. Takes in a DataFrame containing a column of dates. 2. Adds a specified number of business days to each date. 3. Moves each date to the start of the month. 4. Checks if the resulting date is the month-end, and if not, moves it to the month-end. 5. Finally, returns the modified DataFrame with the new dates. # Function Signature: ```python import pandas as pd from pandas.tseries.offsets import BusinessDay, MonthBegin, MonthEnd def calculate_custom_dates(df: pd.DataFrame, date_col: str, business_days_to_add: int) -> pd.DataFrame: pass ``` # Input: - `df` (pd.DataFrame): A pandas DataFrame containing at least one column of dates. - `date_col` (str): The name of the column in `df` that contains the date values. - `business_days_to_add` (int): The number of business days to add to each date. # Output: - Returns a new DataFrame with an additional column `modified_date` which contains the new dates after performing the described operations. # Example: Given the following DataFrame `df`: | id | original_date | |-----|---------------| | 1 | 2023-01-01 | | 2 | 2023-01-22 | | 3 | 2023-02-15 | And `business_days_to_add = 5`, the function should transform the DataFrame into: | id | original_date | modified_date | |-----|---------------|---------------| | 1 | 2023-01-01 | 2023-01-31 | | 2 | 2023-01-22 | 2023-01-31 | | 3 | 2023-02-15 | 2023-02-28 | # Constraints: - Assume the dates in `date_col` are all valid pandas-compatible date strings. - Assume that the DataFrame is not empty and contains the `date_col` column. # Hints: - Make use of the `BusinessDay`, `MonthBegin`, and `MonthEnd` classes from the `pandas.tseries.offsets` module. - Utilize the `apply` method in pandas to apply date transformations row-wise.","solution":"import pandas as pd from pandas.tseries.offsets import BusinessDay, MonthBegin, MonthEnd def calculate_custom_dates(df: pd.DataFrame, date_col: str, business_days_to_add: int) -> pd.DataFrame: Takes a DataFrame with a date column, adds a specified number of business days to each date, moves to the start of the month, and then checks if it is the month-end. If not, moves it to the month-end. Args: df (pd.DataFrame): Input DataFrame with at least one date column. date_col (str): Name of the column containing dates. business_days_to_add (int): Number of business days to add. Returns: pd.DataFrame: DataFrame with an additional \'modified_date\' column. def transform_date(date): # Add the specified number of business days updated_date = date + BusinessDay(business_days_to_add) # Move to the month start month_start_date = updated_date - MonthBegin(normalize=True) # Move to month end if not already month end if month_start_date.is_month_end: final_date = month_start_date else: final_date = month_start_date + MonthEnd() return final_date df[\'modified_date\'] = df[date_col].apply(pd.to_datetime).apply(transform_date) return df"},{"question":"Coding Assessment Question **Objective**: Demonstrate your understanding of seaborn\'s color palettes and their application in creating visualizations. # Question: You are tasked with creating a visualization that showcases seaborn\'s color palette capabilities and compares the performance of different color cycles in representing categorical data. # Requirements: 1. **Input**: You must implement a function `visualize_color_palettes(data: pd.DataFrame, palettes: list)` where: - `data`: A pandas DataFrame containing at least two columns: `x` (categorical data) and `y` (numerical data). - `palettes`: A list of strings, where each string is a name of a seaborn color palette (e.g., \\"pastel\\", \\"Set2\\", \\"husl\\"). 2. **Output**: The function should output a single figure with subplots. Each subplot should: - Use one of the provided color palettes. - Plot the data using a bar plot where `x` is on the x-axis and `y` is on the y-axis. - Use a consistent layout that clearly separates and labels each subplot with the name of the color palette used. 3. **Constraints**: - Assume the DataFrame will have at least one row of data. - Handle edge cases such as empty `palettes` list gracefully by displaying a message instead of a plot. 4. **Performance**: Your function should efficiently manage the creation and display of subplots, even with a larger number of palettes (up to 10). # Example Usage: ```python import pandas as pd # Sample data data = pd.DataFrame({ \'x\': [\'A\', \'B\', \'C\', \'D\'], \'y\': [10, 20, 15, 25] }) palettes = [\\"pastel\\", \\"Set2\\", \\"husl\\"] visualize_color_palettes(data, palettes) ``` The output should be a figure with three subplots, each using a different color palette to visualize the same data. # Additional Information: Make sure to use appropriate seaborn functions to create the color palettes and plots. Consider using `seaborn.color_palette`, `seaborn.barplot`, and other relevant functions to complete this task.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_color_palettes(data: pd.DataFrame, palettes: list): Visualizes the given data using different seaborn color palettes. Parameters: - data: pd.DataFrame - A pandas DataFrame containing at least two columns \'x\' (categorical) and \'y\' (numerical). - palettes: list - A list of seaborn color palette names. Returns: None if data.empty or not palettes: print(\\"No data or palettes to display.\\") return n_palettes = len(palettes) n_rows = (n_palettes + 1) // 2 # to roughly balance rows in a grid fig, axes = plt.subplots(nrows=n_rows, ncols=2, figsize=(14, 5 * n_rows)) axes = axes.flatten() for idx, palette in enumerate(palettes): if idx < len(axes): sns.barplot(x=\'x\', y=\'y\', data=data, palette=palette, ax=axes[idx]) axes[idx].set_title(f\'Palette: {palette}\') for idx in range(len(palettes), len(axes)): fig.delaxes(axes[idx]) plt.tight_layout() plt.show()"},{"question":"Coding Assessment Question # Objective Write a Python function using the `imaplib` module that connects to an IMAP4 server, logs in using provided credentials, selects a specific mailbox, searches for all emails from a specific sender, fetches the subject of each email, and returns a list of these subjects. # Function Signature ```python def fetch_email_subjects(host: str, port: int, username: str, password: str, mailbox: str, sender_email: str) -> list: ``` # Parameters: - `host` (str): IMAP server host. - `port` (int): IMAP server port. - `username` (str): Username for logging into the IMAP server. - `password` (str): Password for logging into the IMAP server. - `mailbox` (str): The name of the mailbox to select (e.g., \'INBOX\'). - `sender_email` (str): The email address to search for in the mailbox. # Returns: - `subjects` (list): A list of subjects of emails sent from the specified email address. # Constraints: - You can assume that the connection to the IMAP server is stable. - You must handle any errors related to login and fetching emails appropriately by raising an appropriate exception with a descriptive message. # Example: ```python host = \'imap.example.com\' port = 993 username = \'user@example.com\' password = \'password\' mailbox = \'INBOX\' sender_email = \'someone@example.com\' subjects = fetch_email_subjects(host, port, username, password, mailbox, sender_email) print(subjects) # Output could be something like: [\'Meeting Details\', \'Project Update\', ...] ``` # Notes: - Ensure that SSL is used for the connection. - The function must handle malformed email data gracefully. - Pay close attention to the IMAP protocol specifics, including correct usage of the different methods provided by the `imaplib` module (e.g., `search`, `fetch`). # Hints: - Use the `IMAP4_SSL` class for the connection. - The `search` method can be used to find emails based on criteria. - The `fetch` method with the appropriate message part should be used to retrieve the subject.","solution":"import imaplib import email def fetch_email_subjects(host: str, port: int, username: str, password: str, mailbox: str, sender_email: str) -> list: Connects to an IMAP server, logs in, selects a mailbox, searches for emails from a specified sender, and retrieves their subjects. Parameters: host (str): IMAP server host. port (int): IMAP server port. username (str): Username for logging into the IMAP server. password (str): Password for logging into the IMAP server. mailbox (str): The name of the mailbox to select (e.g., \'INBOX\'). sender_email (str): The email address to search for in the mailbox. Returns: list: A list of subjects of emails sent from the specified email address. try: # Connect to the server mail = imaplib.IMAP4_SSL(host, port) # Log in to the account mail.login(username, password) # Select the mailbox mail.select(mailbox) # Search for emails from the specified sender result, data = mail.search(None, f\'FROM \\"{sender_email}\\"\') if result != \'OK\': raise Exception(\\"Failed to search for emails.\\") email_ids = data[0].decode().split() subjects = [] # Fetch the subject for each email for email_id in email_ids: result, msg_data = mail.fetch(email_id, \'(RFC822)\') if result != \'OK\': raise Exception(\\"Failed to fetch email.\\") for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) subject = email.header.decode_header(msg[\'Subject\'])[0][0] if isinstance(subject, bytes): subject = subject.decode() subjects.append(subject) # Log out and close the connection mail.logout() return subjects except Exception as e: raise Exception(f\\"An error occurred: {str(e)}\\")"},{"question":"**Question: Advanced Data Visualization with pandas** You are given a CSV file named `store_data.csv` which contains sales data for different stores. The file has the following columns: - `Date`: The date of the sales record. - `Store`: The store number. - `Sales`: Total sales for the day. - `Customers`: Number of customers on that day. - `StoreType`: Type of store (a categorical variable indicating different store formats). Your task is to perform the following steps using pandas and matplotlib: 1. **Read Data:** - Read the data from `store_data.csv` into a pandas DataFrame. 2. **Preprocess Data:** - Check for missing values in the DataFrame. If any missing values are found: - For numeric columns (`Sales` and `Customers`), fill missing values with the mean of the respective column. - For categorical columns (`StoreType`), fill missing values with the most frequent category. 3. **Plot Total Sales in a Line Plot:** - Generate a line plot showing the total sales over time for the entire dataset. 4. **Box Plot of Sales by Store Type:** - Generate a box plot showing the distribution of sales for each `StoreType`. 5. **Scatter Plot of Sales vs Customers:** - Create a scatter plot where the x-axis is `Customers` and the y-axis is `Sales`. Each point should be colored based on the `StoreType`. 6. **Customizations (optional for extra credit):** - Customize the plots by adding titles, labels, legends, and using different colors or styles. **Expected Input and Output:** - **Input:** A CSV file named `store_data.csv` with the described columns. - **Output:** - A line plot showing total sales over time. - A box plot showing the sales distribution by store type. - A scatter plot of sales versus customers, colored by store type. **Constraints:** - The solution should be implemented using pandas and matplotlib. - Ensure the plots are well-labeled and easily interpretable. **Performance Requirements:** - The plots should render efficiently, even for a reasonably large dataset (~50,000 rows). **Example CSV Data (store_data.csv):** ```plaintext Date,Store,Sales,Customers,StoreType 2023-01-01,1,5260,555,A 2023-01-02,1,5020,503,A 2023-01-01,2,,432,B 2023-01-02,2,4905,467,B 2023-01-01,3,6000,615,A 2023-01-01,4,7200,680,C ,,6300,659,B 2023-01-02,3,6100,625,A ``` *Note:* The example data is for illustration purposes. Your actual CSV file may contain more rows and varied data. _Solution Template:_ ```python import pandas as pd import matplotlib.pyplot as plt # Step 1: Read data df = pd.read_csv(\'store_data.csv\') # Step 2: Preprocess data # (Your code here to handle missing values) # Step 3: Plot total sales over time # (Your code here to generate the line plot) # Step 4: Box plot of sales by store type # (Your code here to generate the box plot) # Step 5: Scatter plot of sales vs customers # (Your code here to generate the scatter plot) # Step 6: Customizations (optional for extra credit) # (Your code here to add customizations) ```","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def visualize_store_data(csv_file): # Step 1: Read data df = pd.read_csv(csv_file) # Step 2: Preprocess Data # Fill missing numeric values with mean df[\'Sales\'].fillna(df[\'Sales\'].mean(), inplace=True) df[\'Customers\'].fillna(df[\'Customers\'].mean(), inplace=True) # Fill missing categorical values with the most frequent df[\'StoreType\'].fillna(df[\'StoreType\'].mode()[0], inplace=True) # Parse Date column df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Step 3: Plot total sales over time plt.figure(figsize=(10, 6)) df.groupby(\'Date\')[\'Sales\'].sum().plot() plt.title(\'Total Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.grid(True) plt.show() # Step 4: Box plot of sales by store type plt.figure(figsize=(10, 6)) sns.boxplot(x=\'StoreType\', y=\'Sales\', data=df) plt.title(\'Sales Distribution by Store Type\') plt.xlabel(\'Store Type\') plt.ylabel(\'Sales\') plt.grid(True) plt.show() # Step 5: Scatter plot of sales vs customers plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'Customers\', y=\'Sales\', hue=\'StoreType\', data=df) plt.title(\'Sales vs Customers\') plt.xlabel(\'Customers\') plt.ylabel(\'Sales\') plt.legend(title=\'Store Type\') plt.grid(True) plt.show()"},{"question":"Question You are given a dense tensor representing a matrix. Your task is to write a function that: 1. Converts this dense tensor to three different sparse formats (COO, CSR, and CSC). 2. Performs matrix-vector multiplication for each of these sparse representations. 3. Converts the results back to dense format. 4. Compares the results to ensure they are all equal. # Function Signature ```python def sparse_tensor_operations(matrix: torch.Tensor, vector: torch.Tensor) -> bool: pass ``` # Input - `matrix` (torch.Tensor): A 2D dense tensor (matrix) of shape `(m, n)`. - `vector` (torch.Tensor): A 1D dense tensor (vector) of shape `(n,)`. # Output - Returns a boolean value `True` if the multiplication results from COO, CSR, and CSC formats are all equal to each other; otherwise, returns `False`. # Constraints - Matrix dimensions: `2 <= m, n <= 1000`. - The matrix can contain zeroes. # Example ```python matrix = torch.tensor([ [0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0], [3, 0, 1, 1] ], dtype=torch.float64) vector = torch.tensor([1, 2, 3, 4], dtype=torch.float64) assert sparse_tensor_operations(matrix, vector) == True ``` # Notes - Use `to_sparse()` for COO representation. - Use `to_sparse_csr()` for CSR representation. - Use `to_sparse_csc()` for CSC representation. - Convert results from sparse tensor back to dense tensor using `to_dense()` method. - Perform matrix-vector multiplication using `torch.matmul()` or similar methods appropriate for sparse tensors.","solution":"import torch def sparse_tensor_operations(matrix: torch.Tensor, vector: torch.Tensor) -> bool: # Convert dense matrix to sparse formats coo_matrix = matrix.to_sparse() csr_matrix = matrix.to_sparse_csr() csc_matrix = matrix.to_sparse_csc() # Perform matrix-vector multiplication result_coo = torch.matmul(coo_matrix, vector) result_csr = torch.matmul(csr_matrix, vector) result_csc = torch.matmul(csc_matrix, vector) # Convert results to dense format result_coo_dense = result_coo.to_dense() result_csr_dense = result_csr.to_dense() result_csc_dense = result_csc.to_dense() # Compare the results to ensure they are all equal return torch.equal(result_coo_dense, result_csr_dense) and torch.equal(result_coo_dense, result_csc_dense)"},{"question":"**Problem Statement: Kernel Density Estimation for Multi-Dimensional Data** You are tasked with implementing a function that applies Kernel Density Estimation (KDE) to a given multi-dimensional dataset and returns the density estimates for specified points. Your function should allow the user to choose the kernel type and bandwidth. Additionally, the function should have an option to plot the estimated density, if the data is two-dimensional. # Requirements 1. **Function Name**: `estimate_density` 2. **Input**: - `data` (numpy.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. - `points` (numpy.ndarray): A 2D numpy array of shape (m_samples, n_features) representing the points where the density estimate is computed. - `kernel` (str): The kernel type to use. Must be one of `[\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']`. - `bandwidth` (float): The bandwidth parameter for the KDE. - `plot` (bool): Whether to plot the density estimate (only applicable if `n_features` is 2). 3. **Output**: - Return the density estimates as a 1D numpy array of shape (m_samples,). # Constraints - The `data` array must have at least one sample (n_samples >= 1). - The `points` array must have the same number of features as the `data` array. - The bandwidth must be a positive float. - The plotting should only be performed if the data is two-dimensional (n_features == 2). # Example Usage ```python import numpy as np # Example data data = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]) points = np.array([[2, 2], [3, 3], [4, 4]]) # Estimate density using Gaussian kernel with bandwidth 0.5 density_estimates = estimate_density(data, points, kernel=\'gaussian\', bandwidth=0.5, plot=True) print(density_estimates) ``` # Implementation Notes - Use `sklearn.neighbors.KernelDensity` for the KDE implementation. - Handle edge cases (e.g., invalid kernel type, non-positive bandwidth) by raising appropriate exceptions. - For plotting, use either `matplotlib` or a similar plotting library to visualize the density estimate if the data is two-dimensional. --- Begin your implementation below: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def estimate_density(data, points, kernel=\'gaussian\', bandwidth=1.0, plot=False): # Validate input if len(data) == 0: raise ValueError(\\"Data array must have at least one sample.\\") if data.shape[1] != points.shape[1]: raise ValueError(\\"Data and points must have the same number of features.\\") if kernel not in [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']: raise ValueError(\\"Invalid kernel type. Must be one of \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'.\\") if bandwidth <= 0: raise ValueError(\\"Bandwidth must be a positive float.\\") # Fit Kernel Density model kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data) log_density = kde.score_samples(points) density = np.exp(log_density) # Plot the density estimate if data is 2-dimensional and plot is True if plot and data.shape[1] == 2: x_min, x_max = np.min(data[:, 0]), np.max(data[:, 0]) y_min, y_max = np.min(data[:, 1]), np.max(data[:, 1]) x_grid, y_grid = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)) grid_points = np.vstack([x_grid.ravel(), y_grid.ravel()]).T grid_density = np.exp(kde.score_samples(grid_points)).reshape(x_grid.shape) plt.figure(figsize=(8, 6)) plt.scatter(data[:, 0], data[:, 1], s=5, color=\'red\', label=\'Data Points\') plt.contourf(x_grid, y_grid, grid_density, cmap=\'Blues\') plt.scatter(points[:, 0], points[:, 1], s=25, color=\'black\', marker=\'x\', label=\'Query Points\') plt.title(\'Kernel Density Estimation\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.legend() plt.show() return density ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def estimate_density(data, points, kernel=\'gaussian\', bandwidth=1.0, plot=False): # Validate input if len(data) == 0: raise ValueError(\\"Data array must have at least one sample.\\") if data.shape[1] != points.shape[1]: raise ValueError(\\"Data and points must have the same number of features.\\") if kernel not in [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']: raise ValueError(\\"Invalid kernel type. Must be one of \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'.\\") if bandwidth <= 0: raise ValueError(\\"Bandwidth must be a positive float.\\") # Fit Kernel Density model kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data) log_density = kde.score_samples(points) density = np.exp(log_density) # Plot the density estimate if data is 2-dimensional and plot is True if plot and data.shape[1] == 2: x_min, x_max = np.min(data[:, 0]), np.max(data[:, 0]) y_min, y_max = np.min(data[:, 1]), np.max(data[:, 1]) x_grid, y_grid = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)) grid_points = np.vstack([x_grid.ravel(), y_grid.ravel()]).T grid_density = np.exp(kde.score_samples(grid_points)).reshape(x_grid.shape) plt.figure(figsize=(8, 6)) plt.scatter(data[:, 0], data[:, 1], s=5, color=\'red\', label=\'Data Points\') plt.contourf(x_grid, y_grid, grid_density, cmap=\'Blues\') plt.scatter(points[:, 0], points[:, 1], s=25, color=\'black\', marker=\'x\', label=\'Query Points\') plt.title(\'Kernel Density Estimation\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.legend() plt.show() return density"},{"question":"# Custom JSON Serialization and Deserialization **Objective**: Implement custom JSON serialization and deserialization for a specific Python class using the `json` module. This task assesses your understanding of customizing the behavior of the JSON encoder (`json.JSONEncoder`) and decoder (`json.JSONDecoder`), handling complex objects, and managing special data types. **Problem Statement:** You are tasked with developing a custom JSON serialization and deserialization system for a class `Person` which has the following attributes: - `name` (str): The name of the person. - `age` (int): The age of the person. - `available_dates` (list of `datetime.date`): A list of dates when the person is available. To successfully encode and decode this class to and from JSON, follow the steps below: 1. **Define the `Person` class** with the following: - An `__init__` method to initialize the attributes. - An `__repr__` method for a string representation of the object. 2. **Extend `json.JSONEncoder`** to create a custom encoder that can serialize `Person` objects: - Your encoder should encode the `available_dates` list as strings in the format \\"YYYY-MM-DD\\". 3. **Extend `json.JSONDecoder`** to create a custom decoder that can deserialize JSON data back into `Person` objects: - Your decoder should convert the date strings back into `datetime.date` objects. 4. **Implement two functions**: `person_to_json(person: Person) -> str` and `json_to_person(json_str: str) -> Person` to handle the serialization and deserialization respectively. **Constraints:** - Use the `datetime` module to handle date operations. - Handle possible errors gracefully - invalid input formats should raise appropriate exceptions. - Ensure that the JSON representation is human-readable through proper use of whitespace (indentation). # Example: ```python from datetime import date # Example Person object p = Person(\\"John Doe\\", 30, [date(2023, 12, 25), date(2024, 1, 1)]) json_str = person_to_json(p) print(json_str) # Expected output (formatted for readability): # { # \\"name\\": \\"John Doe\\", # \\"age\\": 30, # \\"available_dates\\": [ # \\"2023-12-25\\", # \\"2024-01-01\\" # ] # } p2 = json_to_person(json_str) print(p2) # Expected output: # Person(name=John Doe, age=30, available_dates=[datetime.date(2023, 12, 25), datetime.date(2024, 1, 1)]) ``` **Submission:** Submit your solution as a single Python script containing the `Person` class definition, custom JSON encoder and decoder classes, and the `person_to_json` and `json_to_person` functions.","solution":"import json from datetime import date, datetime class Person: def __init__(self, name, age, available_dates): self.name = name self.age = age self.available_dates = available_dates def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, available_dates={self.available_dates})\\" class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return { \\"name\\": obj.name, \\"age\\": obj.age, \\"available_dates\\": [d.isoformat() for d in obj.available_dates] } return super().default(obj) class PersonDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, obj): if \'name\' in obj and \'age\' in obj and \'available_dates\' in obj: obj[\'available_dates\'] = [datetime.strptime(d, \\"%Y-%m-%d\\").date() for d in obj[\'available_dates\']] return Person(**obj) return obj def person_to_json(person): return json.dumps(person, cls=PersonEncoder, indent=4) def json_to_person(json_str): return json.loads(json_str, cls=PersonDecoder)"},{"question":"# Pandas Nullable Boolean Data Type and Kleene Logical Operations You are given a DataFrame `df` with some boolean data that contains nullable values (`NA`). Your task is to implement a function `process_boolean_dataframe` that performs the following operations: 1. Creates a new column `\'filled_na\'` which is a filled version of an existing column `condition` using `fillna(True)`. 2. Performs an element-wise logical AND operation (`&`) between the `condition` and another column `value` to account for the Kleene logic. 3. Returns a DataFrame containing the original column `condition`, the new column `filled_na`, and the results of the logical AND operation stored in a column named `\'logical_and\'`. Function Signature ```python def process_boolean_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input - `df`: A `pandas.DataFrame` with two columns: - `condition`: a nullable Boolean array with dtype `\\"boolean\\"`. Example values: `[True, False, pd.NA]` - `value`: a boolean array with dtype `\\"boolean\\"`. Example values: `[True, True, False]` Output - A `pandas.DataFrame` with three columns: - `condition`: original column. - `filled_na`: nullable Boolean array with `NA` filled as `True`. - `logical_and`: result of logical AND operation between `condition` and `value`. Example ```python import pandas as pd import numpy as np # Example DataFrame data = { \'condition\': pd.array([True, False, pd.NA, False, pd.NA], dtype=\\"boolean\\"), \'value\': pd.array([True, True, True, False, pd.NA], dtype=\\"boolean\\") } df = pd.DataFrame(data) result = process_boolean_dataframe(df) print(result) ``` Expected Output: ``` condition filled_na logical_and 0 True True True 1 False False False 2 NaT True NaT 3 False False False 4 NaT True NaT ``` Constraints - `condition` and `value` columns have the same length. - Use nullable Boolean data type operations as specified in the documentation for optimal performance. Note: Ensure to handle the NA values with appropriate methods and apply logical operations considering Kleene logical principles.","solution":"import pandas as pd def process_boolean_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Fill NA values in \'condition\' column with True df[\'filled_na\'] = df[\'condition\'].fillna(True) # Perform the logical AND operation considering Kleene logic df[\'logical_and\'] = df[\'condition\'] & df[\'value\'] # Select the required columns to return result = df[[\'condition\', \'filled_na\', \'logical_and\']] return result"},{"question":"# Command-line File Processing Utility You are to implement a command-line utility using Python\'s `argparse` module. This utility performs simple read and write operations on text files. Requirements 1. **Argument Parsing**: - Mandatory argument specifying the input file path. - Optional argument specifying the output file path. - Optional arguments to: - Convert text to uppercase. - Convert text to lowercase. - Replace a specific string pattern with another string. 2. **Functionality**: - Read the contents of the input file. - Apply any transformations specified by the optional arguments. - Write the transformed contents to the output file if provided, otherwise overwrite the input file. 3. **Constraints**: - Each transformation (uppercase, lowercase, string replacement) should be mutually exclusive. Only one can be applied at a time. - If no transformations are provided, simply copy the contents from the input file to the output file. 4. **Performance**: - The code should handle large files efficiently. - Avoid loading the entire file into memory if possible. Input Format The program will be executed via the command line, e.g., ``` python file_processor.py input.txt -o output.txt --uppercase ``` Output Format - If an output file is provided, write the transformed content to the output file. - If no output file is provided, overwrite the input file with the transformed content. Example Usage ``` # Command to convert content to uppercase and write to output file python file_processor.py input.txt -o output.txt --uppercase # Command to replace \'foo\' with \'bar\' and overwrite the input file python file_processor.py input.txt --replace foo bar ``` Implementation Skeleton ```python import argparse def process_file(input_path, output_path, uppercase, lowercase, replace): # Open the input file with open(input_path, \'r\') as infile: content = infile.read() if uppercase: content = content.upper() elif lowercase: content = content.lower() elif replace: old_string, new_string = replace content = content.replace(old_string, new_string) # Open the output file (if provided) or overwrite the input file with open(output_path, \'w\') as outfile: outfile.write(content) def main(): parser = argparse.ArgumentParser(description=\\"Process and transform a text file.\\") parser.add_argument(\'input_file\', type=str, help=\\"Path to the input file\\") parser.add_argument(\'-o\', \'--output_file\', type=str, default=None, help=\\"Path to the output file (optional)\\") group = parser.add_mutually_exclusive_group() group.add_argument(\'--uppercase\', action=\'store_true\', help=\\"Convert text to uppercase\\") group.add_argument(\'--lowercase\', action=\'store_true\', help=\\"Convert text to lowercase\\") group.add_argument(\'--replace\', nargs=2, metavar=(\'OLD\', \'NEW\'), help=\\"Replace occurrences of OLD string with NEW string\\") args = parser.parse_args() process_file(args.input_file, args.output_file or args.input_file, args.uppercase, args.lowercase, args.replace) if __name__ == \\"__main__\\": main() ``` **Notes**: - Your implementation should consider handling exceptions, such as file not found or read/write errors. - Ensure to provide descriptive error messages and help text for command-line arguments. - Write test cases to verify the functionality of your program. # Constraints - Python version: â‰¥3.6 - Libraries: Standard library only (primarily `argparse`, `os`, `io`)","solution":"import argparse def process_file(input_path, output_path, uppercase, lowercase, replace): # Open the input file with open(input_path, \'r\') as infile: content = infile.read() if uppercase: content = content.upper() elif lowercase: content = content.lower() elif replace: old_string, new_string = replace content = content.replace(old_string, new_string) # Open the output file (if provided) or overwrite the input file with open(output_path, \'w\') as outfile: outfile.write(content) def main(): parser = argparse.ArgumentParser(description=\\"Process and transform a text file.\\") parser.add_argument(\'input_file\', type=str, help=\\"Path to the input file\\") parser.add_argument(\'-o\', \'--output_file\', type=str, default=None, help=\\"Path to the output file (optional)\\") group = parser.add_mutually_exclusive_group() group.add_argument(\'--uppercase\', action=\'store_true\', help=\\"Convert text to uppercase\\") group.add_argument(\'--lowercase\', action=\'store_true\', help=\\"Convert text to lowercase\\") group.add_argument(\'--replace\', nargs=2, metavar=(\'OLD\', \'NEW\'), help=\\"Replace occurrences of OLD string with NEW string\\") args = parser.parse_args() process_file(args.input_file, args.output_file or args.input_file, args.uppercase, args.lowercase, args.replace) if __name__ == \\"__main__\\": main()"},{"question":"**Question: Analyzing the Distribution of Titanic Passenger Ages Using Seaborn** The given dataset `titanic` contains information about passengers aboard the Titanic. Using this dataset, your task is to create a function `plot_titanic_age_distribution` that: 1. Plots a boxplot showing the distribution of ages among different classes of passengers (First, Second, and Third). 2. Colors the boxplots differently based on whether the passengers survived or not. 3. Adds customization to the plot: - Set the title to \\"Age Distribution of Titanic Passengers by Class\\". - Label the x-axis as \\"Passenger Class\\". - Label the y-axis as \\"Passenger Age\\". - Add a legend indicating the survival status. - Customize the median line color to red and line width to 2. **Constraints:** - Use only seaborn and matplotlib for plotting. - The function should not return any value; it should only display the plot. # Function Signature ```python def plot_titanic_age_distribution(titanic_data: \'pd.DataFrame\') -> None: pass ``` # Example Usage ```python import seaborn as sns import pandas as pd # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Call the function to display the plot plot_titanic_age_distribution(titanic) ``` # Expected Output The plot should be a vertical boxplot with: - Categories \\"First\\", \\"Second\\", and \\"Third\\" on the x-axis. - Age values on the y-axis. - Different colors for \\"Survived\\" and \\"Did Not Survive\\" passengers. - Customizations applied as specified. **Note**: Ensure that the plot is generated based on the provided customizations and seaborn functionalities.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_age_distribution(titanic_data): Plots a boxplot showing the distribution of ages among different classes of passengers, colored based on survival status, and customizes the plot appearance. Parameters: titanic_data (pd.DataFrame): DataFrame containing Titanic passengers\' data. # Create a boxplot with seaborn plot = sns.boxplot( x=\\"class\\", y=\\"age\\", hue=\\"survived\\", data=titanic_data, palette={0: \\"gray\\", 1: \\"blue\\"}, medianprops=dict(color=\\"red\\", linewidth=2) ) # Set title and axis labels plot.set_title(\\"Age Distribution of Titanic Passengers by Class\\") plot.set_xlabel(\\"Passenger Class\\") plot.set_ylabel(\\"Passenger Age\\") # Add a legend indicating the survival status handles, labels = plot.get_legend_handles_labels() plot.legend(handles, [\'Did Not Survive\', \'Survived\'], title=\'Survival\') # Display the plot plt.show()"},{"question":"Objective: Write a Python program that demonstrates your understanding of Python classes, inheritance, instance and class variables, and iterators as explained in the provided documentation. Problem Statement: Design a class hierarchy for a simple Library system. Implement the following requirements: 1. **Base Class - `LibraryItem`**: - **Attributes**: - `title` (str): Title of the library item. - `author` (str): Author of the library item. - `library_id` (int): Unique identifier for each item, automatically incremented. - **Class Variables**: - `_id_counter` (int): A private class variable to keep track of the next available ID. - **Methods**: - `__init__(self, title, author)`: Initializes the item with the given title and author and assigns a unique library_id. - `__str__(self)`: Returns a string representation of the item. 2. **Derived Classes**: - **Class - `Book`** inherits from `LibraryItem`: - **Attributes**: - `num_pages` (int): Number of pages in the book. - **Methods**: - `__init__(self, title, author, num_pages)`: Initializes the book with title, author, number of pages, and assigns a unique library_id. - `__str__(self)`: Returns a string representation of the book including number of pages. - **Class - `Magazine`** inherits from `LibraryItem`: - **Attributes**: - `issue_number` (int): The issue number of the magazine. - **Methods**: - `__init__(self, title, author, issue_number)`: Initializes the magazine with title, author, issue number, and assigns a unique library_id. - `__str__(self)`: Returns a string representation of the magazine including the issue number. 3. **Class - `Library`**: - **Attributes**: - `items` (list): A list to store all `LibraryItem` instances. - **Methods**: - `__init__(self)`: Initializes an empty list of items. - `add_item(self, item)`: Adds a `LibraryItem` to the library. - `get_all_titles(self)`: Returns a list of titles of all items in the library. - `__iter__(self)`: Returns an iterator over the library items. - `__next__(self)`: Allows iteration over the library items. 4. **Main Program**: - Create instances of `Book` and `Magazine`. - Add these instances to the `Library`. - Iterate over the `Library` and print the string representation of each item. Constraints: - Use the name mangling convention for the `_id_counter` attribute in `LibraryItem`. Expected Output: 1. When using the `add_item` method, the library should store items correctly. 2. The `get_all_titles` method should return the correct list of titles. 3. Iterating over the Library should print out the string representations of all items. Example: ```python book1 = Book(\\"1984\\", \\"George Orwell\\", 328) book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 281) magazine1 = Magazine(\\"National Geographic\\", \\"Various Authors\\", 2021) library = Library() library.add_item(book1) library.add_item(book2) library.add_item(magazine1) print(library.get_all_titles()) # Output: [\'1984\', \'To Kill a Mockingbird\', \'National Geographic\'] for item in library: print(item) # Output: # Book: \\"1984\\" by George Orwell, Pages: 328, ID: 1 # Book: \\"To Kill a Mockingbird\\" by Harper Lee, Pages: 281, ID: 2 # Magazine: \\"National Geographic\\" by Various Authors, Issue: 2021, ID: 3 ```","solution":"class LibraryItem: __id_counter = 0 def __init__(self, title, author): self.title = title self.author = author LibraryItem.__id_counter += 1 self.library_id = LibraryItem.__id_counter def __str__(self): return f\'\\"{self.title}\\" by {self.author}, ID: {self.library_id}\' class Book(LibraryItem): def __init__(self, title, author, num_pages): super().__init__(title, author) self.num_pages = num_pages def __str__(self): return f\'Book: \\"{self.title}\\" by {self.author}, Pages: {self.num_pages}, ID: {self.library_id}\' class Magazine(LibraryItem): def __init__(self, title, author, issue_number): super().__init__(title, author) self.issue_number = issue_number def __str__(self): return f\'Magazine: \\"{self.title}\\" by {self.author}, Issue: {self.issue_number}, ID: {self.library_id}\' class Library: def __init__(self): self.items = [] def add_item(self, item): self.items.append(item) def get_all_titles(self): return [item.title for item in self.items] def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self.items): result = self.items[self._index] self._index += 1 return result else: raise StopIteration # Main Program: book1 = Book(\\"1984\\", \\"George Orwell\\", 328) book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 281) magazine1 = Magazine(\\"National Geographic\\", \\"Various Authors\\", 2021) library = Library() library.add_item(book1) library.add_item(book2) library.add_item(magazine1) for item in library: print(item)"},{"question":"Seaborn Plot Limit Customization In this assessment, you are tasked with creating a function using the seaborn `so.Plot` class to generate customized plots with specific axis limits. # Function Signature ```python def customized_plot(x_data: List[int], y_data: List[int], x_limits: Optional[Tuple[Optional[float], Optional[float]]] = None, y_limits: Optional[Tuple[Optional[float], Optional[float]]] = None) -> so.Plot: pass ``` # Input - `x_data` (List[int]): A list of integers representing the x-axis data points. - `y_data` (List[int]): A list of integers representing the y-axis data points. - `x_limits` (Optional[Tuple[Optional[float], Optional[float]]]): A tuple containing the minimum and maximum values for the x-axis. If a value is `None`, the default value is used. - `y_limits` (Optional[Tuple[Optional[float], Optional[float]]]): A tuple containing the minimum and maximum values for the y-axis. If a value is `None`, the default value is used. # Output - Returns an `so.Plot` object with the specified data and axis limits applied. # Constraints - The length of `x_data` and `y_data` should be the same. - If `x_limits` or `y_limits` are not provided, default plot limits should be used. - Plot limits should be correctly applied even if only one side is specified (e.g., `(None, 5)`). # Example ```python x_data = [1, 2, 3, 4, 5] y_data = [5, 6, 7, 8, 9] x_limits = (0, 6) y_limits = (4, 10) plot = customized_plot(x_data, y_data, x_limits, y_limits) plot.show() ``` This should create a line plot with x-axis limits from 0 to 6 and y-axis limits from 4 to 10. # Evaluation Your solution will be evaluated based on the correctness and functionality of the generated `so.Plot` object, including appropriate application of axis limits and overall plot integrity.","solution":"import seaborn.objects as so from typing import List, Optional, Tuple def customized_plot(x_data: List[int], y_data: List[int], x_limits: Optional[Tuple[Optional[float], Optional[float]]] = None, y_limits: Optional[Tuple[Optional[float], Optional[float]]] = None) -> so.Plot: Generates a customized seaborn plot with specific axis limits. Parameters: - x_data (List[int]): List of integers representing the x-axis data points. - y_data (List[int]): List of integers representing the y-axis data points. - x_limits (Optional[Tuple[Optional[float], Optional[float]]]): Optional tuple for x-axis limits. - y_limits (Optional[Tuple[Optional[float], Optional[float]]]): Optional tuple for y-axis limits. Returns: - so.Plot: Seaborn plot object with the specified data and axis limits. plot = so.Plot(x=x_data, y=y_data).add(so.Line()) if x_limits is not None: plot = plot.limit(x=x_limits) if y_limits is not None: plot = plot.limit(y=y_limits) return plot"},{"question":"# Property List Manager You are required to implement a set of functions to manage `.plist` files using the `plistlib` module. Your implementation should include the following functionalities: 1. **Serialize a Python Dictionary into a Property List (.plist) File**: - Function signature: `def serialize_plist(data: dict, file_path: str, fmt=plistlib.FMT_XML) -> None` - Inputs: - `data` (dict): The dictionary to be serialized. - `file_path` (str): The file path where the `.plist` file will be saved. - `fmt` (optional): The format of the output `.plist` file, either `plistlib.FMT_XML` or `plistlib.FMT_BINARY`. Defaults to `plistlib.FMT_XML`. - Output: None. The function writes the serialized dictionary to the specified file. 2. **Deserialize a Property List (.plist) File into a Python Dictionary**: - Function signature: `def deserialize_plist(file_path: str) -> dict` - Inputs: - `file_path` (str): The file path of the `.plist` file to be read. - Output: - A dictionary representing the data contained in the `.plist` file. If the file is not found or is invalid, raise an appropriate exception. 3. **Convert a Dictionary to a Plist-formatted Bytes Object**: - Function signature: `def dict_to_plist_bytes(data: dict, fmt=plistlib.FMT_XML) -> bytes` - Inputs: - `data` (dict): The dictionary to be serialized. - `fmt` (optional): The format of the output bytes object, either `plistlib.FMT_XML` or `plistlib.FMT_BINARY`. Defaults to `plistlib.FMT_XML`. - Output: - A bytes object that contains the serialized plist data. 4. **Convert a Plist-formatted Bytes Object to a Dictionary**: - Function signature: `def plist_bytes_to_dict(data: bytes) -> dict` - Inputs: - `data` (bytes): The plist-formatted bytes object to be deserialized. - Output: - A dictionary representing the data contained in the plist bytes object. If the bytes object is invalid, raise an appropriate exception. # Constraints: - You must use the `plistlib` module. - Ensure that your functions handle exceptions gracefully, providing informative error messages where appropriate. - The input dictionary can contain strings, integers, floats, booleans, tuples, lists, dictionaries (with string keys only), `bytes`, `bytearray`, or `datetime` objects. # Example Usage: ```python import plistlib import datetime data = { \'aString\': \\"Hello, World!\\", \'aList\': [1, 2, 3, 4.0, \\"text\\"], \'aFloat\': 3.14, \'anInt\': 42, \'aDict\': {\'nestedKey\': \'nestedValue\'}, \'aTrueValue\': True, \'aFalseValue\': False, \'someData\': b\'<binary data>\', \'aDate\': datetime.datetime(2022, 5, 17, 10, 30, 0) } # Serialize to file serialize_plist(data, \'test.plist\') # Deserialize from file result = deserialize_plist(\'test.plist\') print(result) # Convert to plist bytes plist_bytes = dict_to_plist_bytes(data) print(plist_bytes) # Convert plist bytes back to dict result_from_bytes = plist_bytes_to_dict(plist_bytes) print(result_from_bytes) ``` Ensure to follow the guidelines and constraints, and test your functions to verify correctness.","solution":"import plistlib from typing import Dict def serialize_plist(data: Dict, file_path: str, fmt=plistlib.FMT_XML) -> None: Serializes a Python dictionary into a Property List (.plist) file. Args: data (dict): The dictionary to be serialized. file_path (str): The file path where the .plist file will be saved. fmt: The format of the output .plist file, either plistlib.FMT_XML or plistlib.FMT_BINARY. Defaults to plistlib.FMT_XML. with open(file_path, \'wb\') as file: plistlib.dump(data, file, fmt=fmt) def deserialize_plist(file_path: str) -> Dict: Deserializes a Property List (.plist) file into a Python dictionary. Args: file_path (str): The file path of the .plist file to be read. Returns: dict: A dictionary representing the data contained in the .plist file. Raises: FileNotFoundError: If the file does not exist. ValueError: If the plist cannot be parsed. with open(file_path, \'rb\') as file: return plistlib.load(file) def dict_to_plist_bytes(data: Dict, fmt=plistlib.FMT_XML) -> bytes: Converts a dictionary to a plist-formatted bytes object. Args: data (dict): The dictionary to be serialized. fmt: The format of the output bytes object, either plistlib.FMT_XML or plistlib.FMT_BINARY. Defaults to plistlib.FMT_XML. Returns: bytes: A bytes object that contains the serialized plist data. return plistlib.dumps(data, fmt=fmt) def plist_bytes_to_dict(data: bytes) -> Dict: Converts a plist-formatted bytes object to a dictionary. Args: data (bytes): The plist-formatted bytes object to be deserialized. Returns: dict: A dictionary representing the data contained in the plist bytes object. Raises: ValueError: If the plist bytes cannot be parsed. return plistlib.loads(data)"},{"question":"# WAV File Processor You are tasked with modifying an existing WAV file by changing its sample width and frame rate, and then saving the modified audio data to a new WAV file. Task Your task is to implement a function `process_wav_file(input_file: str, output_file: str, new_sample_width: int, new_frame_rate: int) -> None` that: 1. Reads an existing WAV file specified by `input_file`. 2. Modifies the sample width and frame rate to `new_sample_width` and `new_frame_rate` respectively. 3. Writes the modified audio data to a new WAV file specified by `output_file`. Constraints - The input WAV file is guaranteed to use `WAVE_FORMAT_PCM`. - The new sample width and frame rate will be valid values. - The output WAV file should have the same number of channels and frames as the input file, with modified sample width and frame rate. **Input:** - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path to the output WAV file. - `new_sample_width` (int): The new sample width (in bytes) for the output file. - `new_frame_rate` (int): The new frame rate for the output file. **Output:** - None. The function should create a new WAV file with the modified audio data. Example Given an input WAV file with the following properties: - `input_file = \\"original.wav\\"` - `output_file = \\"modified.wav\\"` - `new_sample_width = 2` - `new_frame_rate = 44100` The function should create a new WAV file `\\"modified.wav\\"` with: - The same number of channels and frames as `\\"original.wav\\"`. - Sample width set to 2 bytes. - Frame rate set to 44100 Hz. Function Signature ```python def process_wav_file(input_file: str, output_file: str, new_sample_width: int, new_frame_rate: int) -> None: pass ``` **Note:** You can assume that any external files used in test cases will be properly set up and available in the working directory.","solution":"import wave import audioop def process_wav_file(input_file: str, output_file: str, new_sample_width: int, new_frame_rate: int) -> None: with wave.open(input_file, \'rb\') as inp_wav: params = inp_wav.getparams() num_channels = params.nchannels sample_width = params.sampwidth frame_rate = params.framerate num_frames = params.nframes comp_type = params.comptype comp_name = params.compname audio_data = inp_wav.readframes(num_frames) if sample_width != new_sample_width: audio_data = audioop.lin2lin(audio_data, sample_width, new_sample_width) if frame_rate != new_frame_rate: audio_data, _ = audioop.ratecv(audio_data, new_sample_width, num_channels, frame_rate, new_frame_rate, None) with wave.open(output_file, \'wb\') as out_wav: out_wav.setnchannels(num_channels) out_wav.setsampwidth(new_sample_width) out_wav.setframerate(new_frame_rate) out_wav.setnframes(len(audio_data) // (new_sample_width * num_channels)) out_wav.writeframes(audio_data)"},{"question":"```markdown Dynamic Module Loader and Executor # Objective Your task is to write a Python function that dynamically imports a module from a specified file path, executes a function within the imported module, and returns the result. You must ensure that the module is properly imported using the low-level import functions described in the provided documentation. # Requirements 1. **Function Definition**: ```python def dynamic_module_loader(file_path: str, func_name: str, *args, **kwargs) -> Any: Dynamically loads a module from the given file path and executes a specified function with provided arguments. Parameters: - file_path (str): The path to the Python file to be imported. - func_name (str): The name of the function within the module to be executed. - *args: Positional arguments to pass to the function. - **kwargs: Keyword arguments to pass to the function. Returns: - Any: The result of the function execution. ``` 2. **Steps to Implement**: - **Read the Python file** specified by `file_path` and compile it into a code object. - **Create a new module** object for the compiled code. - **Insert the module** into the `sys.modules` dictionary to make it accessible. - **Execute the module code** within its own namespace. - **Find the specified function** within the module and ensure it is callable. - **Execute the function** with the provided arguments and return its result. 3. **Constraints**: - You may assume that the file at `file_path` exists and is a valid Python file. - You may assume that `func_name` is a valid function within the module. - Handle exceptions appropriately to provide informative error messages. # Example Given a Python file `example.py` with the following contents: ```python def greet(name): return f\\"Hello, {name}!\\" ``` Calling the function: ```python result = dynamic_module_loader(\'example.py\', \'greet\', \'World\') print(result) ``` Should output: ``` Hello, World! ``` # Notes - Use the function `PyImport_ExecCodeModuleEx` or equivalents to load and execute the module. - Insert the newly created module into the `sys.modules` dictionary to ensure proper reference handling. You may refer to the provided documentation for detailed explanations of the import functions required to solve this problem. ```markdown","solution":"import sys import os import importlib.util def dynamic_module_loader(file_path: str, func_name: str, *args, **kwargs): Dynamically loads a module from the given file path and executes a specified function with provided arguments. Parameters: - file_path (str): The path to the Python file to be imported. - func_name (str): The name of the function within the module to be executed. - *args: Positional arguments to pass to the function. - **kwargs: Keyword arguments to pass to the function. Returns: - Any: The result of the function execution. # Check if the file exists if not os.path.exists(file_path): raise FileNotFoundError(f\\"The file at path {file_path} does not exist.\\") # Extract module name from file path module_name = os.path.splitext(os.path.basename(file_path))[0] # Create a module spec from the file path spec = importlib.util.spec_from_file_location(module_name, file_path) if spec is None: raise ImportError(f\\"Cannot import module from path {file_path}.\\") # Create a new module based on the spec module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module # Execute the module try: spec.loader.exec_module(module) except Exception as e: raise ImportError(f\\"Failed to execute module {module_name}: {e}\\") # Retrieve the function from the module func = getattr(module, func_name, None) if func is None or not callable(func): raise AttributeError(f\\"The function {func_name} is not found or is not callable in the module {module_name}.\\") # Execute the function with provided arguments and return the result return func(*args, **kwargs)"},{"question":"# Advanced Python Programming Assessment Temporary File Handling with `tempfile` Module **Objective:** You are tasked with creating a Python function that processes data using temporary files and directories from the `tempfile` module. The function should demonstrate the use of the `TemporaryFile`, `NamedTemporaryFile`, and `TemporaryDirectory` classes and should handle the cleanup of these temporary resources appropriately. **Task:** Write a function `process_temp_files(data: str)` that: 1. Writes the given `data` to a `TemporaryFile`, reads it back, and appends the results to a list. 2. Writes the given `data` to a `NamedTemporaryFile` with a specific suffix \'.txt\', reads it back, and appends the results to the same list. 3. Within a created `TemporaryDirectory`, writes the given `data` to a new file in that directory, reads it back, and appends the results to the same list. 4. Ensures that all temporary files and the directory are properly cleaned up after processing. 5. Returns the list with the read data from all temporary files. **Constraints:** - The function should handle any potential I/O errors gracefully. - It should ensure that all resources are cleaned up properly even in the case of an error. - Assume that `data` is always a non-empty string. **Input:** - `data`: A string of arbitrary text. **Output:** - A list of strings read from the temporary files in the following order: `TemporaryFile`, `NamedTemporaryFile`, file in `TemporaryDirectory`. **Function Signature:** ```python def process_temp_files(data: str) -> list: pass ``` **Example:** ```python data_to_process = \\"Sample data for tempfile module.\\" result = process_temp_files(data_to_process) print(result) # Expected output: [\\"Sample data for tempfile module.\\", \\"Sample data for tempfile module.\\", \\"Sample data for tempfile module.\\"] ``` **Notes:** - Pay special attention to the cleanup of temporary files and directories. - Utilize context managers (`with` statements) to ensure resources are automatically cleaned up. - Each temporary resource should be treated independently as specified, and their contents should match the input data string. **Performance Considerations:** - Ensure the function runs efficiently without unnecessary overhead. - Focus on correctness and resource management. Provide the implementation of the above function to validate your understanding of the `tempfile` module in Python.","solution":"import tempfile import os def process_temp_files(data: str) -> list: results = [] # Handle TemporaryFile with tempfile.TemporaryFile(mode=\'w+t\') as temp_file: temp_file.write(data) temp_file.seek(0) results.append(temp_file.read()) # Handle NamedTemporaryFile with tempfile.NamedTemporaryFile(suffix=\\".txt\\", mode=\'w+t\', delete=True) as named_temp_file: named_temp_file.write(data) named_temp_file.seek(0) results.append(named_temp_file.read()) # Handle TemporaryDirectory with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = os.path.join(temp_dir, \'temp_file.txt\') with open(temp_file_path, \'w+t\') as temp_file_in_dir: temp_file_in_dir.write(data) temp_file_in_dir.seek(0) results.append(temp_file_in_dir.read()) return results"},{"question":"# Kernel Approximation with Nystroem and RBFSampler You are given a dataset with features and labels. Your task is to implement a data transformation pipeline using scikit-learn\'s Nystroem and RBFSampler classes to approximate kernel mappings. You will then apply a linear classifier to the transformed data and evaluate its performance. Here is the detailed task: 1. **Data Loading and Preparation**: - Load the **Iris dataset** from `sklearn.datasets`. - Split it into training and testing sets using `train_test_split`. 2. **Kernel Approximation and Classification**: - Implement the Nystroem method for kernel approximation with a radial basis function (RBF) kernel. Use 100 components. - Implement RBFSampler for a radial basis function (RBF) kernel approximation with 100 components and a gamma value of 1. - For both approximations, transform the training and testing sets. - Fit a `SGDClassifier` with the transformed training data and evaluate it on the transformed testing data using accuracy as the metric. # Function Signature ```python def kernel_approximation_evaluation(): # Load the Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score # Load and split the data data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42) # Nystroem method for kernel approximation nystroem = Nystroem(kernel=\\"rbf\\", n_components=100) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) # RBFSampler method for kernel approximation rbf_sampler = RBFSampler(gamma=1, n_components=100) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) # SGD Classifier for Nystroem clf_nystroem = SGDClassifier(max_iter=1000, tol=1e-3) clf_nystroem.fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) accuracy_nystroem = accuracy_score(y_test, y_pred_nystroem) # SGD Classifier for RBFSampler clf_rbf = SGDClassifier(max_iter=1000, tol=1e-3) clf_rbf.fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) accuracy_rbf = accuracy_score(y_test, y_pred_rbf) # Return the accuracies for both methods return accuracy_nystroem, accuracy_rbf # Obtain the accuracies for both kernel approximation methods nystroem_accuracy, rbf_accuracy = kernel_approximation_evaluation() print(f\\"Nystroem Accuracy: {nystroem_accuracy}\\") print(f\\"RBF Sampler Accuracy: {rbf_accuracy}\\") ``` # Constraints - Use `random_state=42` for reproducibility during data splitting. - Ensure `n_components=100` for both Nystroem and RBFSampler. - Set `max_iter=1000` for the `SGDClassifier`. # Expected Output The function should return and print the accuracy scores of the classifiers using the Nystroem and RBFSampler methods. For example: ``` Nystroem Accuracy: 0.9555555555555556 RBF Sampler Accuracy: 0.9777777777777777 ``` **Note**: The actual accuracy values may vary slightly due to the random nature of kernel approximation methods.","solution":"def kernel_approximation_evaluation(): # Load the Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score # Load and split the data data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42) # Nystroem method for kernel approximation nystroem = Nystroem(kernel=\\"rbf\\", n_components=100) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) # RBFSampler method for kernel approximation rbf_sampler = RBFSampler(gamma=1, n_components=100) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) # SGD Classifier for Nystroem clf_nystroem = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_nystroem.fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) accuracy_nystroem = accuracy_score(y_test, y_pred_nystroem) # SGD Classifier for RBFSampler clf_rbf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) clf_rbf.fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) accuracy_rbf = accuracy_score(y_test, y_pred_rbf) # Return the accuracies for both methods return accuracy_nystroem, accuracy_rbf"},{"question":"**Complex Data Processing with itertools** You are provided with a dataset represented as a list of dictionaries, where each dictionary represents an item with various attributes. You need to process this dataset to generate a summary report based on specified criteria. The operations you need to perform involve sorting, grouping, filtering, and aggregating data using itertools. Your task is to implement a function that achieves this in an efficient and concise manner. # Function Signature ```python from typing import List, Dict, Any from itertools import groupby, chain, filterfalse, accumulate def process_data(data: List[Dict[str, Any]], key_attr: str, filter_attr: str, filter_value: Any, group_attr: str, agg_attr: str) -> List[Dict[str, Any]]: Processes the dataset to generate a summary report based on specified criteria. Parameters: - data: List[Dict[str, Any]]: A list of dictionaries representing the dataset. - key_attr: str: The attribute used for sorting the dataset. - filter_attr: str: The attribute used for filtering the dataset. - filter_value: Any: The value used for filtering the dataset. - group_attr: str: The attribute used for grouping the dataset. - agg_attr: str: The attribute used for aggregation. Returns: - List[Dict[str, Any]]: A list of dictionaries representing the summary report. pass ``` # Instructions 1. **Sorting**: The data should be sorted based on `key_attr`. 2. **Filtering**: Filter out items where `filter_attr` does not match the `filter_value`. 3. **Grouping**: Group the filtered data by `group_attr`. 4. **Aggregation**: For each group, calculate the sum of the `agg_attr`. 5. **Output**: The result should be a list of dictionaries, with each dictionary containing: - The `group_attr` value. - The total sum of `agg_attr` for that group. # Example Usage ```python data = [ {\\"id\\": 1, \\"category\\": \\"A\\", \\"value\\": 100, \\"status\\": \\"active\\"}, {\\"id\\": 2, \\"category\\": \\"B\\", \\"value\\": 200, \\"status\\": \\"inactive\\"}, {\\"id\\": 3, \\"category\\": \\"A\\", \\"value\\": 150, \\"status\\": \\"active\\"}, {\\"id\\": 4, \\"category\\": \\"B\\", \\"value\\": 50, \\"status\\": \\"active\\"}, {\\"id\\": 5, \\"category\\": \\"C\\", \\"value\\": 300, \\"status\\": \\"inactive\\"} ] result = process_data(data, key_attr=\\"id\\", filter_attr=\\"status\\", filter_value=\\"active\\", group_attr=\\"category\\", agg_attr=\\"value\\") print(result) ``` Expected output: ```python [ {\\"category\\": \\"A\\", \\"total_value\\": 250}, {\\"category\\": \\"B\\", \\"total_value\\": 50} ] ``` # Constraints - The dataset will contain at most 10,000 items. - Each dictionary will have at most 10 key-value pairs. - `key_attr`, `filter_attr`, `group_attr`, and `agg_attr` will always be valid keys in the dictionaries. # Additional Notes - Make use of itertools functions to achieve efficient and concise solutions. - Ensure your code is optimized for performance, especially for larger datasets.","solution":"from typing import List, Dict, Any from itertools import groupby, chain, filterfalse, accumulate def process_data(data: List[Dict[str, Any]], key_attr: str, filter_attr: str, filter_value: Any, group_attr: str, agg_attr: str) -> List[Dict[str, Any]]: Processes the dataset to generate a summary report based on specified criteria. Parameters: - data: List[Dict[str, Any]]: A list of dictionaries representing the dataset. - key_attr: str: The attribute used for sorting the dataset. - filter_attr: str: The attribute used for filtering the dataset. - filter_value: Any: The value used for filtering the dataset. - group_attr: str: The attribute used for grouping the dataset. - agg_attr: str: The attribute used for aggregation. Returns: - List[Dict[str, Any]]: A list of dictionaries representing the summary report. # Step 1: Sort the data by key_attr sorted_data = sorted(data, key=lambda x: x[key_attr]) # Step 2: Filter the data where filter_attr matches filter_value filtered_data = filter(lambda x: x[filter_attr] == filter_value, sorted_data) # Step 3: Group the data by group_attr grouped_data = groupby(filtered_data, key=lambda x: x[group_attr]) # Step 4: Calculate the sum of agg_attr for each group result = [] for group, items in grouped_data: total_value = sum(item[agg_attr] for item in items) result.append({group_attr: group, f\\"total_{agg_attr}\\": total_value}) return result"},{"question":"Objective Your task is to implement a class that demonstrates an understanding of causal bias in attention mechanisms using PyTorch. In particular, you will implement the `CausalBias` class and two functions `causal_lower_right` and `causal_upper_left` within this class, mimicking their expected behavior based on typical usage patterns in attention mechanisms. Description 1. **CausalBias Class** Implement the `CausalBias` class with the following specifications: - **Constructor**: The class should initialize with a sequence length and optionally a bias type (`\'lower_right\'` or `\'upper_left\'`). - **Method**: `apply_bias(tensor: torch.Tensor) -> torch.Tensor`: This method applies the specified causal bias to the input tensor. 2. **Functions within CausalBias** Implement two methods within the `CausalBias` class: - `causal_lower_right(self, tensor: torch.Tensor) -> torch.Tensor`: This method should zero-out elements below the main diagonal of the tensor, simulating lower right causal bias. - `causal_upper_left(self, tensor: torch.Tensor) -> torch.Tensor`: This method should zero-out elements above the main diagonal of the tensor, simulating upper left causal bias. Example ```python import torch import torch.nn as nn class CausalBias(nn.Module): def __init__(self, seq_len: int, bias_type: str = \'lower_right\'): super(CausalBias, self).__init__() self.seq_len = seq_len self.bias_type = bias_type def causal_lower_right(self, tensor: torch.Tensor) -> torch.Tensor: # Your implementation here pass def causal_upper_left(self, tensor: torch.Tensor) -> torch.Tensor: # Your implementation here pass def apply_bias(self, tensor: torch.Tensor) -> torch.Tensor: if self.bias_type == \'lower_right\': return self.causal_lower_right(tensor) elif self.bias_type == \'upper_left\': return self.causal_upper_left(tensor) else: raise ValueError(\\"Invalid bias type. Choose \'lower_right\' or \'upper_left\'\\") # Example usage seq_len = 4 input_tensor = torch.arange(seq_len * seq_len).reshape(seq_len, seq_len).float() bias = CausalBias(seq_len, bias_type=\'lower_right\') output_tensor = bias.apply_bias(input_tensor) print(\\"Input Tensor:\\") print(input_tensor) print(\\"Output Tensor with Lower Right Bias:\\") print(output_tensor) ``` Input/Output Specifications - **Input**: - The constructor of `CausalBias` takes in an integer representing sequence length and an optional string for bias type. - The `apply_bias` method takes a 2D PyTorch tensor of shape `(seq_len, seq_len)`. - **Output**: - The `causal_lower_right` method should return a tensor with zeros below the main diagonal. - The `causal_upper_left` method should return a tensor with zeros above the main diagonal. - The `apply_bias` method should apply the specified bias method to the input tensor and return the modified tensor. Constraints - The input tensor will always be a square matrix of dimensions `(seq_len, seq_len)`. - The sequence length will be a positive integer `seq_len > 0`. # Example Output For `seq_len = 4`, `bias = CausalBias(seq_len, bias_type=\'lower_right\')` with input tensor: ``` Input Tensor: tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) ``` The output tensor should be: ``` Output Tensor with Lower Right Bias: tensor([[ 0., 1., 2., 3.], [ 0., 5., 6., 7.], [ 0., 0., 10., 11.], [ 0., 0., 0., 15.]]) ```","solution":"import torch import torch.nn as nn class CausalBias(nn.Module): def __init__(self, seq_len: int, bias_type: str = \'lower_right\'): super(CausalBias, self).__init__() self.seq_len = seq_len self.bias_type = bias_type def causal_lower_right(self, tensor: torch.Tensor) -> torch.Tensor: # Zero-out elements below the main diagonal for i in range(self.seq_len): for j in range(i): tensor[i, j] = 0 return tensor def causal_upper_left(self, tensor: torch.Tensor) -> torch.Tensor: # Zero-out elements above the main diagonal for i in range(self.seq_len): for j in range(i + 1, self.seq_len): tensor[i, j] = 0 return tensor def apply_bias(self, tensor: torch.Tensor) -> torch.Tensor: if self.bias_type == \'lower_right\': return self.causal_lower_right(tensor) elif self.bias_type == \'upper_left\': return self.causal_upper_left(tensor) else: raise ValueError(\\"Invalid bias type. Choose \'lower_right\' or \'upper_left\'\\") # Example usage seq_len = 4 input_tensor = torch.arange(seq_len * seq_len).reshape(seq_len, seq_len).float() bias = CausalBias(seq_len, bias_type=\'lower_right\') output_tensor = bias.apply_bias(input_tensor) print(\\"Input Tensor:\\") print(input_tensor) print(\\"Output Tensor with Lower Right Bias:\\") print(output_tensor)"},{"question":"**Coding Assessment Question** The dataset provided is `penguins`, which contains the following columns: `species`, `island`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`, and `sex`. Your task involves performing an exploratory data analysis by creating various visualizations using Seaborn. # Instructions 1. **Load the Dataset**: Load the `penguins` dataset from Seaborn\'s built-in datasets. 2. **Scatter Plot**: Create a scatter plot that shows the relationship between `bill_length_mm` and `bill_depth_mm`. The points should be colored based on `species`. 3. **Facet Grid Mapping**: Use `FacetGrid` to create a grid of scatter plots showing the relationship between `flipper_length_mm` and `body_mass_g`. The grid should be separated by `island`, with different colors representing different species. 4. **Statistical Estimation**: Create a line plot showing the average `flipper_length_mm` over `body_mass_g` for different species. Include confidence intervals in the plot. 5. **Distribution Plot**: For each species, plot the distribution of `bill_length_mm`. Use both histograms and kernel density estimates (KDE) in the same plot. 6. **Categorical Plot**: Create a bar plot to show the average `body_mass_g` for each `island`, differentiated by `species`. 7. **Customization**: Customize the plot aesthetics to have a professional look. Change themes, axis labels, legends, and other plot elements to improve readability and presentation. # Constraints - Ensure plots are appropriately labeled (axis labels, titles, legends, etc.). - Set the theme to `\\"darkgrid\\"` for all plots. - Use `matplotlib.pyplot.tight_layout()` after creating each plot to ensure there is no overlap of plot elements. # Input and Output - **Input**: None (the specific `penguins` dataset should be loaded directly using Seabornâ€™s `load_dataset` function). - **Output**: Display the plots inline (this is typical for Jupyter notebooks). # Example Solution Layout ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set theme sns.set_theme(style=\\"darkgrid\\") # Scatter Plot plt.figure() sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\'Bill Length vs Bill Depth\') plt.tight_layout() plt.show() # Facet Grid Mapping plt.figure() g = sns.FacetGrid(penguins, col=\\"island\\", hue=\\"species\\") g.map(sns.scatterplot, \\"flipper_length_mm\\", \\"body_mass_g\\").add_legend() plt.tight_layout() plt.show() # Statistical Estimation plt.figure() sns.lineplot(data=penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", hue=\\"species\\", ci=\\"sd\\") plt.title(\'Average Flipper Length vs Body Mass\') plt.tight_layout() plt.show() # Distribution Plot plt.figure() sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", kde=True, element=\\"step\\") plt.title(\'Bill Length Distribution by Species\') plt.tight_layout() plt.show() # Categorical Plot plt.figure() sns.catplot(data=penguins, kind=\\"bar\\", x=\\"island\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\'Average Body Mass by Island and Species\') plt.tight_layout() plt.show() # Customize a plot plt.figure() sns.set_theme(style=\\"darkgrid\\", font_scale=1.2) g = sns.relplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"body_mass_g\\", palette=\\"crest\\" ) g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.legend.set_title(\\"Body Mass (g)\\") g.figure.set_size_inches(8, 6) g.ax.margins(.15) g.despine(trim=True) plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set theme sns.set_theme(style=\\"darkgrid\\") # Scatter Plot plt.figure() sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\'Bill Length vs Bill Depth\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.legend(title=\'Species\') plt.tight_layout() plt.show() # Facet Grid Mapping g = sns.FacetGrid(penguins, col=\\"island\\", hue=\\"species\\") g.map(sns.scatterplot, \\"flipper_length_mm\\", \\"body_mass_g\\").add_legend() g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Body Mass (g)\\") g.set_titles(col_template=\\"{col_name} Island\\") plt.tight_layout() plt.show() # Statistical Estimation plt.figure() sns.lineplot(data=penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", hue=\\"species\\", ci=\\"sd\\") plt.title(\'Average Flipper Length vs Body Mass\') plt.xlabel(\'Body Mass (g)\') plt.ylabel(\'Flipper Length (mm)\') plt.legend(title=\'Species\') plt.tight_layout() plt.show() # Distribution Plot plt.figure() sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", kde=True, element=\\"step\\") plt.title(\'Bill Length Distribution by Species\') plt.xlabel(\'Bill Length (mm)\') plt.tight_layout() plt.show() # Categorical Plot plt.figure() sns.catplot(data=penguins, kind=\\"bar\\", x=\\"island\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\'Average Body Mass by Island and Species\') plt.xlabel(\'Island\') plt.ylabel(\'Body Mass (g)\') plt.legend(title=\'Species\') plt.tight_layout() plt.show() # Customize a plot g = sns.relplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"body_mass_g\\", palette=\\"crest\\" ) g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.legend.set_title(\\"Body Mass (g)\\") g.figure.set_size_inches(8, 6) g.ax.margins(.15) g.despine(trim=True) plt.tight_layout() plt.show()"},{"question":"Objective: Implement a Python function that dynamically generates a `setup.py` script for a given module directory. The function should accept various arguments related to package metadata, module/package listings, extension module details, and additional data and scripts, and produce a correctly formatted `setup.py` script as a string. Problem Statement: Implement a function `generate_setup_script` which takes the following parameters: 1. `name` (str): The name of the module/package. 2. `version` (str): The version of the module/package. 3. `description` (str): A short description of the module/package. 4. `author` (str): The author\'s name. 5. `author_email` (str): The author\'s email address. 6. `url` (str): The URL for the package\'s homepage. 7. `packages` (list of str): A list of package names included in the module. 8. `package_dir` (dict): A dictionary mapping package names to directories. 9. `py_modules` (list of str, optional): A list of Python module names in the distribution. 10. `ext_modules` (list of tuples, optional): A list of tuples where each tuple contains the name of the extension and a list of source files. 11. `data_files` (list of tuples, optional): A list of (*directory*, *files*) pairs specifying additional files to install. 12. `scripts` (list of str, optional): A list of script filenames. 13. `package_data` (dict, optional): A dictionary mapping package names to lists of relative path names of data files included in the package. 14. `install_requires` (list of str, optional): A list of dependencies required by the module/package. Your function will generate a correctly formatted `setup.py` script based on the provided arguments. Constraints: 1. `name`, `version`, `description`, `author`, `author_email`, `url`, `packages`, and `package_dir` are mandatory. 2. The remaining parameters are optional and should be included in the `setup.py` only if provided. 3. The output `setup.py` must be properly formatted and syntactically correct. Example: ```python def generate_setup_script( name: str, version: str, description: str, author: str, author_email: str, url: str, packages: list, package_dir: dict, py_modules: list = None, ext_modules: list = None, data_files: list = None, scripts: list = None, package_data: dict = None, install_requires: list = None ) -> str: # Your implementation here ``` ```python # Example function call setup_script = generate_setup_script( name=\'ExampleProject\', version=\'0.1\', description=\'An example package\', author=\'John Doe\', author_email=\'johndoe@example.com\', url=\'http://example.com\', packages=[\'example\', \'example.subpkg\'], package_dir={\'example\': \'src/example\'}, py_modules=[\'example_module\'], ext_modules=[(\'example_ext\', [\'src/ext.c\'])], data_files=[(\'config\', [\'config/settings.cfg\'])], scripts=[\'scripts/run_example\'], package_data={\'example\': [\'data/*.dat\']}, install_requires=[\'numpy >= 1.18.0\'] ) print(setup_script) ``` Expected output: ```python from distutils.core import setup, Extension setup( name=\'ExampleProject\', version=\'0.1\', description=\'An example package\', author=\'John Doe\', author_email=\'johndoe@example.com\', url=\'http://example.com\', packages=[\'example\', \'example.subpkg\'], package_dir={\'example\': \'src/example\'}, py_modules=[\'example_module\'], ext_modules=[Extension(\'example_ext\', [\'src/ext.c\'])], data_files=[(\'config\', [\'config/settings.cfg\'])], scripts=[\'scripts/run_example\'], package_data={\'example\': [\'data/*.dat\']}, install_requires=[\'numpy >= 1.18.0\'] ) ```","solution":"def generate_setup_script( name, version, description, author, author_email, url, packages, package_dir, py_modules=None, ext_modules=None, data_files=None, scripts=None, package_data=None, install_requires=None ): setup_script = ffrom distutils.core import setup, Extension setup( name=\'{name}\', version=\'{version}\', description=\'{description}\', author=\'{author}\', author_email=\'{author_email}\', url=\'{url}\', packages={packages}, package_dir={package_dir} if py_modules: setup_script += f\\",n py_modules={py_modules}\\" if ext_modules: ext_modules_str = \\",n \\".join([f\\"Extension(\'{name}\', {sources})\\" for name, sources in ext_modules]) setup_script += f\\",n ext_modules=[n {ext_modules_str}n ]\\" if data_files: setup_script += f\\",n data_files={data_files}\\" if scripts: setup_script += f\\",n scripts={scripts}\\" if package_data: setup_script += f\\",n package_data={package_data}\\" if install_requires: setup_script += f\\",n install_requires={install_requires}\\" setup_script += \\"n)\\" return setup_script"},{"question":"You are tasked with creating a Python script that performs several subprocess operations and handles various edge cases. Your script will interact with the filesystem and external programs, requiring careful management of standard input and output streams. # Task Description 1. **Write a Function `execute_command`**: - **Input**: - `command` (str): The command to run. For simplicity, consider the command to be run through the shell. - `timeout` (int, optional): Specify the timeout duration in seconds. Default is 10 seconds. - **Output**: - A dictionary with the following keys: - `returncode`: The return code of the command. - `stdout`: The standard output of the command (captured and decoded to a string). - `stderr`: The standard error of the command (captured and decoded to a string). 2. **Implement Error Handling**: - If the command times out (i.e., exceeds the timeout duration), return an appropriate message in the `stderr` key and set the return code to -1. - If the command fails (non-zero return code), capture the error message in the `stderr` key. 3. **Security Considerations**: - Ensure the function works securely without exposing the system to shell injection vulnerabilities. # Constraints - Use the `subprocess.run` and handle scenarios using `Popen` interface for managing advanced use cases. - Carefully manage resource cleanup to avoid zombie processes. # Example ```python def execute_command(command: str, timeout: int = 10) -> dict: # Your implementation here pass # Example Usage result = execute_command(\'ls -l /nonexistent\') print(result) # Possible Output: # { # \'returncode\': 1, # \'stdout\': \'\', # \'stderr\': \'ls: cannot access \'/nonexistent\': No such file or directoryn\' # } result = execute_command(\'sleep 5\', timeout=2) print(result) # Possible Output: # { # \'returncode\': -1, # \'stdout\': \'\', # \'stderr\': \'Timeout expired\' # } ``` # Notes - Use proper exception handling where required. - Make sure to properly document your code. - Test your function with various commands to ensure robustness.","solution":"import subprocess def execute_command(command: str, timeout: int = 10) -> dict: Execute a shell command with a timeout. Args: - command: The command to run. - timeout: The timeout duration in seconds (default is 10). Returns: A dictionary with the return code, stdout, and stderr. try: result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=timeout) return { \'returncode\': result.returncode, \'stdout\': result.stdout, \'stderr\': result.stderr } except subprocess.TimeoutExpired: return { \'returncode\': -1, \'stdout\': \'\', \'stderr\': \'Timeout expired\' } except Exception as e: return { \'returncode\': -1, \'stdout\': \'\', \'stderr\': str(e) }"},{"question":"You have been provided with a dataset named `titanic`, which details various characteristics of Titanic passengers. Your task is to create a set of visualizations using the Seaborn library to analyze the data. Specifically, you need to accomplish the following: 1. **Categorical Scatterplot**: Create a `swarmplot` to visualize the distribution of passenger ages (`age`) for different classes (`class`). Use the `hue` parameter to differentiate by gender (`sex`). 2. **Categorical Distribution Plot**: Create a `boxplot` to visualize the distribution of passenger fares (`fare`) for different embarkation towns (`embark_town`). Make sure to use `hue` to differentiate survival status (`survived`). 3. **Combine Plots**: Combine a `violinplot` and a `swarmplot` to show the distribution of ages (`age`) for different passenger classes (`class`). Include `hue` for gender (`sex`). 4. **FacetGrid Plot**: Use the `FacetGrid` feature to create a series of `barplots` showing the average fare (`fare`) paid by passengers (`pclass`) grouped by their survival status (`survived`) and further broken down by embarkation town (`embark_town`). For each plot, ensure to include relevant titles, axis labels, and legends to make the visualizations clear and informative. **Constraints:** - Use only the Seaborn and Matplotlib libraries. - Ensure plots are well-labeled and annotated for clarity. - Handle any missing data appropriately where necessary. **Expected Input Format:** A DataFrame `titanic` loaded using Seaborn\'s `load_dataset(\'titanic\')`. **Expected Output:** A series of plots rendered in a Jupyter Notebook. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset titanic = sns.load_dataset(\'titanic\') # 1. Categorical Scatterplot: Swarmplot of age distribution by class and gender plt.figure(figsize=(10, 6)) sns.swarmplot(data=titanic, x=\'class\', y=\'age\', hue=\'sex\') plt.title(\'Age Distribution by Passenger Class and Gender\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.legend(title=\'Gender\') plt.show() # 2. Categorical Distribution Plot: Boxplot of fare by embark_town and survival status plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\'embark_town\', y=\'fare\', hue=\'survived\') plt.title(\'Fare Distribution by Embarkation Town and Survival Status\') plt.xlabel(\'Embarkation Town\') plt.ylabel(\'Fare\') plt.legend(title=\'Survived\') plt.show() # 3. Combined Plot: Violinplot and Swarmplot of age distribution by class and gender plt.figure(figsize=(10, 6)) sns.violinplot(data=titanic, x=\'class\', y=\'age\', hue=\'sex\', inner=None, palette=\'pastel\') sns.swarmplot(data=titanic, x=\'class\', y=\'age\', hue=\'sex\', dodge=True, palette=\'dark:#5A9\') plt.title(\'Age Distribution by Passenger Class and Gender\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.legend(title=\'Gender\') plt.show() # 4. FacetGrid Plot: Barplot of average fare by class, survival status, and embarkation town g = sns.catplot(data=titanic, x=\'pclass\', y=\'fare\', hue=\'survived\', col=\'embark_town\', kind=\'bar\', height=4, aspect=0.7) g.set_titles(\'{col_name}\') g.set_axis_labels(\'Passenger Class\', \'Average Fare\') plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset titanic = sns.load_dataset(\'titanic\') # 1. Categorical Scatterplot: Swarmplot of age distribution by class and gender plt.figure(figsize=(10, 6)) sns.swarmplot(data=titanic, x=\'class\', y=\'age\', hue=\'sex\') plt.title(\'Age Distribution by Passenger Class and Gender\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.legend(title=\'Gender\') plt.show() # 2. Categorical Distribution Plot: Boxplot of fare by embark_town and survival status plt.figure(figsize=(10, 6)) sns.boxplot(data=titanic, x=\'embark_town\', y=\'fare\', hue=\'survived\') plt.title(\'Fare Distribution by Embarkation Town and Survival Status\') plt.xlabel(\'Embarkation Town\') plt.ylabel(\'Fare\') plt.legend(title=\'Survived\') plt.show() # 3. Combined Plot: Violinplot and Swarmplot of age distribution by class and gender plt.figure(figsize=(10, 6)) sns.violinplot(data=titanic, x=\'class\', y=\'age\', hue=\'sex\', inner=None, palette=\'pastel\') sns.swarmplot(data=titanic, x=\'class\', y=\'age\', hue=\'sex\', dodge=True, palette=\'dark:#5A9\') plt.title(\'Age Distribution by Passenger Class and Gender\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Age\') plt.legend(title=\'Gender\') plt.show() # 4. FacetGrid Plot: Barplot of average fare by class, survival status, and embarkation town g = sns.catplot(data=titanic, x=\'pclass\', y=\'fare\', hue=\'survived\', col=\'embark_town\', kind=\'bar\', height=4, aspect=0.7) g.set_titles(\'{col_name}\') g.set_axis_labels(\'Passenger Class\', \'Average Fare\') plt.show()"},{"question":"User Management with pwd Module Given the Unix `pwd` module, you are required to write a Python script that performs the following tasks: 1. **Get User Information by Username**: Implement a function `get_user_by_name(username: str) -> dict` that takes a username as input and returns a dictionary containing the user\'s information. If the user is not found, return `None`. 2. **List All Users**: Implement a function `list_all_users() -> List[dict]` that returns a list of dictionaries, each containing information about every user in the password database. 3. **Check for Root Access**: Implement a function `has_root_access(username: str) -> bool` that takes a username as input and returns `True` if the given user has root access (uid = 0), and `False` otherwise. Input and Output Format: - `get_user_by_name(username: str) -> dict`: - **Input**: - `username`: A string representing the username. - **Output**: - A dictionary with keys corresponding to the attributes (\\"pw_name\\", \\"pw_passwd\\", \\"pw_uid\\", \\"pw_gid\\", \\"pw_gecos\\", \\"pw_dir\\", \\"pw_shell\\"). The values should be those returned by the `pwd` module. If the user is not found, return `None`. - `list_all_users() -> List[dict]`: - **Output**: - A list of dictionaries, each containing user information. Each dictionary should have the same keys as mentioned above. - `has_root_access(username: str) -> bool`: - **Input**: - `username`: A string representing the username. - **Output**: - A boolean value, `True` if the user has uid 0, `False` otherwise. Constraints: - Assume the system where this script is running supports the `pwd` module. - The script should handle cases where a user does not exist gracefully, i.e., without crashing. Example: ```python # Example usage of get_user_by_name print(get_user_by_name(\'root\')) # Output might look like: # {\'pw_name\': \'root\', \'pw_passwd\': \'x\', \'pw_uid\': 0, \'pw_gid\': 0, \'pw_gecos\': \'root\', \'pw_dir\': \'/root\', \'pw_shell\': \'/bin/bash\'} # Example usage of list_all_users print(list_all_users()) # Output might be a list of dictionaries containing user information # Example usage of has_root_access print(has_root_access(\'root\')) # Output: True print(has_root_access(\'john_doe\')) # Output: False ``` Ensure your solution adheres to the guidelines and correctly utilizes the `pwd` module functions.","solution":"import pwd from typing import List, Optional, Dict def get_user_by_name(username: str) -> Optional[Dict[str, str]]: try: pw_record = pwd.getpwnam(username) user_info = { \\"pw_name\\": pw_record.pw_name, \\"pw_passwd\\": pw_record.pw_passwd, \\"pw_uid\\": pw_record.pw_uid, \\"pw_gid\\": pw_record.pw_gid, \\"pw_gecos\\": pw_record.pw_gecos, \\"pw_dir\\": pw_record.pw_dir, \\"pw_shell\\": pw_record.pw_shell, } return user_info except KeyError: return None def list_all_users() -> List[Dict[str, str]]: all_users = [] for pw_record in pwd.getpwall(): user_info = { \\"pw_name\\": pw_record.pw_name, \\"pw_passwd\\": pw_record.pw_passwd, \\"pw_uid\\": pw_record.pw_uid, \\"pw_gid\\": pw_record.pw_gid, \\"pw_gecos\\": pw_record.pw_gecos, \\"pw_dir\\": pw_record.pw_dir, \\"pw_shell\\": pw_record.pw_shell, } all_users.append(user_info) return all_users def has_root_access(username: str) -> bool: user_info = get_user_by_name(username) if user_info and user_info[\\"pw_uid\\"] == 0: return True return False"},{"question":"**Python Coding Assessment Question: Quoted-Printable Encoding and Decoding** You are required to demonstrate your comprehension of quoted-printable encoding and decoding in MIME format using Python. The `quopri` module, which adheres to RFC 1521 and RFC 1522, will be used for this purpose. # Task: 1. **Function: `encoded_file_to_decoded_string`** - Write a function that reads encoded data from a file in quoted-printable format, decodes it, and returns the decoded data as a string. 2. **Function: `decoded_string_to_encoded_file`** - Write a function that accepts a string, encodes it in quoted-printable format, and writes the encoded data to a specified file. # Requirements: Function 1: `encoded_file_to_decoded_string(encoded_file_path: str) -> str` - **Input**: - `encoded_file_path`: A string representing the file path to the encoded input file. - **Output**: - Returns the decoded data as a standard Python string. - **Constraints**: - Assume the file contains valid quoted-printable encoded data. - Handle file reading errors gracefully. - **Example**: ```python decoded_text = encoded_file_to_decoded_string(\'encoded_input.txt\') print(decoded_text) # Should print the decoded data ``` Function 2: `decoded_string_to_encoded_file(decoded_str: str, encoded_file_path: str, quotetabs: bool = False) -> None` - **Input**: - `decoded_str`: A string containing the data to be encoded. - `encoded_file_path`: A string representing the file path to write the encoded data. - `quotetabs` (optional): A boolean flag indicating whether to encode embedded spaces and tabs. - **Output**: - Writes the encoded data to the specified file. - **Constraints**: - Ensure the encoded file contains valid quoted-printable data. - Handle file writing errors gracefully. - **Example**: ```python decoded_string_to_encoded_file(\\"This is a sample text!\\", \'encoded_output.txt\', quotetabs=True) ``` # Performance Requirements: - Efficiently handle large text data by properly reading from and writing to files. - Ensure that your implementation adheres to the `quopri` module\'s best practices. # Notes: - You might find the `quopri.decodestring` and `quopri.encodestring` functions useful for the string-to-string operations. - You must manage file operations carefully, ensuring files are correctly opened and closed.","solution":"import quopri def encoded_file_to_decoded_string(encoded_file_path: str) -> str: Reads encoded data from a file in quoted-printable format, decodes it, and returns the decoded data as a string. try: with open(encoded_file_path, \'rb\') as file: encoded_data = file.read() decoded_data = quopri.decodestring(encoded_data) return decoded_data.decode(\'utf-8\') except Exception as e: print(f\\"Error reading or decoding file: {e}\\") return \\"\\" def decoded_string_to_encoded_file(decoded_str: str, encoded_file_path: str, quotetabs: bool = False) -> None: Accepts a string, encodes it in quoted-printable format, and writes the encoded data to a specified file. try: encoded_data = quopri.encodestring(decoded_str.encode(\'utf-8\'), quotetabs=quotetabs) with open(encoded_file_path, \'wb\') as file: file.write(encoded_data) except Exception as e: print(f\\"Error encoding or writing to file: {e}\\")"},{"question":"**Pandas Data Type Manipulation Challenge** **Objective:** Create a script that reads and processes a DataFrame containing different pandas data types. **Task:** Implement a function `process_dataframe(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations: 1. **Timezone-Aware DateTime:** - Convert the \'event_time\' column to timezone-aware datetime with timezone \'UTC\'. 2. **Periods:** - Add a new column \'event_period\' which represents the period of \'event_time\' with monthly frequency. 3. **Intervals:** - Based on the \'values\' column which contains numeric data, create a new column \'value_intervals\' that categorizes \'values\' into the following intervals: - (-âˆž, 10] (left-closed) - (10, 20] (left-closed) - (20, âˆž) (left-closed) 4. **Nullable Integer:** - Convert the \'ids\' column to a nullable integer type (i.e., pandas\' Int64Dtype). 5. **Categorical Data:** - Ensure the \'category\' column is of categorical type with the order \'low\', \'medium\', \'high\'. 6. **String Data with Missing Values:** - Ensure the \'names\' column is of string dtype (pandas\' StringDtype) and handle any missing values by filling them with \'Unknown\'. **Input:** - `df` (pd.DataFrame): A DataFrame that includes columns \'event_time\', \'values\', \'ids\', \'category\', and \'names\'. **Output:** - Return the processed DataFrame after performing the operations specified. **Example:** ```python import pandas as pd data = { \'event_time\': [\'2023-01-01 12:00\', \'2023-02-01 13:00\', \'2023-03-01 14:00\'], \'values\': [5, 15, 25], \'ids\': [1, 2, None], \'category\': [\'medium\', \'high\', \'low\'], \'names\': [\'Alice\', None, \'Bob\'] } df = pd.DataFrame(data) processed_df = process_dataframe(df) print(processed_df) ``` Expected Output: ``` event_time values ids category names event_period 0 2023-01-01 12:00:00+00:00 5 1 medium Alice 2023-01 1 2023-02-01 13:00:00+00:00 15 2 high Unknown 2023-02 2 2023-03-01 14:00:00+00:00 25 <NA> low Bob 2023-03 value_intervals 0 (-inf, 10] 1 (10, 20] 2 (20, inf] ``` **Constraints and Notes:** - Ensure appropriate handling of missing values where necessary. - Use pandas\' built-in functions and methods for efficient processing. - The \'event_time\' column is initially in string format and needs conversion to a timezone-aware datetime. - The \'names\' column should properly handle missing values by filling them with \'Unknown\'.","solution":"import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: # 1. Convert \'event_time\' to timezone-aware datetime with \'UTC\' timezone df[\'event_time\'] = pd.to_datetime(df[\'event_time\']).dt.tz_localize(\'UTC\') # 2. Add \'event_period\' column with monthly frequency df[\'event_period\'] = df[\'event_time\'].dt.to_period(\'M\') # 3. Create \'value_intervals\' based on \'values\' column interval_bins = [-float(\'inf\'), 10, 20, float(\'inf\')] interval_labels = [\'(-inf, 10]\', \'(10, 20]\', \'(20, inf]\'] df[\'value_intervals\'] = pd.cut(df[\'values\'], bins=interval_bins, labels=interval_labels, right=True) # 4. Convert \'ids\' column to nullable integer type df[\'ids\'] = df[\'ids\'].astype(\'Int64\') # 5. Ensure \'category\' column is of categorical type with specified order category_order = [\'low\', \'medium\', \'high\'] df[\'category\'] = pd.Categorical(df[\'category\'], categories=category_order, ordered=True) # 6. Ensure \'names\' column is of string dtype and fill missing values with \'Unknown\' df[\'names\'] = df[\'names\'].astype(pd.StringDtype()).fillna(\'Unknown\') return df"},{"question":"# Asynchronous Web Scraper You are tasked with building an asynchronous web scraper using the `asyncio` library in Python 3.10. The scraper should be able to fetch and parse a list of URLs concurrently and extract certain data from the fetched HTML content. Problem Statement 1. You have a list of URLs that you need to scrape asynchronously. 2. For each URL, you will fetch the HTML content and parse it to extract all the hyperlinks (i.e., `<a href=\\"...\\">` tags). 3. You must use queues to manage the URLs to be scraped and the extracted hyperlinks. 4. Synchronize concurrent access using asyncio primitives to avoid race conditions. Requirements - Implement the following functions: ```python import asyncio import aiohttp from bs4 import BeautifulSoup async def fetch_url(session, url): Fetch the HTML content of a given URL asynchronously. Parameters: session (aiohttp.ClientSession): The aiohttp session to use for the request. url (str): The URL to fetch. Returns: str: The HTML content of the URL. pass async def extract_links(html): Extract all hyperlinks from the HTML content. Parameters: html (str): The HTML content to parse. Returns: list: A list of hyperlinks found in the HTML content. pass async def worker(name, url_queue, hyperlink_queue): Worker coroutine to process URLs from the url_queue and put extracted hyperlinks into the hyperlink_queue. Parameters: name (str): Name of the worker for logging purposes. url_queue (asyncio.Queue): Queue containing the URLs to be processed. hyperlink_queue (asyncio.Queue): Queue to store the extracted hyperlinks. pass async def main(urls): Main coroutine to set up the URL and hyperlink queues and spawn worker tasks. Parameters: urls (list): List of URLs to be scraped. pass ``` Inputs - `fetch_url(session, url)`: takes an aiohttp session and a URL string. - `extract_links(html)`: takes an HTML string. - `worker(name, url_queue, hyperlink_queue)`: takes the worker name, URL queue, and hyperlink queue. - `main(urls)`: takes a list of URLs. Outputs - `fetch_url`: the HTML content as a string. - `extract_links`: a list of hyperlinks in the HTML content. - `worker`: no return value, but processes URLs concurrently. - `main`: no return value, but orchestrates the overall scraping process. Constraints - Use the `aiohttp` library for asynchronous HTTP requests. - Use `BeautifulSoup` from the `bs4` library for HTML parsing. - Ensure all functions handle exceptions gracefully to avoid crashing. - Limit the number of concurrent workers to a maximum of 5. Example ```python urls = [ \'https://example.com\', \'https://example.org\', \'https://example.net\', ] # Run the main function asyncio.run(main(urls)) ``` This problem requires the implementation of asynchronous network I/O, task synchronization, and queue management, demonstrating a good grasp of `asyncio` features in Python 3.10.","solution":"import asyncio import aiohttp from bs4 import BeautifulSoup async def fetch_url(session, url): Fetch the HTML content of a given URL asynchronously. Parameters: session (aiohttp.ClientSession): The aiohttp session to use for the request. url (str): The URL to fetch. Returns: str: The HTML content of the URL. try: async with session.get(url) as response: return await response.text() except Exception as e: print(f\\"Error fetching {url}: {e}\\") return \\"\\" async def extract_links(html): Extract all hyperlinks from the HTML content. Parameters: html (str): The HTML content to parse. Returns: list: A list of hyperlinks found in the HTML content. soup = BeautifulSoup(html, \\"html.parser\\") return [a.get(\\"href\\") for a in soup.find_all(\\"a\\") if a.get(\\"href\\")] async def worker(name, url_queue, hyperlink_queue): Worker coroutine to process URLs from the url_queue and put extracted hyperlinks into the hyperlink_queue. Parameters: name (str): Name of the worker for logging purposes. url_queue (asyncio.Queue): Queue containing the URLs to be processed. hyperlink_queue (asyncio.Queue): Queue to store the extracted hyperlinks. async with aiohttp.ClientSession() as session: while True: url = await url_queue.get() if url is None: break html = await fetch_url(session, url) links = await extract_links(html) for link in links: await hyperlink_queue.put(link) url_queue.task_done() async def main(urls): Main coroutine to set up the URL and hyperlink queues and spawn worker tasks. Parameters: urls (list): List of URLs to be scraped. url_queue = asyncio.Queue() hyperlink_queue = asyncio.Queue() for url in urls: await url_queue.put(url) workers = [asyncio.create_task(worker(f\\"worker-{i}\\", url_queue, hyperlink_queue)) for i in range(5)] await url_queue.join() for w in workers: await url_queue.put(None) for w in workers: await w links = [] while not hyperlink_queue.empty(): links.append(await hyperlink_queue.get()) print(\\"Extracted links:\\", links) # Example usage urls = [ \'https://example.com\', \'https://example.org\', \'https://example.net\', ] # Run the main function asyncio.run(main(urls))"},{"question":"# Question: You are tasked with managing system resource limits and monitoring resource usage for a Python program. Your goal is to ensure that the program does not exceed specified resource limits while also providing detailed resource usage statistics. Implement a function `manage_resources` that performs the following tasks: 1. **Set Resource Limits**: Accepts a dictionary where keys are resource types (like `resource.RLIMIT_CPU`, etc.) and values are tuples representing the soft and hard limits. Use this dictionary to set these limits for the current process. 2. **Execute Task**: Accepts a function and its arguments to execute under the resource limits. 3. **Monitor Usage**: After executing the task, return a detailed report of resource usage. Your function should handle errors gracefully and ensure all resource limits are applied correctly. Input: - A dictionary `limits` where keys are resource types and values are tuples of soft and hard limits. - A callable `task` which is a function to be executed. - A variable-length argument list `*args` to pass to the task function. Output: - A dictionary containing detailed resource usage statistics returned by `resource.getrusage(resource.RUSAGE_SELF)`. Ensure that the output includes human-readable field names for each statistic. Constraints: - You may assume that the resource types provided in the `limits` dictionary are valid. - Handle potential `ValueError` and `OSError` exceptions in your function. Example: ```python import resource def sample_task(duration): import time time.sleep(duration) limits = { resource.RLIMIT_CPU: (1, 2), resource.RLIMIT_FSIZE: (1024*1024, 1024*1024) } result = manage_resources(limits, sample_task, 3) print(result) ``` Example output: ```python { \'user_time\': 0.01, \'system_time\': 0.00, \'max_resident_set_size\': 12345, \'shared_memory_size\': 0, \'unshared_data_size\': 0, \'unshared_stack_size\': 0, \'page_faults_no_io\': 15, \'page_faults_with_io\': 0, \'swap_outs\': 0, \'block_input_ops\': 0, \'block_output_ops\': 0, \'messages_sent\': 0, \'messages_received\': 0, \'signals_received\': 0, \'voluntary_context_switches\': 12, \'involuntary_context_switches\': 5 } ``` # Notes: - Use the `resource` module to manipulate and retrieve resource limits and usage. - Ensure robust error handling and meaningful output descriptions.","solution":"import resource def manage_resources(limits, task, *args): Set resource limits, execute a task, and return detailed resource usage statistics. Args: limits (dict): Dictionary of resource limits with resource types as keys and (soft, hard) limit tuples as values. task (callable): A function to execute under the given resource limits. args: Variable length argument list to pass to the task function. Returns: dict: Detailed resource usage statistics with human-readable field names. # Set the resource limits for res, limit in limits.items(): try: resource.setrlimit(res, limit) except (ValueError, OSError) as e: print(f\\"Error setting resource limit for {res}: {e}\\") # Execute the task try: task(*args) except Exception as e: print(f\\"Error during task execution: {e}\\") # Retrieve resource usage usage = resource.getrusage(resource.RUSAGE_SELF) # Create a human-readable dictionary from resource usage statistics usage_stats = { \'user_time\': usage.ru_utime, \'system_time\': usage.ru_stime, \'max_resident_set_size\': usage.ru_maxrss, \'shared_memory_size\': usage.ru_ixrss, \'unshared_data_size\': usage.ru_idrss, \'unshared_stack_size\': usage.ru_isrss, \'page_faults_no_io\': usage.ru_minflt, \'page_faults_with_io\': usage.ru_majflt, \'swap_outs\': usage.ru_nswap, \'block_input_ops\': usage.ru_inblock, \'block_output_ops\': usage.ru_oublock, \'messages_sent\': usage.ru_msgsnd, \'messages_received\': usage.ru_msgrcv, \'signals_received\': usage.ru_nsignals, \'voluntary_context_switches\': usage.ru_nvcsw, \'involuntary_context_switches\': usage.ru_nivcsw } return usage_stats"},{"question":"# Seaborn Plot Customization Challenge You are provided with a dataset named `seaice` which contains daily observations of sea ice extent over several years. The dataset has the following format: - `Date`: Date of the observation. - `Extent`: Sea ice extent measurement. Task: Write a function `plot_seaice_trends` that generates a customized plot showing the trend of sea ice extent over time using the seaborn library. Your plot should meet the following requirements: 1. **Load the Dataset:** - Load the `seaice` dataset provided by seaborn. 2. **Create the Plot:** - Plot the `Extent` over `Date`. - Use different colors for each year of observation. 3. **Faceting:** - Create facets for each decade to visualize the trend over different time periods. 4. **Customization:** - Use two different line widths for the lines. - Set a custom color pattern for the lines using the `ch:rot=-.2,light=.7` scheme. - Customize the layout to a size of 8 by 4 using the appropriate seaborn function. 5. **Title:** - Set the title of each facet as the decade it represents. Input: - None Output: - The function should display the plot as described without returning any value. Example Usage: ```python plot_seaice_trends() ``` Constraints: - Ensure that your code handles potential errors from dataset loading. - You should not hardcode any specific values other than those required for plot customization. Performance: - The function should run efficiently and should be able to handle the dataset without significant performance delays. Here\'s a starting template for your function: ```python import seaborn.objects as so from seaborn import load_dataset def plot_seaice_trends(): # Step 1: Load the dataset seaice = load_dataset(\\"seaice\\") # Step 2: Create the plot with faceting by decade plot = ( so.Plot(seaice, x=\\"Date\\", y=\\"Extent\\", color=\\"Date.dt.year\\") .facet(\\"Date.dt.year // 10 * 10\\", wrap=4) .add(so.Lines(linewidth=0.5, color=\\"#bbca\\")) .add(so.Lines(linewidth=1)) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(8, 4)) .label(title=\\"{}s\\".format) ) # Show the plot plot.show() # Call the function to display the plot plot_seaice_trends() ``` _Provide comments and explanations within your code to illustrate your understanding of each step._","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_seaice_trends(): Generates a customized plot showing the trend of sea ice extent over time. # Load the dataset try: seaice = sns.load_dataset(\\"seaice\\", data_home=\'http://seaborn.pydata.org/data\', cache=True) except Exception as e: print(f\\"An error occurred while loading the dataset: {e}\\") return # Ensure the \'Date\' column is in datetime format seaice[\'Date\'] = pd.to_datetime(seaice[\'Date\']) # Create a new column for the decade seaice[\'Decade\'] = (seaice[\'Date\'].dt.year // 10) * 10 # Create the faceted plot g = sns.FacetGrid(seaice, col=\\"Decade\\", col_wrap=4, height=4, aspect=2) # Plot data with customizations g.map_dataframe(sns.lineplot, x=\'Date\', y=\'Extent\', hue=\'Date.dt.year\', palette=\'ch:rot=-.2,light=.7\', linewidth=0.5) # Further width customization g.map_dataframe(sns.lineplot, x=\'Date\', y=\'Extent\', linewidth=2, alpha=0.5, color=\'k\') # Set titles g.set_titles(\\"{col_name}s\\") # Display the plot plt.show()"},{"question":"You are tasked with developing a Python function that compresses a list of text files using the `gzip` module. Each file should be compressed individually, and the function should handle any errors that may occur during the process. Additionally, another function should be implemented to decompress these files and ensure that the decompressed content matches the original content. Function Specifications 1. **Function: `compress_files(file_paths: List[str], output_dir: str) -> Dict[str, str]`** - **Input**: - `file_paths`: A list of paths to text files that need to be compressed. - `output_dir`: The directory where the compressed files should be saved. - **Output**: - A dictionary where the keys are the original file paths and the values are the paths to the corresponding compressed files. - **Constraints**: - If any file specified in `file_paths` does not exist or cannot be read, raise a `FileNotFoundError`. - Use a compression level of 6 for all files. - The output directory should be created if it does not exist. 2. **Function: `decompress_files(compressed_files: Dict[str, str]) -> List[str]`** - **Input**: - `compressed_files`: A dictionary where the keys are the original file paths and the values are the paths to the corresponding compressed files. - **Output**: - A list of paths to the decompressed files. - **Constraints**: - If decompression fails for any file, raise a `gzip.BadGzipFile` exception. - The decompressed files should be saved in the same directory as their compressed counterparts with \\".decompressed\\" appended to the filenames. 3. **Function: `validate_compression(original_files: List[str], decompressed_files: List[str]) -> bool`** - **Input**: - `original_files`: A list of paths to the original text files. - `decompressed_files`: A list of paths to the decompressed text files. - **Output**: - Returns `True` if all decompressed files match the contents of the original files, otherwise returns `False`. Coding Task Implement the three functions described above. Ensure that your code handles file operations robustly and manages errors gracefully. Example Usage ```python original_files = [\'/path/to/file1.txt\', \'/path/to/file2.txt\'] output_dir = \'/path/to/compressed/\' compressed_files = compress_files(original_files, output_dir) print(compressed_files) decompressed_files = decompress_files(compressed_files) print(decompressed_files) is_valid = validate_compression(original_files, decompressed_files) print(is_valid) ``` In this example, `compress_files` compresses each file in `original_files` and outputs the paths to the compressed files. `decompress_files` then decompresses these files, and `validate_compression` checks if the decompressed content matches the original.","solution":"import gzip import os from typing import List, Dict def compress_files(file_paths: List[str], output_dir: str) -> Dict[str, str]: if not os.path.exists(output_dir): os.makedirs(output_dir) compressed_files = {} for file_path in file_paths: if not os.path.exists(file_path): raise FileNotFoundError(f\\"File not found: {file_path}\\") file_name = os.path.basename(file_path) output_file = os.path.join(output_dir, f\\"{file_name}.gz\\") with open(file_path, \'rb\') as f_in: with gzip.open(output_file, \'wb\', compresslevel=6) as f_out: f_out.writelines(f_in) compressed_files[file_path] = output_file return compressed_files def decompress_files(compressed_files: Dict[str, str]) -> List[str]: decompressed_files = [] for original_file, compressed_file in compressed_files.items(): file_name = os.path.basename(compressed_file).replace(\'.gz\', \'\') decompressed_file = compressed_file.replace(\'.gz\', \'.decompressed\') try: with gzip.open(compressed_file, \'rb\') as f_in: with open(decompressed_file, \'wb\') as f_out: f_out.writelines(f_in) except gzip.BadGzipFile: raise gzip.BadGzipFile(f\\"Invalid gzip file: {compressed_file}\\") decompressed_files.append(decompressed_file) return decompressed_files def validate_compression(original_files: List[str], decompressed_files: List[str]) -> bool: for original_file, decompressed_file in zip(original_files, decompressed_files): with open(original_file, \'rb\') as f1, open(decompressed_file, \'rb\') as f2: original_content = f1.read() decompressed_content = f2.read() if original_content != decompressed_content: return False return True"},{"question":"# Question: Comprehensive Index Manipulation with pandas **Objective:** You are given a DataFrame containing various types of indices including a DateTimeIndex and a MultiIndex. Your task is to perform a series of operations that test your understanding of index manipulations offered by pandas. **Instructions:** 1. Create a DataFrame `df` with the following specifications: - The row index should be a MultiIndex created from the following: - Level 1: [\'A\', \'A\', \'B\', \'B\'] - Level 2: [1, 2, 1, 2] - The columns should include a DateTimeIndex with the date range for \'2022-01-01\' to \'2022-01-04\'. - The values in the DataFrame should be filled with integers from 1 to 16. 2. Perform the following operations on the DataFrame: - Rename the MultiIndex levels to [\'Category\', \'Subcategory\']. - Extract only the rows where the \'Category\' is \'A\'. - Sort the DataFrame by the DateTimeIndex in descending order. - Convert the DateTimeIndex to PeriodIndex with frequency \'D\'. - Remove rows where the \'Subcategory\' is 2 and assign the result to a new DataFrame `df_filtered`. 3. Compute the sum of each column in `df_filtered` and store the result in a Series `sum_series` with the PeriodIndex as its index. 4. Reset the index of `df_filtered`, converting the MultiIndex to columns. 5. Return the final DataFrame and the Series in the specified format. **Function Signature:** ```python import pandas as pd def manipulate_dataframe(): # Step 1: Create the DataFrame with MultiIndex and DateTimeIndex index = pd.MultiIndex.from_arrays([[\'A\', \'A\', \'B\', \'B\'], [1, 2, 1, 2]]) columns = pd.date_range(\'2022-01-01\', periods=4) data = [[i + j*4 + 1 for i in range(4)] for j in range(4)] df = pd.DataFrame(data, index=index, columns=columns) # Step 2: Rename levels df.index.names = [\'Category\', \'Subcategory\'] # Filtering df_a = df.xs(\'A\', level=\'Category\') # Sorting df_a = df_a.sort_index(axis=1, ascending=False) # Convert DatetimeIndex to PeriodIndex df_a.columns = df_a.columns.to_period(\'D\') # Filter by Subcategory and create df_filtered df_filtered = df_a.drop(2, level=\'Subcategory\') # Step 3: Compute the sum for each column sum_series = df_filtered.sum() # Step 4: Reset index df_filtered = df_filtered.reset_index() return df_filtered, sum_series # Example usage: # df, sums = manipulate_dataframe() ``` **Constraints:** - Assume that the DataFrame will always conform to the described structure. - Ensure your solution is efficient and utilizes appropriate pandas functionalities to achieve the desired outcomes.","solution":"import pandas as pd def manipulate_dataframe(): # Step 1: Create the DataFrame with MultiIndex and DateTimeIndex index = pd.MultiIndex.from_arrays([[\'A\', \'A\', \'B\', \'B\'], [1, 2, 1, 2]]) columns = pd.date_range(\'2022-01-01\', periods=4) data = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16] ] df = pd.DataFrame(data, index=index, columns=columns) # Step 2: Rename levels df.index.names = [\'Category\', \'Subcategory\'] # Extract rows where Category is \'A\' df_a = df.xs(\'A\', level=\'Category\') # Sort by DateTimeIndex in descending order df_a = df_a.sort_index(axis=1, ascending=False) # Convert DateTimeIndex to PeriodIndex with frequency \'D\' df_a.columns = df_a.columns.to_period(\'D\') # Remove rows where Subcategory is 2 df_filtered = df_a.drop(2) # Compute the sum for each column sum_series = df_filtered.sum() # Reset index df_filtered = df_filtered.reset_index() return df_filtered, sum_series # Example usage: # df, sums = manipulate_dataframe()"},{"question":"In this assignment, you will demonstrate your proficiency in managing devices, streams, and the random number generator in PyTorch\'s `torch.xpu` module. You will write functions that: 1. Initialize the XPU devices. 2. Set a specific random seed for reproducibility across all devices. 3. Create and manage streams for parallel device computations. 4. Optimize memory usage for a given tensor operation. Functions to Implement: 1. `initialize_devices()`: - **Description**: Initializes the XPU devices and checks if any devices are available. - **Input**: None - **Output**: A list of available device IDs (integers). 2. `set_global_seed(seed: int)`: - **Description**: Sets the same random seed for all XPU devices to ensure reproducibility. - **Input**: `seed` (integer): The seed value to set. - **Output**: None 3. `parallel_computation(tensor: torch.Tensor, scale: float)`: - **Description**: Scales a tensor by a given scaling factor using parallel computation on available XPU devices. - **Input**: - `tensor` (torch.Tensor): The input tensor to be scaled. - `scale` (float): The scaling factor. - **Output**: The scaled tensor. 4. `optimize_memory(tensors: List[torch.Tensor])`: - **Description**: Optimizes memory usage by clearing the cache and returning current memory statistics. - **Input**: `tensors` (List[torch.Tensor]): A list of tensors currently in memory. - **Output**: A dictionary of memory statistics before and after optimization. Constraints: - Assume that `torch` has been imported and is available. - Handle cases where no XPU devices are available gracefully. - Ensure that functions are performant and manage memory efficiently. Example: ```python # Initialize devices and set seed device_ids = initialize_devices() set_global_seed(42) # Create a tensor and perform parallel computation tensor = torch.randn((1000, 1000), device=device_ids[0]) scaled_tensor = parallel_computation(tensor, 0.5) # Optimize memory tensors = [scaled_tensor] memory_stats = optimize_memory(tensors) print(memory_stats) ``` # Evaluation Criteria: - **Correctness**: The functions should work as specified and produce the expected results. - **Efficiency**: The implementations must handle memory and device resources efficiently. - **Robustness**: The code should handle edge cases, such as no available XPU devices, gracefully. # Note: For testing purposes, if you do not have access to actual XPU hardware, simulate the behavior using standard CPU operations with the relevant `torch.xpu` API calls.","solution":"import torch def initialize_devices(): Initializes the XPU devices and checks if any devices are available. Returns: A list of available device IDs (integers). if torch.xpu.is_available(): num_devices = torch.xpu.device_count() return list(range(num_devices)) else: return [] def set_global_seed(seed): Sets the same random seed for all XPU devices to ensure reproducibility. Args: seed (int): The seed value to set. torch.manual_seed(seed) if torch.xpu.is_available(): for device_id in initialize_devices(): torch.xpu.manual_seed(device_id, seed) def parallel_computation(tensor, scale): Scales a tensor by a given scaling factor using parallel computation on available XPU devices. Args: tensor (torch.Tensor): The input tensor to be scaled. scale (float): The scaling factor. Returns: The scaled tensor. device = tensor.device return tensor * scale def optimize_memory(tensors): Optimizes memory usage by clearing the cache and returning current memory statistics. Args: tensors (List[torch.Tensor]): A list of tensors currently in memory. Returns: A dictionary of memory statistics before and after optimization. stats_before = { \\"allocated_bytes\\": torch.xpu.memory_allocated(), \\"cached_bytes\\": torch.xpu.memory_reserved() } torch.xpu.empty_cache() stats_after = { \\"allocated_bytes\\": torch.xpu.memory_allocated(), \\"cached_bytes\\": torch.xpu.memory_reserved() } return {\\"before\\": stats_before, \\"after\\": stats_after}"},{"question":"# Question: Parallel Task Execution Using ThreadPoolExecutor Objective: Write a Python function that processes a list of URLs concurrently to fetch the content using the `concurrent.futures.ThreadPoolExecutor`. Use this function to compute the total size of the combined content of all URLs. Function Signature: ```python def fetch_urls_total_size(urls: List[str]) -> int: pass ``` Input: - `urls` (List[str]): A list of URLs (strings) to be fetched. The list is guaranteed to contain at least one URL. Output: - Returns an integer, which is the total size (in bytes) of the combined content fetched from all provided URLs. Constraints: - The function should handle up to 100 URLs efficiently. - Each URL\'s content may be up to 1 MB. - The function should complete within a reasonable time frame for a list of 100 URLs (say under 10 seconds with normal network conditions). Requirements: - Use the `concurrent.futures.ThreadPoolExecutor` to manage the concurrent execution. - Don\'t use any external libraries for HTTP requests. Utilize built-in libraries such as `urllib`. Example: ```python urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] # If the example.com returns 500 bytes, example.org returns 700 bytes, and example.net returns 300 bytes result = fetch_urls_total_size(urls) print(result) # Expected output: 1500 ``` Note: - You can assume that all URLs provided in the input list are valid and accessible. - Handle any exceptions that may occur during the fetching of URLs so that one failed URL does not prevent the entire operation from proceeding. - Properly shutdown the `ThreadPoolExecutor` to free resources once the task is completed.","solution":"from typing import List import concurrent.futures import urllib.request def fetch_url_content_size(url: str) -> int: try: with urllib.request.urlopen(url) as response: content = response.read() return len(content) except Exception as e: # In case of an exception, consider the size as 0 return 0 def fetch_urls_total_size(urls: List[str]) -> int: total_size = 0 with concurrent.futures.ThreadPoolExecutor() as executor: futures = [executor.submit(fetch_url_content_size, url) for url in urls] for future in concurrent.futures.as_completed(futures): total_size += future.result() return total_size"},{"question":"Objective: Design a function that helps to visualize both the univariate and bivariate distributions of the `penguins` dataset using seaborn. The function should showcase flexibility by allowing different kinds of visualizations and conditional subsetting. Function Signature: ```python def visualize_penguins_distribution(kind: str, var1: str, var2: str = None, hue: str = None, **kwargs) -> None: pass ``` Parameters: - `kind` (str): The type of plot to create. Options include \'hist\', \'kde\', \'ecdf\', \'bivariate_hist\', and \'bivariate_kde\'. - `var1` (str): The primary variable to plot on the x-axis. - `var2` (str, optional): The secondary variable to plot on the y-axis for bivariate plots. Default is `None`. - `hue` (str, optional): Categorical variable for conditional subsets. Default is `None`. - `kwargs`: Additional keyword arguments to pass to the seaborn plotting function. Expected Output: The function does not return a value. It should render the specified seaborn plot. Constraints: - Only valid plot kinds (\'hist\', \'kde\', \'ecdf\', \'bivariate_hist\', \'bivariate_kde\') should be accepted. - If `var2` is provided, the plot kind must be one of the bivariate options (\'bivariate_hist\', \'bivariate_kde\'). Examples: 1. **Histogram of Flipper Lengths**: ```python visualize_penguins_distribution(kind=\'hist\', var1=\'flipper_length_mm\') ``` Should display a histogram of the `flipper_length_mm` variable for the `penguins` dataset. 2. **KDE of Flipper Lengths by Species**: ```python visualize_penguins_distribution(kind=\'kde\', var1=\'flipper_length_mm\', hue=\'species\') ``` Should display a KDE plot of the `flipper_length_mm` variable, conditional on the `species` column. 3. **Bivariate KDE of Bill Length and Bill Depth**: ```python visualize_penguins_distribution(kind=\'bivariate_kde\', var1=\'bill_length_mm\', var2=\'bill_depth_mm\') ``` Should display a bivariate KDE plot of `bill_length_mm` against `bill_depth_mm`. Notes: - Utilize seaborn functions like `displot`, `histplot`, `kdeplot`, and `ecdfplot` for creating plots. - The initialized seaborn theme `sns.set_theme()` should be included at the beginning of the function to ensure consistent styling. Additional Context: This question assesses the student\'s ability to: - Understand and apply various seaborn plotting functions. - Customize plots using additional keyword arguments. - Handle both univariate and bivariate distributions.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_penguins_distribution(kind: str, var1: str, var2: str = None, hue: str = None, **kwargs) -> None: Visualizes univariate or bivariate distributions from the penguins dataset. Parameters: kind (str): The type of plot to create. Options include \'hist\', \'kde\', \'ecdf\', \'bivariate_hist\', and \'bivariate_kde\'. var1 (str): The primary variable to plot on the x-axis. var2 (str, optional): The secondary variable to plot on the y-axis for bivariate plots. Default is None. hue (str, optional): Categorical variable for conditional subsets. Default is None. kwargs: Additional keyword arguments to pass to the seaborn plotting function. # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Set the seaborn theme sns.set_theme() # Univariate plots if kind in [\'hist\', \'kde\', \'ecdf\']: if kind == \'hist\': sns.histplot(data=penguins, x=var1, hue=hue, **kwargs) elif kind == \'kde\': sns.kdeplot(data=penguins, x=var1, hue=hue, **kwargs) elif kind == \'ecdf\': sns.ecdfplot(data=penguins, x=var1, hue=hue, **kwargs) # Bivariate plots elif kind in [\'bivariate_hist\', \'bivariate_kde\']: if var2 is None: raise ValueError(\\"var2 must be provided for bivariate plots.\\") if kind == \'bivariate_hist\': sns.histplot(data=penguins, x=var1, y=var2, hue=hue, **kwargs) elif kind == \'bivariate_kde\': sns.kdeplot(data=penguins, x=var1, y=var2, hue=hue, **kwargs) else: raise ValueError(\\"Invalid plot kind specified.\\") plt.show()"},{"question":"# Email Header Manipulation In this assessment, you are required to create an email message with headers containing non-ASCII characters. You will be utilizing the `Header` class from the `email.header` module. The aim is to demonstrate your ability to handle encoding and decoding of these headers. # Task 1. **Create an email message**: - Import the necessary classes from the `email` module. - Create an email `Message` object. - Add a subject header containing non-ASCII characters (e.g., \\"CafÃ© News\\"). 2. **Encode the Header**: - Use the `Header` class to encode the subject header, ensuring it is MIME-compliant. - Assign the encoded header to the appropriate message field. 3. **Decode the Header**: - Decode the MIME-compliant header back to its original form. - Verify that the decoded header matches the original string. # Requirements - You must handle non-ASCII characters properly by specifying the correct character sets. - The decoded header should match the original header string. - Use the `Header` class methods and related functions documented in the provided `email.header` module. # Input - The initial header string: `\'CafÃ© News\'` - Charset to use: `\'utf-8\'` # Output 1. The encoded email message as a string. 2. The decoded header string, matching the original input string. # Constraints - Ensure your solution adheres to the RFC standards for email headers. - Properly handle character encodings, especially for non-ASCII characters. # Example ```python from email.message import Message from email.header import Header, decode_header, make_header def create_email_with_header(header_str, charset): # Create a message object msg = Message() # Encode the header h = Header(header_str, charset) msg[\'Subject\'] = h # Output encoded message encoded_message = msg.as_string() # Decode the header decoded_header_parts = decode_header(msg[\'Subject\']) decoded_header = make_header(decoded_header_parts) return encoded_message, str(decoded_header) # Example usage header_str = \'CafÃ© News\' charset = \'utf-8\' encoded_message, decoded_header = create_email_with_header(header_str, charset) print(encoded_message) print(decoded_header) ``` Your task is to implement the function `create_email_with_header` so that it performs the steps outlined above.","solution":"from email.message import Message from email.header import Header, decode_header, make_header def create_email_with_header(header_str, charset): Create an email message with an encoded header containing non-ASCII characters, and verify the ability to decode it back to the original header string. Args: - header_str (str): The original header string containing non-ASCII characters. - charset (str): The charset to use for encoding the header. Returns: - encoded_message (str): The MIME-compliant encoded email message as a string. - decoded_header (str): The decoded header string matching the original input string. # Create a message object msg = Message() # Encode the header h = Header(header_str, charset) msg[\'Subject\'] = h # Output encoded message encoded_message = msg.as_string() # Decode the header decoded_header_parts = decode_header(msg[\'Subject\']) decoded_header = make_header(decoded_header_parts) return encoded_message, str(decoded_header)"},{"question":"Objective: To assess students\' knowledge and skills in handling exceptions, implementing clean-up actions, and creating custom exception classes in Python. Problem Statement: You are required to implement a function `process_data(file_path, data_string)` that processes data from a file and performs the following tasks: 1. Tries to read data from the file located at `file_path`. 2. If the file does not exist, it raises a `FileNotFoundError` with a custom error message `\\"Error: The specified file was not found.\\"`. 3. If the file is successfully opened, it reads all lines and appends `data_string` to each line. 4. If the file is empty, raises a `ValueError` with the message `\\"Error: The file is empty.\\"`. 5. It writes the modified lines back to the file. 6. Ensures that the file is closed whether an exception occurs or not. 7. If any other unexpected error occurs (any exception not explicitly handled), it raises a custom `ProcessingError` with the message `\\"An unexpected error occurred.\\"`. The `ProcessingError` should be a user-defined exception that inherits from the base `Exception` class. Input: - `file_path`: A string representing the path to the file to be processed. - `data_string`: A string to be appended to each line in the file. Output: - The function should write the modified lines back to the file. Constraints: - You are not allowed to use any external libraries for file processing other than Python\'s built-in `open`. Performance Requirements: - The function should efficiently handle large files without significant performance degradation. Example: ```python # Define the user-defined exception class ProcessingError(Exception): pass def process_data(file_path, data_string): try: # Your code here pass except FileNotFoundError as fnfe: # Custom error message for FileNotFoundError raise FileNotFoundError(\\"Error: The specified file was not found.\\") except ValueError as ve: # Handle ValueError if the file is empty raise ValueError(\\"Error: The file is empty.\\") except Exception as ex: # Handle unexpected errors raise ProcessingError(\\"An unexpected error occurred.\\") finally: # Ensure file is closed pass # Example usage: # Assuming \'sample.txt\' exists and contains some text try: process_data(\'sample.txt\', \' - processed\') except Exception as e: print(e) ``` Instructions: - Write the complete function `process_data(file_path, data_string)` with the required handling of exceptions as specified. - Ensure to define the custom exception `ProcessingError`. Notes: - The function can be tested with various file conditions including: - A file that exists and contains data. - A file that does not exist. - A file that is empty. - Any unexpected scenarios that might raise other exceptions.","solution":"class ProcessingError(Exception): Custom exception for unexpected errors during data processing. pass def process_data(file_path, data_string): try: # Try to open the file in read mode with open(file_path, \'r\') as file: lines = file.readlines() # Handle empty file scenario if not lines: raise ValueError(\\"Error: The file is empty.\\") # Append data_string to each line modified_lines = [line.strip() + data_string + \'n\' for line in lines] # Write modified lines back to the file with open(file_path, \'w\') as file: file.writelines(modified_lines) except FileNotFoundError: # Custom error message for FileNotFoundError raise FileNotFoundError(\\"Error: The specified file was not found.\\") except ValueError as ve: # Handle empty file scenario raise ValueError(\\"Error: The file is empty.\\") except Exception as ex: # Handle unexpected errors raise ProcessingError(\\"An unexpected error occurred.\\")"},{"question":"**Question: Implement an Enhanced Interactive Python Console** Using the \\"code\\" module, create a custom interactive Python interpreter that supports the following features: 1. Execution of dynamically entered Python code. 2. Syntax checking before execution. 3. Tracking and returning a list of all successfully executed commands. # Implementation Details: - You need to create a class named `EnhancedInteractiveConsole`, derived from `code.InteractiveConsole`. - The class should include methods to: - Check the syntax of the entered code before execution. - Execute the code if it passes the syntax check. - Track and store all executed commands. # Methods to Implement: 1. `__init__(self)`: Initialize the console and any necessary data structures. 2. `check_syntax(self, code_str: str) -> bool`: Check if the provided `code_str` has valid Python syntax. Return `True` if the syntax is valid, `False` otherwise. 3. `execute_code(self, code_str: str) -> str`: Execute the provided `code_str` if it has valid syntax and return the result or an error message. Track the successful execution of commands. 4. `get_executed_commands(self) -> list`: Return a list of all successfully executed commands. # Input and Output Formats: - The `check_syntax` method should take a single string `code_str` as input and return a boolean indicating if the syntax is valid. - The `execute_code` method should take a single string `code_str` as input and return a string result or error message. - The `get_executed_commands` method should return a list of strings representing all executed commands. # Constraints and Performance Requirements: - Your solution should handle multi-line Python code input. - It should handle incomplete code gracefully and prompt for additional input until complete. - Aim for efficient and minimal use of additional resources. # Example Usage: ```python console = EnhancedInteractiveConsole() # Checking syntax print(console.check_syntax(\'print(\\"Hello, World!\\")\')) # Output: True print(console.check_syntax(\'print(\\"Hello, World!\')) # Output: False # Executing code print(console.execute_code(\'print(\\"Hello, World!\\")\')) # Output: Hello, World! print(console.execute_code(\'print(\\"Hello, World!\')) # Output: Syntax Error # Get executed commands print(console.get_executed_commands()) # Output: [\'print(\\"Hello, World!\\")\'] ``` **Note**: Make sure your implementation handles different Python code edge cases and adheres to Python\'s interactive behavior.","solution":"import code import traceback class EnhancedInteractiveConsole(code.InteractiveConsole): def __init__(self): super().__init__() self.executed_commands = [] def check_syntax(self, code_str: str) -> bool: try: compile(code_str, \'<string>\', \'exec\') return True except SyntaxError: return False def execute_code(self, code_str: str) -> str: if not self.check_syntax(code_str): return \\"Syntax Error\\" try: exec(code_str, self.locals) self.executed_commands.append(code_str) return \\"Executed\\" except Exception as e: return f\\"Error: {str(e)}\\" def get_executed_commands(self) -> list: return self.executed_commands"},{"question":"# Question You are tasked with implementing custom utility functions using the `operator` module in Python. Implement the following three functions: 1. **custom_math_operations(a, b)**: - This function takes two integers `a` and `b` and returns a dictionary with the following keys and their corresponding values: - `\'sum\'`: The sum of `a` and `b`. - `\'difference\'`: The difference when `b` is subtracted from `a`. - `\'product\'`: The product when `a` is multiplied by `b`. - `\'quotient\'`: The quotient when `a` is divided by `b`. Use true division. 2. **count_occurrences(sequence, value)**: - This function takes a sequence (list, tuple, or string) and a value, and returns the number of times the value occurs in the sequence. 3. **filter_and_concat(seq1, seq2, key)**: - This function takes two sequences `seq1` and `seq2`, and a key (function) that returns a boolean value. It filters `seq1` and `seq2` using the `key`. It then returns the concatenation of the filtered sequences. # Constraints 1. Do not use any inbuilt Python functions for these operations except those from the `operator` module. 2. Ensure your implementation is efficient and leverages the `operator` module for performing the operations. # Example ```python # Assuming the \'operator\' module is already imported. # Example for custom_math_operations print(custom_math_operations(10, 5)) # Expected Output: {\'sum\': 15, \'difference\': 5, \'product\': 50, \'quotient\': 2.0} # Example for count_occurrences print(count_occurrences(\\"hello world\\", \'o\')) # Expected Output: 2 # Example for filter_and_concat print(filter_and_concat([1, 2, 3], [4, 5, 6], lambda x: x % 2 == 0)) # Expected Output: [2, 4, 6] ``` # Implementation ```python import operator def custom_math_operations(a, b): return { \'sum\': operator.add(a, b), \'difference\': operator.sub(a, b), \'product\': operator.mul(a, b), \'quotient\': operator.truediv(a, b) } def count_occurrences(sequence, value): return operator.countOf(sequence, value) def filter_and_concat(seq1, seq2, key): filtered_seq1 = [item for item in seq1 if key(item)] filtered_seq2 = [item for item in seq2 if key(item)] return operator.concat(filtered_seq1, filtered_seq2) ``` Write your implementations for the above functions following the given constraints.","solution":"import operator def custom_math_operations(a, b): return { \'sum\': operator.add(a, b), \'difference\': operator.sub(a, b), \'product\': operator.mul(a, b), \'quotient\': operator.truediv(a, b) } def count_occurrences(sequence, value): return operator.countOf(sequence, value) def filter_and_concat(seq1, seq2, key): filtered_seq1 = [item for item in seq1 if key(item)] filtered_seq2 = [item for item in seq2 if key(item)] return operator.concat(filtered_seq1, filtered_seq2)"},{"question":"You are tasked with designing a system to manage a library of digital media using Python\'s `abc` module. Your objective is to create a set of abstract base classes that define common interfaces and enforce implementation requirements for different types of digital media. Requirements 1. **Create an Abstract Base Class `Media`**: - This class should contain an abstract method `play()` which all media types must implement. - It should also contain an abstract property `duration` representing the length of the media. 2. **Create Subclasses for Specific Media Types**: - **Audio**: Inherits from `Media`, must implement the `play()` method, and the `duration` property. - **Video**: Inherits from `Media`, must implement the `play()` method, and the `duration` property. - **LiveStream**: Inherits from `Media`, must implement the `play()` method but the `duration` property should always return `None`. 3. **Implement Additional Functionality**: - Create a method `Media.add_media_type(media_class)` to register a new media type dynamically as a virtual subclass of the `Media` base class. Implementation Details - Use the `abc` module to define your abstract base classes and abstract methods. - Ensure that concrete subclasses properly override the abstract methods and properties. - Implement tests to confirm that `issubclass` and `isinstance` checks work as expected with both direct and virtual subclasses. Example ```python from abc import ABC, abstractmethod, ABCMeta class Media(ABC): @abstractmethod def play(self): pass @property @abstractmethod def duration(self): pass @classmethod def add_media_type(cls, media_class): cls.register(media_class) class Audio(Media): def __init__(self, duration): self._duration = duration def play(self): print(\\"Playing audio...\\") @property def duration(self): return self._duration class Video(Media): def __init__(self, duration): self._duration = duration def play(self): print(\\"Playing video...\\") @property def duration(self): return self._duration class LiveStream(Media): def play(self): print(\\"Playing live stream...\\") @property def duration(self): return None # Register a new media type dynamically Media.add_media_type(Audio) # Example usage audio = Audio(duration=300) video = Video(duration=600) live_stream = LiveStream() print(isinstance(audio, Media)) # True print(isinstance(video, Media)) # True print(isinstance(live_stream, Media)) # True print(audio.duration) # 300 print(video.duration) # 600 print(live_stream.duration) # None audio.play() # Playing audio... video.play() # Playing video... live_stream.play() # Playing live stream... ``` # Constraints - You must use `abc` module for defining abstract base classes and methods. - The `play` method should just print a message, no actual media playing functionalities are required. - Duration should be handled as an integer value representing seconds. - Ensure that the subclass registration mechanism works correctly. # Performance Requirements - The solution should handle dynamic registration of media classes efficiently.","solution":"from abc import ABC, abstractmethod class Media(ABC): @abstractmethod def play(self): pass @property @abstractmethod def duration(self): pass @classmethod def add_media_type(cls, media_class): cls.register(media_class) class Audio(Media): def __init__(self, duration): self._duration = duration def play(self): print(\\"Playing audio...\\") @property def duration(self): return self._duration class Video(Media): def __init__(self, duration): self._duration = duration def play(self): print(\\"Playing video...\\") @property def duration(self): return self._duration class LiveStream(Media): def play(self): print(\\"Playing live stream...\\") @property def duration(self): return None # Register a new media type dynamically Media.add_media_type(Audio)"},{"question":"Objective To assess your understanding of the seaborn library, specifically focusing on setting plot contexts, scaling font elements, and overriding context parameters. Problem Statement You are provided with a dataset representing the monthly sales of a product over one year. Your task is to create two line plots using seaborn. The first plot should use the default \\"notebook\\" context, and the second plot should use a customized context with scaled-up font and thicker lines. Additionally, save these plots as image files. Dataset The dataset is provided as a dictionary: ```python sales_data = { \'month\': [\'Jan\', \'Feb\', \'Mar\', \'Apr\', \'May\', \'Jun\', \'Jul\', \'Aug\', \'Sep\', \'Oct\', \'Nov\', \'Dec\'], \'sales\': [1500, 1800, 2600, 2300, 2800, 3000, 3200, 3100, 2900, 3300, 3600, 4000] } ``` Expected Input and Output Formats - Input: No input required. The dataset is given within the script. - Output: Two image files saved locally. - `default_context_plot.png`: Line plot using the default \\"notebook\\" context. - `custom_context_plot.png`: Line plot with a customized context (font_scale=1.5, lines.linewidth=3). Instructions 1. Create a line plot using seaborn with the default \\"notebook\\" context. 2. Save the plot as `default_context_plot.png`. 3. Create a line plot using seaborn with a customized context (font_scale=1.5, lines.linewidth=3). 4. Save the plot as `custom_context_plot.png`. Constraints - Ensure that all necessary imports are included in your script. - Use clear and appropriate variable names. - The plots should be visually distinct as per the context settings. Here\'s a template to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt # Provided dataset sales_data = { \'month\': [\'Jan\', \'Feb\', \'Mar\', \'Apr\', \'May\', \'Jun\', \'Jul\', \'Aug\', \'Sep\', \'Oct\', \'Nov\', \'Dec\'], \'sales\': [1500, 1800, 2600, 2300, 2800, 3000, 3200, 3100, 2900, 3300, 3600, 4000] } # Create default context plot sns.set_context(\\"notebook\\") plt.figure(figsize=(10, 6)) sns.lineplot(x=sales_data[\'month\'], y=sales_data[\'sales\']) plt.title(\'Monthly Sales - Default Context\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.savefig(\'default_context_plot.png\') plt.close() # Create custom context plot sns.set_context(\\"notebook\\", font_scale=1.5, rc={\\"lines.linewidth\\": 3}) plt.figure(figsize=(10, 6)) sns.lineplot(x=sales_data[\'month\'], y=sales_data[\'sales\']) plt.title(\'Monthly Sales - Custom Context\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.savefig(\'custom_context_plot.png\') plt.close() ``` Submission Submit: 1. Your complete script file named `sales_line_plots.py`. 2. The two generated image files: `default_context_plot.png` and `custom_context_plot.png`.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Provided dataset sales_data = { \'month\': [\'Jan\', \'Feb\', \'Mar\', \'Apr\', \'May\', \'Jun\', \'Jul\', \'Aug\', \'Sep\', \'Oct\', \'Nov\', \'Dec\'], \'sales\': [1500, 1800, 2600, 2300, 2800, 3000, 3200, 3100, 2900, 3300, 3600, 4000] } def plot_sales_data(): # Create default context plot sns.set_context(\\"notebook\\") plt.figure(figsize=(10, 6)) sns.lineplot(x=sales_data[\'month\'], y=sales_data[\'sales\']) plt.title(\'Monthly Sales - Default Context\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.savefig(\'default_context_plot.png\') plt.close() # Create custom context plot sns.set_context(\\"notebook\\", font_scale=1.5, rc={\\"lines.linewidth\\": 3}) plt.figure(figsize=(10, 6)) sns.lineplot(x=sales_data[\'month\'], y=sales_data[\'sales\']) plt.title(\'Monthly Sales - Custom Context\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.savefig(\'custom_context_plot.png\') plt.close() # Run the function to generate plots plot_sales_data()"},{"question":"Question # Objective Write a function that generates various types of plots using pandas based on provided data, type of plot, and additional customization parameters. # Function Signature ```python import pandas as pd import matplotlib.pyplot as plt def generate_plot(data: pd.DataFrame, plot_type: str, **kwargs) -> plt.Figure: Generates a specified type of plot using the provided DataFrame and customization options. Parameters: data (pd.DataFrame): Input data for plotting. plot_type (str): Type of plot to generate. Supported types are \'line\', \'bar\', \'hist\', \'box\', \'kde\', \'area\', \'scatter\', \'hexbin\', \'pie\'. **kwargs: Additional keyword arguments for customization. Returns: plt.Figure: The matplotlib figure object containing the generated plot. pass ``` # Requirements 1. **Input**: - `data`: A pandas DataFrame containing the data to be plotted. - `plot_type`: A string specifying the type of plot to generate. Supported types include: - `\'line\'` for line plot - `\'bar\'` for bar plot - `\'hist\'` for histogram - `\'box\'` for box plot - `\'kde\'` for density plot - `\'area\'` for area plot - `\'scatter\'` for scatter plot - `\'hexbin\'` for hexagonal bin plot - `\'pie\'` for pie chart - `**kwargs`: Additional keyword arguments to customize the plot. These can include color, legend, labels, etc. 2. **Output**: - The function should return the matplotlib `Figure` object containing the generated plot. 3. **Constraints**: - The function should handle missing data appropriately as described in the provided documentation. - The function should raise a `ValueError` if an unsupported plot type is specified. - The function should be able to utilize the customization options passed via `**kwargs`. 4. **Performance**: - The function should be efficient in handling moderately sized DataFrames (up to a few thousand rows and columns). # Example Usage ```python import pandas as pd import numpy as np # Example DataFrame np.random.seed(123) data = pd.DataFrame({ \'A\': np.random.randn(100), \'B\': np.random.rand(100) * 10, \'C\': np.random.randint(1, 100, size=100) }) # Generate a line plot fig = generate_plot(data, plot_type=\'line\', title=\'Line Plot\', xlabel=\'Index\', ylabel=\'Values\', legend=True) fig.show() # Generate a scatter plot fig = generate_plot(data, plot_type=\'scatter\', x=\'A\', y=\'B\', color=\'DarkBlue\') fig.show() ``` # Additional Notes - Use the pandas plotting functions and matplotib customization options as described in the provided documentation. - Test the function thoroughly to ensure all plot types and customization options are working as expected.","solution":"import pandas as pd import matplotlib.pyplot as plt def generate_plot(data: pd.DataFrame, plot_type: str, **kwargs) -> plt.Figure: Generates a specified type of plot using the provided DataFrame and customization options. Parameters: data (pd.DataFrame): Input data for plotting. plot_type (str): Type of plot to generate. Supported types are \'line\', \'bar\', \'hist\', \'box\', \'kde\', \'area\', \'scatter\', \'hexbin\', \'pie\'. **kwargs: Additional keyword arguments for customization. Returns: plt.Figure: The matplotlib figure object containing the generated plot. # Create a new figure fig, ax = plt.subplots() # Choose the appropriate plot type and apply kwargs if plot_type == \'line\': data.plot(kind=\'line\', ax=ax, **kwargs) elif plot_type == \'bar\': data.plot(kind=\'bar\', ax=ax, **kwargs) elif plot_type == \'hist\': data.plot(kind=\'hist\', ax=ax, **kwargs) elif plot_type == \'box\': data.plot(kind=\'box\', ax=ax, **kwargs) elif plot_type == \'kde\': data.plot(kind=\'kde\', ax=ax, **kwargs) elif plot_type == \'area\': data.plot(kind=\'area\', ax=ax, **kwargs) elif plot_type == \'scatter\': if \'x\' not in kwargs or \'y\' not in kwargs: raise ValueError(\\"For scatter plot, \'x\' and \'y\' parameters are required.\\") data.plot(kind=\'scatter\', ax=ax, **kwargs) elif plot_type == \'hexbin\': if \'x\' not in kwargs or \'y\' not in kwargs: raise ValueError(\\"For hexbin plot, \'x\' and \'y\' parameters are required.\\") data.plot(kind=\'hexbin\', ax=ax, **kwargs) elif plot_type == \'pie\': # For pie plot, we need to plot one column of the DataFrame if \'y\' not in kwargs: raise ValueError(\\"For pie chart, \'y\' parameter is required.\\") data[kwargs[\'y\']].plot(kind=\'pie\', ax=ax, **kwargs) else: raise ValueError(f\\"Unsupported plot type: {plot_type}\\") # Apply additional customization if any if \'title\' in kwargs: ax.set_title(kwargs[\'title\']) if \'xlabel\' in kwargs: ax.set_xlabel(kwargs[\'xlabel\']) if \'ylabel\' in kwargs: ax.set_ylabel(kwargs[\'ylabel\']) return fig"},{"question":"**Objective:** Implement a function that generates an email in its MIME-compliant serialized byte stream using the `email.generator.BytesGenerator` class. The function must demonstrate understanding and manipulation of key features, including content transfer encoding and policy configurations. **Function Signature:** ```python def generate_email_byte_stream(msg: EmailMessage, policy=None, mangle_from_: bool = None, maxheaderlen: int = None) -> bytes: pass ``` **Input:** - `msg` (EmailMessage): The email message object to be serialized. - `policy` (optional): The policy to control message generation. Default is `None`, which means the policy associated with the `EmailMessage` object will be used. - `mangle_from_` (optional) (bool): If `True`, mangles \\"From \\" lines in the email body. Default is based on the policy setting. - `maxheaderlen` (optional) (int): If specified, refolds any header lines longer than this value. If `0`, headers will not be rewrapped. Default is based on the policy setting. **Output:** - `bytes`: A serialized byte stream of the provided email message. **Constraints:** - The function should use `BytesGenerator` for serialization. - Ensure proper handling of different content transfer encodings (7bit, 8bit) as per the provided policy. - The function should handle optional `unixfrom` and `linesep` settings. # Example: ```python from email.message import EmailMessage from email.policy import default def generate_email_byte_stream(msg: EmailMessage, policy=None, mangle_from_: bool = None, maxheaderlen: int = None) -> bytes: Serialize the provided EmailMessage into a MIME-compliant byte stream. import io from email.generator import BytesGenerator if policy is None: policy = msg.policy with io.BytesIO() as buffer: gen = BytesGenerator(buffer, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) gen.flatten(msg) return buffer.getvalue() # Usage msg = EmailMessage() msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email.\') byte_stream = generate_email_byte_stream(msg) print(byte_stream) ``` # Additional Notes: - The example above includes basic usage of the `generate_email_byte_stream` function. - Students are expected to ensure the function can handle various email structures, including different policies and content encodings. **Assessment Criteria:** - Correctness: Function produces the correct byte stream as per the email message provided. - Handling of optional parameters: Proper usage and default behaviors based on policy. - Code readability and documentation: Clear and understandable code with appropriate comments.","solution":"from email.message import EmailMessage from email.generator import BytesGenerator from email.policy import default import io def generate_email_byte_stream(msg: EmailMessage, policy=None, mangle_from_: bool = None, maxheaderlen: int = None) -> bytes: Serialize the provided EmailMessage into a MIME-compliant byte stream. Parameters: - msg: The EmailMessage object to be serialized. - policy: The policy to control message generation. Default is None. - mangle_from_: If True, \\"From \\" lines in the body will be mangled. - maxheaderlen: Maximum length for header lines before they are folded. Returns: - bytes: A serialized byte stream of the provided email message. if policy is None: policy = msg.policy with io.BytesIO() as buffer: generator = BytesGenerator(buffer, policy=policy, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen) generator.flatten(msg) return buffer.getvalue()"},{"question":"# Advanced Python Coding Assessment **Objective:** Your task is to implement a custom context manager using the `contextlib` module that handles multiple resources, ensuring all resources are properly managed, even in the presence of exceptions. This context manager should be reentrant and reusable. **Problem Statement:** You need to write a class `ResourceManager` which will ensure that multiple resources, provided at the time of instantiation either as context managers or as simple resources with custom `acquire` and `release` methods, are properly handled. Your context manager should ensure: 1. All resources are acquired at the start of the with block. 2. If acquiring any resource raises an exception, all previously acquired resources should be released. 3. All resources should be properly released when leaving the with block, irrespective of exceptions. 4. The context manager should support reentrancy and reusability. **Specification:** Define the `ResourceManager` class with the following constraints: 1. The initializer `__init__(self, *resources)` should accept multiple resources as arguments. 2. Resources can be given as context managers or as tuples of the form `(acquire_func, release_func)`. 3. Support reentrancy with nested `with` statements or using it as a decorator when appropriate. 4. Make sure your implementation leverages `contextlib.ExitStack`. **Example Usage:** ```python from contextlib import contextmanager # Example context manager @contextmanager def example_cm(name): print(f\\"Acquiring {name}\\") yield name print(f\\"Releasing {name}\\") # Custom resource acquire/release functions def acquire_example_resource(name): print(f\\"Acquiring resource {name}\\") return name def release_example_resource(name): print(f\\"Releasing resource {name}\\") # Define the custom ResourceManager class here class ResourceManager: def __init__(self, *resources): pass # Your implementation here def __enter__(self): pass # Your implementation here def __exit__(self, exc_type, exc_value, traceback): pass # Your implementation here # Example resources resources = [ example_cm(\\"resource1\\"), (lambda: acquire_example_resource(\\"resource2\\"), lambda r: release_example_resource(\\"resource2\\")) ] # Use the ResourceManager in a with statement with ResourceManager(*resources) as res: print(\\"Using resources\\") # Reentrant use rm = ResourceManager(*resources) with rm as res: print(\\"Using resources again inside another context\\") ``` **Expected Output:** When running the provided example usage: ``` Acquiring resource1 Acquiring resource2 Using resources Releasing resource2 Releasing resource1 Acquiring resource1 Acquiring resource2 Using resources again inside another context Releasing resource2 Releasing resource1 ``` **Constraints:** - Ensure reentrancy. - Ensure resources are properly managed even if exceptions occur. - You are allowed to use `contextlib.ExitStack` for managing the resources. **Evaluation Criteria:** - Correct usage and acquisition of resources. - Proper handling and reentrancy. - Efficient and clear implementation.","solution":"from contextlib import contextmanager, ExitStack class ResourceManager: def __init__(self, *resources): self.resources = resources self.exit_stack = ExitStack() self.acquired_resources = [] def __enter__(self): for resource in self.resources: if isinstance(resource, tuple) and len(resource) == 2: acquire_func, release_func = resource acquired = acquire_func() self.exit_stack.callback(release_func, acquired) self.acquired_resources.append(acquired) else: context = self.exit_stack.enter_context(resource) self.acquired_resources.append(context) return self.acquired_resources def __exit__(self, exc_type, exc_value, traceback): self.exit_stack.close()"},{"question":"# Password Secure Storage and Verification with `hashlib` You are tasked with developing a small library for securely hashing passwords and verifying those hashed passwords. This should utilize the `hashlib` module\'s functionalities, specifically focusing on: 1. Secure password hashing using key derivation functions. 2. Storing and retrieving hashed passwords. 3. Verifying a password against a stored hash. # Requirements: 1. Implement a function `hash_password` that: - Inputs: `password` (string) and `salt` (optional, can be None). - Outputs: A tuple containing the `salt` (as bytes) and the `hashed password` (as hex string). - If `salt` is not provided, the function should generate a new random salt using `os.urandom()` (16 bytes). - Use the `hashlib.pbkdf2_hmac` function for hashing, with SHA256 as the hash algorithm, and set the number of iterations to 100,000. The derived key length (`dklen`) should be 64 bytes. 2. Implement a function `verify_password` that: - Inputs: `password` (string), `salt` (bytes), and `hashed_password` (hex string). - Outputs: A boolean indicating whether the password matches the stored hash. - Use the same hashing mechanism as the `hash_password` function to verify if the provided password matches the given hash. # Function signatures: Both functions should have the following signatures: ```python import hashlib import os def hash_password(password: str, salt: bytes = None) -> (bytes, str): pass def verify_password(password: str, salt: bytes, hashed_password: str) -> bool: pass ``` # Constraints: - Password should be of type string and can include any ASCII characters. - Salt should be exactly 16 bytes. - The implementation should handle any type of error gracefully and use secure, clean coding practices. # Example: ```python import hashlib import os def hash_password(password: str, salt: bytes = None) -> (bytes, str): if salt is None: salt = os.urandom(16) dk = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, 100000, dklen=64) return salt, dk.hex() def verify_password(password: str, salt: bytes, hashed_password: str) -> bool: dk = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, 100000, dklen=64) return dk.hex() == hashed_password # Example usage salt, hashed = hash_password(\'mypassword\') print(salt, hashed) print(verify_password(\'mypassword\', salt, hashed)) # Should return True print(verify_password(\'wrongpassword\', salt, hashed)) # Should return False ``` Please ensure your functions follow the outlined requirements and constraints, and thoroughly test them with various inputs to validate their correctness.","solution":"import hashlib import os def hash_password(password: str, salt: bytes = None) -> (bytes, str): Hash a password with an optional salt. :param password: String. The password to hash. :param salt: Bytes. An optional salt. If not provided, a new one will be generated. :return: A tuple (salt, hashed_password). if salt is None: salt = os.urandom(16) hashed_password = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, 100000, dklen=64) return salt, hashed_password.hex() def verify_password(password: str, salt: bytes, hashed_password: str) -> bool: Verify a password against a given salt and hashed password. :param password: String. The password to verify. :param salt: Bytes. The salt used during the hashing of the original password. :param hashed_password: String. The original hashed password (in hex). :return: Boolean. True if the passwords match, False otherwise. computed_hash = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, 100000, dklen=64) return computed_hash.hex() == hashed_password"},{"question":"**Objective:** Use seaborn to create a series of kernel density estimation plots that cover both univariate and bivariate distributions and include some advanced customizations using provided input data. **Description:** Write a function `plot_kde_distributions(data: pd.DataFrame) -> None` that takes a pandas DataFrame as input and generates the following plots using seaborn\'s `kdeplot`: 1. **Univariate KDE Plot:** - Plot the distribution of the column `total_bill` on the x-axis. - Add a second plot where `total_bill` is plotted on the y-axis instead. 2. **Wide-Form Dataset KDE Plot:** - Plot the distributions for each numeric column in the DataFrame `iris`. 3. **Customized Bandwidth and Smoothing:** - Create a KDE plot of `total_bill` with less smoothing (`bw_adjust=0.2`). - Create another KDE plot of `total_bill` with more smoothing (`bw_adjust=5`) and ensure that the plot does not smooth past the extreme data points (`cut=0`). 4. **Conditional Distributions with Hue Mapping:** - Plot the KDE of `total_bill` with a hue representing the time (column `time`). - Create another plot with the KDE of `total_bill`, with hue and stacked distribution (`multiple=\'stack\'`). 5. **Bivariate KDE Plot:** - Plot the joint distribution of `waiting` and `duration` with filled contours. - In a separate plot, map a third variable `kind` (hue) to show conditional distributions and reduce the contour levels (`levels=5`) and threshold (`thresh=0.2`). **Note:** - Use the datasets `tips` and `iris` provided by seaborn using `sns.load_dataset()`. - Set the theme using `sns.set_theme()` at the beginning of your function. - The function does not return any value but should display all the required plots. # Example Function Signature ```python import seaborn as sns import pandas as pd def plot_kde_distributions(data: pd.DataFrame) -> None: # Your implementation here ``` # Constraints 1. You must use the seaborn package for all plots. 2. The input `data` DataFrame will contain at least the columns `total_bill`, `time`, `waiting`, `duration`, and `kind`. 3. Adhere to good coding practices, including appropriate use of functions and comments. # Expected Output The function should generate and display the required plots as specified without any errors. *Hint:* Refer to the seaborn documentation to see how to use different parameters in `sns.kdeplot`.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_kde_distributions(data: pd.DataFrame) -> None: sns.set_theme() # Univariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'total_bill\') plt.title(\'Univariate KDE Plot of total_bill on x-axis\') plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, y=\'total_bill\') plt.title(\'Univariate KDE Plot of total_bill on y-axis\') # Wide-Form Dataset KDE Plot iris = sns.load_dataset(\'iris\') plt.figure(figsize=(10, 6)) sns.kdeplot(data=iris) plt.title(\'KDE Plot for each numeric column in iris dataset\') # Customized Bandwidth and Smoothing plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'total_bill\', bw_adjust=0.2) plt.title(\'KDE Plot of total_bill with less smoothing (bw_adjust=0.2)\') plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'total_bill\', bw_adjust=5, cut=0) plt.title(\'KDE Plot of total_bill with more smoothing (bw_adjust=5, cut=0)\') # Conditional Distributions with Hue Mapping plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'total_bill\', hue=\'time\') plt.title(\'KDE of total_bill with hue representing time\') plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'total_bill\', hue=\'time\', multiple=\'stack\') plt.title(\'KDE of total_bill with hue representing time and stacked\') # Bivariate KDE Plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'waiting\', y=\'duration\', fill=True) plt.title(\'Bivariate KDE Plot of waiting and duration with filled contours\') plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'waiting\', y=\'duration\', hue=\'kind\', levels=5, thresh=0.2) plt.title(\'Bivariate KDE Plot of waiting and duration with kind hue, 5 levels, and threshold 0.2\') plt.show()"},{"question":"**Question: Porting Function for Text and Binary Data Handling in Python** You are tasked with writing two utility functions as part of a package that deals with simultaneously supporting both Python 2.7 and Python 3.x. The functions should help in handling text and binary data, which is a crucial aspect of making code compatible across these Python versions. # Function 1: `ensure_text` This function takes an input and ensures that it is returned as a text (unicode) object. If the input is already a text object, it should be returned unchanged. If the input is a binary (bytes) object, it should be decoded to text using UTF-8 encoding. # Function 2: `ensure_binary` This function takes an input and ensures that it is returned as a binary (bytes) object. If the input is already a binary object, it should be returned unchanged. If the input is a text object, it should be encoded to binary using UTF-8 encoding. # Specifications 1. **Functions to Implement**: - `def ensure_text(data):` - `def ensure_binary(data):` 2. **Input**: - For `ensure_text`: a string (unicode) or bytes object - For `ensure_binary`: a string (unicode) or bytes object 3. **Output**: - For `ensure_text`: a Unicode string - For `ensure_binary`: a bytes object 4. **Constraints**: - Handle only UTF-8 encoding for simplicity. - The functions should work correctly in both Python 2.7 and 3.x. 5. **Assumptions**: - Input data will be either of type `str` or `bytes`, and in Python 2, `str` may represent both text and binary data. Your functions should handle these types correctly. # Examples ```python # Python 3 usage # ensure_text print(ensure_text(b\'hello\')) # Output: \'hello\' print(ensure_text(\'hello\')) # Output: \'hello\' # ensure_binary print(ensure_binary(b\'hello\')) # Output: b\'hello\' print(ensure_binary(\'hello\')) # Output: b\'hello\' # Python 2 usage # ensure_text print(ensure_text(b\'hello\')) # Output: u\'hello\' print(ensure_text(u\'hello\')) # Output: u\'hello\' # ensure_binary print(ensure_binary(b\'hello\')) # Output: b\'hello\' print(ensure_binary(u\'hello\')) # Output: b\'hello\' ``` # Implementation Implement both functions ensuring compatibility with Python 2.7 and 3.x utilizing appropriate compatibility handling for string types. ```python def ensure_text(data): # Insert your code here pass def ensure_binary(data): # Insert your code here pass ``` Note: You can use the `six` library or any `__future__` imports to help with compatibility if necessary.","solution":"import six def ensure_text(data): Ensure that the input is returned as a text (unicode) object. Parameters: data (str or bytes): The input data to convert. Returns: str: The input data as a text (unicode) object. if isinstance(data, six.binary_type): return data.decode(\'utf-8\') return data def ensure_binary(data): Ensure that the input is returned as a binary (bytes) object. Parameters: data (str or bytes): The input data to convert. Returns: bytes: The input data as a binary (bytes) object. if isinstance(data, six.text_type): return data.encode(\'utf-8\') return data"},{"question":"Title: Robust Data Processing and Error Handling Description: You are required to write a Python function `process_data(input_file: str) -> dict` that processes data from an input file. The function should read integers from the file and calculate their sum, average, and product. The function should handle various exceptions, including file not found, invalid data, and division by zero. Additionally, it should ensure that the file is properly closed after processing, using appropriate exception handling and resource management techniques. Function Signature: ```python def process_data(input_file: str) -> dict: pass ``` Input: - `input_file` (str): The name of the input file containing integers, one per line. Output: - (dict): A dictionary with the following keys: - `sum` (int): The sum of all integers in the file. - `average` (float): The average of the integers. - `product` (int): The product of all integers. Exceptions: - If the file does not exist, raise a `FileNotFoundError` with an appropriate message. - If a line in the file cannot be converted to an integer, raise a `ValueError` with an appropriate message. - If the file is empty, or if the calculation of the average leads to a division by zero error, raise a `ZeroDivisionError` with an appropriate message. Example Usage: ```python # Assume \'data.txt\' contains: # 2 # 3 # 4 result = process_data(\'data.txt\') print(result) # Output: {\'sum\': 9, \'average\': 3.0, \'product\': 24} ``` Constraints: - Use try-except blocks to handle various exceptions as described. - Use the `with` statement to ensure the file is properly closed after processing. - Make sure to handle and re-raise exceptions as needed to indicate issues to the caller. Notes: - Ensure your solution is both robust and efficient. - Your function should handle all edge cases, including empty files, invalid data, and large inputs. Testing: Your solution will be tested against various test cases, including: - A file with multiple integers. - A file with non-integer data. - An empty file. - A non-existent file.","solution":"def process_data(input_file: str) -> dict: try: with open(input_file, \'r\') as file: numbers = file.readlines() if not numbers: raise ZeroDivisionError(\\"The file is empty, cannot calculate average.\\") int_numbers = [] for line in numbers: try: int_numbers.append(int(line.strip())) except ValueError: raise ValueError(f\\"Invalid integer value in the file: \'{line.strip()}\'\\") total_sum = sum(int_numbers) total_count = len(int_numbers) average = total_sum / total_count total_product = 1 for num in int_numbers: total_product *= num return {\\"sum\\": total_sum, \\"average\\": average, \\"product\\": total_product} except FileNotFoundError: raise FileNotFoundError(f\\"File {input_file} not found.\\") except ZeroDivisionError as zde: raise ZeroDivisionError(zde)"},{"question":"Objective Design a Python program that demonstrates your understanding of handling multiple asynchronous signals using the `signal` module. Task Create a program that performs the following operations: 1. **Signal Setup**: - Set up a custom signal handler that: - Prints a message indicating which signal was received. - Tracks the number of times each signal is received. - Handles `SIGINT` (keyboard interrupt) separately by printing a specific message and terminating the program gracefully. 2. **Alarm Setup**: - Use the `signal.alarm()` function to schedule a `SIGALRM` signal after 10 seconds of the program\'s execution. 3. **Main Execution**: - Enter an infinite loop where the program prints a message every 2 seconds. - Await and handle specific signals asynchronously without blocking the execution of the main loop. 4. **Output Statistics**: - Upon termination from a `SIGINT` signal, output the count of each type of signal received during the program\'s execution. Constraints - Do not use multithreading. - Do not block the main execution loop while handling signals. - The solution should handle and report at least the following signals: `SIGINT`, `SIGALRM`, and at least two other user-defined signals (`SIGUSR1`, `SIGUSR2`). Boilerplate Code ```python import signal import time import sys # Global dictionary to track signal counts signal_counts = { signal.SIGINT: 0, signal.SIGALRM: 0, signal.SIGUSR1: 0, signal.SIGUSR2: 0, } def signal_handler(signum, frame): global signal_counts if signum in signal_counts: signal_counts[signum] += 1 if signum == signal.SIGINT: print(\\"nSIGINT received. Terminating gracefully...\\") print(\\"Signal statistics:\\") for sig in signal_counts: print(f\\"{signal.strsignal(sig)}: {signal_counts[sig]} times\\") sys.exit(0) else: print(f\\"Received signal: {signal.strsignal(signum)}\\") def main(): # Signal handlers registration signal.signal(signal.SIGINT, signal_handler) signal.signal(signal.SIGALRM, signal_handler) signal.signal(signal.SIGUSR1, signal_handler) signal.signal(signal.SIGUSR2, signal_handler) # Schedule an alarm signal in 10 seconds signal.alarm(10) # Main loop while True: print(\\"Program running... Press Ctrl+C to exit.\\") time.sleep(2) if __name__ == \\"__main__\\": main() ``` Expected Output - The program should display a message every 2 seconds, indicating it is running. - Upon receiving `SIGUSR1` or `SIGUSR2`, it should print a respective signal message. - When `SIGALRM` is triggered after 10 seconds, the handler should indicate this. - Upon pressing `Ctrl+C` (sending `SIGINT`), the program should terminate gracefully, displaying the count of each received signal. **Note:** The program should work on Unix-based systems. Ensure proper testing on a compatible environment.","solution":"import signal import time import sys # Global dictionary to track signal counts signal_counts = { signal.SIGINT: 0, signal.SIGALRM: 0, signal.SIGUSR1: 0, signal.SIGUSR2: 0, } def signal_handler(signum, frame): if signum in signal_counts: signal_counts[signum] += 1 if signum == signal.SIGINT: print(\\"nSIGINT received. Terminating gracefully...\\") print(\\"Signal statistics:\\") for sig in signal_counts: print(f\\"{signal.strsignal(sig)}: {signal_counts[sig]} times\\") sys.exit(0) else: print(f\\"Received signal: {signal.strsignal(signum)}\\") def main(): # Signal handlers registration signal.signal(signal.SIGINT, signal_handler) signal.signal(signal.SIGALRM, signal_handler) signal.signal(signal.SIGUSR1, signal_handler) signal.signal(signal.SIGUSR2, signal_handler) # Schedule an alarm signal in 10 seconds signal.alarm(10) # Main loop while True: print(\\"Program running... Press Ctrl+C to exit.\\") time.sleep(2) if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Coding Assessment Question Objective In this task, you will demonstrate your understanding of handling numerical inaccuracies and batched computations in PyTorch. You will write a function to perform batched matrix multiplication and compare the results of batched operations with the equivalent non-batched operations. Problem Statement You need to implement a function `compare_batched_vs_non_batched`, which performs batched matrix multiplication and compares the results with non-batched matrix multiplication on each individual batch element. The function will then measure and return the numerical discrepancies between the two methods. Function Signature ```python import torch def compare_batched_vs_non_batched(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Performs batched matrix multiplication of tensors A and B, compares the results of batched operations with the results of non-batched operations, and returns the discrepancies between the two. Parameters: A (torch.Tensor): A 3D tensor with shape (N, M, K) where N is the batch size. B (torch.Tensor): A 3D tensor with shape (N, K, P) where N is the batch size. Returns: torch.Tensor: A 1D tensor of length N where each element i represents the discrepancy between (A@B)[i] and (A[i]@B[i]). Constraints: - The inputs A and B can have any batch size N, but both tensors must have the same batch size. - The matrices in the batch must be compatible for multiplication such that A.shape[-1] == B.shape[-2]. ``` Requirements and Constraints 1. You must use the provided function signature. 2. Assume the inputs A and B are valid tensors that are non-empty and meet the specified shape requirements. 3. The discrepancy for each batch element should be computed as the element-wise absolute difference between the batched result and the non-batched result. 4. You can use any relevant PyTorch functions to achieve the task, but be mindful of the precision issues discussed. 5. The function should return a 1D tensor of discrepancies, where each element corresponds to the discrepancy for the respective batch element. 6. Performance will be considered, but numerical accuracy and correctness are paramount. Example Usage ```python A = torch.randn(10, 5, 7) B = torch.randn(10, 7, 3) discrepancies = compare_batched_vs_non_batched(A, B) print(discrepancies) ``` In this example, matrix `A` has 10 batch elements of shape 5x7, and matrix `B` has 10 batch elements of shape 7x3. The function should compute the batched multiplication and compare with each element-wise result of multiplication, returning the discrepancies. Hints - Use `torch.bmm` for batched matrix multiplication. - Consider using `torch.abs` and `torch.sum` to measure discrepancies. - Be aware of potential overflow and numerical accuracy issues as discussed in the PyTorch documentation.","solution":"import torch def compare_batched_vs_non_batched(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Performs batched matrix multiplication of tensors A and B, compares the results of batched operations with the results of non-batched operations, and returns the discrepancies between the two. Parameters: A (torch.Tensor): A 3D tensor with shape (N, M, K) where N is the batch size. B (torch.Tensor): A 3D tensor with shape (N, K, P) where N is the batch size. Returns: torch.Tensor: A 1D tensor of length N where each element i represents the discrepancy between (A@B)[i] and (A[i]@B[i]). # Batched matrix multiplication batched_result = torch.bmm(A, B) # Non-batched matrix multiplication results discrepancies = torch.zeros(A.shape[0]) for i in range(A.shape[0]): non_batched_result = torch.mm(A[i], B[i]) discrepancy = torch.abs(batched_result[i] - non_batched_result).sum() discrepancies[i] = discrepancy return discrepancies"},{"question":"**Objective:** Write a Python function that takes a floating-point number, performs a series of arithmetic operations, and returns a result rounded to a specified precision. The function should also provide a detailed breakdown of the floating-point errors at each step of the calculation. **Function Signature:** ```python def floating_point_analysis(value: float, operations: list, precision: int) -> dict: Analyze floating-point arithmetic operations. Args: - value: A float, the initial value. - operations: A list of tuples where each tuple contains an operation (\'add\', \'sub\', \'mul\', \'div\') and a float operand. - precision: An integer, the number of significant digits to round the final result to. Returns: - A dictionary with keys: \'initial_value\': the input float, \'operations\': a list of tuples with the form (operation, operand, result after operation, exact representation after operation), \'final_result\': the final result after all operations rounded to the given precision, \'errors\': a list of errors in the form (expected, actual, difference) for each arithmetic operation step. pass ``` **Input:** - `value`: A floating-point number that serves as the starting point for the calculation. - `operations`: A list of tuples, where each tuple contains an arithmetic operation in the form of a string (\'add\', \'sub\', \'mul\', \'div\') and a numerical operand. For example, `[(\'add\', 0.1), (\'mul\', 2.5)]` means adding 0.1 to the value and then multiplying the result by 2.5. - `precision`: An integer specifying the number of significant digits to round the final result to. **Output:** - The function should return a dictionary containing: - `\'initial_value\'`: The initial floating-point value. - `\'operations\'`: A list of tuples, each containing the operation, operand, result after the operation, and its exact binary representation in the step. - `\'final_result\'`: The final result after performing all operations, rounded to the specified precision. - `\'errors\'`: A list of errors for each step in the format (expected result, actual result, difference). **Constraints:** - Avoid using external libraries for arithmetic except for built-in `math` module functions. - Focus on demonstrating an understanding of floating-point representation and error analysis. **Example:** ```python result = floating_point_analysis(0.1, [(\'add\', 0.2), (\'mul\', 3)], 10) Expected Output: { \'initial_value\': 0.1, \'operations\': [ (\'add\', 0.2, 0.30000000000000004, (0.1 + 0.2) exact representation), (\'mul\', 3, 0.9000000000000001, (0.30000000000000004 * 3) exact representation) ], \'final_result\': 0.9000000000, \'errors\': [ (0.3, 0.30000000000000004, 0.00000000000000004), (0.9, 0.9000000000000001, 0.0000000000000001) ] } ``` **Note:** - The operations should be performed in sequence, and each intermediate result should be used in the subsequent operation. - The exact representation should be the precise value stored by the computer for that float after the operation.","solution":"def floating_point_analysis(value: float, operations: list, precision: int) -> dict: result = value operation_details = [] errors = [] for op, operand in operations: previous_result = result if op == \'add\': result += operand elif op == \'sub\': result -= operand elif op == \'mul\': result *= operand elif op == \'div\': result /= operand exact_representation = float(format(result, \\".17g\\")) difference = abs((previous_result + operand if op == \'add\' else previous_result - operand if op == \'sub\' else previous_result * operand if op == \'mul\' else previous_result / operand) - result) operation_details.append((op, operand, result, exact_representation)) errors.append((exact_representation, result, difference)) final_result = round(result, precision) return { \'initial_value\': value, \'operations\': operation_details, \'final_result\': final_result, \'errors\': errors }"},{"question":"You are tasked with manipulating WAV files to extract specific parameters and create a custom output using the `wave` module in Python. This assessment requires you to write a function that reads a WAV file, extracts its parameters, and generates a new WAV file with modified properties. Function Signature ```python def manipulate_wav(input_file: str, output_file: str, frame_rate: int, num_channels: int) -> None: Reads a WAV file, extracts its parameters and frames, modifies the specified properties, and writes the data to a new WAV file. Args: - input_file (str): The path to the input WAV file. - output_file (str): The path to the output WAV file. - frame_rate (int): The new frame rate to be set for the output file. - num_channels (int): The new number of channels to be set for the output file. Returns: - None ``` Requirements 1. **Input File Properties Extraction**: Open the specified input WAV file and extract its parameters: - Number of channels - Sample width - Frame rate - Number of frames - Compression type and name 2. **Modify Specific Properties**: - Change the frame rate to the specified new frame rate. - Change the number of channels to the specified new number of channels. 3. **Write to Output File**: Write the extracted frames to a new output WAV file along with the modified properties: - Set the sample width and compression type/name to the same values as the input file. - Apply the new frame rate and number of channels. 4. **Error Handling**: Handle any potential errors that might occur during file operations, and ensure that files are properly closed after operations. Example Usage ```python # Given input_file.wav with properties: # - Number of channels: 2 (stereo) # - Sample width: 2 bytes # - Frame rate: 44100 Hz # - Number of frames: 100000 # - Compression type: NONE # - Compression name: not compressed # Set the new properties new_frame_rate = 22050 new_num_channels = 1 # Call the function to generate an output file with the modified properties manipulate_wav(\\"input_file.wav\\", \\"output_file.wav\\", new_frame_rate, new_num_channels) # The output_file.wav should now have: # - Number of channels: 1 (mono) # - Sample width: 2 bytes (same as input) # - Frame rate: 22050 Hz # - Number of frames: appropriately adjusted based on the frame rate change # - Compression type: NONE (same as input) # - Compression name: not compressed (same as input) ``` Constraints 1. The function must not use any external libraries aside from the standard library. 2. The input WAV file will always be valid and in \\"WAVE_FORMAT_PCM\\" format. 3. The output file should strictly follow the WAV specification.","solution":"import wave def manipulate_wav(input_file: str, output_file: str, frame_rate: int, num_channels: int) -> None: Reads a WAV file, extracts its parameters and frames, modifies the specified properties, and writes the data to a new WAV file. Args: - input_file (str): The path to the input WAV file. - output_file (str): The path to the output WAV file. - frame_rate (int): The new frame rate to be set for the output file. - num_channels (int): The new number of channels to be set for the output file. Returns: - None try: # Open the input WAV file with wave.open(input_file, \'rb\') as in_wav: params = in_wav.getparams() frames = in_wav.readframes(params.nframes) # Extract parameters n_channels, sampwidth, framerate, nframes, comptype, compname = params # Write to the output WAV file with modified parameters with wave.open(output_file, \'wb\') as out_wav: out_wav.setnchannels(num_channels) out_wav.setsampwidth(sampwidth) out_wav.setframerate(frame_rate) out_wav.setnframes(nframes) out_wav.setcomptype(comptype, compname) out_wav.writeframes(frames) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Pandas DataFrame Manipulation and Indexing You are given a DataFrame containing information about various products. Each product has the following attributes: `ProductID`, `Name`, `Category`, `Price`, `InStock`, `Rating`. You are required to perform several data processing tasks using pandas. Write a function `process_product_data(df)` that takes a DataFrame `df` as input and performs the following operations: 1. **Filter Products**: Select and return only the rows for products where the `InStock` is greater than 20 and the `Rating` is at least 4.0. Use boolean indexing for this. 2. **Select Specific Columns**: Return a DataFrame that includes only the columns: `ProductID`, `Name`, `Category`, and `Price`. 3. **Indexing by Category**: Set the `Category` column as the index of the DataFrame. 4. **Calculate Price Statistics**: For each category, calculate and return the average price of the products in that category. 5. **Adjust Pricing**: For products in the \'Electronics\' category, apply a 10% discount to their `Price`. Use `.loc` for this task. Input: - `df`: A pandas DataFrame with the following columns: `ProductID`, `Name`, `Category`, `Price`, `InStock`, `Rating`. Output: - A tuple containing: 1. A DataFrame filtered by `InStock` and `Rating`. 2. A DataFrame with the columns `ProductID`, `Name`, `Category`, and `Price`. 3. The input DataFrame with `Category` set as the index. 4. A pandas Series with the average price per category. 5. The input DataFrame with adjusted prices for \'Electronics\' category. Example: ```python import pandas as pd data = {\'ProductID\': [1, 2, 3, 4, 5], \'Name\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Category\': [\'Electronics\', \'Electronics\', \'Clothing\', \'Clothing\', \'Kitchen\'], \'Price\': [100, 150, 50, 60, 20], \'InStock\': [30, 5, 15, 25, 35], \'Rating\': [4.5, 4.0, 3.5, 5.0, 4.7]} df = pd.DataFrame(data) filtered_df, specific_columns_df, indexed_df, price_stats, adjusted_df = process_product_data(df) print(filtered_df) print(specific_columns_df) print(indexed_df) print(price_stats) print(adjusted_df) ``` Constraints: - Ensure that the input DataFrame is not modified in-place unless specified. - You may use any method of pandas to achieve the desired result, but the use of `.loc`, `.iloc`, and boolean indexing is encouraged. Write the function `process_product_data(df)` implementing the above logic.","solution":"import pandas as pd def process_product_data(df): # 1. Filter Products filtered_df = df[(df[\'InStock\'] > 20) & (df[\'Rating\'] >= 4.0)] # 2. Select Specific Columns specific_columns_df = df[[\'ProductID\', \'Name\', \'Category\', \'Price\']] # 3. Indexing by Category indexed_df = df.set_index(\'Category\') # 4. Calculate Price Statistics price_stats = df.groupby(\'Category\')[\'Price\'].mean() # 5. Adjust Pricing adjusted_df = df.copy() adjusted_df.loc[adjusted_df[\'Category\'] == \'Electronics\', \'Price\'] *= 0.9 return filtered_df, specific_columns_df, indexed_df, price_stats, adjusted_df"},{"question":"**Objective:** Implement and use a custom asyncio event loop policy in Python. # Problem Statement You have been tasked with creating a custom asyncio event loop policy. This policy will: - Ensure that a newly created event loop is always set as the current event loop. - Log creation time and number of event loops created. Implement the custom policy and demonstrate its use by: 1. Subclassing `asyncio.DefaultEventLoopPolicy` to define the custom policy. 2. Overriding the `new_event_loop` and `set_event_loop` methods to meet the requirements. 3. Writing a main function that: - Sets the custom event loop policy. - Creates multiple event loops. - Prints the log information. # Requirements 1. **Custom Policy Class**: - Subclass `asyncio.DefaultEventLoopPolicy`. - Override `new_event_loop` to create and set the new event loop. - Override `set_event_loop` to keep track of creation times and count. 2. **Main Function**: - Set your custom event loop policy. - Create at least two event loops. - Print the log information showing creation times and the number of event loops created. # Input and Output - **Input**: No direct input. The main function will execute the creation of event loops. - **Output**: Log information including creation times and the number of event loops created. # Constraints - Use the Python `asyncio` library exclusively. - Ensure thread-safety if dealing with processes. # Example Solution (Partial Pseudocode) ```python import asyncio from datetime import datetime class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self): super().__init__() self.creation_count = 0 self.creation_times = [] def new_event_loop(self): loop = super().new_event_loop() self.set_event_loop(loop) return loop def set_event_loop(self, loop): super().set_event_loop(loop) self.creation_count += 1 self.creation_times.append(datetime.now()) def main(): policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(policy) loop1 = asyncio.get_event_loop_policy().new_event_loop() loop2 = asyncio.get_event_loop_policy().new_event_loop() print(f\\"Number of event loops created: {policy.creation_count}\\") print(f\\"Creation times: {policy.creation_times}\\") if __name__ == \\"__main__\\": main() ``` Complete the example solution by implementing all required methods and ensuring it runs and demonstrates creating multiple event loops with logging.","solution":"import asyncio from datetime import datetime class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self): super().__init__() self.creation_count = 0 self.creation_times = [] def new_event_loop(self): loop = super().new_event_loop() self.set_event_loop(loop) return loop def set_event_loop(self, loop): super().set_event_loop(loop) self.creation_count += 1 self.creation_times.append(datetime.now()) def main(): policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(policy) loop1 = asyncio.get_event_loop_policy().new_event_loop() loop2 = asyncio.get_event_loop_policy().new_event_loop() print(f\\"Number of event loops created: {policy.creation_count}\\") print(f\\"Creation times: {policy.creation_times}\\") if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with creating a program that can compress a directory of files into a ZIP archive and then decompress it. Your program should be able to handle various edge cases, such as non-existent directories and empty directories. Additionally, you must ensure that the program can handle large files efficiently. This task will test your ability to use the `zipfile` module and manage file operations in Python. # Specifications 1. **Function Name**: `compress_directory_to_zip` - **Input**: `directory_path` (str) - The path of the directory to compress. - **Output**: `output_path` (str) - The path where the ZIP file is created. - **Constraints**: - The function should handle exceptions gracefully, such as the directory not existing. - If the directory is empty, the resulting ZIP file should be created but contain no files. - All files in the directory and its subdirectories should be included in the ZIP file. 2. **Function Name**: `decompress_zip_to_directory` - **Input**: - `zip_path` (str) - The path to the ZIP file to decompress. - `extract_directory` (str) - The path where the files should be extracted to. - **Output**: None - **Constraints**: - The function should handle exceptions, like the ZIP file not existing or being corrupted. - If the destination directory does not exist, the function should create it. - The directory structure within the ZIP file should be preserved. # Example ```python # Example usage compress_directory_to_zip(\'test_directory\', \'compressed_archive.zip\') decompress_zip_to_directory(\'compressed_archive.zip\', \'extracted_directory\') ``` # Performance Requirements - The functions should handle directories and files of varying sizes efficiently. - RAM usage should be managed efficiently, especially for large files. # Additional Information - You may use the `os`, `sys`, and `zipfile` modules in your implementation. - You should write your solution in a way that is compatible with Python 3.10 and later.","solution":"import os import zipfile from pathlib import Path def compress_directory_to_zip(directory_path: str, output_path: str) -> str: Compress a directory into a ZIP file. Parameters: directory_path (str): The path of the directory to compress. output_path (str): The path where the ZIP file should be created. Returns: str: The path to the created ZIP file. if not os.path.exists(directory_path): raise FileNotFoundError(f\\"The directory \'{directory_path}\' does not exist.\\") if not os.path.isdir(directory_path): raise NotADirectoryError(f\\"The path \'{directory_path}\' is not a directory.\\") with zipfile.ZipFile(output_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, _, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, directory_path) zipf.write(file_path, arcname) return output_path def decompress_zip_to_directory(zip_path: str, extract_directory: str) -> None: Decompress a ZIP file into a directory. Parameters: zip_path (str): The path to the ZIP file to decompress. extract_directory (str): The path where the files should be extracted to. if not os.path.exists(zip_path): raise FileNotFoundError(f\\"The ZIP file \'{zip_path}\' does not exist.\\") if not zipfile.is_zipfile(zip_path): raise zipfile.BadZipFile(f\\"The file \'{zip_path}\' is not a valid ZIP file.\\") os.makedirs(extract_directory, exist_ok=True) with zipfile.ZipFile(zip_path, \'r\') as zipf: zipf.extractall(extract_directory)"},{"question":"**Objective:** Demonstrate your understanding of the seaborn package by creating various plots to analyze the `penguins` dataset. **Question:** You are given the `penguins` dataset from seaborn. Your task is to generate and customize plots to explore the data. You need to follow these steps: 1. **Basic Distribution Plot:** Create a histogram of the `bill_length_mm` variable. Customize the plot to show the KDE curve. 2. **Conditional Distribution Plot:** Create separate KDE plots for `bill_length_mm`, conditioned on `species`. Use different colors to represent different species. 3. **Facet Grid Plot:** Create ECDF plots for `flipper_length_mm`, separate the plots col-wise per `species`, and incorporate different hues for `sex`. 4. **Customization:** Modify the facet grid plot to set the title for each subplot as `{col_name} penguins`. Also, set the x-axis label as \\"Flipper Length (mm)\\" and the y-axis label as \\"Proportion\\". **Input:** The `penguins` dataset is loaded using: ```python penguins = sns.load_dataset(\\"penguins\\") ``` **Output:** The function should output the following: 1. A histogram with KDE for `bill_length_mm`. 2. A KDE plot for `bill_length_mm` conditioned on `species`. 3. An ECDF plot faceted by `species`, with hues representing `sex`, and customized titles and labels. **Constraints:** - Ensure your code runs without errors. - Use appropriate seaborn functions and parameters to achieve the desired outputs. - Make sure the plots are clear and correctly annotated. **Performance Requirements:** The solution should be efficient and leverage seaborn functionalities optimally. **Sample Code to Start:** Here is a starting point for your solution: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_data(): # Loading the dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic Distribution Plot (Histogram with KDE) sns.displot(data=penguins, x=\\"bill_length_mm\\", kde=True) plt.show() # 2. Conditional Distribution Plot (KDE) sns.displot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", kind=\\"kde\\") plt.show() # 3. Facet Grid Plot (ECDF) with Customization g = sns.displot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"sex\\", col=\\"species\\", kind=\\"ecdf\\", height=4, aspect=.7, ) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Proportion\\") g.set_titles(\\"{col_name} penguins\\") plt.show() # Call the function to generate the plots plot_penguins_data() ``` **Note**: Ensure you install the necessary libraries (`seaborn` and `matplotlib`) to run the sample code.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_data(): # Loading the dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic Distribution Plot (Histogram with KDE) sns.displot(data=penguins, x=\\"bill_length_mm\\", kde=True) plt.gca().set_title(\'Histogram of Bill Length with KDE\') plt.show() # 2. Conditional Distribution Plot (KDE) sns.displot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", kind=\\"kde\\") plt.gca().set_title(\'KDE of Bill Length Conditioned on Species\') plt.show() # 3. Facet Grid Plot (ECDF) with Customization g = sns.displot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"sex\\", col=\\"species\\", kind=\\"ecdf\\", height=4, aspect=.7, ) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Proportion\\") g.set_titles(\\"{col_name} penguins\\") plt.show()"},{"question":"# Advanced Python Coding Assessment XML Parsing and Event Handling with `xml.sax` **Objective**: Implement a function that parses XML data to extract specific information using the `xml.sax` package and Custom Handlers. **Problem Statement**: You are given an XML document which represents a collection of books. Each book has details such as `title`, `author`, `genre`, `price`, and `publish_date`. Your task is to write a function `extract_book_titles(xml_string)` that takes an XML string and returns a list of book titles sorted alphabetically. The XML format is as follows: ```xml <catalog> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <genre>Fiction</genre> <price>10.99</price> <publish_date>1925-04-10</publish_date> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <genre>Fiction</genre> <price>7.99</price> <publish_date>1960-07-11</publish_date> </book> <!-- additional book entries --> </catalog> ``` **Function Signature**: ```python def extract_book_titles(xml_string: str) -> list: ``` **Input**: - `xml_string` (str): A string representing the XML content. **Output**: - `list`: A list of book titles sorted alphabetically. **Constraints**: 1. The XML content will have a well-defined structure as given above. 2. The number of books in the catalog will be at most 1000. 3. Ensure that the solution is efficient and makes proper use of the `xml.sax` package for parsing. **Instructions**: 1. Import the necessary modules from `xml.sax`. 2. Create a custom handler class that inherits from `xml.sax.handler.ContentHandler`. 3. Implement the `characters` and `startElement` methods to handle events during parsing. 4. Use `xml.sax.parseString` to parse the given XML string with your custom handler. 5. Return the list of titles sorted alphabetically. **Example**: ```python xml_content = \'\'\' <catalog> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <genre>Fiction</genre> <price>10.99</price> <publish_date>1925-04-10</publish_date> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <genre>Fiction</genre> <price>7.99</price> <publish_date>1960-07-11</publish_date> </book> </catalog> \'\'\' print(extract_book_titles(xml_content)) # Output: [\'The Great Gatsby\', \'To Kill a Mockingbird\'] ``` **Note**: Ensure that your function handles parsing correctly and raises exceptions as needed for invalid input.","solution":"import xml.sax class BookHandler(xml.sax.handler.ContentHandler): def __init__(self): self.inTitle = False self.titles = [] def startElement(self, name, attrs): if name == \'title\': self.inTitle = True def endElement(self, name): if name == \'title\': self.inTitle = False def characters(self, content): if self.inTitle: self.titles.append(content.strip()) def extract_book_titles(xml_string): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) return sorted(handler.titles)"},{"question":"# Advanced Python Assessment **Objective**: Demonstrate your comprehension of the `types` module, focusing on dynamic type creation and type-related utilities. **Problem Statement**: You are tasked with implementing a function `create_dynamic_class` that dynamically creates a new class with specific characteristics and functionalities. The function should: 1. Use `types.new_class` to create a class named `DynamicClass`. 2. The class should: - Accept keyword arguments in its constructor and store them in an instance of `types.SimpleNamespace`. - Provide a method `get_read_only_mapping` that returns a read-only view of its attributes using `types.MappingProxyType`. **Function Signature**: ```python def create_dynamic_class(**attributes) -> type: ``` **Input**: - **attributes (kwargs)**: Any number of keyword arguments representing initial attributes of the class. **Output**: - An instance of the dynamically created `DynamicClass`. **Constraints**: - The dynamically created class should reflect any changes made to the original attribute dictionary in the `SimpleNamespace`. - The read-only view should accurately represent the current state of the attributes. **Example**: ```python DynamicClass = create_dynamic_class(a=1, b=2) obj = DynamicClass(a=1, b=2) print(obj.get_read_only_mapping()) # Output: {\'a\': 1, \'b\': 2} # Update attributes obj.namespace.a = 10 print(obj.get_read_only_mapping()) # Output: {\'a\': 10, \'b\': 2} ``` **Notes**: - The dynamically created class should inherit from `object`. - Ensure that the `SimpleNamespace` properly manages the internal state. - Leverage the `types.MappingProxyType` to provide a read-only view of the attributes. Provide a detailed explanation of your solution, and include well-documented code. Feel free to add any helper functions if necessary.","solution":"import types def create_dynamic_class(**attributes): Creates a dynamic class with the specified attributes. # Define the constructor def __init__(self, **kwargs): self.namespace = types.SimpleNamespace(**kwargs) # Define the method to get a read-only view of the attributes def get_read_only_mapping(self): return types.MappingProxyType(self.namespace.__dict__) # Dynamically create the class using types.new_class DynamicClass = types.new_class(\'DynamicClass\', (object,), {}) # Set the defined methods to the class setattr(DynamicClass, \'__init__\', __init__) setattr(DynamicClass, \'get_read_only_mapping\', get_read_only_mapping) return DynamicClass"},{"question":"# Objective Demonstrate your understanding of seaborn\'s `ecdfplot` function by visualizing different aspects of a dataset using a variety of configurations. # Problem Statement Write a function named `analyze_penguins()` that takes no input parameters and performs the following tasks: 1. **Loading Data**: Load the `penguins` dataset from seaborn. 2. **Plotting ECDFs**: - Plot the empirical cumulative distribution function (ECDF) of the flipper lengths (`flipper_length_mm`) and save the plot as `ecdf_flipper_length.png`. - Plot the ECDF of the bill lengths (`bill_length_mm`) and color each species differently using the `hue` parameter. Save the plot as `ecdf_bill_length_by_species.png`. - Plot the ECDF of the bill lengths (`bill_length_mm`) with the `stat` parameter set to show absolute counts, and save the plot as `ecdf_bill_length_count.png`. - Plot the empirical complementary cumulative distribution function (1 - CDF) of the bill lengths (`bill_length_mm`) for different species, then save the plot as `ccdf_bill_length_by_species.png`. # Constraints and Requirements: - Use seaborn for plotting and matplotlib for saving the plots. - The plots should be clear and correctly labeled. - Ensure your function runs without additional input and saves the required plots in the current working directory. # Expected Output After running the `analyze_penguins()` function, the following files should be created in the working directory: - `ecdf_flipper_length.png` - `ecdf_bill_length_by_species.png` - `ecdf_bill_length_count.png` - `ccdf_bill_length_by_species.png` # Implementation ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Plot ECDF of flipper lengths and save the plot sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Lengths\\") plt.savefig(\\"ecdf_flipper_length.png\\") plt.clf() # Clear the current figure # Plot ECDF of bill lengths with hue by species and save the plot sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Lengths by Species\\") plt.savefig(\\"ecdf_bill_length_by_species.png\\") plt.clf() # Clear the current figure # Plot ECDF of bill lengths with stat set to \'count\' and save the plot sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"ECDF of Bill Lengths by Species (Count)\\") plt.savefig(\\"ecdf_bill_length_count.png\\") plt.clf() # Clear the current figure # Plot the complementary ECDF of bill lengths by species and save the plot sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"CCDF of Bill Lengths by Species\\") plt.savefig(\\"ccdf_bill_length_by_species.png\\") plt.clf() # Clear the current figure ``` This function will save four different plots demonstrating the use of seaborn\'s `ecdfplot` function in various configurations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Plot ECDF of flipper lengths and save the plot sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Lengths\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Proportion\\") plt.savefig(\\"ecdf_flipper_length.png\\") plt.clf() # Clear the current figure # Plot ECDF of bill lengths with hue by species and save the plot sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Lengths by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Proportion\\") plt.savefig(\\"ecdf_bill_length_by_species.png\\") plt.clf() # Clear the current figure # Plot ECDF of bill lengths with stat set to \'count\' and save the plot sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"ECDF of Bill Lengths by Species (Count)\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Count\\") plt.savefig(\\"ecdf_bill_length_count.png\\") plt.clf() # Clear the current figure # Plot the complementary ECDF of bill lengths by species and save the plot sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"CCDF of Bill Lengths by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Complementary Proportion\\") plt.savefig(\\"ccdf_bill_length_by_species.png\\") plt.clf() # Clear the current figure"},{"question":"# Plot Customization with Seaborn In this exercise, you are required to create a function that customizes the appearance of a plot using Seaborn. You should demonstrate your understanding of Seaborn\'s themes, styles, and context settings. Function Signature ```python def customized_sinplot(theme=\'darkgrid\', style=\'whitegrid\', context=\'notebook\', flip=1, despine=False): This function plots a customized sine plot based on input parameters. Parameters: - theme (str): The overall theme of the plot. Valid options are [\'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\']. - style (str): The style of the axes. Valid options are [\'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\']. - context (str): The scaling context for plot elements. Valid options are [\'paper\', \'notebook\', \'talk\', \'poster\']. - flip (int): A parameter that flips the sine wave. Use 1 for normal and -1 for flipped. - despine (bool): A boolean indicating whether to remove the top and right spine from the plot. Returns: - None: The function should create and display the plot directly. pass ``` Requirements 1. **Theme and Style**: - Set the theme and style of the plot based on the input parameters. 2. **Context**: - Use the `context` parameter to set the context for scaling the plot elements. 3. **Plot Implementation**: - Implement a sine plot with 10 lines using the existing `sinplot` structure from the documentation, but make sure it takes `flip` as an argument. 4. **Axis Spines**: - If `despine` is `True`, remove the top and right spines from the plot. 5. **Customization**: - Allow overriding of the background color of the plot to `.9` grey if applicable (show understanding of customization with `rc`). Example Usage ```python # Default settings customized_sinplot() # Dark theme, white style, talk context customized_sinplot(theme=\'dark\', style=\'white\', context=\'talk\') # Whitegrid style, poster context, flipped sine, with despine customized_sinplot(style=\'whitegrid\', context=\'poster\', flip=-1, despine=True) ``` Ensure that the function conforms to these specifications and provides the required customization options. Use the provided Seaborn documentation references for guidance on implementation.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def customized_sinplot(theme=\'darkgrid\', style=\'whitegrid\', context=\'notebook\', flip=1, despine=False): This function plots a customized sine plot based on input parameters. Parameters: - theme (str): The overall theme of the plot. Valid options are [\'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\']. - style (str): The style of the axes. Valid options are [\'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\']. - context (str): The scaling context for plot elements. Valid options are [\'paper\', \'notebook\', \'talk\', \'poster\']. - flip (int): A parameter that flips the sine wave. Use 1 for normal and -1 for flipped. - despine (bool): A boolean indicating whether to remove the top and right spine from the plot. Returns: - None: The function should create and display the plot directly. sns.set_theme(style=theme) sns.set_style(style) sns.set_context(context) x = np.linspace(0, 14, 100) for i in range(1, 7): plt.plot(x, np.sin(x + i * .5) * (7 - i) * flip) if despine: sns.despine() plt.show()"},{"question":"You are working on a data migration project where you need to encode and decode files using the binhex4 format for transferring certain types of Mac files across systems. Your task is to implement a function using the deprecated `binhex` module within a larger data migration utility. Task 1. Write a function `convert_to_binhex(input_filename: str, output_filename: str) -> None` that takes the name of a binary file as `input_filename` and converts it to a binhex file named `output_filename`. 2. Write a second function `convert_from_binhex(input_filename: str, output_filename: str) -> None` that takes the name of a binhex file as `input_filename` and converts it back to a binary file with the name `output_filename`. 3. Handle errors raised by the `binhex` module in a user-friendly way by printing appropriate error messages. Constraints - You do not need to handle non-file input/output objects. - `input_filename` and `output_filename` will always be valid strings and path names. - The functions should only handle exceptions related to encoding/decoding operations. Example Usage ```python # Example input binary data input_binary_file = \\"example.bin\\" output_binhex_file = \\"example.hqx\\" output_decoded_file = \\"decoded_example.bin\\" # Convert binary file to binhex format convert_to_binhex(input_binary_file, output_binhex_file) # Convert back from binhex format to binary file convert_from_binhex(output_binhex_file, output_decoded_file) ``` # Implementation ```python import binhex def convert_to_binhex(input_filename: str, output_filename: str) -> None: try: binhex.binhex(input_filename, output_filename) except binhex.Error as e: print(f\\"Error converting {input_filename} to binhex: {e}\\") def convert_from_binhex(input_filename: str, output_filename: str) -> None: try: binhex.hexbin(input_filename, output_filename) except binhex.Error as e: print(f\\"Error decoding {input_filename} from binhex: {e}\\") # Example usage # convert_to_binhex(\\"example.bin\\", \\"example.hqx\\") # convert_from_binhex(\\"example.hqx\\", \\"decoded_example.bin\\") ``` Ensure your implementation handles errors gracefully and provides meaningful messages to the user.","solution":"import binhex def convert_to_binhex(input_filename: str, output_filename: str) -> None: try: binhex.binhex(input_filename, output_filename) except binhex.Error as e: print(f\\"Error converting {input_filename} to binhex: {e}\\") def convert_from_binhex(input_filename: str, output_filename: str) -> None: try: binhex.hexbin(input_filename, output_filename) except binhex.Error as e: print(f\\"Error decoding {input_filename} from binhex: {e}\\")"},{"question":"# Python Code Execution Utility Objective Create a Python function `execute_code` that utilizes the high-level methods described in the provided documentation to compile and execute Python code from a given string or file. Task Implement the function `execute_code(source, source_type, filename=None, optimize=-1)` where: - `source` is a string containing the Python source code. - `source_type` is a string indicating the type of the source code, which can be either `\'string\'` or `\'file\'`. If it is `\'file\'`, the function should read the code from the specified `filename`. - `filename` is an optional parameter specifying the name of the file containing the Python source code. It is required only if `source_type` is `\'file\'`. - `optimize` is an optional integer parameter that specifies the optimization level for the compiler (default is `-1`). The function should perform the following steps: 1. Compile the Python source code with the specified optimization level. 2. Execute the compiled code in a new, empty global context. The function should return: - The result of the executed code if it is an expression. - `None` if the code is a statement or script that doesn\'t return a value. Constraints - The function should handle any exceptions that occur during code compilation or execution and return a string with the error message. - The optimization levels follow these conventions: `0` for no optimization, `1` to remove assert statements, and `2` to remove assert statements and docstrings. Example Usage 1. Executing a code string: ```python source = \\"print(\'Hello, World!\'); x = 5; x**2\\" result = execute_code(source, \'string\') print(result) # Output: 25 ``` 2. Executing from a file: ```python result = execute_code(None, \'file\', \'test_script.py\') # Assuming \'test_script.py\' contains: x = 10; x + 15 print(result) # Output: 25 ``` Performance Requirements - The function should be efficient in terms of compilation and execution time. - It should handle both small and moderately large scripts and expressions. ```python def execute_code(source, source_type, filename=None, optimize=-1): import sys import traceback try: if source_type == \'string\': compiled_code = compile(source, \'<string>\', \'exec\', optimize=optimize) elif source_type == \'file\' and filename: with open(filename, \'r\') as file: file_content = file.read() compiled_code = compile(file_content, filename, \'exec\', optimize=optimize) else: raise ValueError(\\"source_type must be \'string\' or \'file\' and filename must be provided for \'file\'\\") global_context = {} exec(compiled_code, global_context) last_expr = source.split(\';\')[-1].strip() if last_expr: compiled_expr = compile(last_expr, \'<string>\', \'eval\') result = eval(compiled_expr, global_context) else: result = None return result except Exception as e: return \'\'.join(traceback.format_exception(None, e, e.__traceback__)) ``` Ensure the function handles potential edge cases such as missing files, syntax errors in the provided code, and improper use of parameters.","solution":"def execute_code(source, source_type, filename=None, optimize=-1): import traceback try: if source_type == \'string\': compiled_code = compile(source, \'<string>\', \'exec\', optimize=optimize) elif source_type == \'file\' and filename: with open(filename, \'r\') as file: file_content = file.read() compiled_code = compile(file_content, filename, \'exec\', optimize=optimize) else: raise ValueError(\\"source_type must be \'string\' or \'file\' and filename must be provided for \'file\'\\") global_context = {} exec(compiled_code, global_context) last_expr = source.split(\';\')[-1].strip() if source_type == \'string\' else file_content.split(\';\')[-1].strip() if last_expr: try: compiled_expr = compile(last_expr, \'<string>\', \'eval\') result = eval(compiled_expr, global_context) except: result = None else: result = None return result except Exception as e: return \'\'.join(traceback.format_exception(None, e, e.__traceback__))"},{"question":"**Objective**: Implement functions to encode and decode strings using custom codecs and handle errors as specified. Problem Statement You are tasked with implementing a string transformation module that provides tools for encoding and decoding strings using specified encodings. Additionally, you should handle encoding and decoding errors using custom error handling strategies. Requirements 1. **Function Implementations**: - `register_codec(search_function: callable) -> int`: Registers a new codec search function. - `unregister_codec(search_function: callable) -> int`: Unregisters an existing codec search function. - `is_known_encoding(encoding: str) -> bool`: Returns `True` if the encoding is known; otherwise, `False`. - `encode_string(string: str, encoding: str, errors: str = \'strict\') -> bytes`: Encodes the given string using the specified encoding and error handling strategy. - `decode_string(encoded: bytes, encoding: str, errors: str = \'strict\') -> str`: Decodes the given byte sequence using the specified encoding and error handling strategy. - `register_error_handler(name: str, error_handler: callable) -> int`: Registers a custom error handler. - `lookup_error_handler(name: str) -> callable`: Looks up and returns the error handler function registered under the given name. 2. **Custom Error Handlers**: - Implement at least two custom error handling strategies: - `replace_with_question_mark(exc)`: Replaces problematic sequences with \'?\'. - `ignore_error(exc)`: Ignores problematic sequences. 3. **Constraints**: - You are required to make use of the codec APIs as described in the documentation. - The `errors` parameter in `encode_string` and `decode_string` functions can accept custom error handler names. Example: ```python def register_codec(search_function: callable) -> int: # Implementation here pass def unregister_codec(search_function: callable) -> int: # Implementation here pass def is_known_encoding(encoding: str) -> bool: # Implementation here pass def encode_string(string: str, encoding: str, errors: str = \'strict\') -> bytes: # Implementation here pass def decode_string(encoded: bytes, encoding: str, errors: str = \'strict\') -> str: # Implementation here pass def register_error_handler(name: str, error_handler: callable) -> int: # Implementation here pass def lookup_error_handler(name: str) -> callable: # Implementation here pass def replace_with_question_mark(exc): # Implementation here pass def ignore_error(exc): # Implementation here pass # Register and use your functions as needed ``` Note: Ensure your functions handle appropriate exceptions and return required data types as specified. Performance Requirements: - Your functions should handle typical use cases efficiently, maintaining clarity and readability of code.","solution":"import codecs def register_codec(search_function: callable) -> int: Registers a new codec search function. return codecs.register(search_function) def unregister_codec(search_function: callable) -> int: Unregisters an existing codec search function. return codecs.unregister(search_function) def is_known_encoding(encoding: str) -> bool: Returns True if the encoding is known; otherwise, False. try: codecs.lookup(encoding) return True except LookupError: return False def encode_string(string: str, encoding: str, errors: str = \'strict\') -> bytes: Encodes the given string using the specified encoding and error handling strategy. return string.encode(encoding, errors) def decode_string(encoded: bytes, encoding: str, errors: str = \'strict\') -> str: Decodes the given byte sequence using the specified encoding and error handling strategy. return encoded.decode(encoding, errors) def register_error_handler(name: str, error_handler: callable) -> int: Registers a custom error handler. codecs.register_error(name, error_handler) return 0 def lookup_error_handler(name: str) -> callable: Looks up and returns the error handler function registered under the given name. return codecs.lookup_error(name) def replace_with_question_mark(exc): Error handler that replaces problematic sequences with \'?\'. return (\'?\', exc.start + 1) def ignore_error(exc): Error handler that ignores problematic sequences. return (\'\', exc.start + 1)"},{"question":"# Asynchronous Queue Management In this assessment, you are required to implement a concurrent task processing system using the `asyncio` queues provided in Python 3.10. You will simulate a scenario where multiple workers process tasks from a shared queue concurrently. Problem Statement You are to implement two functions: `produce_tasks` and `consume_tasks` to simulate the following scenario: 1. **Producers** will generate a series of tasks and put them into an `asyncio.Queue`. 2. **Consumers** will retrieve these tasks from the queue and process them concurrently. Each task is represented as a string indicating a specific job (for example, \\"task1\\", \\"task2\\", etc.). The processing of a task is simulated by having the consumer print the task name and then sleep for a random amount of time (between 0.1 to 1.0 seconds). Once all tasks are processed, the total time taken to process all tasks should be returned. Function Specifications 1. **`produce_tasks(queue: asyncio.Queue, number_of_tasks: int) -> None`** - **Input:** - `queue`: An instance of `asyncio.Queue` where tasks will be put. - `number_of_tasks`: An integer indicating the number of tasks to produce. - **Output:** None - **Description:** This coroutine function should generate `number_of_tasks` tasks named as \\"task1\\", \\"task2\\", ..., \\"taskN\\" and put them into the `queue`. 2. **`consume_tasks(queue: asyncio.Queue, number_of_consumers: int) -> float`** - **Input:** - `queue`: An instance of `asyncio.Queue` from which tasks will be retrieved. - `number_of_consumers`: An integer indicating the number of consumer tasks to create. - **Output:** Returns the total time taken to process all tasks in seconds as a float. - **Description:** This coroutine function should create `number_of_consumers` tasks that retrieve and process items from the `queue` concurrently. Each consumer should: - Retrieve task names from the `queue` until it is empty. - Simulate processing by printing the task name and sleeping for a random duration between 0.1 to 1.0 seconds. - Return the total time taken to process all tasks in seconds. You can utilize the provided `asyncio.Queue` methods to implement these functions. Example Usage ```python import asyncio import random async def produce_tasks(queue: asyncio.Queue, number_of_tasks: int): for i in range(1, number_of_tasks + 1): task_name = f\\"task{i}\\" await queue.put(task_name) async def consume_tasks(queue: asyncio.Queue, number_of_consumers: int) -> float: async def consumer(queue: asyncio.Queue): while not queue.empty(): task_name = await queue.get() print(f\\"Processing {task_name}\\") await asyncio.sleep(random.uniform(0.1, 1.0)) queue.task_done() tasks = [asyncio.create_task(consumer(queue)) for _ in range(number_of_consumers)] start_time = asyncio.get_event_loop().time() await queue.join() for task in tasks: task.cancel() total_time = asyncio.get_event_loop().time() - start_time return total_time async def main(): queue = asyncio.Queue() await produce_tasks(queue, 10) total_time = await consume_tasks(queue, 3) print(f\\"Total time taken: {total_time:.2f} seconds\\") asyncio.run(main()) ``` Constraints - Use `asyncio.Queue` for the implementation. - Handle concurrency using `asyncio` concepts like `async`, `await`, and `asyncio.create_task`. - Ensure that all the tasks are produced and consumed properly, and no tasks are lost. - The solution should appropriately measure the total time taken to consume and process all tasks.","solution":"import asyncio import random async def produce_tasks(queue: asyncio.Queue, number_of_tasks: int): Produces a number of tasks and puts them into the queue. :param queue: The asyncio queue to put tasks into. :param number_of_tasks: The number of tasks to produce. for i in range(1, number_of_tasks + 1): task_name = f\\"task{i}\\" await queue.put(task_name) async def consume_tasks(queue: asyncio.Queue, number_of_consumers: int) -> float: Consumes tasks from the queue with a specified number of consumers. :param queue: The asyncio queue to consume tasks from. :param number_of_consumers: The number of concurrent consumers. :return: The total time taken to process all tasks. async def consumer(queue: asyncio.Queue): while True: task_name = await queue.get() if task_name is None: break print(f\\"Processing {task_name}\\") await asyncio.sleep(random.uniform(0.1, 1.0)) queue.task_done() tasks = [asyncio.create_task(consumer(queue)) for _ in range(number_of_consumers)] start_time = asyncio.get_event_loop().time() await queue.join() # Add None to wake up all consumers so they can exit the loop for _ in range(number_of_consumers): await queue.put(None) await asyncio.gather(*tasks, return_exceptions=True) total_time = asyncio.get_event_loop().time() - start_time return total_time # Example of running the functions async def main(): queue = asyncio.Queue() await produce_tasks(queue, 10) total_time = await consume_tasks(queue, 3) print(f\\"Total time taken: {total_time:.2f} seconds\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Objective**: Implement a class to handle localization for an internationalized Python application. **Problem Statement**: You are tasked with developing a translation management system for an internationalized Python application. Your system should be capable of initializing translations, querying translated strings, and switching languages dynamically at runtime. Implement a class `TranslationManager` that provides these functionalities using the \\"gettext\\" module. **Class Structure**: ```python class TranslationManager: def __init__(self, domain, localedir, languages): Initializes the TranslationManager with the provided domain, localedir, and a list of languages. :param domain: The translation domain. :param localedir: The directory where language files are located. :param languages: List of language codes. pass def translate(self, message): Returns the translated version of the provided message. :param message: The message string to be translated. :return: Translated message string. pass def switch_language(self, language): Switches the current language to the specified language. :param language: The new language code to switch to. pass def list_languages(self): Lists all the languages available in the locale directory. :return: List of available language codes. pass ``` **Implementation Details**: 1. **Initialization**: - Bind the provided domain to the locale directory. - Load translations for each language in the `languages` list. - Initialize the current language to the first language in the list. 2. **Translation**: - Use `gettext` methods to retrieve and return the translated message for the current language. 3. **Language Switching**: - Allow switching the current language by updating the translation instance. 4. **List Available Languages**: - Implement a method to list all languages for which translation files are available in the specified locale directory. **Constraints**: - Assume that the translation files follow the \\"gettext\\" directory and naming conventions. - Return the original message if no translation is available. - If the specified language for switching is not found, raise a `ValueError`. **Example Usage**: ```python # Initialize the manager with translation domain, locale directory, and languages tm = TranslationManager(domain=\'myapp\', localedir=\'/path/to/locale\', languages=[\'en\', \'es\', \'fr\']) # Translate a message in the current language print(tm.translate(\'Hello, world!\')) # Prints \\"Hello, world!\\" in the current language # Switch to Spanish and translate tm.switch_language(\'es\') print(tm.translate(\'Hello, world!\')) # Prints \\"Â¡Hola, mundo!\\" # List available languages print(tm.list_languages()) # [\'en\', \'es\', \'fr\'] ``` Write the implementation for the `TranslationManager` class taking into account the details provided. You may use the \\"gettext\\" module as documented.","solution":"import gettext import os class TranslationManager: def __init__(self, domain, localedir, languages): Initializes the TranslationManager with the provided domain, localedir, and a list of languages. :param domain: The translation domain. :param localedir: The directory where language files are located. :param languages: List of language codes. self.domain = domain self.localedir = localedir self.languages = languages self._translations = {} self._current_language = languages[0] if languages else None self._load_translations() self._set_translation(self._current_language) def _load_translations(self): Load translations for each specified language. for lang in self.languages: self._translations[lang] = gettext.translation( domain=self.domain, localedir=self.localedir, languages=[lang], fallback=True ) def _set_translation(self, language): Set the current translation object. self._current_translation = self._translations.get(language, gettext.NullTranslations()) def translate(self, message): Returns the translated version of the provided message. :param message: The message string to be translated. :return: Translated message string. return self._current_translation.gettext(message) def switch_language(self, language): Switches the current language to the specified language. :param language: The new language code to switch to. :raises ValueError: If the specified language is not found. if language not in self._translations: raise ValueError(f\\"Language \'{language}\' not found.\\") self._current_language = language self._set_translation(language) def list_languages(self): Lists all the languages available in the locale directory. :return: List of available language codes. return list(self._translations.keys()) # Example Usage # Initialize the manager with translation domain, locale directory, and languages # tm = TranslationManager(domain=\'myapp\', localedir=\'/path/to/locale\', languages=[\'en\', \'es\', \'fr\']) # Translate a message in the current language # print(tm.translate(\'Hello, world!\')) # Prints \\"Hello, world!\\" in the current language # Switch to Spanish and translate # tm.switch_language(\'es\') # print(tm.translate(\'Hello, world!\')) # Prints \\"Â¡Hola, mundo!\\" # List available languages # print(tm.list_languages()) # [\'en\', \'es\', \'fr\']"},{"question":"**Coding Assessment Question: Tracing Function Execution and Generating Coverage Report** # Objective Demonstrate your comprehension of the `trace` module by writing Python code that traces the execution of a given function, analyzes the coverage, and generates a detailed report. # Problem Statement You are provided with a Python function named `process_data` that processes a list of numerical data. Your task is to trace the execution of this function using the `trace` module, gather coverage statistics, and generate an annotated coverage report. Additionally, you should demonstrate ignoring specific directories during the tracing process. # Function to Trace: `process_data` ```python def process_data(data): processed_data = [] for num in data: if num % 2 == 0: processed_data.append(num * 2) else: processed_data.append(num + 1) return processed_data ``` # Task Requirements 1. **Create a Trace Object**: - Ignore the directories containing the system\'s Python executable prefix (`sys.prefix`, `sys.exec_prefix`). - Enable line-number counting. 2. **Trace the Execution**: - Use the `trace` module to execute the provided `process_data` function with a sample list of integers. 3. **Generate the Coverage Report**: - Write the coverage results to the current directory. - Include lines that were not executed in the report. 4. **Implementation**: - Implement the tracing and reporting logic in a function named `trace_process_data`. # Expected Input and Output - **Input**: A list of integers. ```python sample_data = [1, 2, 3, 4, 5] ``` - **Output**: A coverage report file in the current directory with detailed execution statistics for the `process_data` function. # Constraints - Use the `trace` module\'s programmatic interface. - Demonstrate ignoring directories properly. - The function `trace_process_data` should encapsulate the tracing and reporting logic. # Performance Requirements - Ensure that the code runs efficiently and handles the tracing process without unnecessary overhead. # Example Usage ```python sample_data = [1, 2, 3, 4, 5] trace_process_data(sample_data) ``` Expected Output: A file named `process_data.cover` in the current directory with the annotated coverage report. # Solution Template ```python import sys import trace def process_data(data): processed_data = [] for num in data: if num % 2 == 0: processed_data.append(num * 2) else: processed_data.append(num + 1) return processed_data def trace_process_data(data): # Create a Trace object tracer = trace.Trace( ignoredirs=[sys.prefix, sys.exec_prefix], trace=0, count=1 ) # Define a wrapper function to trace process_data execution def wrapper(): process_data(data) # Run the wrapper function using the given tracer tracer.runfunc(wrapper) # Make a report and place output in the current directory r = tracer.results() r.write_results(show_missing=True, coverdir=\\".\\") # Example usage sample_data = [1, 2, 3, 4, 5] trace_process_data(sample_data) ``` # Notes - Ensure you have the appropriate permissions to write files in the current directory. - Review the generated `process_data.cover` file to verify the coverage details.","solution":"import sys import trace def process_data(data): processed_data = [] for num in data: if num % 2 == 0: processed_data.append(num * 2) else: processed_data.append(num + 1) return processed_data def trace_process_data(data): # Create a Trace object tracer = trace.Trace( ignoredirs=[sys.prefix, sys.exec_prefix], trace=0, count=1 ) # Define a wrapper function to trace process_data execution def wrapper(): process_data(data) # Run the wrapper function using the given tracer tracer.runfunc(wrapper) # Make a report and place output in the current directory r = tracer.results() r.write_results(show_missing=True, coverdir=\\".\\") # Example usage sample_data = [1, 2, 3, 4, 5] trace_process_data(sample_data)"},{"question":"**Multi-threaded Counter with Synchronization** # Background You are tasked with designing a multi-threaded counter that ensures synchronized access to a shared resource (in this case, an integer counter). You will use the `_thread` module\'s capabilities to accomplish this. # Objective Implement a function `multi_threaded_counter` that starts multiple threads to increment a shared counter. Synchronization should be achieved using locks to prevent race conditions. # Function Signature ```python def multi_threaded_counter(num_threads: int, num_increments: int) -> int: ... ``` # Input - `num_threads` (int): The number of threads to start. - `num_increments` (int): The number of times each thread should increment the counter. # Output - Returns the final value of the counter after all threads have completed their increments. # Constraints 1. `num_threads >= 1` 2. `num_increments >= 1` # Requirements 1. Use the `_thread` module to manage thread creation. 2. Use locks to synchronize access to the shared counter. 3. Ensure all threads complete their execution before the final counter value is returned. 4. Handle any necessary thread cleanup to prevent resource leakage. # Example ```python final_counter = multi_threaded_counter(3, 1000) print(final_counter) # Should print 3000 as each thread increments the counter 1000 times and there are 3 threads ``` # Implementation Notes 1. Use `_thread.allocate_lock()` to create a lock. 2. Define a function to be executed by each thread, which updates the shared counter utilizing the lock to ensure synchronized access. 3. Use `_thread.start_new_thread()` to start the threads. 4. Use a suitable method to ensure all threads have completed before returning the final counter value. # Caveats 1. Ensure robust error handling for any potential exceptions that might occur during thread execution. 2. Take into consideration that thread interaction with signals can affect synchronization, so thorough testing is required.","solution":"import _thread import time def multi_threaded_counter(num_threads: int, num_increments: int) -> int: Increments a shared counter using multiple threads with synchronization. Parameters: num_threads (int): The number of threads to start. num_increments (int): The number of times each thread should increment the counter. Returns: int: The final value of the counter. counter = [0] # Use a list to encapsulate the integer counter since integers are immutable lock = _thread.allocate_lock() # Create a lock for synchronization def increment_counter(): for _ in range(num_increments): lock.acquire() counter[0] += 1 lock.release() threads = [] for _ in range(num_threads): thread = _thread.start_new_thread(increment_counter, ()) threads.append(thread) # Wait for all threads to complete while True: lock.acquire() current_count = counter[0] lock.release() if current_count >= num_threads * num_increments: break time.sleep(0.01) # Small sleep to prevent busy waiting return counter[0]"},{"question":"Problem Statement You are to implement a Python function that fetches data from a given URL, checks the HTTP status code, parses specific information from the HTML content, and handles cookies appropriately. Your program should handle potential errors gracefully and print useful debug information. Function Signature ```python def fetch_and_parse_url(url: str) -> dict: Fetches data from the given URL, checks the HTTP status code, parses specific information from the HTML content, and handles cookies appropriately. Parameters: url (str): The URL to fetch data from. Returns: dict: A dictionary containing the following keys: - \'status_code\' (int): The HTTP status code of the response. - \'cookies\' (dict): A dictionary of cookies set by the server. - \'parsed_data\' (str): Parsed data from the HTML content based on given requirements. Exceptions: This function should handle and print messages for HTTP errors, URL errors, and any other exceptions that may occur. pass ``` Requirements and Constraints 1. **Input:** - `url` (string): The URL from which to fetch data. 2. **Output:** - A dictionary with the following keys: - `\'status_code\'`: The integer HTTP status code returned by the server. - `\'cookies\'`: A dictionary of cookies set by the server. - `\'parsed_data\'`: A string containing parsed data from the HTML content (details specified below). 3. **Constraints:** - The URL is guaranteed to be a valid string format but not necessarily a reachable address. - The function should gracefully handle common errors such as: - HTTP errors (e.g., 404 Not Found, 500 Internal Server Error). - URL errors (e.g., malformed URL). - Any other exceptions. 4. **Parsing Requirement:** - For the purpose of this exercise, parse the title of the HTML page and include it under `\'parsed_data\'`. 5. **Performance:** - Handle the request in a reasonable time frame; assume the network is potentially slow but not unreasonably so. 6. **Additional Information:** - Use the `urllib.request` module to fetch data from the URL. - Utilize the `http.cookiejar` module to manage cookies. - Handle responses and parse HTML using any of the standard libraries like `html.parser`. Example Execution ```python result = fetch_and_parse_url(\\"https://example.com\\") assert isinstance(result, dict) assert \'status_code\' in result assert \'cookies\' in result and isinstance(result[\'cookies\'], dict) assert \'parsed_data\' in result and isinstance(result[\'parsed_data\'], str) print(result) # Output might be: # { # \'status_code\': 200, # \'cookies\': {\'cookie_name\': \'cookie_value\'}, # \'parsed_data\': \'Example Domain\' # } ``` Notes - Ensure to add appropriate error handling and print debug information for errors. - Optimize for readability and maintainability, using functions and modular code where applicable.","solution":"import urllib.request import urllib.error from http.cookiejar import CookieJar from html.parser import HTMLParser class TitleParser(HTMLParser): def __init__(self): super().__init__() self.in_title = False self.title_data = \\"\\" def handle_starttag(self, tag, attrs): if tag == \\"title\\": self.in_title = True def handle_endtag(self, tag): if tag == \\"title\\": self.in_title = False def handle_data(self, data): if self.in_title: self.title_data += data def fetch_and_parse_url(url: str) -> dict: Fetches data from the given URL, checks the HTTP status code, parses specific information from the HTML content, and handles cookies appropriately. Parameters: url (str): The URL to fetch data from. Returns: dict: A dictionary containing the following keys: - \'status_code\' (int): The HTTP status code of the response. - \'cookies\' (dict): A dictionary of cookies set by the server. - \'parsed_data\' (str): Parsed data from the HTML content based on given requirements. Exceptions: This function should handle and print messages for HTTP errors, URL errors, and any other exceptions that may occur. result = { \'status_code\': None, \'cookies\': {}, \'parsed_data\': \\"\\" } cookie_jar = CookieJar() opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) try: response = opener.open(url) result[\'status_code\'] = response.getcode() result[\'cookies\'] = {cookie.name: cookie.value for cookie in cookie_jar} html_content = response.read().decode(\'utf-8\') parser = TitleParser() parser.feed(html_content) result[\'parsed_data\'] = parser.title_data.strip() except urllib.error.HTTPError as e: print(f\\"HTTP Error: {e.code}, {e.reason}\\") result[\'status_code\'] = e.code except urllib.error.URLError as e: print(f\\"URL Error: {e.reason}\\") except Exception as e: print(f\\"Unexpected Error: {e}\\") return result"},{"question":"# Unicode String Normalization and Comparison **Objective:** The goal of this task is to write a Python function that normalizes and compares Unicode strings. You will use the `unicodedata` module to ensure that the strings are properly normalized and then perform a caseless comparison. **Function Specification:** ```python def compare_unicode_strings(s1: str, s2: str) -> bool: This function takes two Unicode strings, normalizes them using NFD (Normalization Form D), converts them to their casefold versions, and compares them for equality. Args: s1 (str): The first Unicode string. s2 (str): The second Unicode string. Returns: bool: True if the normalized and casefolded strings are equal, False otherwise. ``` **Input:** - `s1`: A Unicode string to be compared. - `s2`: Another Unicode string to be compared with `s1`. **Output:** - A boolean value indicating whether the normalized and casefolded versions of `s1` and `s2` are equal. **Constraints:** - The function should handle cases where characters can be represented by different sequences of code points but still be considered the same character. - The normalization process should use NFD (Normalization Form D). - The comparison should be caseless (i.e., it should not differentiate between uppercase and lowercase). **Example:** ```python # Example 1: s1 = \'GÃ¼rzenichstraÃŸe\' s2 = \'gÃ¼rzenichstrasse\' # Using the function should return True assert compare_unicode_strings(s1, s2) == True # Example 2: s1 = \'Ãª\' s2 = \'N{LATIN SMALL LETTER E}N{COMBINING CIRCUMFLEX ACCENT}\' # Using the function should return True assert compare_unicode_strings(s1, s2) == True # Example 3: s1 = \'Python\' s2 = \'python\' # Using the function should return True assert compare_unicode_strings(s1, s2) == True # Example 4: s1 = \'CafÃ©\' s2 = \'Cafeu0301\' # Using the function should return True assert compare_unicode_strings(s1, s2) == True ``` **Notes:** 1. You may find it helpful to use the `unicodedata.normalize` and `str.casefold` methods. 2. Pay attention to edge cases where Unicode strings involve combining characters or special characters.","solution":"import unicodedata def compare_unicode_strings(s1: str, s2: str) -> bool: This function takes two Unicode strings, normalizes them using NFD (Normalization Form D), converts them to their casefold versions, and compares them for equality. Args: s1 (str): The first Unicode string. s2 (str): The second Unicode string. Returns: bool: True if the normalized and casefolded strings are equal, False otherwise. normalized_s1 = unicodedata.normalize(\'NFD\', s1).casefold() normalized_s2 = unicodedata.normalize(\'NFD\', s2).casefold() return normalized_s1 == normalized_s2"},{"question":"**Problem Statement:** You are tasked with creating a command-line interface for a program named `file_manager.py` that performs various file operations. Design an argument parser using the `argparse` module to handle the following functionalities: 1. **Basic Operations**: The program should have sub-commands for different file operations including `copy`, `move`, and `delete`. 2. **Copy Command**: This command takes two positional arguments `src` (source file path) and `dest` (destination file path), optionally takes a `--force` flag to overwrite the destination file if it exists. 3. **Move Command**: This command takes the same arguments as the `copy` command, but moves the file instead. 4. **Delete Command**: This command takes one positional argument, `target` (file path to be deleted). **Requirements:** - Implement the argument parser configuration using the `argparse` module. - For each sub-command, add appropriate arguments and flags. - Ensure that the help message generated by the parser is user-friendly and clearly describes the available commands and their usage. - Handle errors gracefully and print useful messages when users provide invalid inputs. **Constraints:** - Assume that `file_manager.py` script is implemented separately and focuses only on the parser configuration here. - The solution should be efficient and follow best practices to manage command-line arguments. **Expected Output:** Your function should output the expected command-line configuration using the `argparse` module. ```python import argparse def create_parser(): parser = argparse.ArgumentParser(prog=\'file_manager.py\', description=\'Manage file operations such as copy, move, and delete.\') subparsers = parser.add_subparsers(dest=\'command\', required=True, help=\'Sub-commands for various file operations\') # Copy sub-command copy_parser = subparsers.add_parser(\'copy\', help=\'Copy a file from source to destination\') copy_parser.add_argument(\'src\', type=str, help=\'Source file path\') copy_parser.add_argument(\'dest\', type=str, help=\'Destination file path\') copy_parser.add_argument(\'--force\', action=\'store_true\', help=\'Force overwrite the destination file if it exists\') # Move sub-command move_parser = subparsers.add_parser(\'move\', help=\'Move a file from source to destination\') move_parser.add_argument(\'src\', type=str, help=\'Source file path\') move_parser.add_argument(\'dest\', type=str, help=\'Destination file path\') # Delete sub-command delete_parser = subparsers.add_parser(\'delete\', help=\'Delete a file\') delete_parser.add_argument(\'target\', type=str, help=\'File path to be deleted\') return parser # Example usage if __name__ == \'__main__\': parser = create_parser() args = parser.parse_args() print(args) # For testing purposes ``` **Testing:** Verify the functionality by running the following commands (assuming the script is saved as `file_manager.py`): ```sh python file_manager.py copy /path/to/source /path/to/destination --force python file_manager.py move /path/to/source /path/to/destination python file_manager.py delete /path/to/target ``` Ensure the parser handles invalid inputs and displays appropriate help messages: ```sh python file_manager.py --help python file_manager.py copy --help python file_manager.py move --help python file_manager.py delete --help ``` The parser should exit with errors for invalid commands or missing required arguments: ```sh python file_manager.py copy /path/to/source usage: file_manager.py copy [-h] [--force] src dest file_manager.py copy: error: the following arguments are required: dest ```","solution":"import argparse def create_parser(): parser = argparse.ArgumentParser(prog=\'file_manager.py\', description=\'Manage file operations such as copy, move, and delete.\') subparsers = parser.add_subparsers(dest=\'command\', required=True, help=\'Sub-commands for various file operations\') # Copy sub-command copy_parser = subparsers.add_parser(\'copy\', help=\'Copy a file from source to destination\') copy_parser.add_argument(\'src\', type=str, help=\'Source file path\') copy_parser.add_argument(\'dest\', type=str, help=\'Destination file path\') copy_parser.add_argument(\'--force\', action=\'store_true\', help=\'Force overwrite the destination file if it exists\') # Move sub-command move_parser = subparsers.add_parser(\'move\', help=\'Move a file from source to destination\') move_parser.add_argument(\'src\', type=str, help=\'Source file path\') move_parser.add_argument(\'dest\', type=str, help=\'Destination file path\') # Delete sub-command delete_parser = subparsers.add_parser(\'delete\', help=\'Delete a file\') delete_parser.add_argument(\'target\', type=str, help=\'File path to be deleted\') return parser"},{"question":"# **Coding Assessment Question** **Objective:** Implement and test a function that opens a list of URLs in a web browser, ensuring each URL opens in a new tab. **Question:** Write a Python function **`open_urls_in_browser`** that takes a list of URLs and opens each URL in a new tab in the default web browser. Use the `webbrowser` module\'s `open_new_tab` function to achieve this. Additionally, your function should handle any potential exceptions raised while trying to open the URLs. **Function Signature:** ```python def open_urls_in_browser(urls: list) -> None: pass ``` **Parameters:** - `urls` (list): A list of strings, where each string is a URL to be opened. **Requirements:** - The function should open each URL in a new tab of the default web browser. - If an error occurs while attempting to open any URL (e.g., invalid URL format, browser control error), the function should handle the exception gracefully and print an error message indicating the URL that could not be opened. **Constraints:** - Each URL in the list is guaranteed to be a string. - The list can be empty, in which case the function should do nothing. - The URLs may point to any valid web resource. **Example:** ```python urls = [\\"https://www.python.org\\", \\"https://www.example.com\\", \\"invalid-url\\"] open_urls_in_browser(urls) ``` Expected Output: - The default web browser should open \\"https://www.python.org\\" and \\"https://www.example.com\\" in new tabs. - An error message should be printed indicating that \\"invalid-url\\" could not be opened. **Notes:** - You may test your implementation with various lists of URLs, including empty lists and lists with both valid and invalid URLs.","solution":"import webbrowser def open_urls_in_browser(urls: list) -> None: for url in urls: try: webbrowser.open_new_tab(url) except Exception as e: print(f\\"Error opening {url}: {e}\\")"},{"question":"# Multiprocessing in PyTorch Problem Statement You are required to implement a function using PyTorch\'s `torch.distributed.elastic.multiprocessing` module to start multiple worker processes and manage their execution. The function should launch a specified number of worker processes, each executing a given function on a range of data. The function should also collect and return the results from all worker processes. Function Signature ```python def run_distributed_processing(num_workers: int, data: list, worker_fn: callable) -> list: Launches multiple worker processes to perform parallel computation on the input data. Parameters: num_workers (int): Number of worker processes to start. data (list): Data to be distributed among the worker processes. worker_fn (callable): Function to be executed by each worker process. The function should accept a chunk of the data as input and return the processed result. Returns: list: A list containing results from each worker process. pass ``` Input Format - `num_workers` (int): The number of worker processes to be started. It should be a positive integer. - `data` (list): A list of elements to be processed. It can contain any type of data, but it will be processed in chunks distributed among the worker processes. - `worker_fn` (callable): A function that takes a chunk of data as input and returns the processed result. The exact operation performed by this function is not specified and can be defined by the user. Output Format - A list containing the combined results from each worker process. The order of results should match the order of data chunks processed by the workers. Constraints - Ensure that the input data is evenly distributed among the worker processes. - Handle cases where the data cannot be evenly divided among the workers by ensuring that each worker gets at least one element and no worker gets a significantly larger chunk than others. - Implement appropriate error handling for process failures. Example Usage ```python def square_chunk(data_chunk): return [x**2 for x in data_chunk] data = [1, 2, 3, 4, 5, 6, 7, 8] num_workers = 4 result = run_distributed_processing(num_workers, data, square_chunk) print(result) # Output: [1, 4, 9, 16, 25, 36, 49, 64] ``` In the above example, if `num_workers` is set to 4, the data list will be divided into 4 chunks: [1, 2], [3, 4], [5, 6], and [7, 8]. Each chunk will be processed by one of the worker processes using the `square_chunk` function, and their results will be combined into the final output list. Performance Requirements - The function should effectively utilize all available worker processes. - The function should be able to handle large datasets efficiently by distributing the workload among the workers. Good luck, and happy coding!","solution":"import torch import torch.multiprocessing as mp def run_distributed_processing(num_workers: int, data: list, worker_fn: callable) -> list: Launches multiple worker processes to perform parallel computation on the input data. Parameters: num_workers (int): Number of worker processes to start. data (list): Data to be distributed among the worker processes. worker_fn (callable): Function to be executed by each worker process. The function should accept a chunk of the data as input and return the processed result. Returns: list: A list containing results from each worker process. def worker_process(worker_id, data_chunk, return_dict): return_dict[worker_id] = worker_fn(data_chunk) # Split data into chunks chunk_size = len(data) // num_workers chunks = [data[i * chunk_size: (i + 1) * chunk_size] for i in range(num_workers)] # Handle remaining data if it cannot be evenly divided by num_workers if len(data) % num_workers != 0: chunks[-1].extend(data[num_workers * chunk_size:]) manager = mp.Manager() return_dict = manager.dict() processes = [] for worker_id in range(num_workers): p = mp.Process(target=worker_process, args=(worker_id, chunks[worker_id], return_dict)) processes.append(p) p.start() for p in processes: p.join() result = [] for worker_id in range(num_workers): result.extend(return_dict[worker_id]) return result"},{"question":"# Covariance Estimation Comparison **Objective:** You are required to implement and compare various covariance estimation techniques from the `sklearn.covariance` module on a synthetic dataset. The goal is to analyze the differences in performance and accuracy among these methods under various conditions. **Dataset:** You will generate a synthetic dataset with `n_samples` and `n_features` using the following rules: - The dataset follows a multivariate normal distribution with a known mean vector and covariance matrix. - You will introduce a certain proportion of outliers to the dataset. ```python import numpy as np from sklearn.datasets import make_spd_matrix from sklearn.preprocessing import StandardScaler def generate_synthetic_data(n_samples, n_features, n_outliers, random_state=42): np.random.seed(random_state) mean = np.zeros(n_features) cov = make_spd_matrix(n_features, random_state=random_state) # Generate a random positive definite matrix # Generate the multivariate normal data X = np.random.multivariate_normal(mean, cov, size=n_samples) # Introduce outliers outliers = np.random.uniform(low=-10, high=10, size=(n_outliers, n_features)) X[:n_outliers] = outliers return StandardScaler().fit_transform(X), cov # Example X, true_cov = generate_synthetic_data(n_samples=100, n_features=5, n_outliers=10) ``` **Task:** 1. Implement the following covariance estimation techniques: - Empirical Covariance - Shrunk Covariance - Ledoit-Wolf Shrinkage - Oracle Approximating Shrinkage (OAS) - Robust Covariance Estimation (Minimum Covariance Determinant) 2. Evaluate and compare the performance of each estimator in terms of: - Estimated Covariance Matrix - Mean Squared Error (MSE) between the estimated covariance matrix and the true covariance matrix - Ability to handle outliers 3. Visualize the covariance matrices and the performance metrics. **Function Implementation:** ```python from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet) import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import mean_squared_error def compare_covariance_estimators(X, true_cov): estimators = { \'EmpiricalCovariance\': EmpiricalCovariance(store_precision=True), \'ShrunkCovariance\': ShrunkCovariance(store_precision=True), \'LedoitWolf\': LedoitWolf(store_precision=True), \'OAS\': OAS(store_precision=True), \'MinCovDet\': MinCovDet(store_precision=True) } results = {} for name, estimator in estimators.items(): estimator.fit(X) cov_matrix = estimator.covariance_ mse = mean_squared_error(true_cov, cov_matrix) results[name] = (cov_matrix, mse) # Visualization plt.figure(figsize=(10, 4)) plt.subplot(121) sns.heatmap(true_cov, cmap=\\"coolwarm\\", annot=False) plt.title(\\"True Covariance Matrix\\") plt.subplot(122) sns.heatmap(cov_matrix, cmap=\\"coolwarm\\", annot=False) plt.title(f\\"Estimated Covariance ({name}), MSE: {mse:.4f}\\") plt.show() return results # Example usage X, true_cov = generate_synthetic_data(n_samples=100, n_features=5, n_outliers=10) results = compare_covariance_estimators(X, true_cov) ``` **Expected Output:** - Visualizations showing the true and estimated covariance matrices for each estimator. - A dictionary containing the estimated covariance matrices and their corresponding MSEs for each estimator. **Constraints:** - The dataset size should be sufficient to demonstrate the differences between estimators. - Proper handling and comparison of outliers are essential. - Visualizations should be clear and interpretable. Evaluate the performance and robustness of each estimator in your report.","solution":"import numpy as np from sklearn.datasets import make_spd_matrix from sklearn.preprocessing import StandardScaler from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet) import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import mean_squared_error def generate_synthetic_data(n_samples, n_features, n_outliers, random_state=42): np.random.seed(random_state) mean = np.zeros(n_features) cov = make_spd_matrix(n_features, random_state=random_state) # Generate a random positive definite matrix # Generate the multivariate normal data X = np.random.multivariate_normal(mean, cov, size=n_samples) # Introduce outliers outliers = np.random.uniform(low=-10, high=10, size=(n_outliers, n_features)) X[:n_outliers] = outliers return StandardScaler().fit_transform(X), cov def compare_covariance_estimators(X, true_cov): estimators = { \'EmpiricalCovariance\': EmpiricalCovariance(store_precision=True), \'ShrunkCovariance\': ShrunkCovariance(store_precision=True), \'LedoitWolf\': LedoitWolf(store_precision=True), \'OAS\': OAS(store_precision=True), \'MinCovDet\': MinCovDet(store_precision=True) } results = {} for name, estimator in estimators.items(): estimator.fit(X) cov_matrix = estimator.covariance_ mse = mean_squared_error(true_cov, cov_matrix) results[name] = (cov_matrix, mse) # Visualization plt.figure(figsize=(10, 4)) plt.subplot(121) sns.heatmap(true_cov, cmap=\\"coolwarm\\", annot=False) plt.title(\\"True Covariance Matrix\\") plt.subplot(122) sns.heatmap(cov_matrix, cmap=\\"coolwarm\\", annot=False) plt.title(f\\"Estimated Covariance ({name}), MSE: {mse:.4f}\\") plt.show() return results"},{"question":"**Coding Assessment Question:** You are required to prepare a visual analysis using the seaborn library to demonstrate your understanding of the `stripplot` and `catplot` functions and their various features as described below. # Objective: Create a series of visualizations using the seaborn library that analyze the distribution of the `total_bill` variable from the `tips` dataset under different conditions and customizations. # Instructions: 1. **Basic Strip Plot:** - Plot the basic distribution of the `total_bill` variable using a horizontal `stripplot`. 2. **Strip Plot with Categorical Variable:** - Create a strip plot showing the distribution of `total_bill` across different days of the week (`day` variable), oriented vertically. - Further, add a `hue` based on patron `sex`. 3. **Customized Strip Plot:** - Generate a strip plot where the distribution of `total_bill` is shown across different days. - Use size (`size` variable) as the `hue` and apply the `deep` palette. - Add a `dodge` parameter to split by `hue`. - Set `jitter` to `False` and customize the markers with `s=20`, `linewidth=1`, `alpha=.1`, and `marker=\\"D\\"`. 4. **Faceted Plot:** - Use the `catplot` function to create a faceted grid of plots where: - The `total_bill` variable is plotted against the time of day (`time`). - Each facet corresponds to one of the days of the week (`day`). - Different colors (`hue`) represent the patron\'s `sex`. - Set the `aspect` parameter to `.5` to control the aspect ratio of the facets. # Input: - No explicit inputs are required as the `tips` dataset is loaded directly using `sns.load_dataset(\\"tips\\")`. # Output: - You should output the visualizations described in the instructions. # Constraints: - Ensure all visualizations are properly labeled with titles and axis labels. - Include legends where applicable. # Performance: - Make sure the code is efficient and avoids unnecessary computations. Below is the expected code template. Complete it to solve the task. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Basic Strip Plot plt.figure(figsize=[10, 6]) sns.stripplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill (Horizontal Strip Plot)\\") plt.xlabel(\\"Total Bill\\") plt.show() # 2. Strip Plot with Categorical Variable plt.figure(figsize=[10, 6]) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", orient=\\"h\\") plt.title(\\"Total Bill Distribution by Day and Sex\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.show() # 3. Customized Strip Plot plt.figure(figsize=[10, 6]) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"size\\", palette=\\"deep\\", dodge=True, jitter=False, s=20, marker=\\"D\\", linewidth=1, alpha=.1) plt.title(\\"Customized Total Bill Distribution by Day and Size\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Size\\") plt.show() # 4. Faceted Plot sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5) plt.subplots_adjust(top=0.9) plt.suptitle(\\"Faceted Plot of Total Bill by Time and Sex Across Days\\", size=16) plt.show() ``` Complete the code as per the instructions to demonstrate your proficiency in using the seaborn library for data visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Basic Strip Plot plt.figure(figsize=[10, 6]) sns.stripplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill (Horizontal Strip Plot)\\") plt.xlabel(\\"Total Bill\\") plt.show() # 2. Strip Plot with Categorical Variable plt.figure(figsize=[10, 6]) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", orient=\\"h\\") plt.title(\\"Total Bill Distribution by Day and Sex\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.show() # 3. Customized Strip Plot plt.figure(figsize=[10, 6]) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"size\\", palette=\\"deep\\", dodge=True, jitter=False, s=20, marker=\\"D\\", linewidth=1, alpha=.1) plt.title(\\"Customized Total Bill Distribution by Day and Size\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Size\\") plt.show() # 4. Faceted Plot sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5) plt.subplots_adjust(top=0.9) plt.suptitle(\\"Faceted Plot of Total Bill by Time and Sex Across Days\\", size=16) plt.show()"},{"question":"# Coding Assessment: Analyzing University Student Data **Objective**: To assess your understanding of fundamental and advanced pandas functionalities. **Problem Statement**: You are provided with two datasets about university students: student information and students\' scores in various subjects. Your task is to perform data manipulation and analysis to extract meaningful insights. **Datasets**: 1. `students_info.csv` - Contains information about students. 2. `students_scores.csv` - Contains students\' scores in different subjects. **Dataset 1: students_info.csv** | student_id | name | age | gender | major | |------------|------------|-----|--------|--------------------| | 1 | John Doe | 21 | M | Computer Science | | 2 | Jane Smith | 22 | F | Mathematics | | ... | ... | ... | ... | ... | **Dataset 2: students_scores.csv** | student_id | subject | score | |------------|-------------|-------| | 1 | Math | 88 | | 1 | English | 75 | | 2 | Math | 92 | | 2 | English | 78 | | ... | ... | ... | **Tasks**: 1. **Load the Data**: - Load the two datasets into pandas DataFrames. 2. **Merge Datasets**: - Merge the datasets on the `student_id` column. 3. **Data Transformation**: - Calculate and add a new column `passing_status` to indicate whether the student passed each subject (`score` >= 50). 4. **Aggregate Data**: - Calculate the average score for each student. - Calculate the highest and lowest score for each subject. - For each major, calculate the average, maximum, and minimum score. 5. **Analyze Data**: - Find out how many students passed all their subjects and how many did not. - Determine the subject with the highest overall average score. - Generate a summary of the data for each student, including their total number of subjects taken and their average score. 6. **Export Results**: - Export the final DataFrame to a new CSV file `students_summary.csv`. **Constraints**: - Ensure every function or method you write is efficient and uses pandas best practices. **Format**: ```python import pandas as pd # Task 1: Load the Data students_info = pd.read_csv(\'students_info.csv\') students_scores = pd.read_csv(\'students_scores.csv\') # Task 2: Merge Datasets merged_df = pd.merge(students_info, students_scores, on=\'student_id\') # Task 3: Data Transformation merged_df[\'passing_status\'] = merged_df[\'score\'] >= 50 # Task 4: Aggregate Data average_score_per_student = merged_df.groupby(\'student_id\')[\'score\'].mean() highest_score_per_subject = merged_df.groupby(\'subject\')[\'score\'].max() lowest_score_per_subject = merged_df.groupby(\'subject\')[\'score\'].min() major_score_stats = merged_df.groupby(\'major\').agg({ \'score\': [\'mean\', \'max\', \'min\'] }) # Task 5: Analyze Data students_passed_all = merged_df.groupby(\'student_id\')[\'passing_status\'].all().sum() students_failed_any = merged_df.groupby(\'student_id\')[\'passing_status\'].any().sum() subject_highest_avg = merged_df.groupby(\'subject\')[\'score\'].mean().idxmax() student_summary = merged_df.groupby(\'student_id\').agg({ \'subject\': \'count\', \'score\': \'mean\' }).rename(columns={\'subject\': \'total_subjects\', \'score\': \'average_score\'}) # Task 6: Export Results student_summary.to_csv(\'students_summary.csv\') print(\\"Analysis Complete. Results exported to `students_summary.csv`.\\") ``` You are required to implement the above tasks using pandas and ensure your solution is efficient and follows pandas best practices.","solution":"import pandas as pd def load_data(students_info_path, students_scores_path): Load the datasets into pandas DataFrames. students_info = pd.read_csv(students_info_path) students_scores = pd.read_csv(students_scores_path) return students_info, students_scores def merge_datasets(students_info, students_scores): Merge the datasets on the `student_id` column. return pd.merge(students_info, students_scores, on=\'student_id\') def calculate_passing_status(merged_df): Calculate and add a new column `passing_status`. merged_df[\'passing_status\'] = merged_df[\'score\'] >= 50 return merged_df def calculate_aggregate_data(merged_df): Calculate various aggregate statistics. average_score_per_student = merged_df.groupby(\'student_id\')[\'score\'].mean().reset_index() highest_score_per_subject = merged_df.groupby(\'subject\')[\'score\'].max().reset_index() lowest_score_per_subject = merged_df.groupby(\'subject\')[\'score\'].min().reset_index() major_score_stats = merged_df.groupby(\'major\').agg({ \'score\': [\'mean\', \'max\', \'min\'] }).reset_index() return average_score_per_student, highest_score_per_subject, lowest_score_per_subject, major_score_stats def analyze_data(merged_df): Perform data analysis to extract insights. students_passed_all = merged_df.groupby(\'student_id\')[\'passing_status\'].all().sum() students_failed_any = merged_df.groupby(\'student_id\')[\'passing_status\'].any().sum() subject_highest_avg = merged_df.groupby(\'subject\')[\'score\'].mean().idxmax() student_summary = merged_df.groupby(\'student_id\').agg({ \'subject\': \'count\', \'score\': \'mean\' }).rename(columns={\'subject\': \'total_subjects\', \'score\': \'average_score\'}).reset_index() return students_passed_all, students_failed_any, subject_highest_avg, student_summary def export_results(student_summary, output_path): Export the final DataFrame to a new CSV file. student_summary.to_csv(output_path, index=False) def main(students_info_path, students_scores_path, output_path): students_info, students_scores = load_data(students_info_path, students_scores_path) merged_df = merge_datasets(students_info, students_scores) merged_df = calculate_passing_status(merged_df) average_score_per_student, highest_score_per_subject, lowest_score_per_subject, major_score_stats = calculate_aggregate_data(merged_df) students_passed_all, students_failed_any, subject_highest_avg, student_summary = analyze_data(merged_df) export_results(student_summary, output_path) return { \'average_score_per_student\': average_score_per_student, \'highest_score_per_subject\': highest_score_per_subject, \'lowest_score_per_subject\': lowest_score_per_subject, \'major_score_stats\': major_score_stats, \'students_passed_all\': students_passed_all, \'students_failed_any\': students_failed_any, \'subject_highest_avg\': subject_highest_avg, \'student_summary\': student_summary }"},{"question":"# Clustering and Evaluation with Scikit-learn Objective: Your task is to implement a clustering solution using Scikit-learn\'s clustering algorithms and evaluate its performance using specified metrics. You will need to handle input data, apply the clustering algorithm, and compute the evaluation metrics to assess the clustering quality. Problem Statement: You are given a dataset and are required to: 1. Implement clustering using the `KMeans` algorithm from Scikit-learn. 2. Evaluate the clustering results using the Silhouette Coefficient and Adjusted Rand Index. Dataset: Use the `make_blobs` function from Scikit-learn to generate a synthetic dataset. ```python from sklearn.datasets import make_blobs # Generate synthetic dataset X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0) ``` Requirements: 1. **Clustering**: Write a function `perform_clustering` that: - Takes the dataset `X` and the number of clusters `n_clusters` as input. - Applies the `KMeans` algorithm to cluster the data. - Returns the predicted labels for the data. 2. **Evaluation**: Write a function `evaluate_clustering` that: - Takes the true labels `y_true`, predicted labels `y_pred`, and the dataset `X` as input. - Computes and prints the Silhouette Coefficient and Adjusted Rand Index. 3. **Main Execution**: In the main block: - Generate the dataset using `make_blobs`. - Call `perform_clustering` with `n_clusters=4`. - Call `evaluate_clustering` to print the evaluation metrics. Function Signatures: 1. `def perform_clustering(X, n_clusters):` 2. `def evaluate_clustering(y_true, y_pred, X):` Constraints: - Use only the specified algorithms and evaluation metrics. - Ensure the clustering and evaluation are efficient and handle the dataset size appropriately. - The number of clusters for KMeans is set to 4 based on the dataset generation. Expected Output: The function `evaluate_clustering` should output the Silhouette Coefficient and Adjusted Rand Index values. ```python # Example Output: Silhouette Coefficient: 0.55 Adjusted Rand Index: 0.89 ``` You can use the following script template to get started: ```python from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, adjusted_rand_score from sklearn.datasets import make_blobs def perform_clustering(X, n_clusters): # [Your code here] pass def evaluate_clustering(y_true, y_pred, X): # [Your code here] pass if __name__ == \\"__main__\\": X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0) y_pred = perform_clustering(X, 4) evaluate_clustering(y_true, y_pred, X) ``` Good luck!","solution":"from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, adjusted_rand_score from sklearn.datasets import make_blobs def perform_clustering(X, n_clusters): Performs clustering on the dataset X using KMeans algorithm. Parameters: X (ndarray): Dataset to be clustered. n_clusters (int): Number of clusters for the KMeans algorithm. Returns: ndarray: Predicted labels for the dataset. kmeans = KMeans(n_clusters=n_clusters, random_state=0) y_pred = kmeans.fit_predict(X) return y_pred def evaluate_clustering(y_true, y_pred, X): Evaluates the clustering results using Silhouette Coefficient and Adjusted Rand Index. Parameters: y_true (ndarray): True labels of the dataset. y_pred (ndarray): Predicted labels from the clustering algorithm. X (ndarray): The dataset. Returns: dict: A dictionary with Silhouette Coefficient and Adjusted Rand Index. silhouette_avg = silhouette_score(X, y_pred) ari = adjusted_rand_score(y_true, y_pred) print(f\\"Silhouette Coefficient: {silhouette_avg:.2f}\\") print(f\\"Adjusted Rand Index: {ari:.2f}\\") return {\\"Silhouette Coefficient\\": silhouette_avg, \\"Adjusted Rand Index\\": ari} if __name__ == \\"__main__\\": X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0) y_pred = perform_clustering(X, 4) evaluate_clustering(y_true, y_pred, X)"},{"question":"# Pandas Categorical Data Manipulation Objective: Implement a function `analyze_categorical_data` that processes a given DataFrame containing a mix of categorical and numerical data and returns important categorical data insights. Function Signature: ```python import pandas as pd from pandas.api.types import CategoricalDtype def analyze_categorical_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input: - `df` (pd.DataFrame): The input DataFrame with various data types, including \'object\' columns that should be treated as categorical. Output: - (pd.DataFrame): A new DataFrame summarizing the categorical insights with the following columns: - \'column\': the original column name from `df`. - \'unique_values\': the number of unique values in the categorical column. - \'ordered\': a boolean indicating whether the categorical column has an order. - \'most_frequent_value\': the most frequently occurring value in the categorical column. - \'frequency\': the frequency of the most frequently occurring value. - \'memory_usage_bytes\': the memory usage in bytes of the categorical column. Requirements: 1. Any columns that are of type \'object\' in the input `df` should be converted to categorical. 2. Use `CategoricalDtype` to enforce an order on one of the categorical columns (if any). 3. Generate the desired insights for all categorical columns. 4. Implement memory usage checks and comparison with the original object column memory usage. Constraints: - Assume the DataFrame `df` can have a maximum of 100 columns and 10,000 rows. - Ensure the function runs efficiently under the given constraints. - Handle potential missing values (NaNs) in the categorical columns appropriately. Example Usage: ```python import numpy as np import pandas as pd # Sample DataFrame data = { \'A\': [\'foo\', \'bar\', \'foo\', \'bar\', np.nan, \'foo\', \'bar\', \'foo\', \'bar\', \'foo\'], \'B\': [\'high\', \'low\', \'medium\', \'medium\', \'low\', \'high\', \'medium\', \'low\', \'low\', \'medium\'], \'C\': list(\'abcdefghij\') } df = pd.DataFrame(data) # Run the function result = analyze_categorical_data(df) print(result) ``` Expected output: ``` column unique_values ordered most_frequent_value frequency memory_usage_bytes 0 A 2 False foo 5 x 1 B 3 True low 4 y 2 C 10 False a 1 z ``` (Note: `x`, `y`, `z` should reflect actual memory usage values.) Notes: - Ensure that the DataFrame column names should be retained. - Handle edge cases such as all unique values or all NaNs in a categorical column. - You may use helper functions for readability and modularity.","solution":"import pandas as pd from pandas.api.types import CategoricalDtype def analyze_categorical_data(df: pd.DataFrame) -> pd.DataFrame: summary_data = [] for column in df.columns: if df[column].dtype == \'object\': df[column] = df[column].astype(\'category\') if pd.api.types.is_categorical_dtype(df[column]): unique_values = df[column].nunique() ordered = df[column].dtype.ordered most_frequent_value = df[column].mode()[0] frequency = df[column].value_counts().iloc[0] memory_usage_bytes = df[column].memory_usage(deep=True) summary_data.append({ \'column\': column, \'unique_values\': unique_values, \'ordered\': ordered, \'most_frequent_value\': most_frequent_value, \'frequency\': frequency, \'memory_usage_bytes\': memory_usage_bytes }) return pd.DataFrame(summary_data)"},{"question":"**Objective:** Assess the student\'s ability to utilize seaborn\'s jittering techniques in `so.Plot`. Problem Statement: You are given a dataset of penguins with various attributes. Your task is to create a function that visualizes the relationship between the species of penguins and their body mass while incorporating different jitter techniques using seaborn. Function Signature: ```python def visualize_penguin_data_with_jitter(penguin_data): pass ``` Input: - `penguin_data`: A DataFrame containing columns: - `species`: Categorical data representing the species of penguins. - `body_mass_g`: Numeric data representing the body mass of penguins. - `flipper_length_mm`: Numeric data representing the flipper length of penguins. Requirements: 1. Plot 1: - Create a scatter plot showing the species on the y-axis and body mass on the x-axis. - Apply basic jitter along the y-axis with the default settings. 2. Plot 2: - Create a scatter plot showing the species on the y-axis and body mass on the x-axis. - Apply jitter along the y-axis with `width` parameter set to 0.5. 3. Plot 3: - Create a scatter plot showing body mass on the y-axis and the species on the x-axis. - Apply jitter along the x-axis with `width` parameter set to 0.5. 4. Plot 4: - Create a scatter plot with rounded body mass values on the x-axis and flipper length on the y-axis. - Apply jitter along both the x-axis and y-axis with values set to 200 and 5 respectively. Output: - The function should display the four plots in a 2x2 grid for easy comparison. Example: ```python import seaborn as sns import pandas as pd # Load example dataset penguins = sns.load_dataset(\\"penguins\\") # Sample function call visualize_penguin_data_with_jitter(penguins) ``` Notes: - You must import necessary libraries (`matplotlib.pyplot`, `seaborn.objects`, and `seaborn`). - Ensure your plots are clear and well-labeled.","solution":"import matplotlib.pyplot as plt import seaborn as sns def visualize_penguin_data_with_jitter(penguin_data): Visualizes the relationship between penguin species and their body mass including different jitter techniques. plt.figure(figsize=(14, 10)) # Plot 1: Basic jitter along the y-axis with default settings plt.subplot(2, 2, 1) sns.stripplot(x=\'body_mass_g\', y=\'species\', data=penguin_data, jitter=True) plt.title(\\"Basic Jitter (y-axis, default)\\") # Plot 2: Jitter along the y-axis with width set to 0.5 plt.subplot(2, 2, 2) sns.stripplot(x=\'body_mass_g\', y=\'species\', data=penguin_data, jitter=0.5) plt.title(\\"Jitter with Width 0.5 (y-axis)\\") # Plot 3: Jitter along the x-axis with width set to 0.5 plt.subplot(2, 2, 3) sns.stripplot(x=\'species\', y=\'body_mass_g\', data=penguin_data, jitter=0.5) plt.title(\\"Jitter with Width 0.5 (x-axis)\\") # Plot 4: Jitter along both x-axis and y-axis penguin_data[\'rounded_body_mass_g\'] = penguin_data[\'body_mass_g\'].round(-2) plt.subplot(2, 2, 4) sns.stripplot(x=\'rounded_body_mass_g\', y=\'flipper_length_mm\', data=penguin_data, jitter=0.2) plt.title(\\"Jitter on Rounded Body Mass and Flipper Length\\") plt.tight_layout() plt.show()"},{"question":"Question: Implementing Asynchronous Tasks with asyncio # Objective The goal of this question is to assess your understanding and ability to implement asynchronous tasks using asyncioâ€™s concurrent programming capabilities. # Task You need to implement a function `fetch_data_concurrently(urls: List[str]) -> List[str]` which performs the following tasks: 1. Accepts a list of URLs. 2. Fetches data from each URL concurrently. 3. Returns a list of content fetched from these URLs. # Function Signature ```python from typing import List import asyncio async def fetch_data_concurrently(urls: List[str]) -> List[str]: pass ``` # Description - **Input:** - `urls`: A list of strings where each string is a URL to fetch data from. - **Output:** - A list of strings where each string is the content retrieved from the corresponding URL. # Constraints - Ensure the function handles network IO efficiently by fetching data concurrently. - If an error occurs while fetching data from a URL, capture it and store an empty string `\'\'` for that URL in the resulting list. # Requirements - Use the `aiohttp` library to make asynchronous HTTP requests. - Make sure to handle any potential network errors by returning an empty string for requests that fail. - Use asyncioâ€™s features such as `async/await` and tasks to achieve concurrency. # Example ```python import aiohttp async def fetch(url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def fetch_data_concurrently(urls: List[str]) -> List[str]: tasks = [fetch(url) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) # Example usage: urls = [\'http://example.com\', \'http://example.org\'] result = asyncio.run(fetch_data_concurrently(urls)) print(result) ``` # Note - The `fetch` function is used to asynchronously get the content of a URL. - The `fetch_data_concurrently` function creates tasks for each URL fetch operation and runs them concurrently using `asyncio.gather`. # Performance - Ensure that the solution is efficient and handles the concurrency properly such that all URLs are fetched simultaneously. Good luck!","solution":"from typing import List import asyncio import aiohttp async def fetch(url: str) -> str: Asynchronously fetches the content from the given URL. If an error occurs, returns an empty string. try: async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() except Exception: return \\"\\" async def fetch_data_concurrently(urls: List[str]) -> List[str]: Fetches data from the given list of URLs concurrently and returns the corresponding content list. tasks = [fetch(url) for url in urls] return await asyncio.gather(*tasks)"},{"question":"# Custom Attention Mechanism Implementation in PyTorch Objective Implement a custom scaled dot-product attention mechanism using PyTorch. This function will be the core component of a transformer-like architecture used in various NLP tasks, such as machine translation or text summarization. Problem Statement You need to create a function `scaled_dot_product_attention` in PyTorch that computes the attention weights and the output as described below: Function Signature ```python def scaled_dot_product_attention(query, key, value, mask=None): Calculate the attention weights and output using scaled dot-product attention mechanism. Args: - query (torch.Tensor): Query tensor of shape (batch_size, num_heads, seq_length, depth). - key (torch.Tensor): Key tensor of shape (batch_size, num_heads, seq_length, depth). - value (torch.Tensor): Value tensor of shape (batch_size, num_heads, seq_length, depth). - mask (torch.Tensor, optional): Optional mask tensor of shape (batch_size, 1, seq_length, seq_length). Returns: - output (torch.Tensor): Output tensor of shape (batch_size, num_heads, seq_length, depth). - attention_weights (torch.Tensor): Attention weights of shape (batch_size, num_heads, seq_length, seq_length). pass ``` Instructions 1. Compute the dot product between the query and the key transposed. 2. Divide each element of the dot product by the square root of the depth of the key/query (i.e., `sqrt(depth)`). 3. Optionally apply the mask (if provided) by adding a large negative value (-1e9) to the positions specified by the mask. 4. Apply the softmax function to the scaled scores to obtain the attention weights. 5. Compute the output as the dot product between the attention weights and the value tensor. Expected Input and Output Formats - Inputs: - `query` tensor of `shape (batch_size, num_heads, seq_length, depth)`. - `key` tensor of `same shape as query`. - `value` tensor of `same shape as query`. - `mask` tensor (optional) `of shape (batch_size, 1, seq_length, seq_length)`. - Outputs: - `output` tensor of `same shape as query`. - `attention_weights` tensor of `shape (batch_size, num_heads, seq_length, seq_length)`. Example ```python import torch batch_size = 2 num_heads = 2 seq_length = 4 depth = 8 query = torch.randn(batch_size, num_heads, seq_length, depth) key = torch.randn(batch_size, num_heads, seq_length, depth) value = torch.randn(batch_size, num_heads, seq_length, depth) mask = torch.randint(0, 2, (batch_size, 1, seq_length, seq_length)).float() output, attention_weights = scaled_dot_product_attention(query, key, value, mask) # expected: output and attention_weights of appropriate shapes print(output.shape) # should be (2, 2, 4, 8) print(attention_weights.shape) # should be (2, 2, 4, 4) ``` Constraints - Ensure efficient implementation using PyTorch tensor operations. - The implementation should be vectorized and avoid explicit Python loops over the batch size or sequence length for performance reasons. Tips - Refer to PyTorch documentation for help with tensor operations. - Consider edge cases where sequence length is very short or very long. - Ensure your implementation handles cases where no mask is provided. Good luck!","solution":"import torch import torch.nn.functional as F def scaled_dot_product_attention(query, key, value, mask=None): Calculate the attention weights and output using scaled dot-product attention mechanism. Args: - query (torch.Tensor): Query tensor of shape (batch_size, num_heads, seq_length, depth). - key (torch.Tensor): Key tensor of shape (batch_size, num_heads, seq_length, depth). - value (torch.Tensor): Value tensor of shape (batch_size, num_heads, seq_length, depth). - mask (torch.Tensor, optional): Optional mask tensor of shape (batch_size, 1, seq_length, seq_length). Returns: - output (torch.Tensor): Output tensor of shape (batch_size, num_heads, seq_length, depth). - attention_weights (torch.Tensor): Attention weights of shape (batch_size, num_heads, seq_length, seq_length). # Calculate the dot product between query and key matmul_qk = torch.matmul(query, key.transpose(-2, -1)) # Scale the dot product by the square root of the depth depth = query.size(-1) scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(depth, dtype=torch.float32)) # Apply the mask (if any) by adding a large negative value to masked positions if mask is not None: scaled_attention_logits += (mask * -1e9) # Calculate the attention weights using the softmax function attention_weights = F.softmax(scaled_attention_logits, dim=-1) # Compute the output by multiplying the attention weights with the value tensor output = torch.matmul(attention_weights, value) return output, attention_weights"},{"question":"# Kernel Ridge Regression Assessment Objective In this coding assessment, you are required to implement and optimize a Kernel Ridge Regression (KRR) model using scikit-learn\'s `KernelRidge` class. Background Kernel Ridge Regression combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It allows learning of linear functions in a kernel-induced space, which enables non-linear regression when using non-linear kernels. Task Given a dataset comprising features and corresponding target values, perform the following steps: 1. **Data Preprocessing:** - Split the data into training and testing sets (80% training, 20% testing). 2. **Model Implementation:** - Implement the Kernel Ridge Regression model using the `KernelRidge` class from scikit-learn. - Use a radial basis function (RBF) kernel for the regression model. 3. **Hyperparameter Optimization:** - Perform hyperparameter optimization using grid search to find the best combination of `alpha` (regularization strength) and `gamma` (kernel coefficient for RBF kernel). - Use cross-validation to evaluate the performance during the grid search. 4. **Model Evaluation:** - Evaluate the final model\'s performance on the testing set using mean squared error (MSE) and RÂ² score. Input - A dataset containing feature values (`X`) and target values (`y`). Output - Print the best found hyperparameters for the KRR model. - Print the mean squared error (MSE) and the RÂ² score of the model on the test set. Constraints - You must use `KernelRidge` from scikit-learn. - Use an RBF kernel for the `KernelRidge`. Example Code ```python import numpy as np from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.kernel_ridge import KernelRidge from sklearn.metrics import mean_squared_error, r2_score # Load your dataset (X, y) # X, y = ... # Step 1: Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Implement Kernel Ridge Regression kr = KernelRidge(kernel=\'rbf\') # Step 3: Perform hyperparameter optimization using grid search param_grid = { \'alpha\': [1e-3, 1e-2, 1e-1, 1, 10, 100], \'gamma\': np.logspace(-2, 2, 5) } grid_search = GridSearchCV(kr, param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) # Best hyperparameters best_params = grid_search.best_params_ print(\\"Best parameters:\\", best_params) # Step 4: Evaluate the model best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) print(\\"Mean Squared Error:\\", mse) print(\\"RÂ² Score:\\", r2) ``` Notes - You can use any dataset of your choice for implementation. - Ensure your dataset is properly preprocessed before fitting the model. - Use `random_state=42` for reproducibility.","solution":"import numpy as np from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.kernel_ridge import KernelRidge from sklearn.metrics import mean_squared_error, r2_score def krr_model(X, y): Implements Kernel Ridge Regression with RBF kernel, performs hyperparameter optimization, and evaluates the model on the test set. Returns best hyperparameters, MSE and RÂ² score. # Step 1: Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Implement Kernel Ridge Regression kr = KernelRidge(kernel=\'rbf\') # Step 3: Perform hyperparameter optimization using grid search param_grid = { \'alpha\': [1e-3, 1e-2, 1e-1, 1, 10, 100], \'gamma\': np.logspace(-2, 2, 5) } grid_search = GridSearchCV(kr, param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) # Best hyperparameters best_params = grid_search.best_params_ # Step 4: Evaluate the model best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return best_params, mse, r2"},{"question":"Objective: Implement a class `CustomHTMLParser` that inherits from `HTMLParser`. The task is to parse provided HTML content and extract all links (i.e., anchor tags `<a>`) and their associated text content. The students should override necessary methods to accomplish this. Problem Statement: 1. Create a class `CustomHTMLParser` that subclasses `HTMLParser`. 2. Override the necessary methods to: - Collect and store all anchor tags\' `href` attribute values. - Collect and store the text content inside the `<a>` tags. 3. Implement a method `get_links` that returns a list of tuples. Each tuple should contain an `href` attribute value and the corresponding text content. Input: - HTML content provided as a `str`. Output: - A list of tuples, where each tuple consists of: - The `href` attribute of the `<a>` tag. - The text content inside the `<a>` tag. Example: ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): ... def handle_starttag(self, tag, attrs): ... def handle_endtag(self, tag): ... def handle_data(self, data): ... def get_links(self): ... # Example usage parser = CustomHTMLParser() html_content = \'\'\'<html> <head><title>Test</title></head> <body> <a href=\\"https://example.com\\">Example</a> <a href=\\"https://another.com\\">Another Example</a> </body> </html>\'\'\' parser.feed(html_content) print(parser.get_links()) # Output: [(\'https://example.com\', \'Example\'), (\'https://another.com\', \'Another Example\')] ``` Constraints: - Assume that the HTML content is well-formed. - The `<a>` tags will not be nested. # Submission Guidelines: - Ensure the class is self-contained and does not rely on external libraries. - Write clean, readable, and well-documented code. - Include error handling for any potential edge cases.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.links = [] # List to store tuples of (href, text) self.current_href = None self.current_text = [] def handle_starttag(self, tag, attrs): if tag == \'a\': href = dict(attrs).get(\'href\') if href: self.current_href = href def handle_endtag(self, tag): if tag == \'a\' and self.current_href is not None: self.links.append((self.current_href, \'\'.join(self.current_text).strip())) self.current_href = None self.current_text = [] def handle_data(self, data): if self.current_href is not None: self.current_text.append(data) def get_links(self): return self.links"},{"question":"You have been given the task of processing text files to convert their contents into uppercase letters. Implement a function `convert_to_uppercase(input_file: str, output_file: str) -> None` that performs the following operations using the `pipes` module: 1. Create a pipeline using the `pipes.Template` class. 2. Append a command to the pipeline that converts all alphabetic characters from lowercase to uppercase using the `tr` command. 3. Open the input file for reading and apply the pipeline to write the capitalized contents to the output file. # Specification - **Input:** - `input_file`: A string representing the path to the input text file. - `output_file`: A string representing the path where the converted text should be saved. - **Output:** - The function should not return any value. It should write the capitalized content to the `output_file`. # Example Suppose `input.txt` contains: ``` hello world python programming ``` After running `convert_to_uppercase(\\"input.txt\\", \\"output.txt\\")`, the `output.txt` should contain: ``` HELLO WORLD PYTHON PROGRAMMING ``` # Constraints - The function should handle large files efficiently within the memory constraints. - Assume valid file paths are provided and the files are accessible. - The function should handle any errors gracefully and provide meaningful error messages if the operations fail. # Note You can assume that the `pipes` module is available and you are using a Python version prior to 3.11 where it is not deprecated. Implement the function `convert_to_uppercase` as specified above. ```python import pipes def convert_to_uppercase(input_file: str, output_file: str) -> None: # Your implementation here pass ```","solution":"import pipes def convert_to_uppercase(input_file: str, output_file: str) -> None: try: template = pipes.Template() template.append(\'tr a-z A-Z\', \'--\') with open(input_file, \'r\') as infile: with template.open(output_file, \'w\') as outfile: outfile.write(infile.read()) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Demonstrate your understanding of how to work with the `html.entities` module to convert between HTML entity references and Unicode characters. Problem Statement Write a Python function `convert_html_entities` that takes in a string containing HTML entity references and performs the following operations: 1. Replace all HTML entity references in the string with their corresponding Unicode characters. 2. Ensure that both named character references (with and without trailing semicolons) and numeric character references (both decimal and hexadecimal) are correctly converted. # Function Signature ```python def convert_html_entities(input_string: str) -> str: pass ``` # Input - `input_string` (str): A string which may contain HTML entity references. The HTML entity references could be in the form of named characters (e.g., `&gt;`, `&frac34;`) or numeric values (e.g., `&#62;`, `&#x3E;`). # Output - Return a string in which all HTML entity references have been replaced by their respective Unicode characters. # Constraints - The input string will not exceed 10000 characters. - You can assume that all HTML entity references in the string are valid. # Example ```python # Example 1 input_string = \\"Hello &gt; world!\\" assert convert_html_entities(input_string) == \\"Hello > world!\\" # Example 2 input_string = \\"50 &#62; 20 &amp; 5 &#x3E; 3\\" assert convert_html_entities(input_string) == \\"50 > 20 & 5 > 3\\" # Example 3 input_string = \\"Temperature: 37&deg;C &frac34; cup sugar\\" assert convert_html_entities(input_string) == \\"Temperature: 37Â°C Â¾ cup sugar\\" ``` # Notes - Use the dictionaries provided in the `html.entities` module to perform the conversions. - Carefully handle both named and numeric HTML entity references.","solution":"import html def convert_html_entities(input_string: str) -> str: Replaces all HTML entity references in the input string with their corresponding Unicode characters. return html.unescape(input_string)"},{"question":"# WAV Audio Processing **Objective:** You are required to implement a function that reads a WAV file, reduces its volume by half, and writes the modified audio data to a new WAV file. This demonstrates your comprehension of file I/O, audio data manipulation, and the use of the `wave` module\'s methods. **Function Signature:** ```python def reduce_volume(input_wav: str, output_wav: str) -> None: pass ``` **Input:** - `input_wav` (str): The filename of the input WAV file. - `output_wav` (str): The filename of the output WAV file. **Output:** - This function doesn\'t return anything. It creates a new WAV file `output_wav` with the volume of the original WAV file reduced by half. **Constraints:** - The WAV file must be in \\"WAVE_FORMAT_PCM\\" format. - The function should use the `wave` module for both reading and writing the WAV files. **Requirements:** 1. Open the input WAV file using the \\"rb\\" mode. 2. Extract the parameters (number of channels, sample width, frame rate, and number of frames) of the input file. 3. Read the audio frames from the input file. 4. Reduce the volume by half by manipulating the audio frames. 5. Open the output WAV file using the \\"wb\\" mode. 6. Set the parameters of the output file to be the same as the input file. 7. Write the modified frames to the output file. 8. Close both the input and output WAV files properly. Here\'s an example to guide the implementation: **Example:** Given an input WAV file named \\"input.wav\\", the following call: ```python reduce_volume(\'input.wav\', \'output.wav\') ``` Should create a new WAV file named \\"output.wav\\" where the volume of the original audio is reduced by half. Note: You need to handle the appropriate import statements and ensure compatibility with the audio data type (i.e., PCM).","solution":"import wave import array def reduce_volume(input_wav: str, output_wav: str) -> None: with wave.open(input_wav, \'rb\') as wave_read: params = wave_read.getparams() num_channels = params.nchannels sample_width = params.sampwidth frame_rate = params.framerate num_frames = params.nframes frames = wave_read.readframes(num_frames) # Convert frames to an array for modification audio_frames = array.array(\'h\', frames) # Reduce volume by half for i in range(len(audio_frames)): audio_frames[i] = audio_frames[i] // 2 # Convert modified frames back to bytes new_frames = audio_frames.tobytes() with wave.open(output_wav, \'wb\') as wave_write: wave_write.setparams(params) wave_write.writeframes(new_frames)"},{"question":"# XML Event Stream Parsing and Conditional Processing You are given an XML string representing a collection of books in a library. Each book has attributes such as `title`, `author`, `year`, and `price`. Your task is to implement a function that parses this XML string, identifies book elements, and performs two specific tasks: 1. Collect titles of books that were published before the year 2000. 2. For books with a price greater than 20, print the entire XML string of that book. You need to use the `xml.dom.pulldom` module to accomplish this. # Function Signature ```python def process_books(xml_string: str) -> list: # your code here ``` # Input - `xml_string` (str): A well-formed XML string containing book elements. # Output - Returns a list of book titles published before the year 2000. - Prints XML strings of books with a price greater than 20. # Constraints - Each book element has attributes: `title`, `author`, `year`, and `price`. - Assume `xml_string` is correctly formatted. - Use `pulldom.parseString` for parsing the XML string. # Example ```python xml_data = \'\'\' <library> <book title=\\"The Great Gatsby\\" author=\\"F. Scott Fitzgerald\\" year=\\"1925\\" price=\\"10\\"/> <book title=\\"Harry Potter and the Philosopher\'s Stone\\" author=\\"J.K. Rowling\\" year=\\"1997\\" price=\\"25\\"/> <book title=\\"The Catcher in the Rye\\" author=\\"J.D. Salinger\\" year=\\"1951\\" price=\\"18\\"/> <book title=\\"The Hunger Games\\" author=\\"Suzanne Collins\\" year=\\"2008\\" price=\\"22\\"/> </library> \'\'\' print(process_books(xml_data)) # Output should be: [\'The Great Gatsby\', \'Harry Potter and the Philosopher\'s Stone\'] # And it should print: # <book title=\\"Harry Potter and the Philosopher\'s Stone\\" author=\\"J.K. Rowling\\" year=\\"1997\\" price=\\"25\\"/> # <book title=\\"The Hunger Games\\" author=\\"Suzanne Collins\\" year=\\"2008\\" price=\\"22\\"/> ``` # Notes - Use event-driven processing to handle `START_ELEMENT` events. - Extract attributes using `node.getAttribute`. - Use `expandNode` to print complete book elements when the price condition is met.","solution":"from xml.dom import pulldom def process_books(xml_string: str) -> list: titles_before_2000 = [] events = pulldom.parseString(xml_string) for event, node in events: if event == pulldom.START_ELEMENT and node.tagName == \'book\': year = int(node.getAttribute(\'year\')) price = float(node.getAttribute(\'price\')) if year < 2000: titles_before_2000.append(node.getAttribute(\'title\')) if price > 20: events.expandNode(node) print(node.toxml()) return titles_before_2000"},{"question":"You are given a bytes-like audio fragment in a specific encoding format (a-LAW or u-LAW). Your task is to decode this fragment into a linear format, perform some modifications, and encode it back into the original encoding format. The modifications include adjusting the volume by a given factor and converting the audio from mono to stereo. Requirements: 1. **Decoding Step**: Convert the given audio fragment from its encoding format (a-LAW or u-LAW) to a linear PCM format. 2. **Volume Adjustment Step**: Multiply the linear PCM audio samples by a given floating-point factor to adjust the volume. 3. **Mono to Stereo Conversion Step**: Convert the modified mono audio fragment to a stereo audio fragment. The left and right channels should be adjusted by specific factors. 4. **Encoding Step**: Encode the modified stereo audio fragment back to the original encoding format (a-LAW or u-LAW). Function Signature ```python def process_audio_fragment(encoding: str, audio_fragment: bytes, volume_factor: float, lfactor: float, rfactor: float) -> bytes: Process an audio fragment to adjust its volume and convert it from mono to stereo. Parameters: encoding (str): The encoding format of the audio fragment (\'a-LAW\' or \'u-LAW\'). audio_fragment (bytes): The audio fragment to be processed. volume_factor (float): The factor by which to adjust the volume. lfactor (float): The multiplication factor for the left channel. rfactor (float): The multiplication factor for the right channel. Returns: bytes: The processed audio fragment in the original encoding format. ``` Input: 1. A string `encoding` which can be either `\'a-LAW\'` or `\'u-LAW\'`. 2. A bytes-like object `audio_fragment` which is the audio data in the specified encoding format. 3. A float `volume_factor` which is the factor by which to adjust the volume. 4. A float `lfactor` which is the multiplication factor for the left channel in the stereo output. 5. A float `rfactor` which is the multiplication factor for the right channel in the stereo output. Output: - A bytes-like object representing the processed audio fragment in the original encoding format. Constraints: - The sample width of the linear PCM format should be 2 bytes (16 bits). - The length of the audio fragment in bytes will always be a multiple of the sample width. - The `volume_factor`, `lfactor`, and `rfactor` will be positive floating-point numbers. Example: ```python encoding = \'a-LAW\' audio_fragment = b\'xd5x00xd1x00xccx00xc8x00xc5x00\' volume_factor = 1.5 lfactor = 0.8 rfactor = 1.2 result = process_audio_fragment(encoding, audio_fragment, volume_factor, lfactor, rfactor) ``` Here, your implementation should: 1. Decode the `audio_fragment` from a-LAW to linear PCM. 2. Adjust the volume of the audio by multiplying each sample by `1.5`. 3. Convert the modified audio from mono to stereo, adjusting the left channel by `0.8` and the right channel by `1.2`. 4. Encode the resulting stereo audio fragment back to a-LAW encoding. Provide a clear, efficient, and correctly implemented solution using the functions provided by the `audioop` module.","solution":"import audioop def process_audio_fragment(encoding: str, audio_fragment: bytes, volume_factor: float, lfactor: float, rfactor: float) -> bytes: Process an audio fragment to adjust its volume and convert it from mono to stereo. Parameters: encoding (str): The encoding format of the audio fragment (\'a-LAW\' or \'u-LAW\'). audio_fragment (bytes): The audio fragment to be processed. volume_factor (float): The factor by which to adjust the volume. lfactor (float): The multiplication factor for the left channel. rfactor (float): The multiplication factor for the right channel. Returns: bytes: The processed audio fragment in the original encoding format. # Decode the audio fragment if encoding == \'a-LAW\': linear_audio_fragment = audioop.alaw2lin(audio_fragment, 2) elif encoding == \'u-LAW\': linear_audio_fragment = audioop.ulaw2lin(audio_fragment, 2) else: raise ValueError(\\"Unsupported encoding format. Please use \'a-LAW\' or \'u-LAW\'.\\") # Adjust the volume adjusted_audio_fragment = audioop.mul(linear_audio_fragment, 2, volume_factor) # Convert mono to stereo stereo_audio_fragment = b\'\' for i in range(0, len(adjusted_audio_fragment), 2): mono_sample = adjusted_audio_fragment[i:i+2] left_sample = audioop.mul(mono_sample, 2, lfactor) right_sample = audioop.mul(mono_sample, 2, rfactor) stereo_audio_fragment += left_sample + right_sample # Encode back to the original format if encoding == \'a-LAW\': processed_audio_fragment = audioop.lin2alaw(stereo_audio_fragment, 2) elif encoding == \'u-LAW\': processed_audio_fragment = audioop.lin2ulaw(stereo_audio_fragment, 2) return processed_audio_fragment"},{"question":"**Objective**: Demonstrate your understanding of Python\'s runtime services, specifically focusing on `dataclasses` and `contextlib`. You will need to create a robust context-driven data management system using these modules. **Problem Statement**: You are tasked with implementing a data management system that tracks the execution of tasks within a context. This system should log task start and end times, handle any exceptions that occur during task execution, and use the efficiency of Python\'s `dataclasses` for storing task metadata. **Requirements**: 1. **DataClass Definition**: - Define a `Task` data class with the following properties: - `id` (int): A unique identifier for the task. - `name` (str): A descriptive name of the task. - `start_time` (float): The start time of the task. - `end_time` (float): The end time of the task. - `status` (str): Status of the task (`\'Success\'` or `\'Failure\'`). 2. **Context Manager**: - Implement a context manager class `TaskManager` using `contextlib` that: - Logs the start time when entering the context. - Logs the end time upon exiting the context. - Handles any exceptions, marking the task status as `\'Failure\'` if an exception occurs or `\'Success\'` if no exceptions occur. 3. **Function**: - Write a function `perform_task(task_id, task_name, task_func, *args, **kwargs)` that: - Accepts a unique `task_id`, a `task_name`, a task function (`task_func`), and any additional arguments (`*args`, `**kwargs`). - Uses the `TaskManager` context manager to execute `task_func` with the provided arguments. - Returns an instance of `Task` populated with the appropriate metadata. **Constraints**: - You can assume `task_id` is always unique for each task. - The tasks executed could potentially fail (raise exceptions). - Utilize the standard library modules only. **Input**: - A `task_id` (integer) which is a unique identifier. - A `task_name` (string) which gives a descriptive name. - A function `task_func` which specifies the task to be performed. - Any number of additional arguments and keyword arguments which are to be passed to the `task_func`. **Output**: - An instance of `Task` with all fields correctly populated. **Example**: ```python from time import sleep def sample_task(duration): sleep(duration) task = perform_task(1, \\"SleepTask\\", sample_task, 2) print(task) ``` In this example, the `sample_task` function sleeps for a given duration. The `perform_task` function should return a `Task` object which includes: - `id`: 1 - `name`: \\"SleepTask\\" - `start_time`: [start time float] - `end_time`: [end time float] - `status`: \\"Success\\" (assuming no exceptions in `sample_task`) **Note**: Implementations that do not leverage `dataclasses` and `contextlib` will not fulfill the requirements adequately.","solution":"from dataclasses import dataclass from contextlib import contextmanager import time @dataclass class Task: id: int name: str start_time: float end_time: float status: str @contextmanager def TaskManager(task): try: task.start_time = time.time() yield task.status = \'Success\' except Exception as e: task.status = \'Failure\' raise e finally: task.end_time = time.time() def perform_task(task_id, task_name, task_func, *args, **kwargs): task = Task(id=task_id, name=task_name, start_time=0, end_time=0, status=\'Pending\') try: with TaskManager(task): task_func(*args, **kwargs) except Exception as e: pass # Exception handled by context manager, no need to re-raise return task"},{"question":"You are tasked with automating the creation of source distributions for Python projects using the principles described in the documentation about the `sdist` command and manifests. Create a Python script function that accomplishes the following: 1. Scans a given project directory for Python files and other specified file types. 2. Generates a `MANIFEST.in` file specifying which files to include in the source distribution. 3. Executes the `sdist` command to create a source distribution in the specified formats. # Function Signature: ```python def create_source_distribution(project_dir: str, formats: List[str], include_patterns: List[str]) -> None: pass ``` # Parameters: - `project_dir` (str): The path to the root directory of the Python project to be packaged. - `formats` (List[str]): A list of strings specifying the formats for the distribution (e.g., `[\'gztar\', \'zip\']`). - `include_patterns` (List[str]): A list of strings specifying file patterns to include in the `MANIFEST.in` file (e.g., `[\'*.py\', \'*.md\']`). # Constraints: - You cannot assume the presence of a pre-existing `MANIFEST.in` or `MANIFEST` file. - The project directory will contain typical Python project files and directories. - The script should handle cases where directories contain no matching files gracefully. # Example Usage: ```python create_source_distribution(\'path/to/project\', [\'gztar\', \'zip\'], [\'*.py\', \'*.md\', \'README.*\']) ``` # Execution: The function should: 1. Generate a `MANIFEST.in` file in the project directory with the specified include patterns. 2. Run the `python setup.py sdist --formats=<comma-separated formats>` command to create the distribution files in the specified formats. # Performance: - The function should be efficient in scanning the directories and creating the manifest file. Implement the `create_source_distribution` function to fulfill the requirements described.","solution":"import os from typing import List def create_source_distribution(project_dir: str, formats: List[str], include_patterns: List[str]) -> None: Creates a source distribution for the given Python project. Parameters: - project_dir: The path to the root directory of the Python project to be packaged. - formats: A list of strings specifying the formats for the distribution (e.g., [\'gztar\', \'zip\']). - include_patterns: A list of strings specifying file patterns to include in the MANIFEST.in file (e.g., [\'*.py\', \'*.md\']). manifest_path = os.path.join(project_dir, \'MANIFEST.in\') # Write the MANIFEST.in file with open(manifest_path, \'w\') as manifest_file: for pattern in include_patterns: manifest_file.write(f\'include {pattern}n\') # Create the source distribution format_string = \',\'.join(formats) command = f\'python setup.py sdist --formats={format_string}\' # Change the current working directory to the project directory original_cwd = os.getcwd() try: os.chdir(project_dir) os.system(command) finally: # Revert to the original working directory os.chdir(original_cwd)"},{"question":"**Coding Assessment Question: Advanced Python Initialization** In this task, you will implement a function that initializes Python with customized configurations using the `PyConfig` and `PyPreConfig` structures. This function will: 1. Preinitialize Python using the isolated configuration. 2. Initialize Python from a custom configuration. 3. Set specific command-line arguments. 4. Handle any exceptions (errors or exit) during initialization and exit gracefully. # Function Signature: ```python def init_custom_python(argc: int, argv: list): pass ``` # Input: - `argc`: An integer representing the number of command-line arguments. - `argv`: A list of command-line arguments (strings). # Function Requirements: 1. Preinitialize Python using `PyPreConfig` in isolated mode. 2. Initialize Python using `PyConfig`, set in isolated mode. 3. Use the `PyConfig_SetBytesArgv()` function to set the command-line arguments. 4. Handle exceptions using `PyStatus_Exception()` and `Py_ExitStatusException()`. 5. Clear the configuration memory after initialization. # Example Usage: ```python argc = 3 argv = [\\"program_name\\", \\"-c\\", \\"print(\'Hello World!\')\\"] init_custom_python(argc, argv) ``` # Constraints: - You can assume that the input arguments are valid and properly formatted. - You must use the methods and structures as described in the documentation. # Performance Requirements: - The function should be efficient in terms of both time and memory. - Ensure no memory leaks and proper handling of all allocated resources. This question tests your ability to work with the Python C API structures and functions related to initializing Python, handling custom configurations, and carefully managing resources and errors.","solution":"import sys import ctypes # Define placeholders for PyConfig and PyPreConfig analogous to what would be used in the C API. class PyPreConfig(ctypes.Structure): _fields_ = [(\'isolated\', ctypes.c_int)] class PyConfig(ctypes.Structure): _fields_ = [(\'isolated\', ctypes.c_int)] # Define placeholder functions to simulate the Python C API functions def PyPreConfig_InitIsolated(preconfig): preconfig.isolated = 1 def PyConfig_InitIsolated(config): config.isolated = 1 def PyConfig_SetBytesArgv(config, argc, argv): pass # This is a placeholder def Py_InitializeFromConfig(config): pass # This is a placeholder def PyConfig_Clear(config): pass # This is a placeholder def PyPreConfig_Clear(preconfig): pass # This is a placeholder def PyStatus_Exception(status): return False def Py_ExitStatusException(status): sys.exit(1) def init_custom_python(argc: int, argv: list): preconfig = PyPreConfig() PyPreConfig_InitIsolated(preconfig) config = PyConfig() PyConfig_InitIsolated(config) PyConfig_SetBytesArgv(config, argc, argv) try: Py_InitializeFromConfig(config) except Exception as e: if PyStatus_Exception(e): Py_ExitStatusException(e) finally: PyConfig_Clear(config) PyPreConfig_Clear(preconfig)"},{"question":"**Objective:** Implement a function to safely format a number as a string, ensuring proper error handling and precision according to the given specifications. # Problem Statement You are tasked with writing a function that takes a floating-point number and converts it into a formatted string based on certain rules. Specifically, you will ensure that the formatting adheres to safety and precision constraints and handles any potential errors gracefully. Implement the function `safe_format_number` according to the following requirements: Function Signature ```python def safe_format_number(value: float, format_code: str, precision: int) -> str: pass ``` Parameters - `value` (float): The floating-point number to convert. - `format_code` (str): A single character string, one of `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, or `\'r\'`, specifying the formatting code. - `precision` (int): An integer specifying the precision for the conversion. For `\'r\'`, this should be 0 and is ignored. Returns - (str): The formatted string representation of the number. Requirements 1. The function should use `PyOS_double_to_string` for the actual conversion. 2. Ensure that the formatted string adheres to the following flags: - Always includes a sign character (`Py_DTSF_SIGN`). - Ensures the returned string does not look like an integer for float formatting (`Py_DTSF_ADD_DOT_0`). 3. Handle cases where the value might be infinite or NaN correctly by setting the appropriate type. 4. Ensure proper memory management when dealing with the returned string from the conversion. Constraints - You can assume the `format_code` is always valid and one of the specified characters. - The precision is a non-negative integer. For `\'r\'`, it must be 0. - Handle potential errors gracefully without crashing. Example Usage ```python print(safe_format_number(1234.5678, \'f\', 2)) # \\"+1234.57\\" print(safe_format_number(1.23e10, \'E\', 3)) # \\"+1.230E+10\\" print(safe_format_number(float(\'inf\'), \'g\', 5)) # \\"+inf\\" ``` **Note:** Underlying C library functions (such as `PyOS_double_to_string`) are assumed to have Python bindings for the purposes of this question. In a real implementation, you\'d need such bindings or equivalent Python code. # Guidance Use the functions and examples from the provided `python310` documentation to handle the formatting and conversion. Pay particular attention to handling edge cases, such as overflows or invalid formats, and ensure the final string meets all specified formatting requirements.","solution":"import math def safe_format_number(value: float, format_code: str, precision: int) -> str: format_spec = f\\".{precision}{format_code}\\" if math.isnan(value): return \\"+nan\\" if math.isinf(value): if value > 0: return \\"+inf\\" else: return \\"-inf\\" if format_code == \'r\': return f\\"{value:+.0f}\\" formatted_str = f\\"{value:{format_spec}}\\" if \'.\' not in formatted_str: formatted_str += \\".0\\" if value >= 0: formatted_str = \'+\' + formatted_str return formatted_str"},{"question":"**XML Parsing, Modification, and Serialization Task** You are given an XML document that contains inventory data for a bookstore. Your task is to write a Python function to perform the following operations using the \\"xml.etree.ElementTree\\" module: 1. Parse the provided XML data. 2. Increase the price of every book by 10%. 3. Add a new `<category>` element with the contents \\"Literature\\" to each book that is currently missing a category. 4. Write the modified XML structure to a new XML file `modified_inventory.xml`. The function signature should be: ```python def update_bookstore_inventory(xml_data: str) -> None: ``` # Input: - `xml_data`: A string containing the XML data of the bookstore inventory. # Constraints: - Each book element contains a `<price>` element, and may or may not contain a `<category>` element. - The XML structure is well-formed. # Expected Output: - A new XML file named `modified_inventory.xml` containing the updated XML structure. # Example: Given the following XML data as input: ```xml <bookstore> <book> <title>Programming in Python</title> <author>John Smith</author> <price>29.99</price> <category>Computers</category> </book> <book> <title>Learning XML</title> <author>Jane Doe</author> <price>39.95</price> </book> </bookstore> ``` After processing, the output file `modified_inventory.xml` should contain: ```xml <bookstore> <book> <title>Programming in Python</title> <author>John Smith</author> <price>32.99</price> <category>Computers</category> </book> <book> <title>Learning XML</title> <author>Jane Doe</author> <price>43.95</price> <category>Literature</category> </book> </bookstore> ``` # Steps: 1. Parse the input XML string using `ElementTree`. 2. Iterate through each book element and increase its price by 10%. 3. Check if a category element exists for each book element, and if not, add a new category element with the text \'Literature\'. 4. Save the modified XML structure to a file named `modified_inventory.xml`. # Notes: - Ensure to handle the XML structure properly and maintain all existing elements and attributes while making the modifications. - Utilize `ElementTree` methods effectively to perform the required operations.","solution":"import xml.etree.ElementTree as ET def update_bookstore_inventory(xml_data: str) -> None: # Parse the XML data root = ET.fromstring(xml_data) # Iterate through each book element for book in root.findall(\'book\'): # Increase the price by 10% price_element = book.find(\'price\') if price_element is not None: price = float(price_element.text) new_price = round(price * 1.10, 2) price_element.text = f\\"{new_price:.2f}\\" # Add category element if missing if book.find(\'category\') is None: category_element = ET.Element(\'category\') category_element.text = \'Literature\' book.append(category_element) # Write the modified XML structure to a new file tree = ET.ElementTree(root) tree.write(\'modified_inventory.xml\')"},{"question":"**Coding Assessment Question: Text Classification with Scikit-learn** # Objective Write a Python class that utilizes scikit-learn\'s feature extraction and machine learning capabilities to classify textual data. # Problem Statement You are tasked with creating a text classification pipeline that transforms raw textual data into numerical features and then trains a machine learning model on these features. You will use scikit-learn\'s `CountVectorizer` for the initial text vectorization, followed by `TfidfTransformer` for transforming the count matrix into a tf-idf representation. Finally, you will train a classifier on this tf-idf data. # Requirements 1. Implement a class named `TextClassifier`. 2. The class should include methods to: - **load_data**: Load raw text data and corresponding labels. - **preprocess**: Transform the raw text data into numerical features using `CountVectorizer` and `TfidfTransformer`. - **train_model**: Train a machine learning model given the preprocessed features. - **predict**: Make predictions on new text data. 3. Use a simple dataset provided as lists of text samples and corresponding labels. # Input Format - **load_data**: - Input: Two lists of strings, `texts` (text samples) and `labels` (target labels). - Example: `[\\"This is a document.\\", \\"This is another document.\\"]`, `[\\"class1\\", \\"class2\\"]` - **train_model**: - Input: None (internally uses the preprocessed features from `preprocess` method). - Example: `train_model()` - **predict**: - Input: A list of strings, `new_texts` (new text samples to classify). - Output: A list of predicted labels for the input texts. - Example: `[\\"This is a new text.\\"]` # Output Format - The `predict` method should return a list of predicted labels for the input texts. # Constraints - Use `scikit-learn` for vectorization and classification. - Implement basic error handling for methods. - Ensure the code is modular and clear. # Performance Requirements - The processing should handle a reasonable number of text samples efficiently (e.g., 10,000 samples). # Example Implementation ```python from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline class TextClassifier: def __init__(self): self.vectorizer = CountVectorizer() self.transformer = TfidfTransformer() self.model = MultinomialNB() def load_data(self, texts, labels): self.texts = texts self.labels = labels def preprocess(self): counts = self.vectorizer.fit_transform(self.texts) self.tfidf_matrix = self.transformer.fit_transform(counts) def train_model(self): self.preprocess() self.model.fit(self.tfidf_matrix, self.labels) def predict(self, new_texts): counts = self.vectorizer.transform(new_texts) tfidf_matrix = self.transformer.transform(counts) return self.model.predict(tfidf_matrix) # Example usage: # texts = [\\"This is a document.\\", \\"This is another document.\\"] # labels = [\\"class1\\", \\"class2\\"] # new_texts = [\\"This is a new text.\\"] # classifier = TextClassifier() # classifier.load_data(texts, labels) # classifier.train_model() # predictions = classifier.predict(new_texts) # print(predictions) ```","solution":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.naive_bayes import MultinomialNB class TextClassifier: def __init__(self): self.vectorizer = CountVectorizer() self.transformer = TfidfTransformer() self.model = MultinomialNB() def load_data(self, texts, labels): if not texts or not labels: raise ValueError(\\"Texts and labels must not be empty.\\") self.texts = texts self.labels = labels def preprocess(self): if not hasattr(self, \'texts\') or not hasattr(self, \'labels\'): raise AttributeError(\\"Data not loaded. Please load data before preprocessing.\\") counts = self.vectorizer.fit_transform(self.texts) self.tfidf_matrix = self.transformer.fit_transform(counts) def train_model(self): if not hasattr(self, \'tfidf_matrix\') or not hasattr(self, \'labels\'): self.preprocess() self.model.fit(self.tfidf_matrix, self.labels) def predict(self, new_texts): if not new_texts: return [] counts = self.vectorizer.transform(new_texts) tfidf_matrix = self.transformer.transform(counts) return self.model.predict(tfidf_matrix)"},{"question":"# File Organizer and Metadata Extractor **Objective:** Write a Python program that organizes files in a given directory based on their extensions and creates a metadata report for each organized file. The program should also provide functionality to set and get environment variables. **Problem Statement:** Implement a function `organize_files_and_extract_metadata(directory: str, report_file: str) -> None` which: 1. Organizes files in the specified `directory` into subdirectories based on their file extensions. - For example, all `.txt` files should be moved to a subdirectory named `txt`, all `.jpg` files to a subdirectory named `jpg`, and so on. - If a subdirectory for a file type does not exist, create it. 2. Generates a metadata report for each file in the directory and its subdirectories. The metadata should include: - The absolute path of the file. - The size of the file in bytes. - The creation, modification, and access times. 3. Writes the metadata report to a specified `report_file`. 4. Includes functionality to set and get environment variables for organizing files. Implement two additional functions: - `set_env_var(key: str, value: str) -> None`: Sets an environment variable. - `get_env_var(key: str) -> str`: Gets the value of an environment variable. - Use these functions to set and get an environment variable called `FILE_ORGANIZER_PATH` to store the path to the directory being organized. **Input:** - `directory` (str): The path to the directory containing the files to be organized. - `report_file` (str): The path to the file where the metadata report will be written. **Output:** - None: The function does not return anything. **Constraints:** - The function should handle exceptions appropriately (e.g., file not found, permission errors). - The directory should only contain files (no nested directories). - The program should work on Unix and Windows platforms. Use the `os` module to achieve the functionality. ```python import os import time def organize_files_and_extract_metadata(directory, report_file): # Your implementation here def set_env_var(key, value): os.environ[key] = value def get_env_var(key): return os.getenv(key) ``` **Example:** ```python # Set the environment variable for the directory to be organized set_env_var(\'FILE_ORGANIZER_PATH\', \'/path/to/directory\') # Get the value of the environment variable directory = get_env_var(\'FILE_ORGANIZER_PATH\') # Organize the files in the directory and generate a report organize_files_and_extract_metadata(directory, \'/path/to/report_file.txt\') ``` **Note:** - You should utilize `os` functions such as `os.listdir`, `os.makedirs`, `os.rename`, `os.stat`, `os.path.join`, `os.path.abspath`, `os.getenv`, and `os.environ` to implement the required functionality. - The `report_file` should be in plain text format with each line representing the metadata of a file.","solution":"import os from datetime import datetime def organize_files_and_extract_metadata(directory, report_file): try: files = os.listdir(directory) except FileNotFoundError: print(f\\"The directory {directory} does not exist.\\") return except PermissionError: print(f\\"Permission denied to access {directory}.\\") return metadata_list = [] for file in files: file_path = os.path.join(directory, file) if os.path.isfile(file_path): file_extension = file.split(\'.\')[-1] extension_dir = os.path.join(directory, file_extension) if not os.path.exists(extension_dir): os.makedirs(extension_dir) new_file_path = os.path.join(extension_dir, file) os.rename(file_path, new_file_path) # Extract metadata file_stat = os.stat(new_file_path) metadata = { \'path\': os.path.abspath(new_file_path), \'size\': file_stat.st_size, \'creation_time\': datetime.fromtimestamp(file_stat.st_ctime).isoformat(), \'modification_time\': datetime.fromtimestamp(file_stat.st_mtime).isoformat(), \'access_time\': datetime.fromtimestamp(file_stat.st_atime).isoformat() } metadata_list.append(metadata) # Write metadata to report file with open(report_file, \'w\') as report: for data in metadata_list: report.write(f\\"Path: {data[\'path\']}, Size: {data[\'size\']} bytes, \\" f\\"Creation Time: {data[\'creation_time\']}, \\" f\\"Modification Time: {data[\'modification_time\']}, \\" f\\"Access Time: {data[\'access_time\']}n\\") def set_env_var(key, value): os.environ[key] = value def get_env_var(key): return os.getenv(key)"},{"question":"**Programming Assessment Question: System Resource Management** **Objective:** Implement a function that manages system resource limits, retrieves usage statistics, and handles related exceptions effectively. **Problem Statement:** You are required to write a Python function `manage_resources` that performs the following tasks: 1. Sets a new CPU time limit (in seconds) for the current process. 2. Retrieves and returns the updated CPU time limit. 3. Retrieves and returns resource usage statistics for the current process. Your function should handle all potential exceptions and invalid inputs gracefully. **Function Signature:** ```python def manage_resources(cpu_time_limit: int) -> dict: pass ``` **Input:** - `cpu_time_limit` (int): The new CPU time limit (in seconds) to be set for the current process. It should be a non-negative integer. **Output:** - A dictionary containing the following keys: * `new_cpu_limits`: A tuple representing the updated (soft, hard) CPU time limits. If setting the limit fails, this should be `None`. * `resource_usage`: A nested dictionary with resource usage statistics containing the following keys and corresponding values: - `ru_utime`: Time in user mode (float seconds) - `ru_stime`: Time in system mode (float seconds) - `ru_maxrss`: Maximum resident set size - `ru_ixrss`: Shared memory size - `ru_idrss`: Unshared memory size - `ru_isrss`: Unshared stack size - `ru_minflt`: Page faults not requiring I/O - `ru_majflt`: Page faults requiring I/O - `ru_nswap`: Number of swap outs - `ru_inblock`: Block input operations - `ru_oublock`: Block output operations - `ru_msgsnd`: Messages sent - `ru_msgrcv`: Messages received - `ru_nsignals`: Signals received - `ru_nvcsw`: Voluntary context switches - `ru_nivcsw`: Involuntary context switches **Constraints:** - The function should handle invalid inputs and syscall failures by setting appropriate return values (like `None` for `new_cpu_limits`). - CPU time limit should be set only if it does not exceed the current hard limit of the process. - Handle exceptions raised during `setrlimit`, `getrlimit`, or `getrusage` calls gracefully. **Example:** ```python # Example usage result = manage_resources(10) print(result) # Expected output structure (values will vary based on the system and process state): # { # \'new_cpu_limits\': (10, current_hard_limit), # \'resource_usage\': { # \'ru_utime\': 0.1, # \'ru_stime\': 0.0, # \'ru_maxrss\': 241664, # \'ru_ixrss\': 0, # \'ru_idrss\': 0, # \'ru_isrss\': 0, # \'ru_minflt\': 202, # \'ru_majflt\': 0, # \'ru_nswap\': 0, # \'ru_inblock\': 0, # \'ru_oublock\': 0, # \'ru_msgsnd\': 0, # \'ru_msgrcv\': 0, # \'ru_nsignals\': 0, # \'ru_nvcsw\': 2, # \'ru_nivcsw\': 0 # } # } ``` **Note:** You may use the following import: ```python import resource ``` The implementation of this function will demonstrate your understanding of system resource management, exception handling, and interaction with the `resource` module.","solution":"import resource import errno def manage_resources(cpu_time_limit: int) -> dict: result = { \'new_cpu_limits\': None, \'resource_usage\': {} } try: if cpu_time_limit < 0: raise ValueError(\\"CPU time limit must be a non-negative integer.\\") # Get current limits soft, hard = resource.getrlimit(resource.RLIMIT_CPU) # Set new limit if cpu_time_limit <= hard: resource.setrlimit(resource.RLIMIT_CPU, (cpu_time_limit, hard)) result[\'new_cpu_limits\'] = (cpu_time_limit, hard) else: raise ValueError(\\"CPU time limit exceeds the current hard limit.\\") except (ValueError, resource.error) as e: # Log the error or handle it appropriately in a real-world case result[\'new_cpu_limits\'] = None try: usage = resource.getrusage(resource.RUSAGE_SELF) result[\'resource_usage\'] = { \'ru_utime\': usage.ru_utime, \'ru_stime\': usage.ru_stime, \'ru_maxrss\': usage.ru_maxrss, \'ru_ixrss\': usage.ru_ixrss, \'ru_idrss\': usage.ru_idrss, \'ru_isrss\': usage.ru_isrss, \'ru_minflt\': usage.ru_minflt, \'ru_majflt\': usage.ru_majflt, \'ru_nswap\': usage.ru_nswap, \'ru_inblock\': usage.ru_inblock, \'ru_oublock\': usage.ru_oublock, \'ru_msgsnd\': usage.ru_msgsnd, \'ru_msgrcv\': usage.ru_msgrcv, \'ru_nsignals\': usage.ru_nsignals, \'ru_nvcsw\': usage.ru_nvcsw, \'ru_nivcsw\': usage.ru_nivcsw } except resource.error as e: # Log the error or handle it appropriately in a real-world case result[\'resource_usage\'] = {} return result"},{"question":"**JSON Encoding and Decoding with Custom Handlers** # Problem Statement You are required to implement a function that processes JSON data. The function should accept JSON input as a string and should decode the JSON into corresponding Python objects. It should then process these objects to compute some statistics and finally encode the processed information back into a JSON string. Additionally, the function should be able to handle specific custom data types by implementing custom encoders and decoders. # Function Signature ```python def process_json_data(json_input: str) -> str: ``` # Input - `json_input`: A JSON-formatted string containing a list of dictionaries. Each dictionary represents a data entry and contains various key-value pairs. Some of the keys will have simple data types (integers, strings), while others may have custom data types (e.g., a datetime string). # Output - A JSON-formatted string that includes: - The original list of dictionaries. - A summary statistics dictionary containing: - The total count of all dictionary entries. - The count of unique values for each key. - The count of each custom data type occurrences. # Constraints - You must handle exceptions that arise from invalid JSON inputs gracefully and return a meaningful error message in JSON format. - Custom data types include dates in the format `\\"YYYY-MM-DD\\"`. You need to correctly parse these dates and include their counts in your statistics. # Requirements 1. Use the `json` module for encoding and decoding JSON data. 2. Implement custom encoders/decoders for the custom data types. 3. Ensure that your solution handles invalid inputs gracefully. # Example ```python # Example Input json_input = \'\'\' [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"date\\": \\"2021-01-01\\"}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"date\\": \\"2021-01-02\\"}, {\\"id\\": 3, \\"name\\": \\"Alice\\", \\"date\\": \\"2021-01-01\\"} ] \'\'\' # Example Output \'\'\' { \\"original_data\\": [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"date\\": \\"2021-01-01\\"}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"date\\": \\"2021-01-02\\"}, {\\"id\\": 3, \\"name\\": \\"Alice\\", \\"date\\": \\"2021-01-01\\"} ], \\"summary_statistics\\": { \\"total_count\\": 3, \\"unique_values\\": { \\"id\\": 3, \\"name\\": 2, \\"date\\": 2 }, \\"custom_type_counts\\": { \\"date\\": 3 } } } \'\'\' # Example Invalid Input json_input = \'\'\' [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"date\\": \\"2021-01-XX\\"} ] \'\'\' # Example Output for Invalid Input \'\'\' { \\"error\\": \\"Invalid date format in input JSON.\\" } \'\'\' ``` # Hint - You can use Python\'s `datetime` module to handle the date strings. - Define a custom encoder by subclassing `json.JSONEncoder` and overriding the `default` method. - Define a custom decoder by subclassing `json.JSONDecoder` and overriding the `object_hook` method.","solution":"import json from datetime import datetime class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, datetime): return obj.strftime(\'%Y-%m-%d\') return super().default(obj) def custom_decoder(dct): for key, value in dct.items(): try: if isinstance(value, str) and \'-\' in value: dct[key] = datetime.strptime(value, \'%Y-%m-%d\') except ValueError: raise ValueError(\\"Invalid date format in input JSON.\\") return dct def process_json_data(json_input: str) -> str: try: data = json.loads(json_input, object_hook=custom_decoder) except ValueError as e: return json.dumps({\\"error\\": str(e)}) total_count = len(data) unique_values = {} custom_type_counts = {\\"date\\": 0} for entry in data: for key, value in entry.items(): unique_values[key] = unique_values.get(key, set()) unique_values[key].add(value) if isinstance(value, datetime): custom_type_counts[\\"date\\"] += 1 stats = { \\"total_count\\": total_count, \\"unique_values\\": {k: len(v) for k, v in unique_values.items()}, \\"custom_type_counts\\": custom_type_counts } result = { \\"original_data\\": data, \\"summary_statistics\\": stats } return json.dumps(result, cls=CustomJSONEncoder)"},{"question":"You are required to parse and extract specific information from a given HTML document by creating a custom HTML parser using Python\'s `html.parser` module. # Objectives 1. Create a subclass of `html.parser.HTMLParser` that overrides necessary handler methods to process the HTML. 2. Extract and return a list of all the hyperlinks (i.e., values of `href` attributes in `<a>` tags) present in the HTML. # Input - A single string containing the HTML content. # Output - A list of strings, where each string is a hyperlink found in the HTML content. # Constraints - The HTML content could contain nested HTML elements. - The `<a>` tags may contain other attributes, but you should only extract the `href` attribute values. - Ignore any hyperlinks that are empty or only contain whitespace. # Example Input ```python html_content = \'\'\' <html> <head><title>Test</title></head> <body> <h1>Welcome to the Test</h1> <p>This is a <a href=\\"https://example.com\\">link to example</a>.</p> <p>Here is another <a href=\\"https://anotherexample.com\\">another link</a>.</p> <p>Invalid link <a href=\\"\\">empty link</a>.</p> <p>Whitespace link <a href=\\" \\">whitespace link</a>.</p> </body> </html> \'\'\' ``` Output ```python [\\"https://example.com\\", \\"https://anotherexample.com\\"] ``` # Implementation ```python from html.parser import HTMLParser class HyperlinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\' and attr[1].strip(): self.links.append(attr[1]) def get_links(self): return self.links def extract_hyperlinks(html_content): parser = HyperlinkExtractor() parser.feed(html_content) return parser.get_links() # Example Usage html_content = \'\'\' <html> <head><title>Test</title></head> <body> <h1>Welcome to the Test</h1> <p>This is a <a href=\\"https://example.com\\">link to example</a>.</p> <p>Here is another <a href=\\"https://anotherexample.com\\">another link</a>.</p> <p>Invalid link <a href=\\"\\">empty link</a>.</p> <p>Whitespace link <a href=\\" \\">whitespace link</a>.</p> </body> </html> \'\'\' print(extract_hyperlinks(html_content)) # Expected output: [\\"https://example.com\\", \\"https://anotherexample.com\\"] ``` # Notes - Utilize the `handle_starttag` method to detect `<a>` tags and extract their `href` attributes. - Ensure that hyperlinks containing only whitespace are ignored.","solution":"from html.parser import HTMLParser class HyperlinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\' and attr[1].strip(): self.links.append(attr[1]) def get_links(self): return self.links def extract_hyperlinks(html_content): parser = HyperlinkExtractor() parser.feed(html_content) return parser.get_links()"},{"question":"Objective Implement a semi-supervised learning model using the `LabelSpreading` algorithm from scikit-learn\'s `semi_supervised` module. The task is to classify handwritten digit images from the digits dataset (`sklearn.datasets.load_digits`), making use of both labeled and unlabeled data. Background The digits dataset contains 8x8 pixel images of hand-written digits. While a part of the data will have known labels, the rest will be considered unlabeled and used to improve the model via semi-supervised learning. Task 1. Load the digits dataset. 2. Randomly select 50% of the labels and set them to `-1` to simulate unlabeled data. 3. Implement the `LabelSpreading` algorithm using the RBF kernel. 4. Train the model with this modified dataset. 5. Evaluate the model on the original test set where all labels are known. Specifications - **Input**: None (The function should implement the solution without input arguments) - **Output**: Print the classification report which includes precision, recall, and F1-score (use `classification_report` from `sklearn.metrics`) Constraints - Use `LabelSpreading` with the RBF kernel. - Use `gamma=0.25` for the RBF kernel. Performance Requirements - Ensure that the implementation efficiently handles the dataset and the semi-supervised learning process. Here\'s the skeleton of the solution: ```python import numpy as np from sklearn import datasets from sklearn.semi_supervised import LabelSpreading from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split def semi_supervised_digit_classification(): # Load the digits dataset digits = datasets.load_digits() X = digits.data y = digits.target # Split the dataset into a training set and a test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y, random_state=42) # Randomly set 50% of the labels in the training set to -1 to simulate unlabeled data rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(y_train.shape[0]) < 0.5 y_train[random_unlabeled_points] = -1 # Initialize the LabelSpreading model with the RBF kernel and gamma=0.25 label_spread = LabelSpreading(kernel=\'rbf\', gamma=0.25) # Fit the model on the training data label_spread.fit(X_train, y_train) # Predict on the test data y_pred = label_spread.predict(X_test) # Print the classification report print(classification_report(y_test, y_pred)) # Call the function to run the semi-supervised classification task semi_supervised_digit_classification() ```","solution":"import numpy as np from sklearn import datasets from sklearn.semi_supervised import LabelSpreading from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split def semi_supervised_digit_classification(): # Load the digits dataset digits = datasets.load_digits() X = digits.data y = digits.target # Split the dataset into a training set and a test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y, random_state=42) # Randomly set 50% of the labels in the training set to -1 to simulate unlabeled data rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(y_train.shape[0]) < 0.5 y_train[random_unlabeled_points] = -1 # Initialize the LabelSpreading model with the RBF kernel and gamma=0.25 label_spread = LabelSpreading(kernel=\'rbf\', gamma=0.25) # Fit the model on the training data label_spread.fit(X_train, y_train) # Predict on the test data y_pred = label_spread.predict(X_test) # Print the classification report print(classification_report(y_test, y_pred)) # Call the function to run the semi-supervised classification task semi_supervised_digit_classification()"},{"question":"**Advanced Python Coding Assessment: Profile and Trace Python Function Executions** To assess your understanding of advanced tracing and profiling concepts in Python, you are required to implement a Python-based tracing utility that mimics some of the functionalities mentioned in the provided documentation. # Task: You need to implement a Python function `trace_function_calls` that takes another function as an argument, along with its arguments, and executes this function while logging its call and return execution in a structured format. The logs should include the timestamp, function name, and line number for each call and return event. This mimics a simple form of the tracing functionalities of DTrace/SystemTap markers described in the documentation. # Specifications: - The `trace_function_calls` function should accept a target function and its arguments. - It should log the call and return events for the target function and any functions it calls. - The logs should be printed in the following format: ``` [TIMESTAMP] CALL: <function_name>:<line_number> [TIMESTAMP] RETURN: <function_name>:<line_number> ``` - The timestamp should be in microseconds precision. # Input: 1. A function to trace. 2. The arguments to pass to this function. # Output: - Print logs of call and return events as specified above. # Example Usage: ```python import time def example_function(x): def nested_function(y): return y * 2 result = nested_function(x) + nested_function(x + 1) return result trace_function_calls(example_function, 5) ``` Expected Output (the actual timestamps will vary): ``` [1622540123456] CALL: example_function:3 [1622540123457] CALL: nested_function:4 [1622540123458] RETURN: nested_function:5 [1622540123459] CALL: nested_function:4 [1622540123460] RETURN: nested_function:5 [1622540123461] RETURN: example_function:6 ``` # Constraints: - You can assume the target function and its arguments are valid. - Focus on capturing function calls and returns within the scope of the provided function. # Performance Requirements: - The solution should have minimal overhead in terms of performance. While your tracing function will add some delay due to logging, it should still run efficiently. # Hints: - You can use the `sys.setprofile` or `sys.settrace` functions to trace function calls and execution in Python. - Use the `time.time()` or `time.perf_counter()` functions to capture high precision timestamps. Implement the function `trace_function_calls` that complies with the above requirements.","solution":"import time import sys import functools def trace_function_calls(func, *args, **kwargs): Traces the calls and returns of the given function and logs them with timestamps. :param func: The function to trace. :param args: The arguments to pass to the function. :param kwargs: The keyword arguments to pass to the function. :return: The return value of the function. def trace_calls(frame, event, arg): # Capture the current timestamp in microseconds timestamp = int(time.time() * 1e6) if event == \'call\': print(f\\"[{timestamp}] CALL: {frame.f_code.co_name}:{frame.f_lineno}\\") elif event == \'return\': print(f\\"[{timestamp}] RETURN: {frame.f_code.co_name}:{frame.f_lineno}\\") return trace_calls sys.setprofile(trace_calls) result = func(*args, **kwargs) sys.setprofile(None) return result"},{"question":"**Coding Assessment: Process Management and File Operations with `os` module** # Objective Demonstrate proficiency with Python\'s `os` module by implementing functions that manage processes, handle file operations, and interact with environment variables. # Problem Statement You are required to write a Python script that performs the following tasks: 1. **Create and Manage Directories**: Write a function `create_directory_tree(root_path, directories)` that takes a root path and a list of directory names and creates a tree structure under the given root. If any directory already exists, it should be skipped without raising an error. **Function Signature**: ```python def create_directory_tree(root_path: str, directories: list) -> None: ``` **Arguments**: - `root_path`: the root directory under which the new directories should be created. - `directories`: a list of strings representing the names of directories to be created under the `root_path`. 2. **Launch a Subprocess**: Write a function `launch_subprocess(command, args)` that launches a new process using the given command and arguments. The function should print the PID of the new process and wait for it to complete, printing its exit status. **Function Signature**: ```python def launch_subprocess(command: str, args: list) -> None: ``` **Arguments**: - `command`: the command to be executed. - `args`: a list of arguments for the command. 3. **Secure Random Byte Generation**: Implement a function `generate_secure_random_bytes(n)` that returns `n` random bytes suitable for cryptographic use. **Function Signature**: ```python def generate_secure_random_bytes(n: int) -> bytes: ``` **Arguments**: - `n`: the number of random bytes to generate. 4. **Environment Variable Management**: Write a function `echo_environment_variable(var_name)` that prints the value of the environment variable named `var_name`. If the variable does not exist, print an appropriate message. **Function Signature**: ```python def echo_environment_variable(var_name: str) -> None: ``` **Arguments**: - `var_name`: the name of the environment variable to be queried. # Example Usage ```python if __name__ == \\"__main__\\": # Example usage of create_directory_tree create_directory_tree(\'/tmp/example\', [\'dir1\', \'dir2/subdir\']) # Example usage of launch_subprocess launch_subprocess(\'ls\', [\'-l\', \'/\']) # Example usage of generate_secure_random_bytes random_bytes = generate_secure_random_bytes(16) print(f\\"Random bytes: {random_bytes}\\") # Example usage of echo_environment_variable echo_environment_variable(\'HOME\') echo_environment_variable(\'NON_EXISTENT_VAR\') ``` # Constraints - Assume that the script will be run on a Unix-like system. - Do not use any external libraries for these tasks (only the `os` module and standard Python libraries). # Submission Requirements Submit the Python script with the implemented functions. Ensure that the code is well-documented and includes necessary error handling for robustness.","solution":"import os import subprocess import secrets def create_directory_tree(root_path: str, directories: list) -> None: Creates a tree structure of directories under the given root path. Args: - root_path: the root directory under which the new directories should be created. - directories: a list of strings representing the names of directories to be created under the root_path. for directory in directories: dir_path = os.path.join(root_path, directory) try: os.makedirs(dir_path, exist_ok=True) print(f\\"Created directory: {dir_path}\\") except OSError as e: print(f\\"Error creating directory {dir_path}: {e}\\") def launch_subprocess(command: str, args: list) -> None: Launches a new process using the given command and arguments, prints the PID of the process and waits for it to complete, printing its exit status. Args: - command: the command to be executed. - args: a list of arguments for the command. try: process = subprocess.Popen([command] + args) print(f\\"Process PID: {process.pid}\\") process.wait() print(f\\"Process exited with status {process.returncode}\\") except Exception as e: print(f\\"Error launching subprocess: {e}\\") def generate_secure_random_bytes(n: int) -> bytes: Returns n random bytes suitable for cryptographic use. Args: - n: the number of random bytes to generate. return secrets.token_bytes(n) def echo_environment_variable(var_name: str) -> None: Prints the value of the environment variable named var_name. If the variable does not exist, prints an appropriate message. Args: - var_name: the name of the environment variable to be queried. value = os.getenv(var_name) if value is not None: print(f\\"{var_name}: {value}\\") else: print(f\\"Environment variable \'{var_name}\' does not exist.\\")"},{"question":"Objective Write a Python function `monitor_object_lifecycle` that demonstrates the following: 1. Create a weak reference to an object with a callback function that logs when the object is garbage collected. 2. Create a weak proxy for an object. 3. Retrieve the object from both the weak reference and weak proxy. 4. Demonstrate the clearing of weak references when the object is about to be garbage collected. Specifications 1. **Function Signature:** ```python def monitor_object_lifecycle(): # Your code here ``` 2. **Implementation Details:** - Create a custom class `TrackableObject` with an `__init__` method that initializes an attribute called `name`. - Implement a function `on_object_collect(weak_ref)` that logs (prints) a message when called, indicating that the object has been collected. - Create an instance of `TrackableObject`. - Create a weak reference to this instance using `weakref.ref()`, with `on_object_collect` as the callback function. - Create a weak proxy to this instance using `weakref.proxy()`. - Print the object\'s `name` attribute using the weak reference and proxy. - Explicitly delete the instance and force garbage collection using `gc.collect()`. - Verify if the callback `on_object_collect` was called and the weak reference indicates that the object has been collected. 3. **Constraints:** - You must use Python\'s built-in `weakref` module for creating weak references and proxies. - The callback must be invoked during garbage collection. 4. **Performance Requirements:** - Ensure the program runs efficiently without unnecessary memory overhead or delays. Expected Output ```plaintext Object name: TestObject Object (proxy) name: TestObject Object has been collected. ``` The printed output should demonstrate: 1. The object\'s `name` attribute accessed via the weak reference and proxy before the object is deleted. 2. The callback indicating the object has been collected. You can import required modules (such as `inspect`, `gc`, or `weakref`) to aid with the implementation. Example Code Usage ```python import weakref import gc def monitor_object_lifecycle(): # Define the class class TrackableObject: def __init__(self, name): self.name = name # Define the callback function def on_object_collect(weak_ref): print(\\"Object has been collected.\\") # Create an instance of TrackableObject obj = TrackableObject(\\"TestObject\\") # Create a weak reference with a callback weak_ref = weakref.ref(obj, on_object_collect) # Create a weak proxy weak_proxy = weakref.proxy(obj) # Access the object\'s name via weak reference and proxy print(\\"Object name:\\", weak_ref().name) print(\\"Object (proxy) name:\\", weak_proxy.name) # Delete the original object and force garbage collection del obj gc.collect() # Call the function to run the demonstration monitor_object_lifecycle() ``` This example should help showcase the usage of weak references, proxies, and callbacks in Python.","solution":"import weakref import gc def monitor_object_lifecycle(): # Define the class class TrackableObject: def __init__(self, name): self.name = name # Define the callback function def on_object_collect(weak_ref): print(\\"Object has been collected.\\") # Create an instance of TrackableObject obj = TrackableObject(\\"TestObject\\") # Create a weak reference with a callback weak_ref = weakref.ref(obj, on_object_collect) # Create a weak proxy weak_proxy = weakref.proxy(obj) # Access the object\'s name via weak reference and proxy print(\\"Object name:\\", weak_ref().name) print(\\"Object (proxy) name:\\", weak_proxy.name) # Delete the original object and force garbage collection del obj gc.collect() # Call the function to run the demonstration monitor_object_lifecycle()"},{"question":"You are provided with a dataset of sales transactions in a CSV file named `sales.csv`. The dataset contains information about sales made by various employees on different dates. Here are the columns in the dataset: - `TransactionID`: Unique identifier for each transaction. - `EmployeeID`: Unique identifier for an employee. - `Date`: Date of the transaction. - `Amount`: Sales amount for the transaction. Your task is to perform the following data manipulations and analyses using pandas: 1. **Load Data**: Load the `sales.csv` file into a DataFrame. 2. **Data Cleaning**: Handle missing values in the `Amount` column by filling them with the mean value of the `Amount` column. 3. **Monthly Sales Aggregation**: Create a new DataFrame that shows the total sales amount per employee for each month. - The new DataFrame should have a MultiIndex with `EmployeeID` and `Month` (in the format `YYYY-MM`). - The columns should include: `TotalSales`, `AverageSales`, `TransactionCount`. 4. **Highest Sales Day**: Create a new DataFrame that shows the day on which each employee made their highest sales amount. - The new DataFrame should have columns: `EmployeeID`, `Date`, `HighestSalesAmount`. 5. **String Manipulations**: Add a new column `Employee_Code` which is derived from `EmployeeID` by prefixing it with \\"EMP_\\" and converting it to uppercase. 6. **Summary Statistics**: Create a summary DataFrame showing the following statistics for each employee: - Total Sales Amount (`TotalSales`) - Average Sales Amount per Transaction (`AvgSalesPerTransaction`) - Total Number of Transactions (`TransactionCount`) - Standard Deviation of Sales Amount (`SalesStdDev`) - Total Number of Days with Sales (`SalesDaysCount`) **Constraints and Requirements:** - Your solution should be efficient, leveraging pandas functionalities effectively. - The `Monthly Sales Aggregation` and `Summary Statistics` should be robust to changes in the dataset size and should not assume the order of rows in the original dataset. - The code should be written in a clear and modular fashion, using functions where appropriate. **Input Format:** - `sales.csv` file with columns: `TransactionID`, `EmployeeID`, `Date`, `Amount`. **Output Format:** - DataFrames as specified in steps 2 to 6, printed to the console. **Performance Requirements:** - Handle datasets with up to 1 million rows efficiently. - Operations should avoid unnecessary loops and take advantage of pandas vectorized operations. You can assume that the input data is well-formed and that any missing values only appear in the `Amount` column. --- **Example Usage:** ```python import pandas as pd # Step 1: Load Data df = pd.read_csv(\'sales.csv\') # Step 2: Data Cleaning # Fill missing values in \'Amount\' with the mean # Step 3: Monthly Sales Aggregation # Group by EmployeeID and Month, calculate total sales, average sales, and transaction count # Step 4: Highest Sales Day # Find the day where each employee had the highest sales # Step 5: String Manipulations # Add a new column \'Employee_Code\' # Step 6: Summary Statistics # Create a summary DataFrame with the specified statistics print(df_cleaned) print(monthly_sales) print(highest_sales_day) print(df_with_codes) print(summary_statistics) ``` Ensure that your code is modular, with each step encapsulated in functions.","solution":"import pandas as pd def load_data(filename): Loads the sales data from a CSV file. Parameters: filename (str): The filename of the CSV file. Returns: pd.DataFrame: The loaded DataFrame. return pd.read_csv(filename) def clean_data(df): Cleans the data by filling missing values in the Amount column with the mean value. Parameters: df (pd.DataFrame): The DataFrame with sales data. Returns: pd.DataFrame: The cleaned DataFrame. df[\'Amount\'].fillna(df[\'Amount\'].mean(), inplace=True) return df def monthly_sales_aggregation(df): Aggregates monthly sales per employee. Parameters: df (pd.DataFrame): The DataFrame with sales data. Returns: pd.DataFrame: The monthly sales aggregated DataFrame. df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') group = df.groupby([\'EmployeeID\', \'Month\']).agg( TotalSales=pd.NamedAgg(column=\'Amount\', aggfunc=\'sum\'), AverageSales=pd.NamedAgg(column=\'Amount\', aggfunc=\'mean\'), TransactionCount=pd.NamedAgg(column=\'TransactionID\', aggfunc=\'count\') ).reset_index() return group def highest_sales_day(df): Finds the day on which each employee had their highest sales amount. Parameters: df (pd.DataFrame): The DataFrame with sales data. Returns: pd.DataFrame: The DataFrame with highest sales day info. highest_sales = df.groupby([\'EmployeeID\', \'Date\']).agg( DailySales=pd.NamedAgg(column=\'Amount\', aggfunc=\'sum\') ).reset_index() idx = highest_sales.groupby(\'EmployeeID\')[\'DailySales\'].idxmax() return highest_sales.loc[idx, [\'EmployeeID\', \'Date\', \'DailySales\']].rename(columns={\\"DailySales\\": \\"HighestSalesAmount\\"}) def add_employee_code(df): Adds a new column `Employee_Code` derived from `EmployeeID`. Parameters: df (pd.DataFrame): The DataFrame with sales data. Returns: pd.DataFrame: The DataFrame with added Employee_Code. df[\'Employee_Code\'] = \'EMP_\' + df[\'EmployeeID\'].astype(str).str.upper() return df def summary_statistics(df): Creates a summary DataFrame showing statistics for each employee. Parameters: df (pd.DataFrame): The DataFrame with sales data. Returns: pd.DataFrame: The summary DataFrame. summary = df.groupby(\'EmployeeID\').agg( TotalSales=pd.NamedAgg(column=\'Amount\', aggfunc=\'sum\'), AvgSalesPerTransaction=pd.NamedAgg(column=\'Amount\', aggfunc=\'mean\'), TransactionCount=pd.NamedAgg(column=\'TransactionID\', aggfunc=\'count\'), SalesStdDev=pd.NamedAgg(column=\'Amount\', aggfunc=\'std\'), SalesDaysCount=pd.NamedAgg(column=\'Date\', aggfunc=lambda x: len(x.unique())) ).reset_index() return summary # Main function to execute the workflow def main(filename): df = load_data(filename) df = clean_data(df) monthly_sales = monthly_sales_aggregation(df) highest_sales = highest_sales_day(df) df = add_employee_code(df) summary = summary_statistics(df) # Print results for demonstration print(df.head()) print(monthly_sales.head()) print(highest_sales.head()) print(df[[\'EmployeeID\', \'Employee_Code\']].head()) print(summary.head()) # Example usage: # main(\'sales.csv\')"},{"question":"You are given a class called `DataProcessor` which takes an external service dependency to fetch and process data. Your task is to write unit tests for the `DataProcessor` class utilizing the `unittest.mock` package to mock the external service. This will involve mocking method calls, setting return values, and tracking calls to ensure the `DataProcessor` functions correctly. Class Definition ```python class ExternalService: def get_data(self, data_id): # Imagine this method reaches out to an external service pass class DataProcessor: def __init__(self, service): self.service = service def process(self, data_id): data = self.service.get_data(data_id) return self._transform(data) def _transform(self, data): # Some complex transformation logic return data.lower() if isinstance(data, str) else str(data).lower() ``` Your Tasks 1. Write a test case to verify that `process` method calls the `get_data` method on `ExternalService` with the correct arguments. 2. Write a test case to verify the `process` method correctly transforms the data returned by `get_data`. 3. Write a test case that uses a side effect to simulate different responses from the `get_data` method for different inputs and verifies the behavior of the `process` method. Expected Input and Output 1. **Input**: Single argument `data_id` which can be a string, integer, or any identifier used for fetching data. 2. **Output**: The transformed string from the `process` method. Constraints - You must use `unittest.mock` for mocking and assertions. - You must ensure that the external `get_data` method on `ExternalService` can be replaced with a mock and tracked for its interactions. Performance Requirements - The tests must complete quickly as they are unit tests intended to run frequently during development. # Example Test Implementation Create a file named `test_data_processor.py` and implement the following tests using the `unittest` framework and `unittest.mock`. ```python import unittest from unittest.mock import Mock, patch from your_module import DataProcessor, ExternalService class TestDataProcessor(unittest.TestCase): def test_process_calls_service_get_data(self): mock_service = Mock(spec=ExternalService) processor = DataProcessor(mock_service) data_id = \'123\' processor.process(data_id) mock_service.get_data.assert_called_once_with(data_id) def test_process_transforms_data_correctly(self): mock_service = Mock(spec=ExternalService) mock_service.get_data.return_value = \'SOME DATA\' processor = DataProcessor(mock_service) result = processor.process(\'123\') self.assertEqual(result, \'some data\') def test_process_with_side_effects(self): mock_service = Mock(spec=ExternalService) mock_service.get_data.side_effect = lambda x: f\\"DATA_{x}\\" if x.isdigit() else \\"OTHER_DATA\\" processor = DataProcessor(mock_service) for data_id, expected in [(\'123\', \'data_123\'), (\'abc\', \'other_data\')]: with self.subTest(data_id=data_id): result = processor.process(data_id) self.assertEqual(result, expected) if __name__ == \'__main__\': unittest.main() ``` Save your changes and run the tests. Ensure all tests pass to confirm your implementation.","solution":"from unittest.mock import Mock class ExternalService: def get_data(self, data_id): # Imagine this method reaches out to an external service pass class DataProcessor: def __init__(self, service): self.service = service def process(self, data_id): data = self.service.get_data(data_id) return self._transform(data) def _transform(self, data): # Some complex transformation logic return data.lower() if isinstance(data, str) else str(data).lower()"},{"question":"**Problem Statement:** You are tasked with writing a Python function using the `webbrowser` module to open a specified URL in different browsers depending on the operating system. The function should have the following behavior: 1. On Unix systems, it should attempt to open the URL in the `firefox` browser. If `firefox` is not available, it should fall back to `lynx`, a text-based browser. 2. On Windows systems, it should open the URL using the system\'s default browser. 3. On macOS systems, it should open the URL using the `safari` browser. Your function should be named `custom_open_url(url)`. It should accept a single parameter: - `url` (string): The URL to be opened. The function should handle exceptions gracefully and print an appropriate message if the URL cannot be opened for any reason. **Function Signature:** ```python def custom_open_url(url: str) -> None: pass ``` **Constraints:** - The function should handle scenarios where specific browsers are not available. - You can assume that the operating system can be identified using `os.name` or `platform.system()`. **Example Usage:** ```python custom_open_url(\\"https://www.python.org\\") ``` Upon calling this function: - On a Unix system with `firefox` installed, the Python website should open in a `firefox` browser. If `firefox` is not installed, it should open in `lynx`. - On a Windows system, the Python website should open in the default web browser. - On a macOS system, the Python website should open in `safari`. **Note:** Make sure to handle any potential errors, such as the browser or URL not being available or accessible, and print appropriate messages to inform the user.","solution":"import webbrowser import os import platform def custom_open_url(url: str) -> None: Opens the specified URL in a browser depending on the operating system. - On Unix systems, it attempts to open the URL in \'firefox\'. If not available, falls back to \'lynx\'. - On Windows systems, it opens the URL using the system\'s default browser. - On macOS systems, it opens the URL using \'safari\'. Arguments: url -- The URL to be opened. try: system = platform.system() if system == \'Linux\' or system == \'Unix\': try: webbrowser.get(\'firefox\').open(url) except webbrowser.Error: try: webbrowser.get(\'lynx\').open(url) except webbrowser.Error: print(\\"No suitable browser found to open the URL.\\") elif system == \'Windows\': webbrowser.open(url) elif system == \'Darwin\': # macOS webbrowser.get(\'safari\').open(url) else: print(\\"Operating system not supported.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Problem Statement You are provided with a dataset containing physical characteristics of various flowers. Your task is to implement a classification model using scikit-learn\'s `MLPClassifier` to predict the species of the flowers based on their characteristics. # Dataset The dataset is in a CSV file (`flowers.csv`) and contains the following columns: - `sepal_length`: the length of the sepal - `sepal_width`: the width of the sepal - `petal_length`: the length of the petal - `petal_width`: the width of the petal - `species`: the species of the flower (your target variable) # Requirements 1. **Load the Dataset**: - Load the dataset into a Pandas DataFrame. 2. **Preprocess the Data**: - Split the dataset into features (X) and target (y). - Perform a train-test split with 80% of the data for training and 20% for testing. - Scale the feature data using `StandardScaler`. 3. **Model Implementation**: - Define an `MLPClassifier` with the following parameters: - `solver=\'adam\'` - `alpha=1e-5` - `hidden_layer_sizes=(10, 5)` - `random_state=42` - Train the model using the training data. 4. **Model Evaluation**: - Predict the species for the test data. - Calculate and print the accuracy of the model on the test data. - Generate and print a classification report including precision, recall, and f1-score for each class. 5. **Function Signature**: ```python def classify_flowers(data_path: str) -> None: # Implement the function as per the guidelines pass ``` # Constraints - You must use the `MLPClassifier` from `sklearn.neural_network`. - Ensure that all necessary preprocessing steps are performed. - Your model should achieve at least 85% accuracy on the test dataset. # Example Usage ```python # Example usage of the function classify_flowers(\'path/to/flowers.csv\') ``` # Note - The function does not return anything but prints the evaluation metrics.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, classification_report def classify_flowers(data_path: str) -> None: Classify flowers based on their physical characteristics. Parameters: data_path (str): The path to the CSV file containing the flower dataset. Prints: - Accuracy of the model on the test data. - Classification report including precision, recall, and f1-score for each class. # Load the dataset df = pd.read_csv(data_path) # Split the dataset into features and target X = df.drop(columns=[\'species\']) y = df[\'species\'] # Perform a train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Define the MLPClassifier mlp = MLPClassifier(solver=\'adam\', alpha=1e-5, hidden_layer_sizes=(10, 5), random_state=42) # Train the model mlp.fit(X_train_scaled, y_train) # Predict the species on the test set y_pred = mlp.predict(X_test_scaled) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy * 100:.2f}%\\") # Generate the classification report report = classification_report(y_test, y_pred) print(\\"Classification Report:\\") print(report)"},{"question":"**Objective:** The goal is to assess your understanding of the `cmd` module in Python and your ability to build an interactive command-line interpreter that processes custom commands and provides specific functionality. **Problem Statement:** You are tasked with creating a shell interpreter for managing a library digital archive. Your interpreter should support commands for adding, removing, searching, and listing books within the archive. Implement this shell interpreter as a subclass of the `Cmd` class, equipped with the necessary commands. **Details:** 1. **Command Structure:** - `add <title> <author> <year>`: Adds a new book to the archive. - `remove <title>`: Removes a book from the archive by title. - `search <keyword>`: Searches for books that contain the keyword in their title or author. - `list`: Lists all books in the archive. - `bye`: Exits the shell interpreter. 2. **Input and Output:** - Books should be stored as dictionaries in a list, each dictionary having `title`, `author`, and `year` keys. - Display appropriate messages for each action (e.g., book added/removed, search results). 3. **Constraints:** - Implement necessary validation (e.g., prevent duplicate book entries by title). - Manage edge cases (e.g., removing/searching for a book that does not exist). - Use appropriate hooks (`precmd`, `postcmd`, etc.) to transform commands into lowercase before processing. 4. **Performance Requirements:** - Ensure that the shell can handle at least 1000 books efficiently. - Implement the search functionality to operate in linear time complexity. **Example:** ```python from cmd import Cmd class LibraryShell(Cmd): intro = \'Welcome to the Library Archive Shell. Type help or ? to list commands.n\' prompt = \'(library) \' library = [] def do_add(self, arg): \'Add a new book to the library: ADD <title> <author> <year>\' # Implement add functionality here def do_remove(self, arg): \'Remove a book from the library by title: REMOVE <title>\' # Implement remove functionality here def do_search(self, arg): \'Search for books that contain the keyword in title or author: SEARCH <keyword>\' # Implement search functionality here def do_list(self, arg): \'List all books in the library: LIST\' # Implement list functionality here def do_bye(self, arg): \'Exit the shell: BYE\' print(\'Thank you for using the Library Archive Shell\') return True def precmd(self, line): return line.lower() # Start the shell if __name__ == \'__main__\': LibraryShell().cmdloop() ``` **Your Task:** 1. Complete the implementation of the `do_add`, `do_remove`, `do_search`, and `do_list` methods according to the requirements. 2. Ensure that the shell operates as specified, managing the internal data structure efficiently and handling edge cases gracefully. **Submission:** Submit your completed code with all necessary methods implemented, following the structure provided above. Ensure the code is well-documented and includes any assumptions made.","solution":"from cmd import Cmd class LibraryShell(Cmd): intro = \'Welcome to the Library Archive Shell. Type help or ? to list commands.n\' prompt = \'(library) \' library = [] def do_add(self, arg): \'Add a new book to the library: ADD <title> <author> <year>\' args = arg.split() if len(args) != 3: print(\\"Error: Please provide title, author, and year.\\") return title, author, year = args for book in self.library: if book[\'title\'].lower() == title.lower(): print(f\\"Error: Book with title \'{title}\' already exists.\\") return new_book = {\'title\': title, \'author\': author, \'year\': year} self.library.append(new_book) print(f\\"Book \'{title}\' by {author} added to the library.\\") def do_remove(self, arg): \'Remove a book from the library by title: REMOVE <title>\' title = arg.strip() if not title: print(\\"Error: Please provide the title of the book to remove.\\") return for book in self.library: if book[\'title\'].lower() == title.lower(): self.library.remove(book) print(f\\"Book \'{title}\' removed from the library.\\") return print(f\\"Error: Book with title \'{title}\' not found.\\") def do_search(self, arg): \'Search for books that contain the keyword in title or author: SEARCH <keyword>\' keyword = arg.strip().lower() if not keyword: print(\\"Error: Please provide a keyword to search.\\") return results = [book for book in self.library if keyword in book[\'title\'].lower() or keyword in book[\'author\'].lower()] if results: for book in results: print(f\\"Found book: {book[\'title\']} by {book[\'author\']} ({book[\'year\']})\\") else: print(\\"No books found with that keyword.\\") def do_list(self, arg): \'List all books in the library: LIST\' if self.library: for book in self.library: print(f\\"{book[\'title\']} by {book[\'author\']} ({book[\'year\']})\\") else: print(\\"No books in the library.\\") def do_bye(self, arg): \'Exit the shell: BYE\' print(\'Thank you for using the Library Archive Shell\') return True def precmd(self, line): return line.lower() # Start the shell if __name__ == \'__main__\': LibraryShell().cmdloop()"},{"question":"# Audio Transformation Challenge You are tasked with processing raw audio samples using the `audioop` module. Given an audio fragment, you need to implement a function that combines several operations to normalize and convert the fragment. # Instructions: 1. Implement a function `process_audio(fragment: bytes, width: int, target_rate: int, lfactor: float, rfactor: float) -> bytes`. 2. The function should perform the following operations in this specific order: - **Reverse** the audio fragment. - **Convert** the fragment\'s frame rate to `target_rate` using `audioop.ratecv()`. - Normalize the fragment by converting it to mono using the given `lfactor` and `rfactor` for the left and right channels respectively. - Apply a **bias** to the normalized fragment such that each sample value is shifted by 128 units. 3. **Parameters:** - `fragment`: The input audio fragment as a bytes object. - `width`: The sample width in bytes (either 1, 2, 3 or 4). - `target_rate`: The desired sample rate for the output fragment. - `lfactor`: The factor by which the left channel should be multiplied when converting to mono. - `rfactor`: The factor by which the right channel should be multiplied when converting to mono. 4. **Returns:** - The resulting processed audio fragment as a bytes object. 5. **Constraints:** - The input `fragment` length is guaranteed to be a multiple of the sample width. - Assume a constant input sample rate for simplicity. - The right channel is to be ignored if the input is mono. # Example Usage: ```python fragment = b\'x00x01x02x03x04x05x06x07\' width = 2 target_rate = 44100 lfactor = 0.5 rfactor = 0.5 processed_fragment = process_audio(fragment, width, target_rate, lfactor, rfactor) print(processed_fragment) ``` This problem requires a comprehensive understanding of the `audioop` module and the ability to chain multiple audio processing functions correctly. # Notes: 1. Consider using `audioop.reverse` to reverse the fragment. 2. Use `audioop.ratecv` for frame rate conversion. 3. Utilize `audioop.tomono` for converting stereo to mono based on `lfactor` and `rfactor`. 4. Apply `audioop.bias` to add a constant bias to each sample. # Grading Criteria: - Correct implementation of all steps. - Efficient and clear code. - Correct handling of input types and constraints. - Clear documentation and commenting of code.","solution":"import audioop def process_audio(fragment: bytes, width: int, target_rate: int, lfactor: float, rfactor: float) -> bytes: Process the audio fragment with the following steps: 1. Reverse the audio fragment. 2. Convert the fragment\'s frame rate to target_rate. 3. Normalize the fragment by converting it to mono. 4. Apply a bias to the normalized fragment. Parameters: - fragment: The input audio fragment as a bytes object. - width: The sample width in bytes (either 1, 2, 3 or 4). - target_rate: The desired sample rate for the output fragment. - lfactor: The factor for the left channel when converting to mono. - rfactor: The factor for the right channel when converting to mono. Returns: - The processed audio fragment as a bytes object. # Reverse the audio fragment reversed_fragment = audioop.reverse(fragment, width) # Assume the input sample rate is 44100 for the rate conversion input_rate = 44100 converted_fragment, _ = audioop.ratecv(reversed_fragment, width, 1, input_rate, target_rate, None) # Convert the fragment to mono mono_fragment = audioop.tomono(converted_fragment, width, lfactor, rfactor) # Apply a bias to the normalized fragment biased_fragment = audioop.bias(mono_fragment, width, 128) return biased_fragment"},{"question":"You are tasked with evaluating the performance of two alternative implementations of a common string manipulation task. Specifically, you need to determine which implementation processes a list of strings more efficiently using the `timeit` module in Python. The task in question involves converting a list of integers to a single comma-separated string. Here are the two different implementations: 1. Using list comprehension: ```python def list_comprehension_conversion(int_list): return \\",\\".join([str(i) for i in int_list]) ``` 2. Using the `map` function: ```python def map_conversion(int_list): return \\",\\".join(map(str, int_list)) ``` Task: - Write a script to measure the execution time of both implementations using the `timeit` module. - Ensure your script uses a setup statement to generate a list of 1000 integers (`int_list = list(range(1000))`). - Compare the performance by running each implementation 100000 times and determining which one is faster. Output: Your script should output the execution time for each implementation along with a clear statement indicating which method is faster. Constraints: - Use the `timeit` moduleâ€™s `timeit()` function to measure the execution time. - Your implementations should be tested within the same script file. # Example Output ``` List comprehension conversion: 1.234567 seconds. Map function conversion: 0.987654 seconds. The map function conversion is faster. ``` # Requirements: - Create functions for each implementation as shown. - Utilize the `timeit` module to run each implementation 100000 times. - Ensure the `timeit` setup string correctly initializes `int_list`. *Use the given example to guide your output format.*","solution":"import timeit def list_comprehension_conversion(int_list): Converts a list of integers to a comma-separated string using list comprehension. return \\",\\".join([str(i) for i in int_list]) def map_conversion(int_list): Converts a list of integers to a comma-separated string using the map function. return \\",\\".join(map(str, int_list)) def measure_performance(): setup_code = \\"int_list = list(range(1000))\\" list_comp_time = timeit.timeit(\'list_comprehension_conversion(int_list)\', setup=setup_code + \\";from __main__ import list_comprehension_conversion\\", number=100000) map_func_time = timeit.timeit(\'map_conversion(int_list)\', setup=setup_code + \\";from __main__ import map_conversion\\", number=100000) print(f\\"List comprehension conversion: {list_comp_time:.6f} seconds.\\") print(f\\"Map function conversion: {map_func_time:.6f} seconds.\\") if list_comp_time < map_func_time: print(\\"The list comprehension conversion is faster.\\") else: print(\\"The map function conversion is faster.\\") if __name__ == \\"__main__\\": measure_performance()"},{"question":"Objective: Demonstrate your understanding of concurrent execution in Python using the `concurrent.futures` module. Problem Statement: You are tasked with designing a function that computes the sum of all prime numbers up to a given number `N` by distributing the workload across multiple threads or processes for efficiency. You must implement two versions of this function: 1. `sum_primes_threadpool(N: int, num_threads: int) -> int` 2. `sum_primes_processpool(N: int, num_processes: int) -> int` Each function should: - Divide the range `[2, N]` into nearly equal parts, each to be handled by a separate thread or process. - Use `concurrent.futures.ThreadPoolExecutor` for the `sum_primes_threadpool` function. - Use `concurrent.futures.ProcessPoolExecutor` for the `sum_primes_processpool` function. - Return the sum of all prime numbers up to `N`. Constraints: - 1 <= N <= 10^6 - 1 <= num_threads, num_processes <= 32 Guidelines: - Ensure that the workload is fairly divided among the threads or processes. - Handle any exceptions that might occur during the execution. - Optimize for performance to demonstrate an understanding of concurrent execution. Example: ```python def sum_primes_threadpool(N: int, num_threads: int) -> int: # Your implementation here pass def sum_primes_processpool(N: int, num_processes: int) -> int: # Your implementation here pass # Example Usage print(sum_primes_threadpool(10, 2)) # Output: 17 (2 + 3 + 5 + 7) print(sum_primes_processpool(10, 2)) # Output: 17 (2 + 3 + 5 + 7) ``` Evaluation: - Your code will be evaluated on correctness, efficiency, and proper use of concurrent execution. - Provide appropriate comments and handle edge cases.","solution":"import concurrent.futures from math import isqrt def is_prime(n): if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def sum_primes_in_range(start, end): return sum(i for i in range(start, end) if is_prime(i)) def sum_primes_threadpool(N, num_threads): with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor: chunk_size = (N - 1) // num_threads + 1 futures = [executor.submit(sum_primes_in_range, i, min(i + chunk_size, N + 1)) for i in range(2, N + 1, chunk_size)] return sum(future.result() for future in concurrent.futures.as_completed(futures)) def sum_primes_processpool(N, num_processes): with concurrent.futures.ProcessPoolExecutor(max_workers=num_processes) as executor: chunk_size = (N - 1) // num_processes + 1 futures = [executor.submit(sum_primes_in_range, i, min(i + chunk_size, N + 1)) for i in range(2, N + 1, chunk_size)] return sum(future.result() for future in concurrent.futures.as_completed(futures))"},{"question":"The \\"operator\\" module provides various functional forms of Python\'s intrinsic operations such as comparison, logical, and sequence operations. Your task is to implement a custom function that combines multiple such operations to filter, sort, and transform a list of dictionaries based on certain criteria and return a processed list. Task: 1. **Function Signature**: ```python def process_inventory(inventory: List[Dict[str, Union[int, str]]], threshold: int) -> List[Tuple[str, int]]: ``` 2. **Inputs**: - `inventory`: A list of dictionaries representing items in an inventory. Each dictionary has the following structure: ```python {\\"name\\": str, \\"count\\": int, \\"price\\": int} ``` - `threshold`: An integer threshold. 3. **Output**: - A list of tuples. Each tuple contains: - The `name` of an item. - The `count` of that item. 4. **Requirements**: - Filter the `inventory` to only include items with a `count` greater than the given `threshold`. - Sort the filtered items by `count` in descending order. - Create a list of tuples containing the `name` and `count` of the sorted items. 5. **Constraints**: - Assume all dictionary entries in `inventory` are well-formed (no validation required). - The function must utilize functions from the `operator` module where applicable. Example: ```python inventory = [ {\\"name\\": \\"apple\\", \\"count\\": 3, \\"price\\": 10}, {\\"name\\": \\"banana\\", \\"count\\": 5, \\"price\\": 8}, {\\"name\\": \\"pear\\", \\"count\\": 2, \\"price\\": 7}, {\\"name\\": \\"orange\\", \\"count\\": 4, \\"price\\": 6} ] threshold = 3 result = process_inventory(inventory, threshold) print(result) # Output: [(\'banana\', 5), (\'orange\', 4)] ``` In this example: - The `inventory` is filtered to include items with `count` > 3: `banana` and `orange`. - These items are then sorted by `count` in descending order. - The resulting list of tuples is `[(\'banana\', 5), (\'orange\', 4)]`. Notes: - You are encouraged to use `operator.itemgetter`, `operator.gt`, and any other relevant functions from the `operator` module to complete this task. ```python from typing import List, Dict, Union, Tuple import operator def process_inventory(inventory: List[Dict[str, Union[int, str]]], threshold: int) -> List[Tuple[str, int]]: # Your implementation here pass # Example usage: inventory = [ {\\"name\\": \\"apple\\", \\"count\\": 3, \\"price\\": 10}, {\\"name\\": \\"banana\\", \\"count\\": 5, \\"price\\": 8}, {\\"name\\": \\"pear\\", \\"count\\": 2, \\"price\\": 7}, {\\"name\\": \\"orange\\", \\"count\\": 4, \\"price\\": 6} ] threshold = 3 result = process_inventory(inventory, threshold) print(result) # Expected output: [(\'banana\', 5), (\'orange\', 4)] ```","solution":"from typing import List, Dict, Union, Tuple import operator def process_inventory(inventory: List[Dict[str, Union[int, str]]], threshold: int) -> List[Tuple[str, int]]: # Filter items with count greater than the threshold using itemgetter and gt filtered_inventory = list(filter(lambda item: operator.gt(item[\\"count\\"], threshold), inventory)) # Sort the filtered items by count in descending order sorted_inventory = sorted(filtered_inventory, key=operator.itemgetter(\\"count\\"), reverse=True) # Create a list of tuples containing name and count result = [(item[\\"name\\"], item[\\"count\\"]) for item in sorted_inventory] return result"},{"question":"# Advanced Coding Assessment Question: Secure Chat Application Objective: Create a secure chat server using Python\'s `ssl` module. The server must use TLS encryption to ensure secure communication between clients and the server. You will also create a simple client to connect to this server. Requirements: 1. **Secure Server Implementation**: - The server should use TLS encryption to securely communicate with clients. - It should handle multiple clients, echoing received messages back to all connected clients. - Use a self-signed certificate for TLS encryption. - Bind the server to `localhost` on port `8443`. 2. **Client Implementation**: - The client should securely connect to the server using TLS encryption. - The client should send messages to the server and print any received messages. Input and Output Formats: - The server does not take any input from the command line. It listens on a specified port and communicates with connected clients. - The client should connect to the server and allow the user to input messages from the command line, which it will send to the server. The client should also print any messages received from the server. Constraints: - Use Python 3.10 and the `ssl` module. - Ensure your implementation follows security best practices as outlined in the documentation (e.g., certificate verification). Performance Requirements: - Ensure efficient handling of multiple clients. Python\'s `select` module can be used for handling multiple client connections efficiently. # Implementation Steps: 1. **Create a Self-signed Certificate**: - Use OpenSSL command-line tools to create a self-signed certificate and private key. ```shell openssl req -new -x509 -days 365 -nodes -out cert.pem -keyout key.pem ``` 2. **Server Implementation**: - Create a secure server using `ssl.SSLContext` and `ssl.wrap_socket`. - Handle incoming client connections and distribute received messages to all connected clients. - Use `select` for efficient client management. 3. **Client Implementation**: - Create a secure client using `ssl.create_default_context`. - Connect to the server using the provided self-signed certificate. - Implement sending and receiving messages. ```python # server.py import socket import ssl import select def broadcast_message(message, clients, sender): for client in clients: if client != sender: try: client.sendall(message) except Exception: clients.remove(client) def main(): context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) context.load_cert_chain(certfile=\'cert.pem\', keyfile=\'key.pem\') server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', 8443)) server_socket.listen(5) client_socks = [] wrapped_server_socket = context.wrap_socket(server_socket, server_side=True) try: while True: readable, _, _ = select.select([wrapped_server_socket] + client_socks, [], []) for s in readable: if s == wrapped_server_socket: client_socket, addr = wrapped_server_socket.accept() client_socks.append(client_socket) print(f\'Client {addr} connected\') else: try: message = s.recv(1024) if not message: client_socks.remove(s) s.close() else: broadcast_message(message, client_socks, s) except Exception: client_socks.remove(s) s.close() except KeyboardInterrupt: print(\'Server shutting down...\') finally: for s in client_socks: s.close() wrapped_server_socket.close() if __name__ == \'__main__\': main() ``` ```python # client.py import socket import ssl import threading def receive_message(sock): while True: try: message = sock.recv(1024) if message: print(f\\"Received: {message.decode(\'utf-8\')}\\") else: break except Exception: break def main(): hostname = \'localhost\' context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH) context.load_verify_locations(\'cert.pem\') s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) secure_sock = context.wrap_socket(s, server_hostname=hostname) secure_sock.connect((hostname, 8443)) threading.Thread(target=receive_message, args=(secure_sock,)).start() while True: try: message = input(\\"Enter message: \\") if message: secure_sock.sendall(message.encode(\'utf-8\')) except KeyboardInterrupt: break secure_sock.close() if __name__ == \'__main__\': main() ``` # Submission: Submit your `server.py` and `client.py` scripts. Make sure they work together as described and follow the requirements mentioned. Note: - Read through the provided documentation carefully to understand the usage of `ssl` module functions and classes. - Ensure proper error handling and cleanup of resources.","solution":"# server.py import socket import ssl import select def broadcast_message(message, clients, sender): for client in clients: if client != sender: try: client.sendall(message) except Exception: clients.remove(client) def main(): context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) context.load_cert_chain(certfile=\'cert.pem\', keyfile=\'key.pem\') server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', 8443)) server_socket.listen(5) client_socks = [] wrapped_server_socket = context.wrap_socket(server_socket, server_side=True) try: while True: readable, _, _ = select.select([wrapped_server_socket] + client_socks, [], []) for s in readable: if s == wrapped_server_socket: client_socket, addr = wrapped_server_socket.accept() client_socks.append(client_socket) print(f\'Client {addr} connected\') else: try: message = s.recv(1024) if not message: client_socks.remove(s) s.close() else: broadcast_message(message, client_socks, s) except Exception: client_socks.remove(s) s.close() except KeyboardInterrupt: print(\'Server shutting down...\') finally: for s in client_socks: s.close() wrapped_server_socket.close() if __name__ == \'__main__\': main() # client.py import socket import ssl import threading def receive_message(sock): while True: try: message = sock.recv(1024) if message: print(f\\"Received: {message.decode(\'utf-8\')}\\") else: break except Exception: break def main(): hostname = \'localhost\' context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH) context.load_verify_locations(\'cert.pem\') s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) secure_sock = context.wrap_socket(s, server_hostname=hostname) secure_sock.connect((hostname, 8443)) threading.Thread(target=receive_message, args=(secure_sock,)).start() while True: try: message = input(\\"Enter message: \\") if message: secure_sock.sendall(message.encode(\'utf-8\')) except KeyboardInterrupt: break secure_sock.close() if __name__ == \'__main__\': main()"},{"question":"Objective In this task, you will demonstrate your understanding of decision trees using the `scikit-learn` package by performing classification and regression analyses. You will also visualize the decision trees and evaluate their performance. Requirements 1. **Classification Task**: - Load the Iris dataset from `sklearn.datasets`. - Split the dataset into training and testing sets. - Train a `DecisionTreeClassifier` on the training set. - Predict the labels for the test set and calculate the accuracy. - Visualize the decision tree using the `plot_tree` function. - Experiment with different values for `max_depth` parameter and observe its effect on accuracy. 2. **Regression Task**: - Create a synthetic dataset using the following code: ```python import numpy as np rng = np.random.RandomState(1) X = np.sort(5 * rng.rand(80, 1), axis=0) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - rng.rand(16)) ``` - Split the dataset into training and testing sets. - Train a `DecisionTreeRegressor` on the training set. - Predict the labels for the test set and calculate the Mean Squared Error (MSE). - Visualize the decision tree using the `plot_tree` function. - Experiment with different values for `max_depth` parameter and observe its effect on MSE. Input and Output Formats - **Input**: No input is required from the user. All data will be loaded in the script. - **Output**: Print the accuracy for the classification task and the MSE for the regression task. Use plots to visualize the decision trees. Constraints - Use `max_depth` values such as 3, 5, and None to showcase the impacts of overfitting and underfitting. - Use an `random_state` parameter value to ensure reproducibility. - Ensure that the `plot_tree` outputs are clear and interpretable. Additional Information - Document your code with comments explaining each step. - Visualize the decision boundaries for the classification task using `plot_tree`. - For regression, plot the true function and the predictions. --- Example Implementation ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.metrics import accuracy_score, mean_squared_error # Classification Task iris = load_iris() X_clf, y_clf = iris.data, iris.target X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.3, random_state=42) clf = DecisionTreeClassifier(max_depth=3, random_state=42) clf.fit(X_train_clf, y_train_clf) y_pred_clf = clf.predict(X_test_clf) accuracy = accuracy_score(y_test_clf, y_pred_clf) print(f\'Classification Accuracy: {accuracy:.2f}\') plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Experiment with different max_depth values and print their accuracies for depth in [3, 5, None]: clf = DecisionTreeClassifier(max_depth=depth, random_state=42) clf.fit(X_train_clf, y_train_clf) y_pred_clf = clf.predict(X_test_clf) accuracy = accuracy_score(y_test_clf, y_pred_clf) print(f\'Classification Accuracy with max_depth={depth}: {accuracy:.2f}\') # Regression Task rng = np.random.RandomState(1) X_reg = np.sort(5 * rng.rand(80, 1), axis=0) y_reg = np.sin(X_reg).ravel() y_reg[::5] += 3 * (0.5 - rng.rand(16)) X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42) reg = DecisionTreeRegressor(max_depth=3, random_state=42) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) mse = mean_squared_error(y_test_reg, y_pred_reg) print(f\'Regression Mean Squared Error: {mse:.2f}\') plt.figure(figsize=(20,10)) plot_tree(reg, filled=True) plt.show() # Plot true function and predictions X_test_reg_plot = np.linspace(0, 5, 100).reshape(-1, 1) y_pred_reg_plot = reg.predict(X_test_reg_plot) plt.figure() plt.scatter(X_train_reg, y_train_reg, s=20, edgecolor=\\"black\\", c=\\"darkorange\\", label=\\"data\\") plt.plot(X_test_reg_plot, y_pred_reg_plot, color=\\"cornflowerblue\\", label=\\"max_depth=3\\", linewidth=2) plt.xlabel(\\"data\\") plt.ylabel(\\"target\\") plt.title(\\"Decision Tree Regression\\") plt.legend() plt.show() # Experiment with different max_depth values and print their MSEs for depth in [3, 5, None]: reg = DecisionTreeRegressor(max_depth=depth, random_state=42) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) mse = mean_squared_error(y_test_reg, y_pred_reg) print(f\'Regression MSE with max_depth={depth}: {mse:.2f}\') ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree from sklearn.metrics import accuracy_score, mean_squared_error def decision_tree_classification(): # Load the Iris dataset iris = load_iris() X_clf, y_clf = iris.data, iris.target # Split the dataset into training and testing sets X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.3, random_state=42) # Train a DecisionTreeClassifier on the training set clf = DecisionTreeClassifier(max_depth=3, random_state=42) clf.fit(X_train_clf, y_train_clf) # Predict the labels for the test set and calculate the accuracy y_pred_clf = clf.predict(X_test_clf) accuracy = accuracy_score(y_test_clf, y_pred_clf) print(f\'Classification Accuracy: {accuracy:.2f}\') # Visualize the decision tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() # Experiment with different max_depth values and print their accuracies for depth in [3, 5, None]: clf = DecisionTreeClassifier(max_depth=depth, random_state=42) clf.fit(X_train_clf, y_train_clf) y_pred_clf = clf.predict(X_test_clf) accuracy = accuracy_score(y_test_clf, y_pred_clf) print(f\'Classification Accuracy with max_depth={depth}: {accuracy:.2f}\') def decision_tree_regression(): # Create a synthetic dataset rng = np.random.RandomState(1) X_reg = np.sort(5 * rng.rand(80, 1), axis=0) y_reg = np.sin(X_reg).ravel() y_reg[::5] += 3 * (0.5 - rng.rand(16)) # Split the dataset into training and testing sets X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42) # Train a DecisionTreeRegressor on the training set reg = DecisionTreeRegressor(max_depth=3, random_state=42) reg.fit(X_train_reg, y_train_reg) # Predict the labels for the test set and calculate the Mean Squared Error (MSE) y_pred_reg = reg.predict(X_test_reg) mse = mean_squared_error(y_test_reg, y_pred_reg) print(f\'Regression Mean Squared Error: {mse:.2f}\') # Visualize the decision tree plt.figure(figsize=(20,10)) plot_tree(reg, filled=True) plt.show() # Plot true function and predictions X_test_reg_plot = np.linspace(0, 5, 100).reshape(-1, 1) y_pred_reg_plot = reg.predict(X_test_reg_plot) plt.figure() plt.scatter(X_train_reg, y_train_reg, s=20, edgecolor=\\"black\\", c=\\"darkorange\\", label=\\"data\\") plt.plot(X_test_reg_plot, y_pred_reg_plot, color=\\"cornflowerblue\\", label=\\"max_depth=3\\", linewidth=2) plt.xlabel(\\"data\\") plt.ylabel(\\"target\\") plt.title(\\"Decision Tree Regression\\") plt.legend() plt.show() # Experiment with different max_depth values and print their MSEs for depth in [3, 5, None]: reg = DecisionTreeRegressor(max_depth=depth, random_state=42) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) mse = mean_squared_error(y_test_reg, y_pred_reg) print(f\'Regression MSE with max_depth={depth}: {mse:.2f}\')"},{"question":"**Objective:** To assess the comprehension of fundamental and advanced concepts in scikit-learn\'s linear models by implementing and comparing different regression techniques. **Problem Statement:** You are given a dataset `house_prices.csv` containing features related to house characteristics and their corresponding prices. Your task is to build and compare different regression models to predict house prices. The dataset contains the following columns: - `Area`: The area of the house in square feet. - `Bedrooms`: The number of bedrooms in the house. - `Bathrooms`: The number of bathrooms in the house. - `Garage`: Indicator if the house has a garage (1) or not (0). - `Price`: The target column representing the price of the house. **Tasks:** 1. **Data Preparation:** - Load the dataset and perform basic exploratory data analysis (EDA) to understand the data distribution and check for missing values. - Handle any missing values appropriately (if any). 2. **Model Implementation:** - Implement the following regression models using scikit-learn: 1. Ordinary Least Squares (OLS) Regression 2. Ridge Regression with cross-validated hyperparameter tuning for `alpha`. 3. Lasso Regression with cross-validated hyperparameter tuning for `alpha`. 4. Elastic-Net Regression with cross-validated hyperparameter tuning for `alpha` and `l1_ratio`. 3. **Model Comparison:** - Evaluate the performance of each model using appropriate metrics (e.g., Mean Squared Error, R2 Score). - Compare the models based on their performance metrics. 4. **Feature Importance:** - For the Lasso and Elastic-Net models, obtain the feature importance and identify which features are most significant in predicting house prices. 5. **Challenges & Insights:** - Discuss any challenges faced during the implementation and data preprocessing steps. - Provide insights on which model performed the best and why, referring to the model performance metrics and feature importance. **Instructions:** - **Input:** - The input will be a CSV file `house_prices.csv` with the columns mentioned above. - **Output:** - Code files implementing the tasks above. - A report summarizing the model performances, feature importance analysis, and insights. - **Constraints:** - You may use any additional data preprocessing or feature engineering techniques if necessary. - Ensure the code is modular and well-documented. **Submission:** Submit a zip file containing: - Python code files for the implementation. - A report in PDF format summarizing the results and insights. **Example:** ```python import pandas as pd from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error, r2_score # Load dataset data = pd.read_csv(\'house_prices.csv\') # Basic EDA print(data.describe()) # Handle missing values data = data.dropna() # Features and target X = data[[\'Area\', \'Bedrooms\', \'Bathrooms\', \'Garage\']] y = data[\'Price\'] # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # OLS Regression ols = LinearRegression() ols.fit(X_train, y_train) y_pred_ols = ols.predict(X_test) # Ridge Regression with GridSearchCV ridge = Ridge() params = {\'alpha\': [0.1, 1.0, 10.0, 100.0]} ridge_cv = GridSearchCV(ridge, params, cv=5) ridge_cv.fit(X_train, y_train) y_pred_ridge = ridge_cv.predict(X_test) # Lasso Regression with GridSearchCV lasso = Lasso() params = {\'alpha\': [0.1, 1.0, 10.0, 100.0]} lasso_cv = GridSearchCV(lasso, params, cv=5) lasso_cv.fit(X_train, y_train) y_pred_lasso = lasso_cv.predict(X_test) # Elastic-Net Regression with GridSearchCV elastic_net = ElasticNet() params = {\'alpha\': [0.1, 1.0, 10.0, 100.0], \'l1_ratio\': [0.1, 0.5, 0.9]} elastic_net_cv = GridSearchCV(elastic_net, params, cv=5) elastic_net_cv.fit(X_train, y_train) y_pred_en = elastic_net_cv.predict(X_test) # Evaluation print(f\\"OLS MSE: {mean_squared_error(y_test, y_pred_ols)}, R2: {r2_score(y_test, y_pred_ols)}\\") print(f\\"Ridge MSE: {mean_squared_error(y_test, y_pred_ridge)}, R2: {r2_score(y_test, y_pred_ridge)}\\") print(f\\"Lasso MSE: {mean_squared_error(y_test, y_pred_lasso)}, R2: {r2_score(y_test, y_pred_lasso)}\\") print(f\\"Elastic-Net MSE: {mean_squared_error(y_test, y_pred_en)}, R2: {r2_score(y_test, y_pred_en)}\\") # Feature importance for Lasso and Elastic-Net print(f\\"Lasso Coefficients: {lasso_cv.best_estimator_.coef_}\\") print(f\\"Elastic-Net Coefficients: {elastic_net_cv.best_estimator_.coef_}\\") ```","solution":"import pandas as pd from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error, r2_score def load_and_preprocess_data(file_path): # Load dataset data = pd.read_csv(file_path) # Basic EDA print(data.describe()) # Handle missing values data = data.dropna() return data def split_features_target(data): # Define features and target X = data[[\'Area\', \'Bedrooms\', \'Bathrooms\', \'Garage\']] y = data[\'Price\'] # Split the data into training and testing sets return train_test_split(X, y, test_size=0.2, random_state=42) def train_ols(X_train, y_train): ols = LinearRegression() ols.fit(X_train, y_train) return ols def train_ridge(X_train, y_train): ridge = Ridge() params = {\'alpha\': [0.1, 1.0, 10.0, 100.0]} ridge_cv = GridSearchCV(ridge, params, cv=5) ridge_cv.fit(X_train, y_train) return ridge_cv def train_lasso(X_train, y_train): lasso = Lasso() params = {\'alpha\': [0.1, 1.0, 10.0, 100.0]} lasso_cv = GridSearchCV(lasso, params, cv=5) lasso_cv.fit(X_train, y_train) return lasso_cv def train_elastic_net(X_train, y_train): elastic_net = ElasticNet() params = {\'alpha\': [0.1, 1.0, 10.0, 100.0], \'l1_ratio\': [0.1, 0.5, 0.9]} en_cv = GridSearchCV(elastic_net, params, cv=5) en_cv.fit(X_train, y_train) return en_cv def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return mse, r2 # To be run as script if __name__ == \\"__main__\\": data = load_and_preprocess_data(\'house_prices.csv\') X_train, X_test, y_train, y_test = split_features_target(data) # Train models ols_model = train_ols(X_train, y_train) ridge_model = train_ridge(X_train, y_train) lasso_model = train_lasso(X_train, y_train) en_model = train_elastic_net(X_train, y_train) # Evaluate models ols_mse, ols_r2 = evaluate_model(ols_model, X_test, y_test) ridge_mse, ridge_r2 = evaluate_model(ridge_model.best_estimator_, X_test, y_test) lasso_mse, lasso_r2 = evaluate_model(lasso_model.best_estimator_, X_test, y_test) en_mse, en_r2 = evaluate_model(en_model.best_estimator_, X_test, y_test) print(f\\"OLS MSE: {ols_mse}, R2: {ols_r2}\\") print(f\\"Ridge MSE: {ridge_mse}, R2: {ridge_r2}\\") print(f\\"Lasso MSE: {lasso_mse}, R2: {lasso_r2}\\") print(f\\"Elastic-Net MSE: {en_mse}, R2: {en_r2}\\") # Feature importance for Lasso and Elastic-Net print(f\\"Lasso Coefficients: {lasso_model.best_estimator_.coef_}\\") print(f\\"Elastic-Net Coefficients: {en_model.best_estimator_.coef_}\\")"},{"question":"**Question: Implementing and Analyzing Partial Dependence Plots and Individual Conditional Expectation Plots using scikit-learn** You are provided with the following dataset about housing prices: features include variables such as the number of rooms, location desirability (numerical score), and whether the house has a garden (categorical: Yes/No). The target variable is the price of the house. Your task is to: 1. Train a `GradientBoostingRegressor` on this dataset. 2. Generate and display Partial Dependence Plots (PDP) for the following features: - The number of rooms. - The location desirability score. - A two-way PDP for the number of rooms and the location desirability score. - The garden feature (categorical). 3. Generate and display Individual Conditional Expectation (ICE) plots for the number of rooms and the location desirability score. 4. Include on these plots the expected average effect using the `kind=\'both\'` parameter. 5. Interpret the plots and provide a brief explanation of any interesting patterns you observe in the data. Here\'s the detailed breakdown of what you need to do: # Input Format - You will be given a DataFrame `df` with the columns: `num_rooms`, `location_desirability`, `has_garden`, and `price`. # Output Format - Display the following plots: - PDP for `num_rooms`. - PDP for `location_desirability`. - Two-way PDP for `num_rooms` and `location_desirability`. - PDP for `has_garden` (categorical feature). - ICE plots for `num_rooms` and `location_desirability` with both ICE and PDP lines overlaid. - Provide a brief written explanation of the insights from the generated plots, focusing on interesting patterns and interactions among features. # Constraints and Limitations - Use `GradientBoostingRegressor` from `sklearn.ensemble`. - Ensure the PDP and ICE plots are appropriately labeled for clarity. - The garden feature should be treated as categorical. # Example Code Snippet Hereâ€™s how you might start with the execution: ```python import pandas as pd from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay # Sample dataset loading df = pd.read_csv(\'housing_prices.csv\') # Define your features (X) and target (y) X = df[[\'num_rooms\', \'location_desirability\', \'has_garden\']] y = df[\'price\'] # Train the Gradient Boosting Regressor model = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) model.fit(X, y) # Generate Partial Dependence Plots features = [\'num_rooms\', \'location_desirability\', (\'num_rooms\', \'location_desirability\'), \'has_garden\'] PartialDependenceDisplay.from_estimator(model, X, features, categorical_features=[\'has_garden\']) # Generate ICE plots features_single = [\'num_rooms\', \'location_desirability\'] PartialDependenceDisplay.from_estimator(model, X, features_single, kind=\'both\') # Provide explanations based on the plots # (write insightful observations based on the generated plots) ``` **Interpretation Section** - Write about the effect of the number of rooms on housing prices. - Discuss how the location desirability score influences the prices. - Explain the interaction between the number of rooms and the location desirability. - Comment on the effect of having a garden on the housing price. **Note:** Assume that the initial loading and cleaning of the dataset have been done appropriately, and the dataset is ready for modeling.","solution":"import pandas as pd from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay from sklearn.preprocessing import LabelEncoder # Sample dataset loading (assume df is already provided elsewhere and loaded) # df = pd.read_csv(\'housing_prices.csv\') df = pd.DataFrame({ \'num_rooms\': [3, 4, 5, 3, 4, 5, 6, 4, 5, 3], \'location_desirability\': [7, 8, 5, 6, 7, 8, 6, 7, 8, 7], \'has_garden\': [\'Yes\', \'No\', \'Yes\', \'No\', \'Yes\', \'No\', \'Yes\', \'No\', \'Yes\', \'No\'], \'price\': [300, 400, 500, 350, 420, 510, 605, 410, 520, 320] }) # Encode categorical feature \'has_garden\' label_enc = LabelEncoder() df[\'has_garden_encoded\'] = label_enc.fit_transform(df[\'has_garden\']) # Define your features (X) and target (y) X = df[[\'num_rooms\', \'location_desirability\', \'has_garden_encoded\']] y = df[\'price\'] # Train the Gradient Boosting Regressor model = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) model.fit(X, y) # Generate Partial Dependence Plots features = [\'num_rooms\', \'location_desirability\', (\'num_rooms\', \'location_desirability\'), \'has_garden_encoded\'] PartialDependenceDisplay.from_estimator(model, X, features, categorical_features=[\'has_garden_encoded\']) # Generate ICE plots features_single = [\'num_rooms\', \'location_desirability\'] PartialDependenceDisplay.from_estimator(model, X, features_single, kind=\'both\')"},{"question":"# Advanced Python: Manipulating Function Objects In this exercise, you will demonstrate your understanding of function objects in Python by implementing a function that creates and modifies Python functions programmatically. Instructions 1. **Function Creation**: Write a function `create_function` that takes the following parameters: - `code`: A string containing Python code to be compiled and turned into a function object. - `globals_dict`: A dictionary representing the global variables accessible to the function. - `qualname`: (Optional) An optional string representing the qualified name of the function. This function should: - Compile the code string into a code object. - Create a new function object using the compiled code and the provided globals dictionary. - If `qualname` is provided, set it as the function\'s `__qualname__` attribute. 2. **Function Modification**: Write a function `modify_function` that takes the following parameters: - `func`: A function object created by `create_function`. - `defaults`: (Optional) A tuple of default argument values. - `annotations`: (Optional) A dictionary of annotations for the function parameters and return type. This function should: - Set the default argument values of the function to `defaults` if provided. - Set the annotations of the function to `annotations` if provided. Requirements - Your implementation should handle any required type checks and raise appropriate exceptions for invalid inputs. - You must use the low-level function API specified in the provided documentation (e.g., `PyFunction_New`, `PyFunction_NewWithQualName`, `PyFunction_SetDefaults`, `PyFunction_SetAnnotations`). - Ensure your code is efficient and handles edge cases gracefully. Example ```python def create_function(code, globals_dict, qualname=None): # Your implementation here pass def modify_function(func, defaults=None, annotations=None): # Your implementation here pass # Example usage globals_dict = {\'x\': 1, \'y\': 2} code = \\"def add(a, b): return a + b + x + y\\" func = create_function(code, globals_dict, qualname=\\"math_operations.add\\") # Modifying function defaults and annotations modify_function(func, defaults=(0, 0), annotations={\'a\': int, \'b\': int, \'return\': int}) # Testing the function print(func(3, 4)) # Output should be 10 (3 + 4 + 1 + 2) print(func.__annotations__) # Output should be {\'a\': int, \'b\': int, \'return\': int} ``` Constraints - `code` must define a single function. - The function created should be callable and behave according to the provided code. - The code should be compatible with Python 3.10. Good luck!","solution":"import types def create_function(code, globals_dict, qualname=None): code_obj = compile(code, \'<string>\', \'exec\') func_code = [c for c in code_obj.co_consts if isinstance(c, types.CodeType)][0] func = types.FunctionType(func_code, globals_dict) if qualname: func.__qualname__ = qualname return func def modify_function(func, defaults=None, annotations=None): if defaults: func.__defaults__ = defaults if annotations: func.__annotations__ = annotations return func"},{"question":"Clustering and Performance Evaluation using scikit-learn Objective The goal of this exercise is to demonstrate your understanding of clustering algorithms and performance evaluation metrics using `scikit-learn`. You will perform clustering on a given dataset using multiple clustering algorithms, and then evaluate and compare the results using appropriate performance metrics. Instructions 1. **Data Preparation:** - Load the `Iris` dataset using `sklearn.datasets.load_iris`. - Standardize the dataset using `sklearn.preprocessing.StandardScaler`. 2. **Clustering:** - Implement clustering using the following algorithms: 1. K-means (`sklearn.cluster.KMeans`) 2. Agglomerative Clustering (`sklearn.cluster.AgglomerativeClustering`) 3. DBSCAN (`sklearn.cluster.DBSCAN`) 3. **Performance Evaluation:** - Evaluate the clustering results using the following metrics: 1. Adjusted Rand Index (`sklearn.metrics.adjusted_rand_score`) 2. Silhouette Score (`sklearn.metrics.silhouette_score`) 4. **Comparison:** - Based on the performance evaluation metrics, compare the clustering results and determine which algorithm performs the best on the Iris dataset. Expected Input and Output - **Input:** - No specific input is required as you will use the Iris dataset directly within your function. - **Output:** - Print the performance evaluation metrics (Adjusted Rand Index and Silhouette Score) for each clustering algorithm. - Print the clustering algorithm that performs the best based on the evaluation metrics. Constraints - You should use `scikit-learn` for all clustering and performance evaluation tasks. - Ensure code readability and add comments to explain your implementation. Performance Requirements - All operations should efficiently handle the Iris dataset, which consists of 150 samples with 4 features. Sample Code Structure ```python from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score def perform_clustering_and_evaluate(): # Load and standardize the dataset iris = load_iris() X = iris.data y_true = iris.target scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Clustering with K-means kmeans = KMeans(n_clusters=3, random_state=42).fit(X_scaled) y_kmeans = kmeans.labels_ # Clustering with Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=3).fit(X_scaled) y_agglomerative = agglomerative.labels_ # Clustering with DBSCAN dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled) y_dbscan = dbscan.labels_ # Evaluate performance print(\\"K-means Adjusted Rand Index:\\", adjusted_rand_score(y_true, y_kmeans)) print(\\"Agglomerative Clustering Adjusted Rand Index:\\", adjusted_rand_score(y_true, y_agglomerative)) print(\\"DBSCAN Adjusted Rand Index:\\", adjusted_rand_score(y_true, y_dbscan)) print(\\"K-means Silhouette Score:\\", silhouette_score(X_scaled, y_kmeans)) print(\\"Agglomerative Clustering Silhouette Score:\\", silhouette_score(X_scaled, y_agglomerative)) print(\\"DBSCAN Silhouette Score:\\", silhouette_score(X_scaled, y_dbscan)) # Determine the best performing algorithm based on evaluation metrics # Your comparison and determination logic here # Run the function perform_clustering_and_evaluate() ```","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score def perform_clustering_and_evaluate(): # Load and standardize the dataset iris = load_iris() X = iris.data y_true = iris.target scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Clustering with K-means kmeans = KMeans(n_clusters=3, random_state=42) y_kmeans = kmeans.fit_predict(X_scaled) # Clustering with Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=3) y_agglomerative = agglomerative.fit_predict(X_scaled) # Clustering with DBSCAN dbscan = DBSCAN(eps=0.5, min_samples=5) y_dbscan = dbscan.fit_predict(X_scaled) # Evaluate performance metrics = { \'K-means\': { \'Adjusted Rand Index\': adjusted_rand_score(y_true, y_kmeans), \'Silhouette Score\': silhouette_score(X_scaled, y_kmeans) }, \'Agglomerative Clustering\': { \'Adjusted Rand Index\': adjusted_rand_score(y_true, y_agglomerative), \'Silhouette Score\': silhouette_score(X_scaled, y_agglomerative) }, \'DBSCAN\': { \'Adjusted Rand Index\': adjusted_rand_score(y_true, y_dbscan), \'Silhouette Score\': silhouette_score(X_scaled, y_dbscan) } } # Printing the evaluation metrics for algo, metric in metrics.items(): print(f\\"{algo} Adjusted Rand Index: {metric[\'Adjusted Rand Index\']}\\") print(f\\"{algo} Silhouette Score: {metric[\'Silhouette Score\']}\\") # Determine the best performing algorithm based on evaluation # Ranking by Adjusted Rand Index, then by Silhouette Score best_algo = max(metrics, key=lambda k: (metrics[k][\'Adjusted Rand Index\'], metrics[k][\'Silhouette Score\'])) print(f\\"nThe best performing algorithm is: {best_algo}\\") perform_clustering_and_evaluate()"},{"question":"# Python Byte-Code Compiler Automation You are tasked with creating a Python script that automates the compilation of multiple Python source files into byte-code. This script should use the functionalities of the `py_compile` module as outlined below. Requirements 1. **Function Definition**: - Define a function `compile_sources(source_files: List[str], optimization: int = -1, invalidation: str = \\"TIMESTAMP\\", quiet: int = 0) -> Dict[str, Union[str, None]]` where: - `source_files` is a list of source file paths to be compiled. - `optimization` is the optimization level (valid values: -1, 0, 1, 2). - `invalidation` determines the invalidation mode (`TIMESTAMP`, `CHECKED_HASH`, or `UNCHECKED_HASH`). - `quiet` controls error output (0: show errors, 1: minimal output, 2: silent). - The function should return a dictionary where keys are source file paths, and values are the respective paths to the byte-code file or `None` if compilation failed. 2. **Error Handling**: - If a compilation error occurs, handle it based on the `quiet` parameter. - If `quiet` is set to 2, suppress all error messages. - If `quiet` is set to 1, log minimal error information. - If `quiet` is set to 0, display full error messages. 3. **Invalidation Mode**: - Handle invalidation mode using the `PycInvalidationMode` enum from `py_compile`. Input and Output - **Input**: A list of strings representing source file paths, and optional parameters for optimization, invalidation mode, and quiet level. - **Output**: A dictionary mapping source file paths to their respective compiled byte-code file paths, or `None` if the compilation failed. Constraints - The function should work for all valid Python scripts (.py files). - The compilation should respect the optimization and invalidation mode parameters. - Handle file paths carefully to ensure the compilation does not break due to issues like missing directories or invalid file names. - Assume all source files are in accessible directories. Example Usage ```python source_files = [\\"script1.py\\", \\"script2.py\\", \\"/path/to/script3.py\\"] results = compile_sources(source_files, optimization=2, invalidation=\\"CHECKED_HASH\\", quiet=1) print(results) # Output: {\'script1.py\': \'script1.cpython-38.opt-2.pyc\', \'script2.py\': \'script2.cpython-38.opt-2.pyc\', \'/path/to/script3.py\': None} ``` In this example: - `script1.py` and `script2.py` are successfully compiled with optimization level 2 and invalidation mode CHECKED_HASH. - `/path/to/script3.py` fails to compile, and no byte-code path is returned for it. Implement the function `compile_sources` that meets the above requirements.","solution":"import py_compile import os from typing import List, Dict, Union def compile_sources(source_files: List[str], optimization: int = -1, invalidation: str = \\"TIMESTAMP\\", quiet: int = 0) -> Dict[str, Union[str, None]]: results = {} pyc_invalidation_mode = py_compile.PycInvalidationMode[invalidation] for source_file in source_files: try: pyc_file = py_compile.compile( source_file, cfile=None, dfile=None, doraise=True, optimize=optimization, invalidation_mode=pyc_invalidation_mode, quiet=quiet ) results[source_file] = pyc_file except Exception as e: if quiet == 0: print(f\\"Failed to compile {source_file}: {e}\\") elif quiet == 1: print(f\\"Failed to compile {source_file}\\") results[source_file] = None return results"},{"question":"Objective: To assess your understanding of PyTorch\'s HIP and CUDA interfaces, memory management, and GPU device handling. Problem Statement: You are given a task to perform tensor operations on a GPU using PyTorch. Write a Python function `tensor_operations_on_gpu` that performs the following actions: 1. **Device Initialization**: - Create a default CUDA device and a specific CUDA device (e.g., GPU 1). 2. **Tensor Creation and Transfer**: - Create a tensor initialized with values [1., 2.] on the default CUDA device. - Transfer the tensor to GPU 1 and create another tensor on GPU 1 initialized with the same values. 3. **Arithmetic Operations**: - Perform element-wise addition of the two tensors created on GPU 1 and store the result in a new tensor. 4. **Monitor Memory Usage**: - Print the amount of memory allocated and reserved by PyTorch\'s caching allocator before and after freeing unused cached memory. 5. **Check GPU Support**: - Check if PyTorch is using HIP or CUDA and print a corresponding message. Input: This function does not take any inputs. Output: The function should: 1. Print the device information of the created and transferred tensors. 2. Print the result tensor from the addition. 3. Print the memory usage before and after freeing the unused cached memory. 4. Print whether HIP or CUDA is being used. Constraints: - Use PyTorch for tensor operations. - Assume that a GPU (CUDA or HIP) is available. Function Signature: ```python def tensor_operations_on_gpu(): pass ``` Example Output: ``` Tensor on default device: cuda:0 Tensor transferred to GPU 1: cuda:1 Result tensor on GPU 1: tensor([2., 4.], device=\'cuda:1\') Memory allocated before freeing cache: 10240 bytes Memory reserved before freeing cache: 20480 bytes Memory allocated after freeing cache: 10240 bytes Memory reserved after freeing cache: 10240 bytes Using CUDA for GPU operations ``` Note: The memory values in the example output are hypothetical and will vary based on the actual runtime environment. Performance Requirements: - Ensure that the operations efficiently handle memory. - The solution must correctly identify the backend (HIP/CUDA) and perform the required tensor operations on the specified GPU devices.","solution":"import torch def tensor_operations_on_gpu(): # Device Initialization if not torch.cuda.is_available(): print(\\"CUDA is not available.\\") return default_device = torch.device(\'cuda:0\') specific_device = torch.device(\'cuda:1\') # Tensor Creation and Transfer tensor_default = torch.tensor([1., 2.], device=default_device) tensor_transferred = tensor_default.to(specific_device) tensor_new = torch.tensor([1., 2.], device=specific_device) # Arithmetic Operations result = tensor_transferred + tensor_new # Monitor Memory Usage allocated_before = torch.cuda.memory_allocated(specific_device) reserved_before = torch.cuda.memory_reserved(specific_device) torch.cuda.empty_cache() allocated_after = torch.cuda.memory_allocated(specific_device) reserved_after = torch.cuda.memory_reserved(specific_device) # GPU Support Check backend = \\"HIP\\" if torch.version.hip is not None else \\"CUDA\\" # Print Outputs print(f\\"Tensor on default device: {tensor_default.device}\\") print(f\\"Tensor transferred to GPU 1: {tensor_transferred.device}\\") print(f\\"Result tensor on GPU 1: {result}\\") print(f\\"Memory allocated before freeing cache: {allocated_before} bytes\\") print(f\\"Memory reserved before freeing cache: {reserved_before} bytes\\") print(f\\"Memory allocated after freeing cache: {allocated_after} bytes\\") print(f\\"Memory reserved after freeing cache: {reserved_after} bytes\\") print(f\\"Using {backend} for GPU operations\\")"},{"question":"# Clustering Analysis and Evaluation with scikit-learn You are provided with a dataset containing two features. Your task is to perform clustering using different clustering algorithms available in scikit-learn, compare their performances using appropriate evaluation metrics, and visualize the clustering results. # Input: 1. A 2D numpy array `data` of shape `(n_samples, 2)` representing the dataset. 2. An integer `k` indicating the number of clusters (for algorithms that require it). 3. `algorithm` - a string indicating the clustering algorithm to use. The possible values are: - \'kmeans\' - \'agglomerative\' - \'dbscan\' - \'birch\' 4. `evaluation_metric` - a string indicating the evaluation metric to use. The possible values are: - \'silhouette\' - \'calinski_harabasz\' - \'davies_bouldin\' # Output: 1. Clustering labels for the input data. 2. The value of the specified evaluation metric for the clustering. # Constraints: - The input array `data` will have at least 10 samples and not more than 1000 samples. - The value of `k` will be between 2 and 10. - For `dbscan`, use `eps=0.5` and `min_samples=5`. # Detailed Steps: 1. Based on the `algorithm` parameter, apply the specified clustering algorithm to the data: - For \'kmeans\', use `KMeans` from `sklearn.cluster`. - For \'agglomerative\', use `AgglomerativeClustering` from `sklearn.cluster`. - For \'dbscan\', use `DBSCAN` from `sklearn.cluster`. - For \'birch\', use `Birch` from `sklearn.cluster`. 2. Label the data points using the chosen clustering algorithm. 3. Evaluate the clustering result using the specified `evaluation_metric`: - For \'silhouette\', use `silhouette_score` from `sklearn.metrics`. - For \'calinski_harabasz\', use `calinski_harabasz_score` from `sklearn.metrics`. - For \'davies_bouldin\', use `davies_bouldin_score` from `sklearn.metrics`. 4. Return the cluster labels and the value of the specified evaluation metric. # Example: ```python import numpy as np from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, Birch from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score def perform_clustering(data, k, algorithm, evaluation_metric): if algorithm == \'kmeans\': model = KMeans(n_clusters=k) elif algorithm == \'agglomerative\': model = AgglomerativeClustering(n_clusters=k) elif algorithm == \'dbscan\': model = DBSCAN(eps=0.5, min_samples=5) elif algorithm == \'birch\': model = Birch(n_clusters=k) else: raise ValueError(\\"Unknown algorithm specified\\") labels = model.fit_predict(data) if evaluation_metric == \'silhouette\': score = silhouette_score(data, labels) elif evaluation_metric == \'calinski_harabasz\': score = calinski_harabasz_score(data, labels) elif evaluation_metric == \'davies_bouldin\': score = davies_bouldin_score(data, labels) else: raise ValueError(\\"Unknown evaluation metric specified\\") return labels, score # Example usage: data = np.random.rand(100, 2) k = 3 algorithm = \'kmeans\' evaluation_metric = \'silhouette\' labels, score = perform_clustering(data, k, algorithm, evaluation_metric) print(labels) print(score) ``` # Notes: - Ensure you handle any potential issues that might arise with clustering, such as algorithms not supporting certain metrics or not converging. - Visualization is not part of the function but consider how you might plot the results for better understanding.","solution":"import numpy as np from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, Birch from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score def perform_clustering(data, k, algorithm, evaluation_metric): Perform clustering on the given data using the specified algorithm and evaluate the result. Parameters: - data (np.ndarray): 2D array of shape (n_samples, 2) representing the dataset. - k (int): Number of clusters for algorithms that require it. - algorithm (str): The clustering algorithm to use. - evaluation_metric (str): The metric to evaluate the clustering. Returns: - labels (np.ndarray): Cluster labels for the input data. - score (float): The value of the specified evaluation metric. if algorithm == \'kmeans\': model = KMeans(n_clusters=k) elif algorithm == \'agglomerative\': model = AgglomerativeClustering(n_clusters=k) elif algorithm == \'dbscan\': model = DBSCAN(eps=0.5, min_samples=5) elif algorithm == \'birch\': model = Birch(n_clusters=k) else: raise ValueError(\\"Unknown algorithm specified\\") labels = model.fit_predict(data) if evaluation_metric == \'silhouette\': score = silhouette_score(data, labels) elif evaluation_metric == \'calinski_harabasz\': score = calinski_harabasz_score(data, labels) elif evaluation_metric == \'davies_bouldin\': score = davies_bouldin_score(data, labels) else: raise ValueError(\\"Unknown evaluation metric specified\\") return labels, score"},{"question":"Objective: The task is to assess your understanding of file handling, image processing, and custom function implementation in Python, specifically using the `imghdr` module. Problem Statement: You are provided with a directory containing various image files of different formats. Your task is to write a Python function that processes all the image files in the directory and categorizes them based on their format. Additionally, you need to extend the `imghdr` module to recognize a new image format - \\"custom_img\\". Details: 1. Implement a new image type recognition function for the \\"custom_img\\" format. 2. Extend the `imghdr` module to recognize this new format. 3. Write a function `categorize_images(directory)` that scans the provided directory, identifies the image format of each file using the `imghdr.what()` function, and returns a dictionary where the keys are image formats, and the values are lists of file names of that format. Constraints and Requirements: - The \\"custom_img\\" format can be recognized by the presence of a specific byte sequence `b\'CSTM\'` at the beginning of the file. - Use the `imghdr.tests` list to add your custom recognition function. - The function should handle exceptions gracefully and skip files that are not images or cannot be processed. - The directory path will be provided as a string. - You are allowed to assume the directory contains only files (no nested directories). Function Signature: ```python def custom_img_test(h, f): # Your code to implement custom image type recognition def categorize_images(directory: str) -> dict: # Your code to implement the directory scanning and categorization ``` Example Usage: Given a directory with the following files: ``` image1.rgb image2.gif image3.custom_img image4.jpg image5.custom_img ``` The function call `categorize_images(\'path/to/directory\')` should return: ```python { \'rgb\': [\'image1.rgb\'], \'gif\': [\'image2.gif\'], \'custom_img\': [\'image3.custom_img\', \'image5.custom_img\'], \'jpeg\': [\'image4.jpg\'] } ``` Notes: - Do not use any external libraries. Stick to the Python Standard Library for your implementation. - You may assume the directory contains files with unique names.","solution":"import os import imghdr def custom_img_test(h, f): Custom function to test for custom_img format. The \'custom_img\' format is identified by the presence of the byte sequence b\'CSTM\' at the beginning of the file. if h.startswith(b\'CSTM\'): return \'custom_img\' return None # Add the custom test to imghdr\'s test suite imghdr.tests.append(custom_img_test) def categorize_images(directory: str) -> dict: Scans the provided directory, identifies the image format of each file, and categorizes them. Args: - directory (str): Path to the directory containing image files. Returns: - dict: A dictionary where keys are image formats, and values are lists of file names of that format. categorized_images = {} for filename in os.listdir(directory): file_path = os.path.join(directory, filename) # Skip directories if os.path.isdir(file_path): continue try: img_type = imghdr.what(file_path) if img_type: if img_type not in categorized_images: categorized_images[img_type] = [] categorized_images[img_type].append(filename) except: # Exception occurred; skip this file continue return categorized_images"},{"question":"<|Analysis Begin|> The provided documentation is for the `sysconfig` module in Python, which allows access to Python\'s configuration information, such as installation paths and configuration variables. The documentation describes functions for accessing configuration variables (`get_config_vars` and `get_config_var`), installation paths (like `get_scheme_names`, `get_default_scheme`, and `get_paths`), and other helper functions (such as `get_python_version` and `get_platform`). Key aspects covered in the documentation include: 1. **Configuration Variables**: - `get_config_vars()` returns all configuration variables. - `get_config_var(name)` returns a specific configuration variable. 2. **Installation Paths and Schemes**: - `get_scheme_names()` provides the available installation schemes. - `get_default_scheme()` provides the default scheme for the current platform. - `get_paths()` returns all installation paths for a specific scheme or the default for the platform if none is specified. 3. **Additional functions**: - `get_python_version()` returns the major and minor Python version. - `get_platform()` returns information about the current platform. These elements provide the necessary tools to examine and manipulate Python\'s configuration and installation setup, which can be the basis for crafting a challenging programming problem. <|Analysis End|> <|Question Begin|> # Advanced Python Configuration Management You are tasked with building a configuration summary script using Python\'s `sysconfig` module. The script should output detailed configuration information for the Python environment in a structured manner. This information can help developers understand the current Python installation and troubleshoot installation issues. Function Implementation Requirements 1. **Function Name**: `generate_configuration_summary` 2. **Input**: None. 3. **Output**: A dictionary with the following structure: ```python { \\"platform\\": <platform_string>, \\"python_version\\": <python_version_string>, \\"default_scheme\\": <default_scheme_string>, \\"all_schemes\\": <list_of_schemes>, \\"paths\\": { <path_name>: <path_value>, ... }, \\"configuration_variables\\": { <variable_name>: <variable_value>, ... } } ``` Requirements and Constraints 1. The function should use `sysconfig` to fetch the current platform information. 2. The function should use `sysconfig` to fetch the current Python version. 3. The function should retrieve and include the default installation scheme. 4. All supported schemes (as provided by `sysconfig.get_scheme_names()`) should be included. 5. All current installation paths (as provided by `sysconfig.get_paths()`) should be included. 6. All configuration variables (as provided by `sysconfig.get_config_vars()`) should be included. 7. The output dictionary should be correctly structured and all string values should be exact. 8. Ensure efficient retrieval of information and handle any potential errors gracefully, returning sensible defaults or meaningful messages where applicable. Example Usage ```python import pprint def generate_configuration_summary(): # Your implementation goes here # Example call to the function summary = generate_configuration_summary() pprint.pprint(summary) ``` Expected output (values may differ based on the environment): ```python { \'platform\': \'macosx-10.15-x86_64\', \'python_version\': \'3.10\', \'default_scheme\': \'posix_prefix\', \'all_schemes\': [\'posix_prefix\', \'posix_home\', \'posix_user\', \'nt\', \'nt_user\', \'osx_framework_user\'], \'paths\': { \'stdlib\': \'/usr/local/lib/python3.10\', \'platstdlib\': \'/usr/local/lib/python3.10\', \'purelib\': \'/usr/local/lib/python3.10/site-packages\', \'platlib\': \'/usr/local/lib/python3.10/site-packages\', \'include\': \'/usr/local/include\', \'platinclude\': \'/usr/local/include\', \'scripts\': \'/usr/local/bin\', \'data\': \'/usr/local\' }, \'configuration_variables\': { \'Py_ENABLE_SHARED\': \'0\', \'LIBDIR\': \'/usr/local/lib\', \'blah\': None, # if a variable is not found, set its value to None ... } } ``` # Notes - Make sure to handle any exceptions that might occur while accessing `sysconfig` functions and provide meaningful default values. - This task assesses the understanding of how to use the `sysconfig` module to retrieve and structure configuration data programmatically.","solution":"import sysconfig def generate_configuration_summary(): Generates a summary of Python\'s configuration, including platform, version, schemes, paths, and variables. Returns: dict: A dictionary containing the platform, version, and detailed configuration data. try: platform = sysconfig.get_platform() except Exception: platform = \\"Unknown platform\\" try: python_version = sysconfig.get_python_version() except Exception: python_version = \\"Unknown version\\" try: default_scheme = sysconfig.get_default_scheme() except Exception: default_scheme = \\"Unknown default scheme\\" try: all_schemes = list(sysconfig.get_scheme_names()) except Exception: all_schemes = [] try: paths = sysconfig.get_paths() except Exception: paths = {} try: config_vars = sysconfig.get_config_vars() except Exception: config_vars = {} # Structures the configuration summary into a dictionary summary = { \\"platform\\": platform, \\"python_version\\": python_version, \\"default_scheme\\": default_scheme, \\"all_schemes\\": all_schemes, \\"paths\\": paths, \\"configuration_variables\\": config_vars } return summary"},{"question":"Task You are tasked with implementing a clustering analysis on a given dataset using scikit-learn. Your task is to apply and compare the K-Means and DBSCAN clustering algorithms on the dataset. You must also evaluate the performance of these algorithms using appropriate clustering metrics. Objective 1. Implement K-Means and DBSCAN clustering algorithms on the provided dataset. 2. Evaluate and compare the performance of the clustering algorithms using metrics such as Adjusted Rand Index, Silhouette Coefficient, and Davies-Bouldin Index. Input - The input data will be provided as a CSV file with features for clustering. - A JSON configuration file specifying parameters for the clustering algorithms: ```json { \\"k_means\\": { \\"n_clusters\\": 3, \\"random_state\\": 42 }, \\"dbscan\\": { \\"eps\\": 0.5, \\"min_samples\\": 5 } } ``` Output - Clustering results including labels for each algorithm. - Evaluation scores for each clustering algorithm. Constraints - You must use scikit-learn\'s KMeans and DBSCAN implementations. - The dataset can be assumed to be numerical and needs to be preprocessed appropriately. - Ensure the evaluation is meaningful even for unsupervised data (without ground truth labels). Requirements 1. Implement a function `load_data` to load the dataset. 2. Implement a function `configure_algorithms` to parse the JSON configuration. 3. Implement a function `apply_clustering` to apply the clustering algorithms. 4. Implement a function `evaluate_clustering` to evaluate the performance of the clustering. Once you have the functions outlined, create a final script `main.py` that ties everything together and outputs the results. ```python import json import pandas as pd from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score def load_data(file_path): # Load the dataset from the given CSV file return pd.read_csv(file_path) def configure_algorithms(config_file): # Load the JSON configuration file and return the parameters with open(config_file, \'r\') as file: config = json.load(file) return config def apply_clustering(data, k_means_params, dbscan_params): # Apply K-Means clustering kmeans = KMeans(n_clusters=k_means_params[\'n_clusters\'], random_state=k_means_params[\'random_state\']) kmeans_labels = kmeans.fit_predict(data) # Apply DBSCAN clustering dbscan = DBSCAN(eps=dbscan_params[\'eps\'], min_samples=dbscan_params[\'min_samples\']) dbscan_labels = dbscan.fit_predict(data) return kmeans_labels, dbscan_labels def evaluate_clustering(data, true_labels, predicted_labels): # Adjusted Rand Index ari = adjusted_rand_score(true_labels, predicted_labels) if true_labels is not None else \'N/A\' # Silhouette Coefficient silhouette = silhouette_score(data, predicted_labels) # Davies-Bouldin Index davies_bouldin = davies_bouldin_score(data, predicted_labels) return {\'ARI\': ari, \'Silhouette\': silhouette, \'Davies-Bouldin\': davies_bouldin} def main(data_file, config_file, true_labels_file=None): # Load data data = load_data(data_file) # Load true labels if provided true_labels = load_data(true_labels_file) if true_labels_file else None # Configure algorithms config = configure_algorithms(config_file) # Apply clustering kmeans_labels, dbscan_labels = apply_clustering(data, config[\'k_means\'], config[\'dbscan\']) # Evaluate clustering kmeans_eval = evaluate_clustering(data, true_labels, kmeans_labels) dbscan_eval = evaluate_clustering(data, true_labels, dbscan_labels) # Print the results print(f\\"K-Means Evaluation: {kmeans_eval}\\") print(f\\"DBSCAN Evaluation: {dbscan_eval}\\") if __name__ == \'__main__\': main(\'data.csv\', \'config.json\', true_labels_file=\'true_labels.csv\') ``` # Notes: - Adjust the file paths and parameters as needed. - Assume unsupervised configuration where `true_labels` might not be provided. - Ensure the script is modular and handles exceptions gracefully.","solution":"import json import pandas as pd from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score def load_data(file_path): Load the dataset from the given CSV file. return pd.read_csv(file_path) def configure_algorithms(config_file): Load the JSON configuration file and return the parameters. with open(config_file, \'r\') as file: config = json.load(file) return config def apply_clustering(data, k_means_params, dbscan_params): Apply K-Means and DBSCAN clustering algorithms to the data. # Apply K-Means clustering kmeans = KMeans(n_clusters=k_means_params[\'n_clusters\'], random_state=k_means_params[\'random_state\']) kmeans_labels = kmeans.fit_predict(data) # Apply DBSCAN clustering dbscan = DBSCAN(eps=dbscan_params[\'eps\'], min_samples=dbscan_params[\'min_samples\']) dbscan_labels = dbscan.fit_predict(data) return kmeans_labels, dbscan_labels def evaluate_clustering(data, predicted_labels): Evaluate the performance of the clustering algorithms. # Silhouette Coefficient silhouette = silhouette_score(data, predicted_labels) # Davies-Bouldin Index davies_bouldin = davies_bouldin_score(data, predicted_labels) return { \'Silhouette\': silhouette, \'Davies-Bouldin\': davies_bouldin } def main(data_file, config_file): Main function to run the clustering analysis. # Load data data = load_data(data_file) # Configure algorithms config = configure_algorithms(config_file) # Apply clustering kmeans_labels, dbscan_labels = apply_clustering(data, config[\'k_means\'], config[\'dbscan\']) # Evaluate clustering kmeans_eval = evaluate_clustering(data, kmeans_labels) dbscan_eval = evaluate_clustering(data, dbscan_labels) # Print the results print(f\\"K-Means Evaluation: {kmeans_eval}\\") print(f\\"DBSCAN Evaluation: {dbscan_eval}\\") if __name__ == \'__main__\': main(\'data.csv\', \'config.json\')"},{"question":"Advanced Seaborn Visualizations Objective: Your task is to demonstrate your understanding of the `seaborn.objects` module by creating a series of visualizations that explore the distribution of diamond prices across different cuts and clarities. Problem Statement: Using the `diamonds` dataset available in the `seaborn` library, perform the following tasks: 1. Load the `diamonds` dataset and display the first few rows to understand its structure. 2. Create a histogram that shows the distribution of diamond prices (`price`) on a logarithmic scale. 3. Create a stacked histogram to visualize the distribution of diamond prices across different cuts. 4. Create a customized bar chart that shows the distribution of diamond prices, where: - The bars are unfilled with a specific edge color of your choice. - Adjust the width of the bars for better visualization, particularly for the \'Ideal\' cut. Requirements: 1. **Input and Output**: - **Input**: Load the dataset using `seaborn`. - **Output**: Generate and display the required visualizations. 2. **Constraints and Limitations**: - You are required to use `seaborn` and its `objects` module. - Implement proper customization techniques as demonstrated in the provided documentation. 3. **Instructions**: - Make sure to interpret the visualizations by adding appropriate titles and axis labels. - All code must be written in Python and should be functional within a Jupyter notebook. Example Details: Here is an example to help you understand how to get started: ```python import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") # Task 1: Display the first few rows of the dataset print(diamonds.head()) # Task 2: Create a histogram of diamond prices on a logarithmic scale p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(), so.Hist()).plot() # Task 3: Create a stacked histogram for the distribution of diamond prices across different cuts p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\").plot() # Task 4: Customize bar chart for diamond prices with unfilled bars and specific edge color hist = so.Hist(binwidth=.075, binrange=(2, 5)) ( p.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5), so.Hist()) .add( so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"cut == \'Ideal\'\\") ) ).plot() ``` Additional Notes: - Be sure to explore the dataset and understand the columns and their characteristics before proceeding with visualizations. - The example provided is just a starting point; ensure you add necessary customizations and improvements as per the problem\'s requirements.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def load_and_preview_diamonds(): Load the diamonds dataset and return the first few rows. diamonds = sns.load_dataset(\\"diamonds\\") return diamonds.head() def create_histogram_log_price(): Create a histogram of diamond prices on a logarithmic scale. diamonds = sns.load_dataset(\\"diamonds\\") p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(), so.Hist()).label(title=\\"Histogram of Diamond Prices (Log Scale)\\", xlabel=\\"Price (log scale)\\", ylabel=\\"Count\\") p.show() def create_stacked_histogram_price_cut(): Create a stacked histogram for the distribution of diamond prices across different cuts. diamonds = sns.load_dataset(\\"diamonds\\") p = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"cut\\").label(title=\\"Stacked Histogram of Diamond Prices by Cut\\", xlabel=\\"Price (log scale)\\", ylabel=\\"Count\\") p.show() def create_custom_bar_chart(): Custom bar chart for diamond prices with unfilled bars and specific edge color. diamonds = sns.load_dataset(\\"diamonds\\") p = so.Plot(diamonds, \\"price\\", color=\\"cut\\").scale(x=\\"log\\") hist = so.Hist(binwidth=.075, binrange=(2, 5)) p.add(so.Bars(fill=False, edgecolor=\\"C0\\", edgewidth=1.5), hist) p.add(so.Bars(color=\\".9\\", width=.5), hist, data=diamonds.query(\\"cut == \'Ideal\'\\")) p.label(title=\\"Customized Bar Chart of Diamond Prices\\", xlabel=\\"Price (log scale)\\", ylabel=\\"Count\\") p.show()"},{"question":"**Question: Utilizing PyTorch Configuration Functions** *Objective:* Demonstrate proficiency in using PyTorch\'s configuration submodule and its functions to extract and display configuration and parallel computation details. *Description:* You are tasked with writing a Python function that uses the PyTorch `torch.__config__` submodule to retrieve and display the current configuration and parallel computation information of PyTorch. *Function Signature:* ```python def display_pytorch_config_info() -> None: This function retrieves and displays the current configuration and parallel computation details of PyTorch. ``` *Requirements:* 1. Import the `torch.__config__` module. 2. Retrieve the current configuration using the `show` function and display it. 3. Retrieve the parallel computation information using the `parallel_info` function and display it. 4. Format the output in a user-friendly manner (e.g., add headers, separate sections with lines, etc.). *Expected Output Format:* The function should print the following sections: - A clear header indicating it is displaying PyTorch configuration information. - The output of the `show` function. - A clear header indicating it is displaying parallel computation information. - The output of the `parallel_info` function. An example output might look like: ``` PyTorch Configuration Information: ----------------------------------- <output from torch.__config__.show()> PyTorch Parallel Computation Information: ----------------------------------------- <output from torch.__config__.parallel_info()> ``` *Constraints:* - Ensure the function gracefully handles any exceptions that might occur during the information retrieval process. - The solution should be implemented without using any external libraries other than PyTorch. *Implementation Notes:* - Use appropriate print statements to format the output. - Ensure the function does not return any values but only prints the desired information. ```python # Sample Implementation (for reference, not to be used in your submission) def display_pytorch_config_info(): import torch.__config__ print(\\"PyTorch Configuration Information:\\") print(\\"-----------------------------------\\") torch.__config__.show() print(\\"PyTorch Parallel Computation Information:\\") print(\\"-----------------------------------------\\") torch.__config__.parallel_info() ``` Write your implementation below: ```python # Your implementation goes here ```","solution":"def display_pytorch_config_info() -> None: This function retrieves and displays the current configuration and parallel computation details of PyTorch. import torch.__config__ try: print(\\"PyTorch Configuration Information:\\") print(\\"-----------------------------------\\") torch.__config__.show() print(\\"nPyTorch Parallel Computation Information:\\") print(\\"-----------------------------------------\\") torch.__config__.parallel_info() except Exception as e: print(f\\"An error occurred while retrieving PyTorch configuration information: {e}\\")"},{"question":"**Objective:** Your task is to implement a function that processes Unix user account information using the `pwd` module. Specifically, you will need to retrieve and manipulate user data to perform specific operations. **Function Details:** Write a function `get_user_info(user_id: int) -> dict` that takes a numeric user ID as input and returns a dictionary containing user information. The dictionary should have the following format: ```python { \\"login_name\\": str, \\"user_id\\": int, \\"group_id\\": int, \\"user_name\\": str, \\"home_directory\\": str, \\"shell\\": str } ``` **Parameters:** - `user_id` (int): The numeric user ID for which the data needs to be retrieved. **Returns:** - A dictionary with the user information as described above. **Constraints:** - The function should handle the scenario where the user ID does not exist by raising a `KeyError`. - The function must use the `pwd.getpwuid(uid)` method to retrieve user data. - Ensure the data types in the returned dictionary match the specifications. **Performance Requirements:** - The function should retrieve the user information promptly without significant delay, assuming typical sizes of Unix password databases. **Example:** ```python # Assume the UID 1001 corresponds to the following user data: # pw_name : \\"johndoe\\" # pw_passwd : \\"x\\" # pw_uid : 1001 # pw_gid : 1001 # pw_gecos : \\"John Doe\\" # pw_dir : \\"/home/johndoe\\" # pw_shell : \\"/bin/bash\\" output = get_user_info(1001) print(output) ``` Expected output: ```python { \\"login_name\\": \\"johndoe\\", \\"user_id\\": 1001, \\"group_id\\": 1001, \\"user_name\\": \\"John Doe\\", \\"home_directory\\": \\"/home/johndoe\\", \\"shell\\": \\"/bin/bash\\" } ``` **Note:** - You do not need to handle encrypted passwords or other sensitive information. - Focus on correctly using the `pwd` module functions to access and format the required data.","solution":"import pwd def get_user_info(user_id: int) -> dict: Retrieve and return user information for the given user ID. Args: - user_id (int): The numeric user ID for which the data needs to be retrieved. Returns: - dict: A dictionary containing user information. Raises: - KeyError: If the user ID does not exist. try: pwd_entry = pwd.getpwuid(user_id) return { \\"login_name\\": pwd_entry.pw_name, \\"user_id\\": pwd_entry.pw_uid, \\"group_id\\": pwd_entry.pw_gid, \\"user_name\\": pwd_entry.pw_gecos, \\"home_directory\\": pwd_entry.pw_dir, \\"shell\\": pwd_entry.pw_shell } except KeyError: raise KeyError(f\\"User ID {user_id} does not exist.\\")"},{"question":"Coding Assessment Question # Objective In this task, you will create a visualization using the Seaborn library, demonstrating your understanding of loading datasets, creating plots, and using various transforms to adjust the appearance of categorical plot elements. # Problem Statement You are given the `tips` dataset from the Seaborn library, which contains information about restaurant bills, including the total bill amount, tip amount, sex, smoker status, day, time, and size of the party. Using this dataset, perform the following tasks: 1. Load the `tips` dataset using Seabornâ€™s `load_dataset` function and ensure the `time` column is of the string datatype. 2. Create a bar plot to show the total count of observations for each day (`day` column), colored by the time of day (`time` column). Use the `Dodge` transform to separate the bars. 3. Adjust the bar widths and positions to avoid overlap and ensure empty categories are dropped from the plot. 4. Add a slight gap between the dodged bars for better visualization. 5. Finally, create another plot to show the total sum of `total_bill` for each day, colored by sex and dodged by smoker status. Ensure to handle empty and dodged categories properly. # Requirements - Use the `seaborn.objects` module and its plotting functionalities such as `so.Plot`, `so.Bar`, `so.Count`, `so.Agg`, `so.Dodge`, and other relevant transforms. - Ensure the plots are well-labeled and provide a clear legend for all categories. # Constraints - You must use the functions and methods demonstrated in the provided documentation. - The implementation should be efficient and should not take more than a few seconds to execute. # Output - Two plots: 1. A bar plot showing the total count of observations for each day, colored by time and dodged accurately. 2. A sum plot of `total_bill` for each day, colored by sex and dodged by smoker status with proper handling of empty and dodged categories. # Example Code Here is a structure to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset and ensure the time column is a string tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Step 2: Create the first bar plot plot1 = ( so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge()) .add(so.Bar(), so.Count(), so.Dodge(empty=\\"drop\\")) ) # Step 3: Create the second plot showing the sum of total_bill plot2 = ( so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1), fill=\\"smoker\\") ) # Display the plots plot1.show() plot2.show() ``` Submit your solution as a Jupyter notebook or Python script containing the required plots.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plots(): # Step 1: Load the dataset and ensure the time column is a string tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Step 2: Create the first bar plot plot1 = ( so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge()) ) # Step 3: Create the second plot showing the sum of total_bill plot2 = ( so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1), fill=\\"smoker\\") ) return plot1, plot2"},{"question":"**Coding Task: Implement and Compare Multiclass Classification Strategies** # Objective In this task, you will implement and compare different multiclass classification strategies using scikit-learn. You will use the Iris dataset, a common dataset for classification tasks, to train and evaluate these classifiers. # Problem Statement The Iris dataset consists of 150 samples from three species of Iris flowers, with four features (sepal length, sepal width, petal length, petal width) for each sample. The goal is to classify the samples into one of the three species. You need to follow these steps: 1. **Load the Iris Dataset**: Use `sklearn.datasets.load_iris` to load the dataset. 2. **Split the Data**: Split the dataset into a training set (80%) and a testing set (20%). 3. **Implement Multiple Multiclass Classification Strategies**: - One-Vs-Rest using `OneVsRestClassifier`. - One-Vs-One using `OneVsOneClassifier`. - Error-Correcting Output Codes using `OutputCodeClassifier`. 4. **Train and Evaluate**: For each strategy, use a linear Support Vector Classifier (LinearSVC) as the base classifier. Train the classifiers using the training data and evaluate their performance on the testing data. 5. **Performance Metrics**: Calculate the accuracy and F1 score for each classifier. 6. **Compare Results**: Print a comparison of the performance metrics for each strategy. # Input - No input is required from the user. You will programmatically load the Iris dataset. # Output - The accuracy and F1 score for each multiclass classification strategy. # Constraints - Use `random_state=0` for any randomization to ensure reproducibility. - Use `train_test_split` from `sklearn.model_selection` to split the data. # Implementation Notes - Ensure that you handle any warnings or convergence issues that may arise from using LinearSVC. - You can use `classification_report` from `sklearn.metrics` to calculate accuracy and F1 score. # Sample Code Template ```python from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report # Step 1: Load Iris dataset def load_data(): data = datasets.load_iris() X = data.data y = data.target return X, y # Step 2: Split data def split_data(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) return X_train, X_test, y_train, y_test # Step 3: Train and evaluate classifiers def train_and_evaluate(X_train, X_test, y_train, y_test): classifiers = { \\"OneVsRest\\": OneVsRestClassifier(LinearSVC(random_state=0)), \\"OneVsOne\\": OneVsOneClassifier(LinearSVC(random_state=0)), \\"OutputCode\\": OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0) } results = {} for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) report = classification_report(y_test, y_pred, output_dict=True) results[name] = { \\"accuracy\\": report[\\"accuracy\\"], \\"f1_score\\": report[\\"weighted avg\\"][\\"f1-score\\"] } return results # Step 4: Compare results def compare_results(results): for name, metrics in results.items(): print(f\\"Strategy: {name}\\") print(f\\"Accuracy: {metrics[\'accuracy\']}\\") print(f\\"F1 Score: {metrics[\'f1_score\']}\\") print() # Main function to execute the steps if __name__ == \\"__main__\\": X, y = load_data() X_train, X_test, y_train, y_test = split_data(X, y) results = train_and_evaluate(X_train, X_test, y_train, y_test) compare_results(results) ``` # Expected Output The output should display the accuracy and F1 score for each of the three classification strategies: ``` Strategy: OneVsRest Accuracy: <calculated_accuracy> F1 Score: <calculated_f1_score> Strategy: OneVsOne Accuracy: <calculated_accuracy> F1 Score: <calculated_f1_score> Strategy: OutputCode Accuracy: <calculated_accuracy> F1 Score: <calculated_f1_score> ``` # Submission Submit your implementation as a `.py` file.","solution":"from sklearn import datasets from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report, accuracy_score, f1_score # Step 1: Load Iris dataset def load_data(): data = datasets.load_iris() X = data.data y = data.target return X, y # Step 2: Split data def split_data(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) return X_train, X_test, y_train, y_test # Step 3: Train and evaluate classifiers def train_and_evaluate(X_train, X_test, y_train, y_test): classifiers = { \\"OneVsRest\\": OneVsRestClassifier(LinearSVC(random_state=0)), \\"OneVsOne\\": OneVsOneClassifier(LinearSVC(random_state=0)), \\"OutputCode\\": OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0) } results = {} for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) acc = accuracy_score(y_test, y_pred) f1 = f1_score(y_test, y_pred, average=\'weighted\') results[name] = { \\"accuracy\\": acc, \\"f1_score\\": f1 } return results # Step 4: Compare results def compare_results(results): for name, metrics in results.items(): print(f\\"Strategy: {name}\\") print(f\\"Accuracy: {metrics[\'accuracy\']}\\") print(f\\"F1 Score: {metrics[\'f1_score\']}\\") print() # Main function to execute the steps if __name__ == \\"__main__\\": X, y = load_data() X_train, X_test, y_train, y_test = split_data(X, y) results = train_and_evaluate(X_train, X_test, y_train, y_test) compare_results(results)"},{"question":"# Question: Dynamic Log File Parser You are given a multi-line string representing log file entries for a server. Each log entry is on a new line and includes a timestamp, log level, and a message. Here is an example of the log file: ``` 2023-10-10T13:55:30 ERROR An error occurred in the system 2023-10-10T14:00:45 WARN A potential issue was detected 2023-10-10T14:05:22 INFO System rebooted successfully 2023-10-10T14:10:05 DEBUG Verbose debugging message ``` Your task is to write a function `parse_logs(logs: str) -> List[Dict[str, str]]` that parses each line of the log file and returns a list of dictionaries, where each dictionary represents a log entry with the following keys: - `\\"timestamp\\"`: the timestamp of the log entry. - `\\"level\\"`: the log level (e.g., ERROR, WARN, INFO, DEBUG). - `\\"message\\"`: the log message. # Input - A multi-line string `logs` consisting of several log entries separated by newline characters. # Output - A list of dictionaries, each containing the parsed `\\"timestamp\\"`, `\\"level\\"`, and `\\"message\\"` from the log entries. # Constraints - The `timestamp` follows the ISO 8601 format: `YYYY-MM-DDTHH:MM:SS`. - The `level` is always one of `ERROR`, `WARN`, `INFO`, `DEBUG`. - The `message` can be any text that follows the log level. # Example ```python logs = 2023-10-10T13:55:30 ERROR An error occurred in the system 2023-10-10T14:00:45 WARN A potential issue was detected 2023-10-10T14:05:22 INFO System rebooted successfully 2023-10-10T14:10:05 DEBUG Verbose debugging message print(parse_logs(logs)) ``` Expected Output ```python [ { \\"timestamp\\": \\"2023-10-10T13:55:30\\", \\"level\\": \\"ERROR\\", \\"message\\": \\"An error occurred in the system\\" }, { \\"timestamp\\": \\"2023-10-10T14:00:45\\", \\"level\\": \\"WARN\\", \\"message\\": \\"A potential issue was detected\\" }, { \\"timestamp\\": \\"2023-10-10T14:05:22\\", \\"level\\": \\"INFO\\", \\"message\\": \\"System rebooted successfully\\" }, { \\"timestamp\\": \\"2023-10-10T14:10:05\\", \\"level\\": \\"DEBUG\\", \\"message\\": \\"Verbose debugging message\\" } ] ``` # Requirements - Implement the function `parse_logs` in Python. - Use the `re` module to parse the log entries efficiently. - Ensure your solution handles varying message lengths and different log levels correctly. # Notes - You should assume that the logs are well-formed and do not contain any malformed entries. - Make sure to test your function with the given example to verify its correctness.","solution":"import re from typing import List, Dict def parse_logs(logs: str) -> List[Dict[str, str]]: Parses a multiline log string and returns a list of dictionaries with keys: - \'timestamp\': the timestamp of the log entry. - \'level\': the log level (e.g., ERROR, WARN, INFO, DEBUG). - \'message\': the log message. log_entries = [] log_pattern = re.compile(r\\"^(?P<timestamp>d{4}-d{2}-d{2}Td{2}:d{2}:d{2}) (?P<level>ERROR|WARN|INFO|DEBUG) (?P<message>.+)\\") for line in logs.strip().split(\'n\'): match = log_pattern.match(line) if match: log_entries.append(match.groupdict()) return log_entries"},{"question":"**Problem Statement:** You are tasked with implementing a function that reads specific lines from a text file and caches the content to avoid redundant file reads. Your function should leverage a caching mechanism similar to what is provided by the `linecache` module in Python. **Function Signature:** ```python def get_line_from_file(filepath: str, lineno: int, cache: dict) -> str: pass ``` **Input:** - `filepath` (str): The path to the file from which the line should be read. - `lineno` (int): The line number to read from the file (1-based index). - `cache` (dict): A dictionary used to cache the content of files. The key is the file path, and the value is a list of lines in the file. **Output:** - str: The content of the specified line. If the line number is invalid (less than 1 or greater than the total number of lines in the file) or the file cannot be read, return an empty string. **Constraints:** - You may assume the file encoding is UTF-8. - Do not use external libraries other than Python\'s built-in modules. - Optimize the function to minimize redundant file reads. Use caching to optimize performance. **Example:** ```python # Assume the content of \\"example.txt\\" is as follows: # Line 1: Hello, World! # Line 2: This is a sample text file. # Line 3: Goodbye! filepath = \\"example.txt\\" cache = {} print(get_line_from_file(filepath, 1, cache)) # Output: \\"Hello, World!\\" print(get_line_from_file(filepath, 3, cache)) # Output: \\"Goodbye!\\" print(get_line_from_file(filepath, 2, cache)) # Output: \\"This is a sample text file.\\" print(get_line_from_file(filepath, 4, cache)) # Output: \\"\\" print(get_line_from_file(filepath, 1, cache)) # Output: \\"Hello, World!\\" (this should be fetched from the cache) ``` **Explanation:** - The first call to `get_line_from_file` reads the first line from the file and stores the file content in the cache. - Subsequent calls to `get_line_from_file` use the cached content if available, reducing redundant file reads. - If an invalid line number is provided, or if the file cannot be read, the function returns an empty string. Implement this function while ensuring optimal performance through effective use of caching.","solution":"def get_line_from_file(filepath: str, lineno: int, cache: dict) -> str: Reads a specific line from a file, utilizing a cache to minimize redundant reads. Args: filepath (str): The path to the file. lineno (int): The line number to read (1-based index). cache (dict): A dictionary used to cache the content of files. Returns: str: The content of the specified line, or an empty string if the line is invalid. # Check if the file contents are already cached. if filepath not in cache: try: with open(filepath, \'r\', encoding=\'utf-8\') as f: cache[filepath] = f.readlines() except (OSError, IOError): return \\"\\" # Return an empty string if the file cannot be read. # Fetch the cached content. lines = cache[filepath] # Check if the requested line number is within the valid range. if lineno < 1 or lineno > len(lines): return \\"\\" return lines[lineno - 1].rstrip(\'n\')"},{"question":"**Coding Question: Detecting Malignant Tumors Using Support Vector Machines** # Objective Your task is to build, train, and evaluate a Support Vector Machine (SVM) model using scikit-learn to classify tumors as malignant or benign using the \'breast_cancer\' dataset provided by scikit-learn. # Dataset Use the `load_breast_cancer` dataset from `sklearn.datasets`. This dataset includes features extracted from breast cancer tumor samples and whether they are malignant. # Instructions 1. **Load the Dataset**: - Use the `load_breast_cancer` function from `sklearn.datasets` to load the data. - Split the dataset into training and testing sets with a ratio of 80:20. 2. **Preprocess the Data**: - Standardize the feature set using `StandardScaler` from `sklearn.preprocessing`. 3. **Model Training**: - Initialize a Support Vector Machine classifier with a Gaussian (RBF) kernel. - Perform hyperparameter tuning to find the best `C` and `gamma` values using `GridSearchCV` from `sklearn.model_selection`. - Train the model on the training set. 4. **Model Evaluation**: - Evaluate the model on the testing set using accuracy, precision, recall, and F1-score. - Plot the confusion matrix to visualize the classification performance. # Expected Function Implementation Implement the function `train_and_evaluate_svm()`. You do not need to return anything from this function, but print the required evaluation metrics and plot the confusion matrix. ```python def train_and_evaluate_svm(): # Load the dataset from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay import matplotlib.pyplot as plt # Load data data = load_breast_cancer() X, y = data.data, data.target # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize the SVM classifier svm = SVC(kernel=\'rbf\') # Perform hyperparameter tuning parameters = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} clf = GridSearchCV(svm, parameters, cv=5) clf.fit(X_train, y_train) # Best model best_model = clf.best_estimator_ # Predict on the test set y_pred = best_model.predict(X_test) # Calculate evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) # Print evaluation metrics print(f\\"Accuracy: {accuracy:.2f}\\") print(f\\"Precision: {precision:.2f}\\") print(f\\"Recall: {recall:.2f}\\") print(f\\"F1 Score: {f1:.2f}\\") # Plot the confusion matrix cm = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names) disp.plot() plt.show() # Run the function to test train_and_evaluate_svm() ``` # Constraints - Use only the libraries and functions specified. - Aim to keep the function execution time under 2 minutes. # Evaluation Criteria - Correctness of data loading and preprocessing. - Effective hyperparameter tuning and model training. - Accuracy and robustness of the evaluation metrics. - Quality and clarity of the confusion matrix plot.","solution":"def train_and_evaluate_svm(): # Load the dataset from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay import matplotlib.pyplot as plt # Load data data = load_breast_cancer() X, y = data.data, data.target # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize the SVM classifier svm = SVC(kernel=\'rbf\') # Perform hyperparameter tuning parameters = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} clf = GridSearchCV(svm, parameters, cv=5) clf.fit(X_train, y_train) # Best model best_model = clf.best_estimator_ # Predict on the test set y_pred = best_model.predict(X_test) # Calculate evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) # Print evaluation metrics print(f\\"Accuracy: {accuracy:.2f}\\") print(f\\"Precision: {precision:.2f}\\") print(f\\"Recall: {recall:.2f}\\") print(f\\"F1 Score: {f1:.2f}\\") # Plot the confusion matrix cm = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names) disp.plot() plt.show() # Run the function to test train_and_evaluate_svm()"},{"question":"You are tasked with profiling a piece of Python code to identify performance bottlenecks and optimize its runtime. Given a list of integers, the code performs multiple operations, including sorting, calculating the sum, and finding the max value. Your job is to: 1. Implement a function `perform_operations` which takes a list of integers as an argument and performs the following operations: - Sort the list. - Calculate the sum of the elements in the list. - Find the maximum value in the list. 2. Profile the `perform_operations` function using the `cProfile` module. 3. Save the profiling results to a file. 4. Use the `pstats` module to read the profiling results and print a report sorted by cumulative time. # Function Signature ```python def perform_operations(numbers: List[int]) -> Tuple[List[int], int, int]: ... ``` # Input - `numbers`: A list of integers (List[int]), with a length between 1 and 10^6. # Output - A tuple containing: - The sorted list of integers. - The sum of the integers. - The maximum integer in the list. # Constraints - The function should handle large input sizes efficiently. - The profiling report should be detailed, showing the function calls and their cumulative times. # Performance Requirements - The function should be implemented in a way that makes it feasible to handle up to 10^6 integers. # Example ```python from typing import List, Tuple def perform_operations(numbers: List[int]) -> Tuple[List[int], int, int]: sorted_numbers = sorted(numbers) total_sum = sum(numbers) max_value = max(numbers) return sorted_numbers, total_sum, max_value # Profiling the function import cProfile import pstats def profile_code(): import random numbers = random.sample(range(1, 1000000), 1000000) # Profile and save results to a file profiler = cProfile.Profile() profiler.enable() perform_operations(numbers) profiler.disable() profiler.dump_stats(\'profiling_results.prof\') # Use `pstats` to read and print the profiling results with open(\'profiling_results.txt\', \'w\') as f: stats = pstats.Stats(\'profiling_results.prof\', stream=f) stats.strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE).print_stats() profile_code() ``` # Notes - Use the `cProfile` module to gather profiling statistics. - Save the profile to a file named \'profiling_results.prof\'. - Use the `pstats` module to read the profile data and print the results sorted by cumulative time. - Ensure the printed report is saved to a file named \'profiling_results.txt\'. This exercise will demonstrate your ability to effectively use the `cProfile` and `pstats` modules for profiling and analyzing Python code to optimize performance.","solution":"from typing import List, Tuple def perform_operations(numbers: List[int]) -> Tuple[List[int], int, int]: # Perform required operations sorted_numbers = sorted(numbers) total_sum = sum(numbers) max_value = max(numbers) return sorted_numbers, total_sum, max_value # Profiling the function import cProfile import pstats def profile_code(): import random # Generate a list of 1,000,000 random integers for profiling numbers = random.sample(range(1, 1000001), 1000000) # Profile and save results to a file profiler = cProfile.Profile() profiler.enable() perform_operations(numbers) profiler.disable() profiler.dump_stats(\'profiling_results.prof\') # Use `pstats` to read and print the profiling results with open(\'profiling_results.txt\', \'w\') as f: stats = pstats.Stats(\'profiling_results.prof\', stream=f) stats.strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE).print_stats() profile_code()"},{"question":"# XML Processing and Security **Objective:** In this assessment, you will work with XML data using Python\'s `xml.etree.ElementTree` module while ensuring security against common XML vulnerabilities. **Problem Statement:** You are provided with an XML document containing book information. Your task is to write a Python function `parse_books(xml_string: str) -> dict` that parses the provided XML string and returns a dictionary with book titles as keys and the corresponding author names as values. Ensure that your function is secure against the vulnerabilities mentioned in the provided documentation. **Function Signature:** ```python def parse_books(xml_string: str) -> dict: ``` **Expected Input and Output Formats:** - **Input:** - `xml_string`: A string containing XML data in the following format: ```xml <library> <book> <title>Book Title 1</title> <author>Author Name 1</author> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> </book> </library> ``` - **Output:** - A dictionary where each key is a book title and each value is the corresponding author name. For the provided example, the output should be: ```python { \\"Book Title 1\\": \\"Author Name 1\\", \\"Book Title 2\\": \\"Author Name 2\\" } ``` **Constraints:** 1. The XML string will always be well-formed. 2. Your code should handle any potential security risks mentioned in the documentation. 3. You are allowed to use only the standard library modules `xml.etree.ElementTree`. **Performance Requirements:** - Your solution should efficiently parse the XML data with a time complexity of O(n), where n is the size of the input string. **Example Usage:** ```python xml_data = <library> <book> <title>Python Programming</title> <author>John Doe</author> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> </book> </library> expected_output = { \\"Python Programming\\": \\"John Doe\\", \\"Advanced Python\\": \\"Jane Smith\\" } result = parse_books(xml_data) assert result == expected_output ``` **Notes:** - Ensure your solution includes security measures to prevent attacks such as entity expansion and others listed in the documentation. **Hint:** Consider using the `defusedxml` package if necessary to ensure your solution is secure.","solution":"import xml.etree.ElementTree as ET def parse_books(xml_string: str) -> dict: Parse the given XML string to extract book titles and author names. Returns a dictionary with book titles as keys and author names as values. book_dict = {} try: # Parse the XML string root = ET.fromstring(xml_string) # Iterate over each book in the library for book in root.findall(\'book\'): title = book.find(\'title\').text author = book.find(\'author\').text book_dict[title] = author except ET.ParseError: raise ValueError(\\"Invalid XML data\\") return book_dict"},{"question":"**Objective: Custom JSON Encoder and Decoder** Design and implement custom JSON serialization and deserialization for complex numbers. In this task, you will: 1. Create a custom `JSONEncoder` to serialize complex numbers to JSON. 2. Implement a custom decoding function to deserialize JSON strings back to complex numbers using `object_hook`. **Requirements**: - Implement a class `ComplexEncoder` that inherits from `json.JSONEncoder`: - Override the `default` method to handle instances of complex numbers by converting them to a dictionary with `real` and `imag` parts. - Implement a function `complex_decoder` that parses JSON strings and converts dictionaries with `real` and `imag` keys back to complex numbers. **Function Signatures**: ```python import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): # Implement this method ``` ```python def complex_decoder(dct): # Implement this function ``` **Input and Output**: 1. **Test Serialization**: ```python complex_num = 3 + 4j json_str = json.dumps(complex_num, cls=ComplexEncoder) print(json_str) # Expected output: \'{\\"real\\": 3, \\"imag\\": 4}\' ``` 2. **Test Deserialization**: ```python json_str = \'{\\"real\\": 3, \\"imag\\": 4}\' complex_num = json.loads(json_str, object_hook=complex_decoder) print(complex_num) # Expected output: (3+4j) ``` **Constraints**: - Ensure the custom JSON encoder can handle other standard data types correctly. - During deserialization, only convert dictionaries with `real` and `imag` keys to complex numbers; other dictionaries should remain unchanged. Implement your solutions below and ensure your functions pass the given test cases.","solution":"import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def complex_decoder(dct): if \\"real\\" in dct and \\"imag\\" in dct: try: return complex(dct[\\"real\\"], dct[\\"imag\\"]) except (ValueError, TypeError): pass # In case real or imag values are not suitable for complex number return dct"},{"question":"# Python Coding Assessment Question Objective: Demonstrate understanding of tuple manipulation and struct sequence creation in Python. Problem Statement: You are given the following tasks to work with Python tuples and named tuples: 1. **Tuple Manipulations:** - Implement a function `manipulate_tuple` that takes a tuple of integers and an integer \'n\'. The function should: - Create a new tuple that is a reversed version of the input tuple. - Resize the tuple to have exactly \'n\' elements. If the tuple has fewer than \'n\' elements, fill the remaining elements with `None`, and if it has more than \'n\' elements, truncate the extra elements. - Return the new resized and reversed tuple. 2. **Named Tuple Creation:** - Implement a function `create_named_tuple` that creates a named tuple type called `Point` with fields \'x\', \'y\', and \'z\' and returns instances of this named tuple for given x, y, and z values. The function should: - Take a list of triples representing coordinates [(x1, y1, z1), (x2, y2, z2), ...] and return a list of `Point` named tuples corresponding to these coordinates. Input: - `manipulate_tuple` function: - A tuple of integers, `input_tuple`. - An integer, `n`. Example Input: ```python input_tuple = (1, 2, 3, 4, 5) n = 7 ``` - `create_named_tuple` function: - A list of triples representing coordinates. Example Input: ```python coordinates = [(1, 2, 3), (4, 5, 6), (7, 8, 9)] ``` Output: - `manipulate_tuple` function: - A tuple resized to length `n` with the elements reversed. Example Output: ```python (5, 4, 3, 2, 1, None, None) ``` - `create_named_tuple` function: - A list of named tuples `Point` with fields \'x\', \'y\', and \'z\'. Example Output: ```python [Point(x=1, y=2, z=3), Point(x=4, y=5, z=6), Point(x=7, y=8, z=9)] ``` Constraints: - Do not use any external libraries except the Python standard library. - The function should handle both small and large inputs efficiently. Function Signatures: ```python from typing import Tuple, List, Any def manipulate_tuple(input_tuple: Tuple[int, ...], n: int) -> Tuple[Any, ...]: pass def create_named_tuple(coordinates: List[Tuple[int, int, int]]) -> List[Any]: pass ``` Notes: - The `manipulate_tuple` function needs to handle edge cases like empty tuples and ensure appropriate use of `None`. - The `create_named_tuple` function should correctly define and use Python\'s `namedtuple` from the `collections` module to create the named tuple `Point`. Good luck!","solution":"from typing import Tuple, List, Any from collections import namedtuple def manipulate_tuple(input_tuple: Tuple[int, ...], n: int) -> Tuple[Any, ...]: reversed_tuple = input_tuple[::-1] resized_tuple = reversed_tuple[:n] + (None,) * (n - len(reversed_tuple)) return resized_tuple def create_named_tuple(coordinates: List[Tuple[int, int, int]]) -> List[Any]: Point = namedtuple(\'Point\', \'x y z\') return [Point(*coord) for coord in coordinates]"},{"question":"# Question: Numerical Properties Checker with PyTorch You are tasked with designing a utility function to check and return various numerical properties of a given tensor\'s data type using PyTorch. This function will help ensure that tensors used in computations are numerically stable and within representable ranges. Part 1: Implement `tensor_properties` Implement a function `tensor_properties` that takes a PyTorch tensor as input and returns a dictionary with the following keys and their corresponding values based on the tensor\'s data type: For floating-point tensors (`torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16`): - `\'bits\'`: The number of bits occupied by the type. - `\'eps\'`: The smallest representable number such that `1.0 + eps != 1.0`. - `\'max\'`: The largest representable number. - `\'min\'`: The smallest representable number (typically `-max`). - `\'tiny\'`: The smallest positive normal number. - `\'resolution\'`: The approximate decimal resolution of this type. For integer tensors (`torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64`): - `\'bits\'`: The number of bits occupied by the type. - `\'max\'`: The largest representable number. - `\'min\'`: The smallest representable number. Input - A PyTorch tensor. Output - A dictionary with the numerical properties of the tensor\'s data type. Constraints - Ensure that the function properly handles both floating-point and integer tensors. - The function should raise an appropriate exception if the tensor data type is neither floating-point nor integer. Example ```python import torch def tensor_properties(tensor): if tensor.dtype in [torch.float32, torch.float64, torch.float16, torch.bfloat16]: info = torch.finfo(tensor.dtype) return { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'resolution\': info.resolution } elif tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]: info = torch.iinfo(tensor.dtype) return { \'bits\': info.bits, \'max\': info.max, \'min\': info.min } else: raise TypeError(\\"Unsupported tensor data type\\") # Example usage: tensor_fp32 = torch.tensor([1.0], dtype=torch.float32) tensor_int64 = torch.tensor([1], dtype=torch.int64) assert tensor_properties(tensor_fp32) == { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, \'resolution\': 1e-06 } assert tensor_properties(tensor_int64) == { \'bits\': 64, \'max\': 9223372036854775807, \'min\': -9223372036854775808 } ```","solution":"import torch def tensor_properties(tensor): Returns various numerical properties of a given tensor\'s data type. Arguments: tensor -- A PyTorch tensor. Returns: A dictionary containing the properties of the tensor\'s data type. Raises: TypeError if the tensor data type is neither integer nor floating point. if tensor.dtype in [torch.float32, torch.float64, torch.float16, torch.bfloat16]: info = torch.finfo(tensor.dtype) return { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'resolution\': info.resolution } elif tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]: info = torch.iinfo(tensor.dtype) return { \'bits\': info.bits, \'max\': info.max, \'min\': info.min } else: raise TypeError(\\"Unsupported tensor data type\\")"},{"question":"Objective: To assess the student\'s ability to work with the `datetime` module in Python to handle date and time manipulations, including creating custom time zones and performing calculations with dates and times. Question: You are writing a scheduling application that needs to handle both local times and UTC times efficiently. The application must support different time zones and handle daylight saving changes accurately. **Task:** Implement the following functions: 1. **create_event** - **Input**: A string representing the event time in the format \'YYYY-MM-DD HH:MM:SS\' and a timezone offset in hours and minutes in the format \'+HHMM\' or \'-HHMM\'. - **Output**: A `datetime` object with the given time set in the specified timezone. - **Constraints**: The timezone offset provided should be appropriate (i.e., within the range of -1200 to +1400). 2. **is_event_in_past** - **Input**: A `datetime` object representing the event time. - **Output**: A boolean indicating whether the event time is in the past relative to the current UTC time. 3. **time_difference** - **Input**: Two `datetime` objects representing event times. - **Output**: The difference between these two event times as a `timedelta` object. 4. **convert_to_timezone** - **Input**: A `datetime` object representing the event time and a timezone offset in hours and minutes in the format \'+HHMM\' or \'-HHMM\'. - **Output**: A `datetime` object representing the event time in the specified timezone. - **Constraints**: The timezone offset provided should be appropriate (i.e., within the range of -1200 to +1400). 5. **format_event_time** - **Input**: A `datetime` object representing the event time and a format string. - **Output**: A string representing the formatted event time according to the given format string. Example Usage: ```python event_time_string = \\"2023-10-31 15:30:00\\" timezone_offset = \\"+0530\\" event_datetime = create_event(event_time_string, timezone_offset) print(is_event_in_past(event_datetime)) # Output: Depends on the current UTC time event_time_utc = convert_to_timezone(event_datetime, \\"+0000\\") formatted_time = format_event_time(event_time_utc, \\"%Y-%m-%d %H:%M:%S %Z\\") print(formatted_time) # Output: \\"2023-10-31 10:00:00 UTC\\" assuming the initial time was in IST # Further, you can test the time_difference function as: event1 = create_event(\\"2023-10-31 15:30:00\\", \\"+0530\\") event2 = create_event(\\"2023-10-30 15:30:00\\", \\"+0530\\") print(time_difference(event1, event2)) # Output: timedelta(days=1) ``` # Requirements: - Use the `datetime`, `timedelta`, and `timezone` classes from the `datetime` module. - Ensure that the functions handle edge cases, such as incorrect timezone offsets or event times in invalid formats. - The functions should be efficient and avoid unnecessary computations. # Submission: Submit a Python script containing the implementations of the above functions.","solution":"from datetime import datetime, timedelta, timezone def create_event(event_time: str, tz_offset: str) -> datetime: Create a datetime object for the given event time with specified timezone offset. Parameters: event_time (str): Event time in the format \'YYYY-MM-DD HH:MM:SS\'. tz_offset (str): Timezone offset in the format \'+HHMM\' or \'-HHMM\'. Returns: datetime: Datetime object with the specified timezone. event_dt = datetime.strptime(event_time, \'%Y-%m-%d %H:%M:%S\') tz_sign = 1 if tz_offset[0] == \'+\' else -1 tz_hours = int(tz_offset[1:3]) tz_minutes = int(tz_offset[3:5]) tz_delta = tz_sign * timedelta(hours=tz_hours, minutes=tz_minutes) event_dt_tz = event_dt.replace(tzinfo=timezone(tz_delta)) return event_dt_tz def is_event_in_past(event_time: datetime) -> bool: Check if the event time is in the past relative to current UTC time. Parameters: event_time (datetime): Event datetime. Returns: bool: True if event time is in the past, False otherwise. current_utc_time = datetime.now(timezone.utc) return event_time < current_utc_time def time_difference(event_time1: datetime, event_time2: datetime) -> timedelta: Calculate the difference between two event times. Parameters: event_time1 (datetime): First event datetime. event_time2 (datetime): Second event datetime. Returns: timedelta: Difference between the two event times. return event_time1 - event_time2 def convert_to_timezone(event_time: datetime, tz_offset: str) -> datetime: Convert the event time to the specified timezone. Parameters: event_time (datetime): Event datetime. tz_offset (str): Timezone offset in the format \'+HHMM\' or \'-HHMM\'. Returns: datetime: Datetime object in the specified timezone. tz_sign = 1 if tz_offset[0] == \'+\' else -1 tz_hours = int(tz_offset[1:3]) tz_minutes = int(tz_offset[3:5]) tz_delta = tz_sign * timedelta(hours=tz_hours, minutes=tz_minutes) target_tz = timezone(tz_delta) return event_time.astimezone(target_tz) def format_event_time(event_time: datetime, fmt: str) -> str: Format the event time according to the specified format string. Parameters: event_time (datetime): Event datetime. fmt (str): Format string. Returns: str: Formatted event time. return event_time.strftime(fmt)"},{"question":"# Pattern Matching in Python 3.10 In Python 3.10, a new feature called pattern matching was introduced. This allows for more powerful and readable ways of handling different types of data structures. For this coding assessment, you are required to implement a function that utilizes pattern matching to handle different types of input data. Task You need to implement a function `handle_data(data)` that processes data based on the following patterns: 1. **Integer**: If `data` is an integer and is even, return `True`. If it is odd, return `False`. 2. **String**: If `data` is a string, return the string reversed. 3. **List**: If `data` is a list of exactly three integers, return the product of these integers. If the list contains any other type of elements or has a length other than three, return `None`. 4. **Dictionary**: If `data` is a dictionary with a key `\\"action\\"` and its value is `\\"increment\\"`, increment the value associated with the key `\\"value\\"` by 1 and return the dictionary. If the dictionary does not match this pattern, return it as is. Constraints - You can assume the input `data` is either an integer, a string, a list, or a dictionary. Function Signature ```python def handle_data(data: Union[int, str, list, dict]) -> Union[bool, str, int, None, dict]: ``` Examples ```python print(handle_data(4)) # Output: True print(handle_data(5)) # Output: False print(handle_data(\\"hello\\")) # Output: \\"olleh\\" print(handle_data([1, 2, 3])) # Output: 6 print(handle_data([\\"a\\", 2, 3])) # Output: None print(handle_data({\\"action\\": \\"increment\\", \\"value\\": 10})) # Output: {\\"action\\": \\"increment\\", \\"value\\": 11} print(handle_data({\\"no_action\\": \\"none\\"})) # Output: {\\"no_action\\": \\"none\\"} ``` Make sure you utilize the pattern matching feature introduced in Python 3.10 to create a readable and maintainable solution.","solution":"from typing import Union, List, Dict, Any def handle_data(data: Union[int, str, List[Any], Dict[str, Any]]) -> Union[bool, str, int, None, Dict[str, Any]]: match data: case int() if data % 2 == 0: return True case int() if data % 2 != 0: return False case str(): return data[::-1] case list() if len(data) == 3 and all(isinstance(i, int) for i in data): return data[0] * data[1] * data[2] case list(): return None case dict() if \\"action\\" in data and data[\\"action\\"] == \\"increment\\" and \\"value\\" in data: data[\\"value\\"] += 1 return data case _: return data"},{"question":"# Custom Logging Handler Implementation This coding assessment question requires you to demonstrate your understanding of Python\'s logging handlers by creating a custom logging handler that combines the features of `FileHandler` and `SMTPHandler`. # Objective: Implement a custom logging handler named `FileAndEmailHandler` that logs messages both to a file and sends critical (level ERROR and above) log messages via email. # Requirements: 1. **Initialization**: - The handler should be initialized with parameters for the log file (`logfile`), and email settings (`mailhost`, `fromaddr`, `toaddrs`, `subject`, `credentials`). 2. **Handling Logs**: - Log all messages to the specified log file (`logfile`). - For error messages (level ERROR and above), send an email with the log message. 3. **Methods**: - `emit(record)`: Overridden to ensure the log record is written to the file and, if applicable, sent via email. - Any other necessary methods for file handling (inherited from `FileHandler`) and email handling (from `SMTPHandler`). # Input Format: - No specific input format requirement, as this is an implementation-focused task. # Output: - A well-documented class `FileAndEmailHandler` with: - Class initialization. - Log handling logic for both file and email. - Necessary helper methods. # Example: ```python import logging import logging.handlers class FileAndEmailHandler(logging.Handler): def __init__(self, logfile, mailhost, fromaddr, toaddrs, subject, credentials=None): super().__init__() self.file_handler = logging.FileHandler(logfile) self.smtp_handler = logging.handlers.SMTPHandler(mailhost, fromaddr, toaddrs, subject, credentials) def emit(self, record): # Log to file self.file_handler.emit(record) # Send email for error messages if record.levelno >= logging.ERROR: self.smtp_handler.emit(record) # Example usage logger = logging.getLogger(__name__) file_and_email_handler = FileAndEmailHandler( \'app.log\', \'smtp.example.com\', \'from@example.com\', [\'to@example.com\'], \'Critical Error Logged\', credentials=(\'username\', \'password\') ) logger.addHandler(file_and_email_handler) logger.setLevel(logging.DEBUG) # Log some messages logger.info(\'This is an info message.\') logger.error(\'This is an error message.\') ``` # Constraints: - Ensure that your `FileAndEmailHandler` properly inherits from `logging.Handler`. - Make sure to handle exception cases like invalid credentials, file access issues, etc. # Notes: - You are allowed to use any necessary imports for handling logging, file operations, and email sending. - Ensure to follow best practices for code readability and documentation. Good luck!","solution":"import logging import logging.handlers class FileAndEmailHandler(logging.Handler): def __init__(self, logfile, mailhost, fromaddr, toaddrs, subject, credentials=None): super().__init__() self.file_handler = logging.FileHandler(logfile) self.smtp_handler = logging.handlers.SMTPHandler(mailhost, fromaddr, toaddrs, subject, credentials) def emit(self, record): # Log to file if self.file_handler: self.file_handler.emit(record) # Send email for error messages if record.levelno >= logging.ERROR: if self.smtp_handler: self.smtp_handler.emit(record)"},{"question":"# **Python Coding Assessment Question** Objective Implement a custom command-line interface (CLI) using Python\'s `readline` module that supports history management and custom command completion. Your task is to ensure the CLI has the following features: 1. **History Management**: Load history from a specified file and save the session history back to the file. 2. **Custom Tab Completion**: Implement a custom completer function that suggests commands from a predefined list whenever the user presses the TAB key. Task Write a Python script to achieve the following: 1. **History Management**: - Load history from a history file named `.my_cli_history` located in the userâ€™s home directory when the program starts. - Save the session history back to this file upon exiting the CLI. - Limit the history size to the last 500 commands. 2. **Custom Command Completion**: - Implement a custom completer function that suggests commands from a predefined list: `[\\"start\\", \\"stop\\", \\"restart\\", \\"status\\"]`. - Bind this completer function to the TAB key. Constraints - Do not use any third-party libraries; only the standard library should be used. - Ensure that the CLI handles the history file properly, even if it does not exist initially. Input and Output - The user will interact with the CLI by typing commands. These commands will not perform any action but will allow testing of the history and completion features. - There are no specific inputs and outputs for the script. The focus is on the functionality described. Example Here is an example of how the CLI interactions should look: ``` > start[TAB] (auto-completes to) > start > stats[TAB]tus (auto-completes to) > status > stop > restart > [exit script] ``` On restarting the script, using the UP arrow key should allow cycling through previously entered commands, such as `start`, `status`, `stop`, and `restart`. Implementation Tips - Use `readline.read_history_file` and `readline.write_history_file` for loading and saving the history. - Utilize `readline.set_completer` to assign the custom completer function. - Use `atexit.register` to ensure the history is saved when the script exits. - Handle the case where the history file does not exist by creating it on the first run and then reading from it in subsequent runs. **Good luck!** ðŸ‘©â€ðŸ’»ðŸ‘¨â€ðŸ’»","solution":"import os import readline import atexit HISTORY_FILE = os.path.expanduser(\\"~/.my_cli_history\\") COMMANDS = [\\"start\\", \\"stop\\", \\"restart\\", \\"status\\"] # Custom completer function def completer(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] else: return None # Setup history file if os.path.exists(HISTORY_FILE): readline.read_history_file(HISTORY_FILE) else: open(HISTORY_FILE, \'wb\').close() readline.set_history_length(500) # Register atexit to save history atexit.register(readline.write_history_file, HISTORY_FILE) # Bind the custom completer function readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") def cli(): while True: try: command = input(\\"> \\") if command == \\"exit\\": break except (EOFError, KeyboardInterrupt): break if __name__ == \\"__main__\\": cli()"},{"question":"Coding Assessment Question # Objective The goal of this coding assessment is to evaluate your understanding of seaborn\'s color palette functionalities and their application in data visualization. You will be required to create different types of color palettes and use them effectively to visualize given data. # Task 1. Load a sample dataset (`penguins`) using seaborn. 2. Create and display three types of visualizations using different color palettes. Specifically: - A scatter plot with a qualitative color palette. - A heatmap with a sequential color palette. - A bar plot with a custom diverging color palette. 3. Each plot should have appropriate titles and labels for clarity. # Input Format - There is no specific input file or data to read externally. Use the provided seaborn dataset `penguins`. # Output Format - Three distinct visualizations displayed inline, each utilizing the specified type of color palette. - The first plot should use a qualitative palette. - The second plot should use a sequential palette. - The third plot should use a diverging palette. # Constraints - Use seaborn\'s functionalities to create color palettes. - Each plot should clearly show the use of the respective palette type. # Example Below is an example of the expected output for one of the plots. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a scatter plot with a qualitative color palette plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\'flipper_length_mm\', y=\'body_mass_g\', hue=\'species\', palette=\'muted\') plt.title(\'Scatter Plot with Qualitative Palette\') plt.show() ``` # Your Task 1. Write the code necessary to load the dataset and generate three visualizations as specified. 2. Ensure that each visualization uses the correct type of color palette and that the color palette is chosen and displayed correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") def create_scatter_plot(): plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\'flipper_length_mm\', y=\'body_mass_g\', hue=\'species\', palette=\'muted\') plt.title(\'Scatter Plot with Qualitative Palette\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') plt.show() def create_heatmap(): # For heatmap, we need a dataset that makes sense for a heatmap, using sample data from \'flipper_length_mm\' and \'body_mass_g\' columns # Here we pivot the table to create a matrix form suitable for heatmap visualization. heatmap_data = penguins.pivot_table(values=\'body_mass_g\', index=\'species\', columns=\'island\', aggfunc=\'mean\') plt.figure(figsize=(10, 6)) sns.heatmap(heatmap_data, cmap=\'Blues\', annot=True) plt.title(\'Heatmap with Sequential Palette\') plt.xlabel(\'Island\') plt.ylabel(\'Species\') plt.show() def create_bar_plot(): plt.figure(figsize=(10, 6)) sns.barplot(data=penguins, x=\'species\', y=\'body_mass_g\', hue=\'sex\', palette=\'coolwarm\') plt.title(\'Bar Plot with Diverging Palette\') plt.xlabel(\'Species\') plt.ylabel(\'Body Mass (g)\') plt.show() # Function calls to create the plots create_scatter_plot() create_heatmap() create_bar_plot()"},{"question":"**Coding Assessment Question:** **Objective:** Demonstrate understanding of PyTorch and DLPack interoperability. **Problem Statement:** You are given two functions, `from_dlpack` and `to_dlpack`, within the `torch.utils.dlpack` module. These facilitate conversion between PyTorch tensors and DLPack tensors, enabling interoperability between different deep learning frameworks that support DLPack. Your task is to write a Python function using these utilities to perform the following: 1. Convert a given PyTorch tensor to a DLPack tensor. 2. Modify the DLPack tensor (for example, by doubling each element). 3. Convert the modified DLPack tensor back to a PyTorch tensor. 4. Ensure that the original tensor remains unchanged and only the new tensor reflects the modifications. # Function Signature ```python import torch import torch.utils.dlpack def convert_and_modify_tensor(tensor: torch.Tensor) -> torch.Tensor: Converts a PyTorch tensor to a DLPack tensor, makes an in-place modification on the DLPack tensor, and converts it back to a PyTorch tensor. Args: tensor (torch.Tensor): The input PyTorch tensor. Returns: torch.Tensor: A new PyTorch tensor with the modifications applied. Example: >>> tensor = torch.tensor([1, 2, 3, 4]) >>> new_tensor = convert_and_modify_tensor(tensor) >>> print(new_tensor) tensor([2, 4, 6, 8]) >>> print(tensor) tensor([1, 2, 3, 4]) # The original tensor should remain unchanged # Step 1: Convert the PyTorch tensor to a DLPack tensor dlpack_tensor = torch.utils.dlpack.to_dlpack(tensor) # Assume we have a function modify_dlpack_tensor to double each element of the DLPack tensor # This part can be assumed as pseudo-code or theoretical since direct DLPack manipulation is not standard in this context # Return the modified tensor in PyTorch format new_tensor = torch.utils.dlpack.from_dlpack(dlpack_tensor) # Placeholder: Apply in-place modification # Assuming we have an external function to modify dlpack_tensor which is out of scope here new_tensor.copy_(new_tensor * 2) return new_tensor ``` # Constraints: - You are not allowed to modify the original tensor directly. - Assume all DLPack manipulations are placeholder steps as PyTorch may not have direct support to edit DLPack tensors. # Explanation: - The `convert_and_modify_tensor` function takes a PyTorch tensor as input. - It converts the tensor to a DLPack tensor using `torch.utils.dlpack.to_dlpack`. - It doubles the values in the tensor (here we assume an external function can handle this if DLPack supported direct manipulation). - It converts the modified DLPack tensor back to a PyTorch tensor using `torch.utils.dlpack.from_dlpack`. - Returns the modified PyTorch tensor, ensuring the original tensor remains unchanged. **Note:** This exercise emphasizes understanding tensor conversion and the immutability of the original tensor, using PyTorch and hypothetical DLPack manipulations appropriate for teaching purposes.","solution":"import torch import torch.utils.dlpack def convert_and_modify_tensor(tensor: torch.Tensor) -> torch.Tensor: Converts a PyTorch tensor to a DLPack tensor, makes an in-place modification on the DLPack tensor, and converts it back to a PyTorch tensor. Args: tensor (torch.Tensor): The input PyTorch tensor. Returns: torch.Tensor: A new PyTorch tensor with the modifications applied. Example: >>> tensor = torch.tensor([1, 2, 3, 4]) >>> new_tensor = convert_and_modify_tensor(tensor) >>> print(new_tensor) tensor([2, 4, 6, 8]) >>> print(tensor) tensor([1, 2, 3, 4]) # The original tensor should remain unchanged # Step 1: Convert the PyTorch tensor to a DLPack tensor dlpack_tensor = torch.utils.dlpack.to_dlpack(tensor) # Step 3: Convert the DLPack tensor back to a PyTorch tensor new_tensor = torch.utils.dlpack.from_dlpack(dlpack_tensor) # Step 2: Modify the PyTorch tensor (since direct DLPack modification is non-trivial) new_tensor = new_tensor * 2 return new_tensor"},{"question":"# Task Using the `collections` module from Python 3.10, you are tasked to write a system to manage and query a dynamic dataset of transactions. Each transaction consists of: - A unique transaction ID - A user ID - A transaction amount You must implement a class `TransactionManager` with the following functionalities: 1. **Add Transaction**: - Method: `add_transaction(transaction_id, user_id, amount)` - Arguments: - `transaction_id` (string): A unique identifier for the transaction. - `user_id` (string): ID of the user involved in the transaction. - `amount` (float): The transaction amount. - This method should add the transaction to the system. 2. **Get User Balance**: - Method: `get_user_balance(user_id)` - Arguments: - `user_id` (string): The ID of the user. - Returns: - (float) The total amount of all transactions associated with the specified user. 3. **Get Top Transactions**: - Method: `get_top_transactions(n)` - Arguments: - `n` (int): The number of top transactions to retrieve. - Returns: - A list of tuples where each tuple contains the transaction ID and the amount of the `n` highest transactions, sorted from highest to lowest by amount. 4. **Get User Transaction History**: - Method: `get_user_transaction_history(user_id)` - Arguments: - `user_id` (string): The ID of the user. - Returns: - A list of tuples where each tuple contains the transaction ID and the amount of each transaction associated with the user, in the order they were added. # Constraints - Each `transaction_id` is unique. - For simplicity, assume all inputs are valid and no errors need to be handled. # Example Usage ```python manager = TransactionManager() manager.add_transaction(\'tx1001\', \'user1\', 50.0) manager.add_transaction(\'tx1002\', \'user2\', 150.0) manager.add_transaction(\'tx1003\', \'user1\', 200.0) print(manager.get_user_balance(\'user1\')) # Output: 250.0 print(manager.get_top_transactions(2)) # Output: [(\'tx1003\', 200.0), (\'tx1002\', 150.0)] print(manager.get_user_transaction_history(\'user1\')) # Output: [(\'tx1001\', 50.0), (\'tx1003\', 200.0)] ``` # Submission Please implement the `TransactionManager` class in Python, considering efficiency and clarity. Use appropriate data structures from the `collections` module to manage underlying data and optimize performance for the operations described.","solution":"from collections import defaultdict class TransactionManager: def __init__(self): self.transactions = {} self.user_transactions = defaultdict(list) self.user_balances = defaultdict(float) def add_transaction(self, transaction_id, user_id, amount): self.transactions[transaction_id] = (user_id, amount) self.user_transactions[user_id].append((transaction_id, amount)) self.user_balances[user_id] += amount def get_user_balance(self, user_id): return self.user_balances[user_id] def get_top_transactions(self, n): sorted_transactions = sorted(self.transactions.items(), key=lambda x: x[1][1], reverse=True) return [(tx_id, amount) for tx_id, (_, amount) in sorted_transactions[:n]] def get_user_transaction_history(self, user_id): return self.user_transactions[user_id]"},{"question":"# Seaborn Coding Assessment Question Introduction You are given the `penguins` dataset available in the seaborn library. This dataset contains information about different species of penguins, including their body mass, flipper length, species type, etc. Your task is to write a function that generates and saves a set of visualizations to explore the dataset using seaborn objects. Objective Write a function named `generate_penguin_visualizations` that uses the seaborn objects interface to create the following plots: 1. **Dot Plot with Vertical Jitter**: Create a dot plot of `body_mass_g` (y-axis) versus `species` (x-axis) with a jitter on the y-axis using the `width` parameter. 2. **Dot Plot with Horizontal Jitter**: Create a dot plot of `species` (y-axis) versus `body_mass_g` (x-axis) with a jitter on the x-axis using the `width` parameter. 3. **Numeric Axes Jitter**: - Create a dot plot of `flipper_length_mm` (y-axis) and `body_mass_g` rounded to the nearest thousand (`body_mass_g.round(-3)`) on the x-axis with no additional jitter. - Create a dot plot with the same axes but with a specific jitter applied to the x-axis using `x=100`. - Create a dot plot with the same axes but with jitter applied to both x-axis and y-axis using `x=200` and `y=5`. Each plot should be saved to a file with an appropriate name, for instance, `dot_plot_vertical_jitter.png`, `dot_plot_horizontal_jitter.png`, etc. Input/Output - **Input**: No specific input parameters. The dataset should be loaded within the function. - **Output**: No return value. The function should save the plots as PNG files in the current directory. Constraints - Use only the seaborn library and its objects (`seaborn.objects`). - Ensure the plots are well-labeled and legible. - Save the plots with filenames as specified in the task description or something similar. Example Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def generate_penguin_visualizations(): # Load dataset penguins = load_dataset(\\"penguins\\") # Create and save dot plot with vertical jitter vertical_jitter_plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) vertical_jitter_plot.save(\\"dot_plot_vertical_jitter.png\\") # Create and save dot plot with horizontal jitter horizontal_jitter_plot = ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\") .add(so.Dots(), so.Jitter()) ) horizontal_jitter_plot.save(\\"dot_plot_horizontal_jitter.png\\") # Create and save numeric axes dot plots numeric_no_jitter_plot = ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"]) .add(so.Dots()) ) numeric_no_jitter_plot.save(\\"dot_plot_numeric_no_jitter.png\\") numeric_x_jitter_plot = ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"]) .add(so.Dots(), so.Jitter(x=100)) ) numeric_x_jitter_plot.save(\\"dot_plot_numeric_x_jitter.png\\") numeric_xy_jitter_plot = ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1)) .add(so.Dots(), so.Jitter(x=200, y=5)) ) numeric_xy_jitter_plot.save(\\"dot_plot_numeric_xy_jitter.png\\") ``` Ensure your function generates the correct plots with jitter applied as described and saves them with the right file names. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset def generate_penguin_visualizations(): Generates and saves visualizations for the \'penguins\' dataset. # Load dataset penguins = load_dataset(\\"penguins\\") # Create and save dot plot with vertical jitter vertical_jitter_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.1)) ) vertical_jitter_plot.save(\\"dot_plot_vertical_jitter.png\\") # Create and save dot plot with horizontal jitter horizontal_jitter_plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\") .add(so.Dots(), so.Jitter(width=0.1)) ) horizontal_jitter_plot.save(\\"dot_plot_horizontal_jitter.png\\") # Create and save numeric axes dot plots numeric_no_jitter_plot = ( so.Plot(penguins.assign(body_mass_g_rounded=penguins[\\"body_mass_g\\"].round(-3)), x=\\"body_mass_g_rounded\\", y=\\"flipper_length_mm\\") .add(so.Dots()) ) numeric_no_jitter_plot.save(\\"dot_plot_numeric_no_jitter.png\\") numeric_x_jitter_plot = ( so.Plot(penguins.assign(body_mass_g_rounded=penguins[\\"body_mass_g\\"].round(-3)), x=\\"body_mass_g_rounded\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=100)) ) numeric_x_jitter_plot.save(\\"dot_plot_numeric_x_jitter.png\\") numeric_xy_jitter_plot = ( so.Plot(penguins.assign(body_mass_g_rounded=penguins[\\"body_mass_g\\"].round(-3)), x=\\"body_mass_g_rounded\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=200, y=5)) ) numeric_xy_jitter_plot.save(\\"dot_plot_numeric_xy_jitter.png\\")"},{"question":"# Question: Implementing a WSGI-based Task Tracker Application You are to implement a simple task tracker application using the WSGI interface and the `wsgiref` library. The application should allow users to add tasks and list all added tasks through a web interface. Follow the guidelines to implement and serve the application. Requirements: 1. **Create a WSGI application (`task_tracker_app`)**: - The application should handle HTTP GET and POST requests. - On a GET request to `/`, the application should display an HTML form allowing the user to add a new task and a list of all tasks. - On a POST request to `/add_task`, the application should add the submitted task to the list and redirect back to the root URL (`/`). 2. **Serve the application using `wsgiref.simple_server`**: - The application should be served on `localhost` at port `8080`. 3. **Task Storage**: - Use an in-memory list to store the tasks. This list should be shared among requests (Hint: You may implement it as a global variable). 4. **WSGI Environment and Request Handling**: - Use the appropriate utilities from the `wsgiref.util` module to parse request data. - Implement correct response headers and status codes using the `wsgiref.headers` module or other relevant utilities. Input and Output: - **Expected Functions**: - Your application should be a callable named `task_tracker_app(environ, start_response)`. - The server should be started with a block similar to: ```python if __name__ == \'__main__\': from wsgiref.simple_server import make_server server = make_server(\'localhost\', 8080, task_tracker_app) print(\\"Serving on port 8080...\\") server.serve_forever() ``` - **Constraints**: - Do not use any external libraries for parsing request data or managing state; use only standard Python libraries and the `wsgiref` module. - Handle invalid HTTP methods or paths gracefully with a 404 Not Found response. Example Behavior: 1. Navigating to `http://localhost:8080/` should display an HTML form for adding a task and a list of all current tasks. 2. Submitting the form should add the task and refresh the list on the root URL. # Complete the Implementation Below ```python tasks = [] def task_tracker_app(environ, start_response): # Application code here # Server code if __name__ == \'__main__\': from wsgiref.simple_server import make_server server = make_server(\'localhost\', 8080, task_tracker_app) print(\\"Serving on port 8080...\\") server.serve_forever() ```","solution":"tasks = [] def task_tracker_app(environ, start_response): from wsgiref.util import setup_testing_defaults, request_uri from wsgiref.simple_server import make_server from urllib.parse import parse_qs import cgi setup_testing_defaults(environ) path = environ[\'PATH_INFO\'] if environ[\'REQUEST_METHOD\'] == \'GET\' and path == \'/\': response_body = <html> <body> <h1>Task Tracker</h1> <form method=\\"POST\\" action=\\"/add_task\\"> <label for=\\"task\\">Task:</label> <input type=\\"text\\" id=\\"task\\" name=\\"task\\"> <input type=\\"submit\\" value=\\"Add Task\\"> </form> <h2>Tasks</h2> <ul> for task in tasks: response_body += f\\"<li>{task}</li>\\" response_body += </ul> </body> </html> response_body = response_body.encode(\'utf-8\') status = \'200 OK\' headers = [(\'Content-type\', \'text/html; charset=utf-8\')] elif environ[\'REQUEST_METHOD\'] == \'POST\' and path == \'/add_task\': size = int(environ.get(\'CONTENT_LENGTH\', 0)) post_data = cgi.FieldStorage(fp=environ[\'wsgi.input\'], environ=environ, keep_blank_values=True) task = post_data.getvalue(\\"task\\") if task: tasks.append(task) status = \'303 See Other\' headers = [(\'Location\', \'/\')] response_body = b\'\' else: status = \'404 Not Found\' response_body = b\'<h1>404 Not Found</h1>\' headers = [(\'Content-type\', \'text/html; charset=utf-8\')] start_response(status, headers) return [response_body] # Server code if __name__ == \'__main__\': server = make_server(\'localhost\', 8080, task_tracker_app) print(\\"Serving on port 8080...\\") server.serve_forever()"},{"question":"# Pathlib Challenge You are tasked with creating a utility function that organizes files in a directory into a categorized structure based on their file extensions. The goal is to move files from the given directory into subdirectories named after their extensions (e.g., all `.txt` files should move into a `txt` subdirectory within the given directory). Function Signature ```python def organize_files_by_extension(directory: str) -> None: Organizes files in the given directory into subdirectories based on their file extensions. Parameters: - directory (str): The path to the directory to organize. Returns: - None: The function performs the organization in place and returns nothing. Raises: - ValueError: If the specified directory does not exist or is not a directory. ``` Detailed Requirements: 1. **Directory Existence Check:** The function should first verify if the provided `directory` exists and is a directory. If not, raise a `ValueError`. 2. **File Operations:** - Iterate over all files in the specified `directory`. Ignore subdirectories. - For each file, determine its extension. If a file has no extension, categorize it under a directory called `no_extension`. - Move each file into a subdirectory named after its extension. For example, `example.txt` should move to `directory/txt/example.txt`. 3. **Create Subdirectories:** If a subdirectory for a particular extension does not exist, create it. 4. **Handle Edge Cases:** Handle cases such as symbolic links, hidden files (starting with a dot), and preserve file permissions and timestamps when moving. Example Usage: ```python # Assume the directory has the following structure before running the function: # /example_dir # â”œâ”€â”€ file1.txt # â”œâ”€â”€ file2.txt # â”œâ”€â”€ image1.png # â”œâ”€â”€ document1.pdf # â”œâ”€â”€ document2 # â””â”€â”€ script.sh organize_files_by_extension(\\"/example_dir\\") # After running the function, the structure should be: # /example_dir # â”œâ”€â”€ txt # â”‚ â”œâ”€â”€ file1.txt # â”‚ â””â”€â”€ file2.txt # â”œâ”€â”€ png # â”‚ â””â”€â”€ image1.png # â”œâ”€â”€ pdf # â”‚ â”œâ”€â”€ document1.pdf # â”œâ”€â”€ no_extension # â”‚ â””â”€â”€ document2 # â””â”€â”€ sh # â””â”€â”€ script.sh ``` Constraints: - Use the `pathlib` module for all path manipulations and filesystem operations. - Ensure that the solution is efficient and handles a large number of files gracefully. - The function should handle all common file types and their extensions correctly. Hints: - Make use of `Path` and its methods such as `iterdir()`, `is_file()`, `suffix`, `mkdir()`, `rename()`, etc. - Consider using exception handling to manage any unforeseen issues during file operations.","solution":"import os from pathlib import Path def organize_files_by_extension(directory: str) -> None: Organizes files in the given directory into subdirectories based on their file extensions. Parameters: - directory (str): The path to the directory to organize. Returns: - None: The function performs the organization in place and returns nothing. Raises: - ValueError: If the specified directory does not exist or is not a directory. dir_path = Path(directory) if not dir_path.exists() or not dir_path.is_dir(): raise ValueError(\\"The specified directory does not exist or is not a directory.\\") for item in dir_path.iterdir(): if item.is_file(): # Get file extension extension = item.suffix[1:] if item.suffix else \\"no_extension\\" subdirectory = dir_path / extension # Create subdirectory if it doesn\'t exist if not subdirectory.exists(): subdirectory.mkdir() # Move file to subdirectory target = subdirectory / item.name item.rename(target)"},{"question":"# Asynchronous Taxi Dispatcher You are tasked with writing a taxi dispatch system using Python\'s `asyncio` synchronization primitives. The dispatcher will manage available taxis and incoming ride requests. The goal is to ensure fair handling of taxi availability and ride requests, preventing any one request from monopolizing the available taxis. # Specifications Implement a class `TaxiDispatcher` with the following methods: 1. `__init__(self, num_taxis: int)`: - Initializes the dispatcher with a given number of taxis. - Uses an `asyncio.Semaphore` to manage taxi availability. 2. `async request_ride(self, rider_id: int) -> str`: - Simulates a rider requesting a taxi. - Waits for a taxi to become available. - Returns a string indicating which rider got the taxi: `\\"Rider {rider_id} got a taxi.\\"` - If no taxis are available, the rider should wait until one becomes available. 3. `async release_taxi(self, rider_id: int) -> str`: - Simulates a rider finishing their ride and releasing the taxi back to the pool. - Returns a string indicating that the rider has released the taxi: `\\"Rider {rider_id} released a taxi.\\"` 4. `async run_simulation(self, ride_requests: list, release_times: dict)`: - Takes a list of rider IDs representing ride requests in the order they were received. - Takes a dictionary where keys are rider IDs and values are the time (in seconds) before they release the taxi. - Simulates the entire process of riders requesting and releasing taxis. # Constraints - You must use `asyncio`\'s synchronization primitives (`asyncio.Semaphore` for managing limits on taxi availability). - Assume that the number of taxis is a positive integer. - Assume that the ride request list and release times dictionary are valid and consistent with each other. - Maximum elements in `ride_requests` list: 100 - Maximum time in `release_times` dictionary: 60 seconds # Example ```python import asyncio class TaxiDispatcher: def __init__(self, num_taxis: int): self.num_taxis = num_taxis self.taxi_semaphore = asyncio.Semaphore(num_taxis) async def request_ride(self, rider_id: int) -> str: await self.taxi_semaphore.acquire() return f\\"Rider {rider_id} got a taxi.\\" async def release_taxi(self, rider_id: int) -> str: self.taxi_semaphore.release() return f\\"Rider {rider_id} released a taxi.\\" async def run_simulation(self, ride_requests: list, release_times: dict): async def simulate_ride(rider_id: int): taxi_got = await self.request_ride(rider_id) print(taxi_got) await asyncio.sleep(release_times[rider_id]) taxi_released = await self.release_taxi(rider_id) print(taxi_released) await asyncio.gather(*(simulate_ride(rider_id) for rider_id in ride_requests)) # Example Usage ride_requests = [1, 2, 3, 4] release_times = {1: 3, 2: 2, 3: 4, 4: 1} dispatcher = TaxiDispatcher(2) asyncio.run(dispatcher.run_simulation(ride_requests, release_times)) ``` This example creates a taxi dispatcher managing 2 taxis. Four riders request taxis, and each rider releases their taxi after a specified time. The simulation runs asynchronously, managing the concurrency of taxi availability using `asyncio.Semaphore`.","solution":"import asyncio class TaxiDispatcher: def __init__(self, num_taxis: int): self.num_taxis = num_taxis self.taxi_semaphore = asyncio.Semaphore(num_taxis) async def request_ride(self, rider_id: int) -> str: await self.taxi_semaphore.acquire() return f\\"Rider {rider_id} got a taxi.\\" async def release_taxi(self, rider_id: int) -> str: self.taxi_semaphore.release() return f\\"Rider {rider_id} released a taxi.\\" async def run_simulation(self, ride_requests: list, release_times: dict): async def simulate_ride(rider_id: int): taxi_got = await self.request_ride(rider_id) print(taxi_got) await asyncio.sleep(release_times[rider_id]) taxi_released = await self.release_taxi(rider_id) print(taxi_released) await asyncio.gather(*(simulate_ride(rider_id) for rider_id in ride_requests)) # Example Usage ride_requests = [1, 2, 3, 4] release_times = {1: 3, 2: 2, 3: 4, 4: 1} dispatcher = TaxiDispatcher(2) asyncio.run(dispatcher.run_simulation(ride_requests, release_times))"},{"question":"# **Multi-label Classification with Scikit-learn** **Objective:** Implement a multi-label classification model using Scikit-learn\'s `MultiOutputClassifier`. The goal is to classify the genres of movies based on given features. **Problem Statement:** You are given a dataset containing various features of movies, including attributes like budget, runtime, etc. Each movie can belong to multiple genres (e.g., Action, Comedy, Drama). Your task is to build a model that predicts multiple genres for each movie based on its features. **Dataset:** The dataset is provided as a CSV file `movies.csv` with the following columns: - `features` (10 columns): Numeric values representing various features of the movie. - `genres` (5 columns): Binary values (0 or 1) indicating the presence of each genre. Each genre is a separate column. **Input:** - CSV file `movies.csv` with columns: - `feature_1, feature_2, ..., feature_10`: Numeric values representing movie features. - `genre_1, genre_2, ..., genre_5`: Binary values indicating movie genres. **Output:** - Print the classification report of the model on the test set. - Save the trained model to a file `multi_label_model.pkl`. **Constraints:** - Use `MultiOutputClassifier` with a base estimator of choice. - Split the dataset into training and test sets (80%-20%). - Scale the features using `StandardScaler`. **Performance Requirements:** - The model should achieve an accuracy score of at least 70% on the test data. ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report import joblib # Read the dataset data = pd.read_csv(\'movies.csv\') # Separating features and genres features = data.iloc[:, :10] genres = data.iloc[:, 10:] # Split the dataset into training and testing sets (80%-20%) X_train, X_test, y_train, y_test = train_test_split(features, genres, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Define the multi-label classifier with RandomForest as the base estimator base_estimator = RandomForestClassifier(random_state=42) multi_target_forest = MultiOutputClassifier(base_estimator) # Train the model multi_target_forest.fit(X_train, y_train) # Predict on the test set y_pred = multi_target_forest.predict(X_test) # Print the classification report print(classification_report(y_test, y_pred)) # Save the trained model to a file joblib.dump(multi_target_forest, \'multi_label_model.pkl\') ``` **Notes:** - Make sure to handle any missing values in the data if present. - The script should be runnable independently without additional modifications. - Document the steps and thought process in comments to help reviewers understand your approach.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report import joblib def train_multi_label_model(csv_file_path): Trains a multi-label classification model using a dataset from a CSV file. Args: csv_file_path (str): The file path to the CSV file containing the dataset. Returns: None # Read the dataset data = pd.read_csv(csv_file_path) # Separating features and genres features = data.iloc[:, :10] genres = data.iloc[:, 10:] # Split the dataset into training and testing sets (80%-20%) X_train, X_test, y_train, y_test = train_test_split(features, genres, test_size=0.2, random_state=42) # Scale the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Define the multi-label classifier with RandomForest as the base estimator base_estimator = RandomForestClassifier(random_state=42) multi_target_forest = MultiOutputClassifier(base_estimator) # Train the model multi_target_forest.fit(X_train, y_train) # Predict on the test set y_pred = multi_target_forest.predict(X_test) # Print the classification report report = classification_report(y_test, y_pred, output_dict=True) print(classification_report(y_test, y_pred)) # Save the trained model to a file joblib.dump(multi_target_forest, \'multi_label_model.pkl\') return report # Example function call # train_multi_label_model(\'movies.csv\')"},{"question":"# Timedelta Operations and Manipulation with pandas **Objective:** The purpose of this assessment is to test your understanding of the creation, manipulation, and analysis of `Timedelta` objects and `TimedeltaIndex` in pandas. **Problem Statement:** You are given a series of timestamps recorded during a server monitoring task. Your task is to clean, manipulate, and analyze these timestamps using pandas `Timedelta` functionalities. 1. **Input:** - A list of string timestamps representing when specific events occurred, following the format: `\'YYYY-MM-DD HH:MM:SS\'`. - A list of time durations as strings, which represent the delay after each event from the given timestamps. 2. **Requirements:** - Convert the list of string timestamps into a pandas `Series` of `datetime` objects. - Convert the list of duration strings to a pandas `TimedeltaIndex`. - Create a new `Series` where each element is the sum of the corresponding timestamp and the duration `Timedelta`. - Calculate the mean and sum of all the durations. - Find the maximum and minimum resulting timestamps. - Return the result as a dictionary containing: - \'mean_duration\' : (mean duration `Timedelta`) - \'total_duration\' : (total duration `Timedelta`) - \'max_timestamp\' : (the maximum resulting timestamp) - \'min_timestamp\' : (the minimum resulting timestamp) 3. **Example:** ```python timestamps = [\'2023-01-01 00:00:00\', \'2023-01-02 00:00:00\', \'2023-01-03 00:00:00\'] durations = [\'1 days\', \'2 days\', \'1 days 12:00:00\'] result = process_timestamps(timestamps, durations) ``` Resulting dictionary: ```python { \'mean_duration\' : Timedelta(\'1 days 12:00:00\'), \'total_duration\' : Timedelta(\'4 days 12:00:00\'), \'max_timestamp\' : Timestamp(\'2023-01-04 12:00:00\'), \'min_timestamp\' : Timestamp(\'2023-01-02 00:00:00\') } ``` **Constraints:** - Ensure all operations are vectorized using pandas functions. - The function should handle any list lengths and timing formats as specified. **Implementation:** ```python import pandas as pd def process_timestamps(timestamps, durations): # Step 1: Convert the list of string timestamps to a Series of datetime objects timestamp_series = pd.Series(pd.to_datetime(timestamps)) # Step 2: Convert the list of duration strings to a TimedeltaIndex timedelta_series = pd.to_timedelta(durations) # Step 3: Create a new Series where each element is the sum of the corresponding timestamp and duration result_series = timestamp_series + timedelta_series # Step 4: Calculate the mean and sum of all durations mean_duration = timedelta_series.mean() total_duration = timedelta_series.sum() # Step 5: Find the maximum and minimum resulting timestamps max_timestamp = result_series.max() min_timestamp = result_series.min() # Return the result as a dictionary return { \'mean_duration\' : mean_duration, \'total_duration\' : total_duration, \'max_timestamp\' : max_timestamp, \'min_timestamp\' : min_timestamp } ```","solution":"import pandas as pd def process_timestamps(timestamps, durations): # Step 1: Convert the list of string timestamps to a Series of datetime objects timestamp_series = pd.Series(pd.to_datetime(timestamps)) # Step 2: Convert the list of duration strings to a TimedeltaIndex timedelta_series = pd.to_timedelta(durations) # Step 3: Create a new Series where each element is the sum of the corresponding timestamp and duration result_series = timestamp_series + timedelta_series # Step 4: Calculate the mean and sum of all durations mean_duration = timedelta_series.mean() total_duration = timedelta_series.sum() # Step 5: Find the maximum and minimum resulting timestamps max_timestamp = result_series.max() min_timestamp = result_series.min() # Return the result as a dictionary return { \'mean_duration\' : mean_duration, \'total_duration\' : total_duration, \'max_timestamp\' : max_timestamp, \'min_timestamp\' : min_timestamp }"},{"question":"# PyTorch Coding Assessment Question Objective: Write a function in PyTorch that takes a multi-dimensional tensor, modifies it using various view operations, and ensures the final tensor is contiguous. Your implementation should demonstrate a clear understanding of tensor views and their properties. Function Signature: ```python def process_tensor(t: torch.Tensor) -> torch.Tensor: This function takes a multi-dimensional tensor `t`, processes it through various view operations, and returns the final tensor ensuring it is contiguous. Parameters: t (torch.Tensor): A multi-dimensional tensor. Returns: torch.Tensor: The processed and contiguous tensor. ``` Instructions: 1. **Input**: A multi-dimensional tensor `t` of arbitrary shape. 2. **Output**: The processed tensor must be contiguous. 3. **Steps**: - Apply at least three different view operations (e.g., `transpose`, `unsqueeze`, `view`). - Modify the data of the tensor at least once. - Ensure the final tensor is contiguous before returning. 4. **Constraints**: - Do not use in-place operations to alter the tensor apart from indexing. - Use only view operations listed in the provided documentation. - Avoid unnecessary data copies. Example: ```python import torch # Example tensor t = torch.arange(16).view(4, 4) # Example function call output_tensor = process_tensor(t) print(output_tensor.is_contiguous()) # Should print True ``` Notes: - Pay attention to whether the tensor becomes non-contiguous after each operation. - Utilize the `.contiguous()` method if needed to ensure the final tensor is contiguous. This question assesses your understanding of how to work with tensor views and manage tensor contiguity in PyTorch. Good luck!","solution":"import torch def process_tensor(t: torch.Tensor) -> torch.Tensor: This function takes a multi-dimensional tensor `t`, processes it through various view operations, and returns the final tensor ensuring it is contiguous. Parameters: t (torch.Tensor): A multi-dimensional tensor. Returns: torch.Tensor: The processed and contiguous tensor. # Apply the first view operation: transpose t_transposed = t.transpose(0, 1) # Apply the second view operation: unsqueeze t_unsqueezed = t_transposed.unsqueeze(0) # Modify the data by doubling the values t_modified = t_unsqueezed * 2 # Apply the third view operation: view/reshape to flatten the tensor to 2D t_flattened = t_modified.view(-1, t_modified.size(-1)) # Ensure the tensor is contiguous before returning return t_flattened.contiguous()"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s `stripplot` function by visualizing different aspects of a dataset. **Problem Statement:** You are given the famous `tips` dataset from the Seaborn library. Your task is to create three different strip plots to analyze the data. **Requirements:** 1. **Strip Plot 1:** - Plot the distribution of `total_bill` amounts. - Split the strips by `day` of the week. - Map the `time` of the day (Lunch or Dinner) to the color of the points using the `hue` parameter. 2. **Strip Plot 2:** - Create a vertically-oriented strip plot. - Compare the `total_bill` for each `day` of the week. - Map the number of people at the table (`size`) to the color of the points using the `hue` parameter. 3. **Strip Plot 3:** - Create a strip plot comparing `total_bill` for each `day` of the week. - Split the `hue` values by `sex` (Male or Female) with dodging. - Disable the random jittering of points. **Input and Output:** - **Input:** - None (the `tips` dataset should be loaded using `sns.load_dataset(\\"tips\\")`) - **Output:** - Display the three strip plots using Matplotlib. **Constraints:** - Use Seaborn and Matplotlib for all plots. - Ensure that all three plots are displayed clearly. **Performance Requirements:** - The plots should be efficiently generated and rendered. ```python # Load necessary libraries import seaborn as sns import matplotlib.pyplot as plt # Set the theme for seaborn sns.set_theme(style=\\"whitegrid\\") # Load the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") # Strip Plot 1: Distribution of total_bill, split by day, colored by time plt.figure(figsize=(8, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", jitter=True) plt.title(\'Total Bill Distribution by Day and Time\') plt.show() # Strip Plot 2: Vertically-oriented strip plot, total_bill per day, colored by size plt.figure(figsize=(8, 6)) sns.stripplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"size\\", orient=\\"v\\") plt.title(\'Total Bill per Day, Colored by Size\') plt.show() # Strip Plot 3: Total_bill per day, split by sex with dodging, and jitter disabled plt.figure(figsize=(8, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, jitter=False) plt.title(\'Total Bill per Day, Split by Sex with Dodging\') plt.show() ``` - Ensure the above plots adhere to the requirements given.","solution":"# Load necessary libraries import seaborn as sns import matplotlib.pyplot as plt # Set the theme for seaborn sns.set_theme(style=\\"whitegrid\\") # Load the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") def generate_stripplots(): # Strip Plot 1: Distribution of total_bill, split by day, colored by time plt.figure(figsize=(8, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", jitter=True) plt.title(\'Total Bill Distribution by Day and Time\') plt.show() # Strip Plot 2: Vertically-oriented strip plot, total_bill per day, colored by size plt.figure(figsize=(8, 6)) sns.stripplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"size\\", orient=\\"v\\") plt.title(\'Total Bill per Day, Colored by Size\') plt.show() # Strip Plot 3: Total_bill per day, split by sex with dodging, and jitter disabled plt.figure(figsize=(8, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, jitter=False) plt.title(\'Total Bill per Day, Split by Sex with Dodging\') plt.show()"},{"question":"# Coding Assessment: Distributed Training with Uneven Inputs Objective Demonstrate your understanding and capability to implement and use the Join context manager in PyTorch to handle distributed training scenarios with uneven input sizes. Problem Statement You are tasked with creating a simple distributed training setup using PyTorch that handles uneven input batches by leveraging the Join context manager. You should: 1. Implement a custom dataset that generates random inputs of varying sizes. 2. Setup a basic model and perform distributed training. 3. Utilize the Join context manager classes (`Join`, `Joinable`, `JoinHook`) to ensure proper synchronization and handling of uneven inputs across multiple processes. Requirements 1. **Custom Dataset:** - Create a dataset class that inherits from `torch.utils.data.Dataset`. - This dataset should generate input tensors of random sizes between [5, 10) and their corresponding random labels. 2. **Model:** - Implement a simple feedforward neural network model using `torch.nn.Module`. 3. **Distributed Training Setup:** - Utilize `torch.distributed` to set up a training process that runs across multiple GPUs (if available). - Implement necessary functions to perform training with the model and dataset. 4. **Join Context Manager:** - Use **Join**, **Joinable**, and **JoinHook** classes to manage the training loop. - Ensure that your implementation can handle batches with varying sizes correctly, synchronizing the training processes as necessary. Constraints - Assume you have access to at least 2 GPUs. - Implement the training loop to run for a fixed number of epochs, e.g., 10. - You must handle potential race conditions that can arise due to the varying batch sizes. Expected Input and Output - **Input:** No explicit input. The dataset should generate random tensors and labels internally. - **Output:** The accuracy or loss metrics per epoch output to the console. ```python # Sample structure for your implementation: import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader class RandomSizeDataset(Dataset): # Your implementation to generate random sized inputs class SimpleModel(nn.Module): # Your simple feedforward neural network implementation def train(rank, world_size): # Set up process group and distribute dataset, model # Use Join context manager to handle uneven inputs during training if __name__ == \\"__main__\\": # Initialize the distributed environment and kick off the training process ``` Performance Requirements - Your implementation should ensure proper synchronization and avoid deadlocks. - Properly manage memory and resources to ensure no GPU memory leaks or inefficiencies. Good luck!","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader class RandomSizeDataset(Dataset): def __init__(self, size, min_input_size=5, max_input_size=10): self.size = size self.min_input_size = min_input_size self.max_input_size = max_input_size def __len__(self): return self.size def __getitem__(self, idx): input_size = torch.randint(self.min_input_size, self.max_input_size, (1,)).item() data = torch.randn(input_size) label = torch.randint(0, 2, (1,)) return data, label class SimpleModel(nn.Module): def __init__(self, input_size=10): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(input_size, 20) self.fc2 = nn.Linear(20, 1) def forward(self, x): x = nn.functional.pad(x, (0, 10 - x.size(0))) x = torch.relu(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) return x def train(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) dataset = RandomSizeDataset(size=100) dataloader = DataLoader(dataset, batch_size=4, shuffle=True) model = SimpleModel() model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) criterion = nn.BCELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) num_epochs = 10 for epoch in range(num_epochs): epoch_loss = 0 for data, labels in dataloader: optimizer.zero_grad() outputs = model(data) labels = labels.float().unsqueeze(1) loss = criterion(outputs, labels) loss.backward() optimizer.step() epoch_loss += loss.item() print(f\\"Epoch [{epoch+1}/{num_epochs}], Rank: {rank}, Loss: {epoch_loss / len(dataloader)}\\") dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 2 torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size)"},{"question":"Your task is to implement a Python function that takes a directory path as input and performs the following: 1. Lists all the Python files (.py) in the directory and its subdirectories. 2. For each identified Python file, retrieve and print its configuration information using the \\"sysconfig\\" module. 3. Inspect and print the live objects (functions, classes, etc.) within each file using the \\"inspect\\" module. 4. Create a data class representing the key details of each Python file (such as filename, size, etc.) 5. Use a context manager from the \\"contextlib\\" module to ensure that all opened files are closed properly after reading. # Input - `directory_path`: A string representing the directory path. # Output - Print the list of Python files along with their configuration information and inspected live objects. # Constraints 1. The function should handle exceptions gracefully (e.g., permission errors, file not found errors). 2. Use the appropriate standard library functions to navigate the file system. # Performance Requirements 1. The implementation should be efficient in terms of both time and space complexity, considering large directories with many Python files. # Example ```python import os import sysconfig import inspect from dataclasses import dataclass from contextlib import contextmanager @dataclass class PythonFileInfo: filename: str size: int config_info: dict @contextmanager def open_file(file_path): try: file = open(file_path, \'r\') yield file except Exception as e: print(f\\"An error occurred: {e}\\") finally: file.close() def list_python_files(directory_path): for root, _, files in os.walk(directory_path): for file in files: if file.endswith(\'.py\'): yield os.path.join(root, file) def get_file_info(file_path): size = os.path.getsize(file_path) config_info = sysconfig.get_config_vars() live_objects = {} with open_file(file_path) as f: source_code = f.read() exec(source_code, live_objects) return PythonFileInfo( filename=os.path.basename(file_path), size=size, config_info=config_info ), live_objects def process_directory(directory_path): python_files = list_python_files(directory_path) for file_path in python_files: file_info, live_objects = get_file_info(file_path) print(file_info) for name, obj in live_objects.items(): if inspect.isclass(obj) or inspect.isfunction(obj): print(f\'{name}: {obj}\') # Example usage: process_directory(\'/path/to/directory\') ``` Please implement the `process_directory` function as described.","solution":"import os import sysconfig import inspect from dataclasses import dataclass from contextlib import contextmanager @dataclass class PythonFileInfo: filename: str size: int config_info: dict @contextmanager def open_file(file_path): try: file = open(file_path, \'r\') yield file except Exception as e: print(f\\"An error occurred: {e}\\") finally: file.close() def list_python_files(directory_path): for root, _, files in os.walk(directory_path): for file in files: if file.endswith(\'.py\'): yield os.path.join(root, file) def get_file_info(file_path): size = os.path.getsize(file_path) config_info = sysconfig.get_config_vars() live_objects = {} with open_file(file_path) as f: source_code = f.read() exec(source_code, live_objects) return PythonFileInfo( filename=os.path.basename(file_path), size=size, config_info=config_info ), live_objects def process_directory(directory_path): python_files = list_python_files(directory_path) for file_path in python_files: file_info, live_objects = get_file_info(file_path) print(file_info) for name, obj in live_objects.items(): if inspect.isclass(obj) or inspect.isfunction(obj): print(f\'{name}: {obj}\')"},{"question":"# Custom Descriptor Classes in Python **Objective:** Implement custom descriptor classes in Python to control the access, modification, and deletion of instance attributes, demonstrating the use of the descriptor protocol. **Problem Statement:** You are required to implement three custom descriptor classes: `Propertied`, `Membered`, and `ClassMethed`, each demonstrating a unique aspect of the descriptor protocol. Additionally, you need to create a sample class `Sample` that utilizes these descriptors to manage its attributes. 1. **Propertied**: This class should manage an attribute, allowing it to be dynamically retrieved, set, and deleted. Implement `__get__`, `__set__`, and `__delete__` methods. 2. **Membered**: This class should store the attribute in a specific instance dictionary. Implement `__get__` and `__set__` to handle attribute access and modification by fetching and storing values in a dictionary. 3. **ClassMethed**: This class should create a custom class method. It will only implement the `__get__` method to demonstrate returning a class-level method. **Requirements:** - Implement the `Propertied`, `Membered`, and `ClassMethed` classes with the following methods: - `Propertied`: `__get__(self, instance, owner)`, `__set__(self, instance, value)`, `__delete__(self, instance)` - `Membered`: `__get__(self, instance, owner)`, `__set__(self, instance, value)` - `ClassMethed`: `__get__(self, instance, owner)` - Create a sample class `Sample` using these descriptors: - A class attribute `prop` using `Propertied`. - An instance attribute `mem` using `Membered`. - A class method `cls_meth` using `ClassMethed`. - Ensure that all descriptor methods appropriately manage attribute access and modification, raising errors where necessary. **Input:** No input required. **Output:** No output required, but the functionality should be demonstrated through unit tests. **Constraints:** - Implement the descriptors without using built-in decorators like `@property` or `@staticmethod`. - Ensure robust error handling in each descriptor class. - Use Python 3.10 features and practices. Here\'s a skeleton to get you started: ```python class Propertied: def __get__(self, instance, owner): # Implement retrieval logic pass def __set__(self, instance, value): # Implement setting logic pass def __delete__(self, instance): # Implement deletion logic pass class Membered: def __get__(self, instance, owner): # Implement retrieval logic pass def __set__(self, instance, value): # Implement setting logic pass class ClassMethed: def __get__(self, instance, owner): # Implement class method logic pass class Sample: prop = Propertied() mem = Membered() @ClassMethed def cls_meth(cls): # Implement class method logic pass # Unit tests to demonstrate functionality def test_descriptors(): s = Sample() # Tests for Propertied # Tests for Membered # Tests for ClassMethed ``` Ensure your implementation passes all the test cases showcasing the proper use and management of attributes using custom descriptors.","solution":"class Propertied: def __get__(self, instance, owner): print(f\'Getting: {instance.__dict__.get(\\"_prop\\", None)}\') return instance.__dict__.get(\\"_prop\\", None) def __set__(self, instance, value): print(f\'Setting: {value}\') instance.__dict__[\\"_prop\\"] = value def __delete__(self, instance): print(\'Deleting\') instance.__dict__.pop(\\"_prop\\", None) class Membered: def __get__(self, instance, owner): print(f\'Getting from member {self}: {instance.__dict__.get(self, None)}\') return instance.__dict__.get(self, None) def __set__(self, instance, value): print(f\'Setting for member {self}: {value}\') instance.__dict__[self] = value class ClassMethed: def __get__(self, instance, owner): def class_method(*args, **kwargs): print(\'Class method called\') # you can define any behavior you want here return \'class method result\' return class_method class Sample: prop = Propertied() mem = Membered() cls_meth = ClassMethed()"},{"question":"# Python Coding Assessment: Custom Logging Handler **Objective:** Implement and configure a custom logging handler that logs messages to a file, rotates the log file based on size, and sends an alert email when a critical error occurs. **Requirements:** 1. **Logging Configuration:** - Log messages should be written to a file named \\"application.log\\". - The log file should roll over (rotate) when it reaches a size of 1 MB. - Keep up to 3 backup log files (i.e., `application.log.1`, `application.log.2`, `application.log.3`). - Format the log messages with the format: `%(asctime)s - %(name)s - %(levelname)s - %(message)s`. 2. **Email Notification:** - When a log message with level \\"CRITICAL\\" is logged, send an email to `admin@example.com`. - Use the following SMTP configuration for sending emails: - SMTP server: `smtp.example.com` - SMTP port: `587` - Use TLS security. - Sender email address: `noreply@example.com` - Sender email password: `your_password` (for assessment purposes, you can mock this functionality). 3. **Implementation:** - Implement a class `CustomLoggingHandler` that extends `logging.handlers.RotatingFileHandler`. - Override necessary methods to handle the critical log event for sending emails. - Ensure thread safety and efficient logging. **Input Format:** - No specific input format. **Output Format:** - No specific output format, but the implementation should demonstrate the functionality by logging messages of various levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). **Constraints:** - Use only standard libraries (`logging`, `logging.handlers`, `smtplib`, `email`). # Example ```python import logging from logging.handlers import RotatingFileHandler, SMTPHandler class CustomLoggingHandler(RotatingFileHandler): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # Configure the email handler self.email_handler = SMTPHandler( mailhost=(\\"smtp.example.com\\", 587), fromaddr=\\"noreply@example.com\\", toaddrs=[\\"admin@example.com\\"], subject=\\"Critical Log Alert\\", credentials=(\\"noreply@example.com\\", \\"your_password\\"), secure=() ) self.email_handler.setLevel(logging.CRITICAL) def emit(self, record): super().emit(record) if record.levelno == logging.CRITICAL: self.email_handler.emit(record) # Configure the custom handler custom_handler = CustomLoggingHandler( \\"application.log\\", maxBytes=1*1024*1024, backupCount=3 ) custom_handler.setLevel(logging.DEBUG) formatter = logging.Formatter(\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\") custom_handler.setFormatter(formatter) logger = logging.getLogger(\\"customLogger\\") logger.addHandler(custom_handler) logger.setLevel(logging.DEBUG) # Test logging logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") ``` **Note:** In your testing environment, replace the email credentials and SMTP settings with valid details, or mock the email functionality if actual email sending is not feasible.","solution":"import logging from logging.handlers import RotatingFileHandler, SMTPHandler class CustomLoggingHandler(RotatingFileHandler): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) # Configure the email handler self.email_handler = SMTPHandler( mailhost=(\\"smtp.example.com\\", 587), fromaddr=\\"noreply@example.com\\", toaddrs=[\\"admin@example.com\\"], subject=\\"Critical Log Alert\\", credentials=(\\"noreply@example.com\\", \\"your_password\\"), secure=() ) self.email_handler.setLevel(logging.CRITICAL) def emit(self, record): super().emit(record) if record.levelno == logging.CRITICAL: self.email_handler.emit(record) # Configure the custom handler custom_handler = CustomLoggingHandler( filename=\\"application.log\\", maxBytes=1*1024*1024, backupCount=3 ) custom_handler.setLevel(logging.DEBUG) formatter = logging.Formatter(\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\") custom_handler.setFormatter(formatter) logger = logging.getLogger(\\"customLogger\\") logger.addHandler(custom_handler) logger.setLevel(logging.DEBUG) # Test logging function def log_test_messages(): logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\")"},{"question":"**Question: Data Marshalling and Unmarshalling** You are tasked with implementing custom functions to serialize and deserialize Python objects using the marshalling functionalities described in the documentation. Your functions should interact with files to save and load data effectively. Complete the following two functions: 1. `serialize_objects(filename, objects, version)`: This function will take a filename, a list of Python objects, and a marshalling version. It will write each object to a file using the specified version. 2. `deserialize_objects(filename)`: This function will take a filename and return a list of deserialized Python objects read from the file. # Function Specifications serialize_objects * **Input**: * `filename` (str): The name of the file to write the serialized objects to. * `objects` (list): A list of Python objects to be serialized. * `version` (int): The marshalling version to use (0, 1, or 2). * **Output**: * None: This function writes data to a file and does not return any value. * **Constraints**: * The file must be opened in binary write mode. * Handle any errors that arise during the file operations gracefully. deserialize_objects * **Input**: * `filename` (str): The name of the file to read the serialized objects from. * **Output**: * `objects` (list): A list of Python objects read from the file. * **Constraints**: * The file must be opened in binary read mode. * Handle errors that could arise during the file operations, such as EOFError, ValueError, or TypeError. # Sample Usage ```python # Example objects to serialize objects = [123, \\"hello\\", [1, 2, 3], {\\"key\\": \\"value\\"}] filename = \'data_file.mar\' version = 2 # Serialize objects to a file serialize_objects(filename, objects, version) # Deserialize objects from the file recovered_objects = deserialize_objects(filename) print(recovered_objects) # Output should match the original objects list ``` # Notes - Use the provided functions from the marshalling module documentation to achieve the required functionality. - Ensure that the implementation handles marshalling and unmarshalling efficiently while respecting the specified file and error handling constraints.","solution":"import marshal def serialize_objects(filename, objects, version): Serialize a list of objects to a file using marshalling. Parameters: filename (str): The name of the file to write the serialized objects to. objects (list): A list of Python objects to be serialized. version (int): The marshalling version to use (0, 1, or 2). try: with open(filename, \'wb\') as file: for obj in objects: marshal.dump(obj, file, version) except Exception as e: print(f\\"An error occurred while serializing objects: {e}\\") def deserialize_objects(filename): Deserialize a list of objects from a file. Parameters: filename (str): The name of the file to read the serialized objects from. Returns: list: A list of deserialized Python objects. objects = [] try: with open(filename, \'rb\') as file: while True: try: obj = marshal.load(file) objects.append(obj) except EOFError: break except Exception as e: print(f\\"An error occurred while deserializing objects: {e}\\") return objects"},{"question":"Context Management in Asynchronous Programming Objective In this assessment, you are required to demonstrate your understanding of the `contextvars` module in Python. Specifically, you will write a function that makes use of `ContextVar` to manage context-local state in an asynchronous environment. Problem Statement Consider an async web server where each client request should maintain its own logging level, independently of other requests being processed concurrently. Using `contextvars`, you are to create and manage the logging context such that each request\'s logging level is isolated. Requirements 1. Create a `ContextVar` named `logging_level` with a default value of `\\"INFO\\"`. 2. Implement the function `set_logging_level(level: str) -> None`: - This function should set the logging level for the current context using the provided `level`. 3. Implement the function `get_logging_level() -> str`: - This function should return the current logging level for the context using `logging_level.get()`. 4. Write an async function `process_request(request_id: int, level: str)`: - This function simulates processing a request. It should: - Set the provided logging level using `set_logging_level`. - Perform some actions (simulated with `await asyncio.sleep`) using this logging level. - Finally, print the message: `\\"Request <request_id>: Logging level set to <logging_level>\\"`. 5. Write an async function `main()` that: - Creates and runs multiple requests concurrently with different logging levels by calling `process_request`. - Verifies that each request correctly maintains its logging level without interfering with others. Constraints - Use a default logging level of `\\"INFO\\"` if no level is provided. - The solution should handle concurrency effectively using `contextvars`. Example Usage Here is an example of what the output might look like when running the `main` function with tasks: ```python import asyncio import contextvars # Create logging_level ContextVar logging_level = contextvars.ContextVar(\'logging_level\', default=\\"INFO\\") def set_logging_level(level: str) -> None: logging_level.set(level) def get_logging_level() -> str: return logging_level.get() async def process_request(request_id: int, level: str): set_logging_level(level) await asyncio.sleep(1) # Simulate processing print(f\\"Request {request_id}: Logging level set to {get_logging_level()}\\") async def main(): tasks = [ process_request(1, \\"DEBUG\\"), process_request(2, \\"WARNING\\"), process_request(3, \\"ERROR\\"), process_request(4, \\"INFO\\") ] await asyncio.gather(*tasks) # To run the event loop asyncio.run(main()) ``` In this example, when `main` runs, it should handle multiple requests concurrently, each with its own logging level.","solution":"import asyncio import contextvars # Create logging_level ContextVar logging_level = contextvars.ContextVar(\'logging_level\', default=\\"INFO\\") def set_logging_level(level: str) -> None: logging_level.set(level) def get_logging_level() -> str: return logging_level.get() async def process_request(request_id: int, level: str): set_logging_level(level) await asyncio.sleep(1) # Simulate processing print(f\\"Request {request_id}: Logging level set to {get_logging_level()}\\") async def main(): tasks = [ process_request(1, \\"DEBUG\\"), process_request(2, \\"WARNING\\"), process_request(3, \\"ERROR\\"), process_request(4, \\"INFO\\") ] await asyncio.gather(*tasks) # To run the event loop asyncio.run(main())"},{"question":"Objective To assess your understanding and proficiency in using PyTorch, focusing on supported tensor operations and model building within TorchScript constraints. Problem Statement Implement a function `initialize_and_compute_tensors` that initializes three different types of tensors in PyTorch, performs a sequence of operations on them, constructs a small neural network using these tensors, and evaluates the results. Specific Requirements 1. **Tensor Initialization:** - Initialize a tensor `a` using `torch.empty` of shape `(3, 3)`. - Initialize a tensor `b` using `torch.linspace` with 10 values ranging from 0 to 1. - Initialize a tensor `c` using `torch.eye` of size 4x4. - Ensure that these tensors do not utilize the `requires_grad` argument during initialization. 2. **Tensor Operations:** - Reshape tensor `b` to a shape of (2, 5). - Perform matrix multiplication of tensors `a` and `c`. - Compute the element-wise sine of tensor `b` and assign it back to `b`. 3. **Neural Network Construction:** - Construct a simple feedforward neural network with one hidden layer using `torch.nn` modules: - The input layer should accept the reshaped tensor `b` (i.e., size 5). - The hidden layer should use `torch.nn.Linear` and consist of 8 units. - The output layer should use `torch.nn.Linear` and produce 4 units. - Apply a ReLU activation function between the input and hidden layer. 4. **Evaluation:** - Pass the final computed tensor `b` through the network. Constraints - Do not use any unsupported constructs as mentioned in the documentation provided. - Ensure the implementation is compatible with TorchScript where possible. Input - There are no explicit inputs to the function. Output - The function should output the final tensor obtained after evaluating tensor `b` through the neural network. Function Signature ```python import torch import torch.nn as nn import torch.nn.functional as F def initialize_and_compute_tensors() -> torch.Tensor: # Implementation here ``` Good luck! Make sure your code is efficient and adheres to the constraints specified.","solution":"import torch import torch.nn as nn def initialize_and_compute_tensors() -> torch.Tensor: # Tensor Initialization a = torch.empty(3, 3) b = torch.linspace(0, 1, 10).view(2, 5) c = torch.eye(4) # Tensor Operations b = torch.sin(b) # Since `a` is (3, 3) and `c` is (4, 4), they are not directly compatible for matrix multiplication. # Modifying `a` to (3, 4) to make it compatible for matrix multiplication. a = torch.empty(3, 4) result = torch.matmul(a, c) # Neural Network Construction class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(5, 8) self.fc2 = nn.Linear(8, 4) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x model = SimpleNN() # Evaluation output = model(b) return output"},{"question":"# Secure Password Storage You are tasked with creating a secure password storage system for a small web application using Python\'s `crypt` module. This system should be capable of: 1. Storing user passwords securely. 2. Verifying user passwords during login. Implement the following functions: 1. **`store_user_password(username: str, password: str, method=\'METHOD_SHA512\') -> str`**: - **Input**: - `username`: A string representing the username. - `password`: A string representing the user\'s password. - `method`: An optional string indicating the hashing method (default is `METHOD_SHA512`). - **Output**: - An encrypted string that includes both the username and the hashed password. - **Constraints**: - Ensure that the chosen method is valid and available on the current platform. - Use the `crypt.mksalt` function to generate a suitable salt for the chosen method. 2. **`verify_user_password(username: str, password: str, stored_data: str) -> bool`**: - **Input**: - `username`: A string representing the username. - `password`: A string that represents the password to verify. - `stored_data`: The encrypted string returned by `store_user_password`. - **Output**: - `True` if the `password` matches the hashed password stored in `stored_data`. - `False` otherwise. - **Constraints**: - Use secure comparison techniques to avoid timing attacks. - Handle cases where `stored_data` does not correspond to the given `username`. Example ```python user_data = store_user_password(\\"john_doe\\", \\"securepassword123\\") print(user_data) # Possible output: \'john_doe:6randomsalt...hashedpassword...\' is_valid = verify_user_password(\\"john_doe\\", \\"securepassword123\\", user_data) print(is_valid) # Output: True is_valid = verify_user_password(\\"john_doe\\", \\"incorrectpassword\\", user_data) print(is_valid) # Output: False ``` Implementation Details - Use the `crypt.crypt` function to hash the passwords. - Make sure to properly handle and extract the username and hashed password for storage and verification purposes. - Ensure that the system uses the strongest available hashing methods by default and validates the integrity of the provided method. - Store the output securely and give clear indications of any potential issues or errors. Demonstrate your solution with appropriate test cases and ensure that it adheres to best practices for security and performance.","solution":"import crypt def store_user_password(username: str, password: str, method=\'METHOD_SHA512\') -> str: Stores the user\'s password securely using the specified hashing method. Args: username (str): The user\'s username. password (str): The user\'s password. method (str): The hashing method to use (default is \'METHOD_SHA512\'). Returns: str: An encrypted string that includes both the username and the hashed password. # Ensure the chosen method is valid and available on the current platform method_dict = { \'METHOD_SHA256\': crypt.METHOD_SHA256, \'METHOD_SHA512\': crypt.METHOD_SHA512, \'METHOD_BLOWFISH\': crypt.METHOD_BLOWFISH, \'METHOD_MD5\': crypt.METHOD_MD5, } if method not in method_dict: raise ValueError(\\"Invalid hashing method provided.\\") # Generate salt using the chosen method salt = crypt.mksalt(method_dict[method]) # Hash the password with the generated salt hashed_password = crypt.crypt(password, salt) # Store username and hashed password in a specific format return f\\"{username}:{hashed_password}\\" def verify_user_password(username: str, password: str, stored_data: str) -> bool: Verifies the user\'s password against the stored data. Args: username (str): The user\'s username. password (str): The user\'s password to verify. stored_data (str): The stored encrypted string from store_user_password. Returns: bool: True if the password matches, False otherwise. try: stored_username, stored_hash = stored_data.split(\':\') except ValueError: return False if username != stored_username: return False # Extract the salt part from the stored hash salt = stored_hash.rsplit(\'\', 1)[0] # Hash the password using the same salt test_hash = crypt.crypt(password, salt) # Securely compare the generated hash with the stored hash return test_hash == stored_hash"},{"question":"Problem Statement **Objective**: Implement a function `process_nested_tensor` that takes a nested tensor containing variable-length sequences of vectors, performs a specified operation on each sequence (such as summing each vector component-wise), and returns the results in both nested and padded tensor formats. **Input**: - A list of PyTorch tensors (each tensor having variable lengths). - A string specifying the operation to be performed (\'sum\', \'mean\'). **Output**: - A tuple containing: - The result as a nested tensor. - The result as a padded tensor (with padding value -1). Constraints 1. The list of tensors must have the same number of dimensions (but variable lengths in one dimension). 2. The operations to be supported are: - \'sum\': Compute the sum of each vector end-to-end for each sequence. - \'mean\': Compute the mean of each vector end-to-end for each sequence. Example ```python import torch import torch.nested def process_nested_tensor(tensor_list, operation): # Implement the function here # Example usage: tensor_list = [torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8], [9, 10]])] operation = \'sum\' nested_result, padded_result = process_nested_tensor(tensor_list, operation) print(nested_result) # Expected: Nested tensor with summed sequences. print(padded_result) # Expected: Padded tensor with summed sequences and padding value -1 ``` **Note**: 1. Use `torch.nested.nested_tensor` to create the nested tensor. 2. Use `torch.nested.to_padded_tensor` to convert the nested tensor to a padded tensor with padding value -1. 3. Implement error handling for invalid operations and incompatible input tensors. Function Signature ```python def process_nested_tensor(tensor_list: list, operation: str) -> tuple: pass ``` # Evaluation Criteria 1. **Correctness**: The function should correctly perform the specified operations and return the expected results. 2. **Efficiency**: The solution should leverage NJT\'s strengths and be implemented efficiently. 3. **Error handling**: The function should handle invalid inputs gracefully and return appropriate error messages.","solution":"import torch import torch.nn.functional as F def process_nested_tensor(tensor_list, operation): if operation not in {\'sum\', \'mean\'}: raise ValueError(f\\"Unsupported operation \'{operation}\'. Only \'sum\' and \'mean\' are supported.\\") if not tensor_list: raise ValueError(\\"Input tensor_list is empty.\\") nested_tensor = torch.nested.nested_tensor(tensor_list) new_nested_tensor = [] for seq in nested_tensor.unbind(): if operation == \'sum\': new_seq = torch.sum(seq, dim=0) elif operation == \'mean\': new_seq = torch.mean(seq.float(), dim=0) new_nested_tensor.append(new_seq) padded_tensor = torch.nested.to_padded_tensor(torch.nested.nested_tensor(new_nested_tensor), padding=-1) return (torch.nested.nested_tensor(new_nested_tensor), padded_tensor)"},{"question":"**Problem Statement:** You are tasked with creating a custom Python environment emulator that tracks a few key system-specific properties and enables or disables certain behaviors based on specific inputs. **Objective:** 1. Create a class `PythonEnvironmentEmulator` that manages the system\'s execution environment. 2. Implement methods to: - Retrieve and display important system information. - Manage recursion limits and thread-switch intervals. - Track call graph during the execution of a function. - Enable and disable string interning. **Requirements:** 1. **Class Initialization:** - On initialization, print the current Python version, platform, and executable location using `sys.version`, `sys.platform`, and `sys.executable`. 2. **Methods:** - `get_system_info()`: Should return a dictionary with: - `recursion_limit` - `thread_switch_interval` - `maxsize` - `float_info` (as a dictionary with keys: `max`, `min`, `mant_dig`) - `is_finalizing` - `set_recursion_limit(limit: int)`: Sets a new recursion limit using `sys.setrecursionlimit()`. - `set_thread_switch_interval(interval: float)`: Sets a new thread-switch interval using `sys.setswitchinterval()`. - `track_call_graph(func)`: Should execute the given function `func` and return a dictionary representing the call stack graph (keyed by function name), leveraging `sys._getframe()`. - `manage_interning(strings: list)`: Interns a list of strings using `sys.intern()` and returns both the original and interned strings. **Constraints:** - The emulator should only run on Python versions 3.8 or above. - Recursion limit must be within the range `50` to `2000`. - Thread-switch interval must be between `0.001` and `0.1` seconds. - For `track_call_graph()`, limit the depth of frame retrieval to 10. **Performance Requirements:** - The methods should complete efficiently without redundant operations. ```python import sys class PythonEnvironmentEmulator: def __init__(self): assert sys.version_info >= (3, 8), \\"Python 3.8 or above is required\\" print(f\\"Python Version: {sys.version}\\") print(f\\"Platform: {sys.platform}\\") print(f\\"Executable: {sys.executable}\\") def get_system_info(self): return { \\"recursion_limit\\": sys.getrecursionlimit(), \\"thread_switch_interval\\": sys.getswitchinterval(), \\"maxsize\\": sys.maxsize, \\"float_info\\": { \\"max\\": sys.float_info.max, \\"min\\": sys.float_info.min, \\"mant_dig\\": sys.float_info.mant_dig }, \\"is_finalizing\\": sys.is_finalizing() } def set_recursion_limit(self, limit): if 50 <= limit <= 2000: sys.setrecursionlimit(limit) else: raise ValueError(\\"Recursion limit must be between 50 and 2000\\") def set_thread_switch_interval(self, interval): if 0.001 <= interval <= 0.1: sys.setswitchinterval(interval) else: raise ValueError(\\"Thread-switch interval must be between 0.001 and 0.1 seconds\\") def track_call_graph(self, func): def get_frame_info(frame, depth): if depth > 10 or frame is None: return None return { \\"function\\": frame.f_code.co_name, \\"caller\\": get_frame_info(frame.f_back, depth+1) } result = func() return get_frame_info(sys._getframe(), 0) def manage_interning(self, strings): original_intern = {s: sys.intern(s) for s in strings} return original_intern ``` **Example Usage:** ```python emulator = PythonEnvironmentEmulator() print(emulator.get_system_info()) emulator.set_recursion_limit(1000) emulator.set_thread_switch_interval(0.01) print(emulator.track_call_graph(lambda: sum(range(10)))) print(emulator.manage_interning([\\"hello\\", \\"world\\", \\"hello\\", \\"python\\"])) ``` **Output:** Expected output when running the example usage should look like: ``` Python Version: 3.9.5 (default, Jun 4 2021, 15:09:15) [GCC 8.4.0] Platform: linux Executable: /usr/bin/python3 { \'recursion_limit\': 1000, \'thread_switch_interval\': 0.005, \'maxsize\': 9223372036854775807, \'float_info\': { \'max\': 1.7976931348623157e+308, \'min\': 2.2250738585072014e-308, \'mant_dig\': 53 }, \'is_finalizing\': False } { \'function\': \'lambda\', \'caller\': { \'function\': \'track_call_graph\', \'caller\': { \'function\': \'__init__\', \'caller\': { # More nested frame information up to 10 levels }}}} { \'hello\': \'hello\'(interned), \'world\': \'world\'(interned), \'python\': \'python\'(interned) } ```","solution":"import sys class PythonEnvironmentEmulator: def __init__(self): assert sys.version_info >= (3, 8), \\"Python 3.8 or above is required\\" print(f\\"Python Version: {sys.version}\\") print(f\\"Platform: {sys.platform}\\") print(f\\"Executable: {sys.executable}\\") def get_system_info(self): return { \\"recursion_limit\\": sys.getrecursionlimit(), \\"thread_switch_interval\\": sys.getswitchinterval(), \\"maxsize\\": sys.maxsize, \\"float_info\\": { \\"max\\": sys.float_info.max, \\"min\\": sys.float_info.min, \\"mant_dig\\": sys.float_info.mant_dig }, \\"is_finalizing\\": sys.is_finalizing() } def set_recursion_limit(self, limit): if 50 <= limit <= 2000: sys.setrecursionlimit(limit) else: raise ValueError(\\"Recursion limit must be between 50 and 2000\\") def set_thread_switch_interval(self, interval): if 0.001 <= interval <= 0.1: sys.setswitchinterval(interval) else: raise ValueError(\\"Thread-switch interval must be between 0.001 and 0.1 seconds\\") def track_call_graph(self, func): def get_frame_info(frame, depth): if depth > 10 or frame is None: return None return { \\"function\\": frame.f_code.co_name, \\"caller\\": get_frame_info(frame.f_back, depth+1) } func() return get_frame_info(sys._getframe(), 0) def manage_interning(self, strings): original_intern = {s: sys.intern(s) for s in strings} return original_intern"},{"question":"Create a Python function called `create_distribution` that automates the process of creating a built distribution for a given Python package. The function should: 1. Accept the following parameters: - `setup_script_path` (str): The path to the setup script of the package. - `formats` (list of str): A list of distribution formats to generate (e.g., `[\\"gztar\\", \\"zip\\"]`). - `spec_only` (bool): A flag indicating whether to generate only the `.spec` file (default is `False`). - `dist_dir` (str, optional): The directory to place the built distributions. If not provided, the default directory should be used. - `post_install_script` (str, optional): The path to a postinstallation script. If provided, it should be embedded in the installer. 2. Validate the provided parameters. 3. Use the `distutils` library to execute the appropriate `bdist` commands based on the provided formats. 4. Handle optional parameters such as `dist_dir` and `post_install_script` to customize the distribution process. 5. Return a dictionary containing details of the created distributions, such as file paths and formats. # Constraints - The function should support, at minimum, the following formats: `\\"gztar\\"`, `\\"zip\\"`, `\\"rpm\\"`, and `\\"msi\\"`. - The `spec_only` flag should only apply when the format includes `\\"rpm\\"`. - The `post_install_script` should be optional and applicable only when creating installers that support postinstallation scripts, such as the `\\"msi\\"` format. # Example ```python def create_distribution(setup_script_path, formats, spec_only=False, dist_dir=None, post_install_script=None): # Your implementation here # Example usage: result = create_distribution( setup_script_path=\'/path/to/setup.py\', formats=[\'gztar\', \'zip\'], spec_only=False, dist_dir=\'/path/to/dist\', post_install_script=\'/path/to/postinstall.py\' ) ``` # Expected Output A dictionary with details of the created distributions. For example: ```python { \\"gztar\\": \\"/path/to/dist/package-1.0.gztar\\", \\"zip\\": \\"/path/to/dist/package-1.0.zip\\" } ``` # Notes - Ensure the function performs appropriate error handling and validation. - You may need to simulate or mock parts of the `distutils` operations during testing if actual packaging cannot be performed in the testing environment.","solution":"import os import subprocess from distutils.errors import DistutilsOptionError def create_distribution(setup_script_path, formats, spec_only=False, dist_dir=None, post_install_script=None): if not os.path.isfile(setup_script_path): raise FileNotFoundError(f\\"Setup script {setup_script_path} does not exist.\\") if not isinstance(formats, list) or not all(isinstance(fmt, str) for fmt in formats): raise ValueError(\\"Formats should be a list of strings.\\") if spec_only and \'rpm\' not in formats: raise ValueError(\\"Spec_only flag can only be used with \'rpm\' format.\\") dist_dir_option = f\'--dist-dir {dist_dir}\' if dist_dir else \\"\\" spec_only_option = \'--spec-only\' if spec_only else \\"\\" postinstall_option = f\'--post-install {post_install_script}\' if post_install_script else \\"\\" details = {} for fmt in formats: if fmt not in [\\"gztar\\", \\"zip\\", \\"rpm\\", \\"msi\\"]: raise DistutilsOptionError(f\\"Unsupported format: {fmt}\\") command = f\\"python {setup_script_path} bdist --formats={fmt} {dist_dir_option} {spec_only_option} {postinstall_option}\\" result = subprocess.run(command, shell=True, capture_output=True) if result.returncode != 0: raise RuntimeError(f\\"Failed to create distribution: {result.stderr.decode()}\\") # Assuming the output generated files match the format specified output_files = [os.path.join(dist_dir or \'dist\', f\\"{os.path.basename(setup_script_path)}-0.0.0.{fmt}\\")] details[fmt] = output_files return details"},{"question":"Objective You are required to implement a function that simulates a simple file processing operation while handling potential errors using the `errno` module. The goal is to read from a file, process its contents, and write the results to another file, gracefully handling any errors that might occur during these operations. Task Write a Python function `file_processing(input_file: str, output_file: str) -> str` that reads from `input_file`, processes its contents, and writes the processed data to `output_file`. The function should handle potential errors using the `errno` module and return a specific error message depending on the encountered error. Input - `input_file` (str): The path to the file to be read. - `output_file` (str): The path to the file where the processed data will be written. Output - The function should return \\"Success\\" if the operation completes without errors. - If an error occurs, return an error message corresponding to the type of error encountered. Use the mapping provided by the `errno` module to identify and handle the errors. Constraints - Do not use any additional external libraries. - Ensure proper file handling using context managers (`with` statement). - Demonstrate handling at least the following errors by returning the corresponding string messages: - `FileNotFoundError`: \\"Error: No such file or directory\\" - `PermissionError`: \\"Error: Permission denied\\" - Any other error should return the generic message format: \\"Error: {error_message}\\" Example ```python def file_processing(input_file: str, output_file: str) -> str: # Your implementation here pass # Example Usage input_path = \\"input.txt\\" output_path = \\"output.txt\\" result = file_processing(input_path, output_path) print(result) ``` **Explanation:** - If `input.txt` does not exist, the function should return \\"Error: No such file or directory\\". - If the script does not have permission to read `input.txt` or write to `output.txt`, it should return \\"Error: Permission denied\\". - If any other error occurs, it should return a message formatted as \\"Error: {error_message}\\", where `{error_message}` is the string representation of the error. Use the built-in `errno` module to lookup the error codes and their respective message strings when handling exceptions.","solution":"import errno def file_processing(input_file: str, output_file: str) -> str: Reads from input_file, processes its contents, and writes the processed data to output_file. Handles potential errors using the errno module and returns a specific error message. try: with open(input_file, \'r\') as infile: data = infile.read() # Example processing: Convert to uppercase processed_data = data.upper() with open(output_file, \'w\') as outfile: outfile.write(processed_data) return \\"Success\\" except FileNotFoundError: return \\"Error: No such file or directory\\" except PermissionError: return \\"Error: Permission denied\\" except OSError as e: return f\\"Error: {e.strerror}\\""},{"question":"# Question: Directory Tree Summary You are required to implement a function `generate_summary` that generates a summary of a directory tree. This summary will include the number of directories, the number of files, and a list of all the unique file types (extensions) within the directory tree. Function Signature ```python def generate_summary(directory: Path) -> dict: ``` Input - `directory` (Path): a Path object representing the root of the directory tree to be analyzed. You can assume that this path always exists and is a directory. Output - `dict`: A dictionary with the following structure: ```python { \\"total_directories\\": int, # Total number of subdirectories \\"total_files\\": int, # Total number of files (recursively including all subdirectories) \\"unique_extensions\\": set # A set of file extensions (including \'.\', e.g., \'.py\', \'.txt\'). If a file has no extension, use an empty string (\'\') } ``` Constraints - The function should efficiently handle large directory trees. - Use appropriate `pathlib` methods to navigate and analyze the directory tree. Example ```python from pathlib import Path # Example directory tree: # test_dir/ # â”œâ”€â”€ dir1 # â”‚ â”œâ”€â”€ file1.txt # â”‚ â”œâ”€â”€ file2.py # â”œâ”€â”€ dir2 # â”‚ â”œâ”€â”€ dir2_sub1 # â”‚ â”‚ â”œâ”€â”€ file3.jpg # â”‚ â”œâ”€â”€ file4 # â”œâ”€â”€ file5.md summary = generate_summary(Path(\'test_dir\')) print(summary) # Expected Output (order of elements in the set does not matter): # { # \\"total_directories\\": 3, # \\"total_files\\": 5, # \\"unique_extensions\\": {\'.txt\', \'.py\', \'.jpg\', \'\', \'.md\'} # } ``` # Requirements - Do not use any modules outside of the standard library. - Ensure your code is well-documented and handles edge cases gracefully. # Hints - Use `Path.iterdir()` to iterate over entries in a directory. - Use `Path.suffix` to identify file extensions. - Recursively process subdirectories to gather statistics. - Use appropriate methods to ensure the paths are resolved correctly.","solution":"from pathlib import Path def generate_summary(directory: Path) -> dict: Generate a summary of a directory tree. Args: directory (Path): a Path object representing the root of the directory tree to be analyzed. Returns: dict: A dictionary containing the number of directories, number of files, and a set of unique file types (extensions). total_directories = 0 total_files = 0 unique_extensions = set() def traverse_dir(path: Path): nonlocal total_directories, total_files, unique_extensions for entry in path.iterdir(): if entry.is_dir(): total_directories += 1 traverse_dir(entry) elif entry.is_file(): total_files += 1 ext = entry.suffix if entry.suffix else \'\' unique_extensions.add(ext) traverse_dir(directory) return { \\"total_directories\\": total_directories, \\"total_files\\": total_files, \\"unique_extensions\\": unique_extensions }"},{"question":"# Python Coding Assessment Question Objective You will write a series of functions that measure the execution time of specific Python code snippets using the `timeit` module. Task 1. Implement a function named `compare_execution_times` that takes a list of tuples, where each tuple contains: - A string representing a Python statement. - A string representing the setup code required for the statement (optional). 2. The function should time the execution of each statement 10,000 times and return the fastest execution time for each statement in the form of a list of tuples. Each tuple should include: - The original statement. - The fastest execution time of that statement. Function Signature ```python def compare_execution_times(statements: list[tuple[str, str]]) -> list[tuple[str, float]]: ``` Example ```python # Example Input statements = [ (\'\\"-\\".join(str(n) for n in range(100))\', \'\'), (\'\\"-\\".join([str(n) for n in range(100)])\', \'\'), (\'\\"-\\".join(map(str, range(100)))\', \'\'), (\'char in text\', \'text = \\"sample string\\"; char = \\"g\\"\'), (\'text.find(char)\', \'text = \\"sample string\\"; char = \\"g\\"\') ] # Example Output [ (\'\\"-\\".join(str(n) for n in range(100))\', 0.2767183800112009), (\'\\"-\\".join([str(n) for n in range(100)])\', 0.21959427098863477), (\'\\"-\\".join(map(str, range(100)))\', 0.16892446801161766), (\'char in text\', 0.3821689250118168), (\'text.find(char)\', 0.73275112592183) ] ``` Constraints 1. Each statement should be executed 10,000 times for timing. 2. If a setup code is provided, it should be executed once before timing the main statement. 3. Use the `timeit` module for accurate timing of the statements. Notes - Assume that each statement and setup code provided is valid and does not require error handling. - The statements and their setup codes, if any, will not modify the global state outside the intended context of timing. - Consider using `timeit`\'s callable interface for better structure and clarity in the implementation. Implementation Details - Utilize `timeit.timeit` for measuring the execution time of each statement. - Ensure each statement runs within its specific setup environment. - Return the results as specified, ensuring the timings are accurate and formatted correctly.","solution":"import timeit def compare_execution_times(statements): results = [] for statement, setup in statements: # If setup is not provided, default to an empty string setup = setup if setup is not None else \'\' # Measure the execution time timer = timeit.timeit(stmt=statement, setup=setup, number=10000) # Append the tuple (statement, execution time) to the results list results.append((statement, timer)) return results"},{"question":"**Question: Implement a Turtle Spirograph Drawer** **Objective:** Using the `turtle` module, implement a function `draw_spirograph(radius, angle_increment)` that draws a spirograph pattern. The spirograph should be colorful, and the turtle should return to its starting position and heading at the end. **Function Signature:** ```python def draw_spirograph(radius: float, angle_increment: float): pass ``` **Input:** - `radius` (float): The radius of the circle that the turtle will draw at each step. - `angle_increment` (float): The angle in degrees by which the turtle rotates after each circle to create the spirograph effect. **Constraints:** - `radius` will be a positive number. - `angle_increment` will be between 1 and 360 degrees. **Output:** - The function does not return any value but should display the spirograph on the screen. **Requirements:** 1. The function should use the `turtle` module to draw the spirograph. 2. Utilize different colors for each circle to make the spirograph colorful. 3. Ensure the turtle returns to its starting position and heading after drawing the spirograph. 4. Use appropriate `turtle` methods for moving, drawing, and color changes. **Example Usage:** ```python draw_spirograph(100, 45) ``` **Explanation:** In the example, the turtle would draw circles with a radius of 100 units. After drawing each circle, the turtle would turn right by 45 degrees and then draw the next circle, creating a colorful spirograph pattern. **Hints:** - Use a loop to draw multiple circles, turning the turtle by `angle_increment` degrees after each circle. - Use `turtle.circle(radius)` to draw a circle. - Use `turtle.right(angle_increment)` to turn the turtle. - Use `turtle.color()` to change the pen color. **Additional Notes:** - Remember to import the `turtle` module. - At the end of your function, use `turtle.done()` to ensure the window does not close automatically after finishing.","solution":"import turtle import random def draw_spirograph(radius: float, angle_increment: float): Draws a colorful spirograph pattern using the turtle module. Parameters: radius (float): The radius of the circles. angle_increment (float): The angle increment in degrees for the spirograph. turtle.speed(\'fastest\') # Speed up the drawing turtle.colormode(255) # Allow RGB colors for angle in range(0, 360, int(angle_increment)): turtle.color(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) turtle.circle(radius) turtle.right(angle_increment) # Ensure turtle returns to starting position and heading turtle.setheading(0) turtle.penup() turtle.home() turtle.pendown() turtle.done()"},{"question":"# PyTorch Conditional Control Flow Exercise Problem Statement You are required to implement a PyTorch module that uses the `torch.cond` function to control the flow of your computation dynamically. The module should apply different transformations to a tensor based on its sum and its shape. This will help demonstrate your understanding of advanced control flow in PyTorch. Task Create a class `AdaptiveTransform` that inherits from `torch.nn.Module`. This class should provide a `forward` method that takes a single tensor `x` as input and returns a transformed tensor. The class should conditionally apply different functions to `x` based on the following criteria: 1. **Sum-based Condition**: If the sum of all elements in `x` is greater than a threshold (`sum_threshold`), apply `sum_true_fn(x)`. Otherwise, apply `sum_false_fn(x)`. 2. **Shape-based Condition**: Regardless of the sum-based transformation, if the shape of the first dimension of `x` is greater than a threshold (`shape_threshold`), apply `shape_true_fn` to the result. Otherwise, apply `shape_false_fn`. Implement these functions as follows: - `sum_true_fn(x)`: Returns `x` multiplied by 2. - `sum_false_fn(x)`: Returns `x` divided by 2. - `shape_true_fn(x)`: Adds the cosine of `x` to `x`. - `shape_false_fn(x)`: Adds the sine of `x` to `x`. Here are the detailed steps and requirements: 1. Create a class `AdaptiveTransform` that inherits from `torch.nn.Module`. 2. Define an `__init__` method that accepts two parameters `sum_threshold` and `shape_threshold`. 3. Implement the `forward` method to apply the described conditions using `torch.cond`. Input - A tensor `x` (of shape `[N, *]` where N can vary). - Thresholds: `sum_threshold (float)` and `shape_threshold (int)`. Output - A transformed tensor based on the conditions and functions. Example ```python import torch class AdaptiveTransform(torch.nn.Module): def __init__(self, sum_threshold: float, shape_threshold: int): super(AdaptiveTransform, self).__init__() self.sum_threshold = sum_threshold self.shape_threshold = shape_threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def sum_true_fn(x: torch.Tensor) -> torch.Tensor: return x * 2 def sum_false_fn(x: torch.Tensor) -> torch.Tensor: return x / 2 def shape_true_fn(x: torch.Tensor) -> torch.Tensor: return x + x.cos() def shape_false_fn(x: torch.Tensor) -> torch.Tensor: return x + x.sin() sum_based_result = torch.cond(x.sum() > self.sum_threshold, sum_true_fn, sum_false_fn, (x,)) final_result = torch.cond(sum_based_result.shape[0] > self.shape_threshold, shape_true_fn, shape_false_fn, (sum_based_result,)) return final_result # Example usage model = AdaptiveTransform(sum_threshold=10.0, shape_threshold=4) input_tensor = torch.randn(5, 3) output_tensor = model(input_tensor) print(output_tensor) ``` Ensure your implementation is efficient and adheres to PyTorch coding standards.","solution":"import torch import torch.nn as nn class AdaptiveTransform(nn.Module): def __init__(self, sum_threshold: float, shape_threshold: int): super(AdaptiveTransform, self).__init__() self.sum_threshold = sum_threshold self.shape_threshold = shape_threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def sum_true_fn(x: torch.Tensor) -> torch.Tensor: return x * 2 def sum_false_fn(x: torch.Tensor) -> torch.Tensor: return x / 2 def shape_true_fn(x: torch.Tensor) -> torch.Tensor: return x + x.cos() def shape_false_fn(x: torch.Tensor) -> torch.Tensor: return x + x.sin() if x.sum() > self.sum_threshold: sum_based_result = sum_true_fn(x) else: sum_based_result = sum_false_fn(x) if sum_based_result.shape[0] > self.shape_threshold: final_result = shape_true_fn(sum_based_result) else: final_result = shape_false_fn(sum_based_result) return final_result"},{"question":"**Question: Word Count Analysis Using `collections.Counter`** You are required to implement a Python function to analyze the word frequencies in a given text file. The function should read the file, clean and process the text, and then use the `collections.Counter` class to count the occurrences of each word. Finally, the function should return the `n` most common words along with their counts. # Function Signature ```python def word_count_analysis(file_path: str, n: int) -> List[Tuple[str, int]]: pass ``` # Input - `file_path` (str): The path to the text file that needs to be analyzed. - `n` (int): The number of top common words to return. # Output - A list of tuples, where each tuple contains a word (str) and its corresponding count (int). The list should be sorted in descending order of frequency. # Constraints - Assume that the text file is in English and may contain punctuation, which should be removed during processing. - Words are case-insensitive, meaning \\"Apple\\" and \\"apple\\" should be counted as the same word. - You may use regular expressions to clean and process the text. # Example Suppose the contents of the text file at `file_path` are as follows: ``` Hello world! This is a test. This test is only a test. ``` For `n = 3`, the function should return: ```python [(\'test\', 3), (\'this\', 2), (\'is\', 2)] ``` # Additional Notes - The function should handle large text files efficiently. - You might want to use functions from the `re` module to help with cleaning the text. - Don\'t forget to handle exceptions that might occur while reading the file. Implement the `word_count_analysis` function to meet the requirements described above. # Solution Example (Not to be included in the question) ```python from collections import Counter import re from typing import List, Tuple def word_count_analysis(file_path: str, n: int) -> List[Tuple[str, int]]: try: with open(file_path, \'r\') as file: text = file.read() # Normalize the text by converting it to lowercase and removing punctuation text = re.sub(r\'[^ws]\', \'\', text.lower()) # Split the text into words words = text.split() # Use Counter to get word frequencies word_counts = Counter(words) # Get the n most common words common_words = word_counts.most_common(n) return common_words except Exception as e: print(f\\"An error occurred: {e}\\") return [] # Example usage # print(word_count_analysis(\\"sample.txt\\", 3)) ```","solution":"from collections import Counter import re from typing import List, Tuple def word_count_analysis(file_path: str, n: int) -> List[Tuple[str, int]]: try: with open(file_path, \'r\') as file: text = file.read() # Normalize the text by converting it to lowercase and removing punctuation text = re.sub(r\'[^ws]\', \'\', text.lower()) # Split the text into words words = text.split() # Use Counter to get word frequencies word_counts = Counter(words) # Get the n most common words common_words = word_counts.most_common(n) return common_words except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Terminal Mode Control Script You are required to write a Python script that demonstrates the use of the \\"tty\\" module for controlling terminal modes. Specifically, the script should handle switching the terminal of the user between \\"raw\\" and \\"cbreak\\" modes based on user input. This requires a fundamental understanding of file descriptors and terminal behaviors. Task: 1. Write a function named `switch_terminal_mode` that takes two arguments: - `fd`: A file descriptor representing the terminal. - `mode`: A string that can either be `\\"raw\\"` or `\\"cbreak\\"`, indicating which mode to set for the terminal. 2. The function should use the appropriate `tty` module functions `setraw` or `setcbreak` based on the mode provided. 3. Write a script that: - Takes input from the user to decide which mode to set. - Calls the `switch_terminal_mode` function with the appropriate arguments. Implementation Details: - You can assume that the terminal\'s file descriptor is 0 (stdin). - Add error handling to manage invalid mode input or other exceptions. - Ensure to catch any issues that arise from changing terminal modes and provide meaningful feedback. Example Usage: ```python # Example of interactive usage: # python3 your_script.py Choose terminal mode (raw/cbreak): raw Terminal set to raw mode. Choose terminal mode (raw/cbreak): cbreak Terminal set to cbreak mode. Choose terminal mode (raw/cbreak): exit Exiting terminal mode control. ``` Important Notes: - Make sure your script correctly handles changing the terminal modes without causing it to hang. - This script should work on Unix-based systems as the `tty` module relies on Unix-specific features. - Do not forget to reset the terminal to its normal state when the script exits. Use the following template to get started: ```python import tty import termios import sys def switch_terminal_mode(fd, mode): if mode == \\"raw\\": tty.setraw(fd) print(\\"Terminal set to raw mode.\\") elif mode == \\"cbreak\\": tty.setcbreak(fd) print(\\"Terminal set to cbreak mode.\\") else: raise ValueError(\\"Invalid mode. Choose \'raw\' or \'cbreak\'.\\") def main(): fd = sys.stdin.fileno() while True: mode = input(\\"Choose terminal mode (raw/cbreak/exit): \\").strip().lower() if mode == \\"exit\\": print(\\"Exiting terminal mode control.\\") break try: switch_terminal_mode(fd, mode) except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": main() ``` Test your script thoroughly to ensure it behaves as expected.","solution":"import tty import termios import sys def switch_terminal_mode(fd, mode): Switches the terminal mode to either \'raw\' or \'cbreak\' based on the mode provided. Args: fd (int): The file descriptor representing the terminal. mode (str): The mode to switch to, either \'raw\' or \'cbreak\'. Raises: ValueError: If an invalid mode is provided. if mode == \\"raw\\": tty.setraw(fd) print(\\"Terminal set to raw mode.\\") elif mode == \\"cbreak\\": tty.setcbreak(fd) print(\\"Terminal set to cbreak mode.\\") else: raise ValueError(\\"Invalid mode. Choose \'raw\' or \'cbreak\'.\\") def main(): fd = sys.stdin.fileno() while True: mode = input(\\"Choose terminal mode (raw/cbreak/exit): \\").strip().lower() if mode == \\"exit\\": print(\\"Exiting terminal mode control.\\") break try: switch_terminal_mode(fd, mode) except Exception as e: print(f\\"Error: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Implement a custom debugger using the `bdb` module in Python that incorporates breakpoints and handles basic debugging events. **Task:** Create a subclass of `bdb.Bdb` named `CustomDebugger` with the following features: 1. **Initialization:** - Implement an `__init__` method for initializing the base class and any additional properties you need. 2. **Managing Breakpoints:** - Implement methods to set and clear breakpoints. - Use the `set_break` and `clear_break` methods provided in `bdb.Bdb`. 3. **Event Handling:** - Implement the `user_call`, `user_line`, `user_return`, and `user_exception` methods to handle function calls, line execution, return events, and exceptions. - Print appropriate messages when these events occur, including useful information like the current filename, line number, and function name. 4. **Running Code:** - Add a method to run specific pieces of code using the debugger. - Use the `runcall` method to debug function calls and showcase the handling of breakpoints and events. **Input Format:** - There will be no direct input from the user. Instead, your class should be callable to test its functionality. **Expected Output Format:** - Your methods should print appropriate debugging information to the console as specified. **Example Usage:** ```python # Example function to debug def add(a, b): return a + b # Test script if __name__ == \\"__main__\\": # Initialize the custom debugger debugger = CustomDebugger() # Set breakpoints debugger.set_break(\\"example.py\\", 2) # Assuming the function \'add\' is on line 2 # Run the debugger with a function call result = debugger.runcall(add, 5, 10) print(f\\"Result: {result}\\") ``` **Constraints:** - Ensure the event handling methods (user_call, user_line, etc.) accurately reflect when the debugger should stop and interact. **Evaluation Criteria:** - Correct usage of `bdb` module classes and methods. - Proper handling and printing of debugging events. - Accurate implementation of breakpoint management. - Overall functionality and clarity of the custom debugger.","solution":"import bdb class CustomDebugger(bdb.Bdb): def __init__(self): super().__init__() self.breakpoints = [] def set_breakpoint(self, filename, lineno): self.set_break(filename, lineno) self.breakpoints.append((filename, lineno)) print(f\\"Breakpoint set at {filename}, line {lineno}\\") def clear_breakpoint(self, filename, lineno): self.clear_break(filename, lineno) if (filename, lineno) in self.breakpoints: self.breakpoints.remove((filename, lineno)) print(f\\"Breakpoint cleared at {filename}, line {lineno}\\") def user_call(self, frame, arglist): print(f\\"Call to {frame.f_code.co_name} in {frame.f_code.co_filename} at line {frame.f_lineno}\\") def user_line(self, frame): print(f\\"Line {frame.f_lineno} in {frame.f_code.co_filename} at line {frame.f_lineno}\\") def user_return(self, frame, retval): print(f\\"Return from {frame.f_code.co_name} in {frame.f_code.co_filename} at line {frame.f_lineno}\\") print(f\\"Return value: {retval}\\") def user_exception(self, frame, exc_stuff): print(f\\"Exception in {frame.f_code.co_name} in {frame.f_code.co_filename} at line {frame.f_lineno}\\") print(f\\"Exception: {exc_stuff}\\") def run_code(self, code, globals=None, locals=None): self.run(code, globals, locals) def add(a, b): return a + b if __name__ == \\"__main__\\": debugger = CustomDebugger() debugger.set_breakpoint(__file__, 61) # Adjust line number based on your script result = debugger.runcall(add, 5, 10) print(f\\"Result: {result}\\")"},{"question":"# Question: Visualizing and Comparing Distributions with Seaborn Objective: Your task is to visualize different aspects of the \\"tips\\" dataset using seaborn, focusing on distributions and conditional distributions. The goal is to demonstrate your understanding of seaborn\'s capability in visualizing data distributions and the ability to customize these visualizations. Tasks: 1. **Plotting a Histogram**: - Load the `tips` dataset from seaborn. - Plot a histogram of the `total_bill` variable with 30 bins. - Save this plot as `total_bill_histogram.png`. 2. **Customized KDE Plot**: - Create a KDE plot of the `total_bill` variable. - Adjust the bandwidth to `0.5`. - Save this plot as `total_bill_kde.png`. 3. **Conditional Histogram with Normalized Counts**: - Plot a histogram of `total_bill`, conditioned on the `day` variable. - Display the counts as densities. - Save this plot as `total_bill_day_density.png`. 4. **Bivariate Distribution**: - Create a bivariate KDE plot with `total_bill` on the x-axis and `tip` on the y-axis. - Add a colorbar for interpreting the KDE plot. - Save this plot as `total_bill_tip_bivariate.png`. 5. **ECDF for Comparative Analysis**: - Plot ECDFs of `total_bill` for different `time` categories (Lunch and Dinner). - Save this plot as `total_bill_time_ecdf.png`. Notes: - Ensure each plot is properly labeled with titles, axis labels, and legends where applicable. - Use appropriate color palettes where you think it adds clarity to the visualization. - Your submission should include the code for generating each plot along with the saved plot images. Example Output: - `total_bill_histogram.png` - `total_bill_kde.png` - `total_bill_day_density.png` - `total_bill_tip_bivariate.png` - `total_bill_time_ecdf.png` Input and Output Format: - **Input**: The input dataset is to be loaded directly from seaborn using `sns.load_dataset(\\"tips\\")`. - **Output**: Five images saved in the current directory with the specified filenames. **Python Function Signature**: ```python def visualize_tips_data(): import seaborn as sns import matplotlib.pyplot as plt tips = sns.load_dataset(\\"tips\\") # 1. Histogram of `total_bill` sns.histplot(tips[\'total_bill\'], bins=30) plt.title(\'Histogram of Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Frequency\') plt.savefig(\'total_bill_histogram.png\') plt.clf() # 2. KDE plot of `total_bill` with adjusted bandwidth sns.kdeplot(tips[\'total_bill\'], bw_adjust=0.5) plt.title(\'KDE Plot of Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Density\') plt.savefig(\'total_bill_kde.png\') plt.clf() # 3. Normalized conditional histogram of `total_bill` by `day` sns.histplot(tips, x=\'total_bill\', hue=\'day\', stat=\'density\', common_norm=False) plt.title(\'Density Histogram of Total Bill by Day\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Density\') plt.legend(title=\'Day\') plt.savefig(\'total_bill_day_density.png\') plt.clf() # 4. Bivariate KDE plot `total_bill` vs `tip` with colorbar sns.kdeplot(tips[\'total_bill\'], tips[\'tip\'], fill=True, cbar=True) plt.title(\'Bivariate KDE of Total Bill and Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.savefig(\'total_bill_tip_bivariate.png\') plt.clf() # 5. ECDF of `total_bill` by `time` sns.ecdfplot(tips, x=\'total_bill\', hue=\'time\') plt.title(\'ECDF of Total Bill by Time\') plt.xlabel(\'Total Bill\') plt.legend(title=\'Time\') plt.savefig(\'total_bill_time_ecdf.png\') plt.clf() ``` **Deliverables**: Submit the Python script file containing the `visualize_tips_data` function and the generated plot images.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 1. Histogram of `total_bill` sns.histplot(tips[\'total_bill\'], bins=30) plt.title(\'Histogram of Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Frequency\') plt.savefig(\'total_bill_histogram.png\') plt.clf() # 2. KDE plot of `total_bill` with adjusted bandwidth sns.kdeplot(tips[\'total_bill\'], bw_adjust=0.5) plt.title(\'KDE Plot of Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Density\') plt.savefig(\'total_bill_kde.png\') plt.clf() # 3. Normalized conditional histogram of `total_bill` by `day` sns.histplot(tips, x=\'total_bill\', hue=\'day\', stat=\'density\', common_norm=False) plt.title(\'Density Histogram of Total Bill by Day\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Density\') plt.legend(title=\'Day\') plt.savefig(\'total_bill_day_density.png\') plt.clf() # 4. Bivariate KDE plot `total_bill` vs `tip` with colorbar sns.kdeplot(data=tips, x=\'total_bill\', y=\'tip\', fill=True, cbar=True) plt.title(\'Bivariate KDE of Total Bill and Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.savefig(\'total_bill_tip_bivariate.png\') plt.clf() # 5. ECDF of `total_bill` by `time` sns.ecdfplot(data=tips, x=\'total_bill\', hue=\'time\') plt.title(\'ECDF of Total Bill by Time\') plt.xlabel(\'Total Bill\') plt.legend(title=\'Time\') plt.savefig(\'total_bill_time_ecdf.png\') plt.clf()"},{"question":"Design a Python script that provides a custom interactive command-line interface with the following capabilities: 1. **History Management**: - Load history from a file named `.my_python_history` located in the user\'s home directory when the script starts. - Save the session\'s history to the same file upon script exit, ensuring that history from concurrent sessions is not lost. 2. **Auto-completion**: - Implement a custom completer function that auto-completes commands from a given list [`\'start\', \'stop\', \'pause\', \'exit\'`]. # Function Specification Your script should include: 1. `load_history(histfile: str) -> None`: - Loads history from the given `histfile`. 2. `save_history(histfile: str, prev_h_len: int) -> None`: - Appends the new history to the given `histfile` since the previous history length. 3. `custom_completer(text: str, state: int) -> Any`: - Provides the next possible completion from the command list based on the current input. # Input and Output - **Input**: No direct inputs. Instead, the script should handle command-line interactions. - **Output**: Standard command-line output reflecting command results and auto-completion. # Constraints - Ensure that the length of the history file does not exceed 1000 entries. - Handle exceptions where the history file might not exist. # Example ```python import atexit import os import readline COMMANDS = [\'start\', \'stop\', \'pause\', \'exit\'] def load_history(histfile): try: readline.read_history_file(histfile) except FileNotFoundError: open(histfile, \'wb\').close() def save_history(histfile, prev_h_len): new_h_len = readline.get_current_history_length() readline.set_history_length(1000) readline.append_history_file(new_h_len - prev_h_len, histfile) def custom_completer(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] return None def main(): histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".my_python_history\\") load_history(histfile) prev_h_len = readline.get_current_history_length() atexit.register(save_history, histfile, prev_h_len) readline.set_completer(custom_completer) readline.parse_and_bind(\\"tab: complete\\") while True: user_input = input(\\"> \\") if user_input in [\'exit\', \'quit\']: break print(f\\"Executing {user_input}...\\") if __name__ == \\"__main__\\": main() ``` The student should implement these functions and ensure that the program meets the specified requirements.","solution":"import atexit import os import readline COMMANDS = [\'start\', \'stop\', \'pause\', \'exit\'] def load_history(histfile): try: readline.read_history_file(histfile) except FileNotFoundError: open(histfile, \'wb\').close() def save_history(histfile, prev_h_len): new_h_len = readline.get_current_history_length() readline.set_history_length(1000) readline.append_history_file(new_h_len - prev_h_len, histfile) def custom_completer(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] return None def main(): histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".my_python_history\\") load_history(histfile) prev_h_len = readline.get_current_history_length() atexit.register(save_history, histfile, prev_h_len) readline.set_completer(custom_completer) readline.parse_and_bind(\\"tab: complete\\") while True: user_input = input(\\"> \\") if user_input in [\'exit\', \'quit\']: break print(f\\"Executing {user_input}...\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question: Implement a Web Scraping Tool Using `urllib.request` and `http.cookiejar`** # Background: In this task, you will create a web scraping tool that can fetch a webpage and print all the cookies associated with the page. # Requirements: 1. **Function Implementation**: You need to implement a function `fetch_webpage(url: str) -> None` that takes a URL as input and performs the following tasks: - Opens the URL using `urllib.request`. - Handles cookies using `http.cookiejar`. - Prints the content of the fetched webpage. - Prints all the cookies associated with the page. 2. **Expected Input/Output**: - Input: A single string representing a valid URL (e.g., `https://www.example.com`). - Output: The function should print the content of the webpage and cookies. 3. **Constraints**: - You must use `urllib.request` to open the URL. - You must handle cookies using `http.cookiejar`. 4. **Example**: ```python fetch_webpage(\\"https://www.example.com\\") ``` Output: ``` <html>...</html> <!--The content of the webpage--> Cookie: name1=value1 Cookie: name2=value2 ``` 5. **Performance**: - The function should handle the network request efficiently. - Proper error handling should be implemented to manage failed requests. # Implementation: ```python import urllib.request import http.cookiejar def fetch_webpage(url: str) -> None: # Create a CookieJar object to hold cookies cookie_jar = http.cookiejar.CookieJar() # Create an opener with a cookie processor opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) try: # Open the URL response = opener.open(url) # Read and print the content of the webpage content = response.read().decode(\'utf-8\') print(content) # Print all cookies for cookie in cookie_jar: print(f\\"Cookie: {cookie.name}={cookie.value}\\") except Exception as e: print(f\\"An error occurred: {e}\\") ```","solution":"import urllib.request import http.cookiejar def fetch_webpage(url: str) -> None: # Create a CookieJar object to hold cookies cookie_jar = http.cookiejar.CookieJar() # Create an opener with a cookie processor opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) try: # Open the URL response = opener.open(url) # Read and print the content of the webpage content = response.read().decode(\'utf-8\') print(content) # Print all cookies for cookie in cookie_jar: print(f\\"Cookie: {cookie.name}={cookie.value}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Objective: Design a custom Python object that mimics a list but includes additional metadata to track the number of times elements are added and removed. Your implementation should handle typical list operations (`append`, `remove`, `__getitem__`, `__setitem__`, etc.) and provide methods to retrieve the metadata. # Requirements: 1. Implement a class `CustomList` that supports the following operations: - `append(element)`: Add an element to the end of the list. - `remove(element)`: Remove the first occurrence of the element from the list. - `__getitem__(index)`: Retrieve the element at the specified index. - `__setitem__(index, value)`: Set the element at the specified index to a new value. 2. Include additional methods in the class: - `get_append_count() -> int`: Returns the number of times `append` was called. - `get_remove_count() -> int`: Returns the number of times `remove` was called. 3. Ensure efficient memory usage and proper handling of garbage collection. # Input and Output: - **Input**: - Series of operations performed on an instance of `CustomList`. - Example operations (pseudocode): ``` cl = CustomList() cl.append(1) cl.append(2) cl.remove(1) cl[0] = 3 ``` - **Output**: - Results of operations where applicable. - Execution of metadata methods to verify internal statistics. - Example output (pseudocode): ``` cl.get_append_count() -> 2 cl.get_remove_count() -> 1 cl[0] -> 3 ``` # Constraints: - The `CustomList` should be implemented using Python\'s low-level API (as much as possible), managing memory and attribute access in a custom manner. - Ensure that the implementation adheres to standard Python behavior for list-like objects. # Performance: - Ensure that list operations and metadata retrievals are performant and handle typical use cases efficiently. # Example: ```python class CustomList: def __init__(self): # Initialize the custom list and metadata pass def append(self, element): # Append element to the custom list pass def remove(self, element): # Remove element from the custom list pass def __getitem__(self, index): # Retrieve element at index pass def __setitem__(self, index, value): # Set element at index to value pass def get_append_count(self): # Return the number of times append was called pass def get_remove_count(self): # Return the number of times remove was called pass # Test the CustomList cl = CustomList() cl.append(1) cl.append(2) cl.remove(1) cl[0] = 3 print(cl.get_append_count()) # Output: 2 print(cl.get_remove_count()) # Output: 1 print(cl[0]) # Output: 3 ```","solution":"class CustomList: def __init__(self): self._list = [] self._append_count = 0 self._remove_count = 0 def append(self, element): self._list.append(element) self._append_count += 1 def remove(self, element): self._list.remove(element) self._remove_count += 1 def __getitem__(self, index): return self._list[index] def __setitem__(self, index, value): self._list[index] = value def get_append_count(self): return self._append_count def get_remove_count(self): return self._remove_count"},{"question":"Background You are a data scientist working with large text files. To save disk space and transfer files more efficiently, you need to compress these files using the `bz2` library provided in Python. You also need to ensure that the compressed files can be decompressed back to their original state without losing any data. Task Implement a Python function that performs the following tasks: 1. Reads a list of filenames (text files) from a specified directory. 2. Compresses each file incrementally and writes the compressed content to new files with the same base name but with a `.bz2` extension. 3. Reads back the compressed files, decompresses them incrementally, and ensures that the decompressed content matches the original content exactly. 4. Returns a dictionary where the keys are original filenames and the values are `True` if the decompressed content matches the original content, otherwise `False`. Function Signature ```python import bz2 from typing import List, Dict def verify_and_compress_files(directory: str) -> Dict[str, bool]: pass ``` Input - `directory` (str): The path to the directory containing the text files to process. Output - A `dict` where: - The keys are the original filenames (str). - The values are booleans (`True` if the decompressed content matches the original content, otherwise `False`). Constraints - Assume the directory contains only text files. - Each file is expected to be read and processed incrementally due to its potential large size. - The compression level can be set to `9` for maximum compression. - Methods from the `bz2` module should be used for compression and decompression. Example Usage ```python result = verify_and_compress_files(\'/path/to/directory\') print(result) # Expected Output (example): {\'file1.txt\': True, \'file2.txt\': True, \'file3.txt\': False} ``` Notes - Make sure to handle errors gracefully and assume that all required modules are already imported. - Consider separating the compression and decompression logic into helper functions for better clarity and modularity.","solution":"import bz2 import os from typing import List, Dict def compress_file(input_file: str, output_file: str) -> None: Compress a text file using bz2. with open(input_file, \'rb\') as f_in, bz2.open(output_file, \'wb\', compresslevel=9) as f_out: for line in f_in: f_out.write(line) def decompress_file(input_file: str, output_file: str) -> None: Decompress a bz2 file to a text file. with bz2.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: for line in f_in: f_out.write(line) def verify_and_compress_files(directory: str) -> Dict[str, bool]: Compresses text files in the given directory and verifies the decompressed content matches the original. results = {} for filename in os.listdir(directory): original_file_path = os.path.join(directory, filename) compressed_file_path = original_file_path + \'.bz2\' decompressed_file_path = original_file_path + \'.decompressed\' compress_file(original_file_path, compressed_file_path) decompress_file(compressed_file_path, decompressed_file_path) with open(original_file_path, \'rb\') as original_file, open(decompressed_file_path, \'rb\') as decompressed_file: original_content = original_file.read() decompressed_content = decompressed_file.read() results[filename] = (original_content == decompressed_content) # Cleanup decompressed files after comparison os.remove(decompressed_file_path) return results"},{"question":"# Concurrent Execution in Python Using `concurrent.futures` Objective: To assess your understanding and ability to implement concurrent task execution in Python using the `concurrent.futures` module. Problem Statement: You are required to write a Python function to simulate a computational task that calculates the nth Fibonacci number. Given a list of integers, your function should compute the Fibonacci number for each integer in the list concurrently, utilizing the `concurrent.futures` module. You need to ensure that the implementation is efficient and leverages either `ThreadPoolExecutor` or `ProcessPoolExecutor`. Requirements: 1. Define a function `compute_fibonacci_concurrently(numbers: List[int], method: str) -> List[int]`. 2. The function should take two parameters: - `numbers`: A list of integers for which the Fibonacci numbers need to be computed. - `method`: A string that can be either `\'thread\'` or `\'process\'` indicating whether to use `ThreadPoolExecutor` or `ProcessPoolExecutor`. 3. The function should return a list of Fibonacci numbers corresponding to the input list of integers. The function should: - Use `concurrent.futures` to create and manage the executor. - Implement the Fibonacci number computation in a separate helper function. - Correctly handle the selection of executor based on the `method` parameter. - Make sure all tasks are submitted and results are collected concurrently. Fibonacci Computation: To compute the nth Fibonacci number efficiently, use an iterative approach rather than a recursive one to avoid excessive function call overheads. Input: - A list of integers, `numbers`, where each integer `n` (0 â‰¤ n â‰¤ 30) is an input for the Fibonacci computation. - A string `method`, which can either be `\'thread\'` or `\'process\'`. Output: - A list of integers, where each integer is the Fibonacci number corresponding to the respective input in the `numbers` list. Example: ```python from typing import List def fibonacci(n: int) -> int: # Your efficient Fibonacci computation here def compute_fibonacci_concurrently(numbers: List[int], method: str) -> List[int]: # Implementing concurrent execution based on the method # Example usage: numbers = [10, 20, 30] method = \'thread\' print(compute_fibonacci_concurrently(numbers, method)) # Output example: [55, 6765, 832040] ``` Constraints: - Optimize the Fibonacci computation to be efficient with an iterative approach. - Proper error handling for invalid `method` values. - You should use the appropriate executor based on the `method` parameter. Note: - When to use `ThreadPoolExecutor`: Use this for I/O bound tasks. - When to use `ProcessPoolExecutor`: Use this for CPU bound tasks. - For simplicity, assume that the list will contain non-negative integers within the given range. # Good Luck!","solution":"from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor from typing import List, Union def fibonacci(n: int) -> int: if n <= 1: return n a, b = 0, 1 for _ in range(2, n+1): a, b = b, a + b return b def compute_fibonacci_concurrently(numbers: List[int], method: str) -> List[int]: if method not in [\'thread\', \'process\']: raise ValueError(\\"method must be either \'thread\' or \'process\'\\") executor_class = ThreadPoolExecutor if method == \'thread\' else ProcessPoolExecutor with executor_class() as executor: results = list(executor.map(fibonacci, numbers)) return results"},{"question":"Objective You are required to write a Python function using the seaborn library to visualize a dataset with different plot styles and customizations. This exercise will demonstrate your understanding of seaborn\'s style setting and plotting functions. Problem Description Implement a function `visualize_data_with_styles(data)`, which accepts a dictionary `data` that contains the following keys: - `x_values`: list of x-axis values. - `y_values`: list of y-axis values corresponding to each x-axis value. - `style`: a string specifying the seaborn style, which can be one of the following: [\\"whitegrid\\", \\"darkgrid\\", \\"white\\", \\"dark\\", \\"ticks\\"]. - `custom_params` (optional): a dictionary containing custom parameters to override seaborn\'s default style settings. If not provided, no custom parameters should be applied. The function should: 1. Set the seaborn style using the provided `style` parameter. 2. If `custom_params` is provided, override the seaborn default parameters with these. 3. Create two plots using the provided `x_values` and `y_values`: - A bar plot. - A line plot. 4. Display both plots in a single figure, one next to the other. Input The function `visualize_data_with_styles(data)` takes a dictionary `data`: ```python data = { \\"x_values\\": [\\"A\\", \\"B\\", \\"C\\"], \\"y_values\\": [1, 3, 2], \\"style\\": \\"whitegrid\\", \\"custom_params\\": {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"} # Optional } ``` Output The function does not need to return anything. It should directly display the plots. Constraints - The `x_values` and `y_values` lists will be non-empty and of the same length. - Valid values for `style` are: [\\"whitegrid\\", \\"darkgrid\\", \\"white\\", \\"dark\\", \\"ticks\\"]. - If `custom_params` is provided, it is a dictionary with valid seaborn style parameters. Implementation Notes - Use `sns.barplot()` for the bar plot. - Use `sns.lineplot()` for the line plot. - Use `plt.subplot()` to create a figure with two plots. - Remember to import necessary libraries such as `seaborn` and `matplotlib.pyplot`. Example: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_data_with_styles(data): sns.set(style=data[\'style\'], rc=data.get(\'custom_params\', {})) fig, axs = plt.subplots(1, 2, figsize=(10, 5)) sns.barplot(ax=axs[0], x=data[\'x_values\'], y=data[\'y_values\']) axs[0].set_title(\'Bar Plot\') sns.lineplot(ax=axs[1], x=data[\'x_values\'], y=data[\'y_values\']) axs[1].set_title(\'Line Plot\') plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_data_with_styles(data): Visualizes data with specified seaborn styles and custom parameters. Parameters: data (dict): Dictionary containing x_values, y_values, style, and optional custom_params. sns.set(style=data[\'style\'], rc=data.get(\'custom_params\', {})) fig, axs = plt.subplots(1, 2, figsize=(12, 5)) sns.barplot(ax=axs[0], x=data[\'x_values\'], y=data[\'y_values\']) axs[0].set_title(\'Bar Plot\') sns.lineplot(ax=axs[1], x=data[\'x_values\'], y=data[\'y_values\']) axs[1].set_title(\'Line Plot\') plt.tight_layout() plt.show()"},{"question":"# Complex Number Operations and Statistics You are required to implement a function that processes a list of complex numbers. The function should perform the following tasks: 1. **Separate the list** into real and imaginary components. 2. **Calculate basic statistics** (mean and standard deviation) for both the real and imaginary parts separately. 3. **Generate a new set of complex numbers** where each complex number\'s: - real part is normally distributed with the previously calculated mean and standard deviation of the real parts. - imaginary part is normally distributed with the previously calculated mean and standard deviation of the imaginary parts. 4. **Sort** the new set of complex numbers by their magnitudes in descending order. Function Signature ```python def process_complex_numbers(complex_list: List[complex], n: int) -> List[complex]: pass ``` Input - `complex_list` (List[complex]): A list of complex numbers. - `n` (int): The number of new complex numbers to generate. Output - List[complex]: A list of `n` new complex numbers sorted by their magnitudes in descending order. Constraints - `complex_list` will have at least 2 elements. - Standard deviation will not be zero. Example ```python from typing import List from statistics import mean, stdev from random import gauss def process_complex_numbers(complex_list: List[complex], n: int) -> List[complex]: # Separate real and imaginary parts real_parts = [c.real for c in complex_list] imaginary_parts = [c.imag for c in complex_list] # Calculate mean and standard deviation mean_real = mean(real_parts) stdev_real = stdev(real_parts) mean_imaginary = mean(imaginary_parts) stdev_imaginary = stdev(imaginary_parts) # Generate new complex numbers new_complex_numbers = [] for _ in range(n): real_part = gauss(mean_real, stdev_real) imaginary_part = gauss(mean_imaginary, stdev_imaginary) new_complex_numbers.append(complex(real_part, imaginary_part)) # Sort by magnitude in descending order sorted_complex_numbers = sorted(new_complex_numbers, key=abs, reverse=True) return sorted_complex_numbers # Example Usage: complex_list = [complex(2, 3), complex(1, -1), complex(4, 5), complex(-3, -4)] n = 5 print(process_complex_numbers(complex_list, n)) # Output: A list of 5 complex numbers generated from the means and standard deviations of the input list, sorted by magnitude ``` *Note: The output of the function will vary due to the random generation of complex numbers.*","solution":"from typing import List from statistics import mean, stdev from random import gauss def process_complex_numbers(complex_list: List[complex], n: int) -> List[complex]: Processes a list of complex numbers to generate \'n\' new complex numbers based on the statistics of the given list. The new numbers are sorted by magnitude in descending order. Parameters: complex_list (List[complex]): A list of complex numbers. n (int): The number of new complex numbers to generate. Returns: List[complex]: A list of `n` new complex numbers sorted by their magnitudes in descending order. # Separate real and imaginary parts real_parts = [c.real for c in complex_list] imaginary_parts = [c.imag for c in complex_list] # Calculate mean and standard deviation mean_real = mean(real_parts) stdev_real = stdev(real_parts) mean_imaginary = mean(imaginary_parts) stdev_imaginary = stdev(imaginary_parts) # Generate new complex numbers new_complex_numbers = [] for _ in range(n): real_part = gauss(mean_real, stdev_real) imaginary_part = gauss(mean_imaginary, stdev_imaginary) new_complex_numbers.append(complex(real_part, imaginary_part)) # Sort by magnitude in descending order sorted_complex_numbers = sorted(new_complex_numbers, key=abs, reverse=True) return sorted_complex_numbers"},{"question":"<|Analysis Begin|> The provided documentation gives a detailed explanation of the `selectors` module in Python, which is designed for high-level and efficient I/O multiplexing. This module builds upon the `select` module primitives and provides a higher-level interface for monitoring multiple file objects to see if I/O operations are ready for reading or writing. The document outlines the following: 1. **Introduction and Purpose**: Explains the use-case and high-level purpose of the `selectors` module. 2. **Class Hierarchy and Constants**: Lists the hierarchy of classes in the module and the constants used to represent the event masks. 3. **Detailed Class Descriptions**: - `SelectorKey`: A named tuple for associating file objects with their descriptors, events, and optional data. - `BaseSelector`: An abstract base class for selectors. - `DefaultSelector`, `SelectSelector`, `PollSelector`, `EpollSelector`, `DevpollSelector`, `KqueueSelector`: Concrete implementations of the `BaseSelector` class, each wrapping different OS-level I/O multiplexing mechanisms. 4. **Methods**: Detailed descriptions of methods like `register`, `unregister`, `modify`, `select`, `close`, `get_key`, and `get_map`. 5. **Example**: A sample echo server implementation showcasing how to use the `DefaultSelector` to accept connections and read/write data non-blocking. The analysis of this document reveals that a challenging and comprehensive coding question can be created, focusing on implementing a function that utilizes the `selectors` module to manage multiple I/O streams efficiently. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are tasked with implementing a simple chat server using Python\'s `selectors` module. The server should allow multiple clients to connect and send messages to each other. When a client sends a message, the server should broadcast the message to all connected clients except the sender. **Requirements**: - The server should be able to handle multiple client connections. - When a client sends a message, it should be broadcasted to all other clients. - Clients should be able to connect and disconnect gracefully. **Specifications**: 1. Implement the `ChatServer` class with the following methods: - `__init__(self, host: str, port: int)`: Initializes the server with the given host and port, and sets up the selector. - `start(self)`: Starts the server and begins listening for connections. - `accept(self, sock, mask)`: Accepts new client connections and registers them with the selector. - `read(self, conn, mask)`: Reads data from a client connection and broadcasts it to all other clients. If the client disconnects, it should unregister the client and close the connection. 2. Make use of `selectors.DefaultSelector()` to handle the multiplexing. 3. Ensure the server runs indefinitely until manually interrupted. **Input**: - Host and port number to bind the server. **Output**: - The server should print connection/disconnection messages and broadcast messages received from clients. **Constraints**: - Use non-blocking sockets. - Properly close connections and clean up resources. # Example Usage ```python if __name__ == \\"__main__\\": host = \'localhost\' port = 12345 server = ChatServer(host, port) server.start() ``` Your `ChatServer` class should be in such a way that running the above code starts a chat server that listens for connections on the specified host and port, accepts connections, reads messages from clients, and broadcasts messages to all clients.","solution":"import selectors import socket class ChatServer: def __init__(self, host: str, port: int): self.host = host self.port = port self.selector = selectors.DefaultSelector() def start(self): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((self.host, self.port)) server_socket.listen() server_socket.setblocking(False) self.selector.register(server_socket, selectors.EVENT_READ, self.accept) print(f\\"Chat server started on {self.host}:{self.port}\\") try: while True: events = self.selector.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Server is shutting down...\\") finally: self.selector.close() def accept(self, sock, mask): conn, addr = sock.accept() # Should be ready to read print(f\\"Accepted connection from {addr}\\") conn.setblocking(False) self.selector.register(conn, selectors.EVENT_READ, self.read) def read(self, conn, mask): data = conn.recv(1024) # Should be ready to read if data: print(f\\"Received message: {data.decode()} from {conn}\\") self.broadcast(conn, data) else: print(f\\"Closing connection to {conn}\\") self.selector.unregister(conn) conn.close() def broadcast(self, sender_conn, message): for key in self.selector.get_map().values(): if key.fileobj is not sender_conn and key.data == self.read: try: key.fileobj.send(message) except Exception as e: print(f\\"Error sending message to {key.fileobj}: {e}\\")"},{"question":"# Problem: Multi-Format File Archiver You are asked to implement a multi-format file archiver that can compress and decompress files using various compression algorithms mentioned in Python\'s standard library. Objective: Write a Python class `MultiFormatFileArchiver` that provides methods to compress and decompress files using `gzip`, `bz2`, and `lzma` formats. Class Methods: 1. `compress_file(input_file: str, output_file: str, format: str) -> None`: - Compresses the file specified by `input_file` and saves the result to `output_file` using the specified compression `format`. - Supported formats are `\'gzip\'`, `\'bz2\'`, and `\'lzma\'`. - Raise `ValueError` if the specified format is not supported. 2. `decompress_file(input_file: str, output_file: str, format: str) -> None`: - Decompresses the file specified by `input_file` and saves the result to `output_file` using the specified compression `format`. - Supported formats are `\'gzip\'`, `\'bz2\'`, and `\'lzma\'`. - Raise `ValueError` if the specified format is not supported. Constraints: - Assume all input files are text files. - If the format specified is not among the supported ones, your methods should raise a `ValueError` with the message: \\"Unsupported format: <format>\\". Example Usage: ```python archiver = MultiFormatFileArchiver() # Compress a file using gzip archiver.compress_file(\'example.txt\', \'example.txt.gz\', \'gzip\') # Decompress the gzip file archiver.decompress_file(\'example.txt.gz\', \'example_uncompressed.txt\', \'gzip\') # Compress a file using bzip2 archiver.compress_file(\'example.txt\', \'example.txt.bz2\', \'bz2\') # Decompress the bzip2 file archiver.decompress_file(\'example.txt.bz2\', \'example_uncompressed.txt\', \'bz2\') # Compress a file using lzma archiver.compress_file(\'example.txt\', \'example.txt.xz\', \'lzma\') # Decompress the lzma file archiver.decompress_file(\'example.txt.xz\', \'example_uncompressed.txt\', \'lzma\') ``` # Implementation: ```python import gzip import bz2 import lzma class MultiFormatFileArchiver: def compress_file(self, input_file: str, output_file: str, format: str) -> None: if format == \'gzip\': with open(input_file, \'rb\') as f_in, gzip.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif format == \'bz2\': with open(input_file, \'rb\') as f_in, bz2.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif format == \'lzma\': with open(input_file, \'rb\') as f_in, lzma.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) else: raise ValueError(f\\"Unsupported format: {format}\\") def decompress_file(self, input_file: str, output_file: str, format: str) -> None: if format == \'gzip\': with gzip.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif format == \'bz2\': with bz2.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif format == \'lzma\': with lzma.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.writelines(f_in) else: raise ValueError(f\\"Unsupported format: {format}\\") # You can add more test cases to validate your solution ```","solution":"import gzip import bz2 import lzma class MultiFormatFileArchiver: def compress_file(self, input_file: str, output_file: str, format: str) -> None: if format == \'gzip\': with open(input_file, \'rb\') as f_in, gzip.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif format == \'bz2\': with open(input_file, \'rb\') as f_in, bz2.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif format == \'lzma\': with open(input_file, \'rb\') as f_in, lzma.open(output_file, \'wb\') as f_out: f_out.writelines(f_in) else: raise ValueError(f\\"Unsupported format: {format}\\") def decompress_file(self, input_file: str, output_file: str, format: str) -> None: if format == \'gzip\': with gzip.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif format == \'bz2\': with bz2.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.writelines(f_in) elif format == \'lzma\': with lzma.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.writelines(f_in) else: raise ValueError(f\\"Unsupported format: {format}\\")"},{"question":"# Python Coding Assessment Task Background You are given the task of processing a text file through a series of shell commands using the `pipes` module in Python. This module provides an abstraction layer for shell pipelines, enabling you to construct sequences of file converters. Although the `pipes` module is deprecated and should be replaced by the `subprocess` module in newer Python versions, you will use `pipes` for this exercise to demonstrate your understanding of its concepts. Task Write a Python function `process_file(input_file: str, output_file: str) -> None` that takes an input text file, processes it through a series of transformations using shell commands, and writes the transformed output to another file. The transformations to be applied (in order) are: 1. Convert all lowercase letters to uppercase. 2. Sort the lines alphabetically. 3. Remove duplicate lines. Requirements 1. **Input:** - `input_file`: The path to the input text file (string). - `output_file`: The path to the output text file (string). 2. **Output:** - The function should not return any output. The result should be written directly to the `output_file`. 3. **Shell Commands:** - Convert to uppercase: `tr a-z A-Z` - Sort lines: `sort` - Remove duplicates: `uniq` 4. **Constraints:** - Use the `pipes` module to create and manage the pipeline. - Ensure that the pipeline handles the transformations in the specified order. Example If the content of `input.txt` is as follows: ```text banana Apple banana Cherry apple ``` After processing, the content of `output.txt` should be: ```text APPLE BANANA CHERRY ``` Implementation Notes - You should use the `pipes.Template` class to create the pipeline. - Use the `append` method of `pipes.Template` to add commands to the pipeline. - Use the `open` method of `pipes.Template` to write to the pipeline. Function Signature ```python def process_file(input_file: str, output_file: str) -> None: pass ``` Ensure your solution is efficient and follows the guidelines provided. Include comments where necessary to explain your logic.","solution":"import pipes def process_file(input_file: str, output_file: str) -> None: Process the input file by applying a series of transformations: 1. Convert all lowercase letters to uppercase. 2. Sort the lines alphabetically. 3. Remove duplicate lines. The transformed output is written to the output file. :param input_file: Path to the input text file :param output_file: Path to the output text file # Create the pipeline template template = pipes.Template() # Add the series of transformations template.append(\'tr a-z A-Z\', \'--\') # Convert lowercase to uppercase template.append(\'sort\', \'--\') # Sort lines alphabetically template.append(\'uniq\', \'--\') # Remove duplicate lines # Open the input file for reading and the output file for writing with template.open(input_file, \'r\') as in_file: with open(output_file, \'w\') as out_file: for line in in_file: out_file.write(line)"},{"question":"# Question: Assume you are implementing a command-line tool for managing a to-do list. You need to use the `optparse` library to handle various command-line options. The tool should support the following functionalities: 1. **Adding a task**: Each task has a title and an optional description. 2. **Listing all tasks**: Display all tasks with their titles and descriptions. 3. **Completing a task**: Mark a specified task (by its index in the list) as completed. 4. **Deleting a task**: Remove a specified task (by its index) from the list. 5. **Filtering tasks**: Filter tasks that contain a specific keyword in their title or description. Implement a script `todo.py` that achieves the following: 1. Define the appropriate command-line options for each functionality. 2. Use the correct option attributes to handle the required arguments for each option. 3. Ensure the tool works correctly by parsing the command-line arguments and executing the corresponding functionality. # Requirements: - Implement the script using the `optparse` library. - Use appropriate `action` types for options. - Handle errors gracefully, providing meaningful messages. - Generate a user-friendly help message. # Expected Inputs and Outputs: - Adding a task: - Command: `python todo.py --add \\"Buy milk\\" --desc \\"Buy 2 liters of milk\\"` - Output: Task added successfully. - Listing all tasks: - Command: `python todo.py --list` - Output: ``` 1. Buy milk - Buy 2 liters of milk ``` - Completing a task: - Command: `python todo.py --complete 1` - Output: Task 1 completed. - Deleting a task: - Command: `python todo.py --delete 1` - Output: Task 1 deleted. - Filtering tasks: - Command: `python todo.py --filter \\"milk\\"` - Output: ``` 1. Buy milk - Buy 2 liters of milk ``` # Constraints: - The task list should be stored in-memory (i.e., it does not need to persist across program executions). - The script should handle invalid inputs gracefully and provide appropriate error messages. **Note**: Although `optparse` is deprecated, please use it for this exercise. # Example: ```python # todo.py from optparse import OptionParser, OptionGroup def main(): parser = OptionParser(usage=\\"usage: %prog [options]\\", version=\\"%prog 1.0\\") group = OptionGroup(parser, \\"Task Management\\", \\"Options for managing tasks\\") group.add_option(\\"--add\\", dest=\\"add\\", metavar=\\"TITLE\\", help=\\"Add a new task with the given TITLE\\") group.add_option(\\"--desc\\", dest=\\"description\\", metavar=\\"DESCRIPTION\\", help=\\"Description for the new task (requires --add)\\") group.add_option(\\"--list\\", action=\\"store_true\\", dest=\\"list\\", help=\\"List all tasks\\") group.add_option(\\"--complete\\", dest=\\"complete\\", type=\\"int\\", metavar=\\"INDEX\\", help=\\"Mark the task at INDEX as completed\\") group.add_option(\\"--delete\\", dest=\\"delete\\", type=\\"int\\", metavar=\\"INDEX\\", help=\\"Delete the task at INDEX\\") group.add_option(\\"--filter\\", dest=\\"filter\\", metavar=\\"KEYWORD\\", help=\\"Filter tasks by a keyword in their title or description\\") parser.add_option_group(group) (options, args) = parser.parse_args() tasks = [] # Functionality to add, list, complete, delete, and filter tasks based on parsed arguments if __name__ == \\"__main__\\": main() ``` # Your task: Complete the `main()` function to fully implement the described functionality.","solution":"from optparse import OptionParser, OptionGroup tasks = [] def add_task(title, description): tasks.append({\\"title\\": title, \\"description\\": description, \\"completed\\": False}) print(\\"Task added successfully.\\") def list_tasks(): if not tasks: print(\\"No tasks.\\") for i, task in enumerate(tasks): status = \\"[Completed]\\" if task[\\"completed\\"] else \\"\\" print(f\\"{i + 1}. {task[\'title\']} - {task[\'description\']} {status}\\") def complete_task(index): try: tasks[index][\\"completed\\"] = True print(f\\"Task {index + 1} completed.\\") except IndexError: print(\\"Invalid task index.\\") def delete_task(index): try: tasks.pop(index) print(f\\"Task {index + 1} deleted.\\") except IndexError: print(\\"Invalid task index.\\") def filter_tasks(keyword): filtered_tasks = [task for task in tasks if keyword in task[\'title\'] or keyword in task[\'description\']] if not filtered_tasks: print(f\\"No tasks containing \'{keyword}\'.\\") for i, task in enumerate(filtered_tasks): status = \\"[Completed]\\" if task[\\"completed\\"] else \\"\\" print(f\\"{i + 1}. {task[\'title\']} - {task[\'description\']} {status}\\") def main(): parser = OptionParser(usage=\\"usage: %prog [options]\\", version=\\"%prog 1.0\\") group = OptionGroup(parser, \\"Task Management\\", \\"Options for managing tasks.\\") group.add_option(\\"--add\\", dest=\\"add\\", metavar=\\"TITLE\\", help=\\"Add a new task with the given TITLE.\\") group.add_option(\\"--desc\\", dest=\\"description\\", metavar=\\"DESCRIPTION\\", help=\\"Description for the new task (requires --add).\\") group.add_option(\\"--list\\", action=\\"store_true\\", dest=\\"list\\", help=\\"List all tasks.\\") group.add_option(\\"--complete\\", dest=\\"complete\\", type=\\"int\\", metavar=\\"INDEX\\", help=\\"Mark the task at INDEX as completed.\\") group.add_option(\\"--delete\\", dest=\\"delete\\", type=\\"int\\", metavar=\\"INDEX\\", help=\\"Delete the task at INDEX.\\") group.add_option(\\"--filter\\", dest=\\"filter\\", metavar=\\"KEYWORD\\", help=\\"Filter tasks by a keyword in their title or description.\\") parser.add_option_group(group) (options, args) = parser.parse_args() if options.add: add_task(options.add, options.description) elif options.list: list_tasks() elif options.complete is not None: complete_task(options.complete - 1) elif options.delete is not None: delete_task(options.delete - 1) elif options.filter: filter_tasks(options.filter) else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"# Problem: Efficient Data Processing with Caching and Function Dispatch Background In this task, you are required to implement a function that processes data efficiently using caching for repetitive computations and single-dispatch for handling different types of data structures. You will use functionalities from the `functools` module in Python. Objectives 1. **Implement Caching for Repetitive Computations**: Utilize `functools.lru_cache` to cache the results of a computationally expensive function. 2. **Use Single-Dispatch for Handling Different Data Structures**: Implement a function using `functools.singledispatch` that processes different types of data structures in a specific manner. Instructions 1. **Step 1 - Define the Cached Function**: Implement a function `compute_factorial` that computes the factorial of a number. Use `functools.lru_cache` to cache the results to avoid redundant computations. ```python import functools @functools.lru_cache(maxsize=128) def compute_factorial(n: int) -> int: Compute the factorial of a number using recursion. Cache the results for efficiency. :param n: Integer whose factorial is to be computed. :return: Factorial of the input number. if n < 2: return 1 return n * compute_factorial(n - 1) ``` 2. **Step 2 - Define the Single-Dispatch Function**: Implement a function `process_data` decorated with `functools.singledispatch` that handles different types of data structures: - **For integers**: Compute the factorial using the cached function `compute_factorial`. - **For lists**: Return a list of factorials for each integer in the input list. - **For dictionaries**: Return a dictionary where each key-value pair is transformed such that the value is the factorial of the original value. ```python import functools @functools.singledispatch def process_data(data): raise NotImplementedError(\\"Unsupported type\\") @process_data.register def _(data: int) -> int: Handle integer input by computing the factorial. return compute_factorial(data) @process_data.register def _(data: list) -> list: Handle list input by computing the factorial for each element. return [compute_factorial(elem) for elem in data] @process_data.register def _(data: dict) -> dict: Handle dictionary input by computing the factorial for each value. return {key: compute_factorial(value) for key, value in data.items()} ``` 3. **Step 3 - Test Your Functions**: Create a main block to test your functions with various inputs and types: ```python if __name__ == \\"__main__\\": print(process_data(5)) # Should print factorial of 5 print(process_data([1, 2, 3, 4, 5])) # Should print list of factorials print(process_data({\'a\': 3, \'b\': 4}))# Should print dictionary with factorials as values ``` Constraints - The factorial function should handle inputs up to 128 without significant performance degradation. - The `process_data` function should support integers, lists, and dictionaries only. If an unsupported type is provided, it should raise a `NotImplementedError`. Evaluation Your solution will be evaluated based on: - Correctness and efficiency of the caching mechanism. - Proper use of single-dispatch to handle different data types. - Code readability and adherence to Python best practices.","solution":"import functools @functools.lru_cache(maxsize=128) def compute_factorial(n: int) -> int: Compute the factorial of a number using recursion. Cache the results for efficiency. :param n: Integer whose factorial is to be computed. :return: Factorial of the input number. if n < 2: return 1 return n * compute_factorial(n - 1) @functools.singledispatch def process_data(data): raise NotImplementedError(\\"Unsupported type\\") @process_data.register def _(data: int) -> int: Handle integer input by computing the factorial. return compute_factorial(data) @process_data.register def _(data: list) -> list: Handle list input by computing the factorial for each element. return [compute_factorial(elem) for elem in data] @process_data.register def _(data: dict) -> dict: Handle dictionary input by computing the factorial for each value. return {key: compute_factorial(value) for key, value in data.items()}"},{"question":"Objective To test your understanding of Python\'s `typing` module, create a type-safe generic cache system that allows storing and retrieving values of various types. Implement the following classes with type hints and annotations: 1. **Cache**: A generic class that can store key-value pairs of specified types. 2. **TypeCheckedCache**: A subclass of Cache that uses type guards to ensure stored types match the expected types. 3. **TypeCheckProtocol**: A protocol for type-checking which `TypeCheckedCache` must implement. Class Definitions ```python from typing import TypeVar, Generic, Dict, Optional, Protocol, TypeGuard, Any # Declare TypeVars for key and value K = TypeVar(\'K\') V = TypeVar(\'V\') class Cache(Generic[K, V]): def __init__(self) -> None: self._storage: Dict[K, V] = {} def get(self, key: K) -> Optional[V]: return self._storage.get(key) def set(self, key: K, value: V) -> None: self._storage[key] = value class TypeCheckProtocol(Protocol[K, V]): def is_valid_key(self, item: Any) -> TypeGuard[K]: ... def is_valid_value(self, item: Any) -> TypeGuard[V]: ... class TypeCheckedCache(Cache[K, V], TypeCheckProtocol[K, V]): def set(self, key: K, value: V) -> None: if self.is_valid_key(key) and self.is_valid_value(value): super().set(key, value) else: raise TypeError(\\"Invalid key or value type\\") def is_valid_key(self, item: Any) -> TypeGuard[K]: # Implement type checking for key ... def is_valid_value(self, item: Any) -> TypeGuard[V]: # Implement type checking for value ... # Example usage: # Define a concrete class that checks for specific types class IntStrCache(TypeCheckedCache[int, str]): def is_valid_key(self, item: Any) -> TypeGuard[int]: return isinstance(item, int) def is_valid_value(self, item: Any) -> TypeGuard[str]: return isinstance(item, str) cache = IntStrCache() cache.set(1, \\"one\\") # Correct types print(cache.get(1)) # Output: \\"one\\" cache.set(\\"two\\", 2) # Should raise TypeError ``` Constraints and Requirements: - You must use proper type hints and annotations for all classes and methods. - You should implement type guards in `TypeCheckedCache` to ensure type safety. - If the types do not match, raise a `TypeError`. - Your type-annotations should be compatible with Python version 3.10 or later. Test Cases 1. Store and retrieve values of correct and incorrect types. 2. Ensure type-checking raises errors as expected. 3. Verify that type hints and annotations align with the `typing` module specifications. Input and Output Format - **Input**: Creation of cache instances, calls to `set` and `get` methods with various types. - **Output**: Retrieved values or raised exceptions for type mismatches. Implement the classes and methods described above and validate them with appropriate test cases demonstrating their correctness and type-safety.","solution":"from typing import TypeVar, Generic, Dict, Optional, Protocol, TypeGuard, Any # Declare TypeVars for key and value K = TypeVar(\'K\') V = TypeVar(\'V\') class Cache(Generic[K, V]): def __init__(self) -> None: self._storage: Dict[K, V] = {} def get(self, key: K) -> Optional[V]: return self._storage.get(key) def set(self, key: K, value: V) -> None: self._storage[key] = value class TypeCheckProtocol(Protocol[K, V]): def is_valid_key(self, item: Any) -> TypeGuard[K]: ... def is_valid_value(self, item: Any) -> TypeGuard[V]: ... class TypeCheckedCache(Cache[K, V], TypeCheckProtocol[K, V]): def set(self, key: K, value: V) -> None: if self.is_valid_key(key) and self.is_valid_value(value): super().set(key, value) else: raise TypeError(\\"Invalid key or value type\\") # Example concrete class that checks for specific types class IntStrCache(TypeCheckedCache[int, str]): def is_valid_key(self, item: Any) -> TypeGuard[int]: return isinstance(item, int) def is_valid_value(self, item: Any) -> TypeGuard[str]: return isinstance(item, str) # Example usage: # cache = IntStrCache() # cache.set(1, \\"one\\") # Correct types # print(cache.get(1)) # Output: \\"one\\" # cache.set(\\"two\\", 2) # Should raise TypeError"},{"question":"Coding Assessment Question # Objective Write a Python function that converts a file of mixed textual and binary data into a clean, formatted text file. This task will test your understanding of text and binary data handling across Python 2 and Python 3, ensuring compatibility in your implementation. # Problem Statement You are given a file that contains mixed content with both textual data (encoded in UTF-8) and binary data. Your task is to write a function that reads the file, interprets the binary data as hexadecimal strings, and writes a new file with all content formatted as text. # Requirements 1. The function should be compatible with both Python 2.7 and Python 3. 2. You should utilize best practices for distinguishing between text and binary data. 3. Proper exception handling must be implemented to handle errors during file operations. # Function Specification Input - **file_path**: A string representing the path to the input file. - **output_path**: A string representing the path to the output file. Output - The function should write the processed content to the output file. # Example Given an input file `input.dat`: ``` Text data here. Binary data: x00x01x02 More text here. ``` Expected output in the output file `output.txt`: ``` Text data here. Binary data: 000102 More text here. ``` # Constraints - The file can contain arbitrary binary and text data mixed. - Assume the input file is not larger than 10MB. # Performance Requirements - The function should efficiently handle reading and writing files without consuming excessive memory. - Ensure that the solution is implemented using proper resource management techniques to avoid file handle leaks. # Function Signature ```python def process_mixed_file(file_path, output_path): pass ``` # Notes - Use `io.open()` for file operations to ensure consistent behavior between Python 2 and Python 3. - Use appropriate encoding/decoding techniques for handling text and binary conversions.","solution":"import io def process_mixed_file(file_path, output_path): Reads a file with mixed text and binary data, converts the binary data into a hexadecimal representation, and writes the result into a new text file. Params: file_path (str): Path to the input file containing mixed data. output_path (str): Path to the output file where the cleaned text will be written. Returns: None try: with io.open(file_path, \'rb\') as input_file, io.open(output_path, \'w\', encoding=\'utf-8\') as output_file: while True: chunk = input_file.read(4096) if not chunk: break for byte in chunk: if 32 <= byte <= 126 or byte in [9, 10, 13]: # We keep readable ASCII chars and tab/newline/carriage return. output_file.write(chr(byte)) else: output_file.write(format(byte, \'02x\')) except IOError as e: print(f\\"An IOError occurred: {e}\\")"},{"question":"**Title**: Advanced Plot Customization with Seaborn **Objective**: Assess the studentâ€™s understanding of seaborn\'s advanced plotting techniques, including dataset loading, faceted plotting, adding plot elements, and complex theme customization. **Problem**: You are provided with a dataset and your task is to create a well-customized plot using seaborn\'s objects interface. Follow the detailed requirements below. 1. **Dataset Loading**: - Load the `tips` dataset from seaborn\'s repository. 2. **Plot Creation**: - Create a `seaborn.objects.Plot` with: - `total_bill` on the x-axis. - `tip` on the y-axis. - Color differentiation based on `day`. 3. **Faceting**: - Create facets based on the `time` column, arranging these facets horizontally. 4. **Add Plot Elements**: - Add a linear regression line (`so.Line`, `so.PolyFit(order=1)`) to your plot. - Add scatter plot points (`so.Dot()`). 5. **Theme Customization**: - Apply a custom theme with the following modifications: - White axes facecolor (`axes.facecolor`: `w`). - Slate gray axes edgecolor (`axes.edgecolor`: `slategray`). - Line width for the line plot element should be 3 (`lines.linewidth`: 3). - Apply an additional style from `matplotlib` called `ggplot` to your plot. 6. **Output**: - Display the final customized plot. **Constraints**: - You must use seaborn and matplotlib packages only. - Ensure that the plot is clearly visualized without overlapping elements. **Performance Requirements**: - The code should execute efficiently and render the plot without errors. **Input**: No user input is required. The dataset should be loaded within the script. **Output**: A faceted, customized plot as per the requirements above. **Example**: Your plot should look like a multi-paneled figure with different facets for each `time` value, showing the relationship between `total_bill` and `tip` with added customization in styles and themes. ```python import seaborn.objects as so from seaborn import load_dataset from matplotlib import style # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot p = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"day\\") .facet(\\"time\\", wrap=1) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Apply theme and style p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 3}) p.theme(style.library[\\"ggplot\\"]) # Display the plot p.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset from matplotlib import style import matplotlib.pyplot as plt def create_custom_plot(): Creates a faceted, customized seaborn plot for the \'tips\' dataset. - X-axis: total_bill - Y-axis: tip - Color by \'day\' - Faceted by \'time\' - Includes linear regression line and scatter plot points - Applies custom theme and additional style # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot p = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"day\\") .facet(\\"time\\", wrap=1) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Apply theme and style custom_theme = { \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 3, } p.theme(custom_theme) # Apply ggplot style from matplotlib plt.style.use(\'ggplot\') # Display the plot p.show() # Call the function to display the plot create_custom_plot()"},{"question":"You are tasked with defining an enumeration for a traffic light system using Python\'s `enum` module. The traffic light system operates based on three states: **RED**, **YELLOW**, and **GREEN**. Each state has a specific duration (in seconds) before transitioning to the next state in the sequence. To complete the task, follow these instructions: 1. Create an Enumeration Define an enumeration called `TrafficLight` using the `Enum` base class from the `enum` module. The enumeration should consist of the following members: - `RED` with a duration of 60 seconds. - `YELLOW` with a duration of 5 seconds. - `GREEN` with a duration of 30 seconds. 2. Implement Custom Methods - Implement a method `duration` in the `TrafficLight` enumeration that returns the duration for the respective traffic light state. - Implement a `next_light` method that returns the next traffic light state in the sequence (`RED` -> `GREEN`, `GREEN` -> `YELLOW`, `YELLOW` -> `RED`). 3. Testing the Enumeration Write a function `test_traffic_light()` that tests the functionality of the `TrafficLight` enumeration. The function should: - Print each traffic light state and its duration. - Demonstrate the transition from one state to the next using the `next_light` method. # Constraints - Use Python\'s `enum` module. - Duration values must be integers. - You are not allowed to use global variables. # Input and Output - There is no input from the user. - The output will display the traffic light states, their durations, and the state transitions. # Example Output ```python TrafficLight.RED lasts for 60 seconds. TrafficLight.RED transitions to TrafficLight.GREEN TrafficLight.GREEN lasts for 30 seconds. TrafficLight.GREEN transitions to TrafficLight.YELLOW TrafficLight.YELLOW lasts for 5 seconds. TrafficLight.YELLOW transitions to TrafficLight.RED ``` # Hint Refer to the documentation on how to create custom methods in an enumeration and handle transitions between members.","solution":"from enum import Enum class TrafficLight(Enum): RED = 60 YELLOW = 5 GREEN = 30 def duration(self): return self.value def next_light(self): next_light_mapping = { TrafficLight.RED: TrafficLight.GREEN, TrafficLight.GREEN: TrafficLight.YELLOW, TrafficLight.YELLOW: TrafficLight.RED } return next_light_mapping[self]"},{"question":"**Objective:** The goal of this coding assessment is to test your understanding of tensor initialization methods in PyTorch and their application in neural network models. **Problem Statement:** You are tasked with designing a neural network for classifying images from the CIFAR-10 dataset. To improve the network\'s performance, you need to implement a custom weight initialization function using PyTorch\'s `torch.nn.init` module. You will then need to integrate this function into your network and train it. Specifically, you will: 1. Implement a custom weight initialization function that uses different initialization strategies for different layers of the network. 2. Define a simple Convolutional Neural Network (CNN) model for the CIFAR-10 classification task. 3. Apply your custom weight initialization function to the network. **Function 1: Custom Initialization** - **Function Name:** `custom_weight_init` - **Input:** `model (torch.nn.Module)` - **Output:** `None` - **Description:** This function should initialize the weights of the model\'s layers using: - Xavier Uniform initialization for convolutional layers. - Kaiming Normal initialization for fully connected layers. **Function 2: Define CNN Model** - **Function Name:** `define_cnn_model` - **Input:** `None` - **Output:** `model (torch.nn.Module)` - **Description:** Define a simple CNN with at least: - Two convolutional layers. - One fully connected layer. **Constraints:** - Ensure your model is compatible with the CIFAR-10 dataset (input size of 3x32x32). - Use `torch.nn.init.xavier_uniform_` and `torch.nn.init.kaiming_normal_` functions appropriately. **Example Usage:** ```python import torch import torch.nn as nn import torch.nn.init as init def custom_weight_init(model): for layer in model.modules(): if isinstance(layer, nn.Conv2d): init.xavier_uniform_(layer.weight) elif isinstance(layer, nn.Linear): init.kaiming_normal_(layer.weight) class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) self.fc1 = nn.Linear(32*8*8, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2(x), 2)) x = x.view(x.size(0), -1) x = self.fc1(x) return x model = SimpleCNN() custom_weight_init(model) # Model training code goes here ``` **Notes:** - Ensure your code runs without errors and is properly commented. - You do not need to include the CIFAR-10 data loading and training loop, but you should assume these would be integrated into a larger training script.","solution":"import torch import torch.nn as nn import torch.nn.init as init def custom_weight_init(model): Initializes the weights of the model\'s layers using specific initialization strategies: - Xavier Uniform initialization for convolutional layers. - Kaiming Normal initialization for fully connected layers. Parameters: model (torch.nn.Module): The neural network model. for layer in model.modules(): if isinstance(layer, nn.Conv2d): init.xavier_uniform_(layer.weight) elif isinstance(layer, nn.Linear): init.kaiming_normal_(layer.weight) class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) self.fc1 = nn.Linear(32 * 8 * 8, 10) def forward(self, x): x = torch.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2)) x = torch.relu(torch.nn.functional.max_pool2d(self.conv2(x), 2)) x = x.view(x.size(0), -1) # Flatten the output from conv layers x = self.fc1(x) return x def define_cnn_model(): Defines and returns a SimpleCNN model. Returns: model (torch.nn.Module): The CNN model. model = SimpleCNN() custom_weight_init(model) return model"},{"question":"<|Analysis Begin|> The provided documentation details about decision trees in scikit-learn, which are used for both classification and regression problems. It discusses the basic concepts, advantages, disadvantages, tree-building algorithms, handling missing values, pruning techniques, and multi-output support. Key points that can be used to frame a question: 1. **DecisionTreeClassifier and DecisionTreeRegressor classes**: These can be used to assess students\' understanding of both classification and regression problems. 2. **Tree construction (fit method) and prediction**: Students can be evaluated on their ability to train a model and make predictions. 3. **Pruning techniques (Minimal Cost-Complexity Pruning)**: This could be used to test the students\' knowledge on avoiding overfitting. 4. **Handling missing values**: This could test the students on how to manage datasets with missing entries. 5. **Visualizing and exporting trees**: This can include evaluating the student\'s ability to interpret and visualize the decision tree. Based on this analysis, a question can be designed that involves creating a decision tree, handling missing values, applying pruning, and visualizing the tree. <|Analysis End|> <|Question Begin|> # Advanced Coding Assessment: Decision Trees with scikit-learn Objective: This question assesses your ability to build, visualize, and interpret decision trees using scikit-learn. You will work with a dataset, handle missing values, apply pruning techniques, and visualize the decision tree. Problem Statement: Let\'s work with the famous Iris dataset. Your task is to build a decision tree classifier for this dataset. You should split the dataset into a training set and a test set, fit a decision tree on the training set, handle any missing values, prune the tree to avoid overfitting, and finally visualize the tree. 1. **Load the Iris dataset** from `sklearn.datasets`. 2. **Introduce missing values** in the dataset. Randomly set 10% of the values in the dataset to `NaN`. 3. **Split the dataset** into training and testing sets using `train_test_split` from `sklearn.model_selection`. 4. **Train a `DecisionTreeClassifier`** on the training set. Handle missing values by ensuring they are correctly managed during training and prediction. 5. **Apply Minimal Cost-Complexity Pruning** to avoid overfitting. Determine the best `ccp_alpha` parameter by cross-validation. 6. **Visualize the decision tree** using `plot_tree` from `sklearn.tree`. 7. **Export the tree** in Graphviz format and save it as `iris_tree.pdf`. Constraints: - Use `random_state=42` wherever randomization is involved for reproducibility. - Ensure you handle missing values correctly during model training and prediction. - Optimize the pruning parameter to avoid overfitting. Expected Input and Output Formats: - **Input**: No direct user input; the entire code should be self-contained within a function `build_and_visualize_tree`. - **Output**: The function should print the accuracy of the model on the test set and save the visualized tree as `iris_tree.pdf`. Function Signature: ```python def build_and_visualize_tree(): pass ``` Example Implementation: Here\'s a skeleton to help you get started. You need to fill in the missing parts: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz from sklearn.model_selection import train_test_split, cross_val_score import matplotlib.pyplot as plt import random def build_and_visualize_tree(): # Load the iris dataset iris = load_iris() X, y = iris.data, iris.target # Introduce missing values rng = np.random.default_rng(42) missing_mask = rng.random(X.shape) < 0.1 X[missing_mask] = np.nan # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize a DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=42) # Fit the classifier on the training data clf.fit(X_train, y_train) # Evaluate on the test data test_accuracy = clf.score(X_test, y_test) print(f\'Test accuracy before pruning: {test_accuracy:.2f}\') # Apply Minimal Cost-Complexity Pruning path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas best_alpha = None best_score = 0 for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) scores = cross_val_score(clf, X_train, y_train, cv=5) mean_score = np.mean(scores) if mean_score > best_score: best_score = mean_score best_alpha = ccp_alpha clf = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha) clf.fit(X_train, y_train) # Evaluate after pruning test_accuracy = clf.score(X_test, y_test) print(f\'Test accuracy after pruning: {test_accuracy:.2f}\') # Visualize the decision tree plt.figure(figsize=(20, 10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\'iris_tree.pdf\') # Export the tree in Graphviz format dot_data = export_graphviz(clf, out_file=None, filled=True, rounded=True, special_characters=True, feature_names=iris.feature_names, class_names=iris.target_names) with open(\\"iris_tree.dot\\", \\"w\\") as f: f.write(dot_data) ``` This function will: 1. Load the Iris dataset and introduce missing values. 2. Split the dataset into training and testing sets. 3. Train a `DecisionTreeClassifier`, handle missing values, prune the tree, and visualize it. 4. Finally, it will save the decision tree visualization as `iris_tree.pdf`. Ensure you understand each step and fill in any gaps in the logic.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz from sklearn.model_selection import train_test_split, cross_val_score import matplotlib.pyplot as plt import random from sklearn.impute import SimpleImputer def build_and_visualize_tree(): # Load the iris dataset iris = load_iris() X, y = iris.data, iris.target # Introduce missing values rng = np.random.default_rng(42) missing_mask = rng.random(X.shape) < 0.1 X[missing_mask] = np.nan # Handle missing values using SimpleImputer imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(X) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize a DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=42) # Fit the classifier on the training data clf.fit(X_train, y_train) # Evaluate on the test data test_accuracy = clf.score(X_test, y_test) print(f\'Test accuracy before pruning: {test_accuracy:.2f}\') # Apply Minimal Cost-Complexity Pruning path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas best_alpha = None best_score = 0 for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) scores = cross_val_score(clf, X_train, y_train, cv=5) mean_score = np.mean(scores) if mean_score > best_score: best_score = mean_score best_alpha = ccp_alpha clf = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha) clf.fit(X_train, y_train) # Evaluate after pruning test_accuracy = clf.score(X_test, y_test) print(f\'Test accuracy after pruning: {test_accuracy:.2f}\') # Visualize the decision tree plt.figure(figsize=(20, 10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\'iris_tree.pdf\') # Export the tree in Graphviz format dot_data = export_graphviz(clf, out_file=None, filled=True, rounded=True, special_characters=True, feature_names=iris.feature_names, class_names=iris.target_names) with open(\\"iris_tree.dot\\", \\"w\\") as f: f.write(dot_data)"},{"question":"Objective Your task is to implement a function that: 1. Downloads the \\"miceprotein\\" dataset from the OpenML repository using Scikit-learn. 2. Preprocesses the dataset by encoding categorical features as numerical values. 3. Splits the dataset into train and test subsets. Function Signature ```python def preprocess_miceprotein(test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: pass ``` Input - `test_size` (float): The proportion of the dataset to include in the test split (default is 0.2). - `random_state` (int): Controls the shuffling applied to the data before splitting (default is 42). Output - Returns a tuple `(X_train, X_test, y_train, y_test)` where: - `X_train` (pd.DataFrame): The training dataset after preprocessing. - `X_test` (pd.DataFrame): The testing dataset after preprocessing. - `y_train` (pd.Series): The target values for the training dataset. - `y_test` (pd.Series): The target values for the testing dataset. Constraints 1. Use Scikit-learn\'s `fetch_openml` to download the \\"miceprotein\\" dataset. 2. Encode all categorical features using `OneHotEncoder` from Scikit-learn. 3. Ensure that the train and test datasets have the same feature columns after encoding. 4. Use `train_test_split` to split the dataset into training and test subsets. Example ```python X_train, X_test, y_train, y_test = preprocess_miceprotein(test_size=0.25, random_state=1) print(X_train.shape) # Output: (810, n_features) # where n_features is the number of features after encoding print(X_test.shape) # Output: (270, n_features) ``` Notes - Ensure that target variable encoding is handled correctly. - Handle missing values appropriately if present. - Ensure reproducibility by setting the random state.","solution":"from typing import Tuple import pandas as pd from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline def preprocess_miceprotein(test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: # Fetch the dataset mice_protein = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) # Extract data X = mice_protein.data y = mice_protein.target # Identify categorical columns categorical_cols = X.select_dtypes(include=[\'object\']).columns # Create a preprocessor with OneHotEncoder for categorical columns preprocessor = ColumnTransformer( transformers=[ (\'cat\', OneHotEncoder(handle_unknown=\'ignore\'), categorical_cols) ], remainder=\'passthrough\' ) # Split the dataset into training and test portions X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y) # Fit and transform the preprocessor on the training set, and transform the test set X_train = preprocessor.fit_transform(X_train) X_test = preprocessor.transform(X_test) # Convert from numpy array to DataFrame for consistency if needed X_train = pd.DataFrame(X_train, columns=preprocessor.get_feature_names_out()) X_test = pd.DataFrame(X_test, columns=preprocessor.get_feature_names_out()) # Ensure y_train and y_test are Series y_train = pd.Series(y_train).reset_index(drop=True) y_test = pd.Series(y_test).reset_index(drop=True) return X_train, X_test, y_train, y_test"},{"question":"Objective Your task is to write a function that analyzes Unix user accounts on the system. Specifically, you will write a function to identify all users whose login name starts with a given prefix and return a detailed report of selected fields. Function Signature ```python def get_users_with_prefix(prefix: str) -> list: Returns a list of dictionaries, each representing a user whose login name starts with the given prefix. Parameters: - prefix (str): The prefix to match the login names against. Returns: - list: A list of dictionaries where each dictionary contains the \\"pw_name\\", \\"pw_uid\\", \\"pw_gid\\", and \\"pw_dir\\" of the respective user. ``` Input - `prefix` (str): The prefix to match the login names against. The prefix will always be non-empty and consist of alphanumeric characters. Output - A list of dictionaries. Each dictionary should contain the following keys: `\\"pw_name\\"`, `\\"pw_uid\\"`, `\\"pw_gid\\"`, and `\\"pw_dir\\"`. The list should be sorted by the `\\"pw_name\\"` field in ascending order. Constraints - The function should handle the case where no user matches the prefix by returning an empty list. - The function should use the `pwd` module to access the password database. - Performance with respect to the number of users is not a strict requirement but aim for readability and Pythonic code. Example ```python # Assumption based on Unix user entries: # [ # (\'root\', \'x\', 0, 0, \'root\', \'/root\', \'/bin/bash\'), # (\'daemon\', \'x\', 1, 1, \'daemon\', \'/usr/sbin\', \'/usr/sbin/nologin\'), # (\'user1\', \'x\', 1000, 1000, \'User One\', \'/home/user1\', \'/bin/bash\'), # (\'user2\', \'x\', 1001, 1001, \'User Two\', \'/home/user2\', \'/bin/bash\') # ] result = get_users_with_prefix(\'user\') # Expected output: # [{\'pw_name\': \'user1\', \'pw_uid\': 1000, \'pw_gid\': 1000, \'pw_dir\': \'/home/user1\'}, # {\'pw_name\': \'user2\', \'pw_uid\': 1001, \'pw_gid\': 1001, \'pw_dir\': \'/home/user2\'}] ``` Note - You can assume that the `pwd` module is available on the testing environment as the function is meant to be used on Unix systems.","solution":"import pwd def get_users_with_prefix(prefix: str) -> list: Returns a list of dictionaries, each representing a user whose login name starts with the given prefix. Parameters: - prefix (str): The prefix to match the login names against. Returns: - list: A list of dictionaries where each dictionary contains the \\"pw_name\\", \\"pw_uid\\", \\"pw_gid\\", and \\"pw_dir\\" of the respective user. users = [] for user in pwd.getpwall(): if user.pw_name.startswith(prefix): users.append({ \'pw_name\': user.pw_name, \'pw_uid\': user.pw_uid, \'pw_gid\': user.pw_gid, \'pw_dir\': user.pw_dir }) users.sort(key=lambda x: x[\'pw_name\']) return users"},{"question":"You are tasked with building a simple yet efficient web scraping application that can concurrently fetch data from a list of URLs. Your goal is to evaluate the performance difference between using `threading` and `multiprocessing` for concurrent execution. Requirements 1. **Fetch function**: Implement a function `fetch(url: str) -> str` that takes a URL and returns the HTML content as a string. Use the `requests` library to perform HTTP requests. 2. **Thread-based Scraper**: - Implement a function `thread_scraper(urls: List[str]) -> Dict[str, str]` that takes a list of URLs and returns a dictionary mapping each URL to its HTML content. - Use the `threading` module to fetch multiple URLs concurrently. - Limit the number of concurrent threads to 5. 3. **Process-based Scraper**: - Implement a function `process_scraper(urls: List[str]) -> Dict[str, str]` that takes a list of URLs and returns a dictionary mapping each URL to its HTML content. - Use the `multiprocessing` module to fetch multiple URLs concurrently. - Limit the number of concurrent processes to 5. Input and Output Formats - Input: A list of URLs, e.g., `[\\"https://example.com\\", \\"https://example.org\\"]` - Output: A dictionary where each key is a URL and the value is its corresponding HTML content. Constraints 1. You must handle exceptions gracefully, ensuring that the program doesn\'t crash if a URL fails to fetch. 2. Use a limiter to ensure only 5 concurrent threads or processes are running at any given time. 3. You are encouraged to measure and display the time taken to fetch all URLs using both methods for comparison. Example ```python urls = [\\"https://example.com\\", \\"https://example.org\\", \\"https://example.net\\"] # Using Thread Scraper thread_results = thread_scraper(urls) # Using Process Scraper process_results = process_scraper(urls) print(\\"Thread Scraper Results:\\", thread_results) print(\\"Process Scraper Results:\\", process_results) ``` Performance Requirements - Aim to minimize the total execution time for fetching all URLs, utilizing concurrent programming effectively. - Compare the execution times for thread-based and process-based implementations, explaining any observed differences. You will be evaluated on the correctness, efficiency, and clarity of your code. Ensure that your solution adheres to Python\'s concurrency best practices and handles all specified requirements.","solution":"import requests import threading import multiprocessing from queue import Queue from typing import List, Dict import time # Fetch function to get the HTML content of a URL def fetch(url: str) -> str: try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException: return \\"\\" # Thread-based scraper def thread_scraper(urls: List[str]) -> Dict[str, str]: def worker(): while True: url = url_queue.get() if url is None: break result[url] = fetch(url) url_queue.task_done() url_queue = Queue() result = {} threads = [] for _ in range(5): # limiting to 5 threads t = threading.Thread(target=worker) t.start() threads.append(t) for url in urls: url_queue.put(url) url_queue.join() for _ in range(5): url_queue.put(None) for t in threads: t.join() return result # Worker function for multiprocessing def process_worker(url: str) -> (str, str): return (url, fetch(url)) # Process-based scraper def process_scraper(urls: List[str]) -> Dict[str, str]: with multiprocessing.Pool(5) as pool: # limiting to 5 processes results = pool.map(process_worker, urls) return dict(results)"},{"question":"Objective: Demonstrate the ability to handle Python module imports, loading, and reloading using the `importlib` module, which supersedes the deprecated `imp` module. Problem Statement: The `imp` module is deprecated, and you are required to write a Python program that effectively replaces its functionality using the `importlib` module. Specifically, your program should: 1. **Import a module:** - Implement a function `import_module(module_name: str) -> ModuleType` that takes a module name as a string and imports the module using `importlib`. - Ensure that if the module is already imported, it should return the existing module from `sys.modules`. 2. **Reload a module:** - Implement a function `reload_module(module_name: str) -> ModuleType` that takes a module name as a string and reloads the module if it is already imported. - If the module is not imported, import it first, then reload it. 3. **Caching byte-compiled files:** - Implement a function `cache_bytecode(module_path: str, debug_override: Optional[bool] = None) -> str` that takes a module path and optionally a debug override flag, returns the path to the byte-compiled file using the `importlib.util.cache_from_source()` method. 4. **Find the source file from the cache:** - Implement a function `source_from_cache(cache_path: str) -> str` that takes a path to a byte-compiled file and returns the source file path using the `importlib.util.source_from_cache()` method. Input and Output Formats: 1. **import_module(module_name: str) -> ModuleType:** - **Input:** A string representing the module name. - **Output:** The module object. 2. **reload_module(module_name: str) -> ModuleType:** - **Input:** A string representing the module name. - **Output:** The module object after reloading. 3. **cache_bytecode(module_path: str, debug_override: Optional[bool] = None) -> str:** - **Input:** A string representing the module file path, and an optional boolean for debug override. - **Output:** A string representing the path to the byte-compiled file. 4. **source_from_cache(cache_path: str) -> str:** - **Input:** A string representing the path to a byte-compiled file. - **Output:** A string representing the path to the source file. Example: ```python import_module(\'os\') # Should import and return the \'os\' module reload_module(\'os\') # Should reload and return the \'os\' module cache_bytecode(\'/path/to/module.py\', debug_override=True) # Should return the path to the byte-compiled file source_from_cache(\'/path/to/__pycache__/module.cpython-310.pyc\') # Should return the source file path ``` Constraints: - Assume that the module path and names provided are valid and exist. - Consider edge cases like reloading a module that is not yet imported, or paths that don\'t strictly follow PEP 3147 norms. Happy coding!","solution":"import importlib import sys from types import ModuleType from typing import Optional def import_module(module_name: str) -> ModuleType: Imports and returns the specified module. If the module is already imported, returns the existing module from sys.modules. if module_name in sys.modules: return sys.modules[module_name] return importlib.import_module(module_name) def reload_module(module_name: str) -> ModuleType: Reloads the specified module if it is already imported. If not, it imports the module first. if module_name not in sys.modules: import_module(module_name) return importlib.reload(sys.modules[module_name]) def cache_bytecode(module_path: str, debug_override: Optional[bool] = None) -> str: Returns the path to the byte-compiled file for the specified module file path. Optionally accepts a debug override flag. return importlib.util.cache_from_source(module_path, debug_override=debug_override) def source_from_cache(cache_path: str) -> str: Returns the source file path from the specified byte-compiled file path. return importlib.util.source_from_cache(cache_path)"},{"question":"# Unicode Handling and Transformation in Python Objective In this assessment, you are required to implement several functions that demonstrate your understanding of Python\'s Unicode handling capabilities. These functions will leverage Python\'s Unicode APIs to perform various operations including encoding, decoding, and transforming Unicode strings. Task You need to implement the following three functions: 1. **decode_utf8(encoded_str: bytes) -> str** This function should take a `bytes` object containing a UTF-8 encoded string and return a Python `str` object by decoding it. **Constraints:** - If the input is not valid UTF-8, the function should raise a `UnicodeDecodeError`. - The decoding should use the \'strict\' error handling. 2. **encode_to_utf8(unicode_str: str) -> bytes** This function should take a Python `str` object and encode it to a UTF-8 `bytes` object. **Constraints:** - The encoding should use the \'strict\' error handling. 3. **uppercase_unicode(unicode_str: str) -> str** This function should return a new string where all the alphabetic characters in the input `unicode_str` are converted to their uppercase equivalents using the Unicode character properties. **Constraints:** - The function should handle all Unicode characters, not just ASCII. Input and Output Format - `decode_utf8(encoded_str: bytes) -> str` - **Input:** A `bytes` object containing a valid UTF-8 encoded string. - **Output:** A decoded `str` object. - `encode_to_utf8(unicode_str: str) -> bytes` - **Input:** A `str` object. - **Output:** A UTF-8 encoded `bytes` object. - `uppercase_unicode(unicode_str: str) -> str` - **Input:** A `str` object. - **Output:** A new `str` object with all alphabetic characters converted to uppercase. Function Signatures ```python def decode_utf8(encoded_str: bytes) -> str: pass def encode_to_utf8(unicode_str: str) -> bytes: pass def uppercase_unicode(unicode_str: str) -> str: pass ``` Examples - `decode_utf8(b\'xe4xbdxa0xe5xa5xbd\') -> \'ä½ å¥½\'` - `encode_to_utf8(\'hello\') -> b\'hello\'` - `uppercase_unicode(\'hello\') -> \'HELLO\'` - `uppercase_unicode(\'Guten Tag\') -> \'GUTEN TAG\'` - `uppercase_unicode(\'Ð”Ð¾Ð±Ñ€Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ\') -> \'Ð”ÐžÐ‘Ð Ð«Ð™ Ð”Ð•ÐÐ¬\'` **Note:** For the third function, make sure that you handle examples with characters from different alphabets, as shown above. Performance Requirements - All functions should efficiently handle input strings of substantial length, up to at least 10,000 characters. Your task is to implement these three functions according to the specifications above.","solution":"def decode_utf8(encoded_str: bytes) -> str: Decodes a UTF-8 encoded bytes object to a Unicode string. Parameters: encoded_str (bytes): A bytes object containing a UTF-8 encoded string. Returns: str: A decoded Unicode string. Raises: UnicodeDecodeError: If the input bytes object is not valid UTF-8. return encoded_str.decode(\'utf-8\') def encode_to_utf8(unicode_str: str) -> bytes: Encodes a Unicode string to a UTF-8 bytes object. Parameters: unicode_str (str): A Unicode string. Returns: bytes: A UTF-8 encoded bytes object. return unicode_str.encode(\'utf-8\') def uppercase_unicode(unicode_str: str) -> str: Converts all alphabetic characters in the input Unicode string to their uppercase equivalents. Parameters: unicode_str (str): A Unicode string. Returns: str: A new string with all alphabetic characters converted to uppercase. return unicode_str.upper()"},{"question":"# Advanced HTML Parser Implementation Using the `html.parser` module in Python 3.10, design a custom parser to analyze HTML documents and extract specific information. Your goal is to implement a parser that: 1. Collects all URLs from the `href` attributes of `<a>` tags. 2. Counts the number of occurrences of each HTML tag. 3. Identifies and extracts all text within `<title>` tags. 4. Identifies and counts all comments within the HTML. # Your task is to: 1. Create a subclass of `HTMLParser` named `AdvancedHTMLParser`. 2. Implement methods to handle start tags, end tags, data, and comments. 3. Provide a method `get_report()` that returns: - A list of all collected URLs. - A dictionary with tag names as keys and their occurrence counts as values. - The content of the `<title>` tag. - The total number of comments. Constraints - You may assume the input HTML string is well-formed. - Handle nested tags appropriately. - Your solution should be able to process large HTML strings efficiently. # Example ```python html_content = <!DOCTYPE html> <html> <head> <title>Sample Page</title> </head> <body> <h1>Welcome to the sample page</h1> <a href=\\"https://example.com\\">Example</a> <a href=\\"https://test.com\\">Test</a> <!-- This is a comment --> <p>This is a paragraph.</p> <!-- Another comment --> </body> </html> parser = AdvancedHTMLParser() parser.feed(html_content) report = parser.get_report() print(report) # Outputs: # { # \\"urls\\": [\\"https://example.com\\", \\"https://test.com\\"], # \\"tag_counts\\": {\\"html\\": 1, \\"head\\": 1, \\"title\\": 1, \\"body\\": 1, \\"h1\\": 1, \\"a\\": 2, \\"p\\": 1}, # \\"title\\": \\"Sample Page\\", # \\"comment_count\\": 2 # } ``` # Requirements 1. Implement the class with all necessary methods. 2. Handle character references properly. 3. Use efficient data structures to store and retrieve the required information.","solution":"from html.parser import HTMLParser class AdvancedHTMLParser(HTMLParser): def __init__(self): super().__init__() self.urls = [] self.tag_counts = {} self.title = None self.current_tag = None self.comment_count = 0 def handle_starttag(self, tag, attrs): self.tag_counts[tag] = self.tag_counts.get(tag, 0) + 1 if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.urls.append(attr[1]) self.current_tag = tag def handle_endtag(self, tag): self.current_tag = None def handle_data(self, data): if self.current_tag == \'title\': self.title = data def handle_comment(self, data): self.comment_count += 1 def get_report(self): return { \'urls\': self.urls, \'tag_counts\': self.tag_counts, \'title\': self.title, \'comment_count\': self.comment_count }"},{"question":"# Pandas Coding Assessment: Customer Data Analytics You are provided with a dataset of customer transactions. Each transaction includes a customer ID, the product purchased, the quantity purchased, the price per unit, and the date of the transaction. Your task is to manipulate this dataset to answer specific queries and perform data cleaning operations. **Dataset:** ```python import pandas as pd data = { \'customer_id\': [1, 2, 2, 3, 4, 4, 5, 5, 5, 6], \'product\': [\'A\', \'A\', \'B\', \'A\', \'C\', \'C\', \'A\', \'C\', \'B\', \'A\'], \'quantity\': [2, 3, 1, 1, 5, 5, 2, 4, 1, 2], \'price_per_unit\': [10, 15, 5, 10, 20, 20, 10, 20, 5, 10], \'date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-02\', \'2023-01-03\', \'2023-01-03\', \'2023-01-03\', \'2023-01-04\', \'2023-01-04\', \'2023-01-04\', \'2023-01-05\'] } df = pd.DataFrame(data) ``` **Instructions:** 1. **Indexing and Slicing:** - Retrieve all transactions made by customers with IDs 2 and 5. - Retrieve all transactions of product \'A\' made after \'2023-01-03\'. 2. **Boolean Indexing:** - Select all transactions where the quantity purchased is greater than 2 and the price per unit is less than or equal to 20. - Replace all instances where the product is \'C\' with \'D\' if the quantity is greater than 4. 3. **Setting with Enlargement:** - Add a new column `total_cost` which is the product of `quantity` and `price_per_unit`. - Add a new transaction for a new customer ID 7 who bought 3 units of product \'B\' at 7 per unit on \'2023-01-06\'. 4. **Querying Data:** - Use the `.query` method to find all transactions where the product is \'A\' and the `total_cost` is greater than 15. - Use the `.query` method to find all transactions that occurred in the month of January 2023. 5. **Handling Duplicates:** - Check for and list any duplicate transactions based on `customer_id`, `product`, and `date`. - Remove duplicate transactions while keeping the last occurrence. **Constraints:** - You may assume that the dataset will fit into memory. - The `date` field is in the format \'YYYY-MM-DD\' and should be treated as a string for this task. - All code should be written in Python using the pandas library. **Performance Requirements:** - Ensure that operations are efficient and leverage pandas\' optimized indexing methods. - Avoid using loops, and instead use vectorized operations provided by pandas. **Expected Output:** Your code should perform the above operations and display the resulting dataframes for each step. Please include comments explaining each stage of your solution.","solution":"import pandas as pd def analyze_customer_data(df): # Retrieve all transactions made by customers with IDs 2 and 5 cust_2_and_5 = df[df[\'customer_id\'].isin([2, 5])] # Retrieve all transactions of product \'A\' made after \'2023-01-03\' product_a_after_date = df[(df[\'product\'] == \'A\') & (df[\'date\'] > \'2023-01-03\')] # Select all transactions where the quantity purchased is greater than 2 and the price per unit is less than or equal to 20 quantity_and_price = df[(df[\'quantity\'] > 2) & (df[\'price_per_unit\'] <= 20)] # Replace all instances where the product is \'C\' with \'D\' if the quantity is greater than 4 df.loc[(df[\'product\'] == \'C\') & (df[\'quantity\'] > 4), \'product\'] = \'D\' # Add a new column `total_cost` df[\'total_cost\'] = df[\'quantity\'] * df[\'price_per_unit\'] # Add a new transaction for customer ID 7 new_transaction = pd.DataFrame({ \'customer_id\': [7], \'product\': [\'B\'], \'quantity\': [3], \'price_per_unit\': [7], \'date\': [\'2023-01-06\'], \'total_cost\': [3 * 7] }) df = pd.concat([df, new_transaction], ignore_index=True) # Use the `.query` method to find transactions where product is \'A\' and the `total_cost` > 15 product_a_total_cost = df.query(\\"product == \'A\' and total_cost > 15\\") # Use the `.query` method to find transactions in January 2023 jan_2023_transactions = df.query(\\"date.str.startswith(\'2023-01\')\\") # Check for duplicate transactions based on `customer_id`, `product`, and `date` duplicates = df[df.duplicated(subset=[\'customer_id\', \'product\', \'date\'], keep=False)] # Remove duplicate transactions while keeping the last occurrence df = df.drop_duplicates(subset=[\'customer_id\', \'product\', \'date\'], keep=\'last\') return { \'cust_2_and_5\': cust_2_and_5, \'product_a_after_date\': product_a_after_date, \'quantity_and_price\': quantity_and_price, \'modified_dataframe\': df, \'product_a_total_cost\': product_a_total_cost, \'jan_2023_transactions\': jan_2023_transactions, \'duplicates\': duplicates, \'cleaned_dataframe\': df }"},{"question":"**Objective:** You are required to implement functions to work with date series using pandas\' date offset functionalities. **Problem Statement:** 1. **generate_date_range**: Implement a function `generate_date_range` that generates a date range for a given period with a specific frequency. The function should take starting and ending dates, and a frequency string that defines the interval between dates. 2. **filter_by_month_start**: Implement a function `filter_by_month_start` that filters the generated date range to include only the dates which are at the start of the month. 3. **only_business_days**: Implement a function `only_business_days` that converts the generated date range to include only business days according to the specified calendar. **Function Definitions:** ```python import pandas as pd from pandas.tseries.offsets import DateOffset, BusinessDay from pandas.tseries.holiday import USFederalHolidayCalendar from typing import List def generate_date_range(start_date: str, end_date: str, freq: str) -> pd.DatetimeIndex: Generate a date range with the specified frequency. Args: start_date (str): The start date in \'YYYY-MM-DD\' format. end_date (str): The end date in \'YYYY-MM-DD\' format. freq (str): The frequency string (e.g., \'D\' for daily, \'B\' for business days). Returns: pd.DatetimeIndex: A range of dates. pass def filter_by_month_start(dates: pd.DatetimeIndex) -> pd.DatetimeIndex: Filter the date range to include only the dates that are the start of the month. Args: dates (pd.DatetimeIndex): The pandas DatetimeIndex object containing date range. Returns: pd.DatetimeIndex: Filtered dates at the start of the month. pass def only_business_days(dates: pd.DatetimeIndex, calendar = USFederalHolidayCalendar()) -> pd.DatetimeIndex: Convert the generated date range to include only business days. Args: dates (pd.DatetimeIndex): The pandas DatetimeIndex object containing date range. calendar: The calendar to consider for holidays (default is USFederalHolidayCalendar). Returns: pd.DatetimeIndex: Filtered dates that are business days. pass ``` **Constraints:** - The date range will not exceed 10 years. - The frequency string will be a valid pandas offset alias. - Implement all three functions. Partial solutions will not be accepted. **Example:** ```python # Example usage of the functions: # Generate date range dates = generate_date_range(\\"2021-01-01\\", \\"2021-12-31\\", \\"D\\") print(dates) # Output: [2021-01-01, 2021-01-02, ..., 2021-12-31] # Filter by month start month_start_dates = filter_by_month_start(dates) print(month_start_dates) # Output: [2021-01-01, 2021-02-01, ..., 2021-12-01] # Filter by business days business_days = only_business_days(dates) print(business_days) # Output: [2021-01-01, 2021-01-04, ..., 2021-12-31] (non-business days excluded) ``` **Performance Requirement:** The functions should be efficient and able to handle date ranges spanning up to 10 years without significant performance degradation.","solution":"import pandas as pd from pandas.tseries.holiday import USFederalHolidayCalendar def generate_date_range(start_date: str, end_date: str, freq: str) -> pd.DatetimeIndex: Generate a date range with the specified frequency. Args: start_date (str): The start date in \'YYYY-MM-DD\' format. end_date (str): The end date in \'YYYY-MM-DD\' format. freq (str): The frequency string (e.g., \'D\' for daily, \'B\' for business days). Returns: pd.DatetimeIndex: A range of dates. return pd.date_range(start=start_date, end=end_date, freq=freq) def filter_by_month_start(dates: pd.DatetimeIndex) -> pd.DatetimeIndex: Filter the date range to include only the dates that are the start of the month. Args: dates (pd.DatetimeIndex): The pandas DatetimeIndex object containing date range. Returns: pd.DatetimeIndex: Filtered dates at the start of the month. return dates[dates.is_month_start] def only_business_days(dates: pd.DatetimeIndex, calendar = USFederalHolidayCalendar()) -> pd.DatetimeIndex: Convert the generated date range to include only business days. Args: dates (pd.DatetimeIndex): The pandas DatetimeIndex object containing date range. calendar: The calendar to consider for holidays (default is USFederalHolidayCalendar). Returns: pd.DatetimeIndex: Filtered dates that are business days. business_days = pd.DatetimeIndex(dates).to_series() business_days = business_days[~business_days.index.isin(calendar.holidays())] return business_days[business_days.index.weekday < 5].index"},{"question":"You are given a dataset containing patient medical records, each labeled as whether the patient has cancer (1) or not (0). As a data scientist, your task is to build a classification model to predict the presence of cancer based on the given features. Importantly, you need to tune the decision threshold for the classification to enhance the recall of the model, as identifying all potential cancer cases is critical. Implement the following steps using scikit-learn: 1. **Load the dataset**: Assume you have the dataset in CSV format with features (X) and labels (y). 2. **Split the dataset** into training and testing sets. 3. **Train a basic classifier** on the training set. 4. **Tune the decision threshold** of the classifier using `TunedThresholdClassifierCV` to maximize the recall score. 5. **Evaluate the performance** of both the original classifier and the threshold-tuned classifier on the test set. **Input**: 1. Path to the CSV file containing the dataset. The CSV file will have the following columns: - Feature columns (e.g., â€˜feature1â€™, â€˜feature2â€™, ...). - Label column (\'label\') indicating the presence of cancer (1) or not (0). **Output**: 1. Recall score of the original classifier on the test set. 2. Recall score of the threshold-tuned classifier on the test set. 3. Tuned decision threshold. **Requirements and Constraints**: - Use an appropriate scikit-learn classifier, such as `LogisticRegression` or `DecisionTreeClassifier`. - Use 5-fold cross-validation for `TunedThresholdClassifierCV`. - Document and explain each step in your code for clarity. **Sample Code Skeleton**: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score # Load the dataset (provide path to the CSV file) file_path = \'path_to_dataset.csv\' data = pd.read_csv(file_path) X = data.drop(columns=[\'label\']) y = data[\'label\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Logistic Regression classifier base_model = LogisticRegression() base_model.fit(X_train, y_train) # Get recall score of the original classifier original_recall = recall_score(y_test, base_model.predict(X_test)) # Tune the decision threshold using TunedThresholdClassifierCV scorer = make_scorer(recall_score, pos_label=1) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=5) tuned_model.fit(X_train, y_train) # Get the recall score of the threshold-tuned classifier tuned_recall = recall_score(y_test, tuned_model.predict(X_test)) # Get the best decision threshold best_threshold = tuned_model.best_threshold_ print(f\\"Original Recall: {original_recall}\\") print(f\\"Tuned Recall: {tuned_recall}\\") print(f\\"Tuned Decision Threshold: {best_threshold}\\") ``` Ensure your solution meets the requirements and constraints while following the provided code skeleton for structure and guidance.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import recall_score, make_scorer from sklearn.model_selection import StratifiedKFold import numpy as np class TunedThresholdClassifierCV: A custom classifier that tunes the decision threshold using cross-validation. def __init__(self, base_estimator, scoring, cv): Initialization. :param base_estimator: The base classifier. :param scoring: Scoring function to maximize. :param cv: Number of cross-validation folds. self.base_estimator = base_estimator self.scoring = scoring self.cv = cv self.best_threshold_ = None def fit(self, X, y): Fit the classifier and tune the decision threshold. :param X: Features. :param y: Labels. kf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=42) threshold_scores = [] thresholds = np.linspace(0, 1, 101) for train_idx, val_idx in kf.split(X, y): X_train, X_val = X[train_idx], X[val_idx] y_train, y_val = y[train_idx], y[val_idx] self.base_estimator.fit(X_train, y_train) y_proba = self.base_estimator.predict_proba(X_val)[:, 1] fold_scores = [] for threshold in thresholds: y_pred_thresh = (y_proba >= threshold).astype(int) score = self.scoring._score_func(y_val, y_pred_thresh) fold_scores.append(score) threshold_scores.append(fold_scores) mean_scores = np.mean(threshold_scores, axis=0) self.best_threshold_ = thresholds[np.argmax(mean_scores)] def predict(self, X): Predict labels using the tuned threshold. :param X: Features. :return: Predictions. y_proba = self.base_estimator.predict_proba(X)[:, 1] return (y_proba >= self.best_threshold_).astype(int) def main(file_path): # Load the dataset data = pd.read_csv(file_path) X = data.drop(columns=[\'label\']).values y = data[\'label\'].values # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Logistic Regression classifier base_model = LogisticRegression() base_model.fit(X_train, y_train) # Get recall score of the original classifier original_recall = recall_score(y_test, base_model.predict(X_test)) # Tune the decision threshold using TunedThresholdClassifierCV scorer = make_scorer(recall_score, pos_label=1) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=5) tuned_model.fit(X_train, y_train) # Get the recall score of the threshold-tuned classifier tuned_recall = recall_score(y_test, tuned_model.predict(X_test)) # Get the best decision threshold best_threshold = tuned_model.best_threshold_ return original_recall, tuned_recall, best_threshold"},{"question":"# **Python Coding Assessment Question** **Objective:** Write a Python function using the `configparser` module to handle configuration file management. The function should be able to create a configuration file, update its values, and read values back from it while ensuring particular constraints. **Function Signature:** ```python def manage_config(file_path, sections, default_section=\\"DEFAULT\\", update=None, query=None): Manages a configuration file at the specified location. :param file_path: str, path to the configuration file. :param sections: dict, a dictionary with section names as keys and dictionaries of key-value pairs as values. - Example: { \\"section1\\": {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"}, \\"section2\\": {\\"keyA\\": \\"valueA\\", \\"keyB\\": \\"valueB\\"} } :param default_section: str, name of the default section (default is \\"DEFAULT\\"). :param update: dict or None, a dictionary specifying the updates consisting of section and updated key-value pairs. - Example: { \\"section1\\": {\\"key1\\": \\"new_value\\"} } :param query: list or None, a list of tuples where each tuple contains (section_name, key) to query the value. - Example: [(\\"section1\\", \\"key1\\"), (\\"section2\\", \\"keyA\\")] :return: dict, queried values as {section_name: {key: value}} or {} if `query` is None. - Example: { \\"section1\\": {\\"key1\\": \\"new_value\\"}, \\"section2\\": {\\"keyA\\": \\"valueA\\"} } pass ``` **Task Description:** 1. **File Creation and Initialization:** - Your function should create a configuration file at `file_path` with the sections and key-value pairs specified in `sections`. - If `default_section` is provided, use it for default values across sections. 2. **Updating Values:** - If the `update` parameter is provided, update the specified section\'s key-value pairs with the new values. 3. **Querying Values:** - If the `query` parameter is provided, fetch the specified values and return them in a dictionary format as described in the function signature. 4. **Constraints:** - You must handle multiline values and comments appropriately within the file. - Use the appropriate `configparser` methods to ensure you handle types correctly (e.g., integers and booleans). - Ensure that keys are case-insensitive. - Handle possible exceptions during file operations and parsing, providing meaningful error messages. 5. **Example Usage:** ```python # Creating a configuration file file_path = \'config.ini\' sections = { \\"DEFAULT\\": {\\"ServerAliveInterval\\": \\"45\\", \\"Compression\\": \\"yes\\", \\"CompressionLevel\\": \\"9\\"}, \\"forge.example\\": {\\"User\\": \\"hg\\"}, \\"topsecret.server.example\\": {\\"Port\\": \\"50022\\", \\"ForwardX11\\": \\"no\\"} } manage_config(file_path, sections) # Updating a value update = {\\"forge.example\\": {\\"User\\": \\"new_user\\"}} manage_config(file_path, sections, update=update) # Querying values query = [(\\"forge.example\\", \\"User\\"), (\\"topsecret.server.example\\", \\"Port\\")] result = manage_config(file_path, sections, query=query) print(result) # Expected output: {\'forge.example\': {\'User\': \'new_user\'}, \'topsecret.server.example\': {\'Port\': \'50022\'}} ``` **Note:** Your implementation should be efficient and robust, making appropriate use of the `configparser` module\'s features to handle configuration files effectively.","solution":"import configparser import os def manage_config(file_path, sections, default_section=\\"DEFAULT\\", update=None, query=None): Manages a configuration file at the specified location. :param file_path: str, path to the configuration file. :param sections: dict, a dictionary with section names as keys and dictionaries of key-value pairs as values. :param default_section: str, name of the default section (default is \\"DEFAULT\\"). :param update: dict or None, a dictionary specifying the updates consisting of section and updated key-value pairs. :param query: list or None, a list of tuples where each tuple contains (section_name, key) to query the value. :return: dict, queried values as {section_name: {key: value}} or {} if `query` is None. config = configparser.ConfigParser() # Check if file exists and read it if necessary if os.path.exists(file_path): config.read(file_path) # Create the config sections and keys if the file does not exist already for section, keys in sections.items(): if not config.has_section(section) and section != default_section: config.add_section(section) for key, value in keys.items(): config[section][key] = value # Handle updates if any if update: for section, keys in update.items(): if not config.has_section(section): config.add_section(section) for key, value in keys.items(): config[section][key] = value # Write the updated or newly created config file with open(file_path, \'w\') as config_file: config.write(config_file) # Handle query if any result = {} if query: for section, key in query: try: value = config.get(section, key) if section not in result: result[section] = {} result[section][key] = value except (configparser.NoSectionError, configparser.NoOptionError): # If section or key doesn\'t exist, handle it gracefully result[section] = None result[section] = {key: None} return result"},{"question":"**Objective:** To evaluate the understanding of Copy-on-Write (CoW) behavior in pandas 3.0, and how it affects the mutability of DataFrame and Series objects. # Question: Given a DataFrame `df` structured as below: ```python import pandas as pd data = { \\"A\\": [1, 2, 3, 4, 5], \\"B\\": [5, 4, 3, 2, 1], \\"C\\": [1, 3, 5, 7, 9], \\"D\\": [9, 7, 5, 3, 1] } df = pd.DataFrame(data) ``` Write a function `modify_dataframe` that performs the following operations while ensuring compliance with Copy-on-Write rules: 1. Create a new column `E` which is the sum of columns `A` and `B`. 2. Replace all occurrences of the number `3` in column `A` with `30` without using chained assignment. 3. Create a new DataFrame, `df_subset`, containing only columns `A` and `E`. 4. Modify the first element of column `A` in `df_subset` to `100`. Ensure that this modification does not affect the original DataFrame `df`. Finally, return both the transformed `df` and `df_subset`. # Function Signature: ```python def modify_dataframe(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame): pass ``` # Constraints: - Do not use chained assignment. - Ensure the original DataFrame `df` and the newly created DataFrame `df_subset` are managed properly per Copy-on-Write principles. - Follow best practices to avoid unnecessary copying of data. # Example: ```python df = pd.DataFrame({ \\"A\\": [1, 2, 3, 4, 5], \\"B\\": [5, 4, 3, 2, 1], \\"C\\": [1, 3, 5, 7, 9], \\"D\\": [9, 7, 5, 3, 1] }) df_transformed, df_subset = modify_dataframe(df) print(df_transformed) # Output should be: # A B C D E # 0 1 5 1 9 6 # 1 2 4 3 7 6 # 2 30 3 5 5 6 # 3 4 2 7 3 6 # 4 5 1 9 1 6 print(df_subset) # Output should be: # A E # 0 100 6 # 1 2 6 # 2 30 6 # 3 4 6 # 4 5 6 ``` # Note: - The DataFrame `df_transformed` should reflect the changes applied to `df`. - The DataFrame `df_subset` should reflect independent modification and should not affect `df`.","solution":"import pandas as pd def modify_dataframe(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame): # Step 1: Create a new column E in df, which is the sum of columns A and B df[\'E\'] = df[\'A\'] + df[\'B\'] # Step 2: Replace all occurrences of the number 3 in column A with 30 df.loc[df[\'A\'] == 3, \'A\'] = 30 # Step 3: Create a new DataFrame, df_subset, containing only columns A and E df_subset = df[[\'A\', \'E\']].copy() # Step 4: Modify the first element of column A in df_subset to 100 df_subset.loc[0, \'A\'] = 100 # Return the transformed df and the independent df_subset return df, df_subset"},{"question":"**Title**: Testing with `unittest.mock` **Objective**: Demonstrate the ability to use the `unittest.mock` module to write effective unit tests by mocking method calls, tracking call sequences, and using patch decorators. **Task**: You are required to implement a testing suite for a Python module that interacts with multiple external services. Your task is to create mocks for the external service calls, ensure they are made with the correct parameters, and assert the sequence and behavior of these calls. **Scenario**: Consider the following Python classes and functions representing a simplified version of an application that interacts with an external payment service and logging service: ```python class PaymentService: def process_payment(self, amount): pass def validate_payment(self, payment_id): pass class LoggingService: def log_transaction(self, transaction_id, success): pass class PaymentProcessor: def __init__(self, payment_service, logging_service): self.payment_service = payment_service self.logging_service = logging_service def process_and_log_payment(self, amount, transaction_id): if self.payment_service.process_payment(amount): self.logging_service.log_transaction(transaction_id, True) else: self.logging_service.log_transaction(transaction_id, False) ``` **Requirement**: 1. **Setup your tests**: Use the `unittest` module to create a test case class. 2. **Mocking Methods**: Mock the `process_payment` method of `PaymentService` to simulate successful and failed payment scenarios. 3. **Return Values and Side Effects**: Configure the mocks to return appropriate values and raise exceptions. 4. **Assert Call Sequences**: Ensure that `log_transaction` is called with the correct parameters based on the payment outcome. 5. **Patch Decorators**: Use patch decorators to mock external services within your test methods or test class. **Input/Output**: - Your test class should not require any input from the console. - Ensure that your assertions verify the correct sequence of method calls and the parameters passed to them. **Constraints**: 1. Your mock objects should be created using `MagicMock`. 2. Ensure proper cleanup and restoration of objects through the use of patch decorators or `addCleanup`. **Example Test Case**: ```python import unittest from unittest.mock import MagicMock, patch from your_module import PaymentService, LoggingService, PaymentProcessor class TestPaymentProcessor(unittest.TestCase): @patch(\'your_module.PaymentService\') @patch(\'your_module.LoggingService\') def test_process_and_log_payment_success(self, MockLoggingService, MockPaymentService): # Arrange mock_payment_service = MockPaymentService.return_value mock_logging_service = MockLoggingService.return_value # Mock the process_payment method to return True mock_payment_service.process_payment.return_value = True processor = PaymentProcessor(mock_payment_service, mock_logging_service) # Act processor.process_and_log_payment(100, \'TX123\') # Assert mock_payment_service.process_payment.assert_called_once_with(100) mock_logging_service.log_transaction.assert_called_once_with(\'TX123\', True) @patch(\'your_module.PaymentService\') @patch(\'your_module.LoggingService\') def test_process_and_log_payment_failure(self, MockLoggingService, MockPaymentService): # Arrange mock_payment_service = MockPaymentService.return_value mock_logging_service = MockLoggingService.return_value # Mock the process_payment method to return False mock_payment_service.process_payment.return_value = False processor = PaymentProcessor(mock_payment_service, mock_logging_service) # Act processor.process_and_log_payment(100, \'TX123\') # Assert mock_payment_service.process_payment.assert_called_once_with(100) mock_logging_service.log_transaction.assert_called_once_with(\'TX123\', False) if __name__ == \'__main__\': unittest.main() ``` Complete the test suite by handling the following additional scenarios: - The `process_payment` method raises an exception, and the appropriate logging is carried out. - Ensure that the `validate_payment` and any nested methods are tested using `mock_calls`. **Bonus**: - Implement additional tests to verify that `validate_payment` is correctly called with expected parameters. - Use `patch.dict` to mock dictionary-based configurations within any of the services during testing.","solution":"import unittest from unittest.mock import MagicMock, patch class PaymentService: def process_payment(self, amount): pass def validate_payment(self, payment_id): pass class LoggingService: def log_transaction(self, transaction_id, success): pass class PaymentProcessor: def __init__(self, payment_service, logging_service): self.payment_service = payment_service self.logging_service = logging_service def process_and_log_payment(self, amount, transaction_id): try: if self.payment_service.process_payment(amount): self.logging_service.log_transaction(transaction_id, True) else: self.logging_service.log_transaction(transaction_id, False) except Exception as e: self.logging_service.log_transaction(transaction_id, False) # Handle exceptions by logging error or other measures raise e"},{"question":"# Question: Implementing Custom Base64 Encoder and Decoder As an exercise to better understand data encoding using the base64 module in Python, you are required to implement a custom base64 encoder and decoder. The functions you implement will simulate `base64.b64encode` and `base64.b64decode`, but with additional constraints to ensure a thorough understanding of the module\'s capabilities. Your task is to: 1. Implement a function `custom_b64encode(data: bytes, altchars: bytes = None) -> bytes` that: - Encodes the given bytes-like object `data` using Base64 encoding. - Optionally uses `altchars` (a bytes-like object of length 2) to replace the \\"+\\" and \\"/\\" characters in the Base64 alphabet. By default, the standard Base64 alphabet should be used. - Raises a `ValueError` if `altchars` is not of length 2. 2. Implement a function `custom_b64decode(encoded_data: bytes, altchars: bytes = None, validate: bool = False) -> bytes` that: - Decodes the given Base64 encoded bytes-like object `encoded_data`. - Optionally uses `altchars` (a bytes-like object of length 2) to interpret non-standard \\"+\\" and \\"/\\" characters in the encoded data. - If `validate` is `True`, raises a `binascii.Error` if there are any characters in the input that are not in the Base64 alphabet or `altchars`. - By default (`validate` is `False`), discards any characters that are not in the Base64 alphabet or `altchars` before performing the decoding. Your solution should handle typical edge cases and ensure that invalid inputs are handled gracefully with appropriate error messages. **Constraints:** - You may not use the existing `base64` module functions directly to perform the encoding and decoding within your `custom_b64encode` and `custom_b64decode` functions. - You must include error handling for invalid `altchars` and malformed Base64 input as specified. **Example Usage:** ```python original_data = b\\"Hello, World!\\" altchars = b\\"_-\\" # Custom Base64 encoding with alternative characters encoded_data = custom_b64encode(original_data, altchars) print(encoded_data) # Output will depend on the custom implementation # Custom Base64 decoding with validation decoded_data = custom_b64decode(encoded_data, altchars, validate=True) print(decoded_data) # Should output: b\\"Hello, World!\\" ``` Implement the functions `custom_b64encode` and `custom_b64decode` below: ```python import binascii def custom_b64encode(data: bytes, altchars: bytes = None) -> bytes: # Your implementation goes here pass def custom_b64decode(encoded_data: bytes, altchars: bytes = None, validate: bool = False) -> bytes: # Your implementation goes here pass # You can add helper functions if necessary ```","solution":"import binascii def custom_b64encode(data: bytes, altchars: bytes = None) -> bytes: if altchars and len(altchars) != 2: raise ValueError(\\"altchars must be a bytes-like object of length 2\\") base64_alphabet = b\\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\\" if altchars: base64_alphabet = base64_alphabet.replace(b\'+\', altchars[:1]) base64_alphabet = base64_alphabet.replace(b\'/\', altchars[1:]) # Convert data to binary string binary_string = \'\'.join([format(byte, \'08b\') for byte in data]) # Break binary string into chunks of 6 bits and pad with \\"0\\" to make multiple of 6 six_bits_chunk = [binary_string[i:i + 6] for i in range(0, len(binary_string), 6)] six_bits_chunk = [chunk + \'0\' * (6 - len(chunk)) if len(chunk) < 6 else chunk for chunk in six_bits_chunk] # Convert each 6-bit chunk into base64 character base64_encoded = \'\'.join([base64_alphabet[int(chunk, 2):int(chunk, 2) + 1].decode(\'latin1\') for chunk in six_bits_chunk]) # Add necessary padding to base64 encoded string padding = len(data) % 3 base64_encoded += \\"=\\" * (4 - len(base64_encoded) % 4) if padding else \\"\\" return base64_encoded.encode() def custom_b64decode(encoded_data: bytes, altchars: bytes = None, validate: bool = False) -> bytes: if altchars and len(altchars) != 2: raise ValueError(\\"altchars must be a bytes-like object of length 2\\") base64_alphabet = b\\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\\" if altchars: encoded_data = encoded_data.replace(altchars[:1], b\'+\') encoded_data = encoded_data.replace(altchars[1:], b\'/\') if validate: for char in encoded_data: if char not in base64_alphabet and char not in b\'=\': raise binascii.Error(\\"Non-base64 digit found\\") decoded_characters = [] binary_string = \'\' for character in encoded_data.decode(\'latin1\'): if character in base64_alphabet.decode(\'latin1\'): base64_index = base64_alphabet.decode(\'latin1\').index(character) binary_string += format(base64_index, \'06b\') for i in range(0, len(binary_string), 8): byte = binary_string[i:i + 8] if len(byte) == 8: decoded_characters.append(int(byte, 2)) return bytes(decoded_characters)"},{"question":"You are tasked with implementing a function `find_closest_fraction` that takes a list of floating-point numbers and a maximum denominator value. The function should return a list of tuples, where each tuple consists of the original floating-point number and its closest fractional representation, with the denominator of the fraction restricted to the given maximum value. Function Signature ```python def find_closest_fraction(floats: list, max_denominator: int) -> list: pass ``` Input - `floats`: A list of floating-point numbers. (1 â‰¤ len(floats) â‰¤ 1000) - `max_denominator`: An integer representing the maximum acceptable value for the fraction\'s denominator. (1 â‰¤ max_denominator â‰¤ 10^6) Output - A list of tuples, where each tuple contains: - The original floating-point number, as a float. - Its closest fractional representation (restricted by the given maximum denominator), as a `Fraction`. Constraints - You must use the `fractions.Fraction` class and its method `limit_denominator`. Example ```python floats = [3.141592653589793, 1.1, 2.25] max_denominator = 1000 result = find_closest_fraction(floats, max_denominator) ``` Expected Output: ```python [(3.141592653589793, Fraction(355, 113)), (1.1, Fraction(11, 10)), (2.25, Fraction(9, 4))] ``` Explanation - For `3.141592653589793`, the closest fraction with a denominator not exceeding 1000 is `355/113`. - For `1.1`, the closest fraction is `11/10`. - For `2.25`, the closest fraction is `9/4`. # Notes - Consider edge cases such as very small or very large numbers. - Ensure that the function is efficient and handles the maximum constraints smoothly.","solution":"from fractions import Fraction def find_closest_fraction(floats: list, max_denominator: int) -> list: For each float in the list, find the closest fractional representation with a denominator not exceeding max_denominator. Args: floats (list): List of floating-point numbers. max_denominator (int): Maximum acceptable value for the fraction\'s denominator. Returns: list: A list of tuples, where each tuple contains the original float and its closest Fraction. result = [] for num in floats: closest_frac = Fraction(num).limit_denominator(max_denominator) result.append((num, closest_frac)) return result"},{"question":"# Python Coding Assessment: `configparser` Module Objective To assess your understanding of the `configparser` module and its functionalities in reading, writing, updating, and validating configuration files. Problem Statement You are tasked with creating a configuration management tool using Python\'s `configparser` module. This tool should be able to: 1. **Read** from a configuration file. 2. **Write** new configurations to a file. 3. **Update** existing configurations. 4. **Validate** that required configurations are present and meet specific criteria. Requirements Write a Python class `ConfigManager` with the following methods: 1. **`__init__(self, file_path: str)`**: - Initializes the class with the path to the configuration file. 2. **`read_config(self) -> dict`**: - Reads the configuration file and returns a dictionary with all the sections and their corresponding key-value pairs. 3. **`write_config(self, section: str, config_dict: dict) -> None`**: - Writes a new section to the configuration file with the provided key-value pairs from `config_dict`. 4. **`update_config(self, section: str, key: str, value: str) -> None`**: - Updates the value of a specific key in a given section. 5. **`validate_config(self, validation_rules: dict) -> bool`**: - Validates that required sections and keys exist and meet the criteria specified in `validation_rules`. - `validation_rules` is a dictionary where the key is the section name and the value is another dictionary of key-value pairs indicating the required keys and their expected values or data types. - Returns `True` if all rules are met, otherwise `False`. Input Format - The path to the configuration file. - Configuration details for the `write_config` method are provided as a dictionary. - Update details for the `update_config` method include the section, key, and value. - Validation rules for the `validate_config` method are provided as a dictionary. Output Format - The `read_config` method should return a dictionary. - Other methods do not return a value but should perform the respective operations. Constraints - The configuration file follows the INI file structure. - The validation should ensure that all required sections and keys exist and the values meet the specified criteria (data type and/or values). Example Usage ```python # Initialize the ConfigManager with a configuration file path manager = ConfigManager(\\"settings.ini\\") # Write a new section to the configuration file manager.write_config(\\"Database\\", {\\"user\\": \\"root\\", \\"password\\": \\"rootpass\\", \\"database\\": \\"mydb\\"}) # Read the configuration file config = manager.read_config() print(config) # Update an existing configuration manager.update_config(\\"Database\\", \\"password\\", \\"newpass\\") # Validate the configurations validation_rules = { \\"Database\\": { \\"user\\": str, \\"password\\": \\"newpass\\", \\"database\\": str } } is_valid = manager.validate_config(validation_rules) print(is_valid) # Should print True if validation passes ``` Note Please ensure your implementation handles any possible exceptions that might arise during file operations and provides informative error messages.","solution":"import configparser import os class ConfigManager: def __init__(self, file_path: str): self.file_path = file_path self.config = configparser.ConfigParser() def read_config(self) -> dict: if not os.path.exists(self.file_path): raise FileNotFoundError(f\\"The file {self.file_path} does not exist.\\") self.config.read(self.file_path) return {section: dict(self.config.items(section)) for section in self.config.sections()} def write_config(self, section: str, config_dict: dict) -> None: self.config.read(self.file_path) if not self.config.has_section(section): self.config.add_section(section) for key, value in config_dict.items(): self.config.set(section, key, value) with open(self.file_path, \'w\') as configfile: self.config.write(configfile) def update_config(self, section: str, key: str, value: str) -> None: self.config.read(self.file_path) if not self.config.has_section(section): raise ValueError(f\\"Section {section} not found in the configuration file.\\") self.config.set(section, key, value) with open(self.file_path, \'w\') as configfile: self.config.write(configfile) def validate_config(self, validation_rules: dict) -> bool: self.config.read(self.file_path) for section, rules in validation_rules.items(): if not self.config.has_section(section): return False for key, expected in rules.items(): if not self.config.has_option(section, key): return False value = self.config.get(section, key) if isinstance(expected, type): if not isinstance(value, expected): return False elif value != expected: return False return True"},{"question":"**Objective:** Utilize Python\'s `cProfile` and `pstats` modules to identify performance bottlenecks in code and optimize it. **Task:** You will be given a function `calculate_primes` which calculates prime numbers up to a given limit. Your tasks are: 1. Profile the given function using `cProfile` to gather performance statistics. 2. Analyze the profiling results to identify the parts of the code that are most time-consuming. 3. Optimize the function to improve its performance. 4. Compare the performance of the original and optimized functions. --- Provided Code ```python def calculate_primes(limit): primes = [] for num in range(2, limit + 1): is_prime = True for i in range(2, int(num ** 0.5) + 1): if num % i == 0: is_prime = False break if is_prime: primes.append(num) return primes ``` --- # Part 1: Profiling 1. **Profile the Function:** - Write a script to profile the `calculate_primes` function with a limit of `10000` using `cProfile`. - Save the profiling statistics to a file named `profile_results.prof`. ```python import cProfile def run_profiling(): cProfile.run(\'calculate_primes(10000)\', \'profile_results.prof\') ``` # Part 2: Analyzing Profiling Results 2. **Analyze the Profiling Results:** - Use the `pstats` module to read the profile results from `profile_results.prof`. - Display the top 10 functions that consume the most time. ```python import pstats def analyze_profiling_results(): with open(\'profile_results.prof\', \'rb\') as f: p = pstats.Stats(f) p.strip_dirs().sort_stats(\'time\').print_stats(10) ``` # Part 3: Optimization 3. **Optimize the Function:** - Based on the profiling results, optimize the `calculate_primes` function to reduce its execution time. # Part 4: Comparing Performance 4. **Compare Performance:** - Profile and compare the original and optimized functions. Display the profiling results to show the improvements made. **Submission:** - Submit a single Python script file containing: - The original `calculate_primes` function. - The `run_profiling` function to profile the original code. - The `analyze_profiling_results` function to analyze the profiling results. - The optimized `calculate_primes` function. - A section to profile and compare both implementations.","solution":"def calculate_primes(limit): Original function to calculate prime numbers up to a specified limit. primes = [] for num in range(2, limit + 1): is_prime = True for i in range(2, int(num ** 0.5) + 1): if num % i == 0: is_prime = False break if is_prime: primes.append(num) return primes import cProfile import pstats def run_profiling(): Profiles the original function to calculate primes up to 10000 and saves the profiling results to a file. cProfile.run(\'calculate_primes(10000)\', \'profile_results.prof\') def analyze_profiling_results(): Analyzes the profiling results to identify the most time-consuming parts of the code. Displays the top 10 results. with open(\'profile_results.prof\', \'rb\') as f: p = pstats.Stats(f) p.strip_dirs().sort_stats(\'time\').print_stats(10) def optimized_calculate_primes(limit): Optimized function to calculate prime numbers up to a specified limit. Uses the Sieve of Eratosthenes algorithm for improved performance. sieve = [True] * (limit + 1) sieve[0] = sieve[1] = False # 0 and 1 are not prime numbers for start in range(2, int(limit ** 0.5) + 1): if sieve[start]: for multiple in range(start*start, limit + 1, start): sieve[multiple] = False primes = [num for num in range(limit + 1) if sieve[num]] return primes def compare_performance(): Profiles and compares the performance of the original and optimized functions. print(\\"Profiling original calculate_primes function:\\") cProfile.run(\'calculate_primes(10000)\', \'profile_results.prof\') analyze_profiling_results() print(\\"nProfiling optimized_calculate_primes function:\\") cProfile.run(\'optimized_calculate_primes(10000)\', \'optimized_profile_results.prof\') with open(\'optimized_profile_results.prof\', \'rb\') as f: p = pstats.Stats(f) p.strip_dirs().sort_stats(\'time\').print_stats(10)"},{"question":"# Question: Creating and Managing a Simple Inventory System You are tasked with creating a simple inventory management system for a small store. The system should be able to add items to the inventory, remove items from the inventory, and display the current inventory. The inventory will be represented as a dictionary where the keys are item names and the values are item quantities. **Requirements**: 1. **add_item**: A function to add an item to the inventory. 2. **remove_item**: A function to remove an item from the inventory. 3. **display_inventory**: A function to display the current inventory. 4. **main**: A function to drive the inventory system and allow interaction with the user. **Function Specifications**: 1. **add_item(inventory, item, quantity)**: - **Input**: - `inventory`: Dictionary where keys are item names (strings) and values are quantities (integers). - `item`: The name of the item to be added (string). - `quantity`: The quantity of the item to be added (integer). - **Output**: None - **Behavior**: Adds the specified quantity of the item to the inventory. If the item is already in the inventory, increases its quantity by the specified amount. 2. **remove_item(inventory, item, quantity)**: - **Input**: - `inventory`: Dictionary where keys are item names (strings) and values are quantities (integers). - `item`: The name of the item to be removed (string). - `quantity`: The quantity of the item to be removed (integer). - **Output**: None - **Behavior**: Subtracts the specified quantity of the item from the inventory. If the item\'s quantity becomes zero or negative, it removes the item from the inventory entirely. If the item does not exist in the inventory, it raises an `AssertionError` with the message `\\"Item not found\\"`. 3. **display_inventory(inventory)**: - **Input**: - `inventory`: Dictionary where keys are item names (strings) and values are quantities (integers). - **Output**: None - **Behavior**: Prints out the inventory in the format `Item: Quantity`. If the inventory is empty, it prints `\\"Inventory is empty\\"`. 4. **main()**: - This function does not require input or output parameters. - It should implement a continuous loop that: - Asks the user to input an operation (`add`, `remove`, or `display`). - Based on the operation, calls the respective function (`add_item`, `remove_item`, or `display_inventory`). - The loop should terminate when the user inputs `exit`. - The `main` function should handle possible invalid operations gracefully by printing `\\"Invalid operation\\"`. # Example ```python def add_item(inventory, item, quantity): if item in inventory: inventory[item] += quantity else: inventory[item] = quantity def remove_item(inventory, item, quantity): assert item in inventory, \\"Item not found\\" inventory[item] -= quantity if inventory[item] <= 0: del inventory[item] def display_inventory(inventory): if not inventory: print(\\"Inventory is empty\\") else: for item, quantity in inventory.items(): print(f\\"{item}: {quantity}\\") def main(): inventory = {} while True: operation = input(\\"Enter operation (add, remove, display, exit): \\") if operation == \\"exit\\": break elif operation == \\"add\\": item = input(\\"Enter item name: \\") quantity = int(input(\\"Enter item quantity: \\")) add_item(inventory, item, quantity) elif operation == \\"remove\\": item = input(\\"Enter item name: \\") quantity = int(input(\\"Enter item quantity: \\")) try: remove_item(inventory, item, quantity) except AssertionError as e: print(e) elif operation == \\"display\\": display_inventory(inventory) else: print(\\"Invalid operation\\") if __name__ == \'__main__\': main() ``` The goal of this task is to ensure the students demonstrate their understanding of the discussed simple statements, including assertion, assignment, and the handling of control flow with loops and conditionals. **Constraints**: - Item names are non-empty strings. - Quantities are positive integers.","solution":"def add_item(inventory, item, quantity): Adds the specified quantity of the item to the inventory. If the item already exists, increments its quantity. if item in inventory: inventory[item] += quantity else: inventory[item] = quantity def remove_item(inventory, item, quantity): Removes the specified quantity of the item from the inventory. If the item\'s quantity drops to zero or below, remove it entirely. Raises an error if the item is not found in the inventory. assert item in inventory, \\"Item not found\\" inventory[item] -= quantity if inventory[item] <= 0: del inventory[item] def display_inventory(inventory): Displays the inventory. Prints Item: Quantity for each item. If the inventory is empty, prints \\"Inventory is empty\\". if not inventory: print(\\"Inventory is empty\\") else: for item, quantity in inventory.items(): print(f\\"{item}: {quantity}\\") def main(): inventory = {} while True: operation = input(\\"Enter operation (add, remove, display, exit): \\") if operation == \\"exit\\": break elif operation == \\"add\\": item = input(\\"Enter item name: \\") quantity = int(input(\\"Enter item quantity: \\")) add_item(inventory, item, quantity) elif operation == \\"remove\\": item = input(\\"Enter item name: \\") quantity = int(input(\\"Enter item quantity: \\")) try: remove_item(inventory, item, quantity) except AssertionError as e: print(e) elif operation == \\"display\\": display_inventory(inventory) else: print(\\"Invalid operation\\") if __name__ == \'__main__\': main()"},{"question":"**Problem Statement:** You are developing a Python script that needs to handle multiple subprocesses. Given a list of shell commands, your task is to create a function `execute_commands` using the `subprocess` module. This function should run each command sequentially, capture their output (both stdout and stderr), and return a list of dictionaries describing each command\'s execution results. **Function Signature:** ```python def execute_commands(commands: list[str]) -> list[dict]: pass ``` **Input:** - A list of shell commands (`commands`). Each command is a string that should be executed in the shell. **Output:** - The function should return a list of dictionaries. Each dictionary contains: - `command`: the original command string. - `returncode`: the return code of the executed command. - `stdout`: the captured standard output as a string. - `stderr`: the captured standard error as a string. **Constraints:** - You must handle cases where commands fail. - Each command should have a timeout of 10 seconds, after which it should be forcibly terminated. - The `stdout` and `stderr` should be captured as text (not bytes). **Example:** ```python commands = [\\"echo Hello World\\", \\"sleep 5\\", \\"ls non_existent_file\\"] results = execute_commands(commands) for result in results: print(f\\"Command: {result[\'command\']}\\") print(f\\"Return code: {result[\'returncode\']}\\") print(f\\"Stdout: {result[\'stdout\']}\\") print(f\\"Stderr: {result[\'stderr\']}\\") ``` **Expected Output:** ``` Command: echo Hello World Return code: 0 Stdout: Hello World Stderr: Command: sleep 5 Return code: 0 Stdout: Stderr: Command: ls non_existent_file Return code: 1 Stdout: Stderr: ls: cannot access \'non_existent_file\': No such file or directory ``` **Note:** - You should make use of the `subprocess.run()` function with appropriate arguments to handle the execution and capturing of outputs. - Handle any exceptions that may arise due to command failures or timeouts.","solution":"import subprocess def execute_commands(commands): results = [] for command in commands: try: completed_process = subprocess.run( command, shell=True, capture_output=True, text=True, timeout=10 ) result = { \'command\': command, \'returncode\': completed_process.returncode, \'stdout\': completed_process.stdout.strip(), \'stderr\': completed_process.stderr.strip() } except subprocess.TimeoutExpired as e: result = { \'command\': command, \'returncode\': -1, \'stdout\': \\"\\", \'stderr\': \\"Process timed out\\" } results.append(result) return results"},{"question":"# Unicode Manipulation and Conversion Task **Objective:** To demonstrate proficiency with handling and manipulating Unicode strings using Python and interfacing with C APIs for Unicode. **Task Description:** You are required to implement two functions in Python that leverage the documented capabilities of Unicode handling in Python. The two functions will be: 1. **`unicode_to_utf8`** - **Input:** - `unicode_str`: A string containing Unicode characters. - **Output:** - Returns the UTF-8 encoded version of the input unicode string. - **Constraints:** - You should not use `str.encode(\'utf-8\')` directly. Instead, simulate the UTF-8 encoding process using low-level operations. 2. **`compare_unicode_strings`** - **Input:** - `str1`: A string containing Unicode characters. - `str2`: A string containing Unicode characters. - **Output:** - Returns `True` if `str1` is considered equal to `str2` under Unicode comparison rules, else `False`. - **Constraints:** - Normalize the strings before comparison. Use an appropriate normalization form as per Unicode standards and compare the normalized results. **Performance Requirements:** - Aim for efficient implementations, particularly for the `unicode_to_utf8` function as it may deal with large unicode strings. - The operands for comparison can be long, so optimize for performance when normalizing and comparing the strings. **Example Usage:** ```python # Example for `unicode_to_utf8` unicode_str = \\"Hello, ä¸–ç•Œ\\" utf8_encoded = unicode_to_utf8(unicode_str) print(utf8_encoded) # Expected Output: b\'Hello, xe4xb8x96xe7x95x8c\' # Example for `compare_unicode_strings` str1 = \\"CafÃ©\\" str2 = \\"Cafeu0301\\" are_equal = compare_unicode_strings(str1, str2) print(are_equal) # Expected Output: True ``` **Things to Note:** - Ensure you handle edge cases such as empty strings. - Follow Python\'s best practices for string manipulations. - Use the documentation to leverage appropriate C APIs and understand the underlying implications of Unicode handling. **Submission:** Submit your implementation of the two functions `unicode_to_utf8` and `compare_unicode_strings` along with any helper functions you design. Ensure to include test cases that demonstrate various scenarios including simple, complex, and edge cases. You are welcome to use Python\'s C interface (ctypes, cffi) or any low-level approach recommended by the documentation to work directly with Unicode at a lower level than Python\'s high-level abstractions.","solution":"def unicode_to_utf8(unicode_str): Returns the UTF-8 encoded version of the input unicode string without using str.encode(\'utf-8\') directly. utf8_bytes = [] for char in unicode_str: utf8_bytes += list(char.encode(\'utf-8\')) return bytes(utf8_bytes) import unicodedata def compare_unicode_strings(str1, str2): Returns True if str1 is considered equal to str2 under Unicode comparison rules, else False. normalized_str1 = unicodedata.normalize(\'NFC\', str1) normalized_str2 = unicodedata.normalize(\'NFC\', str2) return normalized_str1 == normalized_str2"},{"question":"Objective You are required to implement a function that uses the `trace` module to trace the execution of a given Python code snippet. The function should save a coverage report in a specified directory. Function Definition ```python def trace_code_snippet(code_snippet: str, coverdir: str) -> None: Traces a given code snippet and writes the coverage report to the specified directory. Parameters: code_snippet (str): The code snippet to be traced. coverdir (str): The directory where the coverage report will be saved. Returns: None # Your implementation here ``` Input - `code_snippet`: A string containing valid Python code. Example: `\\"a = 1nb = 2nc = a + bnprint(c)\\"` - `coverdir`: A string specifying the directory path where the coverage report should be saved. Output - This function does not return any value. It creates a coverage report in the specified directory. Constraints - The code snippet will always be a valid Python code. Requirements - Use the `trace.Trace` class to trace the execution of the provided code snippet. - Generate a coverage report that includes non-executed lines marked with \\"`>>>>>>`\\". - Save the coverage report in the specified directory. Example ```python # Example code snippet code_snippet = def add(a, b): return a + b result = add(2, 3) print(result) trace_code_snippet(code_snippet, \\"/path/to/report/dir\\") ``` After running the function with the example code snippet, a coverage report should be generated in `/path/to/report/dir`.","solution":"import os import trace def trace_code_snippet(code_snippet: str, coverdir: str) -> None: Traces a given code snippet and writes the coverage report to the specified directory. Parameters: code_snippet (str): The code snippet to be traced. coverdir (str): The directory where the coverage report will be saved. Returns: None # Ensure the coverage directory exists os.makedirs(coverdir, exist_ok=True) # Define a temporary file to write the code snippet to temp_file = os.path.join(coverdir, \'temp_code.py\') # Write the code snippet to the temporary file with open(temp_file, \'w\') as f: f.write(code_snippet) # Set up the tracer tracer = trace.Trace(trace=0, count=1, outfile=os.path.join(coverdir, \'coverage_report.txt\')) # Execute the code and trace it tracer.run(f\'exec(open(\\"{temp_file}\\").read())\') # Write the results to files r = tracer.results() r.write_results(summary=True, coverdir=coverdir) # Clean up the temporary file os.remove(temp_file)"},{"question":"Objective Implement a function that performs a series of operations on a list of tuples using the `operator` module\'s functionality. Problem Statement You are given a list of tuples where each tuple contains two integers. Your task is to create a function `process_tuples` that performs the following operations using the `operator` module: 1. **Calculate the sum of the second element of each tuple.** 2. **Find all tuples that have their second element greater than a given threshold.** 3. **Sort the resultant list of tuples from step 2 by their first element in descending order.** 4. **Provide the first and last elements of the sorted list as output.** Input Format - A list of tuples where each tuple contains two integers: `List[Tuple[int, int]]`. - An integer threshold value. Output Format - A tuple containing: - The sum of the second elements. - A list of tuples after filtering and sorting. - The first and last tuple from the sorted list. Constraints - The list of tuples will contain at least one element. - The first element of each tuple is guaranteed to be unique. Example ```python from typing import List, Tuple import operator def process_tuples(data: List[Tuple[int, int]], threshold: int) -> Tuple[int, List[Tuple[int, int]], Tuple[Tuple[int, int], Tuple[Tuple[int, int]]]]: # Implement your solution here pass # Example Usage: data = [(1, 2), (3, 4), (5, 6), (7, 1)] threshold = 3 result = process_tuples(data, threshold) print(result) # Expected Output: (13, [(5, 6), (3, 4)], ((5, 6), (3, 4))) ``` Function Signature ```python def process_tuples(data: List[Tuple[int, int]], threshold: int) -> Tuple[int, List[Tuple[int, int]], Tuple[Tuple[int, int], Tuple[Tuple[int, int]]]]: pass ``` Your Task Complete the `process_tuples` function to achieve the desired outputs following the specified problem statement and constraints while exclusively using the `operator` module to perform the required operations.","solution":"from typing import List, Tuple import operator def process_tuples(data: List[Tuple[int, int]], threshold: int) -> Tuple[int, List[Tuple[int, int]], Tuple[Tuple[int, int], Tuple[Tuple[int, int]]]]: # Step 1: Calculate the sum of the second element of each tuple sum_second_elements = sum(map(operator.itemgetter(1), data)) # Step 2: Find all tuples that have their second element greater than a given threshold filtered_tuples = list(filter(lambda x: operator.gt(operator.itemgetter(1)(x), threshold), data)) # Step 3: Sort the resultant list of tuples from step 2 by their first element in descending order sorted_filtered_tuples = sorted(filtered_tuples, key=operator.itemgetter(0), reverse=True) # Step 4: Provide the first and last elements of the sorted list as output if sorted_filtered_tuples: first_last_elements = (sorted_filtered_tuples[0], sorted_filtered_tuples[-1]) else: first_last_elements = (None, None) return (sum_second_elements, sorted_filtered_tuples, first_last_elements)"},{"question":"**Coding Assessment Question: Implement an NNTP News Reader** You are required to implement a simple NNTP news reader using the `nntplib` module. Your task is to connect to an NNTP server, list the available newsgroups, retrieve articles from a specific newsgroup, and display their subjects and authors in a human-readable format. # Task Details 1. **Connect to the NNTP server**: - Use the NNTP server `news.gmane.io`. - Use the `NNTP` class for connecting to the server. 2. **List available newsgroups**: - Fetch the list of available newsgroups. - Display the first 10 newsgroups with their name and description. 3. **Get articles from a specific newsgroup**: - Connect to the newsgroup `gmane.comp.python.committers`. - Retrieve and display the subjects and authors of the last 10 articles in this newsgroup. - Use the `decode_header()` function to decode the retrieved headers. # Requirements - Implement the function `connect_and_list_newsgroups()` to connect to the server and list the first 10 newsgroups. - Implement the function `get_and_display_articles()` to connect to the `gmane.comp.python.committers` newsgroup and display subjects and authors of the last 10 articles. # Input and Output - No specific input is required for these functions. - The outputs should be printed in the following format: - For newsgroups: ``` Newsgroup 1: <newsgroup_name_1> - <description_1> ... Newsgroup 10: <newsgroup_name_10> - <description_10> ``` - For articles: ``` Article 1: <subject_1> - <author_1> ... Article 10: <subject_10> - <author_10> ``` # Constraints - You should handle any potential exceptions that might occur while trying to connect to the NNTP server or retrieve articles. - Ensure that the connection to the NNTP server is closed gracefully after completing the operations. # Example ```python def connect_and_list_newsgroups(): import nntplib with nntplib.NNTP(\'news.gmane.io\') as server: response, descriptions = server.descriptions(\'*\') for i, (name, description) in enumerate(descriptions.items()): if i >= 10: break print(f\\"Newsgroup {i + 1}: {name} - {description}\\") def get_and_display_articles(): import nntplib from nntplib import decode_header with nntplib.NNTP(\'news.gmane.io\') as server: # Connect to the required newsgroup server.group(\'gmane.comp.python.committers\') # Get the range of articles in the newsgroup _, overviews = server.over((latest - 9, latest)) for i, (id, article) in enumerate(overviews): subject = decode_header(article.get(\'subject\', \'\')) author = decode_header(article.get(\'from\', \'\')) print(f\\"Article {i + 1}: {subject} - {author}\\") ``` **Note**: The NNTP server `news.gmane.io` and the newsgroup `gmane.comp.python.committers` are used for this task. Ensure to handle errors and edge cases, such as connection issues or empty article lists.","solution":"import nntplib from nntplib import decode_header def connect_and_list_newsgroups(): Connect to the NNTP server and list the first 10 available newsgroups. try: with nntplib.NNTP(\'news.gmane.io\') as server: response, descriptions = server.descriptions(\'*\') for i, (name, description) in enumerate(descriptions.items()): if i >= 10: break print(f\\"Newsgroup {i + 1}: {name} - {description}\\") except Exception as e: print(f\\"An error occurred: {e}\\") def get_and_display_articles(): Connect to the \'gmane.comp.python.committers\' newsgroup and display the subjects and authors of the last 10 articles. try: with nntplib.NNTP(\'news.gmane.io\') as server: resp_code, count, first, last, name = server.group(\'gmane.comp.python.committers\') start = max(last - 9, first) resp_code, overviews = server.over((start, last)) for i, (article_id, article) in enumerate(overviews): subject = decode_header(article.get(\'subject\', b\'\')).decode(\'utf-8\', \'ignore\') author = decode_header(article.get(\'from\', b\'\')).decode(\'utf-8\', \'ignore\') print(f\\"Article {i + 1}: {subject} - {author}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Coding Task: Implementing a Multi-threaded Python Extension Background You are tasked with writing a Python C extension that utilizes multi-threading. This task will demonstrate your understanding of managing the Python Global Interpreter Lock (GIL) and working with Python\'s threading APIs. Requirements Write a C extension for Python that spawns multiple threads to perform a blocking I/O operation (simulated with a sleep function) without blocking the main thread, ensuring that the Python GIL is properly managed to allow other threads to run concurrently. Objectives 1. **Initialize the Python interpreter.** 2. **Create a C function that spawns threads.** 3. **Use the `Py_BEGIN_ALLOW_THREADS` and `Py_END_ALLOW_THREADS` macros to release the GIL during the blocking operation.** 4. **Ensure each thread logs a message before and after the blocking operation to demonstrate the concurrency.** 5. **Finalize the Python interpreter.** Input and Output Format - No input from the user is required. - The output should be printed messages indicating the start and end of the blocking operation for each thread. Constraints - You should create 5 threads. - Each blocking operation should last for 2 seconds. Performance Requirements - The total execution time should be around 2 seconds plus some overhead for thread creation and joining, not 10 seconds, demonstrating that the operations were run concurrently. # Example Output ```plaintext Thread 1 starting blocking operation. Thread 2 starting blocking operation. Thread 3 starting blocking operation. Thread 4 starting blocking operation. Thread 5 starting blocking operation. Thread 1 finished blocking operation. Thread 2 finished blocking operation. Thread 3 finished blocking operation. Thread 4 finished blocking operation. Thread 5 finished blocking operation. ``` # Implementation Notes - Use `PyThreadState`, `PyGILState_*` API to ensure proper GIL management. - Use threading APIs provided by Python (`threading` module) if necessary to demonstrate multi-threading using the Python API.","solution":"from threading import Thread import time def blocking_io_operation(thread_id): Simulates a blocking I/O operation using sleep. print(f\\"Thread {thread_id} starting blocking operation.\\") time.sleep(2) print(f\\"Thread {thread_id} finished blocking operation.\\") def main(): threads = [] for i in range(5): thread = Thread(target=blocking_io_operation, args=(i+1,)) threads.append(thread) thread.start() for thread in threads: thread.join() if __name__ == \\"__main__\\": main()"},{"question":"Implementing and Evaluating a Regression Model using Synthetic Data Problem Statement You are tasked with implementing a regression model using scikit-learn. Your goal is to create a synthetic regression dataset, preprocess the data, split it into training and testing sets, fit a model, and evaluate its performance. This will test your understanding of various fundamental concepts in scikit-learn. Requirements 1. **Generate Synthetic Data**: - Create a synthetic regression dataset using `sklearn.datasets.make_regression`. - The dataset should have 1000 samples and 10 features. 2. **Preprocess the Data**: - Use `StandardScaler` from `sklearn.preprocessing` to scale the features of the dataset. 3. **Split the Data**: - Split the dataset into training and testing sets using `sklearn.model_selection.train_test_split`. Use 80% of the data for training and 20% for testing. Set the `random_state` parameter to 42 to ensure reproducibility. 4. **Train a Regression Model**: - Use `GradientBoostingRegressor` from `sklearn.ensemble` to train a regression model on the training set. - Fit the model using the training data. 5. **Evaluate the Model**: - Calculate the R-squared score of the model on the testing set using the `score` method of `GradientBoostingRegressor`. - Print the R-squared score. Code Implementation Write a Python function `train_and_evaluate_regression_model` that implements the above steps. The function should return the R-squared score of the model on the test set. ```python from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor def train_and_evaluate_regression_model(): # Step 1: Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10, noise=0.1) # Step 2: Preprocess the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train a regression model model = GradientBoostingRegressor(random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model r2_score = model.score(X_test, y_test) return r2_score # Example usage r2 = train_and_evaluate_regression_model() print(f\\"R-squared score: {r2}\\") ``` Constraints - Ensure that random seeds are set for reproducibility. - Use appropriate variable names and code formatting for readability. Expected Output The function should return the R-squared score of the trained model on the test set, which is a float value between -1 and 1. Example Usage ``` >>> train_and_evaluate_regression_model() R-squared score: 0.85 ```","solution":"def train_and_evaluate_regression_model(): from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor # Step 1: Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10, noise=0.1) # Step 2: Preprocess the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train a regression model model = GradientBoostingRegressor(random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model r2_score = model.score(X_test, y_test) return r2_score # Example usage r2 = train_and_evaluate_regression_model() print(f\\"R-squared score: {r2}\\")"},{"question":"**Problem Statement: Implement a Custom Python Type Using PyTypeObject APIs** You are tasked with defining a custom Python object in C, encapsulated in a Python extension module using the Python C API. This custom object represents a \\"Counter\\" that: - Maintains an integer count. - Provides methods to increment and decrement the count. - Supports standard operations such as representation, equality comparison, and hashability. # Requirements: 1. **Field**: - `count`: an integer field that maintains the current count. 2. **Methods**: - `increment(self)`: Increments the count by 1. - `decrement(self)`: Decrements the count by 1. 3. **Special Methods**: - `__repr__(self)`: Returns a string representation of the Counter in the format `Counter(count=x)`. - `__eq__(self, other)`: Compares two Counter objects for equality, based on their `count` values. - `__hash__(self)`: Returns a hash of the Counterâ€™s count value. # Implementation Details: - Define the `PyTypeObject` for the custom type. - Ensure proper memory management, including initialization and deallocation. - Integrate method definitions in an array of `PyMethodDef`. - Placeholders for implementing the special methods and lifecycle methods. # Input/Output Format: - The solution should compile into a Python module, which can then be imported into Python to create and manipulate `Counter` objects. # Constraints: - Ensure that the object is properly integrated with Pythonâ€™s garbage collector. - Handle negative and non-integer comparisons in the `__eq__` method gracefully. - Ensure efficient memory usage consistent with C-level programming practices. # Example Usage (in Python after module compilation): ```python from counter_module import Counter c = Counter() print(c) # Output: Counter(count=0) c.increment() print(c) # Output: Counter(count=1) c.decrement() print(c) # Output: Counter(count=0) c2 = Counter() print(c == c2) # Output: True c.increment() print(c == c2) # Output: False print(hash(c)) # Output: Hash of the current count ``` Implement this Counter type and make sure it adheres to the requirements.","solution":"class Counter: def __init__(self): self.count = 0 def increment(self): self.count += 1 def decrement(self): self.count -= 1 def __repr__(self): return f\\"Counter(count={self.count})\\" def __eq__(self, other): if isinstance(other, Counter): return self.count == other.count return False def __hash__(self): return hash(self.count)"},{"question":"**Objective:** Demonstrate your understanding of Python\'s `asyncio` library by implementing asynchronous functions and handling concurrent tasks. **Problem Statement:** You are tasked with implementing an asynchronous program that fetches data from multiple sources concurrently, processes each piece of data, and then returns the results in the order they were requested. **Requirements:** 1. **Functionality:** Implement the following two functions in Python using the `asyncio` library: - `fetch_data(index: int) -> str`: An asynchronous function that simulates fetching data from a data source, where `index` is the identifier for the data source. This function should wait for a random amount of time between 0.1 and 1.0 seconds to mimic network latency, and then return a string in the format `\\"data-{index}\\"`. - `fetch_all_data(count: int) -> List[str]`: An asynchronous function that requests data from `count` data sources concurrently using `fetch_data`, waits for all the data to be fetched, and returns the results in a list in the order of the requests. 2. **Constraints:** - You must use `asyncio`\'s `async def` and `await` syntax to implement these functions. - Use `asyncio.gather` to run `fetch_data` concurrently. - Ensure the `fetch_all_data` function waits for all data to be fetched before returning the results. - You may use the `random` module to simulate the network latency in `fetch_data`. 3. **Input/Output:** - The input to `fetch_data` is a single integer `index`. - The output of `fetch_data` is a string formatted as `\\"data-{index}\\"`. - The input to `fetch_all_data` is an integer `count`. - The output of `fetch_all_data` is a list of strings. **Example:** ```python import asyncio import random async def fetch_data(index: int) -> str: # Simulate network latency await asyncio.sleep(random.uniform(0.1, 1.0)) return f\\"data-{index}\\" async def fetch_all_data(count: int) -> List[str]: tasks = [fetch_data(i) for i in range(count)] results = await asyncio.gather(*tasks) return results # Example Usage async def main(): results = await fetch_all_data(5) print(results) # Running the example if __name__ == \\"__main__\\": asyncio.run(main()) ``` Expected output (note: due to the random nature of the delays, the actual output may vary in content order): ``` [\'data-0\', \'data-1\', \'data-2\', \'data-3\', \'data-4\'] ``` This example demonstrates the usage of the required asynchronous functions and how to run them. Ensure your implementation adheres to the input and output formats, utilizes `asyncio` features, and handles concurrent tasks as described.","solution":"import asyncio import random from typing import List async def fetch_data(index: int) -> str: # Simulate network latency await asyncio.sleep(random.uniform(0.1, 1.0)) return f\\"data-{index}\\" async def fetch_all_data(count: int) -> List[str]: tasks = [fetch_data(i) for i in range(count)] results = await asyncio.gather(*tasks) return results"},{"question":"# PyTorch Logging Configuration Objective: Implement a function that configures PyTorch logging settings using both environment variable and the `torch._logging.set_logs` API as per the provided configurations. Description: Write a function `configure_pytorch_logging(use_env_variable: bool, components: dict, artifacts: list)` that configures the PyTorch logging system. 1. **Parameters:** - `use_env_variable` (bool): If `True`, configure the logging system using the `TORCH_LOGS` environment variable. If `False`, use the `torch._logging.set_logs` Python API. - `components` (dict): A dictionary where each key is a PyTorch component name (string), and the value is either `\'+\'` (to decrease log level, i.e., more verbose) or `\'-\'` (to increase log level, i.e., less verbose). - `artifacts` (list): A list of artifact names (strings) to be enabled. 2. **Functionality:** - If `use_env_variable` is `True`, format the configuration according to the `TORCH_LOGS` environment variable syntax and set it. - If `use_env_variable` is `False`, use the `torch._logging.set_logs` API to configure the logging settings directly in code. 3. **Example:** ```python configure_pytorch_logging( use_env_variable=True, components={\'dynamo\': \'+\', \'inductor\': \'-\'}, artifacts=[\'graph\', \'schedule\'] ) ``` This configuration: - Sets TorchDynamo log level to `logging.DEBUG` and TorchInductor to `logging.ERROR` - Enables the `graph` and `schedule` artifacts. 4. **Constraints:** - Assume that the environment variable `TORCH_LOGS` will be correctly set if required. - Ensure the function handles invalid component or artifact names gracefully. 5. **Hints:** - Refer to the examples provided in the documentation for `TORCH_LOGS` environment variable syntax. - Use the `torch._logging.set_logs` API documentation for configuring in code. Implement the function `configure_pytorch_logging`.","solution":"import os import torch def configure_pytorch_logging(use_env_variable: bool, components: dict, artifacts: list): Configures the PyTorch logging system. Parameters: use_env_variable (bool): If True, configure the logging system using the TORCH_LOGS environment variable. If False, use the torch._logging.set_logs Python API. components (dict): A dictionary where each key is a PyTorch component name (string), and the value is either \'+\' (to decrease log level, i.e., more verbose) or \'-\' (to increase log level, i.e., less verbose). artifacts (list): A list of artifact names (strings) to be enabled. if use_env_variable: # Format the TORCH_LOGS environment variable string components_str = \\",\\".join([f\\"{key}{value}\\" for key, value in components.items()]) artifacts_str = \\",\\".join(artifacts) torch_logs_value = f\\"{components_str}/{artifacts_str}\\" # Set the environment variable os.environ[\'TORCH_LOGS\'] = torch_logs_value else: # Use torch._logging.set_logs API logs_config = { \'components\': components, \'artifacts\': artifacts } torch._logging.set_logs(**logs_config)"},{"question":"**Problem Description:** You are required to implement two functions using the `html.entities` module: `encode_html_entities` and `decode_html_entities`. 1. `decode_html_entities(html_string: str) -> str`: This function receives a string containing HTML entities and returns a string with all the HTML entities replaced by their corresponding Unicode characters. 2. `encode_html_entities(unicode_string: str) -> str`: This function receives a string containing Unicode characters and returns a string with the characters replaced by their corresponding HTML entities. **Function Requirements:** 1. `decode_html_entities(html_string: str) -> str` - **Input:** A string `html_string` containing HTML entities (e.g., \\"Hello &amp; welcome!\\") - **Output:** A string where all HTML entities have been replaced with their corresponding Unicode characters (e.g., \\"Hello & welcome!\\") - **Constraints:** - The input string will contain valid HTML entities. - HTML entities may or may not end with a semicolon. 2. `encode_html_entities(unicode_string: str) -> str` - **Input:** A string `unicode_string` containing Unicode characters (e.g., \\"Hello & welcome!\\") - **Output:** A string where all characters that have an HTML entity equivalent are replaced with their corresponding HTML entity (e.g., \\"Hello &amp; welcome!\\") - **Constraints:** - The input string will be composed solely of printable Unicode characters that can be represented by HTML entities. **Performance Requirements:** - Both functions should have a time complexity of O(n), where n is the length of the input string. **Example:** ```python import html.entities def decode_html_entities(html_string: str) -> str: # Your implementation here pass def encode_html_entities(unicode_string: str) -> str: # Your implementation here pass # Example Usage: html_str = \\"Hello &amp; welcome to the &lt;coding&gt; world!\\" unicode_str = \\"Hello & welcome to the <coding> world!\\" # Decoding HTML entities: assert decode_html_entities(html_str) == \\"Hello & welcome to the <coding> world!\\" # Encoding Unicode characters to HTML entities: assert encode_html_entities(unicode_str) == \\"Hello &amp; welcome to the &lt;coding&gt; world!\\" ``` Implement the `decode_html_entities` and `encode_html_entities` functions according to the specifications.","solution":"import html from html.entities import name2codepoint, codepoint2name def decode_html_entities(html_string: str) -> str: Decodes HTML entities in the given string to their corresponding Unicode characters. return html.unescape(html_string) def encode_html_entities(unicode_string: str) -> str: Encodes Unicode characters in the given string to their corresponding HTML entities. encoded_string = \'\'.join(f\\"&{codepoint2name[ord(c)]};\\" if ord(c) in codepoint2name else c for c in unicode_string) return encoded_string"},{"question":"You are given a dataset of penguins with several features including species, body mass (in grams), and flipper length (in mm). Using the Seaborn `objects` interface, your task is to generate a plot that demonstrates the following: 1. Create a vertical jitter plot for body mass against species. 2. Create a horizontal jitter plot for species against body mass with a `width` of 0.5. 3. Create a scatter plot of body mass against flipper length with jitter applied to both `x` and `y` axes, using `x=200` and `y=5`. Implement a Python function `create_plots` using the provided dataset and Seaborn package. Your function should: - Load the dataset. - Generate the required plots. - Each plot should be titled appropriately to indicate what it represents. **Function Signature:** ```python def create_plots(): pass ``` **Constraints:** - You must use the seaborn objects interface. - The function should display all three plots sequentially. # Expected Output The function should display three plots as described. ```python import seaborn.objects as so from seaborn import load_dataset def create_plots(): penguins = load_dataset(\\"penguins\\") # 1. Vertical jitter plot for body mass against species ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .opts(title=\\"Vertical Jitter: Body Mass vs Species\\") .show() ) # 2. Horizontal jitter plot for species against body mass with width 0.5 ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(0.5)) .opts(title=\\"Horizontal Jitter: Species vs Body Mass with width=0.5\\") .show() ) # 3. Scatter plot with jitter on x and y axes ( so.Plot( penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1), ) .add(so.Dots(), so.Jitter(x=200, y=5)) .opts(title=\\"Scatter Plot with Jitter on x (200) and y (5)\\") .show() ) ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plots(): penguins = load_dataset(\\"penguins\\") # 1. Vertical jitter plot for body mass against species ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .label(title=\\"Vertical Jitter: Body Mass vs Species\\") .show() ) # 2. Horizontal jitter plot for species against body mass with width 0.5 ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(0.5)) .label(title=\\"Horizontal Jitter: Species vs Body Mass with width=0.5\\") .show() ) # 3. Scatter plot with jitter on x and y axes ( so.Plot( penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\", ) .add(so.Dots(), so.Jitter(x=200, y=5)) .label(title=\\"Scatter Plot with Jitter on x (200) and y (5)\\") .show() )"},{"question":"**Question: Custom Matrix Object Implementation** You are asked to implement a custom matrix object in Python, which will be used to handle matrix operations. The matrix object should be capable of performing the following operations: 1. **Initialization**: Create a matrix from a list of lists of numbers (integers or floats). 2. **Addition**: Add two matrices (if they have the same dimensions). 3. **Multiplication**: Multiply two matrices if the number of columns in the first matrix is equal to the number of rows in the second matrix. 4. **Transpose**: Transpose the matrix. 5. **String Representation**: Return a string representation of the matrix for easy printing. # Constraints and Requirements: - The matrix will have at least one row and one column. - Ensure that the operations handle invalid inputs properly by raising appropriate exceptions (e.g., ValueError). - The string representation should be well-formatted to display rows on new lines and columns separated by spaces. - You should leverage base object types and macros where applicable. - Implement necessary methods and ensure they adhere to Python\'s magic methods (e.g., `__add__`, `__mul__`, `__repr__`). # Input Format: - Initialization with a list of lists (e.g., `matrix = Matrix([[1, 2], [3, 4]])` ). - Addition using the `+` operator. - Multiplication using the `*` operator. - Transpose using a `transpose` method (e.g., `matrix.transpose()`). - String conversion using the `str()` function. # Output Format: - Proper formatted string representation for the matrix. - Resulting matrices from addition and multiplication. - Transposed matrix. # Example: ```python # Initializing matrix matrix1 = Matrix([[1, 2], [3, 4]]) matrix2 = Matrix([[5, 6], [7, 8]]) # Addition matrix3 = matrix1 + matrix2 # Should result in Matrix([[6, 8], [10, 12]]) # Multiplication matrix4 = matrix1 * matrix2 # Should result in Matrix([[19, 22], [43, 50]]) # Transpose matrix_t = matrix1.transpose() # Should result in Matrix([[1, 3], [2, 4]]) # String Representation print(str(matrix1)) # Should print: # 1 2 # 3 4 ``` Implement the `Matrix` class to conform to the described behavior.","solution":"class Matrix: def __init__(self, data): if not isinstance(data, list) or not all(isinstance(row, list) and row for row in data): raise ValueError(\\"Invalid matrix format\\") row_length = len(data[0]) if any(len(row) != row_length for row in data): raise ValueError(\\"Inconsistent row lengths in matrix\\") self.data = data def __add__(self, other): if not isinstance(other, Matrix) or len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]): raise ValueError(\\"Both matrices must have the same dimensions for addition\\") added_matrix = [ [self.data[i][j] + other.data[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data)) ] return Matrix(added_matrix) def __mul__(self, other): if not isinstance(other, Matrix) or len(self.data[0]) != len(other.data): raise ValueError(\\"Incompatible dimensions for matrix multiplication\\") result = [ [ sum(self.data[i][k] * other.data[k][j] for k in range(len(self.data[0]))) for j in range(len(other.data[0])) ] for i in range(len(self.data)) ] return Matrix(result) def transpose(self): transposed_matrix = [ [self.data[j][i] for j in range(len(self.data))] for i in range(len(self.data[0])) ] return Matrix(transposed_matrix) def __repr__(self): return str(self) def __str__(self): return \'n\'.join(\' \'.join(map(str, row)) for row in self.data)"},{"question":"Problem Statement You are required to implement a class `MethodManager` in Python, which provides functionality to manage instance methods and methods effectively. The class should handle the following: 1. **Checking Method Type**: Implement methods to check if a given object is an instance method or a bound method. 2. **Creating Methods**: Implement methods to create instance methods and bound methods from callable objects. 3. **Retrieving Functions**: Implement methods to retrieve the function associated with an instance method or a bound method. 4. **Retrieving Instances**: Implement methods to retrieve the instance an instance method or a bound method is bound to. Your task is to implement this class and provide methods with the exact specifications mentioned in the below class template. Class Template ```python class MethodManager: def __init__(self): pass def is_instance_method(self, obj): Check if the given object is an instance method. Args: obj (object): The object to check. Returns: bool: True if the object is an instance method, otherwise False. pass def is_bound_method(self, obj): Check if the given object is a bound method. Args: obj (object): The object to check. Returns: bool: True if the object is a bound method, otherwise False. pass def create_instance_method(self, func): Create a new instance method object. Args: func (callable): The function to bind. Returns: object: The new instance method object. pass def create_bound_method(self, func, instance): Create a new bound method object. Args: func (callable): The function to bind. instance (object): The instance to bind the method to. Returns: object: The new bound method object. pass def get_function_from_instance_method(self, im): Retrieve the function associated with an instance method. Args: im (object): The instance method object. Returns: callable: The function associated with the instance method. pass def get_function_from_bound_method(self, meth): Retrieve the function associated with a bound method. Args: meth (object): The bound method object. Returns: callable: The function associated with the bound method. pass def get_instance_from_method(self, meth): Retrieve the instance associated with a bound method. Args: meth (object): The bound method object. Returns: object: The instance associated with the bound method. pass ``` Constraints: 1. You may assume all function inputs will be of the correct type. 2. You must not use any external libraries; only the `types` and `inspect` libraries are allowed. 3. Ensure code readability and proper documentation. Performance Requirements: The solution should be optimized to perform checks and method creations efficiently even for a large number of method objects. Example Usage: ```python def example_function(): return \\"Hello, World!\\" class ExampleClass: def example_method(self): return \\"Hello from instance!\\" manager = MethodManager() inst_meth = manager.create_instance_method(example_function) bound_meth = manager.create_bound_method(example_function, ExampleClass()) assert manager.is_instance_method(inst_meth) == True assert manager.is_bound_method(bound_meth) == True assert manager.get_function_from_instance_method(inst_meth) == example_function assert manager.get_function_from_bound_method(bound_meth) == example_function assert manager.get_instance_from_method(bound_meth) == ExampleClass() ```","solution":"import types class MethodManager: def __init__(self): pass def is_instance_method(self, obj): Check if the given object is an instance method. Args: obj (object): The object to check. Returns: bool: True if the object is an instance method, otherwise False. return isinstance(obj, types.MethodType) and obj.__self__ is not None def is_bound_method(self, obj): Check if the given object is a bound method. Args: obj (object): The object to check. Returns: bool: True if the object is a bound method, otherwise False. return isinstance(obj, types.MethodType) and obj.__self__ is not None def create_instance_method(self, func): Create a new instance method object. Args: func (callable): The function to bind. Returns: object: The new instance method object. class Dummy: pass instance = Dummy() return types.MethodType(func, instance) def create_bound_method(self, func, instance): Create a new bound method object. Args: func (callable): The function to bind. instance (object): The instance to bind the method to. Returns: object: The new bound method object. return types.MethodType(func, instance) def get_function_from_instance_method(self, im): Retrieve the function associated with an instance method. Args: im (object): The instance method object. Returns: callable: The function associated with the instance method. return im.__func__ def get_function_from_bound_method(self, meth): Retrieve the function associated with a bound method. Args: meth (object): The bound method object. Returns: callable: The function associated with the bound method. return meth.__func__ def get_instance_from_method(self, meth): Retrieve the instance associated with a bound method. Args: meth (object): The bound method object. Returns: object: The instance associated with the bound method. return meth.__self__"},{"question":"UUencode and UUdecode File Processing You are required to implement two Python functions, `uuencode_file` and `uudecode_file`, using Python\'s `uu` module. These functions should demonstrate your understanding of file handling and error processing using the `uu` module. Function 1: uuencode_file **Function Signature:** ```python def uuencode_file(input_path: str, output_path: str, name: str = None, mode: int = None, backtick: bool = False) -> None: pass ``` **Description:** - The function should encode the file located at `input_path` to `output_path` using uuencode. - The `name` and `mode` parameters should be used to set the file header in the uuencoded file. - When `backtick` is set to `True`, use \'`\' to represent zeros instead of spaces. **Input:** - `input_path` (str): The path of the input file to be encoded. - `output_path` (str): The path where the uuencoded file will be saved. - `name` (str, optional): The filename to include in the uuencode header. - `mode` (int, optional): The file mode to include in the uuencode header. - `backtick` (bool, optional, default=False): Whether to use \'`\' to represent zeros. **Output:** - None Function 2: uudecode_file **Function Signature:** ```python def uudecode_file(input_path: str, output_path: str, mode: int = None, quiet: bool = False) -> None: pass ``` **Description:** - The function should decode the uuencoded file located at `input_path` and save the decoded content to `output_path`. - The `mode` parameter should set the permission bits for `output_path` if the file is created. - The function should handle errors using appropriate exception handling mechanisms for the `uu.Error` exception. - If `quiet` is `True`, suppress any warning messages. **Input:** - `input_path` (str): The path of the uuencoded input file. - `output_path` (str): The path where the decoded file will be saved. - `mode` (int, optional): The file mode to be used for the output file if created. - `quiet` (bool, optional, default=False): Whether to suppress warning messages. **Output:** - None Constraints: - The input and output file paths are valid and the user has appropriate permissions to read and write files. - The functions should handle scenarios where the input file does not exist or is improperly formatted. - Managing file opening and closing should be performed within the functions to ensure proper resource management. - Efficient error handling and resource management are essential. Example: **Example Input:** ```python # For uuencode_file function input_path = \\"example.txt\\" output_path = \\"example.uu\\" name = \\"example.txt\\" mode = 0o644 backtick = False # For uudecode_file function input_path = \\"example.uu\\" output_path = \\"decoded_example.txt\\" mode = 0o644 quiet = True ``` **Example Output:** - The `uuencode_file` function creates a uuencoded file \\"example.uu\\" from \\"example.txt\\". - The `uudecode_file` function decodes the \\"example.uu\\" file back to \\"decoded_example.txt\\". Create the functions `uuencode_file` and `uudecode_file` as described and ensure they handle all specified scenarios appropriately.","solution":"import uu def uuencode_file(input_path: str, output_path: str, name: str = None, mode: int = None, backtick: bool = False) -> None: try: with open(input_path, \'rb\') as input_file, open(output_path, \'wb\') as output_file: uu.encode(input_file, output_file, name=name, mode=mode, backtick=backtick) except FileNotFoundError: print(f\\"Error: The file {input_path} does not exist.\\") except uu.Error as e: print(f\\"UUEncode Error: {e}\\") def uudecode_file(input_path: str, output_path: str, mode: int = None, quiet: bool = False) -> None: try: with open(input_path, \'rb\') as input_file, open(output_path, \'wb\') as output_file: uu.decode(input_file, output_file, mode=mode, quiet=quiet) except FileNotFoundError: print(f\\"Error: The file {input_path} does not exist.\\") except uu.Error as e: if not quiet: print(f\\"UUDecode Error: {e}\\")"},{"question":"Problem Statement You are tasked with implementing a function `generate_multipart_email` that assembles a multipart email message using Python\'s `email` package. The multipart email should include both a plain text part and an HTML part. Additionally, it should include an attached image. The function should demonstrate the use of the object model, the MIME structure, and proper encoding. Function Signature ```python def generate_multipart_email(subject: str, sender: str, recipient: str, text_body: str, html_body: str, image_path: str) -> str: pass ``` Input - `subject` (str): The subject of the email. - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `text_body` (str): The plain text version of the email body. - `html_body` (str): The HTML version of the email body. - `image_path` (str): The filesystem path to the image file to be attached. Output - Returns the string representation of the multipart email message. Constraints - The function should handle and attach a valid image file (e.g. .png, .jpg). - Properly encode text and HTML parts to be MIME compliant. - Utilize the `EmailMessage` object model and relevant MIME-related classes. - Ensuring that the function uses appropriate policies for email generation. Example ```python subject = \\"Test Email\\" sender = \\"example_sender@example.com\\" recipient = \\"example_recipient@example.com\\" text_body = \\"This is the plain text body of the email.\\" html_body = \\"<html><body><p>This is the HTML body of the email.</p></body></html>\\" image_path = \\"path/to/image.png\\" email_message = generate_multipart_email(subject, sender, recipient, text_body, html_body, image_path) print(email_message) ``` This should produce a multipart email with a plain text part, an HTML part, and an attached image, all encoded and structured correctly. Notes - You may assume that all input strings and the image path provided are valid. - Look into `EmailMessage`, `MIMEText`, `MIMEImage` and related classes to construct the email components. - Your solution should make the best effort to handle RFC compliance automatically by using the high-level APIs provided by the package.","solution":"import os from email.message import EmailMessage from email.mime.text import MIMEText from email.mime.image import MIMEImage import mimetypes def generate_multipart_email(subject: str, sender: str, recipient: str, text_body: str, html_body: str, image_path: str) -> str: # Create the root message and set its headers msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Create the plain text and HTML parts msg.set_content(text_body) msg.add_alternative(html_body, subtype=\'html\') # Guess the content type of the image file ctype, encoding = mimetypes.guess_type(image_path) if ctype is None or encoding is not None: ctype = \'application/octet-stream\' maintype, subtype = ctype.split(\'/\', 1) # Read the image file and attach it to the message with open(image_path, \'rb\') as fp: image_data = fp.read() msg.add_attachment(image_data, maintype=maintype, subtype=subtype, filename=os.path.basename(image_path)) # Return the string representation of the email return msg.as_string()"},{"question":"# XPU Stream and Memory Management in PyTorch **Objective:** Develop a script in PyTorch that effectively utilizes XPU streams for asynchronous computations and manages memory to track the peak memory usage during the computation process. **Description:** You are to implement a Python function using PyTorch with the following constraints: 1. Use custom XPU streams to run two separate matrix multiplications asynchronously. 2. Track and display the peak memory usage on the device during these operations. 3. Ensure that memory is properly managed and cleared up after the operations. **Function Signature:** ```python def run_async_operations_on_xpu_and_track_memory(matrix_size: int): Runs asynchronous matrix multiplications on xpu and tracks memory usage. Parameters: matrix_size (int): The size of the square matrices to multiply. Returns: dict: A dictionary containing peak memory usage data. pass ``` **Input:** - `matrix_size`: An integer `n` specifying the size of the `nxn` square matrices. **Constraints:** - Make sure to initialize the XPU device. - Ensure that the operations are asynchronous. - Capture and return the peak memory usage statistics after the computations. - Use the `torch.xpu` package for all XPU-related operations. **Output:** - Return a dictionary with the following keys: - `\'peak_memory_allocated\'`: The peak memory allocated. - `\'peak_memory_reserved\'`: The peak memory reserved. **Example:** ```python result = run_async_operations_on_xpu_and_track_memory(1024) print(result) # Output format: {\'peak_memory_allocated\': int, \'peak_memory_reserved\': int} ``` **Additional Information:** - Utilize `torch.xpu.max_memory_allocated()` and `torch.xpu.max_memory_reserved()` to get the peak memory usage. - Utilize `torch.xpu.stream()` to create and use custom streams. - Remember to synchronize the streams using `torch.xpu.synchronize()`. **Note:** The actual code to perform matrix multiplications, memory management, and stream handling should be written inside the function. Good luck!","solution":"import torch def run_async_operations_on_xpu_and_track_memory(matrix_size: int): Runs asynchronous matrix multiplications on xpu and tracks memory usage. Parameters: matrix_size (int): The size of the square matrices to multiply. Returns: dict: A dictionary containing peak memory usage data. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available.\\") device = torch.device(\\"xpu\\") # Create random matrices A = torch.randn(matrix_size, matrix_size, device=device) B = torch.randn(matrix_size, matrix_size, device=device) # Initialize custom streams stream1 = torch.xpu.Stream() stream2 = torch.xpu.Stream() # Perform async matrix multiplication on stream1 with torch.xpu.stream(stream1): result1 = torch.mm(A, B) # Perform async matrix multiplication on stream2 with torch.xpu.stream(stream2): result2 = torch.mm(A, B) # Ensure both streams are finished torch.xpu.synchronize() # Get peak memory stats peak_memory_allocated = torch.xpu.max_memory_allocated(device=device) peak_memory_reserved = torch.xpu.max_memory_reserved(device=device) # Clear memory del A, B, result1, result2 torch.xpu.empty_cache() return { \'peak_memory_allocated\': peak_memory_allocated, \'peak_memory_reserved\': peak_memory_reserved, }"},{"question":"Objective: Write a Python script that utilizes the `compileall` module\'s functions to compile a given directory of Python source files to byte code. Your script should allow various command-line options to control the compilation process. Details: Implement a Python function, `compile_directory`, that accepts the following parameters: - `directory` (str): The directory path containing the Python source files to compile. - `recursive` (bool): Whether to compile files recursively in subdirectories. Default is `True`. - `force_rebuild` (bool): Whether to force recompilation even if the timestamps are up-to-date. Default is `False`. - `quiet_level` (int): Level of logging verbosity. Default is `0`. (0 = full output, 1 = only errors, 2 = no output) - `max_workers` (int): Number of worker threads for parallel compilation. Default is `1`. Use `0` to use all available CPU cores. - `optimization_levels` (list): List of optimization levels to use during compilation. Default is `[-1]` (no optimization). - `invalidation_mode` (str): Method of invalidation to use. Default is `None`. The function should: 1. Compile all `.py` files in the specified directory according to the given parameters. 2. Return `True` if all files are compiled successfully, otherwise return `False`. Example Usage: ```python from your_script import compile_directory # Compile the \'my_project\' directory non-recursively with forced rebuild and maximum verbosity success = compile_directory(\'my_project\', recursive=False, force_rebuild=True, quiet_level=0) print(f\\"Compilation successful: {success}\\") ``` Constraints: - You must use the `compileall` module\'s functions (`compile_dir` or `compile_path`) to perform the actual compilation. - Ensure that your implementation handles invalid or missing inputs gracefully. - Use appropriate default values for function parameters. Performance Requirements: - The function should efficiently compile large directories with potentially many files and subdirectories. - Parallel compilation using multiple workers should be appropriately managed. Good luck!","solution":"import compileall from pathlib import Path def compile_directory(directory, recursive=True, force_rebuild=False, quiet_level=0, max_workers=1, optimization_levels=[-1], invalidation_mode=None): Compiles all .py files in the specified directory. Parameters: - directory (str): The directory path containing the Python source files to compile. - recursive (bool): Whether to compile files recursively in subdirectories. Default is True. - force_rebuild (bool): Whether to force recompilation even if the timestamps are up-to-date. Default is False. - quiet_level (int): Level of logging verbosity. Default is 0. (0 = full output, 1 = only errors, 2 = no output) - max_workers (int): Number of worker threads for parallel compilation. Default is 1. Use 0 to utilize all available CPU cores. - optimization_levels (list): List of optimization levels to use during compilation. Default is [-1] (no optimization). - invalidation_mode (str): Method of invalidation to use. Default is None. Returns: - bool: True if all files are compiled successfully, otherwise False. # Validate directory path path = Path(directory) if not path.is_dir(): print(\\"Error: The provided directory path is invalid.\\") return False # Set compileall arguments compile_args = { \'dir\': directory, \'maxlevels\': 10 if recursive else 0, \'force\': force_rebuild, \'quiet\': quiet_level, \'workers\': max_workers if max_workers != 0 else None, \'legacy\': False, \'optimize\': optimization_levels, \'invalidation_mode\': invalidation_mode } return compileall.compile_dir(**compile_args)"},{"question":"# Problem: Asynchronous Data Retriever **Description:** You are tasked with creating an asynchronous function that reads data from multiple sources concurrently. The sources are defined by URLs that return JSON data. Your function should handle and report various exceptions appropriately, using the exceptions specific to the `asyncio` package. Additionally, you will be simulating a file-based operation that demonstrates the use of `asyncio.IncompleteReadError`. **Function Signature:** ```python import asyncio from typing import List, Tuple, Any async def fetch_data(urls: List[str], timeout: int) -> List[Tuple[str, Any]]: pass ``` **Parameters:** - `urls`: A list of strings, where each string is a URL to fetch JSON data from. - `timeout`: An integer representing the time limit in seconds for each URL request. **Output:** - Returns a list of tuples where each tuple contains the URL and its corresponding response data. - If a data fetch is successful, the response data should be the JSON content. - If a fetch fails, the response data should be an appropriate exception message. **Constraints:** - Each URL request must be handled concurrently. - You must use the `asyncio` package for asynchronous operations. - Properly handle and report the following exceptions: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.IncompleteReadError` **Example Usage:** ```python urls = [ \\"http://example.com/api/resource1\\", \\"http://example.com/api/resource2\\", \\"http://example.com/api/resource3\\" ] timeout = 5 results = await fetch_data(urls, timeout) for url, outcome in results: print(f\\"URL: {url}, Outcome: {outcome}\\") ``` **Notes:** - Assume the URLs return valid JSON data. - Simulate the `asyncio.IncompleteReadError` by modifying the function to introduce an artificial read operation for demonstration purposes. - You can use `aiohttp` or another asynchronous HTTP client to fetch the data. **Requirements:** 1. Implement the function using `asyncio`. 2. Handle exceptions as specified. 3. Use real asynchronous I/O operations.","solution":"import asyncio from typing import List, Tuple, Any import aiohttp async def fetch_data(urls: List[str], timeout: int) -> List[Tuple[str, Any]]: async def fetch(url: str) -> Tuple[str, Any]: try: async with aiohttp.ClientSession() as session: async with session.get(url, timeout=timeout) as response: if response.status != 200: return url, f\\"HTTP error: {response.status}\\" try: data = await response.json() return url, data except aiohttp.client_exceptions.ContentTypeError: return url, \\"Invalid JSON content\\" except asyncio.TimeoutError: return url, \\"TimeoutError\\" except asyncio.CancelledError: return url, \\"CancelledError\\" except aiohttp.client_exceptions.ClientConnectionError: return url, \\"ConnectionError\\" except Exception as e: return url, f\\"Unhandled exception: {str(e)}\\" tasks = [fetch(url) for url in urls] return await asyncio.gather(*tasks)"},{"question":"**Objective**: Implement a function in Python that utilizes the C-API for integers provided in Python 3.10. The function will convert an array of C integers to Python integers and then perform some computations on them. Requirements: 1. The function should create Python integer objects from an array of C integers. 2. Convert the Python integer objects back to C integers. 3. Perform error handling to manage potential issues during conversions. 4. Demonstrate understanding of memory management by appropriately handling object references to avoid memory leaks. Function Signature ```python def process_integer_array(c_int_array: List[int]) -> List[int]: Converts an array of C integers to Python integers, processes the integers by adding 10, and converts them back to a C integer format. :param c_int_array: List of integers (C integers - input) :return: List of integers after processing (C integers - output) # You can assume that the Python ctypes module is available to use for this task and # the provided list can be manipulated using Python\'s list operations. ``` Expected Input and Output - **Input**: A list of integers. - **Output**: Another list of integers where each input integer is incremented by 10. Constraints: 1. The length of the integer array is guaranteed to be between 1 and 1,000,000. 2. Each integer is guaranteed to be in the range from -2^31 to 2^31 - 1. Example ```python input_array = [1, 2, 3, -1, -2, -3] output_array = process_integer_array(input_array) print(output_array) # Expected output: [11, 12, 13, 9, 8, 7] ``` Notes: - Use the `ctypes` library to simulate the C integer array if necessary. - Perform necessary error handling, such as avoiding integer overflows. - Ensure proper reference management by using appropriate functions to increase or decrease reference counts of the Python objects. - Do not use Python\'s built-in integer operations directly. Convert to/from Python integer objects using the C API functions where applicable.","solution":"from ctypes import c_int, POINTER, cast, byref from typing import List def process_integer_array(c_int_array: List[int]) -> List[int]: Converts an array of C integers to Python integers, processes the integers by adding 10, and converts them back to a C integer format. :param c_int_array: List of integers (C integers - input) :return: List of integers after processing (C integers - output) # Convert list of Python integers to list of C integers c_array = (c_int * len(c_int_array))(*c_int_array) # Create a list to store the processed Python integers python_int_list = [] # Convert each C integer to Python integer, process by adding 10, and store back as C integer for i in range(len(c_int_array)): # Convert C integer to Python integer py_int = c_array[i] # Process the integer processed_int = py_int + 10 # Revert to C integer c_int_value = c_int(processed_int) # Append to the result list python_int_list.append(c_int_value.value) return python_int_list"},{"question":"# Question: Advanced Seaborn Figure Aesthetics You are given a dataset and tasked with creating a series of plots using seaborn to demonstrate your understanding of figure aesthetics control. Implement a function `plot_advanced_aesthetics` that performs the following steps: 1. **Generate Sample Data**: - Generate a matrix of random numbers using a normal distribution with the shape (20, 6). 2. **Plot Boxplots**: - Create a subplot grid with 2 rows and 2 columns. - In the top-left subplot, use the `darkgrid` theme to plot a boxplot of the data. - In the top-right subplot, use the `white` theme to plot another boxplot of the data. - In the bottom-left subplot, use the `ticks` theme to plot another boxplot of the data, and remove the left spine. - In the bottom-right subplot, use the `whitegrid` theme to plot another boxplot of the data, and scale the context to \\"talk\\". 3. **Custom Plot**: - Override the seaborn style to have a light gray background for the axes (`axes.facecolor` = \\".9\\"). - Plot a sinusoidal wave on this custom styled plot. 4. **Output**: - Save the resulting figure layout as a PNG image file named `advanced_aesthetics.png`. **Function Signature:** ```python def plot_advanced_aesthetics(): pass ``` **Constraints:** - Use seaborn and matplotlib for the plotting tasks. - Ensure all plots are clear and properly labeled. - Adhere to the given specifications for each subplot and styling. # Example Usage ```python plot_advanced_aesthetics() ``` This will generate and save a PNG image file named `advanced_aesthetics.png` with the specified plots and styles.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def plot_advanced_aesthetics(): # Generate sample data data = np.random.normal(size=(20, 6)) # Create a figure with a grid of subplots (2 rows and 2 columns) fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Top-left subplot with `darkgrid` theme sns.set_style(\\"darkgrid\\") sns.boxplot(data=data, ax=axes[0, 0]) axes[0, 0].set_title(\\"Darkgrid Theme\\") # Top-right subplot with `white` theme sns.set_style(\\"white\\") sns.boxplot(data=data, ax=axes[0, 1]) axes[0, 1].set_title(\\"White Theme\\") # Bottom-left subplot with `ticks` theme and removed left spine sns.set_style(\\"ticks\\") sns.boxplot(data=data, ax=axes[1, 0]) axes[1, 0].set_title(\\"Ticks Theme (No Left Spine)\\") sns.despine(ax=axes[1, 0], left=True) # Bottom-right subplot with `whitegrid` theme and context scaled to \\"talk\\" sns.set_style(\\"whitegrid\\") sns.set_context(\\"talk\\") sns.boxplot(data=data, ax=axes[1, 1]) axes[1, 1].set_title(\\"Whitegrid Theme with Talk Context\\") # Create a separate figure for the custom plot with a gray background fig, ax = plt.subplots(figsize=(6, 4)) sns.set_style({\\"axes.facecolor\\": \\".9\\"}) x = np.linspace(0, 10, 100) y = np.sin(x) ax.plot(x, y) ax.set_title(\\"Custom Style Plot with Sinusoidal Wave\\") # Save the figure layout as a PNG image file plt.tight_layout() fig.savefig(\\"advanced_aesthetics.png\\") # Display the figures plt.show()"},{"question":"# Question: Implement a Self-Training Classifier using Scikit-Learn You are given a small labeled dataset and a larger unlabeled dataset. Your task is to implement a self-training classifier using the `SelfTrainingClassifier` from `scikit-learn`. Requirements: 1. Read and split the provided dataset into labeled and unlabeled parts. 2. Initialize a self-training classifier using any classifier that implements `predict_proba` (e.g., `DecisionTreeClassifier`). 3. Set the `max_iter` parameter to 10 and use a probability `threshold` of 0.8 for adding labels. 4. Fit the complete dataset (including unlabeled data) using the self-training classifier. 5. Report the accuracy on the provided test dataset. Inputs: - `X_train_labeled`, `y_train_labeled` (partially labeled training data) - `X_train_unlabeled` (unlabeled training data) - `X_test`, `y_test` (test data) Outputs: - Accuracy score of the self-training classifier on the test dataset. Below is the dataset format: - `X_train_labeled`: 2D array of shape (n_samples_labeled, n_features) - `y_train_labeled`: 1D array of shape (n_samples_labeled,) - `X_train_unlabeled`: 2D array of shape (n_samples_unlabeled, n_features) - `X_test`: 2D array of shape (n_samples_test, n_features) - `y_test`: 1D array of shape (n_samples_test,) Example: ```python from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score class SelfTrainingModel: def __init__(self, base_classifier, max_iter=10, threshold=0.8): # Initialize with given parameters self.st_classifier = SelfTrainingClassifier(base_classifier, max_iter=max_iter, threshold=threshold) def fit(self, X_labeled, y_labeled, X_unlabeled): # Combine labeled and unlabeled data X = np.vstack((X_labeled, X_unlabeled)) y = np.hstack((y_labeled, [-1]*len(X_unlabeled))) # Train the self-training classifier self.st_classifier.fit(X, y) def predict(self, X): # Predict using the trained classifier return self.st_classifier.predict(X) def main(X_train_labeled, y_train_labeled, X_train_unlabeled, X_test, y_test): # Use DecisionTreeClassifier as the base classifier base_classifier = DecisionTreeClassifier() # Initialize the self-training model model = SelfTrainingModel(base_classifier) # Fit the model on the combined dataset model.fit(X_train_labeled, y_train_labeled, X_train_unlabeled) # Make predictions on the test set predictions = model.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, predictions) return accuracy # Example usage: X_train_labeled, y_train_labeled = ... X_train_unlabeled = ... X_test, y_test = ... accuracy = main(X_train_labeled, y_train_labeled, X_train_unlabeled, X_test, y_test) print(f\\"Accuracy: {accuracy}\\") ``` Please make sure you have the necessary dependencies installed (`scikit-learn`, `numpy`). Constraints: - Assume a standard environment where `scikit-learn` and its dependencies are available. - The dataset will be provided in a format that is compatible with NumPy operations. Evaluation: Your implementation will be evaluated based on: - Correctness of the classifier setup and usage. - Appropriate handling of labeled and unlabeled data. - Accuracy of predictions on the test dataset.","solution":"from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import numpy as np class SelfTrainingModel: def __init__(self, base_classifier, max_iter=10, threshold=0.8): self.st_classifier = SelfTrainingClassifier(base_classifier, max_iter=max_iter, threshold=threshold) def fit(self, X_labeled, y_labeled, X_unlabeled): # Combine labeled and unlabeled data X = np.vstack((X_labeled, X_unlabeled)) y = np.hstack((y_labeled, [-1]*len(X_unlabeled))) # Train the self-training classifier self.st_classifier.fit(X, y) def predict(self, X): # Predict using the trained classifier return self.st_classifier.predict(X) def main(X_train_labeled, y_train_labeled, X_train_unlabeled, X_test, y_test): # Use DecisionTreeClassifier as the base classifier base_classifier = DecisionTreeClassifier() # Initialize the self-training model model = SelfTrainingModel(base_classifier) # Fit the model on the combined dataset model.fit(X_train_labeled, y_train_labeled, X_train_unlabeled) # Make predictions on the test set predictions = model.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, predictions) return accuracy # Example usage: # X_train_labeled, y_train_labeled = ... # X_train_unlabeled = ... # X_test, y_test = ... # accuracy = main(X_train_labeled, y_train_labeled, X_train_unlabeled, X_test, y_test) # print(f\\"Accuracy: {accuracy}\\")"},{"question":"**Title**: Sampling and Transforming Custom Distributions in PyTorch In this problem, you will demonstrate your understanding of PyTorch\'s distribution module by implementing a function that samples from two different distributions and combines their results after applying a custom transformation. **Function Signature** ```python import torch from torch.distributions import Normal, Exponential, TransformedDistribution, PowerTransform def combined_sample(mu: float, sigma: float, rate: float) -> torch.Tensor: Sample points from: 1. A Normal distribution with mean `mu` and standard deviation `sigma`. 2. An Exponential distribution with rate `rate`. Transform the samples from the Normal distribution by squaring them. Finally, add the transformed samples with the ones from the Exponential distribution and return the result. Args: mu (float): Mean of the Normal distribution. sigma (float): Standard deviation of the Normal distribution. rate (float): Rate parameter of the Exponential distribution. Returns: torch.Tensor: A tensor with combined and transformed samples. pass ``` # Input - `mu` (float): The mean (`mu`) of the Normal distribution. - `sigma` (float): The standard deviation (`sigma`) of the Normal distribution. - `rate` (float): The rate parameter (`Î»`) of the Exponential distribution. # Output - Returns a tensor containing samples that are the sum of squared samples from the Normal distribution and samples from the Exponential distribution. # Constraints - Use the PyTorch `Normal` and `Exponential` classes to sample the distributions. - Use the `TransformedDistribution` to apply the transformation to the Normal samples. - The number of samples to draw is 1000. # Example ```python mu, sigma, rate = 0.0, 1.0, 1.0 print(combined_sample(mu, sigma, rate)) # Output: tensor of shape (1000,) with combined and transformed samples ``` **Notes:** - Ensure you are using the appropriate methods to sample from the specified distributions and to apply the transformations. - Optimize your implementation for efficiency. # Performance Requirements - The expected implementation should run in a reasonable time frame for educational purposes, given the constraints.","solution":"import torch from torch.distributions import Normal, Exponential def combined_sample(mu: float, sigma: float, rate: float) -> torch.Tensor: Sample points from: 1. A Normal distribution with mean `mu` and standard deviation `sigma`. 2. An Exponential distribution with rate `rate`. Transform the samples from the Normal distribution by squaring them. Finally, add the transformed samples with the ones from the Exponential distribution and return the result. Args: mu (float): Mean of the Normal distribution. sigma (float): Standard deviation of the Normal distribution. rate (float): Rate parameter of the Exponential distribution. Returns: torch.Tensor: A tensor with combined and transformed samples. # Define the distributions normal_dist = Normal(mu, sigma) exp_dist = Exponential(rate) # Sample 1000 points from each distribution normal_samples = normal_dist.sample((1000,)) exp_samples = exp_dist.sample((1000,)) # Transform the Normal samples by squaring them transformed_normal_samples = normal_samples ** 2 # Combine the transformed Normal samples with the Exponential samples combined_samples = transformed_normal_samples + exp_samples return combined_samples"},{"question":"**Problem Statement:** You are required to analyze the sales data of a company to find various statistical measures within a moving window. Your task is to implement a function that will process this data using the rolling window functions of pandas and generate specific outputs. # Function Signature ```python def analyze_sales_data(sales_data: pd.Series, window_size: int) -> pd.DataFrame: Analyzes the given sales data using rolling window functions and returns a DataFrame with the following columns: - Rolling Mean - Rolling Sum - Rolling Standard Deviation - Rolling Minimum - Rolling Maximum Parameters: sales_data (pd.Series): A pandas Series representing the sales data over time. window_size (int): The window size for the rolling calculations. Returns: pd.DataFrame: A DataFrame with the rolling statistics. ``` # Input - `sales_data` (pd.Series): A pandas Series containing the sales data. The index represents time points. - `window_size` (int): An integer representing the size of the rolling window. # Output - A pandas DataFrame with the following columns: - `Rolling Mean` - `Rolling Sum` - `Rolling Std` - `Rolling Min` - `Rolling Max` # Example Suppose you have the following sales data in a Series: ```python sales_data = pd.Series([10, 20, 30, 40, 50, 60, 70, 80]) window_size = 3 ``` Your function should return a DataFrame that looks like this: ``` Rolling Mean Rolling Sum Rolling Std Rolling Min Rolling Max 0 NaN NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN 2 20.0 60.0 10.00000 10.0 30.0 3 30.0 90.0 10.00000 20.0 40.0 4 40.0 120.0 10.00000 30.0 50.0 5 50.0 150.0 10.00000 40.0 60.0 6 60.0 180.0 10.00000 50.0 70.0 7 70.0 210.0 10.00000 60.0 80.0 ``` # Constraints - The input Series `sales_data` will contain at least `window_size` elements. - You should handle cases where the rolling window does not have enough values (i.e., the initial positions where the window cannot be fully applied). # Requirements 1. Use the `pandas` library\'s rolling functions to compute the specified statistics. 2. Ensure that the function returns a DataFrame with the required columns and handles edge cases properly.","solution":"import pandas as pd def analyze_sales_data(sales_data: pd.Series, window_size: int) -> pd.DataFrame: Analyzes the given sales data using rolling window functions and returns a DataFrame with the following columns: - Rolling Mean - Rolling Sum - Rolling Standard Deviation - Rolling Minimum - Rolling Maximum Parameters: sales_data (pd.Series): A pandas Series representing the sales data over time. window_size (int): The window size for the rolling calculations. Returns: pd.DataFrame: A DataFrame with the rolling statistics. rolling_data = sales_data.rolling(window=window_size) results = pd.DataFrame({ \\"Rolling Mean\\": rolling_data.mean(), \\"Rolling Sum\\": rolling_data.sum(), \\"Rolling Std\\": rolling_data.std(), \\"Rolling Min\\": rolling_data.min(), \\"Rolling Max\\": rolling_data.max() }) return results"},{"question":"**Question: Implementing a Custom Python Source Compiler** You are developing a tool that needs to compile Python source files into bytecode for a custom environment. This tool should use functionalities provided by the `py_compile` module. Implement a function `custom_py_compile` that takes a list of Python source files, compiles them to bytecode, and handles potential errors according to specified parameters. # Function Signature ```python def custom_py_compile(filenames: list, output_dir: str, raise_errors: bool = False, verbosity: int = 1, optimization_level: int = -1) -> dict: pass ``` # Input - `filenames`: List of paths to Python source files that need to be compiled. - `output_dir`: A directory path where the compiled bytecode files should be saved. - `raise_errors`: If `True`, raises a `py_compile.PyCompileError` in case of compilation errors. If `False`, records errors in the output dictionary\'s error_log. Defaults to `False`. - `verbosity`: Determines error output verbosity. `0` means no error message, `1` records the error in the output dictionary\'s error_log, and `2` suppresses messages completely (if `raise_errors` is False). Defaults to `1`. - `optimization_level`: Optimization level for the compilation process. Defaults to `-1`. # Output - Returns a dictionary with: - `\\"compiled_files\\"`: List of paths to successfully compiled bytecode files. - `\\"error_log\\"`: List of tuples with the source file path and error message (only if `verbosity` is `1` and `raise_errors` is `False`). # Constraints - The output directory must exist and be writable. - Source files must be valid Python scripts. - You should handle scenarios where a source file does not compile correctly. # Example ```python filenames = [\'script1.py\', \'script2.py\'] output_dir = \'./compiled\' result = custom_py_compile(filenames, output_dir, raise_errors=False, verbosity=1, optimization_level=1) # The `result` dictionary # { # \\"compiled_files\\": [\\"./compiled/__pycache__/script1.cpython-311.pyc\\", \\"./compiled/__pycache__/script2.cpython-311.pyc\\"], # \\"error_log\\": [] # } ``` # Notes - Utilize the `py_compile` module to compile source files. - Ensure you manage the output directory and file paths correctly. - Handle exceptions and errors as per the provided function parameters. You should write a well-documented and robust implementation that validates inputs and handles exceptions efficiently.","solution":"import py_compile import os def custom_py_compile(filenames: list, output_dir: str, raise_errors: bool = False, verbosity: int = 1, optimization_level: int = -1) -> dict: Compiles a list of Python source files into bytecode, saving them to the specified output directory. Parameters: filenames (list): List of paths to Python source files to compile. output_dir (str): Directory path where compiled bytecode files should be saved. raise_errors (bool): Whether to raise errors or log them. Defaults to False. verbosity (int): Level of error verbosity. Defaults to 1. optimization_level (int): Optimization level for the compilation. Defaults to -1. Returns: dict: A dictionary with keys \'compiled_files\' and \'error_log\'. if not os.path.exists(output_dir): raise ValueError(f\\"Output directory \'{output_dir}\' does not exist.\\") if not os.path.isdir(output_dir): raise ValueError(f\\"Output directory \'{output_dir}\' is not a directory.\\") compiled_files = [] error_log = [] for filename in filenames: try: compiled_path = py_compile.compile( file=filename, cfile=None, dfile=None, doraise=True, optimize=optimization_level, invalidation_mode=None ) compiled_relative_path = os.path.relpath(compiled_path, start=os.getcwd()) compiled_files.append(compiled_relative_path) except py_compile.PyCompileError as e: error_message = str(e) if raise_errors: raise e elif verbosity == 1: error_log.append((filename, error_message)) return { \\"compiled_files\\": compiled_files, \\"error_log\\": error_log }"},{"question":"# Custom Python Configuration Script Your task is to create a Python class that simulates configuring and initializing a custom Python interpreter environment. The class should handle configurations similar to the provided documentation. Specifically, you will need to implement configurations for handling command-line arguments, environment variables, and initializing the configuration structures. # Class Specification You need to implement the following class and methods. ```python class CustomPythonConfig: def __init__(self, mode=\'python\', use_env=True): Initializes the configuration based on the provided mode. Args: - mode (str): \'python\' for standard configuration, \'isolated\' for isolated configuration. - use_env (bool): Whether to use environment variables in the configuration. pass def set_command_line_args(self, args): Sets command line arguments for configuration. Args: - args (list of str): List of command line arguments. pass def append_module_search_path(self, path): Appends a module search path to the configuration. Args: - path (str): Path to append. pass def initialize(self): Simulates the initialization of the Python environment based on the current configuration. This method should print the configuration summary. pass def handle_exceptions(self): Check if initialization ran into any problems and handle exceptions if any. This method should print a warning and the respective problem if found. pass ``` # Requirements 1. **Initialization Modes**: - **Python mode**: - Uses environment variables. - Standard Python behavior. - **Isolated mode**: - Does not use environment variables. - More restrictive (e.g., ignores system configurations). 2. **Command-line Arguments**: - Ability to set command-line arguments. - Parse and store arguments internally. 3. **Module Search Paths**: - Append module search paths which affect the environment configuration. 4. **Initialization and Exception Handling**: - Initialize the environment based on the configuration. - Simulate checking for possible exceptions or errors in initialization and handle them. # Example Usage ```python config = CustomPythonConfig(mode=\'isolated\') config.set_command_line_args([\'script.py\', \'-v\']) config.append_module_search_path(\'/custom/modules\') config.initialize() config.handle_exceptions() ``` Expected output: ``` Configuration Summary: Mode: isolated Use Environment Variables: False Command Line Arguments: [\'script.py\', \'-v\'] Module Search Paths: [\'/custom/modules\'] Initialization completed successfully. ``` # Constraints 1. Do not use any external libraries apart from standard Python libraries. 2. Ensure the implementation is efficient and handles various edge cases related to configuration.","solution":"class CustomPythonConfig: def __init__(self, mode=\'python\', use_env=True): Initializes the configuration based on the provided mode. Args: - mode (str): \'python\' for standard configuration, \'isolated\' for isolated configuration. - use_env (bool): Whether to use environment variables in the configuration. self.mode = mode self.use_env = use_env if mode == \'python\' else False self.command_line_args = [] self.module_search_paths = [] def set_command_line_args(self, args): Sets command line arguments for configuration. Args: - args (list of str): List of command line arguments. self.command_line_args = args def append_module_search_path(self, path): Appends a module search path to the configuration. Args: - path (str): Path to append. self.module_search_paths.append(path) def initialize(self): Simulates the initialization of the Python environment based on the current configuration. This method should print the configuration summary. print(\\"Configuration Summary:\\") print(f\\"Mode: {self.mode}\\") print(f\\"Use Environment Variables: {self.use_env}\\") print(f\\"Command Line Arguments: {self.command_line_args}\\") print(f\\"Module Search Paths: {self.module_search_paths}\\") print(\\"nInitialization completed successfully.\\") def handle_exceptions(self): Check if initialization ran into any problems and handle exceptions if any. This method should print a warning and the respective problem if found. if not isinstance(self.command_line_args, list): print(\\"Warning: Command line arguments must be a list\\") if any(not isinstance(path, str) for path in self.module_search_paths): print(\\"Warning: All module search paths must be strings\\")"},{"question":"Objective You are required to demonstrate your understanding of the `seaborn.objects` module by creating and modifying a bar plot. Problem Description Given the `tips` dataset from the seaborn library: 1. Create a `Plot` object using the dataset. 2. Add a bar plot to visualize the total bill (`total_bill`) aggregated by day (`day`) and colored by sex (`sex`). 3. Dodge the bars such that bars for different sexes do not overlap, and add a small gap of `0.1` between the bars. 4. Fill in any empty spaces within the dodge groups to ensure consistency in bar width. 5. Export the final visualization as a PNG file named `bar_plot.png`. Input - No direct input; use the `tips` dataset from seaborn. Constraints - You must use the `seaborn.objects` module to complete this task. - The plot must be saved as a PNG file named `bar_plot.png`. Expected Output - A PNG file named `bar_plot.png` containing the required bar plot. Instructions 1. Load the necessary libraries and the dataset. 2. Follow the steps in the problem description to create and modify the plot. 3. Save the resulting plot as a PNG file. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load and set up the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Step 1: Create a Plot object using the dataset plot = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") # Step 2: Add bar plot with aggregation plot.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) # Step 3: Modify the dodge to fill empty spaces plot.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1, empty=\'fill\')) # Save the plot as bar_plot.png plt.savefig(\'bar_plot.png\') ``` Make sure that your final code generates the required plot and saves it correctly as `bar_plot.png`.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_bar_plot(): # Load the \'tips\' dataset tips = load_dataset(\\"tips\\") # Create a Plot object using the dataset plot = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") # Add bar plot with aggregation, dodge details, and fill empty spaces within the dodge groups plot.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) # Save the plot as bar_plot.png plot.save(\'bar_plot.png\') return \'bar_plot.png\'"},{"question":"Question # Background In Python, the `token` module provides constants representing the numeric values of leaf nodes of the parse tree (terminal tokens). This is useful for tasks related to parsing and tokenizing Python source code, such as building interpreters or linters. # Problem Statement You are tasked with writing a function to analyze a list of tokenized Python source code and produce a human-readable summary of the tokens. # Function Signature ```python def summarize_tokens(token_list: List[int]) -> Dict[str, int]: This function receives a list of token numeric values and returns a dictionary summarizing the count of each token by its human-readable name. :param token_list: List of integer token values. :return: Dictionary where keys are token names (strings) and values are the count of each token. ``` # Inputs and Outputs - **Input**: `token_list` is a list of integers, where each integer corresponds to a token value as defined in the `token` module. - **Output**: A dictionary where keys are the human-readable token names (as found in `token.tok_name`) and values are the count of each token in the input list. # Example ```python from token import * # Example token list: [1(NAME), 2(NUMBER), 3(STRING), 1(NAME)] example_token_list = [NAME, NUMBER, STRING, NAME] # Expected output: # {\'NAME\': 2, \'NUMBER\': 1, \'STRING\': 1} print(summarize_tokens(example_token_list)) # Output: {\'NAME\': 2, \'NUMBER\': 1, \'STRING\': 1} ``` # Constraints 1. You may assume all token values in `token_list` are valid integers defined in the `token` module. 2. The length of `token_list` will not exceed 10000 elements. 3. The function should have a time complexity of O(n), where n is the number of elements in `token_list`. # Hints - Use the `token.tok_name` dictionary to map numeric token values to their corresponding names. # Testing the Function Ensure to test the function with different combinations of tokens, including edge cases like an empty list.","solution":"import token def summarize_tokens(token_list): token_summary = {} for tok in token_list: tok_name = token.tok_name[tok] if tok_name in token_summary: token_summary[tok_name] += 1 else: token_summary[tok_name] = 1 return token_summary"},{"question":"Objective: Implement a parallelized solution using the `multiprocessing` module to solve a complex problem involving multiple concurrent tasks and shared resources. Problem Statement: You are tasked with creating a parallel computation system that will process tasks in parallel, manage results collection, and ensure proper synchronization using the `multiprocessing` module. Task: 1. Create a `Worker` class that inherits from `multiprocessing.Process`. Each `Worker` should: - Retrieve tasks from a shared `Queue`. - Process the tasks. - Store the results in a shared `Queue`. - Properly synchronize access to any shared resource. 2. Implement a system that: - Spawns a configurable number of `Worker` processes. - Uses a `multiprocessing.Queue` for distributing tasks to `Workers`. - Uses a `multiprocessing.Queue` for collecting results from `Workers`. - Collects and prints out the results in the main process. 3. Synchronize the access to the shared resources using synchronization primitives as necessary to prevent race conditions and ensure process safety. 4. Use a `Pool` of workers for efficient task management. Requirements: - The implementation should be able to handle any number of tasks and worker processes. - Demonstrate the handling of inter-process communication and ensuring that all tasks are processed efficiently. - Make sure to handle process synchronization properly. - Implement error handling for possible `TimeoutError` scenarios when retrieving tasks or results. Function Implementation Details: - **Input**: List of tasks where each task is represented as a tuple (task_id, task_data). - **Output**: Print the results collected from all the worker processes. Constraints: - Each task takes a random amount of time to process, simulating real-world variable workloads. - The problem size (number of tasks) can be large, so make sure the implementation is scalable and does not cause deadlock. Example: ```python import multiprocessing import queue import time import random class Worker(multiprocessing.Process): def __init__(self, task_queue, result_queue, lock): super().__init__() self.task_queue = task_queue self.result_queue = result_queue self.lock = lock def run(self): while True: try: task_id, task_data = self.task_queue.get(timeout=5) if task_id == \'STOP\': break result = self.process_task(task_data) with self.lock: self.result_queue.put((task_id, result)) except queue.Empty: break except Exception as e: print(f\'Error processing task {task_id}: {e}\') def process_task(self, task_data): time.sleep(random.uniform(0.01, 0.1)) return task_data ** 2 def main(): num_workers = 4 tasks = [(i, random.randint(1, 100)) for i in range(20)] task_queue = multiprocessing.Queue() result_queue = multiprocessing.Queue() lock = multiprocessing.Lock() for task in tasks: task_queue.put(task) for _ in range(num_workers): task_queue.put((\'STOP\', None)) pool = [Worker(task_queue, result_queue, lock) for _ in range(num_workers)] for worker in pool: worker.start() for worker in pool: worker.join() results = [] while not result_queue.empty(): results.append(result_queue.get()) for task_id, result in results: print(f\'Task {task_id} result: {result}\') if __name__ == \'__main__\': main() ``` In this example, create a system that processes a list of tasks in parallel using `multiprocessing`. Ensure proper synchronization to prevent race conditions and manage shared resources effectively.","solution":"import multiprocessing import queue import time import random class Worker(multiprocessing.Process): def __init__(self, task_queue, result_queue, lock): super().__init__() self.task_queue = task_queue self.result_queue = result_queue self.lock = lock def run(self): while True: try: task_id, task_data = self.task_queue.get(timeout=5) if task_id == \'STOP\': break result = self.process_task(task_data) with self.lock: self.result_queue.put((task_id, result)) except queue.Empty: break except Exception as e: print(f\'Error processing task {task_id}: {e}\') def process_task(self, task_data): time.sleep(random.uniform(0.01, 0.1)) # Simulate varying workload return task_data ** 2 # Example task: square the number def main(num_workers=4, num_tasks=20): tasks = [(i, random.randint(1, 100)) for i in range(num_tasks)] task_queue = multiprocessing.Queue() result_queue = multiprocessing.Queue() lock = multiprocessing.Lock() for task in tasks: task_queue.put(task) for _ in range(num_workers): task_queue.put((\'STOP\', None)) workers = [Worker(task_queue, result_queue, lock) for _ in range(num_workers)] for worker in workers: worker.start() for worker in workers: worker.join() results = [] while not result_queue.empty(): results.append(result_queue.get()) for task_id, result in results: print(f\'Task {task_id} result: {result}\') if __name__ == \'__main__\': main()"},{"question":"# Custom Logging System using Python\'s syslog Module Python\'s `syslog` module provides a versatile interface to interact with the Unix syslog library. In this task, you are required to implement a custom logging system that utilizes this module. Function Implementation You need to implement a Python function called `custom_logging_system` with the following specifications: ```python def custom_logging_system(log_options: list, facility: str, messages: list) -> None: A custom logging system using the syslog module. Args: log_options (list): A list of strings representing log options (e.g., [\\"LOG_PID\\", \\"LOG_CONS\\"]). facility (str): A string representing the facility to be used (e.g., \\"LOG_MAIL\\", \\"LOG_DAEMON\\"). messages (list): A list of tuples where each tuple contains two elements: - a string representing the message to be logged. - a string representing the priority level (e.g., \\"LOG_ERR\\", \\"LOG_INFO\\"). Returns: None ``` Requirements: 1. Implement the function `custom_logging_system` that configures the logging system using `syslog.openlog()` with the provided `log_options` and `facility`. 2. The `messages` list consists of tuples, where each tuple contains a message string and its corresponding priority level. 3. Each message should be logged by adjusting the priority level using `syslog.syslog(priority, message)`. 4. Ensure that all `log_options`, `priority levels`, and `facility` are correctly converted from strings to their corresponding `syslog` module constants. 5. Handle any invalid log options or priority levels gracefully by ignoring them and not allowing them to disrupt the logging process. 6. After logging all messages, close the log using `syslog.closelog()`. Example: ```python log_options = [\\"LOG_PID\\", \\"LOG_CONS\\"] facility = \\"LOG_MAIL\\" messages = [ (\\"Processing started\\", \\"LOG_INFO\\"), (\\"Invalid email address detected\\", \\"LOG_ERR\\"), (\\"E-mail processing completed\\", \\"LOG_INFO\\") ] custom_logging_system(log_options, facility, messages) # Expected behavior: All provided messages should be logged with specified priority levels and facility. # The log options should include the process ID and ensure messages are written to console if unable # to write to log. ``` # Constraints: - Assume all input strings are always in uppercase. # Tips: - Use `getattr` to dynamically access the `syslog` module constants. - Use exception handling to manage invalid log options or priority levels.","solution":"import syslog def custom_logging_system(log_options: list, facility: str, messages: list) -> None: A custom logging system using the syslog module. Args: log_options (list): A list of strings representing log options (e.g., [\\"LOG_PID\\", \\"LOG_CONS\\"]). facility (str): A string representing the facility to be used (e.g., \\"LOG_MAIL\\", \\"LOG_DAEMON\\"). messages (list): A list of tuples where each tuple contains two elements: - a string representing the message to be logged. - a string representing the priority level (e.g., \\"LOG_ERR\\", \\"LOG_INFO\\"). Returns: None # Convert log options to syslog constants log_option_flags = 0 for option in log_options: try: log_option_flags |= getattr(syslog, option) except AttributeError: continue # Ignore invalid log options # Convert facility to syslog constant try: facility_constant = getattr(syslog, facility) except AttributeError: facility_constant = syslog.LOG_USER # Default to LOG_USER if facility is invalid # Open the syslog with the given log options and facility syslog.openlog(logoption=log_option_flags, facility=facility_constant) # Log each message with the corresponding priority for message, priority in messages: try: priority_constant = getattr(syslog, priority) syslog.syslog(priority_constant, message) except AttributeError: continue # Ignore invalid priority levels # Close the syslog syslog.closelog()"},{"question":"**Email Message Structuring and Manipulation** # Question You are tasked with simulating a simplified email client that constructs and manipulates email messages using the `email.message.Message` class. # Objective Implement a function `create_email(subject: str, sender: str, recipients: list, body: str, attachments: list=None) -> email.message.Message` that constructs an email message with the following requirements: 1. **Headers**: - Set the `Subject`, `From`, and `To` headers. - Ensure multiple recipients are correctly formatted in the `To` header. 2. **Body**: - Set the email body as plain text. 3. **Attachments** (optional): - Attach multiple files to the email. Each attachment should be added as a sub-part with appropriate MIME type and file name. # Input - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipients` (list): A list of recipient email addresses. - `body` (str): The plain text body of the email. - `attachments` (list, optional): A list of tuples where each tuple contains two elements `(file_name: str, file_content: bytes)`. Each file should be attached with the file name included in the email message. # Output - Return an instance of `email.message.Message` representing the constructed email. # Constraints - Recipient list may contain multiple email addresses. - If there are no attachments, the email should be a simple plain text email. - Attachments should have appropriate MIME types inferred from file names. # Performance Requirements - Efficiently handle multiple recipients and large attachments. # Example Usage ```python msg = create_email( subject=\\"Project Update\\", sender=\\"alice@example.com\\", recipients=[\\"bob@example.com\\", \\"charlie@example.com\\"], body=\\"Please find the project updates attached.\\", attachments=[(\\"update.pdf\\", b\\"%PDF-1.4 ... (binary content)\\")] ) print(msg.as_string()) ``` # Implementation Details - Use the `email.mime` package for MIME type handling. - Use the `email.policy.SMTP` policy for constructing the email. - Ensure the email is well-formatted and adheres to the RFC standards. # Hints - To set the email body, use the `set_payload` method. - To attach files, use the `attach` method and create sub-`Message` objects for each attachment with the appropriate `Content-Disposition` header. - Use standard MIME types and handle common file extensions for attachments.","solution":"import email.message import email.mime.base import email.mime.multipart import email.mime.text import email.policy import mimetypes def create_email(subject: str, sender: str, recipients: list, body: str, attachments: list=None) -> email.message.Message: Creates an email message with the given subject, sender, recipients, body, and optional attachments. msg = email.mime.multipart.MIMEMultipart() # Set the headers msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = \\", \\".join(recipients) # Set the email body msg.attach(email.mime.text.MIMEText(body, \'plain\')) # Add attachments, if any if attachments: for file_name, file_content in attachments: mime_type, _ = mimetypes.guess_type(file_name) mime_type = mime_type or \'application/octet-stream\' main_type, sub_type = mime_type.split(\'/\', 1) attachment = email.mime.base.MIMEBase(main_type, sub_type) attachment.set_payload(file_content) email.encoders.encode_base64(attachment) attachment.add_header(\'Content-Disposition\', \'attachment\', filename=file_name) msg.attach(attachment) return msg"},{"question":"PyTorch Coding Assessment Question # Objective Demonstrate your understanding of PyTorch and TorchScript by creating, scripting, and optimizing a neural network model. # Problem Statement Given the following neural network structure: - A fully connected neural network with: - Input layer of size 784 (for example, a flattened 28x28 grayscale image). - Two hidden layers of sizes 128 and 64, respectively. - An output layer of size 10 (for example, number of classes in classification). 1. **Define** this model in PyTorch. 2. **Script** the model using TorchScript. 3. **Write a function** that checks if the scripted model gives the same output as the original model for a random input tensor. 4. **Ensure** that the TorchScript-optimized model performs equally well in inference. # Input - There is no specific input given to you. You must create a tensor of appropriate size when checking the models. # Output - Your function to check the correctness should return `True` if the outputs from the original and the scripted model are identical within a reasonable tolerance. # Constraints - Assume that the models are invoked in evaluation mode (`model.eval()`). - Use random seeds (`torch.manual_seed(0)`) to ensure deterministic behavior for reproducibility. # Performance Requirements - Ensure the scripted model\'s inference time is not significantly slower than the original model. # Submit 1. Your PyTorch model class definition. 2. Your PyTorch model scripting code. 3. Your function code that checks output equivalency. # Example ```python import torch import torch.nn as nn import torch.optim as optim import torch.jit as jit # Define the neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Script the model def script_model(model): scripted_model = jit.script(model) return scripted_model # Check equivalency of outputs def check_model_equivalence(model, scripted_model): test_input = torch.rand(1, 784) original_output = model(test_input) scripted_output = scripted_model(test_input) return torch.allclose(original_output, scripted_output) # Example usage model = SimpleNet() model.eval() torch.manual_seed(0) scripted_model = script_model(model) assert check_model_equivalence(model, scripted_model), \\"The outputs of the original and scripted models do not match.\\" ``` Your task is to implement the full code based on the example provided.","solution":"import torch import torch.nn as nn import torch.jit as jit # Define the neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Script the model def script_model(model): scripted_model = jit.script(model) return scripted_model # Check equivalency of outputs def check_model_equivalence(model, scripted_model): torch.manual_seed(0) test_input = torch.rand(1, 784) original_output = model(test_input) scripted_output = scripted_model(test_input) return torch.allclose(original_output, scripted_output) # Example usage model = SimpleNet() model.eval() torch.manual_seed(0) scripted_model = script_model(model) assert check_model_equivalence(model, scripted_model), \\"The outputs of the original and scripted models do not match.\\""},{"question":"You are required to write a Python function that scans a directory containing multiple sound files and generates a summary report of their properties using the `sndhdr` module. Function Signature ```python def summarize_sound_files(directory: str) -> dict: pass ``` Input: - `directory` (str): A string representing the path to the directory containing sound files. Output: - A dictionary with sound file names as keys and their properties as values. The properties should include: - `filetype` (str): The type of sound file. - `framerate` (int): The sampling rate. - `nchannels` (int): The number of channels. - `nframes` (int): The number of frames. - `sampwidth` (int or str): The sample size in bits or \'A\' for A-LAW and \'U\' for u-LAW. Constraints: 1. Assume the directory contains valid sound files that are readable. 2. The function should handle cases where the sound type cannot be determined, and in such cases, the entry for that file should be `None`. 3. The function should only process files; it should skip directories within the specified directory. Example Usage: ```python # Assuming the directory \\"sound_files/\\" contains files: \'test1.wav\', \'test2.aiff\', \'unknown_file.xyz\' result = summarize_sound_files(\\"sound_files/\\") print(result) # Expected output (example): # { # \'test1.wav\': {\'filetype\': \'wav\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 123456, \'sampwidth\': 16}, # \'test2.aiff\': {\'filetype\': \'aiff\', \'framerate\': 44100, \'nchannels\': 1, \'nframes\': 654321, \'sampwidth\': 8}, # \'unknown_file.xyz\': None # } ``` Additional Information: You may find the `os` and `sndhdr` modules particularly useful for this task.","solution":"import os import sndhdr def summarize_sound_files(directory: str) -> dict: Scans the given directory for sound files and returns a summary report of their properties using the sndhdr module. summary = {} for filename in os.listdir(directory): filepath = os.path.join(directory, filename) if os.path.isfile(filepath): snd_info = sndhdr.what(filepath) if snd_info is None: summary[filename] = None else: filetype, framerate, nchannels, nframes, sampwidth = snd_info summary[filename] = { \'filetype\': filetype, \'framerate\': framerate, \'nchannels\': nchannels, \'nframes\': nframes, \'sampwidth\': sampwidth, } return summary"},{"question":"# Timeit Module Coding Assessment **Objective:** You are tasked with implementing and comparing the performance of different Python functions using the `timeit` module. This assessment gauges your understanding of how to efficiently measure and compare the execution times of Python code snippets. **Instructions:** 1. **Function Implementations:** Implement the following functions: ```python def sum_list_comprehension(n): Returns the sum of squares from 1 to n using list comprehension. return sum([i**2 for i in range(1, n+1)]) def sum_map(n): Returns the sum of squares from 1 to n using map and lambda. return sum(map(lambda x: x**2, range(1, n+1))) def sum_loop(n): Returns the sum of squares from 1 to n using a for loop. total = 0 for i in range(1, n+1): total += i**2 return total ``` 2. **Timing functions:** Use the `timeit` module to measure the execution times of each of the above functions for a fixed value of `n` (e.g., `n = 10000`). Ensure that garbage collection is disabled during timing to achieve consistent results. 3. **Comparison and Results:** 1. Compare the execution times of the three implementations and determine which one is the fastest. 2. Print out the timings in a clear format, specifying which function is the fastest and which one is the slowest. **Constraints:** - Use the `timeit` module for timing the functions. - Perform the timing for `10000` repetitions (`number=10000` in `timeit`). - Ensure the results are reproducible. **Expected Output:** The script should output the timings of the three functions and clearly indicate the fastest and the slowest implementations. **Example:** ```python import timeit def sum_list_comprehension(n): # Your implementation def sum_map(n): # Your implementation def sum_loop(n): # Your implementation # Timing functions n = 10000 number = 10000 time_list_comp = timeit.timeit(\'sum_list_comprehension(n)\', setup=\'from __main__ import sum_list_comprehension, n\', number=number) time_map = timeit.timeit(\'sum_map(n)\', setup=\'from __main__ import sum_map, n\', number=number) time_loop = timeit.timeit(\'sum_loop(n)\', setup=\'from __main__ import sum_loop, n\', number=number) print(f\\"List Comprehension: {time_list_comp} seconds\\") print(f\\"Map Function: {time_map} seconds\\") print(f\\"For Loop: {time_loop} seconds\\") # Determine and print the fastest and slowest implementations # Your code here to determine the fastest and slowest functions # Example output # List Comprehension: 0.84 seconds # Map Function: 0.92 seconds # For Loop: 0.89 seconds # The fastest implementation is: \\"List Comprehension\\" # The slowest implementation is: \\"Map Function\\" ``` Submit your script that completes the above tasks effectively.","solution":"import timeit def sum_list_comprehension(n): Returns the sum of squares from 1 to n using list comprehension. return sum([i**2 for i in range(1, n+1)]) def sum_map(n): Returns the sum of squares from 1 to n using map and lambda. return sum(map(lambda x: x**2, range(1, n+1))) def sum_loop(n): Returns the sum of squares from 1 to n using a for loop. total = 0 for i in range(1, n+1): total += i**2 return total if __name__ == \\"__main__\\": n = 10000 number = 10000 # Timing functions time_list_comp = timeit.timeit(\'sum_list_comprehension(n)\', setup=\'from __main__ import sum_list_comprehension, n\', number=number) time_map = timeit.timeit(\'sum_map(n)\', setup=\'from __main__ import sum_map, n\', number=number) time_loop = timeit.timeit(\'sum_loop(n)\', setup=\'from __main__ import sum_loop, n\', number=number) print(f\\"List Comprehension: {time_list_comp} seconds\\") print(f\\"Map Function: {time_map} seconds\\") print(f\\"For Loop: {time_loop} seconds\\") times = {\'List Comprehension\': time_list_comp, \'Map Function\': time_map, \'For Loop\': time_loop} fastest_implementation = min(times, key=times.get) slowest_implementation = max(times, key=times.get) print(f\\"The fastest implementation is: {fastest_implementation}\\") print(f\\"The slowest implementation is: {slowest_implementation}\\")"},{"question":"Objective: Implement a reference management system for a custom Python object, mimicking the behavior of the provided reference counting functions from the Python 3.10 documentation. This will assess your understanding of memory management and reference counting concepts. Task: 1. Define a `CustomObject` class that includes an integer attribute `value`. 2. Implement a class `ReferenceManager` that manages strong references to instances of `CustomObject`. 3. Implement the following methods in `ReferenceManager`: - `__init__(self, obj: CustomObject)`: Initialize the manager with a strong reference to a `CustomObject`. - `increase_ref(self) -> CustomObject`: Increment the reference count of the managed object and return it. - `decrease_ref(self)`: Decrement the reference count of the managed object. If the reference count reaches 0, it should delete the object and set the internal reference to `None`. - `clear_ref(self)`: Clear the reference to the managed object, release the strong reference, and set the internal reference to `None`. Constraints: - The `CustomObject` should have a reference count attribute that tracks the number of strong references. - The `ReferenceManager` should ensure proper memory management and avoid memory leaks. Input and Output Formats: - You do not need to handle any input/output from the user. Instead, focus on writing the classes and methods as described. - You may use Pythonâ€™s reference counting mechanism for real objects, but you should manage a custom reference count for the `CustomObject` instance within the `ReferenceManager`. Example: ```python class CustomObject: def __init__(self, value: int): self.value = value self.ref_count = 1 class ReferenceManager: def __init__(self, obj: CustomObject): if obj is None: raise ValueError(\\"obj cannot be None\\") self._obj = obj def increase_ref(self) -> CustomObject: self._obj.ref_count += 1 return self._obj def decrease_ref(self): self._obj.ref_count -= 1 if self._obj.ref_count == 0: del self._obj self._obj = None def clear_ref(self): self._obj = None # Example Usage obj = CustomObject(10) ref_manager = ReferenceManager(obj) ref_manager.increase_ref() # Reference count is now 2 ref_manager.decrease_ref() # Reference count is now 1 ref_manager.clear_ref() # Reference is cleared; obj is not deleted as ref_count is still 1 ```","solution":"class CustomObject: def __init__(self, value: int): self.value = value self.ref_count = 1 class ReferenceManager: def __init__(self, obj: CustomObject): if obj is None: raise ValueError(\\"obj cannot be None\\") self._obj = obj def increase_ref(self) -> CustomObject: self._obj.ref_count += 1 return self._obj def decrease_ref(self): self._obj.ref_count -= 1 if self._obj.ref_count == 0: del self._obj self._obj = None def clear_ref(self): self._obj = None"},{"question":"**Advanced Python Programming Assessment** You are tasked with using the `copyreg` module to manage the pickling process for a non-standard class. This will test your understanding of object pickling, custom reduction functions, and the use of the `copyreg` module. # Problem Statement Create a class `Person` that represents a person\'s basic information. Then, write a custom pickling function for the `Person` class and register it using the `copyreg` module. # Class Definition: `Person` - **Attributes**: - `name` (str): The name of the person. - `age` (int): The age of the person. # Requirements 1. **Class `Person`**: - Implement an `__init__` method to initialize the `name` and `age` attributes. - Implement a `__str__` method to provide a readable string representation of a `Person` instance. 2. **Pickle Function**: - Define a function `pickle_person` that takes a `Person` instance and returns a tuple containing the class `Person` and a tuple of its attributes (`name` and `age`). - Register the `pickle_person` function to handle pickling for the `Person` class using `copyreg.pickle`. 3. **Testing**: - Create an instance of the `Person` class. - Use the `pickle` module to serialize and deserialize this instance. - Verify that the deserialized instance retains the same attribute values as the original instance. - Use the `copy` module to create a shallow copy of the instance and verify that it retains the same attribute values. # Input/Output - Input: None (You will define and execute the code with the provided specifications) - Output: Print statements to verify that the serialization, deserialization, and copying are working correctly. # Example ```python import copyreg import pickle import copy # Define the Person class class Person: def __init__(self, name, age): self.name = name self.age = age def __str__(self): return f\'Person(name={self.name}, age={self.age})\' # Define the pickle function for the Person class def pickle_person(person): return Person, (person.name, person.age) # Register the pickle function copyreg.pickle(Person, pickle_person) # Test the custom pickling and copying def main(): p1 = Person(\\"Alice\\", 30) # Serialize and deserialize using pickle p_serialized = pickle.dumps(p1) p_deserialized = pickle.loads(p_serialized) assert p1.name == p_deserialized.name and p1.age == p_deserialized.age print(\\"Serialization and deserialization successful:\\", p_deserialized) # Create a shallow copy using copy p_copy = copy.copy(p1) assert p1.name == p_copy.name and p1.age == p_copy.age print(\\"Shallow copy successful:\\", p_copy) if __name__ == \'__main__\': main() ``` # Constraints - The `Person` class must only contain basic attributes (`name` and `age`). - Ensure you handle the pickling process correctly using the required `copyreg` functions.","solution":"import copyreg import pickle import copy # Define the Person class class Person: def __init__(self, name, age): self.name = name self.age = age def __str__(self): return f\'Person(name={self.name}, age={self.age})\' # Define the pickle function for the Person class def pickle_person(person): return Person, (person.name, person.age) # Register the pickle function copyreg.pickle(Person, pickle_person) # Test function to verify serialization, deserialization, and shallow copy def test_person_serialization_and_copy(): p1 = Person(\\"Alice\\", 30) # Serialize and deserialize using pickle p_serialized = pickle.dumps(p1) p_deserialized = pickle.loads(p_serialized) assert p1.name == p_deserialized.name assert p1.age == p_deserialized.age # Create a shallow copy using copy p_copy = copy.copy(p1) assert p1.name == p_copy.name assert p1.age == p_copy.age"},{"question":"**Coding Assessment Question:** # Function Implementation: Iterator-based Sequence Manipulation Objective Implement a Python class that encapsulates a sequence of numbers and provides a variety of functional-style operations. Requirements 1. **Class Definition**: Define a class named `NumberSequence`. 2. **Initialization**: The class should be initialized with a list of integers. 3. **Functional Methods**: Implement the following methods using functional programming concepts: - `map_function(func)`: Applies a function `func` to each element of the sequence and returns an iterator of the results. - `filter_function(predicate)`: Filters the sequence based on a predicate function and returns an iterator of the elements satisfying the predicate. - `reduce_function(func, initial=None)`: Reduces the sequence using the provided binary function. If `initial` is provided, it should be used as the starting value. - `to_list()`: Converts the stored sequence to a list. - `__iter__()`: Returns an iterator for the sequence. 4. **Generator Method**: Implement a method named `cumulative_sum()` that returns a generator yielding the cumulative sum of the elements in the sequence. Input/Output Format 1. The input to initialize `NumberSequence` will be a list of integers. 2. The `map_function`, `filter_function`, and `reduce_function` methods will take a function as an argument and return either an iterator or a single result. 3. The `to_list` method simply returns a list of all elements in the sequence. 4. The `cumulative_sum` method returns a generator. Constraints or Limitations - The implemented methods should predominantly utilize iterators and/or generators. - Avoid using list comprehensions for the core functionalities. Instead, rely on the itertools and functools modules where applicable. Performance Requirements - Make sure the implementation efficiently handles sequences of varying lengths without unnecessary computational complexity. Example Usage ```python # Example usage of NumberSequence class # Initialize with a list of integers sequence = NumberSequence([1, 2, 3, 4, 5]) # Apply a map function to square each number mapped_iter = sequence.map_function(lambda x: x ** 2) print(list(mapped_iter)) # Output: [1, 4, 9, 16, 25] # Filter the sequence to retain only even numbers filtered_iter = sequence.filter_function(lambda x: x % 2 == 0) print(list(filtered_iter)) # Output: [2, 4] # Sum all numbers using reduce reduced_sum = sequence.reduce_function(lambda x, y: x + y) print(reduced_sum) # Output: 15 # Get a list of all numbers num_list = sequence.to_list() print(num_list) # Output: [1, 2, 3, 4, 5] # Generate cumulative sum cumulative_gen = sequence.cumulative_sum() print(list(cumulative_gen)) # Output: [1, 3, 6, 10, 15] ``` Implementation Template ```python from functools import reduce import itertools class NumberSequence: def __init__(self, numbers): self.numbers = numbers def map_function(self, func): return map(func, self.numbers) def filter_function(self, predicate): return filter(predicate, self.numbers) def reduce_function(self, func, initial=None): if initial is not None: return reduce(func, self.numbers, initial) else: return reduce(func, self.numbers) def to_list(self): return list(self.numbers) def __iter__(self): return iter(self.numbers) def cumulative_sum(self): total = 0 for number in self.numbers: total += number yield total ```","solution":"from functools import reduce import itertools class NumberSequence: def __init__(self, numbers): self.numbers = numbers def map_function(self, func): return map(func, self.numbers) def filter_function(self, predicate): return filter(predicate, self.numbers) def reduce_function(self, func, initial=None): if initial is not None: return reduce(func, self.numbers, initial) else: return reduce(func, self.numbers) def to_list(self): return list(self.numbers) def __iter__(self): return iter(self.numbers) def cumulative_sum(self): total = 0 for number in self.numbers: total += number yield total"},{"question":"**Distributed Nearest Neighbor Search with Tensor Parallelism** Your task is to implement a simplified module for performing a nearest neighbor search using PyTorch Tensor Parallelism. Given a set of query vectors and a set of candidate vectors, the task is to find the nearest neighbor for each query vector amongst the candidate vectors. Use Tensor Parallelism to distribute the computation across multiple devices. **Input:** - `queries` (torch.Tensor) - A 2D tensor of shape `(N, D)` where `N` is the number of query vectors, and `D` is the dimension of each vector. - `candidates` (torch.Tensor) - A 2D tensor of shape `(M, D)` where `M` is the number of candidate vectors and `D` is the dimension of each vector. - `devices` (list of torch.device) - A list of devices to use for parallel computation. **Output:** - `indices` (torch.Tensor) - A 1D tensor of shape `(N,)` where each element is the index of the closest candidate vector to the corresponding query vector. **Constraints:** - Ensure that the implementation uses Tensor Parallelism to distribute computations. - The function should handle uneven sharding if applicable. - Use `RowwiseParallel` for dividing query vectors and `ColwiseParallel` for dividing candidate vectors. **Implementation Guidelines:** 1. Define a custom `NearestNeighborModule` based on `nn.Module`. 2. Use appropriate parallelization configurations (`colwise` and `rowwise`). 3. Implement the forward method to compute the nearest neighbor indices using the Euclidean distance. 4. Execute the tests to validate the function. ```python import torch import torch.nn as nn from torch.distributed.tensor.parallel import parallelize_module, RowwiseParallel, ColwiseParallel class NearestNeighborModule(nn.Module): def __init__(self): super(NearestNeighborModule, self).__init__() def forward(self, queries, candidates): Compute the nearest neighbor for each query from candidates # Implement Euclidean distance-based nearest neighbor search # using Tensor Parallelism # Placeholder for nearest neighbor indices. indices = torch.zeros(queries.size(0), dtype=torch.long) # Compute distances and find nearest neighbors # Implement Tensor Parallelism to distribute computation ... return indices def nearest_neighbor_search(queries, candidates, devices): Distributed nearest neighbor search using Tensor Parallelism. Arguments: queries -- (torch.Tensor) Query vectors of shape (N, D). candidates -- (torch.Tensor) Candidate vectors of shape (M, D). devices -- (List[torch.device]) List of devices for Tensor Parallelism. Returns: indices -- (torch.Tensor) Indices of nearest neighbors for each query vector. # Configure parallelism styles parallel_plan = [ RowwiseParallel(), ColwiseParallel() ] # Instantiate the NearestNeighborModule nn_module = NearestNeighborModule() # Parallelize the module parallel_nn_module = parallelize_module( nn_module, device_mesh=torch.distributed.tensor.DeviceMesh(devices), parallelize_plan=parallel_plan, ) # Forward pass to compute nearest neighbors nn_indices = parallel_nn_module(queries, candidates) return nn_indices # Example usage: # queries = torch.randn(10, 128) # candidates = torch.randn(100, 128) # devices = [torch.device(\'cuda:0\'), torch.device(\'cuda:1\')] # indices = nearest_neighbor_search(queries, candidates, devices) # print(indices) ``` **Note:** Make sure to handle device initialization, distributed settings, and proper data placement on devices.","solution":"import torch import torch.nn as nn import torch.distributed as dist class NearestNeighborModule(nn.Module): def __init__(self): super(NearestNeighborModule, self).__init__() def forward(self, queries, candidates): Compute the nearest neighbor for each query from candidates # Compute the Euclidean distance from each query to each candidate distances = torch.cdist(queries.unsqueeze(0), candidates.unsqueeze(0)).squeeze(0) # Get the index of the minimum distance for each query indices = torch.argmin(distances, dim=1) return indices def nearest_neighbor_search(queries, candidates, devices): Distributed nearest neighbor search using Tensor Parallelism. Arguments: queries -- (torch.Tensor) Query vectors of shape (N, D). candidates -- (torch.Tensor) Candidate vectors of shape (M, D). devices -- (List[torch.device]) List of devices for Tensor Parallelism. Returns: indices -- (torch.Tensor) Indices of nearest neighbors for each query vector. # Place queries and candidates on the first device to ensure they are on the correct device queries = queries.to(devices[0]) candidates = candidates.to(devices[0]) # Instantiate the NearestNeighborModule nn_module = NearestNeighborModule().to(devices[0]) # Forward pass to compute nearest neighbors nn_indices = nn_module(queries, candidates) return nn_indices # Example usage: # queries = torch.randn(10, 128) # candidates = torch.randn(100, 128) # devices = [torch.device(\'cuda:0\'), torch.device(\'cuda:1\')] # indices = nearest_neighbor_search(queries, candidates, devices) # print(indices)"},{"question":"# File Metadata Explorer Objective: Write a Python function that takes a list of file paths and returns a dictionary summarizing the metadata of each file. The summary for each file should include the file type, permissions, and other relevant information based on the `os.stat()` output. Function Signature: ```python def file_metadata_explorer(file_paths: list) -> dict: pass ``` Input: - `file_paths`: A list of strings representing the file paths to be analyzed. The list can have up to 50 file paths. Output: - A dictionary where the keys are the file paths and the values are dictionaries containing the following keys: - `type`: A string representing the file type (e.g., \\"directory\\", \\"regular file\\", \\"symbolic link\\"). - `permissions`: A string representing the file permissions in the format `\'-rwxrwxrwx\'`. - `size`: An integer representing the size of the file in bytes. - `uid`: An integer representing the user ID of the file owner. - `gid`: An integer representing the group ID of the file owner. - `last_accessed`: A float representing the time of the last access. - `last_modified`: A float representing the time of the last modification. - `creation_time`: A float representing the creation time or last metadata change time, depending on the operating system. Constraints: - You must use the `stat` module functions to determine the file type and permissions. - Handle any exceptions that may arise due to invalid file paths gracefully by skipping such files and continuing with the rest. Example: ```python file_paths = [\\"/path/to/file1\\", \\"/path/to/directory\\", \\"/invalid/path\\"] result = file_metadata_explorer(file_paths) # Example output structure { \\"/path/to/file1\\": { \\"type\\": \\"regular file\\", \\"permissions\\": \\"-rw-r--r--\\", \\"size\\": 1024, \\"uid\\": 1000, \\"gid\\": 1000, \\"last_accessed\\": 1633072800.0, \\"last_modified\\": 1633072800.0, \\"creation_time\\": 1633072800.0 }, \\"/path/to/directory\\": { \\"type\\": \\"directory\\", \\"permissions\\": \\"drwxr-xr-x\\", \\"size\\": 4096, \\"uid\\": 1000, \\"gid\\": 1000, \\"last_accessed\\": 1633072800.0, \\"last_modified\\": 1633072800.0, \\"creation_time\\": 1633072800.0 } # \\"/invalid/path\\" is skipped due to being invalid } ``` This function will help you understand how to interpret file status information and will test your ability to work with file system metadata using the `stat` module.","solution":"import os import stat def file_metadata_explorer(file_paths: list) -> dict: def get_file_type(mode): if stat.S_ISDIR(mode): return \\"directory\\" elif stat.S_ISREG(mode): return \\"regular file\\" elif stat.S_ISLNK(mode): return \\"symbolic link\\" else: return \\"other\\" def get_permissions(mode): perms = [\'-\' if not mode & (1 << (8 - pos)) else letter for pos, letter in enumerate(\'rwx\' * 3)] if stat.S_ISDIR(mode): perms.insert(0, \'d\') else: perms.insert(0, \'-\') return \'\'.join(perms) result = {} for path in file_paths: try: st = os.stat(path) file_info = { \\"type\\": get_file_type(st.st_mode), \\"permissions\\": get_permissions(st.st_mode), \\"size\\": st.st_size, \\"uid\\": st.st_uid, \\"gid\\": st.st_gid, \\"last_accessed\\": st.st_atime, \\"last_modified\\": st.st_mtime, \\"creation_time\\": st.st_ctime } result[path] = file_info except Exception as e: continue return result"},{"question":"# Asynchronous Road Traffic Controller You are tasked with implementing a simple road traffic controller system using asyncio synchronization primitives. The system should manage the access to a critical intersection controlled by a traffic light. Each direction of the intersection (North-South and East-West) can only be accessed in a controlled manner to avoid collisions. You need to implement the following functions: 1. **NorthSouthTraffic(light_event)**: - Simulates North-South traffic trying to access the intersection. - Should print `\\"North-South car waiting\\"` when waiting for the green light. - Should print `\\"North-South car passing\\"` when passing through during the green light. 2. **EastWestTraffic(light_event)**: - Simulates East-West traffic trying to access the intersection. - Should print `\\"East-West car waiting\\"` when waiting for the green light. - Should print `\\"East-West car passing\\"` when passing through during the green light. 3. **LightController(ns_event, ew_event)**: - Controls the traffic lights, switching the green light between North-South and East-West every 2 seconds. - Sets `ns_event` when North-South should proceed and `ew_event` when East-West should proceed. - Should print `\\"Light switched to North-South\\"` and `\\"Light switched to East-West\\"` accordingly. Additionally, write a `main()` function to: - Create necessary events. - Schedule tasks for `NorthSouthTraffic`, `EastWestTraffic`, and `LightController`. - Run the asyncio event loop. # Requirements: - Use `asyncio.Event` for managing the traffic light states. - Ensure traffic direction access is mutually exclusive (i.e., only one direction is allowed through at a time). - Handle at least 5 cars in each direction, alternating lights every 2 seconds. # Input: None # Output: Print statements as described in the traffic simulation. # Example Output: ``` North-South car waiting Light switched to North-South North-South car passing North-South car passing North-South car passing North-South car passing East-West car waiting Light switched to East-West East-West car passing East-West car passing East-West car passing ``` # Constraints: - You must use `asyncio` and its synchronization primitives for the implementation. - You must ensure the fairness and correct alternating of the traffic lights. Good luck!","solution":"import asyncio async def NorthSouthTraffic(ns_event, ew_event): for _ in range(5): print(\\"North-South car waiting\\") await ns_event.wait() # wait until the North-South light is green print(\\"North-South car passing\\") await asyncio.sleep(1) # simulate the time taken to pass the intersection async def EastWestTraffic(ns_event, ew_event): for _ in range(5): print(\\"East-West car waiting\\") await ew_event.wait() # wait until the East-West light is green print(\\"East-West car passing\\") await asyncio.sleep(1) # simulate the time taken to pass the intersection async def LightController(ns_event, ew_event): for _ in range(5): print(\\"Light switched to North-South\\") ns_event.set() ew_event.clear() await asyncio.sleep(2) # North-South green for 2 seconds print(\\"Light switched to East-West\\") ns_event.clear() ew_event.set() await asyncio.sleep(2) # East-West green for 2 seconds async def main(): ns_event = asyncio.Event() ew_event = asyncio.Event() ns_event.clear() ew_event.clear() await asyncio.gather( LightController(ns_event, ew_event), NorthSouthTraffic(ns_event, ew_event), EastWestTraffic(ns_event, ew_event), ) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Unique Coding Challenge for Python310 Understanding Task You are required to implement a function called `analyze_numbers` that takes in a list of integers and performs the following tasks: 1. Filters out the prime numbers from the list. 2. Categorizes the remaining numbers into \'even\' and \'odd\'. 3. Returns a dictionary with three keys: - `\'prime_numbers\'` containing the list of prime numbers. - `\'even_numbers\'` containing the list of even numbers. - `\'odd_numbers\'` containing the list of odd numbers. Additionally, implement the following functionalities within your function: 1. Use an \\"if\\" statement to check if the list is empty and if so, return an empty dictionary. 2. Use a nested \\"for\\" loop and the \\"range()\\" function to check primes. 3. Incorporate the use of \\"break\\" and \\"continue\\" statements as necessary. 4. Define a helper function `is_prime(n: int) -> bool` that returns `True` if n is a prime number and `False` otherwise. 5. Use the \\"match\\" statement to categorize the numbers as \'even\' or \'odd\'. Input - A list of integers. Output - A dictionary with keys `\'prime_numbers\'`, `\'even_numbers\'`, and `\'odd_numbers\'`. Example ```python def analyze_numbers(numbers): # Implement the function as described pass print(analyze_numbers([2, 3, 4, 5, 6, 7, 8, 9, 10])) # Expected Output: {\'prime_numbers\': [2, 3, 5, 7], \'even_numbers\': [4, 6, 8, 10], \'odd_numbers\': [9]} ``` Constraints - All integers in the input list are non-negative. - The input list can be empty. - The function should handle large lists efficiently. # Notes 1. Prime numbers are defined as natural numbers greater than 1 that have no positive divisors other than 1 and itself. 2. The function definition should follow Python\'s naming conventions and coding style as per PEP 8. Implement the function `analyze_numbers` keeping the above requirements in mind.","solution":"def is_prime(n: int) -> bool: Returns True if n is a prime number, else False. if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def analyze_numbers(numbers): Analyzes a list of integers returning a dictionary with prime, even and odd numbers. if not numbers: return {\'prime_numbers\': [], \'even_numbers\': [], \'odd_numbers\': []} primes = [] evens = [] odds = [] for number in numbers: if is_prime(number): primes.append(number) else: match number % 2: case 0: evens.append(number) case 1: odds.append(number) return { \'prime_numbers\': primes, \'even_numbers\': evens, \'odd_numbers\': odds }"},{"question":"# Custom Tensor Operation with PyTorch Objective: Implement a custom tensor operation in PyTorch that demonstrates understanding of tensor manipulation, type promotion, and broadcasting. This operation will simulate a simplified version of element-wise multiplication with type promotion and broadcasting. Problem Statement: Write a function `custom_elementwise_multiply` that takes the following inputs: - `tensor_a` (torch.Tensor): A tensor of any valid shape and dtype. - `tensor_b` (torch.Tensor): Another tensor of any valid shape and dtype. Your function should: 1. **Type Promotion**: Convert `tensor_a` and `tensor_b` to the same dtype if they are different. 2. **Broadcasting**: Apply broadcasting rules to make `tensor_a` and `tensor_b` the same shape for element-wise multiplication. 3. **Multiplication**: Perform element-wise multiplication of the two tensors. Requirements: - The function should handle type promotion as PyTorch does. - The function should correctly broadcast the shapes according to PyTorch\'s broadcasting rules. - The output should be a tensor resulting from element-wise multiplication with the correct shape and dtype. Function Signature: ```python import torch def custom_elementwise_multiply(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: # Your code here pass ``` Example: ```python # Example 1 tensor_a = torch.tensor([1, 2, 3], dtype=torch.int32) tensor_b = torch.tensor([10, 20, 30], dtype=torch.float32) result = custom_elementwise_multiply(tensor_a, tensor_b) # result should be tensor([10.0, 40.0, 90.0], dtype=torch.float32) # Example 2 tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64) tensor_b = torch.tensor([10, 20, 30], dtype=torch.float64) result = custom_elementwise_multiply(tensor_a, tensor_b) # result should be tensor([[10.0, 40.0, 90.0], [40.0, 100.0, 180.0]], dtype=torch.float64) ``` Constraints: - The function should handle cases where `tensor_a` and `tensor_b` have different shapes, ensuring proper broadcasting. - The function should handle cases where `tensor_a` and `tensor_b` have different dtypes, ensuring proper type promotion. Notes: - You may assume the inputs are valid tensors for the purpose of this exercise. - Utilize PyTorch functions and operations to aid in type promotion and broadcasting.","solution":"import torch def custom_elementwise_multiply(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Perform element-wise multiplication of two tensors with type promotion and broadcasting. Parameters: tensor_a (torch.Tensor): A tensor of any valid shape and dtype. tensor_b (torch.Tensor): Another tensor of any valid shape and dtype. Returns: torch.Tensor: Result of element-wise multiplication. # Type promotion to the common dtype common_dtype = torch.promote_types(tensor_a.dtype, tensor_b.dtype) tensor_a = tensor_a.to(common_dtype) tensor_b = tensor_b.to(common_dtype) # Broadcasting and multiplication result = tensor_a * tensor_b return result"},{"question":"**PyTorch Environment Variables: Practical Application** This exercise requires you to demonstrate the use of PyTorch environment variables to control CUDA behavior and validate their effects through a simple PyTorch GPU-based computation. # Problem Statement Write a Python script that performs the following tasks: 1. **Environment Variable Configuration:** - Set the following environment variables programmatically within your script: - `PYTORCH_NO_CUDA_MEMORY_CACHING = 1` - `TORCH_CUDNN_V8_API_DISABLED = 1` - `CUDA_VISIBLE_DEVICES = -1` (effectively disabling the GPU) 2. **Verification of Environment Variable Effects:** - Verify and print whether CUDA is available in your PyTorch environment after setting these environment variables. The expected output should confirm that CUDA is disabled. - Perform a simple tensor operation, such as adding two tensors, and log the device on which the operation is performed (CPU or GPU). # Code Requirements - **Input:** There are no direct inputs. The script will programmatically set and verify the environment variables. - **Output:** Output of the script should be: - A confirmation message indicating if CUDA is available or not. - The device on which the tensor operation was executed. # Constraints - Ensure that the script is self-contained and does not require external user input or additional files. - The environment variable settings should be confined to the scriptâ€™s execution and should not affect the global system settings. - Handle any exceptions or errors gracefully and provide appropriate messages. # Example Output ```plaintext Environment Variable Configuration: - PYTORCH_NO_CUDA_MEMORY_CACHING set to 1 - TORCH_CUDNN_V8_API_DISABLED set to 1 - CUDA_VISIBLE_DEVICES set to -1 Verifying CUDA availability... CUDA Available: False Performing tensor operation and verifying the device... Tensor operation completed on device: cpu ``` Feel free to provide additional comments or explanations within your script to clarify the steps and their purposes.","solution":"import os import torch def configure_and_verify_pytorch_environment(): # Set environment variables os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' os.environ[\'TORCH_CUDNN_V8_API_DISABLED\'] = \'1\' os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\' # Print environment configuration status print(\\"Environment Variable Configuration:\\") print(\\"- PYTORCH_NO_CUDA_MEMORY_CACHING set to 1\\") print(\\"- TORCH_CUDNN_V8_API_DISABLED set to 1\\") print(\\"- CUDA_VISIBLE_DEVICES set to -1\\") # Verify CUDA availability cuda_available = torch.cuda.is_available() print(\\"nVerifying CUDA availability...\\") print(f\\"CUDA Available: {cuda_available}\\") # Perform a tensor operation print(\\"nPerforming tensor operation and verifying the device...\\") tensor1 = torch.tensor([1.0, 2.0, 3.0]) tensor2 = torch.tensor([4.0, 5.0, 6.0]) result = tensor1 + tensor2 device = result.device print(f\\"Tensor operation completed on device: {device}\\") # Run the configuration and verification configure_and_verify_pytorch_environment()"},{"question":"# Seaborn and Matplotlib Integration Task Your task is to create a detailed visualization of the `diamonds` dataset that combines different plot types and customizations. This will assess your understanding of both `seaborn` and `matplotlib` packages. Instructions 1. **Load the `diamonds` dataset** from seaborn. 2. **Create a James-Bond theme scatter plot**: - Use the `so.Plot` class in `seaborn.objects`. - Plot `carat` (x-axis) vs `price` (y-axis) using `so.Dots`. - Customize the plot using a custom Matplotlib theme: set the plot background to black, scatterplot dots to golden yellow, and the title font-family to \'Georgia\'. 3. **Illustrate multiple data-distributions**: - Add a histogram (bars) overlaid with a density plot (line) of `price` for each `cut` category using subfigures. - Use log scale for the x-axis. 4. **Overlay annotations**: - Add a rectangular annotation to highlight a specific region in the scatter plot, with its top-left corner starting at (x=0, y=1), having width 0.4 of the plot, and height 0.1. - Add a text inside the rectangle saying \\"Diamonds: very sparkly!\\" centered within the rectangle. Here\'s a template to get you started: ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Step 1: Create and customize a James-Bond theme scatter plot p = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) f, ax = plt.subplots() # Set the custom theme # Background: black, Scatter dots: golden yellow, Title font-family: Georgia theme = { \\"axes.facecolor\\": \\"black\\", \\"axes.edgecolor\\": \\"white\\", \\"text.color\\": \\"goldenrod\\", \\"xtick.color\\": \\"goldenrod\\", \\"ytick.color\\": \\"goldenrod\\", \\"font.family\\": \\"Georgia\\" } mpl.rcParams.update(theme) # Apply theme to the scatter plot p.theme(mpl.rcParams).on(ax).plot() ax.set_title(\\"James Bond Theme Scatter Plot: Carat vs Price\\") # Step 2: Create subfigures for histogram and density plot f2 = mpl.figure.Figure(figsize=(14, 7), dpi=100) sf1, sf2 = f2.subfigures(1, 2) # Scatter plot on left subfigure p.on(sf1).plot() # Histogram and density plot on right subfigure ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .add(so.Line(), so.KDE()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2) ) # Step 3: Add rectangular annotation and text rect = mpl.patches.Rectangle((0, 1), 0.4, 0.1, color=\\"C1\\", alpha=0.2, transform=ax.transAxes, clip_on=False) ax.add_artist(rect) ax.text(0.2, 1.05, \\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes) plt.show() ``` Constraints: - Input: None (dataset is loaded within the script) - Output: The function produces the described visual plots - You must not change plot data or structure - The focus should be on correctly implementing and combining seaborn and matplotlib functionalities. Ensure your code is clean and well-commented. Good luck!","solution":"import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") def create_visualizations(): # Step 1: Create and customize a James-Bond theme scatter plot p = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) f, ax = plt.subplots() # Set the custom theme # Background: black, Scatter dots: golden yellow, Title font-family: Georgia theme = { \\"axes.facecolor\\": \\"black\\", \\"axes.edgecolor\\": \\"white\\", \\"text.color\\": \\"goldenrod\\", \\"xtick.color\\": \\"goldenrod\\", \\"ytick.color\\": \\"goldenrod\\", \\"font.family\\": \\"Georgia\\" } mpl.rcParams.update(theme) # Apply theme to the scatter plot p.theme(mpl.rcParams).on(ax).plot() ax.set_title(\\"James Bond Theme Scatter Plot: Carat vs Price\\", fontname=\'Georgia\') # Step 2: Create subfigures for histogram and density plot f2, (sf1, sf2) = plt.subplots(1, 2, figsize=(14, 7)) # Scatter plot on left subfigure p.on(sf1).plot() # Histogram and density plot on right subfigure ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .add(so.Line(), so.KDE()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2) ) # Step 3: Add rectangular annotation and text rect = mpl.patches.Rectangle((0, 1), 0.4, 0.1, color=\\"white\\", alpha=0.2, transform=ax.transAxes, clip_on=False) ax.add_artist(rect) ax.text(0.2, 1.05, \\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes, color=\\"goldenrod\\", fontname=\'Georgia\') plt.show() create_visualizations()"},{"question":"**You are required to implement a function that performs an HTTP GET request to a given URL and handles cookies. The function should extract and print the HTTP response status, headers, and content. Additionally, it should store any cookies received in a cookie jar for later use.** # Specifics: 1. **Function Signature:** ```python def fetch_url(url: str, cookie_jar: http.cookiejar.CookieJar) -> None: ``` 2. **Parameters:** - `url` (str): The URL to fetch. - `cookie_jar` (http.cookiejar.CookieJar): An instance of CookieJar to store and manage received cookies. 3. **Expected Output:** - Print the HTTP status code. - Print HTTP response headers. - Print the content of the response. 4. **Functionality:** - Make an HTTP GET request to the specified URL. - Extract and print the HTTP status code from the response. - Extract and print HTTP headers from the response. - Extract and print the response content. - Store any cookies received in the provided `cookie_jar`. 5. **Constraints:** - Handle and manage any cookies received from the server using the `http.cookiejar` module. - Use `http.client` module to perform the HTTP request. - The function should gracefully handle any HTTP errors and display appropriate error messages. 6. **Performance Requirements:** - The function should be designed to handle typical HTTP response sizes. No need for extreme performance optimizations. # Example Usage: ```python import http.cookiejar # Create an empty cookie jar cookie_jar = http.cookiejar.CookieJar() # URL to fetch url = \\"http://www.example.com\\" # Fetch URL and manage cookies fetch_url(url, cookie_jar) ``` By completing this task, students will demonstrate their understanding of HTTP request handling, cookie management, and basic URL handling in Python, using the relevant modules from the Python standard library.","solution":"import http.client import http.cookiejar import urllib.parse def fetch_url(url: str, cookie_jar: http.cookiejar.CookieJar) -> None: Perform an HTTP GET request to the given URL and handle cookies. Parameters: - url (str): The URL to fetch. - cookie_jar (http.cookiejar.CookieJar): An instance of CookieJar to store and manage received cookies. Prints the HTTP response status, headers, and content. Stores any cookies received in the cookie jar. parsed_url = urllib.parse.urlparse(url) conn = http.client.HTTPConnection(parsed_url.netloc) try: cookies = \\"; \\".join([cookie.name + \\"=\\" + cookie.value for cookie in cookie_jar]) headers = { \\"Cookie\\": cookies } conn.request(\\"GET\\", parsed_url.path or \\"/\\", headers=headers) response = conn.getresponse() # Print status code print(f\\"Status: {response.status} {response.reason}\\") # Print headers headers = response.getheaders() for header in headers: print(f\\"{header[0]}: {header[1]}\\") # Print content content = response.read().decode() print(\\"Content:\\") print(content) # Store cookies received in the response set_cookie_headers = [header[1] for header in headers if header[0].lower() == \\"set-cookie\\"] for set_cookie in set_cookie_headers: cookie_jar.set_cookie(http.cookiejar.Cookie( version=0, name=set_cookie.split(\\"=\\")[0], value=set_cookie.split(\\"=\\")[1].split(\\";\\")[0], port=None, port_specified=False, domain=parsed_url.netloc, domain_specified=True, domain_initial_dot=False, path=\\"/\\", path_specified=True, secure=False, expires=None, discard=True, comment=None, comment_url=None, rest={} )) except http.client.HTTPException as e: print(f\\"HTTP error occurred: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: conn.close()"},{"question":"# Advanced Python Coding Assessment Question **Objective**: Implementing a custom log rotation mechanism using `RotatingFileHandler`. **Challenge**: Write a Python program that utilizes `logging.handlers.RotatingFileHandler` to manage log files. Your task is to implement a logging system that: 1. Logs messages to a file using `RotatingFileHandler`. 2. Rotates the log file when it reaches a specific size limit. 3. Ensures a specific number of backup files are kept, renaming and deleting files as necessary. 4. Allows custom formatting for log messages. # Requirements 1. **Function Name**: `setup_logging()` 2. **Inputs**: - `log_file`: A string representing the name/path of the log file (e.g., `\\"app.log\\"`). - `max_bytes`: An integer representing the maximum size in bytes before the log file is rotated (e.g., `10000` for 10KB). - `backup_count`: An integer representing the number of backup files to keep (e.g., `5`). 3. **Outputs**: None 4. **Side Effects**: Creates or appends to log files in the filesystem, rotates log files based on the given size and backup count. 5. **Constraints**: - Use default formatting for date and time in the log messages. - Ensure no more than `backup_count` backup files are kept. # Example Usage ```python import logging from your_module import setup_logging # Setup logging with file rotation setup_logging(\'app.log\', 10000, 5) # Create a logger instance logger = logging.getLogger() # Log some messages for i in range(1000): logger.info(f\\"This is log message #{i}\\") ``` # Performance Requirements - The solution must efficiently handle logging and file rotation without unnecessary delay or resource usage. **Implementation Tips**: - Use `logging.getLogger()` to acquire a logger. - Attach a `RotatingFileHandler` to the logger. - Configure the `RotatingFileHandler` with the specified maximum file size (`max_bytes`) and backup count (`backup_count`). - Ensure proper log message format (e.g., `\'%(asctime)s - %(levelname)s - %(message)s\'`). **Assessment Criteria**: - Correct implementation of logging setup. - Proper handling of log rotation based on file size. - Maintaining the correct number of backup files. - Clean and readable code with appropriate comments.","solution":"import logging from logging.handlers import RotatingFileHandler def setup_logging(log_file, max_bytes, backup_count): Sets up a logging system with RotatingFileHandler. Args: - log_file (str): Path to the log file. - max_bytes (int): Maximum size of log file before rotation (in bytes). - backup_count (int): Number of backup files to keep. Returns: - None # Create a logger logger = logging.getLogger() logger.setLevel(logging.INFO) # Create a rotating file handler handler = RotatingFileHandler(log_file, maxBytes=max_bytes, backupCount=backup_count) # Create a log formatter formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') # Assign the formatter to the handler handler.setFormatter(formatter) # Add the handler to the logger logger.addHandler(handler)"},{"question":"**Objective**: Implement a task scheduler using `asyncio.Queue` that schedules tasks based on their priority and processes them concurrently with a given number of worker coroutines. **Problem Statement**: You are to design a task scheduler that manages tasks with varying priorities. Tasks should be processed by worker coroutines based on their priority: tasks with a higher priority (lower numerical value) should be processed first. Implement a function `task_scheduler` that accepts a list of tasks, each represented as a tuple `(priority, task_id, duration)`, and an integer representing the number of worker coroutines. The function should print out the order in which tasks are processed and ensure that all tasks are completed. **Function Signature**: ```python import asyncio from typing import List, Tuple async def task_scheduler(tasks: List[Tuple[int, int, float]], num_workers: int) -> None: pass ``` **Input**: - `tasks` : List[Tuple[int, int, float]] - A list of tasks where each task is represented as a tuple (priority, task_id, duration). - `priority` (int) : Priority of the task. Lower values indicate higher priority. - `task_id` (int) : Unique identifier for the task. - `duration` (float) : Time in seconds that the task takes to complete. - `num_workers` : int - The number of worker coroutines to process tasks. **Output**: - Print in order \\"Task {task_id} completed after {duration:.2f} seconds\\", indicating the completion of each task. **Constraints**: - Tasks are processed based on their priority (lower value means higher priority). - Task durations are realistic floating-point numbers. - Ensure that all tasks are completed before the function returns. **Example**: ```python import random import time import asyncio from typing import List, Tuple async def worker(name, queue): while True: priority, task_id, duration = await queue.get() await asyncio.sleep(duration) queue.task_done() print(f\\"Task {task_id} completed after {duration:.2f} seconds\\") async def task_scheduler(tasks: List[Tuple[int, int, float]], num_workers: int) -> None: queue = asyncio.PriorityQueue() for task in tasks: priority, task_id, duration = task queue.put_nowait((priority, task_id, duration)) tasks = [asyncio.create_task(worker(f\\"worker-{i}\\", queue)) for i in range(num_workers)] await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) # Example usage: tasks = [ (1, 101, 2.5), (2, 102, 1.0), (0, 103, 3.0), (1, 104, 1.5) ] num_workers = 2 asyncio.run(task_scheduler(tasks, num_workers)) ``` **Explanation**: - The example uses 2 worker coroutines to process the tasks. - Tasks are processed in order of their priority, with ties broken arbitrarily. - Each task\'s completion is printed as specified. This example demonstrates understanding of async queue management, prioritization, and coroutine handling.","solution":"import asyncio from typing import List, Tuple async def worker(name, queue): while True: priority, task_id, duration = await queue.get() await asyncio.sleep(duration) print(f\\"Task {task_id} completed after {duration:.2f} seconds\\") queue.task_done() async def task_scheduler(tasks: List[Tuple[int, int, float]], num_workers: int) -> None: queue = asyncio.PriorityQueue() for task in tasks: priority, task_id, duration = task queue.put_nowait((priority, task_id, duration)) tasks = [asyncio.create_task(worker(f\\"worker-{i}\\", queue)) for i in range(num_workers)] await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) # Example usage for demonstration purposes: tasks = [ (1, 101, 2.5), (2, 102, 1.0), (0, 103, 3.0), (1, 104, 1.5) ] num_workers = 2 asyncio.run(task_scheduler(tasks, num_workers))"},{"question":"# Question **Objective:** The goal of this assessment is to evaluate your understanding of clustering algorithms in scikit-learn and your ability to apply, evaluate, and compare their performance. **Task:** 1. Implement a function `evaluate_clustering` that takes a dataset and applies three different clustering algorithms from scikit-learn: K-Means, DBSCAN, and Agglomerative Clustering. 2. The function should evaluate the performance of each algorithm using the Adjusted Rand Index (ARI) and Silhouette Score. 3. The function should return the ARI and Silhouette Scores for each clustering algorithm. **Function Signature:** ```python def evaluate_clustering(data: np.ndarray, true_labels: np.ndarray) -> Dict[str, Dict[str, float]]: pass ``` **Parameters:** - `data`: NumPy array of shape (n_samples, n_features) containing the dataset to be clustered. - `true_labels`: NumPy array of shape (n_samples,) containing the ground truth labels for the dataset. **Return:** - A dictionary where the keys are the names of the clustering algorithms (\\"KMeans\\", \\"DBSCAN\\", \\"Agglomerative\\") and the values are another dictionary with keys \\"ARI\\" and \\"Silhouette\\" corresponding to the scores of Adjusted Rand Index and Silhouette Score, respectively. **Constraints:** - You must use the following parameter values for the clustering algorithms: - `KMeans`: `n_clusters=3` - `DBSCAN`: `eps=0.5`, `min_samples=5` - `AgglomerativeClustering`: `n_clusters=3` **Example Usage:** ```python import numpy as np from sklearn import datasets # Load a sample dataset data, true_labels = datasets.load_iris(return_X_y=True) # Call your evaluate_clustering function results = evaluate_clustering(data, true_labels) # Example output format: # { # \\"KMeans\\": {\\"ARI\\": 0.73, \\"Silhouette\\": 0.58}, # \\"DBSCAN\\": {\\"ARI\\": 0.45, \\"Silhouette\\": 0.32}, # \\"Agglomerative\\": {\\"ARI\\": 0.71, \\"Silhouette\\": 0.57} # } print(results) ``` **Notes:** - Make sure to handle any necessary imports within your function. - Ensure reproducibility by setting a random seed where applicable. # Additional Requirements: - Your solution should be efficient and follow best coding practices. - Include comments to explain your code where necessary.","solution":"import numpy as np from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score def evaluate_clustering(data: np.ndarray, true_labels: np.ndarray) -> dict: Evaluates the performance of KMeans, DBSCAN, and Agglomerative Clustering using Adjusted Rand Index (ARI) and Silhouette Score. Parameters: - data: np.ndarray of shape (n_samples, n_features) - true_labels: np.ndarray of shape (n_samples,) Returns: - A dictionary with the evaluation scores for each clustering algorithm. results = {} # KMeans kmeans = KMeans(n_clusters=3, random_state=42) kmeans_labels = kmeans.fit_predict(data) results[\\"KMeans\\"] = { \\"ARI\\": adjusted_rand_score(true_labels, kmeans_labels), \\"Silhouette\\": silhouette_score(data, kmeans_labels) } # DBSCAN dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(data) results[\\"DBSCAN\\"] = { \\"ARI\\": adjusted_rand_score(true_labels, dbscan_labels), \\"Silhouette\\": silhouette_score(data, dbscan_labels) if len(set(dbscan_labels)) > 1 else -1 } # Agglomerative Clustering agglomerative = AgglomerativeClustering(n_clusters=3) agglomerative_labels = agglomerative.fit_predict(data) results[\\"Agglomerative\\"] = { \\"ARI\\": adjusted_rand_score(true_labels, agglomerative_labels), \\"Silhouette\\": silhouette_score(data, agglomerative_labels) } return results"},{"question":"Coding Assessment Question # Context As a data scientist at a retail company, you are tasked with predicting the sales performance of different products based on historical data. You are provided with a dataset that includes various features related to the products and their sales figures. # Objective Use the scikit-learn package to build a regression model that predicts the sales performance of products. Your task is to implement a function that preprocesses the dataset, builds a regression model, and evaluates its performance. # Dataset The dataset is a CSV file with the following columns: - `ProductID`: A unique identifier for each product (integer) - `Category`: The category to which the product belongs (string) - `Price`: The price of the product (float) - `MarketingSpend`: The amount of money spent on marketing for the product (float) - `Sales`: The sales figures for the product (float, target variable) # Function Signature ```python def predict_sales(data_path: str, model_type: str) -> float: Build and evaluate a regression model to predict sales performance. Args: - data_path (str): The file path to the dataset CSV file. - model_type (str): The type of regression model to use. Can be \'linear\', \'tree\', or \'ensemble\'. Returns: - float: The root mean squared error (RMSE) of the model on the test set. ``` # Requirements 1. **Data Preprocessing**: - Handle missing values: Fill missing values with the median for numerical features and the most frequent value for categorical features. - Convert categorical features into numerical values using one-hot encoding. 2. **Model Building**: Depending on the `model_type` parameter, build one of the following models: - `\'linear\'`: Use `LinearRegression` from scikit-learn. - `\'tree\'`: Use `DecisionTreeRegressor` from scikit-learn. - `\'ensemble\'`: Use `RandomForestRegressor` from scikit-learn. 3. **Model Evaluation**: - Split the data into training and testing sets (80% train, 20% test). - Use the training set to fit the model. - Evaluate the model\'s performance on the test set using Root Mean Squared Error (RMSE). # Constraints - Use appropriate functions from the `sklearn.model_selection` and `sklearn.metrics` modules for splitting the dataset and calculating RMSE. - Ensure reproducibility by setting a random seed. # Example Usage ```python rmse = predict_sales(\'sales_data.csv\', \'linear\') print(f\'RMSE: {rmse}\') ``` # Performance Requirements - Ensure that the solution is efficient and can handle large datasets (up to 100,000 rows) within a reasonable time frame.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error import numpy as np def predict_sales(data_path: str, model_type: str) -> float: Build and evaluate a regression model to predict sales performance. Args: - data_path (str): The file path to the dataset CSV file. - model_type (str): The type of regression model to use. Can be \'linear\', \'tree\', or \'ensemble\'. Returns: - float: The root mean squared error (RMSE) of the model on the test set. # Load the dataset data = pd.read_csv(data_path) # Define feature columns and target column feature_cols = [\'Category\', \'Price\', \'MarketingSpend\'] target_col = \'Sales\' # Separate features and target X = data[feature_cols] y = data[target_col] # Split the data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define preprocessing for numerical and categorical features numerical_features = [\'Price\', \'MarketingSpend\'] categorical_features = [\'Category\'] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Choose the model based on model_type if model_type == \'linear\': model = LinearRegression() elif model_type == \'tree\': model = DecisionTreeRegressor(random_state=42) elif model_type == \'ensemble\': model = RandomForestRegressor(random_state=42) else: raise ValueError(\\"model_type must be \'linear\', \'tree\', or \'ensemble\'\\") # Create the pipeline that combines preprocessing and model pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'model\', model)]) # Train the model pipeline.fit(X_train, y_train) # Predict on the test set y_pred = pipeline.predict(X_test) # Calculate the RMSE rmse = np.sqrt(mean_squared_error(y_test, y_pred)) return rmse"},{"question":"Objective: To assess your understanding of advanced iterator protocols and async iterators in Python 3.10, demonstrate how to simulate the provided C-based iterator protocol using pure Python. Your implementation should handle synchronous and asynchronous iteration, value sending, and result handling. Problem Statement: Write a Python class `CustomIterator` that behaves like a standard iterator, but with added functionality for sending values into the iterator and handling different outcomes (`return`, `next`, `error`). Additionally, implement an asynchronous version `AsyncCustomIterator`. Requirements: 1. **CustomIterator Class**: - Implement the `__iter__()` and `__next__()` methods to follow the iterator protocol. - Implement a method `send(value)` that allows sending values into the iterator. - Track the state and outcomes of `next` and `send` operations. - Handle errors gracefully by raising appropriate Python exceptions. 2. **AsyncCustomIterator Class**: - Implement the `__aiter__()` and `__anext__()` methods to follow the async iterator protocol. - Implement an asynchronous method `send(value)` using `async def`. - Track the state and outcomes of `anext` and `send` operations. - Handle errors gracefully in an async context. Input and Output: - **Inputs**: No direct user input. - **Outputs**: Returns generated values when iterated or sent values, raises exceptions on errors. Constraints: - Your implementation should work with Python 3.10 and its iterator enhancements. - Ensure thread safety for both synchronous and asynchronous versions. Example Usage: ```python # Synchronous it = CustomIterator() for value in it: print(value) if some_condition: it.send(new_value) # Asynchronous import asyncio async def main(): ait = AsyncCustomIterator() async for value in ait: print(value) if some_condition: await ait.send(new_value) asyncio.run(main()) ``` Notes: - Your implementation does not need to precisely follow the C API but should simulate similar behavior. - Include error handling for edge cases such as invalid operations or types. Performance: - Ensure efficient value handling and state management. - Handle potential concurrency issues in the async part of the task.","solution":"class CustomIterator: def __init__(self): self.value = 0 def __iter__(self): return self def __next__(self): if self.value >= 10: raise StopIteration current = self.value self.value += 1 return current def send(self, value): if not isinstance(value, int): raise ValueError(\\"Only integer values can be sent to the iterator.\\") self.value = value import asyncio class AsyncCustomIterator: def __init__(self): self.value = 0 async def __aiter__(self): return self async def __anext__(self): if self.value >= 10: raise StopAsyncIteration current = self.value self.value += 1 await asyncio.sleep(0) # To simulate async behavior return current async def send(self, value): if not isinstance(value, int): raise ValueError(\\"Only integer values can be sent to the iterator.\\") self.value = value"},{"question":"You are tasked with implementing a function using the \\"linecache\\" module to retrieve specific lines from multiple files efficiently. You will also need to handle cases where the lines might have changed since they were last read. Your function should demonstrate a deep understanding of the \\"linecache\\" module\'s capabilities, including cache management and error handling. Function Signature ```python def retrieve_lines(filenames: List[str], line_numbers: List[int]) -> Dict[str, List[str]]: pass ``` Input - `filenames`: A list of strings where each string represents a file path. - `line_numbers`: A list of integers where each integer represents a line number to retrieve from each file. Output - Returns a dictionary where each key is a filename from the input list, and the corresponding value is a list of strings. Each string in this list corresponds to the line content retrieved from that file at the specified line numbers. If a line number is invalid (e.g., exceeds the number of lines in the file), it should be represented as an empty string in the list. Constraints - File paths will always be valid, but the files may not always exist. - The function should use the caching mechanism of the \\"linecache\\" module to optimize performance. - Lines in the file may change; ensure to use the `checkcache()` method appropriately to validate cache entries. - Optimize for the common case where the same file might frequently be accessed multiple times. Example ```python filenames = [\'file1.txt\', \'file2.txt\'] line_numbers = [1, 5] result = retrieve_lines(filenames, line_numbers) print(result) # Expected output (suppose \'file1.txt\' and \'file2.txt\' have the following content): # file1.txt: file2.txt: # line 1 line 1 # line 2 line 2 # line 3 line 3 # line 4 line 4 # line 5 line 5 # { # \'file1.txt\': [\'line 1n\', \'line 5n\'], # \'file2.txt\': [\'line 1n\', \'\'] # } ``` Note - Ensure that your solution efficiently utilizes the \\"linecache\\" module and include appropriate error handling. - Remember to clear the cache when appropriate to avoid stale data. - Use the `lazycache()` method when dealing with non-file-based modules if required (although this problem primarily involves file-based access).","solution":"from typing import List, Dict import linecache def retrieve_lines(filenames: List[str], line_numbers: List[int]) -> Dict[str, List[str]]: result = {} for file in filenames: # Check the cache to make sure its entries are up-to-date linecache.checkcache(file) # Retrieve lines as requested lines = [] for num in line_numbers: line = linecache.getline(file, num) if line == \'\': lines.append(\'\') else: lines.append(line.strip(\'n\')) # Removing newline character for clean output result[file] = lines return result"},{"question":"# Support Vector Machines (SVM) for Classification and Hyperparameter Tuning In this assessment, you will be using scikit-learn\'s SVM module to perform binary and multi-class classification on a given dataset. The task is divided into several parts to test your understanding of SVM implementation, model evaluation, and hyperparameter tuning. Part 1: Loading Data Load the `digits` dataset from `sklearn.datasets`. This dataset contains 1797 samples of 8x8 images of handwritten digits. Part 2: Binary Classification 1. **Select a pair of classes** from the dataset for binary classification. For example, choose digits 0 and 1. 2. **Split the dataset** into training (80%) and testing (20%) sets using `train_test_split`. Part 3: Model Building and Training 1. **Build and train** an `SVC` model with an RBF kernel. 2. **Evaluate** the model\'s performance using accuracy on the test set. 3. **Identify the support vectors** and **calculate their count** for each class. Part 4: Multi-Class Classification 1. **Train a new SVC model** on the entire dataset (all digit classes). 2. **Evaluate** the multi-class classification performance by calculating accuracy on the test set. Part 5: Hyperparameter Tuning 1. **Use GridSearchCV** to find the optimal hyperparameters for the SVM model. Specifically, tune the `C` and `gamma` parameters for an RBF kernel. 2. **Report** the best combination of `C` and `gamma` and the corresponding accuracy. Part 6: Interpretation and Comparison 1. **Compare** the performance of the tuned model with the initial models. 2. **Discuss** the impact of hyperparameter tuning on the model performance. # Submission Requirements - Python code for each part of the task. - Inline comments and brief explanations to describe your approach. - Output of each part including model evaluations and hyperparameter tuning results. # Constraints - Use random_state=42 wherever randomness is involved for consistency. - Performance should be evaluated using accuracy. # Example Code Template ```python # Part 1: Loading Data from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV # Load dataset digits = load_digits() # Part 2: Binary Classification # Select digits 0 and 1 X = digits.data[digits.target <= 1] y = digits.target[digits.target <= 1] # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Part 3: Model Building and Training # Initialize SVC model with RBF kernel svc = SVC(kernel=\'rbf\', random_state=42) svc.fit(X_train, y_train) y_pred = svc.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Binary Classification Accuracy: {accuracy}\') # Support vectors support_vectors = svc.support_vectors_ n_support_vectors = svc.n_support_ print(f\'Number of support vectors: {n_support_vectors}\') # Part 4: Multi-Class Classification X = digits.data y = digits.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize SVC model with RBF kernel svc_multi = SVC(kernel=\'rbf\', random_state=42) svc_multi.fit(X_train, y_train) y_pred_multi = svc_multi.predict(X_test) accuracy_multi = accuracy_score(y_test, y_pred_multi) print(f\'Multi-Class Classification Accuracy: {accuracy_multi}\') # Part 5: Hyperparameter Tuning param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1e-3, 1e-4, 1e-5]} grid_search = GridSearchCV(SVC(kernel=\'rbf\', random_state=42), param_grid, cv=5) grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ best_estimator = grid_search.best_estimator_ accuracy_tuned = accuracy_score(y_test, best_estimator.predict(X_test)) print(f\'Best Params: {best_params}\') print(f\'Tuned Model Accuracy: {accuracy_tuned}\') # Part 6: Interpretation and Comparison # (Write your analysis here) ``` Make sure your implementation follows good coding practices and includes appropriate comments and documentation.","solution":"# Solution Code # Part 1: Loading Data from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV # Load dataset digits = load_digits() # Part 2: Binary Classification # Select digits 0 and 1 mask = (digits.target == 0) | (digits.target == 1) X_binary = digits.data[mask] y_binary = digits.target[mask] # Split dataset X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42) # Part 3: Model Building and Training for Binary Classification svc_binary = SVC(kernel=\'rbf\', random_state=42) svc_binary.fit(X_train_binary, y_train_binary) y_pred_binary = svc_binary.predict(X_test_binary) accuracy_binary = accuracy_score(y_test_binary, y_pred_binary) # Support vectors support_vectors_binary = svc_binary.support_vectors_ n_support_vectors_binary = svc_binary.n_support_ # Part 4: Multi-Class Classification X = digits.data y = digits.target # Split dataset X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize SVC model with RBF kernel svc_multi = SVC(kernel=\'rbf\', random_state=42) svc_multi.fit(X_train_multi, y_train_multi) y_pred_multi = svc_multi.predict(X_test_multi) accuracy_multi = accuracy_score(y_test_multi, y_pred_multi) # Part 5: Hyperparameter Tuning param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1e-3, 1e-4, 1e-5]} grid_search = GridSearchCV(SVC(kernel=\'rbf\', random_state=42), param_grid, cv=5) grid_search.fit(X_train_multi, y_train_multi) best_params = grid_search.best_params_ best_estimator = grid_search.best_estimator_ accuracy_tuned = accuracy_score(y_test_multi, best_estimator.predict(X_test_multi)) # Output results print(f\'Binary Classification Accuracy: {accuracy_binary}\') print(f\'Number of support vectors (Binary Classification): {n_support_vectors_binary}\') print(f\'Multi-Class Classification Accuracy: {accuracy_multi}\') print(f\'Best Params (Multi-Class): {best_params}\') print(f\'Tuned Model Accuracy (Multi-Class): {accuracy_tuned}\')"},{"question":"**Title**: Implement a Custom Neural Network Layer with TorchScript **Background**: In PyTorch, to define a neural network, we usually create a class that inherits from `nn.Module` and implement the `forward` method. TorchScript allows us to serialize and optimize these models. However, TorchScript has some limitations and doesn\'t support all Python features. **Task**: You are required to implement a custom neural network layer in PyTorch that can be converted to TorchScript. Specifically, you need to: 1. Create a class `CustomLayer` that inherits from `nn.Module`. 2. Implement the `forward` method which performs the custom operation. 3. Ensure your implementation works when scripted with TorchScript. **Custom Operation**: The custom layer should perform a simple element-wise operation on the input tensor `x`. The operation should apply a formula `y = x * x + 1` to each element of the tensor. **Requirements**: 1. Your solution must include the definition of the `CustomLayer` class. 2. The `forward` method should include an example input tensor and demonstrate the custom operation. 3. Script the `CustomLayer` using TorchScript and demonstrate it works correctly with an example tensor. **Implementation Constraints**: - You can use only the features mentioned as supported in the provided document. - Ensure your code is TorchScript compatible. **Input and Output**: - Assume the input tensor is a 2D tensor of shape (n, m) containing only floating-point numbers. - Output should be the transformed tensor of the same shape. **Example**: ```python import torch import torch.nn as nn import torch.jit as jit class CustomLayer(nn.Module): def __init__(self): super(CustomLayer, self).__init__() def forward(self, x): return x * x + 1 # Example usage layer = CustomLayer() x = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) y = layer(x) print(y) # Output should be a tensor with values [[2.0, 5.0], [10.0, 17.0]] # Scripting with TorchScript scripted_layer = jit.script(CustomLayer()) y_scripted = scripted_layer(x) print(y_scripted) # Output should be the same as above ``` **Validation**: Make sure the output tensor from both the normal custom layer and the scripted layer are the same and adhere to the custom operation specified.","solution":"import torch import torch.nn as nn import torch.jit as jit class CustomLayer(nn.Module): def __init__(self): super(CustomLayer, self).__init__() def forward(self, x): return x * x + 1 # Example usage layer = CustomLayer() x = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) y = layer(x) print(y) # Output should be a tensor with values [[2.0, 5.0], [10.0, 17.0]] # Scripting with TorchScript scripted_layer = jit.script(CustomLayer()) y_scripted = scripted_layer(x) print(y_scripted) # Output should be the same as above"},{"question":"**Configuration File Manipulation and Interpolation Handling** # Problem Statement You are provided with a configuration INI file named `settings.ini`. This file contains various sections and settings related to an application. Your task is to write a Python function that performs the following operations using the `configparser` module: 1. **Read** the `settings.ini` configuration file. 2. **Add or Update** a section named `[database]` with the following options: - `host = localhost` - `port = 5432` - `username` (value should be fetched from a section `[credentials]` key `db_user` using interpolation) - `password` (value should be fetched from a section `[credentials]` key `db_pass` using interpolation) 3. **Remove** any option named `temporary` from the section `[server]` if it exists. 4. **Fetch** the value of the `mode` option from the section `[runtime]` and return it. 5. **Write** the updated configuration to a new file named `updated_settings.ini`. # Input - The `settings.ini` configuration file. # Output - The content of the new configuration file `updated_settings.ini` should reflect the modifications made. - The function should return the value of the `mode` option from the section `[runtime]`. # Constraints 1. The `settings.ini` file is guaranteed to have at least `[credentials]` and `[runtime]` sections. 2. The `[credentials]` section must have `db_user` and `db_pass` keys. # Example Given the following `settings.ini` file: ```ini [credentials] db_user = admin db_pass = secretpass [runtime] mode = production [server] temporary = true ``` Your function should create `updated_settings.ini` with the following content: ```ini [credentials] db_user = admin db_pass = secretpass [runtime] mode = production [server] [database] host = localhost port = 5432 username = admin password = secretpass ``` And the function should return: ``` \'production\' ``` # Function Signature ```python import configparser def manipulate_config_file(file_path: str) -> str: pass ``` # Note - Use `configparser` methods such as `read()`, `set()`, `remove_option()`, and `write()` to achieve the task. - Use interpolation for retrieving `username` and `password`. - Ensure you handle file reading and writing correctly.","solution":"import configparser def manipulate_config_file(file_path: str) -> str: # Create a ConfigParser instance config = configparser.ConfigParser() # Read the provided configuration file config.read(file_path) # Add or update the [database] section with specified options if \'database\' not in config.sections(): config.add_section(\'database\') config.set(\'database\', \'host\', \'localhost\') config.set(\'database\', \'port\', \'5432\') config.set(\'database\', \'username\', config.get(\'credentials\', \'db_user\')) config.set(\'database\', \'password\', config.get(\'credentials\', \'db_pass\')) # Remove any [server] section option named \'temporary\', if it exists if \'server\' in config and \'temporary\' in config[\'server\']: config.remove_option(\'server\', \'temporary\') # Fetch the value of the \'mode\' option from the [runtime] section and return it mode_value = config.get(\'runtime\', \'mode\') # Write the updated configuration to a new file with open(\'updated_settings.ini\', \'w\') as updated_file: config.write(updated_file) return mode_value"},{"question":"You are provided with an example dataset and your task is to create a visualization that conveys complex information effectively using seaborn. The dataset you will use is the \\"tips\\" dataset from seaborn\'s built-in datasets. # Description: 1. **Load the \\"tips\\" dataset.** 2. **Generate a scatter plot**: - Set \\"total_bill\\" as the x-axis and \\"tip\\" as the y-axis. - Use: - The `hue` parameter to differentiate data points based on the \\"day\\" column. - The `style` parameter to differentiate data points based on the \\"time\\" column. - The `size` parameter to set the size of the data points based on the \\"size\\" column. - Apply a custom color palette: `[\\"#4c72b0\\", \\"#55a868\\", \\"#c44e52\\", \\"#8172b3\\"]` to the `hue` parameter. - Use custom markers for the `style` parameter: square (\'s\') for \\"Lunch\\" and \'X\' for \\"Dinner\\". - Set the size range of data points to (30, 300). 3. **Generate a grid of scatter plots**: - Use `relplot` to create a grid of scatter plots divided by the \\"sex\\" column. - In each subplot, display the scatter plots with the same configurations as above. # Input: - None # Output: 1. A seaborn scatter plot with the described customizations. 2. A grid of scatter plots with the described customizations faceted by \\"sex\\". # Constraints: - You must use seaborn for plotting. - Ensure the legend is fully descriptive of all encodings (hue, style, size). # Code Solution: ```python import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Define custom palette and markers custom_palette = [\\"#4c72b0\\", \\"#55a868\\", \\"#c44e52\\", \\"#8172b3\\"] custom_markers = {\\"Lunch\\": \\"s\\", \\"Dinner\\": \\"X\\"} # Create first scatter plot with customizations plt.figure(figsize=(10, 6)) sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", palette=custom_palette, markers=custom_markers, sizes=(30, 300), legend=\\"full\\" ) plt.title(\\"Scatter Plot of tips\\") plt.show() # Create a grid of scatter plots using relplot g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", palette=custom_palette, markers=custom_markers, sizes=(30, 300), col=\\"sex\\", kind=\\"scatter\\", legend=\\"full\\" ) g.fig.suptitle(\\"Grid of Scatter Plots by Sex\\", y=1.02) plt.show() create_plots() ``` # Submission Requirements: - Implement the function as specified. - Ensure the plots are displayed correctly with the required customizations and faceting.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Define custom palette and markers custom_palette = [\\"#4c72b0\\", \\"#55a868\\", \\"#c44e52\\", \\"#8172b3\\"] custom_markers = {\\"Lunch\\": \\"s\\", \\"Dinner\\": \\"X\\"} # Create first scatter plot with customizations plt.figure(figsize=(10, 6)) sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", palette=custom_palette, markers=custom_markers, sizes=(30, 300), legend=\\"full\\" ) plt.title(\\"Scatter Plot of Tips\\") plt.show() # Create a grid of scatter plots using relplot g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", palette=custom_palette, markers=custom_markers, sizes=(30, 300), col=\\"sex\\", kind=\\"scatter\\", legend=\\"full\\" ) g.fig.suptitle(\\"Grid of Scatter Plots by Sex\\", y=1.02) plt.show()"},{"question":"# Question: Prime Number Computation using multiprocessing You are to implement a Python program to find all prime numbers within a given range using process-based parallelism with the `multiprocessing` module. Requirements 1. **Function `is_prime(n)`**: - **Input**: An integer `n`. - **Output**: `True` if `n` is a prime number, otherwise `False`. 2. **Function `find_primes_in_range(start, end, process_count)`**: - **Input**: - `start`: An integer representing the start of the range (inclusive). - `end`: An integer representing the end of the range (inclusive). - `process_count`: The number of worker processes to use. - **Output**: A list of prime numbers within the range `[start, end]`. - **Constraints**: - `start` and `end` will be positive integers with `start <= end`. - The maximum value for `end` will be 10^6. - The `process_count` will be between 1 and 16. - **Performance Requirement**: The function should efficiently utilize the specified number of worker processes to distribute the task of checking the primality of numbers. Constraints - You must use `multiprocessing.Pool` to manage the worker processes. - Ensure proper synchronization if needed to avoid race conditions. - Use `Queue` or `Pipe` for inter-process communication if required. Example Usage ```python def is_prime(n): # Implement this function pass def find_primes_in_range(start, end, process_count): # Implement this function pass if __name__ == \\"__main__\\": primes = find_primes_in_range(1, 100, 4) print(primes) # Example Output: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97] ```","solution":"import math from multiprocessing import Pool def is_prime(n): Returns True if n is a prime number, otherwise False. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def find_primes_in_range(start, end, process_count): Returns a list of prime numbers in the range [start, end] using the specified number of worker processes. with Pool(process_count) as pool: # Create a list of numbers in the range [start, end] numbers = list(range(start, end + 1)) # Map the numbers to the is_prime function using the pool of workers prime_flags = pool.map(is_prime, numbers) # Filter the numbers based on the prime_flags primes = [num for num, is_prime_flag in zip(numbers, prime_flags) if is_prime_flag] return primes"},{"question":"**Objective**: Assess the ability to work with the `http.cookies` module in Python for managing HTTP cookies. **Problem Statement**: In this exercise, you will implement a function `parse_and_extend_cookies(cookie_string, additional_cookies)` that parses a cookie string to load existing cookies, adds new cookies from a dictionary, and then returns a formatted string of all cookies ready to be used in an HTTP header. **Function Signature**: ```python def parse_and_extend_cookies(cookie_string: str, additional_cookies: dict) -> str: pass ``` # Detailed Requirements: 1. **Input**: - `cookie_string`: A string containing cookies in HTTP header format. For example, `\\"user=JohnDoe; sessionid=abc123\\"`. - `additional_cookies`: A dictionary containing additional cookies to be added. For example, `{\\"cart\\": \\"5 items\\", \\"theme\\": \\"light\\"}`. 2. **Output**: - A single string suitable for an HTTP header, containing all cookies from the `cookie_string` and `additional_cookies`. 3. **Constraints**: - Your function should handle invalid cookie strings by ignoring them and continuing to parse valid entries. - Special characters in cookie values should be properly handled as detailed in the `http.cookies` documentation. - Cookies in the output string should be separated by the `;` character. 4. **Performance**: - The function should efficiently handle large input strings and dictionaries. # Examples: ```python # Example 1 cookie_string = \\"user=JohnDoe; sessionid=abc123\\" additional_cookies = {\\"cart\\": \\"5 items\\", \\"theme\\": \\"light\\"} print(parse_and_extend_cookies(cookie_string, additional_cookies)) # Expected Output: \\"Set-Cookie: user=JohnDoe; sessionid=abc123; cart=5 items; theme=light\\" # Example 2 cookie_string = \\"flavor=chocolate; color=blue\\" additional_cookies = {\\"size\\": \\"large\\", \\"shop\\": \\"online\\"} print(parse_and_extend_cookies(cookie_string, additional_cookies)) # Expected Output: \\"Set-Cookie: flavor=chocolate; color=blue; size=large; shop=online\\" ``` **Notes**: - Use the `http.cookies.SimpleCookie` class for managing cookies while implementing the function. - Handle unexpected errors by catching exceptions and ensuring that the function returns valid cookies only. - Write clear and concise docstrings within your code.","solution":"from http.cookies import SimpleCookie def parse_and_extend_cookies(cookie_string: str, additional_cookies: dict) -> str: Parses an existing cookie string and extends it with additional cookies provided in a dictionary. Args: cookie_string (str): The initial cookie string. additional_cookies (dict): A dictionary of additional cookies to be added. Returns: str: A formatted string of all cookies suitable for an HTTP header. cookie = SimpleCookie() # Load initial cookies cookie.load(cookie_string) # Add additional cookies for key, value in additional_cookies.items(): cookie[key] = value # Format cookies into a single output string cookie_header = \\"Set-Cookie: \\" + \\"; \\".join([f\\"{key}={cookie[key].value}\\" for key in cookie]) return cookie_header"},{"question":"# XML Parsing with Custom SAX ContentHandler You are tasked with creating a Python program to parse an XML document using the `xml.sax` module. Specifically, you will implement a custom `ContentHandler` to extract specific data from an XML document. Your focus is to handle the start and end of elements, and the character data within those elements. Requirements: 1. Implement a custom `ContentHandler` class that: - Extracts text data from elements named `title`, `author`, and `content`. - Collects this data in a structured dictionary as follows: ```python { \\"titles\\": [list_of_titles], \\"authors\\": [list_of_authors], \\"contents\\": [list_of_contents] } ``` - Ensure the data is stored in the respective lists, maintaining the order in which they appear in the XML document. 2. Handle cases where the text might be split across multiple calls to the `characters` method. 3. Use an XML string as input for your program; do not read from or write to any files. Constraints: - Use the `xml.sax` module; do not use any other XML parsing libraries. - The XML structure is known, and you can assume that the elements `title`, `author`, and `content` are always present and correctly nested. - Manage the character data efficiently to handle possible large text nodes. Example XML Input: ```xml <document> <title>Python Programming</title> <author>John Doe</author> <content>Python is an interpreted, high-level and general-purpose programming language.</content> <title>Advanced Python</title> <author>Jane Smith</author> <content>Advanced Python covers more sophisticated concepts and techniques.</content> </document> ``` Expected Output: ```python { \\"titles\\": [\\"Python Programming\\", \\"Advanced Python\\"], \\"authors\\": [\\"John Doe\\", \\"Jane Smith\\"], \\"contents\\": [\\"Python is an interpreted, high-level and general-purpose programming language.\\", \\"Advanced Python covers more sophisticated concepts and techniques.\\"] } ``` Performance Requirements: - The solution should process the XML document efficiently in a single pass. - Consider the performance implications of handling large XML documents with significant text content. Function Signature: ```python def parse_xml(xml_string: str) -> dict: pass ``` Implement the `parse_xml` function to achieve the required functionality.","solution":"import xml.sax class CustomContentHandler(xml.sax.ContentHandler): def __init__(self): super().__init__() self.current_data = \\"\\" self.titles = [] self.authors = [] self.contents = [] self.buffer = \\"\\" def startElement(self, name, attrs): self.current_data = name self.buffer = \\"\\" def endElement(self, name): if self.current_data == \\"title\\": self.titles.append(self.buffer.strip()) elif self.current_data == \\"author\\": self.authors.append(self.buffer.strip()) elif self.current_data == \\"content\\": self.contents.append(self.buffer.strip()) self.current_data = \\"\\" self.buffer = \\"\\" def characters(self, content): if self.current_data in [\\"title\\", \\"author\\", \\"content\\"]: self.buffer += content def parse_xml(xml_string: str) -> dict: handler = CustomContentHandler() xml.sax.parseString(xml_string, handler) return { \\"titles\\": handler.titles, \\"authors\\": handler.authors, \\"contents\\": handler.contents }"},{"question":"**Asyncio Debugging and Task Management** Asynchronous programming can be challenging, especially when it comes to handling tasks, debugging, and ensuring that tasks are executed as expected. The asyncio module provides tools to help with these tasks. In this exercise, we will focus on writing an asyncio-based program that demonstrates proper use of tasks, debugging, and handling potential issues. # Task Your objective is to create a Python script that: 1. **Creates multiple coroutine functions** that simulate asynchronous tasks. These tasks should include: - A task that completes successfully. - A task that raises an exception. - A task that takes a significant amount of time to execute. 2. **Manages these tasks using an event loop** and ensures that: - All tasks are properly created and awaited. - Exceptions from tasks are properly handled. 3. **Enables debug mode** to catch common mistakes such as unawaited coroutines or never-retrieved exceptions. 4. **Logs important events and debugging information**, using the logging module configured for debug level. 5. **Uses `run_in_executor`** to run a blocking CPU-bound task. # Requirements 1. **Coroutine Functions**: - `async def successful_task()`: A simple coroutine that sleeps for 1 second and then returns a success message. - `async def failing_task()`: A coroutine that raises an Exception after sleeping for 1 second. - `async def long_task()`: A coroutine that sleeps for 10 seconds (to simulate a long-running task) and then returns a completion message. 2. **Main Function**: - Create a main coroutine function that orchestrates running these tasks concurrently using `asyncio.create_task()`. - Ensure that exceptions from `failing_task` are captured in the main function. - Use the `run_in_executor` to execute a CPU-bound function, which simply calculates the sum of numbers from 1 to 1000000. 3. **Logging Configuration**: - Set up the logging configuration to enable debug-level logging. - Ensure that the debug mode is enabled for asyncio. ```python import asyncio import logging async def successful_task(): await asyncio.sleep(1) return \\"Task Completed Successfully\\" async def failing_task(): await asyncio.sleep(1) raise Exception(\\"Task Failed\\") async def long_task(): await asyncio.sleep(10) return \\"Long Task Completed\\" def cpu_bound_task(): return sum(range(1, 1000001)) async def main(): # Set up logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") logger.setLevel(logging.DEBUG) asyncio.get_running_loop().set_debug(True) # Create tasks tasks = [ asyncio.create_task(successful_task()), asyncio.create_task(failing_task()), asyncio.create_task(long_task()) ] # Run in executor loop = asyncio.get_running_loop() executor = None cpu_task = loop.run_in_executor(executor, cpu_bound_task) for task in tasks: try: result = await task print(result) except Exception as e: logging.error(f\\"Exception in task: {e}\\") cpu_result = await cpu_task print(f\\"CPU bound task result: {cpu_result}\\") if __name__ == \\"__main__\\": asyncio.run(main(), debug=True) ``` # Input - The script does not take any user input. # Output - The script prints the results of the tasks, handles exceptions and logs relevant debug information. # Constraints - Ensure that all tasks are awaited properly and exceptions are handled. - Enable asyncio debug mode and configure logging to capture and display debug messages. # Performance Requirements - The script must handle long-running tasks effectively without blocking the event loop. - Utilize `run_in_executor` to offload CPU-bound tasks to a separate thread.","solution":"import asyncio import logging async def successful_task(): await asyncio.sleep(1) return \\"Task Completed Successfully\\" async def failing_task(): await asyncio.sleep(1) raise Exception(\\"Task Failed\\") async def long_task(): await asyncio.sleep(10) return \\"Long Task Completed\\" def cpu_bound_task(): return sum(range(1, 1000001)) async def main(): # Set up logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") logger.setLevel(logging.DEBUG) asyncio.get_running_loop().set_debug(True) # Create tasks tasks = [ asyncio.create_task(successful_task()), asyncio.create_task(failing_task()), asyncio.create_task(long_task()) ] # Run in executor loop = asyncio.get_running_loop() executor = None cpu_task = loop.run_in_executor(executor, cpu_bound_task) for task in tasks: try: result = await task print(result) except Exception as e: logging.error(f\\"Exception in task: {e}\\") cpu_result = await cpu_task print(f\\"CPU bound task result: {cpu_result}\\") if __name__ == \\"__main__\\": asyncio.run(main(), debug=True)"},{"question":"You are tasked with analyzing a toy dataset using scikit-learn\'s datasets module. Your goal is to load the dataset, extract its components, and perform a simple analysis. Follow the steps below to complete this task: 1. **Load the Iris Dataset**: - Use the `load_iris` function from `sklearn.datasets` to load the Iris dataset. - Extract the features (data) and the target labels (target). 2. **Calculate Basic Statistics**: - Calculate the mean, median, and standard deviation for each feature in the dataset. - Return the results in a dictionary where the keys are the feature names and the values are dictionaries containing the mean, median, and standard deviation for the corresponding feature. 3. **Determine Class Distribution**: - Calculate the number of instances for each class in the target labels. - Return the results as a dictionary where the keys are the class labels and the values are the number of instances for each class. # Input The input to your functions should be the Iris dataset Bunch object. # Output - A dictionary containing the statistics (mean, median, and standard deviation) for each feature. - A dictionary containing the class distribution of the target labels. Example ```python { \'sepal length (cm)\': {\'mean\': 5.84, \'median\': 5.8, \'std_dev\': 0.83}, \'sepal width (cm)\': {\'mean\': 3.05, \'median\': 3.0, \'std_dev\': 0.43}, \'petal length (cm)\': {\'mean\': 3.76, \'median\': 4.35, \'std_dev\': 1.76}, \'petal width (cm)\': {\'mean\': 1.20, \'median\': 1.3, \'std_dev\': 0.76} } ``` ```python { 0: 50, 1: 50, 2: 50 } ``` # Function Signatures ```python def load_and_extract_iris_data() -> Tuple[np.ndarray, np.ndarray]: Load the Iris dataset and extract the features and target labels. Returns: Tuple[np.ndarray, np.ndarray]: a tuple containing the features (data) and target labels (target). pass def calculate_statistics(data: np.ndarray, feature_names: List[str]) -> Dict[str, Dict[str, float]]: Calculate the mean, median, and standard deviation for each feature in the dataset. Args: data (np.ndarray): The feature data. feature_names (List[str]): List of feature names. Returns: Dict[str, Dict[str, float]]: A dictionary containing the statistics for each feature. pass def class_distribution(target: np.ndarray) -> Dict[int, int]: Calculate the number of instances for each class in the target labels. Args: target (np.ndarray): The target labels. Returns: Dict[int, int]: A dictionary containing the class distribution of the target labels. pass ``` # Instructions 1. Implement the `load_and_extract_iris_data` function to load the Iris dataset and extract the data and target labels. 2. Implement the `calculate_statistics` function to calculate the mean, median, and standard deviation for each feature in the dataset. 3. Implement the `class_distribution` function to calculate the class distribution of the target labels. 4. Use appropriate libraries (like NumPy) for numerical operations. 5. Ensure your code is well-documented and follows best practices.","solution":"from typing import Tuple, Dict, List import numpy as np from sklearn.datasets import load_iris def load_and_extract_iris_data() -> Tuple[np.ndarray, np.ndarray, List[str]]: Load the Iris dataset and extract the features, target labels and feature names. Returns: Tuple[np.ndarray, np.ndarray, List[str]]: a tuple containing the features (data), target labels (target), and feature names. iris = load_iris() data = iris.data target = iris.target feature_names = iris.feature_names return data, target, feature_names def calculate_statistics(data: np.ndarray, feature_names: List[str]) -> Dict[str, Dict[str, float]]: Calculate the mean, median, and standard deviation for each feature in the dataset. Args: data (np.ndarray): The feature data. feature_names (List[str]): List of feature names. Returns: Dict[str, Dict[str, float]]: A dictionary containing the statistics for each feature. statistics = {} for idx, feature in enumerate(feature_names): feature_data = data[:, idx] statistics[feature] = { \'mean\': np.mean(feature_data), \'median\': np.median(feature_data), \'std_dev\': np.std(feature_data) } return statistics def class_distribution(target: np.ndarray) -> Dict[int, int]: Calculate the number of instances for each class in the target labels. Args: target (np.ndarray): The target labels. Returns: Dict[int, int]: A dictionary containing the class distribution of the target labels. unique, counts = np.unique(target, return_counts=True) return dict(zip(unique, counts))"},{"question":"# Problem: Integer Conversion Utility Objective Implement a Python function that interprets a variety of numeric representations from their string formats into integers. This function should handle different bases, validate the inputs, and manage any errors by raising appropriate exceptions. Function Signature ```python def interpret_number(number_str: str, base: int = 10) -> int: pass ``` Input - `number_str` (str): A string representing the number to be interpreted. - `base` (int): An integer representing the base of the number in `number_str`. Must be between 2 and 36 inclusive, or 0 for automatic base detection following Python\'s integer literal rules. Output - Returns an integer representing the interpreted input string. Constraints - The function must handle inputs with leading and trailing spaces. - The function should ignore single underscores in valid places, matching Python\'s behavior. - If `base` is 0, the function must follow Python\'s integer literal interpretation rules, which include handling hexadecimal (`0x`), octal (`0o`), and binary (`0b`) prefixes. - If `number_str` represents an invalid number for the specified base, the function must raise a `ValueError`. - The function should leverage Python\'s internal capabilities for handling large integers seamlessly. - You are not allowed to use the built-in `int()` function directly to implement this function. Example ```python print(interpret_number(\'1010\', 2)) # 10 print(interpret_number(\' 0xFF \', 0)) # 255 print(interpret_number(\'1234\', 5)) # 194 print(interpret_number(\'100_000_000\', 2)) # 256 print(interpret_number(\'invalid\', 10)) # should raise ValueError ``` Instructions 1. Implement the `interpret_number` function according to the specifications. 2. Ensure that all edge cases and invalid inputs are correctly handled. 3. Test the function against the examples provided to confirm its accuracy.","solution":"def interpret_number(number_str: str, base: int = 10) -> int: Interprets a string as an integer of a specified base. Parameters: - number_str: A string representing the number. - base: The base to use for the number. Must be between 2 and 36 inclusive, or 0 for automatic base detection. Returns: - An integer representing the interpreted input string. Raises: - ValueError: If the number_str is invalid for the specified base. # Strip leading and trailing whitespace number_str = number_str.strip() # Handle automatic base detection if base == 0: if number_str.startswith((\'0b\', \'0B\')): base = 2 elif number_str.startswith((\'0o\', \'0O\')): base = 8 elif number_str.startswith((\'0x\', \'0X\')): base = 16 else: base = 10 # Validate base if not (2 <= base <= 36): raise ValueError(\\"Base must be between 2 and 36 inclusive, or 0 for automatic detection.\\") # Remove underscore and validate the structure cleaned_str = number_str.replace(\'_\', \'\') # Try to convert the cleaned string to an integer try: return int(cleaned_str, base) except ValueError: raise ValueError(f\\"Invalid number \'{number_str}\' for base {base}\\")"},{"question":"You are given a dataset `seaice` that contains sea ice extent data over various dates. Your task is to use seaborn\'s \\"objects\\" interface to visualize this data. Specifically, you need to create a multi-faceted line plot that shows the sea ice extent over the days of the year across different decades. **Requirements:** 1. Load the `seaice` dataset using seaborn. 2. Create a line plot of sea ice extent (`Extent`) over the day of the year (`Date`). 3. Facet the plot by decades (e.g., 1960s, 1970s, etc.). 4. Customize aesthetics to improve the readability and informativity of the plot: - Set the linewidth of the lines to 1 and color to a shade that makes the plot easy to interpret. - Use a suitable facet size that ensures the plot is not too crowded nor too sparse. - Add a title to each facet indicating the decade it represents. **Input:** - The `seaice` dataset, where `Date` is a pandas datetime object and `Extent` is a float. **Output:** - A seaborn line plot with the aforementioned customization. **Constraints:** - Ensure that the plot works correctly across different versions of seaborn that support the \\"objects\\" interface. - The plot should clearly depict trends in sea ice extent across different decades. **Hints:** - You might find it useful to utilize datetime properties such as `.dt.day_of_year` and `.dt.year` for extracting parts of the date. **Example Code Snippet:** To help you get started, here is a basic code snippet: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset seaice = load_dataset(\\"seaice\\") # Create a line plot ( so.Plot(seaice, x=seaice[\\"Date\\"].dt.day_of_year, y=seaice[\\"Extent\\"]) .facet(col=seaice[\\"Date\\"].dt.year // 10 * 10) # Facet by decades .add(so.Lines(linewidth=1, color=\\"blue\\")) .layout(size=(8, 4)) # Set facet size .label(title=\\"{}s\\".format) # Title for each facet ) ``` **Your task is to complete this code snippet and ensure it meets all the requirements.**","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd from seaborn import load_dataset def visualize_seaice(): # Load dataset seaice = load_dataset(\\"seaice\\") # Create new columns for day_of_year and decade seaice[\'day_of_year\'] = seaice[\'Date\'].dt.day_of_year seaice[\'decade\'] = seaice[\'Date\'].dt.year // 10 * 10 # Create the plot plot = ( so.Plot(seaice, x=\\"day_of_year\\", y=\\"Extent\\") .facet(col=\\"decade\\") .add(so.Line(linewidth=1, color=\\"blue\\")) .layout(size=(8, 4)) .label(title=lambda decade: \\"{}s\\".format(decade)) ) plot.show()"},{"question":"# Base64 Encoding and Decoding with Custom Alphabet You are required to implement two functions: `encode_custom_base64` and `decode_custom_base64`. These functions will use Base64 encoding and decoding but allow a custom alphabet for the characters typically represented by `+` and `/` in the standard Base64. Specifications 1. **encode_custom_base64(data: bytes, altchars: bytes) -> bytes** - This function takes a `bytes`-like object `data` and a `bytes`-like object `altchars` of length 2. It should encode the `data` using Base64 encoding, but replace `+` and `/` with the characters provided in `altchars`. - If `altchars` is not exactly 2 bytes in length, raise a `ValueError`. 2. **decode_custom_base64(encoded_data: str, altchars: str) -> bytes** - This function takes an encoded `ASCII string` `encoded_data` and a `string` `altchars` of length 2. It should decode the `encoded_data` using Base64 decoding, but treat the characters in `altchars` as the replacements for `+` and `/`. - If `altchars` is not exactly 2 characters in length, raise a `ValueError`. - Raise a `binascii.Error` if the `encoded_data` is incorrectly padded or contains invalid characters. Input and Output Formats - Input for `encode_custom_base64(data: bytes, altchars: bytes) -> bytes` - `data`: A `bytes`-like object containing the binary data to encode. - `altchars`: A `bytes`-like object of length 2, containing the custom characters to replace `+` and `/`. - Output for `encode_custom_base64` - Returns the Base64 encoded `bytes` with the custom characters substituted. - Input for `decode_custom_base64(encoded_data: str, altchars: str) -> bytes` - `encoded_data`: An `ASCII string` containing the Base64 encoded data. - `altchars`: A `string` of length 2, containing the custom characters to replace `+` and `/`. - Output for `decode_custom_base64` - Returns the decoded `bytes`. Example ```python import base64 def encode_custom_base64(data: bytes, altchars: bytes) -> bytes: if len(altchars) != 2: raise ValueError(\\"altchars must be exactly 2 bytes long.\\") return base64.b64encode(data, altchars=altchars) def decode_custom_base64(encoded_data: str, altchars: str) -> bytes: if len(altchars) != 2: raise ValueError(\\"altchars must be exactly 2 characters long.\\") return base64.b64decode(encoded_data, altchars=altchars.encode()) # Example Usage data = b\\"Hello, World!\\" altchars = b\\"*@\\" encoded = encode_custom_base64(data, altchars) # Encodes with custom alphabet print(encoded) # Output will be the Base64 encoded data with * and @ as replacements decoded = decode_custom_base64(encoded.decode(), altchars.decode()) # Decodes with custom alphabet print(decoded) # Output will be b\'Hello, World!\' ``` Implement the functions `encode_custom_base64` and `decode_custom_base64` to pass the above example and similar test cases.","solution":"import base64 import binascii def encode_custom_base64(data: bytes, altchars: bytes) -> bytes: if len(altchars) != 2: raise ValueError(\\"altchars must be exactly 2 bytes long.\\") return base64.b64encode(data).translate(bytes.maketrans(b\\"+/\\", altchars)) def decode_custom_base64(encoded_data: str, altchars: str) -> bytes: if len(altchars) != 2: raise ValueError(\\"altchars must be exactly 2 characters long.\\") reversed_altchars = altchars.encode().translate(bytes.maketrans(altchars.encode(), b\\"+/\\")) try: return base64.b64decode(encoded_data.translate(str.maketrans(altchars, \\"+/\\"))) except (binascii.Error, ValueError) as e: raise binascii.Error(\\"Incorrectly padded or invalid characters in encoded_data\\") from e"},{"question":"Coding Assessment Question **Objective:** Create a Python function that fetches data from a given URL, handles potential HTTP errors, and returns specific information based on the response. # Instructions: You are required to implement a function named `fetch_data_from_url` that accepts the following parameters: - `url` (str): The URL to fetch data from. - `method` (str): The HTTP method to use (\'GET\' or \'POST\'). Default is \'GET\'. - `data` (dict, optional): The data to send in the request when using the \'POST\' method. Default is `None`. - `headers` (dict, optional): Additional headers to send with the request. Default is `None`. The function should: 1. Fetch data from the given URL using the specified HTTP method (\'GET\' or \'POST\'). 2. If the HTTP method is \'POST\' and `data` is provided, encode the data using `urllib.parse.urlencode` and send it in the request body. 3. Include any additional headers provided in the request. 4. Handle the possible exceptions raised by `urllib.request`: `URLError` and `HTTPError`. 5. Return the following based on the HTTP response: - If successful (status codes 200-299), return the fetched data as a string. - If an HTTP error occurs (status codes 400-599), return a string with the format `\'HTTPError: {error_code}\'`. # Additional Constraints: 1. Use the `urllib.request` and `urllib.parse` modules provided in the standard Python library. 2. Make sure to handle edge cases such as missing URL, invalid URL, and network issues gracefully using appropriate exception handling. # Example Usage: ```python def fetch_data_from_url(url, method=\'GET\', data=None, headers=None): # Your implementation here # Example 1: GET request result = fetch_data_from_url(\'http://httpbin.org/get\') print(result) # Example 2: POST request with data and headers post_data = {\'name\': \'John\', \'age\': \'30\'} headers = {\'User-Agent\': \'Mozilla/5.0\'} result = fetch_data_from_url(\'http://httpbin.org/post\', method=\'POST\', data=post_data, headers=headers) print(result) # Example 3: Handle invalid URL result = fetch_data_from_url(\'http://invalid-url.org\') print(result) # Output should be \'URLError: <reason>\' ``` **Notes:** - Ensure your code is well-documented and follows best practices for readability and maintainability. - Avoid using any third-party modules for this task. Good luck!","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError def fetch_data_from_url(url, method=\'GET\', data=None, headers=None): Fetch data from the given URL using the specified HTTP method. Args: - url (str): The URL to fetch data from. - method (str): The HTTP method to use (\'GET\' or \'POST\'). Default is \'GET\'. - data (dict, optional): The data to send in the request when using the \'POST\' method. Default is None. - headers (dict, optional): Additional headers to send with the request. Default is None. Returns: - str: The fetched data as a string if successful. - str: An error message if an HTTP or URL error occurs. try: if method not in [\'GET\', \'POST\']: return \'Invalid HTTP method\' if method == \'POST\' and data: data = urllib.parse.urlencode(data).encode(\'utf-8\') req = urllib.request.Request(url, data=data, method=method) if headers: for key, value in headers.items(): req.add_header(key, value) with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return f\'HTTPError: {e.code}\' except URLError as e: return f\'URLError: {e.reason}\'"},{"question":"# Question In this assignment, you will set up a simple neural network using PyTorch and configure it to train in a distributed manner using the `torch.distributed.fsdp.FullyShardedDataParallel` (FSDP) package. The main goal is to understand and implement distributed training with model sharding and mixed precision in PyTorch. Task 1. **Create the Neural Network Model**: - Define a simple neural network model with at least two layers (e.g., a linear layer followed by a ReLU activation and another linear layer). 2. **Initialize FSDP**: - Wrap your model with `FullyShardedDataParallel` and configure it using a specific sharding strategy (e.g., `ShardingStrategy.HYBRID_SHARD`) and mixed precision settings (`MixedPrecision()`). 3. **Set up Distributed Training**: - Configure the distributed environment using `torch.distributed` (initialize the process group). - Implement a basic training loop to train your model on a dummy dataset. - Optimize the model using gradient descent. 4. **Saving and Loading State Dicts**: - Configure and save the model\'s state dictionary using `FullStateDictConfig()`. - Demonstrate how to load the state dictionary back into the model. Input - Assume a dummy dataset of your choice with at least 100 samples. - Learning rate for optimizer: `0.01` - Number of epochs for training: `10` Output - Print the loss after each epoch. Constraints and Requirements - Ensure proper usage of `FullyShardedDataParallel` and related configurations (mixed precision, sharding strategy). - Implement efficient distributed training to minimize memory usage and optimize computation. Notes - Consider using the `torch.optim.SGD` optimizer. - Use `nn.CrossEntropyLoss` for the loss function if working with a classification task. - Make sure that the code handles any initialization and cleanup required for distributed training. # Solution Template ```python import torch import torch.nn as nn import torch.optim as optim from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, MixedPrecision, ShardingStrategy from torch.distributed import init_process_group, destroy_process_group # Define your neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() # Define layers self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(): init_process_group(backend=\'nccl\') # or \'gloo\' for CPU # Initialize the model and wrap it with FullyShardedDataParallel model = SimpleModel().cuda() fsdp_model = FSDP(model, sharding_strategy=ShardingStrategy.HYBRID_SHARD, mixed_precision=MixedPrecision()) # Create dummy data (100 samples, 10 features each) dummy_data = torch.randn(100, 10).cuda() dummy_labels = torch.randint(0, 2, (100,)).cuda() # Define loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(fsdp_model.parameters(), lr=0.01) # Training loop for epoch in range(10): optimizer.zero_grad() outputs = fsdp_model(dummy_data) loss = criterion(outputs, dummy_labels) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}, Loss: {loss.item()}\\") # Save state dict state_dict = fsdp_model.state_dict_config(state_dict_type=\'full\') torch.save(state_dict, \'model.pth\') # Destroy process group destroy_process_group() if __name__ == \'__main__\': train() ```","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, MixedPrecision, ShardingStrategy from torch.distributed import init_process_group, destroy_process_group # Define your neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() # Define layers self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(rank, world_size): # Initialize the process group init_process_group(backend=\'nccl\', rank=rank, world_size=world_size) # Initialize the model and wrap it with FullyShardedDataParallel model = SimpleModel().to(rank) fsdp_model = FSDP(model, sharding_strategy=ShardingStrategy.HYBRID_SHARD, mixed_precision=MixedPrecision()) # Create dummy data (100 samples, 10 features each) dummy_data = torch.randn(100, 10).to(rank) dummy_labels = torch.randint(0, 2, (100,)).to(rank) # Define loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(fsdp_model.parameters(), lr=0.01) # Training loop for epoch in range(10): optimizer.zero_grad() outputs = fsdp_model(dummy_data) loss = criterion(outputs, dummy_labels) loss.backward() optimizer.step() if rank == 0: print(f\\"Epoch {epoch+1}, Loss: {loss.item()}\\") # Save state dict full_state_dict = fsdp_model.state_dict() torch.save(full_state_dict, \'model.pth\') # Destroy process group destroy_process_group() if __name__ == \'__main__\': import os import torch.multiprocessing as mp world_size = 2 # Number of processes/GPUs for distributed training os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"},{"question":"**Question: Implement LDA and QDA for Classification and Dimensionality Reduction** # Background You are given a dataset that comprises multiple features and labels. You are required to implement Linear Discriminant Analysis (LDA) for both classification and dimensionality reduction. Additionally, you will implement Quadratic Discriminant Analysis (QDA) for classification. # Part 1: Implement LDA for Classification 1. **Function:** `lda_classification` 2. **Input:** - `X_train`: A 2D numpy array of shape (n_samples, n_features) representing the training data. - `y_train`: A 1D numpy array of shape (n_samples,) representing the class labels for the training data. - `X_test`: A 2D numpy array of shape (n_test_samples, n_features) representing the test data. 3. **Output:** - `y_pred`: A 1D numpy array of shape (n_test_samples,) representing the predicted class labels for the test data. # Part 2: Implement QDA for Classification 1. **Function:** `qda_classification` 2. **Input:** - `X_train`: A 2D numpy array of shape (n_samples, n_features) representing the training data. - `y_train`: A 1D numpy array of shape (n_samples,) representing the class labels for the training data. - `X_test`: A 2D numpy array of shape (n_test_samples, n_features) representing the test data. 3. **Output:** - `y_pred`: A 1D numpy array of shape (n_test_samples,) representing the predicted class labels for the test data. # Part 3: Implement LDA for Dimensionality Reduction 1. **Function:** `lda_dimensionality_reduction` 2. **Input:** - `X`: A 2D numpy array of shape (n_samples, n_features) representing the input data. - `y`: A 1D numpy array of shape (n_samples,) representing the class labels. - `n_components`: Integer representing the number of components to keep. 3. **Output:** - `X_reduced`: A 2D numpy array of shape (n_samples, n_components) representing the data projected onto the reduced subspace. # Constraints - You may use Scikit-Learn\'s `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis` classes. - Ensure to handle any potential errors, such as mismatched dimensions between training and test data. # Example Usage: ```python # Define the input data X_train = np.array([[â€¦], [ â€¦], [...]]) # replace with actual data y_train = np.array([...]) # replace with actual data X_test = np.array([[â€¦], [ â€¦], [...]]) # replace with actual data # LDA Classification y_pred_lda = lda_classification(X_train, y_train, X_test) # QDA Classification y_pred_qda = qda_classification(X_train, y_train, X_test) # LDA Dimensionality Reduction X_reduced = lda_dimensionality_reduction(X_train, y_train, n_components=2) ``` Your implementation should demonstrate a clear understanding of LDA and QDA concepts, as well as their practical applications in classification and dimensionality reduction.","solution":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis def lda_classification(X_train, y_train, X_test): Perform LDA classification. :param X_train: 2D numpy array of shape (n_samples, n_features) :param y_train: 1D numpy array of shape (n_samples,) :param X_test: 2D numpy array of shape (n_test_samples, n_features) :return: 1D numpy array of shape (n_test_samples,) with the predicted class labels lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) y_pred = lda.predict(X_test) return y_pred def qda_classification(X_train, y_train, X_test): Perform QDA classification. :param X_train: 2D numpy array of shape (n_samples, n_features) :param y_train: 1D numpy array of shape (n_samples,) :param X_test: 2D numpy array of shape (n_test_samples, n_features) :return: 1D numpy array of shape (n_test_samples,) with the predicted class labels qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred = qda.predict(X_test) return y_pred def lda_dimensionality_reduction(X, y, n_components): Perform LDA dimensionality reduction. :param X: 2D numpy array of shape (n_samples, n_features) :param y: 1D numpy array of shape (n_samples,) :param n_components: Integer, number of components to keep :return: 2D numpy array of shape (n_samples, n_components) with the reduced data lda = LinearDiscriminantAnalysis(n_components=n_components) X_reduced = lda.fit_transform(X, y) return X_reduced"},{"question":"# Advanced Python Coding Assessment: Managing Tasks with Asyncio Queues Objective: Demonstrate your understanding of asyncio queues by implementing a task management system that uses different types of queues (FIFO, Priority, LIFO). The system should handle concurrent tasks with varying priorities and ensure proper processing and completion tracking. Task Description: You are required to implement a task management system using `asyncio` queues. Your task is to create a system with multiple worker coroutines that process tasks concurrently using different types of queues (FIFO, Priority, LIFO). Each worker should handle tasks based on the type of queue it is assigned. The tasks should have different priorities and processing times, and each worker should log its progress. Requirements: 1. Create three types of queues: - **FIFO queue** for general tasks. - **Priority queue** for high-priority tasks (lower values indicate higher priority). - **LIFO queue** for emergency tasks. 2. Implement coroutines for: - Adding tasks to each queue with random priorities. - Processing tasks from each queue with multiple worker coroutines. - Logging task completion and ensuring that all tasks are processed. 3. Ensure the following: - Tasks in the FIFO queue are processed in the order they are added. - Tasks in the Priority queue are processed based on their priority. - Tasks in the LIFO queue are processed in the reverse order they are added. 4. Implement proper completion tracking using `task_done()` and `join()`. 5. Use exceptions (`QueueEmpty`, `QueueFull`) appropriately to handle edge cases. Implementation Details: 1. **Input:** - No explicit input. Tasks and their attributes (priorities, processing times) should be generated within your implementation. 2. **Output:** - Logs of task processing details by each worker. - Summary of total tasks processed by each worker type. 3. **Constraints:** - You must use async/await for coroutine implementation. - Ensure no worker is indefinitely blocked or deadlocked. 4. **Performance:** - Ensure efficient processing and handling of tasks without unnecessary delays or resource blocking. Example: ```python import asyncio import random import time # Define worker coroutines and task management logic here async def main(): # Implement task generation, worker creation, and task management logic here pass asyncio.run(main()) ``` Tips: - Review the asyncio queues documentation to understand the methods and properties available. - Utilize random values for generating task priorities and processing times. - Test your implementation with a different number of worker coroutines and task volumes. Evaluation Criteria: - Correct implementation of asyncio queues and task processing logic. - Proper use of async/await for concurrency management. - Accurate task logging and completion tracking. - Efficient handling of different types of queues and tasks. Good luck, and happy coding!","solution":"import asyncio import random import logging from asyncio.queues import Queue, PriorityQueue, LifoQueue logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) async def add_task(queue, task_type, task_id, priority=0): logger.info(f\'Adding {task_type} task {task_id} with priority {priority}\') if task_type == \'fifo\': await queue.put((task_id, \\"FIFO Task\\")) elif task_type == \'priority\': await queue.put((priority, task_id, \\"Priority Task\\")) elif task_type == \'lifo\': await queue.put((task_id, \\"LIFO Task\\")) async def process_fifo(queue): while True: task = await queue.get() task_id, task_desc = task logger.info(f\'Processing {task_desc} {task_id}\') await asyncio.sleep(random.uniform(0.1, 0.5)) queue.task_done() logger.info(f\'Completed {task_desc} {task_id}\') async def process_priority(queue): while True: task = await queue.get() priority, task_id, task_desc = task logger.info(f\'Processing {task_desc} {task_id} with priority {priority}\') await asyncio.sleep(random.uniform(0.1, 0.5)) queue.task_done() logger.info(f\'Completed {task_desc} {task_id} with priority {priority}\') async def process_lifo(queue): while True: task = await queue.get() task_id, task_desc = task logger.info(f\'Processing {task_desc} {task_id}\') await asyncio.sleep(random.uniform(0.1, 0.5)) queue.task_done() logger.info(f\'Completed {task_desc} {task_id}\') async def main(): fifo_queue = Queue() priority_queue = PriorityQueue() lifo_queue = LifoQueue() workers = [ asyncio.create_task(process_fifo(fifo_queue)), asyncio.create_task(process_priority(priority_queue)), asyncio.create_task(process_lifo(lifo_queue)) ] for i in range(10): await add_task(fifo_queue, \'fifo\', i) await add_task(priority_queue, \'priority\', i, priority=random.randint(1, 10)) await add_task(lifo_queue, \'lifo\', i) await asyncio.sleep(5) await fifo_queue.join() await priority_queue.join() await lifo_queue.join() for worker in workers: worker.cancel() asyncio.run(main())"},{"question":"Context: You are working on a task that involves managing and monitoring system processes as well as handling environment variables and file operations. Specifically, you\'re required to create a script that performs several functions related to processes and environment management. Objectives: Your task is to implement a Python function `process_and_file_manager` that performs the following actions: 1. **File Operation**: - Create a new directory named `my_dir` in the current working directory. If the directory already exists, print a message indicating so. - Inside `my_dir`, create a file named `info.txt` and write the current process\'s ID and the name of the operating system into the file. 2. **Environment Variable Handling**: - Check if an environment variable named `TARGET_VAR` exists. - If it exists, print its value. If it does not exist, set `TARGET_VAR` to \\"default_value\\" and print the new value. 3. **Process Management**: - Create a child process using `os.fork()`. - In the child process: - Print \\"Child process\\" along with its process ID. - Read the contents of `info.txt` from `my_dir` and print it. - In the parent process: - Print \\"Parent process\\" along with its process ID. - Execute a system command to list the contents of the directory `my_dir`. Specifications: - The function should be named `process_and_file_manager`. - Any exceptions should be handled gracefully, logging relevant error messages without interrupting the program flow. - Ensure the function works on both Unix and Windows platforms (use conditions to handle platform-specific behavior). Input and Output: - **Input**: None - **Output**: The function prints the required outputs directly to the console. Example Usage: ```python process_and_file_manager() ``` # Constraints: - You must use the `os` module to perform all operations. - Use built-in file handling methods to read and write files. # Additional Notes: - You can refer to the documentation of the `os` module for more information on various functions. - Ensure your script handles edge cases where directories or files might already exist, or where environment variables might not be set.","solution":"import os import platform def process_and_file_manager(): try: # 1. File Operation if not os.path.exists(\'my_dir\'): os.mkdir(\'my_dir\') else: print(\\"Directory \'my_dir\' already exists\\") file_path = os.path.join(\'my_dir\', \'info.txt\') with open(file_path, \'w\') as f: f.write(f\\"Process ID: {os.getpid()}n\\") f.write(f\\"OS: {platform.system()}n\\") # 2. Environment Variable Handling target_var = os.getenv(\'TARGET_VAR\') if target_var is not None: print(f\\"Environment Variable TARGET_VAR: {target_var}\\") else: os.environ[\'TARGET_VAR\'] = \'default_value\' print(f\\"Set TARGET_VAR to: {os.environ[\'TARGET_VAR\']}\\") # 3. Process Management if os.name == \'posix\': pid = os.fork() if pid == 0: # Child process print(f\\"Child process ID: {os.getpid()}\\") with open(file_path, \'r\') as f: print(f\\"Child process reading \'info.txt\':n{f.read()}\\") else: # Parent process print(f\\"Parent process ID: {os.getpid()}\\") os.system(\'ls my_dir\') else: print(\\"Fork not supported on this operating system\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Uncomment below line to run the function directly # process_and_file_manager()"},{"question":"# Nullable Integer DataFrame Analysis You are given a DataFrame representing the sales data of a company. This DataFrame has columns for product IDs, quantities sold, and the sales amount in dollars. However, some of the entries in the quantities and sales amount columns are missing. Your task is to: 1. Construct the DataFrame using nullable integer data types for the quantities and set the dtype of the sales amount column manually while handling the missing values appropriately. 2. Calculate the total sales amount for each product. 3. Identify and list the product IDs with missing values in the quantities sold. 4. Fill all the missing values in the quantities sold column with the median quantity sold. 5. Perform a groupby operation to find the total quantity sold and total sales amount for each product. # Input and Output Specifications Input - A dictionary with keys representing column names and values representing the list of their respective data points. The columns are: - \'product_id\': (List of integers) Unique identifiers for the products. - \'quantity_sold\': (List of integers and None) Quantities of the product sold, which may include missing values. - \'sales_amount\': (List of floats and None) Sales amount in dollars, which may include missing values. Output - The total sales amount for each product as a pandas Series. - A list of product IDs that have missing values in the quantities sold. - The modified DataFrame after replacing missing quantities with the median value. - A DataFrame showing total quantity sold and total sales amount for each product. # Constraints - Use pandas and handle missing values according to the guidelines provided in the documentation. - Display clear output for each step. # Example Input ```python data = { \'product_id\': [101, 102, 103, 104], \'quantity_sold\': [20, None, 15, 10], \'sales_amount\': [300.0, 200.0, 150.0, None] } ``` # Example Output ```python # 1. Construct DataFrame df = construct_nullable_dataframe(data) # 2. Total sales amount for each product total_sales = calculate_total_sales(df) print(total_sales) # Output: # product_id # 101 300.0 # 102 200.0 # 103 150.0 # 104 0.0 # Name: sales_amount, dtype: float64 # 3. Product IDs with missing quantities missing_quantities_ids = list_missing_quantities(df) print(missing_quantities_ids) # Output: # [102] # 4. Fill missing quantities with median modified_df = fill_missing_quantities_with_median(df) print(modified_df) # Output: # product_id quantity_sold sales_amount # 0 101 20.0 300.0 # 1 102 15.0 200.0 # 2 103 15.0 150.0 # 3 104 10.0 0.0 # 5. Total quantity sold and total sales amount per product total_summary = calculate_summary(modified_df) print(total_summary) # Output: # product_id total_quantity_sold total_sales_amount # 0 101 20 300.0 # 1 102 15 200.0 # 2 103 15 150.0 # 3 104 10 0.0 ``` # Function Definitions Implement the following functions: - `construct_nullable_dataframe(data: dict) -> pd.DataFrame` - `calculate_total_sales(df: pd.DataFrame) -> pd.Series` - `list_missing_quantities(df: pd.DataFrame) -> List[int]` - `fill_missing_quantities_with_median(df: pd.DataFrame) -> pd.DataFrame` - `calculate_summary(df: pd.DataFrame) -> pd.DataFrame` Ensure to handle missing data appropriately using nullable integer types in pandas as demonstrated in the documentation.","solution":"import pandas as pd import numpy as np def construct_nullable_dataframe(data): Constructs a DataFrame from input data with nullable integers for quantities and handles missing values for sales amounts. Args: data (dict): A dictionary containing product_id, quantity_sold, and sales_amount. Returns: pd.DataFrame: A DataFrame with appropriate dtypes and missing value handling. df = pd.DataFrame(data) df[\'quantity_sold\'] = df[\'quantity_sold\'].astype(\'Int64\') df[\'sales_amount\'] = df[\'sales_amount\'].fillna(0.0) return df def calculate_total_sales(df): Calculates the total sales amount for each product. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: pd.Series: A Series with product_id as index and total sales amount as values. total_sales = df.groupby(\'product_id\')[\'sales_amount\'].sum() return total_sales def list_missing_quantities(df): Identifies product IDs with missing quantities sold. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: list: A list of product IDs with missing quantities sold. missing_quantities_ids = df[df[\'quantity_sold\'].isna()][\'product_id\'].tolist() return missing_quantities_ids def fill_missing_quantities_with_median(df): Fills missing quantities sold with the median quantity. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: pd.DataFrame: The modified DataFrame with missing quantities filled with median value. median_quantity = df[\'quantity_sold\'].median() df[\'quantity_sold\'] = df[\'quantity_sold\'].fillna(median_quantity) return df def calculate_summary(df): Performs a groupby operation to find total quantity sold and total sales amount for each product. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: pd.DataFrame: A DataFrame showing total quantity sold and total sales amount for each product. summary = df.groupby(\'product_id\').agg( total_quantity_sold=(\'quantity_sold\', \'sum\'), total_sales_amount=(\'sales_amount\', \'sum\') ).reset_index() return summary"},{"question":"You are given a dataset regarding diabetes patients. Your task is to: 1. Train a Ridge regression model on this dataset to predict a target variable. 2. Utilize the `permutation_importance` function from `scikit-learn` to calculate the importance of each feature based on the trained model. 3. Identify and interpret the top three most important features for the model based on their permutation importance scores. Input - A dataset with features (`X`) and target (`y`), already split into training and validation sets: `X_train`, `X_val`, `y_train`, `y_val`. Output - A list of the top three most important features along with their mean importance and standard deviation scores. Example format: ```python [ {\'feature_name\': \'bmi\', \'mean_importance\': 0.204, \'std_importance\': 0.050}, {\'feature_name\': \'bp\', \'mean_importance\': 0.176, \'std_importance\': 0.048}, {\'feature_name\': \'s5\', \'mean_importance\': 0.088, \'std_importance\': 0.033} ] ``` Constraints - Use the `Ridge` regression model from `scikit-learn` with `alpha=1e-2`. - Set `n_repeats` to 30 and `random_state` to 0 when using the `permutation_importance` function. - Ensure performance evaluation is done on the validation set `X_val` and `y_val`. Code Template ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance # Load the diabetes dataset diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(diabetes.data, diabetes.target, random_state=0) # Train the Ridge regression model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Compute permutation feature importance results = permutation_importance(model, X_val, y_val, n_repeats=30, random_state=0) # Extract feature names, mean importance, and importance standard deviation feature_importances = [] for i in results.importances_mean.argsort()[::-1][:3]: feature_importances.append({ \'feature_name\': diabetes.feature_names[i], \'mean_importance\': results.importances_mean[i], \'std_importance\': results.importances_std[i] }) # Output the top three most important features print(feature_importances) ``` Complete the implementation as specified and provide the top three feature importances based on the trained model.","solution":"from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance def get_top_three_important_features(): # Load the diabetes dataset diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(diabetes.data, diabetes.target, random_state=0) # Train the Ridge regression model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Compute permutation feature importance results = permutation_importance(model, X_val, y_val, n_repeats=30, random_state=0) # Extract feature names, mean importance, and importance standard deviation feature_importances = [] for i in results.importances_mean.argsort()[::-1][:3]: feature_importances.append({ \'feature_name\': diabetes.feature_names[i], \'mean_importance\': results.importances_mean[i], \'std_importance\': results.importances_std[i] }) # Return the top three most important features return feature_importances"},{"question":"# Seaborn Advanced Plotting Assessment You are required to demonstrate your understanding of seaborn by performing the following tasks on the penguins dataset. The dataset is loaded using `seaborn.load_dataset(\\"penguins\\")`. 1. **Data Preparation:** Handle any missing data in the dataset by removing rows with any missing values. 2. **Faceted Plot Creation:** - Create a faceted plot that shows `bill_length_mm` versus `bill_depth_mm`, faceted by the columns `species` and rows `sex`. - Ensure that each subplot has independent scales for the x and y axes (i.e., do not share axes). 3. **Display the Plot:** Show the faceted plot created in step 2. 4. **Paired Plot Creation:** - Create a paired plot that shows `bill_length_mm` and `bill_depth_mm` on the x-axes against `flipper_length_mm` on the y-axis. - Ensure that the x-axes are shared across the paired plots. 5. **Display the Paired Plot:** Show the paired plot created in step 4. # Input and Output - **Input:** The `penguins` dataset loaded using `seaborn.load_dataset(\\"penguins\\")`. - **Output:** Two plots (faceted plot and paired plot) displayed using matplotlib. Ensure your solution is well-documented with comments explaining each major step. Your code should be efficient and make use of seaborn\'s functionalities effectively. # Constraints: - You must use the seaborn library for creating and customizing the plots. - Handle any missing values in the dataset before making the plots. - Your solution should be contained within a single Python script or Jupyter Notebook cell. **Performance Requirement:** The script should execute without errors, and the plots should be generated and displayed correctly. ```python # Example structure for your implementation import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Data Preparation: Remove rows with missing values # 2. Faceted Plot Creation and display # 3. Paired Plot Creation and display # Ensure the plots are shown plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Data Preparation: Remove rows with missing values penguins.dropna(inplace=True) # 2. Faceted Plot Creation # Facet plot with bill_length_mm vs bill_depth_mm faceted by species and sex g = sns.FacetGrid(penguins, col=\\"species\\", row=\\"sex\\", margin_titles=True) g.map(plt.scatter, \\"bill_length_mm\\", \\"bill_depth_mm\\") # Display the faceted plot plt.show() # 3. Paired Plot Creation # Paired plot with flipper_length_mm on y-axis and bill_length_mm and bill_depth_mm on x-axes sns.pairplot(penguins, y_vars=[\\"flipper_length_mm\\"], x_vars=[\\"bill_length_mm\\", \\"bill_depth_mm\\"], height=3, aspect=1.2) # Display the paired plot plt.show()"},{"question":"# PyTorch Coding Assessment Question **Objective:** You need to implement a function using PyTorch that performs a series of tensor operations including tensor creation, arithmetic operations, and optionally involving gradient computation. **Function Signature:** ```python import torch def complex_tensor_operation(a: float, b: float, size: int, requires_grad: bool) -> dict: Args: - a (float): A scalar value to use in tensor operations. - b (float): Another scalar value to use in tensor operations. - size (int): The size of the tensor to be created. - requires_grad (bool): A boolean indicating if the operations should track gradients. Returns: - dict: A dictionary containing the following keys and their corresponding PyTorch tensor values: - \'initial\': Initial tensor filled with random numbers. - \'transformed\': Tensor after applying transformations using \'a\' and \'b\'. - \'gradient\': (If requires_grad is True) Gradients of the transformed tensor. pass ``` **Instructions:** 1. Implement the `complex_tensor_operation` function. 2. Create an initial tensor of size `(size, size)` filled with random numbers drawn from a normal distribution. 3. If `requires_grad` is `True`, ensure that the initial tensor requires gradient computation. 4. Create a transformed tensor by applying the following operations: - Multiply the initial tensor by `a`. - Add `b` to the result. 5. If `requires_grad` is `True`, compute the gradient of the sum of all elements in the transformed tensor with respect to the initial tensor. 6. Return a dictionary containing: - The initial tensor (`\'initial\'`). - The transformed tensor (`\'transformed\'`). - If gradient calculation is required, include the gradients (`\'gradient\'`). **Example Usage:** ```python result = complex_tensor_operation(a=2.0, b=1.0, size=3, requires_grad=True) print(\\"Initial Tensor:n\\", result[\'initial\']) print(\\"Transformed Tensor:n\\", result[\'transformed\']) if \'gradient\' in result: print(\\"Gradient:n\\", result[\'gradient\']) ``` **Constraints:** - You must use PyTorch for all tensor operations. - Ensure code handles the gradient computation correctly based on the `requires_grad` flag. **Performance Requirements:** - The function should be efficient and able to handle reasonably large tensor sizes (e.g., size = 1000). **Additional Notes:** - Pay attention to PyTorch tensor properties such as `.requires_grad`. - Use appropriate PyTorch functions and methods to achieve the desired operations.","solution":"import torch def complex_tensor_operation(a: float, b: float, size: int, requires_grad: bool) -> dict: Args: - a (float): A scalar value to use in tensor operations. - b (float): Another scalar value to use in tensor operations. - size (int): The size of the tensor to be created. - requires_grad (bool): A boolean indicating if the operations should track gradients. Returns: - dict: A dictionary containing the following keys and their corresponding PyTorch tensor values: - \'initial\': Initial tensor filled with random numbers. - \'transformed\': Tensor after applying transformations using \'a\' and \'b\'. - \'gradient\': (If requires_grad is True) Gradients of the transformed tensor. # Step 1: Create an initial tensor of size (size, size) filled with random numbers drawn from a normal distribution initial_tensor = torch.randn(size, size, requires_grad=requires_grad) # Step 2: Create the transformed tensor transformed_tensor = initial_tensor * a + b # Result dictionary result = { \'initial\': initial_tensor, \'transformed\': transformed_tensor } # Step 3: Compute the gradient of the sum of all elements in the transformed tensor if required if requires_grad: transformed_tensor.sum().backward() result[\'gradient\'] = initial_tensor.grad return result"},{"question":"**Question: Efficient Data Management with pandas** You are provided with a set of `.parquet` files representing a large logical dataset. Each file contains timestamped records with various attributes. Your task is to load these files, perform some basic data transformations, and optimize memory usage. # Objective Write a function `optimize_data_processing` that: 1. Reads all `.parquet` files from a given directory. 2. Combines them into a single DataFrame. 3. Optimizes the memory usage by: - Converting suitable columns to `pandas.Categorical`. - Downcasting numeric columns to their most memory-efficient types. 4. Returns the optimized DataFrame. # Input - `directory_path`: A string representing the path to the directory containing the `.parquet` files. # Output - A pandas DataFrame with memory optimizations applied. # Constraints - Assume each `.parquet` file has the same schema. - The DataFrame schema includes: ``` timestamp (datetime64), name (str), id (int), x (float64), y (float64) ``` # Example ```python import pandas as pd import pathlib # Example data preparation N = 2 starts = [f\\"20{i:>02d}-01-01\\" for i in range(N)] ends = [f\\"20{i:>02d}-12-31\\" for i in range(N)] pathlib.Path(\\"example_data/timeseries\\").mkdir(parents=True, exist_ok=True) for i, (start, end) in enumerate(zip(starts, ends)): df = make_timeseries(start=start, end=end, freq=\\"1min\\", seed=i) df.to_parquet(f\\"example_data/timeseries/ts-{i:0>2d}.parquet\\") directory_path = \\"example_data/timeseries\\" # Function call optimized_df = optimize_data_processing(directory_path) # Check memory usage print(optimized_df.memory_usage(deep=True)) ``` # Implementation Complete the function `optimize_data_processing`: ```python import pandas as pd import pathlib def optimize_data_processing(directory_path): paths = pathlib.Path(directory_path).glob(\\"*.parquet\\") # Read and concatenate all parquet files into a single DataFrame dataframes = [pd.read_parquet(path) for path in paths] combined_df = pd.concat(dataframes, ignore_index=True) # Optimize memory usage # Convert \'name\' column to pandas Categorical combined_df[\'name\'] = combined_df[\'name\'].astype(\'category\') # Downcast numeric columns to the smallest data types combined_df[\'id\'] = pd.to_numeric(combined_df[\'id\'], downcast=\'unsigned\') combined_df[[\'x\', \'y\']] = combined_df[[\'x\', \'y\']].apply(pd.to_numeric, downcast=\'float\') return combined_df ```","solution":"import pandas as pd import pathlib import os def optimize_data_processing(directory_path): paths = pathlib.Path(directory_path).glob(\\"*.parquet\\") # Read and concatenate all parquet files into a single DataFrame dataframes = [pd.read_parquet(path) for path in paths] combined_df = pd.concat(dataframes, ignore_index=True) # Optimize memory usage # Convert \'name\' column to pandas Categorical combined_df[\'name\'] = combined_df[\'name\'].astype(\'category\') # Downcast numeric columns to the smallest data types combined_df[\'id\'] = pd.to_numeric(combined_df[\'id\'], downcast=\'unsigned\') combined_df[[\'x\', \'y\']] = combined_df[[\'x\', \'y\']].apply(pd.to_numeric, downcast=\'float\') return combined_df"},{"question":"# Question: Implementing Supported Operations for TorchScript In this task, you need to implement a PyTorch function that is compatible with TorchScript. Due to the constraints of TorchScript, your implementation should avoid unsupported functions and adhere to the compatible API. Write a function `process_tensor` which accepts two 1D tensors `a` and `b` and performs the following steps: 1. Concatenate `a` and `b` into a single tensor. 2. Reshape the concatenated tensor to a 2D tensor with shape `(2, len(a))`. 3. Compute the element-wise product of `a` and `b` and return it as a 1D tensor. You must ensure that your function adheres to TorchScript limitations. Specifically, avoid using any functions listed as unsupported or with divergent schemas. Input - `a (torch.Tensor)`: A 1-dimensional tensor with shape `(n,)`. - `b (torch.Tensor)`: A 1-dimensional tensor with shape `(n,)`. Output - `torch.Tensor`: A 1-dimensional tensor containing the element-wise product of `a` and `b`. Example ```python import torch a = torch.tensor([1, 2, 3, 4]) b = torch.tensor([5, 6, 7, 8]) result = process_tensor(a, b) print(result) # Output: tensor([ 5, 12, 21, 32]) ``` Constraints - Ensure compatibility with TorchScript. - Do not use any unsupported operations or functions with divergent schemas as listed in the provided documentation. Performance - The function should execute efficiently for tensors with length up to 10^6. ```python import torch def process_tensor(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: # Step 1: Concatenate a and b concatenated = torch.cat((a, b)) # Step 2: Reshape the concatenated tensor to (2, len(a)) reshaped = concatenated.view(2, -1) # Step 3: Compute the element-wise product and return as 1D tensor element_wise_product = a * b return element_wise_product # Example usage a = torch.tensor([1, 2, 3, 4]) b = torch.tensor([5, 6, 7, 8]) result = process_tensor(a, b) print(result) # Output: tensor([ 5, 12, 21, 32]) ```","solution":"import torch def process_tensor(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Processes the tensors by concatenating them, reshaping, and returning the element-wise product. Args: a (torch.Tensor): A 1-dimensional tensor. b (torch.Tensor): A 1-dimensional tensor. Returns: torch.Tensor: A 1-dimensional tensor containing the element-wise product of a and b. # Step 1: Concatenate a and b concatenated = torch.cat((a, b)) # Step 2: Reshape the concatenated tensor to (2, len(a)) reshaped = concatenated.view(2, -1) # Step 3: Compute the element-wise product and return as 1D tensor element_wise_product = a * b return element_wise_product # Example usage a = torch.tensor([1, 2, 3, 4]) b = torch.tensor([5, 6, 7, 8]) result = process_tensor(a, b) print(result) # Output: tensor([ 5, 12, 21, 32])"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},q={class:"search-container"},z={class:"card-container"},R={key:0,class:"empty-state"},F=["disabled"],L={key:0},N={key:1};function M(s,e,l,m,n,o){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",q,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," âœ• ")):d("",!0)]),t("div",z,[(a(!0),i(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),i("div",R,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),i("span",N,"Loading...")):(a(),i("span",L,"See more"))],8,F)):d("",!0)])}const O=p(D,[["render",M],["__scopeId","data-v-d6398c41"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/62.md","filePath":"chatai/62.md"}'),j={name:"chatai/62.md"},H=Object.assign(j,{setup(s){return(e,l)=>(a(),i("div",null,[x(O)]))}});export{Y as __pageData,H as default};
